2024-07-09 16:59:23 root INFO     loading model + tokenizer
2024-07-09 16:59:40 root INFO     model + tokenizer loaded
2024-07-09 16:59:54 root INFO     loading model + tokenizer
2024-07-09 17:00:12 root INFO     model + tokenizer loaded
2024-07-09 17:00:12 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on noun - plural_irreg
2024-07-09 17:00:15 root INFO     building operator noun - plural_irreg
2024-07-09 17:00:15 root INFO     [order_1_approx] starting weight calculation for The plural form of memory is memories
The plural form of agency is agencies
The plural form of strategy is strategies
The plural form of policy is policies
The plural form of analysis is analyses
The plural form of army is armies
The plural form of species is species
The plural form of variety is
2024-07-09 17:00:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 17:04:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 17:08:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8389, -0.5762, -0.2886,  ..., -0.9526, -0.3254,  0.5820],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.8018, -0.5864, -0.2776,  ..., -0.8755, -0.3289,  0.5454],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.6230, -0.2529, -2.7012,  ...,  1.7754,  0.4873,  3.7754],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-5.4779e-03, -2.6779e-03,  7.3929e-03,  ..., -7.2670e-03,
         -6.9733e-03, -5.0240e-03],
        [-1.0490e-02,  3.2482e-03, -4.3411e-03,  ...,  9.2850e-03,
         -2.6398e-02, -5.1956e-03],
        [ 1.1398e-02, -5.5466e-03, -1.7120e-02,  ..., -2.0981e-03,
          1.6556e-02, -5.5885e-03],
        ...,
        [-8.1253e-03, -7.8506e-03,  1.1032e-02,  ...,  4.1962e-03,
         -2.2202e-03, -1.2131e-03],
        [-7.2002e-05, -1.1703e-02, -5.3406e-03,  ..., -2.3254e-02,
         -2.5513e-02,  4.5853e-03],
        [-1.2726e-02,  1.9531e-02,  2.4223e-04,  ..., -1.5137e-02,
          1.2207e-04, -2.2888e-04]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0007,  0.0006,  0.0002,  ..., -0.0027,  0.0018,  0.0014],
        [-0.0015, -0.0034,  0.0005,  ...,  0.0006, -0.0021, -0.0012],
        [ 0.0016,  0.0010, -0.0011,  ..., -0.0002,  0.0030,  0.0036],
        ...,
        [ 0.0003, -0.0006,  0.0013,  ...,  0.0008,  0.0017, -0.0015],
        [ 0.0002,  0.0017,  0.0014,  ..., -0.0007, -0.0007, -0.0009],
        [-0.0015,  0.0012, -0.0010,  ...,  0.0012, -0.0003,  0.0008]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.6484,  0.6528, -3.3164,  ...,  1.3594,  0.3452,  3.1211]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1027,  0.0669, -0.2244,  ..., -0.2252, -0.0676, -0.2644]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 17:08:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of memory is memories
The plural form of agency is agencies
The plural form of strategy is strategies
The plural form of policy is policies
The plural form of analysis is analyses
The plural form of army is armies
The plural form of species is species
The plural form of variety is
2024-07-09 17:08:38 root INFO     [order_1_approx] starting weight calculation for The plural form of species is species
The plural form of strategy is strategies
The plural form of analysis is analyses
The plural form of agency is agencies
The plural form of variety is varieties
The plural form of army is armies
The plural form of memory is memories
The plural form of policy is
2024-07-09 17:08:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 17:12:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 17:17:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2949, -0.6943, -0.2212,  ...,  0.5889,  0.3401, -0.2549],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 1.2129, -0.6851, -0.2095,  ...,  0.5332,  0.2957, -0.2678],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8765, -1.8877, -1.0332,  ...,  1.7119,  0.3813, -2.1172],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0033, -0.0045,  0.0016,  ..., -0.0056, -0.0181,  0.0055],
        [-0.0140, -0.0140, -0.0010,  ...,  0.0090, -0.0164, -0.0072],
        [ 0.0077, -0.0154, -0.0128,  ...,  0.0007,  0.0152,  0.0068],
        ...,
        [-0.0179, -0.0131, -0.0105,  ...,  0.0051,  0.0239, -0.0129],
        [-0.0143, -0.0014,  0.0106,  ..., -0.0043, -0.0302,  0.0154],
        [ 0.0023,  0.0005, -0.0004,  ...,  0.0056,  0.0185, -0.0141]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.3866e-03, -2.9850e-03,  3.5238e-04,  ..., -2.4719e-03,
         -8.2207e-04,  1.3885e-03],
        [ 3.8576e-04, -1.4420e-03,  1.5726e-03,  ...,  1.6994e-03,
         -8.6355e-04, -1.4238e-03],
        [-3.3913e-03, -9.0981e-04, -1.4439e-03,  ...,  2.5082e-04,
         -2.8563e-04,  6.1321e-04],
        ...,
        [ 1.2255e-04, -2.3785e-03,  8.3303e-04,  ..., -2.3437e-04,
          3.2921e-03, -2.0838e-04],
        [-3.0594e-03,  1.1139e-03, -2.3007e-05,  ..., -2.0790e-03,
         -4.0512e-03, -9.7418e-04],
        [-4.5443e-04,  3.0565e-04,  6.4850e-05,  ...,  8.8406e-04,
          7.0858e-04, -7.7963e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7324, -1.3594, -1.2285,  ...,  1.1016,  1.0664, -2.1875]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0784, -0.0836,  0.1650,  ..., -0.0553,  0.2303,  0.0329]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 17:17:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of species is species
The plural form of strategy is strategies
The plural form of analysis is analyses
The plural form of agency is agencies
The plural form of variety is varieties
The plural form of army is armies
The plural form of memory is memories
The plural form of policy is
2024-07-09 17:17:04 root INFO     [order_1_approx] starting weight calculation for The plural form of variety is varieties
The plural form of species is species
The plural form of policy is policies
The plural form of analysis is analyses
The plural form of agency is agencies
The plural form of strategy is strategies
The plural form of army is armies
The plural form of memory is
2024-07-09 17:17:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 17:21:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 17:25:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.6738, -0.6855, -0.2878,  ...,  0.3408, -0.1709,  0.5103],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 1.5635, -0.6753, -0.2690,  ...,  0.3088, -0.1770,  0.4626],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.0742,  0.9531, -3.5449,  ..., -0.1592, -2.9844,  4.8047],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0034, -0.0129,  0.0151,  ..., -0.0127,  0.0032, -0.0054],
        [-0.0063,  0.0094, -0.0005,  ...,  0.0075, -0.0057,  0.0228],
        [ 0.0119,  0.0074, -0.0044,  ...,  0.0009,  0.0297, -0.0048],
        ...,
        [-0.0086, -0.0248,  0.0020,  ..., -0.0039,  0.0082, -0.0166],
        [ 0.0030, -0.0123,  0.0046,  ...,  0.0121, -0.0100, -0.0029],
        [-0.0157,  0.0128,  0.0008,  ..., -0.0027,  0.0003,  0.0009]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 2.8801e-04, -3.7313e-04,  2.7084e-04,  ..., -2.3422e-03,
         -1.2932e-03,  4.5276e-04],
        [-9.7370e-04, -1.2321e-03, -2.4152e-04,  ..., -1.8921e-03,
          5.7173e-04, -8.4114e-04],
        [-2.3890e-04, -4.5109e-04,  1.9884e-04,  ..., -9.8896e-04,
         -9.5367e-04,  8.2874e-04],
        ...,
        [-2.0580e-03, -4.1580e-04, -3.9387e-04,  ..., -6.5517e-04,
         -8.6498e-04,  9.8228e-05],
        [-1.5049e-03, -1.5650e-03,  1.3580e-03,  ..., -5.1880e-04,
         -2.5320e-04, -1.8702e-03],
        [-7.1716e-04,  1.5011e-03,  8.6212e-04,  ..., -5.0068e-05,
          1.0986e-03,  6.0034e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.9141,  1.1719, -3.7910,  ..., -1.2812, -3.0840,  4.6602]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.2551, -0.0315,  0.1522,  ...,  0.2246,  0.1538,  0.4089]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 17:25:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of variety is varieties
The plural form of species is species
The plural form of policy is policies
The plural form of analysis is analyses
The plural form of agency is agencies
The plural form of strategy is strategies
The plural form of army is armies
The plural form of memory is
2024-07-09 17:25:27 root INFO     [order_1_approx] starting weight calculation for The plural form of variety is varieties
The plural form of analysis is analyses
The plural form of species is species
The plural form of memory is memories
The plural form of army is armies
The plural form of policy is policies
The plural form of strategy is strategies
The plural form of agency is
2024-07-09 17:25:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 17:29:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 17:33:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7305, -1.1211, -0.1273,  ..., -1.1309, -0.4150,  0.7632],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.7139, -1.1484, -0.1322,  ..., -1.0635, -0.4224,  0.7402],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.5078,  1.9453, -1.5752,  ..., -2.1758, -0.3252,  4.0664],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0094, -0.0105,  0.0041,  ...,  0.0102,  0.0073, -0.0075],
        [-0.0077, -0.0116,  0.0068,  ...,  0.0106,  0.0080, -0.0078],
        [ 0.0084,  0.0079, -0.0075,  ..., -0.0053,  0.0159, -0.0060],
        ...,
        [ 0.0086, -0.0125,  0.0063,  ..., -0.0116, -0.0106, -0.0021],
        [ 0.0046, -0.0129,  0.0196,  ...,  0.0014, -0.0190,  0.0147],
        [-0.0076,  0.0091,  0.0022,  ...,  0.0105,  0.0093,  0.0042]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 1.0777e-03, -1.3599e-03,  6.1798e-04,  ..., -5.5027e-04,
          7.6580e-04, -1.1978e-03],
        [-2.2736e-03, -1.9875e-03,  3.8910e-04,  ..., -1.1325e-06,
         -3.0937e-03, -1.2684e-03],
        [ 2.2769e-04,  2.9778e-04, -1.2264e-03,  ...,  6.6471e-04,
          1.2474e-03,  2.7676e-03],
        ...,
        [ 1.7920e-03,  7.2575e-04,  8.2922e-04,  ..., -4.0007e-04,
          5.9032e-04,  1.7238e-04],
        [-4.6873e-04, -1.4305e-04,  1.7071e-03,  ...,  1.7424e-03,
         -1.5182e-03, -1.0252e-03],
        [-4.4785e-03, -1.3943e-03,  8.1444e-04,  ...,  8.4543e-04,
         -7.6962e-04,  9.7275e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.5938,  2.2109, -1.9004,  ..., -1.5117,  0.6064,  4.0273]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0290, -0.1193,  0.2245,  ...,  0.0024, -0.0932,  0.1532]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 17:33:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of variety is varieties
The plural form of analysis is analyses
The plural form of species is species
The plural form of memory is memories
The plural form of army is armies
The plural form of policy is policies
The plural form of strategy is strategies
The plural form of agency is
2024-07-09 17:33:51 root INFO     [order_1_approx] starting weight calculation for The plural form of variety is varieties
The plural form of memory is memories
The plural form of strategy is strategies
The plural form of species is species
The plural form of agency is agencies
The plural form of policy is policies
The plural form of army is armies
The plural form of analysis is
2024-07-09 17:33:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 17:38:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 17:42:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0206, -0.8799, -0.3848,  ..., -0.6436, -0.1079,  0.8335],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.0243, -0.8613, -0.3557,  ..., -0.5737, -0.1192,  0.7705],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6479,  0.3330, -0.7993,  ..., -1.1553,  2.1445,  3.1914],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0195, -0.0092,  0.0121,  ..., -0.0179, -0.0052, -0.0083],
        [-0.0117, -0.0032,  0.0055,  ..., -0.0053, -0.0186,  0.0026],
        [-0.0030, -0.0007, -0.0194,  ...,  0.0111,  0.0175,  0.0145],
        ...,
        [-0.0066, -0.0021, -0.0101,  ..., -0.0002,  0.0247, -0.0061],
        [-0.0101,  0.0038,  0.0038,  ...,  0.0022, -0.0303,  0.0045],
        [ 0.0205,  0.0152,  0.0118,  ...,  0.0061,  0.0092, -0.0088]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 5.3358e-04, -4.7531e-03,  1.0939e-03,  ..., -1.6422e-03,
          5.1498e-04,  1.2150e-03],
        [ 2.2297e-03, -1.0195e-03,  7.5722e-04,  ..., -1.7405e-04,
         -1.0228e-04, -1.2894e-03],
        [-2.4080e-04,  8.9884e-05, -8.0013e-04,  ...,  3.5906e-04,
         -1.7703e-04,  1.5697e-03],
        ...,
        [-8.6880e-04,  1.1339e-03, -3.0637e-04,  ..., -1.6928e-04,
          2.4891e-03,  6.8951e-04],
        [-1.2131e-03, -1.1225e-03, -1.6594e-04,  ...,  1.3981e-03,
         -1.1272e-03, -1.4234e-04],
        [-1.1587e-03,  1.4610e-03,  1.3123e-03,  ...,  1.3571e-03,
          7.9918e-04, -8.7547e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3193,  0.6152, -0.5010,  ..., -1.2266,  2.7109,  1.8604]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1169, -0.0440, -0.0980,  ..., -0.1639,  0.0729,  0.0421]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 17:42:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of variety is varieties
The plural form of memory is memories
The plural form of strategy is strategies
The plural form of species is species
The plural form of agency is agencies
The plural form of policy is policies
The plural form of army is armies
The plural form of analysis is
2024-07-09 17:42:14 root INFO     [order_1_approx] starting weight calculation for The plural form of policy is policies
The plural form of analysis is analyses
The plural form of memory is memories
The plural form of variety is varieties
The plural form of agency is agencies
The plural form of strategy is strategies
The plural form of army is armies
The plural form of species is
2024-07-09 17:42:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 17:46:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 17:50:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3718, -0.2078,  0.1211,  ...,  0.9536,  0.5371,  0.3538],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3689, -0.2327,  0.1034,  ...,  0.9209,  0.5117,  0.3376],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1227, -2.3828, -0.0781,  ...,  1.3545, -1.9141,  2.7227],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0249, -0.0185,  0.0099,  ..., -0.0002, -0.0189, -0.0005],
        [ 0.0094, -0.0041,  0.0062,  ...,  0.0090, -0.0160,  0.0032],
        [ 0.0019, -0.0012, -0.0059,  ...,  0.0058,  0.0123, -0.0121],
        ...,
        [ 0.0048, -0.0087,  0.0021,  ...,  0.0012,  0.0241, -0.0024],
        [ 0.0030, -0.0047,  0.0036,  ...,  0.0115, -0.0177,  0.0039],
        [-0.0063,  0.0046, -0.0024,  ...,  0.0021,  0.0098, -0.0073]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.7313e-03,  6.9380e-05,  5.2452e-04,  ..., -8.6355e-04,
         -1.0242e-03,  1.8063e-03],
        [-2.5034e-04, -2.1076e-03,  1.4467e-03,  ...,  1.5745e-03,
         -1.4639e-04, -1.4629e-03],
        [-9.6369e-04,  2.3193e-03, -2.1839e-04,  ...,  6.4969e-05,
          1.3218e-03,  1.0519e-03],
        ...,
        [ 9.3937e-04, -9.5177e-04,  4.2272e-04,  ..., -1.0681e-04,
          4.1161e-03, -1.1311e-03],
        [-4.4465e-05,  7.4387e-04, -2.8968e-05,  ..., -3.8552e-04,
         -7.6914e-04, -7.5054e-04],
        [-3.6411e-03, -7.1144e-04,  1.1244e-03,  ...,  1.0262e-03,
          1.3790e-03, -1.5335e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3442, -0.9912,  0.0886,  ...,  1.0703, -1.7383,  2.5000]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0137, -0.1857,  0.1471,  ..., -0.2306,  0.0979, -0.2600]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 17:50:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of policy is policies
The plural form of analysis is analyses
The plural form of memory is memories
The plural form of variety is varieties
The plural form of agency is agencies
The plural form of strategy is strategies
The plural form of army is armies
The plural form of species is
2024-07-09 17:50:39 root INFO     [order_1_approx] starting weight calculation for The plural form of variety is varieties
The plural form of species is species
The plural form of policy is policies
The plural form of memory is memories
The plural form of agency is agencies
The plural form of analysis is analyses
The plural form of army is armies
The plural form of strategy is
2024-07-09 17:50:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 17:54:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 17:59:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9595, -0.1892, -0.6040,  ...,  0.4995, -0.0432, -0.6807],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.8657, -0.1945, -0.5352,  ...,  0.4365, -0.0588, -0.6548],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.2607, -0.5874,  1.0020,  ...,  4.6016, -0.8203,  1.5869],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0059, -0.0108,  0.0077,  ...,  0.0036, -0.0069, -0.0027],
        [ 0.0029, -0.0022,  0.0072,  ...,  0.0070, -0.0174,  0.0020],
        [-0.0075, -0.0194, -0.0109,  ..., -0.0162,  0.0257,  0.0044],
        ...,
        [-0.0118, -0.0188, -0.0065,  ..., -0.0010,  0.0055, -0.0023],
        [-0.0164, -0.0157,  0.0028,  ..., -0.0044, -0.0176,  0.0045],
        [-0.0045, -0.0080, -0.0028,  ..., -0.0145,  0.0141,  0.0033]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-7.0763e-04, -1.1101e-03,  1.0185e-03,  ..., -9.1743e-04,
         -1.3084e-03,  5.8985e-04],
        [ 2.6169e-03, -3.2973e-04,  2.7714e-03,  ...,  2.2564e-03,
         -1.0071e-03, -1.2245e-03],
        [-1.9226e-03, -2.1648e-03, -1.9445e-03,  ..., -7.2479e-04,
         -9.5367e-07,  1.4429e-03],
        ...,
        [ 3.2020e-04,  1.0786e-03,  1.0920e-03,  ...,  2.6436e-03,
          1.3709e-04, -1.6212e-05],
        [-1.9608e-03, -9.4128e-04, -2.4319e-05,  ...,  3.0518e-05,
          3.6407e-04, -1.4610e-03],
        [-4.5872e-04, -9.0790e-04, -7.4100e-04,  ...,  5.2357e-04,
         -7.6962e-04,  2.8777e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7549,  0.0718,  0.9292,  ...,  4.4570, -0.0591,  1.4287]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1321, -0.2170,  0.1370,  ..., -0.2688,  0.0408, -0.0208]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 17:59:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of variety is varieties
The plural form of species is species
The plural form of policy is policies
The plural form of memory is memories
The plural form of agency is agencies
The plural form of analysis is analyses
The plural form of army is armies
The plural form of strategy is
2024-07-09 17:59:04 root INFO     [order_1_approx] starting weight calculation for The plural form of policy is policies
The plural form of memory is memories
The plural form of agency is agencies
The plural form of strategy is strategies
The plural form of variety is varieties
The plural form of species is species
The plural form of analysis is analyses
The plural form of army is
2024-07-09 17:59:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 18:03:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 18:07:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4590, -0.1393, -0.2522,  ...,  0.0579,  0.3608,  1.3906],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.4590, -0.1592, -0.2500,  ...,  0.0577,  0.3318,  1.3779],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3633,  1.3330, -2.5371,  ..., -2.1211, -0.1895,  2.5547],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-3.8910e-03, -6.1760e-03,  1.2909e-02,  ...,  5.2872e-03,
         -3.4962e-03, -1.1269e-02],
        [-1.8631e-02, -8.7891e-03,  8.8692e-04,  ...,  1.4206e-02,
         -2.1301e-02,  1.9436e-03],
        [-1.0231e-02,  3.4370e-03,  1.4160e-02,  ..., -7.1526e-05,
          1.4679e-02, -2.5215e-03],
        ...,
        [-4.0131e-03, -4.5128e-03, -1.2665e-03,  ..., -8.4839e-03,
          4.3335e-03,  8.1635e-03],
        [-1.4191e-02,  1.5259e-03, -2.3994e-03,  ...,  4.7455e-03,
         -3.4637e-03,  2.0390e-03],
        [-1.8244e-03,  7.4234e-03,  4.6539e-04,  ..., -6.9122e-03,
         -1.1044e-03,  1.0178e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.6212e-03,  3.2949e-04,  1.7509e-03,  ...,  1.8120e-05,
          3.2949e-04,  3.3927e-04],
        [-2.3198e-04,  6.6948e-04,  7.8917e-05,  ..., -1.0681e-03,
         -2.4090e-03, -1.0242e-03],
        [-3.3436e-03, -1.5478e-03,  1.6346e-03,  ...,  1.7595e-04,
          1.3418e-03,  9.5987e-04],
        ...,
        [-2.2144e-03, -9.0694e-04,  4.7731e-04,  ..., -1.1721e-03,
          2.3365e-03,  7.7677e-04],
        [-2.5043e-03,  5.5027e-04,  7.8821e-04,  ..., -1.3399e-04,
         -1.5736e-03, -2.4557e-04],
        [ 1.6451e-04,  5.4836e-05,  1.0538e-03,  ...,  2.4700e-04,
         -1.5182e-03,  5.9128e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7002,  1.4004, -3.1797,  ..., -2.4648, -0.4958,  3.0273]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1243,  0.0909, -0.1740,  ..., -0.0706,  0.0183,  0.0965]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 18:07:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of policy is policies
The plural form of memory is memories
The plural form of agency is agencies
The plural form of strategy is strategies
The plural form of variety is varieties
The plural form of species is species
The plural form of analysis is analyses
The plural form of army is
2024-07-09 18:07:29 root INFO     total operator prediction time: 4034.003841638565 seconds
2024-07-09 18:07:29 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on Ving - verb_inf
2024-07-09 18:07:31 root INFO     building operator Ving - verb_inf
2024-07-09 18:07:31 root INFO     [order_1_approx] starting weight calculation for promoting is the active form of promote
involving is the active form of involve
performing is the active form of perform
sitting is the active form of sit
identifying is the active form of identify
existing is the active form of exist
referring is the active form of refer
encouraging is the active form of
2024-07-09 18:07:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 18:11:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 18:15:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2333, -0.1479,  0.3677,  ...,  0.0630,  1.9102, -0.3213],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.2301, -0.1685,  0.3398,  ...,  0.0646,  1.8652, -0.3513],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0374, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2324, -1.1113,  2.2207,  ..., -0.4878, -0.8477, -0.1880],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0090, -0.0005,  0.0101,  ...,  0.0002,  0.0034,  0.0032],
        [ 0.0084, -0.0109, -0.0071,  ...,  0.0142, -0.0022,  0.0119],
        [ 0.0088,  0.0013,  0.0014,  ...,  0.0023, -0.0152, -0.0156],
        ...,
        [-0.0208,  0.0051, -0.0019,  ..., -0.0055, -0.0090,  0.0014],
        [-0.0030,  0.0139,  0.0036,  ..., -0.0133,  0.0105,  0.0159],
        [ 0.0071, -0.0060,  0.0078,  ...,  0.0055,  0.0058,  0.0138]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-3.4928e-04, -1.1110e-03, -5.4455e-04,  ..., -1.6384e-03,
          3.0875e-04,  7.2956e-05],
        [ 9.0313e-04,  8.2684e-04,  1.9670e-05,  ...,  3.3903e-04,
          5.7888e-04,  1.0443e-04],
        [-5.6601e-04,  3.7098e-04,  4.1604e-04,  ...,  2.3532e-04,
         -2.4676e-04, -1.2970e-03],
        ...,
        [ 4.1723e-04,  1.4009e-03, -1.6575e-03,  ...,  3.4952e-04,
         -8.9312e-04,  7.6485e-04],
        [ 9.8228e-04,  8.7261e-04,  1.0290e-03,  ...,  3.3998e-04,
         -9.1124e-04, -2.4772e-04],
        [ 3.7193e-04,  3.3069e-04,  9.0075e-04,  ...,  1.4477e-03,
          1.1635e-03, -3.9554e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0205, -1.2695,  1.5273,  ..., -1.0908, -0.1382, -0.1716]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1124, -0.0982, -0.3682,  ..., -0.1027,  0.0896, -0.1328]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 18:15:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for promoting is the active form of promote
involving is the active form of involve
performing is the active form of perform
sitting is the active form of sit
identifying is the active form of identify
existing is the active form of exist
referring is the active form of refer
encouraging is the active form of
2024-07-09 18:15:28 root INFO     [order_1_approx] starting weight calculation for existing is the active form of exist
encouraging is the active form of encourage
performing is the active form of perform
referring is the active form of refer
identifying is the active form of identify
promoting is the active form of promote
sitting is the active form of sit
involving is the active form of
2024-07-09 18:15:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 18:19:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 18:23:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0620, -0.9702,  0.7549,  ..., -0.4570,  1.7139,  0.4255],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.0710, -1.1162,  0.7871,  ..., -0.4792,  1.8525,  0.4585],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0221, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.4102, -1.4824,  0.6494,  ..., -0.8193,  1.1426, -0.6797],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.5747e-02,  9.4681e-03,  4.6806e-03,  ...,  2.4384e-02,
         -3.2806e-04, -2.8496e-03],
        [ 1.2077e-02,  6.5002e-03, -4.5509e-03,  ...,  1.7990e-02,
          3.1586e-03,  1.0292e-02],
        [ 9.2010e-03,  8.0032e-03,  8.7662e-03,  ..., -7.3547e-03,
          6.7558e-03,  1.0468e-02],
        ...,
        [-1.2978e-02, -1.1047e-02,  1.1292e-02,  ...,  8.6823e-03,
         -3.8300e-03,  1.9669e-02],
        [-2.1179e-02,  2.4796e-05, -2.0676e-03,  ...,  5.7526e-03,
         -2.3468e-02,  5.1880e-04],
        [ 3.3875e-03,  2.3026e-02,  1.5221e-02,  ..., -2.0126e-02,
          1.2077e-02,  9.8724e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 4.9257e-04, -1.1692e-03, -1.5373e-03,  ...,  7.4196e-04,
          5.4836e-04, -5.4216e-04],
        [ 1.2579e-03, -2.4045e-04,  9.3317e-04,  ..., -4.9496e-04,
         -3.6287e-04,  8.2588e-04],
        [-7.9346e-04, -1.3103e-03, -3.1686e-04,  ...,  1.5469e-03,
          6.8474e-04,  2.2471e-05],
        ...,
        [-2.8849e-04,  1.0977e-03, -9.1791e-05,  ...,  1.2493e-04,
          1.2455e-03, -5.7220e-06],
        [-4.6515e-04,  2.1374e-04, -5.8842e-04,  ..., -1.5974e-03,
         -1.7750e-04, -1.0042e-03],
        [-2.4490e-03,  1.0710e-03,  7.4863e-04,  ...,  5.4073e-04,
         -2.6455e-03, -4.4346e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.0273,  0.4053,  1.7256,  ..., -1.9648,  0.9307, -1.3506]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2499,  0.1315, -0.5249,  ..., -0.1821, -0.0380, -0.0544]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 18:23:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for existing is the active form of exist
encouraging is the active form of encourage
performing is the active form of perform
referring is the active form of refer
identifying is the active form of identify
promoting is the active form of promote
sitting is the active form of sit
involving is the active form of
2024-07-09 18:23:46 root INFO     [order_1_approx] starting weight calculation for performing is the active form of perform
encouraging is the active form of encourage
identifying is the active form of identify
promoting is the active form of promote
sitting is the active form of sit
referring is the active form of refer
involving is the active form of involve
existing is the active form of
2024-07-09 18:23:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 18:28:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 18:32:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6289, -0.3125,  1.5127,  ...,  0.7871,  0.9839, -0.2639],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.7246, -0.3848,  1.6328,  ...,  0.8623,  1.0859, -0.3330],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0379, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.6211,  0.0996,  3.4219,  ..., -2.0645,  1.2031,  0.5947],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 5.2414e-03, -1.7151e-02,  1.1040e-02,  ...,  1.9073e-05,
         -3.6430e-03, -1.7014e-02],
        [ 4.5624e-03, -2.4734e-02, -1.3390e-03,  ...,  1.0414e-02,
         -8.5373e-03,  1.8196e-03],
        [ 2.9507e-03, -1.0811e-02,  5.6305e-03,  ..., -4.6730e-03,
         -5.2872e-03,  8.6212e-03],
        ...,
        [-5.0507e-03, -5.2528e-03, -8.6212e-03,  ...,  9.8114e-03,
          1.7242e-02,  2.2125e-04],
        [-7.7286e-03, -2.1576e-02,  1.8555e-02,  ..., -7.6904e-03,
         -1.8524e-02,  8.4229e-03],
        [ 3.6659e-03, -2.6455e-03,  1.8280e-02,  ...,  7.7972e-03,
          1.5289e-02,  1.0483e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.6823e-03, -2.9449e-03, -6.9427e-04,  ..., -1.1768e-03,
          9.6035e-04,  4.2200e-04],
        [ 1.2398e-05, -1.3237e-03,  4.3249e-04,  ..., -6.3419e-04,
         -1.4806e-04, -1.4811e-03],
        [ 8.3447e-04,  1.2875e-04, -2.5845e-04,  ..., -5.6314e-04,
         -1.0185e-03, -1.7385e-03],
        ...,
        [ 3.1567e-04, -3.3331e-04, -6.9380e-04,  ...,  1.1311e-03,
          2.5101e-03, -1.3123e-03],
        [ 2.7919e-04,  6.3038e-04,  2.1911e-04,  ...,  7.0047e-04,
         -8.6308e-04, -2.7704e-04],
        [ 4.7708e-04, -5.9986e-04,  1.6813e-03,  ...,  1.2865e-03,
          4.8757e-04, -6.8760e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1699,  1.6963,  3.1016,  ..., -3.2500,  1.9473,  0.0229]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0944,  0.0080, -0.0473,  ..., -0.5166, -0.0129, -0.3284]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 18:32:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for performing is the active form of perform
encouraging is the active form of encourage
identifying is the active form of identify
promoting is the active form of promote
sitting is the active form of sit
referring is the active form of refer
involving is the active form of involve
existing is the active form of
2024-07-09 18:32:15 root INFO     [order_1_approx] starting weight calculation for involving is the active form of involve
referring is the active form of refer
sitting is the active form of sit
existing is the active form of exist
promoting is the active form of promote
performing is the active form of perform
encouraging is the active form of encourage
identifying is the active form of
2024-07-09 18:32:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 18:36:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 18:40:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9146, -0.5796, -0.6597,  ..., -0.5430,  0.9038, -0.3301],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.9082, -0.6079, -0.6372,  ..., -0.5137,  0.8682, -0.3584],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0270, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7168,  1.7568, -3.1855,  ..., -3.0918,  2.7188,  0.1230],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0022, -0.0084,  0.0122,  ...,  0.0094, -0.0015, -0.0027],
        [ 0.0013, -0.0062,  0.0054,  ...,  0.0064, -0.0064,  0.0128],
        [-0.0071, -0.0006, -0.0071,  ..., -0.0070, -0.0053, -0.0039],
        ...,
        [-0.0180,  0.0048,  0.0028,  ..., -0.0009,  0.0120,  0.0194],
        [-0.0123, -0.0006,  0.0089,  ..., -0.0002, -0.0065,  0.0065],
        [ 0.0073,  0.0128,  0.0014,  ...,  0.0078,  0.0141,  0.0046]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 6.2990e-04,  1.3697e-04, -2.0456e-04,  ..., -6.5804e-05,
          3.6001e-04,  7.7438e-04],
        [ 4.4870e-04, -5.2071e-04, -1.9073e-05,  ..., -1.0576e-03,
         -6.8760e-04, -1.3390e-03],
        [ 1.6117e-04,  1.5039e-03, -3.6478e-04,  ...,  2.3711e-04,
         -6.7472e-04, -5.4419e-05],
        ...,
        [ 2.0714e-03,  1.7071e-03, -3.6669e-04,  ..., -1.1885e-04,
          1.2422e-04,  6.6853e-04],
        [-1.1120e-03,  9.7179e-04,  7.5817e-04,  ...,  9.3031e-04,
         -7.3433e-04, -4.2868e-04],
        [-2.5034e-06,  1.1616e-03,  1.4067e-05,  ..., -2.1958e-04,
         -5.0402e-04,  6.7234e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.8008,  2.2539, -4.3516,  ..., -3.7344,  2.9023,  0.7495]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0989, -0.0446,  0.0329,  ..., -0.3513,  0.1265,  0.0787]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 18:40:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for involving is the active form of involve
referring is the active form of refer
sitting is the active form of sit
existing is the active form of exist
promoting is the active form of promote
performing is the active form of perform
encouraging is the active form of encourage
identifying is the active form of
2024-07-09 18:40:44 root INFO     [order_1_approx] starting weight calculation for identifying is the active form of identify
performing is the active form of perform
existing is the active form of exist
encouraging is the active form of encourage
involving is the active form of involve
promoting is the active form of promote
sitting is the active form of sit
referring is the active form of
2024-07-09 18:40:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 18:45:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 18:49:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2188, -0.3096,  0.4692,  ..., -1.7285,  1.3076, -0.4121],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.2328, -0.3491,  0.4609,  ..., -1.7373,  1.3379, -0.4644],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0491, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1875, -0.3643, -0.2773,  ...,  3.9902,  0.9150,  0.4292],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0135, -0.0166, -0.0006,  ...,  0.0106,  0.0103, -0.0095],
        [ 0.0035,  0.0260,  0.0006,  ...,  0.0023, -0.0094,  0.0118],
        [ 0.0192, -0.0050,  0.0054,  ..., -0.0087, -0.0009,  0.0107],
        ...,
        [-0.0356,  0.0110,  0.0088,  ...,  0.0041, -0.0052,  0.0040],
        [-0.0116, -0.0001,  0.0088,  ...,  0.0107, -0.0002,  0.0056],
        [ 0.0114,  0.0117,  0.0113,  ..., -0.0253,  0.0042,  0.0215]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 2.0933e-04, -3.6001e-05, -1.4954e-03,  ...,  5.3310e-04,
          5.0247e-05, -2.0275e-03],
        [-2.9111e-04,  2.2430e-03,  7.8678e-05,  ...,  1.9085e-04,
         -6.3324e-04, -2.6560e-04],
        [ 1.1978e-03, -4.3297e-04, -7.5340e-04,  ..., -6.9618e-04,
          7.2050e-04,  1.0788e-04],
        ...,
        [-7.7677e-04,  8.2016e-04,  1.0967e-03,  ...,  2.1973e-03,
          4.6015e-04,  2.0671e-04],
        [-7.8106e-04, -5.8889e-04,  5.0116e-04,  ...,  4.7922e-05,
          4.6039e-04, -2.9516e-04],
        [-1.1091e-03,  9.9754e-04,  1.3380e-03,  ..., -9.2220e-04,
         -4.7445e-04,  4.2629e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7520, -0.3269, -0.2043,  ...,  3.3438,  1.5039, -0.0364]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1749, -0.2069, -0.1285,  ..., -0.1884,  0.0825, -0.1510]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 18:49:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for identifying is the active form of identify
performing is the active form of perform
existing is the active form of exist
encouraging is the active form of encourage
involving is the active form of involve
promoting is the active form of promote
sitting is the active form of sit
referring is the active form of
2024-07-09 18:49:13 root INFO     [order_1_approx] starting weight calculation for involving is the active form of involve
referring is the active form of refer
identifying is the active form of identify
sitting is the active form of sit
existing is the active form of exist
promoting is the active form of promote
encouraging is the active form of encourage
performing is the active form of
2024-07-09 18:49:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 18:53:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 18:57:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2109, -0.0649,  0.1125,  ..., -0.6836,  1.3027, -0.4519],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.2167, -0.0923,  0.0989,  ..., -0.6978,  1.3438, -0.5190],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0270, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 6.6211, -0.3418,  2.0156,  ..., -0.7388,  0.3770,  0.9795],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0040, -0.0014,  0.0159,  ..., -0.0094,  0.0044,  0.0050],
        [ 0.0118, -0.0055,  0.0054,  ..., -0.0038, -0.0064, -0.0003],
        [ 0.0026, -0.0090, -0.0009,  ...,  0.0003, -0.0008, -0.0028],
        ...,
        [-0.0250,  0.0064,  0.0035,  ...,  0.0024,  0.0011, -0.0028],
        [-0.0123, -0.0032, -0.0029,  ..., -0.0037,  0.0004,  0.0106],
        [ 0.0065,  0.0054,  0.0025,  ...,  0.0011,  0.0068,  0.0104]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.0166e-03, -1.6251e-03, -1.6689e-05,  ..., -3.8862e-04,
          7.7105e-04,  5.0116e-04],
        [ 7.2241e-04, -6.3610e-04,  6.0034e-04,  ..., -9.7418e-04,
          3.2377e-04, -9.9277e-04],
        [-4.6182e-04,  2.7275e-04,  3.2830e-04,  ..., -2.1517e-04,
          5.2691e-05, -4.9591e-04],
        ...,
        [-3.1757e-04,  7.1907e-04, -5.4932e-04,  ...,  1.7881e-07,
          1.5106e-03,  1.3094e-03],
        [-1.2817e-03,  1.1883e-03, -6.9141e-05,  ...,  9.5320e-04,
         -9.6989e-04, -4.3154e-04],
        [-9.9277e-04,  7.6294e-04,  2.4080e-05,  ...,  1.1456e-04,
         -1.3933e-03,  1.7130e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.2969, -0.2014,  1.9316,  ..., -1.4355,  0.7246,  0.9946]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0768,  0.0804,  0.0150,  ..., -0.1022,  0.1560, -0.1176]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 18:57:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for involving is the active form of involve
referring is the active form of refer
identifying is the active form of identify
sitting is the active form of sit
existing is the active form of exist
promoting is the active form of promote
encouraging is the active form of encourage
performing is the active form of
2024-07-09 18:57:41 root INFO     [order_1_approx] starting weight calculation for promoting is the active form of promote
existing is the active form of exist
involving is the active form of involve
encouraging is the active form of encourage
identifying is the active form of identify
referring is the active form of refer
performing is the active form of perform
sitting is the active form of
2024-07-09 18:57:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 19:02:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 19:06:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8257, -2.3418,  0.8843,  ...,  0.0642,  0.9790,  0.3740],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.8062, -2.3672,  0.8154,  ...,  0.0648,  0.9268,  0.3506],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0374, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.4258, -0.3027,  2.5312,  ...,  0.3567, -1.6367,  1.4639],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0054, -0.0010,  0.0164,  ..., -0.0094, -0.0040, -0.0103],
        [ 0.0012,  0.0004,  0.0058,  ...,  0.0118, -0.0006,  0.0031],
        [-0.0016,  0.0104, -0.0029,  ...,  0.0034,  0.0060, -0.0114],
        ...,
        [-0.0218,  0.0018,  0.0050,  ..., -0.0059, -0.0188,  0.0163],
        [ 0.0077,  0.0133, -0.0142,  ...,  0.0134,  0.0231, -0.0114],
        [-0.0003,  0.0078, -0.0060,  ...,  0.0006,  0.0324, -0.0105]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.0738e-03, -1.6508e-03,  4.3559e-04,  ..., -2.2960e-04,
         -2.0385e-04, -1.7014e-03],
        [-4.0865e-04,  1.8513e-04, -2.2030e-03,  ..., -1.2589e-04,
          9.1612e-05,  8.5640e-04],
        [-1.5211e-04,  3.2604e-05,  2.3174e-04,  ..., -7.7295e-04,
         -5.9700e-04, -2.8825e-04],
        ...,
        [ 9.9468e-04,  1.0805e-03, -4.8256e-04,  ..., -1.4439e-03,
          8.2874e-04, -6.7711e-04],
        [ 8.1921e-04,  2.3785e-03, -8.1205e-04,  ..., -2.7609e-04,
         -9.4509e-04, -1.8418e-04],
        [-2.0084e-03,  1.0300e-03, -4.5538e-04,  ...,  1.1187e-03,
          8.0967e-04,  5.9009e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.5703,  0.0750,  2.8574,  ..., -0.3362, -1.2773,  0.8833]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0999, -0.0009, -0.3008,  ..., -0.3777,  0.0044, -0.2222]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 19:06:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for promoting is the active form of promote
existing is the active form of exist
involving is the active form of involve
encouraging is the active form of encourage
identifying is the active form of identify
referring is the active form of refer
performing is the active form of perform
sitting is the active form of
2024-07-09 19:06:10 root INFO     [order_1_approx] starting weight calculation for encouraging is the active form of encourage
referring is the active form of refer
sitting is the active form of sit
existing is the active form of exist
identifying is the active form of identify
performing is the active form of perform
involving is the active form of involve
promoting is the active form of
2024-07-09 19:06:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 19:10:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 19:14:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6421, -0.4336,  0.4756,  ..., -0.2976,  0.4106,  0.3787],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.6768, -0.4880,  0.4690,  ..., -0.2986,  0.4097,  0.3853],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0261, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5117, -1.6162,  1.0762,  ...,  1.1172, -2.6719,  1.9551],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0101, -0.0037,  0.0162,  ..., -0.0050,  0.0092, -0.0037],
        [ 0.0107, -0.0021,  0.0079,  ...,  0.0050,  0.0057,  0.0051],
        [ 0.0127, -0.0051,  0.0052,  ...,  0.0007,  0.0130, -0.0056],
        ...,
        [-0.0531, -0.0135, -0.0083,  ...,  0.0042, -0.0266,  0.0108],
        [ 0.0020,  0.0032,  0.0189,  ..., -0.0015,  0.0033,  0.0130],
        [ 0.0283,  0.0031,  0.0205,  ..., -0.0145,  0.0202,  0.0071]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.5215e-03, -7.7903e-05, -4.7445e-05,  ..., -1.1396e-04,
          6.5136e-04,  8.4972e-04],
        [-4.4680e-04, -1.5986e-04,  1.6003e-03,  ...,  6.4564e-04,
          6.8378e-04,  6.4087e-04],
        [ 1.3137e-04, -5.9080e-04, -6.7663e-04,  ...,  8.9550e-04,
         -8.2970e-05, -1.8740e-04],
        ...,
        [-1.0433e-03, -9.5510e-04, -1.7147e-03,  ..., -3.5191e-04,
          4.8494e-04,  6.4230e-04],
        [-2.4986e-04,  4.2915e-04,  6.9666e-04,  ...,  3.2997e-04,
         -8.6689e-04,  1.9777e-04],
        [ 1.6844e-04,  3.9482e-04,  1.1072e-03,  ...,  3.1424e-04,
          5.3883e-05,  5.0879e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0312, -1.2871,  1.4473,  ...,  1.0400, -2.8203,  2.6738]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1478, -0.0163,  0.0930,  ..., -0.2656, -0.1255,  0.1088]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 19:14:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for encouraging is the active form of encourage
referring is the active form of refer
sitting is the active form of sit
existing is the active form of exist
identifying is the active form of identify
performing is the active form of perform
involving is the active form of involve
promoting is the active form of
2024-07-09 19:14:38 root INFO     total operator prediction time: 4027.0019915103912 seconds
2024-07-09 19:14:38 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_Ving - Ved
2024-07-09 19:14:40 root INFO     building operator verb_Ving - Ved
2024-07-09 19:14:41 root INFO     [order_1_approx] starting weight calculation for After something is describing, it has described
After something is announcing, it has announced
After something is spending, it has spent
After something is containing, it has contained
After something is continuing, it has continued
After something is operating, it has operated
After something is attending, it has attended
After something is asking, it has
2024-07-09 19:14:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 19:19:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 19:23:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9990, -0.5195,  0.5151,  ..., -0.5210,  1.0176,  0.2505],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 1.0430, -0.5786,  0.5020,  ..., -0.5229,  1.0312,  0.2418],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0468, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([0.1578, 2.8398, 3.2422,  ..., 4.2617, 0.8232, 3.8398], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0134, -0.0439,  0.0060,  ..., -0.0032, -0.0292, -0.0078],
        [-0.0111, -0.0200,  0.0003,  ...,  0.0064,  0.0063, -0.0088],
        [-0.0009, -0.0105, -0.0299,  ..., -0.0071, -0.0013,  0.0063],
        ...,
        [-0.0268, -0.0191, -0.0011,  ..., -0.0234, -0.0099,  0.0072],
        [-0.0022,  0.0130,  0.0207,  ..., -0.0127, -0.0341,  0.0164],
        [-0.0152,  0.0419, -0.0185,  ..., -0.0043, -0.0010, -0.0034]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.0046e-03,  1.5907e-03, -1.5011e-03,  ..., -1.2150e-03,
         -1.3905e-03, -8.3208e-04],
        [ 4.7505e-05,  5.7259e-03,  2.4948e-03,  ...,  2.3804e-03,
          3.9062e-03,  5.3596e-04],
        [-1.1044e-03, -1.2751e-03, -2.9850e-03,  ..., -7.4387e-04,
          2.2144e-03, -2.7637e-03],
        ...,
        [ 2.8439e-03, -1.7757e-03,  2.1572e-03,  ..., -6.8569e-04,
         -7.9203e-04,  1.4601e-03],
        [ 9.3842e-04, -3.0375e-04,  3.0975e-03,  ...,  7.7486e-04,
         -2.0580e-03,  7.0858e-04],
        [-3.6168e-04,  2.5034e-04, -1.0252e-04,  ...,  1.8721e-03,
         -2.9221e-03, -2.4376e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[0.3154, 2.1172, 2.7383,  ..., 3.8867, 1.3418, 3.7500]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2808,  0.0148,  0.2800,  ...,  0.0264, -0.2546,  0.0119]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 19:23:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is describing, it has described
After something is announcing, it has announced
After something is spending, it has spent
After something is containing, it has contained
After something is continuing, it has continued
After something is operating, it has operated
After something is attending, it has attended
After something is asking, it has
2024-07-09 19:23:10 root INFO     [order_1_approx] starting weight calculation for After something is continuing, it has continued
After something is operating, it has operated
After something is announcing, it has announced
After something is describing, it has described
After something is spending, it has spent
After something is attending, it has attended
After something is asking, it has asked
After something is containing, it has
2024-07-09 19:23:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 19:27:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 19:31:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6357, -0.3430, -0.0457,  ...,  0.1227, -0.4287,  0.6924],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.6846, -0.4045, -0.0632,  ...,  0.1274, -0.4832,  0.7407],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0468, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1401,  2.4961,  1.4355,  ...,  1.7441, -2.8828,  2.3340],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0034, -0.0212, -0.0010,  ...,  0.0025, -0.0208, -0.0047],
        [-0.0114, -0.0053, -0.0017,  ..., -0.0164, -0.0060,  0.0097],
        [ 0.0155, -0.0023, -0.0031,  ..., -0.0088,  0.0244, -0.0022],
        ...,
        [-0.0219,  0.0061, -0.0038,  ..., -0.0034,  0.0186,  0.0128],
        [ 0.0178, -0.0042, -0.0003,  ..., -0.0076,  0.0071,  0.0122],
        [ 0.0032,  0.0056, -0.0090,  ..., -0.0131,  0.0074, -0.0149]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.6985e-03, -2.8477e-03,  1.6165e-03,  ...,  5.4359e-05,
          1.8702e-03,  8.3256e-04],
        [ 2.3136e-03, -1.5268e-03,  9.6798e-04,  ..., -1.3866e-03,
          3.7308e-03,  2.3251e-03],
        [ 1.9951e-03,  1.5497e-03, -3.6411e-03,  ..., -2.4281e-03,
          1.2293e-03, -7.7868e-04],
        ...,
        [ 5.8270e-04, -2.4014e-03,  1.8196e-03,  ..., -3.0365e-03,
          1.9550e-03, -1.3423e-04],
        [-2.4376e-03,  1.7853e-03,  3.6097e-04,  ..., -2.3246e-04,
         -1.1957e-04, -4.1223e-04],
        [-2.3880e-03, -2.5630e-04, -1.0610e-04,  ..., -8.6308e-04,
         -2.7504e-03, -1.6060e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2915,  1.1123,  0.4023,  ...,  0.8867, -2.3086,  1.2051]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2727,  0.0700,  0.1151,  ..., -0.0684, -0.1581,  0.2761]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 19:31:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is continuing, it has continued
After something is operating, it has operated
After something is announcing, it has announced
After something is describing, it has described
After something is spending, it has spent
After something is attending, it has attended
After something is asking, it has asked
After something is containing, it has
2024-07-09 19:31:39 root INFO     [order_1_approx] starting weight calculation for After something is announcing, it has announced
After something is containing, it has contained
After something is describing, it has described
After something is operating, it has operated
After something is spending, it has spent
After something is attending, it has attended
After something is asking, it has asked
After something is continuing, it has
2024-07-09 19:31:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 19:35:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 19:40:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8130, -0.1799,  0.8823,  ..., -0.1967,  0.9512, -0.0852],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.8384, -0.2151,  0.8604,  ..., -0.1979,  0.9521, -0.1201],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0468, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3650, -0.4165,  1.5801,  ...,  4.0508, -2.5586,  2.0781],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0250, -0.0222,  0.0023,  ..., -0.0094, -0.0066, -0.0035],
        [ 0.0006, -0.0266, -0.0264,  ..., -0.0179,  0.0118,  0.0025],
        [ 0.0209, -0.0093, -0.0338,  ..., -0.0071,  0.0166,  0.0014],
        ...,
        [-0.0263,  0.0109, -0.0151,  ..., -0.0071, -0.0011, -0.0122],
        [ 0.0172, -0.0007,  0.0116,  ..., -0.0014, -0.0156,  0.0142],
        [ 0.0067,  0.0102, -0.0041,  ..., -0.0057,  0.0075, -0.0247]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.8616e-03,  1.1425e-03,  1.1730e-03,  ..., -2.9087e-05,
          1.2970e-03, -7.2956e-04],
        [-2.7180e-05, -2.8801e-03, -9.6798e-04,  ..., -1.0872e-03,
          2.8248e-03,  1.6570e-05],
        [ 9.7370e-04, -3.1013e-03, -3.0899e-03,  ..., -8.2970e-04,
          2.1684e-04, -7.9060e-04],
        ...,
        [ 3.7308e-03, -2.3537e-03,  2.1057e-03,  ..., -5.6410e-04,
         -3.5906e-04, -3.8719e-04],
        [-8.1253e-04,  4.8041e-04, -2.0618e-03,  ..., -1.6689e-03,
         -1.1435e-03,  2.0313e-03],
        [-4.1294e-04,  6.0844e-04,  8.3494e-04,  ..., -6.6853e-04,
         -1.5087e-03, -2.2907e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9277, -1.0332,  2.0449,  ...,  3.7656, -1.2031,  1.6367]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.2301, -0.1552,  0.1541,  ..., -0.4138, -0.0282,  0.3027]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 19:40:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is announcing, it has announced
After something is containing, it has contained
After something is describing, it has described
After something is operating, it has operated
After something is spending, it has spent
After something is attending, it has attended
After something is asking, it has asked
After something is continuing, it has
2024-07-09 19:40:09 root INFO     [order_1_approx] starting weight calculation for After something is continuing, it has continued
After something is containing, it has contained
After something is operating, it has operated
After something is describing, it has described
After something is spending, it has spent
After something is asking, it has asked
After something is attending, it has attended
After something is announcing, it has
2024-07-09 19:40:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 19:44:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 19:48:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3430,  0.0774,  1.1230,  ..., -0.3250, -0.5156, -0.0931],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3406,  0.0580,  1.0752,  ..., -0.3186, -0.5420, -0.1261],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0468, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.5234, -1.1426, -1.6992,  ...,  5.5312,  1.6006,  4.1953],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0302, -0.0399,  0.0452,  ...,  0.0266, -0.0058, -0.0262],
        [-0.0084, -0.0275, -0.0075,  ...,  0.0006,  0.0073,  0.0214],
        [ 0.0039,  0.0187, -0.0169,  ..., -0.0024, -0.0006,  0.0057],
        ...,
        [-0.0190, -0.0140, -0.0308,  ..., -0.0021, -0.0116, -0.0095],
        [ 0.0308,  0.0022,  0.0278,  ..., -0.0162, -0.0518,  0.0311],
        [ 0.0299,  0.0307, -0.0151,  ..., -0.0062,  0.0036, -0.0338]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-6.2523e-03, -3.1147e-03,  2.0657e-03,  ..., -3.9520e-03,
          9.5367e-04,  1.6594e-03],
        [ 7.4816e-04,  7.9823e-04,  1.3962e-03,  ..., -1.2226e-03,
          2.7657e-03,  2.1954e-03],
        [-1.3180e-03, -9.9468e-04, -6.0558e-05,  ..., -8.5258e-04,
         -1.0509e-03,  9.1267e-04],
        ...,
        [ 4.1199e-03,  4.0650e-05, -4.9829e-05,  ...,  1.3199e-03,
          3.3627e-03, -4.7874e-03],
        [ 2.2736e-03, -5.8222e-04,  2.3994e-03,  ..., -1.1644e-03,
         -3.3455e-03,  1.8101e-03],
        [-1.8110e-03,  1.0681e-04,  3.0823e-03,  ...,  8.6021e-04,
         -1.1387e-03,  9.5892e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.6426, -1.3418, -1.5957,  ...,  6.1562,  1.5293,  4.0312]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.6357,  0.0845, -0.2561,  ...,  0.0768,  0.0746,  0.1134]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 19:48:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is continuing, it has continued
After something is containing, it has contained
After something is operating, it has operated
After something is describing, it has described
After something is spending, it has spent
After something is asking, it has asked
After something is attending, it has attended
After something is announcing, it has
2024-07-09 19:48:36 root INFO     [order_1_approx] starting weight calculation for After something is continuing, it has continued
After something is attending, it has attended
After something is containing, it has contained
After something is operating, it has operated
After something is announcing, it has announced
After something is spending, it has spent
After something is asking, it has asked
After something is describing, it has
2024-07-09 19:48:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 19:52:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 19:57:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7891,  0.2803,  0.4849,  ..., -0.5669, -0.1594, -1.0586],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.8101,  0.2761,  0.4636,  ..., -0.5625, -0.1852, -1.1455],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0468, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1350,  0.4385,  4.2031,  ...,  3.0449, -3.1152,  2.4414],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0125, -0.0391,  0.0056,  ..., -0.0029, -0.0137, -0.0189],
        [-0.0014, -0.0221, -0.0018,  ..., -0.0083,  0.0013, -0.0036],
        [ 0.0048,  0.0049, -0.0169,  ..., -0.0027,  0.0060,  0.0124],
        ...,
        [-0.0362,  0.0009,  0.0003,  ..., -0.0116,  0.0075, -0.0131],
        [ 0.0143,  0.0079,  0.0177,  ..., -0.0251, -0.0385, -0.0005],
        [-0.0081,  0.0340, -0.0067,  ...,  0.0011,  0.0002, -0.0107]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-3.7136e-03, -1.9574e-04, -9.2745e-05,  ..., -2.0194e-04,
         -6.2037e-04, -2.0771e-03],
        [ 1.4305e-04, -4.7398e-04,  1.0176e-03,  ..., -6.4564e-04,
          1.3332e-03, -1.7090e-03],
        [ 3.4165e-04,  2.5177e-04, -1.7271e-03,  ..., -7.5436e-04,
          9.3174e-04,  1.3857e-03],
        ...,
        [ 3.7899e-03, -2.4433e-03,  1.3876e-03,  ..., -2.0561e-03,
          2.4281e-03, -3.4046e-03],
        [ 3.5238e-04,  1.1711e-03,  1.6327e-03,  ..., -8.5735e-04,
         -3.0899e-03, -2.0218e-03],
        [-6.7472e-04,  2.6379e-03, -5.0545e-04,  ...,  2.7180e-05,
         -1.0204e-03,  1.7319e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3481, -0.7041,  1.9492,  ...,  1.3789, -2.7422,  1.1182]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1537,  0.1741,  0.1281,  ..., -0.1077, -0.3936,  0.1704]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 19:57:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is continuing, it has continued
After something is attending, it has attended
After something is containing, it has contained
After something is operating, it has operated
After something is announcing, it has announced
After something is spending, it has spent
After something is asking, it has asked
After something is describing, it has
2024-07-09 19:57:05 root INFO     [order_1_approx] starting weight calculation for After something is announcing, it has announced
After something is attending, it has attended
After something is describing, it has described
After something is continuing, it has continued
After something is asking, it has asked
After something is containing, it has contained
After something is spending, it has spent
After something is operating, it has
2024-07-09 19:57:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 20:01:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 20:05:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0009, -0.5132,  0.8203,  ..., -0.0527,  0.5850,  0.0145],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.0063, -0.5352,  0.7520,  ..., -0.0487,  0.5425, -0.0125],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0468, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3281, -0.4258,  2.9668,  ...,  2.0430, -0.1162,  3.0078],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-6.6986e-03, -6.8474e-03,  5.2185e-03,  ...,  1.1261e-02,
         -1.2550e-02, -6.4964e-03],
        [-1.1520e-02, -1.7761e-02, -5.9738e-03,  ...,  3.9139e-03,
         -1.0902e-02, -3.7785e-03],
        [ 4.7531e-03,  2.5749e-03, -1.1375e-02,  ...,  5.9319e-03,
          2.2446e-02,  1.6785e-03],
        ...,
        [-1.4778e-02, -1.2077e-02, -1.2543e-02,  ..., -2.6108e-02,
          5.4932e-04, -9.7809e-03],
        [ 4.2419e-03, -6.5517e-04,  1.2207e-03,  ..., -1.0094e-02,
         -1.4206e-02,  5.6190e-03],
        [ 1.1703e-02,  1.2344e-02, -1.1444e-05,  ...,  1.8673e-03,
         -8.5831e-04, -1.4244e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0005, -0.0013,  0.0029,  ..., -0.0017,  0.0005, -0.0006],
        [ 0.0012, -0.0035,  0.0031,  ..., -0.0003,  0.0030,  0.0004],
        [-0.0002, -0.0002, -0.0005,  ..., -0.0008,  0.0023, -0.0014],
        ...,
        [ 0.0027, -0.0026,  0.0021,  ..., -0.0003,  0.0018, -0.0013],
        [ 0.0008, -0.0006,  0.0023,  ..., -0.0018, -0.0007,  0.0009],
        [-0.0004,  0.0005, -0.0004,  ...,  0.0010, -0.0010, -0.0015]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3711, -0.5767,  3.3691,  ...,  1.6543, -0.3555,  1.8193]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2236,  0.0360, -0.0740,  ..., -0.0506, -0.2401,  0.0316]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 20:05:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is announcing, it has announced
After something is attending, it has attended
After something is describing, it has described
After something is continuing, it has continued
After something is asking, it has asked
After something is containing, it has contained
After something is spending, it has spent
After something is operating, it has
2024-07-09 20:05:32 root INFO     [order_1_approx] starting weight calculation for After something is asking, it has asked
After something is describing, it has described
After something is continuing, it has continued
After something is announcing, it has announced
After something is operating, it has operated
After something is spending, it has spent
After something is containing, it has contained
After something is attending, it has
2024-07-09 20:05:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 20:09:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 20:13:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2939, -0.0710,  0.7759,  ..., -0.5229, -0.0659, -0.3616],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 1.3730, -0.0999,  0.7734,  ..., -0.5352, -0.0928, -0.4221],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0468, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.3594,  2.6074,  3.5156,  ...,  1.9893,  0.1377,  2.3125],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0003,  0.0028,  0.0185,  ...,  0.0190, -0.0456, -0.0138],
        [-0.0008, -0.0251, -0.0081,  ...,  0.0053, -0.0025, -0.0043],
        [ 0.0087, -0.0145, -0.0040,  ...,  0.0168,  0.0016, -0.0048],
        ...,
        [-0.0076, -0.0036,  0.0044,  ..., -0.0059,  0.0021,  0.0200],
        [ 0.0147,  0.0160,  0.0336,  ...,  0.0348, -0.0143,  0.0305],
        [ 0.0101,  0.0174,  0.0160,  ..., -0.0011,  0.0140, -0.0244]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-9.4748e-04,  6.2752e-04,  1.7633e-03,  ..., -6.5804e-04,
         -1.8368e-03, -3.1548e-03],
        [ 1.8787e-03, -3.2663e-04,  2.4033e-03,  ..., -5.9319e-04,
          6.9141e-04, -8.6069e-05],
        [ 1.1568e-03, -6.8951e-04, -1.6003e-03,  ...,  1.4925e-03,
          2.8458e-03,  1.0910e-03],
        ...,
        [ 3.7727e-03,  4.3201e-04, -2.6536e-04,  ...,  8.2731e-05,
          1.0366e-03,  7.4291e-04],
        [-4.1533e-04,  1.6680e-03,  1.0004e-03,  ...,  1.3046e-03,
         -1.4362e-03,  5.1439e-05],
        [ 1.0471e-03,  4.4060e-04,  2.3155e-03,  ...,  1.6994e-03,
          1.4305e-06, -2.0170e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2354,  2.1172,  3.4238,  ...,  1.6992,  0.3354,  1.4404]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0421,  0.0197,  0.0732,  ...,  0.0856, -0.1510,  0.1195]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 20:13:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is asking, it has asked
After something is describing, it has described
After something is continuing, it has continued
After something is announcing, it has announced
After something is operating, it has operated
After something is spending, it has spent
After something is containing, it has contained
After something is attending, it has
2024-07-09 20:13:59 root INFO     [order_1_approx] starting weight calculation for After something is attending, it has attended
After something is asking, it has asked
After something is announcing, it has announced
After something is containing, it has contained
After something is operating, it has operated
After something is continuing, it has continued
After something is describing, it has described
After something is spending, it has
2024-07-09 20:13:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 20:18:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 20:22:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0361,  0.6821,  0.1440,  ..., -0.3325,  0.4192, -0.0680],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 1.0850,  0.7148,  0.1290,  ..., -0.3367,  0.4121, -0.1016],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0468, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2695,  3.5176,  3.2070,  ...,  2.5195, -1.3369,  3.7598],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.5099e-02, -3.4119e-02,  2.2919e-02,  ...,  2.5749e-03,
         -1.4824e-02,  2.6703e-03],
        [ 2.1088e-02, -1.7944e-02,  2.4433e-03,  ..., -6.5918e-03,
         -2.4261e-03,  6.3782e-03],
        [ 1.8082e-03,  3.2005e-03, -1.8997e-02,  ...,  6.3705e-03,
          2.7298e-02, -3.1891e-03],
        ...,
        [-8.5144e-03,  7.6294e-05, -2.3773e-02,  ..., -3.3875e-02,
         -1.4450e-02,  1.5228e-02],
        [-1.4526e-02,  6.0997e-03,  1.0529e-02,  ..., -8.6823e-03,
         -2.3224e-02,  6.6757e-03],
        [-1.8501e-03,  1.6205e-02,  6.6605e-03,  ..., -4.8943e-03,
          2.1271e-02, -2.9053e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.2907e-03, -5.9605e-04,  8.4019e-04,  ...,  1.4658e-03,
         -1.0657e-04,  6.7425e-04],
        [ 2.8801e-03, -3.4847e-03,  3.1147e-03,  ..., -1.0090e-03,
          3.6526e-03,  1.5545e-03],
        [ 7.6103e-04,  1.7071e-03, -2.0180e-03,  ...,  1.3742e-03,
          1.8501e-03,  4.1103e-04],
        ...,
        [ 2.2125e-03, -2.6073e-03,  2.1057e-03,  ..., -7.5531e-04,
         -3.2616e-04, -1.2732e-03],
        [-2.4948e-03,  5.7869e-03,  2.4533e-04,  ..., -1.4305e-04,
         -2.2907e-03,  1.5917e-03],
        [ 1.2100e-04,  1.6701e-04,  3.0403e-03,  ..., -3.6573e-04,
         -4.4703e-05, -4.4403e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3213,  2.5742,  2.4844,  ...,  1.9717, -0.6733,  4.0078]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.4116,  0.5776,  0.0255,  ..., -0.1593, -0.2030,  0.0877]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 20:22:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is attending, it has attended
After something is asking, it has asked
After something is announcing, it has announced
After something is containing, it has contained
After something is operating, it has operated
After something is continuing, it has continued
After something is describing, it has described
After something is spending, it has
2024-07-09 20:22:25 root INFO     total operator prediction time: 4064.2383484840393 seconds
2024-07-09 20:22:25 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_inf - Ved
2024-07-09 20:22:27 root INFO     building operator verb_inf - Ved
2024-07-09 20:22:27 root INFO     [order_1_approx] starting weight calculation for If the present form is establish, the past form is established
If the present form is relate, the past form is related
If the present form is introduce, the past form is introduced
If the present form is understand, the past form is understood
If the present form is decide, the past form is decided
If the present form is refer, the past form is referred
If the present form is marry, the past form is married
If the present form is follow, the past form is
2024-07-09 20:22:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 20:26:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 20:30:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0356,  0.8398,  1.4912,  ...,  0.1921, -0.0698,  0.4407],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.0297,  0.8174,  1.3691,  ...,  0.1816, -0.0865,  0.4119],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.8223, -2.4219, -0.4756,  ..., -1.0361,  0.9775, -0.5225],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0245, -0.0146, -0.0032,  ...,  0.0166, -0.0159, -0.0170],
        [-0.0032, -0.0196, -0.0020,  ..., -0.0028,  0.0006, -0.0052],
        [ 0.0059, -0.0058, -0.0119,  ..., -0.0089, -0.0042, -0.0022],
        ...,
        [-0.0058, -0.0015, -0.0029,  ..., -0.0148,  0.0027, -0.0009],
        [-0.0034, -0.0070,  0.0061,  ...,  0.0047, -0.0206, -0.0127],
        [-0.0092,  0.0054,  0.0102,  ...,  0.0047, -0.0012, -0.0160]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-5.2834e-04, -2.7370e-04,  1.2255e-03,  ...,  2.0733e-03,
         -1.0643e-03, -6.8998e-04],
        [ 2.2066e-04, -4.9877e-04,  3.3236e-04,  ..., -4.8399e-04,
          4.1103e-04, -6.8903e-05],
        [ 1.3895e-03, -8.0442e-04,  1.8244e-03,  ..., -2.6169e-03,
         -9.6512e-04,  1.3952e-03],
        ...,
        [ 2.6441e-04, -3.2043e-04,  3.8075e-04,  ..., -2.6436e-03,
          1.2608e-03, -3.0923e-04],
        [-1.2817e-03, -8.0109e-04, -1.0843e-03,  ..., -8.0156e-04,
          2.2531e-04, -1.5602e-03],
        [ 8.8453e-05,  1.2398e-04, -3.7384e-04,  ...,  1.5583e-03,
          4.0936e-04, -1.4172e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.0703, -2.9023, -0.2180,  ..., -1.0957,  1.3711,  0.3262]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0363, -0.1576,  0.2073,  ..., -0.0209, -0.2135,  0.4373]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 20:30:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is establish, the past form is established
If the present form is relate, the past form is related
If the present form is introduce, the past form is introduced
If the present form is understand, the past form is understood
If the present form is decide, the past form is decided
If the present form is refer, the past form is referred
If the present form is marry, the past form is married
If the present form is follow, the past form is
2024-07-09 20:30:52 root INFO     [order_1_approx] starting weight calculation for If the present form is decide, the past form is decided
If the present form is refer, the past form is referred
If the present form is marry, the past form is married
If the present form is establish, the past form is established
If the present form is introduce, the past form is introduced
If the present form is follow, the past form is followed
If the present form is relate, the past form is related
If the present form is understand, the past form is
2024-07-09 20:30:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 20:35:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 20:39:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2312, -0.0868,  0.3972,  ..., -0.1533,  0.7700, -0.3174],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.2216, -0.1091,  0.3604,  ..., -0.1453,  0.7290, -0.3481],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6313,  0.4453,  0.3066,  ..., -0.5898,  2.5742, -0.3135],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0115, -0.0221, -0.0102,  ...,  0.0005, -0.0090, -0.0145],
        [-0.0286, -0.0194, -0.0029,  ...,  0.0015,  0.0126,  0.0018],
        [ 0.0063,  0.0022, -0.0414,  ..., -0.0034,  0.0140, -0.0177],
        ...,
        [-0.0097,  0.0259,  0.0125,  ..., -0.0305, -0.0025,  0.0017],
        [ 0.0082,  0.0093,  0.0062,  ...,  0.0056, -0.0421,  0.0082],
        [ 0.0046,  0.0154,  0.0077,  ...,  0.0048,  0.0078, -0.0253]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-5.9414e-04, -8.7118e-04, -1.3990e-03,  ...,  4.2677e-05,
          6.5804e-04, -1.8177e-03],
        [-1.4114e-03,  2.4700e-04, -3.1281e-04,  ..., -6.2418e-04,
          4.3297e-04,  1.1053e-03],
        [ 1.0500e-03, -2.8133e-04, -9.2506e-04,  ..., -3.0994e-05,
         -2.8563e-04,  2.0790e-04],
        ...,
        [-4.0293e-04,  2.2900e-04,  4.0317e-04,  ...,  3.7766e-04,
          2.9335e-03, -1.2617e-03],
        [-1.2207e-03,  1.8969e-03, -7.6151e-04,  ..., -1.3995e-04,
         -1.1787e-03, -1.0805e-03],
        [-2.4891e-04, -1.2531e-03,  1.8816e-03,  ..., -5.0926e-04,
         -1.1997e-03, -4.5443e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2910,  0.9458,  0.5645,  ..., -0.6733,  3.5078, -0.0593]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0671, -0.1560, -0.0741,  ...,  0.2218, -0.1385,  0.2739]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 20:39:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is decide, the past form is decided
If the present form is refer, the past form is referred
If the present form is marry, the past form is married
If the present form is establish, the past form is established
If the present form is introduce, the past form is introduced
If the present form is follow, the past form is followed
If the present form is relate, the past form is related
If the present form is understand, the past form is
2024-07-09 20:39:12 root INFO     [order_1_approx] starting weight calculation for If the present form is decide, the past form is decided
If the present form is understand, the past form is understood
If the present form is establish, the past form is established
If the present form is relate, the past form is related
If the present form is refer, the past form is referred
If the present form is marry, the past form is married
If the present form is follow, the past form is followed
If the present form is introduce, the past form is
2024-07-09 20:39:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 20:43:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 20:47:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0381, -0.2258,  0.5991,  ..., -0.1223, -0.3594, -0.6470],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.0462, -0.2542,  0.5601,  ..., -0.1179, -0.3789, -0.6919],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1201, -1.7852, -1.4688,  ...,  3.2344, -0.3506,  2.8945],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0203, -0.0210,  0.0048,  ...,  0.0072, -0.0003,  0.0014],
        [-0.0148, -0.0154, -0.0095,  ...,  0.0036, -0.0024,  0.0094],
        [ 0.0164, -0.0150, -0.0315,  ..., -0.0184, -0.0219, -0.0136],
        ...,
        [ 0.0127,  0.0118, -0.0075,  ..., -0.0311, -0.0117,  0.0160],
        [ 0.0045,  0.0098,  0.0070,  ..., -0.0243, -0.0221,  0.0157],
        [-0.0085,  0.0044,  0.0024,  ...,  0.0056,  0.0176, -0.0162]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.6016e-03, -1.7481e-03,  1.5450e-03,  ..., -4.7684e-06,
          2.1210e-03, -1.1492e-04],
        [-6.4468e-04, -2.3499e-03,  2.2674e-04,  ...,  3.2759e-04,
          5.3024e-04,  2.3327e-03],
        [ 3.3493e-03, -2.9063e-04, -7.9155e-04,  ..., -2.4986e-03,
         -1.9464e-03,  1.3924e-04],
        ...,
        [ 1.4811e-03, -1.2360e-03,  1.0548e-03,  ..., -3.3760e-03,
          1.3657e-03,  7.4196e-04],
        [ 1.3332e-03,  5.3883e-04,  5.4479e-05,  ..., -1.3523e-03,
         -2.0027e-03,  2.8086e-04],
        [-1.3113e-03, -2.6608e-04,  1.2512e-03,  ...,  7.8154e-04,
         -3.7432e-05,  3.8743e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1938, -2.5586, -1.4941,  ...,  2.7793, -0.1008,  3.8301]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0851, -0.1609,  0.0906,  ...,  0.3640, -0.0242,  0.3596]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 20:47:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is decide, the past form is decided
If the present form is understand, the past form is understood
If the present form is establish, the past form is established
If the present form is relate, the past form is related
If the present form is refer, the past form is referred
If the present form is marry, the past form is married
If the present form is follow, the past form is followed
If the present form is introduce, the past form is
2024-07-09 20:47:35 root INFO     [order_1_approx] starting weight calculation for If the present form is follow, the past form is followed
If the present form is introduce, the past form is introduced
If the present form is establish, the past form is established
If the present form is understand, the past form is understood
If the present form is relate, the past form is related
If the present form is refer, the past form is referred
If the present form is marry, the past form is married
If the present form is decide, the past form is
2024-07-09 20:47:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 20:51:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 20:55:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3999,  0.3579, -0.2603,  ..., -0.0332,  0.2246, -0.5762],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3789,  0.3330, -0.2522,  ..., -0.0289,  0.1919, -0.5913],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.1621, -0.0102, -0.9888,  ..., -0.1792,  0.3560, -1.7393],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0255, -0.0284, -0.0005,  ...,  0.0068, -0.0126, -0.0192],
        [-0.0089, -0.0170,  0.0008,  ..., -0.0014,  0.0117, -0.0010],
        [-0.0019, -0.0051, -0.0146,  ..., -0.0074,  0.0073, -0.0040],
        ...,
        [-0.0148, -0.0053,  0.0050,  ..., -0.0467,  0.0187, -0.0018],
        [ 0.0230,  0.0046,  0.0067,  ..., -0.0130, -0.0459, -0.0021],
        [-0.0237,  0.0098,  0.0230,  ...,  0.0046, -0.0004, -0.0175]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0006, -0.0027,  0.0005,  ..., -0.0006,  0.0019, -0.0008],
        [-0.0002, -0.0035,  0.0006,  ..., -0.0008, -0.0022,  0.0022],
        [ 0.0011, -0.0015, -0.0016,  ..., -0.0025, -0.0002,  0.0019],
        ...,
        [ 0.0004, -0.0008,  0.0014,  ..., -0.0043,  0.0014, -0.0018],
        [-0.0003,  0.0007, -0.0009,  ..., -0.0024, -0.0005, -0.0010],
        [-0.0021,  0.0006,  0.0031,  ...,  0.0007, -0.0001, -0.0012]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4062, -0.5952,  0.0239,  ..., -0.2438,  0.6631, -1.1914]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0397,  0.1075,  0.3574,  ...,  0.2996, -0.0454,  0.2896]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 20:56:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is follow, the past form is followed
If the present form is introduce, the past form is introduced
If the present form is establish, the past form is established
If the present form is understand, the past form is understood
If the present form is relate, the past form is related
If the present form is refer, the past form is referred
If the present form is marry, the past form is married
If the present form is decide, the past form is
2024-07-09 20:56:00 root INFO     [order_1_approx] starting weight calculation for If the present form is decide, the past form is decided
If the present form is understand, the past form is understood
If the present form is relate, the past form is related
If the present form is follow, the past form is followed
If the present form is introduce, the past form is introduced
If the present form is marry, the past form is married
If the present form is establish, the past form is established
If the present form is refer, the past form is
2024-07-09 20:56:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 21:00:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 21:04:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1504,  0.6196, -0.1604,  ..., -1.0078,  0.6299, -0.2903],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 1.0420,  0.5591, -0.1512,  ..., -0.8750,  0.5449, -0.2939],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3396, -0.4575,  2.0273,  ...,  3.7734,  1.8984, -1.8555],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0273, -0.0197, -0.0019,  ...,  0.0081, -0.0059,  0.0078],
        [-0.0113, -0.0138,  0.0046,  ..., -0.0071, -0.0003,  0.0076],
        [-0.0106, -0.0114, -0.0155,  ..., -0.0097, -0.0032, -0.0168],
        ...,
        [-0.0132,  0.0030, -0.0171,  ..., -0.0349,  0.0124,  0.0012],
        [-0.0104,  0.0051,  0.0301,  ..., -0.0153, -0.0219,  0.0098],
        [-0.0041,  0.0158,  0.0075,  ...,  0.0090,  0.0104, -0.0090]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0006,  0.0009,  0.0002,  ...,  0.0014,  0.0015,  0.0014],
        [-0.0003, -0.0012,  0.0013,  ..., -0.0003, -0.0007,  0.0027],
        [ 0.0010, -0.0016,  0.0007,  ..., -0.0017, -0.0005,  0.0004],
        ...,
        [ 0.0010, -0.0008,  0.0025,  ..., -0.0015,  0.0009, -0.0003],
        [-0.0010, -0.0005,  0.0018,  ..., -0.0015, -0.0004, -0.0011],
        [-0.0002, -0.0008,  0.0011,  ..., -0.0008,  0.0008, -0.0008]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2979, -0.2130,  1.7852,  ...,  4.1250,  2.8438, -1.0977]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0083,  0.0630,  0.2299,  ...,  0.0469, -0.0973,  0.2137]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 21:04:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is decide, the past form is decided
If the present form is understand, the past form is understood
If the present form is relate, the past form is related
If the present form is follow, the past form is followed
If the present form is introduce, the past form is introduced
If the present form is marry, the past form is married
If the present form is establish, the past form is established
If the present form is refer, the past form is
2024-07-09 21:04:23 root INFO     [order_1_approx] starting weight calculation for If the present form is understand, the past form is understood
If the present form is establish, the past form is established
If the present form is decide, the past form is decided
If the present form is introduce, the past form is introduced
If the present form is follow, the past form is followed
If the present form is refer, the past form is referred
If the present form is marry, the past form is married
If the present form is relate, the past form is
2024-07-09 21:04:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 21:08:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 21:12:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2263,  0.2554,  0.7612,  ..., -0.0625,  0.2803, -0.8086],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.2063,  0.2257,  0.6650,  ..., -0.0531,  0.2379, -0.7944],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6143, -0.5645, -1.3359,  ...,  0.5405,  0.9658,  1.1602],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.6154e-02, -2.1011e-02,  1.4374e-02,  ...,  5.4855e-03,
         -1.2123e-02, -9.8267e-03],
        [-5.7831e-03, -2.8275e-02,  7.4921e-03,  ...,  9.8877e-03,
          9.1934e-03,  1.0185e-02],
        [ 1.4229e-02, -1.0445e-02, -2.1072e-02,  ..., -2.7313e-03,
         -9.5596e-03, -2.3300e-02],
        ...,
        [-9.0485e-03,  1.1185e-02, -7.3700e-03,  ..., -3.5828e-02,
          2.2217e-02,  1.2207e-03],
        [ 2.2640e-03, -3.0518e-05, -3.1662e-04,  ..., -9.4452e-03,
         -2.1317e-02, -1.0254e-02],
        [-6.1035e-03,  1.4610e-02, -2.2125e-03,  ...,  4.2343e-03,
          1.3153e-02, -2.4841e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 4.9829e-04, -1.4172e-03, -4.7803e-04,  ...,  2.6846e-04,
          1.4114e-04,  1.1759e-03],
        [-1.3943e-03, -1.0548e-03,  1.9073e-03,  ..., -3.8838e-04,
          6.6757e-05,  3.5801e-03],
        [ 1.8530e-03, -2.2936e-04, -1.4753e-03,  ..., -4.0321e-03,
         -5.1403e-04, -1.9989e-03],
        ...,
        [ 9.2077e-04,  5.8365e-04,  3.4981e-03,  ..., -3.7556e-03,
          4.4441e-03, -1.2150e-03],
        [-1.2655e-03, -1.0300e-04,  6.8188e-04,  ..., -9.1934e-04,
         -7.6199e-04, -1.6289e-03],
        [-6.8188e-05,  2.1286e-03,  3.2568e-04,  ...,  7.2241e-04,
         -1.4162e-04, -2.0485e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8096, -0.5415, -1.0645,  ...,  0.9888,  2.0195,  1.3066]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0095,  0.1428,  0.1833,  ...,  0.1715, -0.3564,  0.2341]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 21:12:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is understand, the past form is understood
If the present form is establish, the past form is established
If the present form is decide, the past form is decided
If the present form is introduce, the past form is introduced
If the present form is follow, the past form is followed
If the present form is refer, the past form is referred
If the present form is marry, the past form is married
If the present form is relate, the past form is
2024-07-09 21:12:40 root INFO     [order_1_approx] starting weight calculation for If the present form is marry, the past form is married
If the present form is understand, the past form is understood
If the present form is refer, the past form is referred
If the present form is decide, the past form is decided
If the present form is introduce, the past form is introduced
If the present form is follow, the past form is followed
If the present form is relate, the past form is related
If the present form is establish, the past form is
2024-07-09 21:12:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 21:16:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 21:21:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0359, -0.1396,  0.2085,  ...,  0.6357,  0.1516, -0.7607],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.0289, -0.1670,  0.1879,  ...,  0.6216,  0.1299, -0.8193],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7012, -1.2607, -1.0947,  ...,  1.0176,  0.0029,  1.2646],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0187, -0.0290,  0.0014,  ...,  0.0079, -0.0033, -0.0124],
        [-0.0164, -0.0186, -0.0055,  ...,  0.0069, -0.0025,  0.0090],
        [ 0.0066,  0.0011, -0.0250,  ..., -0.0149, -0.0047, -0.0010],
        ...,
        [-0.0057,  0.0050, -0.0046,  ..., -0.0243, -0.0020, -0.0022],
        [-0.0130, -0.0020,  0.0206,  ..., -0.0023, -0.0113,  0.0129],
        [-0.0029,  0.0089,  0.0029,  ..., -0.0130,  0.0146, -0.0090]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.8915e-03, -3.3112e-03,  1.9035e-03,  ..., -8.8930e-04,
          1.9302e-03, -1.5154e-03],
        [-1.2703e-03, -2.5158e-03,  6.2132e-04,  ...,  1.0357e-03,
         -1.7729e-03,  2.0733e-03],
        [ 1.5669e-03,  2.1744e-04, -2.0828e-03,  ..., -1.5726e-03,
         -2.1744e-04,  1.8797e-03],
        ...,
        [ 1.6384e-03, -1.0281e-03, -4.8876e-04,  ..., -1.6413e-03,
          2.0618e-03, -1.1902e-03],
        [-2.9349e-04,  1.1492e-04,  7.6675e-04,  ..., -1.1559e-03,
         -2.8586e-04, -9.2983e-04],
        [-1.5378e-04, -6.0892e-04,  3.8910e-03,  ...,  6.3419e-05,
         -1.7986e-03,  3.5119e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0156, -1.5977, -0.8457,  ...,  0.6924,  0.2284,  1.2734]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[0.2642, 0.0845, 0.0398,  ..., 0.1259, 0.0205, 0.2139]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 21:21:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is marry, the past form is married
If the present form is understand, the past form is understood
If the present form is refer, the past form is referred
If the present form is decide, the past form is decided
If the present form is introduce, the past form is introduced
If the present form is follow, the past form is followed
If the present form is relate, the past form is related
If the present form is establish, the past form is
2024-07-09 21:21:02 root INFO     [order_1_approx] starting weight calculation for If the present form is follow, the past form is followed
If the present form is introduce, the past form is introduced
If the present form is understand, the past form is understood
If the present form is refer, the past form is referred
If the present form is establish, the past form is established
If the present form is decide, the past form is decided
If the present form is relate, the past form is related
If the present form is marry, the past form is
2024-07-09 21:21:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 21:25:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 21:29:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0508,  0.1873,  0.1312,  ..., -0.1091,  0.0076,  0.2456],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.0425,  0.1643,  0.1064,  ..., -0.0986, -0.0138,  0.2141],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5425, -3.3809, -3.7090,  ..., -2.3438,  2.2148, -2.5625],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0202, -0.0059, -0.0058,  ...,  0.0100, -0.0101, -0.0324],
        [ 0.0050, -0.0178, -0.0132,  ...,  0.0065,  0.0182, -0.0013],
        [ 0.0141, -0.0093, -0.0031,  ..., -0.0011, -0.0040, -0.0145],
        ...,
        [-0.0176, -0.0168, -0.0174,  ..., -0.0435, -0.0002, -0.0079],
        [ 0.0232,  0.0012,  0.0092,  ..., -0.0040,  0.0028, -0.0083],
        [ 0.0057, -0.0019,  0.0167,  ...,  0.0055,  0.0103, -0.0262]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.4896e-03, -1.4744e-03, -1.8902e-03,  ..., -7.2670e-04,
          1.9627e-03, -2.1439e-03],
        [ 1.1463e-03, -1.0014e-03,  1.9550e-04,  ...,  1.2898e-04,
         -1.3905e-03,  2.9964e-03],
        [ 7.6628e-04,  4.3809e-05, -2.3413e-04,  ..., -1.6403e-03,
         -2.3973e-04,  8.6021e-04],
        ...,
        [ 1.4191e-03, -1.0653e-03,  4.7493e-04,  ..., -1.2016e-03,
          4.7684e-04, -2.8439e-03],
        [ 2.0351e-03,  1.2016e-03,  1.0977e-03,  ...,  1.0252e-03,
          2.2125e-03, -4.7803e-04],
        [ 1.9932e-04,  1.2064e-03,  2.9488e-03,  ...,  1.0109e-03,
          1.1787e-03, -2.5158e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3525, -3.8164, -2.8105,  ..., -2.7402,  3.1758, -2.0078]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.2749, -0.0450, -0.0620,  ...,  0.1133, -0.1160,  0.2590]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 21:29:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is follow, the past form is followed
If the present form is introduce, the past form is introduced
If the present form is understand, the past form is understood
If the present form is refer, the past form is referred
If the present form is establish, the past form is established
If the present form is decide, the past form is decided
If the present form is relate, the past form is related
If the present form is marry, the past form is
2024-07-09 21:29:25 root INFO     total operator prediction time: 4018.220869541168 seconds
2024-07-09 21:29:25 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_inf - 3pSg
2024-07-09 21:29:28 root INFO     building operator verb_inf - 3pSg
2024-07-09 21:29:28 root INFO     [order_1_approx] starting weight calculation for I consist, he consists
I remember, he remembers
I represent, he represents
I explain, he explains
I enable, he enables
I operate, he operates
I involve, he involves
I describe, he
2024-07-09 21:29:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 21:33:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 21:37:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5532, -0.2920,  1.2246,  ..., -0.5527, -0.3875,  0.4795],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.5522, -0.3186,  1.1621,  ..., -0.5293, -0.4031,  0.4673],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0078, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.0156, -2.9316, -0.7344,  ...,  3.5059, -2.0215,  3.0996],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.6306e-02, -3.5492e-02,  1.6708e-02,  ..., -1.2756e-02,
         -2.3819e-02, -1.6235e-02],
        [-1.6052e-02, -4.0222e-02,  5.8022e-03,  ..., -6.8245e-03,
          7.3166e-03, -5.7220e-04],
        [-3.4332e-05,  7.4120e-03, -3.1433e-02,  ...,  2.2430e-02,
         -4.6616e-03,  7.6828e-03],
        ...,
        [-9.5444e-03, -5.2376e-03, -1.9958e-02,  ..., -3.3173e-02,
         -1.9058e-02, -1.9470e-02],
        [ 7.9498e-03, -5.0545e-03,  4.8126e-02,  ..., -1.7029e-02,
         -2.7634e-02,  1.2650e-02],
        [ 5.2872e-03,  3.9948e-02,  1.4709e-02,  ..., -6.0158e-03,
          1.9684e-03, -2.4612e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0026, -0.0057,  0.0008,  ..., -0.0004, -0.0015, -0.0008],
        [-0.0009, -0.0038, -0.0002,  ..., -0.0002,  0.0022, -0.0015],
        [-0.0005,  0.0008, -0.0011,  ..., -0.0008, -0.0013,  0.0010],
        ...,
        [ 0.0021,  0.0002,  0.0012,  ..., -0.0017,  0.0034, -0.0011],
        [ 0.0006, -0.0018,  0.0025,  ..., -0.0010, -0.0021,  0.0011],
        [-0.0007, -0.0006,  0.0006,  ..., -0.0004, -0.0039,  0.0013]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.0781, -2.4434, -0.4077,  ...,  3.8125, -3.1211,  3.6797]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2615, -0.0068,  0.1053,  ...,  0.1766,  0.1156,  0.1746]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 21:37:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I consist, he consists
I remember, he remembers
I represent, he represents
I explain, he explains
I enable, he enables
I operate, he operates
I involve, he involves
I describe, he
2024-07-09 21:37:55 root INFO     [order_1_approx] starting weight calculation for I operate, he operates
I represent, he represents
I describe, he describes
I involve, he involves
I explain, he explains
I remember, he remembers
I enable, he enables
I consist, he
2024-07-09 21:37:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 21:42:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 21:46:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5601, -0.6880,  1.1602,  ..., -0.5977,  0.6489,  0.2507],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.5781, -0.7388,  1.1172,  ..., -0.5825,  0.6323,  0.2356],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0078, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.7480, -1.5361, -2.6914,  ...,  4.7539, -5.7148,  3.2656],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0356, -0.0048,  0.0211,  ...,  0.0156, -0.0024, -0.0129],
        [ 0.0046, -0.0250,  0.0168,  ...,  0.0008,  0.0138,  0.0039],
        [ 0.0062,  0.0136, -0.0352,  ...,  0.0141,  0.0108, -0.0117],
        ...,
        [-0.0297, -0.0126, -0.0325,  ..., -0.0375,  0.0189,  0.0076],
        [ 0.0135,  0.0203,  0.0518,  ..., -0.0133, -0.0388,  0.0134],
        [-0.0104,  0.0151, -0.0080,  ..., -0.0155, -0.0051, -0.0437]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-3.5248e-03, -1.5068e-03, -5.9986e-04,  ..., -1.1206e-05,
         -5.7936e-04, -1.1053e-03],
        [ 2.7313e-03, -2.9907e-03,  2.6188e-03,  ..., -1.9407e-03,
          3.9711e-03,  2.1152e-03],
        [ 2.0542e-03, -3.1114e-04, -1.7614e-03,  ...,  1.4496e-03,
          5.7793e-04, -1.8120e-04],
        ...,
        [ 9.3222e-04,  1.9646e-03,  1.5955e-03,  ...,  2.2106e-03,
         -7.6866e-04, -1.3809e-03],
        [ 1.5526e-03,  2.4929e-03,  3.1967e-03,  ..., -9.8133e-04,
         -1.3685e-04,  3.7575e-04],
        [-1.6737e-03,  1.1692e-03, -4.7350e-04,  ...,  6.6042e-04,
         -3.0918e-03, -1.9054e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.6445, -1.5664, -3.8984,  ...,  4.6367, -6.4375,  2.0371]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[0.0468, 0.2576, 0.1331,  ..., 0.3726, 0.1945, 0.1444]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 21:46:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I operate, he operates
I represent, he represents
I describe, he describes
I involve, he involves
I explain, he explains
I remember, he remembers
I enable, he enables
I consist, he
2024-07-09 21:46:23 root INFO     [order_1_approx] starting weight calculation for I describe, he describes
I explain, he explains
I involve, he involves
I consist, he consists
I enable, he enables
I remember, he remembers
I represent, he represents
I operate, he
2024-07-09 21:46:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 21:50:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 21:54:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0179, -1.0625,  0.9590,  ..., -0.6528,  0.2512,  0.8657],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.0231, -1.0918,  0.8892,  ..., -0.6143,  0.2239,  0.8452],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0078, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9126, -4.2656,  0.3057,  ...,  2.6914, -1.7168,  3.4414],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0146, -0.0082,  0.0161,  ..., -0.0023, -0.0093, -0.0218],
        [-0.0002, -0.0190,  0.0179,  ..., -0.0062,  0.0072, -0.0107],
        [-0.0212, -0.0122, -0.0348,  ..., -0.0052,  0.0041,  0.0061],
        ...,
        [-0.0141, -0.0284, -0.0286,  ..., -0.0377, -0.0142,  0.0073],
        [ 0.0011,  0.0052,  0.0222,  ..., -0.0168, -0.0186,  0.0083],
        [ 0.0037,  0.0201, -0.0058,  ..., -0.0032,  0.0133, -0.0393]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 6.1798e-04, -3.2196e-03,  1.0815e-03,  ...,  4.0650e-04,
         -9.4175e-05, -3.0651e-03],
        [ 9.4843e-04, -2.6798e-03,  2.4891e-03,  ...,  2.0361e-04,
          1.4601e-03, -2.1057e-03],
        [-8.8739e-04, -5.1308e-04, -2.0752e-03,  ...,  6.9427e-04,
          1.0624e-03,  1.3380e-03],
        ...,
        [ 1.3981e-03, -1.7023e-03,  2.4452e-03,  ..., -1.8215e-04,
          1.5831e-03, -7.9203e-04],
        [ 9.6512e-04,  5.4073e-04,  8.7786e-04,  ..., -8.1253e-04,
         -6.0272e-04, -6.4659e-04],
        [ 5.0116e-04, -3.2306e-05, -1.9417e-03,  ..., -4.9639e-04,
         -1.1044e-03,  2.5272e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4150, -3.7793,  1.5332,  ...,  3.0195, -1.8047,  3.9199]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2627, -0.2639,  0.1220,  ...,  0.0016, -0.0319, -0.0712]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 21:54:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I describe, he describes
I explain, he explains
I involve, he involves
I consist, he consists
I enable, he enables
I remember, he remembers
I represent, he represents
I operate, he
2024-07-09 21:54:51 root INFO     [order_1_approx] starting weight calculation for I involve, he involves
I consist, he consists
I enable, he enables
I remember, he remembers
I describe, he describes
I represent, he represents
I operate, he operates
I explain, he
2024-07-09 21:54:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 21:59:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 22:03:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2993, -0.3428,  1.9814,  ..., -0.6372, -0.1577,  0.0967],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.3159, -0.3826,  1.9453,  ..., -0.6304, -0.1812,  0.0770],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0078, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.0137, -1.9600,  1.7090,  ...,  3.6641, -1.1855,  6.5820],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0123, -0.0198,  0.0087,  ...,  0.0066, -0.0017, -0.0185],
        [-0.0218, -0.0336, -0.0101,  ..., -0.0122, -0.0036,  0.0015],
        [ 0.0128, -0.0085, -0.0262,  ...,  0.0194,  0.0104, -0.0004],
        ...,
        [ 0.0067, -0.0097, -0.0023,  ..., -0.0205, -0.0184, -0.0068],
        [ 0.0268,  0.0076,  0.0135,  ..., -0.0036, -0.0152,  0.0099],
        [-0.0012,  0.0209,  0.0060,  ..., -0.0117, -0.0017, -0.0164]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0034, -0.0055, -0.0004,  ...,  0.0007,  0.0013, -0.0014],
        [-0.0022, -0.0045,  0.0013,  ...,  0.0022,  0.0034, -0.0014],
        [ 0.0002,  0.0007, -0.0009,  ...,  0.0003,  0.0005,  0.0005],
        ...,
        [ 0.0035, -0.0014,  0.0016,  ..., -0.0012,  0.0032, -0.0007],
        [ 0.0014, -0.0002,  0.0014,  ...,  0.0002, -0.0021, -0.0006],
        [ 0.0005,  0.0014, -0.0006,  ..., -0.0019, -0.0025,  0.0017]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.5840, -2.7363,  1.2227,  ...,  3.9453, -0.4980,  5.9609]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.7944,  0.0211, -0.0861,  ...,  0.5688, -0.1328,  0.1514]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 22:03:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I involve, he involves
I consist, he consists
I enable, he enables
I remember, he remembers
I describe, he describes
I represent, he represents
I operate, he operates
I explain, he
2024-07-09 22:03:18 root INFO     [order_1_approx] starting weight calculation for I remember, he remembers
I involve, he involves
I describe, he describes
I enable, he enables
I operate, he operates
I explain, he explains
I consist, he consists
I represent, he
2024-07-09 22:03:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 22:07:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 22:11:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0195, -0.9453,  1.0586,  ..., -1.2695, -0.1748, -0.0428],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 1.0439, -1.0156,  1.0254,  ..., -1.2500, -0.1980, -0.0709],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0078, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.5000, -2.3125,  0.4062,  ...,  3.5000, -0.9746,  6.3516],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 8.8806e-03, -1.4420e-02,  2.4033e-02,  ...,  7.6180e-03,
         -1.3840e-02, -3.8490e-03],
        [-7.9193e-03, -1.1017e-02,  7.3357e-03,  ...,  6.9885e-03,
          1.6815e-02, -8.1062e-05],
        [-1.5984e-03, -1.4168e-02, -2.5803e-02,  ...,  2.2202e-03,
          2.4071e-03, -1.4519e-02],
        ...,
        [-6.2485e-03, -3.0212e-03, -3.0518e-03,  ..., -2.2934e-02,
         -6.6757e-03,  7.2479e-05],
        [ 1.1887e-02, -6.4812e-03, -5.0812e-03,  ..., -2.6947e-02,
         -3.1708e-02,  8.3160e-03],
        [ 2.3270e-04,  1.1581e-02, -2.6283e-03,  ...,  7.2937e-03,
          1.4008e-02, -1.3748e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0024, -0.0020,  0.0022,  ..., -0.0015,  0.0007, -0.0011],
        [-0.0012, -0.0007,  0.0010,  ...,  0.0004,  0.0022, -0.0005],
        [-0.0018, -0.0015, -0.0007,  ..., -0.0023, -0.0002,  0.0015],
        ...,
        [ 0.0017, -0.0020,  0.0019,  ...,  0.0004,  0.0007, -0.0013],
        [ 0.0032, -0.0009,  0.0002,  ...,  0.0003, -0.0015,  0.0005],
        [-0.0016, -0.0009,  0.0010,  ...,  0.0001, -0.0006,  0.0008]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.6992, -2.1484,  0.7710,  ...,  3.3281, -1.1660,  6.8281]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0341, -0.2095,  0.0651,  ...,  0.1649,  0.0412, -0.1105]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 22:11:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I remember, he remembers
I involve, he involves
I describe, he describes
I enable, he enables
I operate, he operates
I explain, he explains
I consist, he consists
I represent, he
2024-07-09 22:11:43 root INFO     [order_1_approx] starting weight calculation for I enable, he enables
I describe, he describes
I explain, he explains
I operate, he operates
I consist, he consists
I involve, he involves
I represent, he represents
I remember, he
2024-07-09 22:11:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 22:16:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 22:20:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5571, -0.2620,  0.5767,  ..., -0.2705,  0.2322,  0.2900],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.5259, -0.2742,  0.5112,  ..., -0.2444,  0.1978,  0.2566],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0078, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.6699, -1.5391, -2.6719,  ...,  1.4346, -1.3008,  6.9258],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0124, -0.0116,  0.0042,  ...,  0.0085,  0.0003, -0.0172],
        [-0.0102, -0.0159,  0.0181,  ...,  0.0099,  0.0099,  0.0123],
        [-0.0135,  0.0147, -0.0442,  ...,  0.0120,  0.0062, -0.0034],
        ...,
        [ 0.0130, -0.0080, -0.0179,  ..., -0.0236, -0.0162,  0.0095],
        [ 0.0113, -0.0029,  0.0045,  ..., -0.0038, -0.0237, -0.0086],
        [-0.0111,  0.0155,  0.0017,  ..., -0.0010,  0.0060, -0.0301]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-4.9305e-04, -1.0881e-03, -1.2693e-03,  ...,  6.2037e-04,
         -1.0222e-04,  1.1468e-04],
        [ 1.0681e-03,  8.3256e-04, -1.7643e-04,  ...,  9.1076e-04,
          2.6817e-03, -6.2990e-04],
        [-1.4954e-03, -3.3331e-04, -3.4466e-03,  ..., -1.7891e-03,
         -2.0828e-03,  9.6083e-05],
        ...,
        [ 2.7618e-03,  1.9741e-03,  6.8474e-04,  ..., -1.6747e-03,
          1.3685e-03, -9.2983e-06],
        [ 7.0286e-04, -6.4611e-05,  6.3229e-04,  ...,  1.9479e-04,
         -1.0920e-03, -9.1267e-04],
        [-1.1358e-03,  1.6665e-04, -9.1076e-04,  ..., -9.3651e-04,
         -2.9624e-05,  8.6546e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.0352, -1.9863, -2.1914,  ...,  2.0332, -1.0352,  6.8320]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1605, -0.0459,  0.3069,  ...,  0.3586, -0.2659,  0.3879]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 22:20:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I enable, he enables
I describe, he describes
I explain, he explains
I operate, he operates
I consist, he consists
I involve, he involves
I represent, he represents
I remember, he
2024-07-09 22:20:10 root INFO     [order_1_approx] starting weight calculation for I describe, he describes
I involve, he involves
I operate, he operates
I remember, he remembers
I explain, he explains
I consist, he consists
I represent, he represents
I enable, he
2024-07-09 22:20:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 22:24:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 22:28:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1550, -0.2788,  0.8311,  ...,  0.2563, -0.3320,  0.0813],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1627, -0.3083,  0.7910,  ...,  0.2515, -0.3518,  0.0589],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0078, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.3477, -1.6963,  5.4102,  ...,  2.1758, -1.4277,  5.8945],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0054, -0.0316, -0.0079,  ..., -0.0020,  0.0091, -0.0186],
        [ 0.0105, -0.0348,  0.0049,  ..., -0.0027,  0.0101,  0.0097],
        [ 0.0048, -0.0131, -0.0236,  ...,  0.0061,  0.0195,  0.0086],
        ...,
        [-0.0073, -0.0088, -0.0053,  ..., -0.0170, -0.0069, -0.0020],
        [-0.0041,  0.0015,  0.0129,  ..., -0.0181, -0.0192,  0.0150],
        [-0.0149,  0.0317,  0.0033,  ..., -0.0258, -0.0017, -0.0184]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.0643e-03, -4.9210e-03, -1.0738e-03,  ..., -1.5621e-03,
          2.9240e-03, -3.8242e-04],
        [-2.0905e-03, -6.7635e-03,  1.1845e-03,  ...,  1.9503e-03,
          2.5921e-03, -8.0299e-04],
        [ 6.3419e-05,  7.5626e-04, -2.9163e-03,  ...,  7.5102e-04,
         -8.8167e-04,  2.5959e-03],
        ...,
        [ 3.4962e-03, -1.2293e-03,  1.6012e-03,  ..., -6.4707e-04,
          1.6632e-03, -8.2302e-04],
        [ 4.9496e-04, -1.3542e-04, -1.5378e-04,  ..., -5.9080e-04,
         -1.5888e-03,  2.4748e-04],
        [-3.1519e-04,  1.6899e-03, -1.0939e-03,  ..., -1.9913e-03,
         -1.5144e-03,  8.1301e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1289, -1.3486,  5.6250,  ...,  2.3730, -1.3467,  6.2500]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2498,  0.0895,  0.0573,  ...,  0.1335,  0.1536, -0.0018]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 22:28:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I describe, he describes
I involve, he involves
I operate, he operates
I remember, he remembers
I explain, he explains
I consist, he consists
I represent, he represents
I enable, he
2024-07-09 22:28:38 root INFO     [order_1_approx] starting weight calculation for I describe, he describes
I consist, he consists
I explain, he explains
I represent, he represents
I enable, he enables
I operate, he operates
I remember, he remembers
I involve, he
2024-07-09 22:28:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 22:32:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 22:37:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9326, -0.4585,  1.1855,  ..., -0.7642, -0.2646,  1.0078],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.9688, -0.5112,  1.1680,  ..., -0.7642, -0.2942,  1.0479],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0078, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.1172, -1.1104, -1.1543,  ...,  1.7773, -0.1079,  3.5938],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-8.7204e-03,  9.2621e-03,  1.6205e-02,  ...,  2.9541e-02,
         -3.1433e-03, -1.4526e-02],
        [ 1.0452e-03, -1.9165e-02,  5.7144e-03,  ..., -1.3924e-03,
          2.3193e-02, -4.2648e-03],
        [ 1.6724e-02, -5.2872e-03, -3.3508e-02,  ...,  9.2087e-03,
         -5.3864e-03, -5.7983e-03],
        ...,
        [ 7.7286e-03, -8.0338e-03, -4.8218e-03,  ..., -8.3618e-03,
         -1.1108e-02,  1.6586e-02],
        [ 1.6693e-02,  3.0579e-02,  1.5869e-02,  ...,  6.1035e-05,
         -3.3783e-02,  5.7678e-03],
        [ 7.3814e-03,  1.3885e-03,  1.5099e-02,  ..., -1.1009e-02,
         -4.0436e-04, -1.9424e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0018, -0.0026,  0.0004,  ...,  0.0006,  0.0004, -0.0006],
        [-0.0011, -0.0050,  0.0012,  ...,  0.0006,  0.0028, -0.0010],
        [-0.0010, -0.0004, -0.0026,  ..., -0.0007,  0.0012,  0.0012],
        ...,
        [ 0.0012, -0.0009,  0.0005,  ..., -0.0009,  0.0020, -0.0011],
        [ 0.0014, -0.0003,  0.0011,  ..., -0.0010, -0.0035,  0.0006],
        [ 0.0002, -0.0012,  0.0012,  ...,  0.0002, -0.0020, -0.0009]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.2012, -1.2959, -0.0527,  ...,  0.5664,  0.5171,  3.3203]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.4263,  0.0716, -0.2585,  ...,  0.3237, -0.2603,  0.2198]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 22:37:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I describe, he describes
I consist, he consists
I explain, he explains
I represent, he represents
I enable, he enables
I operate, he operates
I remember, he remembers
I involve, he
2024-07-09 22:37:06 root INFO     total operator prediction time: 4057.943491458893 seconds
2024-07-09 22:37:06 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_Ving - 3pSg
2024-07-09 22:37:08 root INFO     building operator verb_Ving - 3pSg
2024-07-09 22:37:08 root INFO     [order_1_approx] starting weight calculation for When something is operating, it operates
When something is seeming, it seems
When something is enabling, it enables
When something is considering, it considers
When something is continuing, it continues
When something is representing, it represents
When something is becoming, it becomes
When something is including, it
2024-07-09 22:37:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 22:41:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 22:45:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6440, -0.9683,  0.8628,  ...,  0.3501,  0.1191,  0.8320],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.7026, -1.1162,  0.8921,  ...,  0.3694,  0.1061,  0.9077],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0158, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.7422, -0.9805, -1.0654,  ...,  2.7910,  0.5874, -1.6562],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0117, -0.0028,  0.0008,  ...,  0.0264, -0.0105, -0.0294],
        [-0.0019, -0.0228,  0.0168,  ..., -0.0018, -0.0109, -0.0069],
        [ 0.0280, -0.0182, -0.0021,  ...,  0.0005, -0.0176,  0.0169],
        ...,
        [-0.0008,  0.0108, -0.0212,  ...,  0.0060, -0.0075, -0.0090],
        [ 0.0185,  0.0114,  0.0038,  ..., -0.0252, -0.0228,  0.0118],
        [-0.0113,  0.0143,  0.0080,  ...,  0.0069,  0.0166, -0.0035]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-5.1193e-03, -1.3514e-03,  4.7035e-03,  ...,  8.4305e-04,
          6.7472e-04, -1.2512e-03],
        [-1.8191e-04, -1.5469e-03,  2.2030e-03,  ..., -4.3488e-04,
          1.3113e-05, -9.1934e-04],
        [ 1.5335e-03,  3.5048e-04, -3.7613e-03,  ..., -2.9545e-03,
         -1.9798e-03,  1.1358e-03],
        ...,
        [ 2.4891e-03, -1.3399e-03,  7.0524e-04,  ..., -6.5804e-04,
         -3.6764e-04, -1.6193e-03],
        [-4.9067e-04,  1.6108e-03,  2.8396e-04,  ..., -1.5965e-03,
         -2.7428e-03,  5.5408e-04],
        [ 6.8378e-04, -6.5231e-04, -5.1928e-04,  ...,  2.1381e-03,
          7.3147e-04, -3.6354e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.0234, -0.9033, -1.5898,  ...,  2.0801,  0.0215, -2.0918]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0319,  0.0376, -0.0304,  ..., -0.1122, -0.4382,  0.1132]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 22:45:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is operating, it operates
When something is seeming, it seems
When something is enabling, it enables
When something is considering, it considers
When something is continuing, it continues
When something is representing, it represents
When something is becoming, it becomes
When something is including, it
2024-07-09 22:45:29 root INFO     [order_1_approx] starting weight calculation for When something is operating, it operates
When something is continuing, it continues
When something is becoming, it becomes
When something is representing, it represents
When something is including, it includes
When something is seeming, it seems
When something is considering, it considers
When something is enabling, it
2024-07-09 22:45:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 22:49:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 22:53:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1069, -0.4863,  0.5830,  ...,  0.2957,  0.3774, -0.3479],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1125, -0.5063,  0.5259,  ...,  0.2754,  0.3389, -0.3735],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0158, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2285,  0.6602,  2.5918,  ...,  1.1084, -1.5547,  4.5703],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 7.5836e-03, -1.9684e-02,  5.0812e-03,  ...,  3.7384e-03,
          1.8244e-03, -1.5930e-02],
        [ 1.9588e-03, -1.5945e-02, -3.5343e-03,  ..., -9.8572e-03,
         -1.6174e-02,  4.2610e-03],
        [ 1.0406e-02, -1.2922e-03,  7.9803e-03,  ...,  6.2561e-03,
          1.3809e-02,  3.1319e-03],
        ...,
        [-6.5041e-03, -2.4433e-03, -1.4816e-02,  ..., -1.3939e-02,
         -1.1063e-04,  1.3733e-03],
        [ 1.3977e-02, -1.0101e-02,  2.1057e-03,  ..., -1.6357e-02,
         -2.8515e-03, -2.6703e-05],
        [ 2.3994e-03,  9.0027e-03, -1.0406e-02,  ...,  4.2000e-03,
         -9.5978e-03, -9.0256e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-3.5706e-03, -2.6226e-03, -3.2949e-04,  ..., -6.0797e-04,
         -1.8768e-03, -4.8399e-05],
        [ 5.2834e-04, -2.7847e-03,  1.8454e-03,  ..., -4.0388e-04,
          6.1274e-04,  1.5726e-03],
        [ 8.6784e-05,  1.7967e-03,  1.4000e-03,  ...,  1.1814e-04,
         -7.8011e-04,  4.1032e-04],
        ...,
        [ 1.9474e-03,  6.0081e-04,  3.3855e-04,  ..., -2.5225e-04,
         -2.6536e-04, -2.1000e-03],
        [-5.1546e-04,  9.4271e-04,  4.8637e-05,  ..., -1.3409e-03,
         -2.8305e-03,  1.6892e-04],
        [-6.7043e-04, -2.2030e-03,  9.9003e-05,  ...,  6.9618e-04,
         -1.2655e-03, -8.2302e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5518,  0.9165,  2.6348,  ...,  1.2920, -1.4336,  3.3281]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.3882,  0.0207,  0.2198,  ...,  0.0713, -0.0647,  0.0647]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 22:53:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is operating, it operates
When something is continuing, it continues
When something is becoming, it becomes
When something is representing, it represents
When something is including, it includes
When something is seeming, it seems
When something is considering, it considers
When something is enabling, it
2024-07-09 22:53:51 root INFO     [order_1_approx] starting weight calculation for When something is continuing, it continues
When something is including, it includes
When something is seeming, it seems
When something is considering, it considers
When something is enabling, it enables
When something is becoming, it becomes
When something is operating, it operates
When something is representing, it
2024-07-09 22:53:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 22:58:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 23:02:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0322, -0.9517,  0.4407,  ..., -0.2313,  0.4043,  0.3140],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 1.0479, -1.0205,  0.4131,  ..., -0.2286,  0.3831,  0.2979],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0158, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.9678, -0.4316,  0.1865,  ...,  1.7959,  0.4297,  4.0781],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0114, -0.0321,  0.0115,  ...,  0.0134,  0.0015, -0.0167],
        [-0.0030, -0.0123,  0.0047,  ..., -0.0047, -0.0045, -0.0075],
        [ 0.0108, -0.0114, -0.0077,  ..., -0.0037,  0.0061, -0.0006],
        ...,
        [-0.0104, -0.0006, -0.0153,  ..., -0.0287, -0.0013, -0.0073],
        [ 0.0099,  0.0079,  0.0032,  ..., -0.0229, -0.0179,  0.0069],
        [-0.0129,  0.0154,  0.0038,  ...,  0.0062, -0.0172, -0.0246]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-4.2305e-03,  1.7872e-03,  1.7176e-03,  ...,  1.6427e-04,
         -4.1699e-04,  1.0929e-03],
        [-1.4234e-04, -2.8744e-03,  2.2011e-03,  ..., -1.1861e-04,
         -2.1458e-04,  9.9564e-04],
        [-9.6369e-04, -9.3842e-04, -2.4033e-04,  ..., -2.8419e-03,
         -1.2827e-03,  1.5659e-03],
        ...,
        [ 1.7738e-03, -2.7924e-03,  2.4281e-03,  ..., -1.8969e-03,
         -2.1458e-05, -1.2531e-03],
        [-2.3508e-04,  1.2064e-03, -5.1546e-04,  ..., -1.1253e-03,
         -2.7142e-03,  4.3154e-05],
        [-1.7843e-03, -8.5735e-04,  1.0281e-03,  ...,  1.4019e-03,
         -1.3218e-03, -1.9054e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.0156, -0.8877, -0.1582,  ...,  0.6025,  0.9531,  1.9453]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1978,  0.1829,  0.0994,  ..., -0.0137, -0.4221, -0.1379]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 23:02:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is continuing, it continues
When something is including, it includes
When something is seeming, it seems
When something is considering, it considers
When something is enabling, it enables
When something is becoming, it becomes
When something is operating, it operates
When something is representing, it
2024-07-09 23:02:14 root INFO     [order_1_approx] starting weight calculation for When something is considering, it considers
When something is including, it includes
When something is enabling, it enables
When something is representing, it represents
When something is operating, it operates
When something is becoming, it becomes
When something is seeming, it seems
When something is continuing, it
2024-07-09 23:02:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 23:06:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 23:10:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3787, -0.5186,  0.4604,  ...,  0.0358,  1.4102, -0.2815],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3750, -0.5615,  0.4290,  ...,  0.0333,  1.3857, -0.3206],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0158, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1833, -2.0586, -1.2012,  ...,  0.5010, -1.0557,  2.0430],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.9531e-02, -1.3496e-02,  6.3858e-03,  ...,  8.4610e-03,
         -1.2909e-02, -1.6739e-02],
        [ 1.4715e-03, -4.2419e-03, -1.5778e-02,  ..., -1.5068e-02,
          4.0779e-03, -3.0003e-03],
        [ 1.8692e-02, -1.9169e-04, -3.1586e-02,  ..., -5.2261e-04,
          5.9395e-03,  4.6158e-03],
        ...,
        [-1.5381e-02, -3.5095e-04, -4.5776e-03,  ..., -1.2268e-02,
          4.0512e-03, -9.1629e-03],
        [ 1.7883e-02, -4.0245e-03, -3.8147e-05,  ..., -5.1117e-03,
         -2.4094e-02,  7.2517e-03],
        [-7.7438e-03,  1.2749e-02,  7.5417e-03,  ...,  6.1684e-03,
          9.5367e-03, -3.9124e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0055,  0.0002,  0.0017,  ..., -0.0016,  0.0010, -0.0004],
        [ 0.0004, -0.0021, -0.0017,  ...,  0.0005,  0.0006,  0.0003],
        [-0.0013, -0.0002,  0.0001,  ..., -0.0019, -0.0003, -0.0028],
        ...,
        [ 0.0002, -0.0012,  0.0020,  ..., -0.0020, -0.0002, -0.0004],
        [ 0.0008,  0.0020,  0.0002,  ..., -0.0004, -0.0011, -0.0012],
        [-0.0015, -0.0022,  0.0011,  ...,  0.0023, -0.0009, -0.0015]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0976, -1.5859, -0.9526,  ...,  0.7920, -0.0776,  1.6973]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1366, -0.2052,  0.3884,  ..., -0.2695, -0.3821,  0.3313]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 23:10:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is considering, it considers
When something is including, it includes
When something is enabling, it enables
When something is representing, it represents
When something is operating, it operates
When something is becoming, it becomes
When something is seeming, it seems
When something is continuing, it
2024-07-09 23:10:38 root INFO     [order_1_approx] starting weight calculation for When something is representing, it represents
When something is considering, it considers
When something is enabling, it enables
When something is continuing, it continues
When something is including, it includes
When something is becoming, it becomes
When something is seeming, it seems
When something is operating, it
2024-07-09 23:10:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 23:14:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 23:19:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3352, -0.9414,  0.8521,  ..., -0.1841,  1.0146, -0.1760],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.3289, -0.9448,  0.7646,  ..., -0.1683,  0.9365, -0.1992],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0158, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2871, -1.7109,  0.9873,  ...,  0.5806,  0.7363,  1.7637],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0046, -0.0059, -0.0024,  ...,  0.0088, -0.0061, -0.0097],
        [-0.0167, -0.0080,  0.0087,  ..., -0.0061, -0.0164,  0.0062],
        [ 0.0062,  0.0007, -0.0062,  ...,  0.0102,  0.0102,  0.0047],
        ...,
        [-0.0119, -0.0112, -0.0136,  ..., -0.0240, -0.0118, -0.0030],
        [ 0.0157, -0.0017, -0.0016,  ..., -0.0053, -0.0160, -0.0017],
        [-0.0011,  0.0164,  0.0003,  ..., -0.0039,  0.0108, -0.0259]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.9188e-03,  1.1997e-03,  2.2316e-03,  ..., -8.2016e-04,
         -1.3895e-03,  1.1091e-03],
        [ 1.1311e-03, -1.5612e-03,  1.4219e-03,  ..., -3.2377e-04,
          9.1553e-04, -6.8855e-04],
        [ 1.2922e-03,  2.1946e-04, -9.8991e-04,  ..., -1.9188e-03,
          1.5106e-03,  5.5075e-05],
        ...,
        [ 8.1444e-04, -2.8753e-04, -5.5408e-04,  ..., -2.2304e-04,
          2.7676e-03, -1.4801e-03],
        [ 5.8937e-04,  7.2813e-04,  2.3403e-03,  ..., -1.9503e-03,
          2.3198e-04, -9.7513e-05],
        [ 7.4530e-04, -1.1148e-03,  6.0463e-04,  ...,  1.2455e-03,
         -2.0447e-03, -1.5116e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6382, -0.7832,  0.7998,  ...,  1.1494,  0.6855,  1.1348]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2400, -0.1744,  0.3418,  ..., -0.0999, -0.3274, -0.1117]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 23:19:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is representing, it represents
When something is considering, it considers
When something is enabling, it enables
When something is continuing, it continues
When something is including, it includes
When something is becoming, it becomes
When something is seeming, it seems
When something is operating, it
2024-07-09 23:19:01 root INFO     [order_1_approx] starting weight calculation for When something is continuing, it continues
When something is considering, it considers
When something is enabling, it enables
When something is representing, it represents
When something is becoming, it becomes
When something is including, it includes
When something is operating, it operates
When something is seeming, it
2024-07-09 23:19:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 23:23:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 23:27:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5513,  0.0649,  0.5688,  ...,  0.4194,  1.1885, -0.9292],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.5166,  0.0415,  0.5005,  ...,  0.3818,  1.0908, -0.9297],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0158, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.8828,  4.0859, -1.5137,  ...,  2.2949, -7.1914,  3.9102],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0158, -0.0261, -0.0107,  ...,  0.0208,  0.0102, -0.0191],
        [-0.0259, -0.0399, -0.0046,  ..., -0.0236, -0.0184,  0.0015],
        [ 0.0024, -0.0031, -0.0227,  ...,  0.0223, -0.0044,  0.0092],
        ...,
        [-0.0134, -0.0103,  0.0040,  ..., -0.0293,  0.0053,  0.0012],
        [ 0.0225,  0.0139,  0.0008,  ..., -0.0044, -0.0205, -0.0091],
        [-0.0139,  0.0318,  0.0064,  ..., -0.0032, -0.0146, -0.0335]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.9182e-03, -2.6550e-03,  1.8272e-03,  ...,  3.0098e-03,
         -1.3351e-05,  1.5507e-03],
        [-2.1706e-03, -2.6245e-03,  1.0815e-03,  ..., -9.7752e-06,
          2.2850e-03,  1.8082e-03],
        [-1.1482e-03, -2.2411e-05,  1.4915e-03,  ..., -1.2264e-03,
         -3.5400e-03, -7.9250e-04],
        ...,
        [ 1.4982e-03,  4.3583e-04,  5.2452e-04,  ..., -2.2202e-03,
          1.0567e-03, -1.7834e-03],
        [ 2.0933e-04,  1.1082e-03, -1.2264e-03,  ..., -8.6784e-05,
         -4.9019e-03,  5.1069e-04],
        [-4.3488e-04,  3.1948e-03,  2.2864e-04,  ...,  3.5114e-03,
         -2.2388e-04, -2.7027e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2695,  3.7070, -1.2266,  ...,  1.5176, -5.7852,  2.6016]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1316, -0.0438,  0.2084,  ..., -0.0278, -0.5615, -0.1969]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 23:27:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is continuing, it continues
When something is considering, it considers
When something is enabling, it enables
When something is representing, it represents
When something is becoming, it becomes
When something is including, it includes
When something is operating, it operates
When something is seeming, it
2024-07-09 23:27:27 root INFO     [order_1_approx] starting weight calculation for When something is seeming, it seems
When something is representing, it represents
When something is continuing, it continues
When something is operating, it operates
When something is including, it includes
When something is becoming, it becomes
When something is enabling, it enables
When something is considering, it
2024-07-09 23:27:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 23:31:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 23:35:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3894, -1.3037, -0.3169,  ...,  0.0518,  0.4744,  0.9053],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3933, -1.4033, -0.3291,  ...,  0.0498,  0.4590,  0.9253],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0158, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4902,  1.6914,  0.2803,  ...,  1.5166, -2.4824,  1.1494],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0024, -0.0002,  0.0111,  ...,  0.0124,  0.0035, -0.0020],
        [-0.0261, -0.0049, -0.0015,  ..., -0.0110, -0.0191, -0.0240],
        [ 0.0010, -0.0029, -0.0005,  ...,  0.0063,  0.0016, -0.0003],
        ...,
        [ 0.0078, -0.0331, -0.0300,  ..., -0.0190, -0.0024, -0.0222],
        [ 0.0390, -0.0024,  0.0193,  ...,  0.0123, -0.0010,  0.0036],
        [-0.0020,  0.0133,  0.0011,  ...,  0.0103,  0.0032, -0.0233]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-3.4962e-03,  2.7580e-03,  2.3880e-03,  ...,  1.4238e-03,
         -2.1458e-05, -1.7796e-03],
        [ 6.0415e-04,  2.7990e-04,  9.2220e-04,  ...,  3.5000e-04,
          8.8835e-04, -2.3499e-03],
        [ 8.0347e-04, -1.8501e-03, -1.9512e-03,  ...,  9.1124e-04,
         -3.1929e-03, -4.0007e-04],
        ...,
        [ 3.0651e-03, -2.2621e-03, -2.1458e-05,  ..., -1.7881e-03,
         -2.4509e-04, -2.4490e-03],
        [ 3.1948e-05,  9.0265e-04, -1.0300e-04,  ..., -3.9148e-04,
         -2.1019e-03, -3.3689e-04],
        [-1.2016e-03,  1.1377e-03, -3.1519e-04,  ...,  1.5287e-03,
         -1.2274e-03, -4.8180e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5986,  1.1787,  0.3474,  ...,  0.8848, -0.3730,  1.0137]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1420,  0.0336,  0.3628,  ..., -0.1016, -0.3318, -0.0012]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 23:35:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is seeming, it seems
When something is representing, it represents
When something is continuing, it continues
When something is operating, it operates
When something is including, it includes
When something is becoming, it becomes
When something is enabling, it enables
When something is considering, it
2024-07-09 23:35:49 root INFO     [order_1_approx] starting weight calculation for When something is operating, it operates
When something is including, it includes
When something is representing, it represents
When something is seeming, it seems
When something is considering, it considers
When something is continuing, it continues
When something is enabling, it enables
When something is becoming, it
2024-07-09 23:35:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 23:40:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 23:44:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1465, -0.1130,  0.9351,  ...,  0.6465,  0.7070, -0.4082],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1586, -0.1403,  0.9019,  ...,  0.6372,  0.6938, -0.4558],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0158, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8213, -0.9897,  0.2930,  ..., -0.5566,  1.6211, -0.9268],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0156, -0.0172,  0.0087,  ...,  0.0088, -0.0052, -0.0338],
        [ 0.0038, -0.0087, -0.0277,  ...,  0.0014, -0.0165, -0.0099],
        [-0.0032,  0.0016,  0.0071,  ...,  0.0097,  0.0080, -0.0128],
        ...,
        [ 0.0081, -0.0069,  0.0038,  ..., -0.0417, -0.0084, -0.0347],
        [ 0.0013,  0.0032,  0.0042,  ..., -0.0051, -0.0068, -0.0027],
        [-0.0016, -0.0049, -0.0169,  ...,  0.0129,  0.0088, -0.0043]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0033,  0.0009,  0.0036,  ...,  0.0019, -0.0003,  0.0002],
        [ 0.0007, -0.0010,  0.0010,  ...,  0.0010,  0.0003, -0.0013],
        [ 0.0007,  0.0014, -0.0007,  ...,  0.0002, -0.0006, -0.0010],
        ...,
        [ 0.0003, -0.0001, -0.0001,  ...,  0.0003, -0.0017, -0.0048],
        [-0.0002,  0.0019, -0.0013,  ..., -0.0014, -0.0012, -0.0012],
        [-0.0011, -0.0018, -0.0028,  ...,  0.0022, -0.0012, -0.0037]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0635, -1.3047,  0.2815,  ...,  0.6006,  3.2012, -1.8223]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1188,  0.3574,  0.1741,  ..., -0.1028, -0.2659, -0.0861]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 23:44:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is operating, it operates
When something is including, it includes
When something is representing, it represents
When something is seeming, it seems
When something is considering, it considers
When something is continuing, it continues
When something is enabling, it enables
When something is becoming, it
2024-07-09 23:44:09 root INFO     total operator prediction time: 4021.4644157886505 seconds
2024-07-09 23:44:09 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on noun - plural_reg
2024-07-09 23:44:12 root INFO     building operator noun - plural_reg
2024-07-09 23:44:12 root INFO     [order_1_approx] starting weight calculation for The plural form of government is governments
The plural form of system is systems
The plural form of development is developments
The plural form of law is laws
The plural form of day is days
The plural form of application is applications
The plural form of director is directors
The plural form of town is
2024-07-09 23:44:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 23:48:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 23:52:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0928,  0.0073, -0.5654,  ...,  0.7705, -0.2188, -0.2067],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 1.0449, -0.0110, -0.5298,  ...,  0.7114, -0.2264, -0.2269],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0977,  1.6113,  0.5020,  ..., -0.5566,  2.9023,  2.2422],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.9012e-02, -9.5978e-03, -1.6117e-03,  ...,  8.2245e-03,
         -1.1032e-02, -1.5076e-02],
        [-1.1284e-02, -8.3466e-03, -5.0507e-03,  ...,  6.2561e-03,
         -1.4717e-02, -5.7678e-03],
        [ 1.0201e-02,  8.3923e-05, -1.9897e-02,  ..., -1.3649e-02,
          1.2497e-02,  5.4092e-03],
        ...,
        [ 4.5052e-03, -2.3743e-02, -6.9427e-03,  ..., -4.5471e-03,
          1.0284e-02, -1.2390e-02],
        [ 1.5888e-03, -3.2730e-03, -1.2627e-03,  ...,  1.5392e-03,
         -2.3544e-02,  1.4755e-02],
        [-2.4776e-03,  4.4632e-04,  5.6725e-03,  ..., -5.9013e-03,
          6.1493e-03, -1.1162e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 7.8964e-04, -5.6458e-04, -2.6560e-04,  ..., -1.2817e-03,
          8.9931e-04, -7.1764e-04],
        [-1.6813e-03, -1.9722e-03, -3.6407e-04,  ...,  1.0014e-03,
          4.3201e-04, -2.7866e-03],
        [ 1.1039e-04, -2.6083e-04, -1.0405e-03,  ..., -2.8872e-04,
          1.5879e-03,  1.7214e-03],
        ...,
        [ 6.1703e-04, -1.1587e-03, -7.3195e-05,  ..., -2.7466e-04,
          2.4509e-03, -9.4557e-04],
        [-4.0531e-05, -1.3618e-03, -1.9045e-03,  ...,  1.7858e-04,
         -7.9727e-04, -2.5883e-03],
        [-7.0953e-04, -9.5510e-04, -2.2268e-04,  ...,  1.4343e-03,
          3.0494e-04, -5.4359e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.4824,  2.4141,  0.7734,  ..., -1.0283,  2.9980,  1.6553]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0612, -0.2578,  0.0619,  ..., -0.5820,  0.2096, -0.1439]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 23:52:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of government is governments
The plural form of system is systems
The plural form of development is developments
The plural form of law is laws
The plural form of day is days
The plural form of application is applications
The plural form of director is directors
The plural form of town is
2024-07-09 23:52:34 root INFO     [order_1_approx] starting weight calculation for The plural form of development is developments
The plural form of system is systems
The plural form of law is laws
The plural form of day is days
The plural form of director is directors
The plural form of application is applications
The plural form of town is towns
The plural form of government is
2024-07-09 23:52:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 23:56:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 00:00:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6802, -0.0933, -0.0541,  ..., -0.1399, -0.6055,  0.0596],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.6519, -0.1127, -0.0641,  ..., -0.1287, -0.5986,  0.0320],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6895, -0.4214, -1.6660,  ...,  2.8203, -0.9512,  0.2539],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0141, -0.0251,  0.0186,  ..., -0.0081, -0.0161, -0.0185],
        [ 0.0025, -0.0020, -0.0004,  ...,  0.0098, -0.0114, -0.0030],
        [ 0.0058, -0.0167, -0.0149,  ..., -0.0300,  0.0278,  0.0269],
        ...,
        [-0.0059, -0.0065, -0.0081,  ...,  0.0096, -0.0024, -0.0081],
        [-0.0123, -0.0128, -0.0066,  ..., -0.0188, -0.0095, -0.0008],
        [ 0.0060,  0.0222,  0.0014,  ..., -0.0072,  0.0404, -0.0083]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-4.5681e-04, -2.2888e-03,  9.4891e-04,  ...,  5.2547e-04,
         -6.9857e-04, -1.2684e-03],
        [-6.0034e-04, -1.3981e-03,  9.1839e-04,  ..., -8.2922e-04,
          1.3075e-03, -7.6389e-04],
        [-7.6914e-04,  6.7759e-04,  1.5202e-03,  ..., -1.5240e-03,
          1.9817e-03,  1.9007e-03],
        ...,
        [-3.7336e-04, -3.6621e-04, -1.7166e-03,  ...,  1.5819e-04,
         -1.6260e-04, -2.6340e-03],
        [ 7.4148e-04, -7.2002e-05,  1.2560e-03,  ...,  2.9492e-04,
         -3.7932e-04, -2.0714e-03],
        [-1.1244e-03,  2.9707e-04, -9.0408e-04,  ..., -1.3351e-05,
          1.3990e-03,  1.1101e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7998, -0.0364, -1.7285,  ...,  2.2539, -1.1553, -0.3306]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0103,  0.0066,  0.1676,  ...,  0.1503, -0.3486,  0.0068]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 00:00:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of development is developments
The plural form of system is systems
The plural form of law is laws
The plural form of day is days
The plural form of director is directors
The plural form of application is applications
The plural form of town is towns
The plural form of government is
2024-07-10 00:00:58 root INFO     [order_1_approx] starting weight calculation for The plural form of system is systems
The plural form of application is applications
The plural form of town is towns
The plural form of government is governments
The plural form of day is days
The plural form of law is laws
The plural form of director is directors
The plural form of development is
2024-07-10 00:00:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 00:05:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 00:09:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7832, -0.5190,  0.3738,  ...,  1.2783, -0.7168,  0.3491],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.7788, -0.5542,  0.3428,  ...,  1.2285, -0.7305,  0.3298],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7827, -4.2500,  3.2656,  ..., -2.5898, -0.7979,  1.4932],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0067, -0.0088,  0.0062,  ..., -0.0114,  0.0002, -0.0062],
        [-0.0036, -0.0180,  0.0062,  ...,  0.0113, -0.0210,  0.0025],
        [-0.0137, -0.0189, -0.0204,  ..., -0.0119,  0.0089,  0.0121],
        ...,
        [-0.0118, -0.0104,  0.0078,  ...,  0.0020,  0.0071,  0.0016],
        [-0.0017, -0.0052, -0.0011,  ..., -0.0122, -0.0184,  0.0093],
        [ 0.0020,  0.0159, -0.0044,  ...,  0.0047,  0.0269,  0.0061]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 6.1369e-04, -2.0766e-04,  9.4509e-04,  ...,  8.2254e-05,
         -5.3883e-04,  6.8045e-04],
        [ 1.3275e-03, -1.1702e-03,  1.3914e-03,  ...,  1.6193e-03,
         -1.8454e-03, -1.5745e-03],
        [ 2.9087e-04, -8.7643e-04,  1.1754e-04,  ...,  6.5994e-04,
          5.8603e-04,  1.1768e-03],
        ...,
        [ 2.7418e-04,  2.2697e-03,  5.4407e-04,  ..., -1.6284e-04,
          5.9938e-04,  1.0815e-03],
        [-7.9918e-04,  1.0996e-03, -1.3371e-03,  ..., -2.2831e-03,
         -5.0020e-04,  1.4305e-03],
        [-1.2236e-03,  1.2743e-04,  1.1921e-03,  ...,  7.9489e-04,
         -3.9530e-04,  1.8187e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0273, -4.2070,  2.2422,  ..., -2.8047, -0.6289,  1.4072]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.2954, -0.2032, -0.0150,  ..., -0.0926, -0.1055,  0.0456]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 00:09:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of system is systems
The plural form of application is applications
The plural form of town is towns
The plural form of government is governments
The plural form of day is days
The plural form of law is laws
The plural form of director is directors
The plural form of development is
2024-07-10 00:09:20 root INFO     [order_1_approx] starting weight calculation for The plural form of application is applications
The plural form of day is days
The plural form of director is directors
The plural form of town is towns
The plural form of development is developments
The plural form of government is governments
The plural form of law is laws
The plural form of system is
2024-07-10 00:09:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 00:13:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 00:17:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4087, -0.4448, -0.3340,  ...,  0.3645,  0.3086, -0.3186],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.4163, -0.4844, -0.3379,  ...,  0.3625,  0.2937, -0.3569],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.7695, -0.2285,  4.0195,  ...,  1.4600, -2.3145,  3.0156],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0118, -0.0032,  0.0085,  ...,  0.0137,  0.0013, -0.0107],
        [-0.0063,  0.0094, -0.0110,  ...,  0.0082, -0.0270,  0.0063],
        [-0.0049,  0.0049, -0.0114,  ..., -0.0094,  0.0068,  0.0062],
        ...,
        [-0.0100, -0.0047, -0.0022,  ..., -0.0053,  0.0173, -0.0108],
        [ 0.0004, -0.0125,  0.0158,  ..., -0.0157, -0.0092,  0.0143],
        [-0.0121,  0.0035,  0.0100,  ..., -0.0005,  0.0062, -0.0086]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 4.8637e-04, -6.4468e-04,  7.6103e-04,  ..., -1.5030e-03,
          1.6298e-03,  8.4829e-04],
        [ 6.8283e-04, -1.2445e-03,  1.9970e-03,  ...,  1.2360e-03,
          7.0524e-04, -2.5291e-03],
        [-1.3838e-03,  2.0103e-03, -6.3181e-04,  ..., -1.1101e-03,
          2.2526e-03,  1.6317e-03],
        ...,
        [ 3.0279e-04,  1.8454e-03,  1.1742e-05,  ...,  1.5097e-03,
          1.2674e-03, -1.5907e-03],
        [ 4.3631e-05,  2.9564e-05,  3.9902e-03,  ...,  1.4019e-03,
          4.8208e-04,  5.7220e-05],
        [-2.6894e-03, -1.0452e-03, -5.0354e-04,  ...,  2.6369e-04,
          9.9468e-04,  4.5633e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.9355, -0.2634,  3.8379,  ...,  0.2119, -1.5625,  2.8535]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0758, -0.4163,  0.0609,  ..., -0.1210, -0.1320, -0.0406]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 00:17:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of application is applications
The plural form of day is days
The plural form of director is directors
The plural form of town is towns
The plural form of development is developments
The plural form of government is governments
The plural form of law is laws
The plural form of system is
2024-07-10 00:17:44 root INFO     [order_1_approx] starting weight calculation for The plural form of government is governments
The plural form of town is towns
The plural form of application is applications
The plural form of day is days
The plural form of development is developments
The plural form of director is directors
The plural form of system is systems
The plural form of law is
2024-07-10 00:17:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 00:22:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 00:26:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9336, -0.3188,  0.6445,  ..., -0.0833,  0.1333, -0.0122],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.9302, -0.3413,  0.6035,  ..., -0.0740,  0.1141, -0.0343],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6074, -0.5835, -4.1250,  ...,  0.4009,  2.3164, -0.5542],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0075,  0.0163,  0.0163,  ..., -0.0098, -0.0126, -0.0070],
        [-0.0090, -0.0110,  0.0036,  ...,  0.0113, -0.0118, -0.0117],
        [ 0.0184, -0.0055, -0.0085,  ..., -0.0073,  0.0110,  0.0198],
        ...,
        [-0.0104,  0.0017, -0.0151,  ...,  0.0098,  0.0127, -0.0162],
        [ 0.0067, -0.0099, -0.0084,  ..., -0.0121, -0.0095, -0.0011],
        [ 0.0078, -0.0123,  0.0020,  ..., -0.0071,  0.0151,  0.0022]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 1.4315e-03, -1.1272e-03, -8.1348e-04,  ..., -3.0804e-03,
         -6.2466e-04,  4.1652e-04],
        [-2.0885e-03, -1.5869e-03,  2.1000e-03,  ...,  1.0347e-03,
          9.2983e-05, -6.3896e-04],
        [ 9.0027e-04,  1.5831e-03, -7.6866e-04,  ..., -8.4305e-04,
         -3.9768e-04,  1.3447e-03],
        ...,
        [ 4.6325e-04, -2.0027e-04, -8.2541e-04,  ..., -5.1785e-04,
          2.4643e-03, -3.0746e-03],
        [-8.4019e-04,  1.1635e-03, -1.6224e-04,  ...,  8.0109e-04,
         -9.0790e-04, -1.6880e-03],
        [ 4.1199e-04,  1.1635e-04,  1.8263e-04,  ...,  8.3733e-04,
          1.8320e-03, -1.4400e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.3340, -0.0872, -4.4180,  ...,  0.4189,  1.6582, -1.3223]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0740, -0.3784,  0.0171,  ..., -0.3918,  0.1265,  0.1382]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 00:26:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of government is governments
The plural form of town is towns
The plural form of application is applications
The plural form of day is days
The plural form of development is developments
The plural form of director is directors
The plural form of system is systems
The plural form of law is
2024-07-10 00:26:08 root INFO     [order_1_approx] starting weight calculation for The plural form of law is laws
The plural form of director is directors
The plural form of system is systems
The plural form of development is developments
The plural form of application is applications
The plural form of town is towns
The plural form of government is governments
The plural form of day is
2024-07-10 00:26:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 00:30:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 00:34:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1094, -1.0117,  0.0437,  ...,  0.3857, -0.4829,  0.4287],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.1080, -1.0684,  0.0317,  ...,  0.3796, -0.5005,  0.4209],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2710, -2.3066, -0.1123,  ..., -2.4922,  1.8818, -0.6113],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0020, -0.0079, -0.0100,  ..., -0.0007,  0.0193, -0.0044],
        [-0.0103, -0.0001,  0.0044,  ...,  0.0034,  0.0048,  0.0060],
        [-0.0019,  0.0055, -0.0018,  ..., -0.0154,  0.0156, -0.0020],
        ...,
        [-0.0130,  0.0005, -0.0203,  ..., -0.0064,  0.0054, -0.0001],
        [-0.0009, -0.0170,  0.0032,  ..., -0.0045, -0.0205,  0.0060],
        [-0.0080,  0.0068,  0.0035,  ...,  0.0006,  0.0235, -0.0198]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 6.4611e-04,  1.6820e-04, -8.4496e-04,  ...,  8.9455e-04,
          8.1348e-04,  1.1635e-03],
        [-3.9244e-04, -1.9913e-03, -8.7261e-04,  ..., -8.6880e-04,
          4.6444e-04, -6.0320e-04],
        [-3.6030e-03, -8.5354e-04,  7.8297e-04,  ..., -1.7424e-03,
          1.6632e-03, -3.2425e-05],
        ...,
        [ 4.7398e-04,  2.3575e-03, -2.5024e-03,  ...,  6.5994e-04,
          4.9305e-04, -3.3784e-04],
        [-1.6451e-03, -4.7417e-03,  1.4019e-04,  ...,  7.2098e-04,
         -1.8463e-03, -1.0929e-03],
        [-2.8172e-03,  1.4629e-03, -4.4799e-04,  ...,  6.4373e-05,
          4.9305e-04, -4.7183e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6133, -1.6953, -0.4792,  ..., -2.4316,  2.4648, -0.7393]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.3818, -0.0885, -0.3123,  ..., -0.0889, -0.1545,  0.2546]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 00:34:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of law is laws
The plural form of director is directors
The plural form of system is systems
The plural form of development is developments
The plural form of application is applications
The plural form of town is towns
The plural form of government is governments
The plural form of day is
2024-07-10 00:34:34 root INFO     [order_1_approx] starting weight calculation for The plural form of system is systems
The plural form of development is developments
The plural form of law is laws
The plural form of government is governments
The plural form of director is directors
The plural form of town is towns
The plural form of day is days
The plural form of application is
2024-07-10 00:34:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 00:38:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 00:42:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6274, -0.2307,  0.4912,  ..., -0.5469, -0.2749,  0.3896],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.6104, -0.2529,  0.4460,  ..., -0.5146, -0.2883,  0.3638],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5986,  0.9062,  2.3145,  ..., -0.6777,  0.0034,  3.4180],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-6.2332e-03, -4.2648e-03,  2.4643e-02,  ..., -1.0468e-02,
         -1.6785e-03, -1.2375e-02],
        [-1.7624e-02,  1.1826e-03, -6.8283e-03,  ..., -3.6812e-03,
         -1.1002e-02, -8.5907e-03],
        [ 9.8267e-03,  1.3008e-03, -9.7427e-03,  ..., -9.4223e-03,
          7.0457e-03,  6.6757e-06],
        ...,
        [-6.1226e-03, -1.6342e-02,  2.4509e-03,  ..., -6.0463e-03,
          4.8904e-03, -2.3632e-03],
        [-2.4204e-03, -8.4991e-03,  1.3367e-02,  ..., -1.4015e-02,
         -2.0508e-02, -4.5776e-04],
        [-2.6855e-03,  1.3298e-02, -1.3199e-02,  ...,  7.9041e-03,
          8.4534e-03, -9.3613e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-8.7798e-05, -2.1052e-04,  1.9989e-03,  ..., -1.2755e-04,
          1.6451e-04, -1.9093e-03],
        [-1.4901e-05, -1.2331e-03,  8.3208e-04,  ..., -2.5892e-04,
         -2.3019e-04, -1.5364e-03],
        [-1.8902e-03,  1.0939e-03, -1.0090e-03,  ..., -4.5514e-04,
         -1.6918e-03, -8.2970e-05],
        ...,
        [ 1.7715e-04,  3.1066e-04, -3.6192e-04,  ...,  3.0279e-04,
          6.1703e-04, -1.4162e-03],
        [ 9.1887e-04, -3.6716e-05,  1.0529e-03,  ..., -1.0967e-05,
         -1.9140e-03, -1.2350e-03],
        [-1.0548e-03, -2.0230e-04,  1.0834e-03,  ..., -7.6294e-06,
         -1.8864e-03,  1.3771e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9072,  1.9863,  1.5684,  ..., -0.4414,  0.3401,  2.8535]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.2015,  0.0271,  0.2432,  ..., -0.2234,  0.1277, -0.0152]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 00:43:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of system is systems
The plural form of development is developments
The plural form of law is laws
The plural form of government is governments
The plural form of director is directors
The plural form of town is towns
The plural form of day is days
The plural form of application is
2024-07-10 00:43:00 root INFO     [order_1_approx] starting weight calculation for The plural form of town is towns
The plural form of system is systems
The plural form of law is laws
The plural form of development is developments
The plural form of government is governments
The plural form of application is applications
The plural form of day is days
The plural form of director is
2024-07-10 00:43:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 00:47:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 00:51:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.7754, -0.2642, -0.1597,  ...,  1.2090,  0.1042,  0.5264],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 1.7529, -0.2883, -0.1652,  ...,  1.1475,  0.0799,  0.5049],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.3008, -2.2500,  2.0840,  ..., -4.8750, -0.7109,  7.3477],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0045, -0.0133,  0.0201,  ..., -0.0131, -0.0085, -0.0141],
        [ 0.0055, -0.0068, -0.0099,  ...,  0.0008, -0.0021, -0.0096],
        [ 0.0113,  0.0014, -0.0084,  ..., -0.0003,  0.0311,  0.0118],
        ...,
        [-0.0258, -0.0225, -0.0046,  ...,  0.0010,  0.0172, -0.0191],
        [-0.0115, -0.0093,  0.0123,  ..., -0.0169, -0.0058,  0.0046],
        [-0.0250,  0.0066, -0.0156,  ..., -0.0157,  0.0237,  0.0096]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-6.8378e-04, -1.0290e-03,  2.7847e-03,  ..., -1.0777e-03,
          1.5202e-03,  6.0606e-04],
        [-1.5640e-03,  1.3304e-03,  8.8692e-05,  ..., -9.4700e-04,
         -9.2983e-04,  4.0174e-04],
        [-5.4073e-04,  4.1938e-04,  2.7347e-04,  ..., -1.7405e-04,
          5.3978e-03,  1.0929e-03],
        ...,
        [-2.1915e-03, -7.7724e-04,  2.2163e-03,  ..., -6.6471e-04,
          3.5248e-03,  8.5115e-04],
        [-1.4029e-03, -4.9162e-04,  6.4182e-04,  ..., -2.2936e-04,
         -1.3561e-03, -9.6560e-04],
        [-3.0499e-03, -8.0347e-05, -2.1286e-03,  ...,  1.7729e-03,
         -1.5221e-03, -1.1368e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.8750, -2.1797,  2.0293,  ..., -4.5859, -0.8008,  7.4180]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2971,  0.2338,  0.0215,  ...,  0.1898, -0.1199, -0.2031]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 00:51:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of town is towns
The plural form of system is systems
The plural form of law is laws
The plural form of development is developments
The plural form of government is governments
The plural form of application is applications
The plural form of day is days
The plural form of director is
2024-07-10 00:51:26 root INFO     total operator prediction time: 4034.058550596237 seconds
2024-07-10 00:51:26 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_3pSg - Ved
2024-07-10 00:51:28 root INFO     building operator verb_3pSg - Ved
2024-07-10 00:51:28 root INFO     [order_1_approx] starting weight calculation for When he involves something, something has been involved
When he seems something, something has been seemed
When he performs something, something has been performed
When he locates something, something has been located
When he loses something, something has been lost
When he asks something, something has been asked
When he decides something, something has been decided
When he manages something, something has been
2024-07-10 00:51:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 00:55:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 00:59:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0697,  0.2268,  0.6123,  ..., -0.1316, -0.6914, -0.3308],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.0793,  0.2183,  0.5864,  ..., -0.1294, -0.7231, -0.3748],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0159, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1274, -1.2539, -2.8730,  ..., -0.3525,  0.0510,  1.0391],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-7.6027e-03, -9.1705e-03,  4.5815e-03,  ...,  1.6546e-03,
         -7.4081e-03, -7.6675e-03],
        [-5.0850e-03,  6.8283e-03,  8.8959e-03,  ..., -3.7918e-03,
          6.3934e-03,  3.3779e-03],
        [ 8.6899e-03, -1.2566e-02,  1.1635e-03,  ...,  1.1032e-02,
          1.0391e-02,  3.3092e-03],
        ...,
        [-5.9395e-03, -8.6441e-03, -2.0218e-04,  ..., -7.7209e-03,
          1.1963e-02, -8.7738e-04],
        [-1.5091e-02,  2.9831e-03, -8.1863e-03,  ..., -1.8387e-02,
         -1.0498e-02,  8.9645e-05],
        [ 2.3804e-03,  1.5244e-02, -1.3371e-03,  ...,  2.6169e-03,
          7.4692e-03,  4.1504e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-9.7990e-05, -7.7343e-04,  8.3256e-04,  ...,  6.4945e-04,
          5.6314e-04,  7.4482e-04],
        [-7.1859e-04, -4.8876e-05,  6.0177e-04,  ...,  1.5821e-03,
          1.6384e-03, -1.6718e-03],
        [ 4.7970e-04,  3.5262e-04, -1.3809e-03,  ...,  9.1648e-04,
          5.4169e-04,  2.3794e-04],
        ...,
        [ 1.8530e-03,  7.0453e-05,  1.5745e-03,  ...,  2.7924e-03,
          2.0523e-03, -3.4771e-03],
        [-1.2197e-03,  3.7432e-04,  1.2541e-03,  ...,  1.7011e-04,
         -7.4053e-04, -1.6661e-03],
        [-2.0332e-03, -9.5367e-05, -5.1498e-04,  ..., -3.8910e-04,
         -2.6751e-04,  1.2779e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0044, -1.2715, -2.7461,  ..., -0.8184, -0.9502,  0.6992]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0065, -0.1851,  0.0995,  ..., -0.4871, -0.3250,  0.2095]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 00:59:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he involves something, something has been involved
When he seems something, something has been seemed
When he performs something, something has been performed
When he locates something, something has been located
When he loses something, something has been lost
When he asks something, something has been asked
When he decides something, something has been decided
When he manages something, something has been
2024-07-10 00:59:54 root INFO     [order_1_approx] starting weight calculation for When he loses something, something has been lost
When he involves something, something has been involved
When he manages something, something has been managed
When he asks something, something has been asked
When he performs something, something has been performed
When he locates something, something has been located
When he decides something, something has been decided
When he seems something, something has been
2024-07-10 00:59:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 01:04:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 01:08:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8960,  0.5771,  0.8091,  ...,  0.4768,  0.3271, -0.7925],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.9878,  0.6284,  0.8320,  ...,  0.5024,  0.3325, -0.9106],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0159, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6899,  5.0898,  2.5137,  ...,  0.4973, -8.4453,  0.9355],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0063, -0.0335,  0.0094,  ...,  0.0146,  0.0084, -0.0146],
        [-0.0245,  0.0206,  0.0021,  ..., -0.0145,  0.0013,  0.0139],
        [ 0.0165, -0.0053, -0.0126,  ..., -0.0113, -0.0009,  0.0055],
        ...,
        [-0.0096, -0.0238, -0.0082,  ..., -0.0157,  0.0049,  0.0168],
        [ 0.0015, -0.0130,  0.0004,  ..., -0.0162,  0.0053, -0.0009],
        [-0.0048,  0.0180, -0.0124,  ...,  0.0053,  0.0181, -0.0066]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.4629e-03, -1.1158e-03,  1.3323e-03,  ...,  8.0442e-04,
          1.9431e-04,  1.8120e-04],
        [-2.0802e-04, -1.0424e-03, -2.0638e-03,  ..., -7.1621e-04,
          2.3174e-03, -1.1339e-03],
        [ 6.0225e-04,  1.4544e-04,  9.4891e-05,  ...,  5.4836e-06,
          1.2941e-03,  1.1253e-03],
        ...,
        [ 8.9264e-04, -1.6766e-03,  1.1253e-03,  ..., -9.7752e-04,
          2.2850e-03, -2.5082e-03],
        [ 1.2779e-03,  1.9252e-04,  3.0327e-04,  ..., -4.7398e-04,
         -5.8842e-04, -8.6498e-04],
        [-3.2921e-03, -2.4014e-03, -1.3399e-03,  ...,  1.3580e-03,
          1.6518e-03,  6.5565e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.6777,  4.1328,  2.2109,  ...,  1.1104, -8.7812,  0.6235]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.3186,  0.0692, -0.0764,  ..., -0.3726,  0.0262, -0.0626]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 01:08:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he loses something, something has been lost
When he involves something, something has been involved
When he manages something, something has been managed
When he asks something, something has been asked
When he performs something, something has been performed
When he locates something, something has been located
When he decides something, something has been decided
When he seems something, something has been
2024-07-10 01:08:22 root INFO     [order_1_approx] starting weight calculation for When he loses something, something has been lost
When he locates something, something has been located
When he performs something, something has been performed
When he manages something, something has been managed
When he seems something, something has been seemed
When he decides something, something has been decided
When he asks something, something has been asked
When he involves something, something has been
2024-07-10 01:08:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 01:12:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 01:16:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5659,  0.5371,  0.7861,  ...,  0.2693, -0.0269,  0.4648],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.6318,  0.5825,  0.8081,  ...,  0.2825, -0.0529,  0.4922],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0159, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([2.8164, 1.8418, 0.2988,  ..., 0.8657, 1.0020, 0.9980], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-3.2471e-02, -6.6986e-03,  4.8714e-03,  ...,  1.8265e-02,
          7.9498e-03, -1.5808e-02],
        [ 3.2501e-03, -3.9291e-03,  6.4392e-03,  ..., -8.3618e-03,
          1.6663e-02,  7.6218e-03],
        [ 9.7656e-03,  7.2937e-03, -2.4734e-02,  ..., -5.5847e-03,
         -1.9897e-02,  4.6234e-03],
        ...,
        [-1.5976e-02,  8.1253e-03, -7.0610e-03,  ...,  1.1292e-03,
         -1.1139e-02,  3.3684e-03],
        [-9.7275e-05,  6.4049e-03,  1.1606e-03,  ..., -3.2949e-04,
         -1.9436e-03, -7.6981e-03],
        [-1.5083e-02,  9.1400e-03,  2.6665e-03,  ..., -1.2131e-02,
          2.0844e-02, -1.2978e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-3.3531e-03, -1.1597e-03,  1.8806e-03,  ...,  7.2956e-04,
          6.6090e-04, -3.7241e-04],
        [ 1.9264e-03, -7.8011e-04, -5.9891e-04,  ...,  2.2011e-03,
          2.4891e-03, -3.7727e-03],
        [-5.7888e-04,  4.6515e-04,  3.9434e-04,  ..., -4.4346e-04,
         -6.2275e-04,  7.3147e-04],
        ...,
        [ 6.4468e-04, -1.6289e-03, -1.4915e-03,  ..., -6.5279e-04,
          1.5354e-03,  1.2195e-04],
        [-8.3065e-04,  5.7888e-04,  2.3193e-03,  ...,  1.2236e-03,
          2.6588e-03, -2.8133e-03],
        [-3.2387e-03, -4.6444e-04,  6.0737e-05,  ..., -2.1286e-03,
         -1.2684e-03,  8.9931e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[2.4082, 2.1172, 0.7471,  ..., 0.6270, 1.1758, 1.3877]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0677, -0.3450,  0.2698,  ..., -0.0181, -0.8887,  0.3291]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 01:16:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he loses something, something has been lost
When he locates something, something has been located
When he performs something, something has been performed
When he manages something, something has been managed
When he seems something, something has been seemed
When he decides something, something has been decided
When he asks something, something has been asked
When he involves something, something has been
2024-07-10 01:16:50 root INFO     [order_1_approx] starting weight calculation for When he seems something, something has been seemed
When he locates something, something has been located
When he involves something, something has been involved
When he manages something, something has been managed
When he performs something, something has been performed
When he decides something, something has been decided
When he asks something, something has been asked
When he loses something, something has been
2024-07-10 01:16:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 01:21:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 01:25:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1963, -0.1438,  0.7471,  ...,  0.0929, -0.4194,  0.3274],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-1.2578, -0.1733,  0.7314,  ...,  0.0948, -0.4526,  0.3230],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0159, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.2734, -0.2048, -0.1733,  ..., -1.0898, -3.6875,  0.1475],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0072, -0.0108, -0.0011,  ...,  0.0029, -0.0058, -0.0120],
        [-0.0199, -0.0016,  0.0077,  ..., -0.0138, -0.0040,  0.0006],
        [ 0.0015, -0.0019, -0.0128,  ..., -0.0096,  0.0016, -0.0038],
        ...,
        [-0.0071, -0.0090, -0.0037,  ..., -0.0185,  0.0089,  0.0039],
        [ 0.0091,  0.0046,  0.0081,  ..., -0.0126,  0.0051, -0.0009],
        [-0.0066, -0.0072,  0.0050,  ...,  0.0070,  0.0041, -0.0005]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0007,  0.0007,  0.0012,  ..., -0.0014,  0.0020,  0.0001],
        [ 0.0002, -0.0006,  0.0013,  ..., -0.0012,  0.0011,  0.0010],
        [-0.0015,  0.0002, -0.0023,  ..., -0.0002, -0.0004,  0.0003],
        ...,
        [ 0.0006,  0.0007, -0.0003,  ..., -0.0006,  0.0019, -0.0040],
        [ 0.0007,  0.0022, -0.0003,  ..., -0.0002, -0.0015, -0.0017],
        [-0.0010,  0.0006, -0.0010,  ...,  0.0003, -0.0009, -0.0008]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6133,  0.0372, -0.2646,  ..., -1.4131, -3.6543,  0.2363]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1628, -0.1460, -0.0853,  ..., -0.0789, -0.2964,  0.0275]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 01:25:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he seems something, something has been seemed
When he locates something, something has been located
When he involves something, something has been involved
When he manages something, something has been managed
When he performs something, something has been performed
When he decides something, something has been decided
When he asks something, something has been asked
When he loses something, something has been
2024-07-10 01:25:17 root INFO     [order_1_approx] starting weight calculation for When he performs something, something has been performed
When he locates something, something has been located
When he seems something, something has been seemed
When he manages something, something has been managed
When he asks something, something has been asked
When he loses something, something has been lost
When he involves something, something has been involved
When he decides something, something has been
2024-07-10 01:25:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 01:29:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 01:33:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3435,  0.0547,  0.1768,  ...,  0.1705, -0.4050, -0.1877],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.3616,  0.0367,  0.1587,  ...,  0.1686, -0.4331, -0.2246],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0159, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.5410,  1.8008,  3.3457,  ..., -1.4961, -0.3774, -1.3594],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0110, -0.0161,  0.0070,  ...,  0.0067, -0.0164, -0.0110],
        [-0.0044,  0.0216,  0.0078,  ...,  0.0030, -0.0022, -0.0036],
        [-0.0041,  0.0129,  0.0036,  ...,  0.0102,  0.0103, -0.0055],
        ...,
        [-0.0148,  0.0037, -0.0043,  ..., -0.0057,  0.0114, -0.0034],
        [ 0.0178, -0.0059, -0.0013,  ..., -0.0358, -0.0304,  0.0115],
        [ 0.0002, -0.0010, -0.0135,  ...,  0.0106,  0.0115, -0.0082]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.9817e-03, -2.1267e-03, -8.8167e-04,  ...,  3.5703e-05,
          2.9564e-04,  2.8348e-04],
        [-8.6594e-04, -2.2335e-03,  1.2836e-03,  ...,  3.6812e-04,
          1.6060e-03, -1.2836e-03],
        [ 1.2684e-03,  6.7329e-04, -8.5878e-04,  ...,  4.3201e-04,
         -5.4836e-06,  1.8454e-04],
        ...,
        [-1.4210e-03,  4.7278e-04,  1.1034e-03,  ..., -1.1215e-03,
          1.6031e-03, -2.0885e-03],
        [ 3.1137e-04,  7.3719e-04,  2.1191e-03,  ..., -3.2558e-03,
         -1.4000e-03, -9.0313e-04],
        [-6.3181e-04,  2.3341e-04, -1.7691e-03,  ...,  6.9666e-04,
          4.9782e-04, -1.1110e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7773,  2.2363,  3.8281,  ..., -1.9883,  0.2158, -1.1807]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.3711, -0.0677,  0.0077,  ..., -0.2603, -0.0367,  0.0058]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 01:33:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he performs something, something has been performed
When he locates something, something has been located
When he seems something, something has been seemed
When he manages something, something has been managed
When he asks something, something has been asked
When he loses something, something has been lost
When he involves something, something has been involved
When he decides something, something has been
2024-07-10 01:33:45 root INFO     [order_1_approx] starting weight calculation for When he decides something, something has been decided
When he involves something, something has been involved
When he performs something, something has been performed
When he loses something, something has been lost
When he seems something, something has been seemed
When he manages something, something has been managed
When he locates something, something has been located
When he asks something, something has been
2024-07-10 01:33:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 01:38:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 01:42:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2118,  0.0043,  0.6489,  ..., -0.3308,  0.3916,  0.6074],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.2167, -0.0131,  0.6084,  ..., -0.3140,  0.3677,  0.5967],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0159, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([1.5439, 4.2695, 1.2832,  ..., 2.1270, 1.6338, 3.0625], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0013, -0.0216,  0.0077,  ...,  0.0074, -0.0154, -0.0049],
        [-0.0183,  0.0009, -0.0013,  ...,  0.0042, -0.0002,  0.0041],
        [ 0.0079, -0.0040, -0.0171,  ...,  0.0035, -0.0066,  0.0008],
        ...,
        [-0.0140, -0.0013,  0.0013,  ...,  0.0028, -0.0013,  0.0038],
        [ 0.0120, -0.0161, -0.0043,  ..., -0.0091, -0.0207, -0.0061],
        [-0.0090,  0.0046,  0.0052,  ...,  0.0009,  0.0068, -0.0081]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.0084e-03, -2.1782e-03,  1.2016e-03,  ..., -7.6151e-04,
          6.1929e-05, -9.0456e-04],
        [-5.7364e-04, -9.3174e-04,  7.0620e-04,  ...,  4.7994e-04,
          1.9512e-03, -1.1454e-03],
        [ 1.4210e-03,  1.1063e-03, -5.8842e-04,  ..., -6.3753e-04,
         -7.5722e-04,  5.4789e-04],
        ...,
        [-4.1068e-05, -9.7132e-04,  1.2970e-03,  ...,  7.3624e-04,
          1.8167e-03, -1.7719e-03],
        [ 4.5252e-04,  1.1902e-03,  9.1171e-04,  ..., -1.3895e-03,
         -5.9605e-04,  2.6464e-05],
        [-4.6015e-04,  2.8896e-04,  4.5252e-04,  ...,  1.6842e-03,
         -2.6250e-04,  1.3676e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[1.4111, 3.8906, 1.4678,  ..., 1.1709, 1.1934, 3.5684]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1355, -0.0462, -0.0006,  ..., -0.3916, -0.2480,  0.0013]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 01:42:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he decides something, something has been decided
When he involves something, something has been involved
When he performs something, something has been performed
When he loses something, something has been lost
When he seems something, something has been seemed
When he manages something, something has been managed
When he locates something, something has been located
When he asks something, something has been
2024-07-10 01:42:11 root INFO     [order_1_approx] starting weight calculation for When he seems something, something has been seemed
When he asks something, something has been asked
When he manages something, something has been managed
When he involves something, something has been involved
When he locates something, something has been located
When he decides something, something has been decided
When he loses something, something has been lost
When he performs something, something has been
2024-07-10 01:42:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 01:46:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 01:50:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6187,  0.6333,  0.4795,  ..., -0.7969,  0.0022, -0.4243],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.6270,  0.6279,  0.4426,  ..., -0.7637, -0.0201, -0.4604],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0159, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9082,  0.2798,  0.1138,  ..., -0.0918,  0.0435,  0.4121],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0052, -0.0219, -0.0004,  ...,  0.0108, -0.0034, -0.0050],
        [-0.0075, -0.0020,  0.0029,  ..., -0.0042, -0.0063,  0.0035],
        [ 0.0181, -0.0101, -0.0074,  ...,  0.0117, -0.0046, -0.0018],
        ...,
        [-0.0040, -0.0055, -0.0040,  ..., -0.0050,  0.0166,  0.0007],
        [-0.0072,  0.0039, -0.0170,  ..., -0.0182, -0.0045,  0.0046],
        [ 0.0012,  0.0055, -0.0107,  ...,  0.0021,  0.0130, -0.0134]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.3046e-03,  7.5817e-05,  2.0618e-03,  ...,  1.5097e-03,
          7.7534e-04, -1.9646e-04],
        [-3.3021e-04, -3.8385e-04, -5.0640e-04,  ...,  4.8375e-04,
          1.3666e-03, -6.6710e-04],
        [ 1.9817e-03, -7.0381e-04, -9.9754e-04,  ..., -2.5129e-04,
          1.8167e-03,  1.5240e-03],
        ...,
        [ 1.3371e-03, -4.0269e-04,  3.3808e-04,  ...,  2.3632e-03,
          2.8687e-03, -3.2196e-03],
        [-1.0386e-03,  3.3975e-05,  8.7881e-04,  ..., -5.4479e-05,
          4.6277e-04, -7.9203e-04],
        [-1.2512e-03,  8.2684e-04,  1.0562e-04,  ..., -1.9684e-03,
         -1.4725e-03,  4.5240e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7959,  0.0684, -0.0918,  ...,  0.2576,  1.0488,  1.2676]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0529, -0.0444,  0.3259,  ..., -0.2380, -0.2175, -0.1066]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 01:50:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he seems something, something has been seemed
When he asks something, something has been asked
When he manages something, something has been managed
When he involves something, something has been involved
When he locates something, something has been located
When he decides something, something has been decided
When he loses something, something has been lost
When he performs something, something has been
2024-07-10 01:50:39 root INFO     [order_1_approx] starting weight calculation for When he involves something, something has been involved
When he manages something, something has been managed
When he decides something, something has been decided
When he loses something, something has been lost
When he seems something, something has been seemed
When he performs something, something has been performed
When he asks something, something has been asked
When he locates something, something has been
2024-07-10 01:50:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 01:54:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 01:59:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8052,  0.2815, -0.0398,  ...,  0.3579, -0.4390, -0.4128],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.8071,  0.2722, -0.0479,  ...,  0.3477, -0.4490, -0.4426],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0159, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.1465,  1.3105, -2.1230,  ..., -5.0938,  1.3633,  1.8008],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0026, -0.0134,  0.0111,  ...,  0.0049, -0.0048, -0.0010],
        [-0.0121,  0.0083,  0.0079,  ...,  0.0139,  0.0085, -0.0058],
        [ 0.0155, -0.0032,  0.0034,  ...,  0.0026,  0.0093, -0.0025],
        ...,
        [-0.0084,  0.0101, -0.0049,  ..., -0.0094,  0.0014, -0.0118],
        [ 0.0097,  0.0055, -0.0065,  ..., -0.0114,  0.0019,  0.0008],
        [ 0.0031, -0.0050,  0.0041,  ...,  0.0097, -0.0027, -0.0049]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.7218e-03, -7.9775e-04,  2.0542e-03,  ...,  2.8753e-04,
          3.3259e-04, -2.8348e-04],
        [-4.4370e-04, -5.1403e-04,  7.5483e-04,  ...,  1.6861e-03,
          6.7806e-04, -3.3569e-04],
        [ 9.5415e-04,  8.7500e-04, -3.4165e-04,  ..., -8.4543e-04,
         -6.6900e-04, -4.0770e-05],
        ...,
        [-1.1677e-04,  6.4468e-04,  6.9141e-04,  ..., -2.0766e-04,
          1.3151e-03, -2.4929e-03],
        [-8.0967e-04,  1.1158e-03,  1.2760e-03,  ..., -8.8310e-04,
         -5.8079e-04, -1.7605e-03],
        [-4.5490e-04, -7.4100e-04,  2.8539e-04,  ...,  8.6689e-04,
         -7.6532e-05, -1.3247e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.7695,  1.2393, -2.3457,  ..., -5.2734,  1.1982,  1.2422]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0271, -0.1136, -0.0249,  ..., -0.2285, -0.2605, -0.1176]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 01:59:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he involves something, something has been involved
When he manages something, something has been managed
When he decides something, something has been decided
When he loses something, something has been lost
When he seems something, something has been seemed
When he performs something, something has been performed
When he asks something, something has been asked
When he locates something, something has been
2024-07-10 01:59:06 root INFO     total operator prediction time: 4057.8172750473022 seconds
2024-07-10 01:59:06 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on adj - superlative
2024-07-10 01:59:08 root INFO     building operator adj - superlative
2024-07-10 01:59:08 root INFO     [order_1_approx] starting weight calculation for If something is the most neat, it is neatest
If something is the most dense, it is densest
If something is the most vague, it is vaguest
If something is the most cruel, it is cruelest
If something is the most tricky, it is trickiest
If something is the most costly, it is costliest
If something is the most subtle, it is subtlest
If something is the most noisy, it is
2024-07-10 01:59:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 02:03:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 02:07:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6543,  0.5693,  0.6533,  ..., -0.2568, -0.1411, -0.2583],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.5928,  0.5054,  0.5469,  ..., -0.2179, -0.1444, -0.2612],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.3193, -2.3906, -2.7324,  ...,  4.3906,  4.0156,  5.9922],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-3.2063e-03, -2.8488e-02, -4.9591e-05,  ...,  2.1271e-02,
         -3.9444e-03, -1.4221e-02],
        [-9.1019e-03, -1.3580e-02,  4.2763e-03,  ...,  9.2773e-03,
         -6.7520e-04,  1.0471e-03],
        [ 5.9414e-04,  9.0599e-05, -4.0283e-03,  ..., -4.8828e-03,
          1.0056e-02,  4.7760e-03],
        ...,
        [-4.8599e-03, -9.4528e-03, -1.9436e-03,  ..., -2.7588e-02,
         -4.7989e-03, -7.5035e-03],
        [ 1.2074e-03, -1.2703e-02,  1.0033e-02,  ..., -1.2684e-03,
         -3.1433e-02,  9.2163e-03],
        [-1.7975e-02,  1.3962e-03, -5.3596e-03,  ..., -2.2324e-02,
         -3.1185e-03, -2.8473e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.9360e-04, -3.4904e-03, -2.3384e-03,  ...,  2.3580e-04,
          3.3283e-04,  5.4777e-05],
        [-2.8300e-04, -1.6975e-03,  2.9335e-03,  ...,  9.3174e-04,
          9.7227e-04, -1.0414e-03],
        [-7.7915e-04,  9.1267e-04, -3.3722e-03,  ...,  5.0545e-04,
         -3.5858e-04,  1.3580e-03],
        ...,
        [ 2.3384e-03,  3.7169e-04, -4.1890e-04,  ..., -2.6932e-03,
          1.5945e-03, -2.4009e-04],
        [ 5.6982e-05,  8.0061e-04,  7.9060e-04,  ...,  5.3406e-04,
         -1.5049e-03,  1.4849e-03],
        [ 1.5879e-04,  1.2541e-04, -1.3471e-05,  ...,  2.2244e-04,
          3.2473e-04, -4.5300e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.1914, -2.2422, -2.7832,  ...,  4.6172,  3.5879,  4.7227]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0735, -0.2808,  0.1754,  ...,  0.0299,  0.0485,  0.0218]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 02:07:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most neat, it is neatest
If something is the most dense, it is densest
If something is the most vague, it is vaguest
If something is the most cruel, it is cruelest
If something is the most tricky, it is trickiest
If something is the most costly, it is costliest
If something is the most subtle, it is subtlest
If something is the most noisy, it is
2024-07-10 02:07:32 root INFO     [order_1_approx] starting weight calculation for If something is the most cruel, it is cruelest
If something is the most subtle, it is subtlest
If something is the most noisy, it is noisiest
If something is the most neat, it is neatest
If something is the most tricky, it is trickiest
If something is the most vague, it is vaguest
If something is the most costly, it is costliest
If something is the most dense, it is
2024-07-10 02:07:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 02:11:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 02:15:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4941,  0.7490,  1.1045,  ...,  0.2188, -0.4167, -0.9404],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.4607,  0.7041,  0.9790,  ...,  0.2002, -0.4060, -0.9272],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7988, -0.3716, -1.6719,  ..., -0.8442, -0.3389,  5.4805],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0040, -0.0002,  0.0008,  ..., -0.0099, -0.0053, -0.0318],
        [-0.0192, -0.0050, -0.0075,  ...,  0.0016, -0.0089, -0.0039],
        [ 0.0031,  0.0065,  0.0029,  ...,  0.0035, -0.0132,  0.0071],
        ...,
        [-0.0022, -0.0156,  0.0096,  ..., -0.0002, -0.0115, -0.0125],
        [ 0.0005, -0.0083,  0.0041,  ...,  0.0031,  0.0077, -0.0090],
        [-0.0073,  0.0006, -0.0161,  ..., -0.0062,  0.0024,  0.0078]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.5659e-03,  2.9588e-04,  1.4648e-03,  ...,  2.4462e-04,
         -2.6298e-04, -9.1076e-04],
        [-4.1819e-04, -2.2621e-03,  1.1492e-03,  ...,  1.0853e-03,
         -2.6131e-04, -1.6975e-03],
        [ 4.3535e-04,  4.6968e-04, -2.0981e-03,  ...,  2.5539e-03,
         -6.9761e-04,  1.0757e-03],
        ...,
        [ 3.5553e-03,  5.1737e-05, -6.1226e-04,  ..., -3.6774e-03,
          2.5997e-03,  5.8126e-04],
        [-7.0906e-04,  1.1339e-03, -1.4877e-03,  ...,  5.4455e-04,
         -9.8467e-05,  1.9035e-03],
        [ 7.9775e-04, -1.6003e-03, -7.7629e-04,  ...,  1.7738e-04,
         -1.2808e-03,  2.5225e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.1992, -0.5981, -2.4551,  ..., -0.8115, -0.7041,  4.8047]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1173, -0.1782,  0.1505,  ...,  0.1630,  0.1116, -0.1831]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 02:15:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most cruel, it is cruelest
If something is the most subtle, it is subtlest
If something is the most noisy, it is noisiest
If something is the most neat, it is neatest
If something is the most tricky, it is trickiest
If something is the most vague, it is vaguest
If something is the most costly, it is costliest
If something is the most dense, it is
2024-07-10 02:15:58 root INFO     [order_1_approx] starting weight calculation for If something is the most vague, it is vaguest
If something is the most neat, it is neatest
If something is the most costly, it is costliest
If something is the most dense, it is densest
If something is the most noisy, it is noisiest
If something is the most cruel, it is cruelest
If something is the most tricky, it is trickiest
If something is the most subtle, it is
2024-07-10 02:15:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 02:20:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 02:24:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2065, -0.7109, -0.1117,  ...,  0.3672, -0.7290, -1.1816],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1882, -0.6611, -0.1072,  ...,  0.3120, -0.6538, -1.0879],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.3184,  2.5391,  1.6973,  ...,  6.0234, -5.9453,  2.3770],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0010, -0.0091,  0.0096,  ..., -0.0170, -0.0067, -0.0135],
        [-0.0082, -0.0096, -0.0094,  ...,  0.0019, -0.0156, -0.0031],
        [ 0.0147,  0.0138, -0.0094,  ...,  0.0278, -0.0048,  0.0039],
        ...,
        [-0.0186, -0.0188,  0.0008,  ..., -0.0247,  0.0059, -0.0256],
        [-0.0092, -0.0210,  0.0123,  ..., -0.0196, -0.0304, -0.0047],
        [-0.0159,  0.0118, -0.0124,  ..., -0.0286,  0.0056, -0.0286]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 2.1708e-04,  7.7200e-04,  2.8992e-04,  ..., -1.8692e-04,
         -8.9979e-04,  8.8882e-04],
        [-1.9526e-04, -2.6436e-03, -5.4777e-05,  ...,  7.7248e-04,
         -4.4250e-04, -1.1110e-03],
        [ 2.2125e-03,  2.0123e-03, -8.1253e-04,  ...,  5.8556e-04,
         -1.4067e-04,  5.8889e-04],
        ...,
        [ 3.7050e-04, -1.1673e-03, -2.1000e-03,  ...,  2.0008e-03,
          3.7122e-04, -1.0118e-03],
        [-7.7057e-04, -4.9496e-04, -2.2030e-04,  ..., -3.7837e-04,
         -9.7418e-04, -5.9223e-04],
        [ 1.1129e-03,  6.7520e-04, -1.7319e-03,  ...,  1.4734e-03,
         -5.4550e-04, -3.3045e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.4473,  2.7246,  1.9619,  ...,  4.9688, -6.6680,  2.2383]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0946, -0.1492,  0.2556,  ...,  0.1357, -0.1959,  0.1222]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 02:24:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most vague, it is vaguest
If something is the most neat, it is neatest
If something is the most costly, it is costliest
If something is the most dense, it is densest
If something is the most noisy, it is noisiest
If something is the most cruel, it is cruelest
If something is the most tricky, it is trickiest
If something is the most subtle, it is
2024-07-10 02:24:24 root INFO     [order_1_approx] starting weight calculation for If something is the most noisy, it is noisiest
If something is the most costly, it is costliest
If something is the most subtle, it is subtlest
If something is the most neat, it is neatest
If something is the most dense, it is densest
If something is the most vague, it is vaguest
If something is the most cruel, it is cruelest
If something is the most tricky, it is
2024-07-10 02:24:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 02:28:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 02:32:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3823,  0.3940, -0.3401,  ..., -0.1865, -0.2114, -0.2979],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.3743,  0.3655, -0.3257,  ..., -0.1709, -0.2219, -0.3186],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-9.5859, -0.1226,  2.3789,  ...,  3.8359,  0.2500,  4.4766],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0075, -0.0080,  0.0126,  ..., -0.0078, -0.0229, -0.0266],
        [-0.0256, -0.0130,  0.0104,  ...,  0.0042, -0.0032,  0.0030],
        [-0.0089, -0.0036, -0.0047,  ...,  0.0040,  0.0118, -0.0043],
        ...,
        [ 0.0047, -0.0177, -0.0058,  ..., -0.0327, -0.0033, -0.0380],
        [ 0.0003,  0.0004,  0.0288,  ..., -0.0219, -0.0356,  0.0092],
        [-0.0113,  0.0119,  0.0008,  ..., -0.0053,  0.0206, -0.0367]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.9545e-03, -1.2226e-03,  1.2684e-03,  ..., -3.4370e-03,
          1.8063e-03,  1.1330e-03],
        [ 1.2531e-03, -2.9583e-03,  1.5707e-03,  ...,  8.8835e-04,
         -1.8063e-03, -1.0548e-03],
        [ 9.4366e-04,  2.0390e-03, -7.5102e-04,  ...,  1.6441e-03,
          8.7404e-04,  5.2166e-04],
        ...,
        [ 2.4414e-03, -9.9754e-04,  6.4492e-05,  ..., -1.9608e-03,
          3.1357e-03,  5.8079e-04],
        [-1.8597e-03,  2.2864e-04, -1.4801e-03,  ..., -2.3098e-03,
         -4.1437e-04,  3.2539e-03],
        [ 2.8801e-03, -3.2768e-03,  2.1720e-04,  ...,  1.3542e-03,
         -9.5844e-04, -3.0575e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-8.9688,  0.0940,  2.2441,  ...,  3.6387,  0.5039,  3.1836]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1003, -0.0973,  0.2527,  ..., -0.0630, -0.1600, -0.2021]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 02:32:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most noisy, it is noisiest
If something is the most costly, it is costliest
If something is the most subtle, it is subtlest
If something is the most neat, it is neatest
If something is the most dense, it is densest
If something is the most vague, it is vaguest
If something is the most cruel, it is cruelest
If something is the most tricky, it is
2024-07-10 02:32:49 root INFO     [order_1_approx] starting weight calculation for If something is the most subtle, it is subtlest
If something is the most cruel, it is cruelest
If something is the most noisy, it is noisiest
If something is the most costly, it is costliest
If something is the most tricky, it is trickiest
If something is the most dense, it is densest
If something is the most vague, it is vaguest
If something is the most neat, it is
2024-07-10 02:32:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 02:37:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 02:41:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3425, -0.5239,  0.1213,  ..., -0.1097, -0.5156,  0.5298],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3215, -0.5293,  0.0978,  ..., -0.0972, -0.5034,  0.4897],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.9219,  1.1436, -1.5127,  ...,  3.2656, -3.7168,  6.4141],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0053, -0.0337, -0.0006,  ...,  0.0149, -0.0260, -0.0085],
        [-0.0117, -0.0238,  0.0083,  ...,  0.0098, -0.0035,  0.0014],
        [ 0.0049,  0.0103, -0.0004,  ...,  0.0011,  0.0181, -0.0060],
        ...,
        [ 0.0041, -0.0253, -0.0102,  ..., -0.0118, -0.0158,  0.0003],
        [ 0.0032, -0.0081, -0.0023,  ..., -0.0029,  0.0015,  0.0153],
        [-0.0085,  0.0117, -0.0017,  ..., -0.0202, -0.0022, -0.0316]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-5.7602e-04, -1.6804e-03,  7.6199e-04,  ...,  5.3549e-04,
         -2.9159e-04,  3.9434e-04],
        [-1.1187e-03, -2.3003e-03,  1.9608e-03,  ...,  5.3644e-05,
         -1.1930e-03, -1.3123e-03],
        [ 6.7616e-04,  2.0683e-04, -1.3828e-03,  ...,  2.1610e-03,
          2.4319e-03,  2.2202e-03],
        ...,
        [ 3.5133e-03,  3.6478e-04,  4.7183e-04,  ..., -1.6499e-03,
          8.6594e-04,  1.6201e-04],
        [ 7.4327e-05,  2.8205e-04, -3.0446e-04,  ..., -1.6823e-03,
          1.0023e-03,  1.7138e-03],
        [-2.6779e-03,  1.3900e-04, -2.9335e-03,  ...,  7.6294e-05,
         -7.2908e-04, -4.9543e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.9658, -0.2939, -1.8096,  ...,  2.7734, -3.6035,  5.5078]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0694, -0.1786,  0.0126,  ...,  0.2876, -0.0218, -0.1627]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 02:41:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most subtle, it is subtlest
If something is the most cruel, it is cruelest
If something is the most noisy, it is noisiest
If something is the most costly, it is costliest
If something is the most tricky, it is trickiest
If something is the most dense, it is densest
If something is the most vague, it is vaguest
If something is the most neat, it is
2024-07-10 02:41:16 root INFO     [order_1_approx] starting weight calculation for If something is the most dense, it is densest
If something is the most tricky, it is trickiest
If something is the most noisy, it is noisiest
If something is the most costly, it is costliest
If something is the most cruel, it is cruelest
If something is the most subtle, it is subtlest
If something is the most neat, it is neatest
If something is the most vague, it is
2024-07-10 02:41:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 02:45:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 02:49:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5278,  0.2114,  0.1694,  ..., -1.3887,  0.0525, -0.1519],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.4658,  0.1747,  0.1316,  ..., -1.1826,  0.0263, -0.1628],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.3975,  4.0039, -5.1172,  ...,  8.7500,  3.1113, -1.6719],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0026, -0.0058,  0.0073,  ...,  0.0041, -0.0161, -0.0156],
        [-0.0109, -0.0098, -0.0082,  ...,  0.0065, -0.0170, -0.0219],
        [ 0.0176,  0.0057,  0.0052,  ...,  0.0066, -0.0054,  0.0127],
        ...,
        [-0.0067, -0.0229,  0.0043,  ..., -0.0137, -0.0155, -0.0213],
        [ 0.0065,  0.0089,  0.0114,  ...,  0.0134, -0.0268, -0.0076],
        [-0.0136, -0.0140, -0.0152,  ..., -0.0399,  0.0086, -0.0136]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 4.4250e-04,  2.7132e-04,  3.1972e-04,  ...,  1.0753e-04,
         -2.2733e-04, -1.2875e-03],
        [ 1.7328e-03, -3.0499e-03, -1.4343e-03,  ...,  1.0519e-03,
          1.2150e-03, -1.4143e-03],
        [-6.1464e-04,  2.4643e-03,  6.2227e-05,  ..., -2.1970e-04,
          3.6955e-04, -2.7919e-04],
        ...,
        [ 1.3466e-03, -9.3937e-05, -1.5268e-03,  ...,  8.0585e-04,
          1.5297e-03, -1.6558e-04],
        [-4.0460e-04, -5.7280e-05, -1.1387e-03,  ...,  1.1225e-03,
         -5.5599e-04,  1.3959e-04],
        [ 2.5539e-03, -9.2888e-04, -2.0752e-03,  ...,  1.2712e-03,
         -2.6166e-05, -2.9182e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7900,  4.5391, -5.5664,  ...,  8.6562,  2.7051, -1.3740]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0042, -0.2385,  0.1093,  ..., -0.0323, -0.2566, -0.1106]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 02:49:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most dense, it is densest
If something is the most tricky, it is trickiest
If something is the most noisy, it is noisiest
If something is the most costly, it is costliest
If something is the most cruel, it is cruelest
If something is the most subtle, it is subtlest
If something is the most neat, it is neatest
If something is the most vague, it is
2024-07-10 02:49:39 root INFO     [order_1_approx] starting weight calculation for If something is the most costly, it is costliest
If something is the most subtle, it is subtlest
If something is the most neat, it is neatest
If something is the most tricky, it is trickiest
If something is the most vague, it is vaguest
If something is the most noisy, it is noisiest
If something is the most dense, it is densest
If something is the most cruel, it is
2024-07-10 02:49:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 02:53:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 02:57:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0469, -0.2449,  0.4353,  ...,  0.5635,  0.4734,  0.1180],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.0359, -0.2500,  0.3660,  ...,  0.4954,  0.4055,  0.0834],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-7.5156, -1.2520, -5.5391,  ..., -0.9697, -3.2949,  4.8203],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0035, -0.0020,  0.0253,  ...,  0.0075, -0.0079, -0.0277],
        [ 0.0020, -0.0369,  0.0110,  ..., -0.0088,  0.0189, -0.0036],
        [-0.0035, -0.0085, -0.0057,  ...,  0.0113,  0.0051, -0.0061],
        ...,
        [-0.0122, -0.0382,  0.0015,  ..., -0.0355, -0.0073, -0.0187],
        [-0.0043,  0.0064,  0.0200,  ...,  0.0075, -0.0115,  0.0069],
        [-0.0218,  0.0216, -0.0091,  ..., -0.0130, -0.0045, -0.0437]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.5916e-04, -1.4887e-03,  1.3447e-03,  ..., -8.3733e-04,
         -9.1195e-05,  2.2316e-03],
        [ 1.0071e-03, -1.9274e-03,  1.3275e-03,  ...,  1.7815e-03,
          6.8617e-04, -8.8644e-04],
        [ 8.6308e-05,  1.5068e-04, -2.1362e-03,  ..., -1.8673e-03,
         -5.4073e-04,  3.6907e-03],
        ...,
        [ 2.1782e-03, -1.1122e-04, -9.1910e-05,  ..., -3.2978e-03,
          4.9174e-05, -6.9952e-04],
        [-1.0052e-03,  4.8637e-04, -6.3610e-04,  ..., -1.5163e-03,
         -5.7101e-05,  2.1324e-03],
        [ 1.7910e-03,  1.8215e-03, -1.8806e-03,  ...,  1.7290e-03,
         -9.8324e-04, -2.2697e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-8.1094, -0.8896, -5.6250,  ..., -1.3369, -3.1641,  4.2617]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.2332, -0.1133,  0.2278,  ..., -0.0225,  0.0690,  0.1324]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 02:58:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most costly, it is costliest
If something is the most subtle, it is subtlest
If something is the most neat, it is neatest
If something is the most tricky, it is trickiest
If something is the most vague, it is vaguest
If something is the most noisy, it is noisiest
If something is the most dense, it is densest
If something is the most cruel, it is
2024-07-10 02:58:00 root INFO     [order_1_approx] starting weight calculation for If something is the most neat, it is neatest
If something is the most vague, it is vaguest
If something is the most cruel, it is cruelest
If something is the most dense, it is densest
If something is the most noisy, it is noisiest
If something is the most tricky, it is trickiest
If something is the most subtle, it is subtlest
If something is the most costly, it is
2024-07-10 02:58:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 03:02:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 03:06:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3066, -0.0173, -0.5625,  ...,  0.7026,  0.5029,  0.4497],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.2991, -0.0370, -0.5249,  ...,  0.6421,  0.4504,  0.4104],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-5.7266, -3.8535,  0.6318,  ...,  0.4614, -0.2793,  4.3477],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0056, -0.0038,  0.0126,  ...,  0.0124, -0.0046, -0.0178],
        [-0.0016, -0.0090, -0.0044,  ..., -0.0003, -0.0085, -0.0083],
        [ 0.0030,  0.0042,  0.0080,  ...,  0.0139, -0.0102,  0.0206],
        ...,
        [ 0.0019, -0.0078, -0.0006,  ..., -0.0143,  0.0042, -0.0184],
        [-0.0017, -0.0028,  0.0070,  ...,  0.0032, -0.0237, -0.0021],
        [-0.0080,  0.0073, -0.0036,  ...,  0.0043, -0.0087, -0.0321]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.7986e-03, -6.0034e-04, -7.9250e-04,  ...,  2.6369e-04,
          6.6853e-04,  2.3232e-03],
        [ 4.0436e-04, -1.4181e-03, -4.2534e-04,  ...,  1.4372e-03,
         -1.2493e-03, -4.8304e-04],
        [ 1.5554e-03,  7.6008e-04, -3.5238e-04,  ..., -5.7411e-04,
         -6.7830e-05,  3.5210e-03],
        ...,
        [ 4.2582e-04, -1.2932e-03,  8.0061e-04,  ..., -1.6661e-03,
          1.5135e-03,  1.3924e-03],
        [-2.1172e-04,  9.3460e-04, -9.4557e-04,  ..., -7.9966e-04,
         -7.8726e-04, -1.5974e-04],
        [ 1.2493e-03,  1.1415e-03, -2.2888e-05,  ..., -6.7139e-04,
          5.9605e-04, -1.9703e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-6.0742, -2.9785,  2.2188,  ...,  0.4456, -0.6538,  3.2871]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.2305, -0.0394,  0.1960,  ..., -0.0059, -0.1343,  0.0146]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 03:06:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most neat, it is neatest
If something is the most vague, it is vaguest
If something is the most cruel, it is cruelest
If something is the most dense, it is densest
If something is the most noisy, it is noisiest
If something is the most tricky, it is trickiest
If something is the most subtle, it is subtlest
If something is the most costly, it is
2024-07-10 03:06:26 root INFO     total operator prediction time: 4037.3779242038727 seconds
2024-07-10 03:06:26 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+er_irreg
2024-07-10 03:06:28 root INFO     building operator verb+er_irreg
2024-07-10 03:06:28 root INFO     [order_1_approx] starting weight calculation for If you provide something, you are a provider
If you send something, you are a sender
If you skydive something, you are a skydiver
If you promote something, you are a promoter
If you deliver something, you are a deliverer
If you believe something, you are a believer
If you mourn something, you are a mourner
If you develop something, you are a
2024-07-10 03:06:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 03:10:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 03:14:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5649, -0.2859,  0.6260,  ...,  0.4326, -0.5127,  0.4653],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.6226, -0.3381,  0.6392,  ...,  0.4551, -0.5713,  0.4929],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.4453, -1.6074, -3.5078,  ..., -5.1992,  0.9380,  0.3477],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0085,  0.0047, -0.0095,  ..., -0.0026, -0.0029, -0.0122],
        [-0.0005, -0.0166,  0.0137,  ...,  0.0061, -0.0066, -0.0070],
        [-0.0008,  0.0101, -0.0137,  ...,  0.0009, -0.0172,  0.0145],
        ...,
        [ 0.0088, -0.0179,  0.0121,  ..., -0.0117,  0.0100, -0.0034],
        [ 0.0205,  0.0009,  0.0023,  ...,  0.0020,  0.0160,  0.0044],
        [ 0.0052, -0.0046, -0.0076,  ..., -0.0014,  0.0172,  0.0103]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-6.2275e-04, -1.5345e-03,  9.4891e-05,  ..., -5.1641e-04,
         -1.0843e-03,  4.7016e-04],
        [-4.9639e-04, -1.2112e-04,  1.9312e-03,  ...,  1.9703e-03,
          5.0735e-04, -1.0481e-03],
        [ 1.2708e-04,  8.9693e-04, -2.2144e-03,  ..., -1.2321e-03,
         -1.2426e-03,  2.4681e-03],
        ...,
        [ 1.4305e-05, -1.0662e-03, -3.9577e-05,  ..., -6.7759e-04,
          1.6022e-03, -2.4967e-03],
        [ 1.7619e-04,  8.0109e-04,  9.1934e-04,  ...,  4.8256e-04,
          1.6613e-03, -2.0158e-04],
        [ 2.1827e-04,  1.3046e-03,  9.2506e-04,  ...,  3.5000e-04,
         -6.0749e-04, -8.6606e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.1641, -1.8027, -4.6641,  ..., -5.3750,  1.5410,  0.3308]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0576, -0.0905,  0.4519,  ..., -0.4651,  0.1309,  0.0878]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 03:14:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you provide something, you are a provider
If you send something, you are a sender
If you skydive something, you are a skydiver
If you promote something, you are a promoter
If you deliver something, you are a deliverer
If you believe something, you are a believer
If you mourn something, you are a mourner
If you develop something, you are a
2024-07-10 03:14:55 root INFO     [order_1_approx] starting weight calculation for If you skydive something, you are a skydiver
If you promote something, you are a promoter
If you deliver something, you are a deliverer
If you send something, you are a sender
If you provide something, you are a provider
If you believe something, you are a believer
If you develop something, you are a developer
If you mourn something, you are a
2024-07-10 03:14:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 03:19:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 03:23:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0413, -0.5225, -0.1464,  ...,  0.7783, -0.2974,  0.0598],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.0313, -0.5127, -0.1425,  ...,  0.6875, -0.2910,  0.0296],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.7363,  1.6309, -6.2539,  ..., -2.8984,  4.1250,  3.4043],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 5.9662e-03,  3.6469e-03,  7.7248e-03,  ...,  3.4504e-03,
         -5.0888e-03,  2.2697e-04],
        [-4.4403e-03, -6.1684e-03,  2.6112e-03,  ...,  1.7471e-03,
         -2.1877e-03,  5.7220e-05],
        [ 1.2039e-02,  8.3313e-03, -3.6011e-03,  ...,  5.0278e-03,
          1.6357e-02,  1.6571e-02],
        ...,
        [-7.4768e-03, -1.9135e-02, -8.6212e-03,  ..., -3.2501e-03,
          1.4946e-02, -4.6463e-03],
        [ 2.9068e-03, -1.2741e-02, -4.4537e-04,  ...,  2.2736e-03,
         -8.3694e-03,  1.4772e-03],
        [-1.4412e-02, -2.5196e-03,  3.0403e-03,  ..., -1.2100e-02,
          1.8387e-02, -1.4877e-04]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-6.8665e-04, -1.8525e-04,  7.0095e-04,  ..., -1.6713e-04,
         -2.0468e-04,  1.7929e-04],
        [ 7.9250e-04, -6.6042e-04,  8.8358e-04,  ..., -9.6416e-04,
          7.0858e-04,  2.8038e-04],
        [-3.5548e-04, -2.1172e-04, -2.4757e-03,  ..., -1.3447e-03,
          3.3641e-04,  1.9236e-03],
        ...,
        [-5.5695e-04, -4.5824e-04,  5.6982e-04,  ...,  1.3590e-03,
          1.2083e-03, -1.8616e-03],
        [ 3.3379e-05,  4.4918e-04,  1.2434e-04,  ...,  6.8808e-04,
         -8.8215e-04, -3.2878e-04],
        [-1.2875e-04, -7.9727e-04,  5.1498e-05,  ..., -3.3045e-04,
          6.1226e-04, -3.9959e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5078,  1.4863, -6.3789,  ..., -2.5938,  4.1016,  3.4961]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0401, -0.2098, -0.1010,  ..., -0.0771, -0.1626, -0.0340]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 03:23:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you skydive something, you are a skydiver
If you promote something, you are a promoter
If you deliver something, you are a deliverer
If you send something, you are a sender
If you provide something, you are a provider
If you believe something, you are a believer
If you develop something, you are a developer
If you mourn something, you are a
2024-07-10 03:23:14 root INFO     [order_1_approx] starting weight calculation for If you provide something, you are a provider
If you deliver something, you are a deliverer
If you promote something, you are a promoter
If you send something, you are a sender
If you mourn something, you are a mourner
If you skydive something, you are a skydiver
If you develop something, you are a developer
If you believe something, you are a
2024-07-10 03:23:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 03:27:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 03:31:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3789, -0.3906, -0.0170,  ...,  0.5996,  0.1823,  0.5498],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.3865, -0.4207, -0.0290,  ...,  0.5796,  0.1604,  0.5386],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.0586, -1.3789, -4.5195,  ..., -0.9941,  1.0498,  0.5439],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-6.8436e-03,  9.1248e-03,  2.7866e-03,  ..., -3.0537e-03,
         -6.4278e-03, -1.6403e-04],
        [ 6.4926e-03,  1.5144e-03, -2.6951e-03,  ..., -1.9207e-03,
          1.2115e-02, -1.5144e-03],
        [ 1.0559e-02,  6.9504e-03,  2.3727e-03,  ...,  5.7399e-05,
         -6.1531e-03,  2.6035e-03],
        ...,
        [-1.4343e-02, -1.6632e-02, -9.6130e-03,  ..., -1.2474e-02,
          3.2063e-03,  6.7101e-03],
        [ 8.1787e-03, -1.4481e-02,  3.0022e-03,  ...,  8.2397e-03,
         -3.8013e-03, -2.1553e-03],
        [ 8.4915e-03,  4.4861e-03, -1.0704e-02,  ..., -5.1537e-03,
          1.0193e-02, -4.6158e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.1005e-03,  1.1482e-03,  1.0557e-03,  ...,  5.6553e-04,
          8.4734e-04, -1.2465e-03],
        [-4.5156e-04,  5.8365e-04, -7.6675e-04,  ..., -1.2140e-03,
          1.0366e-03,  1.0481e-03],
        [-1.0281e-03,  1.6975e-03, -1.9798e-03,  ...,  1.5593e-04,
          1.0910e-03, -7.4744e-05],
        ...,
        [-3.4046e-04,  7.5054e-04,  8.1182e-05,  ...,  1.1425e-03,
          2.9922e-05, -1.2932e-03],
        [ 5.8508e-04, -1.1339e-03, -5.9652e-04,  ..., -7.6532e-04,
          8.1062e-06, -1.0395e-03],
        [ 4.3440e-04,  1.3504e-03, -7.2384e-04,  ...,  2.0814e-04,
         -2.1982e-04, -1.6155e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1543, -1.5557, -4.0234,  ..., -1.4961,  1.5137,  0.7842]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1296,  0.1987, -0.1602,  ..., -0.2898, -0.1946, -0.0674]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 03:31:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you provide something, you are a provider
If you deliver something, you are a deliverer
If you promote something, you are a promoter
If you send something, you are a sender
If you mourn something, you are a mourner
If you skydive something, you are a skydiver
If you develop something, you are a developer
If you believe something, you are a
2024-07-10 03:31:40 root INFO     [order_1_approx] starting weight calculation for If you deliver something, you are a deliverer
If you send something, you are a sender
If you provide something, you are a provider
If you believe something, you are a believer
If you mourn something, you are a mourner
If you skydive something, you are a skydiver
If you develop something, you are a developer
If you promote something, you are a
2024-07-10 03:31:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 03:35:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 03:40:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2510, -0.5034,  0.0906,  ..., -0.0851, -0.6631,  0.4990],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.2627, -0.5459,  0.0746,  ..., -0.0817, -0.6880,  0.4937],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5879, -1.2715, -3.7539,  ..., -0.2810,  1.2236,  1.4111],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0073, -0.0157, -0.0021,  ..., -0.0148,  0.0053, -0.0201],
        [-0.0009,  0.0043,  0.0034,  ...,  0.0092, -0.0116, -0.0047],
        [ 0.0075,  0.0074,  0.0232,  ..., -0.0052,  0.0141,  0.0094],
        ...,
        [-0.0010, -0.0186, -0.0045,  ..., -0.0062,  0.0035,  0.0120],
        [ 0.0073, -0.0094,  0.0101,  ..., -0.0059,  0.0002,  0.0034],
        [ 0.0079, -0.0210,  0.0002,  ..., -0.0083,  0.0184, -0.0106]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.3523e-03, -2.5024e-03,  6.3753e-04,  ..., -1.5078e-03,
          5.8651e-05, -1.0166e-03],
        [-1.7662e-03, -1.4172e-03,  1.7595e-04,  ...,  3.3188e-04,
         -8.8024e-04,  2.4128e-04],
        [ 9.6035e-04,  1.1187e-03, -2.2240e-03,  ..., -1.1930e-03,
         -1.1748e-04,  7.6342e-04],
        ...,
        [-1.5116e-03, -1.5287e-03,  2.4452e-03,  ..., -3.3426e-04,
          4.5276e-04, -6.4945e-04],
        [-8.7261e-04,  2.9778e-04,  1.3423e-04,  ...,  7.1764e-05,
         -6.9904e-04, -9.7156e-05],
        [-1.8668e-04, -1.9908e-04,  1.6060e-03,  ...,  3.7742e-04,
          4.4560e-04, -6.4087e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0938, -2.0020, -3.2734,  ..., -0.0624,  0.8516,  0.8574]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[1.2341e-01, 2.5940e-04, 2.8638e-01,  ..., 7.0190e-02, 4.6387e-02,
         9.9609e-02]], device='cuda:1', dtype=torch.float16,
       grad_fn=<SubBackward0>)
                    
2024-07-10 03:40:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you deliver something, you are a deliverer
If you send something, you are a sender
If you provide something, you are a provider
If you believe something, you are a believer
If you mourn something, you are a mourner
If you skydive something, you are a skydiver
If you develop something, you are a developer
If you promote something, you are a
2024-07-10 03:40:08 root INFO     [order_1_approx] starting weight calculation for If you develop something, you are a developer
If you send something, you are a sender
If you promote something, you are a promoter
If you provide something, you are a provider
If you mourn something, you are a mourner
If you believe something, you are a believer
If you deliver something, you are a deliverer
If you skydive something, you are a
2024-07-10 03:40:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 03:44:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 03:48:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5449, -0.5088, -0.0175,  ..., -0.3655,  0.6807,  0.0071],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.4863, -0.4795, -0.0234,  ..., -0.3059,  0.5835, -0.0142],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5210, -1.4766,  0.5498,  ..., -8.2500, -2.2500,  0.0225],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0006,  0.0060,  0.0010,  ..., -0.0025,  0.0046, -0.0028],
        [ 0.0064,  0.0035, -0.0012,  ...,  0.0011,  0.0019,  0.0082],
        [ 0.0100,  0.0097, -0.0034,  ...,  0.0024,  0.0027,  0.0034],
        ...,
        [ 0.0020, -0.0061, -0.0019,  ..., -0.0045,  0.0048,  0.0039],
        [ 0.0095,  0.0076,  0.0042,  ...,  0.0014,  0.0035,  0.0002],
        [ 0.0009,  0.0099, -0.0040,  ..., -0.0041, -0.0013,  0.0042]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 8.5115e-04, -1.1146e-04, -1.1835e-03,  ..., -6.8331e-04,
         -9.4700e-04, -3.4046e-04],
        [ 1.1082e-03, -2.2984e-04, -2.1591e-03,  ...,  2.2626e-04,
         -7.6890e-06,  6.2752e-04],
        [-1.6212e-04,  2.0683e-04,  1.6928e-05,  ..., -1.4029e-03,
          1.7726e-04, -7.0333e-04],
        ...,
        [ 1.5907e-03, -3.6001e-04,  1.4567e-04,  ...,  4.9877e-04,
         -5.2452e-06, -4.8804e-04],
        [ 1.0242e-03,  1.1854e-03, -1.9407e-04,  ...,  4.0269e-04,
         -4.5300e-04, -4.9639e-04],
        [-7.5817e-04,  9.5129e-04,  2.5868e-04,  ..., -6.8808e-04,
          1.2398e-03,  7.0620e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3269, -1.6279,  0.2515,  ..., -7.8750, -2.4531,  0.2330]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0238, -0.0631, -0.0553,  ..., -0.0858, -0.2517,  0.0707]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 03:48:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you develop something, you are a developer
If you send something, you are a sender
If you promote something, you are a promoter
If you provide something, you are a provider
If you mourn something, you are a mourner
If you believe something, you are a believer
If you deliver something, you are a deliverer
If you skydive something, you are a
2024-07-10 03:48:32 root INFO     [order_1_approx] starting weight calculation for If you promote something, you are a promoter
If you develop something, you are a developer
If you skydive something, you are a skydiver
If you send something, you are a sender
If you believe something, you are a believer
If you mourn something, you are a mourner
If you provide something, you are a provider
If you deliver something, you are a
2024-07-10 03:48:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 03:52:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 03:56:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2559, -0.6279,  0.3101,  ..., -0.1537, -1.3945, -0.1226],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.2568, -0.6802,  0.2903,  ..., -0.1495, -1.4326, -0.1544],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.5977, -1.3271, -1.0547,  ..., -5.9102,  1.9590,  3.5859],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0134, -0.0156,  0.0006,  ..., -0.0053, -0.0137, -0.0076],
        [-0.0029,  0.0007,  0.0067,  ..., -0.0096,  0.0006, -0.0149],
        [-0.0021, -0.0026, -0.0121,  ..., -0.0113,  0.0134, -0.0170],
        ...,
        [-0.0041,  0.0188,  0.0014,  ..., -0.0131,  0.0267, -0.0290],
        [ 0.0134,  0.0032, -0.0014,  ..., -0.0079, -0.0073,  0.0080],
        [-0.0029, -0.0047,  0.0188,  ..., -0.0110,  0.0033, -0.0032]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.1725e-03, -2.1458e-03, -4.6992e-04,  ..., -6.2084e-04,
          3.0003e-03, -1.2469e-04],
        [-4.1819e-04,  4.0936e-04, -3.1686e-04,  ..., -1.7757e-03,
          6.8283e-04, -1.7822e-04],
        [ 6.5613e-04, -3.1605e-03,  1.1635e-04,  ..., -6.7282e-04,
          8.6117e-04,  2.2182e-03],
        ...,
        [ 9.4032e-04,  2.0275e-03, -6.2656e-04,  ...,  6.0320e-04,
          8.1062e-06, -5.3291e-03],
        [-1.0309e-03, -1.0319e-03,  1.2560e-03,  ..., -1.6356e-03,
          2.3365e-04, -4.2200e-04],
        [-3.0651e-03, -1.2646e-03,  6.7711e-05,  ...,  3.6693e-04,
         -1.0271e-03, -4.3273e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.8691, -2.4102, -3.8457,  ..., -6.5078,  0.9561,  2.7363]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1549, -0.1353,  0.2418,  ..., -0.4089,  0.0581, -0.0669]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 03:56:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you promote something, you are a promoter
If you develop something, you are a developer
If you skydive something, you are a skydiver
If you send something, you are a sender
If you believe something, you are a believer
If you mourn something, you are a mourner
If you provide something, you are a provider
If you deliver something, you are a
2024-07-10 03:56:55 root INFO     [order_1_approx] starting weight calculation for If you promote something, you are a promoter
If you believe something, you are a believer
If you mourn something, you are a mourner
If you skydive something, you are a skydiver
If you develop something, you are a developer
If you send something, you are a sender
If you deliver something, you are a deliverer
If you provide something, you are a
2024-07-10 03:56:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 04:01:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 04:05:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5259,  0.2180, -0.4058,  ..., -0.1941, -0.4131,  0.8110],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.5703,  0.2212, -0.4280,  ..., -0.1969, -0.4568,  0.8628],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.1895, -0.6641, -0.1323,  ..., -3.8125,  0.3035,  0.0059],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.1154e-02, -1.1826e-02, -6.8855e-03,  ..., -1.7815e-03,
         -1.1017e-02, -1.6357e-02],
        [-7.0190e-03,  4.0207e-03,  5.7144e-03,  ..., -9.9411e-03,
          3.8147e-05, -4.8027e-03],
        [-5.9280e-03, -1.1391e-02,  3.8624e-03,  ...,  4.8141e-03,
         -3.8338e-03, -2.3651e-04],
        ...,
        [-6.7863e-03,  4.3716e-03, -3.5801e-03,  ..., -1.2535e-02,
          2.0020e-02,  1.1780e-02],
        [ 3.2715e-02,  2.5208e-02, -1.5480e-02,  ..., -1.3451e-02,
          3.3203e-02, -7.7515e-03],
        [ 4.6768e-03,  7.3204e-03,  2.3041e-03,  ..., -1.4145e-02,
          1.9379e-02,  4.1847e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.7271e-03, -1.8177e-03, -5.4407e-04,  ...,  8.8596e-04,
          4.5919e-04,  1.0490e-04],
        [-1.8606e-03, -1.8187e-03,  1.0548e-03,  ...,  3.8934e-04,
          8.2684e-04, -1.6689e-06],
        [ 9.4032e-04,  1.1892e-03, -1.3733e-03,  ...,  5.8031e-04,
          2.4261e-03,  1.5097e-03],
        ...,
        [-1.2779e-03, -1.1911e-03,  1.0843e-03,  ...,  2.3899e-03,
          7.2050e-04, -1.5521e-04],
        [ 1.8892e-03,  4.7760e-03, -2.8610e-03,  ..., -2.4815e-03,
         -4.9400e-04, -2.4261e-03],
        [-4.0770e-04,  1.8730e-03,  3.0160e-05,  ...,  8.8215e-05,
         -1.3180e-03, -1.3361e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0518, -0.7515, -0.2883,  ..., -4.0391,  0.2046, -0.9814]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0494, -0.1962,  0.1926,  ..., -0.1768, -0.1387, -0.0753]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 04:05:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you promote something, you are a promoter
If you believe something, you are a believer
If you mourn something, you are a mourner
If you skydive something, you are a skydiver
If you develop something, you are a developer
If you send something, you are a sender
If you deliver something, you are a deliverer
If you provide something, you are a
2024-07-10 04:05:23 root INFO     [order_1_approx] starting weight calculation for If you provide something, you are a provider
If you deliver something, you are a deliverer
If you believe something, you are a believer
If you skydive something, you are a skydiver
If you mourn something, you are a mourner
If you promote something, you are a promoter
If you develop something, you are a developer
If you send something, you are a
2024-07-10 04:05:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 04:09:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 04:13:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5518,  0.0769,  0.8867,  ...,  0.1398, -0.4380,  0.7847],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.5742,  0.0633,  0.8604,  ...,  0.1412, -0.4646,  0.8013],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0519,  1.1689,  0.3228,  ..., -0.3501, -0.1055,  1.0527],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0066,  0.0197,  0.0025,  ..., -0.0003,  0.0038,  0.0031],
        [-0.0043, -0.0038,  0.0018,  ..., -0.0045, -0.0002, -0.0052],
        [ 0.0267,  0.0181,  0.0074,  ..., -0.0031,  0.0113,  0.0217],
        ...,
        [-0.0329, -0.0053, -0.0049,  ..., -0.0013,  0.0006,  0.0092],
        [ 0.0414, -0.0004,  0.0181,  ...,  0.0078, -0.0035,  0.0032],
        [ 0.0216, -0.0012,  0.0094,  ..., -0.0062,  0.0248,  0.0085]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.4462e-04, -1.1253e-03,  2.4319e-03,  ...,  1.0090e-03,
         -9.6941e-04, -2.8992e-04],
        [-8.2970e-04, -2.4204e-03, -8.1587e-04,  ..., -4.6515e-04,
          1.5783e-03, -1.9073e-06],
        [-1.0557e-03, -4.2975e-05, -2.5558e-03,  ..., -1.1044e-03,
          2.6283e-03, -4.5013e-04],
        ...,
        [-7.0524e-04, -7.6818e-04, -2.2049e-03,  ..., -1.0948e-03,
          1.5373e-03, -1.3924e-03],
        [ 7.0238e-04, -1.7428e-04,  1.7567e-03,  ..., -1.3866e-03,
         -2.9583e-03, -2.0003e-04],
        [ 5.8126e-04,  9.0003e-05, -4.4966e-04,  ..., -3.1042e-04,
         -8.4019e-04, -1.0109e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1759,  0.6650,  0.8823,  ..., -1.1152,  0.2278,  1.2334]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1537, -0.0376,  0.2585,  ...,  0.0572,  0.0245, -0.0241]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 04:13:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you provide something, you are a provider
If you deliver something, you are a deliverer
If you believe something, you are a believer
If you skydive something, you are a skydiver
If you mourn something, you are a mourner
If you promote something, you are a promoter
If you develop something, you are a developer
If you send something, you are a
2024-07-10 04:13:47 root INFO     total operator prediction time: 4039.560284614563 seconds
2024-07-10 04:13:47 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on over+adj_reg
2024-07-10 04:13:50 root INFO     building operator over+adj_reg
2024-07-10 04:13:50 root INFO     [order_1_approx] starting weight calculation for If something is too crowded, it is overcrowded
If something is too populated, it is overpopulated
If something is too stated, it is overstated
If something is too optimistic, it is overoptimistic
If something is too played, it is overplayed
If something is too subscribed, it is oversubscribed
If something is too protected, it is overprotected
If something is too laid, it is
2024-07-10 04:13:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 04:18:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 04:22:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3032, -0.0957,  0.4946,  ...,  1.9531,  0.3853, -0.4194],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.3064, -0.1160,  0.4551,  ...,  1.8584,  0.3562, -0.4495],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8066, -0.3396, -1.5732,  ..., -0.6133,  0.3606,  4.8242],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0128, -0.0094,  0.0067,  ...,  0.0150, -0.0094, -0.0110],
        [-0.0199, -0.0053, -0.0100,  ...,  0.0021, -0.0037, -0.0074],
        [ 0.0088,  0.0049,  0.0076,  ..., -0.0046,  0.0028,  0.0155],
        ...,
        [-0.0056, -0.0241, -0.0078,  ...,  0.0143,  0.0030,  0.0099],
        [ 0.0022, -0.0066,  0.0089,  ..., -0.0124, -0.0037, -0.0011],
        [ 0.0051, -0.0105,  0.0166,  ...,  0.0047,  0.0066,  0.0038]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 4.0364e-04, -1.3399e-03, -5.3883e-04,  ...,  2.7776e-04,
          1.4782e-03,  6.6996e-05],
        [-4.4966e-04, -2.6321e-03,  1.6534e-04,  ...,  6.4707e-04,
          1.0452e-03,  5.1141e-05],
        [-4.3201e-04, -1.1711e-03, -1.0719e-03,  ..., -1.1950e-03,
         -1.5283e-04,  2.1019e-03],
        ...,
        [ 1.7090e-03, -2.2068e-03,  8.4877e-05,  ..., -1.2493e-04,
          5.1641e-04,  2.1243e-04],
        [ 1.3533e-03,  6.8951e-04,  6.1226e-04,  ..., -9.8825e-05,
          1.4925e-04, -7.5436e-04],
        [-1.3580e-03, -1.7548e-03, -5.5122e-04,  ...,  1.6093e-04,
         -1.3123e-03, -6.3300e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5996,  0.1558, -2.2305,  ..., -0.5083,  0.9707,  4.1875]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1943, -0.2949,  0.2930,  ...,  0.0118, -0.0156, -0.0376]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 04:22:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too crowded, it is overcrowded
If something is too populated, it is overpopulated
If something is too stated, it is overstated
If something is too optimistic, it is overoptimistic
If something is too played, it is overplayed
If something is too subscribed, it is oversubscribed
If something is too protected, it is overprotected
If something is too laid, it is
2024-07-10 04:22:16 root INFO     [order_1_approx] starting weight calculation for If something is too played, it is overplayed
If something is too protected, it is overprotected
If something is too laid, it is overlaid
If something is too optimistic, it is overoptimistic
If something is too populated, it is overpopulated
If something is too crowded, it is overcrowded
If something is too subscribed, it is oversubscribed
If something is too stated, it is
2024-07-10 04:22:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 04:26:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 04:30:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6616,  0.2275,  0.6260,  ...,  0.5400,  0.3528, -0.6025],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.7202,  0.2313,  0.6294,  ...,  0.5591,  0.3533, -0.6890],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5400, -1.5723,  1.3799,  ...,  0.8955,  3.6465,  4.6484],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0155, -0.0133,  0.0089,  ...,  0.0103, -0.0310, -0.0085],
        [ 0.0065, -0.0086, -0.0026,  ...,  0.0012, -0.0210,  0.0009],
        [ 0.0058,  0.0130,  0.0004,  ..., -0.0007, -0.0131,  0.0260],
        ...,
        [ 0.0057, -0.0017, -0.0183,  ..., -0.0013,  0.0183, -0.0045],
        [ 0.0070, -0.0046, -0.0106,  ..., -0.0324, -0.0264, -0.0093],
        [-0.0094, -0.0100,  0.0170,  ..., -0.0013, -0.0182, -0.0047]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0020,  0.0002,  0.0001,  ..., -0.0013,  0.0002, -0.0008],
        [ 0.0017, -0.0015,  0.0014,  ..., -0.0004,  0.0001,  0.0019],
        [ 0.0020,  0.0028, -0.0027,  ..., -0.0002, -0.0019,  0.0020],
        ...,
        [ 0.0020,  0.0002,  0.0004,  ...,  0.0014,  0.0009, -0.0001],
        [-0.0010,  0.0010,  0.0007,  ..., -0.0016, -0.0010, -0.0012],
        [-0.0006, -0.0023,  0.0017,  ...,  0.0021, -0.0014, -0.0026]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6709, -1.5605,  0.6553,  ...,  1.2373,  3.8398,  4.7227]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1136, -0.2107,  0.4258,  ..., -0.0370, -0.2429, -0.1080]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 04:30:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too played, it is overplayed
If something is too protected, it is overprotected
If something is too laid, it is overlaid
If something is too optimistic, it is overoptimistic
If something is too populated, it is overpopulated
If something is too crowded, it is overcrowded
If something is too subscribed, it is oversubscribed
If something is too stated, it is
2024-07-10 04:30:42 root INFO     [order_1_approx] starting weight calculation for If something is too laid, it is overlaid
If something is too stated, it is overstated
If something is too optimistic, it is overoptimistic
If something is too subscribed, it is oversubscribed
If something is too played, it is overplayed
If something is too populated, it is overpopulated
If something is too protected, it is overprotected
If something is too crowded, it is
2024-07-10 04:30:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 04:34:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 04:39:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6953, -0.3201,  0.8540,  ..., -0.7314,  0.9287, -0.4900],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.6455, -0.3218,  0.7310,  ..., -0.6421,  0.8159, -0.4858],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.9883, -1.6299, -2.6836,  ...,  1.4355,  1.9922,  1.2637],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0091, -0.0199,  0.0059,  ..., -0.0122, -0.0084, -0.0196],
        [-0.0290,  0.0067,  0.0056,  ...,  0.0026, -0.0135, -0.0023],
        [ 0.0035,  0.0078, -0.0036,  ..., -0.0006,  0.0235,  0.0153],
        ...,
        [-0.0141, -0.0203,  0.0023,  ..., -0.0218,  0.0045, -0.0131],
        [ 0.0054,  0.0147,  0.0068,  ..., -0.0100, -0.0173,  0.0219],
        [-0.0129,  0.0063,  0.0014,  ...,  0.0025,  0.0147, -0.0448]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.5654e-03, -2.3508e-04,  5.3644e-04,  ..., -2.4853e-03,
         -9.0790e-04,  4.3945e-03],
        [-2.0905e-03, -1.8559e-03,  2.9068e-03,  ...,  9.0408e-04,
          3.7265e-04,  1.2088e-04],
        [ 4.7231e-04,  6.5279e-04, -5.6648e-04,  ..., -2.3479e-03,
          7.7534e-04,  4.2763e-03],
        ...,
        [ 1.1711e-03,  6.0081e-05,  2.0905e-03,  ..., -2.5234e-03,
          5.4741e-04, -2.3913e-04],
        [ 1.6928e-05,  1.1673e-03, -1.1644e-03,  ...,  7.3242e-04,
         -5.1594e-04, -6.9475e-04],
        [ 3.9625e-04, -1.7061e-03, -8.7929e-04,  ..., -8.0252e-04,
         -2.9635e-04,  5.5790e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.4121, -3.0625, -2.8359,  ...,  0.4805,  3.3398,  1.3008]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2771, -0.3289,  0.5679,  ...,  0.0671, -0.2466,  0.1720]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 04:39:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too laid, it is overlaid
If something is too stated, it is overstated
If something is too optimistic, it is overoptimistic
If something is too subscribed, it is oversubscribed
If something is too played, it is overplayed
If something is too populated, it is overpopulated
If something is too protected, it is overprotected
If something is too crowded, it is
2024-07-10 04:39:07 root INFO     [order_1_approx] starting weight calculation for If something is too crowded, it is overcrowded
If something is too populated, it is overpopulated
If something is too optimistic, it is overoptimistic
If something is too played, it is overplayed
If something is too protected, it is overprotected
If something is too stated, it is overstated
If something is too laid, it is overlaid
If something is too subscribed, it is
2024-07-10 04:39:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 04:43:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 04:47:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9609,  0.2979,  0.9180,  ...,  1.5508,  0.3032,  0.5171],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.9224,  0.2664,  0.8140,  ...,  1.4082,  0.2590,  0.4709],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.3682, -0.4578,  0.0039,  ...,  0.2158, -0.9214,  3.5859],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0084, -0.0172,  0.0101,  ...,  0.0108, -0.0022,  0.0053],
        [ 0.0106, -0.0002, -0.0074,  ..., -0.0048, -0.0247, -0.0104],
        [ 0.0102,  0.0087,  0.0076,  ..., -0.0164, -0.0048, -0.0013],
        ...,
        [ 0.0002, -0.0120,  0.0072,  ...,  0.0028,  0.0127, -0.0076],
        [ 0.0117,  0.0085, -0.0087,  ..., -0.0032, -0.0114,  0.0136],
        [ 0.0010,  0.0195,  0.0142,  ..., -0.0111,  0.0118, -0.0199]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 1.2374e-04,  1.0490e-05,  4.5109e-04,  ...,  6.1417e-04,
          9.2983e-05,  1.1673e-03],
        [ 5.8126e-04, -1.0424e-03,  1.5030e-03,  ...,  3.8004e-04,
         -1.6880e-03,  1.8482e-03],
        [ 3.4094e-04,  2.7966e-04, -5.5599e-04,  ..., -1.9026e-03,
         -9.1076e-05,  1.3123e-03],
        ...,
        [ 1.4486e-03, -1.8561e-04, -4.2915e-05,  ...,  3.6526e-04,
          2.5463e-04,  9.0647e-04],
        [-3.0470e-04,  8.3685e-05, -4.5371e-04,  ..., -1.3580e-03,
         -5.0974e-04,  1.3905e-03],
        [ 5.1212e-04, -4.1604e-05,  2.9850e-04,  ...,  1.2484e-03,
          3.5167e-04, -1.0223e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.1670, -0.7563, -0.8770,  ..., -0.3452, -0.1255,  3.0273]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1068, -0.2113,  0.1259,  ..., -0.0630, -0.1284,  0.0478]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 04:47:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too crowded, it is overcrowded
If something is too populated, it is overpopulated
If something is too optimistic, it is overoptimistic
If something is too played, it is overplayed
If something is too protected, it is overprotected
If something is too stated, it is overstated
If something is too laid, it is overlaid
If something is too subscribed, it is
2024-07-10 04:47:33 root INFO     [order_1_approx] starting weight calculation for If something is too subscribed, it is oversubscribed
If something is too stated, it is overstated
If something is too optimistic, it is overoptimistic
If something is too played, it is overplayed
If something is too laid, it is overlaid
If something is too crowded, it is overcrowded
If something is too protected, it is overprotected
If something is too populated, it is
2024-07-10 04:47:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 04:51:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 04:55:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9619,  0.6768,  1.5410,  ..., -0.6431,  1.4434, -0.6851],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.9067,  0.6260,  1.3564,  ..., -0.5752,  1.3047, -0.6797],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.7344, -0.3721, -0.1250,  ...,  0.4673,  0.4536,  2.0527],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0095, -0.0373,  0.0128,  ...,  0.0055, -0.0072, -0.0143],
        [-0.0114,  0.0036,  0.0078,  ...,  0.0054,  0.0035, -0.0096],
        [ 0.0238,  0.0098,  0.0051,  ..., -0.0039,  0.0117,  0.0230],
        ...,
        [ 0.0095, -0.0130, -0.0014,  ..., -0.0163,  0.0110, -0.0159],
        [ 0.0047,  0.0146, -0.0006,  ..., -0.0009, -0.0181, -0.0106],
        [-0.0037, -0.0063,  0.0174,  ..., -0.0158,  0.0077, -0.0194]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-3.0136e-03, -1.9932e-03,  7.0286e-04,  ...,  4.1270e-04,
         -2.8586e-04, -1.0467e-04],
        [ 6.6471e-04, -1.7986e-03,  2.0752e-03,  ..., -1.0242e-03,
          4.8208e-04,  6.2525e-05],
        [ 1.7729e-03, -1.0986e-03, -9.3222e-04,  ..., -2.9984e-03,
          6.1512e-04,  1.7033e-03],
        ...,
        [ 2.3041e-03, -9.5606e-04,  1.4935e-03,  ..., -2.2221e-03,
          1.1158e-03,  1.1177e-03],
        [ 4.0007e-04,  3.4924e-03, -7.6151e-04,  ..., -1.4477e-03,
         -8.6117e-04, -1.0614e-03],
        [-1.6670e-03, -3.3302e-03, -2.6393e-04,  ...,  1.0848e-05,
         -9.1314e-04, -4.4632e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1211,  0.2236, -0.5034,  ...,  1.0273, -0.0432,  1.2734]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0388, -0.1061,  0.3931,  ...,  0.3101, -0.2302, -0.0999]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 04:55:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too subscribed, it is oversubscribed
If something is too stated, it is overstated
If something is too optimistic, it is overoptimistic
If something is too played, it is overplayed
If something is too laid, it is overlaid
If something is too crowded, it is overcrowded
If something is too protected, it is overprotected
If something is too populated, it is
2024-07-10 04:55:57 root INFO     [order_1_approx] starting weight calculation for If something is too populated, it is overpopulated
If something is too laid, it is overlaid
If something is too crowded, it is overcrowded
If something is too subscribed, it is oversubscribed
If something is too played, it is overplayed
If something is too stated, it is overstated
If something is too optimistic, it is overoptimistic
If something is too protected, it is
2024-07-10 04:55:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 05:00:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 05:04:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.6143, 0.2172, 0.5264,  ..., 1.3652, 1.2100, 0.4624], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([0.5889, 0.1946, 0.4722,  ..., 1.2695, 1.1348, 0.4299], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5063, -1.0254, -1.2695,  ...,  1.1436,  1.0518,  2.1582],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0016, -0.0136,  0.0029,  ..., -0.0006, -0.0253, -0.0105],
        [-0.0117,  0.0010,  0.0142,  ...,  0.0013,  0.0136, -0.0091],
        [ 0.0033,  0.0098,  0.0068,  ..., -0.0036,  0.0038,  0.0051],
        ...,
        [-0.0237, -0.0110, -0.0106,  ..., -0.0073,  0.0011, -0.0033],
        [ 0.0029, -0.0086,  0.0016,  ..., -0.0114, -0.0200,  0.0291],
        [-0.0050,  0.0114,  0.0120,  ..., -0.0075,  0.0024, -0.0135]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.1973e-03,  1.1559e-03, -5.0545e-04,  ...,  1.1330e-03,
         -6.7997e-04,  3.3092e-04],
        [-1.1034e-03, -3.0003e-03,  4.7150e-03,  ...,  1.1129e-03,
          1.7109e-03,  5.6934e-04],
        [-6.9714e-04,  1.1787e-03, -1.1196e-03,  ..., -8.1778e-04,
         -1.9336e-04, -2.1243e-04],
        ...,
        [ 5.5504e-04,  6.2561e-04,  1.3151e-03,  ..., -4.9496e-04,
          1.5163e-03,  1.2410e-04],
        [ 8.2207e-04,  4.3869e-04, -1.2283e-03,  ..., -6.7091e-04,
         -7.4100e-04, -5.1498e-05],
        [ 1.5831e-04,  1.6069e-04,  4.6706e-04,  ...,  3.0899e-04,
         -6.8331e-04, -1.0900e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2539, -0.7002, -1.0010,  ...,  0.8726,  1.2539,  2.8398]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1925, -0.1571,  0.1423,  ...,  0.1038,  0.1321, -0.0833]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 05:04:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too populated, it is overpopulated
If something is too laid, it is overlaid
If something is too crowded, it is overcrowded
If something is too subscribed, it is oversubscribed
If something is too played, it is overplayed
If something is too stated, it is overstated
If something is too optimistic, it is overoptimistic
If something is too protected, it is
2024-07-10 05:04:21 root INFO     [order_1_approx] starting weight calculation for If something is too protected, it is overprotected
If something is too crowded, it is overcrowded
If something is too subscribed, it is oversubscribed
If something is too populated, it is overpopulated
If something is too stated, it is overstated
If something is too optimistic, it is overoptimistic
If something is too laid, it is overlaid
If something is too played, it is
2024-07-10 05:04:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 05:08:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 05:12:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4219, -0.0382,  0.8496,  ...,  0.3472,  0.1780, -0.4365],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.4280, -0.0601,  0.8198,  ...,  0.3435,  0.1592, -0.4844],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4927,  0.1152, -0.6812,  ...,  0.9941,  0.1812,  4.3750],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0002,  0.0047,  0.0032,  ..., -0.0037, -0.0054, -0.0192],
        [-0.0336, -0.0012,  0.0077,  ...,  0.0054,  0.0073, -0.0112],
        [ 0.0044, -0.0026, -0.0056,  ..., -0.0078,  0.0069,  0.0061],
        ...,
        [-0.0100, -0.0248, -0.0056,  ..., -0.0120,  0.0065,  0.0062],
        [ 0.0011,  0.0077, -0.0208,  ..., -0.0111, -0.0221, -0.0016],
        [ 0.0007,  0.0086, -0.0002,  ...,  0.0026,  0.0127,  0.0028]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.9245e-03, -4.1504e-03,  2.2316e-03,  ...,  6.9284e-04,
          1.8253e-03, -6.1750e-04],
        [ 1.6327e-03, -4.9734e-04,  1.4496e-04,  ..., -1.1501e-03,
          2.6550e-03,  1.9798e-03],
        [-3.1352e-04, -5.2261e-04, -4.5896e-04,  ..., -1.3218e-03,
         -9.8896e-04, -7.3195e-05],
        ...,
        [-3.0780e-04, -8.8692e-04, -2.4700e-04,  ..., -1.1730e-03,
         -2.5094e-05,  6.3324e-04],
        [-1.4744e-03, -2.1458e-03,  1.7853e-03,  ...,  1.9197e-03,
         -8.9836e-04, -2.5120e-03],
        [-4.6682e-04,  1.4648e-03,  1.6487e-04,  ..., -3.1281e-04,
          2.8419e-04,  1.8396e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5547,  0.9233, -1.8066,  ...,  1.4980,  0.9414,  3.3359]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.2373, -0.0406,  0.3394,  ..., -0.1445, -0.2026,  0.2896]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 05:12:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too protected, it is overprotected
If something is too crowded, it is overcrowded
If something is too subscribed, it is oversubscribed
If something is too populated, it is overpopulated
If something is too stated, it is overstated
If something is too optimistic, it is overoptimistic
If something is too laid, it is overlaid
If something is too played, it is
2024-07-10 05:12:46 root INFO     [order_1_approx] starting weight calculation for If something is too played, it is overplayed
If something is too subscribed, it is oversubscribed
If something is too populated, it is overpopulated
If something is too crowded, it is overcrowded
If something is too laid, it is overlaid
If something is too protected, it is overprotected
If something is too stated, it is overstated
If something is too optimistic, it is
2024-07-10 05:12:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 05:17:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 05:21:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3938, -0.7134, -0.1326,  ..., -0.2085,  1.0488,  0.3447],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.3730, -0.6978, -0.1324,  ..., -0.1851,  0.9331,  0.2971],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.7441, -2.0488, -1.6621,  ...,  0.6240,  0.9082,  2.9688],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-8.0109e-05, -1.7700e-02, -2.2392e-03,  ..., -1.6594e-03,
         -1.4412e-02, -1.1925e-02],
        [-9.6893e-03,  1.5945e-03,  1.0147e-02,  ...,  1.2566e-02,
         -2.9922e-02, -8.3008e-03],
        [ 9.6741e-03,  9.8953e-03, -4.1809e-03,  ...,  5.0583e-03,
         -3.4943e-03,  2.0920e-02],
        ...,
        [ 7.7152e-04, -2.1881e-02,  2.6360e-03,  ..., -1.1368e-03,
          2.3518e-03,  4.8828e-04],
        [-1.0040e-02, -4.5395e-03, -4.0245e-03,  ..., -1.6907e-02,
         -1.2512e-02,  1.5167e-02],
        [ 5.9166e-03,  2.4872e-03,  1.5656e-02,  ...,  6.6147e-03,
          7.8201e-03, -2.1240e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.7939e-03,  1.5712e-04, -9.0694e-04,  ..., -7.0047e-04,
         -7.1907e-04,  1.2817e-03],
        [ 5.4407e-04, -9.3746e-04,  6.3229e-04,  ..., -8.2874e-04,
         -7.3195e-04,  3.3975e-04],
        [-7.7343e-04,  2.8915e-03, -7.9155e-05,  ..., -3.7885e-04,
         -1.8148e-03,  1.9398e-03],
        ...,
        [ 4.5228e-04, -1.2589e-03, -1.2130e-04,  ..., -1.7271e-03,
          1.9817e-03, -2.6822e-04],
        [-3.9721e-04, -4.1652e-04, -3.0327e-04,  ..., -1.4973e-03,
         -1.1473e-03,  8.9979e-04],
        [ 4.3106e-04, -2.1591e-03, -4.6253e-05,  ...,  3.1519e-04,
         -1.0347e-04, -1.1854e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1191, -3.1055, -1.6650,  ...,  0.5410,  0.7603,  2.8320]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0584,  0.0857,  0.1884,  ..., -0.1239,  0.1204,  0.0252]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 05:21:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too played, it is overplayed
If something is too subscribed, it is oversubscribed
If something is too populated, it is overpopulated
If something is too crowded, it is overcrowded
If something is too laid, it is overlaid
If something is too protected, it is overprotected
If something is too stated, it is overstated
If something is too optimistic, it is
2024-07-10 05:21:09 root INFO     total operator prediction time: 4039.707620859146 seconds
2024-07-10 05:21:09 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on adj+ly_reg
2024-07-10 05:21:12 root INFO     building operator adj+ly_reg
2024-07-10 05:21:12 root INFO     [order_1_approx] starting weight calculation for The adjective form of sexual is sexually
The adjective form of regional is regionally
The adjective form of additional is additionally
The adjective form of significant is significantly
The adjective form of increasing is increasingly
The adjective form of traditional is traditionally
The adjective form of beautiful is beautifully
The adjective form of apparent is
2024-07-10 05:21:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 05:25:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 05:29:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2026, -0.3848,  0.4004,  ...,  0.0481,  0.0693, -0.3545],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.2045, -0.4033,  0.3550,  ...,  0.0446,  0.0435, -0.3782],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.6621,  1.6436, -1.7490,  ...,  0.3516, -2.3496, -2.1465],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0300, -0.0100,  0.0062,  ..., -0.0176, -0.0252, -0.0049],
        [-0.0101, -0.0017,  0.0059,  ..., -0.0060, -0.0211,  0.0004],
        [ 0.0177, -0.0040, -0.0353,  ...,  0.0033,  0.0365, -0.0117],
        ...,
        [-0.0203,  0.0036,  0.0073,  ..., -0.0135, -0.0178, -0.0109],
        [ 0.0172,  0.0101,  0.0024,  ..., -0.0101,  0.0113, -0.0063],
        [ 0.0106,  0.0178,  0.0021,  ...,  0.0082,  0.0298, -0.0338]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.5898e-03, -5.9795e-04, -4.0770e-05,  ...,  6.4898e-04,
         -7.9060e-04, -1.2579e-03],
        [-1.0052e-03, -1.2741e-03,  9.0027e-04,  ..., -1.8396e-03,
          1.3256e-04,  7.1669e-04],
        [-1.5163e-03, -1.3494e-04, -3.0499e-03,  ..., -1.1194e-04,
          1.7462e-03, -1.3695e-03],
        ...,
        [ 4.7040e-04, -1.2312e-03,  1.5240e-03,  ..., -1.5700e-04,
         -1.1349e-03, -1.8797e-03],
        [-1.1101e-03,  1.4210e-03, -3.8261e-03,  ...,  1.4448e-03,
         -1.6403e-03,  5.1022e-04],
        [-3.3875e-03,  9.2030e-04, -5.9652e-04,  ...,  8.4305e-04,
          1.0004e-03,  1.2608e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.7012,  1.1230, -1.4512,  ..., -1.1680, -1.7754, -2.1445]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0897, -0.0805,  0.2666,  ..., -0.2378,  0.3970,  0.0653]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 05:29:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of sexual is sexually
The adjective form of regional is regionally
The adjective form of additional is additionally
The adjective form of significant is significantly
The adjective form of increasing is increasingly
The adjective form of traditional is traditionally
The adjective form of beautiful is beautifully
The adjective form of apparent is
2024-07-10 05:29:34 root INFO     [order_1_approx] starting weight calculation for The adjective form of beautiful is beautifully
The adjective form of additional is additionally
The adjective form of regional is regionally
The adjective form of significant is significantly
The adjective form of apparent is apparently
The adjective form of sexual is sexually
The adjective form of traditional is traditionally
The adjective form of increasing is
2024-07-10 05:29:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 05:33:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 05:37:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0065, -0.1025, -0.2605,  ..., -0.1976,  1.1504,  0.2563],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.0030, -0.1316, -0.2734,  ..., -0.1973,  1.1455,  0.2404],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7109, -2.6895, -0.8730,  ...,  3.1758,  2.2461, -3.0156],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.0498e-02,  2.5730e-03, -1.1616e-03,  ..., -1.5594e-02,
         -1.3847e-03,  5.6114e-03],
        [-9.2316e-04,  3.2120e-03,  1.0307e-02,  ..., -1.4048e-03,
          2.0924e-03,  9.1171e-04],
        [-8.6365e-03, -7.9803e-03, -8.6975e-04,  ..., -1.2100e-02,
          1.8372e-02, -1.0071e-02],
        ...,
        [-1.2512e-02,  2.3575e-03, -7.6256e-03,  ...,  6.4240e-03,
         -8.3694e-03,  2.3842e-03],
        [-2.4063e-02, -3.3035e-03, -1.8263e-03,  ...,  9.1553e-05,
         -2.8595e-02,  2.6016e-02],
        [ 2.9564e-03,  9.7275e-03,  1.2177e-02,  ...,  8.1940e-03,
          1.6602e-02, -4.1748e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.1839e-03, -1.7185e-03, -5.9700e-04,  ...,  8.2636e-04,
          5.6982e-04,  2.0635e-04],
        [-8.6451e-04, -3.2768e-03, -1.8132e-04,  ..., -7.0000e-04,
          2.4681e-03,  1.3933e-03],
        [-1.5669e-03,  1.1683e-03, -1.6079e-03,  ..., -6.6519e-04,
          3.3741e-03,  5.2404e-04],
        ...,
        [ 6.7711e-04,  1.4420e-03, -7.6723e-04,  ...,  6.3705e-04,
         -8.3399e-04, -1.2865e-03],
        [-4.2987e-04,  4.9400e-03, -1.9894e-03,  ...,  5.1403e-04,
         -3.9597e-03,  5.7173e-04],
        [-3.1137e-04, -5.4359e-04, -1.2436e-03,  ..., -1.8435e-03,
         -5.1975e-04,  9.1195e-06]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0059, -1.9717, -1.6074,  ...,  4.8164,  3.2344, -3.5312]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0437, -0.0246,  0.1967,  ..., -0.3406, -0.0986,  0.0654]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 05:37:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of beautiful is beautifully
The adjective form of additional is additionally
The adjective form of regional is regionally
The adjective form of significant is significantly
The adjective form of apparent is apparently
The adjective form of sexual is sexually
The adjective form of traditional is traditionally
The adjective form of increasing is
2024-07-10 05:37:56 root INFO     [order_1_approx] starting weight calculation for The adjective form of additional is additionally
The adjective form of sexual is sexually
The adjective form of traditional is traditionally
The adjective form of apparent is apparently
The adjective form of regional is regionally
The adjective form of significant is significantly
The adjective form of increasing is increasingly
The adjective form of beautiful is
2024-07-10 05:37:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 05:42:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 05:46:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9268, -0.5542,  0.4551,  ..., -0.6479,  0.2959, -0.2524],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.8403, -0.5396,  0.3818,  ..., -0.5669,  0.2441, -0.2627],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9824, -0.9385, -1.9160,  ...,  0.7188, -2.5371, -2.1348],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0103, -0.0049,  0.0024,  ...,  0.0002,  0.0105,  0.0039],
        [ 0.0091, -0.0009,  0.0010,  ..., -0.0056, -0.0104,  0.0166],
        [-0.0020, -0.0024, -0.0027,  ...,  0.0193,  0.0186,  0.0067],
        ...,
        [ 0.0026,  0.0035, -0.0004,  ..., -0.0146, -0.0182, -0.0083],
        [ 0.0055, -0.0006, -0.0057,  ..., -0.0076, -0.0072, -0.0098],
        [-0.0033,  0.0101, -0.0061,  ...,  0.0138,  0.0271, -0.0103]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.1730e-03,  2.1696e-05, -1.0872e-03,  ..., -3.5107e-05,
          4.9353e-05, -7.3552e-05],
        [ 1.0881e-03,  1.0471e-03,  1.8740e-03,  ..., -1.1215e-03,
         -2.3794e-04,  3.9978e-03],
        [-9.2316e-04,  6.8951e-04, -2.9278e-03,  ...,  1.9622e-04,
          2.8114e-03, -1.4281e-04],
        ...,
        [-2.9087e-05, -1.1263e-03, -7.9107e-04,  ...,  6.0177e-04,
          1.3504e-03, -2.3441e-03],
        [-1.2922e-04, -2.0847e-03, -1.9817e-03,  ..., -1.4582e-03,
         -2.7523e-03,  1.3580e-03],
        [-6.6757e-06, -5.4646e-04, -2.2106e-03,  ..., -2.1315e-04,
         -8.5974e-04,  2.8076e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3345, -1.6816, -1.9170,  ...,  0.2671, -2.3750, -1.8164]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1135, -0.1675,  0.3987,  ..., -0.0706, -0.0980,  0.0805]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 05:46:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of additional is additionally
The adjective form of sexual is sexually
The adjective form of traditional is traditionally
The adjective form of apparent is apparently
The adjective form of regional is regionally
The adjective form of significant is significantly
The adjective form of increasing is increasingly
The adjective form of beautiful is
2024-07-10 05:46:20 root INFO     [order_1_approx] starting weight calculation for The adjective form of increasing is increasingly
The adjective form of additional is additionally
The adjective form of traditional is traditionally
The adjective form of significant is significantly
The adjective form of sexual is sexually
The adjective form of apparent is apparently
The adjective form of beautiful is beautifully
The adjective form of regional is
2024-07-10 05:46:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 05:50:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 05:54:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1185,  0.0688, -0.3162,  ..., -0.2847,  0.6309, -0.3342],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1158,  0.0449, -0.2913,  ..., -0.2496,  0.5503, -0.3401],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2006,  2.0039, -4.8359,  ...,  2.1094,  3.0508,  2.8613],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0163, -0.0116,  0.0005,  ..., -0.0108, -0.0011, -0.0115],
        [-0.0064,  0.0064, -0.0025,  ..., -0.0048, -0.0149, -0.0053],
        [ 0.0058, -0.0047, -0.0031,  ...,  0.0097,  0.0228,  0.0096],
        ...,
        [-0.0061,  0.0050, -0.0069,  ..., -0.0057, -0.0101,  0.0011],
        [-0.0020,  0.0088, -0.0035,  ..., -0.0145, -0.0054, -0.0006],
        [ 0.0002,  0.0015,  0.0021,  ...,  0.0034,  0.0181, -0.0022]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0012, -0.0027,  0.0020,  ...,  0.0018,  0.0014,  0.0011],
        [-0.0011, -0.0011,  0.0001,  ..., -0.0001,  0.0009, -0.0006],
        [ 0.0009,  0.0010, -0.0025,  ..., -0.0012,  0.0026, -0.0019],
        ...,
        [ 0.0007,  0.0011, -0.0019,  ..., -0.0001, -0.0010,  0.0007],
        [-0.0002,  0.0010, -0.0012,  ...,  0.0011,  0.0006, -0.0004],
        [-0.0014, -0.0017, -0.0009,  ...,  0.0006,  0.0003,  0.0001]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3489,  1.1055, -2.8164,  ...,  1.4707,  2.9941,  4.1523]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0432, -0.3257,  0.1971,  ..., -0.1857, -0.0888,  0.1525]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 05:54:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of increasing is increasingly
The adjective form of additional is additionally
The adjective form of traditional is traditionally
The adjective form of significant is significantly
The adjective form of sexual is sexually
The adjective form of apparent is apparently
The adjective form of beautiful is beautifully
The adjective form of regional is
2024-07-10 05:54:45 root INFO     [order_1_approx] starting weight calculation for The adjective form of regional is regionally
The adjective form of increasing is increasingly
The adjective form of additional is additionally
The adjective form of significant is significantly
The adjective form of sexual is sexually
The adjective form of beautiful is beautifully
The adjective form of apparent is apparently
The adjective form of traditional is
2024-07-10 05:54:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 05:59:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 06:03:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8823,  0.4436,  0.6367,  ..., -0.2754,  0.9141, -0.7720],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.8818,  0.4355,  0.5962,  ..., -0.2681,  0.8848, -0.8247],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6470,  2.3633,  0.8672,  ...,  2.2539,  2.3164,  5.1367],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-5.7602e-03, -7.6981e-03,  9.4681e-03,  ..., -1.4664e-02,
          8.4915e-03, -1.0010e-02],
        [-8.3542e-04,  4.6120e-03,  6.5613e-04,  ...,  3.1490e-03,
         -1.2619e-02, -8.5526e-03],
        [-7.2899e-03,  8.0185e-03,  2.1286e-03,  ..., -7.6828e-03,
         -5.1384e-03,  2.1362e-02],
        ...,
        [-2.6062e-02,  3.4332e-05,  1.1139e-03,  ..., -2.5238e-02,
         -7.8278e-03,  7.1945e-03],
        [ 2.3499e-03, -8.6746e-03,  1.6766e-03,  ...,  1.1467e-02,
         -3.5286e-03,  8.2932e-03],
        [-1.5152e-02, -3.5896e-03, -1.0490e-04,  ..., -8.8654e-03,
          5.1689e-03,  2.0809e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.4763e-03, -7.3910e-04,  3.6240e-05,  ..., -2.0027e-05,
          7.8011e-04, -3.1185e-04],
        [-9.6750e-04,  7.4816e-04,  6.1321e-04,  ...,  4.0293e-05,
          2.0103e-03, -1.4315e-03],
        [-7.1049e-04,  1.1950e-03, -7.9107e-04,  ..., -7.9632e-04,
          1.7090e-03, -1.5860e-03],
        ...,
        [ 2.7275e-03,  9.1839e-04,  2.8706e-04,  ..., -1.3351e-03,
         -2.5558e-03, -5.2166e-04],
        [-7.0620e-04, -3.1614e-04, -6.4564e-04,  ..., -9.4235e-05,
         -1.1768e-03,  2.4681e-03],
        [-1.9684e-03, -1.9121e-03,  9.6798e-04,  ..., -7.3910e-04,
         -7.2622e-04,  1.1339e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2676,  2.1719,  1.6426,  ...,  1.7480,  2.1992,  4.5742]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1498, -0.0104,  0.1334,  ..., -0.3826,  0.1769, -0.0709]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 06:03:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of regional is regionally
The adjective form of increasing is increasingly
The adjective form of additional is additionally
The adjective form of significant is significantly
The adjective form of sexual is sexually
The adjective form of beautiful is beautifully
The adjective form of apparent is apparently
The adjective form of traditional is
2024-07-10 06:03:10 root INFO     [order_1_approx] starting weight calculation for The adjective form of significant is significantly
The adjective form of regional is regionally
The adjective form of sexual is sexually
The adjective form of traditional is traditionally
The adjective form of increasing is increasingly
The adjective form of apparent is apparently
The adjective form of beautiful is beautifully
The adjective form of additional is
2024-07-10 06:03:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 06:07:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 06:11:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6035, -0.0083,  0.4939,  ..., -0.3450,  0.2031,  0.2234],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.6304, -0.0314,  0.4680,  ..., -0.3411,  0.1819,  0.2051],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0957,  0.1550,  1.7129,  ..., -1.9336,  2.7305,  2.0605],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0243, -0.0134,  0.0057,  ...,  0.0034, -0.0085, -0.0192],
        [-0.0350, -0.0051,  0.0090,  ...,  0.0143, -0.0277, -0.0145],
        [ 0.0186,  0.0017, -0.0080,  ..., -0.0059,  0.0130,  0.0285],
        ...,
        [ 0.0064, -0.0014, -0.0122,  ..., -0.0144,  0.0035,  0.0113],
        [-0.0191,  0.0017,  0.0097,  ..., -0.0035, -0.0294,  0.0037],
        [ 0.0215, -0.0063, -0.0039,  ..., -0.0058,  0.0367,  0.0031]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.9646e-03, -1.3199e-03, -1.5879e-04,  ...,  1.0462e-03,
          1.4896e-03, -1.2665e-03],
        [-3.1471e-03, -1.8969e-03,  1.2140e-03,  ...,  1.8091e-03,
          1.6403e-03, -6.3419e-05],
        [-2.2554e-04,  4.7636e-04, -6.8092e-04,  ..., -1.0366e-03,
          2.7323e-04,  6.2084e-04],
        ...,
        [ 3.4008e-03, -5.4836e-04, -9.3746e-04,  ..., -1.5426e-04,
         -3.3379e-04, -1.9245e-03],
        [-2.3556e-03,  1.6747e-03,  5.1355e-04,  ...,  1.1539e-03,
         -4.6730e-04, -1.7691e-04],
        [-6.6042e-04, -3.8090e-03,  2.2936e-04,  ...,  3.6049e-04,
         -3.6025e-04,  9.6226e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3789, -0.3438,  3.2012,  ..., -2.0410,  3.0684,  1.7949]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1559, -0.1281,  0.0712,  ..., -0.1039, -0.0262, -0.0583]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 06:11:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of significant is significantly
The adjective form of regional is regionally
The adjective form of sexual is sexually
The adjective form of traditional is traditionally
The adjective form of increasing is increasingly
The adjective form of apparent is apparently
The adjective form of beautiful is beautifully
The adjective form of additional is
2024-07-10 06:11:35 root INFO     [order_1_approx] starting weight calculation for The adjective form of increasing is increasingly
The adjective form of regional is regionally
The adjective form of beautiful is beautifully
The adjective form of sexual is sexually
The adjective form of additional is additionally
The adjective form of traditional is traditionally
The adjective form of apparent is apparently
The adjective form of significant is
2024-07-10 06:11:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 06:15:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 06:19:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.7246, 0.1780, 0.1134,  ..., 0.3169, 0.5913, 0.1672], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([0.7104, 0.1584, 0.0911,  ..., 0.3000, 0.5542, 0.1403], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.5957,  3.4199,  4.7031,  ..., -0.0298, -2.5664, -2.7148],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0389, -0.0129,  0.0150,  ..., -0.0023, -0.0230, -0.0033],
        [-0.0175,  0.0005, -0.0034,  ..., -0.0138, -0.0361,  0.0242],
        [ 0.0016, -0.0015,  0.0103,  ..., -0.0021,  0.0077,  0.0149],
        ...,
        [ 0.0115, -0.0221, -0.0004,  ..., -0.0043, -0.0084, -0.0051],
        [ 0.0057,  0.0063,  0.0071,  ...,  0.0219,  0.0025,  0.0016],
        [ 0.0353,  0.0310, -0.0199,  ...,  0.0179,  0.0655, -0.0047]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.8144e-04, -9.9850e-04, -1.7071e-03,  ...,  3.1710e-04,
         -1.1292e-03,  3.1519e-04],
        [-1.8597e-04, -5.9605e-05,  1.7595e-03,  ...,  6.0415e-04,
          3.9215e-03,  1.9875e-03],
        [-1.9531e-03,  1.9131e-03, -1.7762e-04,  ..., -9.9182e-04,
          2.8343e-03, -1.7061e-03],
        ...,
        [ 1.7986e-03, -7.8344e-04, -2.7237e-03,  ...,  6.8378e-04,
         -1.8301e-03, -2.5702e-04],
        [-1.0786e-03, -1.8797e-03, -7.1049e-04,  ..., -9.0742e-04,
         -2.5406e-03,  6.4564e-04],
        [ 1.7881e-04, -1.1778e-03, -1.1845e-03,  ...,  1.8969e-03,
          6.0606e-04, -5.5075e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.4512,  2.8984,  4.0938,  ...,  0.2849, -0.2500, -0.1953]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0127, -0.1907, -0.1216,  ...,  0.1938,  0.0825, -0.0979]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 06:19:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of increasing is increasingly
The adjective form of regional is regionally
The adjective form of beautiful is beautifully
The adjective form of sexual is sexually
The adjective form of additional is additionally
The adjective form of traditional is traditionally
The adjective form of apparent is apparently
The adjective form of significant is
2024-07-10 06:20:00 root INFO     [order_1_approx] starting weight calculation for The adjective form of beautiful is beautifully
The adjective form of traditional is traditionally
The adjective form of additional is additionally
The adjective form of apparent is apparently
The adjective form of significant is significantly
The adjective form of increasing is increasingly
The adjective form of regional is regionally
The adjective form of sexual is
2024-07-10 06:20:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 06:24:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 06:28:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.2324, 0.4727, 0.3213,  ..., 0.6050, 1.4219, 0.1781], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([0.2098, 0.4304, 0.2712,  ..., 0.5405, 1.2812, 0.1427], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0312,  3.9219, -0.7334,  ..., -1.4844,  0.7646,  3.2617],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0058, -0.0043, -0.0072,  ..., -0.0105,  0.0085,  0.0130],
        [-0.0135, -0.0104,  0.0140,  ..., -0.0052, -0.0137,  0.0158],
        [-0.0146,  0.0061, -0.0136,  ...,  0.0017,  0.0241, -0.0123],
        ...,
        [-0.0025, -0.0033,  0.0045,  ...,  0.0108,  0.0098, -0.0038],
        [ 0.0055, -0.0185, -0.0027,  ..., -0.0109, -0.0135,  0.0078],
        [-0.0076, -0.0059, -0.0046,  ...,  0.0107,  0.0035,  0.0125]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.5144e-03, -1.3943e-03, -1.2531e-03,  ...,  1.3888e-04,
          1.7986e-03,  1.4048e-03],
        [-2.9030e-03, -9.9754e-04, -2.1589e-04,  ...,  8.5449e-04,
          3.1528e-03,  5.8651e-04],
        [-1.5440e-03,  3.7694e-04, -1.8940e-03,  ..., -1.5163e-03,
          2.8210e-03, -1.1501e-03],
        ...,
        [ 1.1234e-03, -2.2113e-04, -2.1863e-04,  ...,  1.4229e-03,
         -4.9305e-04, -5.6744e-04],
        [-2.1005e-04, -1.3590e-03, -2.4872e-03,  ..., -2.0733e-03,
         -1.6928e-03,  4.2582e-04],
        [ 2.9039e-04, -8.3733e-04, -5.8889e-05,  ..., -9.0599e-04,
         -2.5406e-03,  8.4066e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2852,  2.9785, -0.1299,  ..., -0.7427,  0.5327,  3.3301]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2384, -0.2671,  0.1543,  ..., -0.1273,  0.0801,  0.0137]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 06:28:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of beautiful is beautifully
The adjective form of traditional is traditionally
The adjective form of additional is additionally
The adjective form of apparent is apparently
The adjective form of significant is significantly
The adjective form of increasing is increasingly
The adjective form of regional is regionally
The adjective form of sexual is
2024-07-10 06:28:22 root INFO     total operator prediction time: 4030.140667438507 seconds
2024-07-10 06:28:22 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+tion_irreg
2024-07-10 06:28:24 root INFO     building operator verb+tion_irreg
2024-07-10 06:28:24 root INFO     [order_1_approx] starting weight calculation for To compute results in computation
To restore results in restoration
To authorize results in authorization
To degrade results in degradation
To realize results in realization
To globalize results in globalization
To perspire results in perspiration
To observe results in
2024-07-10 06:28:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 06:32:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 06:36:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6675, -0.4883,  0.3044,  ..., -0.7236, -0.1362,  0.3447],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.6919, -0.5327,  0.2834,  ..., -0.7095, -0.1591,  0.3337],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0318, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.7266,  1.0410, -4.4062,  ..., -0.3179, -2.3496,  3.7500],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0097, -0.0035, -0.0164,  ...,  0.0108,  0.0116, -0.0150],
        [-0.0059, -0.0087, -0.0020,  ...,  0.0123,  0.0034,  0.0004],
        [ 0.0154,  0.0023, -0.0139,  ...,  0.0030,  0.0150,  0.0012],
        ...,
        [ 0.0076, -0.0131, -0.0236,  ..., -0.0113,  0.0033, -0.0105],
        [ 0.0142,  0.0131, -0.0199,  ..., -0.0095,  0.0044,  0.0098],
        [-0.0082,  0.0045,  0.0111,  ...,  0.0017,  0.0014, -0.0015]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 1.9741e-04, -2.3901e-04,  7.2098e-04,  ...,  6.5994e-04,
         -3.0613e-04,  6.6280e-04],
        [-6.2561e-04, -1.1444e-03, -1.0836e-04,  ...,  5.3501e-04,
          9.9182e-04, -1.1320e-03],
        [ 9.0694e-04, -7.3671e-05, -5.7697e-04,  ..., -7.7152e-04,
          7.0238e-04, -3.5429e-04],
        ...,
        [ 1.8520e-03,  1.8501e-03, -7.1239e-04,  ...,  2.8896e-04,
          1.7509e-03, -1.0910e-03],
        [ 7.6008e-04,  3.4199e-03, -9.2316e-04,  ..., -5.6934e-04,
         -6.8092e-04, -1.2789e-03],
        [ 4.9973e-04,  1.0729e-03, -8.3399e-04,  ...,  1.3113e-04,
         -4.1509e-04,  5.4240e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.2500,  0.6240, -2.6152,  ...,  0.5142, -1.9297,  3.4414]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2195, -0.0983,  0.1004,  ..., -0.1443, -0.3110, -0.0591]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 06:36:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To compute results in computation
To restore results in restoration
To authorize results in authorization
To degrade results in degradation
To realize results in realization
To globalize results in globalization
To perspire results in perspiration
To observe results in
2024-07-10 06:36:50 root INFO     [order_1_approx] starting weight calculation for To perspire results in perspiration
To realize results in realization
To observe results in observation
To globalize results in globalization
To restore results in restoration
To authorize results in authorization
To degrade results in degradation
To compute results in
2024-07-10 06:36:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 06:41:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 06:45:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6626, -0.0502, -0.2942,  ..., -0.3218,  0.0745,  0.4890],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.6719, -0.0740, -0.3027,  ..., -0.3162,  0.0526,  0.4836],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0318, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8428,  0.9824, -3.0273,  ..., -1.9248, -0.4185,  3.5273],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.2146e-02, -9.7961e-03, -8.7280e-03,  ..., -6.4812e-03,
         -1.2131e-02, -5.6190e-03],
        [ 2.1591e-03,  1.6296e-02,  1.8997e-03,  ...,  1.7441e-02,
         -4.6463e-03,  6.9351e-03],
        [ 1.3657e-02, -3.0518e-05, -4.4250e-03,  ...,  1.2131e-03,
          1.6117e-03,  3.4332e-05],
        ...,
        [ 7.9880e-03, -5.6953e-03, -8.7662e-03,  ...,  4.9973e-04,
          1.2207e-04,  1.1078e-02],
        [ 1.5076e-02,  1.5869e-03,  4.0207e-03,  ...,  3.2616e-03,
         -7.8430e-03,  1.0201e-02],
        [ 6.5536e-03, -2.7199e-03,  2.1393e-02,  ..., -8.9722e-03,
          5.8022e-03, -8.1482e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.3885e-03, -8.4305e-04,  3.2520e-04,  ...,  1.7238e-04,
         -1.4553e-03, -1.5602e-03],
        [-4.5681e-04, -1.9035e-03, -3.9315e-04,  ...,  1.0681e-03,
          9.9850e-04, -2.7122e-03],
        [-1.2093e-03,  7.6294e-05, -1.3409e-03,  ..., -7.3612e-05,
          8.3208e-04,  3.0041e-04],
        ...,
        [ 1.1654e-03,  1.3723e-03,  1.9093e-03,  ..., -4.1652e-04,
          1.2903e-03, -1.7500e-04],
        [-1.0347e-03,  1.8740e-03,  3.9577e-05,  ..., -6.8951e-04,
         -1.5383e-03, -2.4185e-03],
        [-3.5820e-03, -1.4286e-03,  2.1191e-03,  ...,  2.8896e-04,
          3.2568e-04, -1.3742e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3740,  1.3545, -3.1426,  ..., -1.2031,  0.7563,  3.9414]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.3010,  0.1863, -0.1011,  ..., -0.1909, -0.0552, -0.0325]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 06:45:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To perspire results in perspiration
To realize results in realization
To observe results in observation
To globalize results in globalization
To restore results in restoration
To authorize results in authorization
To degrade results in degradation
To compute results in
2024-07-10 06:45:17 root INFO     [order_1_approx] starting weight calculation for To compute results in computation
To degrade results in degradation
To restore results in restoration
To observe results in observation
To authorize results in authorization
To globalize results in globalization
To realize results in realization
To perspire results in
2024-07-10 06:45:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 06:49:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 06:53:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2239, -0.4243,  1.0410,  ...,  0.7480,  0.2920,  0.7808],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.2184, -0.4314,  0.9282,  ...,  0.6812,  0.2520,  0.7290],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0318, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6401, -0.1475, -0.9600,  ...,  0.6719, -4.9922, -1.2285],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.2054e-02,  1.1818e-02, -1.3771e-03,  ..., -1.5278e-03,
          2.6665e-03, -1.5335e-03],
        [-2.1000e-03,  5.2795e-03,  2.4891e-03,  ...,  1.4938e-02,
         -4.2343e-03,  1.6136e-03],
        [ 7.7324e-03,  1.4099e-02,  1.9703e-03,  ...,  7.1182e-03,
          5.7030e-03, -1.0033e-03],
        ...,
        [-7.7438e-03, -7.2670e-03, -6.5994e-03,  ..., -3.2692e-03,
          4.6463e-03, -5.6229e-03],
        [ 3.4882e-02,  1.8326e-02,  3.5477e-04,  ..., -8.3923e-05,
         -9.7656e-04,  3.6564e-03],
        [ 2.1576e-02,  1.1475e-02,  1.1101e-02,  ..., -8.1863e-03,
         -9.0408e-04, -1.2718e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 8.7798e-05,  1.0186e-04, -3.9530e-04,  ..., -3.0112e-04,
         -4.5967e-04,  1.3924e-03],
        [ 1.3561e-03, -1.0796e-03, -1.3752e-03,  ...,  1.5297e-03,
         -6.3181e-05,  1.1892e-03],
        [-1.2798e-03,  1.1539e-03,  8.1921e-04,  ...,  9.8038e-04,
          6.5184e-04, -7.6246e-04],
        ...,
        [ 1.2426e-03, -6.3229e-04, -4.9305e-04,  ...,  9.0027e-04,
          8.7738e-04,  6.9761e-04],
        [ 1.2131e-03,  3.8147e-06,  1.5602e-03,  ...,  1.2484e-03,
         -1.1082e-03, -1.0109e-04],
        [-1.9474e-03,  1.1711e-03,  1.4763e-03,  ..., -7.3016e-05,
         -1.4715e-03,  3.0422e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5303,  0.1992, -1.5791,  ..., -0.1846, -4.6445, -0.6504]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1929,  0.0541, -0.0936,  ..., -0.0737,  0.0064, -0.2019]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 06:53:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To compute results in computation
To degrade results in degradation
To restore results in restoration
To observe results in observation
To authorize results in authorization
To globalize results in globalization
To realize results in realization
To perspire results in
2024-07-10 06:53:43 root INFO     [order_1_approx] starting weight calculation for To authorize results in authorization
To compute results in computation
To degrade results in degradation
To observe results in observation
To realize results in realization
To perspire results in perspiration
To restore results in restoration
To globalize results in
2024-07-10 06:53:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 06:58:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 07:02:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2332,  0.0425, -0.0809,  ..., -0.2468,  0.4192,  0.8086],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.2313,  0.0253, -0.0865,  ..., -0.2264,  0.3838,  0.7798],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0318, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6602,  1.2314, -3.7500,  ...,  1.4375, -0.5928,  1.5410],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0013, -0.0075,  0.0044,  ..., -0.0081, -0.0019, -0.0032],
        [ 0.0021, -0.0140,  0.0010,  ...,  0.0153, -0.0099, -0.0016],
        [ 0.0275, -0.0005, -0.0086,  ..., -0.0162, -0.0023, -0.0098],
        ...,
        [-0.0247,  0.0012, -0.0110,  ...,  0.0014,  0.0110,  0.0003],
        [-0.0045,  0.0026,  0.0025,  ..., -0.0015, -0.0035, -0.0121],
        [-0.0096,  0.0063,  0.0011,  ...,  0.0052,  0.0051, -0.0082]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-3.2854e-04, -4.0507e-04,  3.7074e-05,  ..., -2.8324e-04,
         -1.1673e-03,  1.9264e-04],
        [ 4.2772e-04,  2.0206e-04, -5.6124e-04,  ...,  1.2779e-03,
         -5.5408e-04, -1.8716e-04],
        [ 4.7922e-04,  9.2888e-04, -3.1734e-04,  ..., -2.5797e-04,
          6.3515e-04, -4.8399e-04],
        ...,
        [-2.8634e-04,  2.0862e-04,  6.0749e-04,  ...,  2.1207e-04,
          1.0605e-03, -6.5422e-04],
        [ 1.6518e-03,  6.2180e-04,  6.2656e-04,  ..., -1.6856e-04,
          8.1444e-04, -1.2815e-05],
        [-7.6294e-05, -4.2295e-04,  7.2193e-04,  ...,  1.4186e-04,
         -2.6488e-04, -4.2748e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.9873,  1.0049, -3.7539,  ...,  0.6963, -0.5884,  0.7886]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1392,  0.0151, -0.0527,  ...,  0.0352, -0.0050, -0.0028]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 07:02:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To authorize results in authorization
To compute results in computation
To degrade results in degradation
To observe results in observation
To realize results in realization
To perspire results in perspiration
To restore results in restoration
To globalize results in
2024-07-10 07:02:07 root INFO     [order_1_approx] starting weight calculation for To degrade results in degradation
To observe results in observation
To globalize results in globalization
To realize results in realization
To compute results in computation
To perspire results in perspiration
To restore results in restoration
To authorize results in
2024-07-10 07:02:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 07:06:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 07:10:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5566, -0.6885,  0.3638,  ...,  0.4077, -0.1628,  0.5293],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.5488, -0.7314,  0.3308,  ...,  0.3896, -0.1860,  0.5117],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0318, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4873,  0.9604, -1.6582,  ...,  0.2891,  2.3555,  3.4199],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0128,  0.0049,  0.0105,  ..., -0.0006, -0.0099,  0.0020],
        [ 0.0074, -0.0133,  0.0058,  ...,  0.0175, -0.0073, -0.0067],
        [ 0.0038,  0.0074, -0.0106,  ..., -0.0078, -0.0105,  0.0059],
        ...,
        [-0.0126, -0.0089,  0.0005,  ..., -0.0183, -0.0114,  0.0040],
        [ 0.0050,  0.0055,  0.0072,  ..., -0.0037, -0.0096, -0.0097],
        [ 0.0002,  0.0095,  0.0168,  ..., -0.0085,  0.0108, -0.0056]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.0161e-03,  1.1053e-03, -7.9250e-04,  ...,  3.0303e-04,
         -1.1301e-03, -1.7166e-03],
        [ 8.0681e-04, -2.0278e-04,  1.6594e-04,  ...,  2.2459e-04,
          3.8910e-04, -1.0598e-04],
        [-1.2321e-03,  4.1819e-04, -6.4468e-04,  ..., -2.9612e-04,
          9.4080e-04, -4.9257e-04],
        ...,
        [-8.0681e-04,  1.2178e-03,  8.7643e-04,  ..., -6.1321e-04,
         -5.8985e-04,  8.4782e-04],
        [ 3.6407e-04,  1.7080e-03,  8.2850e-06,  ..., -2.4700e-04,
         -1.8053e-03, -2.0480e-04],
        [-1.8997e-03,  2.0370e-03,  2.0218e-04,  ...,  9.7132e-04,
          3.4809e-05, -7.4816e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1201,  0.3481, -1.2031,  ...,  0.9766,  1.9854,  3.5156]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.4482,  0.0664,  0.2473,  ...,  0.0574, -0.0050, -0.0914]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 07:10:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To degrade results in degradation
To observe results in observation
To globalize results in globalization
To realize results in realization
To compute results in computation
To perspire results in perspiration
To restore results in restoration
To authorize results in
2024-07-10 07:10:31 root INFO     [order_1_approx] starting weight calculation for To compute results in computation
To observe results in observation
To authorize results in authorization
To globalize results in globalization
To realize results in realization
To perspire results in perspiration
To restore results in restoration
To degrade results in
2024-07-10 07:10:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 07:14:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 07:18:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0166, -0.9668,  0.6245,  ...,  0.0193, -0.4358, -0.2046],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.0074, -1.0029,  0.5732,  ...,  0.0173, -0.4487, -0.2355],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0318, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5117,  1.7705, -0.7500,  ..., -2.0078, -2.9941,  3.4902],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0169, -0.0153, -0.0066,  ...,  0.0028, -0.0013, -0.0143],
        [ 0.0024, -0.0065, -0.0112,  ...,  0.0142, -0.0085,  0.0016],
        [-0.0002,  0.0002, -0.0161,  ..., -0.0039,  0.0057,  0.0255],
        ...,
        [-0.0112, -0.0189, -0.0006,  ..., -0.0161, -0.0148, -0.0045],
        [ 0.0135,  0.0250, -0.0011,  ..., -0.0009, -0.0155,  0.0061],
        [-0.0036,  0.0305, -0.0089,  ...,  0.0024,  0.0167, -0.0211]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-9.0790e-04, -9.1887e-04, -2.6131e-04,  ..., -1.1501e-03,
         -8.9169e-04, -4.8399e-04],
        [ 2.9278e-04, -1.9522e-03, -2.8610e-05,  ...,  1.9050e-04,
          1.7834e-03,  3.9291e-04],
        [-3.5629e-03,  1.9493e-03, -6.5470e-04,  ..., -4.9877e-04,
          1.1377e-03,  8.6403e-04],
        ...,
        [-1.9717e-04, -8.2254e-04,  1.3227e-03,  ..., -1.2217e-03,
          1.9932e-03, -7.9632e-05],
        [ 4.7302e-04,  3.3035e-03, -1.4782e-05,  ..., -4.8637e-05,
          6.2752e-04,  2.5845e-04],
        [ 2.0809e-03,  5.6171e-04, -2.4533e-04,  ...,  9.4032e-04,
         -3.5429e-04, -8.4019e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.3379,  2.6484, -1.3984,  ..., -2.2344, -3.6543,  2.8828]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2197,  0.1476,  0.3853,  ...,  0.0277, -0.4045,  0.0844]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 07:18:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To compute results in computation
To observe results in observation
To authorize results in authorization
To globalize results in globalization
To realize results in realization
To perspire results in perspiration
To restore results in restoration
To degrade results in
2024-07-10 07:18:59 root INFO     [order_1_approx] starting weight calculation for To perspire results in perspiration
To authorize results in authorization
To compute results in computation
To globalize results in globalization
To observe results in observation
To degrade results in degradation
To realize results in realization
To restore results in
2024-07-10 07:18:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 07:23:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 07:27:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6187, -0.1917,  0.9858,  ..., -0.3418, -1.1943,  0.0918],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.6055, -0.2112,  0.8901,  ..., -0.3162, -1.1572,  0.0617],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0318, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2725,  0.1052, -3.4980,  ..., -0.9609,  2.7559,  8.5859],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0064, -0.0049,  0.0009,  ...,  0.0041, -0.0058, -0.0038],
        [ 0.0073,  0.0031, -0.0081,  ...,  0.0002, -0.0051,  0.0032],
        [ 0.0134, -0.0047, -0.0052,  ...,  0.0048,  0.0126,  0.0055],
        ...,
        [-0.0085, -0.0111, -0.0031,  ...,  0.0082, -0.0006, -0.0097],
        [ 0.0064,  0.0079,  0.0007,  ..., -0.0085, -0.0002,  0.0017],
        [-0.0138,  0.0060, -0.0046,  ...,  0.0015,  0.0111, -0.0011]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 4.5156e-04, -1.2522e-03,  1.2589e-03,  ...,  1.6766e-03,
          4.6682e-04, -9.6703e-04],
        [ 1.8263e-03,  6.0976e-05,  1.0624e-03,  ...,  5.8174e-04,
          1.0862e-03, -5.4407e-04],
        [ 6.8569e-04, -7.9441e-04, -9.0504e-04,  ...,  7.8583e-04,
          1.6623e-03, -1.2026e-03],
        ...,
        [ 1.7357e-03,  1.1768e-03,  1.4057e-03,  ...,  5.5611e-05,
          2.3842e-03, -5.8079e-04],
        [ 1.0490e-04,  2.5120e-03,  8.8882e-04,  ..., -3.7932e-04,
         -2.9221e-03, -7.9679e-04],
        [-5.7030e-04,  1.3828e-03, -9.7752e-04,  ...,  3.0661e-04,
         -2.1672e-04,  3.5596e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6318,  0.4990, -2.4121,  ..., -1.5225,  2.5195,  8.0625]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.4612, -0.1109,  0.4253,  ...,  0.1841, -0.2881,  0.1170]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 07:27:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To perspire results in perspiration
To authorize results in authorization
To compute results in computation
To globalize results in globalization
To observe results in observation
To degrade results in degradation
To realize results in realization
To restore results in
2024-07-10 07:27:28 root INFO     [order_1_approx] starting weight calculation for To restore results in restoration
To observe results in observation
To degrade results in degradation
To compute results in computation
To authorize results in authorization
To globalize results in globalization
To perspire results in perspiration
To realize results in
2024-07-10 07:27:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 07:31:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 07:35:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3687, -0.4583,  1.4727,  ..., -1.0449,  0.0107, -0.5698],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.4053, -0.5269,  1.4990,  ..., -1.0791, -0.0108, -0.6538],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0318, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1758,  5.1094, -5.3164,  ..., -4.5469, -0.5703,  3.8535],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0184,  0.0047, -0.0033,  ...,  0.0240, -0.0019, -0.0129],
        [ 0.0004,  0.0020, -0.0029,  ...,  0.0098,  0.0072, -0.0025],
        [ 0.0113,  0.0122,  0.0099,  ...,  0.0022,  0.0163, -0.0056],
        ...,
        [-0.0047,  0.0015, -0.0005,  ..., -0.0138, -0.0057, -0.0029],
        [ 0.0080,  0.0065,  0.0084,  ..., -0.0229, -0.0017,  0.0042],
        [-0.0127,  0.0017, -0.0049,  ...,  0.0019,  0.0176, -0.0190]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-9.3269e-04, -1.0395e-03,  5.1689e-04,  ...,  2.4738e-03,
         -5.5885e-04, -1.3123e-03],
        [ 2.4080e-05, -2.2125e-03,  6.4850e-05,  ...,  1.5717e-03,
          1.6308e-03, -8.7070e-04],
        [-1.3781e-03,  2.5501e-03, -7.2050e-04,  ...,  2.0409e-04,
         -2.3866e-04,  3.2425e-04],
        ...,
        [-2.5463e-03,  1.6508e-03,  1.5688e-03,  ..., -1.4420e-03,
          7.8011e-04,  2.6569e-03],
        [ 2.7943e-04,  4.6387e-03, -1.4238e-03,  ..., -1.3094e-03,
         -6.1941e-04, -1.6823e-03],
        [-8.0013e-04,  2.6512e-04,  5.2547e-04,  ...,  8.7452e-04,
         -1.3161e-03, -2.9144e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1886,  4.7070, -4.1133,  ..., -3.8105,  0.8174,  4.0742]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.4851, -0.0742,  0.1697,  ..., -0.0767, -0.2004, -0.1218]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 07:35:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To restore results in restoration
To observe results in observation
To degrade results in degradation
To compute results in computation
To authorize results in authorization
To globalize results in globalization
To perspire results in perspiration
To realize results in
2024-07-10 07:35:56 root INFO     total operator prediction time: 4052.2481961250305 seconds
2024-07-10 07:35:56 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+able_reg
2024-07-10 07:35:59 root INFO     building operator verb+able_reg
2024-07-10 07:35:59 root INFO     [order_1_approx] starting weight calculation for If you can understand something, that thing is understandable
If you can vary something, that thing is variable
If you can accept something, that thing is acceptable
If you can inflate something, that thing is inflatable
If you can improve something, that thing is improvable
If you can achieve something, that thing is achieveable
If you can expect something, that thing is expectable
If you can represent something, that thing is
2024-07-10 07:35:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 07:40:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 07:44:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4521, -0.0178, -0.1665,  ..., -0.8770,  0.4939,  0.0013],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.4556, -0.0391, -0.1766,  ..., -0.8579,  0.4756, -0.0255],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.0723,  1.2188, -3.8281,  ..., -1.4297,  3.0273,  2.1914],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0118, -0.0065,  0.0020,  ..., -0.0058, -0.0063,  0.0040],
        [-0.0153,  0.0026, -0.0037,  ..., -0.0057, -0.0016,  0.0031],
        [ 0.0108, -0.0059, -0.0138,  ..., -0.0077,  0.0141, -0.0058],
        ...,
        [-0.0144, -0.0049,  0.0062,  ..., -0.0034,  0.0008, -0.0040],
        [-0.0022, -0.0042, -0.0143,  ..., -0.0238, -0.0104, -0.0013],
        [ 0.0037,  0.0033, -0.0006,  ..., -0.0188,  0.0081, -0.0098]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.9016e-03,  1.0788e-04,  2.4009e-04,  ..., -6.3896e-04,
          3.6168e-04,  3.3236e-04],
        [-8.2207e-04,  6.0654e-04,  8.9264e-04,  ...,  1.4448e-03,
         -1.9860e-04, -1.3828e-05],
        [-7.0333e-04,  7.5197e-04, -2.3098e-03,  ..., -4.1962e-04,
          1.1196e-03,  1.2054e-03],
        ...,
        [ 1.4076e-03, -1.0796e-03,  2.1191e-03,  ...,  2.9564e-05,
          1.9951e-03, -1.1768e-03],
        [ 2.2268e-04, -5.6171e-04, -1.0681e-03,  ...,  9.8419e-04,
         -7.0953e-04,  1.3199e-03],
        [ 8.5258e-04, -4.6349e-04,  1.3924e-04,  ...,  9.2888e-04,
         -5.4169e-04,  1.1511e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.4395,  1.5117, -4.6758,  ..., -1.8828,  3.1074,  1.7695]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.3113, -0.0022,  0.0100,  ..., -0.0044, -0.0670,  0.1223]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 07:44:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can understand something, that thing is understandable
If you can vary something, that thing is variable
If you can accept something, that thing is acceptable
If you can inflate something, that thing is inflatable
If you can improve something, that thing is improvable
If you can achieve something, that thing is achieveable
If you can expect something, that thing is expectable
If you can represent something, that thing is
2024-07-10 07:44:17 root INFO     [order_1_approx] starting weight calculation for If you can accept something, that thing is acceptable
If you can expect something, that thing is expectable
If you can achieve something, that thing is achieveable
If you can inflate something, that thing is inflatable
If you can represent something, that thing is representable
If you can vary something, that thing is variable
If you can improve something, that thing is improvable
If you can understand something, that thing is
2024-07-10 07:44:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 07:48:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 07:52:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9780, -0.5366, -0.1658,  ..., -0.5991,  1.2109, -0.0164],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.9985, -0.5771, -0.1748,  ..., -0.5806,  1.1855, -0.0442],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9150,  1.2207, -6.6641,  ..., -3.4492,  4.0508,  0.3066],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-3.7651e-03, -1.2993e-02,  4.1122e-03,  ...,  5.1079e-03,
         -2.1255e-02, -1.5381e-02],
        [-1.6968e-02,  7.7095e-03,  4.0588e-03,  ..., -1.0399e-02,
          6.2752e-04,  1.3748e-02],
        [ 1.6357e-02, -1.3733e-03,  6.6147e-03,  ...,  1.1200e-02,
          1.5854e-02, -3.6392e-03],
        ...,
        [ 1.5518e-02, -1.3519e-02, -4.5776e-05,  ..., -4.6959e-03,
          8.0032e-03, -1.5671e-02],
        [-8.1329e-03, -9.0027e-03, -8.6975e-04,  ..., -1.1887e-02,
         -1.7471e-03,  7.2327e-03],
        [ 2.8915e-02, -9.9335e-03,  6.5804e-03,  ..., -1.9928e-02,
          2.0844e-02, -5.8899e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-7.3195e-04, -6.0272e-04, -6.7711e-04,  ..., -6.0558e-05,
         -5.9032e-04, -3.1734e-04],
        [-1.3056e-03,  1.2617e-03, -7.2861e-04,  ..., -1.5869e-03,
         -9.5844e-04, -3.0231e-04],
        [ 9.8419e-04,  1.4124e-03, -1.0681e-03,  ...,  2.7514e-04,
         -6.5947e-04,  3.8385e-04],
        ...,
        [ 5.9128e-04,  1.0872e-03,  9.2411e-04,  ..., -2.1782e-03,
          6.0463e-04, -8.7547e-04],
        [-8.5783e-04, -3.6335e-04,  5.0640e-04,  ..., -3.8648e-04,
          9.1648e-04,  9.0027e-04],
        [ 1.0614e-03,  9.8610e-04, -7.1192e-04,  ..., -2.5749e-05,
         -1.0176e-03, -2.5034e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7412,  1.2324, -6.3477,  ..., -3.2227,  4.0312,  0.8506]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.3167, -0.1038,  0.3569,  ..., -0.1244, -0.2283,  0.0986]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 07:52:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can accept something, that thing is acceptable
If you can expect something, that thing is expectable
If you can achieve something, that thing is achieveable
If you can inflate something, that thing is inflatable
If you can represent something, that thing is representable
If you can vary something, that thing is variable
If you can improve something, that thing is improvable
If you can understand something, that thing is
2024-07-10 07:52:44 root INFO     [order_1_approx] starting weight calculation for If you can achieve something, that thing is achieveable
If you can represent something, that thing is representable
If you can accept something, that thing is acceptable
If you can improve something, that thing is improvable
If you can understand something, that thing is understandable
If you can inflate something, that thing is inflatable
If you can vary something, that thing is variable
If you can expect something, that thing is
2024-07-10 07:52:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 07:56:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 08:01:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0752,  1.1084, -1.2412,  ...,  1.2490,  2.3086,  0.2815],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.0834,  1.1562, -1.2373,  ...,  1.2402,  2.3320,  0.2717],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.5020,  2.4395, -3.7129,  ..., -2.9297,  4.9023,  1.8906],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.1063e-02, -2.5768e-03, -1.0193e-02,  ..., -6.9237e-03,
         -2.4460e-02,  2.6321e-04],
        [-4.2915e-03,  1.5388e-02,  3.9673e-03,  ..., -7.6904e-03,
         -7.1945e-03,  1.0986e-02],
        [ 7.5531e-04,  7.0114e-03, -1.3916e-02,  ..., -1.7548e-04,
          1.4679e-02,  4.8981e-03],
        ...,
        [-7.0343e-03, -1.5961e-02, -1.2505e-02,  ..., -1.0696e-02,
         -7.2212e-03, -1.8250e-02],
        [-1.9875e-03, -2.4147e-03,  2.7847e-04,  ..., -1.3313e-02,
         -1.4938e-02,  5.7220e-05],
        [-1.0700e-03,  1.3947e-02,  2.0905e-03,  ...,  2.9373e-04,
          1.5381e-02, -6.1188e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.0542e-03,  4.6158e-04, -1.0090e-03,  ...,  5.9652e-04,
         -8.3351e-04,  1.1368e-03],
        [ 1.5564e-03,  1.2836e-03,  2.5034e-04,  ...,  9.4652e-04,
          7.9727e-04,  1.9836e-03],
        [-1.4000e-03,  4.8542e-04, -2.9373e-03,  ...,  2.6560e-04,
          7.2479e-05,  6.8665e-04],
        ...,
        [-4.2272e-04, -1.1959e-03, -9.6703e-04,  ..., -2.4147e-03,
         -6.1941e-04, -3.0460e-03],
        [-2.0313e-03,  2.9683e-05, -7.7295e-04,  ..., -1.0204e-03,
         -7.0047e-04, -5.0879e-04],
        [-1.7967e-03,  1.4133e-03, -8.3804e-05,  ...,  2.6093e-03,
          2.2526e-03, -1.4172e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0898,  1.7852, -4.4688,  ..., -2.9355,  5.7266,  0.8955]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1606, -0.0215,  0.1876,  ...,  0.0114, -0.2544, -0.2196]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 08:01:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can achieve something, that thing is achieveable
If you can represent something, that thing is representable
If you can accept something, that thing is acceptable
If you can improve something, that thing is improvable
If you can understand something, that thing is understandable
If you can inflate something, that thing is inflatable
If you can vary something, that thing is variable
If you can expect something, that thing is
2024-07-10 08:01:08 root INFO     [order_1_approx] starting weight calculation for If you can understand something, that thing is understandable
If you can improve something, that thing is improvable
If you can accept something, that thing is acceptable
If you can represent something, that thing is representable
If you can expect something, that thing is expectable
If you can inflate something, that thing is inflatable
If you can vary something, that thing is variable
If you can achieve something, that thing is
2024-07-10 08:01:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 08:05:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 08:09:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4124,  0.6792,  0.6079,  ..., -0.2371,  0.1387, -0.2822],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.4309,  0.6953,  0.5820,  ..., -0.2323,  0.1191, -0.3225],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3142,  4.0352, -2.5020,  ..., -7.8320,  4.2227, -0.5054],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0143,  0.0048,  0.0057,  ...,  0.0035, -0.0143, -0.0009],
        [-0.0033,  0.0014,  0.0158,  ..., -0.0013, -0.0067,  0.0049],
        [-0.0089,  0.0078, -0.0077,  ...,  0.0018, -0.0054,  0.0072],
        ...,
        [ 0.0119, -0.0141, -0.0163,  ..., -0.0217,  0.0014, -0.0064],
        [ 0.0096, -0.0063,  0.0041,  ..., -0.0282, -0.0022,  0.0148],
        [-0.0021, -0.0047, -0.0076,  ..., -0.0080,  0.0083,  0.0018]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0021,  0.0003,  0.0019,  ..., -0.0003, -0.0007, -0.0011],
        [-0.0003,  0.0003,  0.0020,  ...,  0.0021,  0.0010,  0.0004],
        [ 0.0018,  0.0007, -0.0034,  ...,  0.0007,  0.0009,  0.0021],
        ...,
        [-0.0006,  0.0001, -0.0008,  ..., -0.0004,  0.0018, -0.0019],
        [-0.0010, -0.0003, -0.0002,  ..., -0.0006,  0.0014,  0.0007],
        [ 0.0022, -0.0008, -0.0007,  ...,  0.0013, -0.0003,  0.0002]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1455,  4.4844, -2.6992,  ..., -8.1641,  5.1172, -0.4978]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1016, -0.0474,  0.1038,  ...,  0.0913,  0.1584, -0.0174]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 08:09:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can understand something, that thing is understandable
If you can improve something, that thing is improvable
If you can accept something, that thing is acceptable
If you can represent something, that thing is representable
If you can expect something, that thing is expectable
If you can inflate something, that thing is inflatable
If you can vary something, that thing is variable
If you can achieve something, that thing is
2024-07-10 08:09:35 root INFO     [order_1_approx] starting weight calculation for If you can improve something, that thing is improvable
If you can achieve something, that thing is achieveable
If you can understand something, that thing is understandable
If you can vary something, that thing is variable
If you can accept something, that thing is acceptable
If you can expect something, that thing is expectable
If you can represent something, that thing is representable
If you can inflate something, that thing is
2024-07-10 08:09:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 08:13:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 08:17:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7632,  0.6909,  1.4375,  ...,  0.8726,  0.3916,  0.3628],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.6812,  0.6104,  1.2041,  ...,  0.7451,  0.3235,  0.3044],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.2578,  0.8965, -4.5234,  ..., -0.9297,  2.8789, -1.3984],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 9.2316e-03, -5.8365e-03, -1.0780e-02,  ..., -5.4131e-03,
          4.9286e-03, -9.4986e-03],
        [-6.0005e-03,  3.7384e-03,  1.3733e-02,  ...,  2.0657e-03,
         -5.7907e-03,  1.7290e-03],
        [ 1.3794e-02,  7.3547e-03, -1.5472e-02,  ..., -6.7940e-03,
         -3.4142e-03,  5.8823e-03],
        ...,
        [-1.2054e-02, -6.2103e-03,  7.6294e-06,  ...,  3.6392e-03,
         -2.9926e-03, -3.4466e-03],
        [-3.9005e-03,  2.0828e-03, -8.2779e-04,  ...,  5.4626e-03,
          1.1368e-03,  3.0422e-03],
        [ 1.2589e-03,  3.1757e-03, -2.7695e-03,  ..., -6.9885e-03,
          6.2408e-03,  1.4549e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.1511e-03, -4.8161e-04, -2.9087e-04,  ..., -1.3609e-03,
          1.0118e-03,  5.9414e-04],
        [ 8.7500e-04,  5.4312e-04,  6.7616e-04,  ...,  9.3937e-05,
         -7.6294e-04, -1.1182e-04],
        [-1.1234e-03, -2.0802e-04, -9.9945e-04,  ...,  2.9707e-04,
         -5.9700e-04,  2.3403e-03],
        ...,
        [ 1.1425e-03, -1.2875e-04,  1.1772e-04,  ...,  1.1253e-03,
          1.1282e-03, -2.5787e-03],
        [-1.7190e-04,  1.4324e-03, -8.5974e-04,  ...,  9.5272e-04,
         -1.3161e-03,  3.3665e-04],
        [ 1.1930e-03,  7.7343e-04, -9.9850e-04,  ...,  4.4441e-04,
         -1.6479e-03,  3.2592e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2676,  0.2095, -5.0039,  ..., -0.9194,  2.9238, -0.8169]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0928,  0.0676, -0.0012,  ...,  0.0594,  0.0155, -0.0391]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 08:18:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can improve something, that thing is improvable
If you can achieve something, that thing is achieveable
If you can understand something, that thing is understandable
If you can vary something, that thing is variable
If you can accept something, that thing is acceptable
If you can expect something, that thing is expectable
If you can represent something, that thing is representable
If you can inflate something, that thing is
2024-07-10 08:18:02 root INFO     [order_1_approx] starting weight calculation for If you can expect something, that thing is expectable
If you can understand something, that thing is understandable
If you can vary something, that thing is variable
If you can inflate something, that thing is inflatable
If you can represent something, that thing is representable
If you can accept something, that thing is acceptable
If you can achieve something, that thing is achieveable
If you can improve something, that thing is
2024-07-10 08:18:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 08:22:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 08:26:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0435, -0.0771,  0.1614,  ..., -0.5488,  0.8760, -0.3450],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.0525, -0.1028,  0.1458,  ..., -0.5469,  0.8765, -0.3933],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 6.0000,  1.1035, -2.2812,  ..., -1.0762,  0.7178, -0.2031],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0115, -0.0061,  0.0033,  ..., -0.0008, -0.0051, -0.0061],
        [ 0.0070,  0.0127,  0.0012,  ...,  0.0014, -0.0114,  0.0075],
        [ 0.0154, -0.0040,  0.0027,  ..., -0.0047,  0.0235, -0.0128],
        ...,
        [-0.0055, -0.0156, -0.0073,  ..., -0.0147, -0.0031, -0.0039],
        [-0.0111, -0.0158, -0.0014,  ..., -0.0247, -0.0098,  0.0164],
        [ 0.0105, -0.0008, -0.0026,  ...,  0.0081,  0.0093, -0.0138]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.0705e-04, -7.8678e-04,  3.7241e-04,  ..., -3.9196e-04,
         -8.9931e-04,  1.1187e-03],
        [ 1.5497e-05,  1.4381e-03,  8.5354e-04,  ...,  1.3447e-03,
         -6.7520e-04, -1.4257e-04],
        [ 1.6003e-03,  3.2730e-03, -1.1778e-04,  ...,  3.4070e-04,
          5.2547e-04,  8.7214e-04],
        ...,
        [ 7.6628e-04, -2.0587e-04, -2.1706e-03,  ..., -4.5013e-04,
          7.9536e-04, -9.2363e-04],
        [ 4.0340e-04,  7.4911e-04,  5.2118e-04,  ...,  4.5681e-04,
         -1.4091e-04, -7.0381e-04],
        [ 2.9945e-03, -2.3136e-03,  1.4067e-03,  ...,  7.3957e-04,
         -5.1975e-04, -4.2844e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.4453,  1.6221, -2.2051,  ..., -1.3525,  1.1641, -0.7773]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0252, -0.0615, -0.1284,  ..., -0.0959, -0.0440, -0.1322]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 08:26:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can expect something, that thing is expectable
If you can understand something, that thing is understandable
If you can vary something, that thing is variable
If you can inflate something, that thing is inflatable
If you can represent something, that thing is representable
If you can accept something, that thing is acceptable
If you can achieve something, that thing is achieveable
If you can improve something, that thing is
2024-07-10 08:26:30 root INFO     [order_1_approx] starting weight calculation for If you can inflate something, that thing is inflatable
If you can understand something, that thing is understandable
If you can improve something, that thing is improvable
If you can vary something, that thing is variable
If you can represent something, that thing is representable
If you can expect something, that thing is expectable
If you can achieve something, that thing is achieveable
If you can accept something, that thing is
2024-07-10 08:26:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 08:30:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 08:34:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6982, -0.3540,  0.3662,  ..., -0.6895,  0.4978,  0.6279],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.7319, -0.3955,  0.3511,  ..., -0.6841,  0.4905,  0.6416],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1465,  1.7910, -1.2871,  ..., -1.8047,  3.9961,  2.5137],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0102,  0.0079,  0.0008,  ...,  0.0025, -0.0159, -0.0050],
        [ 0.0095,  0.0006,  0.0074,  ..., -0.0097, -0.0052,  0.0040],
        [ 0.0124,  0.0004, -0.0061,  ...,  0.0079,  0.0086, -0.0076],
        ...,
        [ 0.0080,  0.0022, -0.0015,  ..., -0.0066, -0.0018, -0.0152],
        [ 0.0016, -0.0153, -0.0027,  ..., -0.0292, -0.0054,  0.0092],
        [ 0.0088,  0.0110, -0.0005,  ..., -0.0067,  0.0208, -0.0030]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.7996e-03, -1.8473e-03,  1.2789e-03,  ..., -5.5122e-04,
         -3.2711e-04, -1.6050e-03],
        [ 4.1056e-04,  3.9196e-04, -5.1498e-04,  ...,  1.0681e-03,
         -4.3869e-04,  3.1567e-03],
        [ 2.0256e-03,  1.2970e-03, -3.9902e-03,  ...,  3.0756e-04,
         -2.5921e-03,  1.1148e-03],
        ...,
        [ 1.3089e-04,  5.0831e-04,  9.1791e-06,  ..., -1.1549e-03,
          4.3821e-04, -3.9940e-03],
        [-8.7881e-04, -8.8596e-04,  2.1625e-04,  ..., -7.2956e-05,
          3.1090e-04,  1.1349e-03],
        [-1.4019e-04,  1.4915e-03, -8.3542e-04,  ...,  1.9035e-03,
          1.3704e-03, -3.3760e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1648,  1.0840, -1.1250,  ..., -1.4160,  3.5781,  2.2207]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2012, -0.0103,  0.1414,  ...,  0.1031, -0.1390, -0.2495]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 08:34:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can inflate something, that thing is inflatable
If you can understand something, that thing is understandable
If you can improve something, that thing is improvable
If you can vary something, that thing is variable
If you can represent something, that thing is representable
If you can expect something, that thing is expectable
If you can achieve something, that thing is achieveable
If you can accept something, that thing is
2024-07-10 08:34:55 root INFO     [order_1_approx] starting weight calculation for If you can inflate something, that thing is inflatable
If you can expect something, that thing is expectable
If you can achieve something, that thing is achieveable
If you can improve something, that thing is improvable
If you can accept something, that thing is acceptable
If you can understand something, that thing is understandable
If you can represent something, that thing is representable
If you can vary something, that thing is
2024-07-10 08:34:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 08:39:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 08:43:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2771, -0.0493, -0.0833,  ..., -0.7217,  0.4954,  0.7827],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.2788, -0.0701, -0.0938,  ..., -0.7051,  0.4797,  0.7925],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2192,  1.7988, -3.6465,  ...,  1.9072,  7.1602, -1.8477],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0041, -0.0144, -0.0013,  ..., -0.0087, -0.0067, -0.0017],
        [-0.0169, -0.0005,  0.0044,  ..., -0.0086, -0.0115, -0.0077],
        [ 0.0042,  0.0009, -0.0244,  ..., -0.0045,  0.0024,  0.0084],
        ...,
        [ 0.0072,  0.0118,  0.0026,  ..., -0.0007,  0.0055, -0.0146],
        [-0.0048, -0.0089, -0.0051,  ..., -0.0172, -0.0101,  0.0093],
        [-0.0039,  0.0027,  0.0007,  ..., -0.0115, -0.0002,  0.0121]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.2379e-03, -4.8923e-04,  4.1270e-04,  ..., -1.8969e-03,
         -1.4572e-03,  5.9509e-04],
        [-9.8324e-04,  1.8942e-04,  3.7909e-04,  ...,  1.5068e-04,
          3.4761e-04,  3.0637e-05],
        [ 1.1139e-03,  1.4296e-03, -4.5853e-03,  ..., -1.4901e-06,
         -2.1195e-04,  4.6959e-03],
        ...,
        [ 8.2397e-04, -2.3592e-04,  8.8739e-04,  ..., -1.3685e-04,
          5.5122e-04, -2.0218e-03],
        [ 1.1740e-03, -5.1355e-04,  4.5776e-04,  ...,  1.3790e-03,
         -1.6518e-03, -5.4169e-04],
        [ 1.6136e-03,  1.8053e-03, -2.0123e-03,  ..., -2.3174e-03,
         -8.6927e-04,  3.5048e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7773,  2.3398, -4.4180,  ...,  2.6094,  7.1445, -2.7852]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2944,  0.1157,  0.2191,  ..., -0.1321, -0.0906,  0.0707]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 08:43:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can inflate something, that thing is inflatable
If you can expect something, that thing is expectable
If you can achieve something, that thing is achieveable
If you can improve something, that thing is improvable
If you can accept something, that thing is acceptable
If you can understand something, that thing is understandable
If you can represent something, that thing is representable
If you can vary something, that thing is
2024-07-10 08:43:20 root INFO     total operator prediction time: 4040.8879742622375 seconds
2024-07-10 08:43:20 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on un+adj_reg
2024-07-10 08:43:22 root INFO     building operator un+adj_reg
2024-07-10 08:43:23 root INFO     [order_1_approx] starting weight calculation for The opposite of conscious is unconscious
The opposite of interrupted is uninterrupted
The opposite of healthy is unhealthy
The opposite of available is unavailable
The opposite of published is unpublished
The opposite of affected is unaffected
The opposite of reasonable is unreasonable
The opposite of biased is
2024-07-10 08:43:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 08:47:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 08:51:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0645, -0.7441, -0.9561,  ...,  0.2321,  0.5986, -0.3145],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.9116, -0.6772, -0.8003,  ...,  0.1927,  0.4871, -0.3032],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([1.9512, 3.2656, 0.1758,  ..., 0.2959, 3.2422, 3.5938], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0050, -0.0328, -0.0148,  ...,  0.0001, -0.0060, -0.0174],
        [-0.0115, -0.0019, -0.0091,  ..., -0.0078, -0.0262,  0.0102],
        [ 0.0006, -0.0034, -0.0230,  ..., -0.0082,  0.0164,  0.0053],
        ...,
        [ 0.0078, -0.0027,  0.0157,  ...,  0.0076, -0.0081, -0.0028],
        [-0.0395,  0.0032,  0.0023,  ..., -0.0071, -0.0225, -0.0184],
        [ 0.0134, -0.0082, -0.0057,  ..., -0.0028,  0.0198,  0.0026]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.5368e-03, -2.1343e-03, -8.6880e-04,  ...,  2.1057e-03,
          6.4564e-04,  2.4261e-03],
        [-9.8228e-04, -2.7752e-03,  1.5726e-03,  ...,  2.0466e-03,
         -6.4993e-04,  1.1473e-03],
        [-1.2207e-04, -2.2602e-03,  5.9366e-04,  ...,  1.2131e-03,
          1.6479e-03,  1.7376e-03],
        ...,
        [ 3.0327e-03,  3.9935e-04,  1.4791e-03,  ..., -1.4353e-04,
         -8.7214e-04, -1.1425e-03],
        [ 2.3127e-05, -6.0749e-04,  1.2131e-03,  ..., -1.8034e-03,
         -9.6893e-04, -2.1782e-03],
        [-7.3910e-04,  1.8406e-04,  6.0368e-04,  ...,  7.4196e-04,
         -4.4799e-04,  1.3323e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7188,  2.1953,  1.4229,  ..., -0.4844,  2.5156,  3.3652]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.3496, -0.1676,  0.1899,  ...,  0.1051, -0.0185,  0.1694]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 08:51:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of conscious is unconscious
The opposite of interrupted is uninterrupted
The opposite of healthy is unhealthy
The opposite of available is unavailable
The opposite of published is unpublished
The opposite of affected is unaffected
The opposite of reasonable is unreasonable
The opposite of biased is
2024-07-10 08:51:51 root INFO     [order_1_approx] starting weight calculation for The opposite of available is unavailable
The opposite of biased is unbiased
The opposite of reasonable is unreasonable
The opposite of published is unpublished
The opposite of conscious is unconscious
The opposite of healthy is unhealthy
The opposite of interrupted is uninterrupted
The opposite of affected is
2024-07-10 08:51:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 08:56:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 09:00:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0391, -1.0195, -0.5137,  ...,  0.7666, -0.3945,  0.5703],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 1.0117, -1.0469, -0.4968,  ...,  0.7197, -0.4050,  0.5420],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6553, -0.6260, -1.4814,  ..., -0.8599,  3.5977,  4.3555],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0030,  0.0102,  0.0012,  ...,  0.0002,  0.0119, -0.0101],
        [-0.0101, -0.0111, -0.0100,  ..., -0.0157, -0.0084, -0.0083],
        [ 0.0307, -0.0092, -0.0156,  ..., -0.0057,  0.0196,  0.0268],
        ...,
        [ 0.0083, -0.0152,  0.0131,  ..., -0.0076, -0.0004,  0.0127],
        [-0.0075,  0.0015,  0.0066,  ..., -0.0041, -0.0357,  0.0024],
        [-0.0037,  0.0180, -0.0062,  ..., -0.0023,  0.0196, -0.0092]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.0443e-03, -6.0368e-04, -1.1635e-03,  ..., -2.1458e-04,
          2.5063e-03,  2.8820e-03],
        [ 2.8515e-04, -4.8923e-04,  2.8076e-03,  ...,  1.2064e-03,
          1.2541e-03,  4.4250e-04],
        [-3.3741e-03, -1.6823e-03, -9.7418e-04,  ..., -1.0471e-03,
         -3.7885e-04,  1.5106e-03],
        ...,
        [-6.0737e-05, -9.6130e-04,  1.2474e-03,  ...,  2.3193e-03,
          1.3914e-03, -4.1294e-04],
        [-1.6050e-03, -4.6790e-05,  6.6423e-04,  ..., -1.1826e-04,
         -2.0027e-03, -1.2836e-03],
        [ 1.2302e-03,  2.0885e-03, -2.1725e-03,  ...,  2.5749e-04,
         -6.6948e-04, -1.9798e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0254, -2.3477, -1.4277,  ..., -0.2510,  3.8730,  2.7148]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.3311,  0.0356,  0.6152,  ...,  0.1246, -0.0787, -0.1404]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 09:00:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of available is unavailable
The opposite of biased is unbiased
The opposite of reasonable is unreasonable
The opposite of published is unpublished
The opposite of conscious is unconscious
The opposite of healthy is unhealthy
The opposite of interrupted is uninterrupted
The opposite of affected is
2024-07-10 09:00:21 root INFO     [order_1_approx] starting weight calculation for The opposite of available is unavailable
The opposite of conscious is unconscious
The opposite of interrupted is uninterrupted
The opposite of healthy is unhealthy
The opposite of affected is unaffected
The opposite of reasonable is unreasonable
The opposite of biased is unbiased
The opposite of published is
2024-07-10 09:00:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 09:04:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 09:08:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.1465, 0.3564, 0.8755,  ..., 1.2275, 0.7837, 0.5347], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([0.1278, 0.3176, 0.7603,  ..., 1.0928, 0.6943, 0.4785], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1787, -2.5430,  1.2480,  ...,  1.9629,  1.8594, -1.9258],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0040, -0.0012,  0.0065,  ...,  0.0008,  0.0076, -0.0082],
        [-0.0279,  0.0057,  0.0049,  ...,  0.0023, -0.0110, -0.0065],
        [-0.0016,  0.0004, -0.0181,  ..., -0.0201, -0.0019,  0.0136],
        ...,
        [-0.0183, -0.0129, -0.0024,  ...,  0.0053,  0.0011,  0.0103],
        [-0.0046,  0.0120,  0.0092,  ...,  0.0027, -0.0057,  0.0101],
        [ 0.0142, -0.0108,  0.0072,  ..., -0.0183,  0.0057, -0.0013]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.5692e-03,  1.0586e-03,  4.8351e-04,  ..., -2.3365e-04,
          6.5994e-04,  1.3142e-03],
        [ 1.8559e-03, -2.9793e-03,  1.5187e-04,  ..., -1.7900e-03,
          3.8803e-05,  4.2200e-04],
        [ 2.3212e-03, -2.9135e-04, -8.5354e-04,  ...,  6.5613e-04,
          4.5919e-04, -5.6744e-04],
        ...,
        [-2.2240e-03,  1.7776e-03, -1.3733e-04,  ..., -6.2990e-04,
          2.4185e-03, -7.7915e-04],
        [ 4.8828e-04,  5.3549e-04,  1.3037e-03,  ..., -5.0402e-04,
         -1.9817e-03,  6.1607e-04],
        [-2.3437e-04, -3.6287e-04,  1.7729e-03,  ..., -1.2646e-03,
          3.3092e-04,  2.3556e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.6670, -2.9199,  0.5620,  ...,  2.1270,  0.3975, -2.1406]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2751,  0.1862,  0.1901,  ...,  0.5879, -0.2947,  0.3628]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 09:08:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of available is unavailable
The opposite of conscious is unconscious
The opposite of interrupted is uninterrupted
The opposite of healthy is unhealthy
The opposite of affected is unaffected
The opposite of reasonable is unreasonable
The opposite of biased is unbiased
The opposite of published is
2024-07-10 09:08:51 root INFO     [order_1_approx] starting weight calculation for The opposite of biased is unbiased
The opposite of interrupted is uninterrupted
The opposite of published is unpublished
The opposite of conscious is unconscious
The opposite of affected is unaffected
The opposite of reasonable is unreasonable
The opposite of healthy is unhealthy
The opposite of available is
2024-07-10 09:08:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 09:13:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 09:17:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4067, -0.5166,  0.2993,  ...,  0.3071,  0.2312,  0.1411],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3835, -0.5317,  0.2590,  ...,  0.2827,  0.1960,  0.1101],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.1426, -1.3105,  2.5410,  ...,  1.7197,  2.6328,  0.3369],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0037, -0.0140,  0.0028,  ...,  0.0073, -0.0118,  0.0064],
        [-0.0115, -0.0081,  0.0003,  ..., -0.0216, -0.0061,  0.0051],
        [ 0.0029,  0.0056, -0.0049,  ..., -0.0145, -0.0180,  0.0216],
        ...,
        [ 0.0029,  0.0016, -0.0291,  ..., -0.0195,  0.0008, -0.0033],
        [ 0.0121, -0.0213,  0.0123,  ...,  0.0051,  0.0020, -0.0191],
        [ 0.0234,  0.0176, -0.0126,  ..., -0.0227,  0.0383, -0.0018]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.0409e-03,  6.0940e-04, -1.7767e-03,  ...,  8.9359e-04,
          4.1533e-04, -3.9530e-04],
        [-7.2718e-05, -2.5063e-03,  7.5483e-04,  ...,  9.1648e-04,
         -2.1000e-03,  1.7242e-03],
        [ 1.2636e-04, -1.4200e-03,  9.0885e-04,  ...,  1.8044e-03,
          7.8011e-04,  1.5602e-03],
        ...,
        [ 2.4891e-03,  9.3937e-04, -9.1267e-04,  ..., -1.6813e-03,
          1.8997e-03, -3.6831e-03],
        [ 1.0796e-03,  8.9073e-04, -1.3638e-03,  ..., -5.9319e-04,
         -6.7139e-04, -3.8290e-04],
        [-6.3324e-04,  2.7561e-03,  9.4080e-04,  ...,  4.0364e-04,
          2.1286e-03, -1.7643e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7383, -1.4961,  1.0195,  ...,  1.6719,  2.8828, -0.6538]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[0.1050, 0.1708, 0.4390,  ..., 0.3318, 0.0816, 0.2471]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 09:17:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of biased is unbiased
The opposite of interrupted is uninterrupted
The opposite of published is unpublished
The opposite of conscious is unconscious
The opposite of affected is unaffected
The opposite of reasonable is unreasonable
The opposite of healthy is unhealthy
The opposite of available is
2024-07-10 09:17:19 root INFO     [order_1_approx] starting weight calculation for The opposite of healthy is unhealthy
The opposite of available is unavailable
The opposite of reasonable is unreasonable
The opposite of published is unpublished
The opposite of biased is unbiased
The opposite of affected is unaffected
The opposite of interrupted is uninterrupted
The opposite of conscious is
2024-07-10 09:17:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 09:21:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 09:25:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5361,  0.8047, -0.5527,  ...,  0.5723,  0.2627, -0.7793],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.4834,  0.7339, -0.4990,  ...,  0.5020,  0.2141, -0.7573],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.8359, -0.2471,  0.9951,  ..., -1.5732,  0.6987, -2.2773],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0101,  0.0107, -0.0017,  ...,  0.0128, -0.0042, -0.0032],
        [-0.0071, -0.0089, -0.0015,  ..., -0.0224, -0.0126,  0.0146],
        [ 0.0027, -0.0059, -0.0321,  ...,  0.0096,  0.0124,  0.0157],
        ...,
        [ 0.0071, -0.0031,  0.0013,  ..., -0.0226, -0.0027, -0.0033],
        [-0.0085, -0.0100,  0.0411,  ..., -0.0065, -0.0105, -0.0168],
        [ 0.0223, -0.0001, -0.0060,  ..., -0.0219, -0.0063, -0.0139]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.5640e-03, -2.3193e-03,  1.9798e-03,  ...,  1.3423e-04,
         -6.7377e-04, -4.1366e-05],
        [-1.3943e-03, -1.4648e-03,  7.7724e-04,  ...,  3.8471e-03,
          2.0676e-03,  3.2978e-03],
        [ 6.8712e-04,  5.5647e-04, -1.9753e-04,  ...,  1.2274e-03,
         -1.7796e-03,  1.8253e-03],
        ...,
        [-1.1158e-03, -8.2064e-04,  8.7786e-04,  ...,  2.7275e-04,
          1.8847e-04, -2.1648e-03],
        [-3.1710e-05,  2.0909e-04,  7.2765e-04,  ...,  1.9150e-03,
         -1.7824e-03, -8.9836e-04],
        [ 1.0891e-03, -9.1553e-04, -1.7548e-03,  ..., -9.2888e-04,
          3.9625e-04,  6.5184e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.1680, -2.2930,  0.8232,  ..., -0.0625, -0.4458, -3.7383]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0016,  0.0970,  0.3171,  ..., -0.2303,  0.1720,  0.2209]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 09:25:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of healthy is unhealthy
The opposite of available is unavailable
The opposite of reasonable is unreasonable
The opposite of published is unpublished
The opposite of biased is unbiased
The opposite of affected is unaffected
The opposite of interrupted is uninterrupted
The opposite of conscious is
2024-07-10 09:25:47 root INFO     [order_1_approx] starting weight calculation for The opposite of published is unpublished
The opposite of healthy is unhealthy
The opposite of interrupted is uninterrupted
The opposite of biased is unbiased
The opposite of available is unavailable
The opposite of affected is unaffected
The opposite of conscious is unconscious
The opposite of reasonable is
2024-07-10 09:25:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 09:30:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 09:34:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8052, -1.1260, -0.5342,  ..., -0.7280,  0.4504,  0.4697],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.7002, -1.0391, -0.4653,  ..., -0.6157,  0.3665,  0.3923],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9707, -3.5156,  0.3477,  ...,  1.5244,  4.3438,  1.6484],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0025,  0.0073, -0.0099,  ...,  0.0077,  0.0008, -0.0005],
        [-0.0215, -0.0038,  0.0113,  ..., -0.0113, -0.0148, -0.0052],
        [-0.0132, -0.0070,  0.0015,  ..., -0.0052,  0.0004, -0.0033],
        ...,
        [-0.0077, -0.0261, -0.0065,  ...,  0.0110,  0.0091, -0.0042],
        [-0.0095, -0.0068,  0.0413,  ...,  0.0052, -0.0078, -0.0029],
        [ 0.0159,  0.0100,  0.0145,  ..., -0.0121,  0.0135,  0.0061]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.3346e-03,  1.7605e-03, -6.5994e-04,  ..., -2.2926e-03,
          1.9302e-03,  2.0599e-04],
        [ 1.4381e-03, -3.3302e-03,  3.5095e-04,  ...,  2.4033e-03,
         -1.8663e-03, -8.3256e-04],
        [ 6.3705e-04, -9.8133e-04, -1.4820e-03,  ...,  1.2302e-03,
          3.7456e-04, -1.7977e-04],
        ...,
        [ 1.9264e-04, -1.4973e-03, -9.0694e-04,  ...,  9.3937e-04,
          9.0790e-04, -1.3351e-05],
        [-1.0109e-03, -9.9468e-04,  2.2659e-03,  ..., -2.7256e-03,
         -2.0237e-03,  2.2087e-03],
        [-6.3515e-04,  6.0558e-05,  1.6136e-03,  ...,  6.2561e-04,
          2.8849e-04,  6.5851e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.2344, -5.1289, -1.5088,  ...,  2.2930,  3.4609,  1.1582]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1168,  0.0826,  0.1934,  ..., -0.0213,  0.1394, -0.1022]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 09:34:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of published is unpublished
The opposite of healthy is unhealthy
The opposite of interrupted is uninterrupted
The opposite of biased is unbiased
The opposite of available is unavailable
The opposite of affected is unaffected
The opposite of conscious is unconscious
The opposite of reasonable is
2024-07-10 09:34:15 root INFO     [order_1_approx] starting weight calculation for The opposite of reasonable is unreasonable
The opposite of biased is unbiased
The opposite of published is unpublished
The opposite of affected is unaffected
The opposite of available is unavailable
The opposite of conscious is unconscious
The opposite of interrupted is uninterrupted
The opposite of healthy is
2024-07-10 09:34:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 09:38:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 09:42:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0509,  0.0076, -1.1387,  ...,  0.2793,  0.5752,  0.4509],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.0534, -0.0140, -0.9912,  ...,  0.2410,  0.4871,  0.3860],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1929,  0.0862,  1.8047,  ..., -3.4805,  2.2266, -1.0137],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0144,  0.0018, -0.0098,  ...,  0.0137,  0.0023, -0.0143],
        [-0.0185,  0.0079,  0.0008,  ..., -0.0111, -0.0053,  0.0068],
        [-0.0081,  0.0079, -0.0156,  ...,  0.0058,  0.0030,  0.0009],
        ...,
        [ 0.0152, -0.0111,  0.0007,  ...,  0.0133,  0.0023, -0.0019],
        [ 0.0031, -0.0230,  0.0198,  ..., -0.0004, -0.0146,  0.0030],
        [-0.0109, -0.0183,  0.0039,  ..., -0.0063, -0.0040,  0.0051]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0023,  0.0010, -0.0025,  ..., -0.0002,  0.0003,  0.0027],
        [-0.0014, -0.0011,  0.0020,  ...,  0.0009,  0.0024, -0.0005],
        [ 0.0002, -0.0019, -0.0005,  ...,  0.0018, -0.0009, -0.0017],
        ...,
        [ 0.0003, -0.0014,  0.0014,  ...,  0.0008,  0.0016, -0.0023],
        [-0.0014, -0.0016, -0.0009,  ..., -0.0011,  0.0016, -0.0007],
        [ 0.0026,  0.0004, -0.0026,  ...,  0.0021,  0.0007,  0.0003]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9155, -1.4023,  0.6211,  ..., -2.7246,  2.1523, -0.7598]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1030,  0.0066,  0.2252,  ...,  0.0471,  0.3352,  0.2091]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 09:42:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of reasonable is unreasonable
The opposite of biased is unbiased
The opposite of published is unpublished
The opposite of affected is unaffected
The opposite of available is unavailable
The opposite of conscious is unconscious
The opposite of interrupted is uninterrupted
The opposite of healthy is
2024-07-10 09:42:44 root INFO     [order_1_approx] starting weight calculation for The opposite of affected is unaffected
The opposite of conscious is unconscious
The opposite of biased is unbiased
The opposite of reasonable is unreasonable
The opposite of available is unavailable
The opposite of healthy is unhealthy
The opposite of published is unpublished
The opposite of interrupted is
2024-07-10 09:42:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 09:47:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 09:51:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2405, -0.4988, -0.0559,  ...,  0.5469,  0.0181,  0.5098],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.2344, -0.4990, -0.0673,  ...,  0.4846, -0.0086,  0.4531],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.8906, -4.0977, -8.1719,  ...,  3.5586,  0.2363,  0.8945],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0198, -0.0053, -0.0013,  ..., -0.0063,  0.0084, -0.0210],
        [-0.0109, -0.0123,  0.0065,  ..., -0.0068,  0.0011,  0.0183],
        [ 0.0047, -0.0182, -0.0214,  ..., -0.0225,  0.0300,  0.0064],
        ...,
        [-0.0347, -0.0356,  0.0313,  ..., -0.0146, -0.0166,  0.0348],
        [ 0.0241, -0.0266,  0.0364,  ..., -0.0118,  0.0076, -0.0072],
        [ 0.0085, -0.0091, -0.0144,  ...,  0.0145, -0.0252, -0.0041]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.6870e-03, -9.3174e-04,  5.4359e-05,  ..., -2.2354e-03,
          9.3317e-04,  2.8820e-03],
        [-3.6106e-03,  8.3303e-04,  4.6682e-04,  ...,  1.2016e-03,
         -3.9101e-04,  2.8973e-03],
        [-8.0729e-04,  1.4906e-03, -6.6566e-04,  ...,  1.8072e-03,
          1.8787e-03,  5.2834e-04],
        ...,
        [ 3.3340e-03, -5.1193e-03,  4.0841e-04,  ..., -2.8801e-03,
         -6.9809e-04, -3.1490e-03],
        [ 8.6164e-04, -2.3985e-04, -3.0689e-03,  ..., -4.7684e-07,
         -1.6203e-03, -6.7711e-04],
        [-3.7241e-04, -1.2398e-03, -2.2578e-04,  ...,  1.6632e-03,
         -1.9703e-03,  3.6430e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0029e+00, -3.2461e+00, -6.4922e+00,  ...,  3.1406e+00,
          1.3193e+00,  4.8828e-03]], device='cuda:1', dtype=torch.float16,
       grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1675, -0.1268,  0.5464,  ...,  0.1417,  0.0828,  0.1037]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 09:51:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of affected is unaffected
The opposite of conscious is unconscious
The opposite of biased is unbiased
The opposite of reasonable is unreasonable
The opposite of available is unavailable
The opposite of healthy is unhealthy
The opposite of published is unpublished
The opposite of interrupted is
2024-07-10 09:51:13 root INFO     total operator prediction time: 4070.4498426914215 seconds
2024-07-10 09:51:13 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on re+verb_reg
2024-07-10 09:51:15 root INFO     building operator re+verb_reg
2024-07-10 09:51:15 root INFO     [order_1_approx] starting weight calculation for To grow again is to regrow
To consider again is to reconsider
To evaluate again is to reevaluate
To assure again is to reassure
To confirm again is to reconfirm
To occur again is to reoccur
To calculate again is to recalculate
To locate again is to
2024-07-10 09:51:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 09:55:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 09:59:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3281,  0.3445, -0.0229,  ..., -0.1724,  0.1445, -0.5479],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3328,  0.3445, -0.0351,  ..., -0.1672,  0.1266, -0.5996],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0320, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7344,  1.4570, -7.4688,  ..., -2.0664,  3.5078,  3.3320],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 8.3313e-03, -2.3865e-02,  1.4755e-02,  ...,  1.0841e-02,
          8.7967e-03, -1.7563e-02],
        [-1.8402e-02, -5.4817e-03,  1.4328e-02,  ..., -7.7744e-03,
         -1.0880e-02, -1.9745e-02],
        [ 2.9495e-02,  7.3242e-04,  1.4694e-02,  ...,  5.5084e-03,
          1.9043e-02,  9.1705e-03],
        ...,
        [ 7.3128e-03, -1.6739e-02,  5.2032e-03,  ..., -1.9287e-02,
          1.8890e-02, -4.7760e-03],
        [ 7.9651e-03,  1.6861e-03,  7.3738e-03,  ..., -7.4120e-03,
          1.1726e-02,  4.9133e-03],
        [-1.6724e-02,  2.7771e-03, -3.9339e-05,  ..., -7.3929e-03,
          8.6288e-03,  5.0125e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.6518e-03, -1.6031e-03,  4.5478e-05,  ..., -1.5850e-03,
          2.4378e-05,  4.8590e-04],
        [-2.8133e-04, -2.1305e-03,  1.7433e-03,  ...,  5.8079e-04,
         -3.6478e-04, -1.7080e-03],
        [-4.5562e-04,  1.1406e-03, -8.6212e-04,  ..., -2.5806e-03,
          6.4898e-04, -5.1785e-04],
        ...,
        [ 3.0255e-04,  2.5439e-04,  8.2684e-04,  ..., -2.3155e-03,
          4.2915e-06, -1.1292e-03],
        [ 1.2169e-03, -1.3561e-03, -1.4400e-03,  ...,  1.9741e-04,
         -9.1910e-05, -4.2200e-04],
        [ 1.4315e-03, -1.9884e-04,  1.5411e-03,  ..., -2.0237e-03,
          2.0456e-04, -1.8024e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5547,  1.0703, -7.1250,  ..., -0.6357,  4.5078,  3.0020]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1941,  0.0737,  0.2573,  ...,  0.0459, -0.1681, -0.2123]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 09:59:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To grow again is to regrow
To consider again is to reconsider
To evaluate again is to reevaluate
To assure again is to reassure
To confirm again is to reconfirm
To occur again is to reoccur
To calculate again is to recalculate
To locate again is to
2024-07-10 09:59:42 root INFO     [order_1_approx] starting weight calculation for To calculate again is to recalculate
To evaluate again is to reevaluate
To confirm again is to reconfirm
To occur again is to reoccur
To consider again is to reconsider
To locate again is to relocate
To assure again is to reassure
To grow again is to
2024-07-10 09:59:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 10:04:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 10:08:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.3867, 0.0962, 0.7798,  ..., 0.3115, 0.7583, 0.0120], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3958,  0.0835,  0.7578,  ...,  0.3118,  0.7539, -0.0124],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0320, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7295,  1.0078, -4.7578,  ..., -0.7554,  3.8691, -3.3555],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0083, -0.0077, -0.0111,  ..., -0.0002, -0.0020, -0.0160],
        [-0.0140, -0.0023,  0.0194,  ...,  0.0017, -0.0018,  0.0106],
        [-0.0002,  0.0009, -0.0190,  ..., -0.0082, -0.0022, -0.0036],
        ...,
        [ 0.0017, -0.0208, -0.0031,  ..., -0.0061, -0.0042, -0.0021],
        [ 0.0089, -0.0022, -0.0104,  ...,  0.0027,  0.0056, -0.0050],
        [ 0.0060,  0.0021,  0.0014,  ..., -0.0037,  0.0065,  0.0019]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-8.1301e-05, -5.6386e-05,  1.5640e-04,  ...,  6.3610e-04,
          1.1377e-03,  1.9693e-04],
        [ 1.7643e-04, -1.4963e-03,  1.2808e-03,  ...,  2.9564e-05,
         -8.4448e-04,  8.0442e-04],
        [-9.4461e-04, -8.7738e-04, -1.8854e-03,  ..., -1.5526e-03,
          1.2541e-03,  9.0981e-04],
        ...,
        [-2.3246e-04, -1.6317e-03, -1.1909e-04,  ..., -1.0481e-03,
         -4.6444e-04, -1.1253e-04],
        [ 2.3031e-04,  3.1137e-04, -6.7186e-04,  ...,  1.2274e-03,
          4.6968e-04, -2.5082e-03],
        [ 1.2741e-03, -1.0071e-03,  1.0967e-03,  ..., -7.2002e-04,
         -3.5620e-04,  9.2924e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4951, -0.1777, -3.8477,  ..., -0.0552,  4.4414, -3.7871]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1071,  0.1155,  0.2375,  ..., -0.0035, -0.3674,  0.0274]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 10:08:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To calculate again is to recalculate
To evaluate again is to reevaluate
To confirm again is to reconfirm
To occur again is to reoccur
To consider again is to reconsider
To locate again is to relocate
To assure again is to reassure
To grow again is to
2024-07-10 10:08:08 root INFO     [order_1_approx] starting weight calculation for To confirm again is to reconfirm
To evaluate again is to reevaluate
To locate again is to relocate
To calculate again is to recalculate
To assure again is to reassure
To occur again is to reoccur
To grow again is to regrow
To consider again is to
2024-07-10 10:08:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 10:12:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 10:16:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3945, -0.2676,  0.1437,  ..., -0.2047, -0.0576,  0.7896],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.4314, -0.3232,  0.1383,  ..., -0.2156, -0.0838,  0.8662],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0320, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7676,  3.8555, -4.4727,  ..., -0.2637, -1.3311,  0.8145],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0053, -0.0130,  0.0115,  ...,  0.0057,  0.0038, -0.0259],
        [-0.0379,  0.0213,  0.0253,  ...,  0.0004, -0.0148, -0.0064],
        [ 0.0345,  0.0034,  0.0012,  ...,  0.0069,  0.0081,  0.0205],
        ...,
        [-0.0015, -0.0338, -0.0076,  ...,  0.0019,  0.0025,  0.0115],
        [ 0.0190,  0.0006, -0.0153,  ..., -0.0034, -0.0014, -0.0014],
        [ 0.0153,  0.0162, -0.0273,  ...,  0.0038,  0.0118,  0.0059]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.6956e-03,  6.8283e-04, -3.6550e-04,  ...,  9.6703e-04,
          2.0218e-03, -4.3178e-04],
        [-7.1430e-04, -6.2990e-04,  6.7282e-04,  ...,  1.3781e-03,
         -1.1635e-03, -7.6485e-04],
        [-2.5120e-03, -2.4281e-03,  3.5191e-04,  ..., -1.0185e-03,
         -1.7195e-03,  4.6968e-04],
        ...,
        [ 4.6229e-04, -1.2913e-03, -1.0309e-03,  ...,  3.3331e-04,
          1.2684e-03, -1.7090e-03],
        [ 3.0708e-04, -3.5286e-05, -6.6519e-04,  ..., -4.0722e-04,
         -1.4400e-03, -1.6422e-03],
        [-1.6141e-04,  7.6652e-05, -1.6985e-03,  ..., -8.8549e-04,
         -9.2316e-04, -1.8692e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1562,  2.9023, -4.4453,  ..., -0.1926, -1.5859,  0.5903]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0790,  0.0740,  0.3882,  ..., -0.1993, -0.1686,  0.1833]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 10:16:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To confirm again is to reconfirm
To evaluate again is to reevaluate
To locate again is to relocate
To calculate again is to recalculate
To assure again is to reassure
To occur again is to reoccur
To grow again is to regrow
To consider again is to
2024-07-10 10:16:33 root INFO     [order_1_approx] starting weight calculation for To occur again is to reoccur
To locate again is to relocate
To confirm again is to reconfirm
To evaluate again is to reevaluate
To consider again is to reconsider
To grow again is to regrow
To calculate again is to recalculate
To assure again is to
2024-07-10 10:16:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 10:20:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 10:24:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8975, -0.5088, -0.4644,  ...,  0.1547, -0.7695, -0.2654],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.8833, -0.5283, -0.4458,  ...,  0.1466, -0.7607, -0.2900],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0320, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0391,  3.0234, -3.6836,  ..., -0.2393,  0.8779, -0.5293],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0049, -0.0240,  0.0058,  ...,  0.0001,  0.0026, -0.0073],
        [-0.0065, -0.0135,  0.0168,  ..., -0.0038, -0.0141,  0.0009],
        [ 0.0070,  0.0220, -0.0061,  ...,  0.0001,  0.0343,  0.0066],
        ...,
        [ 0.0021,  0.0021, -0.0103,  ..., -0.0099, -0.0115,  0.0006],
        [ 0.0164,  0.0195,  0.0025,  ..., -0.0179,  0.0149, -0.0005],
        [ 0.0013,  0.0113,  0.0010,  ..., -0.0006,  0.0271,  0.0136]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0010, -0.0008,  0.0003,  ..., -0.0013,  0.0020, -0.0004],
        [-0.0002, -0.0035,  0.0010,  ...,  0.0017, -0.0027, -0.0006],
        [-0.0012,  0.0034, -0.0011,  ...,  0.0003,  0.0006,  0.0039],
        ...,
        [ 0.0025,  0.0006,  0.0004,  ..., -0.0029, -0.0004,  0.0004],
        [-0.0008, -0.0025,  0.0004,  ..., -0.0022, -0.0020,  0.0005],
        [-0.0003,  0.0008,  0.0003,  ..., -0.0033,  0.0012,  0.0004]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4551,  2.2070, -3.1172,  ..., -1.0166,  2.9297,  0.1924]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[0.3577, 0.0177, 0.4260,  ..., 0.3455, 0.0522, 0.0366]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 10:24:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To occur again is to reoccur
To locate again is to relocate
To confirm again is to reconfirm
To evaluate again is to reevaluate
To consider again is to reconsider
To grow again is to regrow
To calculate again is to recalculate
To assure again is to
2024-07-10 10:24:58 root INFO     [order_1_approx] starting weight calculation for To calculate again is to recalculate
To assure again is to reassure
To consider again is to reconsider
To locate again is to relocate
To grow again is to regrow
To confirm again is to reconfirm
To occur again is to reoccur
To evaluate again is to
2024-07-10 10:24:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 10:29:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 10:33:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6191, -0.6812,  0.2429,  ..., -0.7480, -0.2505,  0.9668],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.6519, -0.7466,  0.2273,  ..., -0.7451, -0.2783,  1.0000],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0320, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.6289,  4.6406, -3.7695,  ...,  0.9531, -2.3223,  0.6924],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0223, -0.0218, -0.0030,  ...,  0.0038,  0.0103, -0.0081],
        [-0.0106,  0.0151,  0.0075,  ..., -0.0106, -0.0016, -0.0057],
        [ 0.0087,  0.0068, -0.0055,  ...,  0.0102,  0.0154,  0.0089],
        ...,
        [-0.0045, -0.0121, -0.0099,  ..., -0.0007,  0.0043,  0.0050],
        [ 0.0185, -0.0108,  0.0092,  ...,  0.0081,  0.0239, -0.0193],
        [ 0.0057,  0.0135, -0.0051,  ...,  0.0103,  0.0113,  0.0009]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-3.0136e-04, -2.0695e-03, -1.3947e-04,  ..., -1.4961e-04,
          1.6518e-03,  5.4359e-04],
        [ 1.1909e-04, -1.5688e-03,  8.3971e-04,  ...,  9.8944e-06,
         -5.6744e-04,  1.4143e-03],
        [-1.2283e-03,  2.0027e-03, -1.9455e-03,  ..., -1.3390e-03,
         -1.6041e-03, -3.0041e-05],
        ...,
        [ 3.0994e-06,  4.3964e-04, -1.1864e-03,  ..., -2.3708e-03,
         -1.0653e-03, -8.1635e-04],
        [-1.2197e-03,  3.4881e-04,  1.1921e-03,  ..., -9.3269e-04,
         -1.0777e-04, -1.8244e-03],
        [-1.7748e-03,  7.8821e-04, -1.5907e-03,  ..., -5.7697e-04,
         -1.9331e-03,  1.0681e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.7734,  3.2402, -3.9277,  ...,  2.0312, -0.0664,  0.2139]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0277,  0.4167,  0.3306,  ..., -0.0927, -0.3840,  0.2352]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 10:33:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To calculate again is to recalculate
To assure again is to reassure
To consider again is to reconsider
To locate again is to relocate
To grow again is to regrow
To confirm again is to reconfirm
To occur again is to reoccur
To evaluate again is to
2024-07-10 10:33:21 root INFO     [order_1_approx] starting weight calculation for To confirm again is to reconfirm
To consider again is to reconsider
To assure again is to reassure
To calculate again is to recalculate
To evaluate again is to reevaluate
To locate again is to relocate
To grow again is to regrow
To occur again is to
2024-07-10 10:33:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 10:37:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 10:41:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2209,  0.0471,  0.9131,  ..., -0.4253,  0.9932, -0.4631],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.2263,  0.0303,  0.8608,  ..., -0.4053,  0.9609, -0.4978],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0320, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6851, -2.5312, -6.4023,  ...,  0.4570,  1.2715,  0.7559],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0010, -0.0184, -0.0066,  ...,  0.0101, -0.0036, -0.0144],
        [ 0.0053,  0.0017,  0.0046,  ..., -0.0137,  0.0019, -0.0166],
        [ 0.0077, -0.0043, -0.0065,  ..., -0.0173,  0.0175, -0.0098],
        ...,
        [-0.0066, -0.0166, -0.0026,  ..., -0.0102,  0.0029, -0.0008],
        [ 0.0189,  0.0085, -0.0018,  ...,  0.0013,  0.0088, -0.0145],
        [-0.0117,  0.0097,  0.0044,  ...,  0.0023, -0.0046,  0.0100]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-9.1076e-04, -3.4142e-03, -2.6417e-04,  ..., -7.4100e-04,
          6.7616e-04, -9.8228e-04],
        [-8.0490e-04, -1.4925e-03, -2.4533e-04,  ..., -1.7357e-03,
         -4.9925e-04,  4.5955e-05],
        [-3.2387e-03, -8.4400e-05, -5.2452e-06,  ..., -2.1400e-03,
         -6.4898e-04, -3.9215e-03],
        ...,
        [ 1.1272e-03,  3.4189e-04,  1.3113e-05,  ..., -7.4577e-04,
         -1.5092e-04, -1.2465e-03],
        [ 5.2691e-05, -1.9407e-04,  4.3511e-05,  ...,  1.3199e-03,
         -9.5129e-05, -9.2030e-04],
        [-2.5296e-04, -6.5041e-04, -4.9496e-04,  ..., -3.0661e-04,
         -6.5851e-04,  3.7432e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.4961, -2.5156, -6.5430,  ...,  0.6074,  1.5713,  0.9067]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2041,  0.2448,  0.2094,  ..., -0.0019, -0.1555, -0.0408]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 10:41:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To confirm again is to reconfirm
To consider again is to reconsider
To assure again is to reassure
To calculate again is to recalculate
To evaluate again is to reevaluate
To locate again is to relocate
To grow again is to regrow
To occur again is to
2024-07-10 10:41:48 root INFO     [order_1_approx] starting weight calculation for To occur again is to reoccur
To evaluate again is to reevaluate
To locate again is to relocate
To calculate again is to recalculate
To assure again is to reassure
To consider again is to reconsider
To grow again is to regrow
To confirm again is to
2024-07-10 10:41:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 10:46:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 10:50:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1582, -0.7612,  0.4709,  ...,  0.5737,  0.2864,  0.4238],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-1.1650, -0.7988,  0.4353,  ...,  0.5508,  0.2610,  0.4058],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0320, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6582,  1.3096, -4.3594,  ...,  0.2241, -1.8262,  1.7246],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0094, -0.0505, -0.0005,  ...,  0.0052,  0.0071, -0.0478],
        [-0.0293,  0.0247,  0.0173,  ..., -0.0155, -0.0085,  0.0009],
        [ 0.0388,  0.0153, -0.0252,  ...,  0.0148,  0.0283, -0.0036],
        ...,
        [-0.0399, -0.0103,  0.0031,  ..., -0.0038, -0.0224,  0.0290],
        [ 0.0218, -0.0040, -0.0163,  ..., -0.0055, -0.0045, -0.0266],
        [ 0.0184,  0.0258, -0.0056,  ...,  0.0124,  0.0168,  0.0119]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-4.5471e-03, -3.7460e-03,  1.1292e-03,  ...,  1.0729e-04,
          1.0023e-03, -6.9714e-04],
        [-3.3245e-03,  9.2411e-04, -9.7275e-05,  ..., -8.6498e-04,
          6.1226e-04, -8.5020e-04],
        [-5.1880e-04,  2.5196e-03, -2.2087e-03,  ..., -3.1233e-04,
          7.9346e-04,  1.1749e-03],
        ...,
        [ 1.9217e-04,  5.3930e-04, -1.6975e-03,  ...,  1.4210e-03,
         -5.6696e-04,  1.0538e-03],
        [ 3.0098e-03, -2.4490e-03,  8.7214e-04,  ..., -1.5926e-03,
         -3.9177e-03,  1.6537e-03],
        [ 1.8711e-03,  1.9836e-04,  1.6050e-03,  ..., -1.1587e-03,
         -3.1948e-04,  7.0429e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1611,  1.2354, -4.2266,  ..., -0.1882, -2.0059,  2.3438]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.4177,  0.3489,  0.2025,  ...,  0.0826, -0.0807, -0.0323]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 10:50:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To occur again is to reoccur
To evaluate again is to reevaluate
To locate again is to relocate
To calculate again is to recalculate
To assure again is to reassure
To consider again is to reconsider
To grow again is to regrow
To confirm again is to
2024-07-10 10:50:11 root INFO     [order_1_approx] starting weight calculation for To evaluate again is to reevaluate
To consider again is to reconsider
To confirm again is to reconfirm
To occur again is to reoccur
To grow again is to regrow
To assure again is to reassure
To locate again is to relocate
To calculate again is to
2024-07-10 10:50:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 10:54:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 10:58:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1240,  0.4397, -0.5708,  ..., -0.2896, -0.2104,  0.5972],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.1181,  0.4346, -0.5649,  ..., -0.2798, -0.2317,  0.5884],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0320, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.4688,  3.8672, -4.8086,  ..., -4.6133,  1.3262, -0.2344],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0167, -0.0066,  0.0149,  ..., -0.0006,  0.0052, -0.0017],
        [-0.0276, -0.0081,  0.0113,  ...,  0.0117, -0.0089, -0.0068],
        [ 0.0262, -0.0058,  0.0023,  ...,  0.0096,  0.0166,  0.0082],
        ...,
        [ 0.0154, -0.0131, -0.0102,  ..., -0.0220, -0.0046,  0.0167],
        [ 0.0237,  0.0132,  0.0072,  ..., -0.0076,  0.0062,  0.0106],
        [ 0.0008,  0.0128, -0.0040,  ..., -0.0045,  0.0325,  0.0015]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-5.0163e-03, -2.1706e-03, -3.8218e-04,  ..., -5.9605e-04,
          2.9254e-04,  7.0524e-04],
        [-6.3896e-04, -2.1434e-04,  1.2054e-03,  ..., -1.6317e-03,
         -1.1358e-03,  1.0700e-03],
        [-7.2336e-04,  1.7166e-03, -6.0558e-04,  ...,  4.0388e-04,
         -9.6989e-04, -6.7949e-05],
        ...,
        [ 2.1420e-03, -1.3313e-03, -2.6340e-03,  ..., -8.3733e-04,
         -2.2697e-03,  2.6894e-04],
        [-2.5034e-05,  2.0847e-03, -1.7900e-03,  ...,  1.8454e-04,
         -2.9659e-04, -1.4420e-03],
        [ 2.4319e-03, -5.4359e-04,  7.5698e-05,  ..., -3.5238e-04,
          7.5531e-04,  5.5027e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.1572,  2.9805, -4.3945,  ..., -4.2812,  2.4727,  0.1934]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.3604,  0.3176,  0.0322,  ..., -0.0216, -0.2520, -0.0410]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 10:58:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To evaluate again is to reevaluate
To consider again is to reconsider
To confirm again is to reconfirm
To occur again is to reoccur
To grow again is to regrow
To assure again is to reassure
To locate again is to relocate
To calculate again is to
2024-07-10 10:58:31 root INFO     total operator prediction time: 4036.4058215618134 seconds
2024-07-10 10:58:31 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on adj+ness_reg
2024-07-10 10:58:34 root INFO     building operator adj+ness_reg
2024-07-10 10:58:34 root INFO     [order_1_approx] starting weight calculation for The state of being sacred is sacredness
The state of being nice is niceness
The state of being serious is seriousness
The state of being competitive is competitiveness
The state of being interesting is interestingness
The state of being righteous is righteousness
The state of being distinctive is distinctiveness
The state of being sad is
2024-07-10 10:58:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 11:02:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 11:07:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3809, -0.5430, -0.6313,  ...,  1.0293,  1.0605,  0.5615],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 1.2266, -0.5137, -0.5488,  ...,  0.8818,  0.9116,  0.4856],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6279,  3.0977, -2.2930,  ...,  0.1201,  0.3633,  5.2344],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-7.8354e-03, -9.9087e-04,  3.0537e-03,  ...,  9.8724e-03,
          5.1231e-03, -1.9409e-02],
        [ 1.8097e-02, -6.8665e-04, -3.9291e-03,  ..., -7.6866e-04,
         -2.9419e-02,  6.8626e-03],
        [-8.9340e-03, -3.2349e-03, -1.2177e-02,  ..., -1.8631e-02,
          8.5144e-03, -7.8201e-05],
        ...,
        [-7.5378e-03, -9.4452e-03,  3.6201e-03,  ..., -2.6947e-02,
         -6.4201e-03, -2.8915e-03],
        [-1.8280e-02, -4.4174e-03,  5.3978e-03,  ..., -9.0866e-03,
         -2.4872e-02, -5.0392e-03],
        [-2.5482e-03,  4.3755e-03,  6.0463e-03,  ..., -4.5586e-04,
         -6.3286e-03, -8.9035e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0001,  0.0006, -0.0007,  ..., -0.0009,  0.0002,  0.0008],
        [-0.0020,  0.0015,  0.0002,  ...,  0.0007, -0.0020,  0.0011],
        [ 0.0006, -0.0003, -0.0013,  ..., -0.0023,  0.0003,  0.0007],
        ...,
        [-0.0006, -0.0002, -0.0020,  ..., -0.0002,  0.0010,  0.0009],
        [ 0.0014, -0.0020, -0.0008,  ..., -0.0007, -0.0017, -0.0008],
        [ 0.0019,  0.0007,  0.0004,  ..., -0.0014, -0.0013,  0.0005]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5840,  1.3516, -2.1055,  ..., -0.3066,  1.3770,  5.2266]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0016,  0.0209,  0.2803,  ..., -0.1675,  0.0146,  0.1197]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 11:07:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being sacred is sacredness
The state of being nice is niceness
The state of being serious is seriousness
The state of being competitive is competitiveness
The state of being interesting is interestingness
The state of being righteous is righteousness
The state of being distinctive is distinctiveness
The state of being sad is
2024-07-10 11:07:03 root INFO     [order_1_approx] starting weight calculation for The state of being sacred is sacredness
The state of being interesting is interestingness
The state of being competitive is competitiveness
The state of being nice is niceness
The state of being distinctive is distinctiveness
The state of being righteous is righteousness
The state of being sad is sadness
The state of being serious is
2024-07-10 11:07:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 11:11:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 11:15:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7871,  0.3523,  1.0703,  ..., -0.2764,  1.0928,  0.5518],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.7153,  0.3110,  0.9214,  ..., -0.2407,  0.9648,  0.4893],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2070,  3.5273,  3.6934,  ..., -0.4194, -1.4893,  1.0488],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0214, -0.0144,  0.0163,  ..., -0.0130, -0.0124, -0.0068],
        [-0.0176, -0.0058, -0.0038,  ...,  0.0098, -0.0265,  0.0041],
        [ 0.0065,  0.0033, -0.0130,  ...,  0.0115, -0.0006,  0.0114],
        ...,
        [-0.0164, -0.0168,  0.0107,  ..., -0.0164,  0.0051,  0.0112],
        [ 0.0103, -0.0041,  0.0099,  ..., -0.0085, -0.0038, -0.0258],
        [-0.0155,  0.0102, -0.0028,  ..., -0.0097,  0.0239, -0.0098]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0010, -0.0034,  0.0015,  ...,  0.0011, -0.0007,  0.0006],
        [-0.0016, -0.0002, -0.0014,  ...,  0.0035,  0.0007,  0.0006],
        [-0.0009,  0.0006, -0.0011,  ...,  0.0003, -0.0014,  0.0005],
        ...,
        [ 0.0014, -0.0014,  0.0001,  ..., -0.0004, -0.0027, -0.0004],
        [ 0.0012, -0.0012,  0.0009,  ..., -0.0021, -0.0028,  0.0003],
        [-0.0012,  0.0015, -0.0012,  ...,  0.0028,  0.0012, -0.0009]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1113,  1.1543,  3.9258,  ..., -1.5605,  1.0713,  1.1455]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1417,  0.4285,  0.2639,  ..., -0.0788, -0.2378,  0.1421]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 11:15:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being sacred is sacredness
The state of being interesting is interestingness
The state of being competitive is competitiveness
The state of being nice is niceness
The state of being distinctive is distinctiveness
The state of being righteous is righteousness
The state of being sad is sadness
The state of being serious is
2024-07-10 11:15:31 root INFO     [order_1_approx] starting weight calculation for The state of being nice is niceness
The state of being serious is seriousness
The state of being interesting is interestingness
The state of being righteous is righteousness
The state of being distinctive is distinctiveness
The state of being sad is sadness
The state of being sacred is sacredness
The state of being competitive is
2024-07-10 11:15:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 11:19:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 11:23:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1388, -0.0117,  1.2324,  ...,  0.0610,  1.1592,  0.2310],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1375, -0.0336,  1.0693,  ...,  0.0529,  1.0303,  0.1885],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-5.3359, -1.2256, -1.5938,  ...,  1.4824,  2.9023,  4.0703],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0116, -0.0190,  0.0124,  ..., -0.0112,  0.0085,  0.0004],
        [-0.0094, -0.0071, -0.0046,  ..., -0.0076, -0.0037, -0.0008],
        [-0.0010, -0.0045, -0.0089,  ..., -0.0015,  0.0050,  0.0040],
        ...,
        [-0.0046, -0.0031,  0.0070,  ..., -0.0122,  0.0062,  0.0031],
        [ 0.0104, -0.0029, -0.0080,  ..., -0.0085, -0.0197,  0.0018],
        [-0.0066,  0.0155, -0.0085,  ...,  0.0087,  0.0060, -0.0023]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-8.1921e-04, -3.1400e-04,  3.1376e-04,  ..., -6.8855e-04,
          5.0354e-04,  8.9407e-05],
        [-1.8215e-03,  2.6226e-04, -1.5602e-03,  ..., -8.1682e-04,
         -1.2875e-04,  9.5844e-04],
        [ 1.0242e-03, -1.0185e-03, -2.4738e-03,  ..., -7.6389e-04,
          8.9931e-04, -3.8433e-04],
        ...,
        [ 1.4763e-03, -7.0286e-04, -1.0414e-03,  ..., -2.6608e-04,
          2.6202e-04,  4.8447e-04],
        [ 9.0742e-04,  2.2354e-03, -1.0128e-03,  ...,  6.2561e-04,
         -2.6894e-03,  8.8096e-05],
        [ 4.0388e-04,  7.2098e-04, -5.7316e-04,  ...,  8.2016e-04,
         -5.9843e-04, -9.7513e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.1016, -1.5107, -1.3662,  ...,  1.6943,  3.9258,  2.5938]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0624,  0.2327,  0.4656,  ..., -0.1426, -0.2017,  0.0269]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 11:23:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being nice is niceness
The state of being serious is seriousness
The state of being interesting is interestingness
The state of being righteous is righteousness
The state of being distinctive is distinctiveness
The state of being sad is sadness
The state of being sacred is sacredness
The state of being competitive is
2024-07-10 11:23:58 root INFO     [order_1_approx] starting weight calculation for The state of being competitive is competitiveness
The state of being nice is niceness
The state of being interesting is interestingness
The state of being sacred is sacredness
The state of being sad is sadness
The state of being serious is seriousness
The state of being righteous is righteousness
The state of being distinctive is
2024-07-10 11:23:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 11:28:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 11:32:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1587,  0.4827,  0.3384,  ...,  1.3750,  1.4707, -0.2949],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.1337,  0.4241,  0.2749,  ...,  1.1904,  1.2871, -0.3025],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.5352,  0.5098,  1.0000,  ..., -4.1914, -0.3677,  2.2090],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0098,  0.0035, -0.0127,  ..., -0.0132, -0.0068, -0.0067],
        [-0.0144, -0.0177,  0.0105,  ..., -0.0046,  0.0075, -0.0004],
        [ 0.0159, -0.0066, -0.0328,  ...,  0.0019, -0.0004, -0.0028],
        ...,
        [-0.0176, -0.0064,  0.0020,  ..., -0.0224,  0.0005, -0.0028],
        [ 0.0165,  0.0140,  0.0035,  ...,  0.0024, -0.0323, -0.0071],
        [-0.0227,  0.0036,  0.0020,  ...,  0.0061, -0.0048, -0.0045]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 1.6093e-04, -5.8937e-04,  1.0290e-03,  ..., -2.4109e-03,
         -2.6169e-03,  2.1057e-03],
        [-1.1959e-03, -1.2636e-03, -7.8917e-04,  ..., -2.6155e-04,
          1.3313e-03, -7.0524e-04],
        [ 3.0231e-04,  6.4468e-04, -5.3024e-04,  ..., -7.6151e-04,
          5.3215e-04,  9.8419e-04],
        ...,
        [ 8.9502e-04,  1.0071e-03, -1.2136e-04,  ..., -2.2640e-03,
         -1.1539e-03, -7.1573e-04],
        [ 1.1816e-03, -6.7770e-05,  9.8705e-05,  ...,  1.1244e-03,
         -4.7779e-04, -2.3499e-03],
        [-6.9094e-04, -2.0351e-03, -2.6369e-04,  ...,  1.5521e-04,
          1.5421e-03,  8.1253e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.0977, -0.1479,  0.6792,  ..., -4.2969,  0.0906,  2.6621]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0145,  0.1421,  0.0949,  ...,  0.1731, -0.0448,  0.1105]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 11:32:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being competitive is competitiveness
The state of being nice is niceness
The state of being interesting is interestingness
The state of being sacred is sacredness
The state of being sad is sadness
The state of being serious is seriousness
The state of being righteous is righteousness
The state of being distinctive is
2024-07-10 11:32:28 root INFO     [order_1_approx] starting weight calculation for The state of being sacred is sacredness
The state of being nice is niceness
The state of being competitive is competitiveness
The state of being distinctive is distinctiveness
The state of being sad is sadness
The state of being serious is seriousness
The state of being interesting is interestingness
The state of being righteous is
2024-07-10 11:32:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 11:36:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 11:40:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6025, -0.2617,  0.6494,  ...,  0.4253,  1.3584, -0.3098],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.5342, -0.2654,  0.5415,  ...,  0.3657,  1.1826, -0.3149],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1748,  3.8945, -3.3711,  ...,  1.1621, -0.3271,  7.4062],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-8.8501e-03, -5.6229e-03,  7.7438e-03,  ..., -1.4793e-02,
         -7.3318e-03, -1.1307e-02],
        [ 4.1924e-03,  5.2071e-03,  4.7913e-03,  ...,  1.9255e-03,
         -3.0136e-04,  9.7580e-03],
        [-8.6060e-03, -1.1353e-02, -2.8412e-02,  ...,  3.1311e-02,
          1.3535e-02, -1.2466e-02],
        ...,
        [-7.4921e-03,  3.6583e-03, -3.2673e-03,  ..., -1.9958e-02,
          1.0330e-02, -1.0422e-02],
        [-8.8215e-05, -5.6725e-03, -9.9182e-04,  ..., -7.9956e-03,
         -2.7740e-02, -6.9351e-03],
        [-1.2283e-02,  1.6663e-02,  1.2100e-02,  ...,  1.4381e-03,
         -5.8670e-03, -1.5465e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.4529e-03, -3.1614e-04,  7.3671e-04,  ...,  9.1505e-04,
          4.4680e-04,  3.7956e-04],
        [-2.2469e-03, -1.5039e-03,  8.2111e-04,  ...,  7.8011e-04,
          1.7986e-03,  5.5122e-04],
        [-2.3842e-05,  1.5669e-03, -5.9605e-04,  ..., -2.2554e-04,
         -4.5419e-04, -3.2139e-04],
        ...,
        [ 6.9475e-04, -7.6818e-04, -4.0865e-04,  ...,  3.6573e-04,
         -1.2989e-03, -8.6164e-04],
        [-2.9707e-04,  8.5068e-04,  1.7586e-03,  ...,  2.6226e-05,
         -2.1725e-03,  1.4086e-03],
        [-5.5647e-04,  1.1358e-03, -8.5354e-05,  ...,  2.3155e-03,
         -2.0003e-04, -1.1082e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6689,  3.0664, -2.1113,  ...,  0.1201, -0.0989,  6.0039]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.3674, -0.0815,  0.2727,  ...,  0.2732, -0.2261,  0.1569]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 11:40:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being sacred is sacredness
The state of being nice is niceness
The state of being competitive is competitiveness
The state of being distinctive is distinctiveness
The state of being sad is sadness
The state of being serious is seriousness
The state of being interesting is interestingness
The state of being righteous is
2024-07-10 11:40:56 root INFO     [order_1_approx] starting weight calculation for The state of being interesting is interestingness
The state of being sad is sadness
The state of being distinctive is distinctiveness
The state of being competitive is competitiveness
The state of being serious is seriousness
The state of being nice is niceness
The state of being righteous is righteousness
The state of being sacred is
2024-07-10 11:40:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 11:45:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 11:49:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0757, -1.9053,  1.0586,  ...,  0.1121,  0.4282, -0.1733],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.0760, -1.7920,  0.9019,  ...,  0.0981,  0.3608, -0.1876],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.8857,  7.3516,  1.8945,  ..., -3.6680, -0.4153,  5.9258],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0045, -0.0150,  0.0092,  ...,  0.0002,  0.0051, -0.0128],
        [-0.0257, -0.0388,  0.0132,  ..., -0.0015, -0.0307,  0.0271],
        [-0.0009, -0.0070, -0.0214,  ...,  0.0009, -0.0074, -0.0025],
        ...,
        [ 0.0052, -0.0161,  0.0037,  ..., -0.0215,  0.0142, -0.0348],
        [-0.0008, -0.0081, -0.0025,  ..., -0.0061, -0.0106, -0.0089],
        [-0.0262, -0.0072,  0.0033,  ..., -0.0262, -0.0029,  0.0038]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.6546e-04, -2.1648e-03,  2.2149e-04,  ...,  1.6718e-03,
         -8.3971e-04,  1.7900e-03],
        [-6.0883e-03, -1.7700e-03,  7.9536e-04,  ..., -1.5640e-04,
          3.0060e-03,  3.1261e-03],
        [ 2.0580e-03, -7.7915e-04, -1.9646e-03,  ..., -1.2522e-03,
         -6.9475e-04,  3.4630e-05],
        ...,
        [ 3.9597e-03, -9.7609e-04,  1.7357e-03,  ...,  1.2112e-03,
          1.5473e-04, -1.3027e-03],
        [-2.2173e-04, -2.2964e-03,  1.4267e-03,  ...,  1.9045e-03,
         -9.5034e-04,  1.4257e-04],
        [-3.0079e-03, -1.0920e-03,  1.9252e-04,  ..., -1.3523e-03,
         -6.5088e-05,  1.1330e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9517,  5.8594,  1.7705,  ..., -3.5840, -0.1927,  5.1562]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.2781,  0.4080,  0.0570,  ..., -0.0790, -0.1488,  0.1040]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 11:49:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being interesting is interestingness
The state of being sad is sadness
The state of being distinctive is distinctiveness
The state of being competitive is competitiveness
The state of being serious is seriousness
The state of being nice is niceness
The state of being righteous is righteousness
The state of being sacred is
2024-07-10 11:49:24 root INFO     [order_1_approx] starting weight calculation for The state of being sad is sadness
The state of being righteous is righteousness
The state of being competitive is competitiveness
The state of being nice is niceness
The state of being distinctive is distinctiveness
The state of being serious is seriousness
The state of being sacred is sacredness
The state of being interesting is
2024-07-10 11:49:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 11:53:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 11:57:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3628, -0.4602,  0.5356,  ...,  0.8203,  0.5503,  0.1328],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3276, -0.4575,  0.4570,  ...,  0.7266,  0.4785,  0.0973],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([0.3384, 3.1641, 0.9092,  ..., 2.2559, 0.5557, 7.4180], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0192, -0.0154, -0.0081,  ..., -0.0117, -0.0042, -0.0240],
        [-0.0089, -0.0057,  0.0148,  ..., -0.0033, -0.0215,  0.0081],
        [ 0.0028, -0.0033, -0.0164,  ...,  0.0078, -0.0080, -0.0032],
        ...,
        [-0.0120,  0.0156,  0.0071,  ..., -0.0081, -0.0083, -0.0071],
        [ 0.0041, -0.0129, -0.0071,  ..., -0.0013, -0.0167, -0.0136],
        [-0.0218,  0.0181, -0.0064,  ..., -0.0209, -0.0147,  0.0134]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.1396e-03, -4.6158e-04,  6.6757e-06,  ..., -2.7618e-03,
         -2.2626e-04,  2.2202e-03],
        [ 1.4293e-04, -8.4782e-04, -5.6458e-04,  ...,  6.7532e-05,
         -2.6941e-05,  2.1381e-03],
        [ 5.9509e-04,  1.6117e-03, -3.3112e-03,  ...,  9.8038e-04,
         -1.2560e-03,  1.1253e-03],
        ...,
        [ 1.3599e-03, -2.1935e-05, -4.0507e-04,  ..., -6.5684e-05,
         -5.1165e-04, -2.0256e-03],
        [-2.3270e-03,  1.6367e-04,  1.7776e-03,  ...,  6.2752e-04,
         -8.1444e-04,  2.9068e-03],
        [-2.3293e-04, -6.0177e-04, -1.9016e-03,  ...,  1.1959e-03,
         -1.6899e-03, -2.9874e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0449,  2.0039,  0.8188,  ...,  2.2324,  2.3086,  5.1328]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1023,  0.3035,  0.3420,  ...,  0.1534, -0.0324,  0.0257]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 11:57:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being sad is sadness
The state of being righteous is righteousness
The state of being competitive is competitiveness
The state of being nice is niceness
The state of being distinctive is distinctiveness
The state of being serious is seriousness
The state of being sacred is sacredness
The state of being interesting is
2024-07-10 11:57:53 root INFO     [order_1_approx] starting weight calculation for The state of being distinctive is distinctiveness
The state of being righteous is righteousness
The state of being sacred is sacredness
The state of being sad is sadness
The state of being serious is seriousness
The state of being competitive is competitiveness
The state of being interesting is interestingness
The state of being nice is
2024-07-10 11:57:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 12:02:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 12:06:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0035, -1.2988, -0.1682,  ..., -0.0723,  0.8931,  0.8550],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.0015, -1.3184, -0.1691,  ..., -0.0648,  0.8403,  0.8276],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.4492, -0.6211, -1.2070,  ...,  1.9697, -1.9346,  4.6406],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0087, -0.0144,  0.0064,  ...,  0.0141,  0.0015, -0.0330],
        [-0.0111, -0.0284, -0.0103,  ...,  0.0081, -0.0129,  0.0109],
        [ 0.0127, -0.0107, -0.0098,  ..., -0.0023,  0.0054,  0.0154],
        ...,
        [-0.0046, -0.0063,  0.0095,  ..., -0.0016, -0.0043, -0.0010],
        [ 0.0078,  0.0058, -0.0061,  ..., -0.0119, -0.0144, -0.0083],
        [-0.0191,  0.0176,  0.0116,  ..., -0.0138, -0.0078,  0.0012]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-5.7411e-04,  3.1114e-04,  3.7079e-03,  ..., -2.3041e-03,
         -3.2482e-03,  2.3346e-03],
        [-4.8494e-04, -2.9602e-03, -1.9608e-03,  ...,  2.4414e-03,
          2.2163e-03,  2.4796e-04],
        [ 4.9686e-04,  4.8184e-04, -1.9026e-03,  ..., -1.4486e-03,
          1.3485e-03,  4.6992e-04],
        ...,
        [-3.3808e-04, -4.3106e-04, -1.1301e-04,  ...,  1.5345e-03,
         -1.0853e-03, -8.1825e-04],
        [-4.8351e-04,  5.7888e-04, -5.9748e-04,  ...,  1.7786e-04,
         -4.5815e-03,  1.1654e-03],
        [-4.7183e-04, -3.9005e-04, -3.8872e-03,  ...,  1.6766e-03,
         -1.1330e-03,  1.8597e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.2109, -1.5244, -1.1436,  ...,  1.2129, -0.6025,  3.9785]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1982,  0.2434,  0.2834,  ..., -0.1010,  0.1538, -0.0064]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 12:06:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being distinctive is distinctiveness
The state of being righteous is righteousness
The state of being sacred is sacredness
The state of being sad is sadness
The state of being serious is seriousness
The state of being competitive is competitiveness
The state of being interesting is interestingness
The state of being nice is
2024-07-10 12:06:22 root INFO     total operator prediction time: 4068.000171661377 seconds
2024-07-10 12:06:22 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on noun+less_reg
2024-07-10 12:06:24 root INFO     building operator noun+less_reg
2024-07-10 12:06:24 root INFO     [order_1_approx] starting weight calculation for Something without window is windowless
Something without sleeve is sleeveless
Something without emotion is emotionless
Something without sensor is sensorless
Something without effort is effortless
Something without tact is tactless
Something without talent is talentless
Something without thought is
2024-07-10 12:06:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 12:10:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 12:14:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7148,  0.0235,  0.0569,  ..., -0.6348,  0.8838,  0.0931],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.6362,  0.0019,  0.0351,  ..., -0.5449,  0.7612,  0.0591],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0151, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2349,  0.0273, -0.6753,  ...,  1.4688,  2.0020, -1.9414],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0037, -0.0121,  0.0014,  ...,  0.0106, -0.0169, -0.0207],
        [-0.0182, -0.0211,  0.0058,  ..., -0.0093,  0.0021, -0.0120],
        [ 0.0019, -0.0056, -0.0131,  ...,  0.0011,  0.0003,  0.0058],
        ...,
        [-0.0011, -0.0015, -0.0159,  ..., -0.0321, -0.0085, -0.0035],
        [ 0.0150, -0.0087,  0.0185,  ..., -0.0006, -0.0323,  0.0071],
        [-0.0038,  0.0037, -0.0043,  ..., -0.0012,  0.0132, -0.0313]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 5.9128e-05, -2.6360e-03,  8.6880e-04,  ...,  2.6436e-03,
          2.0752e-03, -5.8365e-04],
        [-2.3901e-04, -1.0929e-03,  1.6785e-04,  ..., -3.9983e-04,
          6.4898e-04,  1.3237e-03],
        [-2.3956e-03, -2.8343e-03, -1.3685e-03,  ...,  1.2989e-03,
          9.0408e-04,  9.3269e-04],
        ...,
        [ 7.1335e-04, -8.1730e-04,  1.1015e-03,  ..., -3.1376e-04,
         -7.3195e-04, -8.2684e-04],
        [ 1.3208e-03,  9.1076e-04, -6.9904e-04,  ..., -1.6432e-03,
         -8.3160e-04, -1.5526e-03],
        [-4.2391e-04,  1.0605e-03, -1.4772e-03,  ..., -1.0319e-03,
         -1.8530e-03,  8.7166e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1475,  0.5449, -0.3435,  ...,  2.1543,  2.1758, -1.7148]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0620,  0.1127,  0.0263,  ...,  0.0522, -0.1267,  0.1007]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 12:14:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without window is windowless
Something without sleeve is sleeveless
Something without emotion is emotionless
Something without sensor is sensorless
Something without effort is effortless
Something without tact is tactless
Something without talent is talentless
Something without thought is
2024-07-10 12:14:51 root INFO     [order_1_approx] starting weight calculation for Something without window is windowless
Something without sleeve is sleeveless
Something without talent is talentless
Something without tact is tactless
Something without thought is thoughtless
Something without effort is effortless
Something without sensor is sensorless
Something without emotion is
2024-07-10 12:14:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 12:19:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 12:23:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9150, -0.5288, -0.4116,  ..., -1.4580,  0.5283,  1.0879],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.7803, -0.4863, -0.3521,  ..., -1.2002,  0.4255,  0.9268],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0151, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3848, -2.1270, -0.3535,  ..., -2.9785,  4.5664,  3.1973],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0079, -0.0212,  0.0131,  ...,  0.0068, -0.0016, -0.0049],
        [-0.0107, -0.0242,  0.0077,  ...,  0.0035,  0.0078, -0.0072],
        [ 0.0066,  0.0151, -0.0299,  ...,  0.0130,  0.0098,  0.0029],
        ...,
        [ 0.0072, -0.0050, -0.0135,  ..., -0.0208,  0.0129, -0.0101],
        [-0.0025, -0.0148,  0.0134,  ..., -0.0044, -0.0419,  0.0097],
        [-0.0089,  0.0156,  0.0026,  ..., -0.0055, -0.0004, -0.0336]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 1.7262e-04, -1.6594e-03,  2.0332e-03,  ..., -1.2970e-04,
          1.8272e-03,  1.7757e-03],
        [ 2.0707e-04,  6.1989e-04, -4.2081e-04,  ..., -2.5511e-04,
         -2.8563e-04,  1.0719e-03],
        [-9.6035e-04, -9.7084e-04, -1.9226e-03,  ...,  7.6675e-04,
         -5.9938e-04,  8.5831e-06],
        ...,
        [ 8.8882e-04,  9.3365e-04,  1.6270e-03,  ..., -5.7459e-04,
          4.5967e-04, -1.5144e-03],
        [-1.2054e-03, -6.0225e-04, -7.9060e-04,  ..., -3.9864e-04,
         -2.2926e-03,  6.0320e-04],
        [ 1.9312e-04, -1.4105e-03, -1.4467e-03,  ...,  1.3218e-03,
         -2.6608e-03,  2.1648e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9473, -2.1172, -0.8257,  ..., -1.4580,  4.4883,  2.2559]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0363,  0.0309,  0.1969,  ..., -0.2866, -0.0762,  0.0351]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 12:23:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without window is windowless
Something without sleeve is sleeveless
Something without talent is talentless
Something without tact is tactless
Something without thought is thoughtless
Something without effort is effortless
Something without sensor is sensorless
Something without emotion is
2024-07-10 12:23:18 root INFO     [order_1_approx] starting weight calculation for Something without sensor is sensorless
Something without sleeve is sleeveless
Something without emotion is emotionless
Something without tact is tactless
Something without effort is effortless
Something without window is windowless
Something without thought is thoughtless
Something without talent is
2024-07-10 12:23:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 12:27:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 12:31:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2075, -0.4001,  0.9590,  ..., -0.8525, -0.2598,  0.0954],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.1864, -0.4053,  0.8408,  ..., -0.7646, -0.2625,  0.0631],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0151, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2715, -1.0625,  1.0986,  ...,  5.4180,  2.0742, -0.0854],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0190, -0.0040,  0.0033,  ...,  0.0012,  0.0010, -0.0085],
        [-0.0298, -0.0216, -0.0047,  ...,  0.0170, -0.0193,  0.0051],
        [ 0.0071, -0.0125, -0.0310,  ...,  0.0010, -0.0008,  0.0097],
        ...,
        [-0.0182, -0.0115, -0.0133,  ..., -0.0347, -0.0050,  0.0035],
        [ 0.0060, -0.0159,  0.0213,  ..., -0.0130, -0.0562,  0.0039],
        [-0.0109,  0.0275, -0.0039,  ..., -0.0081,  0.0255, -0.0585]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 2.4414e-04, -1.1559e-03,  2.8915e-03,  ...,  1.7786e-03,
          4.2381e-03, -1.9932e-03],
        [-7.0095e-05,  8.5592e-05,  1.3885e-03,  ...,  1.0576e-03,
          2.1887e-04,  2.9163e-03],
        [-2.0075e-04, -8.1730e-04, -2.4166e-03,  ...,  2.9564e-05,
          2.4223e-04,  1.3981e-03],
        ...,
        [ 2.7609e-04,  4.5538e-04, -2.2125e-04,  ..., -1.8234e-03,
          5.6171e-04, -8.7261e-05],
        [-7.9966e-04,  4.6635e-04, -2.5978e-03,  ..., -3.2749e-03,
         -1.6108e-03, -3.4332e-04],
        [ 2.3441e-03,  3.2139e-04, -1.6680e-03,  ..., -2.7657e-04,
         -6.9714e-04,  3.6335e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1501, -1.0068,  0.4253,  ...,  4.7695,  1.3184, -1.1309]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2500,  0.0193,  0.2676,  ..., -0.1393, -0.2773, -0.1781]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 12:31:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without sensor is sensorless
Something without sleeve is sleeveless
Something without emotion is emotionless
Something without tact is tactless
Something without effort is effortless
Something without window is windowless
Something without thought is thoughtless
Something without talent is
2024-07-10 12:31:46 root INFO     [order_1_approx] starting weight calculation for Something without sleeve is sleeveless
Something without sensor is sensorless
Something without talent is talentless
Something without emotion is emotionless
Something without effort is effortless
Something without window is windowless
Something without thought is thoughtless
Something without tact is
2024-07-10 12:31:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 12:36:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 12:40:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0752, -1.6016,  0.0146,  ...,  0.2561, -0.5718,  1.2578],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.8853, -1.3809, -0.0029,  ...,  0.2042, -0.4890,  1.0371],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0151, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.2207, -2.0742,  0.2939,  ...,  2.1113, -0.2383,  2.3809],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0074,  0.0073,  0.0117,  ..., -0.0039,  0.0031, -0.0041],
        [-0.0061, -0.0243, -0.0091,  ...,  0.0122, -0.0099,  0.0078],
        [ 0.0010, -0.0102, -0.0177,  ..., -0.0011, -0.0017,  0.0199],
        ...,
        [ 0.0046, -0.0052,  0.0039,  ...,  0.0010,  0.0026,  0.0062],
        [ 0.0045,  0.0106,  0.0292,  ..., -0.0052, -0.0051,  0.0079],
        [-0.0020, -0.0019, -0.0087,  ..., -0.0087,  0.0066, -0.0117]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-4.5371e-04, -1.0529e-03, -2.9659e-04,  ...,  1.1959e-03,
          8.5211e-04, -1.1692e-03],
        [ 7.1526e-04,  4.9973e-04, -1.5914e-05,  ...,  2.5940e-03,
         -8.0681e-04,  2.7676e-03],
        [-2.6917e-04,  1.0500e-03, -1.7872e-03,  ..., -3.3379e-04,
          7.8964e-04, -7.7438e-04],
        ...,
        [ 1.1873e-04, -1.0958e-03,  1.0128e-03,  ..., -6.0081e-05,
         -1.2374e-04, -1.7786e-04],
        [ 1.3294e-03,  1.1139e-03, -5.3167e-04,  ..., -2.6588e-03,
         -1.2989e-03, -1.1311e-03],
        [ 5.4264e-04, -8.8310e-04, -2.7218e-03,  ..., -2.6798e-04,
          1.4915e-03, -1.4133e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.1875, -3.4961, -0.1841,  ...,  2.0039, -0.3484,  0.1074]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0173, -0.0459,  0.2927,  ...,  0.0397, -0.0306,  0.0451]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 12:40:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without sleeve is sleeveless
Something without sensor is sensorless
Something without talent is talentless
Something without emotion is emotionless
Something without effort is effortless
Something without window is windowless
Something without thought is thoughtless
Something without tact is
2024-07-10 12:40:09 root INFO     [order_1_approx] starting weight calculation for Something without sleeve is sleeveless
Something without thought is thoughtless
Something without emotion is emotionless
Something without talent is talentless
Something without tact is tactless
Something without effort is effortless
Something without sensor is sensorless
Something without window is
2024-07-10 12:40:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 12:44:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 12:48:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0619, -0.8115, -0.3564,  ..., -0.2515,  0.4561,  1.0371],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.0686, -0.8496, -0.3533,  ..., -0.2393,  0.4270,  1.0293],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0151, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3672,  2.9863, -0.0137,  ..., -0.0566, -0.8228,  4.4688],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0043,  0.0014,  0.0104,  ...,  0.0057, -0.0013,  0.0167],
        [-0.0098, -0.0092, -0.0068,  ..., -0.0041,  0.0105, -0.0115],
        [ 0.0028, -0.0021, -0.0076,  ..., -0.0157,  0.0067,  0.0024],
        ...,
        [ 0.0008, -0.0139,  0.0056,  ..., -0.0080,  0.0248, -0.0174],
        [-0.0011,  0.0013,  0.0104,  ...,  0.0081, -0.0126, -0.0276],
        [-0.0005,  0.0225,  0.0077,  ..., -0.0095,  0.0116, -0.0446]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.0262e-03, -3.9482e-04,  1.1215e-03,  ..., -3.2949e-04,
          4.1986e-04,  1.2760e-03],
        [-1.5831e-04, -3.2101e-03,  1.4610e-03,  ..., -5.3120e-04,
         -6.1750e-05,  8.5497e-04],
        [-1.4133e-03, -6.3467e-04,  1.1425e-03,  ...,  9.0551e-04,
         -5.4741e-04, -3.4833e-04],
        ...,
        [ 1.5030e-03,  4.3201e-04,  8.2493e-04,  ..., -1.4610e-03,
          1.2074e-03,  4.0174e-04],
        [ 2.2392e-03,  4.1275e-03, -1.5011e-03,  ..., -4.4823e-05,
         -2.3403e-03,  9.0313e-04],
        [-7.7534e-04, -5.5027e-04, -7.1716e-04,  ..., -3.8028e-04,
         -2.9259e-03, -1.0128e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.8770,  2.4336, -0.5527,  ..., -0.6167, -1.6270,  3.4707]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0942, -0.1752,  0.1013,  ..., -0.1974,  0.0552, -0.2451]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 12:48:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without sleeve is sleeveless
Something without thought is thoughtless
Something without emotion is emotionless
Something without talent is talentless
Something without tact is tactless
Something without effort is effortless
Something without sensor is sensorless
Something without window is
2024-07-10 12:48:34 root INFO     [order_1_approx] starting weight calculation for Something without sleeve is sleeveless
Something without effort is effortless
Something without thought is thoughtless
Something without talent is talentless
Something without tact is tactless
Something without window is windowless
Something without emotion is emotionless
Something without sensor is
2024-07-10 12:48:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 12:52:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 12:57:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2788,  0.2585,  0.0243,  ..., -0.6689,  0.9189,  0.0573],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.2595,  0.2327,  0.0072,  ..., -0.6138,  0.8452,  0.0284],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0151, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.5469,  0.7798,  4.6602,  ..., -1.0430, -2.2109,  4.1094],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0207, -0.0155, -0.0196,  ..., -0.0129,  0.0051,  0.0049],
        [-0.0084, -0.0040, -0.0085,  ..., -0.0046, -0.0186, -0.0133],
        [ 0.0013,  0.0154, -0.0051,  ...,  0.0011, -0.0178,  0.0006],
        ...,
        [-0.0135,  0.0002, -0.0027,  ..., -0.0079,  0.0003, -0.0011],
        [-0.0040, -0.0094, -0.0003,  ..., -0.0005, -0.0032, -0.0013],
        [ 0.0008,  0.0303, -0.0068,  ..., -0.0119, -0.0081, -0.0300]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 6.6280e-04, -8.0824e-04, -2.1756e-05,  ..., -8.8310e-04,
          1.4009e-03, -5.5170e-04],
        [ 9.5367e-07, -1.5135e-03, -4.7684e-06,  ...,  1.1450e-04,
         -7.6151e-04,  1.9512e-03],
        [-8.5449e-04, -8.9645e-04,  1.2436e-03,  ..., -1.2665e-03,
         -8.5926e-04,  2.8825e-04],
        ...,
        [ 4.3726e-04,  4.9114e-05,  2.0230e-04,  ..., -1.9569e-03,
          1.7653e-03, -3.8195e-04],
        [-2.3594e-03,  2.5349e-03, -1.5459e-03,  ..., -2.4643e-03,
         -2.9907e-03, -8.8501e-04],
        [ 2.2459e-04, -8.7023e-04, -1.6470e-03,  ..., -4.3774e-04,
         -1.4238e-03, -4.7874e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.3418,  1.0059,  4.3203,  ..., -1.1611, -2.2812,  3.8672]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0190,  0.0138,  0.1702,  ..., -0.0981, -0.0850,  0.1467]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 12:57:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without sleeve is sleeveless
Something without effort is effortless
Something without thought is thoughtless
Something without talent is talentless
Something without tact is tactless
Something without window is windowless
Something without emotion is emotionless
Something without sensor is
2024-07-10 12:57:03 root INFO     [order_1_approx] starting weight calculation for Something without sensor is sensorless
Something without emotion is emotionless
Something without tact is tactless
Something without thought is thoughtless
Something without sleeve is sleeveless
Something without talent is talentless
Something without window is windowless
Something without effort is
2024-07-10 12:57:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 13:01:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 13:05:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8691, -0.4805, -0.1349,  ..., -0.5312, -0.5884,  0.9282],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.7964, -0.4741, -0.1326,  ..., -0.4683, -0.5562,  0.8472],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0151, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7139,  0.2859,  0.6611,  ..., -0.5459, -0.6230, -0.0122],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.4053e-02,  7.0953e-04,  1.2451e-02,  ...,  1.5701e-02,
         -1.1154e-02,  1.3428e-03],
        [-1.6373e-02, -3.3966e-02, -2.1332e-02,  ..., -6.6299e-03,
         -2.0676e-02,  8.4877e-05],
        [-1.6670e-03,  1.5015e-02, -3.2883e-03,  ..., -1.4496e-04,
         -5.2643e-04,  1.3840e-02],
        ...,
        [ 8.0109e-03, -8.7433e-03, -1.7151e-02,  ..., -3.7689e-03,
         -1.1185e-02,  2.1496e-03],
        [ 9.9182e-03, -2.3117e-02, -5.1928e-04,  ..., -5.3978e-03,
         -2.9129e-02,  1.8692e-02],
        [-9.7580e-03,  2.6306e-02,  1.1871e-02,  ..., -1.0506e-02,
         -5.2185e-03, -2.2034e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.8873e-03, -1.4000e-03,  1.9121e-03,  ...,  1.8673e-03,
          8.5831e-04,  1.4868e-03],
        [-1.0328e-03,  7.0238e-04, -4.7946e-04,  ...,  1.6332e-04,
         -1.4515e-03,  3.5381e-04],
        [ 5.2452e-05,  1.5812e-03, -2.9926e-03,  ...,  1.0443e-04,
          2.2564e-03,  9.6607e-04],
        ...,
        [-1.0538e-04, -1.2064e-03, -2.0027e-03,  ..., -6.9618e-04,
          1.0891e-03,  6.1035e-05],
        [ 3.8385e-05, -1.1415e-03, -5.2071e-04,  ..., -2.1706e-03,
          9.6083e-05,  4.7350e-04],
        [ 6.8378e-04,  1.2817e-03, -1.2207e-03,  ..., -1.3161e-03,
         -1.7586e-03,  9.1934e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6445,  0.0139,  0.0181,  ..., -0.4299, -0.6621, -0.0232]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1516, -0.0149,  0.0284,  ...,  0.0999, -0.1210,  0.0025]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 13:05:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without sensor is sensorless
Something without emotion is emotionless
Something without tact is tactless
Something without thought is thoughtless
Something without sleeve is sleeveless
Something without talent is talentless
Something without window is windowless
Something without effort is
2024-07-10 13:05:32 root INFO     [order_1_approx] starting weight calculation for Something without emotion is emotionless
Something without effort is effortless
Something without thought is thoughtless
Something without sensor is sensorless
Something without talent is talentless
Something without window is windowless
Something without tact is tactless
Something without sleeve is
2024-07-10 13:05:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 13:09:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 13:13:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1179, -0.6284, -0.2358,  ...,  0.0164,  0.0532,  0.3123],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1265, -0.6606, -0.2395,  ...,  0.0141,  0.0278,  0.2864],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0151, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3706,  0.6260,  2.4883,  ...,  0.7959, -2.3887,  0.2402],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.5266e-02, -1.5533e-02, -2.8458e-02,  ...,  8.4076e-03,
          6.4583e-03,  9.5367e-03],
        [-9.9564e-03, -7.2861e-04,  2.2316e-03,  ...,  3.7632e-03,
         -1.1749e-02,  7.5569e-03],
        [ 5.4054e-03,  1.3550e-02, -2.6703e-05,  ..., -7.7209e-03,
          8.7509e-03,  6.4926e-03],
        ...,
        [-2.0618e-03, -4.5776e-03, -1.0239e-02,  ..., -7.1106e-03,
          8.7051e-03, -1.3611e-02],
        [ 4.4632e-03,  9.6893e-03,  2.7790e-03,  ...,  2.0111e-02,
         -1.8372e-02, -1.3123e-03],
        [-1.0300e-02, -1.0811e-02,  6.2370e-04,  ..., -1.2833e-02,
          1.7151e-02, -3.0792e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 0.0005, -0.0006,  0.0014,  ...,  0.0021,  0.0022, -0.0006],
        [-0.0005, -0.0013,  0.0015,  ...,  0.0024, -0.0001,  0.0019],
        [-0.0014, -0.0005, -0.0002,  ..., -0.0023, -0.0009,  0.0006],
        ...,
        [ 0.0033, -0.0006,  0.0018,  ...,  0.0012, -0.0004,  0.0003],
        [ 0.0030,  0.0012,  0.0004,  ...,  0.0002, -0.0041,  0.0005],
        [-0.0014, -0.0015,  0.0002,  ..., -0.0005,  0.0012, -0.0014]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3623,  0.7446,  2.5977,  ...,  0.7412, -2.0352, -0.4922]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1583, -0.1021,  0.1069,  ..., -0.2107, -0.0013,  0.0919]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 13:14:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without emotion is emotionless
Something without effort is effortless
Something without thought is thoughtless
Something without sensor is sensorless
Something without talent is talentless
Something without window is windowless
Something without tact is tactless
Something without sleeve is
2024-07-10 13:14:01 root INFO     total operator prediction time: 4056.5407767295837 seconds
2024-07-10 13:14:01 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+ment_irreg
2024-07-10 13:14:03 root INFO     building operator verb+ment_irreg
2024-07-10 13:14:03 root INFO     [order_1_approx] starting weight calculation for To involve results in a involvement
To amuse results in a amusement
To advertise results in a advertisement
To disappoint results in a disappointment
To reinforce results in a reinforcement
To appoint results in a appointment
To require results in a requirement
To equip results in a
2024-07-10 13:14:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 13:18:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 13:22:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1096,  0.3083,  1.3115,  ..., -0.3599, -1.1680, -1.2246],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.0933,  0.2671,  1.1230,  ..., -0.3118, -1.0664, -1.1611],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0318, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1162,  2.7969,  3.0547,  ..., -0.5557, -2.3125,  7.6719],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.2146e-02, -1.1353e-02,  9.1553e-05,  ...,  5.6534e-03,
          2.0004e-02, -1.5747e-02],
        [-1.7746e-02,  1.6479e-03,  7.9346e-03,  ..., -8.5640e-04,
         -2.4948e-02, -5.1537e-03],
        [ 8.5907e-03,  1.0429e-02, -6.6376e-04,  ...,  9.3842e-03,
          5.6000e-03,  3.3188e-04],
        ...,
        [-2.1423e-02, -1.0178e-02, -1.5640e-04,  ...,  2.2964e-03,
          4.3449e-03,  8.7585e-03],
        [-4.5490e-04,  1.0138e-03, -3.4714e-04,  ..., -9.5901e-03,
         -2.5578e-03,  6.1340e-03],
        [-1.0223e-02, -5.3253e-03,  5.1117e-03,  ...,  8.9340e-03,
         -2.3727e-03,  1.0399e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-3.1528e-03, -8.7833e-04,  8.1062e-04,  ...,  1.8625e-03,
         -9.7275e-04, -1.1730e-04],
        [-1.0242e-03, -8.1348e-04, -3.9172e-04,  ...,  1.4324e-03,
          6.1274e-04, -4.4084e-04],
        [-4.1437e-04,  1.3952e-03, -1.9627e-03,  ...,  2.3246e-05,
          2.5213e-05,  4.2820e-04],
        ...,
        [-9.2840e-04,  6.3992e-04,  7.6246e-04,  ...,  3.6812e-04,
          1.5793e-03, -9.7847e-04],
        [-6.2704e-04, -2.2268e-04,  1.3409e-03,  ..., -6.0463e-04,
          6.2895e-04, -7.8917e-04],
        [-3.7742e-04, -1.4009e-03, -4.2677e-05,  ...,  1.0252e-04,
         -1.5602e-03, -1.8263e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5483,  2.1133,  2.8438,  ..., -0.1335, -3.7520,  7.1523]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2942,  0.0962,  0.0646,  ..., -0.1523, -0.1433, -0.1575]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 13:22:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To involve results in a involvement
To amuse results in a amusement
To advertise results in a advertisement
To disappoint results in a disappointment
To reinforce results in a reinforcement
To appoint results in a appointment
To require results in a requirement
To equip results in a
2024-07-10 13:22:30 root INFO     [order_1_approx] starting weight calculation for To disappoint results in a disappointment
To equip results in a equipment
To require results in a requirement
To amuse results in a amusement
To appoint results in a appointment
To involve results in a involvement
To reinforce results in a reinforcement
To advertise results in a
2024-07-10 13:22:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 13:26:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 13:30:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1493,  0.1946,  0.3438,  ...,  0.0380, -0.0505, -0.2178],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1558,  0.1742,  0.3069,  ...,  0.0351, -0.0724, -0.2472],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0318, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0928,  1.6250, -5.0469,  ..., -0.4929, -2.2246,  2.3359],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0051, -0.0062,  0.0072,  ...,  0.0086,  0.0144, -0.0255],
        [-0.0064, -0.0217,  0.0119,  ...,  0.0071, -0.0129,  0.0085],
        [-0.0152,  0.0054, -0.0164,  ...,  0.0023,  0.0106, -0.0062],
        ...,
        [-0.0115, -0.0192, -0.0054,  ..., -0.0015,  0.0026, -0.0040],
        [ 0.0028,  0.0181, -0.0139,  ..., -0.0012, -0.0072, -0.0036],
        [-0.0099,  0.0132,  0.0036,  ...,  0.0005,  0.0198, -0.0204]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-3.5076e-03, -2.8152e-03,  9.1362e-04,  ..., -1.4763e-03,
         -1.4229e-03,  1.1091e-03],
        [ 1.3423e-04, -2.2945e-03, -1.0806e-04,  ...,  8.9407e-04,
          6.6757e-04, -8.4782e-04],
        [-1.9436e-03, -6.6340e-05, -5.8270e-04,  ...,  1.5354e-04,
         -6.0987e-04,  6.1369e-04],
        ...,
        [-4.2772e-04, -4.9067e-04,  1.6003e-03,  ..., -1.0195e-03,
          2.0657e-03, -2.6054e-03],
        [-1.1396e-04,  2.6379e-03, -6.7043e-04,  ..., -1.1740e-03,
         -1.0538e-03, -8.6975e-04],
        [-1.0204e-03,  2.5024e-03,  4.9639e-04,  ...,  6.1893e-04,
         -6.7139e-04, -5.3310e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6592,  1.8760, -4.5859,  ..., -0.7578, -1.5762,  1.8916]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.5356, -0.0572,  0.1584,  ..., -0.2957, -0.0974,  0.0346]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 13:30:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To disappoint results in a disappointment
To equip results in a equipment
To require results in a requirement
To amuse results in a amusement
To appoint results in a appointment
To involve results in a involvement
To reinforce results in a reinforcement
To advertise results in a
2024-07-10 13:30:59 root INFO     [order_1_approx] starting weight calculation for To disappoint results in a disappointment
To involve results in a involvement
To appoint results in a appointment
To reinforce results in a reinforcement
To advertise results in a advertisement
To require results in a requirement
To equip results in a equipment
To amuse results in a
2024-07-10 13:30:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 13:35:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 13:39:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3887, -0.5708,  0.7241,  ..., -0.0610,  0.3508,  0.3262],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3625, -0.5752,  0.6416,  ..., -0.0543,  0.3066,  0.2893],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0318, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6289,  3.6230, -1.6318,  ..., -0.7188,  0.1616,  7.2539],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0041, -0.0140, -0.0172,  ..., -0.0002,  0.0023, -0.0132],
        [-0.0256, -0.0010,  0.0031,  ...,  0.0119, -0.0165,  0.0056],
        [ 0.0142,  0.0021, -0.0078,  ...,  0.0020,  0.0087, -0.0067],
        ...,
        [-0.0181, -0.0171,  0.0046,  ...,  0.0070, -0.0120,  0.0105],
        [ 0.0123,  0.0045, -0.0015,  ..., -0.0003, -0.0239, -0.0012],
        [-0.0015,  0.0023, -0.0071,  ...,  0.0037,  0.0069, -0.0007]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-3.8576e-04, -5.2023e-04, -1.7776e-03,  ..., -2.3687e-04,
          7.6294e-04, -3.6144e-04],
        [-2.8658e-04, -4.4918e-04,  1.1611e-04,  ...,  3.3855e-04,
          8.2493e-05, -7.2432e-04],
        [-4.4203e-04,  2.1935e-04, -4.4513e-04,  ...,  7.8082e-05,
          7.8487e-04, -3.4094e-04],
        ...,
        [-4.3845e-04, -1.8382e-04,  5.2166e-04,  ..., -2.1400e-03,
         -7.5102e-04, -5.0020e-04],
        [-8.0156e-04, -2.5330e-03,  4.3845e-04,  ..., -3.0279e-04,
         -1.7509e-03, -2.6822e-06],
        [ 2.5864e-03, -8.1396e-04, -6.4468e-04,  ...,  1.1940e-03,
          4.6730e-05, -1.2312e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.1240,  2.0605, -1.2754,  ..., -0.8711,  0.8486,  6.7344]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1890, -0.0987,  0.0562,  ..., -0.0081,  0.2410,  0.0891]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 13:39:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To disappoint results in a disappointment
To involve results in a involvement
To appoint results in a appointment
To reinforce results in a reinforcement
To advertise results in a advertisement
To require results in a requirement
To equip results in a equipment
To amuse results in a
2024-07-10 13:39:30 root INFO     [order_1_approx] starting weight calculation for To advertise results in a advertisement
To equip results in a equipment
To amuse results in a amusement
To require results in a requirement
To appoint results in a appointment
To reinforce results in a reinforcement
To disappoint results in a disappointment
To involve results in a
2024-07-10 13:39:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 13:43:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 13:47:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5762,  0.0519,  0.2471,  ..., -0.0485, -0.1248,  0.4089],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.6177,  0.0364,  0.2405,  ..., -0.0503, -0.1559,  0.4246],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0318, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6338,  1.0283, -2.0273,  ...,  0.7900,  2.3672,  5.6523],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 9.5520e-03, -2.2736e-03,  9.1476e-03,  ...,  8.7547e-04,
          2.6291e-02, -1.3626e-02],
        [-2.9419e-02, -3.5065e-02,  4.3640e-03,  ...,  8.1329e-03,
         -4.2648e-03, -7.0496e-03],
        [ 1.9028e-02,  1.8387e-02,  3.0212e-03,  ...,  1.0315e-02,
          6.2065e-03, -7.7629e-03],
        ...,
        [-5.1003e-03, -6.0654e-03, -8.0185e-03,  ...,  3.3035e-03,
          1.1215e-03, -1.1253e-04],
        [ 4.3869e-05,  1.3771e-03,  3.1967e-03,  ...,  4.6043e-03,
         -1.8478e-02, -2.0771e-03],
        [ 5.2567e-03,  1.5442e-02,  3.1090e-03,  ..., -1.3733e-04,
          7.4692e-03, -1.8635e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.4286e-03, -1.8444e-03, -2.2745e-04,  ...,  1.3471e-04,
         -7.1144e-04,  6.6423e-04],
        [-1.4811e-03, -1.2379e-03, -5.0020e-04,  ...,  1.4949e-04,
          8.9121e-04, -5.5742e-04],
        [-1.1301e-03,  4.7946e-04, -1.2932e-03,  ..., -1.8239e-04,
         -7.8630e-04, -4.3511e-04],
        ...,
        [-1.9817e-03, -9.5463e-04, -1.3371e-03,  ..., -7.1335e-04,
         -8.8215e-06,  7.6771e-05],
        [ 1.2245e-03,  4.5228e-04, -1.1387e-03,  ...,  6.3479e-05,
         -5.7173e-04, -2.3162e-04],
        [ 4.9973e-04, -9.5034e-04,  1.2045e-03,  ...,  1.9503e-04,
         -1.1749e-03, -8.3923e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0000, -0.3799, -0.9307,  ...,  0.5776,  1.7100,  5.0859]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0887, -0.2493,  0.0291,  ..., -0.0552, -0.2153, -0.0022]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 13:47:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To advertise results in a advertisement
To equip results in a equipment
To amuse results in a amusement
To require results in a requirement
To appoint results in a appointment
To reinforce results in a reinforcement
To disappoint results in a disappointment
To involve results in a
2024-07-10 13:47:56 root INFO     [order_1_approx] starting weight calculation for To reinforce results in a reinforcement
To involve results in a involvement
To amuse results in a amusement
To appoint results in a appointment
To equip results in a equipment
To disappoint results in a disappointment
To advertise results in a advertisement
To require results in a
2024-07-10 13:47:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 13:52:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 13:56:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0874,  0.0486, -0.3901,  ...,  0.4390,  0.5708,  0.9951],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.0879,  0.0338, -0.4160,  ...,  0.4558,  0.5864,  1.0703],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0318, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7783, -0.8662, -3.4980,  ...,  0.1909, -0.1577,  5.6875],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0035, -0.0079, -0.0071,  ...,  0.0006,  0.0006, -0.0029],
        [-0.0063,  0.0031, -0.0117,  ...,  0.0168, -0.0140, -0.0115],
        [ 0.0147,  0.0133,  0.0070,  ...,  0.0009,  0.0027,  0.0028],
        ...,
        [-0.0058, -0.0081, -0.0072,  ..., -0.0109, -0.0051,  0.0096],
        [-0.0062, -0.0024, -0.0008,  ..., -0.0127, -0.0058, -0.0132],
        [ 0.0127, -0.0023,  0.0225,  ...,  0.0113,  0.0107,  0.0020]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0021, -0.0009, -0.0002,  ..., -0.0011,  0.0011,  0.0011],
        [ 0.0003,  0.0007, -0.0012,  ...,  0.0013,  0.0006, -0.0007],
        [-0.0025,  0.0030, -0.0022,  ..., -0.0020, -0.0012,  0.0001],
        ...,
        [-0.0037,  0.0013, -0.0011,  ..., -0.0029,  0.0007,  0.0007],
        [ 0.0030,  0.0005,  0.0016,  ...,  0.0008,  0.0006, -0.0015],
        [-0.0020, -0.0004,  0.0011,  ...,  0.0016, -0.0020, -0.0020]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3906e+00, -1.5996e+00, -3.2109e+00,  ...,  2.2949e-01,
          6.3477e-03,  6.5039e+00]], device='cuda:1', dtype=torch.float16,
       grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1289, -0.1506,  0.1237,  ..., -0.2432,  0.0044,  0.1445]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 13:56:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To reinforce results in a reinforcement
To involve results in a involvement
To amuse results in a amusement
To appoint results in a appointment
To equip results in a equipment
To disappoint results in a disappointment
To advertise results in a advertisement
To require results in a
2024-07-10 13:56:24 root INFO     [order_1_approx] starting weight calculation for To reinforce results in a reinforcement
To equip results in a equipment
To require results in a requirement
To advertise results in a advertisement
To amuse results in a amusement
To involve results in a involvement
To disappoint results in a disappointment
To appoint results in a
2024-07-10 13:56:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 14:00:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 14:04:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0237, -0.4043,  0.7681,  ...,  0.8672, -0.0570,  0.3813],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.0322, -0.4395,  0.7236,  ...,  0.8379, -0.0796,  0.3643],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0318, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1472,  1.0566, -1.4541,  ...,  1.5312,  0.2756,  5.2109],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0127, -0.0109,  0.0045,  ..., -0.0035,  0.0089, -0.0133],
        [ 0.0006, -0.0038,  0.0031,  ...,  0.0132, -0.0048,  0.0039],
        [ 0.0024,  0.0062,  0.0045,  ...,  0.0190, -0.0042,  0.0056],
        ...,
        [-0.0261, -0.0103, -0.0026,  ...,  0.0131,  0.0006,  0.0135],
        [-0.0096, -0.0040,  0.0108,  ...,  0.0028, -0.0103,  0.0115],
        [ 0.0098, -0.0037,  0.0027,  ...,  0.0079,  0.0062, -0.0057]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-8.3828e-04, -1.0891e-03,  4.6802e-04,  ..., -3.3021e-04,
          2.2066e-04, -3.7384e-04],
        [-6.6280e-05, -1.0900e-03, -5.4002e-05,  ...,  1.7300e-03,
          6.9439e-05, -1.1578e-03],
        [ 5.4073e-04,  2.6474e-03, -6.8188e-04,  ...,  8.4782e-04,
          1.9073e-06,  2.0332e-03],
        ...,
        [-1.3256e-04,  7.9250e-04,  1.1921e-04,  ...,  6.3229e-04,
          5.3120e-04, -2.2054e-04],
        [ 1.8654e-03,  8.8596e-04, -3.7599e-04,  ...,  4.0746e-04,
          7.1430e-04, -7.5293e-04],
        [ 9.5892e-04,  6.2418e-04, -7.8917e-05,  ...,  5.5885e-04,
         -1.0777e-04, -2.0504e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0596, -0.0586, -1.7285,  ...,  0.6558,  0.6074,  5.1641]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0943,  0.2036,  0.1113,  ..., -0.4312, -0.1453,  0.0195]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 14:04:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To reinforce results in a reinforcement
To equip results in a equipment
To require results in a requirement
To advertise results in a advertisement
To amuse results in a amusement
To involve results in a involvement
To disappoint results in a disappointment
To appoint results in a
2024-07-10 14:04:53 root INFO     [order_1_approx] starting weight calculation for To require results in a requirement
To advertise results in a advertisement
To disappoint results in a disappointment
To involve results in a involvement
To amuse results in a amusement
To appoint results in a appointment
To equip results in a equipment
To reinforce results in a
2024-07-10 14:04:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 14:09:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 14:13:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3120, -0.0333,  1.0176,  ...,  1.1494, -0.1208, -0.1719],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.3167, -0.0568,  0.9360,  ...,  1.0801, -0.1415, -0.2019],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0318, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2998,  1.3809, -1.6787,  ...,  1.7070, -1.2578,  5.4141],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0096, -0.0271, -0.0044,  ...,  0.0139,  0.0029, -0.0134],
        [-0.0151, -0.0044,  0.0014,  ...,  0.0129, -0.0119, -0.0028],
        [-0.0020,  0.0078,  0.0012,  ...,  0.0082,  0.0055, -0.0063],
        ...,
        [-0.0081, -0.0124, -0.0053,  ...,  0.0060,  0.0005, -0.0019],
        [ 0.0097,  0.0094, -0.0068,  ..., -0.0179, -0.0035, -0.0024],
        [-0.0036,  0.0012,  0.0099,  ..., -0.0025,  0.0155,  0.0089]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.8343e-03, -2.8019e-03,  3.6788e-04,  ...,  6.2466e-04,
          1.3208e-03,  6.1893e-04],
        [ 1.3285e-03, -6.7234e-04, -1.4076e-03,  ...,  1.9703e-03,
          1.6699e-03,  1.0967e-05],
        [-9.2030e-04,  1.3552e-03, -1.0996e-03,  ...,  2.4700e-04,
          8.9550e-04, -3.2401e-04],
        ...,
        [-1.2550e-03, -3.5763e-05,  1.3723e-03,  ..., -8.1778e-04,
         -7.7581e-04, -9.3079e-04],
        [-1.0805e-03, -1.1492e-03,  1.3809e-03,  ..., -1.2560e-03,
         -1.8063e-03, -1.9245e-03],
        [-1.1911e-03, -1.0986e-03,  3.9816e-04,  ...,  1.3590e-03,
          1.0672e-03, -2.0313e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7559,  0.7075, -2.4629,  ...,  2.1992, -1.0205,  6.0781]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2615, -0.0226,  0.0802,  ..., -0.2661,  0.0750, -0.1919]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 14:13:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To require results in a requirement
To advertise results in a advertisement
To disappoint results in a disappointment
To involve results in a involvement
To amuse results in a amusement
To appoint results in a appointment
To equip results in a equipment
To reinforce results in a
2024-07-10 14:13:19 root INFO     [order_1_approx] starting weight calculation for To amuse results in a amusement
To advertise results in a advertisement
To equip results in a equipment
To require results in a requirement
To involve results in a involvement
To reinforce results in a reinforcement
To appoint results in a appointment
To disappoint results in a
2024-07-10 14:13:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-10 14:17:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-10 14:21:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5303, -1.3848,  1.8125,  ...,  0.8154,  0.1716, -0.6704],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.5000, -1.3330,  1.5781,  ...,  0.7192,  0.1313, -0.6611],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0318, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5566,  0.7637, -2.6465,  ...,  2.0547,  1.1289,  2.8984],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 6.7596e-03, -3.4561e-03,  7.6332e-03,  ..., -3.3493e-03,
          6.7520e-03, -3.9558e-03],
        [-1.9867e-02, -1.5961e-02,  7.4310e-03,  ...,  6.5842e-03,
         -1.8356e-02, -1.0071e-03],
        [ 1.7487e-02,  1.1299e-02, -1.8982e-02,  ...,  5.6267e-03,
          1.4427e-02,  3.3855e-03],
        ...,
        [-1.0094e-02, -2.7267e-02,  4.0207e-03,  ...,  5.4970e-03,
          1.2627e-02, -3.8757e-03],
        [-8.7967e-03,  6.6757e-03,  7.2784e-03,  ..., -6.8016e-03,
          1.3943e-03,  4.2953e-03],
        [ 9.6054e-03,  8.1863e-03,  6.1035e-05,  ...,  3.4256e-03,
          5.8327e-03,  1.1597e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.1400e-03, -1.5736e-03, -2.9802e-04,  ...,  7.5221e-05,
          1.2636e-03, -7.8201e-04],
        [ 2.7800e-04, -3.1543e-04, -1.6088e-03,  ...,  5.5838e-04,
          5.0163e-04, -4.8447e-04],
        [-1.1654e-03,  1.6661e-03, -1.6222e-03,  ...,  1.1101e-03,
         -4.7159e-04,  9.8038e-04],
        ...,
        [-1.2407e-03,  1.5211e-04,  2.5215e-03,  ..., -8.5402e-04,
          3.5477e-04, -1.4477e-03],
        [ 2.4343e-04,  1.2302e-03,  6.8092e-04,  ..., -1.4009e-03,
         -5.7316e-04, -7.8201e-05],
        [ 8.0061e-04,  1.5020e-03,  1.2970e-04,  ...,  9.4080e-04,
         -4.7088e-05, -1.0481e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6328, -1.1299, -1.1982,  ...,  0.4717,  0.3799,  4.1406]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.2390, -0.1614, -0.0272,  ..., -0.1951,  0.0026,  0.1223]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-10 14:21:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To amuse results in a amusement
To advertise results in a advertisement
To equip results in a equipment
To require results in a requirement
To involve results in a involvement
To reinforce results in a reinforcement
To appoint results in a appointment
To disappoint results in a
2024-07-10 14:21:48 root INFO     total operator prediction time: 4065.5837914943695 seconds
