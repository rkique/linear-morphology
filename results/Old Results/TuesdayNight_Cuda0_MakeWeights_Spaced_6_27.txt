2024-07-23 22:11:45 root INFO     loading model + tokenizer
2024-07-23 22:12:01 root INFO     model + tokenizer loaded
2024-07-23 22:12:01 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - part
2024-07-23 22:12:01 root INFO     building operator meronyms - part
2024-07-23 22:12:02 root INFO     [order_1_approx] starting weight calculation for A part of a pub is a bar
A part of a comb is a teeth
A part of a radio is a receiver
A part of a table is a tabletop
A part of a sonata is a movement
A part of a guitar is a string
A part of a door is a hinge
A part of a chair is a
2024-07-23 22:12:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 22:15:29 root INFO     loading model + tokenizer
2024-07-23 22:15:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6665, -1.0371,  0.5020,  ...,  0.9707, -1.7227, -0.5640],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9648,  1.2490, -2.2539,  ...,  1.4434, -2.0352,  1.0518],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0081, -0.0226, -0.0002,  ...,  0.0050, -0.0068, -0.0238],
        [ 0.0027,  0.0044,  0.0114,  ..., -0.0025, -0.0110, -0.0129],
        [ 0.0049, -0.0075,  0.0448,  ...,  0.0020, -0.0182,  0.0287],
        ...,
        [ 0.0090,  0.0059, -0.0224,  ...,  0.0012, -0.0200,  0.0124],
        [-0.0067,  0.0129,  0.0026,  ..., -0.0066,  0.0116, -0.0042],
        [-0.0021, -0.0154,  0.0106,  ..., -0.0095,  0.0084,  0.0219]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9102,  1.1445, -2.6543,  ...,  2.0273, -1.6436,  1.4688]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:15:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a pub is a bar
A part of a comb is a teeth
A part of a radio is a receiver
A part of a table is a tabletop
A part of a sonata is a movement
A part of a guitar is a string
A part of a door is a hinge
A part of a chair is a
2024-07-23 22:15:48 root INFO     [order_1_approx] starting weight calculation for A part of a radio is a receiver
A part of a door is a hinge
A part of a table is a tabletop
A part of a chair is a seat
A part of a guitar is a string
A part of a sonata is a movement
A part of a pub is a bar
A part of a comb is a
2024-07-23 22:15:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 22:19:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2773,  0.6406,  0.3845,  ...,  0.1011, -1.3379,  0.2510],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6445, -2.7598, -2.0215,  ...,  2.2422, -1.2744,  3.1504],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0033, -0.0114,  0.0071,  ...,  0.0028,  0.0310, -0.0214],
        [-0.0406,  0.0060,  0.0112,  ...,  0.0092, -0.0087, -0.0007],
        [-0.0038, -0.0182,  0.0186,  ...,  0.0255,  0.0006, -0.0036],
        ...,
        [-0.0106, -0.0003, -0.0206,  ...,  0.0212, -0.0070,  0.0045],
        [ 0.0156, -0.0095, -0.0063,  ...,  0.0060,  0.0108,  0.0133],
        [ 0.0171,  0.0195,  0.0084,  ..., -0.0124,  0.0252,  0.0097]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7217, -0.7578, -1.8828,  ...,  1.7578, -2.6055,  2.2461]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:19:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a radio is a receiver
A part of a door is a hinge
A part of a table is a tabletop
A part of a chair is a seat
A part of a guitar is a string
A part of a sonata is a movement
A part of a pub is a bar
A part of a comb is a
2024-07-23 22:19:32 root INFO     [order_1_approx] starting weight calculation for A part of a pub is a bar
A part of a guitar is a string
A part of a chair is a seat
A part of a table is a tabletop
A part of a radio is a receiver
A part of a sonata is a movement
A part of a comb is a teeth
A part of a door is a
2024-07-23 22:19:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 22:23:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6748, -0.4524,  0.3901,  ...,  0.8652, -1.2148,  0.9688],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.1758,  1.3770, -1.5156,  ..., -5.3359, -2.8438,  3.3320],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0232, -0.0149, -0.0069,  ..., -0.0056, -0.0069, -0.0036],
        [ 0.0074,  0.0164, -0.0047,  ...,  0.0053, -0.0057, -0.0109],
        [-0.0017,  0.0058,  0.0202,  ...,  0.0023, -0.0139, -0.0138],
        ...,
        [-0.0020, -0.0063, -0.0127,  ...,  0.0285, -0.0126, -0.0080],
        [-0.0032,  0.0050,  0.0034,  ...,  0.0116,  0.0110,  0.0081],
        [ 0.0131,  0.0181, -0.0051,  ..., -0.0085,  0.0130,  0.0367]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.6055,  1.7207, -0.8198,  ..., -5.5938, -2.9258,  3.2148]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:23:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a pub is a bar
A part of a guitar is a string
A part of a chair is a seat
A part of a table is a tabletop
A part of a radio is a receiver
A part of a sonata is a movement
A part of a comb is a teeth
A part of a door is a
2024-07-23 22:23:16 root INFO     [order_1_approx] starting weight calculation for A part of a pub is a bar
A part of a sonata is a movement
A part of a door is a hinge
A part of a table is a tabletop
A part of a radio is a receiver
A part of a comb is a teeth
A part of a chair is a seat
A part of a guitar is a
2024-07-23 22:23:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 22:26:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5054, -0.6865,  0.7217,  ...,  0.8643, -0.9102,  1.1016],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.7754, -5.4727, -0.1785,  ..., -2.6289,  0.2358, -3.2695],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0074, -0.0135,  0.0045,  ...,  0.0177, -0.0085, -0.0148],
        [-0.0012,  0.0183,  0.0065,  ...,  0.0194, -0.0111, -0.0183],
        [-0.0095, -0.0023,  0.0061,  ...,  0.0079,  0.0147,  0.0036],
        ...,
        [-0.0096, -0.0081, -0.0053,  ..., -0.0056, -0.0107,  0.0029],
        [ 0.0007, -0.0133, -0.0010,  ...,  0.0142, -0.0134,  0.0065],
        [ 0.0160,  0.0104,  0.0109,  ...,  0.0070,  0.0087,  0.0270]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.9805, -4.3125, -1.2822,  ..., -2.4023,  0.4692, -3.7754]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:26:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a pub is a bar
A part of a sonata is a movement
A part of a door is a hinge
A part of a table is a tabletop
A part of a radio is a receiver
A part of a comb is a teeth
A part of a chair is a seat
A part of a guitar is a
2024-07-23 22:26:58 root INFO     [order_1_approx] starting weight calculation for A part of a sonata is a movement
A part of a chair is a seat
A part of a radio is a receiver
A part of a table is a tabletop
A part of a guitar is a string
A part of a comb is a teeth
A part of a door is a hinge
A part of a pub is a
2024-07-23 22:26:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 22:30:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2642, -0.1925,  0.2686,  ...,  1.6641,  0.9248,  0.2681],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.8555, -0.7061, -0.5796,  ...,  1.9248,  0.8677, -2.9023],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0049, -0.0157, -0.0014,  ..., -0.0043,  0.0079, -0.0015],
        [ 0.0094,  0.0155,  0.0151,  ..., -0.0079,  0.0030, -0.0062],
        [ 0.0061,  0.0087,  0.0131,  ...,  0.0046, -0.0021, -0.0070],
        ...,
        [-0.0096, -0.0153, -0.0018,  ...,  0.0032,  0.0062,  0.0008],
        [-0.0040, -0.0005, -0.0104,  ...,  0.0073, -0.0017, -0.0081],
        [ 0.0203,  0.0110, -0.0027,  ...,  0.0075, -0.0107,  0.0101]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.6797,  0.8184, -0.5024,  ...,  1.2207,  0.6045, -2.3555]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:30:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a sonata is a movement
A part of a chair is a seat
A part of a radio is a receiver
A part of a table is a tabletop
A part of a guitar is a string
A part of a comb is a teeth
A part of a door is a hinge
A part of a pub is a
2024-07-23 22:30:38 root INFO     [order_1_approx] starting weight calculation for A part of a pub is a bar
A part of a door is a hinge
A part of a sonata is a movement
A part of a table is a tabletop
A part of a chair is a seat
A part of a guitar is a string
A part of a comb is a teeth
A part of a radio is a
2024-07-23 22:30:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 22:34:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0312,  0.8853,  0.8145,  ..., -0.1965, -0.8667,  1.0508],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0342,  2.0566, -0.8579,  ..., -2.6992, -0.4153,  0.8896],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0272, -0.0245, -0.0101,  ...,  0.0069, -0.0113, -0.0067],
        [-0.0007,  0.0151, -0.0167,  ...,  0.0047,  0.0050,  0.0049],
        [ 0.0007,  0.0030,  0.0011,  ...,  0.0083,  0.0060,  0.0328],
        ...,
        [ 0.0113, -0.0075,  0.0047,  ...,  0.0059, -0.0349,  0.0310],
        [-0.0043, -0.0013, -0.0051,  ...,  0.0069, -0.0100,  0.0234],
        [ 0.0122,  0.0066, -0.0054,  ..., -0.0041,  0.0114,  0.0368]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0371,  1.9131, -0.8247,  ..., -1.6797, -0.6177,  1.5156]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:34:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a pub is a bar
A part of a door is a hinge
A part of a sonata is a movement
A part of a table is a tabletop
A part of a chair is a seat
A part of a guitar is a string
A part of a comb is a teeth
A part of a radio is a
2024-07-23 22:34:18 root INFO     [order_1_approx] starting weight calculation for A part of a door is a hinge
A part of a radio is a receiver
A part of a chair is a seat
A part of a guitar is a string
A part of a pub is a bar
A part of a table is a tabletop
A part of a comb is a teeth
A part of a sonata is a
2024-07-23 22:34:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 22:37:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0542, -0.2407,  2.4824,  ...,  0.8066, -0.5039,  0.7812],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0811,  2.5859, -0.2773,  ..., -0.9277,  0.2072, -5.0273],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0022, -0.0119,  0.0044,  ...,  0.0044, -0.0129, -0.0168],
        [ 0.0160,  0.0096, -0.0023,  ..., -0.0056, -0.0009, -0.0298],
        [ 0.0097,  0.0050, -0.0076,  ...,  0.0171,  0.0042,  0.0050],
        ...,
        [-0.0002,  0.0129, -0.0108,  ...,  0.0024, -0.0252,  0.0096],
        [ 0.0051, -0.0193,  0.0106,  ..., -0.0015, -0.0231,  0.0152],
        [ 0.0208,  0.0195,  0.0004,  ..., -0.0114, -0.0073, -0.0164]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1586,  3.2051, -0.0081,  ..., -0.4758, -1.0293, -4.7422]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:38:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a door is a hinge
A part of a radio is a receiver
A part of a chair is a seat
A part of a guitar is a string
A part of a pub is a bar
A part of a table is a tabletop
A part of a comb is a teeth
A part of a sonata is a
2024-07-23 22:38:00 root INFO     [order_1_approx] starting weight calculation for A part of a pub is a bar
A part of a guitar is a string
A part of a door is a hinge
A part of a radio is a receiver
A part of a sonata is a movement
A part of a comb is a teeth
A part of a chair is a seat
A part of a table is a
2024-07-23 22:38:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 22:41:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1226, -0.0902,  1.2578,  ...,  1.8633, -0.4775,  0.0725],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9521,  1.0771,  3.2305,  ...,  0.3945, -1.3711,  0.7568],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0231, -0.0250, -0.0143,  ...,  0.0057, -0.0064, -0.0255],
        [-0.0025,  0.0155, -0.0004,  ...,  0.0100, -0.0034,  0.0035],
        [-0.0060,  0.0052,  0.0159,  ...,  0.0142, -0.0014,  0.0081],
        ...,
        [-0.0111, -0.0041, -0.0223,  ...,  0.0173, -0.0157,  0.0093],
        [ 0.0045, -0.0043,  0.0010,  ..., -0.0024, -0.0208,  0.0050],
        [ 0.0071, -0.0282,  0.0138,  ..., -0.0033,  0.0068,  0.0088]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7607,  0.8608,  3.7461,  ...,  0.2891, -1.0479,  0.8525]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:41:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a pub is a bar
A part of a guitar is a string
A part of a door is a hinge
A part of a radio is a receiver
A part of a sonata is a movement
A part of a comb is a teeth
A part of a chair is a seat
A part of a table is a
2024-07-23 22:41:44 root INFO     total operator prediction time: 1783.075397491455 seconds
2024-07-23 22:41:44 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on synonyms - exact
2024-07-23 22:41:44 root INFO     building operator synonyms - exact
2024-07-23 22:41:44 root INFO     [order_1_approx] starting weight calculation for Another word for portion is part
Another word for organized is arranged
Another word for sweets is confectionery
Another word for market is marketplace
Another word for incorrect is wrong
Another word for homogeneous is uniform
Another word for shore is coast
Another word for child is
2024-07-23 22:41:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 22:45:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6802,  0.0298, -0.2448,  ...,  0.3794, -0.0160,  1.8203],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.8359, -3.4902, -4.9492,  ..., -1.0830,  5.5273, -2.5820],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0076,  0.0005,  0.0130,  ..., -0.0027, -0.0080, -0.0035],
        [ 0.0022, -0.0048, -0.0135,  ...,  0.0108, -0.0090, -0.0093],
        [ 0.0038,  0.0328, -0.0145,  ...,  0.0025,  0.0104, -0.0022],
        ...,
        [-0.0102, -0.0003,  0.0061,  ..., -0.0118, -0.0065,  0.0231],
        [-0.0165, -0.0055,  0.0087,  ..., -0.0081, -0.0002,  0.0264],
        [-0.0433, -0.0286, -0.0108,  ..., -0.0024,  0.0030,  0.0122]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.0391, -2.6191, -4.5039,  ..., -0.6523,  5.2578, -1.6299]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:45:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for portion is part
Another word for organized is arranged
Another word for sweets is confectionery
Another word for market is marketplace
Another word for incorrect is wrong
Another word for homogeneous is uniform
Another word for shore is coast
Another word for child is
2024-07-23 22:45:17 root INFO     [order_1_approx] starting weight calculation for Another word for shore is coast
Another word for incorrect is wrong
Another word for sweets is confectionery
Another word for organized is arranged
Another word for portion is part
Another word for market is marketplace
Another word for child is kid
Another word for homogeneous is
2024-07-23 22:45:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 22:48:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9390,  1.1436,  1.7070,  ..., -0.7861,  0.1511,  1.4238],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9688,  2.8945, -7.1680,  ...,  3.4062,  3.2031,  0.2100],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0029, -0.0079,  0.0150,  ..., -0.0063, -0.0025, -0.0108],
        [-0.0109,  0.0028, -0.0108,  ..., -0.0079, -0.0141, -0.0030],
        [ 0.0059, -0.0083, -0.0158,  ...,  0.0088, -0.0014,  0.0096],
        ...,
        [-0.0222, -0.0173,  0.0047,  ...,  0.0033,  0.0046,  0.0333],
        [-0.0184,  0.0009,  0.0008,  ...,  0.0047, -0.0026, -0.0024],
        [ 0.0007,  0.0179, -0.0140,  ..., -0.0214,  0.0212,  0.0040]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7559,  2.6621, -6.6289,  ...,  3.6836,  2.7578,  0.9473]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:48:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for shore is coast
Another word for incorrect is wrong
Another word for sweets is confectionery
Another word for organized is arranged
Another word for portion is part
Another word for market is marketplace
Another word for child is kid
Another word for homogeneous is
2024-07-23 22:48:54 root INFO     [order_1_approx] starting weight calculation for Another word for portion is part
Another word for homogeneous is uniform
Another word for organized is arranged
Another word for market is marketplace
Another word for child is kid
Another word for shore is coast
Another word for sweets is confectionery
Another word for incorrect is
2024-07-23 22:48:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 22:52:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9512, -0.3254,  0.3828,  ...,  0.3169,  0.5186,  1.4316],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7529,  1.4102, -2.5352,  ...,  0.9600,  7.0078, -1.0566],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0152, -0.0044,  0.0193,  ..., -0.0110, -0.0117, -0.0066],
        [ 0.0293, -0.0202, -0.0040,  ...,  0.0283, -0.0040, -0.0093],
        [-0.0077, -0.0007, -0.0008,  ...,  0.0007, -0.0048,  0.0201],
        ...,
        [-0.0082, -0.0171,  0.0042,  ..., -0.0034, -0.0009, -0.0062],
        [-0.0016, -0.0112, -0.0041,  ...,  0.0041,  0.0069,  0.0100],
        [ 0.0115,  0.0126, -0.0067,  ...,  0.0038,  0.0185, -0.0043]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7388,  1.2490, -2.4902,  ...,  1.0869,  7.1133, -1.1035]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:52:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for portion is part
Another word for homogeneous is uniform
Another word for organized is arranged
Another word for market is marketplace
Another word for child is kid
Another word for shore is coast
Another word for sweets is confectionery
Another word for incorrect is
2024-07-23 22:52:38 root INFO     [order_1_approx] starting weight calculation for Another word for shore is coast
Another word for organized is arranged
Another word for incorrect is wrong
Another word for portion is part
Another word for homogeneous is uniform
Another word for sweets is confectionery
Another word for child is kid
Another word for market is
2024-07-23 22:52:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 22:56:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1241, -0.2288,  0.4038,  ...,  1.0928, -0.0029,  0.4248],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.3008, -2.2129, -6.6016,  ..., -0.6489,  1.3125,  3.7930],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0290,  0.0046,  0.0235,  ..., -0.0023, -0.0191,  0.0105],
        [ 0.0080,  0.0294, -0.0059,  ...,  0.0059, -0.0142,  0.0058],
        [ 0.0073,  0.0084, -0.0122,  ..., -0.0166,  0.0152,  0.0344],
        ...,
        [-0.0147,  0.0029,  0.0138,  ...,  0.0293, -0.0080,  0.0155],
        [-0.0015,  0.0030, -0.0112,  ..., -0.0108,  0.0014, -0.0030],
        [-0.0128, -0.0158, -0.0259,  ...,  0.0019,  0.0084,  0.0291]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.0469, -0.4326, -5.8164,  ..., -1.0195,  1.0586,  4.2070]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:56:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for shore is coast
Another word for organized is arranged
Another word for incorrect is wrong
Another word for portion is part
Another word for homogeneous is uniform
Another word for sweets is confectionery
Another word for child is kid
Another word for market is
2024-07-23 22:56:17 root INFO     [order_1_approx] starting weight calculation for Another word for shore is coast
Another word for child is kid
Another word for portion is part
Another word for incorrect is wrong
Another word for homogeneous is uniform
Another word for market is marketplace
Another word for sweets is confectionery
Another word for organized is
2024-07-23 22:56:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 22:59:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1631,  0.3557, -0.5962,  ...,  0.0555,  1.6055,  0.5464],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.0039, -2.9453, -4.1250,  ..., -1.8291,  4.5352,  2.3848],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0080,  0.0055,  0.0018,  ..., -0.0064, -0.0108, -0.0182],
        [ 0.0104,  0.0103,  0.0108,  ...,  0.0036, -0.0091,  0.0198],
        [ 0.0093, -0.0085,  0.0101,  ..., -0.0022, -0.0213,  0.0264],
        ...,
        [-0.0149, -0.0175,  0.0176,  ...,  0.0051, -0.0063,  0.0259],
        [ 0.0034, -0.0349,  0.0054,  ...,  0.0122,  0.0156,  0.0063],
        [-0.0074, -0.0113, -0.0040,  ...,  0.0047,  0.0294,  0.0280]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.8281, -1.9590, -2.8262,  ..., -0.7461,  3.3945,  3.3066]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:59:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for shore is coast
Another word for child is kid
Another word for portion is part
Another word for incorrect is wrong
Another word for homogeneous is uniform
Another word for market is marketplace
Another word for sweets is confectionery
Another word for organized is
2024-07-23 22:59:59 root INFO     [order_1_approx] starting weight calculation for Another word for sweets is confectionery
Another word for organized is arranged
Another word for homogeneous is uniform
Another word for market is marketplace
Another word for child is kid
Another word for incorrect is wrong
Another word for shore is coast
Another word for portion is
2024-07-23 22:59:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 23:03:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3977, -0.0688, -0.9834,  ...,  0.1714,  0.1405,  2.3281],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.7109,  1.3125, -1.3887,  ..., -1.0977,  3.1973, -1.1357],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0134, -0.0161,  0.0057,  ..., -0.0067, -0.0089,  0.0108],
        [-0.0040, -0.0008,  0.0125,  ...,  0.0157, -0.0171,  0.0032],
        [ 0.0115, -0.0016,  0.0209,  ...,  0.0070, -0.0206,  0.0170],
        ...,
        [-0.0135,  0.0068,  0.0011,  ...,  0.0080,  0.0015,  0.0127],
        [-0.0061,  0.0128, -0.0041,  ..., -0.0097, -0.0148, -0.0191],
        [-0.0073,  0.0067, -0.0207,  ...,  0.0082, -0.0043,  0.0031]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.7734,  0.8311, -1.6162,  ..., -0.8101,  2.7070, -1.7949]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:03:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for sweets is confectionery
Another word for organized is arranged
Another word for homogeneous is uniform
Another word for market is marketplace
Another word for child is kid
Another word for incorrect is wrong
Another word for shore is coast
Another word for portion is
2024-07-23 23:03:42 root INFO     [order_1_approx] starting weight calculation for Another word for organized is arranged
Another word for market is marketplace
Another word for portion is part
Another word for sweets is confectionery
Another word for incorrect is wrong
Another word for homogeneous is uniform
Another word for child is kid
Another word for shore is
2024-07-23 23:03:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 23:07:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0417,  0.0532, -0.5322,  ...,  0.5635, -0.1187,  0.4055],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.3125,  4.3750, -1.4561,  ...,  3.0020,  2.0625,  0.5781],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0053, -0.0079,  0.0017,  ...,  0.0186, -0.0097, -0.0063],
        [ 0.0182, -0.0067, -0.0043,  ...,  0.0069, -0.0083,  0.0214],
        [-0.0334,  0.0183, -0.0178,  ..., -0.0093,  0.0140,  0.0123],
        ...,
        [ 0.0076, -0.0264,  0.0032,  ...,  0.0029, -0.0112,  0.0197],
        [-0.0015,  0.0069, -0.0039,  ..., -0.0103, -0.0193,  0.0215],
        [-0.0053, -0.0057,  0.0121,  ..., -0.0109, -0.0209, -0.0007]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.6875,  5.0625, -1.1289,  ...,  3.2461,  2.2773,  1.4551]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:07:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for organized is arranged
Another word for market is marketplace
Another word for portion is part
Another word for sweets is confectionery
Another word for incorrect is wrong
Another word for homogeneous is uniform
Another word for child is kid
Another word for shore is
2024-07-23 23:07:23 root INFO     [order_1_approx] starting weight calculation for Another word for market is marketplace
Another word for child is kid
Another word for incorrect is wrong
Another word for organized is arranged
Another word for shore is coast
Another word for homogeneous is uniform
Another word for portion is part
Another word for sweets is
2024-07-23 23:07:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 23:10:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([1.3125, 0.2400, 0.0232,  ..., 1.0527, 0.7349, 0.2939], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2124,  2.2422, -3.6895,  ..., -1.9355,  3.4824, -2.5527],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0034,  0.0062,  0.0055,  ..., -0.0023, -0.0088, -0.0002],
        [ 0.0107, -0.0061,  0.0129,  ..., -0.0103, -0.0135, -0.0084],
        [-0.0137,  0.0274, -0.0350,  ...,  0.0007,  0.0138,  0.0064],
        ...,
        [-0.0010,  0.0001,  0.0079,  ...,  0.0197, -0.0042,  0.0106],
        [ 0.0002,  0.0006,  0.0256,  ..., -0.0073, -0.0062, -0.0005],
        [-0.0153, -0.0195, -0.0161,  ...,  0.0279, -0.0033,  0.0096]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3271,  1.1367, -3.8711,  ..., -1.9941,  2.3984, -1.4902]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:10:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for market is marketplace
Another word for child is kid
Another word for incorrect is wrong
Another word for organized is arranged
Another word for shore is coast
Another word for homogeneous is uniform
Another word for portion is part
Another word for sweets is
2024-07-23 23:10:56 root INFO     total operator prediction time: 1751.547857761383 seconds
2024-07-23 23:10:56 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on hypernyms - misc
2024-07-23 23:10:56 root INFO     building operator hypernyms - misc
2024-07-23 23:10:56 root INFO     [order_1_approx] starting weight calculation for The pie falls into the category of pastry
The fridge falls into the category of appliance
The cup falls into the category of tableware
The jeans falls into the category of trousers
The desk falls into the category of furniture
The sweater falls into the category of clothes
The necklace falls into the category of jewelry
The blender falls into the category of
2024-07-23 23:10:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 23:14:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.3418,  0.5889, -1.2754,  ...,  0.4941, -0.2262,  0.7139],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.5820,  1.2705, -0.2764,  ...,  1.5459, -2.4297, -1.7490],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0273,  0.0002,  0.0005,  ...,  0.0052, -0.0072, -0.0025],
        [-0.0162,  0.0120, -0.0100,  ...,  0.0078,  0.0021, -0.0024],
        [-0.0031, -0.0095,  0.0110,  ..., -0.0121,  0.0021,  0.0077],
        ...,
        [-0.0099, -0.0073, -0.0055,  ...,  0.0209, -0.0054,  0.0025],
        [ 0.0002,  0.0047,  0.0094,  ...,  0.0005, -0.0038,  0.0217],
        [ 0.0024, -0.0105, -0.0042,  ...,  0.0064,  0.0018,  0.0147]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.1758,  1.9248, -0.0554,  ...,  1.6016, -2.4355, -1.1484]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:14:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The pie falls into the category of pastry
The fridge falls into the category of appliance
The cup falls into the category of tableware
The jeans falls into the category of trousers
The desk falls into the category of furniture
The sweater falls into the category of clothes
The necklace falls into the category of jewelry
The blender falls into the category of
2024-07-23 23:14:34 root INFO     [order_1_approx] starting weight calculation for The jeans falls into the category of trousers
The blender falls into the category of appliance
The sweater falls into the category of clothes
The desk falls into the category of furniture
The necklace falls into the category of jewelry
The pie falls into the category of pastry
The fridge falls into the category of appliance
The cup falls into the category of
2024-07-23 23:14:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 23:18:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2113, -0.5581, -0.1677,  ...,  0.1953, -0.3970,  1.0156],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.0586, -0.4463, -2.1016,  ..., -0.5103,  0.6958,  1.4590],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.9333e-02,  1.5640e-02,  4.5052e-03,  ...,  1.3170e-03,
         -9.0790e-03,  6.7749e-03],
        [ 2.9411e-03,  2.9411e-03, -5.1651e-03,  ...,  7.6752e-03,
          4.2572e-03,  1.3065e-03],
        [-4.5013e-03,  1.3008e-02,  7.0038e-03,  ...,  5.5046e-03,
          5.3406e-05, -1.5945e-03],
        ...,
        [-1.1757e-02, -4.5815e-03,  5.4855e-03,  ...,  1.4183e-02,
         -3.6926e-03, -7.6370e-03],
        [-6.0310e-03,  9.1629e-03,  1.2108e-02,  ...,  1.3328e-04,
         -1.3695e-03, -5.6496e-03],
        [ 7.4768e-04, -8.8959e-03,  2.2163e-03,  ...,  1.1620e-02,
          9.6588e-03,  1.4534e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.8896, -0.7139, -1.5039,  ..., -0.7363,  0.3728,  1.1875]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:18:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The jeans falls into the category of trousers
The blender falls into the category of appliance
The sweater falls into the category of clothes
The desk falls into the category of furniture
The necklace falls into the category of jewelry
The pie falls into the category of pastry
The fridge falls into the category of appliance
The cup falls into the category of
2024-07-23 23:18:15 root INFO     [order_1_approx] starting weight calculation for The jeans falls into the category of trousers
The blender falls into the category of appliance
The necklace falls into the category of jewelry
The fridge falls into the category of appliance
The sweater falls into the category of clothes
The pie falls into the category of pastry
The cup falls into the category of tableware
The desk falls into the category of
2024-07-23 23:18:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 23:21:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4424, -0.4233,  1.0020,  ...,  0.2808, -0.4219,  0.1941],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1348, -1.5781,  0.2080,  ..., -0.6919,  0.3770, -2.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0219, -0.0044, -0.0015,  ..., -0.0055, -0.0192,  0.0028],
        [-0.0089,  0.0269, -0.0003,  ...,  0.0096,  0.0066,  0.0067],
        [-0.0073, -0.0012, -0.0078,  ...,  0.0016, -0.0004,  0.0130],
        ...,
        [-0.0074,  0.0112,  0.0020,  ...,  0.0085, -0.0101,  0.0014],
        [-0.0078, -0.0029, -0.0012,  ..., -0.0010, -0.0052,  0.0025],
        [ 0.0048, -0.0035, -0.0033,  ...,  0.0064,  0.0074,  0.0170]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1460, -1.1094, -0.1675,  ..., -0.5391,  0.6743, -2.5410]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:21:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The jeans falls into the category of trousers
The blender falls into the category of appliance
The necklace falls into the category of jewelry
The fridge falls into the category of appliance
The sweater falls into the category of clothes
The pie falls into the category of pastry
The cup falls into the category of tableware
The desk falls into the category of
2024-07-23 23:21:52 root INFO     [order_1_approx] starting weight calculation for The desk falls into the category of furniture
The cup falls into the category of tableware
The pie falls into the category of pastry
The jeans falls into the category of trousers
The necklace falls into the category of jewelry
The blender falls into the category of appliance
The sweater falls into the category of clothes
The fridge falls into the category of
2024-07-23 23:21:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 23:25:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1541, -0.2388, -0.5132,  ..., -1.0723,  0.8789,  1.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-5.2148, -1.8125, -1.6777,  ..., -3.0488, -0.1558, -0.1523],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0225, -0.0059, -0.0057,  ...,  0.0049, -0.0112,  0.0125],
        [-0.0141,  0.0109,  0.0028,  ..., -0.0021, -0.0103, -0.0007],
        [-0.0080,  0.0076,  0.0066,  ...,  0.0093,  0.0087,  0.0280],
        ...,
        [-0.0073, -0.0013,  0.0039,  ...,  0.0090, -0.0013,  0.0013],
        [ 0.0096,  0.0082,  0.0071,  ...,  0.0033, -0.0054,  0.0041],
        [-0.0148, -0.0127,  0.0030,  ..., -0.0059,  0.0147,  0.0098]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.8594, -1.7168, -2.1816,  ..., -3.4414, -1.0508, -0.4050]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:25:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The desk falls into the category of furniture
The cup falls into the category of tableware
The pie falls into the category of pastry
The jeans falls into the category of trousers
The necklace falls into the category of jewelry
The blender falls into the category of appliance
The sweater falls into the category of clothes
The fridge falls into the category of
2024-07-23 23:25:26 root INFO     [order_1_approx] starting weight calculation for The sweater falls into the category of clothes
The cup falls into the category of tableware
The fridge falls into the category of appliance
The blender falls into the category of appliance
The pie falls into the category of pastry
The necklace falls into the category of jewelry
The desk falls into the category of furniture
The jeans falls into the category of
2024-07-23 23:25:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 23:28:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7891, -1.0371, -0.3945,  ..., -0.3960,  0.1721,  0.3896],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1172, -0.8901,  0.6611,  ..., -5.3711, -1.1777, -2.5684],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 8.5449e-03, -1.8173e-02, -4.0054e-05,  ...,  1.0277e-02,
         -2.6226e-03, -2.3460e-03],
        [-6.9084e-03,  1.3344e-02,  4.4107e-05,  ...,  1.2238e-02,
         -5.8403e-03, -4.8161e-04],
        [ 8.6670e-03, -1.4282e-02, -2.1019e-03,  ...,  2.1362e-03,
         -8.2397e-03, -1.7509e-03],
        ...,
        [-9.9487e-03,  6.9427e-03,  1.1948e-02,  ...,  8.3618e-03,
         -3.0899e-04,  1.1810e-02],
        [ 9.1171e-04, -2.5330e-03,  5.2929e-04,  ...,  1.5579e-02,
          4.7417e-03,  1.0658e-02],
        [-1.0101e-02, -1.2192e-02,  3.7327e-03,  ...,  5.0659e-03,
          1.6251e-02,  6.4392e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9131, -1.0439, -0.2485,  ..., -5.7773, -1.8945, -2.6836]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:29:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sweater falls into the category of clothes
The cup falls into the category of tableware
The fridge falls into the category of appliance
The blender falls into the category of appliance
The pie falls into the category of pastry
The necklace falls into the category of jewelry
The desk falls into the category of furniture
The jeans falls into the category of
2024-07-23 23:29:00 root INFO     [order_1_approx] starting weight calculation for The desk falls into the category of furniture
The blender falls into the category of appliance
The fridge falls into the category of appliance
The jeans falls into the category of trousers
The pie falls into the category of pastry
The cup falls into the category of tableware
The sweater falls into the category of clothes
The necklace falls into the category of
2024-07-23 23:29:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 23:32:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3208, -0.8086,  0.4587,  ..., -0.8804,  0.9238,  0.5664],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.0312, -2.1094, -0.8555,  ..., -6.5703, -3.6914,  1.1592],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0114,  0.0073,  0.0007,  ...,  0.0102,  0.0058,  0.0027],
        [-0.0092,  0.0003,  0.0044,  ...,  0.0143, -0.0035,  0.0119],
        [-0.0002,  0.0070, -0.0008,  ...,  0.0162,  0.0079,  0.0046],
        ...,
        [ 0.0019,  0.0021,  0.0067,  ...,  0.0063, -0.0040,  0.0064],
        [ 0.0036,  0.0027,  0.0067,  ...,  0.0075,  0.0024,  0.0182],
        [-0.0032, -0.0145,  0.0146,  ...,  0.0032,  0.0074,  0.0085]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.3516, -1.9541, -1.1396,  ..., -6.5977, -4.4609,  0.9834]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:32:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The desk falls into the category of furniture
The blender falls into the category of appliance
The fridge falls into the category of appliance
The jeans falls into the category of trousers
The pie falls into the category of pastry
The cup falls into the category of tableware
The sweater falls into the category of clothes
The necklace falls into the category of
2024-07-23 23:32:34 root INFO     [order_1_approx] starting weight calculation for The sweater falls into the category of clothes
The blender falls into the category of appliance
The jeans falls into the category of trousers
The desk falls into the category of furniture
The cup falls into the category of tableware
The necklace falls into the category of jewelry
The fridge falls into the category of appliance
The pie falls into the category of
2024-07-23 23:32:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 23:36:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2224,  0.5723, -0.2344,  ...,  0.8213, -0.3528,  1.1973],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.4297,  0.5552, -2.1074,  ..., -0.9160, -0.9390,  3.0898],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0243, -0.0022,  0.0052,  ..., -0.0032,  0.0053,  0.0020],
        [-0.0145,  0.0030, -0.0079,  ...,  0.0060, -0.0044, -0.0068],
        [-0.0035,  0.0067, -0.0019,  ...,  0.0062, -0.0017,  0.0070],
        ...,
        [-0.0092,  0.0137,  0.0004,  ...,  0.0002,  0.0049,  0.0109],
        [ 0.0075,  0.0064,  0.0043,  ..., -0.0125, -0.0028, -0.0031],
        [-0.0075, -0.0081,  0.0018,  ..., -0.0016,  0.0012,  0.0032]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.7500,  0.6846, -2.0977,  ..., -1.2061, -1.2373,  3.1895]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:36:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sweater falls into the category of clothes
The blender falls into the category of appliance
The jeans falls into the category of trousers
The desk falls into the category of furniture
The cup falls into the category of tableware
The necklace falls into the category of jewelry
The fridge falls into the category of appliance
The pie falls into the category of
2024-07-23 23:36:11 root INFO     [order_1_approx] starting weight calculation for The jeans falls into the category of trousers
The desk falls into the category of furniture
The fridge falls into the category of appliance
The pie falls into the category of pastry
The blender falls into the category of appliance
The necklace falls into the category of jewelry
The cup falls into the category of tableware
The sweater falls into the category of
2024-07-23 23:36:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 23:39:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1837,  0.0488, -0.4785,  ...,  0.5874,  0.8135, -0.3699],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5254, -0.1743,  1.2051,  ..., -4.5078, -2.3770, -4.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0127, -0.0083, -0.0045,  ...,  0.0062,  0.0008,  0.0230],
        [-0.0122,  0.0082, -0.0096,  ...,  0.0094, -0.0120, -0.0086],
        [-0.0138, -0.0004, -0.0081,  ...,  0.0073,  0.0104, -0.0036],
        ...,
        [ 0.0013,  0.0084, -0.0066,  ...,  0.0109, -0.0034,  0.0246],
        [ 0.0023,  0.0107, -0.0058,  ...,  0.0073,  0.0063,  0.0171],
        [-0.0043, -0.0184,  0.0170,  ..., -0.0072,  0.0205,  0.0146]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.2949, -0.2036,  0.3354,  ..., -4.5586, -3.6836, -4.3984]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:39:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The jeans falls into the category of trousers
The desk falls into the category of furniture
The fridge falls into the category of appliance
The pie falls into the category of pastry
The blender falls into the category of appliance
The necklace falls into the category of jewelry
The cup falls into the category of tableware
The sweater falls into the category of
2024-07-23 23:39:39 root INFO     total operator prediction time: 1723.0871977806091 seconds
2024-07-23 23:39:39 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - substance
2024-07-23 23:39:39 root INFO     building operator meronyms - substance
2024-07-23 23:39:39 root INFO     [order_1_approx] starting weight calculation for A glass is made up of silicone
A chocolate is made up of cocoa
A jeans is made up of fabric
A yogurt is made up of milk
A glacier is made up of ice
A doorknob is made up of metal
A cloud is made up of vapor
A beach is made up of
2024-07-23 23:39:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 23:43:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2178,  0.5537,  0.2820,  ..., -0.6357, -0.1404, -0.7021],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3281,  2.1328, -1.0215,  ...,  3.3887, -2.2656, -0.3809],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0104, -0.0080, -0.0048,  ..., -0.0068, -0.0003, -0.0088],
        [-0.0086,  0.0031, -0.0005,  ...,  0.0001,  0.0018, -0.0008],
        [ 0.0015, -0.0027, -0.0075,  ..., -0.0012,  0.0165, -0.0029],
        ...,
        [ 0.0026, -0.0045,  0.0050,  ...,  0.0022, -0.0001,  0.0085],
        [-0.0049, -0.0038, -0.0098,  ..., -0.0036, -0.0039, -0.0009],
        [ 0.0005,  0.0068,  0.0133,  ...,  0.0034,  0.0118,  0.0203]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3364,  1.9639, -1.2227,  ...,  3.3457, -2.5273,  0.3687]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:43:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A glass is made up of silicone
A chocolate is made up of cocoa
A jeans is made up of fabric
A yogurt is made up of milk
A glacier is made up of ice
A doorknob is made up of metal
A cloud is made up of vapor
A beach is made up of
2024-07-23 23:43:21 root INFO     [order_1_approx] starting weight calculation for A cloud is made up of vapor
A glass is made up of silicone
A beach is made up of sand
A glacier is made up of ice
A yogurt is made up of milk
A doorknob is made up of metal
A jeans is made up of fabric
A chocolate is made up of
2024-07-23 23:43:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 23:46:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1132,  0.8008,  1.5576,  ...,  0.9199,  0.0447,  1.0068],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5859,  0.7529,  1.6191,  ...,  3.4609,  1.1426,  1.1367],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.2951e-03, -1.7532e-02, -4.1161e-03,  ...,  4.6349e-03,
         -1.6296e-02,  4.1046e-03],
        [-4.3488e-03, -9.2773e-03, -8.3466e-03,  ...,  4.3182e-03,
          1.0452e-02, -2.9373e-04],
        [-8.1177e-03,  9.2697e-04, -1.2718e-02,  ...,  9.9182e-05,
         -1.1925e-02,  1.8387e-03],
        ...,
        [ 9.6054e-03,  9.5673e-03,  1.0391e-02,  ...,  3.5477e-03,
          1.0330e-02,  9.5901e-03],
        [ 8.9645e-03, -1.6983e-02,  6.6147e-03,  ...,  7.0419e-03,
         -6.4964e-03,  2.0332e-03],
        [ 8.0490e-03,  9.6664e-03, -1.2383e-02,  ..., -1.7639e-02,
          1.1185e-02,  2.7561e-04]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3872,  0.7383,  1.3799,  ...,  3.9336,  1.0205,  1.9316]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:46:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A cloud is made up of vapor
A glass is made up of silicone
A beach is made up of sand
A glacier is made up of ice
A yogurt is made up of milk
A doorknob is made up of metal
A jeans is made up of fabric
A chocolate is made up of
2024-07-23 23:46:56 root INFO     [order_1_approx] starting weight calculation for A glacier is made up of ice
A glass is made up of silicone
A yogurt is made up of milk
A doorknob is made up of metal
A beach is made up of sand
A chocolate is made up of cocoa
A jeans is made up of fabric
A cloud is made up of
2024-07-23 23:46:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 23:50:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5205,  0.2236,  1.0469,  ..., -0.0182,  0.8569,  1.0137],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5181, -1.3535,  6.0742,  ...,  2.6582,  1.4219, -0.2920],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-9.2316e-03, -9.6970e-03,  3.0251e-03,  ...,  5.6839e-03,
         -7.1030e-03, -2.6417e-03],
        [-1.4374e-02,  6.7482e-03, -1.2207e-04,  ...,  2.2297e-03,
         -1.9207e-03,  2.3270e-04],
        [-9.3384e-03,  5.2567e-03, -1.2417e-03,  ...,  3.3703e-03,
          4.7073e-03, -2.1744e-03],
        ...,
        [ 7.3195e-04,  5.3787e-03,  1.3847e-03,  ..., -1.1253e-02,
         -1.3115e-02,  1.6571e-02],
        [-1.8494e-02, -5.7449e-03,  1.1444e-05,  ...,  1.2451e-02,
          1.0376e-03, -9.2392e-03],
        [ 9.1171e-03,  1.6357e-02,  9.2163e-03,  ..., -1.6373e-02,
         -5.0964e-03,  3.8395e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2053, -0.4468,  5.8945,  ...,  2.5332,  2.1387, -0.0295]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:50:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A glacier is made up of ice
A glass is made up of silicone
A yogurt is made up of milk
A doorknob is made up of metal
A beach is made up of sand
A chocolate is made up of cocoa
A jeans is made up of fabric
A cloud is made up of
2024-07-23 23:50:38 root INFO     [order_1_approx] starting weight calculation for A glacier is made up of ice
A jeans is made up of fabric
A cloud is made up of vapor
A chocolate is made up of cocoa
A beach is made up of sand
A glass is made up of silicone
A yogurt is made up of milk
A doorknob is made up of
2024-07-23 23:50:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 23:54:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0878,  1.0137, -0.1836,  ...,  0.1646, -0.2393,  1.1777],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0059, -0.5498,  1.7324,  ...,  0.6025,  0.6475,  5.7617],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0027, -0.0064, -0.0025,  ..., -0.0032, -0.0052, -0.0008],
        [-0.0014, -0.0006,  0.0074,  ...,  0.0025,  0.0100,  0.0001],
        [-0.0021,  0.0064,  0.0056,  ..., -0.0014,  0.0051, -0.0010],
        ...,
        [ 0.0017,  0.0024, -0.0039,  ..., -0.0047, -0.0101,  0.0082],
        [-0.0003, -0.0006,  0.0126,  ...,  0.0076,  0.0022,  0.0036],
        [ 0.0012,  0.0092,  0.0035,  ..., -0.0098,  0.0155,  0.0056]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.6465, -0.8242,  1.3438,  ...,  0.9482,  0.8711,  5.4297]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:54:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A glacier is made up of ice
A jeans is made up of fabric
A cloud is made up of vapor
A chocolate is made up of cocoa
A beach is made up of sand
A glass is made up of silicone
A yogurt is made up of milk
A doorknob is made up of
2024-07-23 23:54:19 root INFO     [order_1_approx] starting weight calculation for A cloud is made up of vapor
A glass is made up of silicone
A beach is made up of sand
A yogurt is made up of milk
A doorknob is made up of metal
A chocolate is made up of cocoa
A jeans is made up of fabric
A glacier is made up of
2024-07-23 23:54:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 23:58:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2563,  1.2168, -0.3037,  ...,  0.4717, -0.1719,  1.8252],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.2734,  2.8906, -1.3311,  ...,  4.7031,  1.1084, -0.2102],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.8959e-03, -3.6888e-03, -4.1809e-03,  ...,  3.8872e-03,
         -1.4124e-03,  8.9493e-03],
        [-5.5618e-03,  2.0943e-03,  3.2597e-03,  ...,  4.9324e-03,
          7.6866e-03,  1.9760e-03],
        [ 1.3435e-02, -7.3471e-03, -1.8177e-03,  ...,  3.5782e-03,
          4.8599e-03, -1.3885e-02],
        ...,
        [ 1.0735e-02,  1.0078e-02,  2.7485e-03,  ..., -7.1106e-03,
         -2.3632e-03,  1.4503e-02],
        [-1.0323e-02, -1.5659e-03, -3.7384e-04,  ...,  5.6686e-03,
          9.0561e-03, -6.1035e-05],
        [ 9.0103e-03,  9.9487e-03,  1.4038e-02,  ..., -5.2567e-03,
         -6.7825e-03, -8.8043e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0449,  3.1758, -1.7734,  ...,  4.5781,  1.0518,  0.2534]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:58:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A cloud is made up of vapor
A glass is made up of silicone
A beach is made up of sand
A yogurt is made up of milk
A doorknob is made up of metal
A chocolate is made up of cocoa
A jeans is made up of fabric
A glacier is made up of
2024-07-23 23:58:01 root INFO     [order_1_approx] starting weight calculation for A glacier is made up of ice
A cloud is made up of vapor
A chocolate is made up of cocoa
A jeans is made up of fabric
A yogurt is made up of milk
A doorknob is made up of metal
A beach is made up of sand
A glass is made up of
2024-07-23 23:58:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 00:01:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2406, -0.8809,  0.7383,  ...,  0.3899, -0.6260,  1.2266],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5571,  2.5391,  0.9731,  ...,  0.2231, -2.8086,  2.2734],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.5259e-02, -1.7853e-02, -6.5765e-03,  ..., -1.3409e-03,
          1.5574e-03, -1.0071e-03],
        [-1.8341e-02, -1.7128e-03,  2.2964e-03,  ...,  1.5503e-02,
          1.9043e-02,  8.9264e-03],
        [ 7.0267e-03, -5.9662e-03,  6.7596e-03,  ..., -9.3231e-03,
         -8.4229e-03,  1.2970e-04],
        ...,
        [ 1.5343e-02,  2.0695e-03,  1.8082e-03,  ...,  1.8173e-02,
         -1.3428e-02,  3.3112e-02],
        [-7.6294e-06,  8.8501e-04, -2.9984e-03,  ...,  5.6305e-03,
          6.3400e-03,  6.2218e-03],
        [-2.5387e-03, -5.7220e-03,  5.4016e-03,  ..., -9.7504e-03,
          1.3786e-02,  9.9945e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6626,  2.1758,  1.3389,  ...,  1.2285, -2.0215,  2.1250]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:01:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A glacier is made up of ice
A cloud is made up of vapor
A chocolate is made up of cocoa
A jeans is made up of fabric
A yogurt is made up of milk
A doorknob is made up of metal
A beach is made up of sand
A glass is made up of
2024-07-24 00:01:42 root INFO     [order_1_approx] starting weight calculation for A yogurt is made up of milk
A glass is made up of silicone
A glacier is made up of ice
A beach is made up of sand
A chocolate is made up of cocoa
A cloud is made up of vapor
A doorknob is made up of metal
A jeans is made up of
2024-07-24 00:01:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 00:05:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9468, -0.8335, -0.2026,  ...,  0.1025, -0.3938,  0.6973],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4219, -1.5576, -0.3354,  ...,  0.2646, -0.0060, -2.6836],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0077, -0.0152, -0.0059,  ...,  0.0101, -0.0009, -0.0041],
        [-0.0049,  0.0068,  0.0044,  ...,  0.0086, -0.0037, -0.0002],
        [ 0.0042,  0.0002, -0.0116,  ..., -0.0069,  0.0054,  0.0047],
        ...,
        [ 0.0139,  0.0035,  0.0072,  ...,  0.0065, -0.0002,  0.0171],
        [ 0.0015, -0.0089, -0.0027,  ...,  0.0131, -0.0120, -0.0071],
        [ 0.0044, -0.0038, -0.0042,  ..., -0.0021,  0.0036, -0.0036]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.8867, -0.9565, -0.6187,  ...,  0.6279, -0.2461, -2.7324]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:05:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A yogurt is made up of milk
A glass is made up of silicone
A glacier is made up of ice
A beach is made up of sand
A chocolate is made up of cocoa
A cloud is made up of vapor
A doorknob is made up of metal
A jeans is made up of
2024-07-24 00:05:22 root INFO     [order_1_approx] starting weight calculation for A glass is made up of silicone
A cloud is made up of vapor
A glacier is made up of ice
A chocolate is made up of cocoa
A jeans is made up of fabric
A beach is made up of sand
A doorknob is made up of metal
A yogurt is made up of
2024-07-24 00:05:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 00:09:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1119, -0.4700,  0.0522,  ...,  0.1136, -0.7529,  2.1680],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.9277, -0.4563,  4.2773,  ...,  5.3867, -0.5215,  1.4160],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0053, -0.0145, -0.0120,  ...,  0.0124, -0.0053, -0.0039],
        [-0.0113,  0.0104, -0.0076,  ...,  0.0150,  0.0134, -0.0052],
        [-0.0054, -0.0094, -0.0158,  ..., -0.0006,  0.0078,  0.0087],
        ...,
        [-0.0032,  0.0064, -0.0036,  ...,  0.0102,  0.0059,  0.0090],
        [-0.0025, -0.0156,  0.0036,  ...,  0.0007,  0.0031, -0.0061],
        [ 0.0121,  0.0196, -0.0068,  ..., -0.0012,  0.0130,  0.0141]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.3828, -0.1082,  4.0156,  ...,  5.5820, -0.4075,  1.5039]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:09:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A glass is made up of silicone
A cloud is made up of vapor
A glacier is made up of ice
A chocolate is made up of cocoa
A jeans is made up of fabric
A beach is made up of sand
A doorknob is made up of metal
A yogurt is made up of
2024-07-24 00:09:04 root INFO     total operator prediction time: 1764.8947565555573 seconds
2024-07-24 00:09:04 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on synonyms - intensity
2024-07-24 00:09:04 root INFO     building operator synonyms - intensity
2024-07-24 00:09:04 root INFO     [order_1_approx] starting weight calculation for A more intense word for tasty is delicious
A more intense word for snack is meal
A more intense word for strong is powerful
A more intense word for guilty is remorseful
A more intense word for interesting is exciting
A more intense word for soon is immediately
A more intense word for like is love
A more intense word for giggle is
2024-07-24 00:09:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 00:12:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1211, -1.1885,  0.9961,  ...,  1.4531,  1.0303,  2.0098],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5854,  0.7988, -0.7490,  ...,  0.6338,  5.7734, -2.4102],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0062,  0.0042,  0.0077,  ...,  0.0133, -0.0148, -0.0158],
        [-0.0081, -0.0044, -0.0032,  ...,  0.0072,  0.0019, -0.0062],
        [-0.0128, -0.0065, -0.0091,  ..., -0.0096,  0.0008, -0.0158],
        ...,
        [-0.0103, -0.0129, -0.0178,  ...,  0.0047, -0.0162,  0.0062],
        [-0.0027,  0.0042,  0.0002,  ...,  0.0027, -0.0193,  0.0064],
        [ 0.0115, -0.0046, -0.0372,  ...,  0.0099, -0.0022,  0.0148]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.8525e-01,  2.6270e-01, -1.7920e-01,  ...,  3.4180e-03,
          5.7031e+00, -3.4688e+00]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SubBackward0>) 

                    
2024-07-24 00:12:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for tasty is delicious
A more intense word for snack is meal
A more intense word for strong is powerful
A more intense word for guilty is remorseful
A more intense word for interesting is exciting
A more intense word for soon is immediately
A more intense word for like is love
A more intense word for giggle is
2024-07-24 00:12:47 root INFO     [order_1_approx] starting weight calculation for A more intense word for strong is powerful
A more intense word for tasty is delicious
A more intense word for snack is meal
A more intense word for like is love
A more intense word for soon is immediately
A more intense word for interesting is exciting
A more intense word for giggle is laugh
A more intense word for guilty is
2024-07-24 00:12:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 00:16:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0840, -0.2209, -0.6133,  ...,  0.7173, -0.2461,  0.4087],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6621, -1.8525, -1.4834,  ..., -1.5830,  0.9150,  1.4961],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-9.4757e-03, -2.4628e-02, -4.9667e-03,  ...,  1.3641e-02,
         -2.0065e-02, -1.4488e-02],
        [ 1.1475e-02,  2.4357e-03,  6.3896e-03,  ..., -1.1017e-02,
         -2.2202e-02, -1.3771e-02],
        [ 7.9803e-03,  6.5269e-03, -8.1253e-03,  ..., -1.3336e-02,
          4.6692e-03,  3.2623e-02],
        ...,
        [-1.7120e-02, -1.7151e-02, -2.0370e-03,  ...,  4.1122e-03,
         -5.6152e-03,  1.9608e-02],
        [-1.1902e-02,  1.1444e-05,  2.2141e-02,  ...,  1.0254e-02,
         -9.0485e-03, -1.5961e-02],
        [ 1.2268e-02, -1.1726e-02,  2.2888e-04,  ...,  6.4850e-03,
         -3.0727e-03,  1.7014e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5137, -1.7793, -1.3379,  ..., -0.7759,  0.5244,  1.2217]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:16:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for strong is powerful
A more intense word for tasty is delicious
A more intense word for snack is meal
A more intense word for like is love
A more intense word for soon is immediately
A more intense word for interesting is exciting
A more intense word for giggle is laugh
A more intense word for guilty is
2024-07-24 00:16:28 root INFO     [order_1_approx] starting weight calculation for A more intense word for soon is immediately
A more intense word for guilty is remorseful
A more intense word for tasty is delicious
A more intense word for like is love
A more intense word for snack is meal
A more intense word for strong is powerful
A more intense word for giggle is laugh
A more intense word for interesting is
2024-07-24 00:16:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 00:20:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0566, -0.1685,  0.8740,  ...,  1.0225,  0.8945,  0.6816],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.1953, -2.4551, -0.2910,  ..., -1.0566,  2.9180,  2.4414],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0091, -0.0251,  0.0228,  ...,  0.0015, -0.0079, -0.0208],
        [ 0.0193,  0.0369, -0.0007,  ...,  0.0111, -0.0073,  0.0119],
        [ 0.0166, -0.0057,  0.0321,  ...,  0.0093,  0.0092,  0.0006],
        ...,
        [-0.0352, -0.0201, -0.0062,  ..., -0.0240, -0.0190, -0.0035],
        [-0.0064,  0.0055,  0.0163,  ..., -0.0074,  0.0024,  0.0100],
        [-0.0184,  0.0114,  0.0004,  ..., -0.0199,  0.0201, -0.0049]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1064, -2.6250, -0.9146,  ..., -0.9990,  3.1484,  1.8770]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:20:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for soon is immediately
A more intense word for guilty is remorseful
A more intense word for tasty is delicious
A more intense word for like is love
A more intense word for snack is meal
A more intense word for strong is powerful
A more intense word for giggle is laugh
A more intense word for interesting is
2024-07-24 00:20:13 root INFO     [order_1_approx] starting weight calculation for A more intense word for interesting is exciting
A more intense word for guilty is remorseful
A more intense word for strong is powerful
A more intense word for soon is immediately
A more intense word for snack is meal
A more intense word for giggle is laugh
A more intense word for tasty is delicious
A more intense word for like is
2024-07-24 00:20:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 00:23:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5928, -0.1978,  1.1426,  ...,  0.6294, -0.3208,  0.9976],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.2266, -0.4927,  1.9375,  ..., -0.3301,  3.2715, -1.8379],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-9.5444e-03,  1.3184e-02,  1.7303e-02,  ...,  1.2978e-02,
         -8.9874e-03,  1.8555e-02],
        [ 1.1162e-02, -6.5079e-03, -2.3537e-03,  ...,  3.8567e-03,
         -3.6888e-03,  9.1553e-05],
        [ 5.3978e-03,  1.2589e-03,  1.3901e-02,  ...,  5.3253e-03,
         -3.3703e-03,  6.8550e-03],
        ...,
        [-3.4241e-02, -2.0386e-02, -6.3553e-03,  ...,  2.1591e-03,
         -1.3565e-02, -3.6255e-02],
        [ 2.9114e-02,  6.7520e-03, -2.2018e-02,  ..., -5.7564e-03,
          3.3665e-03,  7.1373e-03],
        [ 1.3542e-02,  2.7069e-02, -1.7914e-02,  ..., -1.6525e-02,
          1.0712e-02, -9.2468e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.9766, -0.3672,  3.2734,  ..., -0.2585,  2.0234, -1.9619]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:23:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for interesting is exciting
A more intense word for guilty is remorseful
A more intense word for strong is powerful
A more intense word for soon is immediately
A more intense word for snack is meal
A more intense word for giggle is laugh
A more intense word for tasty is delicious
A more intense word for like is
2024-07-24 00:23:57 root INFO     [order_1_approx] starting weight calculation for A more intense word for guilty is remorseful
A more intense word for strong is powerful
A more intense word for giggle is laugh
A more intense word for like is love
A more intense word for interesting is exciting
A more intense word for tasty is delicious
A more intense word for soon is immediately
A more intense word for snack is
2024-07-24 00:23:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 00:27:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2100,  0.1617, -0.1174,  ..., -0.3340,  0.4058,  0.8745],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.9678,  5.4414, -3.7891,  ..., -1.6982,  3.8828, -3.1777],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0077,  0.0052,  0.0175,  ..., -0.0136,  0.0113,  0.0113],
        [-0.0165, -0.0269,  0.0050,  ...,  0.0054, -0.0132,  0.0162],
        [ 0.0168,  0.0227, -0.0195,  ..., -0.0022, -0.0008,  0.0018],
        ...,
        [-0.0085, -0.0206,  0.0118,  ...,  0.0040, -0.0120,  0.0106],
        [-0.0259, -0.0154,  0.0085,  ...,  0.0080,  0.0002,  0.0144],
        [ 0.0276,  0.0292, -0.0157,  ..., -0.0042, -0.0113,  0.0015]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2598,  4.7539, -3.3008,  ..., -1.0928,  2.9141, -1.3574]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:27:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for guilty is remorseful
A more intense word for strong is powerful
A more intense word for giggle is laugh
A more intense word for like is love
A more intense word for interesting is exciting
A more intense word for tasty is delicious
A more intense word for soon is immediately
A more intense word for snack is
2024-07-24 00:27:36 root INFO     [order_1_approx] starting weight calculation for A more intense word for snack is meal
A more intense word for like is love
A more intense word for interesting is exciting
A more intense word for strong is powerful
A more intense word for guilty is remorseful
A more intense word for tasty is delicious
A more intense word for giggle is laugh
A more intense word for soon is
2024-07-24 00:27:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 00:31:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1859,  0.8535, -0.6318,  ...,  0.9526,  0.3081,  0.8213],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.5977, -0.3477, -4.2109,  ..., -0.7339,  3.9492, -0.6641],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0151,  0.0002,  0.0137,  ..., -0.0228, -0.0093,  0.0227],
        [ 0.0162, -0.0022,  0.0006,  ...,  0.0056, -0.0101,  0.0033],
        [-0.0002,  0.0103, -0.0010,  ...,  0.0029,  0.0029,  0.0042],
        ...,
        [-0.0135, -0.0204,  0.0004,  ..., -0.0028, -0.0095,  0.0157],
        [ 0.0009, -0.0006,  0.0177,  ..., -0.0013,  0.0037, -0.0031],
        [ 0.0163,  0.0130, -0.0020,  ...,  0.0111,  0.0014,  0.0010]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.3555, -0.0950, -4.8008,  ..., -0.5156,  3.5781, -0.5874]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:31:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for snack is meal
A more intense word for like is love
A more intense word for interesting is exciting
A more intense word for strong is powerful
A more intense word for guilty is remorseful
A more intense word for tasty is delicious
A more intense word for giggle is laugh
A more intense word for soon is
2024-07-24 00:31:17 root INFO     [order_1_approx] starting weight calculation for A more intense word for tasty is delicious
A more intense word for interesting is exciting
A more intense word for giggle is laugh
A more intense word for soon is immediately
A more intense word for like is love
A more intense word for snack is meal
A more intense word for guilty is remorseful
A more intense word for strong is
2024-07-24 00:31:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 00:35:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9058, -0.7891,  0.5898,  ...,  1.0205,  0.5791,  0.1470],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4785, -3.7227, -3.4590,  ..., -5.1289,  3.0117, -1.1680],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0058, -0.0150,  0.0197,  ..., -0.0132, -0.0089,  0.0037],
        [ 0.0126,  0.0200, -0.0058,  ..., -0.0032,  0.0037, -0.0104],
        [ 0.0166,  0.0077, -0.0075,  ...,  0.0097,  0.0091, -0.0197],
        ...,
        [-0.0080, -0.0048,  0.0139,  ..., -0.0061, -0.0023,  0.0090],
        [ 0.0015,  0.0045,  0.0051,  ..., -0.0152,  0.0099,  0.0020],
        [-0.0162, -0.0221,  0.0083,  ..., -0.0138,  0.0048,  0.0147]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1602, -3.5508, -2.5781,  ..., -4.7969,  3.2578, -1.4072]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:35:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for tasty is delicious
A more intense word for interesting is exciting
A more intense word for giggle is laugh
A more intense word for soon is immediately
A more intense word for like is love
A more intense word for snack is meal
A more intense word for guilty is remorseful
A more intense word for strong is
2024-07-24 00:35:01 root INFO     [order_1_approx] starting weight calculation for A more intense word for giggle is laugh
A more intense word for interesting is exciting
A more intense word for strong is powerful
A more intense word for like is love
A more intense word for soon is immediately
A more intense word for guilty is remorseful
A more intense word for snack is meal
A more intense word for tasty is
2024-07-24 00:35:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 00:38:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1836,  0.0184,  0.3645,  ..., -0.1211,  0.1726,  0.0215],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7891,  0.4600,  0.4180,  ..., -2.2012,  4.2422,  0.2769],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0150, -0.0147,  0.0320,  ...,  0.0087, -0.0048,  0.0004],
        [ 0.0085,  0.0025, -0.0025,  ..., -0.0166, -0.0156,  0.0161],
        [-0.0078, -0.0053,  0.0139,  ..., -0.0058,  0.0060,  0.0292],
        ...,
        [-0.0085, -0.0035,  0.0195,  ...,  0.0045, -0.0103,  0.0069],
        [ 0.0061,  0.0074,  0.0045,  ...,  0.0056,  0.0007, -0.0082],
        [-0.0063,  0.0047, -0.0142,  ..., -0.0044,  0.0149,  0.0165]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.4531,  0.7979,  0.7788,  ..., -1.1592,  3.6289, -0.0234]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:38:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for giggle is laugh
A more intense word for interesting is exciting
A more intense word for strong is powerful
A more intense word for like is love
A more intense word for soon is immediately
A more intense word for guilty is remorseful
A more intense word for snack is meal
A more intense word for tasty is
2024-07-24 00:38:35 root INFO     total operator prediction time: 1771.6040506362915 seconds
2024-07-24 00:38:35 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on hypernyms - animals
2024-07-24 00:38:35 root INFO     building operator hypernyms - animals
2024-07-24 00:38:35 root INFO     [order_1_approx] starting weight calculation for The goat falls into the category of bovid
The viper falls into the category of snake
The orangutan falls into the category of primate
The jackal falls into the category of canine
The vulture falls into the category of raptor
The falcon falls into the category of raptor
The fox falls into the category of canine
The chimpanzee falls into the category of
2024-07-24 00:38:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 00:42:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5938, -0.4385, -0.1521,  ..., -0.0137,  0.2241,  0.7266],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.8984,  0.0624,  4.9141,  ..., -1.1338, -1.5967, -0.1251],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-5.1880e-03, -1.0674e-02,  5.1308e-04,  ...,  1.9150e-03,
         -6.7368e-03, -3.5858e-03],
        [-5.5695e-03, -5.9204e-03,  8.9111e-03,  ...,  1.2283e-02,
          1.2779e-03, -3.1071e-03],
        [-4.1809e-03, -9.2926e-03, -1.6098e-03,  ..., -6.2256e-03,
          2.4853e-03,  2.7447e-03],
        ...,
        [-1.0971e-02,  2.1572e-03,  4.3030e-03,  ...,  1.2703e-03,
          2.0008e-03,  9.9487e-03],
        [-1.6975e-04, -3.9368e-03, -3.7346e-03,  ..., -5.3749e-03,
          4.9820e-03,  3.2501e-03],
        [-6.7139e-04, -1.0521e-02,  8.1177e-03,  ...,  7.0572e-05,
          1.7128e-03,  1.5259e-05]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.1914,  0.2227,  4.8711,  ..., -0.7480, -1.7930,  0.2220]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:42:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The goat falls into the category of bovid
The viper falls into the category of snake
The orangutan falls into the category of primate
The jackal falls into the category of canine
The vulture falls into the category of raptor
The falcon falls into the category of raptor
The fox falls into the category of canine
The chimpanzee falls into the category of
2024-07-24 00:42:14 root INFO     [order_1_approx] starting weight calculation for The orangutan falls into the category of primate
The chimpanzee falls into the category of primate
The fox falls into the category of canine
The vulture falls into the category of raptor
The viper falls into the category of snake
The goat falls into the category of bovid
The jackal falls into the category of canine
The falcon falls into the category of
2024-07-24 00:42:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 00:45:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8154, -0.9819, -0.8413,  ..., -0.9111,  0.4807,  0.2496],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5195,  0.6592, -5.1367,  ...,  0.0000,  0.0825,  1.7598],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.9591e-03, -5.2567e-03, -1.1625e-03,  ..., -2.0409e-03,
         -4.0207e-03, -4.5872e-04],
        [-7.2479e-04,  3.6240e-05,  1.1902e-03,  ...,  1.2306e-02,
          1.6205e-02, -7.4463e-03],
        [ 8.6746e-03,  3.3569e-04,  1.0468e-02,  ..., -1.3336e-02,
         -1.2283e-03,  5.6572e-03],
        ...,
        [-7.1869e-03, -5.0926e-03, -1.5221e-03,  ...,  1.2093e-02,
         -8.6117e-04,  7.8201e-03],
        [-8.2397e-03, -1.0361e-02, -1.4820e-03,  ..., -1.9245e-03,
         -4.4823e-04,  8.6441e-03],
        [-6.9885e-03, -4.2763e-03, -5.1575e-03,  ..., -2.7199e-03,
          2.2831e-03,  1.2131e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8838,  1.5625, -6.4258,  ..., -0.0309, -0.1061,  1.7920]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:45:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The orangutan falls into the category of primate
The chimpanzee falls into the category of primate
The fox falls into the category of canine
The vulture falls into the category of raptor
The viper falls into the category of snake
The goat falls into the category of bovid
The jackal falls into the category of canine
The falcon falls into the category of
2024-07-24 00:45:59 root INFO     [order_1_approx] starting weight calculation for The orangutan falls into the category of primate
The viper falls into the category of snake
The jackal falls into the category of canine
The goat falls into the category of bovid
The falcon falls into the category of raptor
The vulture falls into the category of raptor
The chimpanzee falls into the category of primate
The fox falls into the category of
2024-07-24 00:45:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 00:49:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8633, -1.1084, -0.9570,  ...,  0.5654, -0.8794,  0.8525],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7754,  0.3120,  1.0244,  ..., -1.9297,  1.0547,  1.7119],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.1780e-02, -9.3536e-03, -3.3379e-03,  ...,  5.8022e-03,
         -1.3199e-03, -5.2643e-04],
        [-1.3840e-02,  4.4174e-03,  1.0071e-03,  ...,  1.0094e-02,
          1.4526e-02, -1.4362e-03],
        [ 4.5776e-04, -4.3335e-03, -1.0826e-02,  ...,  3.0594e-03,
         -5.9891e-03,  8.9264e-03],
        ...,
        [-4.1885e-03, -1.0895e-02,  4.8752e-03,  ...,  2.4536e-02,
         -1.2344e-02,  1.4870e-02],
        [-5.8517e-03, -5.9395e-03, -1.0529e-02,  ..., -2.0180e-03,
          1.4259e-02,  9.9182e-05],
        [ 4.7684e-04, -1.6270e-03,  8.4991e-03,  ..., -7.8583e-03,
          1.1826e-02,  1.6251e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6128,  1.0996,  1.4316,  ..., -1.2168,  1.2080,  2.0605]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:49:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The orangutan falls into the category of primate
The viper falls into the category of snake
The jackal falls into the category of canine
The goat falls into the category of bovid
The falcon falls into the category of raptor
The vulture falls into the category of raptor
The chimpanzee falls into the category of primate
The fox falls into the category of
2024-07-24 00:49:33 root INFO     [order_1_approx] starting weight calculation for The chimpanzee falls into the category of primate
The falcon falls into the category of raptor
The fox falls into the category of canine
The vulture falls into the category of raptor
The viper falls into the category of snake
The orangutan falls into the category of primate
The jackal falls into the category of canine
The goat falls into the category of
2024-07-24 00:49:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 00:53:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5088, -1.3359, -0.7490,  ...,  1.1104, -0.7861,  1.3730],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.3711,  2.3242, -2.6035,  ..., -1.3799, -0.2117,  0.7646],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0061, -0.0156, -0.0093,  ...,  0.0066, -0.0044, -0.0049],
        [-0.0083,  0.0020, -0.0123,  ...,  0.0071,  0.0154, -0.0086],
        [-0.0029, -0.0036, -0.0072,  ..., -0.0018, -0.0008,  0.0064],
        ...,
        [-0.0117, -0.0047, -0.0083,  ...,  0.0006,  0.0005,  0.0209],
        [-0.0051, -0.0030,  0.0040,  ..., -0.0030, -0.0011, -0.0046],
        [-0.0079, -0.0053, -0.0025,  ..., -0.0122, -0.0039,  0.0065]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.5312,  2.9336, -2.7715,  ..., -1.1016, -0.0217,  0.7593]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:53:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The chimpanzee falls into the category of primate
The falcon falls into the category of raptor
The fox falls into the category of canine
The vulture falls into the category of raptor
The viper falls into the category of snake
The orangutan falls into the category of primate
The jackal falls into the category of canine
The goat falls into the category of
2024-07-24 00:53:13 root INFO     [order_1_approx] starting weight calculation for The chimpanzee falls into the category of primate
The orangutan falls into the category of primate
The fox falls into the category of canine
The vulture falls into the category of raptor
The falcon falls into the category of raptor
The viper falls into the category of snake
The goat falls into the category of bovid
The jackal falls into the category of
2024-07-24 00:53:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 00:56:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5586, -1.8926, -0.2710,  ...,  1.0605, -0.7407, -0.1985],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0000, -1.2480,  0.1074,  ...,  1.5586,  0.1328,  1.2744],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0075, -0.0112, -0.0049,  ..., -0.0137,  0.0005, -0.0044],
        [-0.0082, -0.0021,  0.0110,  ...,  0.0166,  0.0198,  0.0002],
        [ 0.0002, -0.0026, -0.0054,  ..., -0.0028,  0.0009,  0.0031],
        ...,
        [ 0.0004, -0.0062,  0.0078,  ...,  0.0224,  0.0022,  0.0154],
        [-0.0079, -0.0017,  0.0002,  ..., -0.0026,  0.0089,  0.0082],
        [-0.0103, -0.0039,  0.0049,  ..., -0.0034,  0.0034,  0.0073]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6260, -0.8203, -0.1536,  ...,  1.5264,  0.5186,  1.1621]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:56:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The chimpanzee falls into the category of primate
The orangutan falls into the category of primate
The fox falls into the category of canine
The vulture falls into the category of raptor
The falcon falls into the category of raptor
The viper falls into the category of snake
The goat falls into the category of bovid
The jackal falls into the category of
2024-07-24 00:56:52 root INFO     [order_1_approx] starting weight calculation for The jackal falls into the category of canine
The goat falls into the category of bovid
The falcon falls into the category of raptor
The viper falls into the category of snake
The vulture falls into the category of raptor
The fox falls into the category of canine
The chimpanzee falls into the category of primate
The orangutan falls into the category of
2024-07-24 00:56:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 01:00:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7627, -1.6104, -0.6895,  ...,  0.3950,  0.8560,  0.7690],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.4727,  0.0343,  1.5020,  ...,  0.5293, -0.8701, -1.1504],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 7.0038e-03, -6.0463e-03, -1.3351e-04,  ...,  2.4529e-03,
         -8.9569e-03, -6.5079e-03],
        [-4.2725e-03, -4.8828e-03,  8.7738e-03,  ...,  8.4305e-03,
         -1.0162e-02, -2.1912e-02],
        [ 8.8930e-05, -6.2027e-03, -2.9240e-03,  ..., -6.4621e-03,
          8.7738e-04, -1.2306e-02],
        ...,
        [-2.4994e-02,  3.7193e-03, -2.7294e-03,  ...,  1.0033e-02,
         -8.8577e-03, -1.8139e-03],
        [-1.1597e-03, -7.3814e-04, -5.5580e-03,  ...,  3.2043e-03,
          1.1261e-02,  1.0826e-02],
        [-3.1281e-03, -2.1458e-03,  7.6675e-03,  ..., -3.9978e-03,
         -6.2485e-03, -8.6823e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.5742,  0.3154,  1.5596,  ...,  0.4038, -1.4434, -0.6533]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:00:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The jackal falls into the category of canine
The goat falls into the category of bovid
The falcon falls into the category of raptor
The viper falls into the category of snake
The vulture falls into the category of raptor
The fox falls into the category of canine
The chimpanzee falls into the category of primate
The orangutan falls into the category of
2024-07-24 01:00:31 root INFO     [order_1_approx] starting weight calculation for The vulture falls into the category of raptor
The goat falls into the category of bovid
The fox falls into the category of canine
The orangutan falls into the category of primate
The jackal falls into the category of canine
The falcon falls into the category of raptor
The chimpanzee falls into the category of primate
The viper falls into the category of
2024-07-24 01:00:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 01:04:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9434,  0.1699, -1.4561,  ...,  0.3447, -0.2715,  0.7832],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.9082,  0.1638, -4.5625,  ...,  1.1270, -1.6025,  1.0576],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0233,  0.0008,  0.0011,  ...,  0.0111, -0.0023,  0.0126],
        [-0.0102, -0.0041,  0.0077,  ...,  0.0107,  0.0035,  0.0073],
        [-0.0010, -0.0015, -0.0044,  ...,  0.0084, -0.0166,  0.0114],
        ...,
        [-0.0046, -0.0009,  0.0074,  ...,  0.0094, -0.0089,  0.0119],
        [ 0.0124, -0.0176,  0.0026,  ..., -0.0097, -0.0032,  0.0223],
        [-0.0223, -0.0108, -0.0168,  ..., -0.0119,  0.0135,  0.0065]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.3691, -0.0229, -4.0469,  ...,  1.0439, -1.1582,  2.2246]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:04:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The vulture falls into the category of raptor
The goat falls into the category of bovid
The fox falls into the category of canine
The orangutan falls into the category of primate
The jackal falls into the category of canine
The falcon falls into the category of raptor
The chimpanzee falls into the category of primate
The viper falls into the category of
2024-07-24 01:04:09 root INFO     [order_1_approx] starting weight calculation for The chimpanzee falls into the category of primate
The jackal falls into the category of canine
The orangutan falls into the category of primate
The fox falls into the category of canine
The falcon falls into the category of raptor
The viper falls into the category of snake
The goat falls into the category of bovid
The vulture falls into the category of
2024-07-24 01:04:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 01:07:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0195, -1.5596, -1.1719,  ...,  1.2305,  0.2012,  0.4202],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6689,  1.1504, -2.4375,  ...,  2.1250, -0.5811, -0.4507],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0076,  0.0058, -0.0018,  ...,  0.0034,  0.0037, -0.0001],
        [-0.0075,  0.0002,  0.0119,  ...,  0.0093,  0.0011, -0.0121],
        [-0.0042, -0.0136, -0.0041,  ..., -0.0060, -0.0073, -0.0031],
        ...,
        [-0.0145, -0.0128, -0.0139,  ...,  0.0199,  0.0064,  0.0117],
        [-0.0143, -0.0113, -0.0022,  ...,  0.0096, -0.0007,  0.0150],
        [-0.0067,  0.0050,  0.0054,  ..., -0.0011,  0.0067,  0.0207]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9092,  1.6143, -2.8086,  ...,  2.5645, -0.5469, -0.1797]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:07:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The chimpanzee falls into the category of primate
The jackal falls into the category of canine
The orangutan falls into the category of primate
The fox falls into the category of canine
The falcon falls into the category of raptor
The viper falls into the category of snake
The goat falls into the category of bovid
The vulture falls into the category of
2024-07-24 01:07:50 root INFO     total operator prediction time: 1754.7507934570312 seconds
2024-07-24 01:07:50 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on hyponyms - misc
2024-07-24 01:07:50 root INFO     building operator hyponyms - misc
2024-07-24 01:07:50 root INFO     [order_1_approx] starting weight calculation for A more specific term for a computer is laptop
A more specific term for a weapon is gun
A more specific term for a cup is teacup
A more specific term for a cutlery is knife
A more specific term for a sofa is divan
A more specific term for a church is chapel
A more specific term for a painting is watercolor
A more specific term for a brush is
2024-07-24 01:07:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 01:11:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0104, -0.3464,  1.3232,  ...,  0.5239, -1.5322,  1.4238],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0425, -2.7734, -3.9277,  ..., -1.2383, -1.2734,  5.5273],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0024, -0.0021,  0.0107,  ...,  0.0289, -0.0021, -0.0035],
        [-0.0030,  0.0175,  0.0030,  ..., -0.0016, -0.0004,  0.0028],
        [ 0.0205, -0.0028,  0.0303,  ...,  0.0199, -0.0155,  0.0076],
        ...,
        [-0.0066,  0.0060,  0.0050,  ...,  0.0279,  0.0064, -0.0031],
        [ 0.0147, -0.0224, -0.0019,  ..., -0.0100,  0.0012,  0.0074],
        [-0.0170, -0.0148, -0.0026,  ..., -0.0124,  0.0026,  0.0257]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3027, -2.7266, -3.7988,  ..., -0.6626, -0.4517,  6.1172]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:11:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a computer is laptop
A more specific term for a weapon is gun
A more specific term for a cup is teacup
A more specific term for a cutlery is knife
A more specific term for a sofa is divan
A more specific term for a church is chapel
A more specific term for a painting is watercolor
A more specific term for a brush is
2024-07-24 01:11:28 root INFO     [order_1_approx] starting weight calculation for A more specific term for a cutlery is knife
A more specific term for a computer is laptop
A more specific term for a painting is watercolor
A more specific term for a cup is teacup
A more specific term for a brush is toothbrush
A more specific term for a sofa is divan
A more specific term for a weapon is gun
A more specific term for a church is
2024-07-24 01:11:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 01:14:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0884, -0.8013,  0.2351,  ...,  0.5864, -0.3628,  0.7529],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.2285,  1.9219,  1.7842,  ..., -1.1562, -0.4275, -0.1211],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0034, -0.0092,  0.0223,  ..., -0.0020, -0.0243, -0.0052],
        [ 0.0115,  0.0127,  0.0054,  ...,  0.0180,  0.0054, -0.0072],
        [-0.0177, -0.0084, -0.0082,  ...,  0.0143,  0.0013,  0.0054],
        ...,
        [-0.0036,  0.0014,  0.0096,  ...,  0.0147, -0.0102,  0.0098],
        [ 0.0123,  0.0066, -0.0003,  ...,  0.0154, -0.0123, -0.0058],
        [-0.0034,  0.0132,  0.0016,  ...,  0.0029, -0.0066,  0.0136]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.9961,  2.4023,  2.2402,  ..., -1.3516, -0.7871, -0.1571]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:14:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a cutlery is knife
A more specific term for a computer is laptop
A more specific term for a painting is watercolor
A more specific term for a cup is teacup
A more specific term for a brush is toothbrush
A more specific term for a sofa is divan
A more specific term for a weapon is gun
A more specific term for a church is
2024-07-24 01:15:00 root INFO     [order_1_approx] starting weight calculation for A more specific term for a cutlery is knife
A more specific term for a cup is teacup
A more specific term for a painting is watercolor
A more specific term for a church is chapel
A more specific term for a weapon is gun
A more specific term for a brush is toothbrush
A more specific term for a sofa is divan
A more specific term for a computer is
2024-07-24 01:15:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 01:18:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1946,  0.0387,  0.2297,  ...,  0.4668,  0.6318,  1.7764],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1791, -0.2151, -0.1709,  ...,  0.6475,  2.2324,  4.3281],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0085,  0.0013, -0.0094,  ...,  0.0015, -0.0157,  0.0255],
        [ 0.0044,  0.0118,  0.0028,  ...,  0.0018, -0.0091, -0.0024],
        [ 0.0015,  0.0009, -0.0004,  ..., -0.0040, -0.0161,  0.0064],
        ...,
        [-0.0130,  0.0039,  0.0035,  ...,  0.0186, -0.0127,  0.0024],
        [ 0.0084, -0.0060, -0.0075,  ...,  0.0131, -0.0078,  0.0092],
        [-0.0182,  0.0099,  0.0223,  ..., -0.0161,  0.0121,  0.0021]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1565, -0.5151,  0.2825,  ...,  1.1035,  2.4863,  4.1016]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:18:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a cutlery is knife
A more specific term for a cup is teacup
A more specific term for a painting is watercolor
A more specific term for a church is chapel
A more specific term for a weapon is gun
A more specific term for a brush is toothbrush
A more specific term for a sofa is divan
A more specific term for a computer is
2024-07-24 01:18:33 root INFO     [order_1_approx] starting weight calculation for A more specific term for a weapon is gun
A more specific term for a church is chapel
A more specific term for a sofa is divan
A more specific term for a painting is watercolor
A more specific term for a cutlery is knife
A more specific term for a brush is toothbrush
A more specific term for a computer is laptop
A more specific term for a cup is
2024-07-24 01:18:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 01:22:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1200, -0.1799, -0.2324,  ..., -0.1406, -0.8433,  1.7705],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5479,  0.8687, -1.5430,  ..., -0.8853,  0.1639,  3.4336],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0010,  0.0067, -0.0112,  ..., -0.0082,  0.0028, -0.0063],
        [-0.0059,  0.0100,  0.0094,  ...,  0.0316,  0.0074,  0.0052],
        [ 0.0057,  0.0115,  0.0217,  ..., -0.0203,  0.0242,  0.0200],
        ...,
        [-0.0260,  0.0099,  0.0051,  ...,  0.0199, -0.0087,  0.0098],
        [ 0.0009, -0.0137,  0.0080,  ..., -0.0018,  0.0154,  0.0214],
        [ 0.0005, -0.0049, -0.0113,  ..., -0.0025,  0.0041,  0.0262]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9507,  1.2715, -0.5020,  ..., -0.9502,  0.3789,  3.5938]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:22:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a weapon is gun
A more specific term for a church is chapel
A more specific term for a sofa is divan
A more specific term for a painting is watercolor
A more specific term for a cutlery is knife
A more specific term for a brush is toothbrush
A more specific term for a computer is laptop
A more specific term for a cup is
2024-07-24 01:22:14 root INFO     [order_1_approx] starting weight calculation for A more specific term for a painting is watercolor
A more specific term for a computer is laptop
A more specific term for a weapon is gun
A more specific term for a brush is toothbrush
A more specific term for a sofa is divan
A more specific term for a church is chapel
A more specific term for a cup is teacup
A more specific term for a cutlery is
2024-07-24 01:22:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 01:25:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6221,  0.9570,  1.5781,  ...,  1.6494, -1.8711,  2.2812],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-6.3125,  1.3135, -3.0449,  ...,  1.9639,  0.3540,  2.7773],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.0398e-03,  1.1169e-02, -2.5768e-03,  ...,  2.6360e-03,
          8.9417e-03, -5.7068e-03],
        [ 1.3184e-02, -2.4109e-03, -1.1879e-02,  ...,  1.0239e-02,
          8.5754e-03, -2.6703e-04],
        [-6.3896e-03, -8.1100e-03, -1.3952e-03,  ...,  1.2695e-02,
          5.8441e-03, -7.3853e-03],
        ...,
        [-6.1035e-05, -1.3382e-02,  3.2921e-03,  ...,  2.6062e-02,
         -1.7334e-02,  1.5306e-04],
        [ 2.0370e-03, -3.2692e-03, -2.5902e-03,  ...,  6.5613e-03,
          1.4633e-02,  1.8326e-02],
        [ 1.2344e-02, -1.9474e-03,  1.1734e-02,  ..., -1.6069e-03,
          3.1738e-03,  2.3026e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-7.0078,  2.2207, -2.1035,  ...,  2.5254,  0.6484,  2.2734]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:25:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a painting is watercolor
A more specific term for a computer is laptop
A more specific term for a weapon is gun
A more specific term for a brush is toothbrush
A more specific term for a sofa is divan
A more specific term for a church is chapel
A more specific term for a cup is teacup
A more specific term for a cutlery is
2024-07-24 01:25:45 root INFO     [order_1_approx] starting weight calculation for A more specific term for a brush is toothbrush
A more specific term for a church is chapel
A more specific term for a cup is teacup
A more specific term for a sofa is divan
A more specific term for a cutlery is knife
A more specific term for a weapon is gun
A more specific term for a computer is laptop
A more specific term for a painting is
2024-07-24 01:25:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 01:29:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1021,  0.3394,  0.1145,  ..., -0.6143,  0.5630,  1.5137],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.5391, -2.9414, -2.6816,  ..., -2.5898, -1.3662,  3.2383],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0115, -0.0045,  0.0100,  ...,  0.0008, -0.0114,  0.0118],
        [ 0.0078,  0.0192,  0.0149,  ...,  0.0165, -0.0108,  0.0030],
        [ 0.0201,  0.0313,  0.0033,  ...,  0.0239, -0.0096,  0.0359],
        ...,
        [-0.0032, -0.0001,  0.0095,  ...,  0.0234, -0.0079,  0.0115],
        [ 0.0069,  0.0086,  0.0135,  ...,  0.0046, -0.0123,  0.0080],
        [-0.0160, -0.0102,  0.0025,  ..., -0.0165,  0.0016,  0.0099]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.3281, -2.9531, -2.9238,  ..., -2.1543, -0.7603,  3.3086]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:29:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a brush is toothbrush
A more specific term for a church is chapel
A more specific term for a cup is teacup
A more specific term for a sofa is divan
A more specific term for a cutlery is knife
A more specific term for a weapon is gun
A more specific term for a computer is laptop
A more specific term for a painting is
2024-07-24 01:29:26 root INFO     [order_1_approx] starting weight calculation for A more specific term for a computer is laptop
A more specific term for a cutlery is knife
A more specific term for a church is chapel
A more specific term for a painting is watercolor
A more specific term for a cup is teacup
A more specific term for a brush is toothbrush
A more specific term for a weapon is gun
A more specific term for a sofa is
2024-07-24 01:29:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 01:33:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3357, -0.5669, -0.3887,  ...,  0.3745, -0.2866,  0.6816],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5098, -2.5117, -1.4111,  ..., -1.8662, -1.2881,  1.5439],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 5.6572e-03, -1.3107e-02, -6.9046e-04,  ...,  1.0712e-02,
          4.1924e-03, -2.5513e-02],
        [ 4.5776e-05,  2.2316e-03, -1.1017e-02,  ..., -1.3153e-02,
          4.6082e-03, -3.8090e-03],
        [-6.8207e-03,  3.1586e-03,  1.3107e-02,  ..., -6.3553e-03,
          9.3079e-03, -7.4615e-03],
        ...,
        [-1.0445e-02,  8.0776e-04,  1.4679e-02,  ...,  1.2939e-02,
         -8.9188e-03,  6.6605e-03],
        [ 2.0714e-03,  1.2619e-02, -1.6525e-02,  ...,  3.1013e-03,
          6.9885e-03,  2.1332e-02],
        [-3.4199e-03, -5.0583e-03,  4.1199e-03,  ..., -1.7883e-02,
          5.0735e-03,  2.6550e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5703, -2.1621, -1.4404,  ..., -1.5352, -0.7012,  1.6318]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:33:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a computer is laptop
A more specific term for a cutlery is knife
A more specific term for a church is chapel
A more specific term for a painting is watercolor
A more specific term for a cup is teacup
A more specific term for a brush is toothbrush
A more specific term for a weapon is gun
A more specific term for a sofa is
2024-07-24 01:33:06 root INFO     [order_1_approx] starting weight calculation for A more specific term for a computer is laptop
A more specific term for a cutlery is knife
A more specific term for a sofa is divan
A more specific term for a church is chapel
A more specific term for a cup is teacup
A more specific term for a painting is watercolor
A more specific term for a brush is toothbrush
A more specific term for a weapon is
2024-07-24 01:33:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 01:36:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7090,  0.2267,  0.9668,  ..., -0.5957,  0.0635,  0.8369],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.1113,  0.1970, -2.9004,  ..., -2.0410,  4.1289,  0.2871],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0055,  0.0049,  0.0149,  ...,  0.0004, -0.0162,  0.0197],
        [ 0.0039,  0.0011, -0.0095,  ...,  0.0047, -0.0210, -0.0068],
        [-0.0002,  0.0071,  0.0229,  ..., -0.0033, -0.0203,  0.0061],
        ...,
        [-0.0111, -0.0100,  0.0108,  ..., -0.0013, -0.0068, -0.0112],
        [-0.0159, -0.0228,  0.0035,  ...,  0.0017,  0.0008,  0.0213],
        [-0.0108,  0.0028,  0.0012,  ..., -0.0024,  0.0188,  0.0088]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.8438, -0.3044, -2.4824,  ..., -1.8389,  3.3535,  0.6318]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:36:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a computer is laptop
A more specific term for a cutlery is knife
A more specific term for a sofa is divan
A more specific term for a church is chapel
A more specific term for a cup is teacup
A more specific term for a painting is watercolor
A more specific term for a brush is toothbrush
A more specific term for a weapon is
2024-07-24 01:36:43 root INFO     total operator prediction time: 1733.2538721561432 seconds
2024-07-24 01:36:43 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on antonyms - binary
2024-07-24 01:36:43 root INFO     building operator antonyms - binary
2024-07-24 01:36:43 root INFO     [order_1_approx] starting weight calculation for The opposite of decrement is increment
The opposite of beginning is end
The opposite of front is back
The opposite of drop is lift
The opposite of top is bottom
The opposite of internal is external
The opposite of proceed is retreat
The opposite of backward is
2024-07-24 01:36:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 01:40:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1062, -0.3857,  0.3044,  ..., -1.1787,  0.7842,  0.5303],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.2461,  1.3340, -1.8770,  ..., -3.0312,  4.9492,  0.7881],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0298, -0.0346,  0.0109,  ...,  0.0065, -0.0067,  0.0041],
        [-0.0102, -0.0028, -0.0279,  ...,  0.0078, -0.0072,  0.0078],
        [-0.0351, -0.0035, -0.0271,  ..., -0.0080,  0.0003,  0.0053],
        ...,
        [ 0.0187, -0.0257, -0.0171,  ..., -0.0288,  0.0127,  0.0138],
        [ 0.0089, -0.0166,  0.0326,  ..., -0.0172, -0.0179,  0.0274],
        [-0.0029, -0.0135,  0.0079,  ..., -0.0072,  0.0133,  0.0143]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1309,  1.0938, -1.1738,  ..., -3.6367,  4.1484,  0.8960]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:40:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of decrement is increment
The opposite of beginning is end
The opposite of front is back
The opposite of drop is lift
The opposite of top is bottom
The opposite of internal is external
The opposite of proceed is retreat
The opposite of backward is
2024-07-24 01:40:26 root INFO     [order_1_approx] starting weight calculation for The opposite of top is bottom
The opposite of backward is forward
The opposite of drop is lift
The opposite of internal is external
The opposite of proceed is retreat
The opposite of front is back
The opposite of decrement is increment
The opposite of beginning is
2024-07-24 01:40:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 01:44:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0151, -0.8726,  0.7173,  ..., -0.1660,  0.7666,  0.3425],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7539, -3.1504, -0.3081,  ...,  3.5195,  2.7617, -1.0176],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0282, -0.0331, -0.0079,  ...,  0.0109, -0.0065,  0.0091],
        [-0.0037, -0.0048, -0.0019,  ..., -0.0066, -0.0103,  0.0205],
        [-0.0226, -0.0013, -0.0379,  ...,  0.0014, -0.0091, -0.0044],
        ...,
        [ 0.0066, -0.0214,  0.0035,  ...,  0.0028,  0.0170,  0.0193],
        [-0.0062, -0.0117,  0.0464,  ...,  0.0079, -0.0031, -0.0210],
        [-0.0080,  0.0106,  0.0133,  ..., -0.0103,  0.0205, -0.0187]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3359, -3.9785, -0.4214,  ...,  2.5098,  3.5625, -0.8599]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:44:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of top is bottom
The opposite of backward is forward
The opposite of drop is lift
The opposite of internal is external
The opposite of proceed is retreat
The opposite of front is back
The opposite of decrement is increment
The opposite of beginning is
2024-07-24 01:44:05 root INFO     [order_1_approx] starting weight calculation for The opposite of internal is external
The opposite of drop is lift
The opposite of backward is forward
The opposite of proceed is retreat
The opposite of beginning is end
The opposite of top is bottom
The opposite of front is back
The opposite of decrement is
2024-07-24 01:44:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 01:47:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6621, -0.5166,  0.3232,  ..., -0.0454,  0.8647,  0.5820],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1201, -4.2695, -0.3086,  ..., -1.8848,  2.1289,  1.2646],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0317, -0.0280, -0.0088,  ...,  0.0016,  0.0030, -0.0139],
        [-0.0292, -0.0084,  0.0003,  ..., -0.0074, -0.0098, -0.0025],
        [-0.0029, -0.0308, -0.0419,  ..., -0.0253,  0.0144, -0.0098],
        ...,
        [-0.0392, -0.0116, -0.0115,  ..., -0.0161,  0.0106,  0.0237],
        [-0.0140,  0.0166, -0.0036,  ..., -0.0241, -0.0142, -0.0017],
        [ 0.0017,  0.0420, -0.0148,  ...,  0.0105,  0.0206, -0.0095]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.8145, -4.1367,  0.7490,  ..., -1.2051,  1.3848,  1.3193]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:47:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of internal is external
The opposite of drop is lift
The opposite of backward is forward
The opposite of proceed is retreat
The opposite of beginning is end
The opposite of top is bottom
The opposite of front is back
The opposite of decrement is
2024-07-24 01:47:40 root INFO     [order_1_approx] starting weight calculation for The opposite of top is bottom
The opposite of front is back
The opposite of backward is forward
The opposite of decrement is increment
The opposite of beginning is end
The opposite of proceed is retreat
The opposite of internal is external
The opposite of drop is
2024-07-24 01:47:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 01:51:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.9434, 0.0425, 0.0591,  ..., 0.3118, 0.1461, 0.5830], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4932, -3.8281, -0.0996,  ..., -0.4792, -0.2649,  3.6758],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0023, -0.0276, -0.0029,  ...,  0.0145,  0.0135, -0.0057],
        [ 0.0013, -0.0266,  0.0335,  ...,  0.0047, -0.0317,  0.0163],
        [ 0.0155,  0.0418, -0.0536,  ..., -0.0295,  0.0157,  0.0034],
        ...,
        [ 0.0102,  0.0167, -0.0159,  ..., -0.0134,  0.0193,  0.0127],
        [ 0.0054,  0.0014,  0.0012,  ..., -0.0057,  0.0205, -0.0009],
        [-0.0361, -0.0139,  0.0182,  ...,  0.0219,  0.0036, -0.0093]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1211, -4.3086,  0.1010,  ...,  0.6084,  0.3137,  3.1973]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:51:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of top is bottom
The opposite of front is back
The opposite of backward is forward
The opposite of decrement is increment
The opposite of beginning is end
The opposite of proceed is retreat
The opposite of internal is external
The opposite of drop is
2024-07-24 01:51:18 root INFO     [order_1_approx] starting weight calculation for The opposite of internal is external
The opposite of backward is forward
The opposite of proceed is retreat
The opposite of beginning is end
The opposite of drop is lift
The opposite of decrement is increment
The opposite of top is bottom
The opposite of front is
2024-07-24 01:51:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 01:55:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0249, -1.8164,  0.3625,  ...,  0.2334,  0.7876,  1.1738],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2030, -0.6040, -0.9941,  ...,  2.3633,  3.4102,  1.1738],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0005, -0.0047, -0.0095,  ..., -0.0070,  0.0057, -0.0052],
        [ 0.0038,  0.0090, -0.0151,  ..., -0.0076, -0.0131,  0.0114],
        [-0.0134, -0.0034, -0.0305,  ..., -0.0152, -0.0011,  0.0190],
        ...,
        [-0.0130, -0.0148,  0.0201,  ...,  0.0161,  0.0006,  0.0174],
        [-0.0218, -0.0028,  0.0253,  ..., -0.0118, -0.0196, -0.0038],
        [-0.0123, -0.0194, -0.0020,  ..., -0.0173,  0.0060,  0.0060]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1973, -1.0742, -0.6250,  ...,  2.1543,  3.5723,  0.8438]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:55:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of internal is external
The opposite of backward is forward
The opposite of proceed is retreat
The opposite of beginning is end
The opposite of drop is lift
The opposite of decrement is increment
The opposite of top is bottom
The opposite of front is
2024-07-24 01:55:03 root INFO     [order_1_approx] starting weight calculation for The opposite of proceed is retreat
The opposite of drop is lift
The opposite of decrement is increment
The opposite of beginning is end
The opposite of top is bottom
The opposite of backward is forward
The opposite of front is back
The opposite of internal is
2024-07-24 01:55:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 01:58:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1582, -0.2605,  0.1224,  ...,  0.3701,  0.6265,  1.0010],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5098, -2.4785,  0.3057,  ..., -1.6445,  3.1367, -0.0283],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0131, -0.0251,  0.0099,  ...,  0.0192, -0.0174,  0.0309],
        [-0.0020,  0.0045, -0.0309,  ...,  0.0097, -0.0337,  0.0174],
        [ 0.0041, -0.0102, -0.0270,  ...,  0.0030, -0.0200,  0.0354],
        ...,
        [ 0.0167, -0.0168,  0.0175,  ...,  0.0158, -0.0040,  0.0287],
        [-0.0198, -0.0094,  0.0080,  ..., -0.0186,  0.0013,  0.0009],
        [-0.0074,  0.0020, -0.0065,  ...,  0.0105,  0.0031, -0.0064]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5498, -1.6738,  0.0896,  ..., -0.8408,  2.5938, -0.3855]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:58:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of proceed is retreat
The opposite of drop is lift
The opposite of decrement is increment
The opposite of beginning is end
The opposite of top is bottom
The opposite of backward is forward
The opposite of front is back
The opposite of internal is
2024-07-24 01:58:40 root INFO     [order_1_approx] starting weight calculation for The opposite of top is bottom
The opposite of drop is lift
The opposite of decrement is increment
The opposite of beginning is end
The opposite of internal is external
The opposite of backward is forward
The opposite of front is back
The opposite of proceed is
2024-07-24 01:58:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 02:02:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.7051, -0.6333,  0.4668,  ...,  1.3730,  0.0565, -0.1139],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.2715, -2.1211, -3.0391,  ...,  2.5742,  6.7812, -0.2163],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0052, -0.0349,  0.0012,  ..., -0.0068, -0.0077, -0.0101],
        [-0.0042, -0.0243, -0.0279,  ..., -0.0206, -0.0275,  0.0132],
        [-0.0026, -0.0107, -0.0199,  ..., -0.0022, -0.0154,  0.0143],
        ...,
        [-0.0216, -0.0267, -0.0073,  ..., -0.0218,  0.0112,  0.0127],
        [-0.0294,  0.0159,  0.0080,  ..., -0.0140,  0.0158, -0.0201],
        [ 0.0009, -0.0008, -0.0106,  ...,  0.0034,  0.0093, -0.0201]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.8164, -1.9688, -2.3828,  ...,  2.6211,  7.1094, -0.5845]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:02:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of top is bottom
The opposite of drop is lift
The opposite of decrement is increment
The opposite of beginning is end
The opposite of internal is external
The opposite of backward is forward
The opposite of front is back
The opposite of proceed is
2024-07-24 02:02:24 root INFO     [order_1_approx] starting weight calculation for The opposite of decrement is increment
The opposite of backward is forward
The opposite of internal is external
The opposite of front is back
The opposite of beginning is end
The opposite of drop is lift
The opposite of proceed is retreat
The opposite of top is
2024-07-24 02:02:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 02:06:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0752,  0.3257,  0.3474,  ..., -0.0201,  1.1377, -0.2546],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.6367, -1.0664, -2.8047,  ..., -1.2656,  4.0938,  0.3916],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0274, -0.0225, -0.0024,  ...,  0.0060, -0.0219,  0.0034],
        [ 0.0008,  0.0060, -0.0081,  ..., -0.0021, -0.0134,  0.0165],
        [-0.0010, -0.0093, -0.0199,  ...,  0.0157, -0.0134,  0.0481],
        ...,
        [ 0.0261,  0.0061,  0.0004,  ..., -0.0008,  0.0306,  0.0268],
        [ 0.0035, -0.0007,  0.0146,  ..., -0.0192, -0.0295, -0.0105],
        [-0.0149,  0.0003,  0.0049,  ..., -0.0176, -0.0008, -0.0124]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.1367, -0.9478, -2.0723,  ..., -1.0410,  4.1719,  0.1946]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:06:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of decrement is increment
The opposite of backward is forward
The opposite of internal is external
The opposite of front is back
The opposite of beginning is end
The opposite of drop is lift
The opposite of proceed is retreat
The opposite of top is
2024-07-24 02:06:07 root INFO     total operator prediction time: 1764.162913799286 seconds
2024-07-24 02:06:07 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - member
2024-07-24 02:06:07 root INFO     building operator meronyms - member
2024-07-24 02:06:08 root INFO     [order_1_approx] starting weight calculation for A lion is a member of a pride
A calf is a member of a cattle
A spouse is a member of a couple
A galaxy is a member of a universe
A county is a member of a state
A kitten is a member of a litter
A word is a member of a paragraph
A bee is a member of a
2024-07-24 02:06:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 02:09:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9790, -0.0839, -0.8027,  ...,  0.9492, -1.0547,  1.5088],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.6836,  2.8301, -1.7246,  ...,  1.4336,  1.2051,  3.0117],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0133,  0.0027,  0.0097,  ...,  0.0084, -0.0040,  0.0024],
        [-0.0150, -0.0091,  0.0032,  ...,  0.0028,  0.0083,  0.0080],
        [ 0.0068, -0.0132,  0.0021,  ..., -0.0019,  0.0075, -0.0081],
        ...,
        [-0.0011, -0.0029, -0.0086,  ...,  0.0021, -0.0079, -0.0012],
        [ 0.0141, -0.0209,  0.0006,  ..., -0.0168, -0.0076,  0.0043],
        [-0.0022, -0.0043, -0.0036,  ...,  0.0018,  0.0059,  0.0160]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.0781,  3.2246, -1.8379,  ...,  1.1914,  1.6104,  2.9316]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:09:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A lion is a member of a pride
A calf is a member of a cattle
A spouse is a member of a couple
A galaxy is a member of a universe
A county is a member of a state
A kitten is a member of a litter
A word is a member of a paragraph
A bee is a member of a
2024-07-24 02:09:48 root INFO     [order_1_approx] starting weight calculation for A kitten is a member of a litter
A word is a member of a paragraph
A galaxy is a member of a universe
A bee is a member of a swarm
A lion is a member of a pride
A spouse is a member of a couple
A county is a member of a state
A calf is a member of a
2024-07-24 02:09:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 02:13:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7393,  0.4983, -1.2480,  ...,  0.9531, -0.5479,  0.2908],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3105,  2.5371, -4.4023,  ...,  0.4497,  1.2227,  3.0469],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0039,  0.0098, -0.0266,  ...,  0.0006, -0.0011, -0.0062],
        [-0.0341,  0.0120, -0.0079,  ...,  0.0203, -0.0071, -0.0059],
        [ 0.0060, -0.0265,  0.0141,  ..., -0.0117, -0.0103, -0.0097],
        ...,
        [-0.0050, -0.0052,  0.0042,  ...,  0.0116, -0.0091, -0.0044],
        [ 0.0312,  0.0041,  0.0157,  ..., -0.0058, -0.0129, -0.0158],
        [-0.0113, -0.0146, -0.0055,  ...,  0.0086, -0.0175,  0.0184]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.6162,  3.0977, -4.5234,  ...,  1.2598,  1.0234,  3.9492]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:13:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A kitten is a member of a litter
A word is a member of a paragraph
A galaxy is a member of a universe
A bee is a member of a swarm
A lion is a member of a pride
A spouse is a member of a couple
A county is a member of a state
A calf is a member of a
2024-07-24 02:13:25 root INFO     [order_1_approx] starting weight calculation for A calf is a member of a cattle
A lion is a member of a pride
A word is a member of a paragraph
A galaxy is a member of a universe
A kitten is a member of a litter
A bee is a member of a swarm
A spouse is a member of a couple
A county is a member of a
2024-07-24 02:13:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 02:16:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0325, -0.1396, -1.2070,  ..., -0.1040, -0.1648,  1.0029],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.9648, -2.6016, -0.5361,  ...,  0.8896,  4.5117,  1.9619],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0115,  0.0037, -0.0062,  ..., -0.0006, -0.0135,  0.0010],
        [ 0.0141, -0.0058,  0.0127,  ...,  0.0022,  0.0138, -0.0152],
        [ 0.0169,  0.0130, -0.0149,  ..., -0.0096, -0.0104, -0.0026],
        ...,
        [-0.0305,  0.0044, -0.0031,  ..., -0.0009, -0.0070,  0.0155],
        [ 0.0157, -0.0033,  0.0078,  ..., -0.0153, -0.0053, -0.0075],
        [ 0.0090,  0.0099,  0.0061,  ...,  0.0012,  0.0168,  0.0039]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.4609, -2.6367, -0.8857,  ...,  1.0117,  3.4531,  1.7285]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:16:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A calf is a member of a cattle
A lion is a member of a pride
A word is a member of a paragraph
A galaxy is a member of a universe
A kitten is a member of a litter
A bee is a member of a swarm
A spouse is a member of a couple
A county is a member of a
2024-07-24 02:16:59 root INFO     [order_1_approx] starting weight calculation for A calf is a member of a cattle
A county is a member of a state
A lion is a member of a pride
A kitten is a member of a litter
A word is a member of a paragraph
A spouse is a member of a couple
A bee is a member of a swarm
A galaxy is a member of a
2024-07-24 02:16:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 02:20:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4219,  0.0483, -0.1162,  ..., -0.4141,  0.4102,  0.9551],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.7266, -0.6089,  3.8496,  ...,  2.0312, -3.9180,  1.9170],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 6.9351e-03, -2.5845e-03, -6.7902e-04,  ..., -6.9046e-03,
          1.3336e-02, -1.1330e-03],
        [-4.7607e-03,  6.7062e-03, -8.1177e-03,  ...,  7.0877e-03,
          3.5229e-03, -4.8256e-03],
        [-7.4883e-03, -6.0501e-03,  2.1400e-03,  ..., -2.2537e-02,
          1.7349e-02, -1.1398e-02],
        ...,
        [-5.7220e-04,  5.6534e-03,  8.1482e-03,  ..., -1.2703e-03,
         -4.7760e-03, -7.0763e-03],
        [ 1.2955e-02, -3.0518e-05,  8.5754e-03,  ..., -5.3902e-03,
          3.1662e-04,  6.1111e-03],
        [-1.4770e-04, -4.2725e-03,  6.1264e-03,  ...,  5.0926e-03,
          1.0719e-02,  1.0132e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.1641, -0.5151,  3.2695,  ...,  2.1504, -4.0625,  2.4531]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:20:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A calf is a member of a cattle
A county is a member of a state
A lion is a member of a pride
A kitten is a member of a litter
A word is a member of a paragraph
A spouse is a member of a couple
A bee is a member of a swarm
A galaxy is a member of a
2024-07-24 02:20:35 root INFO     [order_1_approx] starting weight calculation for A spouse is a member of a couple
A county is a member of a state
A bee is a member of a swarm
A word is a member of a paragraph
A lion is a member of a pride
A calf is a member of a cattle
A galaxy is a member of a universe
A kitten is a member of a
2024-07-24 02:20:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 02:24:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0557, -0.3889, -1.5293,  ...,  1.3145, -0.4253,  0.6235],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.4941,  1.6182, -0.3345,  ..., -0.2676, -0.3633,  1.7617],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0051, -0.0032, -0.0098,  ..., -0.0057, -0.0008, -0.0062],
        [-0.0002, -0.0027,  0.0039,  ..., -0.0021, -0.0050,  0.0090],
        [-0.0051, -0.0064,  0.0040,  ...,  0.0070,  0.0052,  0.0018],
        ...,
        [-0.0003, -0.0046, -0.0038,  ...,  0.0132, -0.0112,  0.0117],
        [ 0.0173, -0.0165,  0.0085,  ..., -0.0051, -0.0196, -0.0049],
        [-0.0025, -0.0068, -0.0009,  ...,  0.0010,  0.0112,  0.0112]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5107,  3.0078,  0.1008,  ...,  0.1733,  0.1812,  1.3066]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:24:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A spouse is a member of a couple
A county is a member of a state
A bee is a member of a swarm
A word is a member of a paragraph
A lion is a member of a pride
A calf is a member of a cattle
A galaxy is a member of a universe
A kitten is a member of a
2024-07-24 02:24:15 root INFO     [order_1_approx] starting weight calculation for A spouse is a member of a couple
A county is a member of a state
A bee is a member of a swarm
A galaxy is a member of a universe
A kitten is a member of a litter
A word is a member of a paragraph
A calf is a member of a cattle
A lion is a member of a
2024-07-24 02:24:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 02:27:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5781, -1.2773,  0.2034,  ...,  1.8994,  0.4253,  0.0474],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.9062,  2.5195,  1.8711,  ..., -0.4268, -5.0664,  2.5488],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0058, -0.0090, -0.0008,  ..., -0.0037, -0.0091, -0.0080],
        [-0.0116,  0.0005,  0.0044,  ...,  0.0192,  0.0016, -0.0163],
        [ 0.0040,  0.0033,  0.0011,  ..., -0.0082, -0.0016,  0.0219],
        ...,
        [ 0.0031, -0.0010, -0.0023,  ..., -0.0001, -0.0131,  0.0177],
        [ 0.0137, -0.0277, -0.0092,  ..., -0.0028, -0.0087, -0.0030],
        [ 0.0024,  0.0093,  0.0036,  ..., -0.0016,  0.0073,  0.0155]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.9902,  3.7305,  1.3936,  ...,  0.4043, -5.2383,  2.4023]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:27:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A spouse is a member of a couple
A county is a member of a state
A bee is a member of a swarm
A galaxy is a member of a universe
A kitten is a member of a litter
A word is a member of a paragraph
A calf is a member of a cattle
A lion is a member of a
2024-07-24 02:27:54 root INFO     [order_1_approx] starting weight calculation for A county is a member of a state
A word is a member of a paragraph
A calf is a member of a cattle
A galaxy is a member of a universe
A bee is a member of a swarm
A lion is a member of a pride
A kitten is a member of a litter
A spouse is a member of a
2024-07-24 02:27:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 02:31:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5566,  1.0010,  0.2798,  ...,  0.4856, -0.3970, -0.2949],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.9053,  0.3413,  2.1055,  ..., -2.8672,  1.3838,  2.4785],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.6327e-02,  1.8738e-02,  7.4921e-03,  ..., -1.6842e-03,
          2.0523e-02, -6.1455e-03],
        [-6.1722e-03, -4.8141e-03,  7.1564e-03,  ..., -7.2861e-04,
          1.2035e-03, -1.1818e-02],
        [ 4.4594e-03,  2.2316e-03, -1.7944e-02,  ...,  8.3923e-05,
         -1.2543e-02, -3.8719e-04],
        ...,
        [-1.0201e-02, -1.0506e-02,  1.1780e-02,  ..., -1.2650e-02,
         -5.1804e-03,  9.8572e-03],
        [ 3.0861e-03,  5.4131e-03,  7.6599e-03,  ...,  8.3771e-03,
         -1.2222e-02,  1.5335e-03],
        [-1.3840e-02, -7.5150e-03, -4.0674e-04,  ..., -4.7722e-03,
          1.3115e-02,  2.1393e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3828,  0.2429,  1.1855,  ..., -2.7188,  0.1143,  2.4590]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:31:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A county is a member of a state
A word is a member of a paragraph
A calf is a member of a cattle
A galaxy is a member of a universe
A bee is a member of a swarm
A lion is a member of a pride
A kitten is a member of a litter
A spouse is a member of a
2024-07-24 02:31:37 root INFO     [order_1_approx] starting weight calculation for A spouse is a member of a couple
A lion is a member of a pride
A bee is a member of a swarm
A calf is a member of a cattle
A galaxy is a member of a universe
A county is a member of a state
A kitten is a member of a litter
A word is a member of a
2024-07-24 02:31:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 02:35:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0161,  0.5278,  0.0981,  ..., -0.6904, -0.0742,  0.9048],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.5176,  2.6895, -0.0181,  ..., -0.2461, -0.4316,  4.5234],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0120, -0.0016, -0.0125,  ..., -0.0091, -0.0063,  0.0005],
        [-0.0160,  0.0005, -0.0047,  ...,  0.0115,  0.0075, -0.0082],
        [ 0.0203,  0.0023, -0.0068,  ..., -0.0094, -0.0155,  0.0130],
        ...,
        [-0.0108, -0.0035,  0.0087,  ...,  0.0036,  0.0040,  0.0076],
        [ 0.0182, -0.0061, -0.0048,  ..., -0.0024,  0.0047, -0.0026],
        [-0.0116, -0.0032, -0.0031,  ..., -0.0036, -0.0016, -0.0027]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.4180,  3.6562, -0.4185,  ..., -0.6465, -0.4133,  4.0781]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:35:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A spouse is a member of a couple
A lion is a member of a pride
A bee is a member of a swarm
A calf is a member of a cattle
A galaxy is a member of a universe
A county is a member of a state
A kitten is a member of a litter
A word is a member of a
2024-07-24 02:35:17 root INFO     total operator prediction time: 1749.5912353992462 seconds
2024-07-24 02:35:17 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on noun - plural_irreg
2024-07-24 02:35:17 root INFO     building operator noun - plural_irreg
2024-07-24 02:35:17 root INFO     [order_1_approx] starting weight calculation for The plural form of security is securities
The plural form of child is children
The plural form of species is species
The plural form of story is stories
The plural form of family is families
The plural form of memory is memories
The plural form of army is armies
The plural form of analysis is
2024-07-24 02:35:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 02:39:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8047, -0.2324,  0.0542,  ..., -0.7881,  0.0435,  0.9678],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4353,  0.4346,  0.1152,  ..., -0.8008,  2.1602,  3.0312],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.4816e-02, -1.0223e-03,  2.5558e-03,  ..., -1.7914e-02,
         -1.0834e-03, -1.9653e-02],
        [-2.1301e-02, -1.9440e-02,  2.5177e-04,  ..., -1.8311e-03,
         -5.5695e-03, -1.0262e-03],
        [-3.2787e-03,  1.7578e-02, -1.5945e-02,  ...,  1.1383e-02,
          3.4180e-02,  1.1002e-02],
        ...,
        [-2.3460e-03, -5.4359e-05, -8.9569e-03,  ..., -2.1729e-02,
          2.0691e-02, -2.5177e-03],
        [-1.1749e-02, -1.6251e-03, -7.0038e-03,  ..., -5.8517e-03,
         -2.7252e-02,  1.5610e-02],
        [ 1.9302e-02,  3.1494e-02,  6.2675e-03,  ...,  3.8033e-03,
         -5.5199e-03, -2.5787e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3225,  0.4004,  1.2842,  ..., -1.3877,  1.8613,  3.3203]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:39:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of security is securities
The plural form of child is children
The plural form of species is species
The plural form of story is stories
The plural form of family is families
The plural form of memory is memories
The plural form of army is armies
The plural form of analysis is
2024-07-24 02:39:01 root INFO     [order_1_approx] starting weight calculation for The plural form of memory is memories
The plural form of story is stories
The plural form of analysis is analyses
The plural form of security is securities
The plural form of child is children
The plural form of family is families
The plural form of species is species
The plural form of army is
2024-07-24 02:39:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 02:42:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4060,  0.3130, -0.4487,  ..., -0.4019, -0.3369,  1.5781],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4412,  2.1406, -1.4004,  ..., -2.7578,  0.1738,  2.7695],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0020, -0.0097,  0.0122,  ...,  0.0008, -0.0045, -0.0183],
        [-0.0107, -0.0136, -0.0075,  ...,  0.0070, -0.0150,  0.0051],
        [-0.0039, -0.0031,  0.0129,  ..., -0.0059,  0.0184, -0.0066],
        ...,
        [ 0.0058, -0.0019, -0.0008,  ..., -0.0079, -0.0056, -0.0050],
        [ 0.0087, -0.0056, -0.0036,  ..., -0.0056, -0.0029,  0.0063],
        [-0.0005, -0.0012, -0.0051,  ..., -0.0076,  0.0086,  0.0036]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4839,  2.4668, -2.9961,  ..., -3.6113,  0.2314,  2.8047]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:42:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of memory is memories
The plural form of story is stories
The plural form of analysis is analyses
The plural form of security is securities
The plural form of child is children
The plural form of family is families
The plural form of species is species
The plural form of army is
2024-07-24 02:42:45 root INFO     [order_1_approx] starting weight calculation for The plural form of family is families
The plural form of story is stories
The plural form of army is armies
The plural form of security is securities
The plural form of memory is memories
The plural form of analysis is analyses
The plural form of species is species
The plural form of child is
2024-07-24 02:42:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 02:46:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0830, -0.5820, -0.6377,  ...,  0.2412,  0.0143,  1.0732],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.3369, -3.5352, -3.7266,  ...,  0.1436,  3.6914, -0.0503],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0140,  0.0004,  0.0148,  ...,  0.0127, -0.0074, -0.0154],
        [-0.0055, -0.0195, -0.0025,  ...,  0.0047,  0.0067, -0.0112],
        [ 0.0240,  0.0141, -0.0244,  ..., -0.0025,  0.0190,  0.0023],
        ...,
        [ 0.0012,  0.0088, -0.0095,  ..., -0.0198, -0.0005, -0.0059],
        [-0.0042, -0.0121,  0.0109,  ...,  0.0049, -0.0232,  0.0164],
        [-0.0113, -0.0094, -0.0162,  ...,  0.0057,  0.0017, -0.0160]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3564, -3.5059, -3.3906,  ...,  0.0731,  3.1035, -0.4490]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:46:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of family is families
The plural form of story is stories
The plural form of army is armies
The plural form of security is securities
The plural form of memory is memories
The plural form of analysis is analyses
The plural form of species is species
The plural form of child is
2024-07-24 02:46:21 root INFO     [order_1_approx] starting weight calculation for The plural form of memory is memories
The plural form of child is children
The plural form of security is securities
The plural form of species is species
The plural form of analysis is analyses
The plural form of army is armies
The plural form of story is stories
The plural form of family is
2024-07-24 02:46:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 02:50:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2406, -0.0171,  0.9487,  ..., -0.1165,  0.5195,  0.6650],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1887, -0.8521, -2.1328,  ..., -3.9395,  1.6699,  3.9531],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0177, -0.0029,  0.0139,  ...,  0.0072,  0.0026, -0.0187],
        [-0.0074, -0.0016, -0.0064,  ..., -0.0067, -0.0033, -0.0111],
        [ 0.0203,  0.0080, -0.0066,  ..., -0.0024,  0.0096, -0.0155],
        ...,
        [-0.0001,  0.0075,  0.0046,  ..., -0.0118,  0.0052, -0.0154],
        [-0.0057, -0.0025, -0.0004,  ..., -0.0061, -0.0189,  0.0163],
        [-0.0089, -0.0128, -0.0088,  ..., -0.0084,  0.0074, -0.0016]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9141,  0.0913, -2.8047,  ..., -4.5664,  0.6572,  4.0352]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:50:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of memory is memories
The plural form of child is children
The plural form of security is securities
The plural form of species is species
The plural form of analysis is analyses
The plural form of army is armies
The plural form of story is stories
The plural form of family is
2024-07-24 02:50:03 root INFO     [order_1_approx] starting weight calculation for The plural form of security is securities
The plural form of family is families
The plural form of army is armies
The plural form of story is stories
The plural form of species is species
The plural form of child is children
The plural form of analysis is analyses
The plural form of memory is
2024-07-24 02:50:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 02:53:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.6758, -0.3301, -0.0293,  ...,  0.5596, -0.2076,  0.8149],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-5.4727,  1.2852, -2.5117,  ...,  1.3066, -2.4395,  4.6094],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0023, -0.0199,  0.0213,  ..., -0.0001,  0.0098, -0.0183],
        [-0.0105,  0.0003,  0.0072,  ...,  0.0136,  0.0074,  0.0167],
        [ 0.0087,  0.0189, -0.0075,  ...,  0.0149,  0.0304,  0.0078],
        ...,
        [-0.0100, -0.0115,  0.0078,  ..., -0.0073,  0.0038, -0.0130],
        [ 0.0116, -0.0040, -0.0025,  ..., -0.0008, -0.0221,  0.0185],
        [-0.0113,  0.0066, -0.0067,  ...,  0.0101, -0.0030, -0.0062]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-6.1250,  2.1523, -3.1562,  ...,  0.3833, -3.1113,  4.9570]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:53:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of security is securities
The plural form of family is families
The plural form of army is armies
The plural form of story is stories
The plural form of species is species
The plural form of child is children
The plural form of analysis is analyses
The plural form of memory is
2024-07-24 02:53:45 root INFO     [order_1_approx] starting weight calculation for The plural form of analysis is analyses
The plural form of story is stories
The plural form of family is families
The plural form of child is children
The plural form of species is species
The plural form of army is armies
The plural form of memory is memories
The plural form of security is
2024-07-24 02:53:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 02:57:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7710, -0.7832, -0.0135,  ..., -0.1938, -0.3174,  1.5049],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.7754,  2.2227,  2.3418,  ..., -1.3926, -2.0469, -0.4844],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0202, -0.0273,  0.0138,  ..., -0.0106, -0.0263, -0.0011],
        [ 0.0012, -0.0278, -0.0063,  ..., -0.0041,  0.0041,  0.0176],
        [-0.0062, -0.0004, -0.0020,  ...,  0.0059,  0.0205,  0.0132],
        ...,
        [-0.0229, -0.0216,  0.0020,  ..., -0.0047,  0.0220, -0.0235],
        [ 0.0050, -0.0073, -0.0005,  ..., -0.0046, -0.0170, -0.0037],
        [-0.0027,  0.0311, -0.0083,  ..., -0.0096,  0.0097, -0.0138]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.6055,  2.6309,  3.1562,  ..., -1.7539, -2.9766, -0.8623]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:57:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of analysis is analyses
The plural form of story is stories
The plural form of family is families
The plural form of child is children
The plural form of species is species
The plural form of army is armies
The plural form of memory is memories
The plural form of security is
2024-07-24 02:57:29 root INFO     [order_1_approx] starting weight calculation for The plural form of security is securities
The plural form of analysis is analyses
The plural form of family is families
The plural form of child is children
The plural form of army is armies
The plural form of memory is memories
The plural form of story is stories
The plural form of species is
2024-07-24 02:57:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 03:01:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5879,  0.7354, -0.3369,  ...,  0.8032,  0.4648,  0.7148],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0891, -1.8896,  0.2285,  ...,  1.4688, -1.6689,  3.0469],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.8564e-02, -4.5349e-02,  8.4991e-03,  ...,  3.2997e-04,
         -1.9211e-02, -5.4626e-03],
        [-9.1705e-03, -9.4070e-03, -8.6975e-03,  ...,  4.4289e-03,
         -3.1662e-04, -1.1425e-03],
        [ 7.1335e-03, -3.9291e-03, -1.7532e-02,  ..., -6.8207e-03,
          1.4053e-02, -2.7771e-03],
        ...,
        [-1.3931e-02, -9.4910e-03, -1.0994e-02,  ..., -1.4488e-02,
          1.6235e-02, -5.8899e-03],
        [ 9.8114e-03, -1.8883e-04,  1.2154e-02,  ...,  2.0123e-03,
         -3.0304e-02,  6.9351e-03],
        [ 8.3923e-05,  8.2626e-03, -8.7357e-03,  ...,  7.4692e-03,
          1.3512e-02, -1.6846e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2158, -1.7168,  0.3127,  ...,  1.2324, -2.3105,  3.3906]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:01:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of security is securities
The plural form of analysis is analyses
The plural form of family is families
The plural form of child is children
The plural form of army is armies
The plural form of memory is memories
The plural form of story is stories
The plural form of species is
2024-07-24 03:01:12 root INFO     [order_1_approx] starting weight calculation for The plural form of army is armies
The plural form of memory is memories
The plural form of family is families
The plural form of security is securities
The plural form of analysis is analyses
The plural form of child is children
The plural form of species is species
The plural form of story is
2024-07-24 03:01:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 03:04:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9653, -0.0264,  0.2354,  ...,  0.6152,  0.9443, -0.1521],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1095,  2.5625,  1.4102,  ...,  2.9375, -6.6602,  1.7158],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0178, -0.0010,  0.0174,  ..., -0.0120,  0.0073, -0.0213],
        [-0.0253, -0.0167, -0.0102,  ..., -0.0119, -0.0212,  0.0116],
        [-0.0128, -0.0128, -0.0116,  ..., -0.0077,  0.0094, -0.0061],
        ...,
        [-0.0103, -0.0097,  0.0065,  ..., -0.0145, -0.0125, -0.0097],
        [ 0.0116,  0.0211,  0.0097,  ...,  0.0055, -0.0333,  0.0155],
        [-0.0142,  0.0082,  0.0058,  ..., -0.0157,  0.0045, -0.0190]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0295,  2.8848,  1.5869,  ...,  3.0918, -7.3047,  2.1074]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:04:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of army is armies
The plural form of memory is memories
The plural form of family is families
The plural form of security is securities
The plural form of analysis is analyses
The plural form of child is children
The plural form of species is species
The plural form of story is
2024-07-24 03:04:54 root INFO     total operator prediction time: 1776.5941743850708 seconds
2024-07-24 03:04:54 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on Ving - verb_inf
2024-07-24 03:04:54 root INFO     building operator Ving - verb_inf
2024-07-24 03:04:54 root INFO     [order_1_approx] starting weight calculation for improving is the active form of improve
involving is the active form of involve
reducing is the active form of reduce
teaching is the active form of teach
following is the active form of follow
operating is the active form of operate
establishing is the active form of establish
enjoying is the active form of
2024-07-24 03:04:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 03:08:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5195, -0.3516,  0.9204,  ..., -0.6743,  1.1650,  0.8857],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.2695,  1.2959,  0.9922,  ..., -0.5439,  1.1855,  3.9766],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0037, -0.0168,  0.0242,  ...,  0.0214, -0.0043, -0.0080],
        [ 0.0047, -0.0041,  0.0052,  ...,  0.0135,  0.0045,  0.0064],
        [ 0.0114,  0.0031, -0.0275,  ..., -0.0137,  0.0056, -0.0195],
        ...,
        [-0.0329,  0.0050,  0.0055,  ..., -0.0205, -0.0086,  0.0006],
        [ 0.0045,  0.0025,  0.0025,  ...,  0.0039, -0.0195,  0.0252],
        [ 0.0169, -0.0109, -0.0014,  ..., -0.0150, -0.0008, -0.0151]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4961,  1.2021,  0.8794,  ..., -0.1389,  0.5190,  4.5312]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:08:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for improving is the active form of improve
involving is the active form of involve
reducing is the active form of reduce
teaching is the active form of teach
following is the active form of follow
operating is the active form of operate
establishing is the active form of establish
enjoying is the active form of
2024-07-24 03:08:35 root INFO     [order_1_approx] starting weight calculation for teaching is the active form of teach
following is the active form of follow
enjoying is the active form of enjoy
reducing is the active form of reduce
operating is the active form of operate
involving is the active form of involve
improving is the active form of improve
establishing is the active form of
2024-07-24 03:08:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 03:12:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0492, -0.8018,  1.0664,  ...,  0.1605,  0.8433, -0.6133],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5977,  1.3594,  2.1133,  ..., -1.5859,  0.1133,  0.5381],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-9.1324e-03, -1.3676e-03,  9.9258e-03,  ..., -8.4381e-03,
         -1.2482e-02, -2.3804e-03],
        [ 5.9204e-03, -1.2245e-02,  1.3062e-02,  ...,  1.0811e-02,
          3.9177e-03,  1.9974e-02],
        [ 2.2430e-03, -2.1393e-02, -1.0162e-02,  ..., -5.0659e-03,
          4.7531e-03,  1.9875e-03],
        ...,
        [-2.3376e-02, -7.9575e-03, -3.2959e-03,  ..., -2.0691e-02,
          1.9073e-06, -4.9171e-03],
        [-1.6603e-03,  1.2558e-02,  1.3000e-02,  ..., -1.6907e-02,
         -7.4387e-03,  1.5869e-02],
        [ 4.7150e-03,  3.8776e-03,  1.1398e-02,  ..., -2.2858e-02,
          1.1215e-02, -2.2507e-04]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1094,  1.9102,  1.8418,  ..., -1.7920,  0.0533,  0.7588]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:12:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for teaching is the active form of teach
following is the active form of follow
enjoying is the active form of enjoy
reducing is the active form of reduce
operating is the active form of operate
involving is the active form of involve
improving is the active form of improve
establishing is the active form of
2024-07-24 03:12:15 root INFO     [order_1_approx] starting weight calculation for improving is the active form of improve
establishing is the active form of establish
enjoying is the active form of enjoy
involving is the active form of involve
reducing is the active form of reduce
teaching is the active form of teach
operating is the active form of operate
following is the active form of
2024-07-24 03:12:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 03:15:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9648, -0.3105,  2.7812,  ..., -0.1804,  1.0977,  0.4929],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.5840, -1.3779, -3.0078,  ..., -4.4414, -0.2002,  2.3359],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0015,  0.0006,  0.0138,  ...,  0.0267, -0.0030, -0.0334],
        [ 0.0060,  0.0067, -0.0046,  ...,  0.0079,  0.0049,  0.0016],
        [ 0.0054,  0.0025, -0.0166,  ..., -0.0221, -0.0205,  0.0075],
        ...,
        [-0.0007, -0.0009, -0.0107,  ..., -0.0068, -0.0062, -0.0121],
        [-0.0119,  0.0005,  0.0092,  ...,  0.0055, -0.0199,  0.0187],
        [ 0.0205, -0.0035,  0.0108,  ..., -0.0011, -0.0086, -0.0056]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.3945, -1.2842, -3.7578,  ..., -5.0117, -0.4141,  1.9268]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:15:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for improving is the active form of improve
establishing is the active form of establish
enjoying is the active form of enjoy
involving is the active form of involve
reducing is the active form of reduce
teaching is the active form of teach
operating is the active form of operate
following is the active form of
2024-07-24 03:15:54 root INFO     [order_1_approx] starting weight calculation for involving is the active form of involve
reducing is the active form of reduce
teaching is the active form of teach
establishing is the active form of establish
operating is the active form of operate
enjoying is the active form of enjoy
following is the active form of follow
improving is the active form of
2024-07-24 03:15:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 03:19:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5977, -0.8945,  0.7373,  ..., -0.4854,  1.2969, -0.4353],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 9.9141,  1.7695, -2.5664,  ...,  0.6875, -1.1641,  0.3066],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0016, -0.0036,  0.0130,  ...,  0.0069,  0.0019, -0.0297],
        [ 0.0084, -0.0131, -0.0034,  ...,  0.0028,  0.0063,  0.0015],
        [ 0.0004, -0.0110, -0.0185,  ..., -0.0124,  0.0060, -0.0186],
        ...,
        [-0.0114, -0.0008, -0.0050,  ...,  0.0046,  0.0041,  0.0055],
        [-0.0111,  0.0057,  0.0116,  ..., -0.0104, -0.0224,  0.0248],
        [ 0.0067,  0.0190,  0.0225,  ..., -0.0051,  0.0106,  0.0078]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[10.1875,  2.4512, -3.3320,  ...,  0.8135, -0.5493,  0.8291]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:19:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for involving is the active form of involve
reducing is the active form of reduce
teaching is the active form of teach
establishing is the active form of establish
operating is the active form of operate
enjoying is the active form of enjoy
following is the active form of follow
improving is the active form of
2024-07-24 03:19:38 root INFO     [order_1_approx] starting weight calculation for reducing is the active form of reduce
enjoying is the active form of enjoy
improving is the active form of improve
teaching is the active form of teach
establishing is the active form of establish
operating is the active form of operate
following is the active form of follow
involving is the active form of
2024-07-24 03:19:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 03:23:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7256, -0.7036,  0.9253,  ...,  0.3118,  1.8525,  0.8604],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.3516, -1.9844, -3.8906,  ...,  0.5669,  1.4443,  1.1973],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0211, -0.0195,  0.0365,  ...,  0.0186, -0.0106, -0.0110],
        [-0.0157,  0.0026,  0.0169,  ...,  0.0173,  0.0187,  0.0337],
        [ 0.0206,  0.0412, -0.0028,  ..., -0.0365, -0.0076,  0.0046],
        ...,
        [-0.0273, -0.0090, -0.0030,  ...,  0.0011,  0.0043, -0.0141],
        [-0.0081,  0.0230,  0.0330,  ...,  0.0059, -0.0304,  0.0295],
        [ 0.0021, -0.0094, -0.0006,  ..., -0.0201,  0.0215, -0.0366]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.7852, -1.3848, -4.0781,  ..., -0.9878,  1.3262,  1.8301]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:23:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for reducing is the active form of reduce
enjoying is the active form of enjoy
improving is the active form of improve
teaching is the active form of teach
establishing is the active form of establish
operating is the active form of operate
following is the active form of follow
involving is the active form of
2024-07-24 03:23:24 root INFO     [order_1_approx] starting weight calculation for teaching is the active form of teach
improving is the active form of improve
following is the active form of follow
establishing is the active form of establish
involving is the active form of involve
enjoying is the active form of enjoy
reducing is the active form of reduce
operating is the active form of
2024-07-24 03:23:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 03:27:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1936, -0.3855,  1.7832,  ...,  0.1201,  1.4814,  0.4648],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.5742, -0.6035,  0.0840,  ..., -0.7832,  0.5830,  3.4414],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0049, -0.0073,  0.0133,  ...,  0.0074, -0.0067, -0.0108],
        [ 0.0033, -0.0218,  0.0018,  ...,  0.0092, -0.0010,  0.0085],
        [ 0.0090, -0.0191, -0.0126,  ...,  0.0038, -0.0049, -0.0062],
        ...,
        [-0.0237,  0.0050, -0.0062,  ..., -0.0084,  0.0036,  0.0007],
        [-0.0055,  0.0102,  0.0081,  ..., -0.0180, -0.0136,  0.0130],
        [-0.0006,  0.0048, -0.0018,  ..., -0.0050,  0.0085, -0.0164]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.7617,  0.0317,  0.0994,  ..., -1.1172,  0.4497,  3.7812]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:27:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for teaching is the active form of teach
improving is the active form of improve
following is the active form of follow
establishing is the active form of establish
involving is the active form of involve
enjoying is the active form of enjoy
reducing is the active form of reduce
operating is the active form of
2024-07-24 03:27:05 root INFO     [order_1_approx] starting weight calculation for following is the active form of follow
improving is the active form of improve
operating is the active form of operate
establishing is the active form of establish
involving is the active form of involve
enjoying is the active form of enjoy
teaching is the active form of teach
reducing is the active form of
2024-07-24 03:27:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 03:30:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1354,  0.1968,  1.1045,  ..., -0.6846,  0.6938,  0.0526],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.2734, -0.8701,  0.3281,  ..., -1.7119,  0.2803,  1.6699],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0133,  0.0168,  0.0112,  ...,  0.0114, -0.0068, -0.0075],
        [ 0.0100,  0.0025, -0.0001,  ...,  0.0096,  0.0056,  0.0136],
        [-0.0040,  0.0012,  0.0005,  ...,  0.0049, -0.0019, -0.0125],
        ...,
        [-0.0257, -0.0018,  0.0089,  ..., -0.0083,  0.0006, -0.0086],
        [-0.0077,  0.0058, -0.0053,  ..., -0.0004, -0.0137,  0.0089],
        [ 0.0192, -0.0153,  0.0071,  ..., -0.0203,  0.0033,  0.0052]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.0078, -0.7583,  0.6016,  ..., -2.1523,  1.0469,  1.7656]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:30:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for following is the active form of follow
improving is the active form of improve
operating is the active form of operate
establishing is the active form of establish
involving is the active form of involve
enjoying is the active form of enjoy
teaching is the active form of teach
reducing is the active form of
2024-07-24 03:30:43 root INFO     [order_1_approx] starting weight calculation for following is the active form of follow
involving is the active form of involve
operating is the active form of operate
improving is the active form of improve
enjoying is the active form of enjoy
reducing is the active form of reduce
establishing is the active form of establish
teaching is the active form of
2024-07-24 03:30:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 03:34:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([2.0176, 0.1978, 2.1328,  ..., 0.1602, 0.6362, 0.0033], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5195,  0.0449,  0.2793,  ..., -1.3232,  1.0195,  1.4316],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0012,  0.0047,  0.0115,  ...,  0.0043,  0.0038, -0.0059],
        [ 0.0022, -0.0003,  0.0037,  ...,  0.0085,  0.0020,  0.0042],
        [ 0.0062,  0.0012, -0.0120,  ..., -0.0009, -0.0049, -0.0053],
        ...,
        [-0.0258, -0.0020,  0.0089,  ..., -0.0084, -0.0026, -0.0034],
        [-0.0042,  0.0144,  0.0101,  ..., -0.0081, -0.0163,  0.0023],
        [ 0.0158,  0.0021,  0.0004,  ...,  0.0014,  0.0024, -0.0078]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.2969e+00,  1.6785e-03,  3.9722e-01,  ..., -1.7627e+00,
          5.2344e-01,  7.7197e-01]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SubBackward0>) 

                    
2024-07-24 03:34:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for following is the active form of follow
involving is the active form of involve
operating is the active form of operate
improving is the active form of improve
enjoying is the active form of enjoy
reducing is the active form of reduce
establishing is the active form of establish
teaching is the active form of
2024-07-24 03:34:24 root INFO     total operator prediction time: 1770.4620175361633 seconds
2024-07-24 03:34:24 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_Ving - Ved
2024-07-24 03:34:24 root INFO     building operator verb_Ving - Ved
2024-07-24 03:34:24 root INFO     [order_1_approx] starting weight calculation for After something is replacing, it has replaced
After something is asking, it has asked
After something is telling, it has told
After something is receiving, it has received
After something is requiring, it has required
After something is following, it has followed
After something is considering, it has considered
After something is allowing, it has
2024-07-24 03:34:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 03:38:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2461, -0.5474,  1.0557,  ..., -0.0453,  0.2725,  0.2581],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6855, -0.0433,  1.8682,  ...,  2.2676, -2.3203,  3.7227],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0083, -0.0067, -0.0012,  ...,  0.0077,  0.0058, -0.0064],
        [-0.0020, -0.0341,  0.0085,  ...,  0.0082,  0.0055, -0.0072],
        [ 0.0030, -0.0239, -0.0260,  ...,  0.0025,  0.0024, -0.0140],
        ...,
        [-0.0355, -0.0245, -0.0134,  ..., -0.0213,  0.0077,  0.0030],
        [ 0.0159, -0.0123,  0.0094,  ..., -0.0148, -0.0237,  0.0317],
        [-0.0053, -0.0077, -0.0024,  ...,  0.0093, -0.0138, -0.0111]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1216,  0.1105,  1.7578,  ...,  1.8203, -2.8340,  2.8789]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:38:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is replacing, it has replaced
After something is asking, it has asked
After something is telling, it has told
After something is receiving, it has received
After something is requiring, it has required
After something is following, it has followed
After something is considering, it has considered
After something is allowing, it has
2024-07-24 03:38:07 root INFO     [order_1_approx] starting weight calculation for After something is replacing, it has replaced
After something is telling, it has told
After something is following, it has followed
After something is considering, it has considered
After something is requiring, it has required
After something is receiving, it has received
After something is allowing, it has allowed
After something is asking, it has
2024-07-24 03:38:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 03:41:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4922,  0.3193,  0.6582,  ..., -0.9033,  1.0264,  0.2622],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([1.5791, 2.4531, 3.1172,  ..., 3.5742, 0.9199, 4.1133], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0160, -0.0213,  0.0047,  ...,  0.0125, -0.0210,  0.0021],
        [-0.0138, -0.0242, -0.0048,  ...,  0.0099,  0.0098, -0.0129],
        [ 0.0012,  0.0057, -0.0244,  ...,  0.0083, -0.0009,  0.0056],
        ...,
        [-0.0323, -0.0124, -0.0130,  ..., -0.0383,  0.0280, -0.0158],
        [-0.0010, -0.0069,  0.0100,  ..., -0.0153, -0.0492,  0.0247],
        [-0.0056,  0.0144, -0.0020,  ...,  0.0033,  0.0278, -0.0314]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[1.9131, 2.0684, 3.1035,  ..., 3.2344, 0.5957, 2.8359]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:41:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is replacing, it has replaced
After something is telling, it has told
After something is following, it has followed
After something is considering, it has considered
After something is requiring, it has required
After something is receiving, it has received
After something is allowing, it has allowed
After something is asking, it has
2024-07-24 03:41:49 root INFO     [order_1_approx] starting weight calculation for After something is following, it has followed
After something is replacing, it has replaced
After something is asking, it has asked
After something is requiring, it has required
After something is receiving, it has received
After something is telling, it has told
After something is allowing, it has allowed
After something is considering, it has
2024-07-24 03:41:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 03:45:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2295, -0.2583, -0.5117,  ...,  0.2233,  0.0889,  1.2158],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.5830,  2.9492,  0.2266,  ...,  1.2061, -4.4570, -0.1934],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0219, -0.0113,  0.0159,  ...,  0.0162, -0.0229, -0.0053],
        [-0.0127, -0.0195, -0.0090,  ..., -0.0030,  0.0197, -0.0051],
        [-0.0133,  0.0035, -0.0216,  ...,  0.0054,  0.0037, -0.0199],
        ...,
        [-0.0095, -0.0200, -0.0025,  ..., -0.0308,  0.0211, -0.0092],
        [ 0.0139, -0.0145,  0.0015,  ..., -0.0316, -0.0344,  0.0272],
        [ 0.0058,  0.0121, -0.0166,  ...,  0.0098,  0.0036, -0.0333]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2090,  3.5957, -0.5347,  ...,  1.8945, -4.6758,  0.2517]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:45:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is following, it has followed
After something is replacing, it has replaced
After something is asking, it has asked
After something is requiring, it has required
After something is receiving, it has received
After something is telling, it has told
After something is allowing, it has allowed
After something is considering, it has
2024-07-24 03:45:32 root INFO     [order_1_approx] starting weight calculation for After something is telling, it has told
After something is asking, it has asked
After something is requiring, it has required
After something is allowing, it has allowed
After something is receiving, it has received
After something is replacing, it has replaced
After something is considering, it has considered
After something is following, it has
2024-07-24 03:45:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 03:49:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0186, -0.6309,  1.9678,  ...,  0.3604, -0.1035,  1.1855],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3438, -0.0249, -0.2041,  ..., -1.1357,  1.2793,  3.9707],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.7084e-03, -5.0621e-03,  8.5068e-03,  ...,  2.6054e-03,
         -3.6438e-02, -1.5175e-02],
        [ 6.1798e-04, -1.9302e-02,  1.2329e-02,  ...,  1.3000e-02,
         -7.4730e-03, -1.4603e-02],
        [-5.4245e-03, -1.1002e-02,  3.0136e-04,  ...,  8.3008e-03,
         -1.1978e-02,  1.0765e-02],
        ...,
        [-1.8433e-02, -1.0246e-02, -2.8152e-03,  ..., -1.0635e-02,
          6.1035e-03,  9.3155e-03],
        [-2.0599e-03,  9.8114e-03, -3.6240e-05,  ..., -1.0815e-03,
         -3.2867e-02,  1.7212e-02],
        [ 9.7084e-04,  1.9623e-02, -2.6684e-03,  ..., -1.1253e-02,
          7.5951e-03, -2.3621e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7188,  0.0983, -0.8525,  ..., -0.8936,  1.3838,  2.4844]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:49:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is telling, it has told
After something is asking, it has asked
After something is requiring, it has required
After something is allowing, it has allowed
After something is receiving, it has received
After something is replacing, it has replaced
After something is considering, it has considered
After something is following, it has
2024-07-24 03:49:13 root INFO     [order_1_approx] starting weight calculation for After something is allowing, it has allowed
After something is considering, it has considered
After something is asking, it has asked
After something is requiring, it has required
After something is replacing, it has replaced
After something is following, it has followed
After something is telling, it has told
After something is receiving, it has
2024-07-24 03:49:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 03:52:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1641, -0.4985,  1.0312,  ..., -0.0525,  0.2822,  1.1758],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.2715,  1.8438,  0.3340,  ...,  2.4980, -0.2097,  4.6406],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0274, -0.0104, -0.0117,  ..., -0.0042, -0.0181, -0.0286],
        [ 0.0072, -0.0185,  0.0012,  ...,  0.0036,  0.0178,  0.0123],
        [ 0.0219, -0.0063, -0.0073,  ...,  0.0079,  0.0066,  0.0054],
        ...,
        [-0.0093, -0.0217, -0.0328,  ..., -0.0197,  0.0065,  0.0084],
        [ 0.0197,  0.0093, -0.0186,  ..., -0.0108, -0.0405,  0.0521],
        [-0.0002, -0.0233,  0.0047,  ..., -0.0144,  0.0092, -0.0391]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.6650,  2.1543,  0.0878,  ...,  3.0039, -0.9346,  4.3008]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:52:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is allowing, it has allowed
After something is considering, it has considered
After something is asking, it has asked
After something is requiring, it has required
After something is replacing, it has replaced
After something is following, it has followed
After something is telling, it has told
After something is receiving, it has
2024-07-24 03:52:49 root INFO     [order_1_approx] starting weight calculation for After something is following, it has followed
After something is allowing, it has allowed
After something is telling, it has told
After something is asking, it has asked
After something is requiring, it has required
After something is receiving, it has received
After something is considering, it has considered
After something is replacing, it has
2024-07-24 03:52:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 03:56:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.6992,  0.1578,  1.1855,  ..., -0.1503,  0.2744, -0.7305],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([3.3711, 3.2520, 0.5596,  ..., 3.9414, 0.1333, 5.5781], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.2003e-02, -6.6566e-03,  1.7166e-03,  ..., -4.0207e-03,
         -1.6937e-02, -5.8517e-03],
        [-3.4065e-03, -2.0447e-02, -1.2894e-02,  ...,  9.4299e-03,
          1.2650e-02,  7.8201e-03],
        [ 1.0208e-02, -4.6539e-03, -2.2751e-02,  ...,  1.8406e-03,
         -4.4250e-03, -1.9150e-03],
        ...,
        [-2.4353e-02, -1.5152e-02, -3.1616e-02,  ..., -1.7593e-02,
          2.9388e-02, -3.1071e-03],
        [-1.9684e-03,  7.1716e-03, -2.7046e-03,  ...,  2.9373e-03,
         -4.5197e-02,  1.1009e-02],
        [ 1.3107e-02,  1.3512e-02,  3.2425e-05,  ..., -2.2202e-02,
         -1.6342e-02, -3.4790e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.5703,  3.3574,  0.2317,  ...,  3.8555, -0.3911,  5.5586]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:56:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is following, it has followed
After something is allowing, it has allowed
After something is telling, it has told
After something is asking, it has asked
After something is requiring, it has required
After something is receiving, it has received
After something is considering, it has considered
After something is replacing, it has
2024-07-24 03:56:33 root INFO     [order_1_approx] starting weight calculation for After something is asking, it has asked
After something is replacing, it has replaced
After something is allowing, it has allowed
After something is following, it has followed
After something is telling, it has told
After something is receiving, it has received
After something is considering, it has considered
After something is requiring, it has
2024-07-24 03:56:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 04:00:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2988, -0.0049,  0.7793,  ...,  0.5420,  0.9160,  0.8115],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.7793, -0.0461,  0.5605,  ...,  2.5820, -0.4189,  2.6953],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0149, -0.0089,  0.0049,  ..., -0.0097, -0.0163, -0.0088],
        [-0.0067, -0.0041, -0.0138,  ...,  0.0116,  0.0011, -0.0019],
        [ 0.0255, -0.0059, -0.0143,  ...,  0.0233, -0.0157, -0.0054],
        ...,
        [-0.0422, -0.0228, -0.0249,  ..., -0.0295,  0.0113,  0.0050],
        [-0.0149, -0.0121,  0.0067,  ..., -0.0239, -0.0179,  0.0231],
        [ 0.0125, -0.0156,  0.0034,  ...,  0.0073, -0.0080, -0.0125]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.7441,  0.9912,  0.3496,  ...,  1.6621, -0.2356,  1.0430]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 04:00:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is asking, it has asked
After something is replacing, it has replaced
After something is allowing, it has allowed
After something is following, it has followed
After something is telling, it has told
After something is receiving, it has received
After something is considering, it has considered
After something is requiring, it has
2024-07-24 04:00:15 root INFO     [order_1_approx] starting weight calculation for After something is allowing, it has allowed
After something is requiring, it has required
After something is considering, it has considered
After something is receiving, it has received
After something is asking, it has asked
After something is following, it has followed
After something is replacing, it has replaced
After something is telling, it has
2024-07-24 04:00:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 04:04:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2559, -0.2499,  1.3613,  ...,  0.4324,  0.2349,  0.7549],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([1.7051, 2.2285, 2.2422,  ..., 3.2500, 1.1914, 2.5820], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0228, -0.0219, -0.0004,  ...,  0.0052, -0.0247, -0.0145],
        [-0.0114, -0.0255,  0.0079,  ...,  0.0188,  0.0048,  0.0137],
        [ 0.0137,  0.0035, -0.0065,  ..., -0.0114, -0.0034,  0.0164],
        ...,
        [-0.0276, -0.0259, -0.0225,  ..., -0.0302,  0.0242,  0.0125],
        [ 0.0045, -0.0054,  0.0090,  ..., -0.0213, -0.0586,  0.0261],
        [-0.0006,  0.0213,  0.0047,  ...,  0.0211, -0.0022, -0.0200]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[3.0703, 2.4199, 1.6084,  ..., 4.8555, 0.5845, 1.1582]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 04:04:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is allowing, it has allowed
After something is requiring, it has required
After something is considering, it has considered
After something is receiving, it has received
After something is asking, it has asked
After something is following, it has followed
After something is replacing, it has replaced
After something is telling, it has
2024-07-24 04:04:02 root INFO     total operator prediction time: 1777.5208940505981 seconds
2024-07-24 04:04:02 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_inf - Ved
2024-07-24 04:04:02 root INFO     building operator verb_inf - Ved
2024-07-24 04:04:02 root INFO     [order_1_approx] starting weight calculation for If the present form is perform, the past form is performed
If the present form is remain, the past form is remained
If the present form is receive, the past form is received
If the present form is provide, the past form is provided
If the present form is discover, the past form is discovered
If the present form is believe, the past form is believed
If the present form is refer, the past form is referred
If the present form is add, the past form is
2024-07-24 04:04:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 04:07:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2688,  0.3308,  1.7031,  ...,  0.1886, -0.2244, -0.1729],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0812,  0.8350,  4.5156,  ..., -1.2881, -0.8428,  0.3496],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0056, -0.0019,  0.0026,  ..., -0.0008, -0.0090,  0.0023],
        [ 0.0065, -0.0106, -0.0082,  ...,  0.0041,  0.0079, -0.0090],
        [-0.0014,  0.0062, -0.0138,  ...,  0.0068,  0.0037, -0.0148],
        ...,
        [-0.0307, -0.0004,  0.0063,  ..., -0.0137, -0.0016,  0.0120],
        [ 0.0056,  0.0200,  0.0163,  ..., -0.0124, -0.0134, -0.0047],
        [-0.0085, -0.0083, -0.0086,  ...,  0.0017,  0.0083, -0.0055]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4092,  0.2427,  4.1211,  ..., -0.4448, -1.3477, -0.0063]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 04:07:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is perform, the past form is performed
If the present form is remain, the past form is remained
If the present form is receive, the past form is received
If the present form is provide, the past form is provided
If the present form is discover, the past form is discovered
If the present form is believe, the past form is believed
If the present form is refer, the past form is referred
If the present form is add, the past form is
2024-07-24 04:07:46 root INFO     [order_1_approx] starting weight calculation for If the present form is receive, the past form is received
If the present form is remain, the past form is remained
If the present form is refer, the past form is referred
If the present form is provide, the past form is provided
If the present form is discover, the past form is discovered
If the present form is add, the past form is added
If the present form is perform, the past form is performed
If the present form is believe, the past form is
2024-07-24 04:07:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 04:11:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5996,  0.3643,  0.0991,  ...,  0.6509,  0.4265, -0.0989],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3652,  0.8462,  0.2891,  ...,  2.0195, -3.9141,  0.4053],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0173, -0.0256,  0.0004,  ..., -0.0109,  0.0074, -0.0133],
        [-0.0065, -0.0253, -0.0077,  ..., -0.0004,  0.0153, -0.0173],
        [-0.0123, -0.0024, -0.0117,  ...,  0.0150,  0.0012, -0.0009],
        ...,
        [-0.0029,  0.0082, -0.0059,  ..., -0.0168,  0.0020,  0.0094],
        [ 0.0001,  0.0064,  0.0112,  ..., -0.0194, -0.0317,  0.0089],
        [ 0.0017,  0.0197, -0.0093,  ...,  0.0068, -0.0024, -0.0316]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0215,  0.7271,  0.5498,  ...,  2.3359, -4.2383,  1.0410]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 04:11:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is receive, the past form is received
If the present form is remain, the past form is remained
If the present form is refer, the past form is referred
If the present form is provide, the past form is provided
If the present form is discover, the past form is discovered
If the present form is add, the past form is added
If the present form is perform, the past form is performed
If the present form is believe, the past form is
2024-07-24 04:11:28 root INFO     [order_1_approx] starting weight calculation for If the present form is provide, the past form is provided
If the present form is perform, the past form is performed
If the present form is remain, the past form is remained
If the present form is believe, the past form is believed
If the present form is add, the past form is added
If the present form is refer, the past form is referred
If the present form is receive, the past form is received
If the present form is discover, the past form is
2024-07-24 04:11:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 04:15:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3535,  0.1541,  0.3091,  ...,  0.6279,  0.1323, -0.2949],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6738,  1.7148, -0.2402,  ..., -3.1250, -4.3789,  0.7871],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0025,  0.0002, -0.0019,  ..., -0.0092, -0.0100, -0.0066],
        [-0.0139, -0.0220, -0.0144,  ...,  0.0095,  0.0325, -0.0100],
        [-0.0072, -0.0070, -0.0274,  ..., -0.0024,  0.0070, -0.0172],
        ...,
        [-0.0160,  0.0034, -0.0102,  ..., -0.0132,  0.0045,  0.0090],
        [ 0.0168,  0.0117,  0.0120,  ..., -0.0003, -0.0377,  0.0071],
        [-0.0172, -0.0111,  0.0022,  ..., -0.0175,  0.0148, -0.0368]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.0605,  2.1562, -0.1736,  ..., -2.4258, -4.8477,  0.8374]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 04:15:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is provide, the past form is provided
If the present form is perform, the past form is performed
If the present form is remain, the past form is remained
If the present form is believe, the past form is believed
If the present form is add, the past form is added
If the present form is refer, the past form is referred
If the present form is receive, the past form is received
If the present form is discover, the past form is
2024-07-24 04:15:10 root INFO     [order_1_approx] starting weight calculation for If the present form is refer, the past form is referred
If the present form is add, the past form is added
If the present form is remain, the past form is remained
If the present form is provide, the past form is provided
If the present form is discover, the past form is discovered
If the present form is receive, the past form is received
If the present form is believe, the past form is believed
If the present form is perform, the past form is
2024-07-24 04:15:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 04:18:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1091,  0.9907,  1.4346,  ..., -0.2720,  0.1763,  0.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.5273, -0.1108,  0.1494,  ...,  3.1523,  0.0624,  1.8623],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-3.0365e-02, -1.8417e-02,  4.5433e-03,  ..., -1.2589e-03,
          1.2789e-03, -2.2797e-02],
        [-5.1346e-03, -2.7512e-02, -3.4447e-03,  ...,  1.5974e-05,
         -2.1152e-03, -3.8452e-03],
        [ 7.0877e-03, -1.0994e-02, -1.4824e-02,  ...,  7.3090e-03,
         -4.2534e-03, -8.0109e-04],
        ...,
        [-3.5339e-02,  3.1128e-03, -7.3471e-03,  ..., -2.3987e-02,
         -5.6696e-04,  5.7678e-03],
        [-3.9291e-04,  6.5765e-03,  1.7700e-02,  ..., -1.6861e-02,
         -1.7426e-02,  1.7212e-02],
        [-1.2184e-02,  1.3344e-02, -1.3046e-03,  ..., -5.1651e-03,
         -1.4524e-03, -1.3474e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.5605,  0.2036, -0.2427,  ...,  3.3203, -0.4229,  1.8623]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 04:18:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is refer, the past form is referred
If the present form is add, the past form is added
If the present form is remain, the past form is remained
If the present form is provide, the past form is provided
If the present form is discover, the past form is discovered
If the present form is receive, the past form is received
If the present form is believe, the past form is believed
If the present form is perform, the past form is
2024-07-24 04:18:49 root INFO     [order_1_approx] starting weight calculation for If the present form is believe, the past form is believed
If the present form is refer, the past form is referred
If the present form is perform, the past form is performed
If the present form is remain, the past form is remained
If the present form is receive, the past form is received
If the present form is add, the past form is added
If the present form is discover, the past form is discovered
If the present form is provide, the past form is
2024-07-24 04:18:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 04:22:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9111,  0.1283, -0.9492,  ...,  0.6528,  0.7329, -0.2622],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3262,  1.0840, -0.5952,  ..., -0.1123, -0.6318, -1.2402],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-6.9656e-03, -4.3678e-03, -1.3397e-02,  ..., -8.1329e-03,
          2.5749e-03, -5.3787e-03],
        [-4.8828e-04, -3.1967e-03, -1.0834e-03,  ..., -5.3406e-05,
         -2.9659e-03,  1.9855e-03],
        [ 4.1733e-03, -3.6163e-03, -1.3611e-02,  ..., -2.3499e-03,
         -3.4676e-03, -4.4937e-03],
        ...,
        [-1.5358e-02,  1.7868e-02, -1.5213e-02,  ..., -6.9084e-03,
         -8.9264e-03,  1.4633e-02],
        [ 5.4665e-03,  2.8591e-03,  1.2222e-02,  ..., -2.0584e-02,
         -1.5778e-02,  1.8539e-02],
        [-1.4778e-02, -1.2970e-04,  3.6926e-03,  ..., -3.0804e-04,
          1.5656e-02, -2.5681e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3027,  1.4199, -1.0605,  ..., -0.0629, -1.2051, -2.0078]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 04:22:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is believe, the past form is believed
If the present form is refer, the past form is referred
If the present form is perform, the past form is performed
If the present form is remain, the past form is remained
If the present form is receive, the past form is received
If the present form is add, the past form is added
If the present form is discover, the past form is discovered
If the present form is provide, the past form is
2024-07-24 04:22:32 root INFO     [order_1_approx] starting weight calculation for If the present form is add, the past form is added
If the present form is provide, the past form is provided
If the present form is discover, the past form is discovered
If the present form is refer, the past form is referred
If the present form is believe, the past form is believed
If the present form is perform, the past form is performed
If the present form is remain, the past form is remained
If the present form is receive, the past form is
2024-07-24 04:22:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 04:26:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1055, -0.8584,  0.9307,  ...,  0.0728, -0.2964,  1.1543],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.9453,  0.4539, -1.2285,  ...,  2.4941,  1.2344,  1.9531],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0311, -0.0026, -0.0144,  ..., -0.0103, -0.0051, -0.0128],
        [ 0.0008, -0.0217, -0.0006,  ...,  0.0025,  0.0080,  0.0013],
        [ 0.0098, -0.0023, -0.0224,  ...,  0.0029,  0.0097,  0.0012],
        ...,
        [ 0.0013,  0.0034, -0.0125,  ..., -0.0114,  0.0098,  0.0074],
        [ 0.0068, -0.0037,  0.0076,  ..., -0.0155, -0.0282,  0.0107],
        [-0.0181, -0.0050, -0.0006,  ..., -0.0104,  0.0049, -0.0368]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1367,  1.0713, -1.3672,  ...,  2.0977,  0.3008,  2.3066]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 04:26:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is add, the past form is added
If the present form is provide, the past form is provided
If the present form is discover, the past form is discovered
If the present form is refer, the past form is referred
If the present form is believe, the past form is believed
If the present form is perform, the past form is performed
If the present form is remain, the past form is remained
If the present form is receive, the past form is
2024-07-24 04:26:07 root INFO     [order_1_approx] starting weight calculation for If the present form is discover, the past form is discovered
If the present form is receive, the past form is received
If the present form is add, the past form is added
If the present form is believe, the past form is believed
If the present form is remain, the past form is remained
If the present form is provide, the past form is provided
If the present form is perform, the past form is performed
If the present form is refer, the past form is
2024-07-24 04:26:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 04:29:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0693,  0.8423,  0.4592,  ..., -0.6494,  0.2075, -0.0952],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6221, -0.0405,  2.4414,  ...,  4.7773,  1.8164, -0.0381],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0211, -0.0172,  0.0029,  ..., -0.0146, -0.0043,  0.0009],
        [-0.0133, -0.0203,  0.0004,  ...,  0.0029,  0.0164, -0.0120],
        [ 0.0016, -0.0090, -0.0257,  ...,  0.0013, -0.0037, -0.0119],
        ...,
        [-0.0225, -0.0019, -0.0138,  ..., -0.0374,  0.0061, -0.0046],
        [-0.0061, -0.0032,  0.0266,  ..., -0.0077, -0.0290, -0.0021],
        [-0.0065,  0.0094,  0.0128,  ..., -0.0059,  0.0070, -0.0193]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[0.3789, 0.2480, 2.1699,  ..., 4.2344, 1.9277, 0.8931]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 04:29:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is discover, the past form is discovered
If the present form is receive, the past form is received
If the present form is add, the past form is added
If the present form is believe, the past form is believed
If the present form is remain, the past form is remained
If the present form is provide, the past form is provided
If the present form is perform, the past form is performed
If the present form is refer, the past form is
2024-07-24 04:29:42 root INFO     [order_1_approx] starting weight calculation for If the present form is believe, the past form is believed
If the present form is provide, the past form is provided
If the present form is perform, the past form is performed
If the present form is refer, the past form is referred
If the present form is discover, the past form is discovered
If the present form is receive, the past form is received
If the present form is add, the past form is added
If the present form is remain, the past form is
2024-07-24 04:29:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 04:33:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3188, -0.4561,  1.5488,  ...,  1.2334, -0.1548,  0.2449],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.3242,  1.4834, -2.1035,  ...,  2.8555,  1.9170,  0.2485],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0105,  0.0030, -0.0158,  ...,  0.0143,  0.0077, -0.0145],
        [-0.0039, -0.0227, -0.0028,  ...,  0.0044,  0.0133, -0.0092],
        [ 0.0043,  0.0061, -0.0260,  ...,  0.0062,  0.0132,  0.0058],
        ...,
        [-0.0164, -0.0056, -0.0014,  ..., -0.0100, -0.0126,  0.0089],
        [-0.0033, -0.0131,  0.0064,  ..., -0.0126, -0.0173,  0.0162],
        [-0.0149,  0.0060, -0.0207,  ..., -0.0120,  0.0028, -0.0318]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.6680,  2.0547, -1.8877,  ...,  2.4863,  1.6465, -0.1541]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 04:33:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is believe, the past form is believed
If the present form is provide, the past form is provided
If the present form is perform, the past form is performed
If the present form is refer, the past form is referred
If the present form is discover, the past form is discovered
If the present form is receive, the past form is received
If the present form is add, the past form is added
If the present form is remain, the past form is
2024-07-24 04:33:12 root INFO     total operator prediction time: 1749.8910510540009 seconds
2024-07-24 04:33:12 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_inf - 3pSg
2024-07-24 04:33:12 root INFO     building operator verb_inf - 3pSg
2024-07-24 04:33:12 root INFO     [order_1_approx] starting weight calculation for I promote, he promotes
I apply, he applies
I receive, he receives
I contain, he contains
I include, he includes
I believe, he believes
I prevent, he prevents
I achieve, he
2024-07-24 04:33:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 04:36:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6924,  0.6758,  2.1406,  ..., -0.4512, -0.7227,  0.5107],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5684,  2.4980,  1.2793,  ..., -2.7969,  0.1484,  0.6768],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0021,  0.0050, -0.0073,  ..., -0.0017, -0.0109, -0.0171],
        [ 0.0070, -0.0099, -0.0073,  ..., -0.0070, -0.0079, -0.0045],
        [-0.0008, -0.0077, -0.0232,  ...,  0.0148,  0.0112, -0.0042],
        ...,
        [ 0.0011, -0.0016, -0.0174,  ..., -0.0220, -0.0181, -0.0023],
        [ 0.0021,  0.0104,  0.0083,  ...,  0.0024, -0.0156,  0.0086],
        [ 0.0001,  0.0213,  0.0139,  ..., -0.0038,  0.0106, -0.0089]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7109,  2.9355,  0.2744,  ..., -3.7246, -0.0768, -0.2935]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 04:36:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I promote, he promotes
I apply, he applies
I receive, he receives
I contain, he contains
I include, he includes
I believe, he believes
I prevent, he prevents
I achieve, he
2024-07-24 04:36:56 root INFO     [order_1_approx] starting weight calculation for I receive, he receives
I prevent, he prevents
I promote, he promotes
I include, he includes
I contain, he contains
I believe, he believes
I achieve, he achieves
I apply, he
2024-07-24 04:36:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 04:40:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1172, -0.1152,  1.7002,  ..., -0.4888, -0.0564,  1.5273],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.7266,  0.7036,  2.0449,  ...,  2.5156, -2.0742,  5.9727],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0077, -0.0048, -0.0001,  ..., -0.0005, -0.0084, -0.0124],
        [ 0.0092, -0.0113,  0.0015,  ..., -0.0015,  0.0057,  0.0005],
        [ 0.0095,  0.0161, -0.0175,  ...,  0.0091, -0.0107, -0.0038],
        ...,
        [-0.0140, -0.0161, -0.0141,  ..., -0.0151,  0.0127, -0.0077],
        [-0.0152, -0.0009,  0.0160,  ..., -0.0018, -0.0143,  0.0143],
        [ 0.0133,  0.0224,  0.0122,  ...,  0.0024,  0.0007, -0.0144]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.3008,  0.9316,  1.4141,  ...,  2.2773, -1.3867,  5.1875]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 04:40:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I receive, he receives
I prevent, he prevents
I promote, he promotes
I include, he includes
I contain, he contains
I believe, he believes
I achieve, he achieves
I apply, he
2024-07-24 04:40:39 root INFO     [order_1_approx] starting weight calculation for I achieve, he achieves
I include, he includes
I contain, he contains
I receive, he receives
I prevent, he prevents
I apply, he applies
I promote, he promotes
I believe, he
2024-07-24 04:40:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 04:44:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7178, -0.2710,  0.0703,  ...,  0.0815,  0.7900,  0.3542],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7861, -1.0176, -0.3423,  ...,  2.7930, -3.1504,  3.0898],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.3712e-02, -2.4521e-02,  1.9058e-02,  ..., -2.5940e-04,
         -8.2779e-04, -1.1673e-02],
        [-9.7809e-03, -8.2932e-03,  1.5457e-02,  ..., -8.1482e-03,
          2.0721e-02,  1.0834e-02],
        [-3.4409e-03, -2.9144e-03, -3.7476e-02,  ...,  2.6642e-02,
          1.4633e-02, -1.6785e-04],
        ...,
        [-2.2659e-03, -1.2054e-02, -1.6525e-02,  ..., -1.4969e-02,
         -3.5934e-03,  7.7057e-03],
        [ 1.3535e-02, -2.2888e-05,  1.4023e-02,  ..., -1.9348e-02,
         -3.8483e-02,  4.2152e-03],
        [ 1.0651e-02,  8.9493e-03,  5.9128e-03,  ...,  9.6130e-03,
         -1.9455e-03, -2.0264e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.0254, -0.9375, -0.0525,  ...,  2.4199, -3.0527,  2.2949]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 04:44:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I achieve, he achieves
I include, he includes
I contain, he contains
I receive, he receives
I prevent, he prevents
I apply, he applies
I promote, he promotes
I believe, he
2024-07-24 04:44:09 root INFO     [order_1_approx] starting weight calculation for I promote, he promotes
I achieve, he achieves
I believe, he believes
I prevent, he prevents
I include, he includes
I receive, he receives
I apply, he applies
I contain, he
2024-07-24 04:44:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 04:47:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1655,  0.1860,  0.7393,  ..., -0.2212, -0.3618,  0.9702],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.2520, -2.1836, -1.3867,  ...,  2.3086, -3.3438,  2.3047],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0149,  0.0016, -0.0020,  ...,  0.0038, -0.0005, -0.0042],
        [-0.0006, -0.0084,  0.0052,  ...,  0.0027,  0.0104, -0.0147],
        [ 0.0052,  0.0085, -0.0179,  ...,  0.0007,  0.0072, -0.0096],
        ...,
        [-0.0225, -0.0144, -0.0197,  ..., -0.0188,  0.0131,  0.0261],
        [-0.0114,  0.0105,  0.0122,  ..., -0.0079, -0.0147,  0.0063],
        [-0.0017,  0.0051,  0.0063,  ..., -0.0238,  0.0040, -0.0013]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0605, -2.1582, -1.8818,  ...,  2.4863, -3.1328,  2.7559]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 04:47:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I promote, he promotes
I achieve, he achieves
I believe, he believes
I prevent, he prevents
I include, he includes
I receive, he receives
I apply, he applies
I contain, he
2024-07-24 04:47:53 root INFO     [order_1_approx] starting weight calculation for I believe, he believes
I achieve, he achieves
I contain, he contains
I apply, he applies
I receive, he receives
I prevent, he prevents
I promote, he promotes
I include, he
2024-07-24 04:47:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 04:51:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5845,  0.7148,  1.2402,  ..., -0.2700, -0.0281,  2.0039],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0869, -2.7773, -1.7041,  ...,  3.7285, -1.8730,  0.8125],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0395,  0.0041,  0.0001,  ...,  0.0138,  0.0003,  0.0094],
        [-0.0016, -0.0006,  0.0042,  ...,  0.0071,  0.0078,  0.0035],
        [-0.0092, -0.0104, -0.0117,  ...,  0.0068, -0.0042, -0.0072],
        ...,
        [-0.0055, -0.0019, -0.0124,  ..., -0.0099, -0.0211,  0.0205],
        [ 0.0030,  0.0096,  0.0036,  ..., -0.0177, -0.0192,  0.0196],
        [-0.0073,  0.0127,  0.0070,  ..., -0.0101,  0.0113, -0.0077]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0342, -2.5469, -1.4395,  ...,  4.3555, -1.4219,  0.2803]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 04:51:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I believe, he believes
I achieve, he achieves
I contain, he contains
I apply, he applies
I receive, he receives
I prevent, he prevents
I promote, he promotes
I include, he
2024-07-24 04:51:38 root INFO     [order_1_approx] starting weight calculation for I achieve, he achieves
I contain, he contains
I promote, he promotes
I believe, he believes
I apply, he applies
I receive, he receives
I include, he includes
I prevent, he
2024-07-24 04:51:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 04:55:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.7739, 0.7192, 0.8662,  ..., 0.3142, 0.1648, 0.0466], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8633, -3.3691,  1.1162,  ...,  1.8691,  0.7070,  5.0977],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0036, -0.0162,  0.0089,  ..., -0.0035, -0.0019, -0.0354],
        [-0.0030, -0.0044,  0.0177,  ..., -0.0018, -0.0074,  0.0003],
        [-0.0167, -0.0079, -0.0310,  ..., -0.0050,  0.0030, -0.0048],
        ...,
        [-0.0268, -0.0118, -0.0215,  ...,  0.0036,  0.0137, -0.0030],
        [ 0.0075,  0.0182, -0.0004,  ..., -0.0186, -0.0236, -0.0024],
        [-0.0127,  0.0227,  0.0031,  ..., -0.0157,  0.0160, -0.0125]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.8047, -3.0566,  0.4517,  ...,  1.0605,  1.3174,  4.5547]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 04:55:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I achieve, he achieves
I contain, he contains
I promote, he promotes
I believe, he believes
I apply, he applies
I receive, he receives
I include, he includes
I prevent, he
2024-07-24 04:55:21 root INFO     [order_1_approx] starting weight calculation for I receive, he receives
I prevent, he prevents
I apply, he applies
I believe, he believes
I achieve, he achieves
I contain, he contains
I include, he includes
I promote, he
2024-07-24 04:55:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 04:59:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9648,  0.0083,  1.3418,  ..., -0.0342, -0.7275,  0.5547],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3604, -3.6738,  0.4360,  ...,  4.9688, -2.1016,  4.7812],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0061, -0.0381, -0.0059,  ..., -0.0098, -0.0150, -0.0336],
        [-0.0078, -0.0079,  0.0108,  ...,  0.0106,  0.0067, -0.0098],
        [-0.0017, -0.0111, -0.0193,  ...,  0.0041, -0.0127,  0.0114],
        ...,
        [-0.0079,  0.0030, -0.0080,  ..., -0.0029, -0.0153,  0.0089],
        [-0.0014,  0.0158,  0.0107,  ..., -0.0212, -0.0386,  0.0163],
        [-0.0185,  0.0145,  0.0100,  ...,  0.0049,  0.0030, -0.0158]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1479, -3.5742,  0.8535,  ...,  5.7812, -1.5449,  4.1367]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 04:59:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I receive, he receives
I prevent, he prevents
I apply, he applies
I believe, he believes
I achieve, he achieves
I contain, he contains
I include, he includes
I promote, he
2024-07-24 04:59:03 root INFO     [order_1_approx] starting weight calculation for I apply, he applies
I promote, he promotes
I include, he includes
I believe, he believes
I achieve, he achieves
I prevent, he prevents
I contain, he contains
I receive, he
2024-07-24 04:59:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 05:02:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0513, -0.7451,  0.9854,  ..., -0.3584, -0.2040,  1.6699],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2568, -1.4736,  1.5225,  ...,  2.8145, -0.1392,  5.5430],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0201, -0.0012, -0.0058,  ...,  0.0053, -0.0116, -0.0200],
        [ 0.0024, -0.0081,  0.0217,  ..., -0.0025,  0.0178,  0.0040],
        [-0.0047,  0.0044, -0.0289,  ...,  0.0014, -0.0035, -0.0042],
        ...,
        [-0.0169, -0.0278, -0.0113,  ..., -0.0147, -0.0120, -0.0043],
        [-0.0006,  0.0081,  0.0004,  ..., -0.0080, -0.0322,  0.0190],
        [-0.0136,  0.0065, -0.0075,  ..., -0.0017,  0.0038, -0.0232]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4580, -0.4985,  0.8862,  ...,  3.9258,  0.9565,  5.1016]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 05:02:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I apply, he applies
I promote, he promotes
I include, he includes
I believe, he believes
I achieve, he achieves
I prevent, he prevents
I contain, he contains
I receive, he
2024-07-24 05:02:47 root INFO     total operator prediction time: 1775.353528022766 seconds
2024-07-24 05:02:47 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_Ving - 3pSg
2024-07-24 05:02:47 root INFO     building operator verb_Ving - 3pSg
2024-07-24 05:02:47 root INFO     [order_1_approx] starting weight calculation for When something is becoming, it becomes
When something is continuing, it continues
When something is receiving, it receives
When something is performing, it performs
When something is referring, it refers
When something is publishing, it publishes
When something is happening, it happens
When something is appearing, it
2024-07-24 05:02:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 05:06:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3301, -0.7646,  1.3086,  ..., -0.0913,  0.9385,  0.1299],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.2773,  1.9346, -0.9385,  ...,  3.3203, -1.9043, -0.1680],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0031, -0.0130,  0.0013,  ...,  0.0005, -0.0078, -0.0145],
        [ 0.0037, -0.0126,  0.0152,  ..., -0.0159, -0.0343,  0.0145],
        [-0.0159,  0.0111, -0.0242,  ...,  0.0293, -0.0141,  0.0007],
        ...,
        [-0.0257, -0.0129, -0.0131,  ..., -0.0238,  0.0082,  0.0126],
        [-0.0115,  0.0010,  0.0139,  ..., -0.0302, -0.0224,  0.0171],
        [-0.0082, -0.0042,  0.0133,  ..., -0.0173,  0.0153, -0.0172]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4434,  3.5684, -1.8828,  ...,  3.6094, -1.1172, -1.5908]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 05:06:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is becoming, it becomes
When something is continuing, it continues
When something is receiving, it receives
When something is performing, it performs
When something is referring, it refers
When something is publishing, it publishes
When something is happening, it happens
When something is appearing, it
2024-07-24 05:06:30 root INFO     [order_1_approx] starting weight calculation for When something is appearing, it appears
When something is publishing, it publishes
When something is referring, it refers
When something is performing, it performs
When something is receiving, it receives
When something is happening, it happens
When something is continuing, it continues
When something is becoming, it
2024-07-24 05:06:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 05:10:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0081,  0.4114,  0.9092,  ...,  1.3535,  0.3774, -0.5830],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3809,  0.6055,  1.8164,  ...,  1.4795,  1.0537, -0.6006],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-3.6194e-02, -4.5105e-02,  1.2751e-03,  ..., -2.0935e-02,
         -1.9398e-03, -9.9792e-03],
        [ 2.7008e-03, -1.0239e-02, -1.5045e-02,  ..., -9.5367e-05,
         -1.1497e-02, -2.6459e-02],
        [ 7.7209e-03, -1.7273e-02, -2.8763e-02,  ...,  4.5128e-03,
         -2.6951e-03,  2.3499e-02],
        ...,
        [-7.4921e-03, -2.5730e-03, -1.1635e-04,  ..., -5.4443e-02,
         -6.4201e-03,  2.8076e-03],
        [-1.2238e-02,  4.0741e-03,  1.0681e-02,  ..., -6.0577e-03,
         -4.9896e-02,  4.8599e-03],
        [-9.8572e-03, -1.7891e-03, -8.6670e-03,  ..., -7.8125e-03,
          7.5188e-03, -2.2141e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.6641,  1.7168,  1.4336,  ...,  1.6328,  0.4248, -0.6777]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 05:10:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is appearing, it appears
When something is publishing, it publishes
When something is referring, it refers
When something is performing, it performs
When something is receiving, it receives
When something is happening, it happens
When something is continuing, it continues
When something is becoming, it
2024-07-24 05:10:14 root INFO     [order_1_approx] starting weight calculation for When something is receiving, it receives
When something is referring, it refers
When something is appearing, it appears
When something is becoming, it becomes
When something is performing, it performs
When something is publishing, it publishes
When something is happening, it happens
When something is continuing, it
2024-07-24 05:10:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 05:13:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4746, -0.3972,  0.4995,  ...,  0.5425,  1.1289,  0.0762],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1921, -1.3301, -0.5645,  ...,  2.3340, -1.5938,  2.5078],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.2476e-02, -1.9608e-02, -3.4294e-03,  ..., -1.2199e-02,
         -1.3321e-02, -1.8341e-02],
        [ 4.1351e-03, -6.3667e-03, -1.0345e-02,  ..., -9.1019e-03,
          3.7613e-03, -1.6823e-03],
        [ 1.4313e-02, -4.7455e-03, -3.5614e-02,  ...,  2.1118e-02,
         -1.2884e-03,  8.3466e-03],
        ...,
        [-2.2171e-02,  5.0735e-03, -1.6052e-02,  ..., -1.7349e-02,
          5.9662e-03, -3.4752e-03],
        [-6.1035e-05,  7.9803e-03,  1.9569e-03,  ...,  1.0681e-03,
         -3.3600e-02,  1.4732e-02],
        [-4.1313e-03,  3.0594e-03,  2.3479e-03,  ..., -1.3451e-02,
          1.0780e-02, -4.9469e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7598, -0.8848, -0.6694,  ...,  3.3555, -2.3555,  1.8457]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 05:13:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is receiving, it receives
When something is referring, it refers
When something is appearing, it appears
When something is becoming, it becomes
When something is performing, it performs
When something is publishing, it publishes
When something is happening, it happens
When something is continuing, it
2024-07-24 05:13:57 root INFO     [order_1_approx] starting weight calculation for When something is publishing, it publishes
When something is continuing, it continues
When something is referring, it refers
When something is becoming, it becomes
When something is receiving, it receives
When something is appearing, it appears
When something is performing, it performs
When something is happening, it
2024-07-24 05:13:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 05:17:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4402, -0.3867,  1.3262,  ...,  0.4075,  1.3740, -0.8301],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7217,  0.2444, -2.4414,  ...,  1.5391,  2.8770,  4.5938],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0309, -0.0208,  0.0139,  ...,  0.0241, -0.0180, -0.0168],
        [-0.0114, -0.0412, -0.0089,  ...,  0.0142, -0.0054,  0.0054],
        [-0.0008,  0.0086, -0.0420,  ...,  0.0217, -0.0041,  0.0125],
        ...,
        [-0.0096, -0.0170, -0.0156,  ..., -0.0496, -0.0048, -0.0229],
        [-0.0049,  0.0145,  0.0098,  ..., -0.0125, -0.0698,  0.0230],
        [-0.0183, -0.0103,  0.0070,  ...,  0.0078,  0.0115, -0.0517]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6191,  1.5273, -3.0312,  ...,  1.3174,  2.4023,  3.5078]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 05:17:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is publishing, it publishes
When something is continuing, it continues
When something is referring, it refers
When something is becoming, it becomes
When something is receiving, it receives
When something is appearing, it appears
When something is performing, it performs
When something is happening, it
2024-07-24 05:17:40 root INFO     [order_1_approx] starting weight calculation for When something is receiving, it receives
When something is becoming, it becomes
When something is continuing, it continues
When something is publishing, it publishes
When something is appearing, it appears
When something is happening, it happens
When something is referring, it refers
When something is performing, it
2024-07-24 05:17:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 05:21:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0029, -0.0300,  0.1985,  ..., -0.3835,  1.0752, -0.5176],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.0684,  0.6807,  1.6445,  ...,  3.7754, -0.5845,  2.0898],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0035, -0.0261,  0.0013,  ...,  0.0032, -0.0140, -0.0169],
        [ 0.0081, -0.0075, -0.0050,  ..., -0.0011, -0.0071, -0.0005],
        [ 0.0051,  0.0018, -0.0021,  ...,  0.0273,  0.0025, -0.0026],
        ...,
        [-0.0204, -0.0107, -0.0092,  ..., -0.0169,  0.0131, -0.0007],
        [-0.0007,  0.0153, -0.0033,  ..., -0.0004, -0.0421,  0.0116],
        [-0.0070,  0.0029,  0.0001,  ..., -0.0064, -0.0085, -0.0082]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1816,  0.8477,  2.0137,  ...,  4.6641, -1.1934,  2.1348]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 05:21:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is receiving, it receives
When something is becoming, it becomes
When something is continuing, it continues
When something is publishing, it publishes
When something is appearing, it appears
When something is happening, it happens
When something is referring, it refers
When something is performing, it
2024-07-24 05:21:23 root INFO     [order_1_approx] starting weight calculation for When something is performing, it performs
When something is receiving, it receives
When something is becoming, it becomes
When something is referring, it refers
When something is appearing, it appears
When something is continuing, it continues
When something is happening, it happens
When something is publishing, it
2024-07-24 05:21:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 05:25:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8691, -1.3193,  1.6680,  ...,  1.4873,  1.0205,  0.2690],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6035e+00,  1.9531e-03, -2.1602e+00,  ...,  4.8164e+00,
        -3.2031e+00,  4.9805e-01], device='cuda:0', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0063, -0.0111,  0.0097,  ...,  0.0138, -0.0026, -0.0204],
        [-0.0010, -0.0139, -0.0030,  ...,  0.0174, -0.0144, -0.0010],
        [ 0.0059, -0.0005, -0.0101,  ...,  0.0203, -0.0233,  0.0038],
        ...,
        [-0.0071, -0.0016, -0.0154,  ..., -0.0318, -0.0127, -0.0028],
        [-0.0168,  0.0095,  0.0009,  ...,  0.0003, -0.0312,  0.0144],
        [-0.0152,  0.0002, -0.0071,  ..., -0.0040,  0.0077, -0.0254]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6816,  0.5981, -2.0840,  ...,  4.9219, -3.8633,  0.4907]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 05:25:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is performing, it performs
When something is receiving, it receives
When something is becoming, it becomes
When something is referring, it refers
When something is appearing, it appears
When something is continuing, it continues
When something is happening, it happens
When something is publishing, it
2024-07-24 05:25:07 root INFO     [order_1_approx] starting weight calculation for When something is performing, it performs
When something is appearing, it appears
When something is continuing, it continues
When something is happening, it happens
When something is publishing, it publishes
When something is referring, it refers
When something is becoming, it becomes
When something is receiving, it
2024-07-24 05:25:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 05:28:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8438, -1.3906,  0.8232,  ...,  0.3389,  0.5952,  1.1035],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([4.0000, 0.3086, 1.3682,  ..., 3.3496, 0.7686, 3.8672], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.2650e-02, -3.2310e-03, -1.0300e-02,  ..., -5.7678e-03,
         -8.7509e-03, -3.4847e-03],
        [ 9.3384e-03, -4.2038e-03,  1.0612e-02,  ..., -6.1035e-05,
          5.2643e-04,  4.3030e-03],
        [ 1.0765e-02,  3.5744e-03, -6.9504e-03,  ...,  6.3667e-03,
         -2.1988e-02,  1.4854e-02],
        ...,
        [-4.7684e-03, -1.3763e-02, -1.2100e-02,  ..., -2.1317e-02,
          1.1154e-02, -8.0872e-03],
        [ 5.5389e-03,  9.6588e-03, -1.0536e-02,  ..., -1.8539e-02,
         -2.8946e-02,  2.1652e-02],
        [-2.8992e-04, -2.3613e-03, -8.2016e-03,  ..., -1.7746e-02,
          1.6663e-02, -2.0142e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[2.6602, 0.9795, 1.6426,  ..., 3.0859, 0.5107, 3.8516]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 05:28:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is performing, it performs
When something is appearing, it appears
When something is continuing, it continues
When something is happening, it happens
When something is publishing, it publishes
When something is referring, it refers
When something is becoming, it becomes
When something is receiving, it
2024-07-24 05:28:49 root INFO     [order_1_approx] starting weight calculation for When something is continuing, it continues
When something is publishing, it publishes
When something is receiving, it receives
When something is happening, it happens
When something is appearing, it appears
When something is becoming, it becomes
When something is performing, it performs
When something is referring, it
2024-07-24 05:28:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 05:32:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8491, -0.7861,  0.2368,  ..., -0.0754,  1.0938, -0.3843],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4077,  2.2695,  1.1689,  ...,  6.0234,  1.7881,  1.4004],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0110, -0.0186,  0.0124,  ..., -0.0059, -0.0114,  0.0242],
        [-0.0053, -0.0067, -0.0085,  ..., -0.0029,  0.0034, -0.0114],
        [ 0.0107,  0.0180, -0.0264,  ...,  0.0217, -0.0101,  0.0009],
        ...,
        [-0.0031, -0.0107, -0.0010,  ..., -0.0262,  0.0069, -0.0219],
        [ 0.0044,  0.0041,  0.0245,  ..., -0.0076, -0.0443,  0.0123],
        [ 0.0007,  0.0108,  0.0100,  ...,  0.0022, -0.0071, -0.0188]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[0.1763, 3.3008, 0.6118,  ..., 5.8398, 1.7871, 1.0488]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 05:32:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is continuing, it continues
When something is publishing, it publishes
When something is receiving, it receives
When something is happening, it happens
When something is appearing, it appears
When something is becoming, it becomes
When something is performing, it performs
When something is referring, it
2024-07-24 05:32:32 root INFO     total operator prediction time: 1785.1638588905334 seconds
2024-07-24 05:32:32 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on noun - plural_reg
2024-07-24 05:32:32 root INFO     building operator noun - plural_reg
2024-07-24 05:32:32 root INFO     [order_1_approx] starting weight calculation for The plural form of customer is customers
The plural form of year is years
The plural form of member is members
The plural form of product is products
The plural form of death is deaths
The plural form of office is offices
The plural form of period is periods
The plural form of area is
2024-07-24 05:32:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 05:36:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7637,  0.1904, -0.7949,  ..., -1.0098,  0.3899,  0.3606],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5381, -1.4297,  1.4941,  ..., -1.6660,  3.3594,  3.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.9226e-02, -9.7198e-03,  1.0193e-02,  ..., -2.9182e-04,
         -1.4969e-02, -4.9362e-03],
        [ 4.7646e-03, -7.9880e-03,  1.0242e-03,  ..., -3.8757e-03,
         -2.2659e-03,  4.9438e-03],
        [ 2.1439e-03,  1.2070e-02, -1.2009e-02,  ...,  1.8368e-03,
         -3.3569e-04, -9.5291e-03],
        ...,
        [-1.9653e-02,  6.5804e-05, -3.2177e-03,  ..., -1.6586e-02,
          1.9897e-02,  1.5678e-03],
        [ 1.9073e-03, -3.7994e-03, -7.5150e-04,  ...,  8.2397e-04,
         -2.4673e-02,  2.0386e-02],
        [ 1.0696e-02, -8.7433e-03, -1.1511e-03,  ..., -1.9180e-02,
          8.4991e-03, -9.4223e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7163, -1.1221,  0.9731,  ..., -1.5645,  3.3555,  3.5254]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 05:36:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of customer is customers
The plural form of year is years
The plural form of member is members
The plural form of product is products
The plural form of death is deaths
The plural form of office is offices
The plural form of period is periods
The plural form of area is
2024-07-24 05:36:15 root INFO     [order_1_approx] starting weight calculation for The plural form of member is members
The plural form of year is years
The plural form of period is periods
The plural form of death is deaths
The plural form of office is offices
The plural form of area is areas
The plural form of product is products
The plural form of customer is
2024-07-24 05:36:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 05:39:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6006, -0.2998,  0.4639,  ...,  2.0977,  0.1881,  0.7935],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.9678,  1.8477,  1.9287,  ..., -3.6309, -1.5254, -0.3408],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0058, -0.0238,  0.0159,  ...,  0.0013, -0.0096, -0.0088],
        [-0.0078, -0.0300,  0.0024,  ..., -0.0017,  0.0020,  0.0116],
        [ 0.0062, -0.0124, -0.0134,  ...,  0.0052,  0.0150,  0.0029],
        ...,
        [-0.0191, -0.0013, -0.0022,  ..., -0.0110,  0.0075,  0.0016],
        [-0.0080, -0.0002,  0.0153,  ..., -0.0143, -0.0162, -0.0020],
        [-0.0250, -0.0054, -0.0099,  ...,  0.0022,  0.0159, -0.0164]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.7441,  2.3320,  2.5918,  ..., -4.3047, -1.9844,  0.1045]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 05:39:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of member is members
The plural form of year is years
The plural form of period is periods
The plural form of death is deaths
The plural form of office is offices
The plural form of area is areas
The plural form of product is products
The plural form of customer is
2024-07-24 05:39:58 root INFO     [order_1_approx] starting weight calculation for The plural form of product is products
The plural form of customer is customers
The plural form of area is areas
The plural form of office is offices
The plural form of year is years
The plural form of period is periods
The plural form of member is members
The plural form of death is
2024-07-24 05:39:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 05:43:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2090, -0.0164,  0.0416,  ...,  1.0576, -0.2202,  0.0895],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.5938, -2.5078, -1.4453,  ..., -4.4922, -1.2031,  0.0859],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0271,  0.0009,  0.0352,  ...,  0.0039, -0.0082, -0.0087],
        [-0.0282, -0.0257, -0.0113,  ..., -0.0149,  0.0158,  0.0087],
        [ 0.0177,  0.0373, -0.0292,  ...,  0.0004,  0.0212, -0.0055],
        ...,
        [-0.0269, -0.0239, -0.0155,  ..., -0.0105, -0.0046, -0.0086],
        [-0.0105, -0.0073,  0.0025,  ...,  0.0117, -0.0368,  0.0234],
        [-0.0081,  0.0079,  0.0103,  ...,  0.0008,  0.0011, -0.0271]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.1211, -1.3789, -2.7188,  ..., -4.9219, -2.3320, -0.8184]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 05:43:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of product is products
The plural form of customer is customers
The plural form of area is areas
The plural form of office is offices
The plural form of year is years
The plural form of period is periods
The plural form of member is members
The plural form of death is
2024-07-24 05:43:41 root INFO     [order_1_approx] starting weight calculation for The plural form of year is years
The plural form of death is deaths
The plural form of period is periods
The plural form of area is areas
The plural form of office is offices
The plural form of customer is customers
The plural form of product is products
The plural form of member is
2024-07-24 05:43:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 05:47:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.3633,  1.0117, -0.4458,  ...,  0.4207,  0.1519,  1.0801],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([0.9092, 2.0859, 1.0137,  ..., 1.5039, 0.2646, 1.5020], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.4519e-02, -7.1526e-03,  2.2598e-02,  ...,  6.5384e-03,
         -3.6240e-04, -1.9730e-02],
        [ 6.3362e-03, -2.4536e-02, -1.5419e-02,  ..., -3.1776e-03,
         -3.5706e-03,  1.6327e-03],
        [ 4.0817e-04,  8.5983e-03, -8.1635e-03,  ...,  5.7678e-03,
          8.6594e-03, -8.7128e-03],
        ...,
        [ 5.7602e-03, -7.2289e-03, -1.0254e-02,  ..., -1.9760e-02,
         -3.8147e-05, -1.7080e-03],
        [-4.1733e-03, -4.5433e-03,  7.1487e-03,  ..., -7.1564e-03,
         -1.9516e-02,  1.7967e-03],
        [-2.6646e-03,  4.8065e-04,  6.3610e-04,  ..., -1.4420e-02,
          6.8169e-03, -8.5144e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[0.2173, 1.9229, 0.9180,  ..., 1.4775, 0.6191, 2.0781]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 05:47:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of year is years
The plural form of death is deaths
The plural form of period is periods
The plural form of area is areas
The plural form of office is offices
The plural form of customer is customers
The plural form of product is products
The plural form of member is
2024-07-24 05:47:20 root INFO     [order_1_approx] starting weight calculation for The plural form of area is areas
The plural form of member is members
The plural form of period is periods
The plural form of product is products
The plural form of customer is customers
The plural form of year is years
The plural form of death is deaths
The plural form of office is
2024-07-24 05:47:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 05:51:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.5625, -0.3523,  0.4443,  ..., -0.4319,  0.1901, -0.0608],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2156,  0.6011, -2.3965,  ...,  1.4141,  0.2959,  3.0547],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0010, -0.0390,  0.0170,  ..., -0.0122,  0.0067, -0.0181],
        [-0.0022, -0.0099, -0.0122,  ..., -0.0118, -0.0026,  0.0078],
        [ 0.0288, -0.0017, -0.0127,  ...,  0.0129, -0.0041,  0.0053],
        ...,
        [ 0.0012,  0.0135, -0.0055,  ..., -0.0058,  0.0252, -0.0096],
        [ 0.0002, -0.0130, -0.0004,  ..., -0.0173, -0.0164,  0.0019],
        [ 0.0043,  0.0017, -0.0044,  ..., -0.0035,  0.0093,  0.0065]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6211,  0.6304, -1.7812,  ...,  0.4551,  0.7861,  3.9766]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 05:51:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of area is areas
The plural form of member is members
The plural form of period is periods
The plural form of product is products
The plural form of customer is customers
The plural form of year is years
The plural form of death is deaths
The plural form of office is
2024-07-24 05:51:03 root INFO     [order_1_approx] starting weight calculation for The plural form of office is offices
The plural form of product is products
The plural form of death is deaths
The plural form of customer is customers
The plural form of member is members
The plural form of year is years
The plural form of area is areas
The plural form of period is
2024-07-24 05:51:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 05:54:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1201,  0.2065, -0.0459,  ..., -1.0684, -0.0352, -0.0925],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1729, -0.6763, -1.6875,  ...,  4.0820,  1.1758,  3.2031],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0159, -0.0107,  0.0173,  ..., -0.0031, -0.0100, -0.0227],
        [-0.0159, -0.0155, -0.0203,  ...,  0.0049,  0.0031, -0.0016],
        [-0.0113,  0.0064, -0.0105,  ..., -0.0105,  0.0123,  0.0017],
        ...,
        [-0.0197, -0.0152,  0.0109,  ..., -0.0113,  0.0137, -0.0088],
        [ 0.0089, -0.0023,  0.0003,  ..., -0.0050, -0.0361,  0.0163],
        [-0.0182,  0.0060,  0.0177,  ...,  0.0035,  0.0137, -0.0258]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7324, -0.3933, -1.0137,  ...,  3.1484,  0.4712,  2.9844]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 05:54:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of office is offices
The plural form of product is products
The plural form of death is deaths
The plural form of customer is customers
The plural form of member is members
The plural form of year is years
The plural form of area is areas
The plural form of period is
2024-07-24 05:54:46 root INFO     [order_1_approx] starting weight calculation for The plural form of area is areas
The plural form of office is offices
The plural form of customer is customers
The plural form of member is members
The plural form of year is years
The plural form of death is deaths
The plural form of period is periods
The plural form of product is
2024-07-24 05:54:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 05:58:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0840,  0.7871,  1.1885,  ...,  0.3057,  0.0035, -0.2004],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.9688, -3.0645,  1.3301,  ...,  2.7734,  0.0884,  2.7969],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0161, -0.0032,  0.0190,  ...,  0.0001, -0.0140, -0.0234],
        [ 0.0068, -0.0213, -0.0033,  ...,  0.0066,  0.0070, -0.0127],
        [ 0.0218,  0.0067, -0.0191,  ...,  0.0009, -0.0011,  0.0085],
        ...,
        [-0.0060,  0.0009, -0.0011,  ..., -0.0191,  0.0151,  0.0074],
        [ 0.0019,  0.0014,  0.0032,  ..., -0.0016, -0.0314,  0.0062],
        [-0.0109,  0.0005, -0.0040,  ..., -0.0085,  0.0169, -0.0189]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.0918, -2.9727,  1.4883,  ...,  2.1953,  0.3081,  3.0020]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 05:58:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of area is areas
The plural form of office is offices
The plural form of customer is customers
The plural form of member is members
The plural form of year is years
The plural form of death is deaths
The plural form of period is periods
The plural form of product is
2024-07-24 05:58:27 root INFO     [order_1_approx] starting weight calculation for The plural form of area is areas
The plural form of member is members
The plural form of customer is customers
The plural form of product is products
The plural form of death is deaths
The plural form of period is periods
The plural form of office is offices
The plural form of year is
2024-07-24 05:58:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 06:02:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.2227,  0.5679, -0.7598,  ..., -0.7124, -0.5898,  1.5127],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7344, -3.9355, -0.7861,  ...,  3.3125,  4.2852,  3.4570],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.1581e-02, -1.6113e-02, -4.5776e-05,  ...,  9.0866e-03,
         -3.9825e-03,  3.9902e-03],
        [-1.4778e-02, -2.1896e-03, -4.3640e-03,  ..., -2.6703e-04,
         -5.8403e-03,  1.0315e-02],
        [-1.2894e-03, -2.5463e-03, -1.7822e-02,  ...,  1.6747e-03,
          8.9722e-03, -8.4152e-03],
        ...,
        [-2.8191e-03, -6.5079e-03, -1.6724e-02,  ..., -1.8921e-02,
          1.2085e-02,  4.5280e-03],
        [ 3.5057e-03, -6.4392e-03, -6.4850e-04,  ..., -1.7639e-02,
         -2.3346e-02,  1.4671e-02],
        [-3.5992e-03,  1.6899e-03,  1.5488e-03,  ...,  2.4185e-03,
          4.1580e-03, -2.1439e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0410, -3.2969, -0.0820,  ...,  2.9043,  4.7617,  4.2539]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 06:02:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of area is areas
The plural form of member is members
The plural form of customer is customers
The plural form of product is products
The plural form of death is deaths
The plural form of period is periods
The plural form of office is offices
The plural form of year is
2024-07-24 06:02:05 root INFO     total operator prediction time: 1773.0689446926117 seconds
2024-07-24 06:02:05 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_3pSg - Ved
2024-07-24 06:02:05 root INFO     building operator verb_3pSg - Ved
2024-07-24 06:02:05 root INFO     [order_1_approx] starting weight calculation for When he believes something, something has been believed
When he follows something, something has been followed
When he appears something, something has been appeared
When he applies something, something has been applied
When he intends something, something has been intended
When he manages something, something has been managed
When he becomes something, something has been became
When he allows something, something has been
2024-07-24 06:02:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 06:05:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2722, -0.3362,  1.2227,  ...,  0.0042,  0.5166,  0.4355],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9170, -1.8730,  0.0996,  ...,  0.6226, -0.2656, -0.3086],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0089,  0.0166, -0.0043,  ...,  0.0226, -0.0021, -0.0185],
        [-0.0039, -0.0231,  0.0047,  ..., -0.0100, -0.0116,  0.0012],
        [ 0.0083,  0.0017, -0.0287,  ...,  0.0126,  0.0108,  0.0159],
        ...,
        [-0.0268,  0.0121, -0.0009,  ..., -0.0103, -0.0115, -0.0092],
        [ 0.0000,  0.0056,  0.0041,  ..., -0.0067, -0.0245, -0.0027],
        [-0.0174, -0.0141,  0.0083,  ...,  0.0020, -0.0002, -0.0366]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.8843, -2.4180,  0.7139,  ...,  0.2764, -0.0438, -0.7480]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 06:05:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he believes something, something has been believed
When he follows something, something has been followed
When he appears something, something has been appeared
When he applies something, something has been applied
When he intends something, something has been intended
When he manages something, something has been managed
When he becomes something, something has been became
When he allows something, something has been
2024-07-24 06:05:46 root INFO     [order_1_approx] starting weight calculation for When he follows something, something has been followed
When he manages something, something has been managed
When he becomes something, something has been became
When he believes something, something has been believed
When he applies something, something has been applied
When he allows something, something has been allowed
When he intends something, something has been intended
When he appears something, something has been
2024-07-24 06:05:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 06:09:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1709,  0.3904,  1.0146,  ..., -0.0942,  0.4202,  0.1074],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5039,  0.3086,  0.7188,  ...,  0.3333, -2.9551, -3.8184],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0028,  0.0035, -0.0005,  ...,  0.0239, -0.0006, -0.0012],
        [-0.0157, -0.0063,  0.0133,  ..., -0.0003, -0.0045,  0.0075],
        [ 0.0185, -0.0027, -0.0072,  ...,  0.0023, -0.0036,  0.0048],
        ...,
        [-0.0080, -0.0030,  0.0006,  ...,  0.0017,  0.0031, -0.0051],
        [ 0.0077,  0.0035, -0.0117,  ..., -0.0168, -0.0122, -0.0116],
        [-0.0161,  0.0117, -0.0067,  ...,  0.0109, -0.0065, -0.0228]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.0332,  1.0508, -0.1470,  ..., -0.1475, -2.2051, -4.2852]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 06:09:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he follows something, something has been followed
When he manages something, something has been managed
When he becomes something, something has been became
When he believes something, something has been believed
When he applies something, something has been applied
When he allows something, something has been allowed
When he intends something, something has been intended
When he appears something, something has been
2024-07-24 06:09:28 root INFO     [order_1_approx] starting weight calculation for When he becomes something, something has been became
When he appears something, something has been appeared
When he manages something, something has been managed
When he follows something, something has been followed
When he allows something, something has been allowed
When he believes something, something has been believed
When he intends something, something has been intended
When he applies something, something has been
2024-07-24 06:09:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 06:13:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0501,  0.4751,  1.3994,  ..., -0.4629,  0.3867,  0.6021],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.3438,  3.2930,  1.7578,  ...,  1.4023, -1.3916,  1.1992],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0157,  0.0055,  0.0031,  ...,  0.0138, -0.0112, -0.0138],
        [-0.0080, -0.0057,  0.0034,  ..., -0.0011, -0.0059,  0.0098],
        [ 0.0236,  0.0182, -0.0120,  ...,  0.0134,  0.0071, -0.0094],
        ...,
        [-0.0158, -0.0004, -0.0049,  ..., -0.0214,  0.0118, -0.0103],
        [ 0.0127, -0.0003,  0.0029,  ..., -0.0162, -0.0191, -0.0044],
        [ 0.0125,  0.0153,  0.0007,  ...,  0.0054,  0.0114, -0.0229]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6001,  3.0645,  1.8418,  ...,  1.4805, -0.9575,  0.1934]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 06:13:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he becomes something, something has been became
When he appears something, something has been appeared
When he manages something, something has been managed
When he follows something, something has been followed
When he allows something, something has been allowed
When he believes something, something has been believed
When he intends something, something has been intended
When he applies something, something has been
2024-07-24 06:13:09 root INFO     [order_1_approx] starting weight calculation for When he intends something, something has been intended
When he manages something, something has been managed
When he believes something, something has been believed
When he appears something, something has been appeared
When he follows something, something has been followed
When he allows something, something has been allowed
When he applies something, something has been applied
When he becomes something, something has been
2024-07-24 06:13:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 06:16:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1729,  0.6270,  1.2412,  ...,  0.6650,  0.5742, -0.1414],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9312,  1.2129,  0.4971,  ..., -0.7129,  1.5547, -1.6387],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0139, -0.0184,  0.0161,  ...,  0.0124,  0.0003, -0.0276],
        [-0.0083, -0.0098, -0.0020,  ..., -0.0057,  0.0016, -0.0052],
        [ 0.0051, -0.0026, -0.0187,  ...,  0.0056, -0.0146,  0.0043],
        ...,
        [-0.0057, -0.0132,  0.0051,  ..., -0.0221, -0.0080,  0.0099],
        [ 0.0129,  0.0006,  0.0008,  ..., -0.0188, -0.0093, -0.0093],
        [ 0.0020,  0.0068, -0.0010,  ...,  0.0004, -0.0015, -0.0306]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.8027,  0.6641,  1.0918,  ..., -0.2944,  2.0664, -0.5312]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 06:16:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he intends something, something has been intended
When he manages something, something has been managed
When he believes something, something has been believed
When he appears something, something has been appeared
When he follows something, something has been followed
When he allows something, something has been allowed
When he applies something, something has been applied
When he becomes something, something has been
2024-07-24 06:16:44 root INFO     [order_1_approx] starting weight calculation for When he follows something, something has been followed
When he appears something, something has been appeared
When he becomes something, something has been became
When he intends something, something has been intended
When he manages something, something has been managed
When he applies something, something has been applied
When he allows something, something has been allowed
When he believes something, something has been
2024-07-24 06:16:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 06:20:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5020,  0.7168,  0.0144,  ...,  0.0413,  1.2109,  0.2285],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8926,  0.3022,  0.8691,  ...,  0.5391, -4.1445,  0.3848],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0198, -0.0133,  0.0136,  ..., -0.0020,  0.0092, -0.0249],
        [-0.0071, -0.0002,  0.0061,  ..., -0.0067,  0.0025,  0.0025],
        [ 0.0165,  0.0098, -0.0026,  ...,  0.0061, -0.0004,  0.0140],
        ...,
        [-0.0258, -0.0067,  0.0064,  ..., -0.0168, -0.0058, -0.0026],
        [-0.0046,  0.0027, -0.0087,  ..., -0.0218, -0.0300, -0.0052],
        [ 0.0074,  0.0244, -0.0063,  ...,  0.0131,  0.0126, -0.0411]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7920,  0.8086,  0.5693,  ...,  1.0078, -3.9766,  0.8027]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 06:20:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he follows something, something has been followed
When he appears something, something has been appeared
When he becomes something, something has been became
When he intends something, something has been intended
When he manages something, something has been managed
When he applies something, something has been applied
When he allows something, something has been allowed
When he believes something, something has been
2024-07-24 06:20:25 root INFO     [order_1_approx] starting weight calculation for When he appears something, something has been appeared
When he manages something, something has been managed
When he becomes something, something has been became
When he applies something, something has been applied
When he allows something, something has been allowed
When he intends something, something has been intended
When he believes something, something has been believed
When he follows something, something has been
2024-07-24 06:20:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 06:24:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6538,  0.7290,  1.9629,  ..., -0.0051, -0.2910,  1.5273],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.8984, -0.0345,  1.0723,  ..., -2.9688,  1.1719,  1.0381],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.0132e-02, -1.5778e-02,  1.4915e-02,  ...,  5.7869e-03,
         -1.9958e-02, -2.1622e-02],
        [-3.4332e-04, -7.1716e-04,  5.8174e-03,  ..., -1.8101e-03,
         -2.7428e-03,  5.2910e-03],
        [-2.4796e-03,  7.1411e-03, -1.5450e-03,  ..., -8.9798e-03,
         -8.8501e-03,  1.1581e-02],
        ...,
        [-1.7914e-02, -8.5297e-03, -5.8594e-03,  ..., -5.6763e-03,
          1.0635e-02,  1.0195e-03],
        [ 3.9825e-03, -7.6027e-03,  8.1100e-03,  ..., -1.6174e-02,
         -2.2003e-02,  2.4033e-03],
        [-5.5313e-05,  1.8127e-02,  4.0512e-03,  ...,  8.6784e-04,
          8.5907e-03, -2.9968e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.4102, -0.1757,  1.0605,  ..., -3.2422,  1.3799,  1.3965]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 06:24:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he appears something, something has been appeared
When he manages something, something has been managed
When he becomes something, something has been became
When he applies something, something has been applied
When he allows something, something has been allowed
When he intends something, something has been intended
When he believes something, something has been believed
When he follows something, something has been
2024-07-24 06:24:08 root INFO     [order_1_approx] starting weight calculation for When he allows something, something has been allowed
When he believes something, something has been believed
When he appears something, something has been appeared
When he manages something, something has been managed
When he follows something, something has been followed
When he becomes something, something has been became
When he applies something, something has been applied
When he intends something, something has been
2024-07-24 06:24:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 06:27:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0369,  1.7617, -0.8276,  ..., -0.3203,  0.5972, -0.2112],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.5781,  1.5840, -0.7188,  ...,  1.8086, -2.4141,  1.9160],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0114, -0.0083,  0.0142,  ..., -0.0003,  0.0033, -0.0169],
        [-0.0087,  0.0020,  0.0034,  ...,  0.0002, -0.0129,  0.0084],
        [ 0.0062,  0.0150, -0.0155,  ...,  0.0111, -0.0106,  0.0074],
        ...,
        [-0.0215, -0.0149,  0.0007,  ..., -0.0291,  0.0069,  0.0042],
        [ 0.0309,  0.0063, -0.0079,  ..., -0.0137, -0.0175, -0.0059],
        [-0.0051,  0.0273,  0.0014,  ...,  0.0015,  0.0004, -0.0352]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.1855,  2.6641,  0.4346,  ...,  1.5996, -2.2500,  1.4043]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 06:27:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he allows something, something has been allowed
When he believes something, something has been believed
When he appears something, something has been appeared
When he manages something, something has been managed
When he follows something, something has been followed
When he becomes something, something has been became
When he applies something, something has been applied
When he intends something, something has been
2024-07-24 06:27:51 root INFO     [order_1_approx] starting weight calculation for When he becomes something, something has been became
When he appears something, something has been appeared
When he allows something, something has been allowed
When he follows something, something has been followed
When he applies something, something has been applied
When he believes something, something has been believed
When he intends something, something has been intended
When he manages something, something has been
2024-07-24 06:27:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 06:31:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5562,  0.6978,  0.9321,  ..., -0.2542, -0.7979, -0.2213],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0919, -0.9668, -3.8555,  ...,  0.0723,  0.7808,  0.8779],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0042, -0.0053,  0.0032,  ..., -0.0047, -0.0130, -0.0076],
        [-0.0085,  0.0055,  0.0031,  ..., -0.0028,  0.0003,  0.0078],
        [ 0.0064,  0.0032, -0.0076,  ...,  0.0087,  0.0018,  0.0059],
        ...,
        [-0.0154, -0.0106, -0.0097,  ..., -0.0154,  0.0103,  0.0007],
        [ 0.0039, -0.0042, -0.0057,  ..., -0.0190, -0.0201,  0.0052],
        [ 0.0069,  0.0180,  0.0121,  ...,  0.0070,  0.0116, -0.0204]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2465, -0.4390, -3.8105,  ..., -0.3704,  1.2188,  0.8638]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 06:31:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he becomes something, something has been became
When he appears something, something has been appeared
When he allows something, something has been allowed
When he follows something, something has been followed
When he applies something, something has been applied
When he believes something, something has been believed
When he intends something, something has been intended
When he manages something, something has been
2024-07-24 06:31:34 root INFO     total operator prediction time: 1768.6898589134216 seconds
2024-07-24 06:31:34 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on adj - superlative
2024-07-24 06:31:34 root INFO     building operator adj - superlative
2024-07-24 06:31:34 root INFO     [order_1_approx] starting weight calculation for If something is the most merry, it is merriest
If something is the most happy, it is happiest
If something is the most wealthy, it is wealthiest
If something is the most noisy, it is noisiest
If something is the most hardy, it is hardiest
If something is the most shiny, it is shiniest
If something is the most mild, it is mildest
If something is the most able, it is
2024-07-24 06:31:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 06:35:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5552, -0.7236,  0.5830,  ...,  0.6025,  0.1934, -0.5083],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.6475e-01,  1.9531e-03, -3.5078e+00,  ...,  8.6426e-01,
         9.3652e-01,  4.2285e-01], device='cuda:0', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0164, -0.0069,  0.0022,  ..., -0.0138, -0.0033, -0.0153],
        [-0.0056, -0.0105, -0.0014,  ..., -0.0070, -0.0111, -0.0140],
        [ 0.0101,  0.0253,  0.0066,  ...,  0.0082, -0.0098, -0.0031],
        ...,
        [-0.0207, -0.0041, -0.0111,  ..., -0.0246, -0.0041, -0.0129],
        [ 0.0024, -0.0049,  0.0046,  ...,  0.0054, -0.0243,  0.0042],
        [-0.0054,  0.0216,  0.0040,  ...,  0.0029, -0.0053, -0.0129]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2612,  0.3950, -2.7344,  ...,  0.8159,  0.4570,  0.6123]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 06:35:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most merry, it is merriest
If something is the most happy, it is happiest
If something is the most wealthy, it is wealthiest
If something is the most noisy, it is noisiest
If something is the most hardy, it is hardiest
If something is the most shiny, it is shiniest
If something is the most mild, it is mildest
If something is the most able, it is
2024-07-24 06:35:13 root INFO     [order_1_approx] starting weight calculation for If something is the most mild, it is mildest
If something is the most hardy, it is hardiest
If something is the most wealthy, it is wealthiest
If something is the most merry, it is merriest
If something is the most shiny, it is shiniest
If something is the most noisy, it is noisiest
If something is the most able, it is ablest
If something is the most happy, it is
2024-07-24 06:35:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 06:38:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2756,  1.4102,  0.6401,  ...,  0.6934,  1.4717,  0.4038],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5703, -1.2949, -3.6484,  ..., -1.5430,  3.0820,  5.4688],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0073, -0.0088,  0.0027,  ...,  0.0001, -0.0057, -0.0099],
        [-0.0046, -0.0136,  0.0051,  ...,  0.0016, -0.0098, -0.0095],
        [ 0.0137,  0.0123,  0.0013,  ...,  0.0029,  0.0009, -0.0084],
        ...,
        [-0.0013,  0.0063,  0.0102,  ..., -0.0175, -0.0036,  0.0065],
        [-0.0087, -0.0150, -0.0045,  ..., -0.0129, -0.0193, -0.0122],
        [-0.0204, -0.0383,  0.0064,  ...,  0.0075,  0.0166, -0.0325]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0078, -1.6855, -3.7852,  ..., -1.3506,  3.3691,  5.6562]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 06:38:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most mild, it is mildest
If something is the most hardy, it is hardiest
If something is the most wealthy, it is wealthiest
If something is the most merry, it is merriest
If something is the most shiny, it is shiniest
If something is the most noisy, it is noisiest
If something is the most able, it is ablest
If something is the most happy, it is
2024-07-24 06:38:55 root INFO     [order_1_approx] starting weight calculation for If something is the most mild, it is mildest
If something is the most noisy, it is noisiest
If something is the most happy, it is happiest
If something is the most wealthy, it is wealthiest
If something is the most able, it is ablest
If something is the most merry, it is merriest
If something is the most shiny, it is shiniest
If something is the most hardy, it is
2024-07-24 06:38:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 06:42:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.8730,  0.7090,  0.8027,  ..., -0.0422, -0.7900,  1.1309],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.1445, -3.2422, -3.1602,  ..., -1.9277,  2.8496,  9.2109],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0097, -0.0258,  0.0013,  ...,  0.0028, -0.0037, -0.0092],
        [ 0.0042, -0.0124,  0.0055,  ..., -0.0056,  0.0026, -0.0069],
        [ 0.0049,  0.0156, -0.0114,  ...,  0.0034,  0.0012,  0.0006],
        ...,
        [-0.0012,  0.0069, -0.0029,  ..., -0.0115,  0.0083, -0.0019],
        [-0.0046, -0.0085,  0.0118,  ..., -0.0134, -0.0202, -0.0025],
        [-0.0424, -0.0196,  0.0238,  ..., -0.0049,  0.0063, -0.0330]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.3984, -3.1914, -3.0410,  ..., -2.4082,  2.3027, 10.6953]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 06:42:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most mild, it is mildest
If something is the most noisy, it is noisiest
If something is the most happy, it is happiest
If something is the most wealthy, it is wealthiest
If something is the most able, it is ablest
If something is the most merry, it is merriest
If something is the most shiny, it is shiniest
If something is the most hardy, it is
2024-07-24 06:42:38 root INFO     [order_1_approx] starting weight calculation for If something is the most shiny, it is shiniest
If something is the most happy, it is happiest
If something is the most noisy, it is noisiest
If something is the most mild, it is mildest
If something is the most wealthy, it is wealthiest
If something is the most hardy, it is hardiest
If something is the most able, it is ablest
If something is the most merry, it is
2024-07-24 06:42:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 06:46:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3496,  0.9502,  0.2605,  ...,  1.7637,  1.4453,  0.9463],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.1836, -0.8672, -7.1367,  ...,  0.5132,  2.3359,  4.3789],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0066, -0.0034,  0.0108,  ..., -0.0011,  0.0018, -0.0078],
        [-0.0047, -0.0092, -0.0088,  ...,  0.0079, -0.0120, -0.0109],
        [ 0.0143,  0.0167, -0.0096,  ...,  0.0048,  0.0079,  0.0060],
        ...,
        [-0.0070, -0.0002,  0.0091,  ..., -0.0174, -0.0025, -0.0001],
        [-0.0025, -0.0040,  0.0097,  ..., -0.0049, -0.0218, -0.0027],
        [-0.0216, -0.0244,  0.0012,  ..., -0.0038,  0.0266, -0.0469]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5469, -0.5576, -6.8125,  ..., -0.1685,  2.6016,  4.8828]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 06:46:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most shiny, it is shiniest
If something is the most happy, it is happiest
If something is the most noisy, it is noisiest
If something is the most mild, it is mildest
If something is the most wealthy, it is wealthiest
If something is the most hardy, it is hardiest
If something is the most able, it is ablest
If something is the most merry, it is
2024-07-24 06:46:19 root INFO     [order_1_approx] starting weight calculation for If something is the most noisy, it is noisiest
If something is the most happy, it is happiest
If something is the most able, it is ablest
If something is the most hardy, it is hardiest
If something is the most shiny, it is shiniest
If something is the most wealthy, it is wealthiest
If something is the most merry, it is merriest
If something is the most mild, it is
2024-07-24 06:46:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 06:49:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2659, -0.3276,  1.0527,  ...,  0.2610,  0.0107,  0.7490],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8281,  0.7227, -5.5039,  ..., -0.6514, -1.2178,  5.1680],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0016, -0.0103,  0.0199,  ..., -0.0144, -0.0184,  0.0119],
        [-0.0178, -0.0144, -0.0142,  ..., -0.0201,  0.0056, -0.0212],
        [-0.0123,  0.0209, -0.0079,  ...,  0.0236, -0.0149,  0.0023],
        ...,
        [-0.0131, -0.0205,  0.0074,  ..., -0.0181, -0.0165,  0.0101],
        [ 0.0039,  0.0091,  0.0006,  ..., -0.0144, -0.0299,  0.0255],
        [-0.0114, -0.0364,  0.0129,  ..., -0.0121,  0.0090, -0.0620]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2183,  1.1260, -6.1289,  ..., -0.0117, -1.0225,  5.0156]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 06:49:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most noisy, it is noisiest
If something is the most happy, it is happiest
If something is the most able, it is ablest
If something is the most hardy, it is hardiest
If something is the most shiny, it is shiniest
If something is the most wealthy, it is wealthiest
If something is the most merry, it is merriest
If something is the most mild, it is
2024-07-24 06:49:50 root INFO     [order_1_approx] starting weight calculation for If something is the most able, it is ablest
If something is the most shiny, it is shiniest
If something is the most wealthy, it is wealthiest
If something is the most hardy, it is hardiest
If something is the most mild, it is mildest
If something is the most merry, it is merriest
If something is the most happy, it is happiest
If something is the most noisy, it is
2024-07-24 06:49:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 06:53:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.1628, 0.6001, 0.8872,  ..., 0.1450, 0.5737, 0.1012], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1893, -2.8555, -2.5586,  ...,  2.8008,  2.8711,  3.8203],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0068, -0.0202, -0.0024,  ...,  0.0145, -0.0132, -0.0081],
        [-0.0028, -0.0072, -0.0032,  ...,  0.0023, -0.0103, -0.0009],
        [-0.0052,  0.0184, -0.0115,  ...,  0.0006,  0.0128, -0.0019],
        ...,
        [-0.0057, -0.0123,  0.0013,  ..., -0.0275, -0.0061, -0.0047],
        [ 0.0022, -0.0016,  0.0114,  ..., -0.0056, -0.0348,  0.0109],
        [-0.0117, -0.0203,  0.0012,  ..., -0.0238,  0.0102, -0.0415]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5156, -2.4746, -3.3027,  ...,  2.8613,  2.2812,  3.8750]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 06:53:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most able, it is ablest
If something is the most shiny, it is shiniest
If something is the most wealthy, it is wealthiest
If something is the most hardy, it is hardiest
If something is the most mild, it is mildest
If something is the most merry, it is merriest
If something is the most happy, it is happiest
If something is the most noisy, it is
2024-07-24 06:53:29 root INFO     [order_1_approx] starting weight calculation for If something is the most merry, it is merriest
If something is the most noisy, it is noisiest
If something is the most hardy, it is hardiest
If something is the most able, it is ablest
If something is the most happy, it is happiest
If something is the most wealthy, it is wealthiest
If something is the most mild, it is mildest
If something is the most shiny, it is
2024-07-24 06:53:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 06:57:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1163,  0.2708,  0.7646,  ..., -0.5029,  1.5781,  0.4795],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.3535,  0.6367, -1.5830,  ...,  5.2109, -7.2031,  4.3633],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0011, -0.0105,  0.0021,  ...,  0.0160,  0.0093, -0.0110],
        [ 0.0106, -0.0022,  0.0058,  ..., -0.0063, -0.0174,  0.0015],
        [-0.0042, -0.0027, -0.0080,  ...,  0.0107,  0.0020, -0.0103],
        ...,
        [-0.0083, -0.0169, -0.0092,  ..., -0.0096, -0.0113, -0.0015],
        [ 0.0113, -0.0068,  0.0021,  ..., -0.0030, -0.0138,  0.0090],
        [-0.0173, -0.0026, -0.0040,  ...,  0.0006,  0.0113, -0.0128]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0996,  0.7290, -1.3232,  ...,  6.0000, -7.4766,  4.5430]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 06:57:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most merry, it is merriest
If something is the most noisy, it is noisiest
If something is the most hardy, it is hardiest
If something is the most able, it is ablest
If something is the most happy, it is happiest
If something is the most wealthy, it is wealthiest
If something is the most mild, it is mildest
If something is the most shiny, it is
2024-07-24 06:57:13 root INFO     [order_1_approx] starting weight calculation for If something is the most happy, it is happiest
If something is the most merry, it is merriest
If something is the most hardy, it is hardiest
If something is the most able, it is ablest
If something is the most mild, it is mildest
If something is the most shiny, it is shiniest
If something is the most noisy, it is noisiest
If something is the most wealthy, it is
2024-07-24 06:57:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 07:00:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.4880, 0.9692, 0.5571,  ..., 0.0627, 1.8340, 0.5156], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.0332, -1.5547, -0.5518,  ..., -1.8828, -2.0117,  3.3027],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0131, -0.0186,  0.0024,  ..., -0.0103, -0.0045, -0.0009],
        [ 0.0030, -0.0004, -0.0130,  ...,  0.0033, -0.0141, -0.0050],
        [ 0.0041,  0.0056, -0.0208,  ...,  0.0183, -0.0022,  0.0033],
        ...,
        [-0.0046, -0.0182, -0.0170,  ..., -0.0442, -0.0116, -0.0087],
        [ 0.0020, -0.0246, -0.0055,  ..., -0.0258, -0.0735,  0.0336],
        [-0.0276,  0.0226,  0.0055,  ...,  0.0299,  0.0331, -0.0559]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.5586, -1.3340, -0.0735,  ..., -1.8223, -2.6758,  2.5820]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 07:00:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most happy, it is happiest
If something is the most merry, it is merriest
If something is the most hardy, it is hardiest
If something is the most able, it is ablest
If something is the most mild, it is mildest
If something is the most shiny, it is shiniest
If something is the most noisy, it is noisiest
If something is the most wealthy, it is
2024-07-24 07:00:56 root INFO     total operator prediction time: 1762.1578993797302 seconds
2024-07-24 07:00:56 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+er_irreg
2024-07-24 07:00:56 root INFO     building operator verb+er_irreg
2024-07-24 07:00:56 root INFO     [order_1_approx] starting weight calculation for If you borrow something, you are a borrower
If you provide something, you are a provider
If you listen something, you are a listener
If you determine something, you are a determiner
If you tell something, you are a teller
If you offend something, you are a offender
If you promote something, you are a promoter
If you announce something, you are a
2024-07-24 07:00:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 07:04:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.4609,  0.5098,  0.8965,  ..., -0.0583, -0.6006,  1.1836],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9453,  0.8179, -4.3867,  ..., -0.6318,  7.5898,  6.2891],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0022,  0.0015,  0.0088,  ...,  0.0174, -0.0079, -0.0067],
        [-0.0161, -0.0085,  0.0089,  ...,  0.0136, -0.0073,  0.0070],
        [ 0.0184,  0.0187, -0.0022,  ...,  0.0319,  0.0159,  0.0233],
        ...,
        [-0.0048, -0.0171,  0.0031,  ..., -0.0155,  0.0090, -0.0110],
        [ 0.0111,  0.0027,  0.0053,  ..., -0.0013, -0.0332,  0.0076],
        [ 0.0171,  0.0094,  0.0158,  ...,  0.0010,  0.0013, -0.0109]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7349,  0.7827, -4.2266,  ...,  0.6016,  8.4688,  6.7617]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 07:04:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you borrow something, you are a borrower
If you provide something, you are a provider
If you listen something, you are a listener
If you determine something, you are a determiner
If you tell something, you are a teller
If you offend something, you are a offender
If you promote something, you are a promoter
If you announce something, you are a
2024-07-24 07:04:38 root INFO     [order_1_approx] starting weight calculation for If you tell something, you are a teller
If you determine something, you are a determiner
If you listen something, you are a listener
If you announce something, you are a announcer
If you provide something, you are a provider
If you offend something, you are a offender
If you promote something, you are a promoter
If you borrow something, you are a
2024-07-24 07:04:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 07:08:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1221, -1.6963,  0.4673,  ..., -0.3667, -1.1943,  1.0166],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 6.3008,  0.4175, -1.2637,  ..., -1.8184,  0.4360,  6.2539],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-3.4714e-03,  9.3002e-03,  2.2003e-02,  ...,  7.6180e-03,
          3.7003e-03, -1.7700e-02],
        [-5.4398e-03,  7.5684e-03, -1.6975e-04,  ..., -2.2564e-03,
         -5.4359e-05,  7.8049e-03],
        [-5.6610e-03,  2.0386e-02, -1.1398e-02,  ...,  1.2375e-02,
         -2.0767e-02,  1.8585e-02],
        ...,
        [ 4.7989e-03, -1.2535e-02, -4.9057e-03,  ..., -8.2779e-03,
          1.2863e-02, -1.2131e-03],
        [ 1.6708e-02, -1.0910e-02, -4.7150e-03,  ...,  7.6828e-03,
         -1.6953e-02,  4.9057e-03],
        [ 3.3646e-03, -1.0147e-02,  2.6360e-03,  ..., -9.8877e-03,
          1.7242e-02, -1.2932e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.9297,  0.8652, -1.1074,  ..., -1.8584,  0.4246,  6.3906]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 07:08:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you tell something, you are a teller
If you determine something, you are a determiner
If you listen something, you are a listener
If you announce something, you are a announcer
If you provide something, you are a provider
If you offend something, you are a offender
If you promote something, you are a promoter
If you borrow something, you are a
2024-07-24 07:08:20 root INFO     [order_1_approx] starting weight calculation for If you promote something, you are a promoter
If you offend something, you are a offender
If you tell something, you are a teller
If you announce something, you are a announcer
If you borrow something, you are a borrower
If you listen something, you are a listener
If you provide something, you are a provider
If you determine something, you are a
2024-07-24 07:08:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 07:11:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5923,  0.4277, -0.7227,  ..., -0.1132, -0.7080,  0.9644],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.8594, -2.4258, -3.5293,  ..., -7.7266,  2.4395,  3.2266],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.7624e-03, -9.2773e-03,  1.4442e-02,  ...,  2.1515e-03,
          4.7302e-04, -7.6942e-03],
        [-6.5765e-03,  1.2512e-03,  8.9188e-03,  ...,  7.8125e-03,
         -1.4496e-03,  7.3166e-03],
        [-8.9493e-03,  2.9907e-02, -4.9896e-03,  ...,  4.3793e-03,
         -1.3437e-03,  1.2505e-02],
        ...,
        [-1.2375e-02,  6.3477e-03, -7.4272e-03,  ..., -9.9640e-03,
          6.7596e-03, -3.8071e-03],
        [ 2.5578e-03,  1.0319e-03, -3.0403e-03,  ..., -1.6556e-02,
         -2.2293e-02,  7.3891e-03],
        [ 3.3379e-03, -2.8248e-03,  4.4250e-04,  ..., -6.0196e-03,
          1.0406e-02, -6.6757e-05]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.3242, -2.0137, -3.4121,  ..., -6.8125,  2.2695,  3.0117]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 07:11:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you promote something, you are a promoter
If you offend something, you are a offender
If you tell something, you are a teller
If you announce something, you are a announcer
If you borrow something, you are a borrower
If you listen something, you are a listener
If you provide something, you are a provider
If you determine something, you are a
2024-07-24 07:11:59 root INFO     [order_1_approx] starting weight calculation for If you promote something, you are a promoter
If you announce something, you are a announcer
If you determine something, you are a determiner
If you offend something, you are a offender
If you borrow something, you are a borrower
If you tell something, you are a teller
If you provide something, you are a provider
If you listen something, you are a
2024-07-24 07:11:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 07:15:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8169, -0.1191,  1.2295,  ..., -0.5923,  0.0259,  0.8555],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.1543,  3.6582, -2.4980,  ..., -5.1875,  6.0234,  2.9199],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 9.4452e-03, -1.7281e-03,  6.0043e-03,  ...,  2.0161e-03,
         -1.3481e-02, -1.4465e-02],
        [ 2.7390e-03, -1.2756e-02,  7.7820e-03,  ..., -3.8147e-06,
          4.9438e-03, -1.7761e-02],
        [-1.0544e-02,  7.4959e-03,  1.6174e-02,  ...,  9.9945e-04,
         -6.8054e-03,  6.5918e-03],
        ...,
        [-2.1820e-02, -8.8348e-03, -8.9111e-03,  ..., -1.1047e-02,
         -5.3635e-03, -1.8120e-05],
        [ 4.3259e-03, -7.1182e-03,  5.5580e-03,  ...,  5.8365e-03,
         -3.3783e-02,  2.1973e-03],
        [ 9.3842e-03,  1.4420e-02, -4.7913e-03,  ..., -1.8646e-02,
         -9.3918e-03,  1.3649e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5195,  4.1016, -2.7969,  ..., -4.7969,  6.3867,  2.4121]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 07:15:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you promote something, you are a promoter
If you announce something, you are a announcer
If you determine something, you are a determiner
If you offend something, you are a offender
If you borrow something, you are a borrower
If you tell something, you are a teller
If you provide something, you are a provider
If you listen something, you are a
2024-07-24 07:15:41 root INFO     [order_1_approx] starting weight calculation for If you borrow something, you are a borrower
If you determine something, you are a determiner
If you listen something, you are a listener
If you tell something, you are a teller
If you provide something, you are a provider
If you promote something, you are a promoter
If you announce something, you are a announcer
If you offend something, you are a
2024-07-24 07:15:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 07:19:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7939, -0.5254,  0.6646,  ..., -0.2837,  0.4678,  0.6846],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1672,  2.2793, -1.7363,  ..., -0.2275,  4.4453,  0.5635],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0042,  0.0120,  0.0115,  ...,  0.0095,  0.0008,  0.0102],
        [-0.0037, -0.0098, -0.0023,  ...,  0.0061, -0.0106,  0.0076],
        [-0.0075,  0.0177, -0.0024,  ...,  0.0205, -0.0018, -0.0154],
        ...,
        [-0.0103, -0.0133, -0.0022,  ..., -0.0138,  0.0087, -0.0006],
        [-0.0079,  0.0042,  0.0067,  ..., -0.0045, -0.0229,  0.0095],
        [-0.0008,  0.0113, -0.0030,  ..., -0.0111,  0.0059, -0.0130]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1262,  2.1914, -2.1953,  ..., -0.3684,  4.1523,  0.8213]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 07:19:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you borrow something, you are a borrower
If you determine something, you are a determiner
If you listen something, you are a listener
If you tell something, you are a teller
If you provide something, you are a provider
If you promote something, you are a promoter
If you announce something, you are a announcer
If you offend something, you are a
2024-07-24 07:19:25 root INFO     [order_1_approx] starting weight calculation for If you provide something, you are a provider
If you determine something, you are a determiner
If you announce something, you are a announcer
If you tell something, you are a teller
If you borrow something, you are a borrower
If you listen something, you are a listener
If you offend something, you are a offender
If you promote something, you are a
2024-07-24 07:19:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 07:23:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8584,  0.1104,  0.1455,  ..., -0.4338, -0.5586,  0.6924],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.5547, -0.1166, -3.6875,  ..., -1.9805,  1.1572,  3.0586],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0092, -0.0040,  0.0079,  ...,  0.0050, -0.0027, -0.0074],
        [-0.0073, -0.0048,  0.0109,  ...,  0.0083, -0.0011, -0.0009],
        [ 0.0237,  0.0079, -0.0020,  ...,  0.0026,  0.0108,  0.0017],
        ...,
        [-0.0085, -0.0055, -0.0060,  ..., -0.0070,  0.0056,  0.0090],
        [ 0.0103,  0.0039, -0.0049,  ..., -0.0143, -0.0141,  0.0111],
        [ 0.0117, -0.0050,  0.0020,  ..., -0.0081,  0.0064,  0.0013]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.0664,  0.0577, -3.5684,  ..., -1.8340,  0.6655,  2.7734]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 07:23:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you provide something, you are a provider
If you determine something, you are a determiner
If you announce something, you are a announcer
If you tell something, you are a teller
If you borrow something, you are a borrower
If you listen something, you are a listener
If you offend something, you are a offender
If you promote something, you are a
2024-07-24 07:23:06 root INFO     [order_1_approx] starting weight calculation for If you offend something, you are a offender
If you tell something, you are a teller
If you borrow something, you are a borrower
If you listen something, you are a listener
If you promote something, you are a promoter
If you determine something, you are a determiner
If you announce something, you are a announcer
If you provide something, you are a
2024-07-24 07:23:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 07:26:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0293,  1.0576, -0.5063,  ..., -0.4329,  0.0762,  0.5986],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4668,  2.3594,  0.9458,  ..., -6.9219, -0.2244,  1.4619],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0053,  0.0052,  0.0195,  ...,  0.0125, -0.0033, -0.0119],
        [ 0.0013,  0.0005,  0.0082,  ..., -0.0021, -0.0016, -0.0054],
        [-0.0143,  0.0066, -0.0052,  ...,  0.0093, -0.0011, -0.0119],
        ...,
        [-0.0011,  0.0116, -0.0198,  ..., -0.0065,  0.0051,  0.0067],
        [ 0.0183,  0.0213, -0.0160,  ..., -0.0074, -0.0135,  0.0136],
        [ 0.0056,  0.0042,  0.0049,  ..., -0.0166,  0.0017, -0.0167]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7148,  2.8320,  1.1475,  ..., -6.6094, -0.3647,  0.8047]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 07:26:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you offend something, you are a offender
If you tell something, you are a teller
If you borrow something, you are a borrower
If you listen something, you are a listener
If you promote something, you are a promoter
If you determine something, you are a determiner
If you announce something, you are a announcer
If you provide something, you are a
2024-07-24 07:26:37 root INFO     [order_1_approx] starting weight calculation for If you determine something, you are a determiner
If you offend something, you are a offender
If you borrow something, you are a borrower
If you provide something, you are a provider
If you listen something, you are a listener
If you promote something, you are a promoter
If you announce something, you are a announcer
If you tell something, you are a
2024-07-24 07:26:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 07:30:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0762,  0.4492,  0.7852,  ...,  0.2485, -0.3047,  0.0085],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7402,  3.2480, -2.2070,  ..., -2.6445,  1.3477,  1.8965],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0128, -0.0028,  0.0155,  ..., -0.0091, -0.0085, -0.0041],
        [-0.0095, -0.0071,  0.0337,  ...,  0.0037,  0.0186, -0.0070],
        [-0.0016,  0.0036, -0.0112,  ...,  0.0036,  0.0088,  0.0068],
        ...,
        [-0.0119, -0.0131, -0.0057,  ..., -0.0201,  0.0281,  0.0069],
        [-0.0091,  0.0075, -0.0145,  ..., -0.0072, -0.0236,  0.0046],
        [ 0.0046,  0.0240,  0.0020,  ..., -0.0248,  0.0129, -0.0029]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.0293,  3.4141, -2.6973,  ..., -2.2129,  1.4287,  1.8086]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 07:30:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you determine something, you are a determiner
If you offend something, you are a offender
If you borrow something, you are a borrower
If you provide something, you are a provider
If you listen something, you are a listener
If you promote something, you are a promoter
If you announce something, you are a announcer
If you tell something, you are a
2024-07-24 07:30:05 root INFO     total operator prediction time: 1748.7935876846313 seconds
2024-07-24 07:30:05 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on over+adj_reg
2024-07-24 07:30:05 root INFO     building operator over+adj_reg
2024-07-24 07:30:05 root INFO     [order_1_approx] starting weight calculation for If something is too turned, it is overturned
If something is too enthusiastic, it is overenthusiastic
If something is too stressed, it is overstressed
If something is too represented, it is overrepresented
If something is too spent, it is overspent
If something is too heated, it is overheated
If something is too optimistic, it is overoptimistic
If something is too confident, it is
2024-07-24 07:30:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 07:33:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4780, -0.0513, -0.8071,  ..., -0.4402,  0.7998, -0.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1602, -2.4219, -3.4238,  ..., -0.8354,  1.6387,  0.3350],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0157, -0.0118,  0.0145,  ..., -0.0060,  0.0106, -0.0069],
        [ 0.0092, -0.0065, -0.0032,  ...,  0.0098,  0.0104,  0.0001],
        [-0.0066,  0.0120, -0.0118,  ...,  0.0032,  0.0086,  0.0127],
        ...,
        [-0.0103, -0.0325, -0.0206,  ..., -0.0188, -0.0041, -0.0164],
        [ 0.0094, -0.0069, -0.0137,  ..., -0.0162, -0.0279,  0.0313],
        [-0.0085, -0.0032,  0.0267,  ..., -0.0055, -0.0177, -0.0352]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4375, -3.1680, -3.3496,  ...,  0.5190,  0.8208,  0.7637]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 07:33:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too turned, it is overturned
If something is too enthusiastic, it is overenthusiastic
If something is too stressed, it is overstressed
If something is too represented, it is overrepresented
If something is too spent, it is overspent
If something is too heated, it is overheated
If something is too optimistic, it is overoptimistic
If something is too confident, it is
2024-07-24 07:33:36 root INFO     [order_1_approx] starting weight calculation for If something is too spent, it is overspent
If something is too optimistic, it is overoptimistic
If something is too confident, it is overconfident
If something is too turned, it is overturned
If something is too heated, it is overheated
If something is too stressed, it is overstressed
If something is too represented, it is overrepresented
If something is too enthusiastic, it is
2024-07-24 07:33:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 07:37:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9756,  0.1215,  0.7817,  ..., -0.1890,  1.8857,  0.2966],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.4121, -1.6777,  0.4316,  ...,  0.5400,  4.2188,  0.6655],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0016, -0.0370,  0.0142,  ...,  0.0110, -0.0148, -0.0114],
        [-0.0154, -0.0195,  0.0061,  ...,  0.0199, -0.0105,  0.0072],
        [ 0.0146, -0.0011, -0.0164,  ..., -0.0141,  0.0120,  0.0035],
        ...,
        [-0.0222, -0.0269, -0.0075,  ..., -0.0030,  0.0032,  0.0020],
        [-0.0109, -0.0078, -0.0006,  ..., -0.0037, -0.0282, -0.0160],
        [ 0.0095,  0.0032,  0.0220,  ..., -0.0095,  0.0049, -0.0212]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.4609, -1.8955,  0.3000,  ...,  1.7412,  4.5234,  0.5825]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 07:37:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too spent, it is overspent
If something is too optimistic, it is overoptimistic
If something is too confident, it is overconfident
If something is too turned, it is overturned
If something is too heated, it is overheated
If something is too stressed, it is overstressed
If something is too represented, it is overrepresented
If something is too enthusiastic, it is
2024-07-24 07:37:14 root INFO     [order_1_approx] starting weight calculation for If something is too represented, it is overrepresented
If something is too turned, it is overturned
If something is too confident, it is overconfident
If something is too enthusiastic, it is overenthusiastic
If something is too stressed, it is overstressed
If something is too spent, it is overspent
If something is too optimistic, it is overoptimistic
If something is too heated, it is
2024-07-24 07:37:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 07:40:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9526,  0.7246,  0.7021,  ..., -0.8438,  1.6562,  0.7280],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7148,  0.0695, -1.9609,  ..., -0.7212,  3.1289,  2.0918],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0100, -0.0158,  0.0054,  ..., -0.0106, -0.0237, -0.0197],
        [-0.0124,  0.0004, -0.0063,  ...,  0.0125, -0.0025, -0.0071],
        [-0.0134,  0.0187,  0.0067,  ...,  0.0132,  0.0129,  0.0033],
        ...,
        [-0.0176, -0.0060, -0.0077,  ..., -0.0063,  0.0072,  0.0118],
        [-0.0023,  0.0023,  0.0090,  ..., -0.0130, -0.0393, -0.0120],
        [-0.0032,  0.0031, -0.0025,  ..., -0.0222, -0.0090, -0.0338]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.0430,  0.4370, -2.9102,  ...,  0.0117,  2.9336,  2.8438]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 07:40:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too represented, it is overrepresented
If something is too turned, it is overturned
If something is too confident, it is overconfident
If something is too enthusiastic, it is overenthusiastic
If something is too stressed, it is overstressed
If something is too spent, it is overspent
If something is too optimistic, it is overoptimistic
If something is too heated, it is
2024-07-24 07:40:51 root INFO     [order_1_approx] starting weight calculation for If something is too represented, it is overrepresented
If something is too enthusiastic, it is overenthusiastic
If something is too confident, it is overconfident
If something is too stressed, it is overstressed
If something is too heated, it is overheated
If something is too spent, it is overspent
If something is too turned, it is overturned
If something is too optimistic, it is
2024-07-24 07:40:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 07:44:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0464, -0.2805,  0.1771,  ..., -0.8887,  2.2578,  0.4216],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.8906, -0.7104, -2.3613,  ..., -0.4287,  0.8916,  0.0840],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0013, -0.0398,  0.0119,  ...,  0.0024, -0.0090, -0.0069],
        [-0.0173, -0.0239,  0.0135,  ...,  0.0135,  0.0072,  0.0013],
        [ 0.0014,  0.0107, -0.0255,  ...,  0.0036, -0.0057,  0.0271],
        ...,
        [-0.0150, -0.0349, -0.0118,  ..., -0.0076, -0.0070, -0.0189],
        [-0.0033, -0.0081, -0.0190,  ..., -0.0250, -0.0353,  0.0186],
        [ 0.0043,  0.0144,  0.0141,  ..., -0.0143,  0.0110, -0.0421]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.7715, -0.3518, -2.6309,  ...,  0.6445,  0.3276,  0.1581]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 07:44:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too represented, it is overrepresented
If something is too enthusiastic, it is overenthusiastic
If something is too confident, it is overconfident
If something is too stressed, it is overstressed
If something is too heated, it is overheated
If something is too spent, it is overspent
If something is too turned, it is overturned
If something is too optimistic, it is
2024-07-24 07:44:30 root INFO     [order_1_approx] starting weight calculation for If something is too enthusiastic, it is overenthusiastic
If something is too stressed, it is overstressed
If something is too heated, it is overheated
If something is too turned, it is overturned
If something is too optimistic, it is overoptimistic
If something is too spent, it is overspent
If something is too confident, it is overconfident
If something is too represented, it is
2024-07-24 07:44:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 07:48:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1382,  0.3813,  0.2786,  ..., -0.8296,  1.1074, -0.0042],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6631, -1.7080, -2.7812,  ...,  1.8115,  3.3711,  0.8252],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0090, -0.0165, -0.0034,  ..., -0.0152, -0.0021, -0.0189],
        [ 0.0017, -0.0094,  0.0218,  ..., -0.0012, -0.0013, -0.0156],
        [ 0.0014,  0.0093, -0.0116,  ..., -0.0112,  0.0009, -0.0039],
        ...,
        [-0.0142, -0.0090, -0.0002,  ...,  0.0006, -0.0055, -0.0081],
        [-0.0068, -0.0109,  0.0004,  ..., -0.0228, -0.0032,  0.0028],
        [ 0.0070,  0.0044, -0.0060,  ..., -0.0071, -0.0026,  0.0029]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7148, -2.1328, -2.3184,  ...,  1.9229,  3.1152,  1.0244]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 07:48:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too enthusiastic, it is overenthusiastic
If something is too stressed, it is overstressed
If something is too heated, it is overheated
If something is too turned, it is overturned
If something is too optimistic, it is overoptimistic
If something is too spent, it is overspent
If something is too confident, it is overconfident
If something is too represented, it is
2024-07-24 07:48:08 root INFO     [order_1_approx] starting weight calculation for If something is too confident, it is overconfident
If something is too turned, it is overturned
If something is too heated, it is overheated
If something is too stressed, it is overstressed
If something is too optimistic, it is overoptimistic
If something is too enthusiastic, it is overenthusiastic
If something is too represented, it is overrepresented
If something is too spent, it is
2024-07-24 07:48:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 07:51:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6377,  0.3843,  0.4519,  ..., -0.4778,  0.5654, -0.2385],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9619,  0.2937, -0.7676,  ...,  1.2607,  1.5361, -0.6157],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0077, -0.0189,  0.0066,  ...,  0.0134, -0.0297, -0.0101],
        [-0.0032, -0.0067,  0.0070,  ...,  0.0033, -0.0054,  0.0038],
        [ 0.0092,  0.0164, -0.0054,  ...,  0.0074,  0.0134, -0.0033],
        ...,
        [-0.0143, -0.0109, -0.0135,  ..., -0.0025, -0.0045, -0.0082],
        [-0.0019, -0.0125,  0.0137,  ...,  0.0065, -0.0110,  0.0060],
        [-0.0098,  0.0175,  0.0128,  ...,  0.0006,  0.0179, -0.0205]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.6250,  1.8408, -1.1016,  ...,  0.9775,  1.7109, -0.4658]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 07:51:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too confident, it is overconfident
If something is too turned, it is overturned
If something is too heated, it is overheated
If something is too stressed, it is overstressed
If something is too optimistic, it is overoptimistic
If something is too enthusiastic, it is overenthusiastic
If something is too represented, it is overrepresented
If something is too spent, it is
2024-07-24 07:51:43 root INFO     [order_1_approx] starting weight calculation for If something is too heated, it is overheated
If something is too optimistic, it is overoptimistic
If something is too represented, it is overrepresented
If something is too spent, it is overspent
If something is too turned, it is overturned
If something is too enthusiastic, it is overenthusiastic
If something is too confident, it is overconfident
If something is too stressed, it is
2024-07-24 07:51:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 07:55:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2573,  1.4316,  1.5322,  ...,  0.3682,  1.2646, -0.1941],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7139, -0.6128, -1.7236,  ..., -0.5542,  2.9785,  1.1299],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0129, -0.0038,  0.0078,  ..., -0.0009, -0.0101, -0.0063],
        [-0.0006,  0.0051,  0.0004,  ..., -0.0038, -0.0229,  0.0050],
        [-0.0147,  0.0172,  0.0019,  ...,  0.0120, -0.0015,  0.0026],
        ...,
        [-0.0222, -0.0126, -0.0148,  ..., -0.0213, -0.0074, -0.0092],
        [-0.0276,  0.0101, -0.0112,  ...,  0.0073, -0.0217,  0.0019],
        [ 0.0073, -0.0127,  0.0011,  ..., -0.0256, -0.0217, -0.0124]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0938, -0.3337, -2.4863,  ..., -0.2573,  2.8008,  1.8340]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 07:55:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too heated, it is overheated
If something is too optimistic, it is overoptimistic
If something is too represented, it is overrepresented
If something is too spent, it is overspent
If something is too turned, it is overturned
If something is too enthusiastic, it is overenthusiastic
If something is too confident, it is overconfident
If something is too stressed, it is
2024-07-24 07:55:19 root INFO     [order_1_approx] starting weight calculation for If something is too optimistic, it is overoptimistic
If something is too spent, it is overspent
If something is too represented, it is overrepresented
If something is too confident, it is overconfident
If something is too enthusiastic, it is overenthusiastic
If something is too stressed, it is overstressed
If something is too heated, it is overheated
If something is too turned, it is
2024-07-24 07:55:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 07:59:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3721,  0.9111,  0.6499,  ...,  0.8398,  1.2480, -0.6738],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0898, -0.6611, -1.0322,  ...,  0.6211,  2.9336, -0.0962],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0081,  0.0031, -0.0015,  ...,  0.0029, -0.0157, -0.0147],
        [-0.0120, -0.0008,  0.0126,  ..., -0.0068,  0.0042,  0.0106],
        [-0.0084, -0.0020, -0.0280,  ..., -0.0233,  0.0200,  0.0219],
        ...,
        [ 0.0015, -0.0196, -0.0050,  ...,  0.0007, -0.0103, -0.0047],
        [-0.0211,  0.0065,  0.0126,  ..., -0.0008, -0.0140,  0.0086],
        [-0.0034,  0.0037,  0.0129,  ..., -0.0097, -0.0105, -0.0284]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4351, -1.1250, -0.6250,  ...,  0.9609,  4.0352,  0.2468]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 07:59:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too optimistic, it is overoptimistic
If something is too spent, it is overspent
If something is too represented, it is overrepresented
If something is too confident, it is overconfident
If something is too enthusiastic, it is overenthusiastic
If something is too stressed, it is overstressed
If something is too heated, it is overheated
If something is too turned, it is
2024-07-24 07:59:01 root INFO     total operator prediction time: 1735.9842147827148 seconds
2024-07-24 07:59:01 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on adj+ly_reg
2024-07-24 07:59:01 root INFO     building operator adj+ly_reg
2024-07-24 07:59:01 root INFO     [order_1_approx] starting weight calculation for The adjective form of regional is regionally
The adjective form of obvious is obviously
The adjective form of huge is hugely
The adjective form of strong is strongly
The adjective form of mental is mentally
The adjective form of global is globally
The adjective form of political is politically
The adjective form of clinical is
2024-07-24 07:59:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 08:02:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6104,  0.9414, -0.1121,  ...,  0.3118,  0.2939,  0.1602],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.8438, -0.9443,  1.2900,  ..., -1.2441,  0.1411,  0.6006],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0175,  0.0056,  0.0049,  ..., -0.0057,  0.0031, -0.0051],
        [-0.0056, -0.0174,  0.0166,  ...,  0.0013, -0.0073,  0.0072],
        [-0.0371, -0.0143, -0.0076,  ...,  0.0151, -0.0033,  0.0269],
        ...,
        [-0.0176, -0.0087, -0.0040,  ..., -0.0086,  0.0045, -0.0068],
        [-0.0019,  0.0152,  0.0040,  ..., -0.0180, -0.0247,  0.0089],
        [-0.0060, -0.0002,  0.0088,  ..., -0.0106,  0.0189, -0.0261]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.1582, -1.5615,  0.7861,  ..., -1.1221,  0.4133,  0.1257]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 08:02:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of regional is regionally
The adjective form of obvious is obviously
The adjective form of huge is hugely
The adjective form of strong is strongly
The adjective form of mental is mentally
The adjective form of global is globally
The adjective form of political is politically
The adjective form of clinical is
2024-07-24 08:02:43 root INFO     [order_1_approx] starting weight calculation for The adjective form of regional is regionally
The adjective form of mental is mentally
The adjective form of clinical is clinically
The adjective form of obvious is obviously
The adjective form of strong is strongly
The adjective form of huge is hugely
The adjective form of political is politically
The adjective form of global is
2024-07-24 08:02:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 08:06:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3691,  0.8994,  0.8999,  ..., -0.7139,  0.4668,  0.2307],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5469,  2.1855, -0.3223,  ...,  1.9199, -1.7051,  1.2588],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.4765e-02, -2.4605e-03,  9.8724e-03,  ..., -1.9394e-02,
          7.7438e-04, -8.4381e-03],
        [-2.8648e-03, -1.3618e-02,  1.0941e-02,  ...,  7.0000e-03,
         -2.3666e-02,  9.2163e-03],
        [-1.6968e-02,  4.5776e-05, -1.6251e-02,  ...,  2.0340e-02,
         -2.9755e-04,  1.6449e-02],
        ...,
        [-1.6907e-02,  3.0518e-03,  6.5193e-03,  ..., -1.5030e-02,
          8.1787e-03,  3.2005e-03],
        [-1.0963e-02, -9.0027e-04,  2.1477e-03,  ..., -1.2650e-02,
         -2.1805e-02, -1.0071e-02],
        [-1.5541e-02, -1.2833e-02, -9.7656e-04,  ...,  3.7537e-03,
          1.2627e-02, -3.7842e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.1445,  2.8164,  0.1851,  ...,  1.5479, -2.2910,  2.0215]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 08:06:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of regional is regionally
The adjective form of mental is mentally
The adjective form of clinical is clinically
The adjective form of obvious is obviously
The adjective form of strong is strongly
The adjective form of huge is hugely
The adjective form of political is politically
The adjective form of global is
2024-07-24 08:06:27 root INFO     [order_1_approx] starting weight calculation for The adjective form of regional is regionally
The adjective form of global is globally
The adjective form of clinical is clinically
The adjective form of obvious is obviously
The adjective form of strong is strongly
The adjective form of mental is mentally
The adjective form of political is politically
The adjective form of huge is
2024-07-24 08:06:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 08:10:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0430,  0.0876,  0.2502,  ..., -0.1108,  0.7334,  0.5547],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0957e+00,  7.0020e-01, -1.9531e-03,  ..., -1.3623e-01,
         1.4980e+00,  3.8379e+00], device='cuda:0', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0353,  0.0053,  0.0117,  ..., -0.0022, -0.0165, -0.0108],
        [ 0.0014, -0.0174, -0.0018,  ...,  0.0074, -0.0221, -0.0008],
        [-0.0018, -0.0047, -0.0179,  ...,  0.0040,  0.0088,  0.0076],
        ...,
        [-0.0126, -0.0150,  0.0145,  ..., -0.0239, -0.0035, -0.0056],
        [-0.0194,  0.0032,  0.0110,  ..., -0.0201, -0.0071, -0.0189],
        [-0.0223,  0.0025, -0.0046,  ..., -0.0034,  0.0070, -0.0336]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 9.8242e-01,  9.2773e-03, -1.1270e+00,  ...,  8.5449e-04,
          3.0820e+00,  4.5781e+00]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SubBackward0>) 

                    
2024-07-24 08:10:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of regional is regionally
The adjective form of global is globally
The adjective form of clinical is clinically
The adjective form of obvious is obviously
The adjective form of strong is strongly
The adjective form of mental is mentally
The adjective form of political is politically
The adjective form of huge is
2024-07-24 08:10:11 root INFO     [order_1_approx] starting weight calculation for The adjective form of obvious is obviously
The adjective form of global is globally
The adjective form of strong is strongly
The adjective form of regional is regionally
The adjective form of political is politically
The adjective form of clinical is clinically
The adjective form of huge is hugely
The adjective form of mental is
2024-07-24 08:10:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 08:13:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5327,  0.4709, -0.4260,  ...,  0.3677,  0.7144,  0.1187],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7383, -0.0437, -1.0918,  ...,  0.7593,  2.8984,  1.9375],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0184, -0.0102,  0.0099,  ..., -0.0186,  0.0011, -0.0035],
        [-0.0047,  0.0097,  0.0097,  ..., -0.0013, -0.0008,  0.0168],
        [-0.0061,  0.0012, -0.0163,  ...,  0.0175,  0.0005,  0.0174],
        ...,
        [-0.0377, -0.0163, -0.0139,  ..., -0.0162,  0.0009, -0.0205],
        [-0.0098, -0.0122,  0.0260,  ..., -0.0141, -0.0332, -0.0024],
        [ 0.0037,  0.0028,  0.0009,  ..., -0.0252,  0.0262, -0.0333]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4238, -0.8555, -1.9326,  ..., -0.3208,  3.3145,  2.3105]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 08:13:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of obvious is obviously
The adjective form of global is globally
The adjective form of strong is strongly
The adjective form of regional is regionally
The adjective form of political is politically
The adjective form of clinical is clinically
The adjective form of huge is hugely
The adjective form of mental is
2024-07-24 08:13:54 root INFO     [order_1_approx] starting weight calculation for The adjective form of huge is hugely
The adjective form of political is politically
The adjective form of mental is mentally
The adjective form of clinical is clinically
The adjective form of strong is strongly
The adjective form of regional is regionally
The adjective form of global is globally
The adjective form of obvious is
2024-07-24 08:13:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 08:17:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1611, -0.5234,  0.2163,  ..., -0.5293, -0.1306,  0.3481],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 7.1445,  4.5938, -3.6660,  ...,  1.1934, -1.7764,  2.8184],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0180, -0.0219,  0.0073,  ..., -0.0204, -0.0018,  0.0087],
        [-0.0156, -0.0236,  0.0059,  ...,  0.0021,  0.0016,  0.0057],
        [-0.0012, -0.0042, -0.0243,  ...,  0.0102,  0.0126, -0.0066],
        ...,
        [-0.0054, -0.0279, -0.0164,  ..., -0.0399, -0.0079, -0.0219],
        [-0.0125,  0.0106,  0.0009,  ...,  0.0094, -0.0215, -0.0041],
        [-0.0066,  0.0295, -0.0098,  ...,  0.0014,  0.0038, -0.0586]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.4922,  4.1953, -3.5781,  ...,  0.9897, -1.6006,  3.5234]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 08:17:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of huge is hugely
The adjective form of political is politically
The adjective form of mental is mentally
The adjective form of clinical is clinically
The adjective form of strong is strongly
The adjective form of regional is regionally
The adjective form of global is globally
The adjective form of obvious is
2024-07-24 08:17:28 root INFO     [order_1_approx] starting weight calculation for The adjective form of global is globally
The adjective form of huge is hugely
The adjective form of obvious is obviously
The adjective form of mental is mentally
The adjective form of clinical is clinically
The adjective form of strong is strongly
The adjective form of regional is regionally
The adjective form of political is
2024-07-24 08:17:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 08:21:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9458,  0.3257,  0.0869,  ..., -0.0616, -0.1492,  0.1755],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 6.4375, -1.8633,  1.0908,  ..., -1.4521, -2.0820,  1.8564],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0133, -0.0087,  0.0115,  ..., -0.0193, -0.0012, -0.0067],
        [-0.0070, -0.0131,  0.0109,  ...,  0.0012, -0.0018, -0.0096],
        [-0.0114, -0.0114, -0.0186,  ...,  0.0073, -0.0083,  0.0290],
        ...,
        [-0.0214, -0.0070,  0.0017,  ..., -0.0277,  0.0016, -0.0116],
        [-0.0033,  0.0040,  0.0044,  ..., -0.0130, -0.0204,  0.0189],
        [-0.0107, -0.0033, -0.0033,  ..., -0.0168,  0.0214, -0.0240]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.4297, -1.6191,  0.9849,  ..., -1.5908, -1.8281,  1.9668]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 08:21:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of global is globally
The adjective form of huge is hugely
The adjective form of obvious is obviously
The adjective form of mental is mentally
The adjective form of clinical is clinically
The adjective form of strong is strongly
The adjective form of regional is regionally
The adjective form of political is
2024-07-24 08:21:11 root INFO     [order_1_approx] starting weight calculation for The adjective form of mental is mentally
The adjective form of obvious is obviously
The adjective form of huge is hugely
The adjective form of political is politically
The adjective form of global is globally
The adjective form of strong is strongly
The adjective form of clinical is clinically
The adjective form of regional is
2024-07-24 08:21:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 08:24:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2383,  0.4570, -0.3584,  ..., -0.2920,  0.5879,  0.3811],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0715,  1.9531, -2.5781,  ...,  2.0762,  0.7959,  3.9512],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.8005e-02, -3.6125e-03,  7.8583e-03,  ..., -3.4119e-02,
          6.4354e-03, -7.2937e-03],
        [-3.4103e-03, -1.0567e-02, -4.6043e-03,  ..., -1.6451e-03,
         -3.3493e-03,  8.9417e-03],
        [-2.1011e-02, -5.9967e-03,  6.3133e-03,  ...,  1.3565e-02,
          1.2329e-02,  2.0477e-02],
        ...,
        [-3.3630e-02, -1.1871e-02, -1.7029e-02,  ..., -1.2100e-02,
          4.9591e-03, -1.2787e-02],
        [ 2.9659e-03, -5.1880e-03,  4.5853e-03,  ..., -2.3956e-02,
         -2.4384e-02,  1.3809e-03],
        [ 2.1744e-03,  7.8201e-04,  4.9591e-05,  ..., -1.0277e-02,
          7.5188e-03, -3.4424e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3535,  2.0684, -2.4180,  ...,  1.9336,  0.6870,  4.3203]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 08:24:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of mental is mentally
The adjective form of obvious is obviously
The adjective form of huge is hugely
The adjective form of political is politically
The adjective form of global is globally
The adjective form of strong is strongly
The adjective form of clinical is clinically
The adjective form of regional is
2024-07-24 08:24:54 root INFO     [order_1_approx] starting weight calculation for The adjective form of clinical is clinically
The adjective form of political is politically
The adjective form of regional is regionally
The adjective form of huge is hugely
The adjective form of global is globally
The adjective form of mental is mentally
The adjective form of obvious is obviously
The adjective form of strong is
2024-07-24 08:24:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 08:28:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0791, -0.3101,  0.7363,  ...,  0.6655,  0.6660,  0.0537],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7637, -0.3230,  1.2812,  ...,  1.9551, -1.8848, -0.1465],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0203, -0.0071,  0.0060,  ..., -0.0175, -0.0129, -0.0150],
        [ 0.0007, -0.0121,  0.0049,  ...,  0.0048, -0.0220,  0.0126],
        [-0.0090,  0.0034,  0.0032,  ...,  0.0029,  0.0121,  0.0208],
        ...,
        [-0.0187, -0.0249,  0.0056,  ..., -0.0249, -0.0184,  0.0075],
        [-0.0043,  0.0088,  0.0063,  ..., -0.0026, -0.0052, -0.0045],
        [-0.0072,  0.0129,  0.0082,  ..., -0.0183,  0.0055, -0.0267]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1963, -0.6875,  0.5342,  ...,  1.9307, -1.3965, -0.4648]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 08:28:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of clinical is clinically
The adjective form of political is politically
The adjective form of regional is regionally
The adjective form of huge is hugely
The adjective form of global is globally
The adjective form of mental is mentally
The adjective form of obvious is obviously
The adjective form of strong is
2024-07-24 08:28:37 root INFO     total operator prediction time: 1776.399926185608 seconds
2024-07-24 08:28:37 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+tion_irreg
2024-07-24 08:28:37 root INFO     building operator verb+tion_irreg
2024-07-24 08:28:37 root INFO     [order_1_approx] starting weight calculation for To organize results in organization
To prepare results in preparation
To visualize results in visualization
To privatize results in privatization
To maximize results in maximization
To imagine results in imagination
To standardize results in standardization
To determine results in
2024-07-24 08:28:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 08:32:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4307,  0.0383, -0.5732,  ..., -0.1247, -0.2622, -0.2268],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.2188, -1.1836, -2.4062,  ..., -5.1875,  1.5967,  3.9238],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0073, -0.0118,  0.0028,  ...,  0.0004,  0.0104, -0.0286],
        [ 0.0052, -0.0090,  0.0047,  ..., -0.0070, -0.0035, -0.0057],
        [-0.0181,  0.0007, -0.0237,  ...,  0.0168,  0.0167, -0.0068],
        ...,
        [-0.0175,  0.0060, -0.0177,  ..., -0.0063,  0.0017, -0.0018],
        [-0.0108,  0.0140,  0.0139,  ..., -0.0039, -0.0117, -0.0009],
        [-0.0143, -0.0054,  0.0116,  ..., -0.0083,  0.0183, -0.0142]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.5703, -1.5566, -2.5742,  ..., -4.9336,  2.9688,  4.2891]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 08:32:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To organize results in organization
To prepare results in preparation
To visualize results in visualization
To privatize results in privatization
To maximize results in maximization
To imagine results in imagination
To standardize results in standardization
To determine results in
2024-07-24 08:32:21 root INFO     [order_1_approx] starting weight calculation for To organize results in organization
To determine results in determination
To privatize results in privatization
To standardize results in standardization
To visualize results in visualization
To maximize results in maximization
To prepare results in preparation
To imagine results in
2024-07-24 08:32:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 08:36:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1411, -0.4390, -0.0460,  ..., -0.2676,  0.3545, -0.2300],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8398,  1.0605, -4.0156,  ..., -0.7485, -4.0742,  0.6338],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0050, -0.0027, -0.0049,  ..., -0.0030,  0.0044, -0.0217],
        [-0.0061, -0.0039,  0.0047,  ...,  0.0014, -0.0131, -0.0083],
        [-0.0037,  0.0203, -0.0067,  ...,  0.0128,  0.0008,  0.0019],
        ...,
        [-0.0115, -0.0056, -0.0087,  ..., -0.0011, -0.0016, -0.0011],
        [-0.0031,  0.0287, -0.0080,  ..., -0.0006, -0.0047, -0.0145],
        [-0.0158,  0.0257, -0.0086,  ..., -0.0045, -0.0098, -0.0067]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.4180,  0.9175, -4.1758,  ..., -1.4375, -3.8027,  1.1973]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 08:36:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To organize results in organization
To determine results in determination
To privatize results in privatization
To standardize results in standardization
To visualize results in visualization
To maximize results in maximization
To prepare results in preparation
To imagine results in
2024-07-24 08:36:02 root INFO     [order_1_approx] starting weight calculation for To privatize results in privatization
To determine results in determination
To imagine results in imagination
To organize results in organization
To visualize results in visualization
To prepare results in preparation
To standardize results in standardization
To maximize results in
2024-07-24 08:36:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 08:39:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0156, -0.3608,  0.1328,  ..., -0.6709, -0.3977, -0.9756],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.6914,  2.3398, -4.0430,  ...,  0.3040,  1.6875,  3.4922],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0122, -0.0388,  0.0036,  ..., -0.0007, -0.0186, -0.0220],
        [-0.0028, -0.0124,  0.0056,  ...,  0.0269, -0.0058,  0.0063],
        [-0.0064, -0.0106, -0.0048,  ...,  0.0140, -0.0015,  0.0095],
        ...,
        [-0.0170, -0.0175, -0.0075,  ...,  0.0089, -0.0017, -0.0070],
        [-0.0004,  0.0044,  0.0028,  ..., -0.0138,  0.0022,  0.0231],
        [-0.0041,  0.0042,  0.0196,  ..., -0.0038,  0.0258, -0.0104]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.0391,  2.9062, -4.8242,  ..., -0.0239,  1.3125,  3.0254]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 08:39:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To privatize results in privatization
To determine results in determination
To imagine results in imagination
To organize results in organization
To visualize results in visualization
To prepare results in preparation
To standardize results in standardization
To maximize results in
2024-07-24 08:39:46 root INFO     [order_1_approx] starting weight calculation for To determine results in determination
To imagine results in imagination
To prepare results in preparation
To privatize results in privatization
To maximize results in maximization
To standardize results in standardization
To visualize results in visualization
To organize results in
2024-07-24 08:39:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 08:43:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3237,  0.5825,  0.4368,  ..., -0.6475,  0.8774, -0.4175],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2305,  0.2324, -0.2173,  ..., -0.2239, -1.9775,  3.6113],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0032, -0.0158,  0.0020,  ...,  0.0105, -0.0056, -0.0162],
        [ 0.0031, -0.0021, -0.0040,  ...,  0.0119, -0.0011, -0.0160],
        [-0.0058, -0.0133, -0.0186,  ..., -0.0066, -0.0039, -0.0112],
        ...,
        [-0.0111, -0.0092, -0.0101,  ..., -0.0079, -0.0202, -0.0015],
        [-0.0091,  0.0034,  0.0060,  ..., -0.0067, -0.0066,  0.0049],
        [ 0.0031,  0.0169,  0.0087,  ...,  0.0079,  0.0233, -0.0080]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2383,  0.1777, -0.3398,  ...,  0.1782, -2.2891,  4.3477]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 08:43:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To determine results in determination
To imagine results in imagination
To prepare results in preparation
To privatize results in privatization
To maximize results in maximization
To standardize results in standardization
To visualize results in visualization
To organize results in
2024-07-24 08:43:29 root INFO     [order_1_approx] starting weight calculation for To visualize results in visualization
To maximize results in maximization
To standardize results in standardization
To privatize results in privatization
To organize results in organization
To determine results in determination
To imagine results in imagination
To prepare results in
2024-07-24 08:43:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 08:47:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2261,  0.5601,  0.0995,  ...,  0.1199,  0.2644, -1.4805],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9497,  1.0098, -2.7695,  ..., -1.5762, -2.2715,  3.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0105, -0.0064,  0.0004,  ..., -0.0021,  0.0012, -0.0158],
        [ 0.0063, -0.0107,  0.0003,  ...,  0.0117, -0.0010, -0.0068],
        [-0.0052,  0.0008, -0.0011,  ..., -0.0015, -0.0017, -0.0047],
        ...,
        [-0.0145, -0.0114,  0.0008,  ..., -0.0125, -0.0072, -0.0091],
        [ 0.0048,  0.0061,  0.0079,  ..., -0.0107, -0.0178,  0.0043],
        [-0.0046,  0.0002,  0.0121,  ..., -0.0070,  0.0171, -0.0078]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5625,  1.0898, -3.0723,  ..., -1.5693, -3.0664,  3.4453]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 08:47:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To visualize results in visualization
To maximize results in maximization
To standardize results in standardization
To privatize results in privatization
To organize results in organization
To determine results in determination
To imagine results in imagination
To prepare results in
2024-07-24 08:47:13 root INFO     [order_1_approx] starting weight calculation for To imagine results in imagination
To prepare results in preparation
To determine results in determination
To standardize results in standardization
To organize results in organization
To visualize results in visualization
To maximize results in maximization
To privatize results in
2024-07-24 08:47:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 08:50:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0940, -0.3740,  0.8999,  ...,  0.1805,  0.1892,  0.3362],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1836,  0.5239, -0.4697,  ...,  0.8745, -2.2891,  1.7207],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0078, -0.0132,  0.0058,  ...,  0.0109, -0.0004, -0.0114],
        [ 0.0136, -0.0025,  0.0027,  ...,  0.0177, -0.0133, -0.0014],
        [ 0.0006,  0.0022, -0.0204,  ..., -0.0010, -0.0081,  0.0018],
        ...,
        [-0.0105, -0.0145, -0.0113,  ...,  0.0018, -0.0046, -0.0059],
        [-0.0126,  0.0143,  0.0126,  ..., -0.0130, -0.0073,  0.0042],
        [-0.0012,  0.0096, -0.0010,  ..., -0.0107,  0.0061, -0.0014]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1162,  0.2144, -0.1597,  ...,  0.8892, -3.0938,  2.1328]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 08:50:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To imagine results in imagination
To prepare results in preparation
To determine results in determination
To standardize results in standardization
To organize results in organization
To visualize results in visualization
To maximize results in maximization
To privatize results in
2024-07-24 08:50:57 root INFO     [order_1_approx] starting weight calculation for To imagine results in imagination
To privatize results in privatization
To prepare results in preparation
To visualize results in visualization
To organize results in organization
To maximize results in maximization
To determine results in determination
To standardize results in
2024-07-24 08:50:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 08:54:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1289,  0.1725,  0.6357,  ..., -0.1731, -0.6040, -0.2632],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5742,  0.8813, -3.0078,  ...,  1.1133,  4.3672,  3.9902],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 5.1422e-03, -1.1108e-02,  4.6082e-03,  ...,  3.2635e-03,
         -4.5319e-03, -1.1642e-02],
        [ 1.6708e-02, -1.4206e-02, -6.4945e-04,  ...,  5.5237e-03,
         -3.6812e-04, -5.4665e-03],
        [ 2.1515e-03, -4.3335e-03, -3.0823e-03,  ..., -1.3275e-02,
         -6.3324e-04, -3.6240e-03],
        ...,
        [-1.3306e-02,  9.6893e-04, -1.3840e-02,  ..., -5.3177e-03,
          9.7847e-04,  2.2888e-05],
        [-8.4000e-03,  1.8606e-03, -7.8869e-04,  ..., -1.2693e-03,
         -9.4757e-03,  3.5439e-03],
        [-1.0712e-02,  1.0307e-02, -3.5725e-03,  ...,  3.0365e-03,
         -2.4185e-03,  4.3411e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1992,  1.0596, -2.7559,  ...,  0.6787,  4.5117,  3.9492]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 08:54:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To imagine results in imagination
To privatize results in privatization
To prepare results in preparation
To visualize results in visualization
To organize results in organization
To maximize results in maximization
To determine results in determination
To standardize results in
2024-07-24 08:54:39 root INFO     [order_1_approx] starting weight calculation for To organize results in organization
To determine results in determination
To prepare results in preparation
To privatize results in privatization
To standardize results in standardization
To maximize results in maximization
To imagine results in imagination
To visualize results in
2024-07-24 08:54:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 08:58:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6182,  0.0444,  1.4883,  ..., -0.1934,  0.5518,  0.1313],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9404,  4.1797, -0.9893,  ..., -2.6055, -7.2109, -1.6934],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.2388e-03, -1.4877e-02, -3.0518e-05,  ..., -6.6452e-03,
         -2.1629e-03, -9.5215e-03],
        [-1.0162e-02, -5.1537e-03,  3.6621e-03,  ...,  8.5220e-03,
         -1.9058e-02,  2.4414e-04],
        [-2.0386e-02,  1.5778e-02, -2.1423e-02,  ...,  1.0208e-02,
         -2.6703e-04, -6.1035e-05],
        ...,
        [-1.0406e-02, -1.6327e-02,  8.0414e-03,  ..., -1.1124e-02,
         -5.0697e-03,  2.2469e-03],
        [ 1.7090e-03,  3.6560e-02, -1.4000e-02,  ..., -7.4463e-03,
         -3.0609e-02, -9.9564e-04],
        [-8.7204e-03,  1.6907e-02, -8.8215e-04,  ..., -1.6937e-02,
          1.0582e-02, -2.5806e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0518,  4.5508, -1.3926,  ..., -2.7383, -9.0625, -1.5098]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 08:58:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To organize results in organization
To determine results in determination
To prepare results in preparation
To privatize results in privatization
To standardize results in standardization
To maximize results in maximization
To imagine results in imagination
To visualize results in
2024-07-24 08:58:22 root INFO     total operator prediction time: 1784.5588059425354 seconds
2024-07-24 08:58:22 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+able_reg
2024-07-24 08:58:22 root INFO     building operator verb+able_reg
2024-07-24 08:58:22 root INFO     [order_1_approx] starting weight calculation for If you can avoid something, that thing is avoidable
If you can expand something, that thing is expandable
If you can predict something, that thing is predictable
If you can discover something, that thing is discoverable
If you can expect something, that thing is expectable
If you can renew something, that thing is renewable
If you can understand something, that thing is understandable
If you can achieve something, that thing is
2024-07-24 08:58:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 09:02:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4082,  1.2715,  0.7236,  ..., -0.4351,  0.6016,  0.0505],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8501,  4.6016, -1.8945,  ..., -8.3438,  4.4844, -0.6318],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0202, -0.0094,  0.0009,  ...,  0.0232, -0.0013, -0.0073],
        [ 0.0125, -0.0138,  0.0102,  ..., -0.0033,  0.0014,  0.0019],
        [-0.0103,  0.0003, -0.0156,  ...,  0.0183,  0.0085, -0.0041],
        ...,
        [-0.0102, -0.0183, -0.0156,  ..., -0.0216, -0.0038,  0.0153],
        [ 0.0046, -0.0028,  0.0029,  ..., -0.0215, -0.0275,  0.0129],
        [-0.0126,  0.0077,  0.0080,  ..., -0.0086,  0.0016, -0.0262]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2749,  4.2539, -1.4473,  ..., -8.1797,  5.0742, -1.0791]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 09:02:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can avoid something, that thing is avoidable
If you can expand something, that thing is expandable
If you can predict something, that thing is predictable
If you can discover something, that thing is discoverable
If you can expect something, that thing is expectable
If you can renew something, that thing is renewable
If you can understand something, that thing is understandable
If you can achieve something, that thing is
2024-07-24 09:02:02 root INFO     [order_1_approx] starting weight calculation for If you can expand something, that thing is expandable
If you can expect something, that thing is expectable
If you can discover something, that thing is discoverable
If you can predict something, that thing is predictable
If you can understand something, that thing is understandable
If you can achieve something, that thing is achieveable
If you can renew something, that thing is renewable
If you can avoid something, that thing is
2024-07-24 09:02:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 09:05:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4702,  0.5059, -1.1230,  ..., -0.5576, -0.2717, -0.0594],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6875,  3.0625, -3.5566,  ...,  1.7979,  7.6914,  2.2930],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0065, -0.0065,  0.0028,  ...,  0.0034, -0.0072,  0.0008],
        [-0.0015, -0.0099,  0.0056,  ..., -0.0074, -0.0235, -0.0098],
        [ 0.0112,  0.0131, -0.0198,  ..., -0.0015,  0.0207,  0.0161],
        ...,
        [-0.0319, -0.0154, -0.0103,  ..., -0.0020,  0.0044,  0.0006],
        [ 0.0054, -0.0129,  0.0161,  ..., -0.0112, -0.0373, -0.0012],
        [-0.0120,  0.0098, -0.0018,  ..., -0.0233,  0.0001, -0.0372]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0938,  2.6660, -3.5410,  ...,  1.7715,  7.9648,  1.9766]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 09:05:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can expand something, that thing is expandable
If you can expect something, that thing is expectable
If you can discover something, that thing is discoverable
If you can predict something, that thing is predictable
If you can understand something, that thing is understandable
If you can achieve something, that thing is achieveable
If you can renew something, that thing is renewable
If you can avoid something, that thing is
2024-07-24 09:05:43 root INFO     [order_1_approx] starting weight calculation for If you can expand something, that thing is expandable
If you can avoid something, that thing is avoidable
If you can achieve something, that thing is achieveable
If you can understand something, that thing is understandable
If you can predict something, that thing is predictable
If you can expect something, that thing is expectable
If you can renew something, that thing is renewable
If you can discover something, that thing is
2024-07-24 09:05:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 09:09:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0293,  0.3511, -0.8809,  ..., -0.1063,  0.9390,  0.2405],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5137,  1.5283, -2.4180,  ..., -3.9414,  1.4277,  2.6914],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0163, -0.0076,  0.0068,  ...,  0.0163, -0.0103, -0.0122],
        [-0.0133, -0.0023,  0.0020,  ..., -0.0090,  0.0236,  0.0018],
        [ 0.0071,  0.0022, -0.0087,  ..., -0.0057,  0.0107, -0.0053],
        ...,
        [-0.0096, -0.0111, -0.0127,  ..., -0.0147,  0.0033, -0.0095],
        [-0.0035,  0.0067,  0.0081,  ..., -0.0109, -0.0235,  0.0106],
        [ 0.0013, -0.0159,  0.0093,  ..., -0.0195,  0.0222, -0.0401]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8018,  1.0146, -2.5625,  ..., -3.2207,  1.5527,  3.1855]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 09:09:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can expand something, that thing is expandable
If you can avoid something, that thing is avoidable
If you can achieve something, that thing is achieveable
If you can understand something, that thing is understandable
If you can predict something, that thing is predictable
If you can expect something, that thing is expectable
If you can renew something, that thing is renewable
If you can discover something, that thing is
2024-07-24 09:09:20 root INFO     [order_1_approx] starting weight calculation for If you can renew something, that thing is renewable
If you can discover something, that thing is discoverable
If you can expect something, that thing is expectable
If you can avoid something, that thing is avoidable
If you can predict something, that thing is predictable
If you can achieve something, that thing is achieveable
If you can understand something, that thing is understandable
If you can expand something, that thing is
2024-07-24 09:09:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 09:13:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0852,  1.1230,  0.0214,  ..., -0.2268,  1.1572, -0.1594],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.2832,  0.3950, -3.5234,  ..., -1.8184,  2.0703,  2.4102],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0058,  0.0004, -0.0072,  ...,  0.0168, -0.0228, -0.0039],
        [ 0.0030, -0.0222,  0.0056,  ...,  0.0062,  0.0022, -0.0029],
        [-0.0003,  0.0138, -0.0163,  ...,  0.0051,  0.0015, -0.0009],
        ...,
        [-0.0294, -0.0067, -0.0121,  ..., -0.0124, -0.0053,  0.0013],
        [-0.0184, -0.0028,  0.0046,  ..., -0.0240, -0.0258,  0.0132],
        [-0.0048,  0.0020,  0.0016,  ..., -0.0134,  0.0214, -0.0312]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7461,  0.0222, -3.7383,  ..., -1.0439,  2.2871,  1.8242]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 09:13:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can renew something, that thing is renewable
If you can discover something, that thing is discoverable
If you can expect something, that thing is expectable
If you can avoid something, that thing is avoidable
If you can predict something, that thing is predictable
If you can achieve something, that thing is achieveable
If you can understand something, that thing is understandable
If you can expand something, that thing is
2024-07-24 09:13:02 root INFO     [order_1_approx] starting weight calculation for If you can achieve something, that thing is achieveable
If you can discover something, that thing is discoverable
If you can avoid something, that thing is avoidable
If you can expand something, that thing is expandable
If you can predict something, that thing is predictable
If you can renew something, that thing is renewable
If you can understand something, that thing is understandable
If you can expect something, that thing is
2024-07-24 09:13:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 09:16:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4475,  1.7119, -0.5156,  ...,  0.2271,  2.5117,  0.7158],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7266,  3.3789, -2.1328,  ..., -2.6133,  4.6094,  2.8672],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.4297e-02, -1.2733e-02,  4.1656e-03,  ...,  4.3869e-03,
         -1.8951e-02, -1.1482e-02],
        [ 3.2883e-03, -2.3346e-02,  2.6665e-03,  ..., -2.0599e-04,
         -1.4084e-02,  2.9144e-03],
        [-3.4409e-03,  1.1223e-02, -1.3573e-02,  ...,  7.2975e-03,
          8.7357e-04, -8.3313e-03],
        ...,
        [-1.9745e-02, -2.0950e-02, -1.1505e-02,  ..., -1.6373e-02,
         -1.5182e-02, -1.3382e-02],
        [ 8.3542e-04, -3.3417e-03, -7.6294e-06,  ..., -2.4719e-02,
         -3.2257e-02,  1.3168e-02],
        [-9.5367e-05,  5.0583e-03,  6.0806e-03,  ..., -1.4168e-02,
          2.8229e-02, -1.9058e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0039,  3.2227, -2.1621,  ..., -3.1523,  5.4336,  1.9209]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 09:16:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can achieve something, that thing is achieveable
If you can discover something, that thing is discoverable
If you can avoid something, that thing is avoidable
If you can expand something, that thing is expandable
If you can predict something, that thing is predictable
If you can renew something, that thing is renewable
If you can understand something, that thing is understandable
If you can expect something, that thing is
2024-07-24 09:16:43 root INFO     [order_1_approx] starting weight calculation for If you can achieve something, that thing is achieveable
If you can expect something, that thing is expectable
If you can discover something, that thing is discoverable
If you can renew something, that thing is renewable
If you can understand something, that thing is understandable
If you can avoid something, that thing is avoidable
If you can expand something, that thing is expandable
If you can predict something, that thing is
2024-07-24 09:16:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 09:20:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1672,  0.8735,  0.2167,  ..., -1.0029,  1.1367,  0.2170],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0391,  0.4829, -2.3145,  ..., -3.2363,  3.7070,  1.9736],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0106,  0.0034,  0.0179,  ...,  0.0094, -0.0155, -0.0003],
        [-0.0127, -0.0286,  0.0014,  ..., -0.0035,  0.0063, -0.0051],
        [ 0.0040, -0.0001, -0.0098,  ...,  0.0090,  0.0042, -0.0052],
        ...,
        [-0.0299, -0.0225, -0.0192,  ..., -0.0199, -0.0071, -0.0225],
        [ 0.0003, -0.0099,  0.0155,  ..., -0.0154, -0.0267,  0.0077],
        [-0.0207,  0.0101, -0.0055,  ..., -0.0197,  0.0236, -0.0504]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1221,  0.2460, -2.3750,  ..., -2.9824,  4.2539,  1.3809]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 09:20:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can achieve something, that thing is achieveable
If you can expect something, that thing is expectable
If you can discover something, that thing is discoverable
If you can renew something, that thing is renewable
If you can understand something, that thing is understandable
If you can avoid something, that thing is avoidable
If you can expand something, that thing is expandable
If you can predict something, that thing is
2024-07-24 09:20:23 root INFO     [order_1_approx] starting weight calculation for If you can achieve something, that thing is achieveable
If you can predict something, that thing is predictable
If you can expand something, that thing is expandable
If you can expect something, that thing is expectable
If you can discover something, that thing is discoverable
If you can understand something, that thing is understandable
If you can avoid something, that thing is avoidable
If you can renew something, that thing is
2024-07-24 09:20:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 09:24:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2549,  0.3154,  0.1387,  ..., -0.7598,  0.4302,  0.5776],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2783,  0.7559, -4.8672,  ...,  0.5132,  1.9912,  2.2129],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0054, -0.0067,  0.0098,  ...,  0.0088, -0.0107, -0.0002],
        [ 0.0028, -0.0127, -0.0144,  ...,  0.0004,  0.0153, -0.0037],
        [ 0.0119,  0.0095, -0.0078,  ...,  0.0081,  0.0179, -0.0104],
        ...,
        [-0.0156, -0.0035, -0.0071,  ..., -0.0146,  0.0139, -0.0132],
        [-0.0057, -0.0213,  0.0106,  ..., -0.0201, -0.0234,  0.0171],
        [-0.0017,  0.0008,  0.0022,  ..., -0.0240,  0.0109, -0.0461]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5029,  0.8145, -4.6055,  ...,  1.3145,  1.9346,  1.9023]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 09:24:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can achieve something, that thing is achieveable
If you can predict something, that thing is predictable
If you can expand something, that thing is expandable
If you can expect something, that thing is expectable
If you can discover something, that thing is discoverable
If you can understand something, that thing is understandable
If you can avoid something, that thing is avoidable
If you can renew something, that thing is
2024-07-24 09:24:08 root INFO     [order_1_approx] starting weight calculation for If you can renew something, that thing is renewable
If you can avoid something, that thing is avoidable
If you can expand something, that thing is expandable
If you can discover something, that thing is discoverable
If you can expect something, that thing is expectable
If you can achieve something, that thing is achieveable
If you can predict something, that thing is predictable
If you can understand something, that thing is
2024-07-24 09:24:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 09:27:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6611,  0.0781, -0.1365,  ..., -0.7461,  1.1182,  0.0421],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4580,  3.0273, -4.0469,  ..., -2.1230,  5.0469,  1.6338],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0117, -0.0238, -0.0030,  ...,  0.0092, -0.0164, -0.0146],
        [-0.0097, -0.0098,  0.0115,  ...,  0.0014,  0.0054,  0.0011],
        [ 0.0016,  0.0048, -0.0106,  ...,  0.0107,  0.0244, -0.0180],
        ...,
        [-0.0193, -0.0133, -0.0065,  ..., -0.0141,  0.0200, -0.0201],
        [-0.0124, -0.0104,  0.0012,  ..., -0.0243, -0.0233, -0.0021],
        [ 0.0065,  0.0086, -0.0071,  ..., -0.0313,  0.0292, -0.0403]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0281,  3.2051, -4.1406,  ..., -2.5195,  5.6055,  1.4873]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 09:27:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can renew something, that thing is renewable
If you can avoid something, that thing is avoidable
If you can expand something, that thing is expandable
If you can discover something, that thing is discoverable
If you can expect something, that thing is expectable
If you can achieve something, that thing is achieveable
If you can predict something, that thing is predictable
If you can understand something, that thing is
2024-07-24 09:27:53 root INFO     total operator prediction time: 1770.782764673233 seconds
2024-07-24 09:27:53 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on un+adj_reg
2024-07-24 09:27:53 root INFO     building operator un+adj_reg
2024-07-24 09:27:53 root INFO     [order_1_approx] starting weight calculation for The opposite of expected is unexpected
The opposite of published is unpublished
The opposite of resolved is unresolved
The opposite of known is unknown
The opposite of suitable is unsuitable
The opposite of predictable is unpredictable
The opposite of realistic is unrealistic
The opposite of desirable is
2024-07-24 09:27:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 09:33:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2686,  0.4221, -0.5020,  ..., -0.7578,  1.9053, -0.7051],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.1758,  1.0352,  1.8379,  ..., -2.1367,  2.3340,  1.9619],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.8005e-03, -2.2720e-02, -4.7073e-03,  ..., -5.8136e-03,
         -1.6296e-02, -1.5579e-02],
        [-1.0262e-02, -2.0279e-02, -1.5900e-02,  ..., -2.2156e-02,
         -1.0132e-02,  6.8054e-03],
        [-3.0518e-05, -8.7967e-03, -2.3315e-02,  ..., -4.0054e-03,
         -6.7596e-03,  1.7380e-02],
        ...,
        [-8.7662e-03, -8.2550e-03, -1.0254e-02,  ..., -1.9653e-02,
         -1.1063e-03,  3.7193e-03],
        [-2.9190e-02, -2.0599e-02,  1.3824e-02,  ...,  1.4458e-03,
         -1.8341e-02, -9.1019e-03],
        [-2.0966e-02,  2.5177e-04,  5.5809e-03,  ...,  1.0338e-02,
          2.8961e-02, -2.0538e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.6250,  0.8198,  1.2959,  ..., -1.0654,  1.1162,  2.2871]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 09:33:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of expected is unexpected
The opposite of published is unpublished
The opposite of resolved is unresolved
The opposite of known is unknown
The opposite of suitable is unsuitable
The opposite of predictable is unpredictable
The opposite of realistic is unrealistic
The opposite of desirable is
2024-07-24 09:33:08 root INFO     [order_1_approx] starting weight calculation for The opposite of suitable is unsuitable
The opposite of published is unpublished
The opposite of desirable is undesirable
The opposite of realistic is unrealistic
The opposite of resolved is unresolved
The opposite of known is unknown
The opposite of predictable is unpredictable
The opposite of expected is
2024-07-24 09:33:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 09:38:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7920,  0.6152,  0.4233,  ...,  0.1591,  2.4121, -0.3594],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0039, -1.2461,  0.3184,  ...,  2.2324,  2.6172,  2.8398],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0004, -0.0290,  0.0154,  ..., -0.0142, -0.0163, -0.0112],
        [-0.0103,  0.0105, -0.0142,  ..., -0.0084, -0.0150, -0.0176],
        [ 0.0048, -0.0167, -0.0219,  ..., -0.0145, -0.0056,  0.0049],
        ...,
        [-0.0296, -0.0190, -0.0096,  ...,  0.0065, -0.0033, -0.0087],
        [ 0.0099, -0.0061,  0.0224,  ...,  0.0066, -0.0008, -0.0023],
        [-0.0068, -0.0063, -0.0163,  ...,  0.0091,  0.0110, -0.0075]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9966, -1.5400,  0.5640,  ...,  1.8018,  3.2207,  2.4375]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 09:38:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of suitable is unsuitable
The opposite of published is unpublished
The opposite of desirable is undesirable
The opposite of realistic is unrealistic
The opposite of resolved is unresolved
The opposite of known is unknown
The opposite of predictable is unpredictable
The opposite of expected is
2024-07-24 09:38:33 root INFO     [order_1_approx] starting weight calculation for The opposite of predictable is unpredictable
The opposite of desirable is undesirable
The opposite of published is unpublished
The opposite of suitable is unsuitable
The opposite of resolved is unresolved
The opposite of expected is unexpected
The opposite of realistic is unrealistic
The opposite of known is
2024-07-24 09:38:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 09:43:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.7930, 1.3984, 1.2646,  ..., 0.4246, 1.3193, 0.2356], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8735, -2.2734,  3.2500,  ...,  0.6987,  1.7236,  0.8926],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0113, -0.0364,  0.0015,  ..., -0.0008, -0.0024, -0.0143],
        [ 0.0070, -0.0064, -0.0256,  ...,  0.0165, -0.0211,  0.0153],
        [-0.0228, -0.0095, -0.0350,  ...,  0.0050, -0.0036,  0.0113],
        ...,
        [-0.0066, -0.0358,  0.0085,  ..., -0.0147,  0.0092, -0.0032],
        [-0.0097,  0.0011,  0.0061,  ..., -0.0005, -0.0155, -0.0025],
        [-0.0071, -0.0055, -0.0083,  ..., -0.0124,  0.0196, -0.0186]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1362, -1.9639,  2.8359,  ...,  0.7529,  2.4961,  1.7119]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 09:43:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of predictable is unpredictable
The opposite of desirable is undesirable
The opposite of published is unpublished
The opposite of suitable is unsuitable
The opposite of resolved is unresolved
The opposite of expected is unexpected
The opposite of realistic is unrealistic
The opposite of known is
2024-07-24 09:43:56 root INFO     [order_1_approx] starting weight calculation for The opposite of resolved is unresolved
The opposite of expected is unexpected
The opposite of published is unpublished
The opposite of desirable is undesirable
The opposite of suitable is unsuitable
The opposite of realistic is unrealistic
The opposite of known is unknown
The opposite of predictable is
2024-07-24 09:43:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 09:49:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9038,  0.1113,  1.0762,  ..., -0.9253,  2.3789, -0.3647],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.8867, -3.1582, -1.6279,  ...,  2.5273,  4.3320,  4.3086],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0016, -0.0239,  0.0259,  ...,  0.0152, -0.0071, -0.0119],
        [-0.0010, -0.0096, -0.0229,  ..., -0.0191, -0.0138, -0.0064],
        [-0.0053, -0.0225, -0.0090,  ..., -0.0060, -0.0073,  0.0091],
        ...,
        [-0.0247, -0.0183, -0.0202,  ..., -0.0071, -0.0016,  0.0001],
        [-0.0181, -0.0064,  0.0137,  ..., -0.0055, -0.0217, -0.0131],
        [ 0.0154,  0.0030, -0.0033,  ..., -0.0206,  0.0078, -0.0087]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7139, -3.2910, -2.0977,  ...,  2.5000,  4.0039,  4.3359]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 09:49:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of resolved is unresolved
The opposite of expected is unexpected
The opposite of published is unpublished
The opposite of desirable is undesirable
The opposite of suitable is unsuitable
The opposite of realistic is unrealistic
The opposite of known is unknown
The opposite of predictable is
2024-07-24 09:49:07 root INFO     [order_1_approx] starting weight calculation for The opposite of resolved is unresolved
The opposite of expected is unexpected
The opposite of desirable is undesirable
The opposite of predictable is unpredictable
The opposite of known is unknown
The opposite of suitable is unsuitable
The opposite of realistic is unrealistic
The opposite of published is
2024-07-24 09:49:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 09:54:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.4407, 0.6553, 1.2441,  ..., 1.4258, 1.1689, 0.6577], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0098, -4.1484,  2.2012,  ...,  1.6338,  2.8926, -2.7559],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0080, -0.0181,  0.0128,  ..., -0.0038, -0.0108, -0.0185],
        [-0.0008,  0.0012, -0.0133,  ...,  0.0143, -0.0402, -0.0219],
        [-0.0183,  0.0024, -0.0330,  ..., -0.0117, -0.0095, -0.0086],
        ...,
        [-0.0191, -0.0215,  0.0012,  ..., -0.0280, -0.0021, -0.0011],
        [-0.0154,  0.0218,  0.0039,  ..., -0.0028, -0.0038,  0.0109],
        [ 0.0117,  0.0281, -0.0033,  ..., -0.0149,  0.0204, -0.0174]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5586, -4.5195,  1.2500,  ...,  1.9180,  3.2832, -3.3320]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 09:54:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of resolved is unresolved
The opposite of expected is unexpected
The opposite of desirable is undesirable
The opposite of predictable is unpredictable
The opposite of known is unknown
The opposite of suitable is unsuitable
The opposite of realistic is unrealistic
The opposite of published is
2024-07-24 09:54:22 root INFO     [order_1_approx] starting weight calculation for The opposite of expected is unexpected
The opposite of suitable is unsuitable
The opposite of resolved is unresolved
The opposite of desirable is undesirable
The opposite of predictable is unpredictable
The opposite of known is unknown
The opposite of published is unpublished
The opposite of realistic is
2024-07-24 09:54:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 09:59:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1998, -1.0811,  0.3213,  ..., -0.1602,  2.4844,  0.1147],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5645, -3.2578, -1.9902,  ..., -2.6328,  2.2930,  2.3125],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0022, -0.0210,  0.0143,  ...,  0.0164, -0.0182, -0.0081],
        [ 0.0059, -0.0003,  0.0006,  ...,  0.0002, -0.0257, -0.0061],
        [-0.0136, -0.0188, -0.0142,  ...,  0.0067, -0.0020,  0.0148],
        ...,
        [-0.0272, -0.0204, -0.0112,  ..., -0.0122,  0.0193, -0.0053],
        [-0.0055, -0.0294,  0.0399,  ...,  0.0020, -0.0186, -0.0092],
        [-0.0010,  0.0109, -0.0118,  ..., -0.0065,  0.0342, -0.0194]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.9404, -3.5586, -2.9590,  ..., -2.3047,  2.9258,  2.1406]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 09:59:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of expected is unexpected
The opposite of suitable is unsuitable
The opposite of resolved is unresolved
The opposite of desirable is undesirable
The opposite of predictable is unpredictable
The opposite of known is unknown
The opposite of published is unpublished
The opposite of realistic is
2024-07-24 09:59:41 root INFO     [order_1_approx] starting weight calculation for The opposite of known is unknown
The opposite of suitable is unsuitable
The opposite of predictable is unpredictable
The opposite of realistic is unrealistic
The opposite of published is unpublished
The opposite of desirable is undesirable
The opposite of expected is unexpected
The opposite of resolved is
2024-07-24 09:59:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 10:05:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2695,  0.5972,  0.8379,  ...,  0.9868,  0.8965, -0.5503],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1184, -2.9824, -3.4004,  ...,  2.8516,  4.4258, -2.1367],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0172, -0.0392,  0.0026,  ...,  0.0026, -0.0226, -0.0126],
        [-0.0243, -0.0100, -0.0160,  ..., -0.0161, -0.0128, -0.0106],
        [ 0.0094, -0.0262, -0.0099,  ..., -0.0125, -0.0011, -0.0084],
        ...,
        [-0.0302, -0.0085, -0.0068,  ..., -0.0298,  0.0122,  0.0118],
        [-0.0117, -0.0142,  0.0237,  ...,  0.0091, -0.0161, -0.0287],
        [-0.0095, -0.0026,  0.0004,  ..., -0.0008, -0.0050,  0.0005]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7295, -4.0078, -3.2656,  ...,  3.5156,  4.4102, -2.4199]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 10:05:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of known is unknown
The opposite of suitable is unsuitable
The opposite of predictable is unpredictable
The opposite of realistic is unrealistic
The opposite of published is unpublished
The opposite of desirable is undesirable
The opposite of expected is unexpected
The opposite of resolved is
2024-07-24 10:05:18 root INFO     [order_1_approx] starting weight calculation for The opposite of resolved is unresolved
The opposite of predictable is unpredictable
The opposite of known is unknown
The opposite of desirable is undesirable
The opposite of realistic is unrealistic
The opposite of published is unpublished
The opposite of expected is unexpected
The opposite of suitable is
2024-07-24 10:05:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 10:10:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4844, -0.0674,  0.8677,  ...,  0.2334,  1.2139,  0.4199],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7344,  0.7837,  0.7344,  ..., -0.5703,  1.6396,  1.4932],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0137, -0.0049,  0.0014,  ..., -0.0064, -0.0298, -0.0075],
        [ 0.0086,  0.0052, -0.0243,  ..., -0.0090, -0.0285, -0.0068],
        [ 0.0050, -0.0173, -0.0260,  ..., -0.0217, -0.0222,  0.0182],
        ...,
        [-0.0132, -0.0179, -0.0292,  ..., -0.0287,  0.0195, -0.0002],
        [-0.0208, -0.0121,  0.0377,  ...,  0.0107, -0.0183, -0.0012],
        [-0.0011,  0.0251, -0.0225,  ..., -0.0299,  0.0287, -0.0215]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7920,  1.1738,  1.6533,  ...,  0.5410,  0.0918,  0.8223]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 10:10:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of resolved is unresolved
The opposite of predictable is unpredictable
The opposite of known is unknown
The opposite of desirable is undesirable
The opposite of realistic is unrealistic
The opposite of published is unpublished
The opposite of expected is unexpected
The opposite of suitable is
2024-07-24 10:10:46 root INFO     total operator prediction time: 2573.167674779892 seconds
2024-07-24 10:10:46 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on re+verb_reg
2024-07-24 10:10:46 root INFO     building operator re+verb_reg
2024-07-24 10:10:46 root INFO     [order_1_approx] starting weight calculation for To calculate again is to recalculate
To send again is to resend
To configure again is to reconfigure
To deem again is to redeem
To investigate again is to reinvestigate
To organize again is to reorganize
To engage again is to reengage
To adjust again is to
2024-07-24 10:10:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 10:16:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4995, -0.0745,  1.0762,  ...,  0.3333,  0.2151,  0.0339],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6045,  2.3926, -7.7188,  ..., -2.1328,  0.0797,  4.0391],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0332, -0.0090,  0.0137,  ..., -0.0133, -0.0160, -0.0003],
        [ 0.0040,  0.0028, -0.0013,  ...,  0.0029,  0.0117, -0.0236],
        [ 0.0596, -0.0040, -0.0055,  ...,  0.0166, -0.0239, -0.0072],
        ...,
        [-0.0461, -0.0180, -0.0033,  ..., -0.0118,  0.0064, -0.0103],
        [-0.0046, -0.0108,  0.0094,  ..., -0.0142, -0.0136, -0.0044],
        [-0.0011,  0.0176,  0.0027,  ..., -0.0048,  0.0209, -0.0223]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4277,  2.9375, -7.4102,  ..., -1.6699, -0.6592,  3.8906]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 10:16:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To calculate again is to recalculate
To send again is to resend
To configure again is to reconfigure
To deem again is to redeem
To investigate again is to reinvestigate
To organize again is to reorganize
To engage again is to reengage
To adjust again is to
2024-07-24 10:16:12 root INFO     [order_1_approx] starting weight calculation for To configure again is to reconfigure
To organize again is to reorganize
To investigate again is to reinvestigate
To adjust again is to readjust
To deem again is to redeem
To engage again is to reengage
To send again is to resend
To calculate again is to
2024-07-24 10:16:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 10:21:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0889e-01,  1.1406e+00, -2.8320e-01,  ..., -2.1362e-04,
        -5.8252e-01,  9.0820e-01], device='cuda:0', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9619,  2.3125, -3.9219,  ..., -4.9297,  0.3384, -0.6777],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0453, -0.0052,  0.0063,  ...,  0.0095, -0.0084, -0.0118],
        [-0.0156, -0.0202, -0.0029,  ..., -0.0034,  0.0153, -0.0251],
        [ 0.0399, -0.0067, -0.0152,  ...,  0.0234, -0.0086, -0.0010],
        ...,
        [-0.0032, -0.0043, -0.0146,  ..., -0.0083, -0.0113,  0.0250],
        [ 0.0121,  0.0118,  0.0163,  ...,  0.0120, -0.0193, -0.0064],
        [ 0.0162,  0.0109,  0.0169,  ...,  0.0063,  0.0045,  0.0050]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7754,  2.6738, -4.1016,  ..., -6.0156, -0.0801, -0.9185]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 10:21:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To configure again is to reconfigure
To organize again is to reorganize
To investigate again is to reinvestigate
To adjust again is to readjust
To deem again is to redeem
To engage again is to reengage
To send again is to resend
To calculate again is to
2024-07-24 10:21:31 root INFO     [order_1_approx] starting weight calculation for To adjust again is to readjust
To send again is to resend
To calculate again is to recalculate
To investigate again is to reinvestigate
To organize again is to reorganize
To engage again is to reengage
To deem again is to redeem
To configure again is to
2024-07-24 10:21:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 10:26:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6963,  1.0596,  0.1467,  ...,  0.7285, -0.6812, -0.5034],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.4082,  0.0405, -6.9102,  ..., -1.3262,  0.4980,  0.5645],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0075,  0.0008,  0.0070,  ...,  0.0008, -0.0194, -0.0211],
        [-0.0039, -0.0089, -0.0063,  ..., -0.0209,  0.0105, -0.0171],
        [ 0.0753,  0.0171, -0.0091,  ...,  0.0179, -0.0179, -0.0076],
        ...,
        [-0.0285, -0.0251, -0.0019,  ...,  0.0015, -0.0174,  0.0238],
        [-0.0006,  0.0045, -0.0078,  ...,  0.0017, -0.0325, -0.0024],
        [ 0.0005,  0.0193, -0.0158,  ..., -0.0128,  0.0205, -0.0178]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.2305,  0.5820, -8.7891,  ..., -1.9512,  0.1758,  0.2627]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 10:26:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To adjust again is to readjust
To send again is to resend
To calculate again is to recalculate
To investigate again is to reinvestigate
To organize again is to reorganize
To engage again is to reengage
To deem again is to redeem
To configure again is to
2024-07-24 10:26:49 root INFO     [order_1_approx] starting weight calculation for To adjust again is to readjust
To calculate again is to recalculate
To engage again is to reengage
To organize again is to reorganize
To configure again is to reconfigure
To investigate again is to reinvestigate
To send again is to resend
To deem again is to
2024-07-24 10:26:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 10:31:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.7324,  0.1434,  0.5664,  ..., -0.3667, -0.6104,  1.0732],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.3867, -0.0796, -2.2090,  ..., -0.7500, -2.3203,  1.2539],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0025, -0.0102,  0.0229,  ...,  0.0080, -0.0059, -0.0107],
        [-0.0226, -0.0017,  0.0155,  ..., -0.0021,  0.0090,  0.0030],
        [ 0.0105,  0.0048, -0.0243,  ..., -0.0002,  0.0053, -0.0149],
        ...,
        [-0.0295, -0.0231,  0.0154,  ...,  0.0116,  0.0110,  0.0271],
        [ 0.0225, -0.0036, -0.0132,  ..., -0.0239, -0.0185,  0.0132],
        [ 0.0114, -0.0042,  0.0127,  ...,  0.0193,  0.0119, -0.0060]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.0508,  0.8970, -1.5342,  ..., -1.4014, -2.6094,  0.6948]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 10:31:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To adjust again is to readjust
To calculate again is to recalculate
To engage again is to reengage
To organize again is to reorganize
To configure again is to reconfigure
To investigate again is to reinvestigate
To send again is to resend
To deem again is to
2024-07-24 10:31:56 root INFO     [order_1_approx] starting weight calculation for To investigate again is to reinvestigate
To calculate again is to recalculate
To organize again is to reorganize
To configure again is to reconfigure
To send again is to resend
To adjust again is to readjust
To deem again is to redeem
To engage again is to
2024-07-24 10:31:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 10:36:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5049, -0.1653,  0.9678,  ...,  1.1221, -1.5801,  0.8374],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4678, -1.1523, -2.9512,  ...,  0.5518, -1.2295,  1.3516],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.4933e-02, -8.5220e-03, -3.0689e-03,  ..., -1.2779e-02,
         -2.0084e-03, -1.9104e-02],
        [ 4.8027e-03,  1.0391e-02, -6.1035e-05,  ..., -3.8147e-04,
         -4.6387e-03,  2.9526e-03],
        [ 9.4147e-03,  6.2675e-03, -6.1512e-04,  ..., -3.9368e-03,
         -2.8820e-03,  1.0223e-03],
        ...,
        [-2.0950e-02, -1.3733e-04, -4.5013e-03,  ...,  3.6221e-03,
         -2.9564e-03, -1.7578e-02],
        [ 1.3351e-02, -9.5062e-03, -2.4414e-04,  ..., -1.9569e-03,
          7.5722e-03,  7.0190e-04],
        [ 4.9896e-03,  5.0888e-03, -3.0518e-04,  ..., -3.9177e-03,
          7.5836e-03, -4.2458e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3633,  0.2344, -2.4980,  ...,  0.5986, -3.1777,  1.3838]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 10:36:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To investigate again is to reinvestigate
To calculate again is to recalculate
To organize again is to reorganize
To configure again is to reconfigure
To send again is to resend
To adjust again is to readjust
To deem again is to redeem
To engage again is to
2024-07-24 10:36:32 root INFO     [order_1_approx] starting weight calculation for To send again is to resend
To organize again is to reorganize
To calculate again is to recalculate
To configure again is to reconfigure
To engage again is to reengage
To deem again is to redeem
To adjust again is to readjust
To investigate again is to
2024-07-24 10:36:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 10:41:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8516, -0.4800,  0.7485,  ..., -0.1256, -0.4087,  1.2051],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.5156,  1.8555, -5.1836,  ...,  0.4873, -1.4102, -2.1465],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0001, -0.0105,  0.0004,  ..., -0.0166, -0.0114, -0.0032],
        [ 0.0118, -0.0083,  0.0074,  ...,  0.0041,  0.0207, -0.0047],
        [ 0.0305,  0.0148,  0.0037,  ...,  0.0128,  0.0026,  0.0089],
        ...,
        [-0.0318, -0.0158, -0.0060,  ..., -0.0289, -0.0074,  0.0032],
        [ 0.0037, -0.0046,  0.0125,  ..., -0.0025, -0.0091, -0.0075],
        [ 0.0033,  0.0232,  0.0011,  ..., -0.0088, -0.0019,  0.0065]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3164,  2.6074, -6.5391,  ...,  0.4983, -1.0273, -2.6680]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 10:41:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To send again is to resend
To organize again is to reorganize
To calculate again is to recalculate
To configure again is to reconfigure
To engage again is to reengage
To deem again is to redeem
To adjust again is to readjust
To investigate again is to
2024-07-24 10:41:54 root INFO     [order_1_approx] starting weight calculation for To calculate again is to recalculate
To engage again is to reengage
To configure again is to reconfigure
To adjust again is to readjust
To investigate again is to reinvestigate
To deem again is to redeem
To send again is to resend
To organize again is to
2024-07-24 10:41:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 10:46:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0339,  0.2627,  0.6548,  ..., -0.5742,  0.4463,  0.3701],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1074, -1.1699, -3.8125,  ...,  2.3164, -0.8701,  0.4922],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0201, -0.0093, -0.0022,  ...,  0.0017, -0.0075, -0.0041],
        [-0.0011, -0.0035,  0.0030,  ..., -0.0132,  0.0028,  0.0051],
        [ 0.0370,  0.0020, -0.0160,  ...,  0.0246, -0.0031, -0.0090],
        ...,
        [-0.0253, -0.0084, -0.0001,  ..., -0.0150, -0.0022,  0.0048],
        [ 0.0136, -0.0056, -0.0057,  ...,  0.0057, -0.0188,  0.0038],
        [ 0.0205,  0.0083, -0.0043,  ...,  0.0046,  0.0204,  0.0072]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7041, -0.9043, -4.5430,  ...,  2.7637, -1.8105, -0.0923]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 10:46:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To calculate again is to recalculate
To engage again is to reengage
To configure again is to reconfigure
To adjust again is to readjust
To investigate again is to reinvestigate
To deem again is to redeem
To send again is to resend
To organize again is to
2024-07-24 10:46:58 root INFO     [order_1_approx] starting weight calculation for To deem again is to redeem
To configure again is to reconfigure
To engage again is to reengage
To organize again is to reorganize
To investigate again is to reinvestigate
To adjust again is to readjust
To calculate again is to recalculate
To send again is to
2024-07-24 10:46:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 10:50:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.3057, 0.0962, 0.5693,  ..., 0.2021, 0.4917, 0.5913], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5029,  2.3750, -1.2246,  ...,  2.7383, -1.2314,  2.1055],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0126,  0.0055,  0.0051,  ..., -0.0066, -0.0131, -0.0022],
        [-0.0032,  0.0038, -0.0040,  ...,  0.0184,  0.0002,  0.0128],
        [ 0.0025, -0.0031,  0.0029,  ...,  0.0112,  0.0033, -0.0025],
        ...,
        [-0.0198, -0.0114, -0.0016,  ..., -0.0256,  0.0031,  0.0061],
        [ 0.0050,  0.0165, -0.0087,  ..., -0.0110, -0.0258,  0.0075],
        [ 0.0031,  0.0093,  0.0119,  ..., -0.0073,  0.0063, -0.0071]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.2949,  3.2344, -1.8418,  ...,  2.3770, -0.9150,  1.6807]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 10:50:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To deem again is to redeem
To configure again is to reconfigure
To engage again is to reengage
To organize again is to reorganize
To investigate again is to reinvestigate
To adjust again is to readjust
To calculate again is to recalculate
To send again is to
2024-07-24 10:50:42 root INFO     total operator prediction time: 2396.2113535404205 seconds
2024-07-24 10:50:42 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on adj+ness_reg
2024-07-24 10:50:42 root INFO     building operator adj+ness_reg
2024-07-24 10:50:42 root INFO     [order_1_approx] starting weight calculation for The state of being directed is directedness
The state of being impressive is impressiveness
The state of being strange is strangeness
The state of being huge is hugeness
The state of being helpful is helpfulness
The state of being reasonable is reasonableness
The state of being same is sameness
The state of being competitive is
2024-07-24 10:50:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 10:54:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1743,  0.1284,  1.1133,  ..., -0.6172,  1.2031,  0.4290],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-5.0703, -1.0703, -3.2227,  ...,  0.6152,  3.9238,  3.5742],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0064, -0.0121,  0.0117,  ..., -0.0302, -0.0123, -0.0041],
        [-0.0082, -0.0106, -0.0069,  ..., -0.0021,  0.0004, -0.0147],
        [-0.0099,  0.0080, -0.0124,  ..., -0.0114,  0.0091,  0.0116],
        ...,
        [-0.0255, -0.0206,  0.0117,  ..., -0.0232,  0.0197, -0.0106],
        [-0.0026,  0.0012,  0.0121,  ..., -0.0050, -0.0175,  0.0151],
        [-0.0105,  0.0220,  0.0158,  ..., -0.0066,  0.0219, -0.0182]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.5703, -0.1143, -3.6445,  ...,  0.4741,  3.5879,  3.2188]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 10:54:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being directed is directedness
The state of being impressive is impressiveness
The state of being strange is strangeness
The state of being huge is hugeness
The state of being helpful is helpfulness
The state of being reasonable is reasonableness
The state of being same is sameness
The state of being competitive is
2024-07-24 10:54:26 root INFO     [order_1_approx] starting weight calculation for The state of being impressive is impressiveness
The state of being helpful is helpfulness
The state of being competitive is competitiveness
The state of being huge is hugeness
The state of being same is sameness
The state of being reasonable is reasonableness
The state of being strange is strangeness
The state of being directed is
2024-07-24 10:54:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 10:58:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.9258,  0.0237,  0.1501,  ..., -0.3298,  0.8975, -0.2888],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0703, -2.2305,  0.8770,  ..., -3.1211,  4.6953,  3.1367],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0074, -0.0028, -0.0013,  ..., -0.0168, -0.0166,  0.0034],
        [-0.0097, -0.0202,  0.0003,  ..., -0.0021, -0.0095, -0.0005],
        [ 0.0049, -0.0108, -0.0164,  ..., -0.0014, -0.0145,  0.0008],
        ...,
        [-0.0281, -0.0071,  0.0009,  ..., -0.0049,  0.0219, -0.0087],
        [-0.0069,  0.0049,  0.0008,  ..., -0.0165, -0.0165,  0.0091],
        [ 0.0015,  0.0158, -0.0010,  ..., -0.0025,  0.0214,  0.0113]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0936, -2.4141, -0.0933,  ..., -2.7539,  4.6289,  3.1699]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 10:58:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being impressive is impressiveness
The state of being helpful is helpfulness
The state of being competitive is competitiveness
The state of being huge is hugeness
The state of being same is sameness
The state of being reasonable is reasonableness
The state of being strange is strangeness
The state of being directed is
2024-07-24 10:58:09 root INFO     [order_1_approx] starting weight calculation for The state of being huge is hugeness
The state of being impressive is impressiveness
The state of being strange is strangeness
The state of being competitive is competitiveness
The state of being same is sameness
The state of being directed is directedness
The state of being reasonable is reasonableness
The state of being helpful is
2024-07-24 10:58:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 11:01:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9399,  0.2769, -0.1423,  ..., -0.2395,  1.0449,  0.8467],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.9873,  0.7598, -0.3984,  ...,  0.2812,  0.5747,  5.2578],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0207, -0.0192,  0.0097,  ..., -0.0115, -0.0095, -0.0312],
        [-0.0014, -0.0267, -0.0038,  ..., -0.0049,  0.0057,  0.0102],
        [ 0.0024, -0.0035, -0.0158,  ..., -0.0007,  0.0099,  0.0235],
        ...,
        [-0.0192, -0.0189, -0.0144,  ..., -0.0211, -0.0025, -0.0144],
        [-0.0086,  0.0239,  0.0110,  ..., -0.0061, -0.0276, -0.0140],
        [-0.0039,  0.0176,  0.0002,  ..., -0.0161, -0.0036, -0.0289]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.0977,  1.6963, -0.3394,  ...,  1.0723,  0.7534,  5.3398]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 11:01:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being huge is hugeness
The state of being impressive is impressiveness
The state of being strange is strangeness
The state of being competitive is competitiveness
The state of being same is sameness
The state of being directed is directedness
The state of being reasonable is reasonableness
The state of being helpful is
2024-07-24 11:01:48 root INFO     [order_1_approx] starting weight calculation for The state of being reasonable is reasonableness
The state of being same is sameness
The state of being directed is directedness
The state of being impressive is impressiveness
The state of being strange is strangeness
The state of being helpful is helpfulness
The state of being competitive is competitiveness
The state of being huge is
2024-07-24 11:01:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 11:05:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2830, -0.2710,  0.5361,  ..., -0.2529,  1.3145,  0.2249],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0953,  0.9346, -2.2031,  ..., -2.3984, -0.1895,  6.2656],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-9.5520e-03, -1.5587e-02, -5.0812e-03,  ..., -6.5765e-03,
         -1.9104e-02, -1.7944e-02],
        [-1.3428e-02, -3.2440e-02, -6.6338e-03,  ...,  5.3062e-03,
         -3.4046e-03, -1.4839e-03],
        [ 9.0637e-03, -2.2278e-03, -4.4769e-02,  ..., -8.5831e-04,
          1.0315e-02, -1.5030e-03],
        ...,
        [-6.4011e-03, -1.8692e-02,  6.8512e-03,  ..., -3.5034e-02,
          1.2650e-02, -1.2688e-02],
        [ 5.5008e-03,  1.3016e-02,  9.0179e-03,  ..., -2.8336e-02,
         -4.5990e-02, -9.0027e-03],
        [-2.8107e-02, -5.3406e-05, -1.1688e-02,  ..., -2.0416e-02,
         -8.1253e-03, -3.2104e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4937,  1.4141, -2.4199,  ..., -2.3535, -0.0071,  5.5078]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 11:05:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being reasonable is reasonableness
The state of being same is sameness
The state of being directed is directedness
The state of being impressive is impressiveness
The state of being strange is strangeness
The state of being helpful is helpfulness
The state of being competitive is competitiveness
The state of being huge is
2024-07-24 11:05:33 root INFO     [order_1_approx] starting weight calculation for The state of being helpful is helpfulness
The state of being directed is directedness
The state of being same is sameness
The state of being strange is strangeness
The state of being huge is hugeness
The state of being competitive is competitiveness
The state of being reasonable is reasonableness
The state of being impressive is
2024-07-24 11:05:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 11:09:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.5830, 0.0642, 0.5498,  ..., 1.2891, 1.3438, 0.6348], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.3828,  0.6152, -2.4492,  ...,  0.8813, -2.5938,  2.5508],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0077, -0.0072,  0.0008,  ..., -0.0279, -0.0207, -0.0039],
        [ 0.0154, -0.0116,  0.0079,  ..., -0.0081,  0.0147,  0.0108],
        [ 0.0004,  0.0062, -0.0241,  ...,  0.0182,  0.0027,  0.0090],
        ...,
        [-0.0136, -0.0224, -0.0209,  ..., -0.0271,  0.0290, -0.0176],
        [ 0.0144, -0.0089,  0.0216,  ..., -0.0039, -0.0210, -0.0012],
        [-0.0114,  0.0283, -0.0161,  ..., -0.0176, -0.0141,  0.0022]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.6523,  0.3455, -3.1680,  ...,  0.1689, -2.7168,  3.2363]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 11:09:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being helpful is helpfulness
The state of being directed is directedness
The state of being same is sameness
The state of being strange is strangeness
The state of being huge is hugeness
The state of being competitive is competitiveness
The state of being reasonable is reasonableness
The state of being impressive is
2024-07-24 11:09:06 root INFO     [order_1_approx] starting weight calculation for The state of being impressive is impressiveness
The state of being directed is directedness
The state of being strange is strangeness
The state of being same is sameness
The state of being helpful is helpfulness
The state of being competitive is competitiveness
The state of being huge is hugeness
The state of being reasonable is
2024-07-24 11:09:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 11:12:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6045, -0.8257,  1.1270,  ..., -0.3179,  0.9121,  0.6782],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0493,  2.9688, -2.6953,  ..., -1.1963,  3.1270,  4.5195],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0140, -0.0180,  0.0099,  ...,  0.0021, -0.0126, -0.0207],
        [-0.0069, -0.0089, -0.0134,  ..., -0.0126,  0.0137,  0.0042],
        [-0.0118,  0.0125, -0.0129,  ...,  0.0029,  0.0052,  0.0039],
        ...,
        [-0.0099,  0.0045, -0.0223,  ..., -0.0202,  0.0321, -0.0015],
        [ 0.0045,  0.0076, -0.0012,  ..., -0.0182, -0.0279,  0.0019],
        [-0.0244,  0.0166, -0.0159,  ..., -0.0192, -0.0091, -0.0401]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3848,  3.0684, -2.8125,  ..., -0.9839,  3.4395,  3.8066]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 11:12:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being impressive is impressiveness
The state of being directed is directedness
The state of being strange is strangeness
The state of being same is sameness
The state of being helpful is helpfulness
The state of being competitive is competitiveness
The state of being huge is hugeness
The state of being reasonable is
2024-07-24 11:12:51 root INFO     [order_1_approx] starting weight calculation for The state of being competitive is competitiveness
The state of being impressive is impressiveness
The state of being directed is directedness
The state of being reasonable is reasonableness
The state of being strange is strangeness
The state of being helpful is helpfulness
The state of being huge is hugeness
The state of being same is
2024-07-24 11:12:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 11:16:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7734,  0.9912,  1.1279,  ..., -0.7744,  0.5127, -0.5420],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2422,  5.5586, -2.6289,  ...,  0.3481,  1.9453,  3.0020],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0111, -0.0162,  0.0149,  ..., -0.0036, -0.0464, -0.0283],
        [-0.0366, -0.0497, -0.0040,  ...,  0.0113,  0.0058, -0.0224],
        [ 0.0147, -0.0134, -0.0307,  ..., -0.0042,  0.0101,  0.0066],
        ...,
        [-0.0158, -0.0138,  0.0026,  ..., -0.0071,  0.0133,  0.0141],
        [-0.0019,  0.0296, -0.0074,  ..., -0.0109, -0.0230,  0.0058],
        [-0.0255,  0.0048, -0.0007,  ..., -0.0167,  0.0246, -0.0320]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1621,  6.3789, -2.4180,  ...,  0.1279,  1.7393,  3.6875]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 11:16:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being competitive is competitiveness
The state of being impressive is impressiveness
The state of being directed is directedness
The state of being reasonable is reasonableness
The state of being strange is strangeness
The state of being helpful is helpfulness
The state of being huge is hugeness
The state of being same is
2024-07-24 11:16:32 root INFO     [order_1_approx] starting weight calculation for The state of being impressive is impressiveness
The state of being directed is directedness
The state of being same is sameness
The state of being reasonable is reasonableness
The state of being helpful is helpfulness
The state of being huge is hugeness
The state of being competitive is competitiveness
The state of being strange is
2024-07-24 11:16:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 11:20:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7310, -0.5986, -0.2832,  ...,  0.2915,  1.1348,  0.0208],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.6445,  0.8042,  0.7803,  ...,  2.7891,  2.4258,  8.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0096, -0.0192,  0.0127,  ..., -0.0324, -0.0219, -0.0295],
        [ 0.0052, -0.0012, -0.0016,  ..., -0.0030, -0.0162, -0.0046],
        [ 0.0061, -0.0026, -0.0469,  ...,  0.0381,  0.0034,  0.0129],
        ...,
        [-0.0090, -0.0294,  0.0004,  ..., -0.0206,  0.0010, -0.0235],
        [-0.0128,  0.0172,  0.0103,  ..., -0.0175, -0.0334, -0.0075],
        [-0.0349,  0.0184,  0.0050,  ..., -0.0323, -0.0152, -0.0453]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.4844,  0.6904,  0.7495,  ...,  2.7656,  2.2188,  8.2891]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 11:20:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being impressive is impressiveness
The state of being directed is directedness
The state of being same is sameness
The state of being reasonable is reasonableness
The state of being helpful is helpfulness
The state of being huge is hugeness
The state of being competitive is competitiveness
The state of being strange is
2024-07-24 11:20:15 root INFO     total operator prediction time: 1772.8963913917542 seconds
2024-07-24 11:20:15 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on noun+less_reg
2024-07-24 11:20:15 root INFO     building operator noun+less_reg
2024-07-24 11:20:15 root INFO     [order_1_approx] starting weight calculation for Something without god is godless
Something without window is windowless
Something without error is errorless
Something without gender is genderless
Something without penny is penniless
Something without defence is defenceless
Something without guilt is guiltless
Something without art is
2024-07-24 11:20:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 11:23:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3406,  0.0503, -0.1962,  ...,  0.3799,  0.4780,  1.1641],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3379,  2.1465, -2.6465,  ...,  0.8506,  1.9346,  0.6816],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0175, -0.0074,  0.0067,  ..., -0.0029, -0.0212, -0.0208],
        [-0.0013, -0.0323, -0.0055,  ..., -0.0037, -0.0095, -0.0098],
        [ 0.0042,  0.0071, -0.0452,  ...,  0.0038,  0.0091, -0.0175],
        ...,
        [ 0.0096,  0.0155, -0.0014,  ..., -0.0273,  0.0205, -0.0116],
        [-0.0168, -0.0066,  0.0123,  ..., -0.0045, -0.0369,  0.0049],
        [-0.0183,  0.0232, -0.0095,  ...,  0.0017,  0.0213, -0.0353]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.8174,  2.3203, -3.0371,  ...,  0.7070,  1.9053,  0.1724]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 11:24:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without god is godless
Something without window is windowless
Something without error is errorless
Something without gender is genderless
Something without penny is penniless
Something without defence is defenceless
Something without guilt is guiltless
Something without art is
2024-07-24 11:24:00 root INFO     [order_1_approx] starting weight calculation for Something without error is errorless
Something without penny is penniless
Something without god is godless
Something without art is artless
Something without gender is genderless
Something without window is windowless
Something without guilt is guiltless
Something without defence is
2024-07-24 11:24:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 11:27:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([1.2109, 0.2769, 0.2871,  ..., 0.4512, 0.0481, 1.5166], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.8730, -0.4219, -2.3105,  ..., -1.7686,  3.9844, -0.1357],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.0345e-02, -3.4332e-04, -1.2177e-02,  ...,  1.3931e-02,
         -5.0812e-03,  1.4648e-02],
        [-1.2207e-04, -2.1973e-03, -4.8294e-03,  ..., -5.3406e-05,
         -7.0267e-03,  1.1246e-02],
        [ 3.3264e-03,  9.0332e-03, -8.9264e-03,  ...,  1.8250e-02,
          5.6267e-04, -6.0501e-03],
        ...,
        [ 2.0790e-03, -2.4750e-02, -1.7792e-02,  ..., -4.5563e-02,
         -8.6670e-03,  9.9869e-03],
        [-1.0010e-02, -1.7731e-02,  1.2581e-02,  ..., -1.8250e-02,
         -3.0258e-02, -6.8321e-03],
        [ 9.0408e-04,  1.7899e-02,  2.1606e-02,  ...,  6.3019e-03,
         -1.1587e-03, -3.7048e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.0078, -1.2510, -3.2422,  ..., -0.6201,  4.0117, -0.3359]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 11:27:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without error is errorless
Something without penny is penniless
Something without god is godless
Something without art is artless
Something without gender is genderless
Something without window is windowless
Something without guilt is guiltless
Something without defence is
2024-07-24 11:27:46 root INFO     [order_1_approx] starting weight calculation for Something without gender is genderless
Something without penny is penniless
Something without defence is defenceless
Something without art is artless
Something without god is godless
Something without window is windowless
Something without guilt is guiltless
Something without error is
2024-07-24 11:27:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 11:31:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4961, -1.2197,  0.0309,  ..., -0.3403, -0.1487,  0.1333],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.1250, -0.9932,  2.1562,  ..., -1.2061,  4.2031,  3.1523],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0185, -0.0009,  0.0058,  ..., -0.0162, -0.0012, -0.0026],
        [ 0.0006, -0.0397, -0.0194,  ..., -0.0011, -0.0119, -0.0113],
        [ 0.0120,  0.0258, -0.0334,  ...,  0.0115, -0.0036,  0.0010],
        ...,
        [-0.0061, -0.0104, -0.0053,  ..., -0.0140,  0.0181,  0.0140],
        [-0.0037, -0.0086,  0.0120,  ...,  0.0045, -0.0367,  0.0020],
        [-0.0215,  0.0358,  0.0116,  ..., -0.0122,  0.0323, -0.0407]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.8711, -0.6958,  1.6562,  ..., -0.3760,  4.2266,  2.2793]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 11:31:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without gender is genderless
Something without penny is penniless
Something without defence is defenceless
Something without art is artless
Something without god is godless
Something without window is windowless
Something without guilt is guiltless
Something without error is
2024-07-24 11:31:47 root INFO     [order_1_approx] starting weight calculation for Something without guilt is guiltless
Something without art is artless
Something without window is windowless
Something without error is errorless
Something without defence is defenceless
Something without penny is penniless
Something without god is godless
Something without gender is
2024-07-24 11:31:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 11:37:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9248, -0.1770, -0.7783,  ..., -1.2676,  1.4443,  1.2227],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.5977,  1.5801, -5.6641,  ...,  0.1294,  6.0938,  2.5391],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0044, -0.0208,  0.0065,  ..., -0.0111, -0.0227,  0.0042],
        [-0.0142, -0.0290, -0.0016,  ..., -0.0100,  0.0026, -0.0203],
        [ 0.0093, -0.0015, -0.0526,  ...,  0.0184,  0.0111,  0.0110],
        ...,
        [-0.0146, -0.0144,  0.0152,  ..., -0.0165,  0.0235,  0.0013],
        [-0.0058, -0.0026,  0.0231,  ...,  0.0093, -0.0413,  0.0054],
        [ 0.0034,  0.0115, -0.0013,  ..., -0.0094,  0.0096, -0.0345]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.5156,  2.0312, -4.7812,  ..., -0.1115,  5.7461,  2.3398]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 11:37:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without guilt is guiltless
Something without art is artless
Something without window is windowless
Something without error is errorless
Something without defence is defenceless
Something without penny is penniless
Something without god is godless
Something without gender is
2024-07-24 11:37:03 root INFO     [order_1_approx] starting weight calculation for Something without error is errorless
Something without defence is defenceless
Something without penny is penniless
Something without art is artless
Something without window is windowless
Something without gender is genderless
Something without guilt is guiltless
Something without god is
2024-07-24 11:37:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 11:42:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6118, -0.7373, -0.5967,  ..., -0.3618,  0.5254,  0.2244],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1240, -0.0310, -1.8496,  ..., -2.7422,  0.9277, -1.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0194, -0.0018, -0.0030,  ..., -0.0085, -0.0129, -0.0007],
        [ 0.0015, -0.0167, -0.0058,  ...,  0.0043, -0.0042, -0.0251],
        [ 0.0020,  0.0074, -0.0366,  ..., -0.0121, -0.0046, -0.0040],
        ...,
        [ 0.0009, -0.0026,  0.0023,  ..., -0.0155,  0.0032,  0.0105],
        [-0.0049,  0.0003,  0.0054,  ..., -0.0024, -0.0346,  0.0041],
        [ 0.0115,  0.0013,  0.0008,  ..., -0.0101, -0.0053, -0.0450]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6792, -0.7344, -2.2422,  ..., -3.5117,  1.1816, -1.3164]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 11:42:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without error is errorless
Something without defence is defenceless
Something without penny is penniless
Something without art is artless
Something without window is windowless
Something without gender is genderless
Something without guilt is guiltless
Something without god is
2024-07-24 11:42:13 root INFO     [order_1_approx] starting weight calculation for Something without error is errorless
Something without penny is penniless
Something without defence is defenceless
Something without window is windowless
Something without god is godless
Something without art is artless
Something without gender is genderless
Something without guilt is
2024-07-24 11:42:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 11:45:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0850, -0.5474, -0.1371,  ..., -0.2603,  0.9004,  0.7285],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.3828, -0.8604, -2.5117,  ...,  1.2002,  0.8354, -0.7715],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0101, -0.0078,  0.0189,  ..., -0.0057, -0.0462, -0.0252],
        [-0.0128, -0.0215, -0.0042,  ..., -0.0062, -0.0056, -0.0002],
        [ 0.0036,  0.0213, -0.0191,  ...,  0.0158,  0.0065,  0.0077],
        ...,
        [-0.0025, -0.0266, -0.0065,  ..., -0.0317,  0.0188,  0.0036],
        [-0.0058, -0.0056,  0.0224,  ..., -0.0129, -0.0268,  0.0139],
        [-0.0362,  0.0132,  0.0185,  ..., -0.0079,  0.0090, -0.0293]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.3828, -0.9414, -2.5547,  ...,  1.0410, -0.0991, -1.7070]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 11:45:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without error is errorless
Something without penny is penniless
Something without defence is defenceless
Something without window is windowless
Something without god is godless
Something without art is artless
Something without gender is genderless
Something without guilt is
2024-07-24 11:45:59 root INFO     [order_1_approx] starting weight calculation for Something without guilt is guiltless
Something without defence is defenceless
Something without window is windowless
Something without gender is genderless
Something without god is godless
Something without error is errorless
Something without art is artless
Something without penny is
2024-07-24 11:45:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 11:49:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5654, -0.8779, -1.0527,  ..., -0.5122,  0.3560,  2.1719],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.5254, -2.8594, -4.0703,  ...,  1.8096, -0.4927,  1.5498],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0147, -0.0236,  0.0080,  ...,  0.0229, -0.0113,  0.0080],
        [-0.0145, -0.0060,  0.0108,  ...,  0.0035,  0.0123, -0.0053],
        [ 0.0060,  0.0166, -0.0410,  ..., -0.0158,  0.0045,  0.0130],
        ...,
        [-0.0117, -0.0221,  0.0109,  ..., -0.0128, -0.0022, -0.0188],
        [ 0.0070, -0.0067,  0.0181,  ...,  0.0088, -0.0245,  0.0109],
        [-0.0120,  0.0115,  0.0142,  ..., -0.0125,  0.0045, -0.0368]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9502, -2.0703, -3.3027,  ...,  1.4199, -1.3164,  0.6753]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 11:49:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without guilt is guiltless
Something without defence is defenceless
Something without window is windowless
Something without gender is genderless
Something without god is godless
Something without error is errorless
Something without art is artless
Something without penny is
2024-07-24 11:49:39 root INFO     [order_1_approx] starting weight calculation for Something without art is artless
Something without error is errorless
Something without guilt is guiltless
Something without god is godless
Something without penny is penniless
Something without defence is defenceless
Something without gender is genderless
Something without window is
2024-07-24 11:49:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 11:53:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3931,  0.0016, -0.6777,  ...,  0.1758,  1.0088,  0.7153],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.5469,  1.4619, -1.6367,  ...,  0.3677, -2.4160,  3.1953],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-6.8932e-03, -8.4534e-03,  1.1467e-02,  ..., -3.2959e-03,
         -1.1414e-02, -9.4032e-04],
        [-7.3318e-03, -8.1558e-03, -2.1534e-03,  ...,  4.0054e-03,
          2.0676e-02, -1.5457e-02],
        [ 7.5378e-03,  1.6251e-02, -2.5772e-02,  ..., -1.6083e-02,
          2.6417e-03,  8.9722e-03],
        ...,
        [ 3.3569e-04,  6.8474e-03,  2.1698e-02,  ..., -3.0518e-03,
          1.8188e-02, -7.7515e-03],
        [-7.1640e-03,  8.3313e-03,  8.2397e-03,  ...,  1.6617e-02,
         -1.5686e-02,  5.1975e-05],
        [-3.5858e-03,  1.9470e-02,  7.6294e-03,  ..., -7.4339e-04,
          1.5091e-02, -3.8300e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.0859,  1.3955, -1.6680,  ..., -0.1392, -1.8418,  3.5488]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 11:53:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without art is artless
Something without error is errorless
Something without guilt is guiltless
Something without god is godless
Something without penny is penniless
Something without defence is defenceless
Something without gender is genderless
Something without window is
2024-07-24 11:53:24 root INFO     total operator prediction time: 1989.2264986038208 seconds
2024-07-24 11:53:24 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+ment_irreg
2024-07-24 11:53:24 root INFO     building operator verb+ment_irreg
2024-07-24 11:53:24 root INFO     [order_1_approx] starting weight calculation for To engage results in a engagement
To disagree results in a disagreement
To impair results in a impairment
To agree results in a agreement
To manage results in a management
To excite results in a excitement
To replace results in a replacement
To achieve results in a
2024-07-24 11:53:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 11:57:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2773,  0.5957,  1.4844,  ..., -0.1489, -0.3564, -0.5151],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.6035,  3.7461, -1.6768,  ..., -3.8008,  4.7305,  1.2002],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0102, -0.0233, -0.0047,  ..., -0.0026,  0.0052, -0.0231],
        [ 0.0047, -0.0258,  0.0048,  ...,  0.0056, -0.0072, -0.0115],
        [-0.0033,  0.0018, -0.0170,  ...,  0.0053, -0.0003, -0.0113],
        ...,
        [-0.0032, -0.0007, -0.0040,  ..., -0.0046,  0.0072,  0.0125],
        [ 0.0002,  0.0018,  0.0051,  ..., -0.0026, -0.0073,  0.0107],
        [ 0.0079,  0.0067,  0.0006,  ...,  0.0072,  0.0180, -0.0044]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.8398,  3.3984, -2.1113,  ..., -4.5703,  4.2773,  1.1719]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 11:57:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To engage results in a engagement
To disagree results in a disagreement
To impair results in a impairment
To agree results in a agreement
To manage results in a management
To excite results in a excitement
To replace results in a replacement
To achieve results in a
2024-07-24 11:57:04 root INFO     [order_1_approx] starting weight calculation for To impair results in a impairment
To replace results in a replacement
To achieve results in a achievement
To excite results in a excitement
To engage results in a engagement
To manage results in a management
To disagree results in a disagreement
To agree results in a
2024-07-24 11:57:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 12:00:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7988, -0.1548,  1.8340,  ..., -0.2051, -0.7539,  0.0815],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9873,  1.3887,  0.2417,  ..., -1.2158, -0.0468,  4.0234],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0018,  0.0059,  0.0002,  ..., -0.0094,  0.0019, -0.0076],
        [-0.0088, -0.0175, -0.0002,  ...,  0.0054, -0.0079, -0.0128],
        [ 0.0167, -0.0054,  0.0014,  ...,  0.0145,  0.0020, -0.0156],
        ...,
        [ 0.0086, -0.0114,  0.0064,  ..., -0.0193,  0.0043,  0.0058],
        [-0.0041,  0.0173, -0.0096,  ..., -0.0010, -0.0094,  0.0069],
        [-0.0039,  0.0308,  0.0076,  ..., -0.0082,  0.0077, -0.0289]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.8516,  2.0625,  0.5732,  ..., -1.5400, -1.1758,  3.8262]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 12:00:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To impair results in a impairment
To replace results in a replacement
To achieve results in a achievement
To excite results in a excitement
To engage results in a engagement
To manage results in a management
To disagree results in a disagreement
To agree results in a
2024-07-24 12:00:47 root INFO     [order_1_approx] starting weight calculation for To replace results in a replacement
To impair results in a impairment
To excite results in a excitement
To achieve results in a achievement
To agree results in a agreement
To engage results in a engagement
To manage results in a management
To disagree results in a
2024-07-24 12:00:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 12:04:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4829, -0.7100,  2.2500,  ..., -0.2754,  0.2412,  0.7144],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6094, -3.2051, -0.9146,  ...,  1.6182,  4.1406,  0.7783],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.8147e-06,  7.0000e-03,  4.8637e-03,  ..., -1.1276e-02,
          1.2138e-02, -9.5825e-03],
        [-9.6283e-03, -1.2161e-02, -8.7280e-03,  ...,  5.5695e-04,
         -1.1276e-02, -1.0910e-02],
        [-8.7585e-03, -5.3864e-03, -1.3351e-02,  ...,  1.4008e-02,
          3.8452e-03, -1.4999e-02],
        ...,
        [-2.4689e-02, -1.0155e-02, -7.6408e-03,  ..., -3.4027e-03,
          7.8278e-03,  3.0594e-03],
        [-2.5208e-02,  3.1986e-03, -5.2452e-05,  ...,  2.0771e-03,
         -1.4549e-02, -9.2773e-03],
        [-9.1553e-05,  2.2629e-02,  5.2261e-03,  ..., -1.1024e-02,
         -4.6616e-03, -1.0002e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2617, -2.1387, -1.2812,  ...,  1.6680,  3.2715,  0.0420]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 12:04:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To replace results in a replacement
To impair results in a impairment
To excite results in a excitement
To achieve results in a achievement
To agree results in a agreement
To engage results in a engagement
To manage results in a management
To disagree results in a
2024-07-24 12:04:35 root INFO     [order_1_approx] starting weight calculation for To excite results in a excitement
To manage results in a management
To impair results in a impairment
To agree results in a agreement
To replace results in a replacement
To achieve results in a achievement
To disagree results in a disagreement
To engage results in a
2024-07-24 12:04:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 12:08:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8926, -0.1379, -0.0174,  ...,  0.8071, -1.1826,  0.6743],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0093,  0.7109,  0.7065,  ..., -1.4648,  1.8789,  4.7617],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.5776e-05,  1.4877e-04,  4.1351e-03,  ...,  1.2405e-02,
          1.0330e-02, -1.5945e-02],
        [-2.3575e-03, -7.9269e-03,  5.6343e-03,  ..., -2.8896e-03,
         -1.7975e-02, -1.5144e-03],
        [-5.2261e-03, -1.2772e-02, -4.2496e-03,  ...,  1.5602e-03,
         -3.5038e-03, -1.1169e-02],
        ...,
        [-4.7836e-03, -5.4169e-04, -1.6365e-03,  ..., -5.9319e-03,
          3.2196e-03, -9.5978e-03],
        [-5.5008e-03,  3.5706e-03, -2.0714e-03,  ...,  5.4779e-03,
          5.4703e-03, -6.6757e-03],
        [ 5.8556e-04,  2.4017e-02, -5.6076e-03,  ..., -6.2332e-03,
          2.1408e-02, -6.5231e-04]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-8.0811e-01,  1.3789e+00, -4.3945e-03,  ..., -1.7266e+00,
          1.5947e+00,  4.6445e+00]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SubBackward0>) 

                    
2024-07-24 12:08:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To excite results in a excitement
To manage results in a management
To impair results in a impairment
To agree results in a agreement
To replace results in a replacement
To achieve results in a achievement
To disagree results in a disagreement
To engage results in a
2024-07-24 12:08:19 root INFO     [order_1_approx] starting weight calculation for To agree results in a agreement
To replace results in a replacement
To achieve results in a achievement
To engage results in a engagement
To disagree results in a disagreement
To impair results in a impairment
To manage results in a management
To excite results in a
2024-07-24 12:08:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 12:11:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3848, -0.0771,  1.0586,  ..., -0.2471,  0.3506,  0.9810],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1064,  0.2163, -0.8242,  ..., -1.4121,  0.7046,  2.2715],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0084, -0.0144, -0.0066,  ...,  0.0031, -0.0011, -0.0078],
        [-0.0023, -0.0050,  0.0045,  ...,  0.0107, -0.0055,  0.0021],
        [ 0.0071,  0.0033, -0.0037,  ...,  0.0051,  0.0056,  0.0073],
        ...,
        [-0.0204, -0.0042, -0.0018,  ..., -0.0033,  0.0096, -0.0044],
        [-0.0018, -0.0057,  0.0050,  ...,  0.0099, -0.0062, -0.0161],
        [-0.0054,  0.0142, -0.0011,  ..., -0.0012,  0.0090, -0.0062]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4146,  0.8677, -0.6626,  ..., -1.2861,  0.5552,  1.9297]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 12:11:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To agree results in a agreement
To replace results in a replacement
To achieve results in a achievement
To engage results in a engagement
To disagree results in a disagreement
To impair results in a impairment
To manage results in a management
To excite results in a
2024-07-24 12:11:59 root INFO     [order_1_approx] starting weight calculation for To manage results in a management
To disagree results in a disagreement
To replace results in a replacement
To excite results in a excitement
To agree results in a agreement
To engage results in a engagement
To achieve results in a achievement
To impair results in a
2024-07-24 12:11:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 12:16:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3779, -0.4443,  0.3943,  ...,  0.3296, -0.3008, -0.5723],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1152, -1.4062, -3.7734,  ...,  2.5820,  0.6460,  2.7285],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0025,  0.0014,  0.0034,  ..., -0.0066,  0.0055, -0.0037],
        [ 0.0024, -0.0156, -0.0025,  ..., -0.0093, -0.0109,  0.0002],
        [ 0.0008,  0.0080, -0.0073,  ...,  0.0081,  0.0017, -0.0197],
        ...,
        [-0.0115, -0.0037,  0.0031,  ...,  0.0083,  0.0007,  0.0006],
        [-0.0046, -0.0120,  0.0111,  ..., -0.0050, -0.0122, -0.0048],
        [-0.0104,  0.0220, -0.0113,  ..., -0.0135,  0.0075, -0.0009]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9224, -0.3027, -3.7422,  ...,  2.0254,  0.0244,  2.9355]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 12:16:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To manage results in a management
To disagree results in a disagreement
To replace results in a replacement
To excite results in a excitement
To agree results in a agreement
To engage results in a engagement
To achieve results in a achievement
To impair results in a
2024-07-24 12:16:05 root INFO     [order_1_approx] starting weight calculation for To excite results in a excitement
To disagree results in a disagreement
To achieve results in a achievement
To replace results in a replacement
To impair results in a impairment
To engage results in a engagement
To agree results in a agreement
To manage results in a
2024-07-24 12:16:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 12:20:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4297,  0.1147,  1.0215,  ..., -0.0256, -0.0614, -0.0596],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3457, -1.0459, -4.0078,  ...,  2.0488,  0.6743,  5.8828],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0028, -0.0045, -0.0079,  ..., -0.0029,  0.0019, -0.0211],
        [ 0.0051, -0.0135,  0.0051,  ...,  0.0030,  0.0124,  0.0051],
        [ 0.0009, -0.0064,  0.0009,  ...,  0.0099,  0.0037,  0.0043],
        ...,
        [-0.0172, -0.0085, -0.0040,  ...,  0.0057,  0.0086, -0.0032],
        [-0.0127,  0.0058,  0.0068,  ..., -0.0157, -0.0104,  0.0150],
        [ 0.0044,  0.0055,  0.0083,  ...,  0.0085,  0.0141, -0.0085]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3926, -0.9229, -4.3320,  ...,  1.7695,  1.3115,  6.1680]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 12:20:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To excite results in a excitement
To disagree results in a disagreement
To achieve results in a achievement
To replace results in a replacement
To impair results in a impairment
To engage results in a engagement
To agree results in a agreement
To manage results in a
2024-07-24 12:20:24 root INFO     [order_1_approx] starting weight calculation for To impair results in a impairment
To engage results in a engagement
To excite results in a excitement
To agree results in a agreement
To disagree results in a disagreement
To achieve results in a achievement
To manage results in a management
To replace results in a
2024-07-24 12:20:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 12:24:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6328,  0.4387,  0.8770,  ...,  0.5889, -0.0925, -0.4531],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8984,  0.6973, -3.1387,  ...,  2.0684,  1.2021,  7.1055],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0041, -0.0061,  0.0006,  ..., -0.0099, -0.0039,  0.0013],
        [ 0.0068, -0.0104, -0.0011,  ..., -0.0026, -0.0034,  0.0053],
        [-0.0030,  0.0101, -0.0007,  ...,  0.0090,  0.0020, -0.0126],
        ...,
        [-0.0254, -0.0305, -0.0181,  ..., -0.0022,  0.0024,  0.0110],
        [-0.0229, -0.0077,  0.0024,  ..., -0.0145, -0.0037,  0.0107],
        [-0.0050,  0.0120,  0.0089,  ..., -0.0051,  0.0076, -0.0248]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7559,  1.5303, -3.2891,  ...,  1.5381,  0.4707,  6.9219]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 12:24:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To impair results in a impairment
To engage results in a engagement
To excite results in a excitement
To agree results in a agreement
To disagree results in a disagreement
To achieve results in a achievement
To manage results in a management
To replace results in a
2024-07-24 12:24:33 root INFO     total operator prediction time: 1868.7046554088593 seconds
2024-07-24 12:24:33 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on name - nationality
2024-07-24 12:24:33 root INFO     building operator name - nationality
2024-07-24 12:24:33 root INFO     [order_1_approx] starting weight calculation for hitler was german
strauss was austrian
edison was american
newton was english
copernicus was polish
caesar was roman
wagner was german
beethoven was
2024-07-24 12:24:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
