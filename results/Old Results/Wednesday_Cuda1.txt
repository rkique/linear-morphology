2024-07-03 16:57:56 root INFO     loading model + tokenizer
2024-07-03 16:58:13 root INFO     model + tokenizer loaded
2024-07-03 16:58:13 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on name - occupation
2024-07-03 16:58:16 root INFO     building operator name - occupation
2024-07-03 16:58:16 root INFO     [order_1_approx] starting weight calculation for moses was known for their work as a  prophet
locke was known for their work as a  philosopher
depp was known for their work as a  actor
confucius was known for their work as a  philosopher
lincoln was known for their work as a  president
mozart was known for their work as a  composer
mencius was known for their work as a  philosopher
balzac was known for their work as a 
2024-07-03 16:58:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 17:02:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 17:06:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3193,  0.2825, -0.9131,  ...,  0.1694, -0.5415,  0.8359],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.3069,  0.2581, -0.8379,  ...,  0.1584, -0.5249,  0.7861],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0083, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3662, -1.8359, -1.9062,  ..., -1.1934, -2.8125, -2.5645],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0135,  0.0071, -0.0046,  ..., -0.0070,  0.0284, -0.0012],
        [ 0.0025,  0.0172,  0.0243,  ...,  0.0097,  0.0153, -0.0062],
        [ 0.0108,  0.0036, -0.0016,  ...,  0.0050,  0.0124, -0.0092],
        ...,
        [ 0.0200, -0.0168, -0.0123,  ...,  0.0073, -0.0243, -0.0049],
        [ 0.0062, -0.0095,  0.0053,  ..., -0.0111, -0.0028, -0.0024],
        [-0.0282, -0.0153,  0.0051,  ..., -0.0029,  0.0089,  0.0074]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.3714e-03,  3.1805e-04,  1.1606e-03,  ..., -4.1032e-04,
          8.7261e-04, -5.1403e-04],
        [ 4.2415e-04,  1.0614e-03,  1.9760e-03,  ...,  1.2217e-03,
         -5.6076e-04, -1.3504e-03],
        [ 1.4248e-03,  3.6001e-04,  6.1512e-04,  ...,  3.2330e-04,
          1.2341e-03,  1.0576e-03],
        ...,
        [ 1.6327e-03, -2.3842e-06, -1.7319e-03,  ...,  2.4068e-04,
          1.0672e-03,  1.6766e-03],
        [-6.9046e-04, -2.0528e-04,  1.0653e-03,  ...,  1.1015e-03,
         -3.9458e-05, -9.5940e-04],
        [ 9.0313e-04, -4.0102e-04, -1.2836e-03,  ...,  5.4502e-04,
          1.1044e-03,  1.2722e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0850, -2.7578, -2.4922,  ..., -1.7021, -2.4570, -2.2539]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0532, -0.0313, -0.2323,  ...,  0.0168,  0.0595,  0.1772]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 17:06:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for moses was known for their work as a  prophet
locke was known for their work as a  philosopher
depp was known for their work as a  actor
confucius was known for their work as a  philosopher
lincoln was known for their work as a  president
mozart was known for their work as a  composer
mencius was known for their work as a  philosopher
balzac was known for their work as a 
2024-07-03 17:06:02 root INFO     [order_1_approx] starting weight calculation for mozart was known for their work as a  composer
depp was known for their work as a  actor
locke was known for their work as a  philosopher
mencius was known for their work as a  philosopher
lincoln was known for their work as a  president
balzac was known for their work as a  novelist
moses was known for their work as a  prophet
confucius was known for their work as a 
2024-07-03 17:06:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 17:09:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 17:13:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4202, -0.5635, -1.4043,  ..., -0.1759,  0.4995,  0.7754],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3660, -0.5210, -1.1826,  ..., -0.1423,  0.4138,  0.6704],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0112, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3301, -3.6289, -4.1992,  ..., -2.1758, -0.0117, -3.5391],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0025,  0.0022, -0.0029,  ...,  0.0009,  0.0051, -0.0028],
        [-0.0053,  0.0132,  0.0054,  ..., -0.0002,  0.0027, -0.0091],
        [ 0.0081, -0.0034, -0.0043,  ..., -0.0053,  0.0105, -0.0095],
        ...,
        [ 0.0056,  0.0016,  0.0097,  ..., -0.0022, -0.0046,  0.0037],
        [ 0.0132, -0.0033,  0.0067,  ...,  0.0012, -0.0046,  0.0080],
        [-0.0065,  0.0003,  0.0048,  ...,  0.0031,  0.0118, -0.0022]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-6.3705e-04,  2.9449e-03, -5.2023e-04,  ..., -2.5272e-05,
          8.5735e-04, -1.1396e-04],
        [-1.2894e-03,  2.4533e-04,  1.8616e-03,  ..., -5.1022e-05,
          9.0313e-04, -1.2503e-03],
        [ 4.6301e-04, -2.5129e-04,  1.2245e-03,  ...,  4.1103e-04,
          6.8474e-04, -6.1989e-04],
        ...,
        [ 1.1177e-03,  3.1531e-05,  3.9649e-04,  ...,  1.0771e-04,
         -4.2391e-04,  4.6682e-04],
        [ 4.1652e-04,  1.4238e-03,  1.7262e-04,  ..., -1.0309e-03,
         -7.8535e-04, -1.4887e-03],
        [ 3.9053e-04,  9.1934e-04, -7.6389e-04,  ...,  4.9686e-04,
         -9.6321e-05, -8.0109e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3848, -4.4102, -4.4766,  ..., -2.1074,  0.1361, -3.4375]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2507,  0.0102, -0.1918,  ..., -0.0183,  0.0439,  0.1438]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 17:13:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for mozart was known for their work as a  composer
depp was known for their work as a  actor
locke was known for their work as a  philosopher
mencius was known for their work as a  philosopher
lincoln was known for their work as a  president
balzac was known for their work as a  novelist
moses was known for their work as a  prophet
confucius was known for their work as a 
2024-07-03 17:13:48 root INFO     [order_1_approx] starting weight calculation for mozart was known for their work as a  composer
lincoln was known for their work as a  president
depp was known for their work as a  actor
mencius was known for their work as a  philosopher
balzac was known for their work as a  novelist
confucius was known for their work as a  philosopher
locke was known for their work as a  philosopher
moses was known for their work as a 
2024-07-03 17:13:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 17:17:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 17:21:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1417, -1.6953,  0.3879,  ...,  0.9268, -0.3726, -0.1692],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.1407, -1.7695,  0.3638,  ...,  0.9019, -0.3882, -0.1965],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0112, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.4062, -0.8706, -1.1367,  ..., -3.6621,  0.1814,  0.4084],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0134,  0.0076,  0.0183,  ..., -0.0145, -0.0048,  0.0072],
        [-0.0033, -0.0062,  0.0024,  ...,  0.0053,  0.0025, -0.0105],
        [-0.0033, -0.0051, -0.0077,  ..., -0.0074,  0.0253, -0.0098],
        ...,
        [-0.0040, -0.0063, -0.0088,  ..., -0.0002,  0.0058, -0.0015],
        [ 0.0169, -0.0030,  0.0155,  ..., -0.0105, -0.0009, -0.0098],
        [-0.0051,  0.0223,  0.0014,  ..., -0.0055, -0.0018,  0.0193]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 8.4162e-04,  1.7376e-03,  5.2118e-04,  ..., -1.6093e-06,
          1.0860e-04, -1.4305e-06],
        [-1.4811e-03, -6.0797e-04, -4.1819e-04,  ...,  1.1415e-03,
          1.3046e-03,  3.6001e-05],
        [-8.9693e-04, -3.7804e-03,  2.9697e-03,  ...,  1.5306e-03,
          3.5810e-04,  6.9046e-04],
        ...,
        [ 1.4544e-03, -1.0681e-03,  3.8028e-05,  ...,  1.2875e-03,
         -1.6165e-04,  3.9458e-04],
        [ 2.3723e-04,  1.0598e-04,  7.3528e-04,  ..., -5.8794e-04,
          1.1253e-03,  1.9813e-04],
        [ 1.4553e-03,  5.4264e-04, -1.9112e-03,  ...,  1.3561e-03,
          7.2384e-04, -2.5463e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.9922, -1.1055, -1.3242,  ..., -3.9473,  0.7471,  0.6753]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2012, -0.1771, -0.1567,  ..., -0.1576,  0.0455,  0.3162]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 17:21:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for mozart was known for their work as a  composer
lincoln was known for their work as a  president
depp was known for their work as a  actor
mencius was known for their work as a  philosopher
balzac was known for their work as a  novelist
confucius was known for their work as a  philosopher
locke was known for their work as a  philosopher
moses was known for their work as a 
2024-07-03 17:21:35 root INFO     [order_1_approx] starting weight calculation for confucius was known for their work as a  philosopher
mozart was known for their work as a  composer
moses was known for their work as a  prophet
lincoln was known for their work as a  president
mencius was known for their work as a  philosopher
balzac was known for their work as a  novelist
depp was known for their work as a  actor
locke was known for their work as a 
2024-07-03 17:21:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 17:25:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 17:29:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5957, -0.5747, -0.2278,  ...,  0.8018, -0.5444,  0.6523],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.6265, -0.6318, -0.2362,  ...,  0.8135, -0.5786,  0.6782],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0710, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4170, -4.0078, -4.2227,  ..., -3.4043,  3.1914,  0.7759],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0080,  0.0003, -0.0035,  ...,  0.0033,  0.0012, -0.0034],
        [-0.0022,  0.0036, -0.0025,  ...,  0.0079,  0.0024, -0.0059],
        [ 0.0004,  0.0029,  0.0027,  ...,  0.0032, -0.0041, -0.0086],
        ...,
        [ 0.0057,  0.0009,  0.0001,  ...,  0.0008,  0.0070,  0.0006],
        [ 0.0018, -0.0033, -0.0004,  ...,  0.0040,  0.0060,  0.0066],
        [-0.0007,  0.0033,  0.0054,  ..., -0.0064, -0.0011,  0.0022]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.6603e-03,  3.1233e-04, -6.4325e-04,  ..., -7.0763e-04,
         -6.7043e-04,  7.5006e-04],
        [ 1.1215e-03, -4.6039e-04, -1.1158e-04,  ..., -2.4414e-04,
          6.1369e-04,  1.3423e-04],
        [-1.0567e-03,  9.8646e-05,  1.1797e-03,  ..., -1.8430e-04,
         -2.8729e-04, -1.0061e-03],
        ...,
        [ 2.2984e-03,  8.0299e-04,  3.8767e-04,  ...,  7.3528e-04,
          9.5129e-05,  8.2874e-04],
        [-2.2078e-04,  8.5545e-04, -6.2466e-05,  ..., -5.8031e-04,
          4.2439e-04,  1.2088e-04],
        [-8.2874e-04,  1.3046e-03, -1.7796e-03,  ...,  1.4086e-03,
         -9.4032e-04, -3.4857e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5317, -4.0938, -4.5859,  ..., -4.2070,  3.3164,  0.3704]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1115, -0.0284, -0.1897,  ..., -0.0183, -0.0869,  0.0374]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 17:29:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for confucius was known for their work as a  philosopher
mozart was known for their work as a  composer
moses was known for their work as a  prophet
lincoln was known for their work as a  president
mencius was known for their work as a  philosopher
balzac was known for their work as a  novelist
depp was known for their work as a  actor
locke was known for their work as a 
2024-07-03 17:29:21 root INFO     [order_1_approx] starting weight calculation for moses was known for their work as a  prophet
confucius was known for their work as a  philosopher
locke was known for their work as a  philosopher
mencius was known for their work as a  philosopher
mozart was known for their work as a  composer
balzac was known for their work as a  novelist
lincoln was known for their work as a  president
depp was known for their work as a 
2024-07-03 17:29:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 17:33:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 17:37:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9712,  0.0751,  1.0391,  ...,  1.5889, -0.7422, -0.9155],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.8750,  0.0554,  0.8843,  ...,  1.3779, -0.6743, -0.8613],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0083, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3643, -3.2031,  1.5293,  ..., -4.5703,  1.5547,  1.0410],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0103, -0.0020,  0.0041,  ...,  0.0042, -0.0064,  0.0063],
        [ 0.0008, -0.0030,  0.0043,  ...,  0.0187,  0.0002, -0.0040],
        [-0.0142,  0.0025, -0.0209,  ...,  0.0127,  0.0105, -0.0226],
        ...,
        [-0.0149, -0.0119, -0.0156,  ...,  0.0027, -0.0191, -0.0112],
        [-0.0059, -0.0012, -0.0086,  ...,  0.0172,  0.0083, -0.0197],
        [-0.0233,  0.0010, -0.0116,  ..., -0.0035,  0.0253,  0.0008]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 1.4567e-04, -4.3941e-04,  1.3027e-03,  ..., -8.9264e-04,
         -6.9857e-04,  2.5821e-04],
        [-7.4959e-04,  1.4615e-04,  2.7418e-04,  ...,  5.7983e-04,
         -1.5664e-04, -1.4896e-03],
        [ 8.0204e-04,  4.6110e-04, -4.9412e-05,  ...,  1.3161e-04,
          2.9874e-04,  1.7881e-04],
        ...,
        [-7.3195e-04,  1.6975e-04, -8.8882e-04,  ..., -7.4196e-04,
         -7.1383e-04,  4.9210e-04],
        [ 2.4962e-04,  6.0177e-04, -2.3484e-04,  ...,  9.2411e-04,
          1.1597e-03,  3.1519e-04],
        [-4.1962e-04,  1.2407e-03,  2.2030e-04,  ...,  1.0204e-03,
         -2.5725e-04,  3.0565e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0508, -3.5605,  0.7583,  ..., -4.8359,  0.8813,  0.1440]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0140, -0.0066, -0.0388,  ...,  0.0457, -0.0296,  0.2047]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 17:37:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for moses was known for their work as a  prophet
confucius was known for their work as a  philosopher
locke was known for their work as a  philosopher
mencius was known for their work as a  philosopher
mozart was known for their work as a  composer
balzac was known for their work as a  novelist
lincoln was known for their work as a  president
depp was known for their work as a 
2024-07-03 17:37:06 root INFO     [order_1_approx] starting weight calculation for moses was known for their work as a  prophet
depp was known for their work as a  actor
confucius was known for their work as a  philosopher
lincoln was known for their work as a  president
balzac was known for their work as a  novelist
mencius was known for their work as a  philosopher
locke was known for their work as a  philosopher
mozart was known for their work as a 
2024-07-03 17:37:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 17:41:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 17:44:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9976, -0.3638, -0.4871,  ...,  0.3354,  0.0947, -0.3660],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.9937, -0.3845, -0.4702,  ...,  0.3262,  0.0771, -0.3916],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0083, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9727, -0.6196, -2.0586,  ..., -0.3997,  0.1475, -1.0508],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0057, -0.0043,  0.0064,  ...,  0.0050, -0.0014,  0.0121],
        [-0.0083, -0.0028, -0.0027,  ...,  0.0124, -0.0030, -0.0022],
        [ 0.0144,  0.0163, -0.0092,  ..., -0.0095,  0.0041, -0.0080],
        ...,
        [-0.0022,  0.0010, -0.0224,  ..., -0.0049, -0.0287,  0.0049],
        [ 0.0124,  0.0043, -0.0112,  ..., -0.0151, -0.0104, -0.0102],
        [-0.0066, -0.0062, -0.0015,  ...,  0.0034,  0.0165, -0.0099]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 6.2799e-04, -1.2798e-03,  5.7030e-04,  ...,  9.0075e-04,
         -3.9387e-04, -5.4240e-05],
        [-1.4334e-03, -2.8849e-04, -9.2936e-04,  ...,  3.7980e-04,
          3.0947e-04, -1.6336e-03],
        [-3.6812e-04, -2.6083e-04,  1.8673e-03,  ...,  4.0340e-04,
          9.9182e-04,  7.6437e-04],
        ...,
        [ 1.8339e-03, -5.6791e-04, -8.0585e-04,  ...,  9.2363e-04,
         -8.4496e-04, -4.0936e-04],
        [-2.9325e-04,  3.2842e-05, -8.9598e-04,  ..., -1.0166e-03,
         -2.7919e-04,  7.4005e-04],
        [ 1.6575e-03,  2.8753e-04,  5.1498e-04,  ...,  1.7843e-03,
         -2.0332e-03, -1.4324e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9795, -0.7539, -1.7559,  ..., -0.4448,  0.2166, -1.4170]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1339,  0.0165, -0.1880,  ...,  0.1415, -0.0902, -0.1611]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 17:44:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for moses was known for their work as a  prophet
depp was known for their work as a  actor
confucius was known for their work as a  philosopher
lincoln was known for their work as a  president
balzac was known for their work as a  novelist
mencius was known for their work as a  philosopher
locke was known for their work as a  philosopher
mozart was known for their work as a 
2024-07-03 17:44:52 root INFO     [order_1_approx] starting weight calculation for moses was known for their work as a  prophet
mencius was known for their work as a  philosopher
confucius was known for their work as a  philosopher
depp was known for their work as a  actor
mozart was known for their work as a  composer
locke was known for their work as a  philosopher
balzac was known for their work as a  novelist
lincoln was known for their work as a 
2024-07-03 17:44:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 17:48:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 17:52:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7617, -0.9863, -0.3030,  ...,  0.4219, -0.6025,  0.0878],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.7598, -1.0186, -0.2981,  ...,  0.4038, -0.6074,  0.0646],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0083, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.7383, -0.2837, -3.7676,  ..., -2.8047,  0.1846, -2.9141],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-3.6812e-03, -1.0406e-02, -6.2103e-03,  ...,  6.6681e-03,
          3.8147e-03,  3.5362e-03],
        [-4.8790e-03, -7.3547e-03,  4.4670e-03,  ...,  2.5620e-02,
         -4.9210e-03, -2.1896e-03],
        [ 1.5594e-02, -1.8311e-04,  6.4774e-03,  ..., -4.5657e-05,
          2.1713e-02, -6.7825e-03],
        ...,
        [ 1.5030e-03, -4.9820e-03,  1.2741e-03,  ..., -4.3869e-03,
         -6.5689e-03, -3.4008e-03],
        [ 1.8616e-02,  2.5711e-03,  1.2436e-03,  ...,  5.6572e-03,
          3.8471e-03,  7.1907e-04],
        [ 1.1612e-02,  9.6970e-03, -6.3629e-03,  ..., -7.1640e-03,
          9.0408e-03, -5.2757e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 1.1902e-03, -1.2875e-05, -7.6389e-04,  ...,  8.1348e-04,
         -5.4264e-04, -9.0265e-04],
        [-2.1172e-03,  1.1778e-03,  1.9836e-04,  ...,  1.5318e-04,
          8.6308e-04,  8.5020e-04],
        [-4.5729e-04, -1.9875e-03,  5.7888e-04,  ..., -1.0574e-04,
          6.3848e-04, -2.3055e-04],
        ...,
        [ 2.4719e-03, -1.6556e-03, -1.4229e-03,  ..., -3.0756e-04,
          5.7554e-04,  1.4174e-04],
        [ 2.0828e-03,  8.8644e-04,  7.3338e-04,  ...,  2.7084e-04,
         -5.2547e-04,  1.0738e-03],
        [ 2.8095e-03, -7.2861e-04,  4.0913e-04,  ...,  4.3702e-04,
         -2.8253e-04,  7.6771e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.6758, -0.5303, -3.9707,  ..., -3.3730,  0.4648, -3.1699]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0428,  0.0901, -0.1061,  ...,  0.0945, -0.0111,  0.1002]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 17:52:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for moses was known for their work as a  prophet
mencius was known for their work as a  philosopher
confucius was known for their work as a  philosopher
depp was known for their work as a  actor
mozart was known for their work as a  composer
locke was known for their work as a  philosopher
balzac was known for their work as a  novelist
lincoln was known for their work as a 
2024-07-03 17:52:37 root INFO     [order_1_approx] starting weight calculation for mozart was known for their work as a  composer
locke was known for their work as a  philosopher
lincoln was known for their work as a  president
moses was known for their work as a  prophet
balzac was known for their work as a  novelist
depp was known for their work as a  actor
confucius was known for their work as a  philosopher
mencius was known for their work as a 
2024-07-03 17:52:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 17:56:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 18:00:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6104, -0.7119, -1.4473,  ...,  0.5928, -0.9531,  1.1270],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.5596, -0.6890, -1.2832,  ...,  0.5278, -0.8838,  1.0361],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0112, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6211, -3.2422, -1.3398,  ..., -2.6738,  0.0352, -2.1738],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0044, -0.0010,  0.0021,  ..., -0.0050, -0.0054, -0.0017],
        [-0.0100, -0.0050, -0.0044,  ..., -0.0027, -0.0019, -0.0075],
        [-0.0008, -0.0039, -0.0008,  ...,  0.0007, -0.0004, -0.0020],
        ...,
        [-0.0005,  0.0008,  0.0064,  ...,  0.0011, -0.0073,  0.0030],
        [ 0.0067,  0.0028,  0.0064,  ...,  0.0053, -0.0005,  0.0043],
        [ 0.0042,  0.0105,  0.0052,  ...,  0.0013,  0.0155,  0.0028]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 1.4029e-03, -2.6774e-04,  1.1148e-03,  ...,  3.8052e-04,
         -3.2330e-04, -2.8229e-04],
        [-2.3003e-03,  1.0834e-03, -2.8515e-04,  ..., -8.4686e-04,
          7.0477e-04,  1.2035e-03],
        [-4.1652e-04,  1.0544e-04,  1.3132e-03,  ...,  5.4598e-04,
         -4.3941e-04,  7.4673e-04],
        ...,
        [ 9.5797e-04,  1.8859e-04, -2.2829e-04,  ...,  5.4884e-04,
         -8.6129e-05,  8.0872e-04],
        [-2.4259e-04,  4.0460e-04,  6.7520e-04,  ..., -1.0462e-03,
          2.2812e-03,  4.4632e-04],
        [ 1.0452e-03, -1.5316e-03,  9.3460e-05,  ...,  1.8144e-04,
         -1.5316e-03, -5.0974e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4180, -3.2031, -1.6787,  ..., -2.3340,  0.1937, -2.4121]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1046, -0.1282, -0.1702,  ...,  0.0632, -0.1021,  0.0624]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 18:00:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for mozart was known for their work as a  composer
locke was known for their work as a  philosopher
lincoln was known for their work as a  president
moses was known for their work as a  prophet
balzac was known for their work as a  novelist
depp was known for their work as a  actor
confucius was known for their work as a  philosopher
mencius was known for their work as a 
2024-07-03 18:00:24 root INFO     total operator prediction time: 3727.8349096775055 seconds
2024-07-03 18:00:24 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on adj - superlative
2024-07-03 18:00:26 root INFO     building operator adj - superlative
2024-07-03 18:00:26 root INFO     [order_1_approx] starting weight calculation for If something is the most wealthy, it is wealthiest
If something is the most clever, it is cleverest
If something is the most dense, it is densest
If something is the most weird, it is weirdest
If something is the most shiny, it is shiniest
If something is the most rude, it is rudest
If something is the most noisy, it is noisiest
If something is the most ugly, it is
2024-07-03 18:00:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 18:04:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 18:08:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7944, -0.6172,  0.6216,  ..., -0.5786,  0.6606, -0.5195],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.6992, -0.5698,  0.5073,  ..., -0.4810,  0.5508, -0.4871],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5615, -0.6948, -2.0742,  ...,  1.3965, -3.2207,  0.5488],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0011,  0.0007, -0.0012,  ...,  0.0082, -0.0033, -0.0151],
        [ 0.0034, -0.0021, -0.0070,  ...,  0.0032, -0.0078, -0.0043],
        [ 0.0138,  0.0087, -0.0072,  ...,  0.0130, -0.0011,  0.0098],
        ...,
        [-0.0052, -0.0021,  0.0023,  ..., -0.0241, -0.0018, -0.0116],
        [ 0.0006,  0.0037,  0.0085,  ..., -0.0062, -0.0214,  0.0074],
        [-0.0208, -0.0029, -0.0023,  ..., -0.0146, -0.0101, -0.0159]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-3.9911e-04, -9.8801e-04, -5.3310e-04,  ...,  2.3880e-03,
         -5.8460e-04, -1.0729e-05],
        [ 6.2180e-04, -2.0771e-03, -8.9169e-04,  ...,  2.8553e-03,
         -6.2752e-04, -2.4414e-03],
        [ 2.4188e-04,  2.2106e-03, -4.4107e-06,  ...,  3.2806e-04,
          3.1567e-04,  3.7594e-03],
        ...,
        [ 1.5020e-03, -1.0986e-03, -3.0136e-03,  ...,  5.7173e-04,
          2.5005e-03, -3.0289e-03],
        [-1.7052e-03,  2.3479e-03,  5.1379e-05,  ..., -1.5192e-03,
         -2.2240e-03,  2.7065e-03],
        [-1.5688e-04,  3.7193e-04, -8.4352e-04,  ..., -5.4264e-04,
         -1.0252e-03, -2.5024e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.4648, -1.2852, -2.3848,  ...,  1.1953, -2.9375,  0.0273]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1348, -0.3257,  0.4375,  ..., -0.3572, -0.0681, -0.0714]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 18:08:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most wealthy, it is wealthiest
If something is the most clever, it is cleverest
If something is the most dense, it is densest
If something is the most weird, it is weirdest
If something is the most shiny, it is shiniest
If something is the most rude, it is rudest
If something is the most noisy, it is noisiest
If something is the most ugly, it is
2024-07-03 18:08:16 root INFO     [order_1_approx] starting weight calculation for If something is the most wealthy, it is wealthiest
If something is the most noisy, it is noisiest
If something is the most weird, it is weirdest
If something is the most shiny, it is shiniest
If something is the most rude, it is rudest
If something is the most clever, it is cleverest
If something is the most ugly, it is ugliest
If something is the most dense, it is
2024-07-03 18:08:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 18:12:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 18:16:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3652,  0.5811,  0.9839,  ...,  0.5576, -0.6763, -0.8096],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3416,  0.5459,  0.8765,  ...,  0.5093, -0.6499, -0.8057],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6562, -2.2383, -1.0039,  ..., -1.6094,  1.0234,  6.3008],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0107,  0.0004,  0.0163,  ..., -0.0110, -0.0037, -0.0197],
        [-0.0113, -0.0079,  0.0041,  ..., -0.0038,  0.0040, -0.0023],
        [ 0.0141,  0.0063,  0.0002,  ..., -0.0039, -0.0105, -0.0030],
        ...,
        [-0.0007, -0.0075,  0.0057,  ..., -0.0047, -0.0095, -0.0100],
        [-0.0007, -0.0012,  0.0082,  ...,  0.0019,  0.0044, -0.0027],
        [ 0.0009,  0.0077, -0.0187,  ..., -0.0068,  0.0086,  0.0007]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 8.2970e-04,  4.1914e-04,  1.5936e-03,  ..., -7.9536e-04,
          6.3896e-05,  5.4359e-04],
        [-3.7193e-04, -1.7872e-03,  2.6360e-03,  ...,  1.1330e-03,
         -4.9877e-04, -4.4203e-04],
        [ 1.7109e-03,  6.2466e-04, -9.6512e-04,  ...,  1.8272e-03,
         -1.7881e-03,  3.0565e-04],
        ...,
        [ 1.9112e-03,  4.7398e-04, -2.0695e-04,  ..., -2.4319e-03,
          6.0415e-04,  1.4591e-04],
        [-1.1368e-03,  1.1730e-03,  7.5817e-04,  ..., -1.4496e-04,
         -8.4579e-05,  6.8855e-04],
        [ 6.6280e-05, -7.3147e-04, -1.4648e-03,  ...,  1.6241e-03,
         -1.3409e-03, -1.2321e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8813, -2.4043, -1.3633,  ..., -1.3955,  0.8828,  6.2227]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0329, -0.1881,  0.2158,  ...,  0.1025, -0.0706,  0.0164]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 18:16:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most wealthy, it is wealthiest
If something is the most noisy, it is noisiest
If something is the most weird, it is weirdest
If something is the most shiny, it is shiniest
If something is the most rude, it is rudest
If something is the most clever, it is cleverest
If something is the most ugly, it is ugliest
If something is the most dense, it is
2024-07-03 18:16:02 root INFO     [order_1_approx] starting weight calculation for If something is the most weird, it is weirdest
If something is the most shiny, it is shiniest
If something is the most rude, it is rudest
If something is the most ugly, it is ugliest
If something is the most clever, it is cleverest
If something is the most wealthy, it is wealthiest
If something is the most dense, it is densest
If something is the most noisy, it is
2024-07-03 18:16:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 18:20:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 18:23:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2295,  0.4915,  0.8770,  ..., -0.0129, -0.0195,  0.1597],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.2094,  0.4302,  0.7329,  ..., -0.0080, -0.0367,  0.1205],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0833, -2.8301, -1.9258,  ...,  5.0508,  3.6211,  4.8711],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0038, -0.0276, -0.0073,  ...,  0.0175, -0.0165, -0.0075],
        [-0.0093, -0.0196,  0.0006,  ...,  0.0107, -0.0020,  0.0045],
        [ 0.0072,  0.0046, -0.0044,  ..., -0.0073,  0.0165,  0.0093],
        ...,
        [-0.0027, -0.0092, -0.0033,  ..., -0.0261, -0.0120, -0.0008],
        [ 0.0048, -0.0173,  0.0092,  ..., -0.0013, -0.0303,  0.0035],
        [-0.0096,  0.0201, -0.0107,  ..., -0.0209, -0.0011, -0.0219]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 1.8148e-03, -4.3869e-03, -1.8349e-03,  ...,  1.0567e-03,
          1.0042e-03,  4.8637e-04],
        [-1.2398e-04, -2.7161e-03,  2.7542e-03,  ...,  7.5817e-05,
          1.5850e-03,  7.4387e-04],
        [-2.8133e-05,  1.9817e-03, -1.9588e-03,  ...,  1.5612e-03,
         -1.7083e-04,  1.5736e-03],
        ...,
        [ 3.6793e-03, -2.1877e-03, -6.1655e-04,  ..., -3.9711e-03,
          5.0402e-04,  1.3185e-04],
        [-6.1703e-04,  3.3665e-04,  8.2493e-05,  ..., -4.4632e-04,
         -1.7090e-03,  1.5707e-03],
        [-7.9298e-04,  1.6327e-03, -2.9373e-04,  ...,  1.1759e-03,
         -6.0987e-04, -2.2554e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1672, -2.8145, -2.0547,  ...,  4.8203,  3.5801,  3.4805]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0641, -0.2600,  0.2827,  ...,  0.1550, -0.0941, -0.0264]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 18:23:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most weird, it is weirdest
If something is the most shiny, it is shiniest
If something is the most rude, it is rudest
If something is the most ugly, it is ugliest
If something is the most clever, it is cleverest
If something is the most wealthy, it is wealthiest
If something is the most dense, it is densest
If something is the most noisy, it is
2024-07-03 18:23:50 root INFO     [order_1_approx] starting weight calculation for If something is the most clever, it is cleverest
If something is the most noisy, it is noisiest
If something is the most rude, it is rudest
If something is the most ugly, it is ugliest
If something is the most dense, it is densest
If something is the most shiny, it is shiniest
If something is the most weird, it is weirdest
If something is the most wealthy, it is
2024-07-03 18:23:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 18:27:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 18:31:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5073,  0.4553,  0.5239,  ...,  0.4976,  0.4844, -0.1418],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.4412,  0.3911,  0.4265,  ...,  0.4216,  0.3994, -0.1539],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.1406, -0.2998, -0.4487,  ..., -2.2461, -1.0732,  1.8359],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0038, -0.0061, -0.0075,  ..., -0.0045, -0.0107, -0.0007],
        [ 0.0012, -0.0054, -0.0031,  ...,  0.0054,  0.0020, -0.0104],
        [ 0.0097,  0.0081, -0.0043,  ...,  0.0049,  0.0114,  0.0109],
        ...,
        [-0.0088, -0.0080, -0.0021,  ..., -0.0184, -0.0120, -0.0008],
        [ 0.0121, -0.0050,  0.0036,  ..., -0.0096, -0.0424,  0.0134],
        [-0.0209,  0.0134, -0.0179,  ...,  0.0126,  0.0082, -0.0342]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.6680e-03, -1.8501e-03,  1.3733e-03,  ...,  2.9469e-03,
         -1.4515e-03, -7.2193e-04],
        [ 2.9697e-03, -2.7905e-03,  1.8368e-03,  ...,  2.2149e-04,
          8.9836e-04, -1.7376e-03],
        [ 7.1526e-04,  1.4286e-03, -7.1287e-04,  ..., -9.8169e-05,
          1.3256e-04,  1.5364e-03],
        ...,
        [ 9.1648e-04, -5.7793e-04, -1.4296e-03,  ..., -1.2751e-03,
          6.9904e-04,  2.5272e-05],
        [ 5.3072e-04,  2.3232e-03, -1.0834e-03,  ...,  3.3855e-04,
         -5.7697e-04,  1.5807e-04],
        [ 1.0700e-03,  3.5572e-04, -1.1444e-03,  ...,  5.8079e-04,
          1.5616e-04, -4.0174e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3057, -0.1577, -1.2002,  ..., -2.1973, -2.1914,  1.0566]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0517, -0.5020,  0.3472,  ..., -0.0028,  0.0546,  0.0085]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 18:31:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most clever, it is cleverest
If something is the most noisy, it is noisiest
If something is the most rude, it is rudest
If something is the most ugly, it is ugliest
If something is the most dense, it is densest
If something is the most shiny, it is shiniest
If something is the most weird, it is weirdest
If something is the most wealthy, it is
2024-07-03 18:31:34 root INFO     [order_1_approx] starting weight calculation for If something is the most wealthy, it is wealthiest
If something is the most rude, it is rudest
If something is the most clever, it is cleverest
If something is the most shiny, it is shiniest
If something is the most noisy, it is noisiest
If something is the most dense, it is densest
If something is the most ugly, it is ugliest
If something is the most weird, it is
2024-07-03 18:31:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 18:35:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 18:39:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4707, -0.4180, -0.2211,  ...,  0.2124,  1.2607, -0.5796],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.4375, -0.4216, -0.2113,  ...,  0.1936,  1.1484, -0.5815],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1152, -0.7354, -0.5835,  ...,  2.0234,  3.8867,  5.1523],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0137, -0.0117,  0.0018,  ..., -0.0034, -0.0117, -0.0123],
        [-0.0058, -0.0073,  0.0012,  ...,  0.0158,  0.0043, -0.0091],
        [ 0.0143, -0.0093, -0.0171,  ...,  0.0221,  0.0060,  0.0005],
        ...,
        [-0.0018, -0.0108,  0.0125,  ..., -0.0236, -0.0129, -0.0114],
        [ 0.0062,  0.0011,  0.0216,  ..., -0.0071, -0.0310, -0.0030],
        [-0.0156,  0.0120, -0.0043,  ..., -0.0012,  0.0008, -0.0420]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-5.9509e-04, -1.1215e-03,  1.3185e-04,  ..., -1.5497e-04,
         -6.9952e-04,  1.5287e-03],
        [-4.2391e-04, -2.5368e-03,  2.1248e-03,  ...,  2.4834e-03,
          1.4105e-03, -1.7242e-03],
        [ 5.6362e-04,  2.8920e-04,  1.2321e-03,  ...,  1.2722e-03,
         -6.4945e-04,  2.4338e-03],
        ...,
        [ 2.6245e-03, -7.7248e-04,  6.2513e-04,  ..., -2.5311e-03,
         -5.8115e-05,  1.2493e-03],
        [-1.3323e-03,  1.4553e-03,  4.5848e-04,  ...,  3.2520e-04,
          8.5831e-04,  9.7036e-05],
        [-6.7282e-04, -9.4843e-04,  1.7405e-03,  ...,  2.5635e-03,
          2.0885e-04, -2.4452e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3789, -1.6904, -2.3848,  ...,  2.0254,  3.8555,  4.9375]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1356, -0.2859,  0.4431,  ..., -0.0435, -0.1675, -0.0315]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 18:39:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most wealthy, it is wealthiest
If something is the most rude, it is rudest
If something is the most clever, it is cleverest
If something is the most shiny, it is shiniest
If something is the most noisy, it is noisiest
If something is the most dense, it is densest
If something is the most ugly, it is ugliest
If something is the most weird, it is
2024-07-03 18:39:20 root INFO     [order_1_approx] starting weight calculation for If something is the most rude, it is rudest
If something is the most ugly, it is ugliest
If something is the most dense, it is densest
If something is the most clever, it is cleverest
If something is the most noisy, it is noisiest
If something is the most weird, it is weirdest
If something is the most wealthy, it is wealthiest
If something is the most shiny, it is
2024-07-03 18:39:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 18:43:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 18:47:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4556, -0.0059,  0.7329,  ..., -0.3604,  0.8315,  0.5073],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.4170, -0.0248,  0.6162,  ..., -0.3093,  0.7173,  0.4404],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.3301,  0.5332, -1.0596,  ...,  5.3438, -6.7109,  3.5195],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0006, -0.0050, -0.0014,  ...,  0.0073, -0.0105, -0.0118],
        [-0.0055, -0.0069,  0.0028,  ..., -0.0076, -0.0155,  0.0062],
        [ 0.0047,  0.0068, -0.0014,  ..., -0.0027,  0.0116,  0.0010],
        ...,
        [-0.0125, -0.0090, -0.0157,  ..., -0.0161, -0.0110, -0.0059],
        [ 0.0127, -0.0100,  0.0260,  ..., -0.0024, -0.0086, -0.0012],
        [-0.0123, -0.0026,  0.0073,  ..., -0.0133,  0.0125, -0.0086]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 1.1415e-03, -1.1454e-03,  1.9634e-04,  ..., -4.6492e-04,
         -1.0433e-03, -7.8583e-04],
        [-1.1463e-03, -3.3760e-03,  1.1683e-03,  ...,  1.6623e-03,
          2.8896e-04, -7.6103e-04],
        [ 5.6314e-04,  2.1195e-04, -2.8229e-04,  ...,  5.8460e-04,
          1.1139e-03,  5.1022e-04],
        ...,
        [ 1.1702e-03, -2.6474e-03, -8.2302e-04,  ..., -7.8773e-04,
          1.8730e-03, -1.8196e-03],
        [-7.5483e-04,  4.3273e-05,  7.2145e-04,  ..., -3.7270e-03,
         -2.3079e-03,  2.2259e-03],
        [-5.4169e-04,  1.4472e-04,  1.3065e-04,  ...,  1.0099e-03,
         -9.2411e-04, -1.1940e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.3594,  0.3608, -1.2510,  ...,  5.3086, -6.0078,  3.1797]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0402, -0.1162,  0.1896,  ..., -0.0756,  0.0528, -0.2285]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 18:47:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most rude, it is rudest
If something is the most ugly, it is ugliest
If something is the most dense, it is densest
If something is the most clever, it is cleverest
If something is the most noisy, it is noisiest
If something is the most weird, it is weirdest
If something is the most wealthy, it is wealthiest
If something is the most shiny, it is
2024-07-03 18:47:07 root INFO     [order_1_approx] starting weight calculation for If something is the most ugly, it is ugliest
If something is the most wealthy, it is wealthiest
If something is the most weird, it is weirdest
If something is the most clever, it is cleverest
If something is the most noisy, it is noisiest
If something is the most dense, it is densest
If something is the most shiny, it is shiniest
If something is the most rude, it is
2024-07-03 18:47:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 18:51:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 18:54:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6338, -1.6143,  0.0028,  ...,  0.4368, -0.0061,  0.5283],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.5815, -1.5479, -0.0102,  ...,  0.3894, -0.0256,  0.4731],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.8301, -1.8477, -3.9023,  ...,  4.7891, -0.8438,  3.6992],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0147, -0.0148,  0.0024,  ..., -0.0108, -0.0047, -0.0138],
        [ 0.0074, -0.0173, -0.0056,  ..., -0.0040,  0.0022, -0.0033],
        [-0.0056, -0.0055, -0.0175,  ...,  0.0060,  0.0018, -0.0021],
        ...,
        [-0.0070, -0.0210,  0.0025,  ..., -0.0257, -0.0001, -0.0172],
        [ 0.0012,  0.0044,  0.0136,  ..., -0.0051, -0.0098,  0.0058],
        [-0.0077, -0.0033, -0.0052,  ..., -0.0086, -0.0086, -0.0149]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-8.1778e-05, -1.7586e-03, -1.3247e-03,  ..., -4.1223e-04,
         -7.6437e-04, -5.3596e-04],
        [ 6.1464e-04, -2.0561e-03, -1.4248e-03,  ...,  1.7967e-03,
          6.5184e-04, -7.2145e-04],
        [ 1.6356e-03, -2.1112e-04, -2.2545e-03,  ..., -2.5806e-03,
         -2.1496e-03, -1.0824e-03],
        ...,
        [ 1.6632e-03, -2.5864e-03,  2.1362e-04,  ..., -2.3594e-03,
          1.3008e-03, -9.6703e-04],
        [ 8.5950e-05,  4.9591e-05, -1.2290e-04,  ..., -1.3733e-03,
         -9.1648e-04,  9.2983e-04],
        [ 1.5211e-03, -3.2711e-04, -9.0218e-04,  ...,  2.6550e-03,
         -1.3137e-04, -1.3485e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.0312, -2.0762, -4.2188,  ...,  4.6797, -0.7988,  3.1152]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0536, -0.1860,  0.1938,  ..., -0.2546, -0.0972,  0.2449]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 18:54:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most ugly, it is ugliest
If something is the most wealthy, it is wealthiest
If something is the most weird, it is weirdest
If something is the most clever, it is cleverest
If something is the most noisy, it is noisiest
If something is the most dense, it is densest
If something is the most shiny, it is shiniest
If something is the most rude, it is
2024-07-03 18:54:54 root INFO     [order_1_approx] starting weight calculation for If something is the most weird, it is weirdest
If something is the most shiny, it is shiniest
If something is the most rude, it is rudest
If something is the most wealthy, it is wealthiest
If something is the most dense, it is densest
If something is the most ugly, it is ugliest
If something is the most noisy, it is noisiest
If something is the most clever, it is
2024-07-03 18:54:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 18:58:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 19:02:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6919, -0.9824,  0.4956,  ...,  0.3323,  0.1416,  0.1326],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.6050, -0.9062,  0.4038,  ...,  0.2830,  0.1027,  0.0939],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1953, -0.4329,  1.1895,  ...,  3.3691, -3.4316,  1.5234],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0181, -0.0141,  0.0010,  ..., -0.0009,  0.0007, -0.0158],
        [-0.0107, -0.0186,  0.0085,  ..., -0.0018, -0.0128, -0.0085],
        [-0.0042,  0.0024,  0.0019,  ..., -0.0041, -0.0006, -0.0031],
        ...,
        [-0.0115, -0.0210, -0.0073,  ..., -0.0179,  0.0024, -0.0185],
        [ 0.0146, -0.0119,  0.0104,  ..., -0.0038, -0.0163,  0.0127],
        [ 0.0033,  0.0074, -0.0106,  ..., -0.0088,  0.0020, -0.0168]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.7691e-03, -8.4877e-04,  1.1606e-03,  ...,  9.5177e-04,
         -1.8799e-04, -2.6011e-04],
        [ 5.2023e-04, -2.7390e-03,  1.6537e-03,  ..., -1.6861e-03,
         -7.6008e-04,  1.5898e-03],
        [ 6.7806e-04, -2.0504e-05,  7.4577e-04,  ..., -1.0490e-05,
         -1.1368e-03,  2.4319e-03],
        ...,
        [ 2.9469e-03, -1.8349e-03, -1.1549e-03,  ..., -8.8835e-04,
          7.7248e-04, -1.0176e-03],
        [-1.2913e-03, -7.0620e-04,  2.6560e-04,  ..., -7.0047e-04,
         -9.1255e-05,  2.6550e-03],
        [ 7.6914e-04,  1.3866e-03, -9.8109e-05,  ...,  9.6273e-04,
         -2.0885e-04, -1.2503e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.0742, -0.3477,  0.9741,  ...,  2.6289, -3.2656,  0.8647]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0086, -0.0085,  0.4365,  ..., -0.0874, -0.4141, -0.1703]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 19:02:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most weird, it is weirdest
If something is the most shiny, it is shiniest
If something is the most rude, it is rudest
If something is the most wealthy, it is wealthiest
If something is the most dense, it is densest
If something is the most ugly, it is ugliest
If something is the most noisy, it is noisiest
If something is the most clever, it is
2024-07-03 19:02:41 root INFO     total operator prediction time: 3734.4908900260925 seconds
2024-07-03 19:02:41 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on synonyms - exact
2024-07-03 19:02:43 root INFO     building operator synonyms - exact
2024-07-03 19:02:43 root INFO     [order_1_approx] starting weight calculation for Another word for father is dad
Another word for airplane is aeroplane
Another word for mend is repair
Another word for bicycle is bike
Another word for homogeneous is uniform
Another word for incorrect is wrong
Another word for help is aid
Another word for clothes is
2024-07-03 19:02:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 19:06:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 19:10:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9058, -0.0542, -0.4927,  ...,  1.2871, -0.1982,  0.4341],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.8301, -0.0711, -0.4475,  ...,  1.1396, -0.2015,  0.3823],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0345, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.8750,  0.2778, -2.7852,  ..., -4.8945,  2.1426, -1.2432],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0051, -0.0112,  0.0128,  ...,  0.0009, -0.0117,  0.0011],
        [ 0.0050, -0.0006, -0.0116,  ...,  0.0102, -0.0053, -0.0022],
        [ 0.0127,  0.0073, -0.0179,  ..., -0.0008,  0.0076,  0.0077],
        ...,
        [-0.0001,  0.0040,  0.0188,  ...,  0.0047,  0.0004,  0.0145],
        [ 0.0077, -0.0017, -0.0177,  ...,  0.0061,  0.0004,  0.0064],
        [-0.0117, -0.0050, -0.0075,  ..., -0.0071,  0.0017,  0.0004]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 2.0332e-03,  1.5945e-03, -9.1171e-04,  ...,  1.2946e-04,
         -6.9904e-04,  6.4278e-04],
        [-1.8826e-03, -2.3289e-03, -3.7909e-04,  ..., -2.6226e-05,
         -9.9182e-04,  7.7534e-04],
        [-8.6594e-04,  6.6710e-04, -6.5041e-04,  ...,  1.2798e-03,
          1.2875e-03, -1.3723e-03],
        ...,
        [ 3.2735e-04,  1.0449e-04,  1.8430e-04,  ..., -1.7433e-03,
          2.2335e-03, -2.2459e-04],
        [ 1.3723e-03, -3.7885e-04, -2.2793e-04,  ...,  2.1887e-04,
          4.9496e-04,  5.5361e-04],
        [-7.0095e-05, -2.4128e-03, -1.7433e-03,  ...,  9.4354e-05,
         -1.1473e-03, -2.5063e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.7500, -0.1887, -3.6953,  ..., -3.9980,  2.1328, -1.2383]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2371, -0.0695,  0.1080,  ...,  0.0010,  0.3284,  0.0327]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 19:10:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for father is dad
Another word for airplane is aeroplane
Another word for mend is repair
Another word for bicycle is bike
Another word for homogeneous is uniform
Another word for incorrect is wrong
Another word for help is aid
Another word for clothes is
2024-07-03 19:10:33 root INFO     [order_1_approx] starting weight calculation for Another word for help is aid
Another word for mend is repair
Another word for bicycle is bike
Another word for homogeneous is uniform
Another word for clothes is clothing
Another word for incorrect is wrong
Another word for father is dad
Another word for airplane is
2024-07-03 19:10:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 19:14:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 19:18:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6753,  0.3506,  0.0073,  ..., -0.1602, -0.4731,  0.2891],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.6045,  0.3042, -0.0084,  ..., -0.1384, -0.4441,  0.2396],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0346, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.5586,  0.7314,  0.0664,  ..., -3.9746,  0.7500, -2.3477],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0016, -0.0111, -0.0092,  ...,  0.0187, -0.0040,  0.0084],
        [-0.0061, -0.0105, -0.0085,  ...,  0.0161, -0.0098,  0.0057],
        [-0.0086,  0.0028, -0.0119,  ..., -0.0229,  0.0016,  0.0042],
        ...,
        [-0.0089, -0.0129,  0.0139,  ..., -0.0127, -0.0087,  0.0065],
        [ 0.0038, -0.0137,  0.0058,  ...,  0.0048, -0.0053,  0.0131],
        [ 0.0006, -0.0016,  0.0135,  ...,  0.0006,  0.0041, -0.0094]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-9.4414e-04, -2.8458e-03, -9.1076e-04,  ...,  1.6327e-03,
         -3.5305e-03,  6.7711e-04],
        [-8.4305e-04, -1.0223e-03,  5.1546e-04,  ...,  1.3933e-03,
         -3.1471e-05, -4.8637e-04],
        [-6.3229e-04,  8.9169e-04, -2.5392e-04,  ..., -6.9046e-04,
          1.4095e-03, -1.8082e-03],
        ...,
        [-1.2398e-04, -2.6989e-04,  1.1253e-03,  ..., -3.0365e-03,
          2.6369e-04,  2.5082e-04],
        [ 1.5306e-03, -1.1387e-03,  1.7490e-03,  ..., -6.9523e-04,
          1.3742e-03,  1.7595e-04],
        [-6.2180e-04, -2.8133e-04, -8.8882e-04,  ...,  3.9411e-04,
          1.3995e-04, -1.0920e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3936,  0.8984, -1.3037,  ..., -1.6934, -0.1982, -2.2559]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1187,  0.1067,  0.0397,  ..., -0.2119,  0.1564,  0.2135]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 19:18:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for help is aid
Another word for mend is repair
Another word for bicycle is bike
Another word for homogeneous is uniform
Another word for clothes is clothing
Another word for incorrect is wrong
Another word for father is dad
Another word for airplane is
2024-07-03 19:18:24 root INFO     [order_1_approx] starting weight calculation for Another word for clothes is clothing
Another word for father is dad
Another word for airplane is aeroplane
Another word for incorrect is wrong
Another word for homogeneous is uniform
Another word for mend is repair
Another word for bicycle is bike
Another word for help is
2024-07-03 19:18:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 19:22:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 19:26:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3760,  0.3625, -0.6270,  ...,  0.5669,  0.5879,  0.7598],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.3831,  0.3538, -0.6128,  ...,  0.5474,  0.5605,  0.7520],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0345, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5391, -1.1230,  0.3252,  ..., -5.0547,  1.9434,  1.6104],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0026, -0.0172,  0.0058,  ...,  0.0022, -0.0215,  0.0022],
        [-0.0043, -0.0081,  0.0104,  ...,  0.0108, -0.0145,  0.0010],
        [-0.0032,  0.0201,  0.0106,  ...,  0.0082,  0.0081,  0.0087],
        ...,
        [-0.0068, -0.0016, -0.0029,  ...,  0.0061, -0.0010,  0.0129],
        [ 0.0007,  0.0128, -0.0056,  ..., -0.0134,  0.0156,  0.0120],
        [-0.0264, -0.0132,  0.0030,  ..., -0.0056,  0.0053,  0.0135]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 7.0477e-04, -1.4296e-03, -1.2624e-04,  ...,  7.4148e-04,
         -2.8534e-03, -1.5965e-03],
        [ 1.1625e-03, -3.4523e-04,  2.5730e-03,  ...,  1.8215e-03,
          3.1710e-04,  1.7834e-03],
        [-2.0580e-03,  3.9139e-03, -5.3930e-04,  ...,  1.6832e-04,
          1.6317e-03,  1.5335e-03],
        ...,
        [-1.4296e-03, -1.5621e-03, -2.5845e-03,  ...,  6.4373e-05,
          2.0351e-03,  2.1381e-03],
        [-4.8256e-04,  2.8038e-03, -3.3355e-04,  ..., -4.5586e-03,
         -1.2407e-03,  3.3112e-03],
        [-1.1864e-03, -2.8744e-03,  2.4834e-03,  ...,  4.7970e-04,
          4.3654e-04,  2.7418e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7349, -1.3896, -0.7666,  ..., -3.3965,  2.0645,  1.0020]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.4314,  0.0151, -0.0459,  ..., -0.0310,  0.1757,  0.1829]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 19:26:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for clothes is clothing
Another word for father is dad
Another word for airplane is aeroplane
Another word for incorrect is wrong
Another word for homogeneous is uniform
Another word for mend is repair
Another word for bicycle is bike
Another word for help is
2024-07-03 19:26:14 root INFO     [order_1_approx] starting weight calculation for Another word for mend is repair
Another word for help is aid
Another word for clothes is clothing
Another word for father is dad
Another word for bicycle is bike
Another word for homogeneous is uniform
Another word for airplane is aeroplane
Another word for incorrect is
2024-07-03 19:26:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 19:30:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 19:34:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6782, -0.6777,  0.2546,  ...,  0.3174,  0.6201,  0.7466],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.6333, -0.6782,  0.2137,  ...,  0.2869,  0.5542,  0.6914],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0345, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.4824,  0.4170, -0.8125,  ...,  1.2363,  7.3672, -0.1826],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-6.2752e-03, -1.8021e-02,  2.1866e-02,  ..., -4.8714e-03,
          5.3635e-03, -5.8708e-03],
        [ 2.0996e-02, -4.8218e-03, -1.4900e-02,  ...,  2.1927e-02,
         -1.0406e-02,  5.3101e-03],
        [ 6.0730e-03, -6.0959e-03, -1.5083e-02,  ..., -1.1368e-03,
          7.4043e-03,  2.6642e-02],
        ...,
        [-1.5068e-02, -6.6338e-03,  6.5994e-03,  ...,  1.4648e-03,
         -2.9541e-02, -7.6294e-06],
        [-3.4370e-03, -9.3079e-03,  1.2115e-02,  ..., -2.3010e-02,
          1.1475e-02,  1.5350e-02],
        [ 4.1275e-03, -6.3362e-03,  6.2180e-04,  ...,  6.9695e-03,
          2.9526e-03, -9.3002e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 9.4414e-04, -1.8158e-03,  4.1413e-04,  ..., -2.2352e-04,
          2.4185e-03,  2.0528e-04],
        [ 4.9448e-04,  1.8835e-04, -4.8637e-05,  ..., -1.3380e-03,
         -3.5286e-04,  6.9714e-04],
        [-2.2945e-03,  1.0738e-03, -1.1024e-03,  ..., -1.8454e-03,
          1.8301e-03,  1.3142e-03],
        ...,
        [-1.1587e-03, -8.9121e-04, -1.0568e-04,  ...,  1.1349e-03,
         -9.0790e-04, -2.0771e-03],
        [-4.5633e-04,  6.2990e-04,  1.0490e-03,  ..., -8.3637e-04,
          7.2193e-04, -1.0052e-03],
        [ 3.9721e-04, -1.1120e-03, -3.6144e-04,  ...,  1.4544e-03,
         -1.2712e-03, -2.7809e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7637, -0.9727, -3.3691,  ...,  0.3735,  6.6172, -1.7197]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2527,  0.2264, -0.0094,  ...,  0.2739, -0.0057,  0.3906]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 19:34:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for mend is repair
Another word for help is aid
Another word for clothes is clothing
Another word for father is dad
Another word for bicycle is bike
Another word for homogeneous is uniform
Another word for airplane is aeroplane
Another word for incorrect is
2024-07-03 19:34:02 root INFO     [order_1_approx] starting weight calculation for Another word for bicycle is bike
Another word for help is aid
Another word for incorrect is wrong
Another word for clothes is clothing
Another word for homogeneous is uniform
Another word for mend is repair
Another word for airplane is aeroplane
Another word for father is
2024-07-03 19:34:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 19:38:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 19:41:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3884, -0.0522, -0.4417,  ...,  0.2222, -0.3955,  0.9297],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3611, -0.0686, -0.4087,  ...,  0.2032, -0.3865,  0.8677],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0345, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.9590, -4.6367,  0.4121,  ..., -5.8438,  4.1094, -2.5703],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0066,  0.0066, -0.0155,  ..., -0.0011,  0.0098,  0.0021],
        [-0.0082,  0.0096,  0.0044,  ...,  0.0094, -0.0095, -0.0045],
        [ 0.0022,  0.0025, -0.0464,  ..., -0.0057, -0.0074,  0.0087],
        ...,
        [-0.0164, -0.0182,  0.0199,  ..., -0.0127,  0.0027,  0.0382],
        [-0.0047, -0.0119,  0.0184,  ..., -0.0255,  0.0054,  0.0305],
        [-0.0037, -0.0053,  0.0077,  ...,  0.0107,  0.0127, -0.0035]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 8.0585e-04,  6.9714e-04,  5.7459e-05,  ...,  2.0046e-03,
         -8.0490e-04, -1.0834e-03],
        [-3.1166e-03,  1.4668e-03, -2.5616e-03,  ..., -2.2011e-03,
         -3.9177e-03,  2.0294e-03],
        [-1.1406e-03,  2.6054e-03, -4.1389e-04,  ..., -7.5436e-04,
         -4.2748e-04,  1.7109e-03],
        ...,
        [ 2.0742e-04, -1.3151e-03, -8.8930e-05,  ..., -3.1700e-03,
          2.5520e-03,  4.0894e-03],
        [ 2.3441e-03, -1.4925e-04,  1.8101e-03,  ..., -8.4972e-04,
          8.7595e-04,  1.0395e-03],
        [-4.9877e-04,  2.4548e-03, -1.8940e-03,  ...,  7.1526e-04,
          3.1781e-04, -1.2083e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.0039, -4.3203,  1.5850,  ..., -3.6934,  4.9258, -2.6934]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1954,  0.2322, -0.4648,  ...,  0.0207, -0.1847,  0.0884]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 19:41:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for bicycle is bike
Another word for help is aid
Another word for incorrect is wrong
Another word for clothes is clothing
Another word for homogeneous is uniform
Another word for mend is repair
Another word for airplane is aeroplane
Another word for father is
2024-07-03 19:41:52 root INFO     [order_1_approx] starting weight calculation for Another word for homogeneous is uniform
Another word for airplane is aeroplane
Another word for father is dad
Another word for clothes is clothing
Another word for mend is repair
Another word for help is aid
Another word for incorrect is wrong
Another word for bicycle is
2024-07-03 19:41:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 19:45:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 19:49:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6772, -0.1215, -1.2334,  ...,  0.3672,  0.2104,  0.5645],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.5742, -0.1267, -1.0225,  ...,  0.3027,  0.1562,  0.4668],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0345, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0996, -1.1895,  0.5610,  ..., -1.2031,  0.5039, -2.2773],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0162, -0.0201,  0.0166,  ...,  0.0207, -0.0049, -0.0009],
        [-0.0061,  0.0177,  0.0073,  ...,  0.0167,  0.0069, -0.0051],
        [ 0.0150, -0.0018, -0.0311,  ..., -0.0210,  0.0057, -0.0054],
        ...,
        [-0.0147, -0.0021, -0.0005,  ..., -0.0033, -0.0017,  0.0138],
        [-0.0060, -0.0114,  0.0088,  ...,  0.0019, -0.0084,  0.0175],
        [-0.0118,  0.0043,  0.0084,  ..., -0.0097,  0.0046,  0.0061]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 7.2956e-04,  5.4550e-04,  7.9441e-04,  ..., -7.1716e-04,
         -1.8950e-03, -1.6146e-03],
        [-8.6546e-04,  4.1580e-04, -1.8158e-03,  ...,  2.0065e-03,
          1.7576e-03,  1.2112e-03],
        [-1.7586e-03,  1.0443e-03, -2.7227e-04,  ..., -5.1975e-04,
         -1.0977e-03, -1.4267e-03],
        ...,
        [-1.3285e-03, -3.9434e-04, -1.1120e-03,  ..., -1.2112e-03,
          2.5043e-03,  7.5293e-04],
        [ 6.4135e-04,  6.6376e-04,  1.1711e-03,  ..., -1.1768e-03,
         -3.7742e-04,  1.3828e-03],
        [-3.3975e-05,  6.9714e-04, -1.6470e-03,  ...,  3.7146e-04,
          3.3331e-04, -1.2264e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3203,  0.5137, -1.5898,  ..., -0.3564,  1.3242, -2.2148]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.2404, -0.1650, -0.0296,  ...,  0.2340,  0.2930,  0.0304]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 19:49:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for homogeneous is uniform
Another word for airplane is aeroplane
Another word for father is dad
Another word for clothes is clothing
Another word for mend is repair
Another word for help is aid
Another word for incorrect is wrong
Another word for bicycle is
2024-07-03 19:49:40 root INFO     [order_1_approx] starting weight calculation for Another word for help is aid
Another word for airplane is aeroplane
Another word for clothes is clothing
Another word for homogeneous is uniform
Another word for father is dad
Another word for incorrect is wrong
Another word for bicycle is bike
Another word for mend is
2024-07-03 19:49:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 19:53:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 19:57:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3223, -1.2676, -1.3701,  ...,  1.0645,  1.1230, -0.7412],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.2705, -1.1328, -1.1338,  ...,  0.8760,  0.9277, -0.6738],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0345, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5542, -1.4971, -2.9941,  ..., -0.2100,  1.6113,  4.4883],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0067, -0.0105,  0.0064,  ...,  0.0175, -0.0089, -0.0023],
        [-0.0041,  0.0022, -0.0211,  ...,  0.0146, -0.0072,  0.0002],
        [ 0.0091, -0.0049,  0.0007,  ...,  0.0017,  0.0093,  0.0152],
        ...,
        [-0.0029,  0.0037,  0.0172,  ..., -0.0035, -0.0009, -0.0037],
        [-0.0022, -0.0086,  0.0118,  ...,  0.0094,  0.0049, -0.0036],
        [-0.0038, -0.0161,  0.0011,  ...,  0.0022, -0.0038,  0.0126]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.1063e-03, -2.3186e-04, -1.6823e-03,  ..., -1.1683e-03,
         -1.2894e-03, -1.5564e-03],
        [-1.2598e-03, -5.2834e-04, -1.1787e-03,  ...,  1.6899e-03,
         -1.6861e-03, -8.9025e-04],
        [ 2.9182e-04, -8.5163e-04, -1.6108e-03,  ..., -8.3542e-04,
         -1.5907e-03,  1.5602e-03],
        ...,
        [ 1.6022e-03, -7.1716e-04,  1.0986e-03,  ..., -7.5150e-04,
          2.9354e-03,  7.8964e-04],
        [ 3.9530e-04, -7.7057e-04,  4.6463e-03,  ...,  1.9765e-04,
          1.7333e-04, -1.4877e-03],
        [-2.1732e-04, -2.4529e-03,  4.6074e-05,  ..., -2.3098e-03,
         -1.9875e-03,  5.4264e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8154, -1.0908, -2.3477,  ..., -0.8467, -0.0146,  2.7812]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1307,  0.0776,  0.0728,  ..., -0.0417, -0.3860, -0.1770]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 19:57:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for help is aid
Another word for airplane is aeroplane
Another word for clothes is clothing
Another word for homogeneous is uniform
Another word for father is dad
Another word for incorrect is wrong
Another word for bicycle is bike
Another word for mend is
2024-07-03 19:57:30 root INFO     [order_1_approx] starting weight calculation for Another word for mend is repair
Another word for bicycle is bike
Another word for incorrect is wrong
Another word for help is aid
Another word for father is dad
Another word for clothes is clothing
Another word for airplane is aeroplane
Another word for homogeneous is
2024-07-03 19:57:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 20:01:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 20:05:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1445,  0.6714,  1.5820,  ...,  0.1797, -0.1929,  0.6533],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 1.1064,  0.6484,  1.4521,  ...,  0.1686, -0.2059,  0.6211],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0345, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0430,  2.7656, -6.4648,  ...,  2.6055,  1.1816,  0.3848],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0266, -0.0034,  0.0139,  ...,  0.0003, -0.0015, -0.0055],
        [-0.0121,  0.0159, -0.0047,  ..., -0.0034, -0.0099,  0.0107],
        [ 0.0007, -0.0088, -0.0227,  ...,  0.0031, -0.0077,  0.0013],
        ...,
        [-0.0102,  0.0004,  0.0075,  ...,  0.0206, -0.0021,  0.0261],
        [-0.0099,  0.0030,  0.0139,  ..., -0.0039,  0.0044,  0.0092],
        [ 0.0058,  0.0021, -0.0131,  ..., -0.0157,  0.0071,  0.0118]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-8.6164e-04, -8.4496e-04,  1.4782e-03,  ...,  2.8181e-04,
          5.0783e-05, -1.2646e-03],
        [-9.1028e-04, -2.4176e-04,  8.0872e-04,  ..., -1.3380e-03,
          9.0122e-05,  8.9169e-04],
        [ 1.6136e-03,  7.3528e-04, -1.4048e-03,  ...,  2.3098e-03,
          1.1377e-03,  5.9652e-04],
        ...,
        [-2.1505e-04,  4.0960e-04,  2.9802e-05,  ...,  9.3699e-04,
          2.3854e-04, -6.2084e-04],
        [ 1.5497e-04, -3.8266e-04,  1.0386e-03,  ..., -7.6389e-04,
          1.1003e-04,  1.1473e-03],
        [-9.0420e-05,  1.7452e-03, -1.7774e-04,  ..., -3.5954e-04,
         -8.3828e-04, -5.4300e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0244,  2.7266, -5.4062,  ...,  2.4727,  0.7676,  0.5112]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1123, -0.1759,  0.4897,  ..., -0.0081,  0.0311, -0.0163]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 20:05:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for mend is repair
Another word for bicycle is bike
Another word for incorrect is wrong
Another word for help is aid
Another word for father is dad
Another word for clothes is clothing
Another word for airplane is aeroplane
Another word for homogeneous is
2024-07-03 20:05:20 root INFO     total operator prediction time: 3757.3809475898743 seconds
2024-07-03 20:05:20 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_Ving - Ved
2024-07-03 20:05:22 root INFO     building operator verb_Ving - Ved
2024-07-03 20:05:23 root INFO     [order_1_approx] starting weight calculation for After something is allowing, it has allowed
After something is suffering, it has suffered
After something is containing, it has contained
After something is appointing, it has appointed
After something is performing, it has performed
After something is considering, it has considered
After something is losing, it has lost
After something is creating, it has
2024-07-03 20:05:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 20:09:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 20:13:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5947, -0.9155,  0.0889,  ...,  0.5352,  0.6934, -0.7412],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.6265, -1.0186,  0.0758,  ...,  0.5464,  0.7051, -0.8315],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0468, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1035,  1.7627,  0.7720,  ...,  1.1484, -2.1738,  1.9238],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0170, -0.0172,  0.0030,  ...,  0.0122, -0.0204, -0.0234],
        [ 0.0006, -0.0175,  0.0090,  ..., -0.0043, -0.0308,  0.0095],
        [ 0.0044, -0.0120, -0.0119,  ...,  0.0012, -0.0016,  0.0040],
        ...,
        [-0.0082,  0.0014, -0.0073,  ..., -0.0066,  0.0093,  0.0025],
        [-0.0090, -0.0053, -0.0081,  ..., -0.0114, -0.0497,  0.0228],
        [-0.0068,  0.0177, -0.0091,  ...,  0.0132, -0.0019, -0.0134]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-4.3640e-03, -2.4548e-03,  2.0103e-03,  ...,  1.0548e-03,
         -2.5177e-04, -3.5934e-03],
        [ 1.5507e-03, -1.2894e-03,  2.1820e-03,  ..., -7.2956e-05,
          3.3903e-04,  1.8578e-03],
        [ 8.6260e-04,  3.1452e-03, -2.9659e-03,  ..., -1.6451e-03,
          2.7800e-04,  4.2648e-03],
        ...,
        [ 3.3112e-03, -2.3174e-03,  1.3995e-04,  ..., -1.2856e-03,
         -8.9407e-05, -2.6393e-04],
        [ 2.6798e-04,  1.6155e-03,  8.4019e-04,  ...,  1.9336e-04,
         -4.7531e-03,  4.1199e-04],
        [-1.6189e-04,  2.0275e-03, -1.5039e-03,  ...,  1.0118e-03,
         -1.3924e-03,  1.3351e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1099, -0.2080, -0.1348,  ...,  0.2505, -2.6797,  0.5059]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0134, -0.1154,  0.2113,  ...,  0.1117, -0.3750, -0.1267]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 20:13:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is allowing, it has allowed
After something is suffering, it has suffered
After something is containing, it has contained
After something is appointing, it has appointed
After something is performing, it has performed
After something is considering, it has considered
After something is losing, it has lost
After something is creating, it has
2024-07-03 20:13:13 root INFO     [order_1_approx] starting weight calculation for After something is losing, it has lost
After something is suffering, it has suffered
After something is containing, it has contained
After something is performing, it has performed
After something is allowing, it has allowed
After something is creating, it has created
After something is considering, it has considered
After something is appointing, it has
2024-07-03 20:13:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 20:17:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 20:21:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7793, -0.8999,  0.4834,  ...,  0.0590,  0.3782,  0.4856],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.7622, -0.9390,  0.4390,  ...,  0.0528,  0.3435,  0.4600],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0468, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.6484,  0.6045,  1.2148,  ...,  4.9805, -0.5352,  1.8320],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0069, -0.0201,  0.0050,  ...,  0.0115, -0.0439, -0.0311],
        [ 0.0084, -0.0098, -0.0285,  ..., -0.0028,  0.0124,  0.0035],
        [ 0.0019,  0.0200, -0.0136,  ...,  0.0163, -0.0228,  0.0138],
        ...,
        [-0.0464, -0.0174, -0.0460,  ..., -0.0030,  0.0106,  0.0022],
        [ 0.0123, -0.0036, -0.0020,  ..., -0.0060, -0.0321,  0.0100],
        [ 0.0013,  0.0109,  0.0168,  ...,  0.0209, -0.0160, -0.0381]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.5545e-03, -2.5635e-03,  1.5841e-03,  ..., -1.9789e-05,
         -7.7629e-04, -1.9798e-03],
        [-5.1308e-04, -1.7834e-03,  1.1702e-03,  ...,  1.0471e-03,
          9.9564e-04, -9.1553e-04],
        [-3.5763e-04,  3.3531e-03, -2.1896e-03,  ..., -1.4973e-04,
         -5.8031e-04,  8.4076e-03],
        ...,
        [ 1.1692e-03, -2.0676e-03,  9.4461e-04,  ...,  2.6035e-03,
          1.2589e-03, -3.5038e-03],
        [ 4.5848e-04, -1.6499e-03, -1.3456e-03,  ..., -9.3842e-04,
         -2.7504e-03, -3.5954e-04],
        [-1.6994e-03,  5.7507e-04,  1.7776e-03,  ...,  6.8045e-04,
         -3.6812e-04, -1.7757e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.7148,  0.7812,  0.6938,  ...,  4.9805, -0.3877,  2.4297]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1428,  0.1840,  0.0331,  ...,  0.1136, -0.0129,  0.3972]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 20:21:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is losing, it has lost
After something is suffering, it has suffered
After something is containing, it has contained
After something is performing, it has performed
After something is allowing, it has allowed
After something is creating, it has created
After something is considering, it has considered
After something is appointing, it has
2024-07-03 20:21:04 root INFO     [order_1_approx] starting weight calculation for After something is creating, it has created
After something is losing, it has lost
After something is appointing, it has appointed
After something is considering, it has considered
After something is performing, it has performed
After something is suffering, it has suffered
After something is containing, it has contained
After something is allowing, it has
2024-07-03 20:21:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 20:25:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 20:28:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3896, -0.9741,  0.2666,  ...,  0.1171,  0.1611,  0.4116],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 1.4648, -1.0791,  0.2534,  ...,  0.1182,  0.1449,  0.4163],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0468, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0645,  0.5947,  0.4072,  ...,  1.6436, -1.2305,  3.7715],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0095, -0.0081, -0.0055,  ...,  0.0132, -0.0091, -0.0096],
        [ 0.0014, -0.0232,  0.0083,  ...,  0.0108,  0.0010,  0.0035],
        [ 0.0177, -0.0095, -0.0269,  ..., -0.0024, -0.0044,  0.0043],
        ...,
        [-0.0385,  0.0021,  0.0040,  ..., -0.0123, -0.0143, -0.0013],
        [ 0.0017, -0.0161,  0.0058,  ..., -0.0044, -0.0288,  0.0044],
        [ 0.0016,  0.0320,  0.0066,  ...,  0.0193, -0.0182, -0.0042]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0037, -0.0001,  0.0008,  ..., -0.0019, -0.0016, -0.0015],
        [ 0.0006, -0.0008,  0.0020,  ...,  0.0008,  0.0039, -0.0002],
        [-0.0001,  0.0010, -0.0035,  ..., -0.0021,  0.0007,  0.0019],
        ...,
        [ 0.0005, -0.0032,  0.0045,  ..., -0.0016,  0.0002, -0.0025],
        [-0.0014, -0.0005,  0.0015,  ...,  0.0003, -0.0016,  0.0002],
        [ 0.0004,  0.0017,  0.0012,  ...,  0.0006,  0.0001, -0.0018]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3242,  1.4189, -0.8252,  ...,  0.8462, -0.5249,  2.0938]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2366,  0.2374,  0.2612,  ...,  0.2322, -0.2913,  0.0034]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 20:28:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is creating, it has created
After something is losing, it has lost
After something is appointing, it has appointed
After something is considering, it has considered
After something is performing, it has performed
After something is suffering, it has suffered
After something is containing, it has contained
After something is allowing, it has
2024-07-03 20:28:54 root INFO     [order_1_approx] starting weight calculation for After something is creating, it has created
After something is containing, it has contained
After something is allowing, it has allowed
After something is appointing, it has appointed
After something is considering, it has considered
After something is losing, it has lost
After something is performing, it has performed
After something is suffering, it has
2024-07-03 20:28:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 20:32:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 20:36:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4937, -1.2275,  0.4492,  ...,  0.4265,  0.8218,  0.3899],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.4744, -1.2529,  0.4028,  ...,  0.3987,  0.7686,  0.3601],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0468, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.4902,  3.0879, -0.1914,  ..., -1.0059, -2.0273,  3.3613],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0216, -0.0150,  0.0049,  ...,  0.0037, -0.0078, -0.0211],
        [ 0.0054, -0.0253, -0.0176,  ..., -0.0025, -0.0052, -0.0067],
        [ 0.0182, -0.0085, -0.0304,  ..., -0.0072,  0.0156,  0.0070],
        ...,
        [-0.0193, -0.0338, -0.0049,  ..., -0.0460,  0.0142, -0.0033],
        [-0.0004, -0.0157, -0.0044,  ...,  0.0063, -0.0373,  0.0262],
        [-0.0061,  0.0246,  0.0161,  ...,  0.0032,  0.0090, -0.0452]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.0752e-03, -2.0752e-03, -7.0810e-05,  ..., -2.0370e-03,
         -2.1400e-03,  5.8746e-04],
        [ 3.0994e-06, -8.4639e-05,  6.6137e-04,  ..., -1.5049e-03,
          5.3692e-04,  1.1635e-03],
        [ 1.7500e-03,  7.0286e-04, -3.0098e-03,  ..., -1.8587e-03,
          7.0095e-05,  1.7786e-03],
        ...,
        [ 2.5225e-04, -4.1237e-03,  2.0962e-03,  ..., -1.9140e-03,
          2.3670e-03, -3.9825e-03],
        [ 2.6608e-04,  9.2697e-04,  1.3523e-03,  ...,  1.1282e-03,
         -2.4929e-03, -5.4502e-04],
        [-6.8140e-04,  7.5388e-04,  1.2178e-03,  ...,  4.0889e-04,
         -1.6832e-03, -9.2220e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3691,  2.7793,  0.0967,  ..., -0.4683, -1.5000,  3.0703]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.5493,  0.0206,  0.2983,  ..., -0.1816, -0.3223,  0.1033]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 20:36:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is creating, it has created
After something is containing, it has contained
After something is allowing, it has allowed
After something is appointing, it has appointed
After something is considering, it has considered
After something is losing, it has lost
After something is performing, it has performed
After something is suffering, it has
2024-07-03 20:36:45 root INFO     [order_1_approx] starting weight calculation for After something is containing, it has contained
After something is allowing, it has allowed
After something is considering, it has considered
After something is suffering, it has suffered
After something is performing, it has performed
After something is creating, it has created
After something is appointing, it has appointed
After something is losing, it has
2024-07-03 20:36:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 20:40:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 20:44:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3340, -0.8569,  0.7383,  ..., -0.1262,  0.0234,  0.4578],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3386, -0.9297,  0.7144,  ..., -0.1247,  0.0017,  0.4561],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0468, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.4375,  0.1165,  0.7676,  ...,  1.5312, -1.4424,  2.3906],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0107, -0.0117, -0.0022,  ...,  0.0163, -0.0134, -0.0046],
        [ 0.0019, -0.0090,  0.0081,  ...,  0.0110,  0.0144,  0.0005],
        [ 0.0081, -0.0135, -0.0145,  ..., -0.0049,  0.0142,  0.0041],
        ...,
        [ 0.0041, -0.0037, -0.0173,  ..., -0.0262, -0.0023,  0.0039],
        [ 0.0075, -0.0111, -0.0031,  ...,  0.0032, -0.0242,  0.0199],
        [-0.0103, -0.0056,  0.0006,  ...,  0.0051,  0.0108, -0.0083]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.7052e-03,  1.5330e-04, -2.1992e-03,  ..., -1.6136e-03,
         -1.5621e-03, -3.9983e-04],
        [ 6.2656e-04,  1.7948e-03,  2.7561e-03,  ...,  8.7833e-04,
          3.1052e-03,  7.7009e-04],
        [-6.5470e-04, -4.4227e-04, -2.8534e-03,  ...,  1.2827e-04,
          5.4407e-04, -1.5914e-04],
        ...,
        [ 7.4530e-04, -3.4103e-03,  1.0538e-03,  ..., -1.3752e-03,
         -3.4475e-04, -1.7767e-03],
        [-9.5844e-05,  1.4114e-04,  5.8699e-04,  ..., -5.1260e-04,
         -1.6785e-04, -2.2864e-04],
        [-1.0896e-04,  2.5578e-03, -3.6240e-03,  ..., -2.6054e-03,
          7.7248e-05,  1.4534e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.3027,  0.3691,  0.0586,  ..., -0.0615, -1.2656,  2.3027]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.3274, -0.2164,  0.0403,  ..., -0.1292, -0.0626,  0.1868]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 20:44:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is containing, it has contained
After something is allowing, it has allowed
After something is considering, it has considered
After something is suffering, it has suffered
After something is performing, it has performed
After something is creating, it has created
After something is appointing, it has appointed
After something is losing, it has
2024-07-03 20:44:37 root INFO     [order_1_approx] starting weight calculation for After something is losing, it has lost
After something is creating, it has created
After something is allowing, it has allowed
After something is considering, it has considered
After something is appointing, it has appointed
After something is performing, it has performed
After something is suffering, it has suffered
After something is containing, it has
2024-07-03 20:44:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 20:48:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 20:52:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7754, -0.2771, -0.2805,  ..., -0.0687,  0.0244,  0.8828],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.8359, -0.3313, -0.3074,  ..., -0.0726,  0.0026,  0.9517],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0468, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2566,  2.5957,  0.8750,  ...,  0.6802, -3.5684,  3.3770],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.1215e-02, -1.4526e-02,  1.3161e-02,  ...,  3.8509e-03,
         -7.5531e-03,  2.7485e-03],
        [-4.1580e-04, -4.8866e-03,  6.2485e-03,  ..., -6.7787e-03,
         -1.1925e-02, -5.3787e-03],
        [ 6.5384e-03, -1.7105e-02,  7.3891e-03,  ..., -2.5826e-03,
          1.2093e-02, -7.2136e-03],
        ...,
        [-2.1973e-02, -6.8932e-03, -1.3680e-02,  ..., -1.5442e-02,
          8.1024e-03,  2.5558e-03],
        [ 4.7989e-03, -2.7771e-03,  1.2444e-02,  ...,  4.0054e-05,
         -3.8147e-03,  6.7825e-03],
        [-1.8372e-02,  8.1329e-03,  7.0190e-03,  ..., -1.0590e-02,
         -1.2657e-02, -1.0300e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.1015e-03, -3.1548e-03,  2.8877e-03,  ...,  8.0109e-05,
          1.8301e-03, -1.8024e-04],
        [ 1.8797e-03, -3.3550e-03,  3.0231e-03,  ..., -1.2512e-03,
          8.5068e-04,  1.1625e-03],
        [ 5.7936e-05,  7.2002e-04, -2.1706e-03,  ..., -1.1625e-03,
          2.3689e-03, -1.4806e-04],
        ...,
        [-5.1594e-04, -2.5883e-03,  1.9083e-03,  ..., -2.5005e-03,
         -1.2422e-04, -2.6073e-03],
        [-2.7542e-03,  2.2430e-03,  1.7109e-03,  ...,  9.5654e-04,
         -6.2752e-04,  1.7881e-06],
        [-3.2120e-03,  4.5872e-04, -1.0014e-03,  ..., -3.0613e-04,
         -3.5877e-03, -2.5349e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1885,  1.5596,  0.6636,  ...,  0.2754, -4.2539,  1.2773]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0092, -0.0659,  0.1768,  ..., -0.1458, -0.2625,  0.0296]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 20:52:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is losing, it has lost
After something is creating, it has created
After something is allowing, it has allowed
After something is considering, it has considered
After something is appointing, it has appointed
After something is performing, it has performed
After something is suffering, it has suffered
After something is containing, it has
2024-07-03 20:52:28 root INFO     [order_1_approx] starting weight calculation for After something is containing, it has contained
After something is performing, it has performed
After something is suffering, it has suffered
After something is creating, it has created
After something is allowing, it has allowed
After something is appointing, it has appointed
After something is losing, it has lost
After something is considering, it has
2024-07-03 20:52:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 20:56:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 21:00:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7451, -0.7607, -0.2546,  ...,  0.0082,  0.1531,  0.9492],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.7866, -0.8560, -0.2766,  ...,  0.0064,  0.1365,  1.0039],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0468, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4688,  4.0820, -0.2627,  ...,  0.4546, -4.2109,  0.4717],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.8402e-02, -1.1429e-02,  3.4790e-03,  ...,  1.4442e-02,
         -2.0401e-02, -9.9182e-04],
        [-8.0719e-03, -1.6800e-02, -2.1057e-03,  ..., -8.0109e-05,
          1.2733e-02, -1.1375e-02],
        [ 1.8463e-03, -1.0040e-02, -1.1635e-03,  ...,  1.7166e-05,
          3.9978e-03, -8.1100e-03],
        ...,
        [ 1.1169e-02,  7.4387e-04, -2.5650e-02,  ..., -4.9194e-02,
          1.0178e-02, -1.3962e-03],
        [-5.5161e-03, -1.3329e-02,  1.6296e-02,  ..., -5.3940e-03,
         -1.8463e-02,  1.5091e-02],
        [ 2.1591e-03,  3.6926e-02,  6.4392e-03,  ...,  2.2583e-02,
         -5.5313e-04, -2.8839e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-3.7956e-03,  7.5150e-04,  1.8330e-03,  ...,  1.4153e-03,
         -2.0828e-03, -2.7580e-03],
        [ 4.9162e-04, -1.5039e-03,  1.6193e-03,  ...,  5.2929e-05,
          1.6432e-03, -9.2173e-04],
        [ 1.3800e-03, -8.5926e-04, -2.8267e-03,  ...,  1.4191e-03,
         -1.4281e-04,  7.2861e-04],
        ...,
        [ 3.6602e-03, -6.9714e-04,  5.3787e-04,  ..., -1.3695e-03,
         -1.2636e-04, -2.6360e-03],
        [-1.1930e-03,  9.5940e-04,  2.4605e-03,  ..., -2.4185e-03,
         -3.4561e-03, -9.0837e-05],
        [-6.8378e-04,  4.8943e-03,  8.0204e-04,  ...,  2.1439e-03,
         -1.6317e-03, -1.3990e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3909,  3.3145,  0.0173,  ..., -0.1123, -2.8320,  1.0820]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0573, -0.2157,  0.1035,  ..., -0.1719,  0.0760,  0.2637]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 21:00:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is containing, it has contained
After something is performing, it has performed
After something is suffering, it has suffered
After something is creating, it has created
After something is allowing, it has allowed
After something is appointing, it has appointed
After something is losing, it has lost
After something is considering, it has
2024-07-03 21:00:18 root INFO     [order_1_approx] starting weight calculation for After something is creating, it has created
After something is containing, it has contained
After something is losing, it has lost
After something is allowing, it has allowed
After something is appointing, it has appointed
After something is suffering, it has suffered
After something is considering, it has considered
After something is performing, it has
2024-07-03 21:00:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 21:04:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 21:08:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3750, -0.0781,  0.3669,  ..., -0.5635,  0.7573, -0.3010],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3848, -0.1068,  0.3508,  ..., -0.5679,  0.7593, -0.3516],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0468, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9580,  1.7969,  0.7012,  ...,  3.8672, -0.3450,  2.9082],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 5.3406e-05, -5.0278e-03, -2.3689e-03,  ..., -1.8787e-04,
         -1.5900e-02, -1.4481e-02],
        [-7.5302e-03, -1.8341e-02,  7.6675e-04,  ..., -7.4005e-04,
         -3.0365e-03, -5.5046e-03],
        [ 1.6312e-02, -5.4779e-03, -2.8671e-02,  ..., -9.3079e-04,
          9.4757e-03, -3.2959e-03],
        ...,
        [-2.0386e-02, -1.2726e-02, -1.2199e-02,  ..., -2.5955e-02,
         -1.2466e-02,  1.4679e-02],
        [-1.0124e-02, -5.9509e-04, -4.9667e-03,  ..., -2.1729e-02,
         -3.0792e-02,  1.2711e-02],
        [ 3.2883e-03,  2.1408e-02,  1.4687e-04,  ...,  3.9711e-03,
          1.6708e-03, -1.1749e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-3.6449e-03,  1.0290e-03,  3.8266e-05,  ..., -4.2915e-04,
         -1.6775e-03,  1.1406e-03],
        [ 4.1723e-04,  2.5225e-04,  1.6251e-03,  ..., -1.0920e-04,
          1.0796e-03,  1.2026e-03],
        [ 5.0449e-04,  2.1896e-03, -1.1320e-03,  ..., -1.7226e-04,
          2.2049e-03,  5.4693e-04],
        ...,
        [ 1.5259e-03, -1.9073e-03,  1.3313e-03,  ..., -2.4662e-03,
          1.1492e-03, -8.8310e-04],
        [-7.8964e-04,  1.1129e-03,  2.0599e-04,  ..., -2.6488e-04,
         -2.5902e-03, -2.0866e-03],
        [-1.1997e-03,  7.3314e-05, -2.1019e-03,  ...,  1.0061e-03,
         -1.6537e-03,  6.7377e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4062,  1.3301, -0.2412,  ...,  2.6289, -0.6309,  2.1406]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.3064, -0.0208,  0.1462,  ..., -0.0365, -0.3730, -0.0390]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 21:08:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is creating, it has created
After something is containing, it has contained
After something is losing, it has lost
After something is allowing, it has allowed
After something is appointing, it has appointed
After something is suffering, it has suffered
After something is considering, it has considered
After something is performing, it has
2024-07-03 21:08:08 root INFO     total operator prediction time: 3765.5591535568237 seconds
2024-07-03 21:08:08 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on adj - superlative
2024-07-03 21:08:10 root INFO     building operator adj - superlative
2024-07-03 21:08:10 root INFO     [order_1_approx] starting weight calculation for If something is the most weak, it is weakest
If something is the most hardy, it is hardiest
If something is the most nice, it is nicest
If something is the most angry, it is angriest
If something is the most sad, it is saddest
If something is the most rare, it is rarest
If something is the most fierce, it is fiercest
If something is the most shiny, it is
2024-07-03 21:08:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 21:12:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 21:15:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4988, -0.1484,  0.7856,  ..., -0.3672,  0.9082,  0.3962],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.4519, -0.1549,  0.6567,  ..., -0.3123,  0.7793,  0.3357],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.3711,  0.5781, -1.3408,  ...,  4.6758, -5.5547,  3.3066],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0031, -0.0170, -0.0082,  ...,  0.0079, -0.0040, -0.0151],
        [-0.0016, -0.0077,  0.0048,  ..., -0.0072, -0.0122,  0.0012],
        [ 0.0107,  0.0016,  0.0059,  ..., -0.0016,  0.0109,  0.0003],
        ...,
        [-0.0053, -0.0083, -0.0059,  ..., -0.0176, -0.0054, -0.0035],
        [ 0.0126, -0.0116,  0.0170,  ..., -0.0038, -0.0099,  0.0053],
        [-0.0035, -0.0026,  0.0099,  ..., -0.0142,  0.0025, -0.0039]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.7376e-03, -1.8930e-03, -1.1520e-03,  ...,  2.2793e-04,
         -2.6417e-04, -2.0180e-03],
        [-1.8682e-03, -2.5406e-03,  1.3657e-03,  ...,  9.5987e-04,
          9.4414e-04, -7.3624e-04],
        [ 7.6914e-04, -1.5974e-04,  7.6294e-04,  ...,  3.3975e-04,
         -2.6798e-04,  3.5286e-03],
        ...,
        [ 1.7109e-03, -2.0924e-03,  5.6028e-06,  ..., -2.3651e-03,
          1.6098e-03, -7.6008e-04],
        [ 7.7486e-04,  9.9754e-04,  2.4109e-03,  ..., -2.3422e-03,
         -2.6627e-03,  2.9430e-03],
        [ 2.5129e-04, -7.0572e-05,  6.2227e-04,  ..., -1.0033e-03,
         -2.4567e-03,  9.5367e-07]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.7344,  0.6885, -1.5098,  ...,  4.9883, -5.5938,  3.0996]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1234, -0.1207,  0.2808,  ...,  0.0790, -0.1829,  0.1063]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 21:15:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most weak, it is weakest
If something is the most hardy, it is hardiest
If something is the most nice, it is nicest
If something is the most angry, it is angriest
If something is the most sad, it is saddest
If something is the most rare, it is rarest
If something is the most fierce, it is fiercest
If something is the most shiny, it is
2024-07-03 21:15:58 root INFO     [order_1_approx] starting weight calculation for If something is the most weak, it is weakest
If something is the most rare, it is rarest
If something is the most angry, it is angriest
If something is the most fierce, it is fiercest
If something is the most hardy, it is hardiest
If something is the most nice, it is nicest
If something is the most shiny, it is shiniest
If something is the most sad, it is
2024-07-03 21:15:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 21:19:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 21:23:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3928,  0.0166, -0.4099,  ...,  0.7231,  1.0137, -0.0150],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3633, -0.0014, -0.3767,  ...,  0.6509,  0.9141, -0.0381],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-5.5977,  1.6982,  0.0723,  ...,  2.2930, -0.5605,  5.7891],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.1024e-03,  1.8082e-02,  1.0330e-02,  ...,  7.2594e-03,
          2.3682e-02, -7.9956e-03],
        [-3.8147e-06, -2.1622e-02, -5.3329e-03,  ..., -1.1909e-02,
         -3.3600e-02, -3.9215e-03],
        [ 1.1658e-02, -2.0256e-03, -5.8975e-03,  ..., -2.7332e-03,
         -8.9798e-03,  1.2009e-02],
        ...,
        [-3.0823e-03, -1.7365e-02,  3.8605e-03,  ..., -1.8677e-02,
          1.7567e-03, -9.9182e-03],
        [-5.7831e-03, -4.9591e-03,  2.0874e-02,  ..., -1.6388e-02,
         -4.5013e-03, -9.8724e-03],
        [-1.9875e-03, -2.1103e-02, -1.2825e-02,  ..., -1.4191e-02,
         -3.5583e-02, -1.3443e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-5.1498e-05, -9.5701e-04,  6.6090e-04,  ..., -9.7370e-04,
         -1.9467e-04,  2.0599e-03],
        [-3.1781e-04, -2.1019e-03,  1.0376e-03,  ...,  3.7968e-05,
         -1.4706e-03, -1.2074e-03],
        [ 1.8597e-03, -5.4359e-05, -2.5120e-03,  ...,  6.5231e-04,
          2.2888e-04,  1.5984e-03],
        ...,
        [ 2.6703e-03, -1.0290e-03, -4.9639e-04,  ...,  4.1485e-05,
          7.8297e-04, -1.3027e-03],
        [-8.3685e-04, -3.9816e-04,  9.1314e-04,  ..., -3.5591e-03,
         -9.3699e-04,  3.4046e-04],
        [ 2.7895e-04, -8.9169e-04, -9.6226e-04,  ..., -1.8826e-03,
         -1.5602e-03, -3.5429e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-6.6172,  1.7705, -0.6538,  ...,  1.9219, -0.3921,  5.9375]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1047,  0.0486,  0.1821,  ..., -0.1075, -0.0471,  0.0668]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 21:23:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most weak, it is weakest
If something is the most rare, it is rarest
If something is the most angry, it is angriest
If something is the most fierce, it is fiercest
If something is the most hardy, it is hardiest
If something is the most nice, it is nicest
If something is the most shiny, it is shiniest
If something is the most sad, it is
2024-07-03 21:23:45 root INFO     [order_1_approx] starting weight calculation for If something is the most rare, it is rarest
If something is the most fierce, it is fiercest
If something is the most nice, it is nicest
If something is the most shiny, it is shiniest
If something is the most sad, it is saddest
If something is the most weak, it is weakest
If something is the most hardy, it is hardiest
If something is the most angry, it is
2024-07-03 21:23:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 21:27:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 21:31:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.8037, 0.1503, 0.9751,  ..., 0.4900, 0.0972, 0.7705], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([0.7603, 0.1273, 0.8726,  ..., 0.4490, 0.0705, 0.7227], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7246,  0.4951, -3.3594,  ...,  3.1387,  1.3301,  4.5039],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.5808e-02,  5.1308e-04, -5.2299e-03,  ...,  1.0330e-02,
         -6.1226e-03, -1.0254e-02],
        [-6.3629e-03, -1.3107e-02, -2.2697e-03,  ...,  2.4338e-03,
         -4.1580e-03, -1.0612e-02],
        [ 9.4604e-03,  1.9283e-03,  1.1444e-02,  ...,  7.0038e-03,
          4.5013e-03,  1.2917e-02],
        ...,
        [ 4.7760e-03, -1.7181e-02, -3.2883e-03,  ..., -1.0887e-02,
         -4.3793e-03, -1.1963e-02],
        [-7.7248e-04,  1.6069e-04,  5.8594e-03,  ..., -4.2152e-03,
         -1.9058e-02, -8.5831e-05],
        [-2.2125e-04, -2.3079e-04,  5.6419e-03,  ..., -1.5755e-03,
          4.6158e-04, -9.0179e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.1072e-03, -7.2432e-04, -7.7200e-04,  ..., -5.7459e-05,
         -3.5691e-04, -7.7486e-06],
        [-4.6492e-04, -1.6756e-03,  5.1260e-04,  ...,  9.2888e-04,
         -1.0020e-04, -8.1682e-04],
        [-1.2708e-04, -1.0567e-03,  5.0640e-04,  ...,  0.0000e+00,
          4.8304e-04,  2.8706e-03],
        ...,
        [ 2.6436e-03, -7.7581e-04,  2.4438e-06,  ..., -1.5335e-03,
          4.6635e-04, -9.9182e-04],
        [ 9.5034e-04, -3.8099e-04,  4.9925e-04,  ..., -3.2425e-04,
         -1.8625e-03, -2.3568e-04],
        [ 5.2118e-04, -7.1573e-04,  2.2101e-04,  ..., -2.4796e-04,
         -1.0710e-03, -4.0722e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.8848,  0.4988, -3.7793,  ...,  3.1582,  2.4141,  4.8086]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1467,  0.0501,  0.0233,  ...,  0.0653, -0.0023,  0.0467]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 21:31:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most rare, it is rarest
If something is the most fierce, it is fiercest
If something is the most nice, it is nicest
If something is the most shiny, it is shiniest
If something is the most sad, it is saddest
If something is the most weak, it is weakest
If something is the most hardy, it is hardiest
If something is the most angry, it is
2024-07-03 21:31:34 root INFO     [order_1_approx] starting weight calculation for If something is the most shiny, it is shiniest
If something is the most weak, it is weakest
If something is the most rare, it is rarest
If something is the most hardy, it is hardiest
If something is the most nice, it is nicest
If something is the most angry, it is angriest
If something is the most sad, it is saddest
If something is the most fierce, it is
2024-07-03 21:31:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 21:35:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 21:39:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8110, -0.2434,  0.1908,  ...,  0.4141,  0.5830,  1.4170],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.7407, -0.2499,  0.1534,  ...,  0.3662,  0.5073,  1.3047],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.8633,  1.3223, -3.1855,  ..., -1.3652, -2.5508,  1.8652],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0064, -0.0233,  0.0043,  ..., -0.0026,  0.0019,  0.0030],
        [-0.0101, -0.0027,  0.0031,  ...,  0.0008, -0.0129, -0.0023],
        [ 0.0095,  0.0005,  0.0117,  ...,  0.0075,  0.0011,  0.0150],
        ...,
        [-0.0015, -0.0068,  0.0099,  ..., -0.0079, -0.0152, -0.0160],
        [ 0.0111, -0.0036,  0.0220,  ..., -0.0032, -0.0294,  0.0137],
        [-0.0114,  0.0141, -0.0071,  ..., -0.0154, -0.0081, -0.0150]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.3590e-03, -1.3247e-03,  4.1485e-05,  ...,  5.2547e-04,
          3.0088e-04,  1.4095e-03],
        [-6.7043e-04, -2.9507e-03,  4.7445e-05,  ...,  3.3455e-03,
          1.2903e-03, -1.8559e-03],
        [ 1.9016e-03,  1.7385e-03,  2.9874e-04,  ..., -4.0932e-03,
         -1.0147e-03,  3.6144e-03],
        ...,
        [ 3.2921e-03, -1.7471e-03,  4.5300e-04,  ...,  2.7847e-04,
          2.2774e-03, -3.0098e-03],
        [ 6.7043e-04,  2.5063e-03,  1.9646e-03,  ..., -5.1193e-03,
         -3.3264e-03,  1.9798e-03],
        [-2.4271e-04,  1.7428e-04, -2.4166e-03,  ...,  2.1095e-03,
          1.6403e-04, -2.7580e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.1699,  1.0762, -2.6895,  ..., -1.5127, -2.2656,  1.7100]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1700, -0.2949,  0.4155,  ..., -0.0972, -0.0102, -0.3755]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 21:39:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most shiny, it is shiniest
If something is the most weak, it is weakest
If something is the most rare, it is rarest
If something is the most hardy, it is hardiest
If something is the most nice, it is nicest
If something is the most angry, it is angriest
If something is the most sad, it is saddest
If something is the most fierce, it is
2024-07-03 21:39:21 root INFO     [order_1_approx] starting weight calculation for If something is the most rare, it is rarest
If something is the most nice, it is nicest
If something is the most shiny, it is shiniest
If something is the most angry, it is angriest
If something is the most fierce, it is fiercest
If something is the most weak, it is weakest
If something is the most sad, it is saddest
If something is the most hardy, it is
2024-07-03 21:39:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 21:43:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 21:47:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9326,  0.7681,  0.4373,  ..., -0.4033, -1.0283,  1.3760],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.8687,  0.7188,  0.3799,  ..., -0.3533, -0.9595,  1.2871],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.7949, -3.0977, -1.7295,  ..., -2.8984,  3.6270, 10.1016],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-5.8327e-03, -9.8801e-03, -4.3640e-03,  ..., -3.5172e-03,
         -8.3923e-03, -8.0414e-03],
        [-9.0408e-03, -1.3260e-02, -2.1095e-03,  ...,  8.7585e-03,
         -5.4970e-03, -1.7715e-02],
        [ 1.8568e-03,  4.0245e-03, -4.2343e-03,  ...,  1.3371e-03,
         -7.0572e-04, -1.9550e-05],
        ...,
        [ 6.6757e-04, -5.9280e-03, -1.7357e-03,  ..., -9.7046e-03,
         -6.9275e-03, -9.1400e-03],
        [-5.1842e-03,  1.1398e-02,  3.2673e-03,  ..., -3.4332e-03,
         -1.5884e-02, -1.6041e-03],
        [-1.5495e-02,  8.2169e-03,  6.5918e-03,  ..., -1.9646e-03,
         -2.6169e-03, -4.5471e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 9.8133e-04,  2.3353e-04, -8.7690e-04,  ..., -1.5926e-03,
         -4.9734e-04, -1.3590e-04],
        [-2.4033e-04, -2.0046e-03,  7.8630e-04,  ...,  6.2895e-04,
          8.7547e-04, -2.5749e-03],
        [ 2.8896e-04,  1.9045e-03, -1.4181e-03,  ..., -5.1355e-04,
         -1.0109e-03,  4.2439e-05],
        ...,
        [ 1.4315e-03, -1.9693e-04, -1.2732e-03,  ..., -8.0252e-04,
          3.4499e-04, -2.0199e-03],
        [-4.7755e-04, -2.7061e-04,  1.5068e-03,  ...,  1.7471e-03,
         -6.8808e-04,  7.5102e-04],
        [-3.2163e-04, -9.3818e-05,  1.6251e-03,  ...,  2.8934e-03,
          5.2691e-04,  7.3719e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.7871, -3.2793, -1.8984,  ..., -2.7539,  3.0117, 10.2734]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.4094, -0.2147,  0.3242,  ...,  0.0066, -0.1703,  0.1348]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 21:47:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most rare, it is rarest
If something is the most nice, it is nicest
If something is the most shiny, it is shiniest
If something is the most angry, it is angriest
If something is the most fierce, it is fiercest
If something is the most weak, it is weakest
If something is the most sad, it is saddest
If something is the most hardy, it is
2024-07-03 21:47:08 root INFO     [order_1_approx] starting weight calculation for If something is the most sad, it is saddest
If something is the most weak, it is weakest
If something is the most nice, it is nicest
If something is the most fierce, it is fiercest
If something is the most angry, it is angriest
If something is the most hardy, it is hardiest
If something is the most shiny, it is shiniest
If something is the most rare, it is
2024-07-03 21:47:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 21:51:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 21:54:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0505,  0.4729, -0.2485,  ..., -0.4788,  0.7432, -0.0589],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.0430,  0.4309, -0.2303,  ..., -0.4202,  0.6587, -0.0790],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.2969, -1.1641,  0.2251,  ...,  4.9023,  1.1670,  4.0234],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0063, -0.0034, -0.0019,  ...,  0.0003,  0.0160, -0.0077],
        [ 0.0008, -0.0120,  0.0012,  ...,  0.0078, -0.0215, -0.0083],
        [ 0.0048, -0.0015,  0.0113,  ..., -0.0141, -0.0063,  0.0179],
        ...,
        [ 0.0058, -0.0169,  0.0118,  ..., -0.0211,  0.0059, -0.0061],
        [ 0.0075, -0.0138,  0.0143,  ..., -0.0103, -0.0107,  0.0055],
        [-0.0089, -0.0030, -0.0032,  ..., -0.0059, -0.0085, -0.0107]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.7862e-03, -5.1451e-04, -4.4322e-04,  ...,  2.5582e-04,
          6.7186e-04, -1.5564e-03],
        [ 5.7280e-05, -1.6336e-03,  1.2436e-03,  ...,  1.0834e-03,
          4.7874e-04, -2.1534e-03],
        [ 2.2850e-03,  6.7902e-04,  2.5630e-04,  ..., -1.3485e-03,
          5.1975e-04,  2.6493e-03],
        ...,
        [ 4.6492e-04, -1.4877e-03,  1.0312e-04,  ..., -9.1553e-04,
          2.7835e-05,  5.4884e-04],
        [ 1.4186e-04, -5.1880e-04, -3.1328e-04,  ...,  3.8576e-04,
         -5.3406e-04,  2.8992e-04],
        [ 1.5373e-03, -3.0208e-04,  3.1376e-04,  ...,  4.7016e-04,
         -6.6614e-04, -7.2336e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.8594, -1.0498,  0.0854,  ...,  5.0039, -0.0635,  4.0938]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1735, -0.1957,  0.2737,  ...,  0.1091, -0.0135,  0.0975]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 21:54:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most sad, it is saddest
If something is the most weak, it is weakest
If something is the most nice, it is nicest
If something is the most fierce, it is fiercest
If something is the most angry, it is angriest
If something is the most hardy, it is hardiest
If something is the most shiny, it is shiniest
If something is the most rare, it is
2024-07-03 21:54:55 root INFO     [order_1_approx] starting weight calculation for If something is the most shiny, it is shiniest
If something is the most rare, it is rarest
If something is the most hardy, it is hardiest
If something is the most weak, it is weakest
If something is the most sad, it is saddest
If something is the most angry, it is angriest
If something is the most fierce, it is fiercest
If something is the most nice, it is
2024-07-03 21:54:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 21:58:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 22:02:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3032, -0.6484, -0.1873,  ..., -0.4209,  0.3857,  0.7383],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3018, -0.6860, -0.1910,  ..., -0.4021,  0.3640,  0.7344],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.9824, -0.2788, -0.2288,  ...,  2.7188,  0.0693,  2.2793],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0041, -0.0169, -0.0013,  ...,  0.0158,  0.0044, -0.0059],
        [ 0.0037, -0.0181, -0.0013,  ...,  0.0049, -0.0077, -0.0078],
        [ 0.0002, -0.0007,  0.0160,  ...,  0.0080,  0.0054,  0.0031],
        ...,
        [-0.0075, -0.0113,  0.0090,  ..., -0.0098,  0.0030, -0.0057],
        [-0.0069, -0.0099, -0.0024,  ..., -0.0061, -0.0182,  0.0166],
        [ 0.0027,  0.0049,  0.0059,  ..., -0.0163, -0.0113,  0.0057]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.0977e-03,  6.9952e-04,  1.8477e-04,  ...,  5.1022e-05,
          2.4080e-05, -1.2703e-03],
        [ 1.2410e-04, -1.7004e-03,  1.5068e-03,  ...,  1.1082e-03,
          1.8539e-03, -1.3599e-03],
        [ 8.2588e-04,  3.8862e-05,  1.1587e-03,  ..., -1.1482e-03,
         -4.9496e-04,  2.3880e-03],
        ...,
        [ 6.5279e-04, -1.2789e-03,  7.2718e-04,  ..., -1.4048e-03,
          1.3351e-03, -2.3766e-03],
        [-7.3338e-04, -9.0456e-04, -1.0500e-03,  ...,  8.5783e-04,
         -1.7185e-03,  1.0815e-03],
        [ 8.2588e-04, -2.1482e-04,  1.3888e-04,  ..., -1.6470e-03,
         -5.2691e-04,  6.7472e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.2344,  0.0605,  0.0483,  ...,  2.5742, -0.4644,  1.5850]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.2690, -0.1458,  0.2915,  ..., -0.2189, -0.0398,  0.2493]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 22:02:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most shiny, it is shiniest
If something is the most rare, it is rarest
If something is the most hardy, it is hardiest
If something is the most weak, it is weakest
If something is the most sad, it is saddest
If something is the most angry, it is angriest
If something is the most fierce, it is fiercest
If something is the most nice, it is
2024-07-03 22:02:42 root INFO     [order_1_approx] starting weight calculation for If something is the most rare, it is rarest
If something is the most fierce, it is fiercest
If something is the most angry, it is angriest
If something is the most nice, it is nicest
If something is the most shiny, it is shiniest
If something is the most hardy, it is hardiest
If something is the most sad, it is saddest
If something is the most weak, it is
2024-07-03 22:02:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 22:06:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 22:10:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8818, -0.1553,  0.1876,  ...,  0.4702, -0.4570, -0.4849],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.8403, -0.1697,  0.1597,  ...,  0.4346, -0.4497, -0.4971],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.9531,  1.0352, -2.4316,  ...,  0.2520,  0.0547,  2.7070],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0004, -0.0114, -0.0115,  ...,  0.0015, -0.0088, -0.0027],
        [-0.0166, -0.0068, -0.0069,  ...,  0.0033,  0.0037, -0.0144],
        [ 0.0064,  0.0095, -0.0057,  ...,  0.0144, -0.0039,  0.0053],
        ...,
        [-0.0052, -0.0094,  0.0042,  ..., -0.0115, -0.0124, -0.0144],
        [ 0.0054, -0.0150,  0.0075,  ..., -0.0232, -0.0179,  0.0055],
        [-0.0056, -0.0048, -0.0021,  ...,  0.0009,  0.0092, -0.0121]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-7.8630e-04,  6.5899e-04,  4.5443e-04,  ...,  9.3651e-04,
         -8.3685e-05, -1.4267e-03],
        [ 1.1110e-03, -3.6869e-03,  1.4515e-03,  ...,  8.5640e-04,
          8.2874e-04, -8.1396e-04],
        [ 1.2102e-03,  2.3232e-03, -1.8663e-03,  ...,  1.0386e-03,
         -1.6212e-04,  1.7805e-03],
        ...,
        [ 3.1209e-04, -2.1191e-03, -9.6321e-04,  ...,  1.7571e-04,
          7.8344e-04, -5.7220e-04],
        [-1.5936e-03,  1.8053e-03, -2.8551e-05,  ..., -3.3045e-04,
          1.1361e-04, -6.1989e-04],
        [ 4.0007e-04, -9.6440e-05, -2.2948e-04,  ...,  1.1511e-03,
         -6.0081e-04, -2.5272e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.4492,  0.4697, -3.1035,  ...,  0.3015,  0.1616,  2.6309]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0944, -0.2473,  0.2700,  ...,  0.0013, -0.0333, -0.0763]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 22:10:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most rare, it is rarest
If something is the most fierce, it is fiercest
If something is the most angry, it is angriest
If something is the most nice, it is nicest
If something is the most shiny, it is shiniest
If something is the most hardy, it is hardiest
If something is the most sad, it is saddest
If something is the most weak, it is
2024-07-03 22:10:31 root INFO     total operator prediction time: 3740.4801528453827 seconds
2024-07-03 22:10:31 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on UK_city - county
2024-07-03 22:10:34 root INFO     building operator UK_city - county
2024-07-03 22:10:34 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of leeds is in the county of yorkshire
In the United Kingdom, the city of chichester is in the county of sussex
In the United Kingdom, the city of carlisle is in the county of cumbria
In the United Kingdom, the city of edinburgh is in the county of lowlands
In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of worcester is in the county of worcestershire
In the United Kingdom, the city of reading is in the county of
2024-07-03 22:10:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 22:14:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 22:18:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1217, -0.0573, -1.0430,  ..., -0.5996, -0.2073, -0.4023],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.1183, -0.0771, -1.0166,  ..., -0.5767, -0.2256, -0.4387],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0240, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.0156,  0.4395, -5.7500,  ..., -1.4189,  1.6572, -2.3379],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0301, -0.0084,  0.0334,  ...,  0.0073,  0.0040,  0.0297],
        [ 0.0164, -0.0149, -0.0151,  ...,  0.0124, -0.0046, -0.0201],
        [-0.0186, -0.0032,  0.0297,  ...,  0.0071,  0.0029,  0.0061],
        ...,
        [-0.0062,  0.0029,  0.0115,  ..., -0.0408, -0.0174, -0.0092],
        [-0.0146, -0.0098,  0.0177,  ..., -0.0020,  0.0055,  0.0048],
        [ 0.0009, -0.0364,  0.0093,  ..., -0.0105,  0.0136, -0.0138]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-4.0359e-03,  1.5459e-03,  9.2030e-04,  ..., -1.4057e-03,
          4.4212e-03,  6.2180e-04],
        [ 7.1812e-04, -1.0738e-03,  2.6035e-04,  ...,  2.0294e-03,
         -3.8362e-04, -1.8444e-03],
        [-2.5997e-03,  3.7613e-03, -1.3046e-03,  ..., -2.1572e-03,
          1.6384e-03,  1.0223e-03],
        ...,
        [-2.5139e-03, -1.5192e-03, -2.7943e-04,  ...,  5.9664e-05,
          2.8038e-03,  2.6560e-04],
        [-3.6297e-03,  1.8597e-05, -1.1730e-04,  ..., -2.0676e-03,
          1.7977e-03, -1.3332e-03],
        [-1.9073e-03,  2.5225e-04,  6.3896e-05,  ..., -1.2941e-03,
         -3.5763e-05,  2.5845e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.1445,  0.0908, -4.5586,  ..., -1.0078,  2.7930, -3.1055]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0375,  0.1425, -0.0181,  ..., -0.0800,  0.1075,  0.1177]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 22:18:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of leeds is in the county of yorkshire
In the United Kingdom, the city of chichester is in the county of sussex
In the United Kingdom, the city of carlisle is in the county of cumbria
In the United Kingdom, the city of edinburgh is in the county of lowlands
In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of worcester is in the county of worcestershire
In the United Kingdom, the city of reading is in the county of
2024-07-03 22:18:24 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of reading is in the county of berkshire
In the United Kingdom, the city of worcester is in the county of worcestershire
In the United Kingdom, the city of carlisle is in the county of cumbria
In the United Kingdom, the city of edinburgh is in the county of lowlands
In the United Kingdom, the city of chichester is in the county of sussex
In the United Kingdom, the city of leeds is in the county of yorkshire
In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of portsmouth is in the county of
2024-07-03 22:18:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 22:22:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 22:26:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6567, -0.3196, -1.0361,  ...,  0.1602, -0.7241,  0.6758],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.6729, -0.3506, -1.0166,  ...,  0.1589, -0.7441,  0.6758],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0240, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8154,  0.2710, -5.2812,  ..., -4.8281,  3.7090,  0.2822],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0031, -0.0067, -0.0056,  ...,  0.0045,  0.0009, -0.0031],
        [-0.0168,  0.0025, -0.0027,  ...,  0.0080, -0.0035,  0.0035],
        [ 0.0421, -0.0110, -0.0190,  ..., -0.0051, -0.0052, -0.0126],
        ...,
        [ 0.0163, -0.0148, -0.0057,  ..., -0.0072, -0.0064,  0.0109],
        [ 0.0045, -0.0048, -0.0113,  ..., -0.0096, -0.0083,  0.0011],
        [-0.0147,  0.0050,  0.0034,  ...,  0.0027,  0.0047, -0.0189]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-5.7983e-04,  7.5674e-04, -5.8699e-04,  ..., -1.7281e-03,
         -4.2582e-04, -5.8794e-04],
        [-1.8358e-04,  4.8304e-04, -1.9217e-04,  ..., -5.2452e-04,
         -8.9312e-04, -1.3371e-03],
        [ 1.4820e-03,  1.1945e-04, -1.7014e-03,  ...,  1.2457e-05,
          7.7629e-04, -3.4666e-04],
        ...,
        [ 8.0967e-04,  9.6178e-04, -1.4648e-03,  ...,  5.7220e-04,
          1.7614e-03,  1.0796e-03],
        [ 7.9393e-04,  1.3838e-03, -5.4455e-04,  ..., -5.7876e-05,
          7.2575e-04,  7.6056e-04],
        [-1.1368e-03,  2.5487e-04,  1.8013e-04,  ...,  3.9530e-04,
          7.7057e-04, -9.2697e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4805, -0.7231, -3.6680,  ..., -4.5273,  4.2695, -0.1262]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2610,  0.1611, -0.0236,  ..., -0.0791,  0.2137,  0.0313]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 22:26:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of reading is in the county of berkshire
In the United Kingdom, the city of worcester is in the county of worcestershire
In the United Kingdom, the city of carlisle is in the county of cumbria
In the United Kingdom, the city of edinburgh is in the county of lowlands
In the United Kingdom, the city of chichester is in the county of sussex
In the United Kingdom, the city of leeds is in the county of yorkshire
In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of portsmouth is in the county of
2024-07-03 22:26:17 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of worcester is in the county of worcestershire
In the United Kingdom, the city of carlisle is in the county of cumbria
In the United Kingdom, the city of leeds is in the county of yorkshire
In the United Kingdom, the city of chichester is in the county of sussex
In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of reading is in the county of berkshire
In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of edinburgh is in the county of
2024-07-03 22:26:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 22:30:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 22:34:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3965, -0.4746,  1.0674,  ...,  0.3833,  0.3650,  0.7002],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.3738, -0.4700,  0.9326,  ...,  0.3435,  0.3140,  0.6382],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0240, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0850, -1.7031, -8.1016,  ...,  3.3750, -0.1689, -2.0020],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0016, -0.0011, -0.0064,  ...,  0.0050, -0.0002, -0.0093],
        [-0.0089, -0.0025, -0.0023,  ...,  0.0072, -0.0048, -0.0023],
        [ 0.0133, -0.0057, -0.0144,  ..., -0.0005, -0.0053,  0.0090],
        ...,
        [-0.0133,  0.0028,  0.0267,  ..., -0.0093, -0.0048,  0.0087],
        [ 0.0048,  0.0019, -0.0013,  ..., -0.0201, -0.0068, -0.0072],
        [ 0.0011, -0.0091, -0.0127,  ..., -0.0149,  0.0188, -0.0126]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 7.8344e-04,  1.7910e-03,  1.7319e-03,  ..., -8.6403e-04,
         -1.3351e-03,  4.2725e-04],
        [-1.2217e-03, -8.1158e-04,  1.5688e-03,  ...,  7.4506e-05,
          1.1909e-04, -2.3460e-04],
        [ 1.3094e-03,  2.0256e-03, -9.0790e-04,  ...,  1.4400e-03,
          1.2627e-03,  4.7684e-03],
        ...,
        [-1.8835e-03, -1.1635e-03,  1.0614e-03,  ..., -7.6580e-04,
          5.1022e-05,  2.0957e-04],
        [ 9.0408e-04,  3.0575e-03, -1.6439e-04,  ..., -7.0429e-04,
         -7.5638e-05,  4.3964e-04],
        [-1.9455e-04, -3.5453e-04, -1.5283e-04,  ..., -9.7752e-06,
         -1.0138e-03, -2.6822e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0483, -1.9512, -7.1016,  ...,  1.6514,  0.4194, -2.4727]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2705,  0.0382, -0.1190,  ..., -0.0126,  0.0349,  0.1569]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 22:34:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of worcester is in the county of worcestershire
In the United Kingdom, the city of carlisle is in the county of cumbria
In the United Kingdom, the city of leeds is in the county of yorkshire
In the United Kingdom, the city of chichester is in the county of sussex
In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of reading is in the county of berkshire
In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of edinburgh is in the county of
2024-07-03 22:34:03 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of chichester is in the county of sussex
In the United Kingdom, the city of leeds is in the county of yorkshire
In the United Kingdom, the city of reading is in the county of berkshire
In the United Kingdom, the city of worcester is in the county of worcestershire
In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of edinburgh is in the county of lowlands
In the United Kingdom, the city of carlisle is in the county of
2024-07-03 22:34:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 22:38:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 22:41:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7324,  0.1311, -0.5581,  ...,  0.2441,  0.3860, -0.3157],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.7744,  0.1244, -0.5708,  ...,  0.2502,  0.3816, -0.3618],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0240, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3408,  0.5015, -6.3125,  ...,  0.6128, -0.5298,  0.0508],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0064,  0.0080, -0.0083,  ..., -0.0136,  0.0094, -0.0222],
        [-0.0102, -0.0075, -0.0018,  ...,  0.0167, -0.0066,  0.0054],
        [ 0.0348, -0.0020, -0.0141,  ..., -0.0091, -0.0022, -0.0115],
        ...,
        [-0.0026, -0.0030,  0.0182,  ...,  0.0062,  0.0012,  0.0167],
        [ 0.0116,  0.0058,  0.0062,  ..., -0.0069,  0.0041,  0.0028],
        [-0.0146, -0.0051, -0.0100,  ..., -0.0105,  0.0021,  0.0037]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 6.1631e-05, -8.0395e-04, -2.4045e-04,  ...,  4.1103e-04,
         -2.3532e-04, -1.1797e-03],
        [-2.4872e-03,  2.2483e-04,  7.8773e-04,  ...,  6.8378e-04,
          1.8511e-03, -5.1212e-04],
        [ 1.1215e-03,  1.0147e-03, -2.5392e-04,  ..., -1.7471e-03,
         -3.7956e-04,  2.1687e-03],
        ...,
        [ 2.0826e-04,  3.5524e-05, -4.5156e-04,  ...,  1.5354e-04,
         -3.6073e-04,  1.4696e-03],
        [-1.7428e-04,  7.9727e-04,  8.2302e-04,  ...,  6.6757e-04,
          1.0834e-03,  1.4400e-03],
        [-2.3246e-04, -1.7166e-04, -1.1845e-03,  ..., -1.0643e-03,
          1.3170e-03,  1.8072e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2017, -0.3804, -6.1484,  ...,  0.1704, -0.5957, -0.1908]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2408, -0.0365,  0.2177,  ..., -0.1946,  0.2664,  0.1857]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 22:41:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of chichester is in the county of sussex
In the United Kingdom, the city of leeds is in the county of yorkshire
In the United Kingdom, the city of reading is in the county of berkshire
In the United Kingdom, the city of worcester is in the county of worcestershire
In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of edinburgh is in the county of lowlands
In the United Kingdom, the city of carlisle is in the county of
2024-07-03 22:41:55 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of carlisle is in the county of cumbria
In the United Kingdom, the city of edinburgh is in the county of lowlands
In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of reading is in the county of berkshire
In the United Kingdom, the city of chichester is in the county of sussex
In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of leeds is in the county of yorkshire
In the United Kingdom, the city of worcester is in the county of
2024-07-03 22:41:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 22:45:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 22:49:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0010, -0.8633,  0.9946,  ...,  0.8086, -0.1526,  1.1660],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.9365, -0.8403,  0.8691,  ...,  0.7227, -0.1598,  1.0801],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0240, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7754,  0.7803, -6.3320,  ...,  1.3867,  6.7031, -2.2109],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0047, -0.0008, -0.0183,  ...,  0.0078, -0.0048, -0.0101],
        [-0.0164, -0.0091,  0.0077,  ...,  0.0027, -0.0060, -0.0029],
        [ 0.0504,  0.0300, -0.0459,  ...,  0.0039, -0.0047, -0.0062],
        ...,
        [ 0.0235, -0.0103, -0.0099,  ..., -0.0075, -0.0060,  0.0070],
        [-0.0140,  0.0074, -0.0044,  ..., -0.0065, -0.0135,  0.0105],
        [ 0.0030, -0.0096, -0.0113,  ...,  0.0044,  0.0118, -0.0112]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 1.2894e-03, -6.5684e-05, -1.0529e-03,  ...,  2.1768e-04,
         -1.5087e-03, -5.3453e-04],
        [-7.6723e-04,  1.3483e-04,  4.1389e-04,  ...,  5.3644e-04,
          2.1362e-03, -2.1553e-03],
        [ 6.8569e-04,  7.8773e-04, -3.9363e-04,  ...,  5.2881e-04,
         -2.9831e-03,  9.4700e-04],
        ...,
        [ 9.4509e-04,  3.0684e-04, -6.0177e-04,  ...,  3.1137e-04,
          7.2002e-04,  3.1233e-04],
        [-8.2207e-04,  2.8992e-04, -4.8113e-04,  ...,  1.8549e-04,
         -8.9073e-04,  4.8232e-04],
        [ 3.1137e-04,  3.8743e-04, -1.2255e-03,  ...,  7.4911e-04,
         -1.0557e-03, -7.7534e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0488,  0.5352, -2.4531,  ...,  2.2227,  6.4375, -2.0293]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1177,  0.0598, -0.1597,  ..., -0.1788,  0.2362,  0.0152]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 22:49:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of carlisle is in the county of cumbria
In the United Kingdom, the city of edinburgh is in the county of lowlands
In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of reading is in the county of berkshire
In the United Kingdom, the city of chichester is in the county of sussex
In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of leeds is in the county of yorkshire
In the United Kingdom, the city of worcester is in the county of
2024-07-03 22:49:45 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of edinburgh is in the county of lowlands
In the United Kingdom, the city of carlisle is in the county of cumbria
In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of reading is in the county of berkshire
In the United Kingdom, the city of leeds is in the county of yorkshire
In the United Kingdom, the city of worcester is in the county of worcestershire
In the United Kingdom, the city of chichester is in the county of
2024-07-03 22:49:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 22:53:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 22:57:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1729, -0.7358, -1.1094,  ..., -0.4434,  1.0664, -0.1755],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.1647, -0.7397, -1.0264,  ..., -0.4023,  0.9937, -0.1936],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0240, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3301,  0.0908, -5.8555,  ...,  0.1223,  6.5117,  0.3447],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0195,  0.0042, -0.0024,  ...,  0.0052, -0.0098, -0.0152],
        [-0.0297,  0.0178,  0.0171,  ...,  0.0183, -0.0107, -0.0012],
        [ 0.0195,  0.0097, -0.0267,  ...,  0.0046, -0.0086,  0.0192],
        ...,
        [-0.0046,  0.0079,  0.0060,  ..., -0.0141, -0.0035,  0.0140],
        [ 0.0066,  0.0174, -0.0040,  ..., -0.0058, -0.0030,  0.0175],
        [-0.0193, -0.0118, -0.0032,  ..., -0.0037,  0.0063, -0.0197]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 1.3981e-03,  1.7214e-04, -5.2643e-04,  ...,  8.2302e-04,
         -1.8573e-04, -1.3065e-03],
        [-3.2959e-03, -3.7670e-04,  1.8349e-03,  ...,  7.3624e-04,
          6.1464e-04, -1.4257e-03],
        [ 1.0691e-03,  2.4090e-03, -5.4359e-05,  ...,  1.3578e-04,
         -3.5930e-04, -8.3637e-04],
        ...,
        [ 1.8339e-03,  6.6233e-04,  3.0384e-03,  ...,  6.1226e-04,
          3.3474e-04,  1.0748e-03],
        [ 6.8367e-05,  1.6308e-03,  4.9591e-04,  ...,  5.5432e-06,
          1.2541e-03, -2.3766e-03],
        [ 3.7074e-04,  2.6360e-03, -1.7834e-04,  ...,  3.0804e-04,
          3.8528e-04, -1.2560e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0068, -0.7979, -4.1875,  ..., -0.6328,  5.6641,  0.2485]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2111,  0.0156, -0.1760,  ..., -0.0796,  0.1328,  0.0850]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 22:57:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of edinburgh is in the county of lowlands
In the United Kingdom, the city of carlisle is in the county of cumbria
In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of reading is in the county of berkshire
In the United Kingdom, the city of leeds is in the county of yorkshire
In the United Kingdom, the city of worcester is in the county of worcestershire
In the United Kingdom, the city of chichester is in the county of
2024-07-03 22:57:35 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of carlisle is in the county of cumbria
In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of worcester is in the county of worcestershire
In the United Kingdom, the city of edinburgh is in the county of lowlands
In the United Kingdom, the city of reading is in the county of berkshire
In the United Kingdom, the city of chichester is in the county of sussex
In the United Kingdom, the city of leeds is in the county of
2024-07-03 22:57:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 23:01:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 23:05:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2168,  0.7705,  0.9395,  ..., -0.1362, -0.3799, -0.6660],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-1.1406,  0.7207,  0.8262,  ..., -0.1171, -0.3672, -0.6572],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0240, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2603, -4.8438, -1.9531,  ...,  3.3438,  1.8516, -1.4863],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0067,  0.0055, -0.0050,  ...,  0.0087, -0.0040,  0.0041],
        [-0.0072, -0.0122, -0.0067,  ...,  0.0077,  0.0005, -0.0009],
        [ 0.0277,  0.0207, -0.0115,  ...,  0.0121, -0.0164, -0.0113],
        ...,
        [-0.0173, -0.0316,  0.0020,  ...,  0.0005, -0.0057,  0.0142],
        [ 0.0043,  0.0081,  0.0026,  ..., -0.0227, -0.0084, -0.0017],
        [-0.0133, -0.0051, -0.0081,  ...,  0.0102,  0.0143, -0.0111]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-8.9598e-04,  7.0095e-04, -9.2268e-04,  ...,  6.1989e-06,
         -2.0370e-03, -5.1403e-04],
        [ 2.1100e-04,  2.5344e-04, -2.8849e-04,  ...,  1.1082e-03,
         -1.7977e-03, -1.0900e-03],
        [-2.6226e-04,  2.1100e-04,  1.4067e-03,  ...,  1.8663e-03,
          7.4959e-04,  1.2083e-03],
        ...,
        [-1.2350e-03, -2.0802e-04, -1.1234e-03,  ..., -9.2125e-04,
          1.9608e-03,  1.4420e-03],
        [-2.1114e-03,  1.3065e-04,  1.0233e-03,  ...,  1.0643e-03,
          3.4690e-04,  1.4715e-03],
        [ 1.8930e-04, -1.0710e-03, -1.5640e-03,  ...,  9.4318e-04,
          4.5133e-04, -1.5450e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3291, -4.7812, -0.1328,  ...,  1.8350,  2.5996, -2.2559]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0800, -0.1356, -0.0074,  ..., -0.0977,  0.1160, -0.2147]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 23:05:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of carlisle is in the county of cumbria
In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of worcester is in the county of worcestershire
In the United Kingdom, the city of edinburgh is in the county of lowlands
In the United Kingdom, the city of reading is in the county of berkshire
In the United Kingdom, the city of chichester is in the county of sussex
In the United Kingdom, the city of leeds is in the county of
2024-07-03 23:05:21 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of leeds is in the county of yorkshire
In the United Kingdom, the city of chichester is in the county of sussex
In the United Kingdom, the city of carlisle is in the county of cumbria
In the United Kingdom, the city of worcester is in the county of worcestershire
In the United Kingdom, the city of reading is in the county of berkshire
In the United Kingdom, the city of edinburgh is in the county of lowlands
In the United Kingdom, the city of crawley is in the county of
2024-07-03 23:05:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 23:09:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 23:13:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1343,  0.4648,  0.1348,  ...,  1.7529,  0.3162,  1.0244],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1550,  0.5225,  0.1350,  ...,  1.9053,  0.3347,  1.1592],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0240, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8115, -3.6172, -4.5703,  ..., -3.2012,  3.9258, -4.0508],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0192,  0.0331,  0.0125,  ...,  0.0084,  0.0172, -0.0024],
        [-0.0203,  0.0001, -0.0110,  ...,  0.0165, -0.0142, -0.0018],
        [-0.0020,  0.0221,  0.0102,  ...,  0.0054, -0.0124, -0.0031],
        ...,
        [ 0.0072, -0.0066,  0.0021,  ..., -0.0028,  0.0047,  0.0075],
        [-0.0042, -0.0117,  0.0051,  ...,  0.0017, -0.0113,  0.0088],
        [ 0.0163, -0.0150, -0.0139,  ...,  0.0037,  0.0115,  0.0146]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 8.8310e-04,  7.4482e-04,  6.6757e-05,  ...,  3.4928e-04,
         -1.1539e-03,  2.9421e-04],
        [-1.7989e-04, -8.2588e-04,  7.3862e-04,  ..., -4.4918e-04,
         -1.2946e-04, -9.8991e-04],
        [ 3.4475e-04,  2.1591e-03, -1.8311e-04,  ..., -1.0614e-03,
         -8.7547e-04,  3.6049e-04],
        ...,
        [-1.8835e-04, -1.7595e-03, -5.6362e-04,  ...,  8.8573e-05,
          8.2922e-04, -1.2398e-04],
        [-1.2693e-03, -4.0412e-04,  1.5202e-03,  ...,  6.8188e-04,
          4.1509e-04,  8.8930e-04],
        [-1.0204e-04, -6.5088e-04, -3.3474e-04,  ..., -1.4935e-03,
          1.6098e-03, -1.1969e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0146, -3.3125, -3.6777,  ..., -2.2383,  3.6816, -2.6367]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1396,  0.0195, -0.0064,  ..., -0.1039, -0.0066,  0.2107]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 23:13:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of leeds is in the county of yorkshire
In the United Kingdom, the city of chichester is in the county of sussex
In the United Kingdom, the city of carlisle is in the county of cumbria
In the United Kingdom, the city of worcester is in the county of worcestershire
In the United Kingdom, the city of reading is in the county of berkshire
In the United Kingdom, the city of edinburgh is in the county of lowlands
In the United Kingdom, the city of crawley is in the county of
2024-07-03 23:13:05 root INFO     total operator prediction time: 3751.920312643051 seconds
2024-07-03 23:13:05 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on animal - sound
2024-07-03 23:13:08 root INFO     building operator animal - sound
2024-07-03 23:13:08 root INFO     [order_1_approx] starting weight calculation for The sound that a monkey makes is called a chatter
The sound that a raven makes is called a caw
The sound that a elephant makes is called a trumpet
The sound that a pig makes is called a oink
The sound that a cat makes is called a meow
The sound that a hyena makes is called a laugh
The sound that a duck makes is called a quack
The sound that a alpaca makes is called a
2024-07-03 23:13:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 23:17:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 23:20:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6929, -0.4224, -0.0798,  ...,  0.9570,  0.3774,  1.8965],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.6382, -0.4119, -0.0789,  ...,  0.8481,  0.3252,  1.7549],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3457, -0.0327, -5.3359,  ...,  0.6367,  1.3721, -3.7852],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0050, -0.0111,  0.0037,  ..., -0.0043, -0.0010, -0.0044],
        [-0.0084,  0.0126,  0.0073,  ...,  0.0072, -0.0060, -0.0008],
        [ 0.0054,  0.0058, -0.0198,  ..., -0.0152, -0.0015,  0.0055],
        ...,
        [-0.0068, -0.0058, -0.0019,  ...,  0.0111, -0.0014,  0.0031],
        [-0.0009, -0.0028, -0.0016,  ..., -0.0003, -0.0016,  0.0051],
        [ 0.0003, -0.0055, -0.0112,  ..., -0.0126, -0.0028, -0.0024]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 6.3062e-05, -9.2220e-04,  2.3985e-04,  ...,  4.5729e-04,
          1.2884e-03,  8.4782e-04],
        [ 4.8566e-04,  6.7139e-04,  1.2159e-05,  ...,  3.9148e-04,
         -6.6185e-04, -5.8222e-04],
        [ 1.3847e-03,  1.8635e-03, -7.6485e-04,  ...,  1.3137e-04,
         -1.1244e-03,  8.9550e-04],
        ...,
        [ 1.0300e-04, -5.0831e-04, -2.7871e-04,  ..., -1.1337e-04,
          1.3151e-03, -2.1858e-03],
        [-5.3978e-04,  1.1339e-03,  8.3828e-04,  ...,  3.9124e-04,
         -4.1246e-04,  7.3075e-05],
        [-8.5974e-04, -2.5153e-04,  1.3638e-04,  ..., -8.5306e-04,
          1.6756e-03,  8.0109e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9209, -0.3279, -5.4062,  ...,  0.2151,  0.8545, -3.8867]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1445, -0.1450,  0.1697,  ..., -0.0603, -0.0573, -0.0229]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 23:20:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a monkey makes is called a chatter
The sound that a raven makes is called a caw
The sound that a elephant makes is called a trumpet
The sound that a pig makes is called a oink
The sound that a cat makes is called a meow
The sound that a hyena makes is called a laugh
The sound that a duck makes is called a quack
The sound that a alpaca makes is called a
2024-07-03 23:20:50 root INFO     [order_1_approx] starting weight calculation for The sound that a alpaca makes is called a bray
The sound that a elephant makes is called a trumpet
The sound that a cat makes is called a meow
The sound that a monkey makes is called a chatter
The sound that a hyena makes is called a laugh
The sound that a pig makes is called a oink
The sound that a raven makes is called a caw
The sound that a duck makes is called a
2024-07-03 23:20:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 23:24:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 23:28:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7852, -1.0391, -0.2651,  ...,  0.9863, -1.1299,  0.7012],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.6318, -0.8774, -0.2155,  ...,  0.7686, -0.9185,  0.5552],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.7168,  3.4102,  1.1982,  ...,  3.4297, -1.1699, -5.7578],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0281, -0.0055,  0.0146,  ...,  0.0079,  0.0058,  0.0143],
        [-0.0081,  0.0196,  0.0097,  ...,  0.0107, -0.0134, -0.0062],
        [-0.0111, -0.0020, -0.0028,  ..., -0.0175, -0.0099,  0.0154],
        ...,
        [ 0.0030, -0.0029, -0.0024,  ..., -0.0132, -0.0194,  0.0072],
        [ 0.0047, -0.0283,  0.0021,  ...,  0.0082,  0.0273, -0.0198],
        [ 0.0018,  0.0006,  0.0192,  ...,  0.0096,  0.0085, -0.0135]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.7657e-03,  2.1439e-03,  8.2111e-04,  ..., -3.3207e-03,
         -9.6321e-04,  8.4400e-04],
        [ 9.7370e-04, -1.3411e-04,  2.1248e-03,  ...,  2.0466e-03,
          8.5354e-04, -1.5497e-03],
        [ 1.7624e-03,  1.8759e-03, -2.0242e-04,  ..., -1.3232e-04,
          4.2009e-04,  1.8682e-03],
        ...,
        [ 1.6613e-03,  7.7844e-05,  6.6233e-04,  ...,  5.0306e-04,
          6.0940e-04, -1.1873e-03],
        [ 5.8413e-06,  7.2813e-04, -3.1614e-04,  ...,  3.0017e-04,
         -9.1362e-04, -1.6527e-03],
        [-1.3237e-03,  1.2827e-03,  6.0272e-04,  ..., -1.4477e-03,
          1.2839e-04, -4.0741e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.6777,  2.0234,  0.8809,  ...,  3.4961,  0.0078, -3.6133]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0010,  0.0850,  0.2476,  ..., -0.1074, -0.1260, -0.5181]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 23:28:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a alpaca makes is called a bray
The sound that a elephant makes is called a trumpet
The sound that a cat makes is called a meow
The sound that a monkey makes is called a chatter
The sound that a hyena makes is called a laugh
The sound that a pig makes is called a oink
The sound that a raven makes is called a caw
The sound that a duck makes is called a
2024-07-03 23:28:36 root INFO     [order_1_approx] starting weight calculation for The sound that a cat makes is called a meow
The sound that a duck makes is called a quack
The sound that a raven makes is called a caw
The sound that a alpaca makes is called a bray
The sound that a hyena makes is called a laugh
The sound that a monkey makes is called a chatter
The sound that a elephant makes is called a trumpet
The sound that a pig makes is called a
2024-07-03 23:28:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 23:32:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 23:36:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5566, -0.6270, -0.0853,  ...,  0.2744, -0.3987,  1.7676],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.4907, -0.5781, -0.0818,  ...,  0.2347, -0.3616,  1.5547],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.5078,  1.9258, -4.3711,  ..., -1.6074, -1.9785, -4.6953],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0132,  0.0080, -0.0103,  ...,  0.0170,  0.0078,  0.0188],
        [-0.0087,  0.0056,  0.0108,  ..., -0.0003, -0.0163, -0.0145],
        [-0.0043,  0.0037, -0.0064,  ..., -0.0164,  0.0086,  0.0045],
        ...,
        [ 0.0014, -0.0018,  0.0003,  ...,  0.0021,  0.0008, -0.0047],
        [-0.0053, -0.0060,  0.0042,  ..., -0.0093,  0.0102, -0.0127],
        [ 0.0140,  0.0019, -0.0036,  ..., -0.0128,  0.0133, -0.0174]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 2.4281e-03, -1.0085e-04, -4.3583e-04,  ...,  1.4772e-03,
          5.9080e-04,  1.4477e-03],
        [ 1.1358e-03, -2.7618e-03, -4.1723e-04,  ..., -2.0142e-03,
          3.9625e-04,  4.0817e-04],
        [ 2.0561e-03,  1.4219e-03,  4.5180e-04,  ..., -2.4567e-03,
         -5.9795e-04,  5.4717e-05],
        ...,
        [ 6.6710e-04, -1.9951e-03, -8.0228e-05,  ..., -2.4891e-03,
          3.0804e-03, -2.8286e-03],
        [ 7.1096e-04,  2.7828e-03,  9.2506e-04,  ...,  4.1366e-04,
         -1.6575e-03,  3.4118e-04],
        [ 1.6117e-03,  2.5082e-04, -1.0509e-03,  ..., -5.2023e-04,
         -1.0796e-03, -1.3971e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.7832,  2.2559, -4.4180,  ..., -1.6143, -1.4043, -4.3633]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0966,  0.0981,  0.0825,  ..., -0.1893,  0.1129, -0.0620]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 23:36:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a cat makes is called a meow
The sound that a duck makes is called a quack
The sound that a raven makes is called a caw
The sound that a alpaca makes is called a bray
The sound that a hyena makes is called a laugh
The sound that a monkey makes is called a chatter
The sound that a elephant makes is called a trumpet
The sound that a pig makes is called a
2024-07-03 23:36:22 root INFO     [order_1_approx] starting weight calculation for The sound that a pig makes is called a oink
The sound that a monkey makes is called a chatter
The sound that a cat makes is called a meow
The sound that a alpaca makes is called a bray
The sound that a elephant makes is called a trumpet
The sound that a duck makes is called a quack
The sound that a raven makes is called a caw
The sound that a hyena makes is called a
2024-07-03 23:36:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 23:40:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 23:44:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8994, -0.4575, -0.7266,  ..., -0.2888, -1.2637,  0.3564],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.7671, -0.4155, -0.6016,  ..., -0.2306, -1.0801,  0.2888],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0518, -0.4568, -1.8184,  ..., -2.0859, -2.6172, -2.0957],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.1624e-03,  1.4572e-03,  1.8463e-02,  ...,  1.4114e-02,
          6.8474e-03, -1.7776e-02],
        [-9.5825e-03,  1.4206e-02, -7.6332e-03,  ..., -1.6518e-03,
          3.7613e-03, -4.3716e-03],
        [-6.9504e-03, -2.1973e-03,  1.0315e-02,  ...,  3.4447e-03,
         -4.2725e-04,  2.1301e-02],
        ...,
        [-4.7569e-03, -7.1678e-03, -4.1962e-05,  ...,  2.5749e-03,
         -1.0384e-02,  2.4719e-03],
        [-1.2360e-02, -3.8391e-02, -1.4786e-02,  ..., -2.2339e-02,
          5.0232e-02,  2.4689e-02],
        [-1.8250e-02, -1.0605e-03, -1.4450e-02,  ..., -1.8875e-02,
          1.6449e-02,  2.7618e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.1206e-03, -2.9755e-04,  1.2836e-03,  ...,  8.6975e-04,
          4.9973e-04, -1.3275e-03],
        [ 3.9053e-04,  1.5144e-03, -9.0599e-04,  ..., -5.7411e-04,
         -8.0681e-04,  1.2798e-03],
        [ 2.2011e-03,  1.1616e-03, -1.4553e-03,  ...,  1.1606e-03,
         -2.2469e-03,  1.8311e-04],
        ...,
        [-7.8821e-04, -5.2333e-05, -7.3385e-04,  ..., -3.3855e-04,
          1.7428e-04, -1.4696e-03],
        [-1.6632e-03,  8.3971e-04,  1.4296e-03,  ..., -5.1880e-04,
         -7.2384e-04, -1.1463e-03],
        [-9.2316e-04, -1.1909e-04,  7.9823e-04,  ...,  1.3819e-03,
         -9.9754e-04,  3.5977e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0276, -1.1738, -1.6289,  ..., -0.5811, -0.8877, -0.1631]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1702,  0.1294,  0.1425,  ..., -0.1255, -0.1566, -0.1628]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 23:44:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a pig makes is called a oink
The sound that a monkey makes is called a chatter
The sound that a cat makes is called a meow
The sound that a alpaca makes is called a bray
The sound that a elephant makes is called a trumpet
The sound that a duck makes is called a quack
The sound that a raven makes is called a caw
The sound that a hyena makes is called a
2024-07-03 23:44:09 root INFO     [order_1_approx] starting weight calculation for The sound that a hyena makes is called a laugh
The sound that a raven makes is called a caw
The sound that a pig makes is called a oink
The sound that a cat makes is called a meow
The sound that a duck makes is called a quack
The sound that a monkey makes is called a chatter
The sound that a alpaca makes is called a bray
The sound that a elephant makes is called a
2024-07-03 23:44:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 23:48:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 23:51:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4485, -0.7271,  0.1555,  ...,  0.9482, -0.0496,  0.7939],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.3752, -0.6318,  0.1085,  ...,  0.7485, -0.0610,  0.6396],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.0195, -0.0488, -1.8438,  ..., -2.6855, -3.1992, -2.9883],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.0323e-02, -4.9515e-03, -5.5923e-03,  ...,  6.1264e-03,
          8.3160e-03,  2.0523e-02],
        [ 1.4122e-02, -6.1035e-05, -1.6205e-02,  ...,  1.4282e-02,
         -2.1332e-02,  1.1559e-02],
        [-8.2245e-03,  1.2054e-02, -1.6617e-02,  ..., -2.1744e-03,
         -7.6370e-03,  2.4090e-03],
        ...,
        [-4.8523e-03, -3.9520e-03,  2.7405e-02,  ...,  3.4962e-03,
          1.0941e-02, -1.2527e-02],
        [-2.0218e-02, -9.2621e-03,  2.6733e-02,  ..., -1.5015e-02,
          1.5602e-02, -7.5684e-03],
        [-2.2278e-03, -2.0370e-03,  1.3092e-02,  ..., -1.6327e-02,
          1.7853e-02, -3.6011e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-3.9625e-04,  1.3447e-04, -9.0218e-04,  ...,  1.1234e-03,
          2.1286e-03,  2.8610e-04],
        [ 5.8746e-04, -2.0924e-03,  1.7166e-03,  ...,  4.3068e-03,
         -9.6416e-04,  1.4734e-03],
        [ 1.2655e-03,  1.0166e-03,  1.0705e-04,  ...,  3.3426e-04,
         -1.0271e-03,  2.6321e-04],
        ...,
        [ 1.2913e-03,  5.6076e-04,  1.4420e-03,  ..., -1.2674e-03,
          1.2321e-03,  1.4782e-04],
        [-2.0199e-03,  1.8072e-03,  1.6584e-03,  ..., -1.0643e-03,
         -2.2526e-03,  2.6360e-03],
        [-7.6771e-04,  2.2030e-04,  1.3161e-03,  ..., -2.3842e-07,
         -5.3215e-04,  2.5311e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.2715, -3.0137, -2.3750,  ..., -0.5059, -0.7402,  0.0547]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2939,  0.4983,  0.0522,  ...,  0.0752,  0.2456, -0.1105]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 23:51:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a hyena makes is called a laugh
The sound that a raven makes is called a caw
The sound that a pig makes is called a oink
The sound that a cat makes is called a meow
The sound that a duck makes is called a quack
The sound that a monkey makes is called a chatter
The sound that a alpaca makes is called a bray
The sound that a elephant makes is called a
2024-07-03 23:51:58 root INFO     [order_1_approx] starting weight calculation for The sound that a hyena makes is called a laugh
The sound that a cat makes is called a meow
The sound that a pig makes is called a oink
The sound that a duck makes is called a quack
The sound that a alpaca makes is called a bray
The sound that a raven makes is called a caw
The sound that a elephant makes is called a trumpet
The sound that a monkey makes is called a
2024-07-03 23:51:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-03 23:55:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-03 23:59:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3706, -1.6387,  0.0599,  ...,  0.3972, -0.0708,  0.7090],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3164, -1.4766,  0.0366,  ...,  0.3325, -0.0811,  0.6025],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8340, -0.3599, -1.3730,  ...,  1.0918, -0.2207, -4.4570],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0154,  0.0094, -0.0231,  ..., -0.0093,  0.0101, -0.0034],
        [-0.0130,  0.0247,  0.0027,  ...,  0.0095, -0.0074, -0.0067],
        [-0.0031,  0.0062, -0.0011,  ...,  0.0057,  0.0008,  0.0105],
        ...,
        [ 0.0010, -0.0148, -0.0121,  ...,  0.0005, -0.0198,  0.0015],
        [-0.0060, -0.0174,  0.0186,  ..., -0.0125,  0.0164,  0.0078],
        [ 0.0034,  0.0017,  0.0017,  ...,  0.0023,  0.0031, -0.0084]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 5.1546e-04,  1.3943e-03, -1.0576e-03,  ..., -1.8921e-03,
         -8.3590e-04, -3.9029e-04],
        [ 3.9768e-04, -1.8129e-03,  2.5482e-03,  ...,  1.3733e-03,
          2.3956e-03,  7.0286e-04],
        [-3.3784e-04, -1.4582e-03,  4.5323e-04,  ...,  2.2659e-03,
          6.1226e-04,  1.2417e-03],
        ...,
        [ 1.4114e-03, -2.5330e-03,  1.8978e-03,  ..., -3.3150e-03,
          1.8387e-03, -7.1526e-05],
        [ 1.3304e-03,  7.3910e-04,  1.9512e-03,  ..., -6.6996e-04,
         -1.9989e-03,  1.2302e-03],
        [ 3.0923e-04,  1.2016e-04,  1.9436e-03,  ...,  3.2463e-03,
         -1.5717e-03,  1.1575e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7539,  0.6646, -0.9756,  ...,  0.7168, -0.1201, -3.6211]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0182,  0.1957, -0.2155,  ..., -0.0089,  0.0555,  0.1246]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-03 23:59:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a hyena makes is called a laugh
The sound that a cat makes is called a meow
The sound that a pig makes is called a oink
The sound that a duck makes is called a quack
The sound that a alpaca makes is called a bray
The sound that a raven makes is called a caw
The sound that a elephant makes is called a trumpet
The sound that a monkey makes is called a
2024-07-03 23:59:46 root INFO     [order_1_approx] starting weight calculation for The sound that a alpaca makes is called a bray
The sound that a duck makes is called a quack
The sound that a cat makes is called a meow
The sound that a monkey makes is called a chatter
The sound that a elephant makes is called a trumpet
The sound that a pig makes is called a oink
The sound that a hyena makes is called a laugh
The sound that a raven makes is called a
2024-07-03 23:59:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 00:03:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 00:07:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3145, -0.9097, -0.3271,  ...,  0.2788, -0.1062,  0.8232],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.2522, -0.7778, -0.2664,  ...,  0.2211, -0.1047,  0.6606],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4883,  1.9424,  0.2949,  ...,  2.6816,  0.9419, -4.1953],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0016, -0.0067, -0.0044,  ...,  0.0002,  0.0112, -0.0081],
        [ 0.0011,  0.0254,  0.0178,  ...,  0.0176, -0.0143, -0.0056],
        [-0.0109,  0.0193,  0.0084,  ...,  0.0062, -0.0129,  0.0064],
        ...,
        [-0.0038,  0.0017,  0.0159,  ...,  0.0129, -0.0049, -0.0153],
        [-0.0130, -0.0323, -0.0046,  ...,  0.0063,  0.0096, -0.0053],
        [-0.0100, -0.0323, -0.0103,  ...,  0.0115, -0.0030, -0.0079]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.7242e-03,  6.6280e-05, -1.0929e-03,  ...,  1.1969e-03,
          1.4830e-03, -2.6274e-04],
        [ 5.1498e-04, -1.1444e-03, -9.9897e-05,  ...,  3.0398e-04,
          9.5749e-04, -1.4105e-03],
        [ 1.5469e-03,  3.3665e-04,  1.2369e-03,  ...,  5.6744e-04,
          1.0595e-03, -1.6680e-03],
        ...,
        [ 1.9798e-03, -3.1033e-03, -3.4618e-04,  ..., -2.9125e-03,
          1.5354e-03, -3.3798e-03],
        [ 6.3801e-04, -9.8133e-04,  1.6236e-04,  ..., -7.3290e-04,
         -2.6894e-03,  4.2486e-04],
        [ 8.9645e-04, -1.0052e-03, -1.7529e-03,  ...,  1.4629e-03,
         -2.1248e-03, -2.1858e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7251,  2.2070, -0.1855,  ...,  5.3398,  2.5957, -2.9570]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2507,  0.0641,  0.2546,  ...,  0.1230, -0.5332,  0.1621]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 00:07:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a alpaca makes is called a bray
The sound that a duck makes is called a quack
The sound that a cat makes is called a meow
The sound that a monkey makes is called a chatter
The sound that a elephant makes is called a trumpet
The sound that a pig makes is called a oink
The sound that a hyena makes is called a laugh
The sound that a raven makes is called a
2024-07-04 00:07:33 root INFO     [order_1_approx] starting weight calculation for The sound that a hyena makes is called a laugh
The sound that a pig makes is called a oink
The sound that a raven makes is called a caw
The sound that a duck makes is called a quack
The sound that a alpaca makes is called a bray
The sound that a monkey makes is called a chatter
The sound that a elephant makes is called a trumpet
The sound that a cat makes is called a
2024-07-04 00:07:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 00:11:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 00:15:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2563, -0.4546,  0.1318,  ...,  0.5522, -0.0122,  1.3418],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.2379, -0.4539,  0.1075,  ...,  0.5020, -0.0292,  1.2627],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.0352,  2.1738,  3.9336,  ..., -1.1182, -3.4141, -6.9805],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-4.2953e-03, -1.1299e-02, -1.0124e-02,  ..., -4.5738e-03,
         -2.7390e-03, -1.1650e-02],
        [ 1.1177e-02,  1.0109e-02,  1.1826e-02,  ...,  1.3199e-02,
         -1.6083e-02, -2.1858e-03],
        [-2.6001e-02,  2.2522e-02,  6.1035e-05,  ..., -5.8479e-03,
         -2.7161e-03,  1.5915e-02],
        ...,
        [-9.6436e-03, -1.1749e-02,  1.1635e-02,  ...,  9.1553e-05,
         -1.9257e-02,  2.0432e-02],
        [-1.3519e-02, -6.6109e-03,  8.2397e-03,  ..., -5.7983e-03,
          7.1716e-03,  1.2238e-02],
        [-6.3553e-03, -5.3940e-03, -2.4261e-03,  ...,  6.3629e-03,
          4.5090e-03, -2.3376e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-5.1928e-04, -4.8351e-04,  2.3899e-03,  ...,  9.7752e-05,
         -8.3351e-04, -1.1549e-03],
        [ 1.2493e-03,  9.0897e-05, -9.9373e-04,  ..., -1.0138e-03,
          1.7011e-04, -4.3964e-04],
        [ 4.5776e-05,  2.1195e-04,  1.8196e-03,  ...,  1.0166e-03,
         -1.7710e-03,  9.6560e-04],
        ...,
        [-6.3610e-04, -1.6890e-03,  1.4420e-03,  ..., -3.9597e-03,
          1.5354e-03, -7.8583e-04],
        [-1.6999e-04,  1.6975e-03,  3.1853e-04,  ..., -2.1667e-03,
          3.9458e-04,  2.5749e-03],
        [ 1.5335e-03, -4.0531e-04, -3.4285e-04,  ..., -1.1778e-03,
          9.1553e-04,  5.1737e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1309,  2.0215,  3.0176,  ..., -1.1865, -2.5547, -5.1094]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.3025,  0.1193, -0.0306,  ..., -0.0735, -0.0189, -0.2898]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 00:15:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a hyena makes is called a laugh
The sound that a pig makes is called a oink
The sound that a raven makes is called a caw
The sound that a duck makes is called a quack
The sound that a alpaca makes is called a bray
The sound that a monkey makes is called a chatter
The sound that a elephant makes is called a trumpet
The sound that a cat makes is called a
2024-07-04 00:15:19 root INFO     total operator prediction time: 3731.361686706543 seconds
2024-07-04 00:15:19 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on male - female
2024-07-04 00:15:21 root INFO     building operator male - female
2024-07-04 00:15:21 root INFO     [order_1_approx] starting weight calculation for A female poet is known as a poetess
A female groom is known as a bride
A female policeman is known as a policewoman
A female murderer is known as a murderess
A female daddy is known as a mommy
A female hero is known as a heroine
A female fisherman is known as a fisherwoman
A female superman is known as a
2024-07-04 00:15:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 00:19:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 00:23:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0757,  0.4001,  0.4121,  ..., -0.1616, -1.1406, -0.9287],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.0709,  0.3599,  0.3501,  ..., -0.1355, -1.0439, -0.8872],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1797, -0.5898, -4.3789,  ..., -2.5449, -2.1836,  2.1738],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 9.2850e-03, -1.2497e-02, -2.0714e-03,  ..., -1.0880e-02,
          2.0660e-02, -2.7283e-02],
        [ 1.2665e-03, -7.2441e-03,  1.3550e-02,  ...,  2.6306e-02,
         -7.1144e-03,  1.5915e-02],
        [ 1.5373e-02,  1.7685e-02, -3.1311e-02,  ..., -8.9951e-03,
          7.6790e-03, -1.4591e-03],
        ...,
        [-3.3417e-03, -2.9869e-03,  1.9569e-03,  ...,  1.6575e-03,
         -1.8616e-02,  1.2291e-02],
        [ 6.6757e-04, -7.2784e-03,  2.6436e-03,  ..., -1.7357e-03,
         -4.5776e-03, -4.7684e-06],
        [-1.9073e-02,  3.0766e-03,  5.8441e-03,  ...,  2.0432e-02,
         -1.7471e-03,  5.4169e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-3.4790e-03, -1.4524e-03,  4.8399e-05,  ...,  8.0156e-04,
         -6.4230e-04, -2.6474e-03],
        [-2.1000e-03, -3.1948e-05,  2.2564e-03,  ...,  2.0905e-03,
         -1.7681e-03,  3.3617e-05],
        [ 6.7616e-04,  4.4708e-03,  1.6928e-05,  ...,  1.7529e-03,
         -2.3117e-03, -1.3285e-03],
        ...,
        [ 3.3188e-04,  1.3609e-03, -2.4300e-03,  ..., -2.0351e-03,
          2.8133e-03, -1.1473e-03],
        [ 6.4754e-04, -1.5297e-03, -1.7748e-03,  ..., -1.6174e-03,
          1.1606e-03, -2.4414e-04],
        [-9.8419e-04,  6.0225e-04,  1.6270e-03,  ...,  1.1358e-03,
          4.4763e-05, -9.1648e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0183,  0.0527, -4.2852,  ..., -2.8633, -0.7686,  1.0146]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0178, -0.0492,  0.0716,  ...,  0.1600, -0.0164,  0.0093]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 00:23:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female poet is known as a poetess
A female groom is known as a bride
A female policeman is known as a policewoman
A female murderer is known as a murderess
A female daddy is known as a mommy
A female hero is known as a heroine
A female fisherman is known as a fisherwoman
A female superman is known as a
2024-07-04 00:23:10 root INFO     [order_1_approx] starting weight calculation for A female fisherman is known as a fisherwoman
A female policeman is known as a policewoman
A female daddy is known as a mommy
A female groom is known as a bride
A female murderer is known as a murderess
A female hero is known as a heroine
A female superman is known as a superwoman
A female poet is known as a
2024-07-04 00:23:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 00:27:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 00:30:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4575, -0.7266,  0.3433,  ...,  0.5713, -0.1663,  0.4824],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3923, -0.6602,  0.2720,  ...,  0.4775, -0.1600,  0.4021],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6113,  2.2246, -7.3477,  ..., -0.0388, -0.4736, -3.5312],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0029,  0.0052,  0.0143,  ...,  0.0194,  0.0070, -0.0121],
        [-0.0072, -0.0077, -0.0004,  ...,  0.0063, -0.0041,  0.0037],
        [ 0.0309,  0.0120,  0.0009,  ...,  0.0082,  0.0080, -0.0052],
        ...,
        [-0.0012, -0.0087,  0.0105,  ..., -0.0354, -0.0094,  0.0052],
        [-0.0026, -0.0055,  0.0147,  ...,  0.0161, -0.0283,  0.0042],
        [-0.0076,  0.0088,  0.0119,  ...,  0.0094,  0.0280, -0.0190]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.0872e-03,  4.6325e-04,  6.6090e-04,  ...,  5.7840e-04,
         -8.1825e-04,  3.2425e-05],
        [ 8.1539e-04, -4.7350e-04,  2.0161e-03,  ..., -2.6393e-04,
          3.8552e-04,  2.5702e-04],
        [ 5.2404e-04,  3.6011e-03, -1.2083e-03,  ..., -1.1015e-03,
         -2.3365e-04, -8.3160e-04],
        ...,
        [ 1.2016e-03,  1.4639e-03, -1.1683e-05,  ..., -1.6003e-03,
          2.7676e-03, -4.0436e-03],
        [-3.7146e-04, -3.9124e-04, -2.8729e-04,  ...,  1.3137e-04,
         -1.0891e-03,  9.7561e-04],
        [-1.2856e-03,  1.2054e-03, -1.4591e-04,  ...,  8.3923e-04,
          9.1457e-04, -2.6340e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3965e+00,  2.2070e+00, -6.3164e+00,  ..., -3.9868e-01,
         -1.2207e-03, -3.0508e+00]], device='cuda:1', dtype=torch.float16,
       grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0869, -0.1715, -0.0088,  ..., -0.1521,  0.0761, -0.0519]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 00:31:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female fisherman is known as a fisherwoman
A female policeman is known as a policewoman
A female daddy is known as a mommy
A female groom is known as a bride
A female murderer is known as a murderess
A female hero is known as a heroine
A female superman is known as a superwoman
A female poet is known as a
2024-07-04 00:31:00 root INFO     [order_1_approx] starting weight calculation for A female policeman is known as a policewoman
A female superman is known as a superwoman
A female poet is known as a poetess
A female murderer is known as a murderess
A female fisherman is known as a fisherwoman
A female groom is known as a bride
A female hero is known as a heroine
A female daddy is known as a
2024-07-04 00:31:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 00:35:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 00:38:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1189, -0.8525,  0.7861,  ...,  0.9150,  0.4009,  0.3423],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1148, -0.8188,  0.6733,  ...,  0.8047,  0.3411,  0.2942],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.1016, -6.5586, -2.0898,  ..., -2.8789,  5.3281, -0.3354],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0069,  0.0168,  0.0093,  ..., -0.0012,  0.0327, -0.0280],
        [ 0.0050, -0.0015,  0.0046,  ..., -0.0186, -0.0210, -0.0079],
        [ 0.0068,  0.0233, -0.0162,  ..., -0.0173,  0.0046, -0.0027],
        ...,
        [ 0.0038, -0.0248, -0.0008,  ..., -0.0035, -0.0270,  0.0019],
        [-0.0028,  0.0120,  0.0081,  ...,  0.0028, -0.0120,  0.0003],
        [-0.0129,  0.0117,  0.0181,  ..., -0.0098,  0.0099, -0.0043]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.7962e-03,  1.1826e-04,  1.9989e-03,  ..., -7.8917e-04,
         -1.6184e-03,  5.7602e-04],
        [-2.3670e-03, -4.8780e-04,  6.4373e-04,  ...,  4.2105e-04,
         -1.3494e-04, -1.8587e-03],
        [-7.6008e-04,  3.2425e-03, -1.2398e-03,  ..., -8.4877e-04,
         -1.0657e-04,  3.4809e-05],
        ...,
        [-3.3498e-04, -7.7105e-04,  1.1978e-03,  ...,  2.7132e-04,
          8.1396e-04, -1.2646e-03],
        [-8.2827e-04,  1.4057e-03,  2.3632e-03,  ...,  2.4033e-04,
         -2.9926e-03,  1.5373e-03],
        [-1.8396e-03,  6.5136e-04,  1.0738e-03,  ..., -8.3828e-04,
          1.4820e-03, -2.4757e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5078, -5.1875, -1.6230,  ..., -1.6660,  3.8438, -0.4927]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0542,  0.0648, -0.0281,  ...,  0.1827, -0.0544, -0.2263]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 00:38:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female policeman is known as a policewoman
A female superman is known as a superwoman
A female poet is known as a poetess
A female murderer is known as a murderess
A female fisherman is known as a fisherwoman
A female groom is known as a bride
A female hero is known as a heroine
A female daddy is known as a
2024-07-04 00:38:51 root INFO     [order_1_approx] starting weight calculation for A female superman is known as a superwoman
A female daddy is known as a mommy
A female poet is known as a poetess
A female fisherman is known as a fisherwoman
A female hero is known as a heroine
A female policeman is known as a policewoman
A female groom is known as a bride
A female murderer is known as a
2024-07-04 00:38:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 00:42:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 00:46:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7061, -0.0979,  0.5225,  ...,  0.3914, -1.1270, -0.0663],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.6138, -0.1087,  0.4233,  ...,  0.3301, -0.9976, -0.0865],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.1328, -3.8691, -0.6533,  ...,  1.3223,  4.3555, -2.0723],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0122, -0.0233,  0.0035,  ...,  0.0098,  0.0020, -0.0191],
        [ 0.0318, -0.0229,  0.0041,  ...,  0.0066,  0.0017, -0.0115],
        [ 0.0078, -0.0120, -0.0126,  ..., -0.0163,  0.0161,  0.0039],
        ...,
        [-0.0428, -0.0076, -0.0199,  ..., -0.0079, -0.0260,  0.0191],
        [ 0.0075, -0.0008,  0.0231,  ...,  0.0081, -0.0149,  0.0097],
        [-0.0054, -0.0055,  0.0146,  ...,  0.0006,  0.0036, -0.0074]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0028, -0.0023,  0.0013,  ..., -0.0019, -0.0019,  0.0004],
        [-0.0017, -0.0009, -0.0002,  ...,  0.0001,  0.0014,  0.0002],
        [-0.0010, -0.0002, -0.0022,  ..., -0.0012, -0.0004,  0.0003],
        ...,
        [ 0.0008, -0.0004, -0.0012,  ..., -0.0008,  0.0003, -0.0014],
        [ 0.0007,  0.0019,  0.0019,  ...,  0.0010, -0.0011,  0.0004],
        [-0.0007,  0.0019,  0.0002,  ..., -0.0007,  0.0001, -0.0010]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6836, -3.5703, -0.7583,  ...,  0.2842,  5.2070, -1.5078]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0806,  0.2327,  0.0180,  ..., -0.0977, -0.1868, -0.0494]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 00:46:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female superman is known as a superwoman
A female daddy is known as a mommy
A female poet is known as a poetess
A female fisherman is known as a fisherwoman
A female hero is known as a heroine
A female policeman is known as a policewoman
A female groom is known as a bride
A female murderer is known as a
2024-07-04 00:46:43 root INFO     [order_1_approx] starting weight calculation for A female murderer is known as a murderess
A female groom is known as a bride
A female superman is known as a superwoman
A female policeman is known as a policewoman
A female poet is known as a poetess
A female fisherman is known as a fisherwoman
A female daddy is known as a mommy
A female hero is known as a
2024-07-04 00:46:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 00:50:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 00:54:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2056,  1.0605,  1.8730,  ...,  1.3057,  0.1318, -0.1005],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.1880,  0.9927,  1.6494,  ...,  1.1670,  0.1027, -0.1182],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.9883, -1.7148, -5.0000,  ..., -2.5996,  3.8594,  0.7363],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0138, -0.0167, -0.0027,  ...,  0.0023, -0.0080,  0.0045],
        [ 0.0106, -0.0222, -0.0191,  ...,  0.0088,  0.0014,  0.0097],
        [ 0.0188,  0.0123, -0.0115,  ..., -0.0090, -0.0058, -0.0124],
        ...,
        [ 0.0156, -0.0049,  0.0138,  ..., -0.0205, -0.0101, -0.0139],
        [-0.0021, -0.0078,  0.0117,  ...,  0.0007, -0.0150,  0.0007],
        [-0.0012, -0.0030,  0.0061,  ..., -0.0021,  0.0052,  0.0211]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-3.0441e-03, -1.4429e-03,  2.7447e-03,  ..., -2.7299e-04,
         -1.6499e-03,  1.5616e-04],
        [-5.4693e-04,  1.0567e-03,  1.8940e-03,  ...,  1.0843e-03,
         -1.5678e-03, -3.4237e-04],
        [ 1.4725e-03,  2.8763e-03, -1.8768e-03,  ..., -1.4954e-03,
         -5.2214e-04,  4.4441e-04],
        ...,
        [ 1.7538e-03,  1.8396e-03, -1.7662e-03,  ..., -3.6163e-03,
          3.4294e-03, -2.4204e-03],
        [-4.3249e-04, -2.4376e-03,  1.3628e-03,  ..., -1.1444e-03,
         -4.5395e-04,  6.2370e-04],
        [ 1.0061e-04,  2.3174e-04, -2.5439e-04,  ..., -8.9407e-05,
          4.6670e-05,  7.3051e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1895, -1.1934, -4.8281,  ..., -2.7480,  4.6016, -0.5566]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1528, -0.1119,  0.1285,  ..., -0.3296, -0.1478, -0.3088]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 00:54:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female murderer is known as a murderess
A female groom is known as a bride
A female superman is known as a superwoman
A female policeman is known as a policewoman
A female poet is known as a poetess
A female fisherman is known as a fisherwoman
A female daddy is known as a mommy
A female hero is known as a
2024-07-04 00:54:31 root INFO     [order_1_approx] starting weight calculation for A female fisherman is known as a fisherwoman
A female daddy is known as a mommy
A female hero is known as a heroine
A female superman is known as a superwoman
A female policeman is known as a policewoman
A female murderer is known as a murderess
A female poet is known as a poetess
A female groom is known as a
2024-07-04 00:54:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 00:58:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 01:02:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1652, -0.6382, -0.0782,  ...,  1.1426, -0.6011,  0.9155],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1680, -0.6572, -0.0869,  ...,  1.0684, -0.5986,  0.8838],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0342, -2.2402, -0.6074,  ..., -3.7324, -0.2339,  1.4219],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.3646e-03, -7.6065e-03,  8.2550e-03,  ...,  1.4095e-03,
          1.9180e-02, -3.4576e-02],
        [-1.0803e-02, -1.9318e-02, -1.0147e-02,  ..., -8.6136e-03,
          1.2238e-02,  5.2032e-03],
        [-3.0479e-03,  5.6763e-03,  1.5717e-02,  ...,  1.0300e-02,
          2.6455e-03,  7.2193e-04],
        ...,
        [-3.0556e-03, -1.6266e-02, -3.3630e-02,  ..., -3.6774e-03,
         -1.9241e-02,  1.1024e-02],
        [ 1.2886e-02,  1.1421e-02,  9.9945e-03,  ...,  1.0300e-02,
         -8.6670e-03, -3.4332e-05],
        [-1.0971e-02,  1.2833e-02,  1.6922e-02,  ...,  4.2725e-04,
          1.3985e-02, -1.1883e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.5430e-03, -3.3092e-03,  2.4147e-03,  ...,  7.7677e-04,
          4.6492e-06, -1.8072e-03],
        [ 2.0905e-03, -3.2830e-04,  1.3161e-03,  ..., -5.0211e-04,
          1.5426e-04, -7.2002e-04],
        [-1.2093e-03,  1.9798e-03,  7.2956e-04,  ...,  7.5436e-04,
         -4.8637e-05, -1.9169e-03],
        ...,
        [ 2.6970e-03, -7.5483e-04, -2.2411e-05,  ..., -1.0529e-03,
          3.9291e-04,  2.4376e-03],
        [ 1.7910e-03,  3.3665e-03, -2.5797e-04,  ...,  1.8253e-03,
          6.1750e-05, -1.0672e-03],
        [-2.5444e-03,  2.3818e-04,  5.6076e-04,  ...,  3.4952e-04,
          1.5755e-03,  1.5450e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2292, -1.4707, -1.2188,  ..., -4.2617, -1.7598,  0.1738]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.2666, -0.2280,  0.2363,  ...,  0.1058, -0.2632,  0.1530]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 01:02:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female fisherman is known as a fisherwoman
A female daddy is known as a mommy
A female hero is known as a heroine
A female superman is known as a superwoman
A female policeman is known as a policewoman
A female murderer is known as a murderess
A female poet is known as a poetess
A female groom is known as a
2024-07-04 01:02:22 root INFO     [order_1_approx] starting weight calculation for A female hero is known as a heroine
A female policeman is known as a policewoman
A female daddy is known as a mommy
A female groom is known as a bride
A female murderer is known as a murderess
A female superman is known as a superwoman
A female poet is known as a poetess
A female fisherman is known as a
2024-07-04 01:02:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 01:06:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 01:10:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5469, -0.3081,  0.4111,  ...,  0.8745, -0.2488,  0.7236],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.4736, -0.2969,  0.3301,  ...,  0.7363, -0.2365,  0.6211],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.8906,  3.4961, -5.4961,  ...,  1.7012,  2.5645,  1.5879],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0017, -0.0215, -0.0037,  ...,  0.0124,  0.0134, -0.0168],
        [-0.0046, -0.0211, -0.0044,  ..., -0.0044, -0.0050,  0.0156],
        [ 0.0113, -0.0015,  0.0090,  ..., -0.0109,  0.0096, -0.0012],
        ...,
        [ 0.0025, -0.0182, -0.0128,  ..., -0.0210, -0.0177,  0.0058],
        [ 0.0027, -0.0169,  0.0199,  ..., -0.0051, -0.0223, -0.0048],
        [-0.0075,  0.0025,  0.0085,  ...,  0.0165,  0.0028, -0.0157]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0016, -0.0035,  0.0025,  ...,  0.0004, -0.0015, -0.0002],
        [ 0.0012, -0.0029,  0.0033,  ...,  0.0010, -0.0006, -0.0008],
        [-0.0003,  0.0008, -0.0034,  ...,  0.0004,  0.0015, -0.0008],
        ...,
        [ 0.0035, -0.0002,  0.0027,  ..., -0.0032,  0.0014, -0.0009],
        [-0.0011,  0.0003,  0.0002,  ...,  0.0002, -0.0008,  0.0012],
        [ 0.0013, -0.0023, -0.0016,  ..., -0.0002, -0.0008, -0.0009]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.6914,  2.0820, -4.4297,  ...,  0.9707,  4.5469,  1.4492]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1323,  0.2314, -0.1432,  ...,  0.1884, -0.0868, -0.3643]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 01:10:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female hero is known as a heroine
A female policeman is known as a policewoman
A female daddy is known as a mommy
A female groom is known as a bride
A female murderer is known as a murderess
A female superman is known as a superwoman
A female poet is known as a poetess
A female fisherman is known as a
2024-07-04 01:10:12 root INFO     [order_1_approx] starting weight calculation for A female poet is known as a poetess
A female hero is known as a heroine
A female fisherman is known as a fisherwoman
A female daddy is known as a mommy
A female superman is known as a superwoman
A female groom is known as a bride
A female murderer is known as a murderess
A female policeman is known as a
2024-07-04 01:10:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 01:14:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 01:18:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2627, -0.4243, -0.4758,  ...,  0.4675, -0.3906,  0.4941],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.2192, -0.3938, -0.4043,  ...,  0.3862, -0.3523,  0.4067],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.5566, -0.7461, -3.4043,  ..., -0.0488,  0.9121, -1.0352],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0130, -0.0123,  0.0146,  ...,  0.0046, -0.0123, -0.0085],
        [-0.0080,  0.0156, -0.0008,  ...,  0.0016,  0.0050, -0.0013],
        [ 0.0175,  0.0030,  0.0080,  ..., -0.0079,  0.0119, -0.0021],
        ...,
        [-0.0113, -0.0145,  0.0154,  ...,  0.0096,  0.0003, -0.0006],
        [ 0.0022, -0.0065,  0.0244,  ...,  0.0096, -0.0085,  0.0111],
        [-0.0029, -0.0024,  0.0074,  ...,  0.0055,  0.0075, -0.0031]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.1019e-03, -2.1229e-03,  2.0027e-03,  ...,  9.7227e-04,
         -7.3242e-04,  1.0223e-03],
        [ 5.5599e-04,  5.9366e-05,  2.0905e-03,  ...,  9.3174e-04,
         -2.2531e-04,  9.3079e-04],
        [-3.6526e-04,  1.3056e-03, -2.0838e-04,  ..., -2.5330e-03,
         -1.4770e-04, -5.6458e-04],
        ...,
        [ 1.2054e-03,  2.5654e-04,  1.1883e-03,  ..., -2.1915e-03,
          1.9932e-03, -1.9109e-04],
        [ 8.1921e-04,  2.1625e-04,  2.6646e-03,  ...,  1.0471e-03,
         -6.2418e-04, -2.5177e-04],
        [-5.2452e-06,  1.1482e-03, -8.6689e-04,  ...,  5.7364e-04,
         -1.1578e-03, -1.1187e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.2344, -1.2412, -3.3633,  ...,  0.6421,  1.1982, -1.6875]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.3872, -0.1377, -0.0089,  ..., -0.1100, -0.1384, -0.2390]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 01:18:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female poet is known as a poetess
A female hero is known as a heroine
A female fisherman is known as a fisherwoman
A female daddy is known as a mommy
A female superman is known as a superwoman
A female groom is known as a bride
A female murderer is known as a murderess
A female policeman is known as a
2024-07-04 01:18:02 root INFO     total operator prediction time: 3760.772762298584 seconds
2024-07-04 01:18:02 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on hyponyms - misc
2024-07-04 01:18:04 root INFO     building operator hyponyms - misc
2024-07-04 01:18:04 root INFO     [order_1_approx] starting weight calculation for A more specific term for a month is january
A more specific term for a shirt is polo
A more specific term for a sweater is turtleneck
A more specific term for a cup is teacup
A more specific term for a guitar is ukulele
A more specific term for a song is lullaby
A more specific term for a drum is tambourine
A more specific term for a poem is
2024-07-04 01:18:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 01:21:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 01:25:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5039, -0.5283,  0.5938,  ..., -0.2103,  0.2202,  0.8740],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.4541, -0.5078,  0.5029,  ..., -0.1786,  0.1786,  0.7856],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3735,  0.1157, -5.4727,  ..., -6.1094,  2.2090, -7.0547],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 8.3084e-03, -3.6621e-03,  3.1891e-03,  ...,  6.7596e-03,
          6.6147e-03, -1.1398e-02],
        [-5.8212e-03,  1.2573e-02, -1.1490e-02,  ..., -2.7237e-03,
         -1.5465e-02, -4.9896e-03],
        [ 1.7822e-02, -3.4790e-03, -7.8506e-03,  ..., -2.5673e-03,
          2.7298e-02,  6.8741e-03],
        ...,
        [ 2.4918e-02, -8.3771e-03, -1.4496e-04,  ...,  1.5976e-02,
         -4.9667e-03,  1.6815e-02],
        [ 8.9264e-03, -1.4595e-02,  1.1086e-02,  ...,  8.3084e-03,
          8.7280e-03,  6.3782e-03],
        [ 6.6833e-03, -2.8687e-02, -2.7351e-03,  ...,  1.9226e-03,
          3.1982e-02,  8.7738e-05]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-3.0661e-04, -2.0809e-03,  1.5469e-03,  ...,  1.3199e-03,
          2.6131e-04, -3.3665e-04],
        [-1.6613e-03, -2.5578e-03,  3.0670e-03,  ..., -1.0815e-03,
          6.3479e-05, -1.1635e-03],
        [ 2.2078e-04,  8.8882e-04, -4.3011e-04,  ...,  1.7273e-04,
          2.3389e-04,  4.2248e-04],
        ...,
        [ 1.1930e-03, -3.3140e-04, -4.0054e-04,  ...,  2.5101e-03,
         -5.0974e-04, -1.6804e-03],
        [ 6.5947e-04, -1.7242e-03, -5.1308e-04,  ...,  4.0793e-04,
         -2.6627e-03, -9.6083e-04],
        [-2.1152e-03,  1.0004e-03,  7.6771e-05,  ...,  1.6594e-03,
         -5.1403e-04,  3.0017e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0193, -1.0898, -4.2695,  ..., -3.7871,  3.5156, -6.0664]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1649, -0.3604,  0.2269,  ..., -0.3015, -0.1594,  0.4990]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 01:25:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a month is january
A more specific term for a shirt is polo
A more specific term for a sweater is turtleneck
A more specific term for a cup is teacup
A more specific term for a guitar is ukulele
A more specific term for a song is lullaby
A more specific term for a drum is tambourine
A more specific term for a poem is
2024-07-04 01:25:42 root INFO     [order_1_approx] starting weight calculation for A more specific term for a sweater is turtleneck
A more specific term for a guitar is ukulele
A more specific term for a poem is haiku
A more specific term for a month is january
A more specific term for a drum is tambourine
A more specific term for a cup is teacup
A more specific term for a song is lullaby
A more specific term for a shirt is
2024-07-04 01:25:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 01:29:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 01:33:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2488, -0.4514, -0.3491,  ...,  0.2209,  0.5532,  1.3232],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.2279, -0.4490, -0.3225,  ...,  0.2009,  0.4893,  1.2354],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.2402, -0.5176, -1.4180,  ..., -5.5977,  1.8984, -2.7051],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0116, -0.0046,  0.0078,  ...,  0.0096,  0.0046,  0.0084],
        [-0.0150,  0.0053,  0.0031,  ...,  0.0059, -0.0130, -0.0005],
        [-0.0008,  0.0157,  0.0093,  ...,  0.0014, -0.0005,  0.0110],
        ...,
        [ 0.0163, -0.0085, -0.0090,  ...,  0.0161,  0.0049, -0.0050],
        [ 0.0030, -0.0132,  0.0021,  ..., -0.0067, -0.0175,  0.0185],
        [-0.0089,  0.0032,  0.0030,  ..., -0.0188,  0.0165,  0.0225]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.5354e-03, -6.0463e-04,  1.2131e-03,  ..., -1.7881e-03,
          1.5869e-03, -3.1757e-03],
        [ 4.3535e-04, -2.2449e-03,  1.8463e-03,  ...,  3.1471e-03,
          1.4219e-03,  3.1204e-03],
        [ 2.3842e-06, -1.7052e-03,  6.4671e-05,  ..., -1.2655e-03,
         -8.4352e-04, -2.9430e-03],
        ...,
        [ 1.0481e-03, -1.5354e-03,  2.4796e-04,  ..., -3.2425e-05,
         -7.0667e-04, -3.2425e-03],
        [-8.9836e-04, -1.9951e-03,  1.0662e-03,  ..., -1.1616e-03,
         -2.8687e-03, -4.4537e-04],
        [-2.0752e-03, -1.9779e-03,  1.2684e-03,  ...,  6.9904e-04,
         -2.0008e-03, -1.8072e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0410,  0.1523, -1.1396,  ..., -4.1328,  2.0527, -3.3379]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1324, -0.2301,  0.1887,  ..., -0.1317,  0.0273,  0.0881]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 01:33:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a sweater is turtleneck
A more specific term for a guitar is ukulele
A more specific term for a poem is haiku
A more specific term for a month is january
A more specific term for a drum is tambourine
A more specific term for a cup is teacup
A more specific term for a song is lullaby
A more specific term for a shirt is
2024-07-04 01:33:27 root INFO     [order_1_approx] starting weight calculation for A more specific term for a sweater is turtleneck
A more specific term for a drum is tambourine
A more specific term for a song is lullaby
A more specific term for a poem is haiku
A more specific term for a shirt is polo
A more specific term for a month is january
A more specific term for a guitar is ukulele
A more specific term for a cup is
2024-07-04 01:33:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 01:37:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 01:41:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1943, -0.4470, -0.9756,  ...,  0.7036, -1.1055,  1.4629],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.1808, -0.4485, -0.8892,  ...,  0.6416, -1.0459,  1.3867],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9341, -1.2656, -2.5684,  ..., -2.1094,  2.2344,  4.2148],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0310,  0.0010,  0.0025,  ..., -0.0043,  0.0021, -0.0115],
        [-0.0248,  0.0064, -0.0077,  ...,  0.0171, -0.0114,  0.0056],
        [ 0.0391, -0.0080,  0.0228,  ...,  0.0012,  0.0039,  0.0049],
        ...,
        [-0.0031, -0.0021,  0.0211,  ...,  0.0359, -0.0089,  0.0038],
        [ 0.0111, -0.0140,  0.0013,  ..., -0.0069,  0.0007, -0.0009],
        [ 0.0052, -0.0097,  0.0021,  ..., -0.0056,  0.0044, -0.0033]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 3.8314e-04, -1.5688e-03,  3.4237e-03,  ..., -8.4448e-04,
         -1.5554e-03,  8.2493e-04],
        [-5.8174e-05,  1.0319e-03, -2.9697e-03,  ..., -2.0065e-03,
          3.0947e-04,  6.7520e-04],
        [ 4.9934e-03, -2.4910e-03,  3.2139e-03,  ...,  1.9245e-03,
         -2.7537e-05, -1.9312e-03],
        ...,
        [ 1.4610e-03,  6.3705e-04, -1.2007e-03,  ...,  6.7234e-04,
         -8.2731e-05,  2.9898e-04],
        [ 1.1320e-03,  1.8215e-03,  7.5436e-04,  ...,  1.1358e-03,
         -8.5258e-04, -1.0233e-03],
        [-3.1700e-03, -2.5330e-03, -2.5272e-03,  ..., -1.9464e-03,
         -1.6603e-03, -2.2106e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0527, -1.1602, -2.4043,  ..., -1.4512,  2.6758,  3.5605]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1967, -0.0316,  0.0110,  ..., -0.1063,  0.1219,  0.1810]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 01:41:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a sweater is turtleneck
A more specific term for a drum is tambourine
A more specific term for a song is lullaby
A more specific term for a poem is haiku
A more specific term for a shirt is polo
A more specific term for a month is january
A more specific term for a guitar is ukulele
A more specific term for a cup is
2024-07-04 01:41:17 root INFO     [order_1_approx] starting weight calculation for A more specific term for a cup is teacup
A more specific term for a month is january
A more specific term for a shirt is polo
A more specific term for a song is lullaby
A more specific term for a guitar is ukulele
A more specific term for a drum is tambourine
A more specific term for a poem is haiku
A more specific term for a sweater is
2024-07-04 01:41:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 01:45:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 01:49:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2793, -0.3242, -1.0000,  ...,  1.3867,  0.8242,  0.2612],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.2445, -0.3191, -0.8726,  ...,  1.1963,  0.7090,  0.2129],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0605, -0.5859,  2.4980,  ..., -2.3867, -1.5693, -3.0488],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0016, -0.0140,  0.0049,  ...,  0.0126,  0.0123,  0.0084],
        [-0.0154,  0.0055, -0.0026,  ..., -0.0006,  0.0018, -0.0040],
        [-0.0154,  0.0034,  0.0130,  ...,  0.0075, -0.0025,  0.0242],
        ...,
        [-0.0019,  0.0167, -0.0148,  ...,  0.0281, -0.0081, -0.0059],
        [ 0.0171, -0.0010,  0.0114,  ..., -0.0035,  0.0201, -0.0005],
        [-0.0167, -0.0021,  0.0027,  ..., -0.0008,  0.0081,  0.0170]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0008, -0.0010,  0.0007,  ..., -0.0022, -0.0005, -0.0029],
        [ 0.0009, -0.0015,  0.0028,  ...,  0.0015,  0.0019, -0.0003],
        [ 0.0003, -0.0014,  0.0003,  ..., -0.0004, -0.0006,  0.0011],
        ...,
        [ 0.0024, -0.0025,  0.0028,  ...,  0.0014,  0.0017, -0.0013],
        [ 0.0008,  0.0007,  0.0012,  ...,  0.0013, -0.0004,  0.0005],
        [-0.0008, -0.0013,  0.0021,  ..., -0.0001, -0.0001, -0.0004]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9941, -1.0312, -0.0371,  ..., -2.0879, -0.2734, -2.6094]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0895, -0.5376,  0.4607,  ..., -0.0844, -0.1141,  0.1768]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 01:49:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a cup is teacup
A more specific term for a month is january
A more specific term for a shirt is polo
A more specific term for a song is lullaby
A more specific term for a guitar is ukulele
A more specific term for a drum is tambourine
A more specific term for a poem is haiku
A more specific term for a sweater is
2024-07-04 01:49:07 root INFO     [order_1_approx] starting weight calculation for A more specific term for a cup is teacup
A more specific term for a sweater is turtleneck
A more specific term for a shirt is polo
A more specific term for a drum is tambourine
A more specific term for a song is lullaby
A more specific term for a month is january
A more specific term for a poem is haiku
A more specific term for a guitar is
2024-07-04 01:49:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 01:53:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 01:56:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2744, -0.3127,  0.3174,  ...,  1.1826, -0.7192,  0.5610],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 1.1357, -0.3054,  0.2578,  ...,  1.0156, -0.6543,  0.4863],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0977, -7.0547, -1.5645,  ..., -3.5742,  4.4648, -3.6914],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0096,  0.0012,  0.0018,  ...,  0.0083,  0.0035, -0.0098],
        [-0.0179, -0.0098,  0.0080,  ...,  0.0085, -0.0006, -0.0151],
        [ 0.0056,  0.0063,  0.0042,  ..., -0.0136,  0.0089, -0.0050],
        ...,
        [-0.0101, -0.0015,  0.0231,  ...,  0.0127,  0.0002,  0.0036],
        [-0.0004,  0.0012,  0.0041,  ...,  0.0142, -0.0008,  0.0089],
        [ 0.0040,  0.0043,  0.0061,  ..., -0.0040,  0.0175,  0.0272]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-3.0746e-03, -1.9588e-03,  1.9627e-03,  ...,  1.0884e-04,
          4.6587e-04, -2.8253e-04],
        [-5.9128e-05,  6.1131e-04,  1.2693e-03,  ...,  1.7557e-03,
          9.1362e-04, -1.7138e-03],
        [-3.0756e-04, -3.3989e-03,  2.2869e-03,  ..., -3.2997e-03,
         -1.0681e-04,  1.0872e-04],
        ...,
        [ 1.1492e-03,  4.8256e-04, -1.4305e-05,  ..., -1.0395e-03,
          5.5599e-04,  7.2300e-05],
        [ 2.8610e-05,  1.0557e-03,  1.2350e-03,  ...,  3.1185e-04,
         -1.0586e-03,  4.2248e-04],
        [ 3.6299e-05, -1.5106e-03,  1.4982e-03,  ..., -2.2173e-04,
          5.6982e-04,  3.2654e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3228, -5.6797, -0.7349,  ..., -2.9336,  3.4258, -3.8379]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0642, -0.2360,  0.1677,  ..., -0.0778, -0.1740,  0.1411]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 01:56:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a cup is teacup
A more specific term for a sweater is turtleneck
A more specific term for a shirt is polo
A more specific term for a drum is tambourine
A more specific term for a song is lullaby
A more specific term for a month is january
A more specific term for a poem is haiku
A more specific term for a guitar is
2024-07-04 01:56:55 root INFO     [order_1_approx] starting weight calculation for A more specific term for a guitar is ukulele
A more specific term for a poem is haiku
A more specific term for a month is january
A more specific term for a cup is teacup
A more specific term for a sweater is turtleneck
A more specific term for a drum is tambourine
A more specific term for a shirt is polo
A more specific term for a song is
2024-07-04 01:56:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 02:00:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 02:04:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1770, -0.8486,  0.5083,  ...,  0.1521,  0.2546, -0.7031],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.1721, -0.8760,  0.4690,  ...,  0.1488,  0.2303, -0.7319],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7524,  2.8516, -1.0840,  ..., -0.4097,  2.6074, -5.5469],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0029,  0.0245,  0.0042,  ...,  0.0019, -0.0146, -0.0056],
        [-0.0264,  0.0266, -0.0263,  ...,  0.0046, -0.0315,  0.0032],
        [ 0.0139,  0.0016,  0.0213,  ...,  0.0219,  0.0285, -0.0123],
        ...,
        [-0.0049,  0.0023,  0.0159,  ...,  0.0028, -0.0116,  0.0089],
        [-0.0051, -0.0083,  0.0089,  ..., -0.0034, -0.0036,  0.0008],
        [-0.0124, -0.0133,  0.0140,  ...,  0.0114,  0.0101,  0.0098]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.0676e-03,  7.4625e-04,  7.5722e-04,  ..., -1.1311e-03,
          1.1377e-03, -1.4400e-03],
        [-2.3727e-03,  1.5621e-03,  2.4109e-03,  ..., -4.7159e-04,
         -7.4768e-04,  1.3266e-03],
        [ 9.1553e-04, -2.7657e-04,  1.8668e-04,  ...,  5.4455e-04,
         -3.6812e-04,  5.5408e-04],
        ...,
        [-3.3522e-04,  2.6941e-05,  1.0757e-03,  ...,  3.8147e-05,
          1.4210e-03, -9.2983e-04],
        [-9.8348e-05, -1.5616e-04,  4.7398e-04,  ..., -1.0986e-03,
         -6.7425e-04, -1.8196e-03],
        [-1.8616e-03, -1.4706e-03, -7.1824e-05,  ...,  5.6362e-04,
         -5.1022e-05,  2.9697e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6113,  0.5254, -0.3086,  ...,  0.2036,  3.3770, -5.7344]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[0.1483, 0.1873, 0.1721,  ..., 0.0679, 0.0876, 0.2174]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 02:04:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a guitar is ukulele
A more specific term for a poem is haiku
A more specific term for a month is january
A more specific term for a cup is teacup
A more specific term for a sweater is turtleneck
A more specific term for a drum is tambourine
A more specific term for a shirt is polo
A more specific term for a song is
2024-07-04 02:04:41 root INFO     [order_1_approx] starting weight calculation for A more specific term for a drum is tambourine
A more specific term for a cup is teacup
A more specific term for a song is lullaby
A more specific term for a guitar is ukulele
A more specific term for a sweater is turtleneck
A more specific term for a shirt is polo
A more specific term for a poem is haiku
A more specific term for a month is
2024-07-04 02:04:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 02:08:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 02:12:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4727, -0.5625, -0.1104,  ...,  0.7930, -0.3315,  0.4524],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.4863, -0.6157, -0.1212,  ...,  0.7915, -0.3584,  0.4543],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.8496, -1.9668, -7.5664,  ..., -1.0430,  2.8906, -2.0410],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0080, -0.0046, -0.0197,  ..., -0.0008,  0.0095, -0.0009],
        [-0.0115,  0.0159, -0.0068,  ...,  0.0137, -0.0173,  0.0041],
        [ 0.0213,  0.0133,  0.0228,  ..., -0.0026, -0.0157, -0.0103],
        ...,
        [ 0.0031,  0.0117, -0.0008,  ...,  0.0154,  0.0009,  0.0103],
        [-0.0049,  0.0055, -0.0051,  ..., -0.0014,  0.0023,  0.0122],
        [ 0.0187,  0.0031, -0.0005,  ...,  0.0108, -0.0018,  0.0049]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-8.7357e-04, -3.6120e-04, -6.5994e-04,  ..., -1.8044e-03,
         -1.0471e-03,  1.4420e-03],
        [-6.5994e-04,  1.9016e-03,  1.2836e-03,  ...,  1.0509e-03,
          1.0834e-03,  1.7738e-03],
        [ 2.8629e-03, -8.0776e-04,  2.8248e-03,  ...,  1.5898e-03,
         -1.8167e-04, -5.4073e-04],
        ...,
        [ 1.1539e-03, -4.3297e-04,  7.6628e-04,  ...,  1.6785e-03,
          7.4911e-04, -2.4357e-03],
        [ 1.0109e-03,  3.6240e-05,  2.1458e-04,  ..., -3.4904e-04,
         -9.1791e-04, -9.8038e-04],
        [ 1.5945e-03, -5.2929e-04, -8.6117e-04,  ...,  1.5507e-03,
          1.2827e-03, -1.6003e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.6270, -1.5127, -6.0156,  ..., -0.6680,  2.4473, -2.7754]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1438,  0.1008,  0.0296,  ..., -0.1552, -0.2324,  0.1036]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 02:12:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a drum is tambourine
A more specific term for a cup is teacup
A more specific term for a song is lullaby
A more specific term for a guitar is ukulele
A more specific term for a sweater is turtleneck
A more specific term for a shirt is polo
A more specific term for a poem is haiku
A more specific term for a month is
2024-07-04 02:12:26 root INFO     [order_1_approx] starting weight calculation for A more specific term for a month is january
A more specific term for a cup is teacup
A more specific term for a song is lullaby
A more specific term for a poem is haiku
A more specific term for a guitar is ukulele
A more specific term for a shirt is polo
A more specific term for a sweater is turtleneck
A more specific term for a drum is
2024-07-04 02:12:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 02:16:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 02:20:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3193,  0.0366,  1.3682,  ..., -0.0862, -0.9272, -0.2888],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.2913,  0.0168,  1.1953,  ..., -0.0732, -0.8647, -0.2969],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.4082,  3.4961, -0.4512,  ..., -2.8008,  0.8589, -3.2852],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0307, -0.0106,  0.0134,  ...,  0.0114,  0.0200, -0.0133],
        [-0.0435, -0.0068, -0.0170,  ...,  0.0093, -0.0089, -0.0051],
        [-0.0023,  0.0127,  0.0109,  ...,  0.0017,  0.0340, -0.0054],
        ...,
        [ 0.0046,  0.0005,  0.0116,  ...,  0.0319,  0.0068, -0.0020],
        [ 0.0079,  0.0190,  0.0181,  ..., -0.0146,  0.0263,  0.0394],
        [-0.0026, -0.0111,  0.0227,  ..., -0.0163,  0.0270,  0.0070]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-8.2731e-05, -3.5076e-03,  8.6594e-04,  ...,  1.3504e-03,
         -1.2264e-03,  6.8712e-04],
        [-3.1509e-03, -2.3746e-04,  1.2331e-03,  ..., -6.1655e-04,
          1.3638e-03,  2.2907e-03],
        [ 7.8344e-04, -5.7554e-04,  1.2054e-03,  ...,  2.0504e-04,
         -2.6011e-04,  6.0558e-05],
        ...,
        [ 1.3494e-03, -6.1321e-04,  6.4945e-04,  ...,  9.6893e-04,
          7.4625e-04, -1.1740e-03],
        [ 1.5020e-04,  3.0670e-03, -1.3268e-04,  ..., -6.2943e-04,
         -5.9414e-04, -8.8358e-04],
        [-7.9060e-04, -3.7861e-04, -1.7443e-03,  ..., -1.1492e-03,
         -4.7684e-05,  7.3624e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.0586,  2.3828, -0.7627,  ..., -1.7100,  1.9297, -4.1680]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.2158,  0.4580, -0.4832,  ..., -0.4089, -0.2365, -0.0418]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 02:20:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a month is january
A more specific term for a cup is teacup
A more specific term for a song is lullaby
A more specific term for a poem is haiku
A more specific term for a guitar is ukulele
A more specific term for a shirt is polo
A more specific term for a sweater is turtleneck
A more specific term for a drum is
2024-07-04 02:20:14 root INFO     total operator prediction time: 3729.9032440185547 seconds
2024-07-04 02:20:14 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on name - occupation
2024-07-04 02:20:16 root INFO     building operator name - occupation
2024-07-04 02:20:16 root INFO     [order_1_approx] starting weight calculation for maxwell was known for their work as a  physicist
truman was known for their work as a  president
mencius was known for their work as a  philosopher
edison was known for their work as a  inventor
picasso was known for their work as a  painter
dante was known for their work as a  poet
descartes was known for their work as a  mathematician
hegel was known for their work as a 
2024-07-04 02:20:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 02:24:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 02:28:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8340, -0.2229, -0.3787,  ...,  0.7412, -0.1543,  1.0791],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.7534, -0.2207, -0.3345,  ...,  0.6450, -0.1536,  0.9688],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0447, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5615, -5.1211, -2.0371,  ..., -6.1953, -0.9443, -3.2285],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0123, -0.0021, -0.0087,  ..., -0.0070,  0.0029, -0.0071],
        [ 0.0055,  0.0110,  0.0027,  ...,  0.0043,  0.0087, -0.0137],
        [ 0.0069, -0.0005,  0.0062,  ..., -0.0127,  0.0198, -0.0038],
        ...,
        [-0.0039, -0.0022, -0.0051,  ..., -0.0078, -0.0048,  0.0010],
        [ 0.0132,  0.0019,  0.0021,  ...,  0.0051,  0.0031, -0.0040],
        [-0.0087, -0.0064, -0.0029,  ...,  0.0029, -0.0020,  0.0227]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.3237e-03,  2.9302e-04, -2.9039e-04,  ..., -2.5892e-04,
          2.8419e-04, -9.0694e-04],
        [-1.2147e-04,  4.0197e-04,  6.3896e-04,  ...,  8.9550e-04,
          2.5177e-04,  5.9557e-04],
        [-4.7326e-05, -8.4996e-05,  1.6422e-03,  ..., -4.0531e-05,
         -4.0865e-04,  9.3555e-04],
        ...,
        [ 3.3951e-04, -1.4648e-03, -1.0614e-03,  ..., -3.1853e-04,
         -2.8396e-04,  4.9734e-04],
        [ 1.1063e-03,  1.0643e-03, -1.6251e-03,  ...,  2.1946e-04,
         -1.2124e-04, -6.8808e-04],
        [-2.0039e-04,  8.1301e-05, -1.2177e-04,  ...,  1.4057e-03,
          5.1641e-04, -3.1424e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5679, -5.5117, -1.8223,  ..., -6.2148, -0.5806, -3.2754]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0108, -0.1150, -0.1466,  ..., -0.0867,  0.0493,  0.0789]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 02:28:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for maxwell was known for their work as a  physicist
truman was known for their work as a  president
mencius was known for their work as a  philosopher
edison was known for their work as a  inventor
picasso was known for their work as a  painter
dante was known for their work as a  poet
descartes was known for their work as a  mathematician
hegel was known for their work as a 
2024-07-04 02:28:05 root INFO     [order_1_approx] starting weight calculation for picasso was known for their work as a  painter
maxwell was known for their work as a  physicist
hegel was known for their work as a  philosopher
dante was known for their work as a  poet
mencius was known for their work as a  philosopher
truman was known for their work as a  president
edison was known for their work as a  inventor
descartes was known for their work as a 
2024-07-04 02:28:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 02:32:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 02:35:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5415,  0.1229, -0.2314,  ...,  0.4331, -1.0195,  1.2559],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.5259,  0.1097, -0.2203,  ...,  0.4102, -0.9863,  1.2217],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0047, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.0781, -3.4980, -2.5469,  ..., -3.9922, -2.8281, -2.0352],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0152,  0.0010,  0.0053,  ..., -0.0040, -0.0026,  0.0109],
        [-0.0095, -0.0008,  0.0172,  ..., -0.0013, -0.0040,  0.0010],
        [ 0.0008,  0.0021,  0.0035,  ...,  0.0051,  0.0010, -0.0093],
        ...,
        [ 0.0009,  0.0041, -0.0017,  ..., -0.0072, -0.0079,  0.0095],
        [ 0.0076,  0.0039, -0.0021,  ...,  0.0094, -0.0006, -0.0075],
        [-0.0257, -0.0149, -0.0016,  ..., -0.0013,  0.0014,  0.0013]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-7.5483e-04,  6.8569e-04,  5.4836e-05,  ..., -1.1253e-03,
         -2.2125e-04, -9.4414e-04],
        [-1.1806e-03, -2.7061e-04,  2.3687e-04,  ..., -7.4625e-05,
         -4.4060e-04, -3.4809e-04],
        [-3.9387e-04, -1.5469e-03,  4.0770e-04,  ..., -1.5850e-03,
          3.6120e-04, -1.1806e-03],
        ...,
        [ 5.6934e-04, -1.9693e-04, -3.8624e-04,  ...,  3.7980e-04,
         -1.2875e-04,  3.5992e-03],
        [-2.3422e-03,  1.9159e-03,  7.6199e-04,  ...,  9.6703e-04,
         -1.2207e-04, -4.4179e-04],
        [-6.2370e-04, -9.7179e-04,  1.6248e-04,  ...,  1.4753e-03,
          1.3666e-03, -6.7186e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.0469, -3.6641, -3.1562,  ..., -3.9316, -2.1328, -1.8584]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0065, -0.0834, -0.0586,  ..., -0.1254,  0.0479,  0.0737]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 02:35:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for picasso was known for their work as a  painter
maxwell was known for their work as a  physicist
hegel was known for their work as a  philosopher
dante was known for their work as a  poet
mencius was known for their work as a  philosopher
truman was known for their work as a  president
edison was known for their work as a  inventor
descartes was known for their work as a 
2024-07-04 02:35:51 root INFO     [order_1_approx] starting weight calculation for hegel was known for their work as a  philosopher
maxwell was known for their work as a  physicist
dante was known for their work as a  poet
picasso was known for their work as a  painter
mencius was known for their work as a  philosopher
edison was known for their work as a  inventor
descartes was known for their work as a  mathematician
truman was known for their work as a 
2024-07-04 02:35:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 02:39:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 02:43:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8594,  0.2102, -0.4175,  ...,  0.5366,  0.1758, -0.1210],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.8735,  0.2031, -0.4160,  ...,  0.5283,  0.1587, -0.1484],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0159, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0552, -2.2324, -0.1499,  ..., -3.1523,  0.4136, -0.9673],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.8225e-03, -1.1055e-02, -4.5586e-03,  ..., -6.4888e-03,
          1.4954e-03, -5.7755e-03],
        [-3.6678e-03,  1.9217e-03,  1.1454e-03,  ...,  1.0902e-02,
         -2.2850e-03, -1.0941e-02],
        [ 4.3030e-03,  3.7575e-03,  3.2616e-04,  ...,  9.1400e-03,
         -1.3123e-03, -9.8419e-03],
        ...,
        [-5.4779e-03, -6.2866e-03, -1.1120e-03,  ..., -5.3787e-03,
          2.8954e-03,  8.5449e-03],
        [ 1.1856e-02,  8.2397e-03,  2.0599e-03,  ...,  5.2261e-03,
          1.8234e-03, -1.8196e-03],
        [-8.6365e-03,  3.4828e-03,  5.5313e-05,  ..., -2.0889e-02,
          1.4511e-02,  8.1482e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 2.7800e-04, -1.6391e-04,  1.3065e-04,  ..., -1.1673e-03,
         -8.9836e-04, -1.9670e-04],
        [-4.9019e-04,  3.7408e-04,  5.6410e-04,  ...,  5.1451e-04,
          3.8099e-04, -1.0786e-03],
        [-4.2224e-04, -3.2377e-04,  1.9622e-04,  ..., -6.9261e-05,
          1.1368e-03, -5.7697e-04],
        ...,
        [ 6.3753e-04, -3.7360e-04, -2.8729e-04,  ..., -7.2718e-05,
          8.6880e-04, -9.4771e-06],
        [ 6.0797e-04,  3.3593e-04, -2.2650e-05,  ...,  1.7624e-03,
         -1.4362e-03,  1.4448e-04],
        [-7.2479e-04, -1.7071e-03, -5.6148e-05,  ...,  7.4816e-04,
         -1.5068e-04,  7.1108e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2102, -1.9971, -0.5332,  ..., -3.2344,  0.9673, -0.7656]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0763, -0.1270,  0.0187,  ..., -0.0070,  0.0908,  0.0719]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 02:43:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for hegel was known for their work as a  philosopher
maxwell was known for their work as a  physicist
dante was known for their work as a  poet
picasso was known for their work as a  painter
mencius was known for their work as a  philosopher
edison was known for their work as a  inventor
descartes was known for their work as a  mathematician
truman was known for their work as a 
2024-07-04 02:43:38 root INFO     [order_1_approx] starting weight calculation for edison was known for their work as a  inventor
descartes was known for their work as a  mathematician
maxwell was known for their work as a  physicist
truman was known for their work as a  president
hegel was known for their work as a  philosopher
dante was known for their work as a  poet
picasso was known for their work as a  painter
mencius was known for their work as a 
2024-07-04 02:43:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 02:47:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 02:51:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7397, -0.3765, -1.8105,  ...,  0.3765, -1.0059,  1.3809],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.6899, -0.3777, -1.6270,  ...,  0.3420, -0.9458,  1.2949],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0112, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.1270, -3.3496, -1.5713,  ..., -2.8203, -0.4065, -2.8320],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.3588e-02, -6.3629e-03,  1.9169e-03,  ..., -9.8877e-03,
         -9.4833e-03, -1.2428e-02],
        [-7.1564e-03,  1.0574e-02,  6.6032e-03,  ...,  1.1452e-02,
          3.6964e-03, -4.9362e-03],
        [ 9.3002e-03,  7.6818e-04, -9.9182e-05,  ..., -2.7390e-03,
          1.0509e-03,  8.6021e-04],
        ...,
        [-1.4420e-03, -9.5139e-03,  4.7913e-03,  ...,  5.7716e-03,
          5.6763e-03,  2.2221e-03],
        [ 3.0460e-03,  4.2572e-03,  1.1200e-02,  ...,  1.8425e-03,
          2.4357e-03, -1.4915e-03],
        [-1.1292e-03,  5.5237e-03, -2.3346e-03,  ..., -4.7302e-03,
          8.3008e-03,  9.3460e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-9.8515e-04, -5.2452e-06,  1.9569e-03,  ...,  1.4391e-03,
          7.9918e-04, -7.5674e-04],
        [ 6.5863e-05,  3.7670e-05, -5.5075e-05,  ..., -4.5300e-06,
         -2.6321e-04,  1.4138e-04],
        [ 1.2665e-03,  7.6342e-04,  1.1501e-03,  ..., -5.1737e-04,
          3.2330e-04,  8.5783e-04],
        ...,
        [ 8.1825e-04, -1.9464e-03,  3.2330e-04,  ...,  5.4979e-04,
          9.1743e-04, -1.1683e-05],
        [-3.7050e-04,  8.1110e-04,  1.4019e-03,  ...,  1.7881e-05,
          7.2956e-04,  2.1100e-04],
        [ 7.7915e-04, -3.9387e-04, -4.1962e-04,  ..., -9.1791e-04,
         -4.3392e-04, -6.0558e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1445, -3.5762, -1.8418,  ..., -2.6602,  0.1521, -2.8105]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0103, -0.0561, -0.0691,  ..., -0.0692,  0.0114,  0.0539]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 02:51:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for edison was known for their work as a  inventor
descartes was known for their work as a  mathematician
maxwell was known for their work as a  physicist
truman was known for their work as a  president
hegel was known for their work as a  philosopher
dante was known for their work as a  poet
picasso was known for their work as a  painter
mencius was known for their work as a 
2024-07-04 02:51:27 root INFO     [order_1_approx] starting weight calculation for descartes was known for their work as a  mathematician
edison was known for their work as a  inventor
dante was known for their work as a  poet
truman was known for their work as a  president
mencius was known for their work as a  philosopher
maxwell was known for their work as a  physicist
hegel was known for their work as a  philosopher
picasso was known for their work as a 
2024-07-04 02:51:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 02:55:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 02:59:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9668, -0.6987, -0.8350,  ...,  1.6533,  0.4683, -0.1920],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.8545, -0.6514, -0.7168,  ...,  1.4082,  0.3909, -0.1969],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0025, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6880,  0.9385, -3.6895,  ..., -0.8379, -3.1406, -0.4897],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.7027e-03, -1.9360e-03, -7.7133e-03,  ..., -1.1169e-02,
          1.5244e-02, -3.2043e-03],
        [-9.5215e-03,  1.4935e-03,  1.8921e-02,  ...,  1.2146e-02,
         -7.7438e-04, -1.2131e-03],
        [ 9.5673e-03,  4.5280e-03,  1.7281e-03,  ...,  1.6222e-03,
         -1.1307e-02, -1.1444e-05],
        ...,
        [-4.5090e-03, -1.2035e-03, -8.6899e-03,  ..., -1.7052e-03,
         -8.1940e-03,  2.1152e-03],
        [ 2.4918e-02,  6.6566e-03, -4.6043e-03,  ...,  1.6357e-02,
         -1.6006e-02, -1.0643e-03],
        [-6.2065e-03, -1.0048e-02, -7.7171e-03,  ...,  1.7090e-03,
          1.6766e-03,  9.1782e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-4.4286e-05,  1.5850e-03, -1.2197e-03,  ..., -8.0061e-04,
         -2.1982e-04,  4.7970e-04],
        [ 2.9945e-04,  3.4511e-05,  7.6115e-05,  ...,  1.2360e-03,
          2.1338e-04,  7.4005e-04],
        [ 1.2875e-03, -1.1873e-03,  1.4086e-03,  ...,  1.8358e-05,
         -2.5749e-05,  1.0433e-03],
        ...,
        [ 5.7602e-04, -1.2989e-03, -9.1982e-04,  ...,  6.2585e-06,
          4.1461e-04,  1.2398e-03],
        [ 9.1171e-04,  1.0252e-03,  6.1703e-04,  ...,  2.5511e-04,
         -1.1158e-03,  9.3555e-04],
        [ 4.3941e-04, -1.3781e-04, -1.9813e-04,  ...,  1.5116e-03,
          1.1718e-04, -1.1504e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6602,  0.6768, -3.8105,  ..., -0.4846, -3.3125, -0.9077]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0514,  0.1164, -0.1072,  ..., -0.0826, -0.0068,  0.1367]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 02:59:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for descartes was known for their work as a  mathematician
edison was known for their work as a  inventor
dante was known for their work as a  poet
truman was known for their work as a  president
mencius was known for their work as a  philosopher
maxwell was known for their work as a  physicist
hegel was known for their work as a  philosopher
picasso was known for their work as a 
2024-07-04 02:59:15 root INFO     [order_1_approx] starting weight calculation for dante was known for their work as a  poet
maxwell was known for their work as a  physicist
mencius was known for their work as a  philosopher
truman was known for their work as a  president
picasso was known for their work as a  painter
descartes was known for their work as a  mathematician
hegel was known for their work as a  philosopher
edison was known for their work as a 
2024-07-04 02:59:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 03:03:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 03:07:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1270,  0.4702,  0.6250,  ...,  1.4766, -0.1914,  0.4441],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1259,  0.4548,  0.5723,  ...,  1.3857, -0.2006,  0.4204],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0030, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.3223, -1.1934, -2.5156,  ..., -2.4590, -4.3242, -2.8125],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0017, -0.0118,  0.0039,  ..., -0.0032,  0.0069, -0.0031],
        [-0.0094,  0.0077, -0.0073,  ...,  0.0037, -0.0110,  0.0039],
        [-0.0019,  0.0009, -0.0012,  ...,  0.0041, -0.0064, -0.0005],
        ...,
        [-0.0031,  0.0212,  0.0007,  ...,  0.0011, -0.0235,  0.0095],
        [-0.0081,  0.0136,  0.0016,  ..., -0.0022,  0.0100, -0.0057],
        [-0.0218,  0.0055, -0.0090,  ..., -0.0016,  0.0173,  0.0112]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 7.9060e-04, -3.9244e-04,  1.2827e-04,  ...,  1.1768e-03,
          9.3365e-04, -2.8062e-04],
        [ 1.7681e-03, -2.1672e-04, -2.6703e-04,  ..., -7.3004e-04,
         -4.9472e-05,  3.3450e-04],
        [ 1.2417e-03, -3.0756e-04, -4.9782e-04,  ...,  6.9284e-04,
         -1.0138e-03, -1.0481e-03],
        ...,
        [ 1.5116e-04,  5.8079e-04, -2.0981e-04,  ...,  9.7632e-05,
         -1.4467e-03,  5.9366e-04],
        [ 3.8075e-04,  1.4477e-03,  9.9564e-04,  ..., -4.9877e-04,
          1.1187e-03,  4.1842e-04],
        [-1.3676e-03, -3.5310e-04, -3.5572e-04,  ...,  3.7551e-05,
          1.0109e-03, -1.6642e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9209, -0.8276, -2.5918,  ..., -2.5684, -3.9551, -2.5488]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0072, -0.0542, -0.0797,  ..., -0.2163,  0.2498,  0.1395]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 03:07:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for dante was known for their work as a  poet
maxwell was known for their work as a  physicist
mencius was known for their work as a  philosopher
truman was known for their work as a  president
picasso was known for their work as a  painter
descartes was known for their work as a  mathematician
hegel was known for their work as a  philosopher
edison was known for their work as a 
2024-07-04 03:07:02 root INFO     [order_1_approx] starting weight calculation for hegel was known for their work as a  philosopher
descartes was known for their work as a  mathematician
edison was known for their work as a  inventor
picasso was known for their work as a  painter
maxwell was known for their work as a  physicist
mencius was known for their work as a  philosopher
truman was known for their work as a  president
dante was known for their work as a 
2024-07-04 03:07:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 03:10:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 03:14:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1793, -0.4688,  0.4651,  ...,  0.2319,  0.0573, -0.3601],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1704, -0.4653,  0.4060,  ...,  0.2137,  0.0359, -0.3662],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0159, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9937,  1.8594, -6.8516,  ..., -3.6211,  3.5664, -1.7715],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-8.8501e-03, -8.5831e-03,  8.2397e-03,  ..., -1.1307e-02,
          1.3046e-02, -5.9929e-03],
        [-1.1795e-02,  6.3858e-03,  1.0010e-02,  ...,  6.6147e-03,
         -6.2990e-04,  1.9073e-03],
        [ 2.4628e-02,  1.0662e-03, -5.4932e-03,  ..., -2.2411e-05,
          9.4299e-03,  1.4086e-03],
        ...,
        [-6.2561e-03, -8.2855e-03,  3.2234e-03,  ..., -3.9635e-03,
          1.9436e-03,  3.7766e-03],
        [ 4.3068e-03,  5.2681e-03,  3.9864e-04,  ..., -8.7280e-03,
         -7.7209e-03, -1.0963e-02],
        [-1.2520e-02, -9.1858e-03, -2.1706e-03,  ...,  6.8970e-03,
          3.0270e-03,  4.7760e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-3.6478e-04, -1.1806e-03,  9.2649e-04,  ...,  4.5228e-04,
          1.0557e-03, -1.2331e-03],
        [-1.3676e-03, -1.1387e-03,  8.7357e-04,  ...,  2.4414e-04,
          3.2449e-04,  2.9049e-03],
        [ 9.0313e-04,  2.4891e-03,  7.1812e-04,  ..., -3.0279e-04,
          1.9908e-04, -1.1902e-03],
        ...,
        [-2.7275e-04, -8.7357e-04, -8.8692e-04,  ...,  1.2379e-03,
         -5.6791e-04,  1.7605e-03],
        [-1.0800e-04,  3.9291e-04, -1.1396e-03,  ...,  1.0242e-03,
          2.5845e-04,  8.5878e-04],
        [ 4.0531e-04,  1.7643e-03, -4.7565e-05,  ...,  2.1696e-05,
          2.2137e-04, -1.4420e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.8799,  1.1445, -5.9258,  ..., -4.0977,  3.8574, -1.8779]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1442,  0.0166, -0.0694,  ...,  0.0553,  0.1083,  0.0864]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 03:14:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for hegel was known for their work as a  philosopher
descartes was known for their work as a  mathematician
edison was known for their work as a  inventor
picasso was known for their work as a  painter
maxwell was known for their work as a  physicist
mencius was known for their work as a  philosopher
truman was known for their work as a  president
dante was known for their work as a 
2024-07-04 03:14:49 root INFO     [order_1_approx] starting weight calculation for truman was known for their work as a  president
picasso was known for their work as a  painter
hegel was known for their work as a  philosopher
descartes was known for their work as a  mathematician
dante was known for their work as a  poet
edison was known for their work as a  inventor
mencius was known for their work as a  philosopher
maxwell was known for their work as a 
2024-07-04 03:14:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 03:18:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 03:22:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5474, -1.0762,  0.1763,  ...,  0.0190, -0.2981, -0.2488],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.4910, -1.0225,  0.1384,  ...,  0.0174, -0.2888, -0.2576],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0022, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2280, -2.9727, -1.5615,  ...,  0.5825, -2.0078, -0.8691],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0031, -0.0088, -0.0026,  ..., -0.0053,  0.0015, -0.0016],
        [ 0.0045,  0.0070,  0.0011,  ..., -0.0022, -0.0023, -0.0038],
        [ 0.0021,  0.0077, -0.0003,  ..., -0.0059, -0.0044, -0.0101],
        ...,
        [-0.0094, -0.0037, -0.0012,  ...,  0.0016,  0.0003,  0.0051],
        [ 0.0035, -0.0008, -0.0057,  ..., -0.0127,  0.0270, -0.0039],
        [-0.0020, -0.0013, -0.0067,  ..., -0.0109,  0.0007,  0.0119]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 8.9407e-06, -5.9986e-04, -1.4877e-03,  ..., -5.6207e-05,
         -7.0858e-04, -7.1383e-04],
        [-3.9649e-04, -9.2173e-04,  3.7336e-04,  ...,  1.5497e-04,
          5.3787e-04, -2.0456e-04],
        [ 1.3614e-04, -3.4690e-04,  9.5558e-04,  ...,  1.4553e-03,
         -5.9414e-04,  1.0805e-03],
        ...,
        [ 1.3027e-03,  6.9666e-04,  1.7023e-04,  ..., -4.4727e-04,
         -4.6790e-05,  1.9588e-03],
        [ 1.9670e-04, -3.8195e-04, -1.0300e-04,  ...,  1.1330e-03,
         -2.4867e-04,  1.1034e-03],
        [-5.4932e-04,  3.4380e-04, -1.0805e-03,  ...,  8.0395e-04,
         -2.1768e-04,  1.9133e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1609, -2.8242, -1.4736,  ...,  0.4570, -1.1611, -0.7734]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1134, -0.1534, -0.1449,  ...,  0.1365, -0.0136,  0.0813]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 03:22:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for truman was known for their work as a  president
picasso was known for their work as a  painter
hegel was known for their work as a  philosopher
descartes was known for their work as a  mathematician
dante was known for their work as a  poet
edison was known for their work as a  inventor
mencius was known for their work as a  philosopher
maxwell was known for their work as a 
2024-07-04 03:22:33 root INFO     total operator prediction time: 3737.150194168091 seconds
2024-07-04 03:22:33 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - substance
2024-07-04 03:22:35 root INFO     building operator meronyms - substance
2024-07-04 03:22:36 root INFO     [order_1_approx] starting weight calculation for A roof is made up of shingles
A bottle is made up of glass
A plastic is made up of polymer
A penny is made up of metal
A bread is made up of flour
A jeans is made up of fabric
A beard is made up of hair
A diamond is made up of
2024-07-04 03:22:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 03:26:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 03:30:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5386, -0.8701, -0.3069,  ..., -0.7153,  0.8071,  0.4648],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.5254, -0.8945, -0.2998,  ..., -0.6709,  0.7627,  0.4419],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6650,  1.3184,  4.0898,  ..., -0.4175, -2.7148,  2.6172],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0006, -0.0111, -0.0141,  ...,  0.0064,  0.0012, -0.0025],
        [-0.0139, -0.0148,  0.0146,  ...,  0.0071, -0.0057,  0.0063],
        [ 0.0008, -0.0020, -0.0153,  ..., -0.0007,  0.0050,  0.0014],
        ...,
        [ 0.0213,  0.0019,  0.0171,  ..., -0.0039, -0.0168, -0.0069],
        [ 0.0099,  0.0041,  0.0028,  ...,  0.0003, -0.0038, -0.0098],
        [ 0.0035,  0.0123,  0.0036,  ..., -0.0025,  0.0070,  0.0103]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.6031e-03,  1.6403e-04, -1.2732e-03,  ...,  4.9114e-04,
          2.7180e-04,  2.1505e-04],
        [-1.0834e-03,  5.3072e-04, -9.8705e-05,  ..., -2.3341e-04,
          6.6185e-04,  1.0452e-03],
        [ 4.8280e-04, -9.4128e-04, -3.7122e-04,  ...,  5.9986e-04,
         -2.6762e-05,  1.7595e-04],
        ...,
        [ 9.5701e-04,  9.7275e-05,  2.2831e-03,  ...,  3.4833e-04,
          1.3027e-03, -7.9679e-04],
        [ 3.0303e-04,  2.1057e-03,  1.3084e-03,  ..., -7.7868e-04,
         -1.7281e-03, -2.5320e-04],
        [-9.9373e-04,  2.1439e-03,  1.8954e-05,  ..., -1.4563e-03,
         -2.2030e-03,  1.0996e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.8350,  0.8091,  3.7949,  ..., -0.0149, -2.0703,  2.9453]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0813,  0.4609,  0.1243,  ..., -0.1112, -0.3030,  0.1969]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 03:30:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A roof is made up of shingles
A bottle is made up of glass
A plastic is made up of polymer
A penny is made up of metal
A bread is made up of flour
A jeans is made up of fabric
A beard is made up of hair
A diamond is made up of
2024-07-04 03:30:22 root INFO     [order_1_approx] starting weight calculation for A bottle is made up of glass
A bread is made up of flour
A beard is made up of hair
A plastic is made up of polymer
A penny is made up of metal
A diamond is made up of carbon
A jeans is made up of fabric
A roof is made up of
2024-07-04 03:30:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 03:34:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 03:38:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5352, -0.4944,  0.7354,  ...,  0.9385, -0.1160,  0.0825],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.4990, -0.4851,  0.6382,  ...,  0.8364, -0.1238,  0.0549],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.1465,  1.8555,  0.7632,  ...,  3.2070,  1.0859, -1.0049],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 5.6915e-03,  1.4648e-03, -1.1883e-03,  ...,  5.1003e-03,
         -3.0746e-03,  1.5717e-02],
        [-1.0582e-02, -4.1275e-03,  3.3836e-03,  ...,  3.6564e-03,
          4.4174e-03,  4.0779e-03],
        [-7.6294e-04,  1.8158e-03, -6.1569e-03,  ...,  1.2360e-03,
          3.9005e-03,  7.3853e-03],
        ...,
        [-1.4046e-02,  1.8616e-03,  3.2711e-03,  ..., -5.3596e-03,
         -1.0986e-03,  5.8212e-03],
        [ 1.2733e-02,  6.6376e-03,  5.3406e-03,  ...,  1.1383e-02,
          3.8147e-06, -1.0513e-02],
        [-1.4832e-02,  1.2955e-02,  1.5869e-03,  ..., -1.2932e-02,
          1.4563e-03,  1.7410e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 2.0504e-04, -7.9441e-04,  2.3234e-04,  ..., -8.6975e-04,
         -7.2765e-04,  1.5230e-03],
        [ 2.9135e-04, -6.0129e-04,  7.9060e-04,  ..., -7.4291e-04,
          1.3733e-03,  2.9898e-04],
        [-2.4624e-03, -1.6394e-03,  3.9315e-04,  ...,  1.6575e-03,
         -2.8181e-04, -1.2016e-03],
        ...,
        [-1.1617e-04, -2.7905e-03,  4.1723e-04,  ..., -1.0757e-03,
          2.2030e-03, -6.2883e-05],
        [ 6.9904e-04,  1.8673e-03,  6.4039e-04,  ...,  2.7218e-03,
         -2.0542e-03, -5.8949e-05],
        [-1.1110e-03, -1.6928e-05,  6.5327e-04,  ...,  1.0544e-04,
         -6.7329e-04, -1.3769e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.0938,  1.9443,  0.2830,  ...,  3.4082,  0.4883, -0.9204]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2615,  0.0068,  0.1132,  ..., -0.1665, -0.1863,  0.1093]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 03:38:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A bottle is made up of glass
A bread is made up of flour
A beard is made up of hair
A plastic is made up of polymer
A penny is made up of metal
A diamond is made up of carbon
A jeans is made up of fabric
A roof is made up of
2024-07-04 03:38:10 root INFO     [order_1_approx] starting weight calculation for A diamond is made up of carbon
A bottle is made up of glass
A bread is made up of flour
A penny is made up of metal
A roof is made up of shingles
A plastic is made up of polymer
A beard is made up of hair
A jeans is made up of
2024-07-04 03:38:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 03:42:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 03:45:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1543, -1.2109, -0.7783,  ...,  0.6104,  0.3462,  0.1350],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 1.0371, -1.1387, -0.6836,  ...,  0.5298,  0.2874,  0.0987],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5898, -1.1621,  0.0132,  ..., -0.6890, -0.3489, -2.4883],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0084, -0.0168, -0.0138,  ..., -0.0062,  0.0086, -0.0033],
        [-0.0168,  0.0009,  0.0054,  ...,  0.0135, -0.0086,  0.0114],
        [ 0.0068, -0.0062, -0.0121,  ...,  0.0070,  0.0085,  0.0133],
        ...,
        [ 0.0103,  0.0060,  0.0089,  ..., -0.0079,  0.0011,  0.0066],
        [ 0.0206,  0.0055,  0.0083,  ...,  0.0027, -0.0102, -0.0125],
        [ 0.0020, -0.0107,  0.0118,  ..., -0.0081,  0.0130, -0.0069]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 7.8678e-04, -8.3447e-04,  4.3917e-04,  ..., -6.9952e-04,
          3.7074e-04, -4.0388e-04],
        [-7.5388e-04,  8.4925e-04,  4.4465e-04,  ...,  2.3770e-04,
          6.1274e-05,  4.9496e-04],
        [-3.3379e-04, -2.7561e-03,  1.5676e-04,  ...,  1.7653e-03,
         -6.2370e-04,  9.7466e-04],
        ...,
        [-6.9082e-05, -3.0231e-04, -4.1533e-04,  ..., -6.1893e-04,
          1.0977e-03, -2.3246e-04],
        [ 3.6812e-03,  8.2588e-04,  1.6379e-04,  ...,  1.1749e-03,
         -1.3008e-03, -2.9492e-04],
        [-8.4591e-04, -3.4094e-04,  4.2343e-04,  ..., -2.6035e-03,
         -9.0694e-04,  3.8147e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.6738, -0.5776,  0.0740,  ...,  0.8687,  0.5166, -2.1953]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1226, -0.0633,  0.0872,  ...,  0.2198, -0.0269,  0.2349]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 03:45:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A diamond is made up of carbon
A bottle is made up of glass
A bread is made up of flour
A penny is made up of metal
A roof is made up of shingles
A plastic is made up of polymer
A beard is made up of hair
A jeans is made up of
2024-07-04 03:45:58 root INFO     [order_1_approx] starting weight calculation for A beard is made up of hair
A penny is made up of metal
A jeans is made up of fabric
A roof is made up of shingles
A bread is made up of flour
A bottle is made up of glass
A diamond is made up of carbon
A plastic is made up of
2024-07-04 03:45:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 03:49:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 03:53:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4651, -0.3098,  0.5293,  ...,  0.8667, -0.2434, -0.2710],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.4348, -0.3110,  0.4553,  ...,  0.7715, -0.2407, -0.2795],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7773, -2.5371,  1.0566,  ..., -0.8350, -1.8398,  2.9688],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0130, -0.0103,  0.0043,  ...,  0.0037, -0.0046, -0.0175],
        [ 0.0018,  0.0011, -0.0028,  ...,  0.0117,  0.0058,  0.0073],
        [-0.0002,  0.0034, -0.0037,  ...,  0.0024,  0.0086,  0.0031],
        ...,
        [ 0.0019,  0.0036, -0.0034,  ..., -0.0051, -0.0048,  0.0020],
        [ 0.0124,  0.0019,  0.0090,  ...,  0.0051, -0.0053, -0.0016],
        [ 0.0007,  0.0039,  0.0088,  ..., -0.0116,  0.0017,  0.0078]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 6.4564e-04, -2.5415e-04,  6.2084e-04,  ...,  4.8208e-04,
         -7.7963e-04, -6.7711e-04],
        [-6.3896e-04,  2.8753e-04,  1.6327e-03,  ...,  6.2466e-04,
          1.2169e-03, -9.1434e-05],
        [-1.4019e-03, -2.4557e-04,  1.6761e-04,  ...,  2.0542e-03,
          7.0429e-04,  1.4238e-03],
        ...,
        [ 3.4404e-04, -1.0805e-03, -2.4652e-04,  ..., -1.2884e-03,
          5.0211e-04, -1.5106e-03],
        [-1.1072e-03,  8.1348e-04,  1.1520e-03,  ...,  7.2479e-04,
          1.0996e-03,  9.2804e-05],
        [-2.8610e-04,  6.1870e-05, -8.4162e-05,  ..., -2.5501e-03,
         -1.4191e-03,  7.1669e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.0078, -2.0547,  0.8359,  ..., -0.1509, -1.9287,  3.0938]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1023, -0.0807,  0.0497,  ..., -0.2925, -0.0630,  0.1432]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 03:53:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A beard is made up of hair
A penny is made up of metal
A jeans is made up of fabric
A roof is made up of shingles
A bread is made up of flour
A bottle is made up of glass
A diamond is made up of carbon
A plastic is made up of
2024-07-04 03:53:43 root INFO     [order_1_approx] starting weight calculation for A bread is made up of flour
A beard is made up of hair
A bottle is made up of glass
A diamond is made up of carbon
A jeans is made up of fabric
A roof is made up of shingles
A plastic is made up of polymer
A penny is made up of
2024-07-04 03:53:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 03:57:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 04:01:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8604, -0.9258, -0.7876,  ..., -0.5742, -0.1036,  1.7227],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.8018, -0.8955, -0.7075,  ..., -0.5063, -0.1143,  1.6006],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.4785, -0.3672, -0.7129,  ..., -2.5703, -0.0408,  2.2383],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0263, -0.0077, -0.0014,  ..., -0.0160, -0.0125,  0.0130],
        [-0.0306,  0.0069, -0.0004,  ...,  0.0073,  0.0054,  0.0088],
        [ 0.0199,  0.0035, -0.0068,  ...,  0.0098,  0.0087, -0.0120],
        ...,
        [ 0.0141, -0.0026, -0.0087,  ...,  0.0034,  0.0016, -0.0099],
        [ 0.0096,  0.0052,  0.0155,  ...,  0.0117, -0.0105, -0.0033],
        [-0.0148,  0.0112,  0.0015,  ..., -0.0073,  0.0084, -0.0058]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.3914e-03, -7.9870e-04, -3.4118e-04,  ..., -1.0815e-03,
         -7.8678e-04,  2.4147e-03],
        [-1.2836e-03, -6.3705e-04, -1.3056e-03,  ...,  5.3978e-04,
          9.3174e-04, -1.5440e-03],
        [ 2.7633e-04, -7.4911e-04, -9.7752e-06,  ...,  1.3924e-03,
         -1.3380e-03,  8.1682e-04],
        ...,
        [-3.9959e-04,  1.1787e-03, -1.0881e-03,  ..., -1.3151e-03,
          1.7083e-04, -1.4484e-05],
        [ 1.1816e-03,  4.0817e-04,  7.4053e-04,  ...,  2.5406e-03,
         -3.1834e-03,  2.7752e-04],
        [-6.1846e-04,  9.0599e-05, -2.3317e-04,  ..., -1.6851e-03,
         -1.5154e-03,  9.0313e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.6953, -0.4275, -1.0449,  ..., -2.1758,  0.0211,  2.8789]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0759,  0.1324,  0.2489,  ..., -0.1005, -0.3220,  0.0842]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 04:01:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A bread is made up of flour
A beard is made up of hair
A bottle is made up of glass
A diamond is made up of carbon
A jeans is made up of fabric
A roof is made up of shingles
A plastic is made up of polymer
A penny is made up of
2024-07-04 04:01:30 root INFO     [order_1_approx] starting weight calculation for A diamond is made up of carbon
A penny is made up of metal
A roof is made up of shingles
A jeans is made up of fabric
A plastic is made up of polymer
A bottle is made up of glass
A beard is made up of hair
A bread is made up of
2024-07-04 04:01:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 04:05:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 04:09:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2158, -0.7041,  0.4868,  ...,  1.4238, -1.7061, -0.1108],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-1.1592, -0.7021,  0.4290,  ...,  1.2979, -1.6143, -0.1313],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.0977, -0.0796, -0.8916,  ..., -1.1211, -1.9141, -0.8311],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0198, -0.0130,  0.0018,  ..., -0.0026,  0.0005, -0.0231],
        [-0.0370,  0.0133, -0.0124,  ...,  0.0070, -0.0075,  0.0032],
        [ 0.0087, -0.0051,  0.0075,  ...,  0.0149,  0.0037, -0.0045],
        ...,
        [ 0.0047, -0.0191, -0.0097,  ..., -0.0220,  0.0040,  0.0046],
        [ 0.0227,  0.0002,  0.0121,  ...,  0.0055,  0.0180, -0.0184],
        [ 0.0103, -0.0098,  0.0038,  ..., -0.0033,  0.0030,  0.0133]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 3.8958e-04, -2.4462e-04,  2.4462e-04,  ..., -9.1016e-05,
         -1.2010e-04, -2.4629e-04],
        [-7.4816e-04, -1.4677e-03,  1.2410e-04,  ...,  5.5313e-04,
          1.2884e-03, -7.2622e-04],
        [-1.4973e-03, -4.6754e-04, -6.6376e-04,  ...,  6.7425e-04,
         -5.1641e-04, -8.5449e-04],
        ...,
        [ 1.3523e-03, -1.9159e-03,  1.2617e-03,  ..., -1.3008e-03,
          2.1839e-03, -7.6866e-04],
        [ 1.2798e-03,  3.1281e-03, -2.2831e-03,  ..., -1.6537e-03,
         -1.3371e-03,  6.4945e-04],
        [-1.4982e-03, -1.1158e-03,  5.3263e-04,  ...,  8.8692e-05,
          6.0415e-04, -1.9665e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.5234,  0.8359, -0.9863,  ..., -0.1987, -2.0273, -1.1621]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0981, -0.1274,  0.1515,  ..., -0.0679, -0.0268,  0.1432]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 04:09:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A diamond is made up of carbon
A penny is made up of metal
A roof is made up of shingles
A jeans is made up of fabric
A plastic is made up of polymer
A bottle is made up of glass
A beard is made up of hair
A bread is made up of
2024-07-04 04:09:16 root INFO     [order_1_approx] starting weight calculation for A jeans is made up of fabric
A roof is made up of shingles
A penny is made up of metal
A beard is made up of hair
A plastic is made up of polymer
A diamond is made up of carbon
A bread is made up of flour
A bottle is made up of
2024-07-04 04:09:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 04:13:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 04:17:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1018, -0.5186,  0.9575,  ...,  0.9121, -0.3545,  1.0605],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.0871, -0.4929,  0.8062,  ...,  0.7861, -0.3315,  0.9443],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.8086, -0.3799,  1.0596,  ...,  1.1426, -0.0425,  0.8760],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0180, -0.0070,  0.0143,  ..., -0.0008,  0.0035, -0.0131],
        [-0.0170,  0.0080,  0.0126,  ...,  0.0049,  0.0054,  0.0219],
        [ 0.0141, -0.0097,  0.0044,  ..., -0.0061,  0.0105, -0.0057],
        ...,
        [ 0.0012, -0.0061,  0.0021,  ..., -0.0015, -0.0092,  0.0072],
        [ 0.0041,  0.0023, -0.0005,  ...,  0.0084,  0.0020, -0.0061],
        [-0.0156,  0.0037,  0.0155,  ...,  0.0006,  0.0066, -0.0055]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 7.6473e-05,  1.5604e-04,  1.5688e-04,  ...,  1.9150e-03,
         -9.1410e-04,  1.1425e-03],
        [-3.6979e-04, -9.8324e-04,  4.3964e-04,  ..., -7.0238e-04,
          5.5647e-04,  5.2118e-04],
        [-6.1417e-04, -3.7193e-04, -7.7581e-04,  ...,  1.4505e-03,
          1.8263e-04,  8.2016e-05],
        ...,
        [ 1.2112e-03, -3.7575e-04, -2.0444e-05,  ..., -3.0518e-04,
          1.5659e-03, -5.1975e-04],
        [ 1.6236e-04,  1.6575e-03, -1.0157e-03,  ...,  1.2770e-03,
         -1.8902e-03,  1.1516e-04],
        [-1.1930e-03, -3.5548e-04,  4.7064e-04,  ..., -1.0853e-03,
         -8.9455e-04, -1.6537e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.5566, -0.2859,  1.1719,  ...,  1.4873,  0.1527,  0.5542]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0967, -0.1760,  0.0240,  ..., -0.0176, -0.0497, -0.1934]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 04:17:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A jeans is made up of fabric
A roof is made up of shingles
A penny is made up of metal
A beard is made up of hair
A plastic is made up of polymer
A diamond is made up of carbon
A bread is made up of flour
A bottle is made up of
2024-07-04 04:17:03 root INFO     [order_1_approx] starting weight calculation for A bottle is made up of glass
A plastic is made up of polymer
A penny is made up of metal
A jeans is made up of fabric
A diamond is made up of carbon
A bread is made up of flour
A roof is made up of shingles
A beard is made up of
2024-07-04 04:17:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 04:21:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 04:24:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6011, -0.2891, -0.0820,  ..., -0.1958, -0.1885,  2.3516],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.5298, -0.2822, -0.0823,  ..., -0.1644, -0.1853,  2.1094],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8940, -1.3086, -1.5820,  ...,  1.6035, -3.0195, -0.4590],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0053, -0.0061, -0.0083,  ...,  0.0110,  0.0095, -0.0176],
        [-0.0143, -0.0002,  0.0160,  ...,  0.0146,  0.0052,  0.0068],
        [ 0.0222, -0.0031, -0.0152,  ...,  0.0009,  0.0174, -0.0108],
        ...,
        [-0.0080, -0.0111, -0.0131,  ...,  0.0107, -0.0172,  0.0032],
        [ 0.0215,  0.0038,  0.0062,  ...,  0.0130, -0.0127, -0.0012],
        [ 0.0048,  0.0077,  0.0030,  ..., -0.0089,  0.0143,  0.0031]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-3.3855e-04,  1.3733e-03,  6.0081e-05,  ..., -3.2973e-04,
         -1.0862e-03,  6.4945e-04],
        [ 3.0899e-04,  2.0051e-04, -3.8934e-04,  ..., -8.3637e-04,
         -1.3208e-03,  9.8515e-04],
        [-2.2964e-03, -2.8095e-03, -6.4754e-04,  ...,  4.4942e-05,
         -1.1196e-03,  5.8079e-04],
        ...,
        [-1.4076e-03, -8.6212e-04, -6.0940e-04,  ...,  1.0357e-03,
          1.0118e-03,  4.4966e-04],
        [ 1.1120e-03, -5.7888e-04,  2.9354e-03,  ..., -1.8406e-04,
         -5.9557e-04,  6.0749e-04],
        [-1.7538e-03, -5.0545e-05,  1.4758e-04,  ..., -6.1417e-04,
         -4.3726e-04,  4.1938e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6636, -1.8008, -2.0059,  ...,  1.1641, -3.2832,  0.4624]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0232,  0.1349, -0.0241,  ..., -0.0338, -0.1844,  0.1526]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 04:24:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A bottle is made up of glass
A plastic is made up of polymer
A penny is made up of metal
A jeans is made up of fabric
A diamond is made up of carbon
A bread is made up of flour
A roof is made up of shingles
A beard is made up of
2024-07-04 04:24:50 root INFO     total operator prediction time: 3735.0648210048676 seconds
2024-07-04 04:24:50 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on hypernyms - misc
2024-07-04 04:24:53 root INFO     building operator hypernyms - misc
2024-07-04 04:24:53 root INFO     [order_1_approx] starting weight calculation for The dress falls into the category of clothes
The postcard falls into the category of card
The cup falls into the category of tableware
The perfume falls into the category of toiletry
The hairnet falls into the category of net
The peach falls into the category of fruit
The juicer falls into the category of utensil
The tub falls into the category of
2024-07-04 04:24:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 04:28:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 04:32:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3074,  0.6812, -0.0401,  ..., -0.2656, -0.2832, -0.1934],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.2856,  0.6274, -0.0449,  ..., -0.2290, -0.2742, -0.2036],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.5430,  0.8076, -1.3682,  ..., -0.6548,  0.2676,  0.4805],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0384,  0.0083, -0.0055,  ..., -0.0082,  0.0084, -0.0130],
        [-0.0159,  0.0089, -0.0052,  ...,  0.0235,  0.0041, -0.0022],
        [ 0.0153,  0.0267, -0.0032,  ...,  0.0012,  0.0108,  0.0023],
        ...,
        [-0.0149, -0.0058,  0.0031,  ...,  0.0216,  0.0007,  0.0028],
        [-0.0005,  0.0170,  0.0067,  ..., -0.0041,  0.0117,  0.0022],
        [-0.0135, -0.0099, -0.0033,  ...,  0.0171,  0.0043,  0.0097]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 1.5297e-03, -1.9121e-03,  5.9986e-04,  ...,  1.0042e-03,
          1.4105e-03, -8.9288e-05],
        [ 4.2200e-05, -1.5926e-03,  2.2602e-03,  ...,  2.0752e-03,
          4.4489e-04, -4.5509e-03],
        [ 9.8038e-04, -1.6842e-03, -2.3232e-03,  ...,  4.9162e-04,
          1.0824e-03, -1.0097e-04],
        ...,
        [ 2.7418e-04, -8.5974e-04,  2.0683e-04,  ...,  7.8869e-04,
          1.0319e-03, -2.3365e-04],
        [ 1.9016e-03,  2.5845e-03, -2.5673e-03,  ..., -7.3910e-04,
         -8.3685e-04,  1.2112e-03],
        [ 6.6471e-04, -4.1842e-04, -1.0138e-03,  ...,  3.1996e-04,
         -4.6921e-04,  1.3256e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.7266, -0.9375, -1.7822,  ..., -1.7676, -0.8516, -0.1606]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0526, -0.2686,  0.1389,  ..., -0.0325, -0.0197,  0.0416]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 04:32:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The dress falls into the category of clothes
The postcard falls into the category of card
The cup falls into the category of tableware
The perfume falls into the category of toiletry
The hairnet falls into the category of net
The peach falls into the category of fruit
The juicer falls into the category of utensil
The tub falls into the category of
2024-07-04 04:32:43 root INFO     [order_1_approx] starting weight calculation for The perfume falls into the category of toiletry
The juicer falls into the category of utensil
The postcard falls into the category of card
The tub falls into the category of container
The cup falls into the category of tableware
The hairnet falls into the category of net
The dress falls into the category of clothes
The peach falls into the category of
2024-07-04 04:32:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 04:36:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 04:40:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2087, -0.6558, -0.5928,  ..., -1.0859, -0.0122, -0.0901],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1920, -0.6182, -0.5176,  ..., -0.9263, -0.0309, -0.1072],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.3105, -1.1357, -3.5879,  ..., -1.5938, -2.1660, -0.0381],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0307,  0.0102,  0.0085,  ..., -0.0230,  0.0065, -0.0014],
        [ 0.0020,  0.0016, -0.0046,  ...,  0.0107, -0.0170,  0.0013],
        [ 0.0022,  0.0079, -0.0099,  ...,  0.0090,  0.0046,  0.0047],
        ...,
        [-0.0092, -0.0059, -0.0059,  ...,  0.0093,  0.0075, -0.0009],
        [ 0.0178,  0.0005, -0.0072,  ..., -0.0124,  0.0163, -0.0039],
        [-0.0104,  0.0091,  0.0025,  ..., -0.0017, -0.0023,  0.0110]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.2093e-03,  2.6855e-03,  1.5011e-03,  ..., -1.0643e-03,
         -1.7042e-03, -1.4534e-03],
        [ 6.3181e-04, -1.5235e-04, -2.5606e-04,  ...,  2.5105e-04,
         -1.3857e-03, -7.1430e-04],
        [-1.4420e-03,  8.1921e-04,  4.8923e-04,  ...,  1.6060e-03,
         -1.8120e-03, -3.9530e-04],
        ...,
        [ 9.9373e-04,  5.3072e-04,  1.1873e-03,  ..., -3.2592e-04,
          3.1018e-04, -4.8757e-04],
        [-1.0037e-04,  1.5907e-03,  8.3208e-05,  ..., -1.3418e-03,
         -2.8419e-03,  1.4858e-03],
        [ 6.6280e-04, -5.1022e-04, -1.1206e-05,  ...,  1.3714e-03,
          1.8167e-04, -2.5558e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.6602, -0.4126, -3.5410,  ..., -1.6807, -2.5137, -0.2400]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0080, -0.0370,  0.0772,  ...,  0.0875,  0.0669,  0.0294]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 04:40:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The perfume falls into the category of toiletry
The juicer falls into the category of utensil
The postcard falls into the category of card
The tub falls into the category of container
The cup falls into the category of tableware
The hairnet falls into the category of net
The dress falls into the category of clothes
The peach falls into the category of
2024-07-04 04:40:33 root INFO     [order_1_approx] starting weight calculation for The dress falls into the category of clothes
The tub falls into the category of container
The hairnet falls into the category of net
The peach falls into the category of fruit
The postcard falls into the category of card
The juicer falls into the category of utensil
The perfume falls into the category of toiletry
The cup falls into the category of
2024-07-04 04:40:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 04:44:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 04:48:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3857, -0.5747, -0.8237,  ...,  0.4058, -0.1414,  0.9575],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.3433, -0.5356, -0.7036,  ...,  0.3484, -0.1401,  0.8398],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.9131, -0.4067, -1.0654,  ...,  0.0511,  2.2324,  1.0645],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 7.0984e-02, -1.4954e-02,  9.0561e-03,  ..., -1.4755e-02,
         -1.8112e-02,  2.9488e-03],
        [-3.2867e-02,  2.2522e-02, -6.1836e-03,  ...,  2.0935e-02,
          8.7967e-03,  9.7656e-03],
        [ 1.6174e-02,  6.0349e-03,  7.9651e-03,  ..., -4.8409e-03,
          7.7438e-04,  8.9874e-03],
        ...,
        [-1.7609e-02, -7.9041e-03,  7.9041e-03,  ..., -3.8223e-03,
         -1.0483e-02,  2.9278e-03],
        [ 6.0730e-03,  5.1727e-03,  8.2626e-03,  ...,  3.9139e-03,
         -6.1531e-03,  7.4053e-04],
        [-5.6763e-03, -1.5717e-03, -3.0041e-05,  ...,  5.4703e-03,
         -5.9128e-03,  9.2545e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0005,  0.0006,  0.0015,  ...,  0.0009,  0.0006,  0.0005],
        [-0.0002, -0.0006,  0.0010,  ...,  0.0005, -0.0009, -0.0002],
        [-0.0001,  0.0009, -0.0017,  ..., -0.0008,  0.0002,  0.0007],
        ...,
        [ 0.0009, -0.0005,  0.0004,  ..., -0.0008,  0.0010, -0.0012],
        [ 0.0014,  0.0012, -0.0015,  ..., -0.0004, -0.0007,  0.0012],
        [ 0.0004,  0.0004, -0.0007,  ..., -0.0007, -0.0016, -0.0013]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2402, -1.2295, -1.0918,  ..., -1.6758,  1.8350,  0.8276]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0068, -0.0148,  0.2074,  ..., -0.1328,  0.1659,  0.0480]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 04:48:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The dress falls into the category of clothes
The tub falls into the category of container
The hairnet falls into the category of net
The peach falls into the category of fruit
The postcard falls into the category of card
The juicer falls into the category of utensil
The perfume falls into the category of toiletry
The cup falls into the category of
2024-07-04 04:48:24 root INFO     [order_1_approx] starting weight calculation for The cup falls into the category of tableware
The juicer falls into the category of utensil
The dress falls into the category of clothes
The hairnet falls into the category of net
The perfume falls into the category of toiletry
The peach falls into the category of fruit
The tub falls into the category of container
The postcard falls into the category of
2024-07-04 04:48:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 04:52:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 04:56:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0352,  0.6851, -0.9907,  ..., -0.9897,  0.6333,  1.0889],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.8638,  0.5679, -0.7954,  ..., -0.7832,  0.5015,  0.9014],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.4609, -1.4297, -4.8750,  ..., -1.9902,  0.8525,  1.9854],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0243,  0.0038,  0.0039,  ..., -0.0010, -0.0061, -0.0037],
        [-0.0048,  0.0028,  0.0102,  ...,  0.0054, -0.0021,  0.0118],
        [ 0.0048, -0.0021, -0.0058,  ..., -0.0007, -0.0003,  0.0087],
        ...,
        [-0.0075, -0.0071, -0.0014,  ..., -0.0039, -0.0117,  0.0043],
        [-0.0033,  0.0007,  0.0039,  ..., -0.0013, -0.0050,  0.0013],
        [-0.0215, -0.0144,  0.0069,  ...,  0.0006,  0.0065,  0.0024]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.4414e-04,  1.6670e-03,  1.5240e-03,  ...,  7.5006e-04,
          6.9332e-04,  1.0052e-03],
        [ 2.2602e-04, -5.4884e-04,  4.4513e-04,  ...,  2.8729e-05,
          2.3055e-04,  1.5390e-04],
        [-2.1052e-04,  8.3685e-05, -1.3638e-04,  ...,  6.3753e-04,
          1.1533e-04, -8.8310e-04],
        ...,
        [-4.4286e-05, -3.3522e-04, -5.0068e-05,  ...,  7.9536e-04,
          5.2214e-04, -9.6512e-04],
        [ 3.9148e-04,  3.2997e-04, -1.2612e-04,  ..., -6.1893e-04,
         -1.0786e-03,  4.5443e-04],
        [-1.7300e-03, -5.4169e-04, -2.0981e-04,  ..., -1.0624e-03,
         -8.9645e-04, -3.7694e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.1797, -1.4541, -5.3867,  ..., -2.7070,  1.1172,  1.9629]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1893, -0.1328,  0.0175,  ...,  0.1582,  0.1055,  0.0059]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 04:56:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The cup falls into the category of tableware
The juicer falls into the category of utensil
The dress falls into the category of clothes
The hairnet falls into the category of net
The perfume falls into the category of toiletry
The peach falls into the category of fruit
The tub falls into the category of container
The postcard falls into the category of
2024-07-04 04:56:13 root INFO     [order_1_approx] starting weight calculation for The peach falls into the category of fruit
The dress falls into the category of clothes
The tub falls into the category of container
The postcard falls into the category of card
The cup falls into the category of tableware
The juicer falls into the category of utensil
The hairnet falls into the category of net
The perfume falls into the category of
2024-07-04 04:56:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 05:00:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 05:04:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2737,  0.3774, -0.9082,  ..., -1.3369,  0.8286, -0.7222],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.2295,  0.3132, -0.7593,  ..., -1.1016,  0.6816, -0.6602],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.0469, -3.0918, -1.3984,  ..., -3.9902, -2.3359, -4.2539],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.2568e-02, -1.2367e-02, -1.9638e-02,  ..., -1.4748e-02,
          8.5068e-03, -1.4458e-03],
        [-1.8463e-03, -1.1009e-02,  3.6621e-04,  ...,  7.6981e-03,
         -2.1400e-03, -2.6703e-05],
        [-1.6241e-03,  1.7899e-02,  6.2332e-03,  ...,  8.1406e-03,
          1.6155e-03,  7.8201e-03],
        ...,
        [ 2.3117e-03, -5.8746e-04, -8.0338e-03,  ..., -3.8090e-03,
          3.0384e-03,  6.7482e-03],
        [ 1.2329e-02,  3.5324e-03, -1.3790e-03,  ..., -9.3307e-03,
          1.0490e-02, -9.9106e-03],
        [ 1.2665e-03,  4.1084e-03,  2.8324e-03,  ..., -1.4206e-02,
          6.4545e-03,  1.6510e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-9.1314e-04,  2.7895e-05,  1.7252e-03,  ..., -4.1127e-04,
         -1.0910e-03, -6.9189e-04],
        [-9.7752e-04, -2.8381e-03,  3.7742e-04,  ...,  2.5439e-04,
          3.4642e-04, -1.1387e-03],
        [ 2.5940e-04,  7.7009e-04, -4.8256e-04,  ...,  1.1778e-03,
          7.9632e-04, -6.9332e-04],
        ...,
        [-1.3471e-04,  1.1559e-03,  4.0460e-04,  ..., -1.8244e-03,
          1.7395e-03, -1.5080e-04],
        [ 1.4973e-03,  7.7724e-04, -6.7425e-04,  ..., -1.1625e-03,
         -3.8314e-04,  8.1730e-04],
        [-6.6614e-04,  1.6441e-03,  1.3676e-03,  ..., -4.9114e-04,
         -2.3842e-04, -1.4458e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.5703, -2.8711, -1.7969,  ..., -4.4492, -2.6230, -3.7871]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.2708, -0.1506, -0.0496,  ..., -0.1481,  0.1011,  0.0490]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 05:04:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The peach falls into the category of fruit
The dress falls into the category of clothes
The tub falls into the category of container
The postcard falls into the category of card
The cup falls into the category of tableware
The juicer falls into the category of utensil
The hairnet falls into the category of net
The perfume falls into the category of
2024-07-04 05:04:02 root INFO     [order_1_approx] starting weight calculation for The peach falls into the category of fruit
The hairnet falls into the category of net
The juicer falls into the category of utensil
The perfume falls into the category of toiletry
The postcard falls into the category of card
The cup falls into the category of tableware
The tub falls into the category of container
The dress falls into the category of
2024-07-04 05:04:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 05:08:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 05:11:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4829, -0.8076,  1.1436,  ...,  0.6016, -0.5762, -0.3599],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.4504, -0.7954,  1.0146,  ...,  0.5459, -0.5527, -0.3689],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9902, -0.7461, -0.0586,  ..., -3.5234, -2.6348, -3.8086],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0237, -0.0096, -0.0240,  ..., -0.0055,  0.0082, -0.0006],
        [-0.0091,  0.0063,  0.0068,  ...,  0.0026, -0.0079, -0.0002],
        [-0.0027,  0.0003, -0.0066,  ...,  0.0033,  0.0023,  0.0085],
        ...,
        [ 0.0049, -0.0016,  0.0043,  ..., -0.0094,  0.0102,  0.0101],
        [ 0.0114, -0.0015, -0.0281,  ..., -0.0143,  0.0124,  0.0097],
        [-0.0071, -0.0044, -0.0122,  ..., -0.0024,  0.0179,  0.0154]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 1.3885e-03,  1.2279e-04, -2.5787e-03,  ..., -2.5153e-04,
          2.4261e-03, -8.5163e-04],
        [-2.1343e-03, -1.5793e-03,  2.8744e-03,  ...,  2.2774e-03,
         -9.6273e-04,  4.7684e-07],
        [-6.1226e-04, -1.2131e-03,  2.4438e-04,  ...,  1.0376e-03,
         -3.8910e-04,  5.9843e-05],
        ...,
        [-2.9421e-04, -3.5620e-04,  2.8515e-04,  ..., -1.4324e-03,
          2.7142e-03,  1.1339e-03],
        [ 5.1832e-04,  8.2350e-04, -3.5191e-03,  ..., -2.9888e-03,
         -5.4407e-04,  1.3895e-03],
        [ 4.2009e-04,  4.8113e-04, -1.6670e-03,  ..., -1.8711e-03,
          5.2035e-05, -3.7885e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.0859, -0.2932, -0.3245,  ..., -3.8828, -3.5117, -4.5000]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1454, -0.0901,  0.0004,  ...,  0.1207,  0.1882,  0.0311]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 05:11:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The peach falls into the category of fruit
The hairnet falls into the category of net
The juicer falls into the category of utensil
The perfume falls into the category of toiletry
The postcard falls into the category of card
The cup falls into the category of tableware
The tub falls into the category of container
The dress falls into the category of
2024-07-04 05:11:54 root INFO     [order_1_approx] starting weight calculation for The postcard falls into the category of card
The tub falls into the category of container
The peach falls into the category of fruit
The cup falls into the category of tableware
The hairnet falls into the category of net
The perfume falls into the category of toiletry
The dress falls into the category of clothes
The juicer falls into the category of
2024-07-04 05:11:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 05:15:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 05:19:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0996, -1.1572, -0.6509,  ...,  0.1897,  1.5020,  1.1338],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.9214, -1.0059, -0.5288,  ...,  0.1560,  1.2158,  0.9414],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.2266,  1.3408,  1.3008,  ...,  0.7227, -0.4312, -0.0156],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0484,  0.0046, -0.0009,  ..., -0.0067, -0.0142,  0.0162],
        [-0.0087,  0.0080,  0.0066,  ...,  0.0012,  0.0015, -0.0092],
        [ 0.0094,  0.0102,  0.0125,  ...,  0.0008,  0.0025,  0.0101],
        ...,
        [-0.0083,  0.0026, -0.0078,  ...,  0.0128, -0.0050,  0.0011],
        [ 0.0027,  0.0077,  0.0082,  ..., -0.0040, -0.0011,  0.0125],
        [-0.0088, -0.0107,  0.0003,  ..., -0.0088,  0.0189,  0.0138]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 3.9053e-04,  6.5804e-04,  4.3893e-04,  ..., -7.1049e-05,
         -2.4748e-04,  1.6689e-03],
        [-1.0376e-03, -1.0386e-03,  1.1549e-03,  ...,  3.8290e-04,
         -5.2691e-04, -7.2002e-04],
        [ 1.8120e-04,  3.5548e-04, -5.6028e-05,  ...,  1.2302e-03,
          7.7820e-04,  2.0146e-04],
        ...,
        [ 4.2248e-04,  8.2588e-04, -8.3625e-05,  ..., -5.3072e-04,
          1.1044e-03, -4.9305e-04],
        [ 1.8129e-03,  1.3628e-03, -1.6212e-03,  ..., -6.1989e-04,
         -1.3554e-04,  8.8882e-04],
        [-9.5463e-04,  1.7822e-05,  2.0909e-04,  ..., -5.9891e-04,
          1.1473e-03,  7.7772e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.6973,  1.3027,  1.4805,  ...,  0.5156,  0.2637, -0.8662]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0822, -0.0085, -0.1582,  ...,  0.0719,  0.1892,  0.0017]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 05:19:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The postcard falls into the category of card
The tub falls into the category of container
The peach falls into the category of fruit
The cup falls into the category of tableware
The hairnet falls into the category of net
The perfume falls into the category of toiletry
The dress falls into the category of clothes
The juicer falls into the category of
2024-07-04 05:19:45 root INFO     [order_1_approx] starting weight calculation for The perfume falls into the category of toiletry
The postcard falls into the category of card
The juicer falls into the category of utensil
The peach falls into the category of fruit
The tub falls into the category of container
The cup falls into the category of tableware
The dress falls into the category of clothes
The hairnet falls into the category of
2024-07-04 05:19:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 05:23:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 05:27:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4497, -1.1572, -0.5518,  ...,  0.8110, -0.0410,  0.3760],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.4209, -1.1162, -0.4990,  ...,  0.7251, -0.0557,  0.3325],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.0547,  0.9688, -1.2871,  ..., -2.8027, -2.5234, -0.1868],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0031, -0.0005, -0.0100,  ..., -0.0087, -0.0053,  0.0006],
        [-0.0101,  0.0196,  0.0166,  ...,  0.0154, -0.0088, -0.0091],
        [ 0.0037, -0.0032, -0.0020,  ..., -0.0049, -0.0001,  0.0039],
        ...,
        [ 0.0036, -0.0081, -0.0074,  ...,  0.0146,  0.0094,  0.0072],
        [ 0.0044, -0.0058,  0.0074,  ..., -0.0054, -0.0049,  0.0051],
        [ 0.0002, -0.0082, -0.0067,  ..., -0.0020,  0.0165,  0.0131]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.8616e-03,  7.6485e-04,  8.3065e-04,  ..., -2.1100e-04,
          4.8637e-04,  9.1648e-04],
        [ 9.1934e-04, -1.2016e-03,  1.3142e-03,  ...,  2.3556e-03,
         -1.0986e-03, -1.3614e-04],
        [-3.1400e-04, -1.4973e-03, -2.2840e-04,  ...,  4.4060e-04,
         -3.4046e-04, -1.0672e-03],
        ...,
        [ 8.3017e-04, -3.0136e-04,  4.8566e-04,  ..., -7.4482e-04,
          3.9053e-04, -1.8921e-03],
        [ 5.8842e-04,  5.5075e-04, -2.6512e-04,  ..., -6.9916e-05,
         -7.7295e-04, -1.1244e-03],
        [-1.4839e-03,  1.1063e-03,  6.4850e-04,  ..., -9.3985e-04,
          4.6158e-04, -7.4625e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.8242,  0.8540, -1.8672,  ..., -3.0840, -2.6016,  0.0312]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1257,  0.0260, -0.0904,  ..., -0.1128,  0.0906, -0.0609]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 05:27:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The perfume falls into the category of toiletry
The postcard falls into the category of card
The juicer falls into the category of utensil
The peach falls into the category of fruit
The tub falls into the category of container
The cup falls into the category of tableware
The dress falls into the category of clothes
The hairnet falls into the category of
2024-07-04 05:27:37 root INFO     total operator prediction time: 3763.968093395233 seconds
2024-07-04 05:27:37 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_inf - Ved
2024-07-04 05:27:39 root INFO     building operator verb_inf - Ved
2024-07-04 05:27:39 root INFO     [order_1_approx] starting weight calculation for If the present form is tell, the past form is told
If the present form is allow, the past form is allowed
If the present form is publish, the past form is published
If the present form is understand, the past form is understood
If the present form is appear, the past form is appeared
If the present form is attend, the past form is attended
If the present form is introduce, the past form is introduced
If the present form is lose, the past form is
2024-07-04 05:27:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 05:31:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 05:35:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0312, -0.4629,  1.1748,  ...,  0.7461, -0.7041,  1.2412],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.0345, -0.4741,  1.0664,  ...,  0.6914, -0.6870,  1.1943],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.7656, -1.7256, -2.7695,  ...,  1.2129, -0.9761,  0.3789],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0275, -0.0167,  0.0059,  ...,  0.0168,  0.0073, -0.0091],
        [ 0.0025, -0.0126,  0.0060,  ..., -0.0114, -0.0028,  0.0038],
        [ 0.0019, -0.0012, -0.0191,  ..., -0.0140, -0.0055, -0.0023],
        ...,
        [-0.0053, -0.0009, -0.0073,  ..., -0.0279, -0.0106,  0.0052],
        [ 0.0119,  0.0050, -0.0052,  ..., -0.0118, -0.0185,  0.0005],
        [-0.0144, -0.0017, -0.0006,  ..., -0.0005,  0.0105, -0.0205]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-9.6703e-04, -2.6531e-03,  5.3644e-05,  ..., -2.7490e-04,
          1.3523e-03,  7.5102e-04],
        [-3.5858e-04,  3.6097e-04,  4.3750e-05,  ..., -5.6791e-04,
          1.1110e-03,  8.7166e-04],
        [ 6.2943e-04, -7.5340e-05, -1.4763e-03,  ..., -2.1648e-03,
         -1.7605e-03, -2.5368e-04],
        ...,
        [ 1.2798e-03,  9.4414e-04,  6.7425e-04,  ..., -3.0518e-03,
          9.0694e-04, -1.8082e-03],
        [-6.1154e-05,  7.5912e-04,  1.3151e-03,  ..., -2.0752e-03,
         -1.8044e-03, -2.4986e-04],
        [ 5.6458e-04,  2.7561e-04,  7.1478e-04,  ..., -7.2575e-04,
         -7.6175e-05, -1.1468e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.7227, -1.3984, -2.2129,  ...,  0.9434, -0.4885,  0.7119]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0457, -0.1399,  0.0643,  ...,  0.2620, -0.0159, -0.0198]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 05:35:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is tell, the past form is told
If the present form is allow, the past form is allowed
If the present form is publish, the past form is published
If the present form is understand, the past form is understood
If the present form is appear, the past form is appeared
If the present form is attend, the past form is attended
If the present form is introduce, the past form is introduced
If the present form is lose, the past form is
2024-07-04 05:35:28 root INFO     [order_1_approx] starting weight calculation for If the present form is lose, the past form is lost
If the present form is attend, the past form is attended
If the present form is allow, the past form is allowed
If the present form is introduce, the past form is introduced
If the present form is appear, the past form is appeared
If the present form is understand, the past form is understood
If the present form is publish, the past form is published
If the present form is tell, the past form is
2024-07-04 05:35:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 05:39:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 05:43:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2756,  0.4478,  0.7871,  ...,  1.4053, -0.2297, -0.5435],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.2693,  0.4375,  0.7334,  ...,  1.3408, -0.2443, -0.5752],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([1.2627, 1.3506, 2.3516,  ..., 2.8047, 0.9951, 0.1611], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0157, -0.0271,  0.0076,  ...,  0.0175, -0.0269,  0.0007],
        [-0.0046, -0.0137,  0.0025,  ...,  0.0007,  0.0124,  0.0219],
        [ 0.0007, -0.0048, -0.0144,  ..., -0.0108,  0.0082, -0.0162],
        ...,
        [-0.0219, -0.0011, -0.0074,  ..., -0.0273, -0.0082,  0.0110],
        [ 0.0093,  0.0164,  0.0102,  ..., -0.0238, -0.0359, -0.0008],
        [-0.0165,  0.0353,  0.0142,  ..., -0.0111, -0.0011, -0.0075]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.5783e-03, -1.5223e-04, -6.0415e-04,  ..., -6.8855e-04,
         -3.4761e-04, -1.3733e-04],
        [ 2.2483e-04, -8.1778e-05, -4.6015e-05,  ..., -2.5539e-03,
         -1.2617e-03,  2.9774e-03],
        [-2.9016e-04, -9.6941e-04, -1.6708e-03,  ..., -2.0370e-03,
         -5.1308e-04, -1.5278e-03],
        ...,
        [ 8.2779e-04, -5.4407e-04,  2.9230e-04,  ..., -2.5425e-03,
          1.2579e-03, -1.0662e-03],
        [ 3.9482e-04,  4.4274e-04,  1.0490e-03,  ..., -1.9512e-03,
         -2.0313e-03, -2.5101e-03],
        [-5.7220e-04,  1.6918e-03,  1.0014e-03,  ..., -6.5136e-04,
          1.0881e-03, -1.4811e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[0.6050, 0.8408, 2.1367,  ..., 2.8359, 1.6504, 0.3091]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0403, -0.0205, -0.2969,  ..., -0.2256,  0.1559,  0.0718]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 05:43:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is lose, the past form is lost
If the present form is attend, the past form is attended
If the present form is allow, the past form is allowed
If the present form is introduce, the past form is introduced
If the present form is appear, the past form is appeared
If the present form is understand, the past form is understood
If the present form is publish, the past form is published
If the present form is tell, the past form is
2024-07-04 05:43:14 root INFO     [order_1_approx] starting weight calculation for If the present form is introduce, the past form is introduced
If the present form is allow, the past form is allowed
If the present form is tell, the past form is told
If the present form is publish, the past form is published
If the present form is attend, the past form is attended
If the present form is understand, the past form is understood
If the present form is lose, the past form is lost
If the present form is appear, the past form is
2024-07-04 05:43:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 05:47:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 05:51:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3601,  0.6782,  0.5522,  ..., -0.1750, -0.1855, -0.2178],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.3525,  0.6509,  0.4949,  ..., -0.1591, -0.1962, -0.2391],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.1270,  0.9526, -1.7031,  ...,  2.5234, -2.5312, -1.6914],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0189, -0.0154,  0.0002,  ...,  0.0190, -0.0075, -0.0051],
        [-0.0230, -0.0201,  0.0061,  ...,  0.0108, -0.0015,  0.0059],
        [ 0.0047,  0.0014, -0.0159,  ..., -0.0052, -0.0130, -0.0050],
        ...,
        [-0.0038,  0.0010, -0.0061,  ..., -0.0231, -0.0084, -0.0028],
        [ 0.0011,  0.0098, -0.0011,  ..., -0.0342, -0.0130,  0.0002],
        [-0.0141,  0.0216,  0.0130,  ...,  0.0154, -0.0036, -0.0109]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.3365e-03, -2.8725e-03,  2.0542e-03,  ...,  5.3358e-04,
         -5.9247e-05,  6.2752e-04],
        [ 1.0383e-04, -2.3766e-03,  4.3583e-04,  ...,  2.8753e-04,
          3.4046e-04,  1.3142e-03],
        [ 6.7425e-04, -1.2083e-03, -9.2149e-05,  ..., -2.4300e-03,
         -5.5313e-04,  1.8435e-03],
        ...,
        [ 1.4076e-03, -4.3344e-04, -6.6757e-05,  ..., -1.9703e-03,
         -7.0286e-04, -1.3447e-04],
        [-4.1819e-04,  3.6049e-03, -5.2166e-04,  ..., -2.5902e-03,
         -2.1954e-03, -1.3161e-03],
        [ 2.0142e-03, -2.3117e-03,  8.2588e-04,  ...,  1.9341e-03,
          1.2493e-04,  5.6124e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7773,  0.3340, -1.1768,  ...,  2.6367, -1.7354, -1.9902]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[0.1790, 0.1665, 0.4875,  ..., 0.1915, 0.0734, 0.4666]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 05:51:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is introduce, the past form is introduced
If the present form is allow, the past form is allowed
If the present form is tell, the past form is told
If the present form is publish, the past form is published
If the present form is attend, the past form is attended
If the present form is understand, the past form is understood
If the present form is lose, the past form is lost
If the present form is appear, the past form is
2024-07-04 05:51:03 root INFO     [order_1_approx] starting weight calculation for If the present form is introduce, the past form is introduced
If the present form is understand, the past form is understood
If the present form is allow, the past form is allowed
If the present form is lose, the past form is lost
If the present form is appear, the past form is appeared
If the present form is attend, the past form is attended
If the present form is tell, the past form is told
If the present form is publish, the past form is
2024-07-04 05:51:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 05:55:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 05:58:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.7173, 0.3420, 1.3506,  ..., 1.3438, 0.7671, 0.0231], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.6426,  0.2969,  1.1504,  ...,  1.1650,  0.6621, -0.0051],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.6719, -2.8711, -2.2148,  ...,  3.3438, -2.4805, -1.3867],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0163, -0.0162, -0.0016,  ...,  0.0127, -0.0010, -0.0113],
        [-0.0108, -0.0193, -0.0135,  ...,  0.0173, -0.0099,  0.0014],
        [-0.0031,  0.0032, -0.0190,  ..., -0.0057,  0.0016, -0.0007],
        ...,
        [-0.0065,  0.0035, -0.0061,  ..., -0.0270, -0.0041, -0.0151],
        [-0.0105,  0.0133,  0.0097,  ..., -0.0330, -0.0322,  0.0205],
        [-0.0159,  0.0038,  0.0048,  ..., -0.0079,  0.0228, -0.0374]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.4853e-03, -9.4748e-04,  6.2037e-04,  ..., -9.6035e-04,
          1.5697e-03, -7.4625e-04],
        [ 2.9898e-04, -9.0170e-04, -5.7745e-04,  ...,  1.5707e-03,
         -6.8045e-04,  2.4357e-03],
        [ 9.8705e-04,  6.2609e-04, -1.7548e-03,  ..., -1.2283e-03,
          9.4032e-04,  2.5058e-04],
        ...,
        [ 2.1877e-03,  2.1195e-04, -1.7464e-05,  ..., -5.3787e-04,
          1.7939e-03, -2.3327e-03],
        [ 1.8616e-03,  2.9888e-03, -1.5759e-04,  ..., -1.1606e-03,
         -2.1744e-03,  1.9813e-04],
        [-1.2140e-03,  1.9979e-04,  9.5749e-04,  ...,  7.7772e-04,
         -1.9102e-03, -1.3838e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.9023, -2.4766, -1.9785,  ...,  3.4766, -1.3906, -1.4424]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0474, -0.0791,  0.0595,  ...,  0.0770, -0.1945,  0.5273]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 05:58:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is introduce, the past form is introduced
If the present form is understand, the past form is understood
If the present form is allow, the past form is allowed
If the present form is lose, the past form is lost
If the present form is appear, the past form is appeared
If the present form is attend, the past form is attended
If the present form is tell, the past form is told
If the present form is publish, the past form is
2024-07-04 05:58:52 root INFO     [order_1_approx] starting weight calculation for If the present form is lose, the past form is lost
If the present form is introduce, the past form is introduced
If the present form is publish, the past form is published
If the present form is appear, the past form is appeared
If the present form is tell, the past form is told
If the present form is attend, the past form is attended
If the present form is allow, the past form is allowed
If the present form is understand, the past form is
2024-07-04 05:58:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 06:02:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 06:06:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1475, -0.6851,  0.9351,  ...,  0.2495,  0.6279, -0.0933],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.1364, -0.7026,  0.8525,  ...,  0.2332,  0.5796, -0.1194],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6851,  0.3613, -0.7344,  ...,  0.0808,  0.5176,  1.2168],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0243, -0.0251, -0.0099,  ...,  0.0108, -0.0179, -0.0101],
        [-0.0251, -0.0146, -0.0033,  ...,  0.0109,  0.0081, -0.0016],
        [-0.0039, -0.0062, -0.0438,  ..., -0.0048,  0.0097, -0.0205],
        ...,
        [-0.0141,  0.0247,  0.0157,  ..., -0.0239, -0.0053, -0.0106],
        [ 0.0172,  0.0106,  0.0037,  ...,  0.0013, -0.0326, -0.0014],
        [ 0.0044,  0.0242,  0.0014,  ..., -0.0100,  0.0110, -0.0242]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.2131e-03, -1.8978e-03, -1.3304e-03,  ...,  5.5695e-04,
          5.1260e-05, -8.8692e-04],
        [-1.8673e-03,  1.9121e-04, -7.4959e-04,  ..., -2.3985e-04,
          1.9007e-03,  1.1711e-03],
        [ 1.0109e-03, -1.6680e-03, -1.0557e-03,  ..., -4.6659e-04,
         -6.1798e-04, -3.1352e-04],
        ...,
        [ 4.1080e-04,  1.2312e-03,  8.8835e-04,  ...,  1.5152e-04,
          3.1185e-03, -2.1725e-03],
        [-1.4229e-03,  1.4677e-03, -4.9257e-04,  ..., -3.0303e-04,
         -1.3075e-03, -1.4420e-03],
        [-4.2319e-04,  9.8705e-04,  2.7084e-03,  ..., -1.5507e-03,
         -8.6498e-04, -1.2732e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2063,  0.5806, -0.6377,  ..., -0.2642,  1.1846,  1.2080]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1088,  0.1310,  0.0257,  ...,  0.1694, -0.4661,  0.3345]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 06:06:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is lose, the past form is lost
If the present form is introduce, the past form is introduced
If the present form is publish, the past form is published
If the present form is appear, the past form is appeared
If the present form is tell, the past form is told
If the present form is attend, the past form is attended
If the present form is allow, the past form is allowed
If the present form is understand, the past form is
2024-07-04 06:06:41 root INFO     [order_1_approx] starting weight calculation for If the present form is tell, the past form is told
If the present form is allow, the past form is allowed
If the present form is lose, the past form is lost
If the present form is appear, the past form is appeared
If the present form is understand, the past form is understood
If the present form is publish, the past form is published
If the present form is attend, the past form is attended
If the present form is introduce, the past form is
2024-07-04 06:06:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 06:10:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 06:14:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1753, -0.6270,  0.5586,  ...,  0.2114, -0.3525, -0.5835],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1849, -0.6699,  0.5229,  ...,  0.2048, -0.3733, -0.6284],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.4062, -1.7129, -0.6475,  ...,  4.2266, -1.1201,  2.8516],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0205, -0.0217,  0.0016,  ...,  0.0040, -0.0041, -0.0033],
        [-0.0179, -0.0210, -0.0120,  ...,  0.0108,  0.0021,  0.0112],
        [ 0.0038, -0.0268, -0.0331,  ..., -0.0114, -0.0226,  0.0032],
        ...,
        [ 0.0055,  0.0102, -0.0079,  ..., -0.0194, -0.0188, -0.0070],
        [ 0.0101,  0.0164,  0.0057,  ..., -0.0229, -0.0109,  0.0074],
        [-0.0109,  0.0110,  0.0130,  ..., -0.0111,  0.0138, -0.0178]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.3065e-03, -9.1553e-04,  4.7684e-04,  ..., -1.1659e-04,
          1.7452e-03, -5.2929e-05],
        [ 8.3351e-04, -2.6836e-03, -1.2474e-03,  ...,  9.6321e-04,
          1.4858e-03,  1.6937e-03],
        [ 1.2741e-03, -2.8496e-03, -1.1168e-03,  ..., -2.0714e-03,
         -1.8196e-03,  3.7718e-04],
        ...,
        [ 1.8101e-03, -2.5225e-04,  1.5774e-03,  ..., -2.5139e-03,
          1.5717e-03, -1.1263e-03],
        [ 1.2341e-03,  8.2016e-04, -2.4581e-04,  ..., -1.9016e-03,
         -2.2659e-03,  1.9574e-04],
        [-6.8903e-04,  3.1281e-04,  1.5278e-03,  ...,  3.3617e-04,
         -1.0548e-03,  8.5640e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7930, -2.6797, -0.7217,  ...,  3.6992, -0.8311,  3.0762]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0975, -0.2031,  0.3188,  ...,  0.1183, -0.1143,  0.4097]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 06:14:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is tell, the past form is told
If the present form is allow, the past form is allowed
If the present form is lose, the past form is lost
If the present form is appear, the past form is appeared
If the present form is understand, the past form is understood
If the present form is publish, the past form is published
If the present form is attend, the past form is attended
If the present form is introduce, the past form is
2024-07-04 06:14:29 root INFO     [order_1_approx] starting weight calculation for If the present form is lose, the past form is lost
If the present form is publish, the past form is published
If the present form is allow, the past form is allowed
If the present form is understand, the past form is understood
If the present form is appear, the past form is appeared
If the present form is introduce, the past form is introduced
If the present form is tell, the past form is told
If the present form is attend, the past form is
2024-07-04 06:14:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 06:18:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 06:22:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9189,  0.7705,  1.3770,  ...,  0.3970, -0.6006, -0.3716],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.8696,  0.7319,  1.2363,  ...,  0.3643, -0.5830, -0.3857],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0881,  1.5879,  2.4023,  ...,  4.2109,  0.2612,  0.1123],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0125, -0.0025,  0.0010,  ...,  0.0189, -0.0094,  0.0059],
        [-0.0284, -0.0186, -0.0075,  ...,  0.0032,  0.0019, -0.0056],
        [-0.0013,  0.0011, -0.0053,  ...,  0.0078, -0.0063, -0.0037],
        ...,
        [-0.0065,  0.0104,  0.0023,  ..., -0.0112, -0.0049,  0.0136],
        [ 0.0106, -0.0034,  0.0157,  ..., -0.0040, -0.0226,  0.0072],
        [-0.0172,  0.0142,  0.0013,  ..., -0.0029,  0.0090, -0.0250]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.5883e-03, -7.0858e-04,  4.5848e-04,  ...,  1.1816e-03,
          9.1696e-04,  1.0004e-03],
        [-1.1959e-03, -1.7147e-03, -7.2122e-05,  ..., -9.1171e-04,
         -9.7322e-04,  1.0691e-03],
        [ 7.5531e-04,  5.2595e-04, -4.2558e-04,  ..., -5.5408e-04,
          2.1896e-03,  6.2656e-04],
        ...,
        [ 1.5802e-03,  6.7425e-04, -1.1396e-03,  ..., -2.6226e-04,
          8.4925e-04, -2.1076e-03],
        [ 2.0742e-04,  1.3151e-03, -1.2646e-03,  ..., -3.9482e-04,
         -4.2391e-04,  2.9469e-04],
        [-3.0184e-04,  1.0338e-03,  1.0757e-03,  ..., -1.3151e-03,
          1.0471e-03, -9.2745e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[0.2197, 1.9043, 1.8936,  ..., 2.4844, 0.8823, 0.9727]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0809, -0.0619,  0.1533,  ...,  0.0451, -0.0590,  0.0887]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 06:22:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is lose, the past form is lost
If the present form is publish, the past form is published
If the present form is allow, the past form is allowed
If the present form is understand, the past form is understood
If the present form is appear, the past form is appeared
If the present form is introduce, the past form is introduced
If the present form is tell, the past form is told
If the present form is attend, the past form is
2024-07-04 06:22:18 root INFO     [order_1_approx] starting weight calculation for If the present form is attend, the past form is attended
If the present form is appear, the past form is appeared
If the present form is lose, the past form is lost
If the present form is introduce, the past form is introduced
If the present form is understand, the past form is understood
If the present form is publish, the past form is published
If the present form is tell, the past form is told
If the present form is allow, the past form is
2024-07-04 06:22:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 06:26:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 06:30:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4600,  0.2046,  0.5752,  ...,  0.8501, -0.2314,  0.0975],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.4307,  0.1803,  0.5068,  ...,  0.7729, -0.2347,  0.0698],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4238, -0.5752, -0.0537,  ...,  0.1045, -1.5752,  1.5234],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0220, -0.0013,  0.0033,  ...,  0.0228, -0.0071, -0.0092],
        [-0.0096, -0.0306, -0.0039,  ..., -0.0085, -0.0070, -0.0058],
        [-0.0079, -0.0028, -0.0249,  ..., -0.0094, -0.0022, -0.0035],
        ...,
        [-0.0114,  0.0054,  0.0013,  ..., -0.0226, -0.0099, -0.0081],
        [ 0.0049,  0.0069,  0.0000,  ..., -0.0105, -0.0235,  0.0007],
        [ 0.0026,  0.0136,  0.0025,  ...,  0.0129, -0.0155, -0.0117]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.0790e-03, -5.7411e-04,  1.8787e-04,  ..., -1.4782e-03,
          5.3787e-04, -7.8678e-04],
        [-7.8917e-04, -7.2765e-04, -2.2912e-04,  ..., -4.1604e-04,
          1.8811e-04,  2.7695e-03],
        [-3.3450e-04, -1.6546e-03, -1.2970e-03,  ..., -3.2310e-03,
          8.9169e-04, -9.0122e-04],
        ...,
        [ 2.2678e-03,  8.2731e-04,  9.0599e-06,  ..., -2.3499e-03,
          2.4090e-03, -1.5535e-03],
        [-6.5660e-04,  2.0313e-03, -1.7080e-03,  ...,  4.9591e-04,
         -7.8630e-04, -1.5497e-04],
        [ 1.3828e-03,  2.8896e-04,  1.9083e-03,  ...,  4.5204e-04,
          1.7500e-04, -4.2534e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7266, -0.5942, -0.4636,  ..., -0.1136, -1.0566,  1.3760]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1600,  0.0076,  0.1278,  ...,  0.1012, -0.0138,  0.1190]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 06:30:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is attend, the past form is attended
If the present form is appear, the past form is appeared
If the present form is lose, the past form is lost
If the present form is introduce, the past form is introduced
If the present form is understand, the past form is understood
If the present form is publish, the past form is published
If the present form is tell, the past form is told
If the present form is allow, the past form is
2024-07-04 06:30:07 root INFO     total operator prediction time: 3748.1112987995148 seconds
2024-07-04 06:30:07 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on over+adj_reg
2024-07-04 06:30:09 root INFO     building operator over+adj_reg
2024-07-04 06:30:09 root INFO     [order_1_approx] starting weight calculation for If something is too enthusiastic, it is overenthusiastic
If something is too loaded, it is overloaded
If something is too done, it is overdone
If something is too filled, it is overfilled
If something is too exposed, it is overexposed
If something is too inflated, it is overinflated
If something is too optimistic, it is overoptimistic
If something is too saturated, it is
2024-07-04 06:30:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 06:34:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 06:37:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0504,  0.3030,  2.3438,  ..., -0.4836,  0.0742, -0.3462],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.0396,  0.2720,  2.0996,  ..., -0.4385,  0.0464, -0.3621],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7510,  0.0157,  0.2246,  ...,  1.2900,  1.9707,  1.7646],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0009, -0.0378, -0.0132,  ..., -0.0023,  0.0011,  0.0014],
        [-0.0172, -0.0150,  0.0249,  ...,  0.0040, -0.0118,  0.0050],
        [ 0.0034,  0.0051,  0.0024,  ...,  0.0039,  0.0040,  0.0018],
        ...,
        [ 0.0075, -0.0022, -0.0018,  ...,  0.0146, -0.0003,  0.0057],
        [ 0.0067, -0.0107,  0.0103,  ..., -0.0038, -0.0176,  0.0013],
        [-0.0081, -0.0054, -0.0013,  ..., -0.0028,  0.0168, -0.0061]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0015, -0.0046, -0.0007,  ..., -0.0005,  0.0018,  0.0036],
        [-0.0012, -0.0028,  0.0032,  ...,  0.0011,  0.0012, -0.0013],
        [-0.0005,  0.0005, -0.0009,  ...,  0.0017, -0.0006, -0.0032],
        ...,
        [ 0.0039,  0.0006, -0.0008,  ..., -0.0017,  0.0012, -0.0007],
        [-0.0016, -0.0025, -0.0010,  ..., -0.0009,  0.0003,  0.0025],
        [-0.0007, -0.0006, -0.0002,  ...,  0.0018,  0.0009, -0.0032]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5244, -0.6465, -0.2698,  ...,  0.9966,  3.1992,  0.8521]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.3652, -0.3311, -0.1227,  ...,  0.2830,  0.4319, -0.3772]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 06:37:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too enthusiastic, it is overenthusiastic
If something is too loaded, it is overloaded
If something is too done, it is overdone
If something is too filled, it is overfilled
If something is too exposed, it is overexposed
If something is too inflated, it is overinflated
If something is too optimistic, it is overoptimistic
If something is too saturated, it is
2024-07-04 06:37:56 root INFO     [order_1_approx] starting weight calculation for If something is too filled, it is overfilled
If something is too saturated, it is oversaturated
If something is too loaded, it is overloaded
If something is too done, it is overdone
If something is too optimistic, it is overoptimistic
If something is too enthusiastic, it is overenthusiastic
If something is too exposed, it is overexposed
If something is too inflated, it is
2024-07-04 06:37:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 06:41:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 06:45:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2128, -0.2529,  1.3750,  ...,  0.0444,  0.9087, -0.1703],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.2115, -0.2690,  1.2432,  ...,  0.0414,  0.8364, -0.1937],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3047,  0.0323, -0.0566,  ..., -0.4434,  0.2490,  0.8340],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0108, -0.0070,  0.0007,  ...,  0.0132, -0.0131, -0.0144],
        [-0.0187,  0.0256,  0.0003,  ...,  0.0078, -0.0023, -0.0154],
        [ 0.0027,  0.0063,  0.0044,  ..., -0.0029, -0.0191,  0.0147],
        ...,
        [-0.0186, -0.0008, -0.0072,  ...,  0.0166, -0.0082,  0.0195],
        [ 0.0184, -0.0030, -0.0160,  ..., -0.0105, -0.0178,  0.0018],
        [ 0.0028, -0.0217, -0.0087,  ...,  0.0068, -0.0077, -0.0029]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-3.6335e-04,  5.7936e-04, -1.5755e-03,  ...,  4.0531e-06,
         -7.0000e-04,  3.6526e-04],
        [-1.1320e-03, -1.9426e-03,  2.1763e-03,  ...,  1.2856e-03,
          1.6108e-03, -3.4237e-04],
        [ 2.0969e-04,  5.0974e-04,  2.3985e-04,  ..., -1.2083e-03,
         -6.4182e-04,  2.1172e-03],
        ...,
        [ 3.9177e-03,  1.3075e-03, -1.4725e-03,  ..., -1.4524e-03,
          2.1133e-03, -6.8569e-04],
        [-2.8038e-04, -5.8746e-04, -1.0509e-03,  ..., -5.8937e-04,
         -2.0180e-03,  2.2755e-03],
        [ 1.1234e-03, -3.4046e-03, -6.9237e-04,  ...,  2.9507e-03,
          6.5708e-04, -8.9264e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2881, -0.3884, -0.7817,  ...,  0.0500,  0.5156,  0.6606]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1692, -0.2773,  0.1365,  ..., -0.0127,  0.0222, -0.0715]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 06:45:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too filled, it is overfilled
If something is too saturated, it is oversaturated
If something is too loaded, it is overloaded
If something is too done, it is overdone
If something is too optimistic, it is overoptimistic
If something is too enthusiastic, it is overenthusiastic
If something is too exposed, it is overexposed
If something is too inflated, it is
2024-07-04 06:45:46 root INFO     [order_1_approx] starting weight calculation for If something is too optimistic, it is overoptimistic
If something is too filled, it is overfilled
If something is too done, it is overdone
If something is too enthusiastic, it is overenthusiastic
If something is too inflated, it is overinflated
If something is too saturated, it is oversaturated
If something is too exposed, it is overexposed
If something is too loaded, it is
2024-07-04 06:45:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 06:49:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 06:53:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3745,  0.6641,  0.6616,  ...,  0.2156,  0.2788, -0.5757],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.3794,  0.6577,  0.6147,  ...,  0.2074,  0.2527, -0.6108],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2588, -0.4419, -1.8242,  ..., -0.9375,  0.6611,  2.8750],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.0676e-02, -2.7374e-02, -1.8921e-03,  ...,  6.2447e-03,
          5.1956e-03, -7.5722e-03],
        [-6.9733e-03,  3.8853e-03,  1.6876e-02,  ..., -5.1804e-03,
         -3.8147e-05, -1.9012e-02],
        [-3.6011e-03,  1.5793e-03,  1.4671e-02,  ..., -1.7487e-02,
         -6.9809e-03,  2.4597e-02],
        ...,
        [ 8.3923e-04, -1.6083e-02,  7.3509e-03,  ..., -1.8234e-02,
          1.4809e-02,  4.2038e-03],
        [ 2.8656e-02,  1.7029e-02, -4.3144e-03,  ...,  2.6367e-02,
          1.1627e-02, -2.1072e-02],
        [-2.8442e-02, -7.4081e-03,  9.6741e-03,  ..., -4.3640e-03,
         -1.4420e-02,  2.3651e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.8234e-03,  3.8695e-04, -2.3098e-03,  ...,  2.7428e-03,
          1.5230e-03, -1.1997e-03],
        [ 4.1485e-05, -7.5150e-04,  3.8452e-03,  ...,  1.3933e-03,
         -1.7796e-03, -7.5293e-04],
        [-5.5885e-04, -1.4791e-03, -1.0633e-03,  ..., -3.3379e-03,
          2.3937e-04,  3.0899e-03],
        ...,
        [ 4.3716e-03, -4.2248e-04, -1.1396e-03,  ..., -2.6531e-03,
          9.0313e-04,  1.0319e-03],
        [-3.4571e-04,  6.4254e-05,  2.4567e-03,  ..., -7.0858e-04,
         -2.1954e-03,  1.6127e-03],
        [-7.0190e-04, -1.0204e-03, -1.1501e-03,  ...,  2.6722e-03,
          7.8678e-04, -2.6779e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9170,  0.4375, -2.2383,  ..., -1.6875,  2.6094,  0.3750]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.3665, -0.5898,  0.7661,  ...,  0.2201,  0.0457, -0.3125]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 06:53:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too optimistic, it is overoptimistic
If something is too filled, it is overfilled
If something is too done, it is overdone
If something is too enthusiastic, it is overenthusiastic
If something is too inflated, it is overinflated
If something is too saturated, it is oversaturated
If something is too exposed, it is overexposed
If something is too loaded, it is
2024-07-04 06:53:34 root INFO     [order_1_approx] starting weight calculation for If something is too inflated, it is overinflated
If something is too exposed, it is overexposed
If something is too enthusiastic, it is overenthusiastic
If something is too optimistic, it is overoptimistic
If something is too loaded, it is overloaded
If something is too saturated, it is oversaturated
If something is too filled, it is overfilled
If something is too done, it is
2024-07-04 06:53:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 06:57:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 07:01:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2830,  0.4612,  1.3887,  ...,  0.8057,  0.7168, -0.8848],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3035,  0.4963,  1.4307,  ...,  0.8438,  0.7495, -1.0088],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.5068, -2.1523,  0.1328,  ..., -2.6914,  4.7031,  3.4336],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0123, -0.0292, -0.0019,  ..., -0.0082, -0.0216, -0.0188],
        [ 0.0080,  0.0019,  0.0089,  ..., -0.0063, -0.0044, -0.0126],
        [ 0.0012, -0.0024, -0.0016,  ..., -0.0038,  0.0067,  0.0196],
        ...,
        [-0.0006,  0.0099,  0.0010,  ..., -0.0070,  0.0117,  0.0141],
        [-0.0026, -0.0075,  0.0268,  ..., -0.0136, -0.0147, -0.0002],
        [ 0.0031, -0.0154,  0.0139,  ...,  0.0003,  0.0031, -0.0042]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0031, -0.0022,  0.0005,  ...,  0.0001,  0.0015, -0.0006],
        [ 0.0012, -0.0029,  0.0045,  ...,  0.0018,  0.0005, -0.0019],
        [ 0.0009,  0.0013, -0.0016,  ..., -0.0045,  0.0006,  0.0014],
        ...,
        [ 0.0013,  0.0004,  0.0005,  ..., -0.0064, -0.0012,  0.0059],
        [-0.0004, -0.0013,  0.0016,  ...,  0.0007,  0.0010, -0.0013],
        [-0.0020, -0.0016,  0.0008,  ..., -0.0003,  0.0005,  0.0006]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6045, -2.3828, -0.0674,  ..., -1.4941,  4.4766,  3.0488]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0439, -0.0710,  0.3142,  ...,  0.7070, -0.6333,  0.1415]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 07:01:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too inflated, it is overinflated
If something is too exposed, it is overexposed
If something is too enthusiastic, it is overenthusiastic
If something is too optimistic, it is overoptimistic
If something is too loaded, it is overloaded
If something is too saturated, it is oversaturated
If something is too filled, it is overfilled
If something is too done, it is
2024-07-04 07:01:19 root INFO     [order_1_approx] starting weight calculation for If something is too filled, it is overfilled
If something is too loaded, it is overloaded
If something is too exposed, it is overexposed
If something is too saturated, it is oversaturated
If something is too optimistic, it is overoptimistic
If something is too inflated, it is overinflated
If something is too done, it is overdone
If something is too enthusiastic, it is
2024-07-04 07:01:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 07:05:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 07:09:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8711, -0.4355,  0.7168,  ...,  0.0080,  0.8604,  0.1260],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.8110, -0.4436,  0.6265,  ...,  0.0052,  0.7734,  0.0911],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.2109e+00, -8.5547e-01,  1.2295e+00,  ...,  2.9297e-03,
         4.9062e+00,  1.9238e+00], device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0058, -0.0305,  0.0236,  ...,  0.0021, -0.0129, -0.0049],
        [-0.0190, -0.0164,  0.0050,  ...,  0.0086, -0.0153, -0.0146],
        [ 0.0022,  0.0109, -0.0112,  ..., -0.0155,  0.0028,  0.0067],
        ...,
        [-0.0219, -0.0160, -0.0122,  ..., -0.0058,  0.0043,  0.0036],
        [-0.0042, -0.0202, -0.0054,  ..., -0.0040, -0.0145,  0.0026],
        [-0.0027, -0.0112,  0.0132,  ..., -0.0035, -0.0069, -0.0229]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.1282e-03, -2.2650e-06, -2.5153e-04,  ..., -1.8196e-03,
         -8.8501e-04,  1.5574e-03],
        [ 5.1403e-04, -2.6512e-04,  1.3733e-03,  ...,  5.8889e-04,
          3.3379e-06,  8.1730e-04],
        [-1.6880e-03,  1.3294e-03, -2.3766e-03,  ..., -4.2953e-03,
         -9.9182e-04,  1.0080e-03],
        ...,
        [ 6.3515e-04, -5.0068e-05, -4.1127e-04,  ..., -2.5635e-03,
          6.8092e-04, -7.2765e-04],
        [-1.8787e-03, -1.6880e-03, -5.7220e-04,  ..., -5.0545e-04,
         -1.0538e-04,  1.4925e-03],
        [-2.2926e-03, -7.6199e-04, -8.2850e-05,  ...,  1.9455e-03,
         -5.2547e-04, -1.0462e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.9570, -1.8613,  1.0391,  ...,  0.0089,  4.9062,  2.2852]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.3455, -0.0818,  0.3911,  ...,  0.0787, -0.0337, -0.0627]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 07:09:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too filled, it is overfilled
If something is too loaded, it is overloaded
If something is too exposed, it is overexposed
If something is too saturated, it is oversaturated
If something is too optimistic, it is overoptimistic
If something is too inflated, it is overinflated
If something is too done, it is overdone
If something is too enthusiastic, it is
2024-07-04 07:09:08 root INFO     [order_1_approx] starting weight calculation for If something is too loaded, it is overloaded
If something is too filled, it is overfilled
If something is too enthusiastic, it is overenthusiastic
If something is too optimistic, it is overoptimistic
If something is too saturated, it is oversaturated
If something is too done, it is overdone
If something is too inflated, it is overinflated
If something is too exposed, it is
2024-07-04 07:09:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 07:13:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 07:16:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7715,  0.4573,  1.2168,  ..., -0.0756,  0.3167,  0.3965],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.7676,  0.4431,  1.1318,  ..., -0.0691,  0.2876,  0.3735],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5020,  1.3047, -0.2119,  ..., -0.5630,  0.0776, -0.7231],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0414, -0.0145,  0.0111,  ..., -0.0262, -0.0102, -0.0176],
        [-0.0179, -0.0127,  0.0118,  ..., -0.0126, -0.0039,  0.0016],
        [ 0.0168,  0.0271, -0.0106,  ...,  0.0160, -0.0132,  0.0041],
        ...,
        [-0.0185, -0.0174, -0.0028,  ..., -0.0022, -0.0052,  0.0085],
        [ 0.0084, -0.0154, -0.0018,  ..., -0.0037, -0.0228,  0.0178],
        [ 0.0128,  0.0159,  0.0026,  ...,  0.0027,  0.0051, -0.0105]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.5406e-03,  1.7204e-03,  2.0027e-03,  ..., -2.8629e-03,
          1.6041e-03,  2.0638e-03],
        [ 9.8419e-04, -4.0359e-03,  4.2534e-03,  ...,  3.2139e-04,
          3.3522e-04, -1.2856e-03],
        [-2.8324e-04,  2.9907e-03, -2.6932e-03,  ..., -1.7157e-03,
         -3.5057e-03,  1.9417e-03],
        ...,
        [-9.1910e-05, -7.2050e-04, -1.4133e-03,  ..., -3.5715e-04,
         -6.0225e-04,  1.5221e-03],
        [-1.1158e-03, -1.3103e-03, -1.0061e-03,  ..., -8.2445e-04,
         -9.4080e-04,  2.0866e-03],
        [ 8.7976e-04,  1.5039e-03, -1.7662e-03,  ...,  1.2484e-03,
         -2.0561e-03, -8.1015e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6504,  1.1738, -1.9580,  ..., -0.2754,  0.1670, -1.4785]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2043, -0.2563,  0.6499,  ...,  0.1987,  0.0049,  0.1233]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 07:16:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too loaded, it is overloaded
If something is too filled, it is overfilled
If something is too enthusiastic, it is overenthusiastic
If something is too optimistic, it is overoptimistic
If something is too saturated, it is oversaturated
If something is too done, it is overdone
If something is too inflated, it is overinflated
If something is too exposed, it is
2024-07-04 07:16:57 root INFO     [order_1_approx] starting weight calculation for If something is too done, it is overdone
If something is too filled, it is overfilled
If something is too loaded, it is overloaded
If something is too enthusiastic, it is overenthusiastic
If something is too inflated, it is overinflated
If something is too exposed, it is overexposed
If something is too saturated, it is oversaturated
If something is too optimistic, it is
2024-07-04 07:16:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 07:20:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 07:24:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4421, -0.6548, -0.2111,  ..., -0.4209,  1.3643,  0.6558],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.4109, -0.6318, -0.1986,  ..., -0.3672,  1.2012,  0.5811],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.8750, -0.0370,  0.2168,  ...,  0.4287, -0.3279,  3.1562],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0087, -0.0206,  0.0128,  ...,  0.0030, -0.0080, -0.0107],
        [-0.0233, -0.0102,  0.0145,  ...,  0.0033, -0.0075, -0.0140],
        [ 0.0148,  0.0163, -0.0124,  ...,  0.0057, -0.0125,  0.0353],
        ...,
        [-0.0151, -0.0224, -0.0042,  ..., -0.0106, -0.0002, -0.0066],
        [-0.0043, -0.0268, -0.0020,  ..., -0.0195, -0.0060,  0.0127],
        [ 0.0056, -0.0026,  0.0078,  ..., -0.0044,  0.0084, -0.0189]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.8845e-03, -1.9312e-05, -7.1621e-04,  ..., -1.8120e-04,
          4.5061e-04,  1.5001e-03],
        [ 1.0185e-03, -1.9300e-04,  2.5425e-03,  ...,  2.8872e-04,
         -8.0395e-04,  4.3845e-04],
        [-8.6403e-04,  2.5406e-03, -1.9131e-03,  ..., -1.6947e-03,
         -1.7405e-03,  3.1166e-03],
        ...,
        [ 4.9782e-04, -1.7338e-03, -8.9645e-04,  ..., -1.2388e-03,
          2.1877e-03, -1.5936e-03],
        [-1.5612e-03, -1.9932e-03, -5.7364e-04,  ..., -1.2245e-03,
         -9.9123e-05,  7.8058e-04],
        [-1.2326e-04, -1.0643e-03, -2.4056e-04,  ...,  1.6375e-03,
          1.8635e-03, -2.8152e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.4707, -0.5127,  0.3279,  ...,  0.5972, -0.5513,  3.2227]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1240,  0.0168,  0.2776,  ..., -0.0202,  0.0125, -0.0486]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 07:24:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too done, it is overdone
If something is too filled, it is overfilled
If something is too loaded, it is overloaded
If something is too enthusiastic, it is overenthusiastic
If something is too inflated, it is overinflated
If something is too exposed, it is overexposed
If something is too saturated, it is oversaturated
If something is too optimistic, it is
2024-07-04 07:24:48 root INFO     [order_1_approx] starting weight calculation for If something is too optimistic, it is overoptimistic
If something is too exposed, it is overexposed
If something is too saturated, it is oversaturated
If something is too inflated, it is overinflated
If something is too done, it is overdone
If something is too loaded, it is overloaded
If something is too enthusiastic, it is overenthusiastic
If something is too filled, it is
2024-07-04 07:24:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 07:28:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 07:32:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1061,  0.6030,  1.2461,  ..., -0.0111,  0.3442, -0.5127],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.1041,  0.6216,  1.2197,  ..., -0.0092,  0.3313, -0.5688],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8110, -0.8350,  0.1240,  ...,  0.7354,  2.0586, -0.1318],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0150, -0.0241, -0.0148,  ..., -0.0026,  0.0117, -0.0265],
        [-0.0063, -0.0064, -0.0033,  ...,  0.0021, -0.0089, -0.0135],
        [ 0.0026,  0.0004,  0.0034,  ..., -0.0034, -0.0083,  0.0104],
        ...,
        [-0.0242, -0.0035,  0.0069,  ...,  0.0143, -0.0157,  0.0021],
        [-0.0035, -0.0051, -0.0108,  ...,  0.0022, -0.0012, -0.0128],
        [-0.0026,  0.0052,  0.0113,  ...,  0.0047, -0.0151,  0.0119]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.1749e-03, -1.7738e-03, -5.0449e-04,  ...,  1.0519e-03,
          1.7443e-03,  1.3409e-03],
        [-1.4343e-03, -1.8167e-03,  3.2825e-03,  ...,  1.7471e-03,
          1.4572e-03, -2.3499e-03],
        [ 3.2282e-04,  8.0299e-04, -3.8090e-03,  ..., -2.3117e-03,
         -1.2054e-03,  5.1641e-04],
        ...,
        [ 3.5000e-03, -1.3180e-03, -2.3689e-03,  ..., -1.7052e-03,
          1.1644e-03,  2.2173e-04],
        [ 1.8940e-03,  1.6603e-03,  1.8454e-04,  ..., -9.1076e-04,
         -2.4433e-03,  3.5191e-04],
        [-1.9312e-05, -8.1825e-04, -4.9877e-04,  ...,  1.0281e-03,
         -1.3313e-03,  5.0068e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2783,  0.4375,  0.2390,  ...,  1.3926,  2.3789, -0.6064]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0737, -0.5869,  0.0984,  ...,  0.0406,  0.1056, -0.1654]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 07:32:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too optimistic, it is overoptimistic
If something is too exposed, it is overexposed
If something is too saturated, it is oversaturated
If something is too inflated, it is overinflated
If something is too done, it is overdone
If something is too loaded, it is overloaded
If something is too enthusiastic, it is overenthusiastic
If something is too filled, it is
2024-07-04 07:32:34 root INFO     total operator prediction time: 3744.5459856987 seconds
2024-07-04 07:32:34 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on Ving - verb_inf
2024-07-04 07:32:36 root INFO     building operator Ving - verb_inf
2024-07-04 07:32:36 root INFO     [order_1_approx] starting weight calculation for happening is the active form of happen
following is the active form of follow
requiring is the active form of require
avoiding is the active form of avoid
continuing is the active form of continue
operating is the active form of operate
teaching is the active form of teach
allowing is the active form of
2024-07-04 07:32:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 07:36:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 07:40:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4575, -1.4785,  0.5908,  ..., -0.4734,  0.5586,  0.7539],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.4729, -1.5938,  0.5752,  ..., -0.4683,  0.5542,  0.7769],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0136, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.8086, -0.7461, -0.9189,  ..., -2.2148,  0.3931,  5.2461],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0131,  0.0106,  0.0111,  ...,  0.0070, -0.0087, -0.0010],
        [ 0.0261,  0.0125,  0.0064,  ...,  0.0179, -0.0079,  0.0172],
        [ 0.0113, -0.0190, -0.0061,  ..., -0.0095,  0.0125, -0.0006],
        ...,
        [-0.0097, -0.0130, -0.0065,  ...,  0.0042,  0.0021, -0.0045],
        [ 0.0020, -0.0005,  0.0208,  ..., -0.0358,  0.0096, -0.0027],
        [ 0.0031, -0.0174,  0.0081,  ..., -0.0020,  0.0105,  0.0037]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0033, -0.0032,  0.0012,  ...,  0.0013,  0.0002, -0.0008],
        [ 0.0003,  0.0011,  0.0006,  ...,  0.0013,  0.0011, -0.0018],
        [-0.0007, -0.0015, -0.0008,  ...,  0.0025, -0.0003,  0.0001],
        ...,
        [ 0.0010, -0.0010,  0.0028,  ...,  0.0029,  0.0022, -0.0021],
        [ 0.0007,  0.0022,  0.0002,  ..., -0.0010, -0.0020, -0.0028],
        [-0.0003,  0.0015, -0.0007,  ...,  0.0000, -0.0021, -0.0024]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.3691, -0.9907,  0.1699,  ..., -1.8203, -1.4883,  4.8906]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0627,  0.0900,  0.0137,  ..., -0.0851, -0.0270,  0.0833]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 07:40:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for happening is the active form of happen
following is the active form of follow
requiring is the active form of require
avoiding is the active form of avoid
continuing is the active form of continue
operating is the active form of operate
teaching is the active form of teach
allowing is the active form of
2024-07-04 07:40:25 root INFO     [order_1_approx] starting weight calculation for allowing is the active form of allow
happening is the active form of happen
following is the active form of follow
requiring is the active form of require
operating is the active form of operate
teaching is the active form of teach
continuing is the active form of continue
avoiding is the active form of
2024-07-04 07:40:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 07:44:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 07:48:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3716, -0.8491,  0.6260,  ..., -0.1077, -0.1299, -0.4863],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3882, -0.9277,  0.6167,  ..., -0.1026, -0.1511, -0.5420],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0215, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.5000,  0.6206, -1.9238,  ...,  0.1475,  2.4219,  1.1406],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0167,  0.0247,  0.0171,  ..., -0.0214, -0.0145,  0.0009],
        [-0.0036,  0.0063,  0.0020,  ...,  0.0027, -0.0073,  0.0007],
        [ 0.0099, -0.0076,  0.0117,  ..., -0.0106,  0.0005, -0.0059],
        ...,
        [-0.0378,  0.0155,  0.0064,  ..., -0.0308, -0.0047,  0.0385],
        [-0.0195,  0.0307,  0.0181,  ..., -0.0261, -0.0071,  0.0359],
        [ 0.0191,  0.0043,  0.0072,  ..., -0.0026,  0.0130, -0.0208]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.3695e-03, -2.9221e-03,  1.2455e-03,  ...,  5.7411e-04,
          9.2840e-04, -2.1610e-03],
        [ 2.8658e-04,  1.1024e-03,  1.6842e-03,  ...,  8.8692e-05,
         -1.1814e-04, -4.0030e-04],
        [-1.6809e-04,  8.0776e-04,  1.0099e-03,  ...,  1.7967e-03,
         -4.1699e-04,  7.4863e-04],
        ...,
        [ 1.5278e-03,  9.5463e-04,  1.6184e-03,  ..., -1.0347e-03,
          1.5984e-03, -5.7745e-04],
        [-5.9175e-04, -5.6505e-04, -5.6314e-04,  ...,  7.5769e-04,
          2.0294e-03, -2.4033e-04],
        [-2.7394e-04, -4.2057e-04, -1.0881e-03,  ...,  3.7360e-04,
         -1.2684e-03, -6.2466e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.7930,  1.4668, -1.4805,  ..., -1.4072,  2.3281,  1.4912]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1710,  0.0887, -0.0435,  ..., -0.1271,  0.1477, -0.0322]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 07:48:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for allowing is the active form of allow
happening is the active form of happen
following is the active form of follow
requiring is the active form of require
operating is the active form of operate
teaching is the active form of teach
continuing is the active form of continue
avoiding is the active form of
2024-07-04 07:48:17 root INFO     [order_1_approx] starting weight calculation for happening is the active form of happen
requiring is the active form of require
operating is the active form of operate
allowing is the active form of allow
teaching is the active form of teach
avoiding is the active form of avoid
following is the active form of follow
continuing is the active form of
2024-07-04 07:48:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 07:52:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 07:56:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0483, -1.1230,  0.9946,  ..., -0.1812,  1.2373, -0.0976],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.0512, -1.1602,  0.9326,  ..., -0.1678,  1.1943, -0.1218],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0136, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.2969, -2.1055, -4.6367,  ...,  0.5010, -0.0122, -0.1230],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0147, -0.0051,  0.0196,  ..., -0.0050, -0.0081, -0.0121],
        [ 0.0044,  0.0102,  0.0013,  ...,  0.0166,  0.0105,  0.0081],
        [ 0.0105, -0.0038, -0.0149,  ..., -0.0008,  0.0047, -0.0092],
        ...,
        [-0.0080, -0.0148, -0.0117,  ...,  0.0069,  0.0134, -0.0089],
        [-0.0100,  0.0093,  0.0022,  ..., -0.0153, -0.0072,  0.0140],
        [ 0.0050,  0.0191,  0.0086,  ...,  0.0006,  0.0131,  0.0030]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.0695e-03, -7.2145e-04,  2.1763e-03,  ...,  7.7438e-04,
          1.9493e-03, -1.2636e-03],
        [ 8.2016e-04, -4.6778e-04, -2.7776e-04,  ..., -2.8133e-05,
          1.8711e-03, -1.2732e-03],
        [ 6.1560e-04,  1.3208e-04, -8.3447e-05,  ...,  2.2449e-03,
          6.1703e-04, -3.5667e-04],
        ...,
        [ 4.2844e-04, -2.0180e-03,  2.7895e-04,  ...,  1.4114e-04,
          3.3617e-04, -3.0851e-04],
        [-7.8440e-05, -1.1225e-03,  1.0300e-04,  ..., -4.0531e-06,
         -1.2608e-03,  5.6553e-04],
        [-6.1178e-04,  8.8811e-05, -2.4948e-03,  ..., -1.1873e-03,
         -1.0338e-03,  2.0390e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.4844, -1.1426, -3.6562,  ...,  0.7070, -0.4004,  0.4375]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1726, -0.0869,  0.0030,  ..., -0.0801,  0.1221,  0.1555]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 07:56:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for happening is the active form of happen
requiring is the active form of require
operating is the active form of operate
allowing is the active form of allow
teaching is the active form of teach
avoiding is the active form of avoid
following is the active form of follow
continuing is the active form of
2024-07-04 07:56:09 root INFO     [order_1_approx] starting weight calculation for operating is the active form of operate
avoiding is the active form of avoid
teaching is the active form of teach
requiring is the active form of require
following is the active form of follow
continuing is the active form of continue
allowing is the active form of allow
happening is the active form of
2024-07-04 07:56:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 08:00:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 08:04:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1924,  0.1172,  0.9478,  ..., -0.3914,  1.1338, -0.7129],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.1907,  0.1044,  0.8999,  ..., -0.3733,  1.1074, -0.7554],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0172, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.8008, -0.7168, -3.3633,  ..., -4.6094,  5.1484,  4.9062],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0075, -0.0055,  0.0043,  ...,  0.0128,  0.0003, -0.0053],
        [ 0.0070, -0.0074, -0.0116,  ...,  0.0094,  0.0067, -0.0164],
        [-0.0126,  0.0127,  0.0052,  ...,  0.0087, -0.0005, -0.0077],
        ...,
        [-0.0257, -0.0080,  0.0068,  ...,  0.0028, -0.0090, -0.0066],
        [-0.0157, -0.0043, -0.0013,  ..., -0.0060, -0.0220,  0.0142],
        [ 0.0005,  0.0199,  0.0143,  ..., -0.0201,  0.0010, -0.0161]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-9.3842e-04, -2.5749e-03,  2.7657e-04,  ...,  3.0117e-03,
          1.6508e-03, -1.3485e-03],
        [ 7.3528e-04, -3.1686e-04,  1.3838e-03,  ...,  2.0638e-03,
          7.3242e-04, -1.8082e-03],
        [ 3.2377e-04, -4.4537e-04,  5.7745e-04,  ...,  8.1158e-04,
         -1.8806e-03, -1.6041e-03],
        ...,
        [ 1.2283e-03, -2.4815e-03,  1.4143e-03,  ...,  9.5844e-04,
          2.6894e-04, -1.4722e-04],
        [-1.0662e-03, -7.9274e-05, -5.4312e-04,  ...,  1.0300e-03,
         -1.0109e-03,  4.5133e-04],
        [-7.1764e-05,  2.1725e-03,  7.5483e-04,  ...,  2.1577e-04,
         -6.5088e-05, -4.0674e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.6582,  0.0918, -3.5781,  ..., -5.4805,  4.7031,  4.5156]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1221,  0.0803, -0.1511,  ..., -0.1222, -0.0992, -0.1453]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 08:04:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for operating is the active form of operate
avoiding is the active form of avoid
teaching is the active form of teach
requiring is the active form of require
following is the active form of follow
continuing is the active form of continue
allowing is the active form of allow
happening is the active form of
2024-07-04 08:04:03 root INFO     [order_1_approx] starting weight calculation for continuing is the active form of continue
following is the active form of follow
operating is the active form of operate
allowing is the active form of allow
teaching is the active form of teach
happening is the active form of happen
avoiding is the active form of avoid
requiring is the active form of
2024-07-04 08:04:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 08:08:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 08:11:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6694, -0.5303,  0.2502,  ..., -0.1519,  0.8232,  0.4541],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.7026, -0.5898,  0.2404,  ..., -0.1494,  0.8364,  0.4648],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0255, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.0156, -0.1084, -2.4883,  ..., -0.4912,  0.4436,  1.8750],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0073, -0.0035,  0.0112,  ..., -0.0047,  0.0122,  0.0062],
        [-0.0035, -0.0058, -0.0084,  ...,  0.0008, -0.0111, -0.0004],
        [ 0.0170,  0.0079, -0.0045,  ..., -0.0012, -0.0054,  0.0011],
        ...,
        [-0.0205, -0.0198, -0.0170,  ..., -0.0015,  0.0106,  0.0037],
        [-0.0254,  0.0155,  0.0133,  ..., -0.0251, -0.0330,  0.0014],
        [-0.0066,  0.0154,  0.0173,  ..., -0.0045,  0.0047, -0.0148]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-9.9087e-04, -2.3766e-03,  4.7636e-04,  ...,  1.4524e-03,
          1.3475e-03,  4.3273e-04],
        [-5.9509e-04,  4.2534e-04,  6.6757e-05,  ...,  1.1282e-03,
          8.1873e-04, -5.4693e-04],
        [ 9.9182e-04,  1.7910e-03, -1.4858e-03,  ...,  3.3903e-04,
          8.6975e-04, -4.1485e-05],
        ...,
        [-9.4795e-04, -7.4053e-04, -1.7262e-03,  ...,  2.9030e-03,
          1.4095e-03, -1.8864e-03],
        [-9.0647e-04,  1.6499e-03, -8.1491e-04,  ..., -1.0605e-03,
         -1.0357e-03, -1.2932e-03],
        [ 1.3161e-03,  5.4216e-04,  3.1948e-04,  ..., -7.2479e-04,
         -2.1744e-04, -1.4734e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.8008, -0.0314, -2.2070,  ..., -1.4092, -0.6230,  1.0742]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0605, -0.0464, -0.0549,  ..., -0.4302, -0.1793,  0.0250]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 08:11:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for continuing is the active form of continue
following is the active form of follow
operating is the active form of operate
allowing is the active form of allow
teaching is the active form of teach
happening is the active form of happen
avoiding is the active form of avoid
requiring is the active form of
2024-07-04 08:11:56 root INFO     [order_1_approx] starting weight calculation for avoiding is the active form of avoid
allowing is the active form of allow
requiring is the active form of require
teaching is the active form of teach
happening is the active form of happen
operating is the active form of operate
continuing is the active form of continue
following is the active form of
2024-07-04 08:11:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 08:15:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 08:19:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2100, -1.0508,  2.1230,  ..., -0.0583,  0.4229,  0.8345],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 1.2969, -1.1689,  2.1621,  ..., -0.0536,  0.4304,  0.8916],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0130, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.9922, -1.7441, -3.8203,  ..., -4.9805,  1.5303,  1.6973],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.4820e-03, -3.0041e-03,  1.0864e-02,  ...,  2.4643e-03,
         -5.8670e-03, -1.8570e-02],
        [ 5.2986e-03,  1.2634e-02,  8.1177e-03,  ...,  5.6229e-03,
          5.5313e-05,  6.1302e-03],
        [ 2.2030e-03, -4.1885e-03,  4.5052e-03,  ...,  6.3705e-03,
          3.5019e-03, -5.4131e-03],
        ...,
        [-3.2227e-02, -1.6060e-03,  1.3565e-02,  ...,  7.7515e-03,
         -1.6296e-02,  1.1818e-02],
        [-9.4147e-03,  3.0441e-03,  7.5836e-03,  ..., -7.0953e-03,
          7.0190e-04,  1.8036e-02],
        [ 2.0065e-02, -2.1896e-03, -1.2980e-03,  ..., -1.6068e-02,
          6.2943e-03, -9.8495e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.5812e-03, -1.4200e-03,  8.0204e-04,  ...,  1.0414e-03,
          1.2074e-03, -2.4128e-03],
        [-2.2388e-04,  7.8297e-04, -2.0480e-04,  ..., -5.3310e-04,
         -2.9922e-04, -9.9659e-05],
        [-9.2685e-05, -2.3651e-03, -1.6794e-03,  ...,  5.1403e-04,
          1.8129e-03, -1.6785e-03],
        ...,
        [-8.4734e-04,  8.9073e-04,  9.3031e-04,  ...,  1.5450e-03,
          6.2180e-04, -9.2173e-04],
        [-8.2302e-04,  1.5879e-04,  1.1673e-03,  ..., -7.9727e-04,
          1.9050e-04, -7.9346e-04],
        [ 3.3951e-04, -4.3011e-04,  3.4285e-04,  ..., -9.6321e-05,
         -6.9618e-04, -1.3208e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.9375, -0.6748, -4.4570,  ..., -5.8672,  2.5332,  1.2334]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0862,  0.2008, -0.0770,  ..., -0.4131,  0.0276, -0.0636]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 08:19:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for avoiding is the active form of avoid
allowing is the active form of allow
requiring is the active form of require
teaching is the active form of teach
happening is the active form of happen
operating is the active form of operate
continuing is the active form of continue
following is the active form of
2024-07-04 08:19:48 root INFO     [order_1_approx] starting weight calculation for avoiding is the active form of avoid
happening is the active form of happen
following is the active form of follow
continuing is the active form of continue
allowing is the active form of allow
teaching is the active form of teach
requiring is the active form of require
operating is the active form of
2024-07-04 08:19:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 08:23:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 08:27:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2651, -1.0576,  1.0791,  ..., -0.0857,  0.9243,  0.2881],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.2778, -1.1621,  1.0781,  ..., -0.0820,  0.9453,  0.2881],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0130, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 6.8633, -0.7793, -0.4668,  ..., -1.2695, -0.0635,  2.9414],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0035, -0.0148,  0.0085,  ...,  0.0083, -0.0013, -0.0004],
        [ 0.0150, -0.0174, -0.0047,  ...,  0.0167,  0.0062, -0.0064],
        [ 0.0128,  0.0023,  0.0025,  ..., -0.0062,  0.0040, -0.0076],
        ...,
        [-0.0277,  0.0038, -0.0060,  ..., -0.0089,  0.0053,  0.0093],
        [-0.0032,  0.0062,  0.0129,  ..., -0.0096, -0.0103, -0.0017],
        [-0.0124,  0.0148,  0.0070,  ..., -0.0031,  0.0002, -0.0098]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.4601e-03, -1.4124e-03,  1.0023e-03,  ...,  5.0926e-04,
          6.2084e-04, -7.6580e-04],
        [ 5.1117e-04,  1.3971e-04,  3.7694e-04,  ...,  4.4107e-04,
          3.7384e-04, -9.7179e-04],
        [ 1.4648e-03,  2.7657e-04, -1.4782e-03,  ..., -5.1379e-05,
          6.3539e-05, -7.6675e-04],
        ...,
        [-4.6229e-04, -5.6410e-04, -6.8665e-04,  ...,  6.0129e-04,
          1.0042e-03,  3.4332e-04],
        [-3.5310e-04,  4.9770e-05,  1.2894e-03,  ...,  2.1636e-04,
         -2.1458e-06, -1.0452e-03],
        [-2.8896e-04,  2.9516e-04, -1.1044e-03,  ...,  4.6492e-04,
         -4.3058e-04, -9.4748e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.6719, -0.0830,  0.0796,  ..., -0.6313, -0.3010,  3.6719]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2028,  0.0562, -0.1774,  ..., -0.2905,  0.2269,  0.0630]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 08:27:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for avoiding is the active form of avoid
happening is the active form of happen
following is the active form of follow
continuing is the active form of continue
allowing is the active form of allow
teaching is the active form of teach
requiring is the active form of require
operating is the active form of
2024-07-04 08:27:40 root INFO     [order_1_approx] starting weight calculation for allowing is the active form of allow
happening is the active form of happen
following is the active form of follow
requiring is the active form of require
continuing is the active form of continue
operating is the active form of operate
avoiding is the active form of avoid
teaching is the active form of
2024-07-04 08:27:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 08:31:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 08:35:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6016,  0.2107,  1.4746,  ..., -0.6235,  0.4521,  0.1004],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.6191,  0.2058,  1.4443,  ..., -0.6147,  0.4421,  0.0820],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0215, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.7305, -0.3652, -1.1602,  ..., -1.4424,  1.3555,  1.7227],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0117,  0.0095,  0.0047,  ..., -0.0059, -0.0057, -0.0007],
        [ 0.0004, -0.0079,  0.0120,  ..., -0.0076, -0.0025, -0.0017],
        [ 0.0058, -0.0048, -0.0179,  ..., -0.0041, -0.0116, -0.0109],
        ...,
        [ 0.0035, -0.0084,  0.0056,  ...,  0.0031,  0.0071,  0.0108],
        [ 0.0160,  0.0117,  0.0070,  ...,  0.0004,  0.0038,  0.0056],
        [ 0.0014, -0.0069, -0.0073,  ...,  0.0029,  0.0049, -0.0064]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.0881e-03, -4.2725e-04, -1.3351e-05,  ..., -5.4502e-04,
          5.6410e-04, -8.4114e-04],
        [ 2.4624e-03,  3.3545e-04,  3.2973e-04,  ..., -8.9169e-05,
          1.6747e-03,  1.5736e-03],
        [-9.1791e-05,  1.3638e-04, -3.0994e-05,  ...,  9.3842e-04,
         -5.3644e-04, -2.6608e-03],
        ...,
        [ 1.3027e-03,  7.4148e-05,  8.4782e-04,  ...,  1.5631e-03,
          2.4509e-03, -1.7786e-04],
        [-6.2704e-04,  1.5221e-03, -2.9707e-04,  ..., -7.1859e-04,
         -3.1757e-04, -9.1362e-04],
        [-2.8062e-04,  9.6917e-05, -6.6948e-04,  ...,  1.7166e-03,
         -2.5988e-05, -1.6146e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.0430,  0.1484, -0.4976,  ..., -2.4922,  0.6279,  1.9219]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0321,  0.0246, -0.2023,  ..., -0.1426,  0.0656,  0.1085]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 08:35:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for allowing is the active form of allow
happening is the active form of happen
following is the active form of follow
requiring is the active form of require
continuing is the active form of continue
operating is the active form of operate
avoiding is the active form of avoid
teaching is the active form of
2024-07-04 08:35:32 root INFO     total operator prediction time: 3776.0759489536285 seconds
2024-07-04 08:35:32 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on un+adj_reg
2024-07-04 08:35:34 root INFO     building operator un+adj_reg
2024-07-04 08:35:34 root INFO     [order_1_approx] starting weight calculation for The opposite of biased is unbiased
The opposite of acceptable is unacceptable
The opposite of certain is uncertain
The opposite of satisfactory is unsatisfactory
The opposite of comfortable is uncomfortable
The opposite of published is unpublished
The opposite of reasonable is unreasonable
The opposite of conscious is
2024-07-04 08:35:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 08:39:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 08:43:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8740,  0.8877, -0.4385,  ...,  0.6543,  0.4912, -0.7988],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.7949,  0.8135, -0.3999,  ...,  0.5752,  0.4211, -0.7769],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.7812, -0.8076,  0.2212,  ..., -2.5801,  1.5186, -2.4863],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0006,  0.0039, -0.0119,  ...,  0.0113,  0.0002, -0.0176],
        [ 0.0049, -0.0121,  0.0002,  ..., -0.0015, -0.0198,  0.0094],
        [ 0.0059, -0.0079, -0.0370,  ...,  0.0218, -0.0022,  0.0175],
        ...,
        [ 0.0012, -0.0127, -0.0016,  ..., -0.0176,  0.0139, -0.0027],
        [-0.0062, -0.0020,  0.0322,  ..., -0.0101, -0.0149, -0.0177],
        [ 0.0166,  0.0047, -0.0098,  ..., -0.0033, -0.0014, -0.0174]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.5707e-03, -8.5413e-05, -2.1553e-04,  ...,  7.7534e-04,
         -1.3733e-03, -4.0317e-04],
        [-6.6853e-04, -2.4319e-03,  7.7868e-04,  ...,  1.2817e-03,
          8.4496e-04,  1.7815e-03],
        [ 2.8172e-03, -1.2169e-03,  7.1907e-04,  ...,  1.5526e-03,
         -6.9141e-04,  2.5940e-03],
        ...,
        [ 1.2970e-03, -3.9577e-04, -4.7922e-04,  ..., -3.8862e-04,
         -5.2071e-04, -9.7179e-04],
        [ 2.7809e-03,  7.4816e-04,  2.9707e-04,  ..., -9.5367e-04,
         -3.1719e-03,  8.8978e-04],
        [ 1.2255e-03, -2.1954e-03, -8.3733e-04,  ..., -3.7766e-04,
         -6.9475e-04,  1.7500e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.2695, -2.0059,  1.4609,  ..., -1.0234,  1.2627, -3.3828]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.3589, -0.0166,  0.2251,  ..., -0.1786,  0.0270,  0.0731]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 08:43:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of biased is unbiased
The opposite of acceptable is unacceptable
The opposite of certain is uncertain
The opposite of satisfactory is unsatisfactory
The opposite of comfortable is uncomfortable
The opposite of published is unpublished
The opposite of reasonable is unreasonable
The opposite of conscious is
2024-07-04 08:43:24 root INFO     [order_1_approx] starting weight calculation for The opposite of biased is unbiased
The opposite of published is unpublished
The opposite of certain is uncertain
The opposite of acceptable is unacceptable
The opposite of reasonable is unreasonable
The opposite of comfortable is uncomfortable
The opposite of conscious is unconscious
The opposite of satisfactory is
2024-07-04 08:43:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 08:47:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 08:51:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4570, -0.7471,  0.3950,  ..., -0.7563,  0.3003,  0.8906],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3848, -0.6836,  0.3079,  ..., -0.6274,  0.2306,  0.7539],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.3193, -0.2886, -3.0039,  ...,  1.0283,  0.6216,  1.4160],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0156,  0.0038, -0.0159,  ...,  0.0135, -0.0073, -0.0242],
        [ 0.0032, -0.0267, -0.0068,  ..., -0.0170, -0.0417,  0.0076],
        [-0.0023, -0.0238, -0.0173,  ..., -0.0062, -0.0128,  0.0143],
        ...,
        [-0.0146, -0.0201, -0.0033,  ..., -0.0126, -0.0097,  0.0117],
        [-0.0239,  0.0013,  0.0353,  ..., -0.0099, -0.0010, -0.0078],
        [-0.0068,  0.0270, -0.0112,  ..., -0.0218,  0.0077, -0.0079]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 1.3542e-03,  1.4505e-03, -1.7252e-03,  ..., -2.1095e-03,
          1.0481e-03,  1.5764e-03],
        [-1.2827e-03, -2.7485e-03,  1.6804e-03,  ...,  1.3237e-03,
         -1.1187e-03,  5.1346e-03],
        [ 5.0640e-04,  8.0347e-05,  4.0674e-04,  ...,  9.6989e-04,
         -3.4180e-03,  2.5773e-04],
        ...,
        [ 1.4191e-03, -2.9564e-04,  5.2977e-04,  ...,  1.7290e-03,
          8.4591e-04, -1.5516e-03],
        [-5.2357e-04, -5.2929e-04,  1.1292e-03,  ..., -5.9605e-04,
          1.5631e-03, -2.0599e-04],
        [-4.2496e-03, -3.0851e-04, -1.2589e-03,  ..., -8.9169e-04,
         -1.5345e-03, -1.7281e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1299, -0.7686, -3.2207,  ...,  0.3667,  1.0801, -0.3203]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0529, -0.0867,  0.4324,  ...,  0.2438, -0.0392,  0.0042]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 08:51:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of biased is unbiased
The opposite of published is unpublished
The opposite of certain is uncertain
The opposite of acceptable is unacceptable
The opposite of reasonable is unreasonable
The opposite of comfortable is uncomfortable
The opposite of conscious is unconscious
The opposite of satisfactory is
2024-07-04 08:51:14 root INFO     [order_1_approx] starting weight calculation for The opposite of published is unpublished
The opposite of acceptable is unacceptable
The opposite of comfortable is uncomfortable
The opposite of certain is uncertain
The opposite of conscious is unconscious
The opposite of satisfactory is unsatisfactory
The opposite of biased is unbiased
The opposite of reasonable is
2024-07-04 08:51:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 08:55:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 08:59:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6465, -1.1289, -0.5488,  ..., -0.5488,  0.7832,  0.8340],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.5469, -1.0156, -0.4661,  ..., -0.4529,  0.6392,  0.7017],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.5010, -2.7891,  0.4062,  ...,  0.6636,  4.5820,  2.1367],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0006, -0.0065, -0.0112,  ..., -0.0019, -0.0054,  0.0025],
        [-0.0114,  0.0141, -0.0013,  ..., -0.0043, -0.0046, -0.0027],
        [ 0.0010,  0.0049, -0.0123,  ...,  0.0006, -0.0094,  0.0023],
        ...,
        [-0.0119, -0.0228,  0.0014,  ...,  0.0224,  0.0033,  0.0014],
        [-0.0120,  0.0022,  0.0416,  ...,  0.0011, -0.0101, -0.0044],
        [-0.0061, -0.0085,  0.0097,  ..., -0.0120,  0.0014,  0.0022]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0011,  0.0041, -0.0022,  ..., -0.0030,  0.0015,  0.0022],
        [ 0.0014, -0.0026, -0.0001,  ...,  0.0024, -0.0017, -0.0008],
        [-0.0004,  0.0003, -0.0012,  ...,  0.0027,  0.0009, -0.0027],
        ...,
        [-0.0004, -0.0028, -0.0004,  ...,  0.0041, -0.0007, -0.0010],
        [-0.0007,  0.0013,  0.0001,  ..., -0.0047,  0.0007,  0.0005],
        [-0.0007,  0.0004,  0.0013,  ..., -0.0010, -0.0007, -0.0011]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1777, -4.4023, -0.8037,  ...,  2.3125,  4.8984,  2.9492]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1212,  0.1340, -0.0856,  ...,  0.4209,  0.1475,  0.1025]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 08:59:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of published is unpublished
The opposite of acceptable is unacceptable
The opposite of comfortable is uncomfortable
The opposite of certain is uncertain
The opposite of conscious is unconscious
The opposite of satisfactory is unsatisfactory
The opposite of biased is unbiased
The opposite of reasonable is
2024-07-04 08:59:06 root INFO     [order_1_approx] starting weight calculation for The opposite of certain is uncertain
The opposite of comfortable is uncomfortable
The opposite of acceptable is unacceptable
The opposite of satisfactory is unsatisfactory
The opposite of biased is unbiased
The opposite of reasonable is unreasonable
The opposite of conscious is unconscious
The opposite of published is
2024-07-04 08:59:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 09:03:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 09:06:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.4268, 0.4233, 0.7734,  ..., 1.1289, 0.8052, 0.4351], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([0.3870, 0.3796, 0.6675,  ..., 1.0010, 0.7114, 0.3826], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9844, -3.5508,  2.9688,  ...,  2.7500,  2.8984, -2.3809],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.9449e-03,  2.8801e-03,  5.4550e-04,  ...,  4.6005e-03,
          5.2452e-05, -2.8305e-03],
        [-1.6663e-02,  7.5684e-03,  5.6229e-03,  ...,  2.3174e-03,
         -1.0384e-02, -1.8158e-03],
        [-1.9455e-03,  2.2079e-02, -5.6915e-03,  ..., -7.2670e-03,
         -1.8021e-02,  2.2675e-02],
        ...,
        [-6.1722e-03, -1.3535e-02, -6.7825e-03,  ...,  3.3951e-03,
          2.6207e-03,  2.3384e-03],
        [-2.7828e-03,  5.0583e-03,  5.6915e-03,  ...,  1.5059e-03,
         -1.7670e-02,  2.0355e-02],
        [ 1.8250e-02, -2.6951e-03,  1.2489e-02,  ..., -2.2003e-02,
         -8.7738e-05, -7.4844e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.3418e-03,  7.9155e-04, -2.8133e-04,  ..., -2.5558e-04,
         -1.7118e-04,  6.1750e-04],
        [ 1.4296e-03, -3.8719e-03, -8.8596e-04,  ...,  1.0490e-03,
         -6.1226e-04, -1.4210e-03],
        [ 2.7466e-03, -3.0088e-04,  1.7986e-03,  ..., -1.5175e-04,
         -4.9257e-04, -3.1948e-05],
        ...,
        [-1.9722e-03,  2.3689e-03, -9.9373e-04,  ..., -1.5335e-03,
          1.7281e-03, -1.4067e-03],
        [ 1.0401e-04, -5.5313e-04,  8.4400e-04,  ..., -1.5330e-04,
         -1.1692e-03,  2.6011e-04],
        [ 5.8937e-04, -3.9673e-04,  3.1624e-03,  ..., -2.2278e-03,
         -1.6680e-03,  2.5024e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4297, -3.4922,  1.9219,  ...,  3.9316,  1.2637, -2.8125]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.4653,  0.0936, -0.1597,  ...,  0.4871, -0.3169,  0.3232]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 09:06:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of certain is uncertain
The opposite of comfortable is uncomfortable
The opposite of acceptable is unacceptable
The opposite of satisfactory is unsatisfactory
The opposite of biased is unbiased
The opposite of reasonable is unreasonable
The opposite of conscious is unconscious
The opposite of published is
2024-07-04 09:06:58 root INFO     [order_1_approx] starting weight calculation for The opposite of conscious is unconscious
The opposite of published is unpublished
The opposite of comfortable is uncomfortable
The opposite of biased is unbiased
The opposite of satisfactory is unsatisfactory
The opposite of reasonable is unreasonable
The opposite of acceptable is unacceptable
The opposite of certain is
2024-07-04 09:06:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 09:10:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 09:14:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3638,  0.0142, -0.1428,  ..., -0.6309,  0.5254,  0.4509],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3774, -0.0054, -0.1586,  ..., -0.6382,  0.5249,  0.4590],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3242, -1.0156,  0.1162,  ...,  1.1133,  4.9023, -4.5703],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0074,  0.0028, -0.0077,  ...,  0.0057, -0.0105,  0.0029],
        [ 0.0023, -0.0062, -0.0175,  ..., -0.0061,  0.0014,  0.0161],
        [-0.0123,  0.0267, -0.0248,  ...,  0.0117, -0.0282,  0.0059],
        ...,
        [-0.0116, -0.0222, -0.0033,  ..., -0.0376, -0.0038,  0.0204],
        [-0.0079,  0.0048, -0.0049,  ...,  0.0233,  0.0068, -0.0087],
        [ 0.0085, -0.0330, -0.0078,  ..., -0.0096, -0.0144, -0.0222]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-5.8975e-03, -1.3580e-03, -8.8215e-04,  ...,  2.1763e-03,
         -2.4033e-04,  1.5268e-03],
        [ 4.9400e-04, -1.9302e-03,  4.8733e-04,  ...,  2.0885e-04,
          9.7179e-04,  9.8705e-04],
        [ 2.1648e-04,  1.1377e-03, -5.0688e-04,  ..., -1.7052e-03,
         -5.0879e-04, -3.3073e-03],
        ...,
        [ 6.7139e-04, -2.5024e-03,  4.0245e-04,  ..., -2.9755e-03,
         -1.0014e-05,  7.7391e-04],
        [-2.9349e-04, -9.7275e-04,  1.4992e-03,  ...,  9.4700e-04,
         -2.4834e-03,  8.2207e-04],
        [ 3.5057e-03,  1.1835e-03,  9.2888e-04,  ..., -2.6488e-04,
          5.3120e-04,  1.4172e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.9316e-01, -7.9883e-01,  2.0142e-03,  ...,  1.7920e+00,
          4.3047e+00, -3.5859e+00]], device='cuda:1', dtype=torch.float16,
       grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.3035, -0.2695,  0.4409,  ...,  0.6479, -0.1486, -0.1081]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 09:14:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of conscious is unconscious
The opposite of published is unpublished
The opposite of comfortable is uncomfortable
The opposite of biased is unbiased
The opposite of satisfactory is unsatisfactory
The opposite of reasonable is unreasonable
The opposite of acceptable is unacceptable
The opposite of certain is
2024-07-04 09:14:45 root INFO     [order_1_approx] starting weight calculation for The opposite of acceptable is unacceptable
The opposite of biased is unbiased
The opposite of conscious is unconscious
The opposite of reasonable is unreasonable
The opposite of published is unpublished
The opposite of satisfactory is unsatisfactory
The opposite of certain is uncertain
The opposite of comfortable is
2024-07-04 09:14:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 09:18:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 09:22:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6953, -0.0298, -1.3574,  ...,  0.0391,  1.3965,  0.6992],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.5952, -0.0493, -1.1416,  ...,  0.0311,  1.1709,  0.5903],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.7344,  1.1875, -2.9727,  ..., -0.7207,  3.9727,  3.8574],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0004, -0.0400, -0.0200,  ..., -0.0001,  0.0074, -0.0341],
        [-0.0208, -0.0100, -0.0100,  ..., -0.0124, -0.0055,  0.0010],
        [-0.0008,  0.0202, -0.0389,  ...,  0.0235, -0.0223,  0.0173],
        ...,
        [-0.0143,  0.0010, -0.0216,  ...,  0.0046, -0.0145,  0.0013],
        [-0.0154, -0.0105,  0.0352,  ...,  0.0007,  0.0025,  0.0003],
        [ 0.0003,  0.0169, -0.0230,  ..., -0.0109,  0.0032, -0.0172]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-5.8413e-04, -3.4332e-03, -2.0466e-03,  ..., -1.2803e-04,
         -7.8201e-05, -7.5102e-05],
        [-1.0462e-03, -1.0157e-03,  6.8855e-04,  ..., -7.7057e-04,
         -1.1435e-03,  1.6289e-03],
        [ 8.2874e-04,  1.7376e-03,  5.1069e-04,  ...,  4.3344e-04,
         -1.0042e-03, -1.5926e-03],
        ...,
        [-1.9283e-03,  2.1911e-04, -8.9979e-04,  ...,  1.0681e-03,
          1.4343e-03, -3.0899e-03],
        [ 6.2656e-04, -2.0742e-04,  5.3835e-04,  ...,  1.5697e-03,
         -2.6369e-04, -1.7681e-03],
        [-1.4372e-03,  3.0823e-03, -3.6507e-03,  ..., -2.6894e-03,
         -1.7166e-03, -1.9798e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.2734,  1.4053, -3.0332,  ..., -1.8936,  4.6250,  3.2891]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1410, -0.1248,  0.2944,  ...,  0.1969, -0.2515,  0.0370]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 09:22:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of acceptable is unacceptable
The opposite of biased is unbiased
The opposite of conscious is unconscious
The opposite of reasonable is unreasonable
The opposite of published is unpublished
The opposite of satisfactory is unsatisfactory
The opposite of certain is uncertain
The opposite of comfortable is
2024-07-04 09:22:33 root INFO     [order_1_approx] starting weight calculation for The opposite of biased is unbiased
The opposite of certain is uncertain
The opposite of reasonable is unreasonable
The opposite of conscious is unconscious
The opposite of comfortable is uncomfortable
The opposite of satisfactory is unsatisfactory
The opposite of published is unpublished
The opposite of acceptable is
2024-07-04 09:22:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 09:26:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 09:30:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4736, -1.1377,  0.6191,  ..., -0.3953,  0.7490,  0.2030],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.4106, -1.0107,  0.4856,  ..., -0.3218,  0.6035,  0.1476],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0371, -3.0156,  0.5137,  ...,  4.1797,  4.3516, -0.3945],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0045, -0.0024, -0.0119,  ...,  0.0069,  0.0066, -0.0095],
        [ 0.0114,  0.0057, -0.0072,  ..., -0.0003, -0.0152, -0.0093],
        [-0.0099, -0.0251, -0.0219,  ..., -0.0286,  0.0034,  0.0046],
        ...,
        [-0.0109,  0.0062, -0.0390,  ...,  0.0193,  0.0224,  0.0027],
        [-0.0167, -0.0145,  0.0565,  ...,  0.0146, -0.0391,  0.0134],
        [ 0.0203,  0.0107, -0.0242,  ..., -0.0367,  0.0291, -0.0210]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0033,  0.0003, -0.0038,  ...,  0.0003,  0.0022, -0.0003],
        [-0.0018,  0.0004, -0.0053,  ...,  0.0066, -0.0013,  0.0034],
        [ 0.0040,  0.0076, -0.0040,  ..., -0.0006, -0.0028,  0.0022],
        ...,
        [-0.0048, -0.0031,  0.0024,  ...,  0.0008,  0.0011, -0.0045],
        [ 0.0019,  0.0020,  0.0011,  ..., -0.0058, -0.0021,  0.0012],
        [-0.0021, -0.0033, -0.0012,  ..., -0.0055, -0.0008, -0.0010]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6104, -3.3398, -0.2046,  ...,  2.3691,  5.6719, -0.6372]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0401,  0.1431,  0.2937,  ..., -0.1160, -0.3030,  0.0373]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 09:30:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of biased is unbiased
The opposite of certain is uncertain
The opposite of reasonable is unreasonable
The opposite of conscious is unconscious
The opposite of comfortable is uncomfortable
The opposite of satisfactory is unsatisfactory
The opposite of published is unpublished
The opposite of acceptable is
2024-07-04 09:30:24 root INFO     [order_1_approx] starting weight calculation for The opposite of comfortable is uncomfortable
The opposite of conscious is unconscious
The opposite of published is unpublished
The opposite of reasonable is unreasonable
The opposite of certain is uncertain
The opposite of satisfactory is unsatisfactory
The opposite of acceptable is unacceptable
The opposite of biased is
2024-07-04 09:30:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 09:34:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 09:38:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0332, -0.6943, -1.1680,  ...,  0.3757,  0.7217, -0.3545],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.8716, -0.6245, -0.9604,  ...,  0.3069,  0.5825, -0.3340],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([0.9727, 3.9160, 1.0859,  ..., 0.6631, 3.4082, 2.8320], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0029, -0.0243, -0.0222,  ...,  0.0076,  0.0049, -0.0275],
        [-0.0023, -0.0002, -0.0146,  ..., -0.0020, -0.0226,  0.0238],
        [ 0.0019,  0.0054, -0.0013,  ..., -0.0194,  0.0189,  0.0066],
        ...,
        [ 0.0088, -0.0146,  0.0153,  ...,  0.0161,  0.0042, -0.0060],
        [-0.0336, -0.0051,  0.0005,  ..., -0.0071, -0.0234, -0.0105],
        [ 0.0124, -0.0142, -0.0032,  ...,  0.0015,  0.0153,  0.0060]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.0485e-03,  2.6345e-04,  3.8218e-04,  ...,  2.9678e-03,
          1.6098e-03,  1.0586e-03],
        [ 8.3923e-05, -2.8267e-03,  3.3493e-03,  ..., -8.1062e-04,
         -1.9989e-03,  2.3899e-03],
        [-8.0681e-04, -1.8673e-03,  7.1859e-04,  ..., -1.9236e-03,
          2.0351e-03,  5.5218e-04],
        ...,
        [ 2.5406e-03,  9.6321e-04,  1.5993e-03,  ..., -5.1117e-04,
         -1.4153e-03, -9.8419e-04],
        [ 9.7847e-04, -1.4191e-03,  1.6165e-03,  ..., -1.4553e-03,
         -1.1721e-03, -2.6073e-03],
        [-2.4796e-04, -1.3943e-03,  2.4033e-03,  ..., -7.3195e-05,
          1.9383e-04,  1.0271e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[1.7480, 3.5312, 1.3887,  ..., 0.9873, 2.3828, 2.6973]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.3247, -0.3115, -0.0205,  ..., -0.0216,  0.0564,  0.0305]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 09:38:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of comfortable is uncomfortable
The opposite of conscious is unconscious
The opposite of published is unpublished
The opposite of reasonable is unreasonable
The opposite of certain is uncertain
The opposite of satisfactory is unsatisfactory
The opposite of acceptable is unacceptable
The opposite of biased is
2024-07-04 09:38:13 root INFO     total operator prediction time: 3759.395793199539 seconds
2024-07-04 09:38:13 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on antonyms - binary
2024-07-04 09:38:16 root INFO     building operator antonyms - binary
2024-07-04 09:38:16 root INFO     [order_1_approx] starting weight calculation for The opposite of rise is sink
The opposite of below is above
The opposite of interior is exterior
The opposite of over is under
The opposite of west is east
The opposite of beginning is end
The opposite of south is north
The opposite of up is
2024-07-04 09:38:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 09:42:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 09:46:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0154, -0.3906, -0.2214,  ...,  0.0206, -0.4438,  0.4385],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.0128, -0.3938, -0.2092,  ...,  0.0247, -0.4307,  0.4021],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6367, -4.5859,  2.6719,  ...,  0.2324,  5.2188,  0.0225],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0138, -0.0149,  0.0077,  ..., -0.0065,  0.0145,  0.0240],
        [-0.0125,  0.0006, -0.0154,  ..., -0.0024, -0.0103,  0.0182],
        [-0.0328, -0.0128,  0.0023,  ..., -0.0056, -0.0038, -0.0226],
        ...,
        [-0.0108, -0.0054,  0.0101,  ...,  0.0025,  0.0047, -0.0034],
        [-0.0015,  0.0377,  0.0366,  ..., -0.0299,  0.0116, -0.0154],
        [-0.0100, -0.0161,  0.0023,  ...,  0.0049, -0.0021, -0.0138]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 1.4000e-03, -1.4381e-03, -4.4346e-04,  ...,  2.0790e-04,
         -1.6336e-03,  9.9659e-04],
        [-1.7643e-05, -2.9430e-03,  2.3441e-03,  ...,  4.6039e-04,
          3.0670e-03,  2.5320e-04],
        [-2.3878e-04, -2.6846e-04,  3.7861e-04,  ...,  1.5631e-03,
         -1.2360e-03, -8.5878e-04],
        ...,
        [ 1.7776e-03,  8.2493e-05,  8.0967e-04,  ..., -2.8038e-04,
          2.8687e-03,  1.3323e-03],
        [-9.4604e-04,  2.1610e-03,  9.9754e-04,  ...,  1.5945e-03,
          2.9397e-04,  3.1853e-03],
        [-9.8228e-04, -2.0447e-03, -7.2956e-04,  ..., -1.2541e-03,
         -1.9350e-03, -5.8889e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1602, -3.7344,  3.4336,  ..., -0.0635,  3.9336, -0.3354]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1617, -0.0203,  0.0988,  ..., -0.2554, -0.1167,  0.2720]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 09:46:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of rise is sink
The opposite of below is above
The opposite of interior is exterior
The opposite of over is under
The opposite of west is east
The opposite of beginning is end
The opposite of south is north
The opposite of up is
2024-07-04 09:46:06 root INFO     [order_1_approx] starting weight calculation for The opposite of up is down
The opposite of rise is sink
The opposite of over is under
The opposite of west is east
The opposite of beginning is end
The opposite of interior is exterior
The opposite of below is above
The opposite of south is
2024-07-04 09:46:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 09:50:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 09:53:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3962, -1.3906, -1.4404,  ..., -0.8174,  1.6572,  0.2981],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.3640, -1.3076, -1.2559,  ..., -0.7061,  1.4570,  0.2500],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.2949, -1.3076, -0.5059,  ..., -1.6562,  0.3867, -1.9238],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.1122e-03, -8.4152e-03, -1.2184e-02,  ..., -1.4877e-02,
         -1.3191e-02, -1.8524e-02],
        [-7.6065e-03, -5.6686e-03,  4.3869e-05,  ..., -1.6113e-02,
         -2.6989e-04, -3.8338e-03],
        [ 3.4027e-03, -9.4147e-03, -8.1940e-03,  ...,  1.1696e-02,
          3.2196e-03,  3.3569e-03],
        ...,
        [-4.2534e-04, -2.0874e-02,  8.5602e-03,  ..., -8.7967e-03,
         -1.0033e-02, -2.3727e-02],
        [ 1.5373e-02,  5.9662e-03,  2.9785e-02,  ..., -5.7144e-03,
          1.1665e-02,  2.7527e-02],
        [ 1.6953e-02,  5.5122e-03,  5.6534e-03,  ..., -1.8520e-03,
          1.2848e-02,  5.0926e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-4.9858e-03, -8.1921e-04, -9.3651e-04,  ..., -3.5744e-03,
         -8.0919e-04, -2.2717e-03],
        [ 0.0000e+00, -2.5630e-04,  1.0662e-03,  ..., -1.6003e-03,
         -1.2989e-03,  7.6580e-04],
        [ 2.6627e-03, -2.5868e-04,  4.0722e-04,  ...,  3.4752e-03,
          1.7433e-03, -2.8973e-03],
        ...,
        [ 4.7684e-07, -7.9823e-04, -1.5354e-03,  ..., -2.7390e-03,
          2.6665e-03, -2.2018e-04],
        [ 1.3285e-03,  2.8062e-04,  2.1076e-04,  ...,  7.9489e-04,
         -9.8801e-04,  1.4420e-03],
        [ 2.5253e-03,  5.8270e-04,  8.8453e-04,  ...,  9.7084e-04,
          1.0071e-03,  7.4029e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5488, -1.3623, -0.0278,  ..., -2.3438,  2.8789, -1.1973]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1523,  0.3225,  0.3528,  ...,  0.1771, -0.0975,  0.1295]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 09:53:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of up is down
The opposite of rise is sink
The opposite of over is under
The opposite of west is east
The opposite of beginning is end
The opposite of interior is exterior
The opposite of below is above
The opposite of south is
2024-07-04 09:53:57 root INFO     [order_1_approx] starting weight calculation for The opposite of up is down
The opposite of south is north
The opposite of rise is sink
The opposite of below is above
The opposite of over is under
The opposite of beginning is end
The opposite of west is east
The opposite of interior is
2024-07-04 09:53:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 09:57:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 10:01:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2148,  0.0044, -0.2372,  ..., -0.0840,  0.7783,  0.0815],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.2039, -0.0150, -0.2214,  ..., -0.0720,  0.6855,  0.0510],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2063, -3.1250, -0.5088,  ...,  1.4629,  2.5117,  1.7305],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.8433e-02, -6.2943e-03,  6.9084e-03,  ..., -3.6011e-03,
         -1.6830e-02, -1.9112e-03],
        [ 4.5738e-03,  4.1580e-04, -2.3926e-02,  ...,  6.3934e-03,
         -3.7140e-02, -2.5406e-03],
        [ 2.6398e-03,  8.3313e-03, -3.3417e-03,  ...,  9.2392e-03,
         -4.6158e-03,  1.8005e-03],
        ...,
        [ 7.8011e-03, -1.5259e-05,  1.7776e-02,  ...,  2.6245e-02,
          8.1482e-03,  1.0330e-02],
        [-1.0956e-02, -5.8136e-03,  7.7820e-04,  ...,  7.2174e-03,
         -2.1973e-02,  2.0874e-02],
        [-2.2690e-02, -1.4313e-02, -3.3569e-04,  ...,  4.0321e-03,
          2.3956e-02, -6.0272e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-3.4580e-03, -1.2712e-03, -4.0126e-04,  ...,  5.0449e-04,
          1.6088e-03, -3.4122e-03],
        [-2.5368e-03, -4.1413e-04, -1.4906e-03,  ...,  1.7757e-03,
         -3.9911e-04,  3.4409e-03],
        [ 5.4407e-04, -1.4901e-05,  3.4122e-03,  ...,  4.1819e-04,
          6.5088e-05, -1.3237e-03],
        ...,
        [ 1.5812e-03,  9.6750e-04, -1.3523e-03,  ..., -5.2571e-05,
          2.0866e-03, -4.6015e-04],
        [ 6.0034e-04,  1.4744e-03, -4.4489e-04,  ..., -3.2139e-04,
         -1.6193e-03,  6.8665e-04],
        [ 1.1797e-03, -6.5517e-04,  1.1110e-03,  ..., -8.7070e-04,
         -5.5265e-04, -8.6498e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0044, -1.9512, -1.2568,  ...,  0.9165,  2.8867,  1.5176]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1295, -0.2522,  0.3728,  ..., -0.2072, -0.2034,  0.2893]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 10:01:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of up is down
The opposite of south is north
The opposite of rise is sink
The opposite of below is above
The opposite of over is under
The opposite of beginning is end
The opposite of west is east
The opposite of interior is
2024-07-04 10:01:48 root INFO     [order_1_approx] starting weight calculation for The opposite of over is under
The opposite of up is down
The opposite of beginning is end
The opposite of below is above
The opposite of west is east
The opposite of south is north
The opposite of interior is exterior
The opposite of rise is
2024-07-04 10:01:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 10:05:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 10:09:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5522, -1.1777,  0.2634,  ..., -0.3491, -0.0645,  0.2827],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.5366, -1.1777,  0.2285,  ..., -0.3188, -0.0812,  0.2517],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.4961, -4.5156,  4.5156,  ..., -0.6470,  3.3418,  3.6914],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.6449e-02, -1.5007e-02, -1.3283e-02,  ...,  2.6688e-02,
          1.8372e-02,  1.7487e-02],
        [-6.3629e-03,  8.2207e-04, -9.5062e-03,  ..., -1.4038e-02,
         -1.5656e-02,  3.1555e-02],
        [-2.1881e-02,  8.3923e-05,  1.1322e-02,  ...,  1.4793e-02,
         -9.1782e-03,  1.1673e-02],
        ...,
        [-1.7109e-03,  1.7242e-03, -1.5671e-02,  ...,  2.3861e-03,
         -3.3360e-03, -1.4877e-04],
        [-1.5053e-02,  3.1113e-02, -4.2763e-03,  ..., -2.7771e-02,
         -5.9395e-03,  8.4076e-03],
        [ 4.4403e-03, -8.2397e-03,  2.6550e-03,  ..., -1.7532e-02,
          1.2802e-02, -2.1210e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0044, -0.0019,  0.0010,  ...,  0.0023,  0.0048, -0.0020],
        [-0.0028, -0.0030,  0.0041,  ...,  0.0020,  0.0030,  0.0038],
        [ 0.0002, -0.0006,  0.0019,  ..., -0.0025,  0.0013,  0.0004],
        ...,
        [ 0.0033,  0.0004, -0.0016,  ...,  0.0003,  0.0011, -0.0031],
        [ 0.0021,  0.0024,  0.0007,  ..., -0.0050, -0.0019,  0.0020],
        [ 0.0017, -0.0004,  0.0003,  ..., -0.0033,  0.0003, -0.0017]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.5664, -4.5195,  3.9082,  ..., -1.7402,  5.8008,  3.0664]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0444,  0.0254,  0.2455,  ..., -0.0403,  0.1141,  0.3237]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 10:09:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of over is under
The opposite of up is down
The opposite of beginning is end
The opposite of below is above
The opposite of west is east
The opposite of south is north
The opposite of interior is exterior
The opposite of rise is
2024-07-04 10:09:39 root INFO     [order_1_approx] starting weight calculation for The opposite of west is east
The opposite of rise is sink
The opposite of interior is exterior
The opposite of beginning is end
The opposite of below is above
The opposite of up is down
The opposite of south is north
The opposite of over is
2024-07-04 10:09:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 10:13:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 10:17:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0469, -0.9751,  0.6372,  ..., -0.9028,  1.0840,  0.3274],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.0429, -1.0195,  0.5981,  ..., -0.8647,  1.0527,  0.3110],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0363, -1.4668, -1.9648,  ..., -0.7036,  5.6172, -1.8623],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0316, -0.0327,  0.0037,  ..., -0.0014,  0.0255, -0.0002],
        [-0.0314,  0.0140, -0.0009,  ..., -0.0076,  0.0047,  0.0182],
        [ 0.0120, -0.0277, -0.0242,  ...,  0.0027,  0.0004, -0.0003],
        ...,
        [ 0.0130,  0.0033,  0.0132,  ...,  0.0045,  0.0005,  0.0021],
        [ 0.0005,  0.0254,  0.0517,  ..., -0.0125, -0.0280, -0.0060],
        [ 0.0004, -0.0363,  0.0089,  ...,  0.0216,  0.0223,  0.0102]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.1057e-03,  3.9339e-04, -7.1430e-04,  ...,  5.4026e-04,
         -9.7179e-04, -1.8940e-03],
        [-9.7942e-04, -1.8177e-03,  2.2774e-03,  ..., -1.2703e-03,
          4.2953e-03,  1.4019e-03],
        [ 1.2560e-03,  2.1839e-04, -1.9236e-03,  ...,  2.1343e-03,
         -3.8128e-03, -1.0481e-03],
        ...,
        [ 2.9221e-03, -1.0548e-03,  1.5106e-03,  ..., -1.6727e-03,
         -2.8248e-03, -1.0099e-03],
        [ 3.9768e-04,  1.7357e-03,  3.9368e-03,  ..., -8.4352e-04,
          8.4496e-04,  2.3603e-05],
        [-3.1013e-03, -1.1578e-03, -9.5367e-05,  ...,  2.4676e-05,
          3.8910e-04,  2.0008e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4070, -3.1035, -0.6963,  ...,  0.0181,  4.4414, -1.7070]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.2646, -0.0570,  0.1488,  ..., -0.1398, -0.3569,  0.0922]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 10:17:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of west is east
The opposite of rise is sink
The opposite of interior is exterior
The opposite of beginning is end
The opposite of below is above
The opposite of up is down
The opposite of south is north
The opposite of over is
2024-07-04 10:17:28 root INFO     [order_1_approx] starting weight calculation for The opposite of west is east
The opposite of up is down
The opposite of rise is sink
The opposite of beginning is end
The opposite of south is north
The opposite of interior is exterior
The opposite of over is under
The opposite of below is
2024-07-04 10:17:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 10:21:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 10:25:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0202, -2.3203, -0.0645,  ..., -0.2114,  0.4355, -0.1455],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.0228, -2.0762, -0.0658,  ..., -0.1727,  0.3513, -0.1534],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.5566, -4.6641,  2.9863,  ..., -1.7129,  1.6104,  2.3379],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0062, -0.0177, -0.0075,  ...,  0.0103,  0.0140, -0.0111],
        [-0.0042,  0.0030,  0.0007,  ..., -0.0154, -0.0032,  0.0181],
        [ 0.0165,  0.0087, -0.0328,  ...,  0.0187,  0.0105,  0.0042],
        ...,
        [-0.0117, -0.0220,  0.0048,  ..., -0.0075,  0.0111, -0.0190],
        [-0.0117,  0.0144,  0.0206,  ..., -0.0130, -0.0079, -0.0057],
        [-0.0122,  0.0016, -0.0032,  ...,  0.0096, -0.0027, -0.0122]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.3332e-03,  1.9979e-04, -1.7338e-03,  ..., -7.1287e-04,
         -2.6855e-03,  5.1308e-03],
        [ 2.1343e-03,  3.9902e-03,  1.9970e-03,  ..., -1.8015e-03,
          7.9250e-04,  1.2131e-03],
        [-5.4026e-04,  2.3785e-03,  6.9261e-05,  ...,  1.2922e-03,
          6.6376e-04, -1.0815e-03],
        ...,
        [-1.6985e-03, -7.3624e-04,  1.5688e-03,  ..., -5.3835e-04,
         -1.1978e-03,  1.7948e-03],
        [-1.0948e-03,  1.1396e-04,  1.3752e-03,  ...,  2.0771e-03,
         -8.6927e-04, -2.2354e-03],
        [-1.7843e-03, -5.8174e-05, -9.6989e-04,  ..., -1.7214e-03,
         -1.8463e-03,  4.7798e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.2773, -5.7695,  3.4883,  ..., -1.5312,  1.6016,  2.8613]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1875,  0.3315, -0.1069,  ..., -0.1838, -0.0537,  0.2147]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 10:25:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of west is east
The opposite of up is down
The opposite of rise is sink
The opposite of beginning is end
The opposite of south is north
The opposite of interior is exterior
The opposite of over is under
The opposite of below is
2024-07-04 10:25:18 root INFO     [order_1_approx] starting weight calculation for The opposite of rise is sink
The opposite of up is down
The opposite of interior is exterior
The opposite of south is north
The opposite of west is east
The opposite of below is above
The opposite of over is under
The opposite of beginning is
2024-07-04 10:25:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 10:29:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 10:33:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3506, -1.1816,  0.0781,  ..., -0.7217,  0.9629,  0.7500],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.3311, -1.1377,  0.0544,  ..., -0.6372,  0.8535,  0.6787],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3828, -2.5938,  0.0684,  ...,  4.8672,  0.9150, -0.3154],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0101, -0.0186, -0.0004,  ...,  0.0145,  0.0109,  0.0164],
        [-0.0047,  0.0009,  0.0048,  ..., -0.0044, -0.0064,  0.0248],
        [-0.0139,  0.0036, -0.0148,  ..., -0.0082,  0.0030,  0.0077],
        ...,
        [ 0.0067, -0.0122, -0.0093,  ...,  0.0020, -0.0187,  0.0004],
        [ 0.0047, -0.0160,  0.0297,  ...,  0.0015, -0.0080,  0.0031],
        [ 0.0098,  0.0045,  0.0110,  ..., -0.0011,  0.0125, -0.0007]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.5650e-03, -1.6909e-03, -9.9182e-05,  ...,  7.5483e-04,
          6.6757e-04, -6.6900e-04],
        [-6.4135e-04,  8.3876e-04,  2.0123e-03,  ...,  1.8120e-03,
          2.8973e-03,  4.9362e-03],
        [-1.9398e-03, -1.2522e-03,  1.1516e-04,  ..., -1.0281e-03,
         -2.9621e-03, -4.6301e-04],
        ...,
        [ 2.9850e-04, -1.8597e-03,  2.6703e-05,  ..., -1.7185e-03,
          4.7255e-04, -1.9236e-03],
        [ 4.0722e-04,  1.5192e-03, -8.6975e-04,  ...,  5.7411e-04,
          1.8997e-03, -1.1188e-04],
        [ 6.8474e-04,  2.9421e-04, -2.3499e-03,  ...,  1.2016e-03,
         -1.4210e-03,  2.6321e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9648, -3.9023, -1.0234,  ...,  3.7070,  1.5703,  0.0286]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0710, -0.1219, -0.0157,  ..., -0.0038,  0.0529,  0.0959]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 10:33:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of rise is sink
The opposite of up is down
The opposite of interior is exterior
The opposite of south is north
The opposite of west is east
The opposite of below is above
The opposite of over is under
The opposite of beginning is
2024-07-04 10:33:07 root INFO     [order_1_approx] starting weight calculation for The opposite of rise is sink
The opposite of south is north
The opposite of beginning is end
The opposite of below is above
The opposite of interior is exterior
The opposite of up is down
The opposite of over is under
The opposite of west is
2024-07-04 10:33:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 10:37:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 10:40:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2041, -1.4912, -1.2168,  ..., -0.5518,  0.8833,  0.6338],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1829, -1.3555, -1.0283,  ..., -0.4592,  0.7437,  0.5425],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6387, -2.6875,  0.0430,  ..., -3.2891, -0.1282, -0.7412],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 8.6212e-04, -3.8414e-03,  4.3411e-03,  ..., -8.0566e-03,
         -8.0643e-03,  8.2111e-04],
        [-1.1879e-02,  1.2833e-02, -1.7059e-02,  ..., -1.7242e-02,
          1.6632e-02, -4.6272e-03],
        [-9.7275e-05,  5.8594e-03,  1.2276e-02,  ..., -1.1124e-02,
         -1.1215e-02,  1.3420e-02],
        ...,
        [ 1.5900e-02, -2.7924e-03, -1.1749e-03,  ...,  7.3128e-03,
         -2.2621e-03,  3.9215e-03],
        [ 2.2812e-02,  4.7302e-04,  2.3132e-02,  ...,  1.0445e-02,
         -8.4686e-03,  1.6632e-02],
        [-1.1444e-04, -4.6272e-03,  1.5312e-02,  ...,  7.9060e-04,
          1.6785e-02,  9.8228e-04]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-4.0512e-03,  6.9046e-04,  2.4452e-03,  ..., -1.0262e-03,
         -1.4448e-03, -1.7328e-03],
        [ 9.6798e-05,  3.0861e-03,  1.4043e-04,  ..., -6.0177e-04,
          2.6112e-03,  9.0694e-04],
        [ 4.9877e-04, -1.1826e-03,  2.3746e-03,  ...,  2.3460e-03,
          2.4452e-03, -1.8864e-03],
        ...,
        [ 1.8692e-03, -7.3242e-04, -4.2820e-04,  ...,  5.4073e-04,
          1.8597e-03, -2.2483e-04],
        [-1.4248e-03,  1.6499e-03, -1.1492e-03,  ...,  1.1635e-03,
         -2.2888e-03,  2.6855e-03],
        [ 1.7166e-03,  1.1997e-03,  1.2751e-03,  ...,  5.2547e-04,
         -4.9591e-05, -1.3266e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1660, -3.2812, -0.2195,  ..., -3.1426,  1.8623,  0.3740]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.3350, -0.1726,  0.0683,  ..., -0.2705, -0.0113,  0.0790]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 10:40:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of rise is sink
The opposite of south is north
The opposite of beginning is end
The opposite of below is above
The opposite of interior is exterior
The opposite of up is down
The opposite of over is under
The opposite of west is
2024-07-04 10:40:57 root INFO     total operator prediction time: 3761.807502746582 seconds
2024-07-04 10:40:57 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on country - capital
2024-07-04 10:41:00 root INFO     building operator country - capital
2024-07-04 10:41:00 root INFO     [order_1_approx] starting weight calculation for The country with sofia as its capital is known as bulgaria
The country with havana as its capital is known as cuba
The country with vienna as its capital is known as austria
The country with bucharest as its capital is known as romania
The country with santiago as its capital is known as chile
The country with ottawa as its capital is known as canada
The country with zagreb as its capital is known as croatia
The country with manila as its capital is known as
2024-07-04 10:41:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 10:45:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 10:48:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0734,  0.6348, -0.3350,  ...,  0.6162,  0.2756,  0.8320],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.0649,  0.5835, -0.3047,  ...,  0.5493,  0.2329,  0.7598],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1074, -3.2969, -2.1406,  ..., -7.2109, -0.0747, -0.8643],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0021,  0.0062, -0.0084,  ...,  0.0102,  0.0118, -0.0036],
        [-0.0055, -0.0115,  0.0132,  ...,  0.0168,  0.0072, -0.0122],
        [ 0.0052, -0.0037,  0.0062,  ...,  0.0036,  0.0038, -0.0058],
        ...,
        [-0.0113,  0.0073,  0.0128,  ...,  0.0043, -0.0021,  0.0025],
        [-0.0026,  0.0093, -0.0027,  ..., -0.0111, -0.0107,  0.0020],
        [-0.0012, -0.0018,  0.0014,  ..., -0.0108,  0.0148,  0.0034]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.3094e-03, -1.0309e-03, -1.9321e-03,  ..., -1.2231e-04,
          7.0190e-04, -1.0433e-03],
        [-2.1863e-04, -1.1110e-03,  1.3819e-03,  ...,  4.2200e-04,
         -5.0426e-05, -1.0242e-03],
        [ 4.5729e-04,  1.1158e-03, -5.3215e-04,  ...,  2.4557e-04,
          2.1827e-04,  5.1069e-04],
        ...,
        [-1.0195e-03,  1.4763e-03,  2.6584e-04,  ...,  3.4451e-04,
          5.0402e-04,  1.8368e-03],
        [-9.5177e-04,  1.7428e-04,  6.5231e-04,  ...,  4.1389e-04,
          1.6701e-04, -1.5390e-04],
        [-6.1131e-04, -1.2674e-03,  1.6356e-04,  ...,  5.3453e-04,
         -1.6394e-03, -1.2217e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2610, -4.2539, -2.8926,  ..., -7.4141,  0.0951, -1.1260]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0696, -0.2155, -0.0034,  ...,  0.0589,  0.0253,  0.0965]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 10:48:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with sofia as its capital is known as bulgaria
The country with havana as its capital is known as cuba
The country with vienna as its capital is known as austria
The country with bucharest as its capital is known as romania
The country with santiago as its capital is known as chile
The country with ottawa as its capital is known as canada
The country with zagreb as its capital is known as croatia
The country with manila as its capital is known as
2024-07-04 10:48:51 root INFO     [order_1_approx] starting weight calculation for The country with vienna as its capital is known as austria
The country with zagreb as its capital is known as croatia
The country with ottawa as its capital is known as canada
The country with sofia as its capital is known as bulgaria
The country with manila as its capital is known as philippines
The country with bucharest as its capital is known as romania
The country with havana as its capital is known as cuba
The country with santiago as its capital is known as
2024-07-04 10:48:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 10:52:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 10:56:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1895,  0.5615,  0.2351,  ..., -0.8711, -0.0059, -0.0950],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1989,  0.5771,  0.2209,  ..., -0.8564, -0.0238, -0.1228],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.8672, -1.6191,  0.4482,  ..., -2.5664, -1.8906,  0.6953],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.3087e-02,  7.5836e-03, -8.2092e-03,  ...,  6.3095e-03,
          2.6321e-02,  4.6310e-03],
        [ 1.5106e-03, -3.4561e-03, -5.3024e-03,  ...,  6.4621e-03,
         -1.6006e-02,  6.6071e-03],
        [ 4.4708e-03,  7.3242e-04,  6.5384e-03,  ...,  7.4654e-03,
          9.4681e-03, -5.9166e-03],
        ...,
        [-1.9287e-02, -3.8147e-04,  3.7422e-03,  ...,  2.8351e-02,
         -2.8015e-02,  1.8024e-03],
        [-6.8321e-03,  2.0294e-03, -5.4321e-03,  ...,  5.9509e-03,
         -5.5962e-03,  1.9440e-02],
        [-2.0264e-02,  8.8501e-03, -8.0185e-03,  ..., -3.4241e-02,
         -2.2888e-05, -8.3084e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.5945e-03, -6.0654e-04, -7.7963e-04,  ...,  2.8629e-03,
          6.4182e-04,  1.4610e-03],
        [ 2.1305e-03, -1.7178e-04,  3.1986e-03,  ...,  9.8705e-05,
          7.7152e-04,  9.6750e-04],
        [-3.8218e-04, -1.9464e-03,  9.9277e-04,  ..., -1.1501e-03,
         -1.2512e-03, -1.3533e-03],
        ...,
        [ 1.6403e-04, -1.5259e-03,  5.0354e-04,  ...,  2.0161e-03,
          1.0633e-04, -6.0272e-04],
        [-1.8930e-03, -1.4944e-03,  2.0170e-04,  ...,  1.2321e-03,
         -2.6846e-04, -7.9250e-04],
        [-2.9907e-03, -2.2831e-03, -1.6308e-03,  ...,  9.3412e-04,
         -2.1744e-04,  5.7364e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3037, -1.7734,  0.0645,  ..., -1.8047, -1.5078,  0.9263]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0779, -0.0199, -0.1205,  ...,  0.0210, -0.4973, -0.2014]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 10:56:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with vienna as its capital is known as austria
The country with zagreb as its capital is known as croatia
The country with ottawa as its capital is known as canada
The country with sofia as its capital is known as bulgaria
The country with manila as its capital is known as philippines
The country with bucharest as its capital is known as romania
The country with havana as its capital is known as cuba
The country with santiago as its capital is known as
2024-07-04 10:56:40 root INFO     [order_1_approx] starting weight calculation for The country with santiago as its capital is known as chile
The country with bucharest as its capital is known as romania
The country with sofia as its capital is known as bulgaria
The country with havana as its capital is known as cuba
The country with zagreb as its capital is known as croatia
The country with ottawa as its capital is known as canada
The country with manila as its capital is known as philippines
The country with vienna as its capital is known as
2024-07-04 10:56:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 11:00:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 11:04:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6099,  0.4368,  0.1776,  ..., -0.7305, -0.8237,  1.7920],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.6001,  0.4194,  0.1538,  ..., -0.6792, -0.8110,  1.7598],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5508, -0.9697,  0.1300,  ..., -0.3604,  0.2764, -4.1094],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0032, -0.0139,  0.0007,  ...,  0.0020,  0.0091, -0.0089],
        [-0.0254, -0.0323,  0.0301,  ...,  0.0173, -0.0165, -0.0184],
        [ 0.0172,  0.0208, -0.0180,  ..., -0.0015,  0.0107, -0.0034],
        ...,
        [ 0.0013,  0.0066,  0.0008,  ..., -0.0097, -0.0062,  0.0027],
        [ 0.0043,  0.0061, -0.0073,  ...,  0.0019, -0.0044,  0.0004],
        [ 0.0211,  0.0054, -0.0181,  ...,  0.0026,  0.0135, -0.0037]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 4.8709e-04, -2.0123e-03, -3.6120e-04,  ...,  1.1902e-03,
          1.0967e-03, -1.1644e-03],
        [-2.1515e-03, -1.5192e-03,  1.9684e-03,  ...,  1.2417e-03,
          3.5381e-04, -2.7885e-03],
        [-1.0319e-03,  1.9455e-04, -1.3876e-04,  ...,  4.3893e-04,
          2.0742e-04, -3.4714e-04],
        ...,
        [-4.3035e-04,  5.3406e-04, -3.5644e-05,  ..., -1.5259e-04,
          3.1304e-04, -4.9496e-04],
        [-6.6042e-04,  9.1195e-05,  8.9502e-04,  ...,  1.3866e-03,
         -1.0166e-03, -1.5593e-03],
        [ 1.5032e-04, -1.7605e-03, -2.8896e-04,  ...,  1.7338e-03,
          4.4703e-05, -5.6076e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.9238, -1.8418,  1.3193,  ..., -0.2686,  0.9727, -1.6914]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1945, -0.4819, -0.0570,  ..., -0.0127, -0.2255,  0.0728]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 11:04:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with santiago as its capital is known as chile
The country with bucharest as its capital is known as romania
The country with sofia as its capital is known as bulgaria
The country with havana as its capital is known as cuba
The country with zagreb as its capital is known as croatia
The country with ottawa as its capital is known as canada
The country with manila as its capital is known as philippines
The country with vienna as its capital is known as
2024-07-04 11:04:30 root INFO     [order_1_approx] starting weight calculation for The country with vienna as its capital is known as austria
The country with bucharest as its capital is known as romania
The country with ottawa as its capital is known as canada
The country with santiago as its capital is known as chile
The country with manila as its capital is known as philippines
The country with zagreb as its capital is known as croatia
The country with havana as its capital is known as cuba
The country with sofia as its capital is known as
2024-07-04 11:04:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 11:08:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 11:12:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2578, -0.0730, -0.7100,  ...,  0.0159, -0.2505, -0.3364],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-1.3281, -0.0914, -0.7217,  ...,  0.0244, -0.2744, -0.3811],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([  0.0420,   3.1133,   4.4531,  ...,   3.8516, -10.1250,  -3.1328],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0147, -0.0089, -0.0003,  ...,  0.0088,  0.0082, -0.0219],
        [ 0.0219, -0.0062, -0.0008,  ...,  0.0138, -0.0044, -0.0008],
        [-0.0252, -0.0060,  0.0088,  ..., -0.0125,  0.0007,  0.0014],
        ...,
        [-0.0240, -0.0162,  0.0005,  ..., -0.0079, -0.0400, -0.0203],
        [ 0.0035, -0.0024,  0.0004,  ..., -0.0204,  0.0206,  0.0139],
        [-0.0121,  0.0055,  0.0171,  ..., -0.0044,  0.0045, -0.0067]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 5.3024e-04, -1.2102e-03, -7.3385e-04,  ...,  3.1471e-04,
          4.2987e-04, -5.3215e-04],
        [ 9.5654e-04,  1.0853e-03,  1.9588e-03,  ..., -9.4843e-04,
          1.4222e-04, -1.7157e-03],
        [-1.4477e-03, -1.7252e-03, -2.2769e-04,  ...,  5.1451e-04,
         -2.1160e-04,  1.7385e-03],
        ...,
        [ 1.3914e-03, -3.1829e-04,  1.8539e-03,  ...,  2.1935e-03,
          1.9264e-03, -2.6340e-03],
        [-6.3229e-04,  1.2245e-03, -1.3008e-03,  ..., -9.7513e-05,
         -1.0967e-05, -8.8215e-05],
        [ 2.6035e-04, -1.4572e-03,  2.7156e-04,  ...,  2.4242e-03,
         -5.9223e-04,  4.9400e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9609,  3.6406,  3.2871,  ...,  3.7168, -8.4609, -4.1172]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0265, -0.2773,  0.1729,  ..., -0.1417,  0.0642,  0.0008]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 11:12:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with vienna as its capital is known as austria
The country with bucharest as its capital is known as romania
The country with ottawa as its capital is known as canada
The country with santiago as its capital is known as chile
The country with manila as its capital is known as philippines
The country with zagreb as its capital is known as croatia
The country with havana as its capital is known as cuba
The country with sofia as its capital is known as
2024-07-04 11:12:21 root INFO     [order_1_approx] starting weight calculation for The country with zagreb as its capital is known as croatia
The country with santiago as its capital is known as chile
The country with sofia as its capital is known as bulgaria
The country with havana as its capital is known as cuba
The country with ottawa as its capital is known as canada
The country with manila as its capital is known as philippines
The country with vienna as its capital is known as austria
The country with bucharest as its capital is known as
2024-07-04 11:12:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 11:16:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 11:20:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5757, -0.0113, -0.4878,  ..., -0.2573,  0.5420, -0.1416],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.6245, -0.0305, -0.5122,  ..., -0.2620,  0.5547, -0.1798],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.3740, -1.0059,  2.6406,  ...,  5.1484, -3.4551, -0.7607],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0245,  0.0062, -0.0021,  ..., -0.0085,  0.0192, -0.0302],
        [-0.0148, -0.0011,  0.0045,  ...,  0.0080, -0.0005,  0.0038],
        [-0.0028,  0.0032, -0.0096,  ..., -0.0089,  0.0028, -0.0117],
        ...,
        [-0.0297, -0.0055,  0.0021,  ...,  0.0166, -0.0089, -0.0019],
        [ 0.0199, -0.0054, -0.0296,  ..., -0.0295,  0.0092,  0.0142],
        [ 0.0035, -0.0174, -0.0147,  ..., -0.0094,  0.0130,  0.0133]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.1029e-04, -6.8235e-04, -3.8171e-04,  ..., -1.1406e-03,
         -1.1654e-03,  1.0433e-03],
        [-1.0319e-03,  6.0320e-04,  1.0548e-03,  ...,  7.1430e-04,
          2.8276e-04, -7.6103e-04],
        [ 6.7616e-04,  2.9325e-05, -1.8048e-04,  ..., -2.6631e-04,
          8.3828e-04,  9.1839e-04],
        ...,
        [ 6.6185e-04,  1.3704e-03,  7.3385e-04,  ...,  7.6914e-04,
         -3.9530e-04,  1.4782e-03],
        [-7.3910e-04, -1.0452e-03,  2.2030e-04,  ...,  4.9734e-04,
         -8.8263e-04, -4.3631e-04],
        [ 7.0953e-04, -1.5278e-03, -3.9554e-04,  ...,  1.0414e-03,
          1.0300e-03,  5.8985e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1240, -2.5508,  2.5215,  ...,  4.6328, -0.8633,  0.1455]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1483, -0.0910,  0.0841,  ...,  0.2251, -0.2244,  0.2937]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 11:20:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with zagreb as its capital is known as croatia
The country with santiago as its capital is known as chile
The country with sofia as its capital is known as bulgaria
The country with havana as its capital is known as cuba
The country with ottawa as its capital is known as canada
The country with manila as its capital is known as philippines
The country with vienna as its capital is known as austria
The country with bucharest as its capital is known as
2024-07-04 11:20:11 root INFO     [order_1_approx] starting weight calculation for The country with santiago as its capital is known as chile
The country with zagreb as its capital is known as croatia
The country with vienna as its capital is known as austria
The country with ottawa as its capital is known as canada
The country with sofia as its capital is known as bulgaria
The country with bucharest as its capital is known as romania
The country with manila as its capital is known as philippines
The country with havana as its capital is known as
2024-07-04 11:20:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 11:24:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 11:27:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7100,  0.1426, -0.3120,  ..., -0.8662, -1.0488,  0.8164],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.6729,  0.1194, -0.2917,  ..., -0.7773, -0.9907,  0.7583],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1797,  0.7183,  1.6055,  ..., -3.1953,  1.9863, -0.2629],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.1185e-03, -2.0485e-03,  3.6907e-03,  ..., -2.3804e-03,
         -9.9258e-03, -6.0043e-03],
        [-1.5869e-02, -5.0659e-03,  6.6299e-03,  ...,  1.0071e-02,
         -7.1602e-03, -8.8043e-03],
        [-7.2403e-03,  1.8036e-02, -1.7593e-02,  ..., -1.1360e-02,
          3.4389e-03, -6.5994e-04],
        ...,
        [-1.2924e-02, -5.9557e-04,  3.0060e-03,  ...,  1.4847e-02,
         -4.4556e-03,  1.0986e-03],
        [ 1.3062e-02,  1.8677e-02, -1.9958e-02,  ..., -1.8021e-02,
          1.3779e-02,  2.6207e-03],
        [-3.5324e-03, -9.5367e-04, -3.7193e-05,  ..., -3.1662e-03,
         -4.9629e-03,  1.4488e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 3.8648e-04, -3.8147e-04,  2.7180e-04,  ..., -1.2608e-03,
         -6.1607e-04,  1.3008e-03],
        [-1.0004e-03,  3.5024e-04,  2.1744e-03,  ..., -5.7220e-05,
         -6.7806e-04,  4.0865e-04],
        [-4.9543e-04,  5.0592e-04, -4.1056e-04,  ..., -7.9441e-04,
         -5.5122e-04,  2.1565e-04],
        ...,
        [ 3.1662e-04, -1.6379e-04,  5.9605e-06,  ...,  9.1934e-04,
          7.7820e-04, -3.5667e-04],
        [-9.6560e-04,  1.9779e-03,  6.0463e-04,  ..., -9.1434e-05,
         -6.5327e-04,  9.5367e-06],
        [-5.3072e-04, -2.4052e-03,  1.5144e-03,  ...,  1.4763e-03,
          7.0572e-04,  6.7854e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2766,  0.7373,  1.6426,  ..., -3.2090,  2.1445, -0.7041]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1095, -0.0915,  0.1962,  ...,  0.1401, -0.1172,  0.1821]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 11:27:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with santiago as its capital is known as chile
The country with zagreb as its capital is known as croatia
The country with vienna as its capital is known as austria
The country with ottawa as its capital is known as canada
The country with sofia as its capital is known as bulgaria
The country with bucharest as its capital is known as romania
The country with manila as its capital is known as philippines
The country with havana as its capital is known as
2024-07-04 11:28:00 root INFO     [order_1_approx] starting weight calculation for The country with havana as its capital is known as cuba
The country with santiago as its capital is known as chile
The country with vienna as its capital is known as austria
The country with ottawa as its capital is known as canada
The country with manila as its capital is known as philippines
The country with bucharest as its capital is known as romania
The country with sofia as its capital is known as bulgaria
The country with zagreb as its capital is known as
2024-07-04 11:28:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 11:31:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 11:35:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8945,  0.3523, -0.7119,  ...,  0.7598,  0.6460, -0.1382],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.9297,  0.3567, -0.7134,  ...,  0.7578,  0.6406, -0.1689],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.1250, -2.6758,  3.7637,  ..., -0.6655, -4.5469, -2.4199],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0066, -0.0034,  0.0049,  ..., -0.0025,  0.0044, -0.0115],
        [-0.0077, -0.0179,  0.0119,  ...,  0.0126,  0.0009, -0.0023],
        [ 0.0180,  0.0052, -0.0145,  ..., -0.0045,  0.0087,  0.0023],
        ...,
        [-0.0002, -0.0029,  0.0018,  ..., -0.0107,  0.0051,  0.0079],
        [ 0.0210,  0.0156, -0.0192,  ..., -0.0103,  0.0052,  0.0010],
        [ 0.0143, -0.0058, -0.0170,  ..., -0.0272,  0.0200,  0.0075]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 4.4203e-04, -2.1887e-04, -6.2847e-04,  ...,  4.2796e-04,
         -4.3249e-04,  9.1171e-04],
        [-3.6955e-04,  4.8208e-04,  1.1110e-03,  ..., -1.1883e-03,
         -4.8637e-05, -1.0824e-04],
        [ 3.8862e-04, -4.6277e-04,  1.7703e-04,  ...,  1.4610e-03,
          1.1444e-03, -1.1959e-03],
        ...,
        [-5.0163e-04, -3.6287e-04,  3.9911e-04,  ..., -4.1914e-04,
          2.8586e-04, -1.4877e-04],
        [ 1.3590e-04,  1.4114e-03,  5.6171e-04,  ...,  9.3079e-04,
         -3.8362e-04, -9.8991e-04],
        [ 1.5688e-03, -1.6966e-03, -8.7833e-04,  ...,  3.9458e-04,
         -1.0586e-03,  2.1629e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3750, -3.7812,  3.3965,  ..., -0.3540, -2.7832, -1.3193]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0456,  0.1178, -0.1310,  ...,  0.0671, -0.3015,  0.1708]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 11:35:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with havana as its capital is known as cuba
The country with santiago as its capital is known as chile
The country with vienna as its capital is known as austria
The country with ottawa as its capital is known as canada
The country with manila as its capital is known as philippines
The country with bucharest as its capital is known as romania
The country with sofia as its capital is known as bulgaria
The country with zagreb as its capital is known as
2024-07-04 11:35:50 root INFO     [order_1_approx] starting weight calculation for The country with santiago as its capital is known as chile
The country with manila as its capital is known as philippines
The country with zagreb as its capital is known as croatia
The country with havana as its capital is known as cuba
The country with sofia as its capital is known as bulgaria
The country with vienna as its capital is known as austria
The country with bucharest as its capital is known as romania
The country with ottawa as its capital is known as
2024-07-04 11:35:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 11:39:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 11:43:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0522,  0.2280, -1.0107,  ...,  0.2241, -0.9751, -0.0940],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.0529,  0.2291, -1.0166,  ...,  0.2303, -1.0195, -0.1215],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3350,  3.6172, -0.5640,  ...,  1.2031,  0.2263, -0.0181],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.0475e-02, -7.4463e-03, -7.1640e-03,  ..., -1.0986e-03,
          4.0436e-03, -3.0518e-05],
        [-8.7280e-03, -4.5395e-04,  5.9662e-03,  ...,  8.3542e-03,
         -2.0782e-02, -1.9485e-02],
        [ 2.2278e-03,  1.4496e-03,  4.1962e-04,  ...,  6.5079e-03,
          2.3254e-02, -2.2125e-03],
        ...,
        [ 1.1978e-03, -9.5901e-03,  4.5280e-03,  ..., -3.7346e-03,
         -1.1566e-02,  4.0665e-03],
        [-2.2934e-02, -3.0746e-03, -8.3771e-03,  ..., -3.9101e-03,
          4.4861e-03,  1.5015e-02],
        [-1.5358e-02, -3.3073e-03, -5.8517e-03,  ..., -2.4475e-02,
          5.7144e-03, -9.1553e-04]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.4471e-03,  9.2316e-04, -5.3978e-04,  ..., -2.8539e-04,
          1.5230e-03, -8.2874e-04],
        [-1.2846e-03,  2.1362e-04,  1.1816e-03,  ...,  6.5756e-04,
          1.1606e-03, -1.3666e-03],
        [-1.9956e-04, -5.6553e-04, -4.8757e-05,  ..., -7.3195e-05,
          5.6648e-04, -5.9891e-04],
        ...,
        [-2.9755e-04, -1.2474e-03, -1.1816e-03,  ...,  4.2963e-04,
          9.1648e-04,  1.4868e-03],
        [ 3.3092e-04, -4.7088e-06, -1.5640e-04,  ...,  9.4593e-05,
          5.3930e-04,  4.9305e-04],
        [-1.1282e-03, -1.0910e-03, -4.9973e-04,  ..., -1.4162e-04,
          7.8678e-04, -4.7874e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1270,  2.7715, -0.5645,  ...,  2.6719,  0.5303, -0.1214]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0250, -0.2688, -0.0299,  ...,  0.1305,  0.0121, -0.0307]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 11:43:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with santiago as its capital is known as chile
The country with manila as its capital is known as philippines
The country with zagreb as its capital is known as croatia
The country with havana as its capital is known as cuba
The country with sofia as its capital is known as bulgaria
The country with vienna as its capital is known as austria
The country with bucharest as its capital is known as romania
The country with ottawa as its capital is known as
2024-07-04 11:43:39 root INFO     total operator prediction time: 3759.214409828186 seconds
2024-07-04 11:43:39 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_inf - 3pSg
2024-07-04 11:43:41 root INFO     building operator verb_inf - 3pSg
2024-07-04 11:43:41 root INFO     [order_1_approx] starting weight calculation for I consider, he considers
I suggest, he suggests
I add, he adds
I receive, he receives
I involve, he involves
I send, he sends
I represent, he represents
I create, he
2024-07-04 11:43:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 11:47:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 11:51:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1080, -1.0908,  0.3088,  ...,  0.4697,  0.2212, -0.3965],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.1109, -1.2051,  0.3015,  ...,  0.4819,  0.2128, -0.4536],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0078, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.7969, -2.4473, -0.9814,  ...,  0.1860, -0.6738, -0.7109],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0087,  0.0024, -0.0009,  ...,  0.0079,  0.0090, -0.0347],
        [-0.0203, -0.0299,  0.0110,  ..., -0.0108, -0.0077, -0.0127],
        [-0.0194,  0.0032, -0.0316,  ...,  0.0040,  0.0044, -0.0045],
        ...,
        [ 0.0151, -0.0128, -0.0011,  ..., -0.0139, -0.0015,  0.0106],
        [ 0.0093,  0.0145,  0.0059,  ..., -0.0146, -0.0195,  0.0133],
        [-0.0056,  0.0050, -0.0046,  ..., -0.0156,  0.0105, -0.0140]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.5978e-03, -1.2455e-03, -1.5163e-03,  ..., -1.6031e-03,
          1.5879e-03, -2.5043e-03],
        [-6.6710e-04, -2.8458e-03,  3.7098e-04,  ..., -3.5286e-05,
          8.4972e-04,  1.9002e-04],
        [-1.1654e-03,  7.1192e-04, -1.8854e-03,  ..., -7.1573e-04,
          1.1845e-03, -3.7313e-05],
        ...,
        [ 3.1853e-03, -1.4811e-03,  1.1463e-03,  ..., -3.0098e-03,
          2.6226e-03, -5.0545e-04],
        [ 1.1482e-03,  1.8187e-03, -1.0223e-03,  ..., -2.0599e-03,
         -2.3766e-03,  2.3670e-03],
        [-2.2316e-03, -1.6222e-03, -1.5411e-03,  ..., -1.3065e-03,
         -1.2827e-03, -1.3828e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.6816, -2.4316, -1.9336,  ...,  0.0454, -0.3230, -1.6133]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0046, -0.2566,  0.0385,  ...,  0.4102, -0.0866, -0.3694]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 11:51:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I consider, he considers
I suggest, he suggests
I add, he adds
I receive, he receives
I involve, he involves
I send, he sends
I represent, he represents
I create, he
2024-07-04 11:51:32 root INFO     [order_1_approx] starting weight calculation for I add, he adds
I send, he sends
I involve, he involves
I suggest, he suggests
I consider, he considers
I represent, he represents
I create, he creates
I receive, he
2024-07-04 11:51:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 11:55:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 11:59:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2993, -1.3936,  0.7812,  ..., -0.3608,  0.2109,  1.3799],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.2820, -1.3828,  0.7007,  ..., -0.3271,  0.1796,  1.3213],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0078, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6885, -0.5781,  2.2715,  ...,  2.0508, -0.1074,  5.2344],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0104, -0.0107, -0.0021,  ...,  0.0027,  0.0039, -0.0157],
        [-0.0076, -0.0060,  0.0145,  ..., -0.0144,  0.0030,  0.0159],
        [ 0.0002, -0.0051, -0.0140,  ...,  0.0008, -0.0018,  0.0026],
        ...,
        [ 0.0048, -0.0139, -0.0212,  ..., -0.0063, -0.0009,  0.0013],
        [-0.0012,  0.0033, -0.0033,  ..., -0.0017, -0.0154,  0.0048],
        [-0.0077,  0.0105, -0.0005,  ..., -0.0135, -0.0028, -0.0122]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.2836e-03, -3.0746e-03,  1.7042e-03,  ...,  1.3695e-03,
          4.2419e-03, -1.1902e-03],
        [-8.9979e-04, -9.3460e-04,  9.7752e-05,  ..., -3.4750e-05,
          7.0047e-04, -8.3637e-04],
        [ 8.2016e-05, -1.9531e-03, -2.5902e-03,  ..., -2.1553e-03,
          7.9727e-04, -1.0633e-04],
        ...,
        [ 3.6983e-03,  3.5739e-04, -9.5129e-04,  ..., -2.2316e-03,
          2.4929e-03, -1.1158e-04],
        [-2.0504e-05,  2.5501e-03, -1.5602e-03,  ..., -3.9756e-05,
         -2.2812e-03,  7.0763e-04],
        [ 5.9271e-04,  2.5940e-03, -1.6785e-04,  ..., -7.7963e-05,
         -1.9646e-03, -1.3208e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5928, -0.9448,  1.8828,  ...,  1.8359, -0.0176,  4.9102]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1663, -0.0399,  0.2408,  ...,  0.3274, -0.2314,  0.0866]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 11:59:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I add, he adds
I send, he sends
I involve, he involves
I suggest, he suggests
I consider, he considers
I represent, he represents
I create, he creates
I receive, he
2024-07-04 11:59:23 root INFO     [order_1_approx] starting weight calculation for I consider, he considers
I involve, he involves
I create, he creates
I send, he sends
I add, he adds
I receive, he receives
I represent, he represents
I suggest, he
2024-07-04 11:59:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 12:03:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 12:07:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6372, -0.8335,  0.4126,  ...,  0.4365,  0.0366,  0.1659],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.6611, -0.9111,  0.3984,  ...,  0.4397,  0.0175,  0.1516],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0078, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-7.1094, -0.6680,  7.2812,  ...,  3.9336, -5.1719,  4.2422],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0042, -0.0076,  0.0121,  ...,  0.0033,  0.0132, -0.0113],
        [-0.0072, -0.0115,  0.0096,  ..., -0.0055,  0.0122, -0.0017],
        [-0.0087, -0.0103, -0.0411,  ...,  0.0201, -0.0031,  0.0148],
        ...,
        [ 0.0122, -0.0183, -0.0051,  ..., -0.0184, -0.0018, -0.0251],
        [ 0.0104, -0.0071,  0.0277,  ..., -0.0180, -0.0510,  0.0106],
        [-0.0041,  0.0103,  0.0083,  ..., -0.0223,  0.0121, -0.0049]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.4238e-03, -3.9749e-03,  4.7874e-04,  ..., -2.0504e-03,
          2.7599e-03, -1.2131e-03],
        [-7.5150e-04, -8.5783e-04, -7.6342e-04,  ..., -2.0981e-03,
          3.4142e-04, -1.0262e-03],
        [ 6.5994e-04, -9.3889e-04, -2.3842e-03,  ...,  2.1896e-03,
         -4.3535e-04,  1.2970e-03],
        ...,
        [ 3.8948e-03, -2.4910e-03,  2.9049e-03,  ..., -1.6251e-03,
          3.1624e-03, -1.5965e-03],
        [ 1.3428e-03, -5.0020e-04,  1.7653e-03,  ..., -1.0509e-03,
         -1.5955e-03, -2.7084e-04],
        [ 1.1263e-03, -4.5657e-04,  7.3195e-05,  ..., -1.3494e-04,
         -8.9884e-04, -1.9383e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-6.6172, -0.7822,  7.2500,  ...,  4.5703, -4.5938,  3.3828]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2864,  0.0995, -0.2864,  ...,  0.5933,  0.2998, -0.1208]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 12:07:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I consider, he considers
I involve, he involves
I create, he creates
I send, he sends
I add, he adds
I receive, he receives
I represent, he represents
I suggest, he
2024-07-04 12:07:12 root INFO     [order_1_approx] starting weight calculation for I send, he sends
I consider, he considers
I represent, he represents
I receive, he receives
I suggest, he suggests
I create, he creates
I add, he adds
I involve, he
2024-07-04 12:07:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 12:11:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 12:14:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3977, -0.7065,  1.0947,  ..., -0.5371, -0.0156,  0.9082],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.4089, -0.7734,  1.0752,  ..., -0.5342, -0.0366,  0.9395],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0078, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.2656, -1.2109, -0.9834,  ...,  1.7051,  0.5845,  3.3711],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0130, -0.0140,  0.0241,  ...,  0.0021, -0.0126, -0.0022],
        [ 0.0074, -0.0189, -0.0020,  ...,  0.0032,  0.0292, -0.0018],
        [ 0.0199,  0.0032, -0.0251,  ...,  0.0246, -0.0240, -0.0105],
        ...,
        [-0.0107, -0.0091, -0.0203,  ...,  0.0090, -0.0157,  0.0183],
        [ 0.0091,  0.0117,  0.0263,  ..., -0.0061, -0.0256,  0.0114],
        [ 0.0220,  0.0259,  0.0208,  ..., -0.0250,  0.0181, -0.0231]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.2030e-03, -4.7798e-03, -1.9150e-03,  ...,  7.7534e-04,
         -1.9741e-04, -7.0143e-04],
        [ 2.0294e-03, -3.4809e-03,  1.4019e-03,  ..., -6.6757e-06,
          1.9150e-03,  1.4524e-03],
        [-4.3535e-04, -8.7738e-05, -2.3880e-03,  ...,  1.1711e-03,
          1.5621e-03,  1.9588e-03],
        ...,
        [ 1.9760e-03, -8.0013e-04, -4.2439e-04,  ...,  5.2643e-04,
          2.0256e-03,  3.0565e-04],
        [ 1.8654e-03, -8.2016e-04,  7.9679e-04,  ..., -6.6662e-04,
         -2.2583e-03,  1.0185e-03],
        [-4.5586e-04,  1.2636e-04,  9.2506e-04,  ..., -1.7443e-03,
         -2.4624e-03, -1.0891e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0996, -1.7109,  0.1328,  ...,  0.3486,  0.8936,  3.5586]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.6250,  0.1287, -0.1295,  ...,  0.3071, -0.2498,  0.2539]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 12:15:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I send, he sends
I consider, he considers
I represent, he represents
I receive, he receives
I suggest, he suggests
I create, he creates
I add, he adds
I involve, he
2024-07-04 12:15:01 root INFO     [order_1_approx] starting weight calculation for I create, he creates
I consider, he considers
I receive, he receives
I involve, he involves
I add, he adds
I suggest, he suggests
I send, he sends
I represent, he
2024-07-04 12:15:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 12:18:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 12:22:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2939, -0.8486,  1.1221,  ..., -0.6943, -0.1384,  0.1646],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 1.3271, -0.9155,  1.0879,  ..., -0.6841, -0.1619,  0.1464],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0078, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.9648, -2.2578, -0.7866,  ...,  2.1895, -0.4580,  6.6250],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0032, -0.0038, -0.0016,  ...,  0.0044, -0.0025, -0.0085],
        [ 0.0007, -0.0170,  0.0127,  ..., -0.0003,  0.0072,  0.0014],
        [-0.0096, -0.0073, -0.0364,  ...,  0.0090,  0.0042,  0.0095],
        ...,
        [-0.0138, -0.0002, -0.0103,  ..., -0.0350, -0.0098,  0.0033],
        [ 0.0075,  0.0070,  0.0078,  ..., -0.0159, -0.0352,  0.0063],
        [ 0.0071,  0.0036, -0.0104,  ..., -0.0087,  0.0167,  0.0113]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.6464e-04, -2.0390e-03,  7.4911e-04,  ..., -1.1940e-03,
          4.8518e-04,  5.6410e-04],
        [-8.9931e-04, -3.6693e-04,  7.9823e-04,  ...,  4.3678e-04,
          2.1420e-03,  5.9128e-04],
        [-1.5059e-03, -9.2173e-04, -1.3914e-03,  ..., -1.2350e-03,
          2.0516e-04,  1.2054e-03],
        ...,
        [ 4.3464e-04, -5.7650e-04, -7.4053e-04,  ..., -3.1424e-04,
          3.6221e-03, -1.6441e-03],
        [ 1.7471e-03,  1.2054e-03, -1.7500e-04,  ...,  9.9182e-05,
         -1.6232e-03,  1.1921e-03],
        [-1.3084e-03, -7.8678e-04, -6.1417e-04,  ..., -3.7670e-04,
         -8.7738e-04,  4.5967e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.9658, -2.4258, -0.6621,  ...,  1.0820, -1.1895,  5.7344]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1093, -0.1100,  0.1750,  ..., -0.0185,  0.1057,  0.0488]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 12:22:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I create, he creates
I consider, he considers
I receive, he receives
I involve, he involves
I add, he adds
I suggest, he suggests
I send, he sends
I represent, he
2024-07-04 12:22:50 root INFO     [order_1_approx] starting weight calculation for I suggest, he suggests
I send, he sends
I create, he creates
I consider, he considers
I represent, he represents
I involve, he involves
I receive, he receives
I add, he
2024-07-04 12:22:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 12:26:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 12:30:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1868, -0.4497,  1.4590,  ..., -0.8472,  0.1954,  0.4900],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1913, -0.4817,  1.3994,  ..., -0.8159,  0.1781,  0.4841],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0078, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7715, -1.2383,  3.0059,  ..., -1.6582, -1.2822,  4.0156],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0046, -0.0005,  0.0117,  ...,  0.0070,  0.0070, -0.0246],
        [ 0.0090, -0.0140, -0.0002,  ...,  0.0057, -0.0014,  0.0009],
        [-0.0079, -0.0159, -0.0253,  ...,  0.0088, -0.0035, -0.0025],
        ...,
        [ 0.0003,  0.0091, -0.0056,  ..., -0.0110,  0.0139,  0.0119],
        [ 0.0249,  0.0076,  0.0187,  ..., -0.0190, -0.0262, -0.0005],
        [-0.0077,  0.0109, -0.0120,  ..., -0.0117, -0.0086, -0.0043]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.0542e-03, -1.5163e-03,  5.8126e-04,  ...,  1.7571e-04,
          4.2725e-04, -1.4992e-03],
        [ 1.8444e-03, -7.4005e-04, -1.7643e-04,  ...,  2.7962e-03,
          8.1539e-05,  3.5048e-04],
        [-1.5068e-03, -1.1225e-03, -2.1744e-03,  ..., -2.5578e-03,
          4.9829e-04,  8.4209e-04],
        ...,
        [ 1.7128e-03,  2.9411e-03,  2.2030e-03,  ..., -1.8625e-03,
          1.2331e-03,  2.0504e-04],
        [ 1.2236e-03,  2.8496e-03, -4.0746e-04,  ...,  1.9312e-04,
         -2.2583e-03, -2.7847e-04],
        [ 1.6718e-03, -7.4816e-04,  1.0586e-03,  ...,  1.1454e-03,
         -1.2293e-03,  2.0828e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6660, -1.0107,  3.3652,  ..., -1.6406, -1.1797,  4.2344]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1724,  0.2507, -0.0407,  ...,  0.2469, -0.1149, -0.1407]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 12:30:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I suggest, he suggests
I send, he sends
I create, he creates
I consider, he considers
I represent, he represents
I involve, he involves
I receive, he receives
I add, he
2024-07-04 12:30:40 root INFO     [order_1_approx] starting weight calculation for I create, he creates
I represent, he represents
I suggest, he suggests
I consider, he considers
I add, he adds
I involve, he involves
I receive, he receives
I send, he
2024-07-04 12:30:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 12:34:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 12:38:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3682, -0.7124,  2.2305,  ..., -0.8188,  0.5078,  0.4556],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3665, -0.7480,  2.1230,  ..., -0.7822,  0.4839,  0.4431],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0078, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.7109, -0.5044,  3.5918,  ...,  3.0898, -0.2183,  4.8594],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0021, -0.0084, -0.0120,  ..., -0.0237,  0.0107, -0.0119],
        [ 0.0061, -0.0126,  0.0089,  ..., -0.0136, -0.0045,  0.0069],
        [ 0.0162,  0.0044, -0.0303,  ..., -0.0102,  0.0059, -0.0014],
        ...,
        [-0.0119, -0.0350,  0.0086,  ..., -0.0091, -0.0140,  0.0048],
        [-0.0080, -0.0138,  0.0170,  ..., -0.0082, -0.0464,  0.0111],
        [ 0.0007,  0.0016, -0.0104,  ..., -0.0142, -0.0039, -0.0285]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 0.0007, -0.0011,  0.0013,  ..., -0.0035,  0.0010,  0.0005],
        [-0.0008, -0.0012,  0.0004,  ..., -0.0004,  0.0010, -0.0009],
        [ 0.0004, -0.0002, -0.0014,  ..., -0.0023,  0.0007,  0.0002],
        ...,
        [ 0.0027, -0.0017,  0.0005,  ..., -0.0014,  0.0003,  0.0012],
        [-0.0006,  0.0005, -0.0002,  ..., -0.0005, -0.0016,  0.0009],
        [-0.0011, -0.0007, -0.0014,  ...,  0.0012, -0.0018,  0.0014]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.3516, -0.2651,  2.8770,  ...,  2.5137,  1.1504,  4.8086]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.2316,  0.0894, -0.0242,  ...,  0.2551, -0.1556,  0.1294]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 12:38:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I create, he creates
I represent, he represents
I suggest, he suggests
I consider, he considers
I add, he adds
I involve, he involves
I receive, he receives
I send, he
2024-07-04 12:38:28 root INFO     [order_1_approx] starting weight calculation for I create, he creates
I add, he adds
I involve, he involves
I suggest, he suggests
I send, he sends
I represent, he represents
I receive, he receives
I consider, he
2024-07-04 12:38:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 12:42:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 12:46:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0310, -0.7725,  0.8486,  ..., -0.5254,  0.1809,  0.9536],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.0284, -0.8779,  0.8662,  ..., -0.5439,  0.1736,  1.0312],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0078, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3330, -0.5425,  0.0879,  ...,  1.4863, -2.6543,  1.0322],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-5.0125e-03, -8.1024e-03, -1.0986e-02,  ..., -1.1894e-02,
          1.0208e-02, -1.0872e-02],
        [-1.9714e-02, -1.0361e-02,  1.9272e-02,  ..., -1.3609e-03,
          1.0307e-02, -1.8005e-03],
        [-5.5923e-03, -7.0953e-04, -1.6205e-02,  ...,  1.2367e-02,
          1.2268e-02, -2.7161e-03],
        ...,
        [ 1.9180e-02, -1.4511e-02, -3.8086e-02,  ..., -1.3107e-02,
          1.3870e-02,  5.1270e-03],
        [ 1.2062e-02, -9.0790e-03,  1.5617e-02,  ..., -1.0521e-02,
         -2.9022e-02,  4.1122e-03],
        [ 9.3994e-03,  2.3209e-02,  6.2828e-03,  ..., -1.9703e-03,
         -1.4954e-03,  2.1219e-05]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 2.2125e-04, -2.9869e-03,  2.1720e-04,  ..., -1.1692e-03,
          9.4843e-04,  3.4952e-04],
        [-1.9944e-04, -7.9393e-04,  1.4029e-03,  ..., -1.4305e-04,
          1.6022e-03,  5.0831e-04],
        [-1.4305e-03,  1.1978e-03,  4.2796e-04,  ..., -6.9046e-04,
         -1.9760e-03,  3.4809e-03],
        ...,
        [ 4.0207e-03,  4.6134e-05, -1.0853e-03,  ..., -7.3814e-04,
          2.9278e-03, -1.1425e-03],
        [ 1.7738e-03, -5.3883e-04,  4.4870e-04,  ...,  6.8617e-04,
         -3.9363e-04,  2.5630e-04],
        [-2.2163e-03,  1.4992e-03,  4.1556e-04,  ..., -3.2759e-04,
         -2.9507e-03,  1.3552e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8389, -0.8379, -0.0287,  ...,  1.3086, -2.2754,  1.3213]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.2715,  0.1228,  0.1650,  ...,  0.6094,  0.0751,  0.2976]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 12:46:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I create, he creates
I add, he adds
I involve, he involves
I suggest, he suggests
I send, he sends
I represent, he represents
I receive, he receives
I consider, he
2024-07-04 12:46:16 root INFO     total operator prediction time: 3755.1786375045776 seconds
2024-07-04 12:46:16 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - member
2024-07-04 12:46:18 root INFO     building operator meronyms - member
2024-07-04 12:46:19 root INFO     [order_1_approx] starting weight calculation for A person is a member of a society
A elephant is a member of a herd
A antelope is a member of a herd
A parishioner is a member of a parish
A bird is a member of a flock
A cow is a member of a herd
A cat is a member of a clowder
A policeman is a member of a
2024-07-04 12:46:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 12:50:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 12:54:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4082, -0.5596, -0.7188,  ...,  0.3967,  0.1023,  0.3777],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3601, -0.5342, -0.6294,  ...,  0.3423,  0.0691,  0.3193],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9941, -0.7524,  1.3301,  ...,  2.5254, -1.0742,  4.7031],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0010, -0.0061, -0.0012,  ...,  0.0046, -0.0118, -0.0078],
        [-0.0008,  0.0181,  0.0032,  ...,  0.0044, -0.0007,  0.0014],
        [-0.0014,  0.0171, -0.0117,  ..., -0.0025,  0.0229,  0.0008],
        ...,
        [-0.0132, -0.0162, -0.0018,  ...,  0.0075, -0.0053,  0.0074],
        [ 0.0045, -0.0046,  0.0069,  ..., -0.0060, -0.0061, -0.0078],
        [-0.0085, -0.0043,  0.0089,  ...,  0.0068,  0.0099, -0.0090]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 1.5574e-03,  4.4441e-04,  7.8869e-04,  ...,  3.3021e-04,
         -1.4067e-03,  5.7697e-05],
        [ 1.2112e-03, -1.4412e-04,  7.2002e-04,  ..., -8.2302e-04,
         -9.6512e-04,  5.2452e-06],
        [-1.3709e-05, -1.1206e-03, -9.0837e-04,  ..., -3.4475e-04,
         -4.1127e-04, -5.8365e-04],
        ...,
        [ 1.9634e-04, -3.7789e-04,  1.4610e-03,  ..., -1.0738e-03,
          1.1337e-04, -1.0252e-03],
        [-1.3089e-04,  6.3086e-04,  2.1553e-04,  ...,  8.7166e-04,
          9.2030e-05, -3.7694e-04],
        [-1.0719e-03,  6.2275e-04,  5.3406e-04,  ..., -9.5367e-04,
         -1.4887e-03,  8.8811e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0137, -0.6152,  0.3428,  ...,  2.7832, -0.8369,  4.6484]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1162, -0.0013, -0.0641,  ..., -0.0684,  0.0443,  0.1425]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 12:54:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A person is a member of a society
A elephant is a member of a herd
A antelope is a member of a herd
A parishioner is a member of a parish
A bird is a member of a flock
A cow is a member of a herd
A cat is a member of a clowder
A policeman is a member of a
2024-07-04 12:54:10 root INFO     [order_1_approx] starting weight calculation for A elephant is a member of a herd
A antelope is a member of a herd
A cow is a member of a herd
A cat is a member of a clowder
A policeman is a member of a police
A parishioner is a member of a parish
A person is a member of a society
A bird is a member of a
2024-07-04 12:54:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 12:58:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 13:01:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8389, -0.4651,  0.3345,  ...,  1.5088, -0.4326, -0.1097],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.7417, -0.4370,  0.2729,  ...,  1.2861, -0.3943, -0.1208],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5371,  0.8579, -1.8750,  ...,  3.4258, -0.3113,  0.6909],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.5701e-02, -2.7847e-03,  1.7151e-02,  ...,  1.4969e-02,
          1.1597e-02, -5.4016e-03],
        [-4.7951e-03, -1.5450e-04, -8.0261e-03,  ..., -9.3231e-03,
         -1.0376e-02,  1.0574e-02],
        [ 2.8366e-02,  5.8899e-03,  5.0507e-03,  ...,  9.8114e-03,
          1.5427e-02, -4.6921e-03],
        ...,
        [-1.7380e-02,  2.1362e-03, -8.2779e-04,  ..., -1.2802e-02,
         -2.1225e-02, -2.3804e-03],
        [ 1.2146e-02, -1.5656e-02, -1.0853e-03,  ...,  7.6294e-06,
          5.1384e-03, -8.5449e-04],
        [-5.1231e-03, -7.5989e-03,  7.0801e-03,  ...,  6.9962e-03,
          7.6294e-03,  8.8120e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-3.3875e-03,  5.7602e-04,  8.3113e-04,  ..., -1.8477e-04,
          1.8492e-03, -1.2817e-03],
        [-9.8228e-05, -3.4809e-04,  1.1253e-04,  ..., -3.4165e-04,
          2.3913e-04, -2.4452e-03],
        [ 1.4315e-03, -2.7218e-03, -6.7425e-04,  ...,  8.7023e-04,
         -1.9288e-04, -6.6519e-05],
        ...,
        [ 2.6054e-03,  8.9216e-04,  1.2798e-03,  ..., -1.8740e-04,
          1.5955e-03, -2.0428e-03],
        [ 2.1858e-03, -1.7385e-03, -2.1629e-03,  ...,  2.3880e-03,
         -1.5297e-03, -1.2016e-04],
        [-2.1305e-03, -1.7262e-03,  1.2522e-03,  ..., -1.7748e-03,
         -2.0909e-04,  1.4615e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8618,  1.1406, -1.7471,  ...,  3.0742,  0.4026,  1.2363]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0299, -0.5967,  0.0210,  ..., -0.2866, -0.2703,  0.1248]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 13:02:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A elephant is a member of a herd
A antelope is a member of a herd
A cow is a member of a herd
A cat is a member of a clowder
A policeman is a member of a police
A parishioner is a member of a parish
A person is a member of a society
A bird is a member of a
2024-07-04 13:02:01 root INFO     [order_1_approx] starting weight calculation for A policeman is a member of a police
A person is a member of a society
A parishioner is a member of a parish
A antelope is a member of a herd
A bird is a member of a flock
A cow is a member of a herd
A elephant is a member of a herd
A cat is a member of a
2024-07-04 13:02:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 13:06:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 13:09:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9922, -0.8750, -0.8364,  ...,  0.6929, -0.2573,  0.9795],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.9258, -0.8521, -0.7544,  ...,  0.6260, -0.2532,  0.9116],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5747,  1.5703,  5.4258,  ...,  0.1189, -1.3643,  3.2227],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0120, -0.0182, -0.0031,  ...,  0.0042, -0.0156, -0.0094],
        [ 0.0135,  0.0148, -0.0034,  ...,  0.0111, -0.0131, -0.0041],
        [-0.0012,  0.0103, -0.0006,  ..., -0.0029, -0.0010,  0.0061],
        ...,
        [ 0.0012,  0.0010,  0.0045,  ...,  0.0009, -0.0135, -0.0095],
        [ 0.0080, -0.0062,  0.0102,  ..., -0.0044, -0.0089,  0.0143],
        [-0.0050, -0.0104, -0.0063,  ...,  0.0044, -0.0092,  0.0090]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 4.5776e-04, -8.2970e-04,  4.9305e-04,  ...,  8.1635e-04,
          3.6812e-04, -2.5711e-03],
        [ 2.1057e-03, -1.1559e-03,  1.3895e-03,  ...,  5.9307e-05,
         -1.3418e-03, -1.1432e-04],
        [ 5.7459e-04, -8.1778e-04,  5.3501e-04,  ...,  1.7538e-03,
          1.5700e-04,  1.5907e-03],
        ...,
        [ 2.9445e-04, -2.8572e-03,  1.1501e-03,  ..., -3.2120e-03,
          1.5574e-03, -1.9274e-03],
        [ 1.4391e-03, -9.5272e-04, -1.1940e-03,  ..., -3.9101e-05,
         -6.2513e-04,  1.0920e-04],
        [ 1.2982e-04,  5.5122e-04, -7.1526e-05,  ...,  3.2282e-04,
         -1.2703e-03,  5.0354e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4419,  0.8892,  5.8828,  ..., -0.5088,  0.1270,  3.2852]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0060, -0.1572, -0.0831,  ..., -0.2224,  0.0177, -0.0226]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 13:09:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A policeman is a member of a police
A person is a member of a society
A parishioner is a member of a parish
A antelope is a member of a herd
A bird is a member of a flock
A cow is a member of a herd
A elephant is a member of a herd
A cat is a member of a
2024-07-04 13:09:53 root INFO     [order_1_approx] starting weight calculation for A cat is a member of a clowder
A parishioner is a member of a parish
A person is a member of a society
A policeman is a member of a police
A cow is a member of a herd
A elephant is a member of a herd
A bird is a member of a flock
A antelope is a member of a
2024-07-04 13:09:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 13:13:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 13:17:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7461, -0.3770,  0.3877,  ...,  0.1964, -0.2896,  0.4009],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.6572, -0.3596,  0.3157,  ...,  0.1702, -0.2717,  0.3381],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7861,  4.0508, -2.4961,  ...,  1.1523,  0.6084,  5.4453],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0046, -0.0134,  0.0055,  ...,  0.0005,  0.0014, -0.0026],
        [-0.0181,  0.0025, -0.0026,  ...,  0.0217, -0.0102,  0.0092],
        [ 0.0005, -0.0074,  0.0022,  ..., -0.0145,  0.0127,  0.0097],
        ...,
        [-0.0115, -0.0205, -0.0040,  ...,  0.0058, -0.0102,  0.0144],
        [ 0.0138, -0.0137, -0.0088,  ..., -0.0077,  0.0003, -0.0036],
        [-0.0060,  0.0114,  0.0011,  ..., -0.0187,  0.0077,  0.0072]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.7447e-03, -6.4850e-04,  6.5994e-04,  ...,  5.1498e-04,
          1.8442e-04,  4.3631e-04],
        [ 2.3174e-03, -7.6151e-04,  1.6613e-03,  ...,  1.9908e-05,
          5.0068e-05,  1.2598e-03],
        [-1.2398e-03, -9.7466e-04,  8.3637e-04,  ...,  9.0408e-04,
          9.3794e-04,  3.8099e-04],
        ...,
        [ 2.1896e-03, -4.1008e-04, -4.7183e-04,  ...,  7.8678e-06,
          5.8270e-04,  4.4560e-04],
        [-3.9768e-04, -1.1625e-03, -8.5163e-04,  ...,  2.1636e-04,
          2.3091e-04,  1.2360e-03],
        [-3.0327e-03, -9.8610e-04, -2.3794e-04,  ..., -2.6283e-03,
         -3.1710e-04, -8.9455e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.1992,  2.5625, -2.4434,  ...,  1.1396,  0.5918,  6.3281]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0717, -0.3081,  0.1841,  ...,  0.0030, -0.0153,  0.0259]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 13:17:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A cat is a member of a clowder
A parishioner is a member of a parish
A person is a member of a society
A policeman is a member of a police
A cow is a member of a herd
A elephant is a member of a herd
A bird is a member of a flock
A antelope is a member of a
2024-07-04 13:17:44 root INFO     [order_1_approx] starting weight calculation for A antelope is a member of a herd
A policeman is a member of a police
A person is a member of a society
A elephant is a member of a herd
A cat is a member of a clowder
A cow is a member of a herd
A bird is a member of a flock
A parishioner is a member of a
2024-07-04 13:17:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 13:21:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 13:25:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8774,  0.3391,  1.0586,  ...,  1.4824, -0.2104,  0.4058],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.9292,  0.3550,  1.0605,  ...,  1.5098, -0.2332,  0.4187],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([1.4727, 0.2749, 4.1094,  ..., 1.4805, 0.2383, 1.9639], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0011,  0.0069,  0.0037,  ...,  0.0078, -0.0135, -0.0003],
        [-0.0132,  0.0107,  0.0145,  ...,  0.0005, -0.0152, -0.0014],
        [ 0.0036, -0.0007, -0.0016,  ...,  0.0085, -0.0008, -0.0024],
        ...,
        [-0.0126, -0.0043,  0.0004,  ..., -0.0038, -0.0069,  0.0030],
        [ 0.0070, -0.0038,  0.0086,  ...,  0.0124,  0.0150, -0.0069],
        [-0.0022, -0.0025, -0.0027,  ...,  0.0016,  0.0121,  0.0022]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 2.3746e-04, -1.4925e-04,  1.5221e-03,  ..., -7.6294e-05,
         -1.2627e-03, -4.6253e-04],
        [ 2.3079e-04, -3.7384e-04,  9.1934e-04,  ..., -3.0375e-04,
         -6.1893e-04, -5.6267e-04],
        [ 5.7936e-05, -1.2589e-03, -1.1539e-03,  ...,  8.9502e-04,
          5.2929e-05, -4.1485e-04],
        ...,
        [-6.3992e-04, -5.0735e-04, -5.7411e-04,  ..., -1.0669e-04,
         -6.9737e-06, -3.9101e-04],
        [ 1.7953e-04,  4.2510e-04,  2.1672e-04,  ...,  7.6866e-04,
          9.0742e-04, -8.2302e-04],
        [-3.2234e-04,  6.8426e-04, -6.0558e-04,  ..., -6.5804e-04,
         -3.6895e-05, -4.3011e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9849, -0.2349,  4.1914,  ...,  1.6045,  0.3245,  2.2734]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1019, -0.1300, -0.0316,  ..., -0.0345, -0.0961,  0.0509]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 13:25:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A antelope is a member of a herd
A policeman is a member of a police
A person is a member of a society
A elephant is a member of a herd
A cat is a member of a clowder
A cow is a member of a herd
A bird is a member of a flock
A parishioner is a member of a
2024-07-04 13:25:35 root INFO     [order_1_approx] starting weight calculation for A parishioner is a member of a parish
A cow is a member of a herd
A policeman is a member of a police
A cat is a member of a clowder
A antelope is a member of a herd
A bird is a member of a flock
A person is a member of a society
A elephant is a member of a
2024-07-04 13:25:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 13:29:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 13:33:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5605, -1.0840, -0.6191,  ...,  1.3945,  0.0107,  0.2791],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.4690, -0.9609, -0.5142,  ...,  1.1338, -0.0122,  0.2150],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3477,  3.5820,  0.8262,  ..., -1.9248, -1.9082,  3.7461],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0039, -0.0075, -0.0039,  ..., -0.0135, -0.0033, -0.0111],
        [-0.0170, -0.0079,  0.0063,  ...,  0.0248, -0.0078,  0.0023],
        [ 0.0012,  0.0027, -0.0054,  ..., -0.0168,  0.0090, -0.0153],
        ...,
        [-0.0135,  0.0021, -0.0033,  ...,  0.0042,  0.0072,  0.0026],
        [ 0.0109, -0.0170,  0.0037,  ..., -0.0159, -0.0158, -0.0101],
        [-0.0040,  0.0012,  0.0115,  ..., -0.0027,  0.0202, -0.0148]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.1702e-03, -6.2847e-04,  8.8692e-05,  ...,  5.0879e-04,
         -6.2180e-04,  5.2929e-04],
        [ 1.0157e-04, -3.9825e-03,  5.8842e-04,  ...,  2.4853e-03,
          1.4520e-04, -2.7523e-03],
        [-4.3225e-04,  7.3719e-04, -8.8072e-04,  ..., -3.2473e-04,
         -2.1420e-03,  2.7523e-03],
        ...,
        [ 1.5030e-03,  1.5574e-03, -1.9956e-04,  ..., -1.4877e-03,
          2.9583e-03, -7.2527e-04],
        [-6.1178e-04,  1.8368e-03,  5.2452e-04,  ...,  7.8535e-04,
         -2.2163e-03,  1.1473e-03],
        [-2.3727e-03,  1.4138e-04, -3.0422e-04,  ..., -2.2125e-04,
          3.3998e-04, -1.6727e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.8257,  1.9531,  1.9375,  ..., -2.0645, -0.3379,  4.4219]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0991, -0.4556,  0.2300,  ...,  0.0469,  0.1354, -0.2803]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 13:33:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A parishioner is a member of a parish
A cow is a member of a herd
A policeman is a member of a police
A cat is a member of a clowder
A antelope is a member of a herd
A bird is a member of a flock
A person is a member of a society
A elephant is a member of a
2024-07-04 13:33:25 root INFO     [order_1_approx] starting weight calculation for A person is a member of a society
A bird is a member of a flock
A parishioner is a member of a parish
A antelope is a member of a herd
A policeman is a member of a police
A elephant is a member of a herd
A cat is a member of a clowder
A cow is a member of a
2024-07-04 13:33:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 13:37:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 13:41:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5645, -0.1887, -0.6577,  ...,  1.2197, -0.0356,  1.2080],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.4902, -0.1835, -0.5557,  ...,  1.0234, -0.0472,  1.0518],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.4453,  3.9785, -0.1514,  ..., -2.2734, -0.4277,  2.8789],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0107, -0.0122,  0.0027,  ...,  0.0022, -0.0016, -0.0092],
        [-0.0291,  0.0223,  0.0173,  ...,  0.0210, -0.0086, -0.0224],
        [ 0.0082, -0.0105,  0.0070,  ..., -0.0010,  0.0221, -0.0034],
        ...,
        [-0.0022, -0.0090,  0.0004,  ..., -0.0154, -0.0031,  0.0046],
        [ 0.0200, -0.0167,  0.0030,  ..., -0.0082, -0.0099, -0.0007],
        [ 0.0009, -0.0006, -0.0121,  ...,  0.0050,  0.0217,  0.0007]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.5554e-03, -2.6474e-03,  1.4391e-03,  ...,  3.8290e-04,
          1.8330e-03,  1.7786e-04],
        [ 2.9373e-04, -7.2527e-04,  1.9760e-03,  ...,  2.1133e-03,
          2.3270e-04, -1.0271e-03],
        [ 6.3896e-05, -5.5599e-04, -5.1451e-04,  ...,  4.1747e-04,
          1.5402e-03,  5.3692e-04],
        ...,
        [ 1.0843e-03, -4.6611e-04,  9.3365e-04,  ..., -3.6526e-03,
          8.3637e-04,  1.0624e-03],
        [-8.4686e-04,  2.9802e-04, -3.5000e-04,  ..., -1.0691e-03,
         -1.6060e-03,  5.0926e-04],
        [ 8.7404e-04, -6.0034e-04, -3.3054e-03,  ..., -3.1471e-05,
         -8.4352e-04, -5.7411e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.8594e+00,  2.2734e+00,  2.5488e-01,  ..., -2.2676e+00,
          2.4414e-04,  2.6719e+00]], device='cuda:1', dtype=torch.float16,
       grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0283, -0.3367,  0.2301,  ...,  0.3477, -0.0191, -0.0895]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 13:41:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A person is a member of a society
A bird is a member of a flock
A parishioner is a member of a parish
A antelope is a member of a herd
A policeman is a member of a police
A elephant is a member of a herd
A cat is a member of a clowder
A cow is a member of a
2024-07-04 13:41:17 root INFO     [order_1_approx] starting weight calculation for A bird is a member of a flock
A parishioner is a member of a parish
A elephant is a member of a herd
A cat is a member of a clowder
A cow is a member of a herd
A antelope is a member of a herd
A policeman is a member of a police
A person is a member of a
2024-07-04 13:41:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-04 13:45:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-04 13:49:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0724, -0.0152,  0.5933,  ...,  1.0830,  1.0645, -0.3018],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.0695, -0.0314,  0.5537,  ...,  1.0391,  1.0273, -0.3291],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5938, -2.3633, -0.5903,  ..., -2.2734,  0.6606,  2.2676],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0022, -0.0080,  0.0025,  ...,  0.0059,  0.0062, -0.0017],
        [-0.0066, -0.0005,  0.0160,  ..., -0.0029,  0.0031, -0.0010],
        [ 0.0071,  0.0078, -0.0015,  ...,  0.0038,  0.0086, -0.0059],
        ...,
        [-0.0016,  0.0009, -0.0101,  ..., -0.0016, -0.0091,  0.0010],
        [ 0.0093, -0.0037, -0.0023,  ..., -0.0082, -0.0223, -0.0046],
        [-0.0112, -0.0026, -0.0145,  ...,  0.0056,  0.0091,  0.0018]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.6108e-03,  3.5524e-05,  1.0271e-03,  ...,  8.0585e-04,
          8.2672e-05, -1.5998e-04],
        [ 1.3294e-03, -3.1567e-04,  9.6607e-04,  ..., -6.4039e-04,
          1.2999e-03,  6.1893e-04],
        [-5.4646e-04, -1.1635e-03,  1.4496e-04,  ...,  6.7234e-04,
          2.2526e-03, -2.5082e-03],
        ...,
        [ 5.3215e-04,  1.3351e-04,  4.5419e-05,  ..., -1.2703e-03,
         -2.2304e-04, -6.4039e-04],
        [-3.0828e-04,  4.3893e-04, -9.1648e-04,  ...,  3.1805e-04,
         -1.6422e-03, -1.3399e-03],
        [-2.1553e-03,  1.4038e-03, -1.2445e-03,  ..., -1.3840e-04,
         -4.4012e-04, -8.5211e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4609, -2.2598, -1.0078,  ..., -2.1758,  0.9160,  2.8242]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1329, -0.0608, -0.1680,  ..., -0.0502, -0.2217,  0.2456]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-04 13:49:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A bird is a member of a flock
A parishioner is a member of a parish
A elephant is a member of a herd
A cat is a member of a clowder
A cow is a member of a herd
A antelope is a member of a herd
A policeman is a member of a police
A person is a member of a
2024-07-04 13:49:08 root INFO     total operator prediction time: 3770.1366395950317 seconds
2024-07-08 22:10:51 root INFO     loading model + tokenizer
2024-07-08 22:11:08 root INFO     model + tokenizer loaded
2024-07-08 22:11:08 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+tion_irreg
2024-07-08 22:11:11 root INFO     building operator verb+tion_irreg
2024-07-08 22:11:11 root INFO     [order_1_approx] starting weight calculation for To maximize results in maximization
To customize results in customization
To illumine results in illumination
To explore results in exploration
To privatize results in privatization
To configure results in configuration
To admire results in admiration
To prepare results in
2024-07-08 22:11:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-08 22:15:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-08 22:19:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4060,  0.4087,  0.0811,  ...,  0.3958,  0.4580, -0.7354],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.4492,  0.4336,  0.0689,  ...,  0.4119,  0.4675, -0.8423],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0318, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4229,  0.7031, -1.9590,  ...,  1.5303, -2.4922,  3.9922],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0125,  0.0020,  0.0037,  ...,  0.0183, -0.0101, -0.0062],
        [-0.0056, -0.0058, -0.0043,  ...,  0.0119,  0.0048, -0.0016],
        [-0.0120, -0.0037, -0.0059,  ..., -0.0010, -0.0021,  0.0051],
        ...,
        [-0.0106, -0.0124,  0.0060,  ..., -0.0082, -0.0080,  0.0052],
        [ 0.0208,  0.0126,  0.0092,  ..., -0.0038, -0.0159, -0.0076],
        [-0.0076,  0.0137,  0.0084,  ...,  0.0002, -0.0036,  0.0008]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-5.6076e-04, -9.4128e-04, -1.1706e-04,  ...,  1.6794e-03,
          8.3256e-04, -2.1315e-04],
        [ 9.2745e-04,  3.2902e-04,  1.6403e-04,  ...,  4.5300e-06,
          1.4877e-03, -6.5994e-04],
        [-1.9951e-03, -1.5545e-03, -1.6651e-03,  ..., -6.4325e-04,
         -1.4849e-03, -1.4124e-03],
        ...,
        [ 5.9271e-04,  3.6931e-04,  1.6584e-03,  ...,  6.3848e-04,
          6.2275e-04, -5.2929e-05],
        [ 4.2305e-03,  2.7676e-03, -1.7242e-03,  ..., -7.8440e-05,
         -1.0300e-04, -1.1129e-03],
        [-9.6083e-04,  4.5061e-04,  2.3174e-04,  ...,  1.3924e-03,
         -5.5981e-04, -8.1730e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5781,  1.8203, -2.1719,  ...,  0.7842, -2.1777,  4.1055]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.3281, -0.0106,  0.0823,  ..., -0.1936, -0.1860, -0.0964]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-08 22:19:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To maximize results in maximization
To customize results in customization
To illumine results in illumination
To explore results in exploration
To privatize results in privatization
To configure results in configuration
To admire results in admiration
To prepare results in
2024-07-08 22:19:16 root INFO     [order_1_approx] starting weight calculation for To explore results in exploration
To customize results in customization
To privatize results in privatization
To admire results in admiration
To prepare results in preparation
To illumine results in illumination
To maximize results in maximization
To configure results in
2024-07-08 22:19:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-08 22:23:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-08 22:27:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6665,  0.6357, -0.4287,  ...,  0.5698, -0.2690, -0.7070],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.6777,  0.6479, -0.4363,  ...,  0.5610, -0.2959, -0.7700],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0318, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1128,  1.0283, -3.9141,  ..., -0.7388, -0.8569,  3.5508],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0206, -0.0117, -0.0034,  ...,  0.0024, -0.0091, -0.0169],
        [-0.0141, -0.0060, -0.0001,  ..., -0.0005, -0.0016, -0.0071],
        [ 0.0253, -0.0008, -0.0139,  ...,  0.0008, -0.0078, -0.0063],
        ...,
        [ 0.0052,  0.0006,  0.0027,  ..., -0.0062, -0.0006,  0.0041],
        [ 0.0115,  0.0057, -0.0047,  ..., -0.0065, -0.0179, -0.0123],
        [-0.0108,  0.0039, -0.0074,  ..., -0.0012, -0.0054, -0.0056]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0014, -0.0012,  0.0003,  ...,  0.0009, -0.0022, -0.0009],
        [-0.0015, -0.0007,  0.0019,  ...,  0.0005,  0.0006, -0.0027],
        [-0.0002,  0.0013,  0.0005,  ...,  0.0003, -0.0021, -0.0001],
        ...,
        [-0.0002,  0.0013,  0.0021,  ..., -0.0004,  0.0021, -0.0018],
        [ 0.0009,  0.0027, -0.0014,  ..., -0.0018, -0.0010, -0.0008],
        [-0.0014,  0.0014,  0.0006,  ..., -0.0008, -0.0010, -0.0003]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8745,  1.3896, -4.1797,  ..., -0.3711, -0.5010,  3.8125]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.3535, -0.0733,  0.1682,  ..., -0.1863, -0.0170, -0.0083]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-08 22:27:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To explore results in exploration
To customize results in customization
To privatize results in privatization
To admire results in admiration
To prepare results in preparation
To illumine results in illumination
To maximize results in maximization
To configure results in
2024-07-08 22:27:21 root INFO     [order_1_approx] starting weight calculation for To configure results in configuration
To privatize results in privatization
To explore results in exploration
To customize results in customization
To admire results in admiration
To prepare results in preparation
To illumine results in illumination
To maximize results in
2024-07-08 22:27:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-08 22:31:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-08 22:35:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5215, -0.8613,  0.6670,  ..., -0.8320, -0.5312, -0.7979],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.5098, -0.9004,  0.6162,  ..., -0.7944, -0.5449, -0.8389],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0318, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.7168,  0.9487, -5.1875,  ...,  0.0371, -1.2324,  5.2617],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0324, -0.0221,  0.0022,  ...,  0.0080, -0.0172, -0.0034],
        [-0.0019,  0.0045,  0.0010,  ...,  0.0139, -0.0298, -0.0153],
        [ 0.0140,  0.0077, -0.0181,  ...,  0.0068, -0.0015,  0.0019],
        ...,
        [-0.0166, -0.0144, -0.0033,  ..., -0.0065, -0.0070, -0.0147],
        [-0.0006,  0.0143,  0.0066,  ..., -0.0042, -0.0082, -0.0005],
        [-0.0082, -0.0094,  0.0266,  ...,  0.0096,  0.0249,  0.0120]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.7285e-04, -1.8969e-03,  1.3123e-03,  ...,  8.6355e-04,
         -1.1644e-03,  3.2187e-05],
        [-4.8876e-04,  1.0376e-03, -3.3951e-04,  ...,  1.4124e-03,
         -4.3535e-04, -4.7755e-04],
        [-7.9632e-05, -2.7180e-05, -1.4715e-03,  ...,  8.9121e-04,
         -8.7082e-05, -1.4982e-03],
        ...,
        [ 6.7139e-04,  1.6870e-03,  1.4472e-04,  ..., -1.6842e-03,
         -3.6240e-04, -1.8463e-03],
        [ 9.4080e-04,  1.6031e-03, -1.3819e-03,  ...,  1.7614e-03,
          2.4557e-04, -1.3103e-03],
        [ 5.1308e-04,  6.0320e-05,  8.1635e-04,  ...,  8.7357e-04,
         -1.2541e-03, -1.4744e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7432,  0.9287, -4.0078,  ..., -1.9561, -0.5527,  4.9883]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2808,  0.0729,  0.0020,  ..., -0.2864, -0.0945,  0.1249]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-08 22:35:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To configure results in configuration
To privatize results in privatization
To explore results in exploration
To customize results in customization
To admire results in admiration
To prepare results in preparation
To illumine results in illumination
To maximize results in
2024-07-08 22:35:26 root INFO     [order_1_approx] starting weight calculation for To illumine results in illumination
To admire results in admiration
To privatize results in privatization
To maximize results in maximization
To configure results in configuration
To prepare results in preparation
To customize results in customization
To explore results in
2024-07-08 22:35:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-08 22:39:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-08 22:43:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9980, -0.8984,  0.7588,  ..., -0.7036,  0.2256, -0.1407],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 1.0732, -1.0215,  0.7676,  ..., -0.7329,  0.2163, -0.1855],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0318, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0303,  2.9609,  0.5205,  ..., -4.0664, -2.7461,  2.8828],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.4511e-02, -6.5079e-03, -2.2919e-02,  ...,  9.3994e-03,
         -3.5286e-05, -9.2545e-03],
        [-1.0689e-02, -1.4305e-02, -8.3389e-03,  ..., -4.7531e-03,
          5.2795e-03, -6.1226e-04],
        [ 7.6561e-03,  6.7444e-03, -7.5378e-03,  ..., -1.3504e-02,
         -2.7618e-03,  1.0666e-02],
        ...,
        [ 8.2016e-03, -8.7891e-03, -1.4030e-02,  ..., -1.3489e-02,
          2.4223e-04, -2.2430e-03],
        [ 3.6102e-02,  1.3580e-02, -1.6205e-02,  ...,  1.2039e-02,
          9.5749e-03, -6.2714e-03],
        [ 1.7059e-02, -3.8147e-06,  3.6125e-03,  ...,  3.9215e-03,
          1.4450e-02, -8.8272e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-5.8413e-04, -3.8862e-05,  2.4354e-04,  ...,  1.7986e-03,
         -2.7084e-04, -8.5211e-04],
        [ 1.6403e-04, -1.1406e-03, -3.8004e-04,  ...,  4.9829e-04,
          1.2798e-03, -4.4537e-04],
        [-3.0479e-03, -1.1978e-03, -5.0306e-04,  ...,  9.4509e-04,
          1.6241e-03,  5.4312e-04],
        ...,
        [-8.4496e-04, -1.3769e-04,  1.2512e-03,  ..., -6.5327e-05,
          3.9029e-04,  2.8038e-04],
        [ 8.5545e-04, -2.7585e-04, -1.6570e-05,  ...,  1.4901e-04,
         -7.8249e-04, -2.1434e-04],
        [ 1.1611e-04,  1.5163e-04,  8.6594e-04,  ...,  1.8492e-03,
          2.1000e-03, -5.7936e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2634,  2.5957,  0.5093,  ..., -3.1992, -1.4980,  3.1133]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2617, -0.0731,  0.1519,  ..., -0.1646, -0.1335, -0.1763]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-08 22:43:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To illumine results in illumination
To admire results in admiration
To privatize results in privatization
To maximize results in maximization
To configure results in configuration
To prepare results in preparation
To customize results in customization
To explore results in
2024-07-08 22:43:32 root INFO     [order_1_approx] starting weight calculation for To configure results in configuration
To prepare results in preparation
To maximize results in maximization
To privatize results in privatization
To customize results in customization
To admire results in admiration
To explore results in exploration
To illumine results in
2024-07-08 22:43:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-08 22:47:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-08 22:51:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1711,  0.1451,  1.1973,  ..., -0.8257, -0.1849,  0.5430],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1753,  0.1328,  1.1367,  ..., -0.7900, -0.2010,  0.5332],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0318, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7461,  0.7856, -6.6094,  ..., -0.2466, -2.6582,  1.9375],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0062, -0.0046, -0.0015,  ..., -0.0051,  0.0009,  0.0034],
        [-0.0039, -0.0047,  0.0157,  ...,  0.0021, -0.0043, -0.0036],
        [ 0.0170,  0.0215, -0.0192,  ...,  0.0040,  0.0032, -0.0116],
        ...,
        [ 0.0015, -0.0134, -0.0091,  ..., -0.0022, -0.0127,  0.0018],
        [ 0.0158,  0.0153, -0.0246,  ...,  0.0004, -0.0100, -0.0074],
        [-0.0073, -0.0023, -0.0099,  ...,  0.0039,  0.0120,  0.0130]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 8.1301e-05, -9.3937e-04,  1.2074e-03,  ..., -4.8351e-04,
         -1.2751e-03,  8.8596e-04],
        [-4.3821e-04,  1.8597e-05,  1.9503e-04,  ..., -5.2071e-04,
         -8.6069e-05, -9.3365e-04],
        [-2.0561e-03, -5.5408e-04,  1.6236e-04,  ...,  8.4496e-04,
          3.0785e-03,  4.3654e-04],
        ...,
        [-3.2806e-04, -1.0977e-03,  1.5488e-03,  ..., -3.3975e-04,
          8.7118e-04, -1.6136e-03],
        [ 5.8174e-04,  1.6046e-04,  6.6042e-04,  ..., -6.5708e-04,
         -8.9884e-04,  3.1853e-04],
        [-2.9373e-04, -6.0558e-05,  3.7360e-04,  ..., -6.3753e-04,
          3.6597e-04, -7.6771e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.8076,  0.1079, -6.0664,  ..., -0.4370, -2.6445,  1.9697]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1317, -0.0273, -0.1161,  ..., -0.2319,  0.0885,  0.0775]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-08 22:51:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To configure results in configuration
To prepare results in preparation
To maximize results in maximization
To privatize results in privatization
To customize results in customization
To admire results in admiration
To explore results in exploration
To illumine results in
2024-07-08 22:51:38 root INFO     [order_1_approx] starting weight calculation for To configure results in configuration
To prepare results in preparation
To explore results in exploration
To maximize results in maximization
To privatize results in privatization
To admire results in admiration
To illumine results in illumination
To customize results in
2024-07-08 22:51:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-08 22:55:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-08 22:59:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8359, -0.4043, -0.4102,  ...,  0.6006, -0.2461, -0.5059],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.8062, -0.4272, -0.3984,  ...,  0.5596, -0.2610, -0.5332],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0318, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8691,  0.2969, -2.6855,  ..., -2.3262, -1.0723,  1.0010],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0063, -0.0078,  0.0024,  ...,  0.0027, -0.0082, -0.0031],
        [-0.0141, -0.0026, -0.0056,  ...,  0.0113, -0.0061, -0.0110],
        [ 0.0124,  0.0006, -0.0117,  ...,  0.0104, -0.0036, -0.0033],
        ...,
        [-0.0108, -0.0015, -0.0061,  ..., -0.0117, -0.0029,  0.0088],
        [ 0.0055,  0.0125,  0.0060,  ..., -0.0099, -0.0071, -0.0041],
        [-0.0003,  0.0046,  0.0026,  ..., -0.0076,  0.0118,  0.0068]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.0276e-04, -1.2932e-03,  1.0347e-03,  ...,  1.1253e-03,
         -3.7694e-04, -5.3215e-04],
        [-4.1008e-04, -7.2289e-04,  2.3186e-04,  ..., -7.3910e-06,
          9.5940e-04, -2.4033e-03],
        [-2.0027e-05,  1.2436e-03, -1.2417e-03,  ..., -4.2486e-04,
         -1.0471e-03,  2.5988e-04],
        ...,
        [ 2.0432e-04,  7.5042e-05,  2.5177e-03,  ...,  1.4448e-03,
          1.4734e-03, -3.5620e-04],
        [ 2.2945e-03,  1.9007e-03, -5.7602e-04,  ..., -7.6771e-04,
          5.8174e-04,  1.2484e-03],
        [ 7.4339e-04,  3.0518e-04,  6.5613e-04,  ...,  1.6999e-04,
         -2.3246e-04,  1.6475e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.8535, -0.1421, -2.5859,  ..., -2.0508, -0.8594,  1.0449]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1448, -0.0401,  0.1707,  ..., -0.2520, -0.0480,  0.2515]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-08 22:59:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To configure results in configuration
To prepare results in preparation
To explore results in exploration
To maximize results in maximization
To privatize results in privatization
To admire results in admiration
To illumine results in illumination
To customize results in
2024-07-08 22:59:44 root INFO     [order_1_approx] starting weight calculation for To customize results in customization
To prepare results in preparation
To privatize results in privatization
To maximize results in maximization
To illumine results in illumination
To explore results in exploration
To configure results in configuration
To admire results in
2024-07-08 22:59:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-08 23:03:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-08 23:07:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5830, -0.3403,  0.6108,  ..., -0.6509,  0.1514, -1.1719],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.5527, -0.3469,  0.5283,  ..., -0.5825,  0.1162, -1.1436],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0318, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.5488,  2.9180, -1.0234,  ..., -0.5283, -2.5938,  3.1621],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-3.6926e-03, -6.0234e-03,  9.1019e-03,  ...,  1.4481e-02,
         -9.0027e-03, -2.7847e-04],
        [-1.8005e-03,  1.7662e-03,  4.6921e-03,  ...,  1.6403e-02,
         -1.1185e-02,  8.0490e-03],
        [ 6.6528e-03,  9.8267e-03, -1.3458e-02,  ...,  9.5367e-03,
          6.9580e-03,  2.1915e-03],
        ...,
        [-1.3336e-02, -1.1047e-02, -3.1872e-03,  ..., -2.7115e-02,
         -1.9073e-05, -1.4168e-02],
        [ 8.9188e-03,  2.3819e-02, -1.8417e-02,  ..., -5.4550e-03,
         -1.2566e-02, -4.8103e-03],
        [-2.2797e-02,  1.4816e-02,  1.4435e-02,  ..., -2.1286e-03,
          6.3744e-03, -6.4697e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0010, -0.0003,  0.0003,  ...,  0.0010,  0.0004,  0.0017],
        [ 0.0019, -0.0007,  0.0015,  ...,  0.0019, -0.0002, -0.0003],
        [-0.0014, -0.0011, -0.0017,  ..., -0.0002,  0.0010,  0.0012],
        ...,
        [-0.0012,  0.0008,  0.0009,  ..., -0.0004,  0.0012,  0.0007],
        [ 0.0002,  0.0023, -0.0026,  ...,  0.0016, -0.0007,  0.0004],
        [ 0.0011,  0.0011,  0.0001,  ...,  0.0007,  0.0011, -0.0017]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4863,  3.1230, -0.7939,  ..., -0.7148, -1.8965,  4.1562]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2334,  0.0756, -0.0851,  ..., -0.2661, -0.0301, -0.2056]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-08 23:07:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To customize results in customization
To prepare results in preparation
To privatize results in privatization
To maximize results in maximization
To illumine results in illumination
To explore results in exploration
To configure results in configuration
To admire results in
2024-07-08 23:07:50 root INFO     [order_1_approx] starting weight calculation for To illumine results in illumination
To maximize results in maximization
To prepare results in preparation
To explore results in exploration
To admire results in admiration
To customize results in customization
To configure results in configuration
To privatize results in
2024-07-08 23:07:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-08 23:11:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-08 23:15:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4265, -0.3835,  0.6553,  ...,  0.0499, -0.3633,  1.0234],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.4456, -0.4236,  0.6323,  ...,  0.0522, -0.3889,  1.0527],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0318, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2412,  1.2656,  0.7900,  ...,  1.3486, -3.0938,  0.5273],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.0376e-02, -2.2240e-03,  4.3793e-03,  ..., -2.2507e-03,
         -5.4893e-03,  5.4264e-04],
        [ 8.8577e-03, -6.6795e-03,  6.1417e-03,  ...,  5.9242e-03,
         -9.8114e-03,  5.9509e-03],
        [ 6.8436e-03,  4.2419e-03, -1.6434e-02,  ...,  6.9122e-03,
         -2.0752e-03,  9.5367e-06],
        ...,
        [-1.0307e-02, -7.9269e-03, -5.1498e-03,  ..., -5.8556e-03,
         -1.0803e-02, -4.3297e-04],
        [ 9.4147e-03,  1.0849e-02,  1.4366e-02,  ..., -9.7580e-03,
         -5.6458e-04, -8.9951e-03],
        [ 5.7507e-04, -1.4381e-03,  2.6398e-03,  ..., -1.5831e-03,
          9.5062e-03, -1.4153e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-9.9277e-04, -6.3992e-04, -4.9353e-04,  ..., -2.9373e-04,
         -9.2888e-04, -3.5143e-04],
        [ 5.9414e-04, -2.8777e-04, -3.0088e-04,  ...,  9.0885e-04,
         -2.0504e-05,  3.0327e-04],
        [-1.3247e-03, -6.2561e-04, -3.3903e-04,  ...,  8.4496e-04,
          3.9577e-04, -3.0518e-04],
        ...,
        [-3.4714e-04,  9.6381e-05,  5.0735e-04,  ...,  9.2220e-04,
          8.4972e-04, -9.9945e-04],
        [ 6.7902e-04, -1.2898e-04,  1.3151e-03,  ...,  3.8671e-04,
          1.6699e-03, -3.8481e-04],
        [-8.8692e-05,  3.5191e-04,  1.1063e-03,  ...,  7.2670e-04,
          3.0375e-04, -9.8991e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3452,  1.1758,  0.9194,  ...,  0.6401, -2.7383,  0.7764]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1648,  0.0802,  0.0818,  ...,  0.0060, -0.1936,  0.1183]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-08 23:15:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To illumine results in illumination
To maximize results in maximization
To prepare results in preparation
To explore results in exploration
To admire results in admiration
To customize results in customization
To configure results in configuration
To privatize results in
2024-07-08 23:15:54 root INFO     total operator prediction time: 3883.3020012378693 seconds
2024-07-08 23:15:54 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on un+adj_reg
2024-07-08 23:15:57 root INFO     building operator un+adj_reg
2024-07-08 23:15:57 root INFO     [order_1_approx] starting weight calculation for The opposite of known is unknown
The opposite of veiled is unveiled
The opposite of satisfactory is unsatisfactory
The opposite of believable is unbelievable
The opposite of happy is unhappy
The opposite of aware is unaware
The opposite of lawful is unlawful
The opposite of published is
2024-07-08 23:15:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-08 23:20:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-08 23:24:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.4739, 0.2913, 0.7178,  ..., 1.1523, 0.7905, 0.5933], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([0.4348, 0.2571, 0.6240,  ..., 1.0312, 0.7046, 0.5371], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7188, -2.9102,  1.5938,  ...,  2.0469,  2.4785, -2.4277],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0007, -0.0025, -0.0010,  ..., -0.0029,  0.0062,  0.0006],
        [ 0.0017, -0.0022, -0.0003,  ...,  0.0045, -0.0118, -0.0089],
        [ 0.0178,  0.0311, -0.0121,  ..., -0.0169, -0.0099,  0.0067],
        ...,
        [-0.0130, -0.0121, -0.0047,  ...,  0.0059, -0.0039,  0.0146],
        [-0.0104,  0.0001,  0.0091,  ...,  0.0133, -0.0156,  0.0162],
        [ 0.0068, -0.0096,  0.0105,  ..., -0.0208, -0.0122, -0.0049]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.5097e-03,  2.7275e-03, -1.5676e-04,  ..., -2.1191e-03,
          9.4748e-04,  1.7109e-03],
        [ 1.9431e-05, -4.4250e-03, -9.8228e-04,  ...,  3.9339e-05,
          6.4373e-05,  1.0405e-03],
        [ 7.6866e-04, -5.6505e-04,  7.8869e-04,  ..., -6.3610e-04,
          1.5001e-03, -3.6383e-04],
        ...,
        [ 8.9169e-04,  1.8907e-04, -1.6499e-03,  ..., -1.2732e-03,
          1.5163e-03, -1.6603e-03],
        [-7.6962e-04,  5.4264e-04, -2.3842e-07,  ...,  1.4763e-03,
         -2.1477e-03,  4.9400e-04],
        [-9.3269e-04, -1.8463e-03,  9.1171e-04,  ..., -5.4789e-04,
         -2.4529e-03,  2.2335e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.2852, -3.0625,  0.9263,  ...,  2.1289,  1.9639, -2.8535]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.3347,  0.0116, -0.0725,  ...,  0.2849, -0.5239,  0.2571]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-08 23:24:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of known is unknown
The opposite of veiled is unveiled
The opposite of satisfactory is unsatisfactory
The opposite of believable is unbelievable
The opposite of happy is unhappy
The opposite of aware is unaware
The opposite of lawful is unlawful
The opposite of published is
2024-07-08 23:24:03 root INFO     [order_1_approx] starting weight calculation for The opposite of aware is unaware
The opposite of believable is unbelievable
The opposite of published is unpublished
The opposite of known is unknown
The opposite of lawful is unlawful
The opposite of happy is unhappy
The opposite of veiled is unveiled
The opposite of satisfactory is
2024-07-08 23:24:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-08 23:28:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-08 23:32:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1283, -0.3750,  0.2048,  ..., -0.2406,  0.0322,  0.7090],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.1039, -0.3650,  0.1564,  ..., -0.2069,  0.0034,  0.6128],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0435,  0.7710, -2.8203,  ...,  3.8262, -0.3838, -0.0566],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0065,  0.0180, -0.0086,  ..., -0.0121,  0.0128, -0.0077],
        [-0.0021, -0.0439, -0.0038,  ...,  0.0010, -0.0386,  0.0072],
        [ 0.0059, -0.0056, -0.0133,  ...,  0.0023, -0.0163,  0.0134],
        ...,
        [-0.0059, -0.0277, -0.0100,  ...,  0.0012, -0.0131, -0.0066],
        [-0.0120, -0.0058,  0.0116,  ..., -0.0004,  0.0097, -0.0151],
        [-0.0150,  0.0250, -0.0013,  ..., -0.0066,  0.0087,  0.0056]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-6.4182e-04,  1.8520e-03, -9.1887e-04,  ..., -2.0409e-03,
          3.0220e-05,  1.2379e-03],
        [-1.9169e-03, -2.0905e-03,  1.2770e-03,  ...,  2.8400e-03,
          5.3692e-04,  1.1930e-03],
        [ 1.7700e-03, -1.0157e-03, -5.6553e-04,  ...,  4.6134e-04,
          3.8004e-04, -1.7548e-04],
        ...,
        [ 6.5041e-04,  2.1005e-04,  3.7074e-05,  ...,  1.5640e-03,
         -1.3266e-03, -1.9312e-03],
        [-6.0654e-04,  2.4414e-04,  2.4152e-04,  ..., -2.0809e-03,
         -1.5059e-03,  1.6270e-03],
        [-2.3556e-03,  1.5726e-03,  2.3055e-04,  ..., -1.0042e-03,
          8.5545e-04, -1.9331e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5293,  0.0405, -3.1250,  ...,  3.3125,  0.3789, -1.1455]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2529, -0.0962,  0.4187,  ...,  0.0948, -0.0811,  0.0703]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-08 23:32:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of aware is unaware
The opposite of believable is unbelievable
The opposite of published is unpublished
The opposite of known is unknown
The opposite of lawful is unlawful
The opposite of happy is unhappy
The opposite of veiled is unveiled
The opposite of satisfactory is
2024-07-08 23:32:10 root INFO     [order_1_approx] starting weight calculation for The opposite of happy is unhappy
The opposite of aware is unaware
The opposite of believable is unbelievable
The opposite of published is unpublished
The opposite of known is unknown
The opposite of satisfactory is unsatisfactory
The opposite of lawful is unlawful
The opposite of veiled is
2024-07-08 23:32:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-08 23:36:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-08 23:40:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3865, -1.4629,  0.0700,  ...,  0.2864,  0.3330, -0.1597],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3330, -1.3477,  0.0427,  ...,  0.2415,  0.2664, -0.1730],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1934,  2.6758, -3.9336,  ...,  1.6367, -5.0625, -0.8408],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0033, -0.0099, -0.0193,  ...,  0.0164,  0.0024, -0.0044],
        [-0.0291, -0.0350,  0.0102,  ..., -0.0180, -0.0245,  0.0017],
        [ 0.0181,  0.0038, -0.0402,  ...,  0.0123,  0.0035, -0.0072],
        ...,
        [ 0.0117, -0.0117,  0.0096,  ...,  0.0025,  0.0044,  0.0200],
        [ 0.0461, -0.0149,  0.0021,  ...,  0.0084,  0.0216, -0.0169],
        [ 0.0117, -0.0160, -0.0012,  ..., -0.0222,  0.0324, -0.0023]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.2131e-03, -6.1560e-04, -1.6270e-03,  ...,  1.1959e-03,
          5.3644e-06,  1.0719e-03],
        [-4.3392e-05, -1.2646e-03,  2.1629e-03,  ...,  1.1806e-03,
          9.6846e-04,  4.7731e-04],
        [-1.9150e-03,  2.0504e-05, -5.1975e-04,  ...,  8.0061e-04,
          2.9635e-04,  1.7815e-03],
        ...,
        [ 2.7790e-03, -6.8188e-04, -1.0414e-03,  ..., -2.1019e-03,
         -3.6001e-04, -6.1655e-04],
        [ 2.9087e-03,  1.4400e-03, -1.9388e-03,  ..., -9.0027e-04,
         -2.1820e-03, -5.3072e-04],
        [-1.1387e-03,  1.2636e-05,  1.7719e-03,  ...,  5.3930e-04,
          1.4782e-04,  4.1676e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1699,  3.5195, -1.9766,  ...,  0.8848, -3.8906,  0.2109]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1610, -0.0342,  0.4551,  ...,  0.0590, -0.1484,  0.0293]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-08 23:40:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of happy is unhappy
The opposite of aware is unaware
The opposite of believable is unbelievable
The opposite of published is unpublished
The opposite of known is unknown
The opposite of satisfactory is unsatisfactory
The opposite of lawful is unlawful
The opposite of veiled is
2024-07-08 23:40:15 root INFO     [order_1_approx] starting weight calculation for The opposite of believable is unbelievable
The opposite of veiled is unveiled
The opposite of known is unknown
The opposite of published is unpublished
The opposite of satisfactory is unsatisfactory
The opposite of aware is unaware
The opposite of lawful is unlawful
The opposite of happy is
2024-07-08 23:40:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-08 23:44:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-08 23:48:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3818, -0.2131, -0.4570,  ...,  0.5137,  1.2227, -0.3154],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3347, -0.2134, -0.4011,  ...,  0.4404,  1.0527, -0.3120],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8965, -0.8115, -3.5312,  ...,  2.6562,  2.0020, -0.5684],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0063, -0.0107, -0.0180,  ..., -0.0056, -0.0058,  0.0017],
        [-0.0047, -0.0186,  0.0121,  ..., -0.0079, -0.0111, -0.0109],
        [-0.0047,  0.0027, -0.0161,  ..., -0.0089,  0.0083,  0.0304],
        ...,
        [-0.0130, -0.0067,  0.0099,  ..., -0.0079, -0.0132,  0.0113],
        [-0.0158, -0.0126,  0.0107,  ..., -0.0098, -0.0244, -0.0121],
        [-0.0104, -0.0225,  0.0069,  ..., -0.0085,  0.0018, -0.0009]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-3.3021e-04, -2.0504e-05, -2.4605e-04,  ..., -1.4324e-03,
         -9.8801e-04,  1.1129e-03],
        [-1.7118e-03, -1.1854e-03,  1.7052e-03,  ...,  1.8911e-03,
          2.0008e-03,  4.2105e-04],
        [ 1.3866e-03, -8.8263e-04, -5.1689e-04,  ..., -2.6970e-03,
          9.3579e-05,  1.7166e-03],
        ...,
        [-7.0000e-04, -1.5316e-03, -1.2131e-03,  ..., -7.8201e-04,
          6.0797e-05, -1.4019e-03],
        [ 6.4850e-05,  5.8842e-04,  1.4811e-03,  ..., -3.9215e-03,
         -2.5120e-03,  1.2770e-03],
        [ 4.7684e-06,  4.8304e-04,  6.9523e-04,  ..., -1.0777e-03,
         -7.5340e-04,  7.5054e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0283, -1.5410, -3.0820,  ...,  2.1113,  2.4102, -0.0454]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1387, -0.0690,  0.3193,  ...,  0.0427, -0.2479,  0.0231]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-08 23:48:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of believable is unbelievable
The opposite of veiled is unveiled
The opposite of known is unknown
The opposite of published is unpublished
The opposite of satisfactory is unsatisfactory
The opposite of aware is unaware
The opposite of lawful is unlawful
The opposite of happy is
2024-07-08 23:48:21 root INFO     [order_1_approx] starting weight calculation for The opposite of aware is unaware
The opposite of veiled is unveiled
The opposite of happy is unhappy
The opposite of satisfactory is unsatisfactory
The opposite of lawful is unlawful
The opposite of published is unpublished
The opposite of believable is unbelievable
The opposite of known is
2024-07-08 23:48:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-08 23:52:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-08 23:56:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.1422, 0.8418, 0.8047,  ..., 0.8130, 0.8062, 0.5830], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([0.1241, 0.7666, 0.6860,  ..., 0.7119, 0.7031, 0.5161], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1725, -0.9688,  2.6094,  ...,  1.6758,  1.2520,  0.2637],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0045, -0.0149,  0.0077,  ..., -0.0142,  0.0082, -0.0001],
        [-0.0024, -0.0078, -0.0214,  ...,  0.0156, -0.0077,  0.0039],
        [-0.0116, -0.0065, -0.0235,  ..., -0.0049,  0.0031,  0.0106],
        ...,
        [-0.0045, -0.0105,  0.0196,  ..., -0.0044, -0.0133, -0.0105],
        [-0.0125, -0.0062,  0.0043,  ...,  0.0204,  0.0051, -0.0065],
        [-0.0018, -0.0162,  0.0023,  ..., -0.0086,  0.0127,  0.0040]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.7695e-03,  9.1171e-04,  3.1114e-04,  ...,  5.0402e-04,
          5.3310e-04,  1.4629e-03],
        [-7.0620e-04, -1.1349e-03, -1.1039e-04,  ...,  1.2150e-03,
         -1.7786e-04, -1.7300e-03],
        [ 3.5191e-04,  3.6192e-04,  5.2929e-05,  ...,  1.1082e-03,
         -2.1124e-04, -1.8263e-03],
        ...,
        [ 8.0109e-05,  1.6012e-03,  4.6825e-04,  ..., -2.2392e-03,
         -5.4741e-04, -4.8590e-04],
        [-7.4339e-04,  7.7534e-04, -1.5450e-03,  ...,  4.6277e-04,
         -1.6251e-03,  1.7109e-03],
        [ 1.2169e-03, -1.2360e-03, -1.3885e-03,  ..., -9.0647e-04,
         -7.1383e-04,  4.3178e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2115, -0.5332,  2.0059,  ...,  2.7051,  2.1191,  0.9175]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0627, -0.0216,  0.1054,  ..., -0.0265, -0.1621,  0.3623]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-08 23:56:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of aware is unaware
The opposite of veiled is unveiled
The opposite of happy is unhappy
The opposite of satisfactory is unsatisfactory
The opposite of lawful is unlawful
The opposite of published is unpublished
The opposite of believable is unbelievable
The opposite of known is
2024-07-08 23:56:28 root INFO     [order_1_approx] starting weight calculation for The opposite of veiled is unveiled
The opposite of known is unknown
The opposite of happy is unhappy
The opposite of aware is unaware
The opposite of satisfactory is unsatisfactory
The opposite of published is unpublished
The opposite of lawful is unlawful
The opposite of believable is
2024-07-08 23:56:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 00:00:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 00:04:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1764, -0.7949,  0.3848,  ...,  0.4966,  0.8281, -0.0305],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1549, -0.6914,  0.2839,  ...,  0.3884,  0.6445, -0.0554],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6455, -0.4421,  3.6016,  ...,  4.1875,  2.3086,  1.2129],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0068, -0.0144, -0.0038,  ..., -0.0076,  0.0041, -0.0246],
        [ 0.0016, -0.0165, -0.0108,  ..., -0.0045, -0.0181, -0.0146],
        [-0.0172, -0.0249, -0.0237,  ..., -0.0231,  0.0155,  0.0402],
        ...,
        [-0.0002, -0.0162, -0.0056,  ..., -0.0096, -0.0134,  0.0129],
        [-0.0111,  0.0065,  0.0194,  ...,  0.0083, -0.0066,  0.0002],
        [ 0.0224,  0.0094, -0.0129,  ..., -0.0260,  0.0306, -0.0117]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0037,  0.0003, -0.0011,  ...,  0.0009,  0.0006, -0.0024],
        [-0.0009, -0.0030,  0.0011,  ...,  0.0037,  0.0029, -0.0004],
        [ 0.0008,  0.0016, -0.0023,  ..., -0.0038,  0.0012,  0.0038],
        ...,
        [ 0.0017,  0.0001, -0.0005,  ..., -0.0024,  0.0030, -0.0017],
        [-0.0022,  0.0011, -0.0015,  ..., -0.0012, -0.0026, -0.0003],
        [ 0.0018, -0.0003,  0.0003,  ...,  0.0021,  0.0007, -0.0006]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[2.8594, 0.5371, 3.2812,  ..., 1.4355, 2.8926, 1.9180]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0421, -0.0967, -0.0564,  ..., -0.1729, -0.2627,  0.0327]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 00:04:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of veiled is unveiled
The opposite of known is unknown
The opposite of happy is unhappy
The opposite of aware is unaware
The opposite of satisfactory is unsatisfactory
The opposite of published is unpublished
The opposite of lawful is unlawful
The opposite of believable is
2024-07-09 00:04:34 root INFO     [order_1_approx] starting weight calculation for The opposite of believable is unbelievable
The opposite of published is unpublished
The opposite of satisfactory is unsatisfactory
The opposite of veiled is unveiled
The opposite of aware is unaware
The opposite of happy is unhappy
The opposite of known is unknown
The opposite of lawful is
2024-07-09 00:04:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 00:08:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 00:12:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8389, -0.2886, -0.2239,  ...,  0.1978, -0.0283,  0.7002],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.7080, -0.2722, -0.1962,  ...,  0.1622, -0.0455,  0.5815],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3901, -5.0938, -2.2168,  ...,  0.2063,  4.1719, -1.3613],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 6.4850e-03, -1.3405e-02, -4.4441e-03,  ...,  8.6975e-04,
         -4.8599e-03, -7.6141e-03],
        [-2.4567e-03, -3.5583e-02, -6.5422e-03,  ..., -5.1918e-03,
          1.0513e-02,  6.1569e-03],
        [ 5.2643e-03,  8.2550e-03, -5.0430e-03,  ...,  2.9831e-03,
          3.3379e-05, -1.1070e-02],
        ...,
        [-7.5912e-03, -8.5526e-03,  6.1150e-03,  ...,  7.3090e-03,
         -1.2817e-03,  4.2953e-03],
        [-1.6617e-02, -3.6716e-03, -1.4000e-03,  ..., -1.6842e-03,
         -1.4160e-02, -4.3411e-03],
        [ 4.0627e-04,  8.0338e-03,  5.3024e-03,  ..., -7.8354e-03,
         -6.4735e-03, -7.2861e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0015,  0.0005, -0.0011,  ...,  0.0002,  0.0005, -0.0007],
        [ 0.0011, -0.0025, -0.0003,  ...,  0.0027,  0.0007,  0.0018],
        [-0.0001,  0.0016, -0.0034,  ...,  0.0018, -0.0006, -0.0008],
        ...,
        [-0.0010,  0.0024,  0.0006,  ...,  0.0020,  0.0025, -0.0011],
        [ 0.0001, -0.0012,  0.0003,  ..., -0.0004, -0.0017,  0.0015],
        [ 0.0010,  0.0003,  0.0001,  ...,  0.0010, -0.0010, -0.0028]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4094, -4.2539, -2.0234,  ...,  0.6797,  3.3594, -1.1611]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2490,  0.1783,  0.3247,  ...,  0.0592, -0.0936, -0.2144]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 00:12:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of believable is unbelievable
The opposite of published is unpublished
The opposite of satisfactory is unsatisfactory
The opposite of veiled is unveiled
The opposite of aware is unaware
The opposite of happy is unhappy
The opposite of known is unknown
The opposite of lawful is
2024-07-09 00:12:41 root INFO     [order_1_approx] starting weight calculation for The opposite of satisfactory is unsatisfactory
The opposite of veiled is unveiled
The opposite of published is unpublished
The opposite of believable is unbelievable
The opposite of happy is unhappy
The opposite of lawful is unlawful
The opposite of known is unknown
The opposite of aware is
2024-07-09 00:12:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 00:16:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 00:20:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9282, -0.1245, -0.4341,  ...,  0.2632,  0.3916,  0.7314],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.9150, -0.1469, -0.4250,  ...,  0.2512,  0.3613,  0.7134],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.7891,  0.2705, -3.1348,  ..., -0.2993, -0.0693, -0.1250],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0160, -0.0187,  0.0030,  ...,  0.0129,  0.0022, -0.0080],
        [ 0.0019, -0.0339, -0.0140,  ..., -0.0109, -0.0193, -0.0084],
        [-0.0086, -0.0136, -0.0263,  ...,  0.0052,  0.0177,  0.0231],
        ...,
        [ 0.0036, -0.0103, -0.0033,  ..., -0.0252,  0.0160,  0.0031],
        [ 0.0112,  0.0029,  0.0218,  ...,  0.0031, -0.0047, -0.0036],
        [-0.0062,  0.0263, -0.0025,  ..., -0.0186,  0.0164,  0.0059]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.4338e-03, -6.7139e-04, -8.0729e-04,  ..., -1.9360e-04,
          1.0147e-03, -5.6553e-04],
        [-1.0805e-03, -2.4378e-04,  2.1248e-03,  ...,  2.3937e-04,
         -1.0862e-03,  5.9986e-04],
        [-2.7122e-03,  2.2411e-05, -9.2554e-04,  ..., -4.6635e-04,
          1.7583e-04,  2.2755e-03],
        ...,
        [-1.7910e-03, -1.0719e-03,  1.6937e-03,  ...,  1.6155e-03,
          1.6594e-03, -2.6970e-03],
        [ 3.2234e-04,  2.2125e-03,  3.8791e-04,  ..., -1.8396e-03,
         -6.9046e-04,  2.2354e-03],
        [ 8.7881e-04,  2.8877e-03,  1.0424e-03,  ..., -1.0748e-03,
          1.6618e-04, -1.3227e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.8828, -0.5610, -2.6289,  ...,  0.2520,  0.5537, -1.6494]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.3298, -0.1134,  0.8521,  ...,  0.4788, -0.2166, -0.2888]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 00:20:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of satisfactory is unsatisfactory
The opposite of veiled is unveiled
The opposite of published is unpublished
The opposite of believable is unbelievable
The opposite of happy is unhappy
The opposite of lawful is unlawful
The opposite of known is unknown
The opposite of aware is
2024-07-09 00:20:50 root INFO     total operator prediction time: 3893.369474172592 seconds
2024-07-09 00:20:50 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on synonyms - exact
2024-07-09 00:20:52 root INFO     building operator synonyms - exact
2024-07-09 00:20:52 root INFO     [order_1_approx] starting weight calculation for Another word for list is listing
Another word for loyal is faithful
Another word for help is aid
Another word for style is manner
Another word for rational is logical
Another word for intelligent is clever
Another word for rock is stone
Another word for bicycle is
2024-07-09 00:20:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 00:25:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 00:28:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 6.1914e-01,  7.3242e-04, -1.3848e+00,  ...,  6.3965e-02,
         9.1309e-02,  6.9678e-01], device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.5386, -0.0194, -1.1758,  ...,  0.0551,  0.0577,  0.5986],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0346, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0918, -1.3711,  1.0137,  ..., -1.9717, -0.0522, -0.9844],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0241,  0.0012,  0.0045,  ...,  0.0100, -0.0163,  0.0062],
        [ 0.0053,  0.0051, -0.0065,  ...,  0.0150, -0.0068, -0.0064],
        [ 0.0178,  0.0009, -0.0263,  ..., -0.0149, -0.0024,  0.0003],
        ...,
        [-0.0129, -0.0068, -0.0044,  ..., -0.0070,  0.0132,  0.0103],
        [ 0.0080, -0.0073,  0.0175,  ..., -0.0015, -0.0077,  0.0137],
        [-0.0164,  0.0036,  0.0067,  ..., -0.0130,  0.0025, -0.0020]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 4.8423e-04, -1.0271e-03,  2.3041e-03,  ..., -1.3676e-03,
         -3.6182e-03, -2.7299e-04],
        [-8.1110e-04, -2.4300e-03,  2.1243e-04,  ...,  2.4967e-03,
          1.1978e-03,  7.1287e-04],
        [-3.5644e-05, -1.9169e-04, -6.9082e-05,  ..., -1.9836e-03,
          1.4400e-04, -6.8665e-04],
        ...,
        [-8.2016e-04, -1.5202e-03, -1.5202e-03,  ..., -1.6570e-04,
          1.5516e-03, -1.5697e-03],
        [-2.8038e-04, -1.9145e-04, -6.7234e-04,  ..., -6.1846e-04,
          3.7575e-04, -2.0351e-03],
        [-2.7180e-05,  1.7061e-03, -1.4820e-03,  ...,  2.0790e-04,
         -1.6952e-04, -1.3180e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1484, -0.6045, -0.1924,  ..., -1.4482, -0.2925, -0.9805]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0464, -0.0467, -0.0098,  ...,  0.1479,  0.1711,  0.1067]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 00:29:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for list is listing
Another word for loyal is faithful
Another word for help is aid
Another word for style is manner
Another word for rational is logical
Another word for intelligent is clever
Another word for rock is stone
Another word for bicycle is
2024-07-09 00:29:00 root INFO     [order_1_approx] starting weight calculation for Another word for rational is logical
Another word for bicycle is bike
Another word for loyal is faithful
Another word for rock is stone
Another word for style is manner
Another word for help is aid
Another word for list is listing
Another word for intelligent is
2024-07-09 00:29:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 00:33:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 00:37:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0000, -0.7227, -0.0613,  ...,  0.6763, -0.1865,  0.3411],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.9312, -0.7183, -0.0714,  ...,  0.6074, -0.1967,  0.2979],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0346, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.6914,  1.7227, -1.0498,  ..., -3.8496,  0.2754, -1.3740],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0061, -0.0039, -0.0018,  ..., -0.0074,  0.0094, -0.0101],
        [-0.0215,  0.0067, -0.0036,  ...,  0.0073, -0.0006,  0.0090],
        [ 0.0084,  0.0041, -0.0035,  ..., -0.0106,  0.0103,  0.0228],
        ...,
        [-0.0131,  0.0018,  0.0002,  ..., -0.0038, -0.0059,  0.0074],
        [-0.0113, -0.0135,  0.0054,  ..., -0.0069, -0.0002,  0.0028],
        [-0.0115,  0.0068, -0.0028,  ..., -0.0123,  0.0139,  0.0143]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.7680e-04, -8.9788e-04,  5.3930e-04,  ...,  1.7033e-03,
         -1.0815e-03, -1.4496e-04],
        [ 9.0694e-04,  2.9111e-04, -1.0462e-03,  ..., -2.1219e-05,
          8.6641e-04,  4.9019e-04],
        [ 1.4830e-03,  2.0206e-05, -8.8739e-04,  ..., -1.9670e-04,
         -5.8794e-04,  2.1629e-03],
        ...,
        [ 2.5916e-04, -1.7548e-03, -1.6260e-03,  ..., -1.2131e-03,
          1.3294e-03,  9.3889e-04],
        [-9.9850e-04,  3.1519e-04, -2.2163e-03,  ...,  2.8086e-04,
         -1.0090e-03, -2.2907e-03],
        [-2.4939e-04,  9.8801e-04, -1.9569e-03,  ..., -1.4343e-03,
         -1.4839e-03,  1.7872e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.4023,  1.0586, -0.8389,  ..., -2.8145, -0.5854, -1.8281]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2849,  0.0715,  0.3167,  ..., -0.0625,  0.3777,  0.4753]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 00:37:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for rational is logical
Another word for bicycle is bike
Another word for loyal is faithful
Another word for rock is stone
Another word for style is manner
Another word for help is aid
Another word for list is listing
Another word for intelligent is
2024-07-09 00:37:05 root INFO     [order_1_approx] starting weight calculation for Another word for list is listing
Another word for bicycle is bike
Another word for loyal is faithful
Another word for intelligent is clever
Another word for help is aid
Another word for style is manner
Another word for rational is logical
Another word for rock is
2024-07-09 00:37:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 00:41:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 00:45:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2014, -1.1641, -0.0942,  ...,  0.7012,  0.0044, -1.0996],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.1807, -1.1084, -0.0936,  ...,  0.6191, -0.0147, -1.0508],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0346, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.1836, -0.4563, -4.6484,  ..., -0.8154, -0.3740,  0.7051],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0017, -0.0078,  0.0036,  ...,  0.0076, -0.0085,  0.0094],
        [-0.0086,  0.0035, -0.0021,  ...,  0.0004, -0.0049, -0.0060],
        [-0.0033, -0.0058,  0.0161,  ...,  0.0079,  0.0114, -0.0150],
        ...,
        [ 0.0062, -0.0170,  0.0117,  ...,  0.0078,  0.0066,  0.0145],
        [ 0.0012, -0.0071,  0.0077,  ..., -0.0022, -0.0164, -0.0119],
        [ 0.0010, -0.0067, -0.0104,  ..., -0.0253, -0.0050,  0.0015]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-9.9182e-04, -1.3781e-03,  1.7376e-03,  ...,  3.9825e-03,
         -4.9686e-04,  1.2207e-03],
        [ 7.9536e-04, -1.1206e-05,  3.4103e-03,  ..., -4.7684e-06,
          3.8767e-04,  6.5327e-05],
        [-2.5511e-04, -1.9093e-03,  1.6384e-03,  ..., -1.0176e-03,
          5.8746e-04,  1.7443e-03],
        ...,
        [ 1.3046e-03,  8.6832e-04,  6.2799e-04,  ..., -1.4954e-03,
          3.9196e-04,  8.8882e-04],
        [ 3.3975e-04,  1.0138e-03, -1.0481e-03,  ..., -2.9802e-04,
          1.1921e-06, -9.3126e-04],
        [ 1.0891e-03,  1.7004e-03, -2.0008e-03,  ..., -3.6740e-04,
          4.9591e-05, -1.0691e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.7715, -1.2959, -4.0430,  ..., -0.4551, -0.5581,  0.4275]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1190,  0.0102,  0.2029,  ..., -0.0339,  0.1335,  0.2162]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 00:45:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for list is listing
Another word for bicycle is bike
Another word for loyal is faithful
Another word for intelligent is clever
Another word for help is aid
Another word for style is manner
Another word for rational is logical
Another word for rock is
2024-07-09 00:45:08 root INFO     [order_1_approx] starting weight calculation for Another word for style is manner
Another word for rational is logical
Another word for rock is stone
Another word for loyal is faithful
Another word for list is listing
Another word for intelligent is clever
Another word for bicycle is bike
Another word for help is
2024-07-09 00:45:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 00:49:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 00:53:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0942,  0.4878, -0.7017,  ...,  0.5015,  0.4146,  0.7407],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.0997,  0.4805, -0.6812,  ...,  0.4824,  0.3879,  0.7295],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0346, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.9062, -0.7290, -0.3662,  ..., -4.9062,  1.6836,  3.3809],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.3016e-02, -1.1040e-02, -7.0915e-03,  ...,  9.5367e-03,
          1.1902e-03, -1.1642e-02],
        [-1.6724e-02,  1.2863e-02,  1.6190e-02,  ...,  3.6240e-03,
         -2.0523e-02, -5.0888e-03],
        [-1.3294e-03,  1.8723e-02,  5.3864e-03,  ...,  1.0559e-02,
          2.0950e-02,  1.7914e-02],
        ...,
        [-3.1471e-05, -1.9608e-02,  8.8882e-04,  ..., -1.1444e-03,
          1.0300e-03,  3.8643e-03],
        [ 2.9182e-04,  7.2441e-03,  4.2152e-04,  ..., -1.1482e-02,
          1.2093e-02,  1.6525e-02],
        [-1.4984e-02, -1.2970e-02, -5.4626e-03,  ...,  1.0765e-02,
          2.6047e-02,  1.2604e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 4.5109e-04, -1.5192e-03,  3.0804e-04,  ...,  1.9016e-03,
         -3.6526e-03, -3.4881e-04],
        [ 4.3869e-04, -2.3627e-04,  1.8854e-03,  ...,  3.8872e-03,
          9.9850e-04,  2.5330e-03],
        [-1.0443e-03,  1.9951e-03,  2.0447e-03,  ...,  2.1315e-04,
          1.5860e-03,  3.8280e-03],
        ...,
        [-7.6294e-05, -2.0885e-03, -1.4830e-04,  ..., -2.4719e-03,
          3.4790e-03, -1.9760e-03],
        [-1.9283e-03,  3.4370e-03,  6.3658e-04,  ..., -4.1618e-03,
         -3.5834e-04,  3.4866e-03],
        [-1.0662e-03, -2.0695e-03,  1.3857e-03,  ...,  3.1853e-03,
          1.0862e-03, -1.3294e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6777, -0.0337, -1.5361,  ..., -4.6992,  1.4297,  2.0918]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2142, -0.2025, -0.1154,  ...,  0.0411,  0.2153,  0.1110]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 00:53:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for style is manner
Another word for rational is logical
Another word for rock is stone
Another word for loyal is faithful
Another word for list is listing
Another word for intelligent is clever
Another word for bicycle is bike
Another word for help is
2024-07-09 00:53:13 root INFO     [order_1_approx] starting weight calculation for Another word for list is listing
Another word for style is manner
Another word for bicycle is bike
Another word for intelligent is clever
Another word for loyal is faithful
Another word for help is aid
Another word for rock is stone
Another word for rational is
2024-07-09 00:53:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 00:57:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 01:01:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6074, -1.0537, -0.2416,  ..., -0.4756,  0.6963,  0.8330],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.5474, -1.0049, -0.2247,  ..., -0.4150,  0.6035,  0.7476],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0346, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6924,  0.1318, -3.8867,  ..., -1.2920,  5.5547,  2.0508],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0026, -0.0024,  0.0275,  ..., -0.0074, -0.0052,  0.0027],
        [-0.0236,  0.0052, -0.0018,  ...,  0.0173, -0.0171, -0.0046],
        [-0.0014,  0.0176, -0.0060,  ...,  0.0158,  0.0071,  0.0210],
        ...,
        [-0.0054, -0.0085,  0.0026,  ..., -0.0026,  0.0013,  0.0119],
        [-0.0106, -0.0307,  0.0016,  ..., -0.0004,  0.0096, -0.0031],
        [-0.0055, -0.0137,  0.0064,  ...,  0.0102,  0.0160,  0.0048]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 1.6487e-04, -3.3474e-04,  2.1610e-03,  ...,  2.2526e-03,
         -2.0218e-03, -7.2861e-04],
        [-9.2983e-04, -2.8801e-04, -1.1711e-03,  ...,  3.4094e-05,
          7.6175e-05, -8.3685e-05],
        [ 1.0796e-03, -2.4378e-04,  6.8474e-04,  ..., -2.0194e-04,
          4.4060e-04,  7.9155e-04],
        ...,
        [ 1.1730e-03, -1.5240e-03,  4.0169e-03,  ...,  1.7567e-03,
          1.3962e-03, -1.3037e-03],
        [-1.0605e-03, -1.9150e-03,  1.8501e-03,  ...,  1.8435e-03,
          1.7178e-04, -9.6703e-04],
        [ 4.3154e-04,  6.9618e-04,  6.5136e-04,  ..., -8.9836e-04,
         -2.1133e-03,  5.7030e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6118, -0.3210, -3.3516,  ..., -1.9902,  4.5859,  0.6172]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2852,  0.0113,  0.1215,  ..., -0.0606, -0.2242, -0.0321]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 01:01:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for list is listing
Another word for style is manner
Another word for bicycle is bike
Another word for intelligent is clever
Another word for loyal is faithful
Another word for help is aid
Another word for rock is stone
Another word for rational is
2024-07-09 01:01:20 root INFO     [order_1_approx] starting weight calculation for Another word for loyal is faithful
Another word for bicycle is bike
Another word for intelligent is clever
Another word for rock is stone
Another word for rational is logical
Another word for help is aid
Another word for style is manner
Another word for list is
2024-07-09 01:01:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 01:05:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 01:09:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1389,  0.0132, -0.1804,  ...,  0.2654, -0.3464,  0.7300],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.1440, -0.0029, -0.1946,  ...,  0.2747, -0.3821,  0.7690],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0346, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.2402, -1.0410, -0.3018,  ..., -1.5176,  2.8555,  2.7891],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0013, -0.0042, -0.0039,  ..., -0.0027, -0.0157, -0.0216],
        [ 0.0043,  0.0191,  0.0059,  ...,  0.0294, -0.0042,  0.0082],
        [ 0.0029,  0.0047, -0.0015,  ..., -0.0047, -0.0172,  0.0055],
        ...,
        [-0.0144, -0.0108,  0.0103,  ..., -0.0121, -0.0010,  0.0269],
        [-0.0025, -0.0194, -0.0013,  ..., -0.0054,  0.0150,  0.0220],
        [ 0.0085,  0.0024, -0.0105,  ...,  0.0029,  0.0185,  0.0176]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-6.8855e-04, -1.0920e-03, -1.8263e-03,  ...,  1.0252e-04,
         -1.8091e-03, -4.2343e-04],
        [ 1.0157e-03, -1.2169e-03,  3.6335e-04,  ...,  1.4801e-03,
          5.2404e-04,  4.0364e-04],
        [-3.5095e-04, -1.0233e-03, -1.2493e-03,  ...,  9.3937e-04,
         -1.4896e-03, -2.1229e-03],
        ...,
        [-7.3004e-04,  1.8311e-03,  1.3828e-03,  ..., -9.9182e-04,
          2.6360e-03,  2.8877e-03],
        [ 3.0339e-05,  4.9543e-04, -4.4823e-04,  ..., -3.1242e-03,
          1.0357e-03,  1.2426e-03],
        [ 6.2847e-04, -1.8406e-03, -1.5841e-03,  ...,  6.3562e-04,
         -1.9093e-03, -6.9189e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6934, -0.2236, -0.9585,  ..., -1.6543,  2.7070,  1.8750]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1006,  0.2407,  0.0615,  ..., -0.1213, -0.0262, -0.0840]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 01:09:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for loyal is faithful
Another word for bicycle is bike
Another word for intelligent is clever
Another word for rock is stone
Another word for rational is logical
Another word for help is aid
Another word for style is manner
Another word for list is
2024-07-09 01:09:26 root INFO     [order_1_approx] starting weight calculation for Another word for bicycle is bike
Another word for rational is logical
Another word for intelligent is clever
Another word for list is listing
Another word for rock is stone
Another word for help is aid
Another word for loyal is faithful
Another word for style is
2024-07-09 01:09:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 01:13:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 01:17:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.6494, -0.7334, -0.0134,  ...,  1.0078,  0.0659,  0.0310],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 1.5459, -0.7231, -0.0238,  ...,  0.9102,  0.0419,  0.0056],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0346, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6514, -1.0029, -1.8125,  ..., -2.7656,  0.3076,  2.4492],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.1013e-03, -1.1017e-02,  1.2810e-02,  ...,  9.1934e-03,
         -2.5253e-03, -8.7357e-03],
        [-9.7046e-03, -2.3651e-04, -8.5449e-03,  ...,  1.3794e-02,
         -1.6495e-02, -3.2684e-02],
        [ 5.2605e-03, -1.0223e-02, -1.6117e-03,  ..., -9.5444e-03,
         -3.7155e-03,  1.7105e-02],
        ...,
        [-3.9673e-04, -4.1008e-04,  1.9714e-02,  ..., -5.3825e-03,
          5.1651e-03,  2.0752e-02],
        [-4.5586e-04, -1.2787e-02,  5.8517e-03,  ..., -8.0109e-03,
          2.5146e-02,  2.2324e-02],
        [ 7.5455e-03,  3.0365e-03, -1.9073e-05,  ...,  5.7220e-03,
          1.8723e-02,  1.9211e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-7.3147e-04, -1.6537e-03, -5.5408e-04,  ...,  3.3998e-04,
         -1.0757e-03,  1.4210e-03],
        [ 1.6441e-03,  5.4836e-04,  9.6464e-04,  ...,  2.2583e-03,
          5.3883e-04,  2.7733e-03],
        [-7.6580e-04, -8.6832e-04,  1.4973e-04,  ...,  5.4300e-05,
          5.4646e-04,  2.6073e-03],
        ...,
        [ 1.0357e-03, -4.3583e-04,  6.5613e-04,  ..., -1.0881e-03,
          1.4973e-03, -1.2455e-03],
        [ 2.6751e-04, -6.5422e-04,  8.0585e-05,  ..., -1.8635e-03,
          2.3174e-03, -1.4734e-04],
        [ 1.5106e-03,  1.1692e-03,  1.0691e-03,  ...,  6.2466e-05,
          1.9813e-04,  2.7924e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2227,  0.4541, -1.9902,  ..., -2.8184,  0.9819,  0.8955]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1918,  0.2190,  0.1699,  ...,  0.2241, -0.4111, -0.0307]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 01:17:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for bicycle is bike
Another word for rational is logical
Another word for intelligent is clever
Another word for list is listing
Another word for rock is stone
Another word for help is aid
Another word for loyal is faithful
Another word for style is
2024-07-09 01:17:30 root INFO     [order_1_approx] starting weight calculation for Another word for bicycle is bike
Another word for help is aid
Another word for intelligent is clever
Another word for style is manner
Another word for rational is logical
Another word for rock is stone
Another word for list is listing
Another word for loyal is
2024-07-09 01:17:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 01:21:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 01:25:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2179, -0.3455, -0.4126,  ...,  0.6621,  0.1860, -0.2720],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.1824, -0.3259, -0.3542,  ...,  0.5503,  0.1375, -0.2664],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0346, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0869,  3.0332,  0.7305,  ..., -3.6113,  4.8672,  4.9258],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0010,  0.0009,  0.0191,  ..., -0.0050,  0.0078, -0.0137],
        [-0.0063,  0.0187, -0.0211,  ...,  0.0111, -0.0078,  0.0160],
        [ 0.0145,  0.0045,  0.0148,  ..., -0.0161, -0.0060,  0.0153],
        ...,
        [-0.0075,  0.0041, -0.0074,  ..., -0.0139,  0.0086, -0.0084],
        [-0.0214, -0.0134, -0.0104,  ...,  0.0014, -0.0056,  0.0009],
        [ 0.0092, -0.0141, -0.0132,  ...,  0.0025,  0.0195,  0.0168]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.1425e-03,  1.2665e-03,  1.3952e-03,  ...,  3.3164e-04,
         -1.5507e-03, -4.4823e-05],
        [ 3.5286e-05, -1.1549e-03,  7.2479e-04,  ...,  4.7779e-04,
          8.5163e-04,  2.5673e-03],
        [-1.9217e-04, -8.0776e-04,  1.9875e-03,  ...,  4.6313e-05,
          3.0575e-03, -7.2622e-04],
        ...,
        [ 4.2605e-04, -7.1049e-04, -7.0190e-04,  ..., -4.9400e-04,
          2.2068e-03,  1.2703e-03],
        [-3.6168e-04, -1.7762e-04, -2.1324e-03,  ..., -1.6880e-04,
         -1.2112e-03, -2.7061e-04],
        [ 1.4687e-03, -9.5463e-04,  5.1165e-04,  ...,  2.4261e-03,
          1.7433e-03, -9.0480e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6084,  2.8477,  0.8447,  ..., -4.4648,  3.8633,  4.2969]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2198, -0.1206,  0.2224,  ..., -0.1179,  0.0028,  0.2135]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 01:25:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for bicycle is bike
Another word for help is aid
Another word for intelligent is clever
Another word for style is manner
Another word for rational is logical
Another word for rock is stone
Another word for list is listing
Another word for loyal is
2024-07-09 01:25:35 root INFO     total operator prediction time: 3883.350134372711 seconds
2024-07-09 01:25:35 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on country - capital
2024-07-09 01:25:38 root INFO     building operator country - capital
2024-07-09 01:25:38 root INFO     [order_1_approx] starting weight calculation for The country with damascus as its capital is known as syria
The country with dhaka as its capital is known as bangladesh
The country with santiago as its capital is known as chile
The country with stockholm as its capital is known as sweden
The country with bern as its capital is known as switzerland
The country with conakry as its capital is known as guinea
The country with oslo as its capital is known as norway
The country with baghdad as its capital is known as
2024-07-09 01:25:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 01:29:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 01:33:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0394, -0.0641, -0.0705,  ..., -0.8984, -0.1897,  0.9224],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.0443, -0.0863, -0.0824,  ..., -0.9414, -0.2214,  1.0156],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3311,  2.5664,  3.1992,  ..., -3.6523, -2.2812, -1.0391],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0066, -0.0222,  0.0030,  ...,  0.0089,  0.0059, -0.0024],
        [ 0.0007, -0.0211,  0.0245,  ...,  0.0458, -0.0045, -0.0290],
        [-0.0104,  0.0015,  0.0018,  ...,  0.0089, -0.0043, -0.0032],
        ...,
        [-0.0051, -0.0090,  0.0044,  ...,  0.0112,  0.0069,  0.0172],
        [-0.0111,  0.0049, -0.0157,  ..., -0.0100, -0.0068,  0.0164],
        [ 0.0019, -0.0038, -0.0025,  ..., -0.0159,  0.0044, -0.0007]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-4.9829e-04,  9.1028e-04,  5.9652e-04,  ...,  5.3692e-04,
          2.5988e-04, -5.8556e-04],
        [ 5.2786e-04,  1.3971e-04,  5.1737e-04,  ...,  1.6346e-03,
         -2.3842e-07, -1.6823e-03],
        [-4.5109e-04, -5.1498e-04,  2.9635e-04,  ...,  1.2279e-04,
         -6.8855e-04, -7.5340e-04],
        ...,
        [ 3.8815e-04, -2.1343e-03, -9.8825e-05,  ...,  1.8711e-03,
         -5.1689e-04,  2.1744e-04],
        [-6.8665e-04,  1.0643e-03, -1.6050e-03,  ...,  4.7374e-04,
         -2.3222e-04, -1.8573e-04],
        [-1.2398e-05,  9.0599e-05, -3.6907e-04,  ...,  4.5300e-04,
         -1.5569e-04, -1.6680e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9165,  2.1719,  2.9922,  ..., -2.7383, -1.8271, -0.3955]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0712, -0.1234, -0.0488,  ..., -0.0373,  0.0879,  0.0003]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 01:33:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with damascus as its capital is known as syria
The country with dhaka as its capital is known as bangladesh
The country with santiago as its capital is known as chile
The country with stockholm as its capital is known as sweden
The country with bern as its capital is known as switzerland
The country with conakry as its capital is known as guinea
The country with oslo as its capital is known as norway
The country with baghdad as its capital is known as
2024-07-09 01:33:43 root INFO     [order_1_approx] starting weight calculation for The country with stockholm as its capital is known as sweden
The country with santiago as its capital is known as chile
The country with baghdad as its capital is known as iraq
The country with bern as its capital is known as switzerland
The country with conakry as its capital is known as guinea
The country with damascus as its capital is known as syria
The country with dhaka as its capital is known as bangladesh
The country with oslo as its capital is known as
2024-07-09 01:33:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 01:37:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 01:41:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2725,  1.6953,  0.5601,  ..., -0.7734, -0.2920,  0.1118],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-1.2275,  1.6572,  0.5049,  ..., -0.7075, -0.2935,  0.0883],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.8740,  0.0977,  0.7959,  ..., -7.0078, -2.4004, -1.9316],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0048,  0.0031,  0.0034,  ...,  0.0009,  0.0040, -0.0025],
        [-0.0006, -0.0077, -0.0024,  ...,  0.0064, -0.0020, -0.0077],
        [ 0.0009,  0.0023,  0.0023,  ...,  0.0026, -0.0002, -0.0033],
        ...,
        [-0.0031, -0.0028,  0.0069,  ..., -0.0004, -0.0043,  0.0058],
        [ 0.0145,  0.0090, -0.0022,  ..., -0.0069,  0.0007, -0.0004],
        [ 0.0040,  0.0092, -0.0009,  ..., -0.0037, -0.0013, -0.0090]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 3.7527e-04,  3.1769e-05, -2.1803e-04,  ..., -6.9857e-04,
         -4.3273e-04,  1.2789e-03],
        [-8.0764e-05, -1.3371e-03,  9.8825e-05,  ...,  2.4092e-04,
          2.3663e-05, -7.1859e-04],
        [-6.7043e-04, -3.1424e-04, -2.3317e-04,  ..., -6.4802e-04,
          7.6199e-04,  1.5478e-03],
        ...,
        [-4.4155e-04, -7.2289e-04,  4.0054e-04,  ...,  8.3923e-05,
          2.9874e-04,  1.9608e-03],
        [ 1.2379e-03, -1.8096e-04,  1.4019e-04,  ..., -1.3900e-04,
         -1.2338e-04, -4.7064e-04],
        [-4.6873e-04, -8.5354e-04, -1.1533e-04,  ...,  4.0674e-04,
         -3.6740e-04, -2.2984e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.3906,  0.3188,  0.3735,  ..., -7.4531, -2.3594, -2.3984]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1136, -0.1940,  0.0412,  ...,  0.0679, -0.1172,  0.0638]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 01:41:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with stockholm as its capital is known as sweden
The country with santiago as its capital is known as chile
The country with baghdad as its capital is known as iraq
The country with bern as its capital is known as switzerland
The country with conakry as its capital is known as guinea
The country with damascus as its capital is known as syria
The country with dhaka as its capital is known as bangladesh
The country with oslo as its capital is known as
2024-07-09 01:41:50 root INFO     [order_1_approx] starting weight calculation for The country with santiago as its capital is known as chile
The country with bern as its capital is known as switzerland
The country with dhaka as its capital is known as bangladesh
The country with damascus as its capital is known as syria
The country with baghdad as its capital is known as iraq
The country with oslo as its capital is known as norway
The country with conakry as its capital is known as guinea
The country with stockholm as its capital is known as
2024-07-09 01:41:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 01:45:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 01:49:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9893,  1.5508, -0.1790,  ...,  0.1960,  0.9966,  0.0359],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.9365,  1.4766, -0.1729,  ...,  0.1796,  0.9043,  0.0097],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-5.7070,  0.2244, -1.5117,  ..., -2.7168, -1.1768, -6.8711],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0160,  0.0134, -0.0142,  ..., -0.0019,  0.0197, -0.0139],
        [-0.0135, -0.0112,  0.0056,  ...,  0.0087, -0.0030, -0.0011],
        [ 0.0108,  0.0128, -0.0081,  ..., -0.0012,  0.0130, -0.0074],
        ...,
        [-0.0059,  0.0004,  0.0067,  ...,  0.0002, -0.0097,  0.0068],
        [-0.0014,  0.0053,  0.0008,  ..., -0.0028,  0.0068, -0.0058],
        [-0.0015,  0.0148, -0.0135,  ...,  0.0047,  0.0100, -0.0057]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.1432e-04, -1.6761e-04, -5.7077e-04,  ..., -9.3651e-04,
         -1.8673e-03, -6.8605e-05],
        [-7.4005e-04, -1.1339e-03,  1.1892e-03,  ...,  1.4296e-03,
          1.8330e-03, -1.6031e-03],
        [-1.8120e-03, -7.5531e-04,  8.8120e-04,  ...,  5.6314e-04,
          2.3975e-03,  1.5175e-04],
        ...,
        [-9.3746e-04, -1.7939e-03,  1.0719e-03,  ..., -5.6839e-04,
          1.1911e-03,  1.7691e-03],
        [-1.5240e-03, -4.0960e-04,  1.2465e-03,  ...,  9.9373e-04,
          5.0783e-04, -3.2043e-03],
        [-3.8719e-04, -1.2779e-03,  1.4553e-03,  ...,  2.1038e-03,
         -1.3752e-03,  3.2425e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.8906, -0.0981, -2.0664,  ..., -3.2695, -0.7666, -6.6523]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2482, -0.3655, -0.0959,  ..., -0.0527, -0.4048,  0.0806]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 01:49:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with santiago as its capital is known as chile
The country with bern as its capital is known as switzerland
The country with dhaka as its capital is known as bangladesh
The country with damascus as its capital is known as syria
The country with baghdad as its capital is known as iraq
The country with oslo as its capital is known as norway
The country with conakry as its capital is known as guinea
The country with stockholm as its capital is known as
2024-07-09 01:49:55 root INFO     [order_1_approx] starting weight calculation for The country with dhaka as its capital is known as bangladesh
The country with bern as its capital is known as switzerland
The country with stockholm as its capital is known as sweden
The country with baghdad as its capital is known as iraq
The country with oslo as its capital is known as norway
The country with conakry as its capital is known as guinea
The country with santiago as its capital is known as chile
The country with damascus as its capital is known as
2024-07-09 01:49:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 01:54:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 01:57:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1934, -0.3743, -0.5854,  ..., -0.1370, -1.1426,  0.6016],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 1.1572, -0.3921, -0.5566,  ..., -0.1255, -1.1162,  0.5718],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4883,  0.1416,  1.4443,  ..., -6.4570,  2.2305, -1.9229],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0023, -0.0064,  0.0026,  ...,  0.0035,  0.0117, -0.0195],
        [-0.0093, -0.0248,  0.0186,  ...,  0.0155,  0.0039, -0.0096],
        [ 0.0059,  0.0015, -0.0107,  ..., -0.0003, -0.0092,  0.0157],
        ...,
        [-0.0138,  0.0143, -0.0071,  ...,  0.0020, -0.0209,  0.0152],
        [-0.0004,  0.0086, -0.0102,  ..., -0.0119, -0.0089,  0.0280],
        [-0.0055, -0.0102, -0.0104,  ...,  0.0014,  0.0097,  0.0021]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 2.3353e-04,  9.7561e-04, -2.2340e-04,  ..., -6.0177e-04,
          8.8358e-04, -2.1696e-04],
        [-1.2560e-03, -7.0953e-04,  1.3466e-03,  ..., -8.7643e-04,
          2.5105e-04, -4.1938e-04],
        [-3.1471e-04,  5.3120e-04,  1.5616e-04,  ...,  9.9838e-05,
          1.4095e-03,  3.8481e-04],
        ...,
        [ 4.3535e-04,  3.3593e-04, -4.2439e-05,  ..., -1.0862e-03,
         -6.0606e-04,  1.7080e-03],
        [ 4.9782e-04,  5.7983e-04,  6.6161e-05,  ..., -1.3227e-03,
         -1.0223e-03,  1.2131e-03],
        [-2.2030e-04,  1.0663e-04,  4.4751e-04,  ..., -2.6703e-04,
         -4.6730e-05, -6.7234e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0303, -0.2180,  1.1611,  ..., -6.2188,  2.8711, -1.5117]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0012, -0.0828,  0.0242,  ...,  0.0380, -0.1476,  0.0164]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 01:58:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with dhaka as its capital is known as bangladesh
The country with bern as its capital is known as switzerland
The country with stockholm as its capital is known as sweden
The country with baghdad as its capital is known as iraq
The country with oslo as its capital is known as norway
The country with conakry as its capital is known as guinea
The country with santiago as its capital is known as chile
The country with damascus as its capital is known as
2024-07-09 01:58:01 root INFO     [order_1_approx] starting weight calculation for The country with dhaka as its capital is known as bangladesh
The country with oslo as its capital is known as norway
The country with conakry as its capital is known as guinea
The country with damascus as its capital is known as syria
The country with baghdad as its capital is known as iraq
The country with santiago as its capital is known as chile
The country with stockholm as its capital is known as sweden
The country with bern as its capital is known as
2024-07-09 01:58:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 02:02:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 02:06:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.4473,  0.1702, -1.8477,  ...,  0.3213,  0.3833, -0.5850],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-1.3379,  0.1461, -1.6338,  ...,  0.2903,  0.3320, -0.5713],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4028,  1.7207, -4.1328,  ..., -3.2188, -3.9336, -2.4668],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0081,  0.0020,  0.0133,  ...,  0.0097, -0.0053, -0.0173],
        [-0.0005, -0.0080, -0.0088,  ...,  0.0150, -0.0002,  0.0027],
        [ 0.0215,  0.0224,  0.0107,  ...,  0.0039,  0.0173, -0.0121],
        ...,
        [-0.0055, -0.0071, -0.0089,  ...,  0.0066, -0.0158, -0.0121],
        [-0.0135,  0.0026,  0.0114,  ...,  0.0026, -0.0079,  0.0223],
        [-0.0028, -0.0015,  0.0029,  ..., -0.0077,  0.0227,  0.0012]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-3.5334e-04,  3.4571e-05, -4.7684e-05,  ..., -1.3399e-04,
         -7.6675e-04, -6.1417e-04],
        [-1.3523e-03, -5.8889e-05,  1.3161e-03,  ...,  1.6117e-03,
          1.1873e-03, -6.2370e-04],
        [ 2.7142e-03,  1.5259e-03, -2.4652e-04,  ..., -7.4434e-04,
         -4.0412e-04,  4.2319e-04],
        ...,
        [ 1.9562e-04,  1.4200e-03,  3.6812e-04,  ...,  6.3753e-04,
         -4.8089e-04, -1.1768e-03],
        [ 1.6451e-04,  7.2098e-04, -2.9969e-04,  ...,  4.3297e-04,
         -2.6274e-04, -1.9417e-03],
        [ 3.5286e-04,  2.8610e-05, -1.6766e-03,  ...,  1.4105e-03,
          4.4346e-05, -9.4986e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9497,  2.6621, -5.3477,  ..., -2.1836, -4.2891, -0.9941]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0800, -0.1111,  0.1984,  ..., -0.0563, -0.0362, -0.0595]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 02:06:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with dhaka as its capital is known as bangladesh
The country with oslo as its capital is known as norway
The country with conakry as its capital is known as guinea
The country with damascus as its capital is known as syria
The country with baghdad as its capital is known as iraq
The country with santiago as its capital is known as chile
The country with stockholm as its capital is known as sweden
The country with bern as its capital is known as
2024-07-09 02:06:07 root INFO     [order_1_approx] starting weight calculation for The country with damascus as its capital is known as syria
The country with dhaka as its capital is known as bangladesh
The country with baghdad as its capital is known as iraq
The country with conakry as its capital is known as guinea
The country with stockholm as its capital is known as sweden
The country with bern as its capital is known as switzerland
The country with oslo as its capital is known as norway
The country with santiago as its capital is known as
2024-07-09 02:06:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 02:10:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 02:14:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4102,  0.5137,  0.1161,  ..., -0.8672, -0.0066,  0.0789],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.4199,  0.5181,  0.1019,  ..., -0.8389, -0.0245,  0.0588],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.3066, -0.6196,  1.6738,  ..., -1.7529, -1.2832, -0.6611],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0264,  0.0008, -0.0074,  ...,  0.0294,  0.0157, -0.0035],
        [-0.0099, -0.0126,  0.0074,  ...,  0.0100, -0.0232,  0.0044],
        [ 0.0043, -0.0004,  0.0029,  ...,  0.0093,  0.0055, -0.0004],
        ...,
        [-0.0171,  0.0086, -0.0055,  ...,  0.0162, -0.0146,  0.0114],
        [ 0.0047, -0.0004, -0.0062,  ...,  0.0112,  0.0003,  0.0198],
        [-0.0075,  0.0202, -0.0077,  ..., -0.0113,  0.0114, -0.0076]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.8225e-03,  2.3139e-04,  1.4410e-03,  ...,  2.6875e-03,
         -7.1001e-04,  4.9448e-04],
        [ 1.1053e-03, -2.2328e-04,  2.7180e-03,  ...,  3.0708e-04,
          5.3644e-04, -1.8406e-04],
        [ 7.5102e-06,  1.2302e-04,  4.0340e-04,  ..., -2.6560e-04,
         -1.4286e-03, -9.2697e-04],
        ...,
        [-1.1864e-03, -1.5593e-03,  8.5163e-04,  ...,  1.1168e-03,
          1.3580e-03, -7.3290e-04],
        [-8.8739e-04, -4.8018e-04,  5.8079e-04,  ...,  3.4070e-04,
          1.3390e-03,  7.4291e-04],
        [-1.4400e-03, -1.4687e-03, -8.1730e-04,  ...,  1.2131e-03,
         -1.3208e-03,  1.3456e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.1738, -0.1025,  1.1465,  ..., -1.0918, -0.7651,  0.3247]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0468,  0.1041, -0.1208,  ...,  0.0906, -0.1984, -0.1030]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 02:14:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with damascus as its capital is known as syria
The country with dhaka as its capital is known as bangladesh
The country with baghdad as its capital is known as iraq
The country with conakry as its capital is known as guinea
The country with stockholm as its capital is known as sweden
The country with bern as its capital is known as switzerland
The country with oslo as its capital is known as norway
The country with santiago as its capital is known as
2024-07-09 02:14:14 root INFO     [order_1_approx] starting weight calculation for The country with baghdad as its capital is known as iraq
The country with oslo as its capital is known as norway
The country with stockholm as its capital is known as sweden
The country with damascus as its capital is known as syria
The country with bern as its capital is known as switzerland
The country with santiago as its capital is known as chile
The country with dhaka as its capital is known as bangladesh
The country with conakry as its capital is known as
2024-07-09 02:14:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 02:18:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 02:22:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2346, -0.5469,  0.2041,  ...,  0.7368, -0.3928,  0.0996],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.2400, -0.5845,  0.1897,  ...,  0.7256, -0.4097,  0.0829],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.0957,  4.9297,  1.6514,  ..., -3.4961, -1.6230,  0.2441],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0119, -0.0035,  0.0058,  ...,  0.0073,  0.0138, -0.0230],
        [-0.0177, -0.0389,  0.0078,  ...,  0.0088, -0.0210, -0.0216],
        [-0.0161, -0.0043,  0.0080,  ..., -0.0055, -0.0067, -0.0152],
        ...,
        [-0.0174, -0.0033,  0.0182,  ...,  0.0127, -0.0133,  0.0162],
        [ 0.0145,  0.0238, -0.0134,  ...,  0.0252, -0.0105,  0.0234],
        [ 0.0131, -0.0270,  0.0087,  ..., -0.0073,  0.0188, -0.0047]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-6.0797e-04, -2.7633e-04, -6.4039e-04,  ...,  8.9073e-04,
         -4.4823e-04, -1.0128e-03],
        [-1.4973e-03, -7.3290e-04,  3.8791e-04,  ..., -2.9182e-04,
          1.0166e-03,  3.9721e-04],
        [-1.7605e-03, -8.9645e-04,  1.3790e-03,  ...,  1.4532e-04,
          3.3677e-05, -6.1703e-04],
        ...,
        [-7.9584e-04,  1.5478e-03,  1.3094e-03,  ...,  1.4000e-03,
          5.6076e-04, -5.6362e-04],
        [ 1.7471e-03,  3.2961e-05, -8.8787e-04,  ...,  7.2718e-04,
         -3.9363e-04, -6.1369e-04],
        [-4.5586e-04, -2.3723e-04, -1.5965e-03,  ..., -6.0558e-04,
         -4.5371e-04,  7.1096e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.2910,  2.6523,  1.5488,  ..., -3.8965, -1.6318, -0.4194]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1447, -0.3357, -0.1139,  ..., -0.1853,  0.1382,  0.1035]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 02:22:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with baghdad as its capital is known as iraq
The country with oslo as its capital is known as norway
The country with stockholm as its capital is known as sweden
The country with damascus as its capital is known as syria
The country with bern as its capital is known as switzerland
The country with santiago as its capital is known as chile
The country with dhaka as its capital is known as bangladesh
The country with conakry as its capital is known as
2024-07-09 02:22:21 root INFO     [order_1_approx] starting weight calculation for The country with oslo as its capital is known as norway
The country with conakry as its capital is known as guinea
The country with bern as its capital is known as switzerland
The country with baghdad as its capital is known as iraq
The country with stockholm as its capital is known as sweden
The country with santiago as its capital is known as chile
The country with damascus as its capital is known as syria
The country with dhaka as its capital is known as
2024-07-09 02:22:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 02:26:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 02:30:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3389,  0.3972,  0.7983,  ..., -0.7227, -0.2798,  1.1279],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 1.4531,  0.4253,  0.8164,  ..., -0.7485, -0.3181,  1.2266],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6777, -0.8218, -2.8828,  ..., -1.3418,  3.7168,  2.0469],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.8787e-03,  8.5754e-03,  8.3771e-03,  ...,  4.5624e-03,
          1.6556e-02, -2.2850e-03],
        [-1.6432e-03,  1.1082e-03, -9.3651e-04,  ...,  2.0905e-02,
          9.2773e-03,  5.9433e-03],
        [ 6.0997e-03,  1.5388e-02, -7.8583e-04,  ..., -2.0157e-02,
          5.8060e-03, -2.8229e-04],
        ...,
        [-1.2726e-02, -1.1917e-02,  1.9501e-02,  ..., -4.3640e-03,
         -9.9564e-04, -3.6621e-04],
        [-6.6757e-03,  6.6757e-06, -1.8406e-03,  ..., -3.5095e-03,
         -2.8458e-03,  5.1689e-03],
        [-5.9471e-03, -1.2836e-03,  4.8714e-03,  ..., -9.9182e-03,
          4.0665e-03,  1.7899e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-3.7074e-05, -8.3351e-04,  6.3610e-04,  ...,  3.3140e-04,
          1.3065e-03,  6.6566e-04],
        [ 1.2836e-03,  4.6515e-04, -1.6487e-04,  ...,  1.4563e-03,
          2.7037e-04,  3.7193e-04],
        [-1.9312e-05, -7.1239e-04,  4.7088e-04,  ..., -8.1062e-05,
          5.9891e-04,  1.1272e-03],
        ...,
        [ 9.3341e-05, -3.9387e-04,  1.6794e-03,  ...,  1.2417e-03,
          2.1820e-03,  1.4133e-03],
        [ 4.1962e-04,  5.2977e-04,  9.5177e-04,  ..., -1.3876e-04,
         -5.8508e-04, -1.0614e-03],
        [-4.6229e-04, -5.4836e-04,  5.1880e-04,  ..., -1.2140e-03,
         -1.3456e-03,  2.6665e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5957, -0.8442, -3.5840,  ..., -1.2725,  3.5664,  1.7354]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1583, -0.0905,  0.0959,  ...,  0.1526, -0.1224,  0.2000]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 02:30:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with oslo as its capital is known as norway
The country with conakry as its capital is known as guinea
The country with bern as its capital is known as switzerland
The country with baghdad as its capital is known as iraq
The country with stockholm as its capital is known as sweden
The country with santiago as its capital is known as chile
The country with damascus as its capital is known as syria
The country with dhaka as its capital is known as
2024-07-09 02:30:27 root INFO     total operator prediction time: 3888.765142917633 seconds
2024-07-09 02:30:27 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on country - language
2024-07-09 02:30:29 root INFO     building operator country - language
2024-07-09 02:30:29 root INFO     [order_1_approx] starting weight calculation for The country of iran primarily speaks the language of persian
The country of taiwan primarily speaks the language of chinese
The country of chile primarily speaks the language of spanish
The country of peru primarily speaks the language of spanish
The country of cyprus primarily speaks the language of greek
The country of cuba primarily speaks the language of spanish
The country of bangladesh primarily speaks the language of bengali
The country of egypt primarily speaks the language of
2024-07-09 02:30:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 02:34:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 02:38:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0969, -0.6680, -1.1025,  ...,  1.1445,  0.4849, -0.5742],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.0903, -0.6260, -0.9473,  ...,  0.9795,  0.4065, -0.5449],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.8887, -3.2637, -3.1875,  ..., -4.3359, -5.0078, -0.7349],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 5.5428e-03, -2.5085e-02, -3.9291e-04,  ..., -2.1629e-03,
         -1.3351e-03,  7.5645e-03],
        [-2.9736e-03, -9.9869e-03, -7.6056e-05,  ...,  1.5022e-02,
          1.1658e-02,  6.4621e-03],
        [-1.1337e-02,  1.3580e-02, -1.0948e-02,  ..., -1.9547e-02,
          6.6376e-03, -1.2253e-02],
        ...,
        [-1.3199e-02,  1.4740e-02, -1.1421e-02,  ..., -6.0349e-03,
         -1.0147e-03, -6.9733e-03],
        [-1.1955e-02,  7.8125e-03, -2.1744e-04,  ..., -1.2329e-02,
         -1.7128e-03, -1.2650e-02],
        [-3.4790e-03, -1.2169e-03,  1.8707e-02,  ..., -1.3142e-03,
          2.9282e-02, -8.6594e-04]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-6.5565e-07, -6.9141e-04,  6.8760e-04,  ...,  6.4278e-04,
          6.0034e-04, -1.5659e-03],
        [-4.4894e-04,  3.8624e-04,  1.5097e-03,  ...,  1.7319e-03,
          8.3637e-04, -1.7929e-03],
        [-1.7624e-03,  6.3515e-04, -3.5977e-04,  ...,  1.2150e-03,
         -5.6744e-05,  5.4741e-04],
        ...,
        [ 2.2221e-03,  1.7786e-03, -3.1972e-04,  ..., -1.8663e-03,
          1.6432e-03, -4.3035e-05],
        [-8.3327e-05,  8.4400e-04,  8.4639e-04,  ..., -8.7881e-04,
          1.4305e-05, -5.2929e-04],
        [-1.3094e-03,  1.5640e-04, -7.0953e-04,  ...,  9.1410e-04,
         -1.1663e-03,  1.0939e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7324, -4.6406, -1.0977,  ..., -2.9961, -2.6074, -1.0732]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0948, -0.1528,  0.1400,  ...,  0.1992,  0.0124,  0.2537]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 02:38:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of iran primarily speaks the language of persian
The country of taiwan primarily speaks the language of chinese
The country of chile primarily speaks the language of spanish
The country of peru primarily speaks the language of spanish
The country of cyprus primarily speaks the language of greek
The country of cuba primarily speaks the language of spanish
The country of bangladesh primarily speaks the language of bengali
The country of egypt primarily speaks the language of
2024-07-09 02:38:34 root INFO     [order_1_approx] starting weight calculation for The country of cyprus primarily speaks the language of greek
The country of iran primarily speaks the language of persian
The country of taiwan primarily speaks the language of chinese
The country of egypt primarily speaks the language of arabic
The country of bangladesh primarily speaks the language of bengali
The country of peru primarily speaks the language of spanish
The country of chile primarily speaks the language of spanish
The country of cuba primarily speaks the language of
2024-07-09 02:38:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 02:42:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 02:46:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5605, -0.1129, -1.9746,  ..., -0.0889,  0.0405, -0.3193],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.5586, -0.1287, -1.8809,  ..., -0.0775,  0.0244, -0.3442],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.3574, -1.9629, -4.9219,  ..., -1.8672,  2.8203, -0.9888],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0028, -0.0011,  0.0011,  ...,  0.0080, -0.0161, -0.0125],
        [-0.0105,  0.0640,  0.0285,  ...,  0.0212,  0.0073, -0.0173],
        [ 0.0134, -0.0549, -0.0372,  ..., -0.0411,  0.0023,  0.0200],
        ...,
        [-0.0049, -0.0163, -0.0088,  ...,  0.0004,  0.0068,  0.0080],
        [-0.0071,  0.0061,  0.0085,  ...,  0.0215, -0.0045,  0.0018],
        [-0.0096, -0.0026,  0.0123,  ...,  0.0020,  0.0106,  0.0100]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 6.6328e-04,  9.8419e-04,  4.5323e-04,  ..., -3.9554e-04,
          1.9908e-04, -3.8242e-03],
        [ 7.2479e-05, -1.9407e-04,  6.5660e-04,  ...,  2.8038e-04,
          1.6880e-03, -7.7343e-04],
        [-7.8487e-04, -6.2275e-04, -1.3390e-03,  ...,  7.2527e-04,
         -5.8985e-04,  3.3259e-05],
        ...,
        [-3.4690e-04, -2.8248e-03, -1.8682e-03,  ..., -1.2665e-03,
          3.2234e-03,  3.9816e-05],
        [ 1.8954e-04,  1.1597e-03,  1.3056e-03,  ...,  4.9305e-04,
          4.9305e-04, -2.8992e-04],
        [-1.2255e-03, -6.0272e-04,  5.5170e-04,  ..., -2.0065e-03,
          7.0286e-04,  1.3199e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.2773, -3.3750, -3.3633,  ..., -1.4424,  1.2910, -0.6943]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0480, -0.0879,  0.0207,  ..., -0.0437,  0.0432,  0.1946]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 02:46:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of cyprus primarily speaks the language of greek
The country of iran primarily speaks the language of persian
The country of taiwan primarily speaks the language of chinese
The country of egypt primarily speaks the language of arabic
The country of bangladesh primarily speaks the language of bengali
The country of peru primarily speaks the language of spanish
The country of chile primarily speaks the language of spanish
The country of cuba primarily speaks the language of
2024-07-09 02:46:39 root INFO     [order_1_approx] starting weight calculation for The country of cyprus primarily speaks the language of greek
The country of taiwan primarily speaks the language of chinese
The country of chile primarily speaks the language of spanish
The country of bangladesh primarily speaks the language of bengali
The country of egypt primarily speaks the language of arabic
The country of iran primarily speaks the language of persian
The country of cuba primarily speaks the language of spanish
The country of peru primarily speaks the language of
2024-07-09 02:46:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 02:50:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 02:54:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5952, -1.1768, -2.2754,  ..., -0.2803,  0.8359, -0.1650],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.5156, -1.0557, -1.8857,  ..., -0.2246,  0.6982, -0.1652],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.7656, -0.2205, -8.7344,  ..., -1.6973,  3.3125, -1.8965],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.1191e-03, -1.4740e-02, -3.6812e-03,  ..., -3.0403e-03,
         -9.8228e-04,  1.1452e-02],
        [-1.8326e-02,  1.6006e-02,  4.9362e-03,  ...,  1.6891e-02,
         -8.4686e-04, -8.2397e-04],
        [ 1.0948e-02,  2.4109e-03, -3.7567e-02,  ..., -2.2415e-02,
          2.2797e-02,  1.4389e-02],
        ...,
        [-5.4550e-04, -1.1719e-02, -1.0357e-03,  ..., -7.5607e-03,
          1.7166e-05,  7.1297e-03],
        [-8.4991e-03, -6.0768e-03,  1.1473e-03,  ...,  9.9258e-03,
         -7.3318e-03, -3.3779e-03],
        [-2.9392e-03, -4.3297e-03, -3.4790e-03,  ..., -1.4771e-02,
          1.3306e-02,  4.9362e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 3.6263e-04, -2.0790e-03,  1.7309e-03,  ...,  2.0194e-04,
         -1.1358e-03, -8.0967e-04],
        [-3.5810e-04,  3.9649e-04,  1.7679e-04,  ..., -7.6580e-04,
          5.7125e-04, -9.9468e-04],
        [-1.8377e-03,  1.0753e-04, -1.1005e-03,  ..., -1.1368e-03,
         -5.8174e-04,  1.5049e-03],
        ...,
        [ 1.9426e-03,  3.5238e-04, -2.2373e-03,  ..., -5.7030e-04,
          2.2066e-04, -9.0265e-04],
        [ 2.1684e-04,  4.3392e-05, -9.5320e-04,  ..., -1.7223e-03,
          6.4850e-04, -6.8378e-04],
        [-1.1883e-03,  1.8492e-03,  4.0889e-05,  ...,  3.1400e-04,
          9.0742e-04,  1.7014e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.2656, -0.9609, -5.8516,  ..., -2.0938,  2.6855, -0.9360]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0479, -0.0171,  0.1993,  ..., -0.1188, -0.0593, -0.0059]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 02:54:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of cyprus primarily speaks the language of greek
The country of taiwan primarily speaks the language of chinese
The country of chile primarily speaks the language of spanish
The country of bangladesh primarily speaks the language of bengali
The country of egypt primarily speaks the language of arabic
The country of iran primarily speaks the language of persian
The country of cuba primarily speaks the language of spanish
The country of peru primarily speaks the language of
2024-07-09 02:54:45 root INFO     [order_1_approx] starting weight calculation for The country of chile primarily speaks the language of spanish
The country of cuba primarily speaks the language of spanish
The country of bangladesh primarily speaks the language of bengali
The country of peru primarily speaks the language of spanish
The country of taiwan primarily speaks the language of chinese
The country of egypt primarily speaks the language of arabic
The country of cyprus primarily speaks the language of greek
The country of iran primarily speaks the language of
2024-07-09 02:54:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 02:58:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 03:02:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3550, -0.9014, -1.5498,  ..., -0.9712,  0.1594,  0.1550],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.3416, -0.8984, -1.4238,  ..., -0.8843,  0.1342,  0.1302],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.1641, -4.7031, -3.5508,  ..., -6.7344, -1.3398, -1.5459],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0044, -0.0082, -0.0027,  ...,  0.0083, -0.0021, -0.0107],
        [ 0.0032,  0.0050,  0.0132,  ...,  0.0175,  0.0025, -0.0080],
        [-0.0124,  0.0010, -0.0174,  ..., -0.0206,  0.0006,  0.0009],
        ...,
        [ 0.0028,  0.0002, -0.0105,  ..., -0.0274, -0.0077,  0.0036],
        [ 0.0041, -0.0091, -0.0037,  ..., -0.0241, -0.0069, -0.0020],
        [-0.0040, -0.0066,  0.0070,  ..., -0.0010,  0.0049,  0.0052]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 1.7428e-04, -5.3883e-04,  1.4219e-03,  ...,  1.6766e-03,
         -6.4278e-04, -1.1158e-03],
        [ 1.4877e-03, -2.0790e-03,  1.6279e-03,  ...,  7.8869e-04,
         -3.9315e-04, -1.6270e-03],
        [-2.0447e-03, -4.2510e-04,  5.4073e-04,  ..., -4.3845e-04,
          1.6832e-03, -6.2346e-05],
        ...,
        [-2.3413e-04,  1.4477e-03, -6.1655e-04,  ..., -1.4000e-03,
          1.3161e-03,  5.5408e-04],
        [-4.2582e-04,  1.5993e-03,  9.3269e-04,  ..., -6.0368e-04,
         -2.4331e-04,  7.5626e-04],
        [-2.7037e-04,  7.4053e-04, -1.6081e-04,  ...,  1.2074e-03,
          2.0909e-04,  8.4543e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.6523, -5.0586, -3.6035,  ..., -6.5664, -1.1758, -1.4375]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0657, -0.1605,  0.1119,  ...,  0.2435,  0.1381,  0.0330]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 03:02:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of chile primarily speaks the language of spanish
The country of cuba primarily speaks the language of spanish
The country of bangladesh primarily speaks the language of bengali
The country of peru primarily speaks the language of spanish
The country of taiwan primarily speaks the language of chinese
The country of egypt primarily speaks the language of arabic
The country of cyprus primarily speaks the language of greek
The country of iran primarily speaks the language of
2024-07-09 03:02:52 root INFO     [order_1_approx] starting weight calculation for The country of cuba primarily speaks the language of spanish
The country of taiwan primarily speaks the language of chinese
The country of iran primarily speaks the language of persian
The country of egypt primarily speaks the language of arabic
The country of chile primarily speaks the language of spanish
The country of peru primarily speaks the language of spanish
The country of bangladesh primarily speaks the language of bengali
The country of cyprus primarily speaks the language of
2024-07-09 03:02:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 03:06:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 03:10:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4229, -0.4192, -1.6553,  ...,  1.0918, -0.3645, -0.5444],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.3777, -0.3962, -1.4023,  ...,  0.9233, -0.3359, -0.5137],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2051, -5.2891, -0.4033,  ..., -2.0820, -0.3027, -3.0352],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0042, -0.0189,  0.0115,  ...,  0.0060,  0.0063, -0.0075],
        [-0.0011,  0.0156,  0.0033,  ...,  0.0124,  0.0070, -0.0012],
        [ 0.0029,  0.0046, -0.0026,  ..., -0.0035,  0.0172, -0.0110],
        ...,
        [ 0.0041, -0.0031, -0.0041,  ..., -0.0023, -0.0047,  0.0030],
        [ 0.0150, -0.0149, -0.0061,  ..., -0.0062, -0.0141,  0.0162],
        [-0.0065, -0.0143,  0.0068,  ..., -0.0101,  0.0152,  0.0013]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 3.1328e-04, -5.1308e-04,  2.2030e-03,  ..., -3.9268e-04,
         -5.8174e-04, -5.8460e-04],
        [ 1.1754e-04, -2.8706e-04, -1.0705e-04,  ..., -1.6153e-04,
         -3.3402e-04,  2.6464e-05],
        [ 5.8603e-04,  1.0481e-03,  2.2106e-03,  ..., -1.1282e-03,
          1.7834e-03,  3.4809e-04],
        ...,
        [ 5.1689e-04, -1.0595e-03, -2.0962e-03,  ..., -2.8267e-03,
         -8.1658e-05,  1.4715e-03],
        [ 1.1768e-03,  5.9032e-04,  1.4706e-03,  ...,  8.8406e-04,
         -1.3828e-03,  4.3273e-04],
        [-4.2176e-04, -1.3151e-03,  5.7030e-04,  ..., -6.8426e-04,
          1.5535e-03,  1.0223e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3164, -5.3438, -0.3538,  ..., -1.8926,  0.5190, -1.9082]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0959, -0.0100,  0.1790,  ...,  0.1973,  0.0019,  0.3420]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 03:10:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of cuba primarily speaks the language of spanish
The country of taiwan primarily speaks the language of chinese
The country of iran primarily speaks the language of persian
The country of egypt primarily speaks the language of arabic
The country of chile primarily speaks the language of spanish
The country of peru primarily speaks the language of spanish
The country of bangladesh primarily speaks the language of bengali
The country of cyprus primarily speaks the language of
2024-07-09 03:10:59 root INFO     [order_1_approx] starting weight calculation for The country of peru primarily speaks the language of spanish
The country of egypt primarily speaks the language of arabic
The country of chile primarily speaks the language of spanish
The country of cuba primarily speaks the language of spanish
The country of iran primarily speaks the language of persian
The country of cyprus primarily speaks the language of greek
The country of taiwan primarily speaks the language of chinese
The country of bangladesh primarily speaks the language of
2024-07-09 03:10:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 03:15:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 03:19:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3943,  0.0225, -0.4351,  ...,  0.8940,  0.7134, -0.5972],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.3665,  0.0036, -0.3923,  ...,  0.7896,  0.6260, -0.5830],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9761, -1.1787, -6.8828,  ..., -1.0156,  1.8965,  0.0820],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0063, -0.0161,  0.0018,  ...,  0.0008, -0.0016, -0.0089],
        [-0.0039, -0.0038,  0.0107,  ...,  0.0223,  0.0149,  0.0026],
        [-0.0094,  0.0011, -0.0063,  ..., -0.0056,  0.0022,  0.0093],
        ...,
        [-0.0020, -0.0118, -0.0010,  ..., -0.0045,  0.0032, -0.0088],
        [-0.0113,  0.0031, -0.0016,  ..., -0.0010, -0.0093,  0.0017],
        [-0.0074, -0.0077,  0.0026,  ..., -0.0200, -0.0028,  0.0069]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0006, -0.0019,  0.0007,  ...,  0.0008, -0.0001, -0.0022],
        [-0.0002,  0.0006,  0.0009,  ...,  0.0013, -0.0005, -0.0013],
        [ 0.0004,  0.0003, -0.0009,  ..., -0.0004,  0.0005,  0.0016],
        ...,
        [ 0.0010, -0.0015,  0.0015,  ..., -0.0006,  0.0027, -0.0008],
        [-0.0012,  0.0008,  0.0002,  ...,  0.0002, -0.0004,  0.0005],
        [ 0.0002,  0.0006, -0.0009,  ...,  0.0005, -0.0008,  0.0011]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9312, -1.6709, -6.6836,  ..., -0.0703,  1.5186,  0.8384]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0299, -0.0520,  0.2585,  ..., -0.0891,  0.1986,  0.1365]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 03:19:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of peru primarily speaks the language of spanish
The country of egypt primarily speaks the language of arabic
The country of chile primarily speaks the language of spanish
The country of cuba primarily speaks the language of spanish
The country of iran primarily speaks the language of persian
The country of cyprus primarily speaks the language of greek
The country of taiwan primarily speaks the language of chinese
The country of bangladesh primarily speaks the language of
2024-07-09 03:19:04 root INFO     [order_1_approx] starting weight calculation for The country of taiwan primarily speaks the language of chinese
The country of bangladesh primarily speaks the language of bengali
The country of egypt primarily speaks the language of arabic
The country of iran primarily speaks the language of persian
The country of peru primarily speaks the language of spanish
The country of cuba primarily speaks the language of spanish
The country of cyprus primarily speaks the language of greek
The country of chile primarily speaks the language of
2024-07-09 03:19:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 03:23:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 03:27:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1218, -0.1230, -0.9712,  ..., -0.1550,  1.7852, -0.5728],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1171, -0.1342, -0.8804,  ..., -0.1344,  1.6357, -0.5713],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8613, -0.6875, -4.4609,  ..., -2.1406,  1.5254, -3.0703],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.4114e-04, -1.1978e-03,  1.5736e-03,  ...,  3.3646e-03,
          1.1719e-02, -2.4910e-03],
        [-1.1070e-02,  1.5068e-02,  1.0635e-02,  ...,  1.3084e-02,
          4.9858e-03, -1.5854e-02],
        [-1.0223e-03, -9.8877e-03, -6.4240e-03,  ..., -1.5888e-03,
          5.7373e-03,  1.5106e-02],
        ...,
        [-6.7406e-03, -1.1044e-03,  9.3536e-03,  ...,  9.0714e-03,
          5.5599e-04, -4.1008e-04],
        [-2.6379e-03,  1.9836e-04,  4.7035e-03,  ..., -8.1635e-03,
          1.0834e-03, -2.6302e-03],
        [-1.9638e-02,  2.6703e-05,  2.6016e-03,  ...,  8.7738e-03,
          6.3858e-03,  1.9951e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-7.8773e-04, -1.9855e-03,  2.6059e-04,  ...,  1.4791e-03,
          1.7786e-04, -2.6932e-03],
        [-8.7738e-04,  2.7714e-03,  1.6699e-03,  ...,  2.7514e-04,
          5.3883e-04, -1.3113e-03],
        [ 6.9332e-04, -1.0490e-03, -5.3549e-04,  ..., -1.0357e-03,
          1.1158e-03,  2.4929e-03],
        ...,
        [ 7.0047e-04, -7.6771e-05, -7.1144e-04,  ..., -1.0414e-03,
         -2.6226e-06, -8.4782e-04],
        [-1.9312e-03,  2.6836e-03,  1.8301e-03,  ..., -4.6349e-04,
         -8.3160e-04,  2.2626e-04],
        [ 2.4247e-04, -1.0309e-03,  2.1708e-04,  ..., -1.4043e-04,
          3.7718e-04,  2.5558e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5908, -1.2305, -4.2656,  ..., -3.1562,  1.3037, -1.9102]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.3479,  0.0945,  0.0591,  ...,  0.1517,  0.0812,  0.3738]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 03:27:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of taiwan primarily speaks the language of chinese
The country of bangladesh primarily speaks the language of bengali
The country of egypt primarily speaks the language of arabic
The country of iran primarily speaks the language of persian
The country of peru primarily speaks the language of spanish
The country of cuba primarily speaks the language of spanish
The country of cyprus primarily speaks the language of greek
The country of chile primarily speaks the language of
2024-07-09 03:27:10 root INFO     [order_1_approx] starting weight calculation for The country of cuba primarily speaks the language of spanish
The country of bangladesh primarily speaks the language of bengali
The country of chile primarily speaks the language of spanish
The country of iran primarily speaks the language of persian
The country of peru primarily speaks the language of spanish
The country of cyprus primarily speaks the language of greek
The country of egypt primarily speaks the language of arabic
The country of taiwan primarily speaks the language of
2024-07-09 03:27:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 03:31:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 03:35:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8228,  0.0786, -1.0430,  ..., -0.1267,  0.3770, -0.2673],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.7461,  0.0586, -0.9087,  ..., -0.1031,  0.3201, -0.2673],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.9805,  0.0371, -4.7695,  ..., -3.7930,  2.3809, -0.9883],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0052, -0.0034,  0.0052,  ..., -0.0002,  0.0011, -0.0005],
        [-0.0116,  0.0018,  0.0046,  ...,  0.0162, -0.0045, -0.0041],
        [-0.0036,  0.0111, -0.0041,  ...,  0.0028,  0.0136, -0.0011],
        ...,
        [-0.0056,  0.0004,  0.0019,  ..., -0.0034,  0.0069,  0.0040],
        [-0.0045,  0.0039,  0.0011,  ..., -0.0072, -0.0043,  0.0116],
        [ 0.0029, -0.0054, -0.0018,  ..., -0.0022,  0.0068, -0.0010]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 4.8041e-05, -1.3638e-03,  4.7135e-04,  ...,  1.4277e-03,
         -5.3120e-04,  7.3314e-05],
        [ 5.9319e-04,  6.3419e-04, -1.6332e-04,  ..., -7.9155e-05,
         -5.3120e-04, -2.4796e-03],
        [-1.0042e-03, -7.7915e-04,  7.0810e-04,  ..., -7.1526e-04,
          5.2309e-04, -6.2037e-04],
        ...,
        [-6.9666e-04,  1.2815e-05,  8.1062e-06,  ..., -5.1260e-05,
          2.2831e-03,  4.3440e-04],
        [-5.6219e-04,  1.7662e-03,  5.7411e-04,  ...,  5.2261e-04,
          1.5140e-04, -4.6968e-04],
        [-2.1725e-03, -6.1178e-04,  9.8610e-04,  ...,  2.1482e-04,
          1.8525e-04, -1.1663e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.6445, -1.0967, -4.5547,  ..., -4.2227,  1.8906, -0.2568]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0509, -0.0928,  0.0784,  ..., -0.0522, -0.0323, -0.0393]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 03:35:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of cuba primarily speaks the language of spanish
The country of bangladesh primarily speaks the language of bengali
The country of chile primarily speaks the language of spanish
The country of iran primarily speaks the language of persian
The country of peru primarily speaks the language of spanish
The country of cyprus primarily speaks the language of greek
The country of egypt primarily speaks the language of arabic
The country of taiwan primarily speaks the language of
2024-07-09 03:35:16 root INFO     total operator prediction time: 3887.6556508541107 seconds
2024-07-09 03:35:16 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+able_reg
2024-07-09 03:35:19 root INFO     building operator verb+able_reg
2024-07-09 03:35:19 root INFO     [order_1_approx] starting weight calculation for If you can adjust something, that thing is adjustable
If you can deliver something, that thing is deliverable
If you can expand something, that thing is expandable
If you can avoid something, that thing is avoidable
If you can prefer something, that thing is preferable
If you can predict something, that thing is predictable
If you can define something, that thing is definable
If you can renew something, that thing is
2024-07-09 03:35:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 03:39:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 03:43:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8086, -0.0676,  0.3074,  ..., -0.1526,  0.2363,  0.5908],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.7744, -0.0864,  0.2693,  ..., -0.1394,  0.2036,  0.5552],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.1699, -0.1082, -5.8125,  ...,  0.0107,  1.7500,  1.8525],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0137, -0.0215, -0.0001,  ...,  0.0013, -0.0030,  0.0003],
        [-0.0074, -0.0066, -0.0146,  ...,  0.0009, -0.0005,  0.0022],
        [ 0.0030,  0.0140, -0.0127,  ..., -0.0049,  0.0186,  0.0007],
        ...,
        [-0.0138,  0.0007, -0.0065,  ..., -0.0021,  0.0008, -0.0034],
        [ 0.0086, -0.0307,  0.0100,  ..., -0.0116, -0.0031,  0.0122],
        [ 0.0007, -0.0007,  0.0080,  ..., -0.0157, -0.0084, -0.0180]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.9341e-03, -2.5597e-03,  4.5967e-04,  ..., -1.4424e-04,
          2.0256e-03,  6.4468e-04],
        [ 4.3988e-05, -3.5167e-05, -9.3412e-04,  ...,  7.6389e-04,
          1.5545e-03,  2.4281e-03],
        [-2.3479e-03,  1.8921e-03, -2.4757e-03,  ..., -4.9353e-04,
          1.2293e-03,  2.9774e-03],
        ...,
        [-6.1846e-04, -2.3441e-03,  2.3270e-03,  ..., -3.6311e-04,
         -5.1928e-04, -2.2621e-03],
        [ 1.1301e-03,  3.7193e-04, -1.8864e-03,  ...,  1.3380e-03,
          3.1781e-04,  9.4604e-04],
        [ 3.0327e-04,  1.0977e-03,  5.0974e-04,  ..., -6.9523e-04,
         -1.5984e-03, -2.6283e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4727, -0.1895, -6.1406,  ...,  0.3147,  1.0576,  1.4941]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0170,  0.1876,  0.1165,  ..., -0.2690,  0.1095,  0.1177]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 03:43:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can adjust something, that thing is adjustable
If you can deliver something, that thing is deliverable
If you can expand something, that thing is expandable
If you can avoid something, that thing is avoidable
If you can prefer something, that thing is preferable
If you can predict something, that thing is predictable
If you can define something, that thing is definable
If you can renew something, that thing is
2024-07-09 03:43:25 root INFO     [order_1_approx] starting weight calculation for If you can renew something, that thing is renewable
If you can define something, that thing is definable
If you can expand something, that thing is expandable
If you can deliver something, that thing is deliverable
If you can prefer something, that thing is preferable
If you can predict something, that thing is predictable
If you can adjust something, that thing is adjustable
If you can avoid something, that thing is
2024-07-09 03:43:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 03:47:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 03:51:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5591,  0.1418, -0.9331,  ..., -0.6431, -0.3267, -0.0795],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.5703,  0.1274, -0.9111,  ..., -0.6196, -0.3452, -0.1074],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5859,  1.5801, -4.6367,  ...,  1.3594,  7.4453,  0.9673],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0138, -0.0073,  0.0083,  ...,  0.0082, -0.0127,  0.0015],
        [-0.0077, -0.0074, -0.0019,  ..., -0.0019, -0.0197, -0.0006],
        [ 0.0015,  0.0202,  0.0048,  ...,  0.0032, -0.0065,  0.0085],
        ...,
        [-0.0126,  0.0039, -0.0033,  ..., -0.0167, -0.0087, -0.0077],
        [-0.0028, -0.0094, -0.0062,  ..., -0.0240, -0.0141, -0.0013],
        [-0.0007,  0.0010,  0.0076,  ...,  0.0042, -0.0037, -0.0091]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-3.1738e-03, -1.1480e-04, -7.9441e-04,  ..., -1.8463e-03,
          1.4048e-03,  1.1063e-03],
        [-9.2649e-04, -2.1477e-03, -6.8521e-04,  ...,  2.1458e-04,
         -2.1935e-05,  1.9073e-06],
        [-7.5150e-04,  2.0733e-03, -1.1196e-03,  ...,  3.8719e-04,
          4.7207e-04,  4.9591e-05],
        ...,
        [-1.6174e-03,  1.0223e-03, -4.1533e-04,  ..., -7.0667e-04,
          1.6460e-03, -2.1610e-03],
        [-7.8917e-04,  4.2677e-05, -1.2531e-03,  ..., -1.6546e-03,
         -6.7902e-04, -1.0252e-03],
        [ 5.1880e-04, -1.0109e-03,  2.1858e-03,  ...,  2.3499e-03,
         -1.5879e-04, -4.2915e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2109,  1.5576, -4.7969,  ...,  1.5596,  7.5781,  1.0391]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1456,  0.2349,  0.0049,  ..., -0.1132,  0.1807, -0.2693]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 03:51:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can renew something, that thing is renewable
If you can define something, that thing is definable
If you can expand something, that thing is expandable
If you can deliver something, that thing is deliverable
If you can prefer something, that thing is preferable
If you can predict something, that thing is predictable
If you can adjust something, that thing is adjustable
If you can avoid something, that thing is
2024-07-09 03:51:32 root INFO     [order_1_approx] starting weight calculation for If you can predict something, that thing is predictable
If you can adjust something, that thing is adjustable
If you can expand something, that thing is expandable
If you can define something, that thing is definable
If you can avoid something, that thing is avoidable
If you can renew something, that thing is renewable
If you can deliver something, that thing is deliverable
If you can prefer something, that thing is
2024-07-09 03:51:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 03:55:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 03:59:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1082, -0.2112, -0.2632,  ..., -0.1906,  1.2705,  0.2323],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1238, -0.2532, -0.2842,  ..., -0.1957,  1.3203,  0.2266],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2646,  2.5273, -0.5693,  ...,  1.6797,  2.3340,  1.5664],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0073, -0.0108,  0.0125,  ...,  0.0183, -0.0125, -0.0045],
        [-0.0145,  0.0038, -0.0004,  ..., -0.0153, -0.0180, -0.0043],
        [ 0.0111,  0.0041, -0.0059,  ...,  0.0018,  0.0016,  0.0082],
        ...,
        [ 0.0075, -0.0033, -0.0042,  ...,  0.0007, -0.0009, -0.0070],
        [ 0.0140,  0.0085, -0.0090,  ..., -0.0258, -0.0219,  0.0083],
        [-0.0121,  0.0156,  0.0028,  ...,  0.0010,  0.0154, -0.0111]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.7962e-03, -1.4038e-03,  5.5027e-04,  ...,  1.0071e-03,
          1.0359e-04,  5.5599e-04],
        [-2.5444e-03,  1.1797e-03,  4.9686e-04,  ...,  7.5626e-04,
          9.6679e-05,  2.4738e-03],
        [ 3.0661e-04, -9.0265e-04, -2.1744e-03,  ..., -1.4648e-03,
          3.8028e-04,  7.4482e-04],
        ...,
        [ 2.8915e-03, -9.0981e-04, -1.0948e-03,  ..., -1.8997e-03,
          1.6785e-03, -1.6165e-03],
        [ 4.3631e-04,  2.3346e-03,  4.3869e-05,  ..., -8.6784e-05,
         -2.5520e-03, -1.7786e-03],
        [-1.1806e-03,  9.5546e-05, -7.5817e-04,  ...,  1.4620e-03,
          1.2579e-03, -1.4305e-06]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3616,  2.4355, -0.3813,  ...,  1.5107,  1.9463,  2.2305]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2211,  0.2144, -0.1678,  ..., -0.2275, -0.0145,  0.0307]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 03:59:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can predict something, that thing is predictable
If you can adjust something, that thing is adjustable
If you can expand something, that thing is expandable
If you can define something, that thing is definable
If you can avoid something, that thing is avoidable
If you can renew something, that thing is renewable
If you can deliver something, that thing is deliverable
If you can prefer something, that thing is
2024-07-09 03:59:38 root INFO     [order_1_approx] starting weight calculation for If you can prefer something, that thing is preferable
If you can avoid something, that thing is avoidable
If you can expand something, that thing is expandable
If you can predict something, that thing is predictable
If you can deliver something, that thing is deliverable
If you can renew something, that thing is renewable
If you can adjust something, that thing is adjustable
If you can define something, that thing is
2024-07-09 03:59:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 04:03:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 04:07:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3826,  0.8203, -0.8281,  ..., -0.4609,  0.0337,  0.0227],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3872,  0.8438, -0.8242,  ..., -0.4514,  0.0135, -0.0019],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2629,  1.3877, -6.1680,  ..., -5.7734,  5.0156, -1.8047],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.4008e-02, -1.0948e-02,  3.7403e-03,  ...,  1.1841e-02,
         -1.3962e-02,  4.6692e-03],
        [-1.3588e-02,  3.2902e-03,  4.1199e-04,  ..., -3.2349e-03,
          6.9427e-04, -1.0803e-02],
        [ 1.4015e-02,  2.8229e-03,  2.9945e-03,  ...,  4.4250e-03,
         -7.6294e-04,  4.1161e-03],
        ...,
        [-7.2556e-03, -1.4313e-02, -8.2855e-03,  ..., -7.2098e-03,
         -3.7785e-03, -1.0567e-02],
        [-4.8523e-03,  6.2218e-03, -9.5367e-05,  ..., -2.7802e-02,
         -3.2978e-03,  8.9188e-03],
        [-9.8724e-03,  6.0577e-03,  7.6485e-04,  ..., -4.0436e-03,
          1.4427e-02,  2.8400e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.7939e-03, -2.9254e-04, -4.0674e-04,  ...,  1.3037e-03,
         -9.7036e-04,  1.5240e-03],
        [-9.3460e-04, -1.5621e-03,  1.9445e-03,  ...,  2.9449e-03,
         -6.1989e-04, -1.2970e-03],
        [-2.3603e-05,  9.2649e-04, -1.8959e-03,  ...,  1.3790e-03,
          1.8001e-04,  2.9087e-04],
        ...,
        [ 3.6716e-04,  3.7432e-04, -6.4850e-05,  ..., -2.8324e-03,
          1.6108e-03, -2.2469e-03],
        [-2.2163e-03,  1.7843e-03, -1.9665e-03,  ..., -1.2703e-03,
         -4.3130e-04,  4.3058e-04],
        [-2.4147e-03,  1.7195e-03, -4.9400e-04,  ...,  2.8419e-03,
         -8.5163e-04, -1.1120e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2175,  1.6719, -6.4531,  ..., -5.1289,  4.5625, -2.0645]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.3594, -0.1072,  0.1144,  ..., -0.0692, -0.1224, -0.1240]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 04:07:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can prefer something, that thing is preferable
If you can avoid something, that thing is avoidable
If you can expand something, that thing is expandable
If you can predict something, that thing is predictable
If you can deliver something, that thing is deliverable
If you can renew something, that thing is renewable
If you can adjust something, that thing is adjustable
If you can define something, that thing is
2024-07-09 04:07:45 root INFO     [order_1_approx] starting weight calculation for If you can deliver something, that thing is deliverable
If you can predict something, that thing is predictable
If you can avoid something, that thing is avoidable
If you can define something, that thing is definable
If you can expand something, that thing is expandable
If you can renew something, that thing is renewable
If you can prefer something, that thing is preferable
If you can adjust something, that thing is
2024-07-09 04:07:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 04:11:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 04:15:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6895,  0.5249,  0.0210,  ..., -0.1670,  0.4353, -0.3936],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.6880,  0.5210,  0.0061,  ..., -0.1599,  0.4102, -0.4307],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1050,  3.7266, -5.4766,  ..., -1.9229,  3.5820,  2.3906],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0111, -0.0095, -0.0006,  ...,  0.0124, -0.0010,  0.0121],
        [-0.0141,  0.0140, -0.0045,  ...,  0.0030, -0.0172, -0.0148],
        [ 0.0024,  0.0160, -0.0119,  ...,  0.0223,  0.0095, -0.0109],
        ...,
        [-0.0233, -0.0023, -0.0045,  ..., -0.0116,  0.0114,  0.0103],
        [-0.0068, -0.0040, -0.0068,  ..., -0.0313, -0.0078,  0.0147],
        [ 0.0019,  0.0050,  0.0011,  ...,  0.0058, -0.0076, -0.0142]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-3.1223e-03, -1.2207e-04, -1.0319e-03,  ..., -2.7180e-03,
          3.0375e-04,  3.3331e-04],
        [-1.3342e-03, -3.9101e-05,  8.5163e-04,  ...,  1.2941e-03,
         -2.8324e-04, -1.0252e-03],
        [ 1.1396e-04,  1.4992e-03, -2.3746e-03,  ...,  4.4441e-04,
         -7.4291e-04,  1.9417e-03],
        ...,
        [ 1.2264e-03, -3.0160e-04,  8.4400e-04,  ..., -1.7715e-04,
          1.4553e-03, -1.1406e-03],
        [-1.1044e-03,  1.1702e-03, -2.5177e-03,  ..., -1.2407e-03,
         -1.2226e-03, -2.1195e-04],
        [ 2.9793e-03, -6.5947e-04,  2.5749e-04,  ...,  1.7471e-03,
         -1.7881e-04, -9.0599e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2148,  3.2910, -6.2422,  ..., -0.9292,  2.7656,  1.8936]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0767, -0.1407,  0.0185,  ..., -0.1179,  0.0939,  0.0164]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 04:15:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can deliver something, that thing is deliverable
If you can predict something, that thing is predictable
If you can avoid something, that thing is avoidable
If you can define something, that thing is definable
If you can expand something, that thing is expandable
If you can renew something, that thing is renewable
If you can prefer something, that thing is preferable
If you can adjust something, that thing is
2024-07-09 04:15:52 root INFO     [order_1_approx] starting weight calculation for If you can avoid something, that thing is avoidable
If you can deliver something, that thing is deliverable
If you can adjust something, that thing is adjustable
If you can prefer something, that thing is preferable
If you can expand something, that thing is expandable
If you can renew something, that thing is renewable
If you can define something, that thing is definable
If you can predict something, that thing is
2024-07-09 04:15:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 04:19:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 04:23:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3784,  0.0347, -0.0153,  ..., -0.5391,  0.6309, -0.0649],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3467,  0.0142, -0.0264,  ..., -0.4778,  0.5576, -0.0869],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2393,  0.4319, -3.3477,  ..., -2.5938,  5.1055,  1.2959],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0140, -0.0099,  0.0052,  ...,  0.0019, -0.0126, -0.0030],
        [-0.0243, -0.0115,  0.0022,  ..., -0.0179, -0.0020, -0.0022],
        [ 0.0009,  0.0098, -0.0040,  ...,  0.0033,  0.0066, -0.0012],
        ...,
        [-0.0098, -0.0123, -0.0145,  ..., -0.0033, -0.0043, -0.0134],
        [-0.0050, -0.0042,  0.0047,  ..., -0.0249, -0.0092,  0.0059],
        [-0.0057,  0.0154,  0.0022,  ..., -0.0134,  0.0102, -0.0208]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.9455e-03, -7.0047e-04, -3.3665e-04,  ...,  1.2922e-04,
          2.3198e-04,  1.1530e-03],
        [-6.8545e-05,  1.1051e-04,  1.7929e-03,  ..., -7.8869e-04,
          1.1921e-03, -1.7405e-05],
        [-1.3323e-03,  1.2264e-03, -1.3876e-03,  ...,  2.2578e-04,
         -1.2951e-03,  1.5507e-03],
        ...,
        [ 1.4601e-03, -7.7915e-04, -6.9571e-04,  ..., -4.4680e-04,
          6.0654e-04, -2.7447e-03],
        [-1.5774e-03, -5.7578e-05,  5.0449e-04,  ..., -1.5450e-03,
          1.0663e-04,  5.4550e-04],
        [ 8.0681e-04,  1.3580e-03,  4.0770e-05,  ...,  1.4868e-03,
         -2.7061e-05,  7.8201e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0693, -0.0422, -3.3633,  ..., -2.6758,  5.1250,  1.0273]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0721, -0.0306,  0.1366,  ..., -0.0674,  0.1339,  0.1381]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 04:23:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can avoid something, that thing is avoidable
If you can deliver something, that thing is deliverable
If you can adjust something, that thing is adjustable
If you can prefer something, that thing is preferable
If you can expand something, that thing is expandable
If you can renew something, that thing is renewable
If you can define something, that thing is definable
If you can predict something, that thing is
2024-07-09 04:23:57 root INFO     [order_1_approx] starting weight calculation for If you can deliver something, that thing is deliverable
If you can predict something, that thing is predictable
If you can renew something, that thing is renewable
If you can adjust something, that thing is adjustable
If you can define something, that thing is definable
If you can prefer something, that thing is preferable
If you can avoid something, that thing is avoidable
If you can expand something, that thing is
2024-07-09 04:23:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 04:28:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 04:32:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0952,  0.3486,  0.0386,  ...,  0.1205,  0.8267, -0.3232],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1091,  0.3643,  0.0257,  ...,  0.1255,  0.8550, -0.3811],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.4297, -0.1956, -3.8984,  ..., -1.8555,  3.0430, -0.3955],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.7384e-04, -9.0027e-03, -2.1477e-03,  ...,  1.0681e-04,
         -2.6550e-03,  2.7180e-03],
        [-9.6512e-03,  9.9182e-05,  1.0475e-02,  ..., -6.6853e-04,
         -1.3199e-02, -1.6861e-03],
        [-4.1771e-03,  1.3000e-02, -5.0392e-03,  ...,  6.9580e-03,
          1.3351e-05, -4.0131e-03],
        ...,
        [-2.2507e-03, -6.7177e-03, -6.3896e-03,  ..., -4.1962e-03,
         -2.8572e-03, -6.2332e-03],
        [ 5.7602e-03,  7.9803e-03, -6.2103e-03,  ..., -1.5747e-02,
          4.7340e-03,  1.6785e-02],
        [ 1.5808e-02, -5.6229e-03,  5.6229e-03,  ..., -4.2496e-03,
          1.2215e-02, -2.5826e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.8381e-03, -6.3705e-04, -4.5872e-04,  ..., -1.7586e-03,
          2.4662e-03,  9.5129e-04],
        [-1.1435e-03, -2.3389e-04, -1.1396e-04,  ...,  2.4796e-04,
         -2.9087e-04,  1.1024e-03],
        [ 6.6757e-06,  4.9686e-04, -1.3123e-03,  ...,  1.5564e-03,
          2.4462e-04,  7.7295e-04],
        ...,
        [ 1.8501e-03, -7.9155e-04, -5.1975e-05,  ..., -1.0700e-03,
          2.6054e-03, -2.9507e-03],
        [ 1.1749e-03,  1.5144e-03, -1.8835e-03,  ..., -1.8921e-03,
          5.7411e-04,  2.1420e-03],
        [ 8.5163e-04, -1.7910e-03,  1.1463e-03,  ...,  1.7893e-04,
         -3.1853e-04, -7.1335e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.5879, -0.1091, -4.4766,  ..., -1.3652,  2.5488, -0.4727]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2018,  0.2710, -0.0598,  ..., -0.1738, -0.0331, -0.0699]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 04:32:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can deliver something, that thing is deliverable
If you can predict something, that thing is predictable
If you can renew something, that thing is renewable
If you can adjust something, that thing is adjustable
If you can define something, that thing is definable
If you can prefer something, that thing is preferable
If you can avoid something, that thing is avoidable
If you can expand something, that thing is
2024-07-09 04:32:03 root INFO     [order_1_approx] starting weight calculation for If you can define something, that thing is definable
If you can prefer something, that thing is preferable
If you can renew something, that thing is renewable
If you can avoid something, that thing is avoidable
If you can predict something, that thing is predictable
If you can expand something, that thing is expandable
If you can adjust something, that thing is adjustable
If you can deliver something, that thing is
2024-07-09 04:32:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 04:36:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 04:40:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3970,  0.3508,  0.0630,  ..., -0.1492, -0.1685, -0.4878],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3977,  0.3457,  0.0465,  ..., -0.1454, -0.1915, -0.5352],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.2080,  0.6816, -4.9766,  ..., -3.8867,  3.3848,  3.7129],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0147, -0.0101, -0.0030,  ...,  0.0078, -0.0033, -0.0121],
        [-0.0046,  0.0046,  0.0007,  ..., -0.0117, -0.0080,  0.0044],
        [ 0.0045,  0.0062, -0.0071,  ..., -0.0069,  0.0023, -0.0066],
        ...,
        [-0.0124,  0.0015, -0.0081,  ...,  0.0056, -0.0014,  0.0051],
        [-0.0093,  0.0096, -0.0092,  ..., -0.0012, -0.0117,  0.0042],
        [ 0.0031, -0.0090,  0.0030,  ..., -0.0124,  0.0096, -0.0154]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.5330e-03, -1.6489e-03, -5.7220e-05,  ...,  7.1573e-04,
         -7.7486e-05, -1.3340e-04],
        [ 2.1210e-03,  1.3790e-03, -6.7902e-04,  ...,  1.3506e-04,
         -2.5034e-04,  3.0684e-04],
        [-1.1387e-03, -3.3784e-04, -3.0041e-03,  ...,  8.7643e-04,
          5.2691e-05,  1.7738e-04],
        ...,
        [-8.3637e-04, -3.2449e-04,  7.0333e-04,  ...,  1.5736e-03,
          4.1628e-04, -9.8944e-05],
        [ 1.2875e-05, -2.5043e-03, -1.2589e-03,  ..., -5.8794e-04,
         -1.5001e-03,  3.9935e-04],
        [-8.1539e-04, -1.4124e-03, -3.4809e-04,  ...,  1.6308e-03,
         -9.8419e-04, -2.3003e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2881,  1.1436, -4.5195,  ..., -2.8477,  3.0449,  3.1875]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2367, -0.0323,  0.1033,  ..., -0.1758,  0.0551, -0.2683]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 04:40:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can define something, that thing is definable
If you can prefer something, that thing is preferable
If you can renew something, that thing is renewable
If you can avoid something, that thing is avoidable
If you can predict something, that thing is predictable
If you can expand something, that thing is expandable
If you can adjust something, that thing is adjustable
If you can deliver something, that thing is
2024-07-09 04:40:06 root INFO     total operator prediction time: 3887.3758578300476 seconds
2024-07-09 04:40:06 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - member
2024-07-09 04:40:08 root INFO     building operator meronyms - member
2024-07-09 04:40:09 root INFO     [order_1_approx] starting weight calculation for A wolf is a member of a pack
A photo is a member of a album
A person is a member of a society
A singer is a member of a choir
A citizen is a member of a citizenry
A student is a member of a class
A senator is a member of a senate
A lion is a member of a
2024-07-09 04:40:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 04:44:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 04:48:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1123, -1.1553,  0.2002,  ...,  0.5957,  0.1902,  0.2068],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1057, -1.0957,  0.1636,  ...,  0.5254,  0.1531,  0.1696],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.7461,  1.9287,  0.7100,  ...,  1.2783, -3.2520,  2.9414],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0026, -0.0198, -0.0065,  ..., -0.0026, -0.0007, -0.0038],
        [-0.0145, -0.0063, -0.0058,  ...,  0.0131, -0.0005, -0.0050],
        [ 0.0265, -0.0078,  0.0164,  ..., -0.0081,  0.0029,  0.0158],
        ...,
        [-0.0027, -0.0094, -0.0063,  ...,  0.0083, -0.0017,  0.0003],
        [ 0.0239, -0.0141,  0.0080,  ..., -0.0128, -0.0206,  0.0064],
        [ 0.0020, -0.0047,  0.0095,  ...,  0.0012, -0.0021,  0.0022]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.0071e-03, -1.8940e-03, -9.9087e-04,  ...,  1.0118e-03,
         -4.4775e-04, -5.5122e-04],
        [ 2.1954e-03, -2.0599e-03,  5.8556e-04,  ...,  1.1396e-03,
          3.0351e-04, -6.8569e-04],
        [-1.1292e-03, -4.4274e-04,  1.5392e-03,  ..., -8.1253e-04,
         -1.0910e-03,  2.8725e-03],
        ...,
        [ 2.1706e-03,  4.5180e-05,  1.1234e-03,  ..., -1.4448e-03,
         -5.5671e-05, -1.3618e-03],
        [ 2.7370e-04,  3.3951e-04,  7.7248e-04,  ..., -5.1308e-04,
          2.7180e-04,  1.4677e-03],
        [-1.8549e-03,  5.9009e-05,  1.0157e-03,  ...,  7.1907e-04,
         -2.8973e-03, -7.0000e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.9629,  1.8818,  1.0293,  ...,  0.9316, -2.5039,  3.4473]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.4006, -0.4031,  0.0341,  ..., -0.1703, -0.0107,  0.1078]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 04:48:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A wolf is a member of a pack
A photo is a member of a album
A person is a member of a society
A singer is a member of a choir
A citizen is a member of a citizenry
A student is a member of a class
A senator is a member of a senate
A lion is a member of a
2024-07-09 04:48:16 root INFO     [order_1_approx] starting weight calculation for A citizen is a member of a citizenry
A singer is a member of a choir
A person is a member of a society
A photo is a member of a album
A wolf is a member of a pack
A student is a member of a class
A lion is a member of a pride
A senator is a member of a
2024-07-09 04:48:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 04:52:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 04:56:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7583, -0.2554,  0.0945,  ...,  1.0059,  0.1553, -0.6211],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.7275, -0.2715,  0.0742,  ...,  0.9331,  0.1274, -0.6372],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([2.2109, 3.8984, 0.3135,  ..., 2.0156, 0.0132, 2.2930], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0038, -0.0011, -0.0025,  ..., -0.0042, -0.0046, -0.0084],
        [-0.0231, -0.0076, -0.0044,  ...,  0.0220, -0.0245,  0.0308],
        [ 0.0086,  0.0075, -0.0196,  ..., -0.0078,  0.0182, -0.0027],
        ...,
        [-0.0125, -0.0139,  0.0001,  ..., -0.0031, -0.0052,  0.0037],
        [ 0.0106,  0.0048,  0.0121,  ...,  0.0062, -0.0038, -0.0025],
        [-0.0044, -0.0010,  0.0070,  ..., -0.0038,  0.0144, -0.0017]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.5278e-03,  7.5769e-04,  1.9779e-03,  ..., -2.1887e-04,
         -2.0561e-03, -1.2980e-03],
        [ 4.4346e-05,  1.4448e-04,  9.5367e-06,  ...,  1.3733e-03,
          7.1287e-04, -3.0112e-04],
        [-6.3181e-05,  5.8937e-04, -1.3008e-03,  ..., -7.1049e-04,
         -3.3355e-04,  1.9073e-03],
        ...,
        [ 1.6718e-03,  3.7622e-04,  3.9268e-04,  ..., -9.4652e-04,
          5.1737e-04, -1.7414e-03],
        [-8.7261e-05, -7.1383e-04,  1.4572e-03,  ..., -1.3828e-04,
         -5.8413e-04,  6.0141e-05],
        [-1.2512e-03, -6.6662e-04,  7.0953e-04,  ..., -7.2098e-04,
         -6.4802e-04, -1.2732e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0000,  3.7754, -0.2153,  ...,  2.0723, -0.2146,  1.8496]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1396,  0.0530,  0.2189,  ...,  0.0043,  0.0129, -0.1858]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 04:56:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A citizen is a member of a citizenry
A singer is a member of a choir
A person is a member of a society
A photo is a member of a album
A wolf is a member of a pack
A student is a member of a class
A lion is a member of a pride
A senator is a member of a
2024-07-09 04:56:25 root INFO     [order_1_approx] starting weight calculation for A wolf is a member of a pack
A student is a member of a class
A singer is a member of a choir
A lion is a member of a pride
A photo is a member of a album
A senator is a member of a senate
A person is a member of a society
A citizen is a member of a
2024-07-09 04:56:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 05:00:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 05:04:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9648, -0.6133, -0.0835,  ...,  0.9756, -0.2300, -0.6211],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.9507, -0.6401, -0.0914,  ...,  0.9277, -0.2443, -0.6519],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.0273, -0.6875,  0.4248,  ..., -2.0820,  2.1934,  1.1250],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0034, -0.0131, -0.0013,  ...,  0.0037, -0.0073,  0.0020],
        [ 0.0103,  0.0097,  0.0065,  ...,  0.0006,  0.0102,  0.0031],
        [ 0.0212,  0.0008, -0.0062,  ...,  0.0120,  0.0130, -0.0075],
        ...,
        [ 0.0083, -0.0079, -0.0092,  ..., -0.0107,  0.0033, -0.0015],
        [ 0.0019, -0.0210, -0.0002,  ..., -0.0047, -0.0210, -0.0118],
        [-0.0017,  0.0010,  0.0100,  ..., -0.0002,  0.0145,  0.0176]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 1.0386e-03, -2.4548e-03,  1.0433e-03,  ...,  1.6050e-03,
         -3.4499e-04, -1.3676e-03],
        [ 1.6365e-03,  5.9986e-04,  5.9223e-04,  ...,  3.8958e-04,
          2.0771e-03,  5.0879e-04],
        [ 7.3528e-04,  5.2929e-04, -1.2422e-04,  ...,  5.8460e-04,
         -9.9659e-04, -1.3673e-04],
        ...,
        [ 2.5196e-03,  6.7091e-04, -6.5947e-04,  ...,  4.4084e-04,
          6.9499e-05, -2.2621e-03],
        [ 5.6207e-05,  3.8862e-04, -9.3794e-04,  ...,  1.1559e-03,
         -1.3685e-04,  9.1171e-04],
        [-4.6420e-04,  6.5756e-04,  2.8095e-03,  ..., -1.6327e-03,
          2.3794e-04,  2.1610e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.0586,  0.3936, -0.3945,  ..., -2.1211,  2.5137,  0.7646]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1333, -0.0629,  0.0266,  ..., -0.0594, -0.0126,  0.2461]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 05:04:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A wolf is a member of a pack
A student is a member of a class
A singer is a member of a choir
A lion is a member of a pride
A photo is a member of a album
A senator is a member of a senate
A person is a member of a society
A citizen is a member of a
2024-07-09 05:04:34 root INFO     [order_1_approx] starting weight calculation for A lion is a member of a pride
A senator is a member of a senate
A person is a member of a society
A citizen is a member of a citizenry
A singer is a member of a choir
A wolf is a member of a pack
A student is a member of a class
A photo is a member of a
2024-07-09 05:04:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 05:08:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 05:12:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.5342, 0.2499, 1.2393,  ..., 0.0088, 0.1554, 0.0573], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([0.4836, 0.2172, 1.0645,  ..., 0.0131, 0.1221, 0.0311], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4854, -1.3711, -0.8164,  ..., -0.6040, -4.4453,  1.2930],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.7150e-03, -9.2621e-03, -3.8052e-03,  ..., -8.1940e-03,
          1.4668e-03, -6.2828e-03],
        [-1.0437e-02, -9.0485e-03, -2.3975e-03,  ...,  1.0101e-02,
         -7.7286e-03,  1.3000e-02],
        [ 2.2354e-02, -6.1607e-03, -9.7046e-03,  ..., -1.9882e-02,
          3.5858e-04, -1.0338e-03],
        ...,
        [-1.3718e-02, -5.0926e-03, -2.5215e-03,  ..., -9.4681e-03,
         -7.2174e-03,  3.4752e-03],
        [ 3.1982e-02, -5.8365e-03, -1.2115e-02,  ..., -3.2616e-03,
          1.1215e-02, -1.3062e-02],
        [-1.6068e-02,  1.5625e-02,  2.6703e-05,  ..., -2.5040e-02,
          1.1101e-02,  4.1122e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.0657e-03, -7.6342e-04,  1.2569e-03,  ..., -4.1556e-04,
          1.6785e-04, -3.6025e-04],
        [ 2.0809e-03, -2.3155e-03,  5.5408e-04,  ...,  9.2793e-04,
          1.2147e-04,  9.9838e-05],
        [ 9.3889e-04, -1.6537e-03, -8.7166e-04,  ...,  5.3167e-04,
         -5.5432e-06, -2.5730e-03],
        ...,
        [ 1.3132e-03, -5.8937e-04,  9.1362e-04,  ..., -1.4248e-03,
          1.2197e-03, -4.9973e-04],
        [ 8.5735e-04, -1.1148e-03, -8.4209e-04,  ...,  3.0384e-03,
         -2.9421e-04, -2.8076e-03],
        [-6.3896e-04,  1.0948e-03, -3.9244e-04,  ..., -1.0052e-03,
         -6.4611e-04, -5.2738e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1091, -0.7236, -0.2642,  ..., -0.3879, -4.6289,  0.5659]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0475,  0.0658, -0.4539,  ..., -0.1705, -0.4346,  0.0851]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 05:12:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A lion is a member of a pride
A senator is a member of a senate
A person is a member of a society
A citizen is a member of a citizenry
A singer is a member of a choir
A wolf is a member of a pack
A student is a member of a class
A photo is a member of a
2024-07-09 05:12:44 root INFO     [order_1_approx] starting weight calculation for A citizen is a member of a citizenry
A person is a member of a society
A senator is a member of a senate
A photo is a member of a album
A student is a member of a class
A lion is a member of a pride
A singer is a member of a choir
A wolf is a member of a
2024-07-09 05:12:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 05:16:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 05:20:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6431, -1.2490, -0.4912,  ...,  0.3928, -0.3149, -0.0189],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.5649, -1.1455, -0.4241,  ...,  0.3369, -0.2917, -0.0399],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7969, -1.9043, -3.8789,  ...,  1.9189,  0.0508,  2.5273],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0047, -0.0207,  0.0084,  ..., -0.0008,  0.0079,  0.0026],
        [-0.0111,  0.0115, -0.0061,  ...,  0.0155,  0.0076, -0.0041],
        [ 0.0237, -0.0078,  0.0175,  ..., -0.0143,  0.0172, -0.0172],
        ...,
        [ 0.0064, -0.0079, -0.0038,  ...,  0.0062, -0.0107,  0.0093],
        [ 0.0141, -0.0205,  0.0043,  ..., -0.0042, -0.0076,  0.0028],
        [ 0.0029, -0.0053,  0.0042,  ..., -0.0027, -0.0095,  0.0052]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.3943e-03, -1.5554e-03,  1.1816e-03,  ...,  5.3692e-04,
          2.3806e-04, -2.0275e-03],
        [-1.8001e-04, -8.6784e-04,  9.2793e-04,  ...,  1.6165e-03,
         -2.5654e-04, -2.3155e-03],
        [ 1.5440e-03, -4.0817e-04,  1.0443e-03,  ..., -1.6985e-03,
          5.4932e-04, -7.3528e-04],
        ...,
        [ 3.9749e-03, -6.7759e-04, -2.7885e-03,  ..., -1.1578e-03,
         -1.8024e-04, -1.9331e-03],
        [ 5.8222e-04, -8.3160e-04, -2.0254e-04,  ...,  2.1477e-03,
          1.1845e-03, -3.8242e-04],
        [-2.4319e-05, -2.0237e-03,  3.0327e-04,  ...,  6.7043e-04,
         -3.8528e-04, -2.0161e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9565, -1.4434, -4.3555,  ...,  2.1973,  0.1643,  3.0195]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.2286, -0.0515, -0.0253,  ..., -0.0720, -0.2803, -0.4021]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 05:20:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A citizen is a member of a citizenry
A person is a member of a society
A senator is a member of a senate
A photo is a member of a album
A student is a member of a class
A lion is a member of a pride
A singer is a member of a choir
A wolf is a member of a
2024-07-09 05:20:52 root INFO     [order_1_approx] starting weight calculation for A citizen is a member of a citizenry
A wolf is a member of a pack
A photo is a member of a album
A lion is a member of a pride
A singer is a member of a choir
A student is a member of a class
A senator is a member of a senate
A person is a member of a
2024-07-09 05:20:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 05:25:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 05:29:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0669,  0.0487,  0.8271,  ...,  0.5801,  0.8350, -0.1271],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.0638,  0.0337,  0.7705,  ...,  0.5547,  0.7964, -0.1504],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.9004, -2.0742, -1.1191,  ..., -2.1777,  0.7495,  0.1313],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0039, -0.0119,  0.0050,  ...,  0.0072,  0.0061,  0.0067],
        [-0.0092,  0.0142,  0.0253,  ...,  0.0006,  0.0085,  0.0121],
        [ 0.0223, -0.0003, -0.0125,  ..., -0.0053, -0.0007, -0.0060],
        ...,
        [-0.0157,  0.0017,  0.0036,  ..., -0.0020, -0.0157, -0.0008],
        [ 0.0129, -0.0057, -0.0033,  ..., -0.0157, -0.0159, -0.0059],
        [ 0.0009,  0.0086, -0.0284,  ..., -0.0067,  0.0095, -0.0104]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.6665e-03, -1.2093e-03,  9.5081e-04,  ...,  1.5202e-03,
         -9.4700e-04,  8.5735e-04],
        [ 1.4246e-04, -1.4601e-03,  1.0900e-03,  ...,  9.5463e-04,
          2.2678e-03,  1.1101e-03],
        [-7.5245e-04, -8.5354e-04,  2.1410e-04,  ...,  3.9101e-05,
         -1.8680e-04, -1.7262e-03],
        ...,
        [ 8.6308e-04, -5.8889e-04,  6.4564e-04,  ..., -1.3990e-03,
          1.1396e-03, -3.0804e-03],
        [ 4.8709e-04,  7.0333e-04,  3.1686e-04,  ...,  9.7656e-04,
         -6.7139e-04,  1.4544e-03],
        [-1.8539e-03,  2.5768e-03, -2.4986e-03,  ...,  9.7752e-06,
         -1.1120e-03, -6.6185e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5996, -2.4062,  0.0732,  ..., -2.1211,  0.8599,  1.5059]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0963, -0.0667, -0.2330,  ..., -0.0379,  0.1611, -0.0116]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 05:29:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A citizen is a member of a citizenry
A wolf is a member of a pack
A photo is a member of a album
A lion is a member of a pride
A singer is a member of a choir
A student is a member of a class
A senator is a member of a senate
A person is a member of a
2024-07-09 05:29:03 root INFO     [order_1_approx] starting weight calculation for A photo is a member of a album
A wolf is a member of a pack
A senator is a member of a senate
A student is a member of a class
A citizen is a member of a citizenry
A person is a member of a society
A lion is a member of a pride
A singer is a member of a
2024-07-09 05:29:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 05:33:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 05:37:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3179, -0.2000,  0.4814,  ...,  0.7793,  0.2603, -0.4854],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.3083, -0.2134,  0.4272,  ...,  0.7173,  0.2260, -0.4973],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8008,  2.0312,  1.2422,  ...,  2.0371, -0.2036, -0.6372],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.3455e-03, -8.4152e-03,  8.9951e-03,  ..., -8.4839e-03,
          6.4468e-03, -8.5068e-03],
        [-1.3382e-02,  9.3155e-03, -9.8648e-03,  ..., -2.8152e-03,
         -1.6174e-02,  1.0849e-02],
        [ 5.4092e-03, -2.7313e-03, -6.1111e-03,  ..., -1.8730e-03,
          3.4790e-03,  7.4196e-03],
        ...,
        [-1.2436e-02,  1.5602e-03, -9.2163e-03,  ..., -1.7059e-02,
         -5.9509e-04, -4.5776e-05],
        [ 2.0050e-02, -7.4768e-03,  6.6795e-03,  ..., -3.2234e-03,
         -1.3596e-02, -1.2527e-02],
        [-3.7079e-03, -1.6975e-03,  1.4725e-02,  ..., -4.9667e-03,
          1.9211e-02, -7.3290e-04]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 4.3178e-04, -2.0180e-03,  3.0637e-04,  ..., -3.0923e-04,
          5.4216e-04, -6.6042e-04],
        [ 1.3990e-03,  3.3045e-04,  1.1244e-03,  ...,  1.3800e-03,
         -4.1723e-04,  2.8515e-04],
        [ 1.8234e-03, -1.0567e-03, -6.3419e-04,  ..., -1.3885e-03,
          9.6226e-04, -2.7442e-04],
        ...,
        [ 3.3607e-03, -8.5402e-04,  3.3855e-03,  ..., -3.0918e-03,
          1.5898e-03, -2.6779e-03],
        [ 1.6251e-03, -8.1110e-04,  1.7900e-03,  ...,  1.2398e-03,
          6.2799e-04, -9.6226e-04],
        [-6.5994e-04, -1.2326e-04,  1.9894e-03,  ..., -8.2111e-04,
          4.7684e-06, -1.8425e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8638,  1.2285,  1.5479,  ...,  2.4043, -0.0552, -1.2246]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.2352, -0.0067, -0.1163,  ..., -0.4080, -0.0078, -0.1580]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 05:37:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A photo is a member of a album
A wolf is a member of a pack
A senator is a member of a senate
A student is a member of a class
A citizen is a member of a citizenry
A person is a member of a society
A lion is a member of a pride
A singer is a member of a
2024-07-09 05:37:13 root INFO     [order_1_approx] starting weight calculation for A senator is a member of a senate
A citizen is a member of a citizenry
A singer is a member of a choir
A person is a member of a society
A photo is a member of a album
A lion is a member of a pride
A wolf is a member of a pack
A student is a member of a
2024-07-09 05:37:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 05:41:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 05:45:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7188,  0.4956,  0.9409,  ...,  0.8940, -0.2004,  0.5942],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.6606,  0.4519,  0.8179,  ...,  0.7954, -0.2009,  0.5361],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6602,  2.2598, -0.2930,  ...,  2.0430,  2.9980,  2.0430],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0116, -0.0083,  0.0016,  ...,  0.0062,  0.0063, -0.0048],
        [-0.0192,  0.0046, -0.0236,  ..., -0.0005, -0.0019, -0.0087],
        [ 0.0134,  0.0152,  0.0144,  ..., -0.0087,  0.0062, -0.0091],
        ...,
        [-0.0150, -0.0037, -0.0005,  ..., -0.0074, -0.0059,  0.0149],
        [ 0.0038, -0.0070, -0.0093,  ...,  0.0044, -0.0218,  0.0056],
        [-0.0032, -0.0030, -0.0018,  ..., -0.0141,  0.0084,  0.0093]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-8.9502e-04, -2.9802e-04,  2.1610e-03,  ..., -7.3910e-06,
         -5.5265e-04,  4.5037e-04],
        [ 1.2884e-03, -9.4318e-04,  1.3914e-03,  ...,  9.5558e-04,
         -1.9073e-04, -1.6584e-03],
        [-8.9312e-04,  1.7004e-03, -1.5860e-03,  ...,  7.1526e-05,
          1.0567e-03, -1.7481e-03],
        ...,
        [-7.3552e-05,  7.1764e-05,  1.4467e-03,  ..., -6.8378e-04,
         -7.9036e-05, -1.0719e-03],
        [ 6.5756e-04, -1.9765e-04, -3.3259e-04,  ...,  1.2550e-03,
          2.5082e-04,  8.5115e-04],
        [-2.9697e-03, -8.3160e-04, -6.2943e-05,  ..., -8.0585e-04,
         -1.2374e-04, -6.6423e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1650,  1.6738, -0.4346,  ...,  1.9121,  2.6914,  1.6816]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0418, -0.4307,  0.1388,  ..., -0.0951,  0.1549, -0.0152]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 05:45:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A senator is a member of a senate
A citizen is a member of a citizenry
A singer is a member of a choir
A person is a member of a society
A photo is a member of a album
A lion is a member of a pride
A wolf is a member of a pack
A student is a member of a
2024-07-09 05:45:22 root INFO     total operator prediction time: 3913.783135652542 seconds
2024-07-09 05:45:22 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on male - female
2024-07-09 05:45:24 root INFO     building operator male - female
2024-07-09 05:45:25 root INFO     [order_1_approx] starting weight calculation for A female husband is known as a wife
A female bull is known as a cow
A female father is known as a mother
A female ram is known as a ewe
A female webmaster is known as a webmistress
A female manager is known as a manageress
A female poet is known as a poetess
A female batman is known as a
2024-07-09 05:45:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 05:49:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 05:53:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0098,  0.4497,  0.1799,  ...,  0.0591, -1.1387,  0.3855],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 1.0469,  0.4629,  0.1683,  ...,  0.0641, -1.1826,  0.3862],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.6230, -2.3965, -0.6748,  ..., -1.3057,  1.3516,  1.1855],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0140, -0.0096,  0.0162,  ...,  0.0103, -0.0087, -0.0140],
        [-0.0222,  0.0128, -0.0061,  ...,  0.0141,  0.0026, -0.0096],
        [ 0.0159, -0.0155, -0.0119,  ..., -0.0139,  0.0119, -0.0178],
        ...,
        [-0.0189, -0.0266, -0.0059,  ...,  0.0126,  0.0006,  0.0189],
        [-0.0028,  0.0088, -0.0023,  ...,  0.0050, -0.0079, -0.0100],
        [-0.0132,  0.0119,  0.0063,  ...,  0.0112,  0.0116,  0.0020]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.1305e-03, -2.3899e-03,  4.7088e-04,  ...,  5.6648e-04,
          1.0490e-04,  2.7800e-04],
        [ 2.7871e-04,  1.0567e-03,  2.8729e-04,  ...,  1.5488e-03,
         -1.1139e-03, -7.7295e-04],
        [ 3.1614e-04,  1.5392e-03, -1.5965e-03,  ..., -6.0081e-04,
         -1.1168e-03, -6.9189e-04],
        ...,
        [ 1.5402e-04, -7.6628e-04,  6.3419e-05,  ..., -8.8406e-04,
         -7.1049e-04,  5.7280e-05],
        [-1.4763e-03,  6.6519e-05, -8.5115e-04,  ...,  1.7462e-03,
          3.0756e-04, -1.1702e-03],
        [-7.6675e-04, -8.2552e-05,  7.2670e-04,  ..., -4.6802e-04,
          1.1206e-03,  1.2455e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.1797, -3.0156,  0.9922,  ..., -0.6831,  2.0742,  0.1543]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0345, -0.2812, -0.1273,  ..., -0.0052, -0.2451,  0.0403]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 05:53:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female husband is known as a wife
A female bull is known as a cow
A female father is known as a mother
A female ram is known as a ewe
A female webmaster is known as a webmistress
A female manager is known as a manageress
A female poet is known as a poetess
A female batman is known as a
2024-07-09 05:53:32 root INFO     [order_1_approx] starting weight calculation for A female husband is known as a wife
A female bull is known as a cow
A female ram is known as a ewe
A female poet is known as a poetess
A female father is known as a mother
A female webmaster is known as a webmistress
A female batman is known as a batwoman
A female manager is known as a
2024-07-09 05:53:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 05:57:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 06:01:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7598, -0.1179,  0.3745,  ...,  0.5913, -0.2371,  0.7197],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.7202, -0.1334,  0.3284,  ...,  0.5430, -0.2424,  0.6753],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9678, -7.8789, -1.2695,  ..., -2.2969,  1.3623,  1.1641],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0003, -0.0069,  0.0120,  ...,  0.0164,  0.0049, -0.0129],
        [-0.0053, -0.0074, -0.0070,  ...,  0.0015, -0.0038, -0.0036],
        [ 0.0172, -0.0014,  0.0072,  ..., -0.0032,  0.0150, -0.0128],
        ...,
        [ 0.0028, -0.0134,  0.0090,  ..., -0.0205,  0.0075, -0.0014],
        [ 0.0205, -0.0067,  0.0195,  ..., -0.0091, -0.0040,  0.0121],
        [-0.0040, -0.0023, -0.0091,  ...,  0.0073,  0.0178,  0.0189]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.9207e-03, -2.4662e-03,  1.3819e-03,  ...,  2.1858e-03,
          2.5368e-03, -1.1024e-03],
        [-9.0694e-04,  3.2520e-04,  6.4135e-04,  ...,  2.9774e-03,
         -1.6375e-03, -3.3855e-03],
        [ 9.7752e-04,  8.5115e-04,  2.3613e-03,  ..., -1.1339e-03,
         -2.8849e-04,  4.6182e-04],
        ...,
        [ 1.1616e-03, -8.8596e-04,  1.3580e-03,  ..., -3.2806e-03,
          1.6909e-03, -4.3678e-03],
        [-7.1287e-04,  1.6441e-03,  1.2169e-03,  ..., -1.6327e-03,
         -1.9102e-03, -3.7742e-04],
        [-2.1496e-03, -1.3170e-03, -2.0103e-03,  ..., -5.9795e-04,
         -1.3113e-06,  1.8854e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7847, -7.3945, -1.8008,  ..., -0.7705,  1.6602,  1.0547]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1170, -0.2749, -0.0343,  ..., -0.3850, -0.4180,  0.1571]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 06:01:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female husband is known as a wife
A female bull is known as a cow
A female ram is known as a ewe
A female poet is known as a poetess
A female father is known as a mother
A female webmaster is known as a webmistress
A female batman is known as a batwoman
A female manager is known as a
2024-07-09 06:01:35 root INFO     [order_1_approx] starting weight calculation for A female bull is known as a cow
A female poet is known as a poetess
A female batman is known as a batwoman
A female ram is known as a ewe
A female father is known as a mother
A female webmaster is known as a webmistress
A female manager is known as a manageress
A female husband is known as a
2024-07-09 06:01:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 06:05:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 06:09:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([1.2246, 0.0259, 0.6870,  ..., 1.2812, 0.2554, 0.7754], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([1.1406, 0.0054, 0.5996,  ..., 1.1504, 0.2145, 0.7139], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5352, -3.2031, -2.0547,  ..., -4.1016,  0.8408,  0.3228],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0022,  0.0043,  0.0017,  ..., -0.0009,  0.0082, -0.0131],
        [ 0.0058, -0.0117,  0.0003,  ..., -0.0025,  0.0004, -0.0172],
        [ 0.0118, -0.0183, -0.0041,  ..., -0.0064, -0.0097,  0.0003],
        ...,
        [ 0.0014, -0.0186,  0.0108,  ..., -0.0289, -0.0195, -0.0057],
        [-0.0018, -0.0068, -0.0015,  ...,  0.0049, -0.0252,  0.0041],
        [ 0.0046, -0.0006, -0.0006,  ...,  0.0113,  0.0244,  0.0135]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.2617e-03, -9.4223e-04,  1.6413e-03,  ...,  7.1049e-04,
          1.4210e-04, -1.9436e-03],
        [ 1.2770e-03, -7.5531e-04,  3.2082e-03,  ..., -5.0354e-04,
          2.3508e-04, -1.0462e-03],
        [ 8.9693e-04,  3.6478e-04, -2.6226e-03,  ..., -5.0664e-06,
          2.9612e-04, -7.7868e-04],
        ...,
        [ 1.7672e-03, -8.9836e-04, -1.1337e-04,  ..., -1.4610e-03,
          7.1526e-04, -4.4022e-03],
        [-4.8161e-04,  9.5749e-04, -1.7309e-03,  ..., -8.3971e-04,
         -2.7428e-03,  3.8147e-06],
        [ 6.7091e-04,  1.1177e-03, -3.1300e-03,  ...,  1.9627e-03,
          6.3467e-04,  1.1663e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9414, -2.7793, -2.5801,  ..., -3.6094,  1.1982, -0.8667]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1028, -0.1675,  0.0887,  ..., -0.1367, -0.3706, -0.0059]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 06:09:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female bull is known as a cow
A female poet is known as a poetess
A female batman is known as a batwoman
A female ram is known as a ewe
A female father is known as a mother
A female webmaster is known as a webmistress
A female manager is known as a manageress
A female husband is known as a
2024-07-09 06:09:39 root INFO     [order_1_approx] starting weight calculation for A female husband is known as a wife
A female poet is known as a poetess
A female father is known as a mother
A female batman is known as a batwoman
A female manager is known as a manageress
A female ram is known as a ewe
A female bull is known as a cow
A female webmaster is known as a
2024-07-09 06:09:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 06:13:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 06:17:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6416,  1.3506,  1.3711,  ..., -0.4307,  0.8613,  0.6504],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.5776,  1.2363,  1.1729,  ..., -0.3696,  0.7500,  0.5771],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.2578, -3.1582, -2.5938,  ..., -1.0977,  0.0486, -0.9507],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.5192e-02, -1.1292e-02, -6.4430e-03,  ...,  5.0087e-03,
         -7.4387e-04,  3.7498e-03],
        [ 6.8817e-03, -6.0234e-03, -7.8430e-03,  ...,  2.7142e-03,
         -3.0785e-03,  1.2016e-03],
        [ 2.8534e-02,  5.8670e-03, -1.6327e-02,  ..., -6.0730e-03,
          1.2993e-02,  1.9226e-02],
        ...,
        [-2.4605e-03,  5.8651e-04, -2.7313e-03,  ..., -1.3512e-02,
          1.3790e-03, -5.0659e-03],
        [ 8.8959e-03, -5.7068e-03,  5.4245e-03,  ...,  4.9820e-03,
         -3.8147e-06,  1.2299e-02],
        [ 5.6572e-03, -1.7662e-03, -2.8381e-03,  ...,  1.5091e-02,
         -3.1700e-03, -4.7073e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.4353e-04, -1.5235e-04,  9.4604e-04,  ...,  1.8005e-03,
         -6.1512e-04, -2.1541e-04],
        [-3.6693e-04, -2.3019e-04,  1.3995e-04,  ..., -1.9014e-04,
          2.8491e-05, -1.0562e-04],
        [-1.1139e-03,  1.5802e-03, -9.6273e-04,  ..., -1.7147e-03,
         -8.6069e-05,  5.0735e-04],
        ...,
        [ 8.7786e-04,  5.0068e-06, -1.5411e-03,  ..., -2.5845e-04,
         -1.6201e-04, -4.4203e-04],
        [-6.5470e-04,  1.5297e-03, -8.2922e-04,  ..., -4.3201e-04,
          4.5514e-04, -4.8685e-04],
        [-5.8031e-04, -1.2243e-04, -8.5354e-04,  ...,  1.0958e-03,
         -1.5984e-03, -3.4523e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.5078, -2.0664, -2.2812,  ..., -1.0576,  0.0180, -1.1328]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1067, -0.2607,  0.1073,  ..., -0.2306,  0.0668, -0.0217]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 06:17:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female husband is known as a wife
A female poet is known as a poetess
A female father is known as a mother
A female batman is known as a batwoman
A female manager is known as a manageress
A female ram is known as a ewe
A female bull is known as a cow
A female webmaster is known as a
2024-07-09 06:17:48 root INFO     [order_1_approx] starting weight calculation for A female father is known as a mother
A female webmaster is known as a webmistress
A female bull is known as a cow
A female poet is known as a poetess
A female manager is known as a manageress
A female husband is known as a wife
A female batman is known as a batwoman
A female ram is known as a
2024-07-09 06:17:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 06:21:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 06:25:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6699, -0.4561, -0.7583,  ...,  0.5312, -0.2920,  0.0812],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.6250, -0.4534, -0.6875,  ...,  0.4814, -0.2869,  0.0549],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2129, -2.0918, -4.5273,  ...,  0.1852,  3.4805, -0.2698],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0112, -0.0116,  0.0054,  ...,  0.0132,  0.0010, -0.0153],
        [-0.0297,  0.0124, -0.0068,  ...,  0.0277,  0.0196,  0.0020],
        [-0.0049,  0.0056,  0.0068,  ..., -0.0051,  0.0034,  0.0056],
        ...,
        [ 0.0061, -0.0117, -0.0131,  ..., -0.0197, -0.0291, -0.0069],
        [-0.0071, -0.0083,  0.0028,  ..., -0.0328, -0.0082,  0.0182],
        [-0.0200,  0.0052, -0.0223,  ..., -0.0098,  0.0253,  0.0142]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-9.6369e-04, -2.1572e-03,  1.4992e-03,  ...,  8.7357e-04,
         -7.4863e-04,  1.5306e-03],
        [-5.9986e-04, -1.9550e-03,  1.9875e-03,  ...,  4.7398e-04,
          1.1158e-03, -4.9686e-04],
        [ 2.8706e-04,  1.5812e-03, -2.1343e-03,  ..., -2.3155e-03,
         -2.2411e-03,  2.2545e-03],
        ...,
        [ 6.9904e-04, -4.2701e-04,  1.4820e-03,  ..., -4.9973e-03,
         -4.5300e-05,  7.9870e-05],
        [-2.3150e-04,  1.6785e-03, -1.5440e-03,  ...,  7.9823e-04,
         -8.2731e-04,  6.6662e-04],
        [-3.1042e-04, -9.9659e-05,  1.3208e-04,  ...,  8.5449e-04,
         -1.1711e-03, -1.1673e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9004, -1.8486, -4.0898,  ...,  0.7046,  3.2988,  1.0312]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.3948, -0.1180,  0.0096,  ..., -0.0977, -0.4380, -0.0119]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 06:25:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female father is known as a mother
A female webmaster is known as a webmistress
A female bull is known as a cow
A female poet is known as a poetess
A female manager is known as a manageress
A female husband is known as a wife
A female batman is known as a batwoman
A female ram is known as a
2024-07-09 06:25:57 root INFO     [order_1_approx] starting weight calculation for A female father is known as a mother
A female husband is known as a wife
A female manager is known as a manageress
A female ram is known as a ewe
A female poet is known as a poetess
A female batman is known as a batwoman
A female webmaster is known as a webmistress
A female bull is known as a
2024-07-09 06:25:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 06:30:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 06:34:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4460, -0.9766, -0.5586,  ...,  2.2188, -0.5166, -0.0046],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.4019, -0.9087, -0.4861,  ...,  1.8984, -0.4724, -0.0279],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.2109,  1.0645, -5.0625,  ..., -2.5410,  3.2500, -1.5186],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0210, -0.0139, -0.0024,  ...,  0.0069,  0.0057, -0.0168],
        [-0.0076, -0.0023, -0.0070,  ..., -0.0021,  0.0140, -0.0033],
        [ 0.0007, -0.0123,  0.0137,  ...,  0.0035,  0.0061, -0.0228],
        ...,
        [-0.0089, -0.0247, -0.0126,  ..., -0.0146, -0.0138, -0.0060],
        [ 0.0117, -0.0180, -0.0005,  ..., -0.0038, -0.0054, -0.0024],
        [-0.0047,  0.0119, -0.0005,  ...,  0.0050,  0.0113,  0.0067]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.7871e-04, -1.7023e-03,  3.4046e-03,  ..., -7.0906e-04,
         -4.2439e-04, -7.6723e-04],
        [ 1.3762e-03, -1.5116e-04,  3.9940e-03,  ...,  1.5345e-03,
          2.7695e-03,  9.0933e-04],
        [ 5.4216e-04, -4.0364e-04, -3.3689e-04,  ...,  4.9639e-04,
         -2.5940e-03, -6.7294e-05],
        ...,
        [ 2.7037e-04,  7.0333e-04,  1.1902e-03,  ..., -5.3177e-03,
          1.4534e-03,  3.8147e-05],
        [ 1.8179e-04,  1.4658e-03,  1.2436e-03,  ..., -1.0366e-03,
         -6.9714e-04,  4.8637e-05],
        [-8.9645e-04, -3.1686e-04,  1.7846e-04,  ...,  6.2037e-04,
         -3.7217e-04,  1.3113e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.7578,  0.7441, -2.6719,  ..., -1.7305,  3.4746, -1.4932]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0144, -0.0571, -0.0499,  ...,  0.0039,  0.0147,  0.3008]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 06:34:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female father is known as a mother
A female husband is known as a wife
A female manager is known as a manageress
A female ram is known as a ewe
A female poet is known as a poetess
A female batman is known as a batwoman
A female webmaster is known as a webmistress
A female bull is known as a
2024-07-09 06:34:05 root INFO     [order_1_approx] starting weight calculation for A female ram is known as a ewe
A female husband is known as a wife
A female poet is known as a poetess
A female bull is known as a cow
A female manager is known as a manageress
A female webmaster is known as a webmistress
A female batman is known as a batwoman
A female father is known as a
2024-07-09 06:34:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 06:38:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 06:42:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2458, -0.0095,  0.3120,  ...,  0.5918,  0.1707,  0.6343],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.2417, -0.0274,  0.2856,  ...,  0.5703,  0.1490,  0.6226],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.3242, -7.8164, -2.7812,  ..., -2.3887,  4.9570, -3.1680],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0006, -0.0078,  0.0107,  ...,  0.0037,  0.0186, -0.0230],
        [ 0.0137, -0.0041,  0.0056,  ..., -0.0090,  0.0063, -0.0074],
        [ 0.0201, -0.0054, -0.0082,  ..., -0.0097,  0.0190, -0.0213],
        ...,
        [ 0.0166, -0.0156, -0.0037,  ..., -0.0077, -0.0141,  0.0087],
        [ 0.0195,  0.0033, -0.0040,  ...,  0.0051, -0.0248,  0.0011],
        [ 0.0182, -0.0149,  0.0056,  ...,  0.0082,  0.0211,  0.0127]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.3847e-03, -2.4166e-03,  1.9741e-03,  ...,  1.0471e-03,
          1.3390e-03, -7.9823e-04],
        [ 2.1267e-04, -2.0409e-03,  1.9627e-03,  ...,  2.0158e-04,
         -1.2064e-03,  4.5276e-04],
        [ 2.1720e-04,  1.4009e-03, -1.8301e-03,  ..., -2.7037e-04,
         -1.6346e-03, -1.6451e-03],
        ...,
        [ 6.6280e-04,  3.2139e-04, -1.1158e-03,  ...,  9.2697e-04,
          5.9986e-04,  1.4782e-05],
        [ 5.4073e-04,  1.5478e-03,  1.3695e-03,  ...,  6.0415e-04,
         -1.9188e-03,  6.0940e-04],
        [-1.8978e-03,  1.6298e-03, -3.0346e-03,  ...,  1.6937e-03,
         -8.9359e-04, -2.4045e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7207, -7.1406, -3.1270,  ..., -1.9629,  4.6367, -2.7559]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1766, -0.0406, -0.0878,  ...,  0.1021, -0.4072, -0.1670]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 06:42:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female ram is known as a ewe
A female husband is known as a wife
A female poet is known as a poetess
A female bull is known as a cow
A female manager is known as a manageress
A female webmaster is known as a webmistress
A female batman is known as a batwoman
A female father is known as a
2024-07-09 06:42:16 root INFO     [order_1_approx] starting weight calculation for A female manager is known as a manageress
A female ram is known as a ewe
A female batman is known as a batwoman
A female bull is known as a cow
A female husband is known as a wife
A female webmaster is known as a webmistress
A female father is known as a mother
A female poet is known as a
2024-07-09 06:42:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 06:46:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 06:50:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2856, -0.5630,  0.1843,  ...,  0.6196,  0.0391,  0.4146],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.2455, -0.5195,  0.1421,  ...,  0.5215,  0.0151,  0.3450],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6348,  1.0488, -7.5195,  ...,  0.0337, -1.1650, -4.7500],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0100,  0.0093,  0.0045,  ...,  0.0010, -0.0013, -0.0017],
        [-0.0068, -0.0035, -0.0120,  ...,  0.0111, -0.0015,  0.0086],
        [ 0.0370,  0.0008,  0.0056,  ..., -0.0132,  0.0063, -0.0132],
        ...,
        [ 0.0072, -0.0025,  0.0066,  ..., -0.0244, -0.0024,  0.0050],
        [ 0.0051, -0.0048,  0.0168,  ...,  0.0014, -0.0185, -0.0013],
        [-0.0021,  0.0081,  0.0090,  ...,  0.0091,  0.0161, -0.0012]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.8730e-03, -4.8423e-04,  1.3046e-03,  ..., -4.4966e-04,
         -8.2397e-04,  7.3576e-04],
        [ 2.2423e-04, -1.6797e-04, -2.4796e-04,  ...,  1.2779e-04,
          5.4550e-04, -1.6475e-04],
        [ 9.6989e-04,  1.8144e-04, -1.8835e-03,  ..., -1.6823e-03,
          2.6417e-04, -1.8158e-03],
        ...,
        [ 1.3447e-03,  1.5640e-03,  3.3331e-04,  ..., -1.6994e-03,
          2.3994e-03, -1.8625e-03],
        [-6.1703e-04, -4.1008e-04,  2.2078e-04,  ..., -4.1580e-04,
         -1.5364e-03,  2.6703e-04],
        [-1.2159e-03,  1.6403e-03, -6.3276e-04,  ..., -1.3924e-03,
          4.9829e-05, -2.9206e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1172,  0.6650, -6.2695,  ...,  0.4021, -0.3657, -3.0547]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0132, -0.1904, -0.1987,  ..., -0.3694,  0.0038,  0.2778]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 06:50:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female manager is known as a manageress
A female ram is known as a ewe
A female batman is known as a batwoman
A female bull is known as a cow
A female husband is known as a wife
A female webmaster is known as a webmistress
A female father is known as a mother
A female poet is known as a
2024-07-09 06:50:26 root INFO     total operator prediction time: 3902.068635702133 seconds
2024-07-09 06:50:26 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on UK_city - county
2024-07-09 06:50:29 root INFO     building operator UK_city - county
2024-07-09 06:50:30 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of newcastle is in the county of northumberland
In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of birmingham is in the county of midlands
In the United Kingdom, the city of preston is in the county of lancashire
In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of liverpool is in the county of lancashire
In the United Kingdom, the city of cardiff is in the county of glamorgan
In the United Kingdom, the city of bradford is in the county of
2024-07-09 06:50:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 06:54:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 06:58:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2030, -0.3154,  0.2827,  ...,  0.3608,  1.3613, -0.5352],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.2010, -0.3411,  0.2605,  ...,  0.3521,  1.3301, -0.5718],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0240, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6484, -4.1484, -5.3984,  ...,  0.2124,  4.3320, -3.5703],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0408,  0.0000, -0.0105,  ...,  0.0092,  0.0020, -0.0139],
        [ 0.0385, -0.0088, -0.0229,  ...,  0.0313, -0.0049,  0.0051],
        [ 0.0126,  0.0320, -0.0096,  ...,  0.0061, -0.0031, -0.0010],
        ...,
        [-0.0462, -0.0137,  0.0373,  ...,  0.0362,  0.0404,  0.0218],
        [ 0.0004, -0.0039, -0.0053,  ..., -0.0157,  0.0059,  0.0166],
        [-0.0253, -0.0106, -0.0419,  ...,  0.0337, -0.0222, -0.0086]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.2617e-03,  5.7173e-04,  2.3246e-04,  ..., -2.5082e-04,
          8.7786e-04,  1.4830e-04],
        [-2.7485e-03, -1.8282e-03,  4.6587e-04,  ...,  1.4896e-03,
          1.9951e-03, -2.0540e-04],
        [ 1.5974e-03,  1.8883e-03, -1.4744e-03,  ..., -1.1063e-03,
         -1.8845e-03,  4.3392e-04],
        ...,
        [ 6.4468e-04, -5.0163e-04, -2.2054e-04,  ...,  1.5187e-04,
          1.0061e-03,  1.1845e-03],
        [ 3.8815e-04,  9.4891e-04,  4.2987e-04,  ...,  1.0169e-04,
         -7.1764e-05, -2.3007e-04],
        [-9.8228e-04,  5.2452e-04, -2.5787e-03,  ...,  1.5488e-03,
          7.0477e-04, -4.9114e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5996, -3.5156, -4.5664,  ..., -0.6045,  4.8672, -3.4941]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0423, -0.1084, -0.1108,  ..., -0.0762,  0.0271, -0.0801]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 06:58:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of newcastle is in the county of northumberland
In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of birmingham is in the county of midlands
In the United Kingdom, the city of preston is in the county of lancashire
In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of liverpool is in the county of lancashire
In the United Kingdom, the city of cardiff is in the county of glamorgan
In the United Kingdom, the city of bradford is in the county of
2024-07-09 06:58:37 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of cardiff is in the county of glamorgan
In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of bradford is in the county of yorkshire
In the United Kingdom, the city of liverpool is in the county of lancashire
In the United Kingdom, the city of birmingham is in the county of midlands
In the United Kingdom, the city of newcastle is in the county of northumberland
In the United Kingdom, the city of preston is in the county of
2024-07-09 06:58:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 07:02:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 07:06:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1219, -0.5039, -0.4280,  ...,  0.0581, -0.3918, -0.0110],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1175, -0.5010, -0.3936,  ...,  0.0572, -0.3801, -0.0331],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0240, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6245, -2.1836, -6.8945,  ...,  0.3398,  3.1582, -2.7773],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0034, -0.0017, -0.0007,  ...,  0.0001,  0.0074, -0.0048],
        [-0.0096,  0.0004, -0.0017,  ...,  0.0169, -0.0154,  0.0135],
        [ 0.0215,  0.0271, -0.0169,  ...,  0.0027, -0.0024, -0.0055],
        ...,
        [-0.0168, -0.0003,  0.0196,  ...,  0.0029, -0.0251,  0.0210],
        [-0.0015, -0.0017, -0.0075,  ..., -0.0080,  0.0123, -0.0033],
        [-0.0167, -0.0035, -0.0084,  ...,  0.0004,  0.0064,  0.0031]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.4269e-04,  2.3663e-04,  1.8954e-05,  ...,  8.8120e-04,
         -1.3084e-03, -1.4853e-04],
        [-1.3552e-03,  5.8651e-04,  9.2649e-04,  ...,  1.8573e-04,
          3.8433e-04,  2.0576e-04],
        [ 3.1471e-03,  7.0381e-04, -1.7071e-03,  ...,  1.0090e-03,
         -8.9598e-04,  8.0776e-04],
        ...,
        [-1.1358e-03, -6.3121e-05,  4.5753e-04,  ..., -1.2481e-04,
         -9.1124e-04,  2.6798e-03],
        [ 4.1962e-05, -7.0691e-05, -1.2140e-03,  ..., -1.0128e-03,
         -1.6212e-05, -6.3515e-04],
        [-1.8787e-03,  5.2977e-04, -1.0386e-03,  ...,  1.1616e-03,
          1.2598e-03, -6.8378e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5405, -2.1191, -5.0938,  ...,  0.0544,  3.5508, -2.4551]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1245,  0.0047, -0.0475,  ...,  0.1226,  0.1049,  0.0541]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 07:06:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of cardiff is in the county of glamorgan
In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of bradford is in the county of yorkshire
In the United Kingdom, the city of liverpool is in the county of lancashire
In the United Kingdom, the city of birmingham is in the county of midlands
In the United Kingdom, the city of newcastle is in the county of northumberland
In the United Kingdom, the city of preston is in the county of
2024-07-09 07:06:48 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of bradford is in the county of yorkshire
In the United Kingdom, the city of preston is in the county of lancashire
In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of liverpool is in the county of lancashire
In the United Kingdom, the city of birmingham is in the county of midlands
In the United Kingdom, the city of cardiff is in the county of glamorgan
In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of newcastle is in the county of
2024-07-09 07:06:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 07:10:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 07:14:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4734,  0.0132,  0.6362,  ..., -1.4385,  0.4424, -0.7266],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.4236, -0.0049,  0.5269,  ..., -1.2168,  0.3679, -0.6802],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0240, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0527, -2.0371, -3.4238,  ...,  1.3887, -0.8760,  2.1367],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.5345e-02, -1.0460e-02, -1.5793e-02,  ..., -6.3324e-04,
          7.2098e-04, -3.1891e-03],
        [ 1.2756e-02,  1.8250e-02, -1.6556e-02,  ...,  7.7209e-03,
          1.0777e-03,  8.7738e-05],
        [-6.6109e-03,  1.5350e-02, -2.6459e-02,  ...,  3.8261e-03,
          3.9368e-03, -1.8707e-02],
        ...,
        [-3.5400e-03, -1.9577e-02,  2.6291e-02,  ...,  7.7629e-04,
          5.2834e-03,  1.9211e-02],
        [ 9.7961e-03,  1.7757e-03,  1.3840e-02,  ..., -2.8717e-02,
         -1.7380e-02,  1.2207e-04],
        [-8.1329e-03,  4.6921e-03, -1.9089e-02,  ...,  3.9597e-03,
         -3.0098e-03, -7.0763e-04]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.1172e-04,  9.6416e-04, -1.3351e-03,  ..., -1.2279e-04,
         -7.1001e-04,  2.2829e-04],
        [ 1.7190e-04,  2.0618e-03,  7.9775e-04,  ...,  1.3161e-03,
          2.6226e-04,  1.0223e-03],
        [ 2.9793e-03,  2.4109e-03, -7.2193e-04,  ...,  1.5583e-03,
         -1.4305e-05,  9.3508e-04],
        ...,
        [ 2.5749e-05, -4.3607e-04, -1.9693e-04,  ..., -5.3978e-04,
          1.6451e-04,  2.5692e-03],
        [ 1.5831e-03,  2.4681e-03,  9.1171e-04,  ...,  6.0511e-04,
         -1.1988e-03,  8.3399e-04],
        [-2.4261e-03,  1.6296e-04, -2.4319e-03,  ...,  2.9707e-04,
          4.7517e-04, -5.7507e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1318, -0.7686, -3.4258,  ...,  0.3223, -1.1064,  2.9668]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0651,  0.0717, -0.3872,  ...,  0.1792, -0.0878,  0.3513]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 07:14:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of bradford is in the county of yorkshire
In the United Kingdom, the city of preston is in the county of lancashire
In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of liverpool is in the county of lancashire
In the United Kingdom, the city of birmingham is in the county of midlands
In the United Kingdom, the city of cardiff is in the county of glamorgan
In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of newcastle is in the county of
2024-07-09 07:14:56 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of cardiff is in the county of glamorgan
In the United Kingdom, the city of newcastle is in the county of northumberland
In the United Kingdom, the city of liverpool is in the county of lancashire
In the United Kingdom, the city of preston is in the county of lancashire
In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of bradford is in the county of yorkshire
In the United Kingdom, the city of birmingham is in the county of midlands
In the United Kingdom, the city of crawley is in the county of
2024-07-09 07:14:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 07:19:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 07:22:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0922,  0.3711,  0.1074,  ...,  1.8193,  0.3369,  0.8145],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1079,  0.4163,  0.1061,  ...,  1.9873,  0.3599,  0.9224],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0240, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0430, -3.4141, -4.6797,  ..., -2.5547,  3.6777, -3.7129],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0050,  0.0141,  0.0143,  ...,  0.0028,  0.0039, -0.0059],
        [-0.0116, -0.0043, -0.0048,  ...,  0.0264, -0.0104, -0.0054],
        [-0.0220,  0.0304,  0.0018,  ..., -0.0005, -0.0257, -0.0030],
        ...,
        [ 0.0082,  0.0019,  0.0003,  ...,  0.0002, -0.0004,  0.0098],
        [-0.0078, -0.0072,  0.0106,  ...,  0.0096, -0.0135,  0.0062],
        [ 0.0091, -0.0124, -0.0232,  ..., -0.0023,  0.0188,  0.0270]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-9.3699e-05,  9.2316e-04,  2.4199e-05,  ...,  1.0386e-03,
         -7.7295e-04,  1.6356e-03],
        [-1.0452e-03, -1.2817e-03,  1.4925e-03,  ...,  5.2094e-05,
          6.6662e-04, -2.4414e-03],
        [ 4.0436e-04,  2.2297e-03, -3.8123e-04,  ..., -4.2176e-04,
         -7.4053e-04,  9.8038e-04],
        ...,
        [-3.6001e-05, -7.3338e-04, -5.2786e-04,  ..., -3.2711e-04,
          8.7690e-04,  6.9857e-05],
        [-2.9206e-04,  4.6539e-04,  1.2674e-03,  ...,  1.2341e-03,
         -2.9922e-04,  9.2804e-05],
        [ 5.4979e-04, -1.8845e-03, -9.8896e-04,  ..., -1.8654e-03,
          2.2030e-03, -5.8651e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4775, -2.1289, -3.2188,  ..., -0.9951,  3.4238, -2.3281]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1422, -0.0459, -0.1083,  ...,  0.0984, -0.0731,  0.3220]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 07:23:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of cardiff is in the county of glamorgan
In the United Kingdom, the city of newcastle is in the county of northumberland
In the United Kingdom, the city of liverpool is in the county of lancashire
In the United Kingdom, the city of preston is in the county of lancashire
In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of bradford is in the county of yorkshire
In the United Kingdom, the city of birmingham is in the county of midlands
In the United Kingdom, the city of crawley is in the county of
2024-07-09 07:23:02 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of cardiff is in the county of glamorgan
In the United Kingdom, the city of bradford is in the county of yorkshire
In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of newcastle is in the county of northumberland
In the United Kingdom, the city of birmingham is in the county of midlands
In the United Kingdom, the city of preston is in the county of lancashire
In the United Kingdom, the city of liverpool is in the county of
2024-07-09 07:23:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 07:27:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 07:31:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0518,  0.2769,  0.4333,  ..., -0.2391, -0.5620, -0.0515],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.9946,  0.2457,  0.3735,  ..., -0.2140, -0.5410, -0.0760],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0240, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3291, -2.4980, -5.4648,  ..., -1.4277,  4.6172, -4.1328],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0094,  0.0016, -0.0008,  ...,  0.0144,  0.0049,  0.0069],
        [ 0.0050,  0.0013, -0.0096,  ...,  0.0107, -0.0082,  0.0020],
        [ 0.0085,  0.0279, -0.0093,  ...,  0.0042, -0.0099,  0.0139],
        ...,
        [ 0.0026,  0.0066,  0.0069,  ...,  0.0012, -0.0068,  0.0108],
        [ 0.0095,  0.0084, -0.0093,  ..., -0.0126,  0.0055, -0.0069],
        [-0.0003, -0.0249, -0.0042,  ..., -0.0107, -0.0011,  0.0034]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 1.6403e-03,  4.9019e-04, -4.6706e-04,  ...,  1.0433e-03,
         -1.2817e-03, -3.2091e-04],
        [ 3.4070e-04,  4.6778e-04, -2.2197e-04,  ..., -6.5982e-05,
          9.8038e-04,  4.9877e-04],
        [ 7.2575e-04,  1.8339e-03, -1.0147e-03,  ..., -1.2417e-03,
         -5.5599e-04,  2.0084e-03],
        ...,
        [ 8.5258e-04,  1.0023e-03, -3.6097e-04,  ..., -1.3113e-04,
         -5.6744e-04,  1.2102e-03],
        [-5.2643e-04, -1.3418e-03,  4.4489e-04,  ...,  9.9087e-04,
         -3.0661e-04, -1.9097e-04],
        [ 8.4972e-04, -9.1934e-04, -1.3485e-03,  ..., -2.1324e-03,
          1.2245e-03,  7.6056e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0361, -1.2393, -2.3652,  ..., -2.5742,  5.0000, -3.6016]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0538,  0.1450,  0.0907,  ...,  0.1040, -0.0158,  0.1621]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 07:31:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of cardiff is in the county of glamorgan
In the United Kingdom, the city of bradford is in the county of yorkshire
In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of newcastle is in the county of northumberland
In the United Kingdom, the city of birmingham is in the county of midlands
In the United Kingdom, the city of preston is in the county of lancashire
In the United Kingdom, the city of liverpool is in the county of
2024-07-09 07:31:14 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of liverpool is in the county of lancashire
In the United Kingdom, the city of cardiff is in the county of glamorgan
In the United Kingdom, the city of preston is in the county of lancashire
In the United Kingdom, the city of bradford is in the county of yorkshire
In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of birmingham is in the county of midlands
In the United Kingdom, the city of newcastle is in the county of northumberland
In the United Kingdom, the city of portsmouth is in the county of
2024-07-09 07:31:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 07:35:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 07:39:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7031, -0.3206, -0.9946,  ...,  0.1483, -0.8364,  0.6426],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.7188, -0.3513, -0.9741,  ...,  0.1470, -0.8555,  0.6401],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0240, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8262,  1.3535, -5.0742,  ..., -4.9375,  4.3164, -0.1782],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0010, -0.0160, -0.0137,  ...,  0.0048,  0.0039,  0.0012],
        [-0.0137,  0.0008, -0.0019,  ...,  0.0091, -0.0100,  0.0103],
        [ 0.0450, -0.0117, -0.0137,  ...,  0.0095, -0.0014, -0.0145],
        ...,
        [ 0.0334, -0.0138, -0.0119,  ..., -0.0118, -0.0163,  0.0192],
        [-0.0121,  0.0128, -0.0075,  ..., -0.0187, -0.0102, -0.0041],
        [-0.0238,  0.0111,  0.0101,  ...,  0.0147,  0.0071, -0.0165]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.3127e-05,  1.0576e-03,  1.9205e-04,  ..., -7.6962e-04,
         -5.0354e-04, -1.4067e-03],
        [-6.7902e-04,  8.5688e-04,  5.1737e-04,  ..., -4.5705e-04,
         -2.4071e-03, -2.4242e-03],
        [ 3.4981e-03, -1.0681e-03, -3.6507e-03,  ...,  2.0828e-03,
          2.0828e-03, -1.1482e-03],
        ...,
        [ 2.2240e-03,  1.2302e-03, -2.4338e-03,  ...,  1.8940e-03,
          2.5043e-03,  1.7376e-03],
        [-5.9605e-06,  1.7309e-03,  3.1948e-04,  ...,  5.7364e-04,
          8.9359e-04,  1.2028e-04],
        [-2.4357e-03,  3.9864e-04,  2.3282e-04,  ...,  5.1975e-04,
          9.4271e-04, -2.1095e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1973,  0.1777, -2.9492,  ..., -4.1562,  4.2852, -1.0430]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1912, -0.0230, -0.0363,  ..., -0.0103,  0.1185, -0.0223]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 07:39:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of liverpool is in the county of lancashire
In the United Kingdom, the city of cardiff is in the county of glamorgan
In the United Kingdom, the city of preston is in the county of lancashire
In the United Kingdom, the city of bradford is in the county of yorkshire
In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of birmingham is in the county of midlands
In the United Kingdom, the city of newcastle is in the county of northumberland
In the United Kingdom, the city of portsmouth is in the county of
2024-07-09 07:39:21 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of cardiff is in the county of glamorgan
In the United Kingdom, the city of newcastle is in the county of northumberland
In the United Kingdom, the city of liverpool is in the county of lancashire
In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of bradford is in the county of yorkshire
In the United Kingdom, the city of preston is in the county of lancashire
In the United Kingdom, the city of birmingham is in the county of
2024-07-09 07:39:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 07:43:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 07:47:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-7.1680e-01, -3.9600e-01,  6.5674e-02,  ...,  2.2168e-01,
         6.1035e-04, -5.6543e-01], device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.6470, -0.3816,  0.0428,  ...,  0.1918, -0.0200, -0.5410],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0240, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6147, -4.7500, -7.0820,  ..., -1.0156,  0.5088, -1.7900],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0242, -0.0025, -0.0055,  ...,  0.0119,  0.0055, -0.0125],
        [ 0.0131,  0.0043, -0.0169,  ..., -0.0063, -0.0054, -0.0117],
        [ 0.0334,  0.0433, -0.0059,  ..., -0.0222, -0.0153, -0.0272],
        ...,
        [ 0.0032,  0.0048,  0.0067,  ..., -0.0030,  0.0081, -0.0049],
        [ 0.0180,  0.0040, -0.0092,  ..., -0.0188,  0.0010, -0.0150],
        [-0.0036, -0.0175,  0.0028,  ...,  0.0076,  0.0096, -0.0054]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 3.7479e-04, -1.6394e-03,  2.7299e-04,  ...,  1.1177e-03,
         -1.5869e-03, -1.4954e-03],
        [-1.0118e-03,  1.2770e-03,  7.9536e-04,  ..., -3.0637e-04,
          6.3515e-04, -1.8597e-05],
        [ 1.8444e-03,  2.4166e-03, -2.2144e-03,  ...,  2.7132e-04,
         -1.4324e-03,  1.2789e-03],
        ...,
        [-2.9087e-04,  2.2614e-04,  1.5888e-03,  ...,  9.4700e-04,
          4.8709e-04,  1.3266e-03],
        [ 4.6062e-04,  6.8188e-04, -4.6158e-04,  ..., -1.1343e-04,
         -1.4305e-06,  5.2261e-04],
        [ 2.4652e-04,  3.1710e-04,  3.2783e-04,  ...,  1.0353e-04,
          5.7507e-04, -8.2302e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5107, -3.2812, -4.2969,  ..., -1.5596,  2.0273, -1.8994]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0864, -0.0041, -0.0462,  ..., -0.1680, -0.1113,  0.0430]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 07:47:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of cardiff is in the county of glamorgan
In the United Kingdom, the city of newcastle is in the county of northumberland
In the United Kingdom, the city of liverpool is in the county of lancashire
In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of bradford is in the county of yorkshire
In the United Kingdom, the city of preston is in the county of lancashire
In the United Kingdom, the city of birmingham is in the county of
2024-07-09 07:47:28 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of liverpool is in the county of lancashire
In the United Kingdom, the city of birmingham is in the county of midlands
In the United Kingdom, the city of preston is in the county of lancashire
In the United Kingdom, the city of newcastle is in the county of northumberland
In the United Kingdom, the city of bradford is in the county of yorkshire
In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of cardiff is in the county of
2024-07-09 07:47:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 07:51:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 07:55:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3354,  0.3696, -0.6836,  ...,  0.0994, -0.6602, -0.7632],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.3135,  0.3367, -0.6147,  ...,  0.0945, -0.6201, -0.7441],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0240, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3931,  3.4551, -1.9551,  ...,  1.1982,  1.3730, -2.7305],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.6739e-02, -1.3290e-02, -1.5129e-02,  ..., -8.5144e-03,
         -5.4474e-03, -2.2675e-02],
        [-1.1536e-02,  9.9258e-03, -1.7136e-02,  ...,  8.4991e-03,
          6.0310e-03,  1.8555e-02],
        [ 2.6016e-03,  5.9052e-03, -1.7715e-02,  ...,  6.4850e-05,
         -6.4888e-03, -1.4999e-02],
        ...,
        [-6.8665e-05, -1.0574e-02,  4.8065e-03,  ..., -2.0905e-03,
         -9.7885e-03, -3.4561e-03],
        [ 1.4992e-02,  4.6997e-03,  1.9379e-03,  ..., -3.0243e-02,
         -6.5689e-03,  1.7643e-03],
        [ 5.3711e-03, -1.6937e-02, -9.0332e-03,  ...,  7.9498e-03,
          7.0343e-03, -6.1302e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 7.1192e-04,  8.0109e-04,  2.0981e-05,  ..., -2.5330e-03,
         -2.4300e-03, -3.1052e-03],
        [ 3.8385e-04,  1.9550e-03,  1.7309e-04,  ..., -8.0252e-04,
          2.2745e-04, -6.5279e-04],
        [-9.9182e-04,  3.0637e-05, -9.2983e-04,  ...,  2.9850e-04,
         -1.0757e-03,  7.5579e-04],
        ...,
        [ 1.3504e-03, -1.0138e-03,  5.9271e-04,  ...,  4.2295e-04,
         -4.1747e-04,  1.5259e-04],
        [-4.2915e-05,  2.1863e-04,  5.5885e-04,  ...,  1.7815e-03,
         -3.1242e-03,  1.9588e-03],
        [ 1.3752e-03, -1.5593e-03, -1.3876e-03,  ...,  1.8644e-04,
          1.9169e-03,  1.6642e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1082,  2.0469, -1.3096,  ..., -0.0449,  3.0449, -2.5918]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0966, -0.0980, -0.0398,  ...,  0.0121, -0.1428,  0.0367]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 07:55:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of liverpool is in the county of lancashire
In the United Kingdom, the city of birmingham is in the county of midlands
In the United Kingdom, the city of preston is in the county of lancashire
In the United Kingdom, the city of newcastle is in the county of northumberland
In the United Kingdom, the city of bradford is in the county of yorkshire
In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of cardiff is in the county of
2024-07-09 07:55:36 root INFO     total operator prediction time: 3906.8150115013123 seconds
2024-07-09 07:55:36 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on hypernyms - misc
2024-07-09 07:55:38 root INFO     building operator hypernyms - misc
2024-07-09 07:55:39 root INFO     [order_1_approx] starting weight calculation for The photo falls into the category of picture
The sofa falls into the category of furniture
The diary falls into the category of journal
The postcard falls into the category of card
The plum falls into the category of fruit
The notebook falls into the category of book
The sweater falls into the category of clothes
The tv falls into the category of
2024-07-09 07:55:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 07:59:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 08:03:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6689,  0.6846, -0.2708,  ...,  1.5781,  0.1021, -0.1250],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.5908,  0.6011, -0.2366,  ...,  1.3350,  0.0715, -0.1339],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.3809, -0.4106, -0.1333,  ..., -2.3828, -1.7148,  1.1133],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.5541e-02,  8.0872e-03, -4.4327e-03,  ..., -8.2169e-03,
         -9.4147e-03,  1.1040e-02],
        [-9.3842e-03,  1.9951e-03,  7.8735e-03,  ..., -9.3460e-04,
         -6.8665e-04, -5.7487e-03],
        [-2.0065e-03,  1.2024e-02, -7.4844e-03,  ...,  5.4207e-03,
         -5.0964e-03,  1.0353e-02],
        ...,
        [-8.6136e-03, -3.6793e-03,  4.0436e-03,  ...,  7.6294e-05,
         -5.8708e-03, -4.6005e-03],
        [-5.0354e-03,  5.8174e-03,  5.3329e-03,  ..., -7.5760e-03,
         -5.0430e-03, -5.5618e-03],
        [ 9.7036e-04,  2.2087e-03,  9.3079e-04,  ..., -1.0017e-02,
         -4.6806e-03,  3.4885e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.9073e-04,  9.3818e-05,  2.5425e-03,  ...,  1.4567e-04,
         -1.2779e-03,  1.9798e-03],
        [-3.6526e-04, -9.4080e-04,  1.5736e-03,  ...,  7.3814e-04,
          3.4523e-04, -1.0672e-03],
        [ 5.5027e-04, -6.9809e-04,  1.0099e-03,  ...,  2.6474e-03,
          1.2064e-03,  2.2912e-04],
        ...,
        [ 9.1219e-04,  8.4543e-04,  1.3018e-03,  ..., -4.2152e-04,
          7.7438e-04, -2.9325e-04],
        [ 1.3275e-03,  9.8896e-04,  4.5800e-04,  ..., -3.5930e-04,
         -2.8110e-04, -1.2894e-03],
        [-1.6510e-04,  9.0599e-04,  1.1158e-04,  ..., -2.7466e-04,
         -1.6165e-04,  1.7822e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.9258,  0.6206, -0.9248,  ..., -2.8320, -2.7188,  1.5459]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0916, -0.1991, -0.1526,  ..., -0.0542, -0.0181,  0.0467]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 08:03:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The photo falls into the category of picture
The sofa falls into the category of furniture
The diary falls into the category of journal
The postcard falls into the category of card
The plum falls into the category of fruit
The notebook falls into the category of book
The sweater falls into the category of clothes
The tv falls into the category of
2024-07-09 08:03:50 root INFO     [order_1_approx] starting weight calculation for The sofa falls into the category of furniture
The plum falls into the category of fruit
The tv falls into the category of device
The postcard falls into the category of card
The notebook falls into the category of book
The photo falls into the category of picture
The diary falls into the category of journal
The sweater falls into the category of
2024-07-09 08:03:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 08:07:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 08:11:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1259,  0.0852, -0.9141,  ...,  0.9771,  1.0928, -0.0597],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1122,  0.0529, -0.7437,  ...,  0.7847,  0.8794, -0.0775],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.1172, -1.0273,  0.7075,  ..., -2.9062, -2.5215, -4.4453],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0088, -0.0127, -0.0066,  ...,  0.0101, -0.0119,  0.0111],
        [-0.0045, -0.0063,  0.0081,  ..., -0.0015, -0.0071, -0.0112],
        [-0.0036,  0.0032, -0.0148,  ..., -0.0009,  0.0007,  0.0064],
        ...,
        [-0.0109,  0.0054,  0.0028,  ...,  0.0022,  0.0003,  0.0121],
        [ 0.0053,  0.0073, -0.0055,  ...,  0.0090, -0.0034, -0.0017],
        [-0.0182, -0.0001,  0.0001,  ..., -0.0038,  0.0036,  0.0100]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 1.8015e-03, -2.5520e-03,  8.7166e-04,  ...,  1.0433e-03,
          4.9591e-04, -1.6475e-04],
        [-1.1473e-03, -1.6642e-03,  2.4376e-03,  ..., -4.8876e-05,
          8.4209e-04, -6.9904e-04],
        [ 6.5470e-04, -8.9741e-04, -1.1206e-03,  ...,  1.5020e-05,
          6.4898e-04, -8.2111e-04],
        ...,
        [ 1.0853e-03,  4.3035e-04,  7.4673e-04,  ..., -1.6022e-03,
          1.1826e-03,  1.1797e-03],
        [ 7.6652e-05,  1.5030e-03, -1.7157e-03,  ..., -9.5177e-04,
         -2.2697e-03,  2.1210e-03],
        [ 3.3402e-04, -1.7333e-04,  2.8849e-04,  ..., -6.1512e-04,
         -2.4629e-04, -1.5364e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.9004, -1.1787, -0.0254,  ..., -3.0645, -3.1582, -3.2852]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1658, -0.0967,  0.1254,  ..., -0.0414,  0.1431, -0.1379]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 08:11:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sofa falls into the category of furniture
The plum falls into the category of fruit
The tv falls into the category of device
The postcard falls into the category of card
The notebook falls into the category of book
The photo falls into the category of picture
The diary falls into the category of journal
The sweater falls into the category of
2024-07-09 08:11:57 root INFO     [order_1_approx] starting weight calculation for The diary falls into the category of journal
The notebook falls into the category of book
The sweater falls into the category of clothes
The photo falls into the category of picture
The sofa falls into the category of furniture
The tv falls into the category of device
The plum falls into the category of fruit
The postcard falls into the category of
2024-07-09 08:11:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 08:16:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 08:19:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2422,  0.5308, -0.7246,  ..., -1.0967,  0.7441,  0.8105],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-1.0205,  0.4294, -0.5757,  ..., -0.8550,  0.5830,  0.6548],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.7148,  0.2527, -4.8438,  ..., -1.4893,  0.3623,  2.0449],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0205, -0.0011,  0.0009,  ..., -0.0042, -0.0116, -0.0026],
        [-0.0050,  0.0039,  0.0064,  ...,  0.0123, -0.0033,  0.0051],
        [ 0.0019, -0.0091,  0.0017,  ...,  0.0016, -0.0027,  0.0009],
        ...,
        [-0.0092, -0.0028, -0.0006,  ...,  0.0015, -0.0111,  0.0016],
        [-0.0121, -0.0014,  0.0075,  ..., -0.0002, -0.0029,  0.0006],
        [-0.0135, -0.0106, -0.0034,  ..., -0.0006,  0.0066,  0.0010]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 1.1945e-04,  5.1212e-04,  4.5443e-04,  ...,  7.5054e-04,
          6.7139e-04, -3.2783e-05],
        [-1.3995e-04,  8.0049e-05,  4.9925e-04,  ...,  5.1737e-04,
         -3.8028e-04,  4.1151e-04],
        [ 1.1177e-03, -4.6849e-05, -6.7043e-04,  ...,  8.1682e-04,
          1.2875e-04, -9.3460e-04],
        ...,
        [ 2.1112e-04,  2.9254e-04, -7.2384e-04,  ..., -3.5143e-04,
          4.4203e-04, -1.7500e-04],
        [ 2.4271e-04, -5.1451e-04, -5.6934e-04,  ..., -3.4404e-04,
         -1.0643e-03, -9.6941e-04],
        [-1.4524e-03,  9.0957e-05,  7.2420e-05,  ..., -7.8487e-04,
         -2.1057e-03,  7.6866e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.5391,  0.1667, -5.4258,  ..., -1.8330,  0.2925,  1.6094]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1857, -0.0441,  0.0104,  ...,  0.0655,  0.0393,  0.0654]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 08:20:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The diary falls into the category of journal
The notebook falls into the category of book
The sweater falls into the category of clothes
The photo falls into the category of picture
The sofa falls into the category of furniture
The tv falls into the category of device
The plum falls into the category of fruit
The postcard falls into the category of
2024-07-09 08:20:01 root INFO     [order_1_approx] starting weight calculation for The tv falls into the category of device
The plum falls into the category of fruit
The notebook falls into the category of book
The sofa falls into the category of furniture
The sweater falls into the category of clothes
The postcard falls into the category of card
The photo falls into the category of picture
The diary falls into the category of
2024-07-09 08:20:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 08:24:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 08:28:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5088, -0.4683,  0.2759,  ...,  0.1373, -0.1084,  0.7686],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.4619, -0.4485,  0.2242,  ...,  0.1213, -0.1152,  0.6797],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.2734, -0.1747, -0.0098,  ...,  2.1367,  1.3896,  0.5029],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0250, -0.0067,  0.0080,  ..., -0.0080, -0.0038, -0.0005],
        [-0.0143,  0.0094,  0.0041,  ...,  0.0080, -0.0014,  0.0047],
        [ 0.0045,  0.0095, -0.0011,  ..., -0.0065, -0.0009,  0.0095],
        ...,
        [-0.0182, -0.0106,  0.0239,  ...,  0.0003, -0.0014,  0.0022],
        [ 0.0212, -0.0109,  0.0017,  ..., -0.0059,  0.0004,  0.0008],
        [-0.0094, -0.0024,  0.0014,  ..., -0.0046,  0.0080,  0.0122]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.4210e-03,  1.1593e-04,  8.9264e-04,  ..., -1.0033e-03,
          2.6608e-04, -1.5187e-04],
        [-1.0061e-03, -1.8616e-03,  1.2083e-03,  ...,  1.5488e-03,
          7.9870e-04, -2.0409e-03],
        [ 2.7895e-04,  1.8673e-03,  2.6083e-04,  ...,  3.2759e-04,
          6.1607e-04, -2.1040e-04],
        ...,
        [-1.4782e-03, -3.4356e-04,  2.1248e-03,  ..., -1.5278e-03,
          1.4954e-03,  4.9400e-04],
        [ 9.5510e-04, -5.7697e-05, -4.7445e-04,  ..., -3.8171e-04,
         -1.4629e-03,  6.8426e-04],
        [-7.9727e-04, -1.6773e-04, -3.0375e-04,  ..., -5.8270e-04,
         -8.2922e-04,  3.3665e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.7188,  0.1049, -0.3103,  ...,  1.4463,  1.3242,  0.6489]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0180, -0.2576,  0.0119,  ...,  0.1080, -0.0635,  0.1433]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 08:28:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The tv falls into the category of device
The plum falls into the category of fruit
The notebook falls into the category of book
The sofa falls into the category of furniture
The sweater falls into the category of clothes
The postcard falls into the category of card
The photo falls into the category of picture
The diary falls into the category of
2024-07-09 08:28:08 root INFO     [order_1_approx] starting weight calculation for The postcard falls into the category of card
The photo falls into the category of picture
The sofa falls into the category of furniture
The tv falls into the category of device
The sweater falls into the category of clothes
The notebook falls into the category of book
The diary falls into the category of journal
The plum falls into the category of
2024-07-09 08:28:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 08:32:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 08:36:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7632, -0.1534, -0.9839,  ..., -0.7900, -2.1191,  0.0804],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.6636, -0.1537, -0.8218,  ..., -0.6499, -1.8203,  0.0461],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7559, -0.8477, -5.2344,  ...,  1.0156, -2.3008,  0.9072],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0170,  0.0116,  0.0028,  ...,  0.0025, -0.0081, -0.0095],
        [-0.0031, -0.0024,  0.0089,  ...,  0.0046, -0.0132, -0.0021],
        [-0.0078,  0.0006, -0.0066,  ...,  0.0002, -0.0019, -0.0028],
        ...,
        [-0.0124,  0.0004,  0.0056,  ...,  0.0035, -0.0104, -0.0061],
        [ 0.0061,  0.0076,  0.0004,  ...,  0.0029,  0.0034, -0.0055],
        [-0.0042, -0.0025,  0.0108,  ...,  0.0030,  0.0081,  0.0036]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0005,  0.0026,  0.0016,  ..., -0.0005, -0.0002,  0.0018],
        [-0.0002, -0.0009,  0.0008,  ...,  0.0012, -0.0010, -0.0014],
        [ 0.0003,  0.0009,  0.0002,  ...,  0.0012,  0.0003, -0.0008],
        ...,
        [ 0.0009,  0.0014,  0.0011,  ...,  0.0003,  0.0012,  0.0002],
        [ 0.0001,  0.0019, -0.0010,  ..., -0.0009, -0.0013,  0.0014],
        [ 0.0003, -0.0002,  0.0006,  ..., -0.0008, -0.0001,  0.0004]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.4551, -0.8413, -5.1992,  ...,  0.6855, -2.3535,  1.4395]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1692, -0.1124, -0.0468,  ..., -0.0616,  0.2122,  0.0327]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 08:36:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The postcard falls into the category of card
The photo falls into the category of picture
The sofa falls into the category of furniture
The tv falls into the category of device
The sweater falls into the category of clothes
The notebook falls into the category of book
The diary falls into the category of journal
The plum falls into the category of
2024-07-09 08:36:16 root INFO     [order_1_approx] starting weight calculation for The photo falls into the category of picture
The notebook falls into the category of book
The plum falls into the category of fruit
The diary falls into the category of journal
The sweater falls into the category of clothes
The tv falls into the category of device
The postcard falls into the category of card
The sofa falls into the category of
2024-07-09 08:36:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 08:40:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 08:44:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2886, -0.8057, -0.7173,  ...,  0.4666,  0.1641, -0.4685],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.2382, -0.7100, -0.5859,  ...,  0.3787,  0.1166, -0.4250],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.4766, -1.9473, -0.9180,  ..., -2.0039, -3.2578, -2.3223],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0254, -0.0048, -0.0184,  ..., -0.0048,  0.0087,  0.0120],
        [-0.0071,  0.0071,  0.0078,  ..., -0.0038, -0.0024,  0.0030],
        [-0.0061, -0.0021, -0.0132,  ..., -0.0002,  0.0028,  0.0024],
        ...,
        [-0.0121, -0.0119,  0.0100,  ...,  0.0017,  0.0032,  0.0037],
        [ 0.0052,  0.0031, -0.0054,  ...,  0.0045, -0.0019,  0.0016],
        [-0.0015,  0.0050, -0.0069,  ..., -0.0083,  0.0028,  0.0146]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 2.3246e-04, -1.3647e-03,  2.7390e-03,  ..., -3.8648e-04,
          6.9141e-04,  9.7752e-06],
        [-4.7827e-04, -1.2760e-03,  1.9798e-03,  ...,  1.2856e-03,
          1.0222e-04, -2.8563e-04],
        [-6.8665e-04,  9.4175e-05, -3.2544e-04,  ...,  7.6246e-04,
         -7.0333e-04,  2.8324e-04],
        ...,
        [ 1.6391e-05, -2.9421e-04,  1.3447e-04,  ..., -1.2331e-03,
          1.3027e-03, -5.2643e-04],
        [ 2.0103e-03,  1.3304e-03,  1.7490e-03,  ..., -1.4801e-03,
         -1.2178e-03,  6.5565e-05],
        [-2.0492e-04,  9.7513e-05,  9.8801e-04,  ..., -3.0613e-04,
          3.1972e-04, -1.3137e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2529, -1.9766, -1.5859,  ..., -2.2852, -4.0664, -2.2773]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0072, -0.2209, -0.0377,  ...,  0.1797, -0.0293,  0.0102]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 08:44:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The photo falls into the category of picture
The notebook falls into the category of book
The plum falls into the category of fruit
The diary falls into the category of journal
The sweater falls into the category of clothes
The tv falls into the category of device
The postcard falls into the category of card
The sofa falls into the category of
2024-07-09 08:44:28 root INFO     [order_1_approx] starting weight calculation for The sofa falls into the category of furniture
The diary falls into the category of journal
The plum falls into the category of fruit
The tv falls into the category of device
The postcard falls into the category of card
The sweater falls into the category of clothes
The photo falls into the category of picture
The notebook falls into the category of
2024-07-09 08:44:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 08:48:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 08:52:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5420, -1.0752, -0.1167,  ...,  0.9170,  0.1355,  0.3098],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.4731, -0.9922, -0.1118,  ...,  0.7778,  0.0971,  0.2532],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.9082, -0.2319, -0.0508,  ...,  3.7090,  1.2695, -1.0000],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0207, -0.0010,  0.0020,  ..., -0.0036, -0.0022, -0.0037],
        [-0.0225,  0.0123,  0.0078,  ...,  0.0106,  0.0033,  0.0035],
        [ 0.0099,  0.0128, -0.0188,  ..., -0.0033,  0.0019,  0.0085],
        ...,
        [-0.0451, -0.0109,  0.0215,  ..., -0.0102, -0.0055,  0.0098],
        [ 0.0005, -0.0104, -0.0009,  ..., -0.0091, -0.0087,  0.0139],
        [-0.0244, -0.0263, -0.0007,  ...,  0.0090,  0.0108,  0.0066]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.3819e-03, -2.7227e-04,  1.5974e-03,  ..., -2.5511e-05,
          6.0177e-04, -8.1158e-04],
        [-2.4929e-03, -2.4681e-03,  2.7943e-03,  ...,  1.3113e-03,
          2.1439e-03, -2.7466e-03],
        [ 1.5507e-03,  2.6627e-03, -2.3136e-03,  ..., -7.7868e-04,
         -5.2023e-04, -3.0613e-04],
        ...,
        [-1.7271e-03, -2.2259e-03,  1.6165e-03,  ..., -1.6232e-03,
          1.2484e-03, -3.0041e-04],
        [ 2.9392e-03,  3.3379e-04, -2.4529e-03,  ..., -1.0262e-03,
         -1.9331e-03,  2.2125e-03],
        [-4.8089e-04, -9.8801e-04, -1.6899e-03,  ..., -7.3576e-04,
         -1.1539e-03,  7.9060e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.1758, -0.3755,  0.1754,  ...,  1.9961,  0.7612, -0.8037]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1967, -0.2800, -0.0333,  ...,  0.0155,  0.0846,  0.1705]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 08:52:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sofa falls into the category of furniture
The diary falls into the category of journal
The plum falls into the category of fruit
The tv falls into the category of device
The postcard falls into the category of card
The sweater falls into the category of clothes
The photo falls into the category of picture
The notebook falls into the category of
2024-07-09 08:52:39 root INFO     [order_1_approx] starting weight calculation for The notebook falls into the category of book
The postcard falls into the category of card
The plum falls into the category of fruit
The tv falls into the category of device
The diary falls into the category of journal
The sweater falls into the category of clothes
The sofa falls into the category of furniture
The photo falls into the category of
2024-07-09 08:52:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 08:56:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 09:00:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4402,  0.7246,  1.2012,  ..., -0.1366,  0.3525, -0.5625],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.4014,  0.6533,  1.0234,  ..., -0.1139,  0.2954, -0.5415],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.3359,  1.2031, -2.4805,  ..., -1.1025, -1.1523,  1.5684],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0184,  0.0002, -0.0031,  ..., -0.0061,  0.0041, -0.0066],
        [-0.0073,  0.0149,  0.0036,  ...,  0.0047, -0.0189,  0.0098],
        [ 0.0012,  0.0067, -0.0163,  ...,  0.0010,  0.0035, -0.0049],
        ...,
        [-0.0154, -0.0037,  0.0103,  ..., -0.0033, -0.0016, -0.0006],
        [-0.0021,  0.0036, -0.0042,  ..., -0.0043,  0.0002, -0.0108],
        [-0.0177, -0.0024, -0.0050,  ..., -0.0049,  0.0078,  0.0157]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0006,  0.0004,  0.0008,  ...,  0.0001,  0.0014,  0.0008],
        [-0.0012,  0.0001,  0.0014,  ...,  0.0010, -0.0005, -0.0011],
        [ 0.0003, -0.0002, -0.0004,  ...,  0.0005,  0.0002, -0.0017],
        ...,
        [ 0.0014, -0.0018, -0.0002,  ..., -0.0024,  0.0013, -0.0010],
        [ 0.0008, -0.0001, -0.0004,  ...,  0.0011, -0.0013, -0.0011],
        [-0.0005,  0.0016,  0.0006,  ...,  0.0007, -0.0008,  0.0013]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.2500,  1.4912, -2.7461,  ..., -1.2617, -1.1562,  1.4180]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.2649, -0.1417, -0.1049,  ..., -0.3018, -0.2620,  0.3406]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 09:00:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The notebook falls into the category of book
The postcard falls into the category of card
The plum falls into the category of fruit
The tv falls into the category of device
The diary falls into the category of journal
The sweater falls into the category of clothes
The sofa falls into the category of furniture
The photo falls into the category of
2024-07-09 09:00:48 root INFO     total operator prediction time: 3909.824850797653 seconds
2024-07-09 09:00:48 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - part
2024-07-09 09:00:50 root INFO     building operator meronyms - part
2024-07-09 09:00:51 root INFO     [order_1_approx] starting weight calculation for A part of a guitar is a string
A part of a sword is a blade
A part of a window is a pane
A part of a gun is a trigger
A part of a church is a altar
A part of a womb is a cervix
A part of a teapot is a spout
A part of a deer is a
2024-07-09 09:00:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 09:05:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 09:08:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5156,  0.1865, -0.3892,  ...,  0.7520,  0.0146,  0.3647],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.4739,  0.1602, -0.3540,  ...,  0.6704, -0.0047,  0.3210],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8789,  1.6074, -3.0625,  ..., -2.7988, -1.4844,  3.7988],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0064, -0.0166,  0.0074,  ...,  0.0278,  0.0162, -0.0073],
        [-0.0106,  0.0196, -0.0006,  ...,  0.0226, -0.0115, -0.0084],
        [ 0.0130,  0.0049,  0.0094,  ..., -0.0230,  0.0193, -0.0072],
        ...,
        [-0.0082, -0.0074, -0.0029,  ...,  0.0159, -0.0162,  0.0140],
        [ 0.0158,  0.0026,  0.0026,  ...,  0.0098,  0.0186,  0.0189],
        [ 0.0059,  0.0087, -0.0076,  ..., -0.0130,  0.0123,  0.0150]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0010,  0.0009,  0.0005,  ..., -0.0010, -0.0004,  0.0022],
        [ 0.0009,  0.0019,  0.0009,  ...,  0.0002, -0.0009, -0.0008],
        [ 0.0021,  0.0020, -0.0018,  ..., -0.0023,  0.0009, -0.0016],
        ...,
        [-0.0011, -0.0016,  0.0017,  ...,  0.0014,  0.0004,  0.0032],
        [-0.0002,  0.0002,  0.0013,  ...,  0.0007,  0.0004,  0.0005],
        [-0.0008, -0.0010, -0.0003,  ...,  0.0005, -0.0003, -0.0013]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5410,  1.7090, -2.1816,  ..., -2.9688, -1.2715,  4.0508]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1677, -0.1335, -0.2318,  ..., -0.1066, -0.0626, -0.0005]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 09:09:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a guitar is a string
A part of a sword is a blade
A part of a window is a pane
A part of a gun is a trigger
A part of a church is a altar
A part of a womb is a cervix
A part of a teapot is a spout
A part of a deer is a
2024-07-09 09:09:02 root INFO     [order_1_approx] starting weight calculation for A part of a gun is a trigger
A part of a deer is a antler
A part of a church is a altar
A part of a guitar is a string
A part of a window is a pane
A part of a sword is a blade
A part of a womb is a cervix
A part of a teapot is a
2024-07-09 09:09:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 09:13:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 09:17:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0767, -0.5352,  0.0400,  ...,  0.0403, -0.3159,  0.5713],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.0694, -0.5107,  0.0273,  ...,  0.0429, -0.2971,  0.5098],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7500,  2.0469, -0.2148,  ..., -3.4395,  2.0137,  3.7930],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0039, -0.0127,  0.0003,  ...,  0.0042,  0.0015, -0.0070],
        [-0.0117,  0.0042,  0.0067,  ...,  0.0152,  0.0076,  0.0053],
        [ 0.0063,  0.0165,  0.0066,  ..., -0.0066,  0.0165,  0.0099],
        ...,
        [ 0.0002,  0.0148, -0.0017,  ..., -0.0121, -0.0170, -0.0022],
        [-0.0023,  0.0008,  0.0189,  ...,  0.0158, -0.0075,  0.0029],
        [-0.0333,  0.0004, -0.0029,  ...,  0.0012,  0.0247,  0.0126]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-8.1396e-04,  8.2016e-05,  9.8133e-04,  ...,  3.9458e-04,
         -5.4264e-04,  1.1921e-03],
        [-1.0271e-03, -4.5276e-04,  1.1271e-04,  ...,  1.4696e-03,
         -1.1845e-03,  3.0446e-04],
        [ 2.6264e-03, -1.3199e-03, -1.0067e-04,  ..., -8.0776e-04,
         -5.2738e-04, -1.0471e-03],
        ...,
        [-9.1410e-04, -5.3883e-04, -1.6232e-03,  ..., -1.8702e-03,
         -4.3917e-04,  3.8433e-04],
        [ 1.3142e-03,  3.5596e-04,  1.1673e-03,  ...,  6.5088e-04,
         -1.5831e-03,  2.0432e-04],
        [-3.3684e-03, -2.7561e-03, -1.1516e-04,  ..., -9.5463e-04,
          1.2779e-03, -5.7793e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.8760,  1.1660,  0.3472,  ..., -3.6836,  1.6973,  3.7969]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1998,  0.1589,  0.0649,  ...,  0.0245, -0.1700,  0.1008]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 09:17:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a gun is a trigger
A part of a deer is a antler
A part of a church is a altar
A part of a guitar is a string
A part of a window is a pane
A part of a sword is a blade
A part of a womb is a cervix
A part of a teapot is a
2024-07-09 09:17:12 root INFO     [order_1_approx] starting weight calculation for A part of a church is a altar
A part of a deer is a antler
A part of a womb is a cervix
A part of a teapot is a spout
A part of a gun is a trigger
A part of a window is a pane
A part of a guitar is a string
A part of a sword is a
2024-07-09 09:17:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 09:21:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 09:25:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6104,  0.1991, -0.1721,  ...,  0.7598, -0.1146,  1.2070],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.5664,  0.1729, -0.1652,  ...,  0.6831, -0.1246,  1.1260],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.7148, -0.3989, -1.2773,  ..., -1.6133, -1.7715, -1.1992],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0008, -0.0342,  0.0102,  ...,  0.0120, -0.0033, -0.0446],
        [-0.0059,  0.0105,  0.0015,  ...,  0.0075, -0.0129,  0.0042],
        [ 0.0065,  0.0050,  0.0007,  ..., -0.0051,  0.0152, -0.0100],
        ...,
        [ 0.0004,  0.0056,  0.0036,  ...,  0.0039, -0.0203, -0.0045],
        [ 0.0128, -0.0183,  0.0090,  ...,  0.0315, -0.0177, -0.0085],
        [ 0.0099, -0.0176,  0.0106,  ..., -0.0523,  0.0052, -0.0035]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.9159e-04, -1.6346e-03, -4.7302e-04,  ...,  7.0047e-04,
          7.2479e-05,  1.7166e-04],
        [-1.6499e-03,  9.7847e-04,  1.5020e-03,  ...,  5.6458e-04,
         -2.3537e-03,  2.4967e-03],
        [ 8.2207e-04,  3.9101e-05,  5.2023e-04,  ...,  1.5659e-03,
          1.3084e-03, -1.4114e-03],
        ...,
        [-5.0354e-04, -3.6240e-05,  1.8444e-03,  ..., -9.2459e-04,
         -1.6308e-03, -1.8921e-03],
        [ 2.7561e-03, -1.0967e-03,  1.6651e-03,  ...,  2.1877e-03,
         -7.1526e-04, -2.2583e-03],
        [-1.3943e-03, -2.1629e-03,  2.1207e-04,  ..., -8.1682e-04,
         -3.7193e-05, -2.4700e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.2754, -1.4238, -0.1768,  ..., -1.5049,  0.0615, -0.2192]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2252,  0.0742, -0.0918,  ..., -0.0956, -0.0012, -0.0036]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 09:25:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a church is a altar
A part of a deer is a antler
A part of a womb is a cervix
A part of a teapot is a spout
A part of a gun is a trigger
A part of a window is a pane
A part of a guitar is a string
A part of a sword is a
2024-07-09 09:25:22 root INFO     [order_1_approx] starting weight calculation for A part of a womb is a cervix
A part of a gun is a trigger
A part of a sword is a blade
A part of a guitar is a string
A part of a deer is a antler
A part of a teapot is a spout
A part of a church is a altar
A part of a window is a
2024-07-09 09:25:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 09:29:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 09:33:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9199, -0.3032,  0.3159,  ...,  0.8369, -0.6250,  0.7075],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.8438, -0.3015,  0.2668,  ...,  0.7422, -0.5835,  0.6406],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7930,  3.8867,  0.5640,  ..., -4.6328, -4.7930,  6.1602],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0005, -0.0214, -0.0024,  ...,  0.0039,  0.0105, -0.0115],
        [-0.0053,  0.0036, -0.0042,  ...,  0.0098, -0.0148,  0.0065],
        [ 0.0105,  0.0124, -0.0092,  ..., -0.0124,  0.0079,  0.0127],
        ...,
        [ 0.0031,  0.0050, -0.0100,  ...,  0.0171, -0.0004,  0.0033],
        [-0.0008, -0.0057,  0.0162,  ...,  0.0017,  0.0074,  0.0057],
        [-0.0250, -0.0024, -0.0083,  ..., -0.0088,  0.0282,  0.0184]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 0.0000e+00,  1.3466e-03, -3.7360e-04,  ...,  1.9131e-03,
         -2.9445e-04,  2.0409e-04],
        [ 2.2411e-05, -1.7490e-03,  6.0606e-04,  ..., -1.3599e-03,
         -9.7561e-04, -4.3678e-04],
        [ 2.3155e-03,  1.9016e-03, -7.8011e-04,  ...,  4.5443e-04,
          8.1158e-04, -1.0109e-03],
        ...,
        [ 1.6127e-03,  9.4795e-04, -9.1076e-04,  ...,  4.3416e-04,
         -5.2547e-04, -8.5354e-04],
        [ 7.6294e-04,  1.2779e-03, -4.8423e-04,  ...,  1.9894e-03,
         -1.0176e-03, -2.7323e-04],
        [-1.5163e-03, -5.9929e-03, -4.7684e-05,  ...,  1.9531e-03,
         -1.1139e-03,  5.0545e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7188,  3.9688,  0.7090,  ..., -4.2539, -2.9375,  4.2031]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.4663,  0.1548,  0.1021,  ..., -0.1364, -0.4565,  0.0160]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 09:33:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a womb is a cervix
A part of a gun is a trigger
A part of a sword is a blade
A part of a guitar is a string
A part of a deer is a antler
A part of a teapot is a spout
A part of a church is a altar
A part of a window is a
2024-07-09 09:33:34 root INFO     [order_1_approx] starting weight calculation for A part of a sword is a blade
A part of a church is a altar
A part of a deer is a antler
A part of a womb is a cervix
A part of a window is a pane
A part of a teapot is a spout
A part of a gun is a trigger
A part of a guitar is a
2024-07-09 09:33:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 09:37:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 09:41:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4600, -0.6802,  0.1470,  ...,  0.7861, -0.4727,  0.7749],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.4146, -0.6538,  0.1152,  ...,  0.6895, -0.4446,  0.6958],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.5625, -7.6484,  0.8066,  ..., -2.7305,  0.4565, -2.6738],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0027, -0.0148,  0.0014,  ...,  0.0153, -0.0070, -0.0161],
        [ 0.0031,  0.0020,  0.0122,  ...,  0.0013, -0.0167, -0.0342],
        [-0.0004, -0.0077,  0.0077,  ..., -0.0076,  0.0298,  0.0013],
        ...,
        [-0.0050, -0.0097,  0.0083,  ..., -0.0203, -0.0147,  0.0015],
        [-0.0029, -0.0130, -0.0009,  ...,  0.0259, -0.0231,  0.0092],
        [ 0.0157, -0.0065,  0.0230,  ..., -0.0036,  0.0077,  0.0179]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.9989e-03, -3.2663e-05, -1.5211e-03,  ..., -2.8944e-04,
         -7.7820e-04,  6.8951e-04],
        [ 7.8440e-04, -2.1095e-03,  1.4126e-04,  ..., -1.1959e-03,
         -3.0441e-03,  1.8930e-03],
        [ 1.2169e-03,  1.8911e-03, -6.4087e-04,  ...,  1.2655e-03,
          4.9067e-04,  1.0300e-03],
        ...,
        [ 1.3180e-03, -1.9255e-03,  1.4296e-03,  ..., -2.2163e-03,
          8.6212e-04, -1.3151e-03],
        [ 2.7561e-04, -9.7942e-04,  1.3113e-05,  ...,  4.5776e-04,
         -9.4318e-04,  9.6130e-04],
        [-1.6212e-03, -1.0300e-03,  6.4373e-04,  ..., -5.3215e-04,
          5.9247e-05, -4.1962e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.0527, -5.0391,  1.1523,  ..., -2.5469, -0.6060, -1.6963]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0939, -0.0173, -0.0457,  ..., -0.4143,  0.0746,  0.3503]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 09:41:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a sword is a blade
A part of a church is a altar
A part of a deer is a antler
A part of a womb is a cervix
A part of a window is a pane
A part of a teapot is a spout
A part of a gun is a trigger
A part of a guitar is a
2024-07-09 09:41:46 root INFO     [order_1_approx] starting weight calculation for A part of a deer is a antler
A part of a gun is a trigger
A part of a teapot is a spout
A part of a womb is a cervix
A part of a sword is a blade
A part of a window is a pane
A part of a guitar is a string
A part of a church is a
2024-07-09 09:41:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 09:45:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 09:49:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0757, -0.6313, -0.0457,  ...,  1.0352, -0.3586,  0.1237],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.0642, -0.5952, -0.0500,  ...,  0.8911, -0.3345,  0.0895],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.5176,  0.3726,  3.4180,  ..., -2.7305, -3.6934,  1.1104],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0034, -0.0129,  0.0045,  ..., -0.0036,  0.0091, -0.0209],
        [-0.0080, -0.0041,  0.0022,  ...,  0.0118,  0.0064,  0.0026],
        [ 0.0089,  0.0040, -0.0006,  ..., -0.0066,  0.0207,  0.0164],
        ...,
        [-0.0159, -0.0005,  0.0099,  ...,  0.0042, -0.0095, -0.0002],
        [-0.0025,  0.0056,  0.0031,  ...,  0.0132, -0.0035,  0.0029],
        [-0.0151, -0.0041, -0.0064,  ..., -0.0231,  0.0080,  0.0225]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 1.5903e-04, -4.1485e-04, -1.6336e-03,  ...,  5.2691e-04,
          1.1158e-03, -1.3895e-03],
        [ 8.9169e-04, -5.3835e-04,  2.2125e-04,  ...,  9.1410e-04,
         -1.4896e-03,  1.7109e-03],
        [ 5.4932e-04, -3.8147e-06,  6.8665e-04,  ..., -1.0138e-03,
         -7.3242e-04, -3.8433e-04],
        ...,
        [ 5.7745e-04, -8.1921e-04,  1.6451e-03,  ..., -1.5631e-03,
         -1.2836e-03, -1.3809e-03],
        [-2.6941e-04,  2.8038e-04, -4.1032e-04,  ..., -1.5306e-03,
         -4.3259e-03,  5.5075e-04],
        [-2.2173e-04,  1.1921e-05,  1.3084e-03,  ..., -5.1975e-05,
          1.8282e-03, -1.0042e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4775,  1.7715,  3.5449,  ..., -2.7500, -4.3398,  1.8145]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0731,  0.0797,  0.1768,  ..., -0.4761, -0.0970,  0.0461]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 09:49:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a deer is a antler
A part of a gun is a trigger
A part of a teapot is a spout
A part of a womb is a cervix
A part of a sword is a blade
A part of a window is a pane
A part of a guitar is a string
A part of a church is a
2024-07-09 09:49:56 root INFO     [order_1_approx] starting weight calculation for A part of a teapot is a spout
A part of a window is a pane
A part of a deer is a antler
A part of a gun is a trigger
A part of a guitar is a string
A part of a church is a altar
A part of a sword is a blade
A part of a womb is a
2024-07-09 09:49:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 09:54:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 09:58:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2312,  0.0488, -1.2988,  ...,  0.5273, -0.8359,  0.6538],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.2023,  0.0271, -1.1172,  ...,  0.4543, -0.7539,  0.5713],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0435,  0.5942, -1.4121,  ..., -0.5493, -0.1528,  2.8711],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0006, -0.0143, -0.0059,  ...,  0.0041,  0.0050, -0.0256],
        [-0.0118, -0.0006,  0.0094,  ...,  0.0152, -0.0098,  0.0141],
        [ 0.0133,  0.0035, -0.0073,  ..., -0.0025,  0.0302, -0.0091],
        ...,
        [ 0.0087,  0.0159, -0.0033,  ..., -0.0262,  0.0044, -0.0169],
        [ 0.0016,  0.0044,  0.0126,  ...,  0.0112, -0.0087, -0.0211],
        [-0.0068, -0.0153,  0.0065,  ..., -0.0058,  0.0157,  0.0057]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.5030e-03, -8.9073e-04, -1.9646e-04,  ...,  8.3303e-04,
         -1.3552e-03,  1.0605e-03],
        [-9.3555e-04,  1.4277e-03,  1.4009e-03,  ...,  1.1816e-03,
         -3.2692e-03,  8.3447e-05],
        [ 2.1725e-03, -2.7485e-03,  5.1498e-04,  ..., -4.2629e-04,
          1.6050e-03, -2.5043e-03],
        ...,
        [ 1.8892e-03, -5.3930e-04,  3.0422e-04,  ..., -1.1463e-03,
          1.1616e-03, -1.9350e-03],
        [-2.1529e-04,  9.8324e-04,  2.0638e-03,  ...,  3.1590e-04,
         -8.9455e-04, -1.5793e-03],
        [-1.3094e-03, -7.6675e-04,  2.4395e-03,  ..., -5.3406e-05,
          3.3355e-04, -2.8744e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2382, -0.0420,  0.0645,  ..., -1.4297,  0.2368,  1.6748]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.4277,  0.0350, -0.1375,  ..., -0.2465, -0.5571, -0.1726]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 09:58:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a teapot is a spout
A part of a window is a pane
A part of a deer is a antler
A part of a gun is a trigger
A part of a guitar is a string
A part of a church is a altar
A part of a sword is a blade
A part of a womb is a
2024-07-09 09:58:08 root INFO     [order_1_approx] starting weight calculation for A part of a guitar is a string
A part of a window is a pane
A part of a church is a altar
A part of a teapot is a spout
A part of a deer is a antler
A part of a sword is a blade
A part of a womb is a cervix
A part of a gun is a
2024-07-09 09:58:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 10:02:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 10:06:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1052, -0.2632, -0.1431,  ...,  1.4795, -0.0919,  0.0545],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.0976, -0.2722, -0.1398,  ...,  1.3535, -0.1034,  0.0306],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0469, -0.9556, -0.1309,  ..., -4.7812, -3.1406,  5.7031],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0087, -0.0096, -0.0042,  ...,  0.0159,  0.0127, -0.0014],
        [ 0.0088, -0.0084, -0.0055,  ...,  0.0239,  0.0065,  0.0054],
        [ 0.0090,  0.0042,  0.0382,  ..., -0.0089,  0.0086,  0.0014],
        ...,
        [ 0.0016, -0.0110,  0.0023,  ...,  0.0146, -0.0071, -0.0083],
        [ 0.0008, -0.0058,  0.0148,  ..., -0.0101,  0.0147,  0.0159],
        [-0.0084, -0.0092,  0.0059,  ..., -0.0093,  0.0316, -0.0045]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-9.5558e-04, -4.1008e-04, -2.3615e-04,  ..., -1.4534e-03,
         -1.1463e-03,  3.3379e-06],
        [ 3.5954e-04,  8.8453e-04,  8.3590e-04,  ..., -6.7949e-05,
          8.8167e-04,  1.5602e-03],
        [-5.3406e-04, -5.7888e-04,  5.7554e-04,  ...,  2.0485e-03,
          1.7157e-03, -8.9169e-05],
        ...,
        [-7.2098e-04, -7.7057e-04,  1.2188e-03,  ..., -1.4954e-03,
          1.2636e-03, -1.0347e-03],
        [ 7.0667e-04,  6.7902e-04,  1.0240e-04,  ..., -3.9935e-04,
          1.8473e-03,  8.6355e-04],
        [-1.4133e-03, -1.0643e-03,  9.2936e-04,  ...,  1.0896e-04,
          8.5592e-04, -2.4166e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9932, -0.5918, -0.1593,  ..., -4.6094, -2.1035,  5.9531]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0428, -0.0109,  0.1379,  ..., -0.5679, -0.3044,  0.2659]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 10:06:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a guitar is a string
A part of a window is a pane
A part of a church is a altar
A part of a teapot is a spout
A part of a deer is a antler
A part of a sword is a blade
A part of a womb is a cervix
A part of a gun is a
2024-07-09 10:06:19 root INFO     total operator prediction time: 3928.2617869377136 seconds
2024-07-09 10:06:19 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on hypernyms - animals
2024-07-09 10:06:21 root INFO     building operator hypernyms - animals
2024-07-09 10:06:21 root INFO     [order_1_approx] starting weight calculation for The cobra falls into the category of snake
The hawk falls into the category of raptor
The anaconda falls into the category of snake
The viper falls into the category of snake
The dog falls into the category of canine
The chimpanzee falls into the category of primate
The gorilla falls into the category of primate
The jaguar falls into the category of
2024-07-09 10:06:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 10:10:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 10:14:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8691, -1.5723,  0.5493,  ...,  0.4004,  0.5991,  0.6133],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.7632, -1.4238,  0.4521,  ...,  0.3445,  0.5029,  0.5293],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1143, -1.4873, -0.2119,  ..., -0.6538, -4.4531,  3.3516],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0058, -0.0094, -0.0015,  ...,  0.0031,  0.0028, -0.0060],
        [ 0.0032, -0.0117,  0.0186,  ...,  0.0171,  0.0098, -0.0034],
        [ 0.0091,  0.0003, -0.0056,  ..., -0.0050,  0.0007, -0.0026],
        ...,
        [-0.0059,  0.0023, -0.0031,  ...,  0.0058, -0.0050,  0.0132],
        [-0.0002, -0.0159, -0.0164,  ..., -0.0104, -0.0035,  0.0066],
        [-0.0112,  0.0016,  0.0093,  ...,  0.0050,  0.0054, -0.0006]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.1945e-04,  2.3985e-04,  4.8923e-04,  ...,  8.7833e-04,
         -7.5054e-04, -1.7338e-03],
        [ 4.9591e-05, -1.8749e-03,  1.3962e-03,  ...,  2.3365e-05,
          2.5272e-04, -8.9502e-04],
        [ 5.5695e-04,  6.1131e-04, -1.0567e-03,  ...,  3.1209e-04,
         -8.5449e-04, -9.1934e-04],
        ...,
        [ 1.4772e-03,  5.3596e-04, -3.1447e-04,  ...,  5.0831e-04,
         -1.8454e-04,  7.6962e-04],
        [ 7.1192e-04,  1.4410e-03, -4.2319e-04,  ...,  1.4791e-03,
         -7.4911e-04,  3.0661e-04],
        [ 7.3481e-04, -2.0504e-03,  1.5450e-03,  ...,  8.6546e-04,
          3.9101e-04, -8.6212e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3613, -2.1680, -0.7451,  ..., -0.6064, -4.3789,  3.6777]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0035, -0.1481, -0.1547,  ...,  0.0583, -0.0351, -0.0797]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 10:14:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The cobra falls into the category of snake
The hawk falls into the category of raptor
The anaconda falls into the category of snake
The viper falls into the category of snake
The dog falls into the category of canine
The chimpanzee falls into the category of primate
The gorilla falls into the category of primate
The jaguar falls into the category of
2024-07-09 10:14:29 root INFO     [order_1_approx] starting weight calculation for The dog falls into the category of canine
The anaconda falls into the category of snake
The gorilla falls into the category of primate
The viper falls into the category of snake
The chimpanzee falls into the category of primate
The hawk falls into the category of raptor
The jaguar falls into the category of feline
The cobra falls into the category of
2024-07-09 10:14:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 10:18:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 10:22:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5581, -0.6074, -1.6699,  ...,  0.5195,  0.0261, -0.4214],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.4907, -0.5605, -1.4082,  ...,  0.4448,  0.0075, -0.3979],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7832, -0.0296,  0.0361,  ...,  0.6025, -1.6816,  0.5869],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0269, -0.0066, -0.0118,  ..., -0.0177,  0.0050, -0.0198],
        [-0.0080,  0.0072,  0.0230,  ...,  0.0209,  0.0063,  0.0047],
        [-0.0090, -0.0081, -0.0001,  ..., -0.0115,  0.0006,  0.0078],
        ...,
        [-0.0100,  0.0035,  0.0075,  ...,  0.0151, -0.0139, -0.0059],
        [ 0.0050, -0.0160, -0.0059,  ..., -0.0168,  0.0075,  0.0091],
        [-0.0053,  0.0040,  0.0082,  ...,  0.0059,  0.0025,  0.0101]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 1.7953e-04, -1.0414e-03,  1.1330e-03,  ...,  6.9332e-04,
          3.8433e-04, -2.1515e-03],
        [-1.2665e-03, -1.0405e-03,  3.4637e-03,  ...,  2.2202e-03,
          1.2770e-03, -1.5221e-03],
        [-2.5249e-04,  5.5075e-05, -1.0414e-03,  ..., -1.1120e-03,
         -7.8392e-04,  1.7900e-03],
        ...,
        [ 4.8804e-04,  9.7394e-05,  1.4563e-03,  ...,  3.6144e-04,
         -1.3721e-04, -1.1787e-03],
        [ 4.9496e-04,  7.5340e-04, -1.3294e-03,  ..., -4.4870e-04,
         -9.8610e-04,  8.8453e-05],
        [-4.7922e-05,  6.6710e-04,  2.1696e-05,  ...,  6.6137e-04,
          1.3103e-03, -1.5354e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6680, -1.0078,  0.3611,  ...,  0.4475, -2.0215,  1.1504]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.2493, -0.2437,  0.2297,  ...,  0.0611, -0.0154, -0.0292]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 10:22:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The dog falls into the category of canine
The anaconda falls into the category of snake
The gorilla falls into the category of primate
The viper falls into the category of snake
The chimpanzee falls into the category of primate
The hawk falls into the category of raptor
The jaguar falls into the category of feline
The cobra falls into the category of
2024-07-09 10:22:39 root INFO     [order_1_approx] starting weight calculation for The chimpanzee falls into the category of primate
The gorilla falls into the category of primate
The viper falls into the category of snake
The anaconda falls into the category of snake
The cobra falls into the category of snake
The jaguar falls into the category of feline
The dog falls into the category of canine
The hawk falls into the category of
2024-07-09 10:22:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 10:26:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 10:30:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3899, -0.8584, -2.1660,  ...,  0.4519, -0.1787,  0.4771],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3267, -0.7632, -1.7637,  ...,  0.3708, -0.1683,  0.3887],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6035, -0.3721, -4.2852,  ...,  0.4634, -0.8066, -0.9619],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 7.7972e-03, -1.4374e-02, -9.9258e-03,  ..., -2.2411e-03,
         -8.3447e-05, -5.7678e-03],
        [-5.6610e-03,  1.1467e-02,  2.5269e-02,  ...,  1.3718e-02,
          7.3395e-03,  3.7956e-03],
        [ 1.4076e-02, -9.4147e-03, -1.5747e-02,  ..., -4.4136e-03,
         -7.1564e-03,  9.5367e-04],
        ...,
        [ 3.9139e-03, -1.6388e-02,  6.2599e-03,  ...,  1.3344e-02,
         -7.2136e-03,  1.7691e-03],
        [-6.7024e-03, -6.8207e-03, -9.7961e-03,  ...,  1.2941e-03,
         -8.3618e-03, -3.2196e-03],
        [-2.4433e-03, -1.6251e-03,  1.8892e-03,  ..., -4.8065e-03,
          6.3667e-03,  1.7487e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-8.5163e-04,  8.0919e-04, -2.2769e-04,  ...,  1.7548e-04,
         -4.1795e-04, -1.0567e-03],
        [-8.4209e-04, -2.1248e-03,  2.7847e-03,  ...,  9.4748e-04,
          3.4237e-04, -8.5592e-04],
        [ 6.8569e-04,  3.1643e-03, -2.0142e-03,  ...,  2.8348e-04,
         -1.5860e-03,  1.0691e-03],
        ...,
        [ 7.1287e-04,  5.6696e-04,  9.5940e-04,  ..., -5.2547e-04,
          2.8276e-04,  1.4029e-03],
        [ 3.0446e-04,  2.0790e-04, -1.9436e-03,  ...,  4.2629e-04,
         -1.9350e-03,  9.9277e-04],
        [ 2.7704e-04, -1.3828e-05, -1.3471e-04,  ...,  6.1989e-04,
          2.8062e-04, -4.3583e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9648, -1.9580, -2.5508,  ...,  0.0247, -0.5566, -0.6621]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1997, -0.1434,  0.1940,  ..., -0.0494, -0.0855, -0.0304]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 10:30:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The chimpanzee falls into the category of primate
The gorilla falls into the category of primate
The viper falls into the category of snake
The anaconda falls into the category of snake
The cobra falls into the category of snake
The jaguar falls into the category of feline
The dog falls into the category of canine
The hawk falls into the category of
2024-07-09 10:30:49 root INFO     [order_1_approx] starting weight calculation for The chimpanzee falls into the category of primate
The hawk falls into the category of raptor
The viper falls into the category of snake
The cobra falls into the category of snake
The jaguar falls into the category of feline
The gorilla falls into the category of primate
The anaconda falls into the category of snake
The dog falls into the category of
2024-07-09 10:30:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 10:34:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 10:38:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2969, -0.8750, -0.4219,  ...,  0.6768, -0.3145,  1.1611],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.2834, -0.8774, -0.3967,  ...,  0.6289, -0.3149,  1.1162],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5615, -1.0908,  2.0977,  ..., -3.5410,  0.9121,  0.0386],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0138, -0.0226, -0.0122,  ..., -0.0246, -0.0044,  0.0099],
        [-0.0015,  0.0124,  0.0045,  ...,  0.0254,  0.0016, -0.0228],
        [-0.0044, -0.0141, -0.0109,  ..., -0.0013,  0.0167, -0.0082],
        ...,
        [-0.0027, -0.0104,  0.0107,  ...,  0.0073,  0.0038,  0.0062],
        [-0.0010, -0.0062, -0.0053,  ..., -0.0092, -0.0093,  0.0027],
        [-0.0039, -0.0007,  0.0146,  ...,  0.0152, -0.0171, -0.0004]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 1.2732e-03,  1.3256e-03, -4.1866e-04,  ...,  2.5511e-05,
          2.0266e-04,  3.0875e-04],
        [-1.1148e-03, -3.9506e-04,  1.2169e-03,  ..., -3.3283e-04,
         -8.9169e-05, -1.0242e-03],
        [ 6.9141e-04,  4.7851e-04, -1.4210e-03,  ...,  7.1704e-05,
          4.2725e-04,  5.4073e-04],
        ...,
        [ 5.1260e-06, -8.8882e-04,  5.0354e-04,  ..., -1.6117e-03,
          1.1644e-03, -6.9332e-04],
        [ 1.2217e-03,  4.4346e-04, -4.9782e-04,  ...,  8.4496e-04,
         -1.1005e-03,  1.0357e-03],
        [ 8.7261e-04, -7.6199e-04,  5.6076e-04,  ...,  1.7376e-03,
          5.5218e-04, -9.8896e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4160, -1.6680,  1.9814,  ..., -3.0859,  1.6914, -0.2532]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1808, -0.0611,  0.1075,  ...,  0.0216, -0.0742, -0.0096]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 10:39:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The chimpanzee falls into the category of primate
The hawk falls into the category of raptor
The viper falls into the category of snake
The cobra falls into the category of snake
The jaguar falls into the category of feline
The gorilla falls into the category of primate
The anaconda falls into the category of snake
The dog falls into the category of
2024-07-09 10:39:01 root INFO     [order_1_approx] starting weight calculation for The dog falls into the category of canine
The gorilla falls into the category of primate
The chimpanzee falls into the category of primate
The jaguar falls into the category of feline
The anaconda falls into the category of snake
The cobra falls into the category of snake
The hawk falls into the category of raptor
The viper falls into the category of
2024-07-09 10:39:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 10:43:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 10:47:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5713, -0.3806, -1.3037,  ...,  0.4270,  0.0355,  0.6826],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.4722, -0.3354, -1.0332,  ...,  0.3435,  0.0128,  0.5527],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.9570, -0.4192, -1.8643,  ...,  2.2480, -2.5645,  0.8726],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.6382e-02, -1.1292e-02, -1.0880e-02,  ..., -8.2321e-03,
          2.0340e-02, -4.5090e-03],
        [-2.1896e-02,  9.2621e-03,  1.8387e-02,  ...,  2.8564e-02,
         -2.3651e-04, -8.1253e-03],
        [ 6.5842e-03, -8.4000e-03, -3.5725e-03,  ..., -4.2381e-03,
          7.9346e-03,  9.9335e-03],
        ...,
        [-2.3315e-02, -6.9580e-03,  1.0956e-02,  ...,  2.1133e-02,
         -7.0381e-03,  2.4071e-03],
        [ 8.4457e-03, -1.4786e-02, -4.8027e-03,  ..., -1.2726e-02,
          1.2276e-02,  1.5274e-02],
        [-6.7482e-03,  3.7155e-03,  5.8327e-03,  ..., -4.9353e-05,
         -3.8795e-03,  1.1505e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-5.4312e-04, -3.5114e-03,  3.5667e-04,  ...,  3.9697e-04,
          1.4668e-03, -7.8201e-04],
        [-2.4033e-04, -3.6931e-04,  1.5841e-03,  ...,  4.0722e-04,
          4.8971e-04, -2.0580e-03],
        [ 8.6164e-04,  2.0456e-04, -7.4863e-04,  ...,  3.7241e-04,
          1.7204e-03,  2.7180e-04],
        ...,
        [ 1.2770e-03, -7.4768e-04,  2.0170e-04,  ...,  2.8205e-04,
          2.7251e-04, -1.9569e-03],
        [ 2.5272e-05, -1.3580e-03, -1.0433e-03,  ...,  7.0572e-04,
          2.3437e-04,  1.6689e-06],
        [-5.8985e-04,  2.3067e-04,  2.8057e-03,  ..., -4.7970e-04,
          1.0538e-03,  7.0333e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.0957, -1.5615, -2.1406,  ...,  1.3145, -2.3984,  1.8262]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.2151, -0.0575,  0.0210,  ..., -0.1063, -0.1567,  0.1333]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 10:47:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The dog falls into the category of canine
The gorilla falls into the category of primate
The chimpanzee falls into the category of primate
The jaguar falls into the category of feline
The anaconda falls into the category of snake
The cobra falls into the category of snake
The hawk falls into the category of raptor
The viper falls into the category of
2024-07-09 10:47:14 root INFO     [order_1_approx] starting weight calculation for The anaconda falls into the category of snake
The cobra falls into the category of snake
The hawk falls into the category of raptor
The viper falls into the category of snake
The gorilla falls into the category of primate
The dog falls into the category of canine
The jaguar falls into the category of feline
The chimpanzee falls into the category of
2024-07-09 10:47:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 10:51:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 10:55:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0500, -0.7729, -0.5117,  ...,  0.4253,  0.6006,  0.7783],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.0471, -0.7158, -0.4417,  ...,  0.3650,  0.5054,  0.6792],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 6.3750, -0.5215,  4.8281,  ..., -2.4238, -1.6396, -0.6479],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.5442e-02, -1.3824e-02, -1.6212e-03,  ...,  5.3024e-03,
         -1.0498e-02, -1.3046e-03],
        [-1.5594e-02, -6.3858e-03,  1.1391e-02,  ...,  1.3138e-02,
         -5.8174e-04,  7.1259e-03],
        [-7.2098e-03, -6.9160e-03, -1.3638e-04,  ..., -5.1727e-03,
         -3.0136e-03,  3.3264e-03],
        ...,
        [-7.5531e-03, -6.5689e-03, -7.8430e-03,  ..., -1.7757e-03,
          1.1158e-04,  4.0665e-03],
        [ 4.1199e-04, -2.4376e-03,  1.7166e-05,  ..., -2.0962e-03,
          1.6403e-04,  8.6021e-04],
        [ 2.1648e-03, -8.4381e-03,  2.9011e-03,  ...,  1.3666e-03,
          4.5853e-03, -1.4105e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0011, -0.0020,  0.0013,  ...,  0.0020, -0.0007, -0.0002],
        [-0.0011, -0.0015,  0.0026,  ...,  0.0017,  0.0005, -0.0019],
        [ 0.0009,  0.0009,  0.0002,  ..., -0.0007, -0.0020,  0.0007],
        ...,
        [ 0.0015,  0.0009, -0.0006,  ..., -0.0011,  0.0005,  0.0005],
        [ 0.0008,  0.0005, -0.0001,  ...,  0.0003, -0.0012,  0.0002],
        [-0.0006, -0.0011, -0.0014,  ..., -0.0009, -0.0005,  0.0007]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.5234, -1.0781,  4.6836,  ..., -2.1660, -1.8018, -0.1152]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0424, -0.1730, -0.0041,  ..., -0.0500, -0.1489,  0.2113]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 10:55:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The anaconda falls into the category of snake
The cobra falls into the category of snake
The hawk falls into the category of raptor
The viper falls into the category of snake
The gorilla falls into the category of primate
The dog falls into the category of canine
The jaguar falls into the category of feline
The chimpanzee falls into the category of
2024-07-09 10:55:22 root INFO     [order_1_approx] starting weight calculation for The jaguar falls into the category of feline
The dog falls into the category of canine
The gorilla falls into the category of primate
The viper falls into the category of snake
The hawk falls into the category of raptor
The cobra falls into the category of snake
The chimpanzee falls into the category of primate
The anaconda falls into the category of
2024-07-09 10:55:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 10:59:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 11:03:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.8389,  1.0625, -1.1074,  ...,  0.8887,  0.1404,  0.3611],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 1.6074,  0.9360, -0.9326,  ...,  0.7500,  0.1042,  0.3003],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0488,  0.5771, -1.1475,  ...,  1.0029, -3.3438,  3.1680],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-3.6621e-04,  7.0801e-03, -2.7561e-03,  ...,  5.4626e-03,
          7.2002e-04, -6.1874e-03],
        [-1.4267e-02,  2.4567e-03,  3.5782e-03,  ...,  1.1993e-02,
          2.6016e-03, -4.4518e-03],
        [ 8.0872e-03,  1.9569e-03, -6.4354e-03,  ..., -7.0229e-03,
         -6.1035e-05,  1.7120e-02],
        ...,
        [-1.3336e-02, -1.7586e-03, -8.0261e-03,  ...,  1.4427e-02,
         -2.1286e-03,  1.6899e-03],
        [ 3.3321e-03, -6.3286e-03, -3.4218e-03,  ..., -9.0332e-03,
          1.1551e-02,  2.3651e-02],
        [-1.3519e-02, -1.0994e-02,  3.3607e-03,  ...,  1.1673e-02,
         -2.5940e-04,  5.6038e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.0424e-03, -1.0605e-03, -2.6035e-04,  ...,  5.7697e-04,
         -2.9254e-04, -2.3861e-03],
        [-8.7833e-04, -1.5914e-05,  2.3804e-03,  ...,  7.4577e-04,
         -2.6631e-04, -9.1600e-04],
        [ 1.2178e-03, -3.3379e-06, -1.0824e-03,  ..., -8.7500e-05,
         -1.8797e-03,  5.3263e-04],
        ...,
        [ 4.9925e-04, -3.1114e-04,  1.5011e-03,  ...,  2.0332e-03,
         -4.9734e-04, -1.4675e-04],
        [ 1.1215e-03,  6.7854e-04,  4.8399e-05,  ...,  5.6696e-04,
         -6.4790e-05,  7.1144e-04],
        [-2.7871e-04, -2.1133e-03,  1.0767e-03,  ...,  1.5192e-03,
          1.2064e-03, -1.1415e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2881,  0.4480, -1.8252,  ...,  0.6611, -3.8320,  3.3496]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0867, -0.0792,  0.1121,  ..., -0.1061, -0.1277, -0.1475]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 11:03:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The jaguar falls into the category of feline
The dog falls into the category of canine
The gorilla falls into the category of primate
The viper falls into the category of snake
The hawk falls into the category of raptor
The cobra falls into the category of snake
The chimpanzee falls into the category of primate
The anaconda falls into the category of
2024-07-09 11:03:35 root INFO     [order_1_approx] starting weight calculation for The hawk falls into the category of raptor
The jaguar falls into the category of feline
The anaconda falls into the category of snake
The dog falls into the category of canine
The cobra falls into the category of snake
The chimpanzee falls into the category of primate
The viper falls into the category of snake
The gorilla falls into the category of
2024-07-09 11:03:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 11:07:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 11:11:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0815, -1.6055, -0.8794,  ...,  0.6621, -0.3389,  1.1865],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.0625, -1.4316, -0.7344,  ...,  0.5444, -0.3088,  1.0107],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 8.8828,  1.1357,  3.0918,  ..., -0.4463, -3.7422,  1.6387],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0094,  0.0044,  0.0024,  ..., -0.0136, -0.0067,  0.0022],
        [-0.0041,  0.0006,  0.0088,  ...,  0.0096, -0.0061, -0.0135],
        [ 0.0003, -0.0067, -0.0016,  ..., -0.0119, -0.0096,  0.0052],
        ...,
        [-0.0137, -0.0027, -0.0025,  ...,  0.0139,  0.0033,  0.0037],
        [-0.0112, -0.0078, -0.0219,  ..., -0.0002,  0.0157,  0.0040],
        [ 0.0021, -0.0006,  0.0177,  ..., -0.0119, -0.0050, -0.0002]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.4586e-03, -7.0381e-04, -3.7384e-04,  ..., -5.7507e-04,
          9.8896e-04, -7.9727e-04],
        [-2.0828e-03, -7.7868e-04,  3.0441e-03,  ...,  1.3180e-03,
          5.5170e-04, -9.2793e-04],
        [ 2.6894e-04,  8.9741e-04, -1.3571e-03,  ..., -4.9829e-04,
         -6.3801e-04,  7.8487e-04],
        ...,
        [ 1.3113e-03, -5.9795e-04,  5.9891e-04,  ...,  1.1253e-03,
         -1.3864e-04, -2.2912e-04],
        [ 8.8978e-04,  6.2847e-04, -2.3556e-03,  ...,  8.6069e-05,
         -6.3515e-04,  1.9312e-03],
        [ 5.2452e-05, -7.7915e-04,  1.0395e-03,  ...,  2.5940e-04,
          1.9431e-04,  5.8460e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 9.1641,  1.6523,  2.5059,  ..., -1.8320, -4.5547,  1.8301]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1207, -0.2981,  0.0877,  ..., -0.1768, -0.0091,  0.0482]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 11:11:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The hawk falls into the category of raptor
The jaguar falls into the category of feline
The anaconda falls into the category of snake
The dog falls into the category of canine
The cobra falls into the category of snake
The chimpanzee falls into the category of primate
The viper falls into the category of snake
The gorilla falls into the category of
2024-07-09 11:11:44 root INFO     total operator prediction time: 3922.5667498111725 seconds
2024-07-09 11:11:44 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+er_irreg
2024-07-09 11:11:46 root INFO     building operator verb+er_irreg
2024-07-09 11:11:47 root INFO     [order_1_approx] starting weight calculation for If you achieve something, you are a achiever
If you perform something, you are a performer
If you write something, you are a writer
If you promote something, you are a promoter
If you organise something, you are a organiser
If you discover something, you are a discoverer
If you destroy something, you are a destroyer
If you recommend something, you are a
2024-07-09 11:11:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 11:15:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 11:19:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5488, -0.0137, -0.5864,  ..., -0.1729, -0.4761,  0.5215],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.5498, -0.0352, -0.5669,  ..., -0.1635, -0.4854,  0.4968],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0229, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0879, -1.8867, -3.2227,  ..., -1.6387,  2.3398,  2.0156],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0018,  0.0099,  0.0155,  ..., -0.0093,  0.0134, -0.0144],
        [ 0.0144,  0.0075,  0.0119,  ..., -0.0028,  0.0086,  0.0143],
        [ 0.0125,  0.0041,  0.0009,  ...,  0.0144,  0.0313,  0.0042],
        ...,
        [-0.0227, -0.0125, -0.0242,  ..., -0.0092, -0.0098, -0.0021],
        [ 0.0136,  0.0046, -0.0067,  ..., -0.0089,  0.0168,  0.0036],
        [ 0.0104, -0.0044, -0.0020,  ..., -0.0034,  0.0157,  0.0128]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.6093e-03, -1.8225e-03,  1.4334e-03,  ...,  3.0470e-04,
         -8.9228e-05, -1.0414e-03],
        [-1.5993e-03,  2.0008e-03,  1.2455e-03,  ..., -2.7466e-04,
          2.6226e-06,  1.3304e-03],
        [ 6.3241e-05, -4.0197e-04, -2.0580e-03,  ..., -2.6655e-04,
          8.2254e-04,  2.0278e-04],
        ...,
        [-1.1864e-03, -6.1226e-04, -8.4019e-04,  ...,  4.3702e-04,
          9.8348e-05, -1.8148e-03],
        [ 4.2224e-04,  1.9817e-03, -1.5612e-03,  ..., -1.1311e-03,
         -7.0381e-04, -1.7157e-03],
        [ 3.1257e-04,  9.5177e-04,  2.3448e-04,  ...,  6.5994e-04,
         -5.4181e-05, -7.5340e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.1172, -1.2812, -2.1836,  ..., -1.7334,  2.4863,  2.5762]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1318, -0.0477,  0.2314,  ..., -0.0911, -0.0784,  0.2571]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 11:19:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you achieve something, you are a achiever
If you perform something, you are a performer
If you write something, you are a writer
If you promote something, you are a promoter
If you organise something, you are a organiser
If you discover something, you are a discoverer
If you destroy something, you are a destroyer
If you recommend something, you are a
2024-07-09 11:19:55 root INFO     [order_1_approx] starting weight calculation for If you achieve something, you are a achiever
If you recommend something, you are a recommender
If you write something, you are a writer
If you organise something, you are a organiser
If you destroy something, you are a destroyer
If you discover something, you are a discoverer
If you perform something, you are a performer
If you promote something, you are a
2024-07-09 11:19:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 11:24:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 11:27:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1313, -0.3354, -0.0784,  ...,  0.2891, -0.3062,  0.1599],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1423, -0.3743, -0.0911,  ...,  0.2866, -0.3325,  0.1416],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.1211, -0.6636, -3.6934,  ...,  0.6191,  0.9014,  2.0215],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0077, -0.0088,  0.0071,  ..., -0.0052, -0.0014, -0.0122],
        [ 0.0028, -0.0071,  0.0042,  ...,  0.0039, -0.0061,  0.0024],
        [ 0.0028,  0.0122,  0.0030,  ..., -0.0079,  0.0052,  0.0166],
        ...,
        [-0.0076, -0.0150, -0.0039,  ..., -0.0076,  0.0021,  0.0064],
        [ 0.0092, -0.0069, -0.0019,  ..., -0.0143, -0.0041,  0.0077],
        [ 0.0208, -0.0094,  0.0124,  ..., -0.0040,  0.0138,  0.0036]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.3638e-03, -3.4370e-03,  3.5458e-03,  ..., -1.1663e-03,
         -2.9969e-04, -2.0733e-03],
        [-1.3943e-03, -3.6597e-04,  6.2037e-04,  ...,  5.5981e-04,
          3.3283e-04,  1.8907e-04],
        [ 3.0661e-04,  2.2564e-03, -3.2310e-03,  ..., -2.7599e-03,
         -6.1178e-04,  1.2093e-03],
        ...,
        [-1.2121e-03, -1.8959e-03,  7.7724e-04,  ..., -4.5776e-05,
          3.5620e-04, -8.1062e-04],
        [-6.2227e-04,  1.4138e-04, -3.8815e-04,  ..., -1.0395e-03,
         -5.9605e-04, -1.3590e-04],
        [-2.2621e-03, -4.1723e-05,  1.6155e-03,  ..., -2.4235e-04,
         -9.7656e-04, -1.5392e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5469, -1.1240, -3.5566,  ...,  0.0903,  0.4080,  1.7949]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1434, -0.0942,  0.2981,  ..., -0.0209,  0.0972,  0.1683]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 11:27:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you achieve something, you are a achiever
If you recommend something, you are a recommender
If you write something, you are a writer
If you organise something, you are a organiser
If you destroy something, you are a destroyer
If you discover something, you are a discoverer
If you perform something, you are a performer
If you promote something, you are a
2024-07-09 11:27:59 root INFO     [order_1_approx] starting weight calculation for If you promote something, you are a promoter
If you perform something, you are a performer
If you achieve something, you are a achiever
If you discover something, you are a discoverer
If you destroy something, you are a destroyer
If you organise something, you are a organiser
If you recommend something, you are a recommender
If you write something, you are a
2024-07-09 11:27:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 11:32:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 11:36:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4795, -0.5532,  0.4050,  ...,  0.1726, -0.5425,  0.2886],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.5181, -0.6357,  0.4084,  ...,  0.1831, -0.6021,  0.2959],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7412,  0.9097, -2.3203,  ...,  2.9473,  0.6621,  2.5586],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0163, -0.0017,  0.0081,  ...,  0.0009, -0.0111,  0.0027],
        [-0.0215, -0.0030,  0.0028,  ..., -0.0035, -0.0071,  0.0050],
        [-0.0013,  0.0142, -0.0015,  ...,  0.0067,  0.0102,  0.0005],
        ...,
        [-0.0140, -0.0042, -0.0096,  ..., -0.0078,  0.0018,  0.0013],
        [ 0.0220, -0.0006,  0.0051,  ..., -0.0054, -0.0091,  0.0073],
        [-0.0034, -0.0011, -0.0005,  ..., -0.0097,  0.0082,  0.0114]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.4643e-03, -1.1206e-03,  6.4135e-04,  ..., -1.0759e-04,
          1.1511e-03, -6.8569e-04],
        [-1.4248e-03,  4.4465e-04,  1.5659e-03,  ..., -5.9032e-04,
         -3.9339e-05,  9.7394e-05],
        [-7.7391e-04,  1.9443e-04, -2.7199e-03,  ..., -1.4153e-03,
          3.8767e-04,  5.0020e-04],
        ...,
        [ 1.0347e-03, -7.9250e-04,  4.2915e-06,  ...,  7.9250e-04,
          2.0943e-03, -2.9373e-03],
        [ 1.3037e-03,  1.0276e-04, -2.0576e-04,  ..., -2.4462e-04,
         -7.2956e-04, -9.4032e-04],
        [ 1.6570e-05,  3.0041e-04, -1.3227e-03,  ...,  2.9469e-04,
         -4.5729e-04,  6.0558e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1113,  0.4192, -2.6855,  ...,  2.2617,  1.2520,  2.3496]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0434, -0.0145,  0.2078,  ..., -0.4106, -0.0796,  0.0600]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 11:36:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you promote something, you are a promoter
If you perform something, you are a performer
If you achieve something, you are a achiever
If you discover something, you are a discoverer
If you destroy something, you are a destroyer
If you organise something, you are a organiser
If you recommend something, you are a recommender
If you write something, you are a
2024-07-09 11:36:05 root INFO     [order_1_approx] starting weight calculation for If you perform something, you are a performer
If you promote something, you are a promoter
If you organise something, you are a organiser
If you achieve something, you are a achiever
If you recommend something, you are a recommender
If you destroy something, you are a destroyer
If you write something, you are a writer
If you discover something, you are a
2024-07-09 11:36:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 11:40:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 11:44:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4507, -0.4053, -0.2477,  ...,  0.1945, -0.3159,  0.5181],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.4832, -0.4585, -0.2634,  ...,  0.1981, -0.3501,  0.5317],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0229, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.0195, -1.1064, -4.1875,  ..., -6.8984, -0.9751,  1.2168],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.1871e-02, -5.4169e-04,  1.4130e-02,  ..., -1.4553e-03,
         -6.6490e-03, -6.3057e-03],
        [ 6.4125e-03,  5.3406e-05,  7.3853e-03,  ...,  1.2634e-02,
          5.7268e-04, -7.0724e-03],
        [ 1.1147e-02,  9.8610e-04, -6.4373e-04,  ...,  4.6444e-04,
         -2.7237e-03,  4.6082e-03],
        ...,
        [ 8.6136e-03,  2.8648e-03, -1.0796e-02,  ..., -1.9836e-02,
         -3.8147e-04,  2.7313e-03],
        [ 1.0620e-02,  3.4599e-03, -6.9809e-04,  ...,  1.2016e-03,
          8.4229e-03,  7.0190e-03],
        [-3.8528e-03, -4.4708e-03,  7.9346e-04,  ..., -2.0561e-03,
          2.0020e-02,  6.5613e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-9.8610e-04, -2.2316e-03,  2.1420e-03,  ..., -5.7888e-04,
          8.3971e-04, -1.1597e-03],
        [ 2.8038e-04, -5.6362e-04,  1.0605e-03,  ...,  3.5524e-04,
          3.4189e-04,  1.6623e-03],
        [ 6.5804e-04,  1.0118e-03, -3.5143e-04,  ..., -1.7190e-04,
          1.0195e-03,  4.4680e-04],
        ...,
        [ 1.6327e-03, -1.3065e-04, -1.6594e-03,  ..., -2.4438e-05,
          8.7690e-04,  7.3195e-04],
        [ 1.0557e-03, -1.0090e-03,  8.2207e-04,  ..., -3.3355e-04,
          6.4278e-04, -8.2636e-04],
        [-1.6556e-03, -6.6042e-05,  1.6251e-03,  ..., -3.8362e-04,
          5.6028e-04, -7.2098e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1074, -1.0742, -4.5859,  ..., -6.8828, -1.2070,  0.9209]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1384, -0.0941,  0.0699,  ..., -0.2092, -0.1382,  0.0560]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 11:44:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you perform something, you are a performer
If you promote something, you are a promoter
If you organise something, you are a organiser
If you achieve something, you are a achiever
If you recommend something, you are a recommender
If you destroy something, you are a destroyer
If you write something, you are a writer
If you discover something, you are a
2024-07-09 11:44:16 root INFO     [order_1_approx] starting weight calculation for If you recommend something, you are a recommender
If you destroy something, you are a destroyer
If you write something, you are a writer
If you perform something, you are a performer
If you discover something, you are a discoverer
If you achieve something, you are a achiever
If you promote something, you are a promoter
If you organise something, you are a
2024-07-09 11:44:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 11:48:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 11:52:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4802, -0.0439,  0.0112,  ...,  0.2113, -0.6094,  0.8530],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.4814, -0.0646, -0.0035,  ...,  0.2010, -0.6143,  0.8325],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0229, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0234,  0.3516, -0.0913,  ..., -0.3340,  0.2129,  6.0078],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 7.8125e-03,  4.7798e-03,  3.7308e-03,  ...,  1.6556e-03,
          2.3289e-03, -5.4817e-03],
        [ 1.4076e-03, -8.7585e-03,  2.5558e-03,  ...,  1.9394e-02,
         -4.2381e-03,  1.5297e-03],
        [ 4.6005e-03, -3.4447e-03,  2.6493e-03,  ..., -2.8915e-03,
          2.9602e-03,  8.1177e-03],
        ...,
        [-1.4595e-02,  3.9215e-03, -9.5520e-03,  ..., -1.5259e-03,
         -9.7084e-04,  1.3763e-02],
        [ 3.0792e-02, -1.1841e-02, -3.9597e-03,  ..., -6.8970e-03,
          1.7975e-02,  1.4313e-02],
        [-1.7059e-02,  7.6675e-04,  1.1272e-03,  ...,  8.4915e-03,
          6.1035e-05,  2.2354e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.4553e-03, -8.3590e-04,  2.6150e-03,  ..., -6.8474e-04,
         -2.2411e-05,  1.3554e-04],
        [ 3.6263e-04, -2.6569e-03,  1.8787e-03,  ...,  1.1883e-03,
          7.5769e-04,  1.2560e-03],
        [ 1.4324e-03,  1.2770e-03, -5.0926e-04,  ...,  1.0052e-03,
         -1.8406e-03,  1.8120e-04],
        ...,
        [ 5.1689e-04,  4.0555e-04, -6.1274e-05,  ...,  1.3971e-03,
          1.8511e-03, -1.3552e-03],
        [ 2.6913e-03,  3.2091e-04,  1.8711e-03,  ...,  9.5367e-06,
         -1.9717e-04, -1.2178e-03],
        [ 4.6325e-04, -4.2963e-04,  2.2411e-05,  ...,  1.0633e-04,
         -1.9703e-03, -5.7411e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9893,  0.3049, -0.1404,  ..., -0.2671,  0.4497,  5.1797]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1348, -0.0606,  0.0524,  ..., -0.3787, -0.0133,  0.0637]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 11:52:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you recommend something, you are a recommender
If you destroy something, you are a destroyer
If you write something, you are a writer
If you perform something, you are a performer
If you discover something, you are a discoverer
If you achieve something, you are a achiever
If you promote something, you are a promoter
If you organise something, you are a
2024-07-09 11:52:29 root INFO     [order_1_approx] starting weight calculation for If you destroy something, you are a destroyer
If you discover something, you are a discoverer
If you recommend something, you are a recommender
If you promote something, you are a promoter
If you achieve something, you are a achiever
If you write something, you are a writer
If you organise something, you are a organiser
If you perform something, you are a
2024-07-09 11:52:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 11:56:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 12:00:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8755,  0.2234,  0.0796,  ..., -0.6577, -0.0515, -0.1821],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.9233,  0.2202,  0.0661,  ..., -0.6577, -0.0739, -0.2203],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.9863, -0.4902, -2.1953,  ..., -4.6172,  2.1973,  3.2422],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.5450e-03, -1.5182e-03,  6.9809e-03,  ...,  5.2872e-03,
         -1.0925e-02, -6.4240e-03],
        [-3.6240e-03, -8.1024e-03,  8.5373e-03,  ...,  1.7042e-03,
          3.6240e-05, -6.8283e-03],
        [ 6.5231e-04, -5.1651e-03,  1.0162e-02,  ...,  9.0790e-03,
          1.9653e-02,  1.8044e-03],
        ...,
        [-1.2245e-02,  1.0376e-03, -5.8861e-03,  ...,  1.0195e-03,
          2.2774e-03,  4.3030e-03],
        [ 1.3180e-03, -4.5319e-03, -1.4343e-02,  ..., -8.0872e-03,
          9.0637e-03,  4.3869e-03],
        [-2.9087e-03, -5.6686e-03, -4.5013e-04,  ..., -6.7291e-03,
          1.0765e-02,  7.3166e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-3.2544e-04, -1.8263e-03,  8.0395e-04,  ...,  1.1196e-03,
          1.7614e-03, -1.3275e-03],
        [-1.0223e-03, -1.4429e-03,  6.8283e-04,  ...,  4.3690e-05,
          1.3638e-03, -1.6613e-03],
        [-7.4339e-04, -1.2836e-03,  8.7452e-04,  ..., -1.4019e-03,
         -1.0872e-03,  2.3251e-03],
        ...,
        [ 6.9094e-04, -6.2847e-04,  4.0412e-04,  ...,  2.1706e-03,
          2.0683e-04, -2.1439e-03],
        [-9.1457e-04, -5.0020e-04, -9.7275e-04,  ..., -9.2077e-04,
          6.7663e-04, -3.6049e-04],
        [-1.8635e-03,  1.6537e-03,  5.0688e-04,  ..., -5.5838e-04,
         -7.3624e-04,  1.8954e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.6035, -0.2791, -2.8809,  ..., -4.2500,  1.6045,  2.8301]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0775, -0.0519,  0.1489,  ..., -0.2883, -0.2302,  0.1208]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 12:00:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you destroy something, you are a destroyer
If you discover something, you are a discoverer
If you recommend something, you are a recommender
If you promote something, you are a promoter
If you achieve something, you are a achiever
If you write something, you are a writer
If you organise something, you are a organiser
If you perform something, you are a
2024-07-09 12:00:34 root INFO     [order_1_approx] starting weight calculation for If you perform something, you are a performer
If you recommend something, you are a recommender
If you discover something, you are a discoverer
If you organise something, you are a organiser
If you destroy something, you are a destroyer
If you promote something, you are a promoter
If you write something, you are a writer
If you achieve something, you are a
2024-07-09 12:00:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 12:04:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 12:08:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6719,  0.2520,  0.3420,  ...,  0.5039, -0.4832,  0.2134],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.6943,  0.2444,  0.3201,  ...,  0.4956, -0.5078,  0.1965],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0229, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.5098,  1.3193, -5.6445,  ..., -3.6348,  0.8501,  2.1582],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-5.4054e-03, -4.1962e-03,  9.3079e-03,  ..., -3.0937e-03,
          1.6289e-03, -1.1024e-02],
        [-2.3746e-03,  7.1220e-03,  1.5945e-03,  ..., -3.0212e-03,
         -4.2648e-03, -1.9932e-03],
        [ 1.5610e-02,  1.0544e-02,  5.2223e-03,  ...,  6.7024e-03,
         -2.4319e-03,  7.8201e-03],
        ...,
        [ 4.9591e-05, -7.0190e-04, -6.2828e-03,  ..., -8.5602e-03,
         -6.2523e-03,  3.5305e-03],
        [ 1.4290e-02,  5.2643e-04,  1.0681e-03,  ..., -1.0170e-02,
         -1.5678e-03,  4.4022e-03],
        [ 1.3947e-02, -2.9869e-03,  9.9373e-04,  ..., -4.7760e-03,
          1.7639e-02,  4.1542e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-3.5810e-04, -3.8147e-06,  2.0676e-03,  ...,  1.0467e-04,
          1.2808e-03, -6.1750e-04],
        [ 5.0640e-04, -8.6737e-04,  1.0223e-03,  ...,  3.7527e-04,
          7.9966e-04,  5.4741e-04],
        [-1.3685e-04,  1.4257e-03, -1.9331e-03,  ...,  3.5238e-04,
         -5.0449e-04,  1.4992e-03],
        ...,
        [ 6.3181e-04,  8.0824e-05,  2.2650e-04,  ...,  7.2622e-04,
         -2.1541e-04, -1.4362e-03],
        [-6.6042e-04, -7.9870e-04,  1.0414e-03,  ..., -6.9046e-04,
          1.3027e-03, -1.5640e-03],
        [ 2.9159e-04,  1.1501e-03,  3.8862e-04,  ...,  3.6573e-04,
         -9.0742e-04, -1.1101e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6777,  1.4062, -5.1250,  ..., -3.4023,  0.5010,  1.6387]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1032, -0.0082,  0.2852,  ..., -0.1741, -0.0259,  0.0148]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 12:08:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you perform something, you are a performer
If you recommend something, you are a recommender
If you discover something, you are a discoverer
If you organise something, you are a organiser
If you destroy something, you are a destroyer
If you promote something, you are a promoter
If you write something, you are a writer
If you achieve something, you are a
2024-07-09 12:08:44 root INFO     [order_1_approx] starting weight calculation for If you achieve something, you are a achiever
If you perform something, you are a performer
If you organise something, you are a organiser
If you discover something, you are a discoverer
If you write something, you are a writer
If you promote something, you are a promoter
If you recommend something, you are a recommender
If you destroy something, you are a
2024-07-09 12:08:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 12:12:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 12:16:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2190, -0.5503,  0.4324,  ...,  0.1178, -0.6240, -0.0621],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.2268, -0.5854,  0.3989,  ...,  0.1140, -0.6377, -0.0903],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0230, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4707, -3.0293, -4.3047,  ..., -1.1113,  4.4727,  4.5391],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-4.4250e-04,  4.3373e-03,  2.8687e-03,  ..., -3.5076e-03,
         -9.3689e-03,  1.0605e-02],
        [ 8.1253e-03, -2.7962e-03, -5.6458e-03,  ..., -7.1526e-05,
         -2.8267e-03,  2.7332e-03],
        [ 9.3536e-03,  1.5640e-03,  5.2147e-03,  ..., -2.2602e-03,
         -2.9392e-03,  9.2506e-05],
        ...,
        [-1.1505e-02, -6.4278e-04, -1.1120e-03,  ..., -6.3820e-03,
         -3.1281e-03,  6.0997e-03],
        [ 4.9286e-03, -7.1487e-03,  3.3283e-04,  ...,  1.9169e-03,
         -1.2146e-02,  1.6108e-03],
        [-5.1498e-03, -3.1662e-03, -6.0196e-03,  ..., -8.3771e-03,
          1.3176e-02,  2.5497e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-7.6866e-04, -1.4648e-03, -1.4429e-03,  ...,  4.8828e-04,
          3.5286e-04,  1.5335e-03],
        [-1.7405e-04,  3.8648e-04,  1.1024e-03,  ..., -1.0986e-03,
          2.5582e-04,  2.8372e-04],
        [-6.3467e-04, -4.0674e-04, -1.6317e-03,  ..., -5.1403e-04,
          1.5774e-03, -1.9908e-05],
        ...,
        [ 8.4877e-04,  2.0123e-04,  2.4719e-03,  ...,  1.5259e-05,
          1.0681e-03,  5.6505e-04],
        [ 1.8120e-05, -3.6740e-04, -3.4904e-04,  ..., -2.7442e-04,
         -8.3828e-04,  3.3927e-04],
        [-4.9114e-05, -2.6083e-04,  5.7817e-05,  ..., -1.8787e-04,
         -7.8917e-05,  1.5426e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4727, -3.1172, -4.3789,  ..., -0.7407,  4.2500,  4.6914]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0670, -0.1077, -0.1670,  ..., -0.2720,  0.0096, -0.0721]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 12:16:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you achieve something, you are a achiever
If you perform something, you are a performer
If you organise something, you are a organiser
If you discover something, you are a discoverer
If you write something, you are a writer
If you promote something, you are a promoter
If you recommend something, you are a recommender
If you destroy something, you are a
2024-07-09 12:16:51 root INFO     total operator prediction time: 3904.7182052135468 seconds
2024-07-09 12:16:51 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_3pSg - Ved
2024-07-09 12:16:53 root INFO     building operator verb_3pSg - Ved
2024-07-09 12:16:54 root INFO     [order_1_approx] starting weight calculation for When he continues something, something has been continued
When he provides something, something has been provided
When he locates something, something has been located
When he requires something, something has been required
When he believes something, something has been believed
When he becomes something, something has been became
When he announces something, something has been announced
When he spends something, something has been
2024-07-09 12:16:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 12:21:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 12:24:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4375,  0.6230,  0.4883,  ...,  0.1562,  0.3672, -0.5400],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.4739,  0.6587,  0.4812,  ...,  0.1598,  0.3630, -0.6143],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0159, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0508,  2.6758,  1.0146,  ..., -0.1738, -1.5312,  1.9268],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0180, -0.0114,  0.0134,  ...,  0.0040, -0.0121, -0.0091],
        [-0.0197, -0.0120,  0.0150,  ...,  0.0006, -0.0139,  0.0113],
        [ 0.0174,  0.0061, -0.0080,  ..., -0.0012,  0.0013,  0.0025],
        ...,
        [ 0.0011, -0.0008, -0.0038,  ..., -0.0123,  0.0029,  0.0033],
        [ 0.0081,  0.0084, -0.0006,  ..., -0.0067,  0.0075, -0.0102],
        [ 0.0068,  0.0029,  0.0031,  ...,  0.0077,  0.0187, -0.0126]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0035, -0.0012,  0.0011,  ...,  0.0002,  0.0009,  0.0005],
        [ 0.0015, -0.0002,  0.0012,  ..., -0.0011,  0.0018,  0.0013],
        [ 0.0012,  0.0014, -0.0056,  ...,  0.0007,  0.0014,  0.0024],
        ...,
        [-0.0010, -0.0022, -0.0005,  ..., -0.0008,  0.0035, -0.0027],
        [-0.0011,  0.0022,  0.0039,  ..., -0.0016,  0.0003, -0.0024],
        [ 0.0013,  0.0002,  0.0014,  ..., -0.0002, -0.0002, -0.0006]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9160,  3.2422,  0.5625,  ..., -0.2861, -1.3516,  1.7529]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1738, -0.0322,  0.1284,  ..., -0.1909, -0.3762, -0.1289]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 12:25:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he continues something, something has been continued
When he provides something, something has been provided
When he locates something, something has been located
When he requires something, something has been required
When he believes something, something has been believed
When he becomes something, something has been became
When he announces something, something has been announced
When he spends something, something has been
2024-07-09 12:25:02 root INFO     [order_1_approx] starting weight calculation for When he provides something, something has been provided
When he requires something, something has been required
When he believes something, something has been believed
When he announces something, something has been announced
When he spends something, something has been spent
When he continues something, something has been continued
When he becomes something, something has been became
When he locates something, something has been
2024-07-09 12:25:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 12:29:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 12:33:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7949,  0.1284, -0.0165,  ...,  0.2617, -0.0527, -0.4099],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.7959,  0.1154, -0.0258,  ...,  0.2551, -0.0692, -0.4392],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0159, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.4199,  1.7549, -1.7559,  ..., -5.2578,  1.0596,  1.4854],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0038, -0.0080,  0.0174,  ...,  0.0053, -0.0051, -0.0022],
        [-0.0152,  0.0075,  0.0065,  ...,  0.0066,  0.0072, -0.0046],
        [-0.0028,  0.0045, -0.0091,  ...,  0.0028,  0.0117, -0.0129],
        ...,
        [-0.0140,  0.0076,  0.0002,  ..., -0.0124,  0.0012, -0.0081],
        [ 0.0016,  0.0035, -0.0129,  ..., -0.0123,  0.0090,  0.0008],
        [-0.0035,  0.0060, -0.0024,  ...,  0.0038, -0.0032,  0.0107]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.0351e-03,  5.2929e-05,  2.0847e-03,  ...,  7.1239e-04,
          7.5960e-04,  3.0327e-04],
        [-5.8031e-04, -7.6962e-04,  1.6680e-03,  ...,  4.0317e-04,
          1.1978e-03, -1.2827e-03],
        [ 8.4162e-04,  1.4858e-03, -1.8492e-03,  ..., -1.4877e-03,
          9.8228e-04,  1.3485e-03],
        ...,
        [-1.1759e-03,  6.7806e-04, -1.8382e-04,  ..., -3.6526e-04,
          1.6460e-03, -3.2272e-03],
        [-6.4516e-04,  3.4332e-04,  9.6226e-04,  ..., -1.9417e-03,
         -8.4305e-04, -8.5068e-04],
        [ 1.6856e-04, -1.0757e-03,  5.0735e-04,  ...,  1.7090e-03,
         -5.4216e-04, -2.8515e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.9590,  1.9502, -2.4004,  ..., -5.0547,  1.2520,  0.7510]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0362, -0.2031,  0.1675,  ..., -0.4526, -0.3333,  0.1124]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 12:33:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he provides something, something has been provided
When he requires something, something has been required
When he believes something, something has been believed
When he announces something, something has been announced
When he spends something, something has been spent
When he continues something, something has been continued
When he becomes something, something has been became
When he locates something, something has been
2024-07-09 12:33:11 root INFO     [order_1_approx] starting weight calculation for When he requires something, something has been required
When he spends something, something has been spent
When he announces something, something has been announced
When he continues something, something has been continued
When he provides something, something has been provided
When he believes something, something has been believed
When he locates something, something has been located
When he becomes something, something has been
2024-07-09 12:33:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 12:37:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 12:41:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8057, -0.0078,  1.2715,  ...,  0.8740,  0.2900, -0.2834],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.8765, -0.0284,  1.2939,  ...,  0.9048,  0.2871, -0.3381],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0159, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3184,  2.1328,  2.2559,  ..., -1.8359,  0.4534, -0.5186],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0153, -0.0143,  0.0109,  ...,  0.0026, -0.0121, -0.0179],
        [ 0.0015,  0.0046,  0.0033,  ..., -0.0059, -0.0102, -0.0022],
        [ 0.0022,  0.0052, -0.0233,  ...,  0.0124, -0.0005,  0.0035],
        ...,
        [ 0.0007, -0.0023,  0.0020,  ...,  0.0017, -0.0027,  0.0103],
        [ 0.0170, -0.0104, -0.0139,  ..., -0.0175, -0.0139, -0.0149],
        [ 0.0133,  0.0145, -0.0058,  ...,  0.0021,  0.0187, -0.0101]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.6436e-03, -1.6813e-03, -7.7677e-04,  ..., -1.6060e-03,
          1.0519e-03, -5.0545e-05],
        [ 2.2507e-04, -1.5373e-03,  4.9973e-04,  ...,  1.1759e-03,
          7.4911e-04, -5.6124e-04],
        [ 3.5667e-03,  1.2226e-03, -1.2960e-03,  ...,  1.7433e-03,
         -1.5545e-04,  3.5429e-04],
        ...,
        [-1.3170e-03, -2.4452e-03,  2.2964e-03,  ..., -1.4925e-03,
          1.0004e-03, -3.5458e-03],
        [ 6.4230e-04,  3.4161e-03,  3.8481e-04,  ..., -1.3828e-03,
         -2.9621e-03, -1.7033e-03],
        [ 5.9414e-04,  4.8494e-04,  8.5354e-04,  ...,  1.0216e-04,
          2.0027e-05, -6.6614e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6035,  1.7949,  1.3770,  ..., -0.8252,  1.2676, -1.2090]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2224, -0.0672,  0.1836,  ..., -0.2358, -0.0047, -0.2029]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 12:41:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he requires something, something has been required
When he spends something, something has been spent
When he announces something, something has been announced
When he continues something, something has been continued
When he provides something, something has been provided
When he believes something, something has been believed
When he locates something, something has been located
When he becomes something, something has been
2024-07-09 12:41:19 root INFO     [order_1_approx] starting weight calculation for When he becomes something, something has been became
When he believes something, something has been believed
When he announces something, something has been announced
When he provides something, something has been provided
When he continues something, something has been continued
When he spends something, something has been spent
When he locates something, something has been located
When he requires something, something has been
2024-07-09 12:41:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 12:45:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 12:49:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4561,  0.1731,  0.1445,  ...,  0.1931,  0.4297,  0.7339],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.4919,  0.1669,  0.1312,  ...,  0.1964,  0.4265,  0.7661],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0159, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7930, -1.2305, -0.5410,  ..., -0.4893,  0.2218,  1.8408],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0019, -0.0289, -0.0034,  ...,  0.0012, -0.0017, -0.0179],
        [-0.0066,  0.0124,  0.0004,  ..., -0.0167, -0.0181,  0.0142],
        [ 0.0054, -0.0033, -0.0049,  ...,  0.0195, -0.0101,  0.0071],
        ...,
        [-0.0084, -0.0082, -0.0025,  ..., -0.0034,  0.0194, -0.0196],
        [ 0.0106,  0.0063,  0.0061,  ..., -0.0226, -0.0119,  0.0064],
        [-0.0204,  0.0072,  0.0022,  ...,  0.0021, -0.0078, -0.0083]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.7485e-03, -3.8834e-03, -5.8794e-04,  ..., -5.5122e-04,
          1.0071e-03,  1.9341e-03],
        [-2.6631e-04, -3.7050e-04, -1.7681e-03,  ...,  9.2793e-04,
          1.7281e-03, -2.0409e-03],
        [-2.2697e-04,  2.4986e-04,  4.9734e-04,  ...,  1.6308e-03,
         -1.5125e-03, -1.8806e-03],
        ...,
        [ 7.8249e-04, -1.7986e-03,  5.2357e-04,  ...,  5.8770e-05,
          2.5063e-03, -2.2030e-03],
        [-5.1785e-04,  2.9926e-03,  7.8249e-04,  ..., -2.1725e-03,
         -1.3571e-03,  9.4652e-05],
        [-3.4294e-03,  6.1131e-04,  1.5697e-03,  ...,  6.7806e-04,
         -1.2465e-03, -1.8764e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7617, -1.4492, -0.3320,  ..., -1.1797,  0.4722,  0.8916]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2893,  0.1254,  0.0457,  ..., -0.4634, -0.1541,  0.0881]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 12:49:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he becomes something, something has been became
When he believes something, something has been believed
When he announces something, something has been announced
When he provides something, something has been provided
When he continues something, something has been continued
When he spends something, something has been spent
When he locates something, something has been located
When he requires something, something has been
2024-07-09 12:49:29 root INFO     [order_1_approx] starting weight calculation for When he believes something, something has been believed
When he locates something, something has been located
When he announces something, something has been announced
When he provides something, something has been provided
When he spends something, something has been spent
When he requires something, something has been required
When he becomes something, something has been became
When he continues something, something has been
2024-07-09 12:49:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 12:53:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 12:57:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0883, -0.3232,  0.8701,  ...,  0.4565,  0.2856, -0.0261],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1009, -0.3687,  0.8545,  ...,  0.4580,  0.2720, -0.0561],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0159, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3496, -2.6250, -1.9102,  ...,  2.0859,  0.4382, -1.0254],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0197, -0.0115,  0.0093,  ..., -0.0024,  0.0012, -0.0138],
        [ 0.0239,  0.0090,  0.0091,  ...,  0.0046,  0.0001, -0.0092],
        [-0.0021, -0.0045, -0.0120,  ..., -0.0076,  0.0113,  0.0051],
        ...,
        [-0.0061,  0.0278,  0.0152,  ..., -0.0022,  0.0003,  0.0075],
        [ 0.0105, -0.0096, -0.0042,  ..., -0.0138, -0.0009,  0.0028],
        [ 0.0230,  0.0023,  0.0046,  ...,  0.0032,  0.0099, -0.0156]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-4.3907e-03, -2.3117e-03, -9.2697e-04,  ...,  1.3041e-04,
          1.9503e-04,  1.1873e-04],
        [ 1.1425e-03, -1.0328e-03, -9.5987e-04,  ...,  1.5945e-03,
          1.9360e-03, -1.1854e-03],
        [ 2.9612e-04,  2.2659e-03, -2.8133e-03,  ..., -1.0290e-03,
          3.8314e-04,  9.2697e-04],
        ...,
        [-6.6996e-05,  2.5821e-04, -2.6703e-05,  ..., -4.7684e-07,
          4.5633e-04, -4.6539e-04],
        [-9.5463e-04,  1.1911e-03,  2.1458e-05,  ..., -2.8172e-03,
         -1.6680e-03, -2.2650e-04],
        [-6.0558e-05, -7.0572e-04,  1.0651e-04,  ..., -2.7251e-04,
         -1.8940e-03, -1.6785e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4551, -1.5068, -1.7441,  ...,  2.3047, -0.1570, -0.9111]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1057, -0.0463,  0.2209,  ..., -0.0870,  0.0181, -0.1168]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 12:57:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he believes something, something has been believed
When he locates something, something has been located
When he announces something, something has been announced
When he provides something, something has been provided
When he spends something, something has been spent
When he requires something, something has been required
When he becomes something, something has been became
When he continues something, something has been
2024-07-09 12:57:38 root INFO     [order_1_approx] starting weight calculation for When he provides something, something has been provided
When he spends something, something has been spent
When he believes something, something has been believed
When he requires something, something has been required
When he continues something, something has been continued
When he becomes something, something has been became
When he locates something, something has been located
When he announces something, something has been
2024-07-09 12:57:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 13:01:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 13:05:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7139,  0.2236,  1.5684,  ...,  0.0979, -0.2974,  0.7637],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.6987,  0.1998,  1.4307,  ...,  0.0904, -0.3062,  0.7246],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0159, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0576, -1.9561, -4.8984,  ...,  2.0820,  4.0547,  2.1484],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0033, -0.0283,  0.0031,  ...,  0.0119, -0.0081, -0.0057],
        [-0.0195, -0.0050,  0.0020,  ...,  0.0152, -0.0135,  0.0003],
        [ 0.0194,  0.0202, -0.0059,  ...,  0.0067, -0.0008,  0.0231],
        ...,
        [-0.0177,  0.0051, -0.0080,  ..., -0.0142, -0.0040, -0.0003],
        [ 0.0282, -0.0020, -0.0086,  ..., -0.0057, -0.0159,  0.0009],
        [-0.0095, -0.0014,  0.0034,  ..., -0.0029,  0.0041,  0.0061]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.1072e-03, -4.8065e-03,  6.1226e-04,  ...,  8.6212e-04,
         -2.4056e-04,  2.0370e-03],
        [-6.9189e-04,  7.0858e-04, -1.7548e-04,  ...,  1.6985e-03,
          1.9684e-03, -2.4319e-03],
        [ 9.4509e-04,  1.3952e-03, -2.7828e-03,  ..., -6.2990e-04,
         -2.2736e-03,  8.6784e-04],
        ...,
        [ 3.1066e-04,  2.1191e-03, -1.0729e-03,  ..., -7.3099e-04,
          2.4509e-03, -4.6420e-04],
        [-5.7697e-05,  2.7514e-04,  3.1424e-04,  ..., -1.3008e-03,
         -2.2335e-03, -1.4687e-03],
        [ 3.8791e-04, -2.9030e-03,  2.3651e-03,  ...,  2.2469e-03,
          6.6948e-04,  2.1420e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9570, -1.2402, -4.8164,  ...,  2.5977,  4.5938,  3.0684]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1565, -0.1113,  0.0360,  ..., -0.2119,  0.0155,  0.0164]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 13:05:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he provides something, something has been provided
When he spends something, something has been spent
When he believes something, something has been believed
When he requires something, something has been required
When he continues something, something has been continued
When he becomes something, something has been became
When he locates something, something has been located
When he announces something, something has been
2024-07-09 13:05:47 root INFO     [order_1_approx] starting weight calculation for When he continues something, something has been continued
When he believes something, something has been believed
When he announces something, something has been announced
When he requires something, something has been required
When he becomes something, something has been became
When he spends something, something has been spent
When he locates something, something has been located
When he provides something, something has been
2024-07-09 13:05:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 13:09:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 13:13:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2766,  0.4888,  0.2021,  ..., -0.0283, -0.0969,  0.4670],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.2983,  0.5059,  0.1884,  ..., -0.0278, -0.1223,  0.4739],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0159, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2490,  1.1494, -0.4971,  ..., -0.6348,  0.1177,  0.0000],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0032, -0.0151,  0.0130,  ...,  0.0069, -0.0029, -0.0135],
        [-0.0098, -0.0042,  0.0108,  ...,  0.0006,  0.0007,  0.0057],
        [ 0.0082,  0.0033, -0.0095,  ...,  0.0017,  0.0002,  0.0068],
        ...,
        [-0.0161, -0.0015, -0.0113,  ..., -0.0038,  0.0135,  0.0029],
        [ 0.0462,  0.0089, -0.0008,  ..., -0.0164, -0.0009,  0.0072],
        [-0.0071,  0.0024, -0.0071,  ..., -0.0091,  0.0071,  0.0084]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.8114e-03, -3.2349e-03,  7.1096e-04,  ...,  2.1458e-04,
         -4.9257e-04, -1.5392e-03],
        [-1.2293e-03, -1.8024e-03,  8.4686e-04,  ...,  2.8801e-03,
          2.9488e-03, -8.1396e-04],
        [-2.8152e-03, -8.3208e-05, -1.2827e-03,  ..., -8.6069e-05,
         -4.8018e-04,  9.1696e-04],
        ...,
        [-1.8482e-03, -1.8654e-03,  1.2426e-03,  ..., -1.6289e-03,
          3.6011e-03, -1.2360e-03],
        [ 2.3060e-03,  2.2240e-03,  1.0195e-03,  ..., -1.9302e-03,
          1.2088e-04, -1.0824e-03],
        [-4.2343e-03,  3.1948e-04,  1.1377e-03,  ...,  1.5507e-03,
         -1.8921e-03,  1.2255e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9336,  1.2852,  0.4502,  ..., -0.6548,  0.8491, -0.7280]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1243, -0.1953,  0.4536,  ..., -0.2339, -0.4285,  0.4312]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 13:13:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he continues something, something has been continued
When he believes something, something has been believed
When he announces something, something has been announced
When he requires something, something has been required
When he becomes something, something has been became
When he spends something, something has been spent
When he locates something, something has been located
When he provides something, something has been
2024-07-09 13:13:57 root INFO     [order_1_approx] starting weight calculation for When he continues something, something has been continued
When he announces something, something has been announced
When he locates something, something has been located
When he provides something, something has been provided
When he becomes something, something has been became
When he spends something, something has been spent
When he requires something, something has been required
When he believes something, something has been
2024-07-09 13:13:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 13:18:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 13:22:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4072,  0.2751, -0.1467,  ...,  0.2178,  0.8457, -0.3242],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.4192,  0.2644, -0.1559,  ...,  0.2117,  0.8203, -0.3613],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0159, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.0156, -0.6201,  1.2812,  ...,  0.5898, -3.9316,  0.2012],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0161, -0.0095, -0.0025,  ...,  0.0038,  0.0014, -0.0154],
        [-0.0082,  0.0044,  0.0197,  ...,  0.0051, -0.0045,  0.0016],
        [ 0.0114,  0.0029, -0.0005,  ...,  0.0117, -0.0042,  0.0129],
        ...,
        [-0.0229, -0.0060,  0.0075,  ..., -0.0092, -0.0008, -0.0143],
        [ 0.0011, -0.0130, -0.0121,  ..., -0.0136, -0.0045, -0.0036],
        [ 0.0064, -0.0069, -0.0093,  ...,  0.0005,  0.0185, -0.0121]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.3422e-03,  8.3542e-04, -3.9506e-04,  ...,  2.8095e-03,
          9.1076e-04,  6.4850e-04],
        [ 3.2187e-05, -2.1572e-03,  9.5844e-04,  ...,  1.9722e-03,
          2.9106e-03, -3.0365e-03],
        [ 8.4925e-04,  1.6785e-03, -7.9012e-04,  ...,  1.3409e-03,
          6.2656e-04, -3.0398e-06],
        ...,
        [-2.2926e-03, -3.0327e-03, -7.2813e-04,  ...,  8.2779e-04,
          4.1628e-04, -1.0824e-03],
        [ 1.0357e-03,  1.6680e-03, -3.1137e-04,  ..., -1.1206e-03,
         -7.6580e-04,  1.6975e-04],
        [-2.5520e-03, -2.7294e-03, -3.0041e-04,  ...,  1.5717e-03,
          4.4823e-04, -2.5196e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7012, -0.9717,  1.1777,  ...,  1.0635, -3.1914,  0.6592]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.4329,  0.0372,  0.2559,  ..., -0.2593, -0.1477, -0.0693]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 13:22:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he continues something, something has been continued
When he announces something, something has been announced
When he locates something, something has been located
When he provides something, something has been provided
When he becomes something, something has been became
When he spends something, something has been spent
When he requires something, something has been required
When he believes something, something has been
2024-07-09 13:22:03 root INFO     total operator prediction time: 3910.3002631664276 seconds
2024-07-09 13:22:03 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on animal - shelter
2024-07-09 13:22:05 root INFO     building operator animal - shelter
2024-07-09 13:22:06 root INFO     [order_1_approx] starting weight calculation for The place seal lives in is called den
The place chimpanzee lives in is called grove
The place snake lives in is called nest
The place trout lives in is called river
The place rat lives in is called nest
The place chinchilla lives in is called nest
The place wolf lives in is called den
The place rabbit lives in is called
2024-07-09 13:22:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 13:26:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 13:30:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0391, -0.1016, -0.4922,  ...,  0.2181, -1.2402, -0.5977],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.0279, -0.1078, -0.4146,  ...,  0.1812, -1.0654, -0.5469],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1641,  0.7266, -0.6318,  ..., -5.4375,  1.4404,  1.4180],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0108, -0.0173,  0.0088,  ...,  0.0047,  0.0023,  0.0018],
        [ 0.0055,  0.0027,  0.0035,  ...,  0.0217, -0.0041, -0.0065],
        [-0.0040,  0.0103,  0.0196,  ..., -0.0060,  0.0193,  0.0132],
        ...,
        [ 0.0021, -0.0098,  0.0117,  ..., -0.0043,  0.0017,  0.0088],
        [ 0.0019, -0.0145,  0.0050,  ..., -0.0167,  0.0050,  0.0108],
        [-0.0066, -0.0056,  0.0056,  ..., -0.0070,  0.0047,  0.0002]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 6.7353e-05,  8.0299e-04,  3.7670e-04,  ..., -6.5327e-04,
         -9.8109e-05,  4.8018e-04],
        [ 8.2493e-05, -2.1152e-03,  3.5191e-03,  ...,  1.5202e-03,
          5.8556e-04, -1.8826e-03],
        [ 3.1185e-04,  1.5202e-03, -6.3241e-05,  ..., -1.2798e-03,
          1.2302e-04,  3.1776e-03],
        ...,
        [ 3.0422e-03,  6.8617e-04,  1.2665e-03,  ..., -2.2163e-03,
          1.2417e-03,  6.6280e-04],
        [ 1.6775e-03,  1.9150e-03,  3.3903e-04,  ..., -1.0071e-03,
         -1.4467e-03,  5.3930e-04],
        [-1.0055e-04, -6.5660e-04, -1.4477e-03,  ...,  4.7159e-04,
         -9.2983e-05,  2.9993e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1323,  0.2007,  0.6309,  ..., -5.5430,  1.6660,  1.4434]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1957, -0.3757,  0.1243,  ...,  0.0743,  0.0440,  0.0620]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 13:30:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place seal lives in is called den
The place chimpanzee lives in is called grove
The place snake lives in is called nest
The place trout lives in is called river
The place rat lives in is called nest
The place chinchilla lives in is called nest
The place wolf lives in is called den
The place rabbit lives in is called
2024-07-09 13:30:15 root INFO     [order_1_approx] starting weight calculation for The place wolf lives in is called den
The place snake lives in is called nest
The place rat lives in is called nest
The place rabbit lives in is called burrow
The place chinchilla lives in is called nest
The place seal lives in is called den
The place trout lives in is called river
The place chimpanzee lives in is called
2024-07-09 13:30:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 13:34:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 13:38:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0742, -0.6650, -0.3320,  ...,  0.0574,  0.2812,  1.3701],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.0624, -0.6333, -0.2981,  ...,  0.0528,  0.2308,  1.2373],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.3789, -3.7285, -0.8330,  ..., -3.6875,  1.7129,  1.0303],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.0970e-03, -1.4969e-02, -1.3138e-02,  ...,  2.8000e-03,
         -8.6060e-03,  3.8147e-05],
        [-2.0370e-03, -6.7177e-03,  3.6507e-03,  ...,  1.5121e-02,
          6.7368e-03, -5.9776e-03],
        [ 1.4534e-02,  9.4070e-03, -7.3862e-04,  ..., -2.4109e-03,
         -5.8174e-05, -8.5907e-03],
        ...,
        [-7.6866e-03,  4.1885e-03, -4.4861e-03,  ..., -4.0207e-03,
          4.2725e-03,  1.5839e-02],
        [-5.0583e-03,  1.3275e-02, -1.4153e-03,  ..., -1.6642e-03,
         -2.6283e-03,  7.6523e-03],
        [-3.9558e-03, -9.6207e-03,  8.1787e-03,  ...,  3.0670e-03,
         -4.2725e-03,  4.2953e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-6.4850e-04, -6.8569e-04,  1.2131e-03,  ...,  1.5392e-03,
         -1.2999e-03,  9.6321e-05],
        [ 4.8518e-04,  5.2214e-05,  1.5097e-03,  ...,  1.9064e-03,
          2.6774e-04, -2.4772e-04],
        [ 3.0088e-04,  1.7679e-04,  3.1352e-05,  ..., -3.1710e-05,
         -1.7014e-03, -5.9271e-04],
        ...,
        [ 3.3975e-04,  1.6184e-03, -9.3460e-05,  ..., -2.1210e-03,
         -2.4986e-04,  1.7319e-03],
        [ 3.3617e-04,  7.0810e-04, -1.1921e-03,  ..., -3.6883e-04,
         -1.8978e-03, -4.6706e-04],
        [-5.8413e-04, -1.1396e-03, -1.4534e-03,  ..., -1.7655e-04,
          1.0929e-03,  1.8311e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.9766, -3.6504, -0.7002,  ..., -3.9883,  2.0820,  1.3418]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0737, -0.2216,  0.0802,  ...,  0.0676,  0.0196,  0.0818]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 13:38:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place wolf lives in is called den
The place snake lives in is called nest
The place rat lives in is called nest
The place rabbit lives in is called burrow
The place chinchilla lives in is called nest
The place seal lives in is called den
The place trout lives in is called river
The place chimpanzee lives in is called
2024-07-09 13:38:21 root INFO     [order_1_approx] starting weight calculation for The place wolf lives in is called den
The place snake lives in is called nest
The place chinchilla lives in is called nest
The place seal lives in is called den
The place chimpanzee lives in is called grove
The place rabbit lives in is called burrow
The place trout lives in is called river
The place rat lives in is called
2024-07-09 13:38:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 13:42:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 13:46:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3489, -0.0117, -0.1641,  ...,  0.1553, -0.7856,  0.2407],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3003, -0.0272, -0.1471,  ...,  0.1337, -0.6914,  0.1903],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.3809, -2.5000, -0.0317,  ..., -5.1562, -2.3164,  0.2368],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0169, -0.0130,  0.0037,  ...,  0.0156,  0.0036, -0.0045],
        [ 0.0098,  0.0093,  0.0195,  ...,  0.0252,  0.0046, -0.0049],
        [-0.0126,  0.0125,  0.0014,  ...,  0.0053,  0.0283, -0.0179],
        ...,
        [ 0.0026, -0.0142, -0.0010,  ..., -0.0051, -0.0099,  0.0122],
        [-0.0096, -0.0086,  0.0023,  ..., -0.0160,  0.0111, -0.0112],
        [-0.0115, -0.0126,  0.0043,  ..., -0.0054,  0.0030, -0.0099]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.7462e-03,  7.8011e-04,  1.9932e-03,  ..., -1.2293e-03,
          6.7663e-04,  6.1607e-04],
        [-1.0519e-03,  2.0370e-03,  1.1988e-03,  ...,  1.4639e-03,
          2.1152e-03, -2.7275e-03],
        [ 9.5654e-04,  2.4052e-03, -2.0409e-03,  ..., -2.0924e-03,
          5.5265e-04, -1.5221e-03],
        ...,
        [-1.4102e-04,  1.1139e-03,  5.1498e-04,  ..., -3.3226e-03,
          1.5841e-03, -2.3403e-03],
        [ 8.3828e-04,  2.4109e-03, -6.3181e-05,  ..., -2.7103e-03,
         -2.0905e-03,  1.2994e-04],
        [-1.7214e-04, -1.8501e-03, -5.2929e-05,  ..., -3.5930e-04,
         -2.0065e-03,  9.2793e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.3105, -3.4121,  0.7832,  ..., -5.1133, -1.1172,  1.0605]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.3696, -0.1639,  0.2998,  ..., -0.2644,  0.0367,  0.3821]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 13:46:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place wolf lives in is called den
The place snake lives in is called nest
The place chinchilla lives in is called nest
The place seal lives in is called den
The place chimpanzee lives in is called grove
The place rabbit lives in is called burrow
The place trout lives in is called river
The place rat lives in is called
2024-07-09 13:46:30 root INFO     [order_1_approx] starting weight calculation for The place rat lives in is called nest
The place seal lives in is called den
The place chinchilla lives in is called nest
The place snake lives in is called nest
The place chimpanzee lives in is called grove
The place trout lives in is called river
The place rabbit lives in is called burrow
The place wolf lives in is called
2024-07-09 13:46:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 13:50:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 13:54:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6094, -0.5322, -0.8359,  ...,  0.8506, -0.9707,  0.2935],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.5444, -0.5083, -0.7290,  ...,  0.7363, -0.8799,  0.2450],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0273, -3.5000, -2.5273,  ..., -3.4844,  1.7432,  4.3125],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0146, -0.0091, -0.0068,  ..., -0.0060, -0.0044, -0.0135],
        [-0.0007,  0.0080,  0.0130,  ...,  0.0100, -0.0111, -0.0025],
        [-0.0139,  0.0036,  0.0124,  ..., -0.0186,  0.0252, -0.0059],
        ...,
        [-0.0056, -0.0118, -0.0004,  ...,  0.0028, -0.0059,  0.0269],
        [ 0.0042, -0.0036,  0.0052,  ..., -0.0095,  0.0060, -0.0186],
        [ 0.0155, -0.0020,  0.0037,  ...,  0.0084, -0.0076,  0.0002]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.2436e-03, -4.9210e-04,  1.1292e-03,  ..., -9.6703e-04,
         -5.7364e-04, -2.1229e-03],
        [-2.0337e-04, -6.2656e-04,  7.8630e-04,  ...,  1.2188e-03,
          3.6454e-04, -4.1103e-04],
        [ 2.4033e-04,  1.5593e-03,  2.7618e-03,  ..., -2.1381e-03,
         -1.1787e-03, -1.8537e-04],
        ...,
        [ 1.8349e-03,  4.6349e-04, -1.2815e-04,  ..., -2.5253e-03,
         -7.3051e-04,  4.3058e-04],
        [ 8.2016e-04,  8.6880e-04,  1.9321e-03,  ..., -7.8535e-04,
         -4.8923e-04, -3.6602e-03],
        [ 1.3866e-03, -2.5043e-03, -1.3618e-03,  ...,  2.9469e-04,
         -1.0729e-05, -1.7405e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4468, -3.2891, -2.6836,  ..., -3.5859,  1.6562,  4.3086]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0966,  0.0770,  0.1133,  ...,  0.1241, -0.5215, -0.2181]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 13:54:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place rat lives in is called nest
The place seal lives in is called den
The place chinchilla lives in is called nest
The place snake lives in is called nest
The place chimpanzee lives in is called grove
The place trout lives in is called river
The place rabbit lives in is called burrow
The place wolf lives in is called
2024-07-09 13:54:38 root INFO     [order_1_approx] starting weight calculation for The place trout lives in is called river
The place chimpanzee lives in is called grove
The place wolf lives in is called den
The place snake lives in is called nest
The place chinchilla lives in is called nest
The place rabbit lives in is called burrow
The place rat lives in is called nest
The place seal lives in is called
2024-07-09 13:54:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 13:58:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 14:02:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2734,  1.1387, -0.6338,  ...,  0.7173, -0.8481,  0.8770],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.2561,  1.0566, -0.5684,  ...,  0.6367, -0.7886,  0.7998],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.3145,  1.2734, -4.7852,  ..., -0.2517,  3.1191,  2.6562],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.3794e-02, -3.6507e-03, -7.0381e-03,  ...,  4.8218e-03,
         -1.9302e-03, -6.9580e-03],
        [-3.6697e-03, -7.6294e-05,  3.6354e-03,  ..., -1.7509e-03,
         -1.4954e-02,  1.0315e-02],
        [-4.1580e-03,  2.5314e-02, -2.7885e-03,  ...,  5.9433e-03,
          8.1177e-03,  6.2637e-03],
        ...,
        [-9.2773e-03, -2.1648e-03, -3.4676e-03,  ...,  3.8681e-03,
         -2.7733e-03,  2.0142e-02],
        [-8.4839e-03, -1.9264e-03,  2.3865e-02,  ..., -1.0849e-02,
         -9.0637e-03,  1.9798e-03],
        [ 8.1635e-03, -1.4992e-03, -1.1429e-02,  ...,  1.2074e-03,
          5.5504e-03,  1.0468e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.2970e-04,  1.3137e-04,  2.9964e-03,  ..., -1.1501e-03,
         -4.2915e-06, -1.0509e-03],
        [ 1.2398e-03, -7.1573e-04,  3.6001e-04,  ...,  1.3580e-03,
          6.6662e-04,  6.7806e-04],
        [ 8.3113e-04, -3.5381e-04, -3.0994e-04,  ..., -1.2989e-03,
         -1.1301e-03,  6.5517e-04],
        ...,
        [ 2.0695e-03,  5.6791e-04,  1.1387e-03,  ..., -1.9684e-03,
         -3.2759e-04, -1.9588e-03],
        [-6.8283e-04,  4.7445e-04,  2.2087e-03,  ..., -1.3561e-03,
         -8.4114e-04, -1.2035e-03],
        [-4.9973e-04,  8.4925e-04, -7.7438e-04,  ...,  1.1520e-03,
         -3.8075e-04,  1.3466e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5166,  1.4697, -5.0664,  ..., -0.3418,  3.8223,  2.7949]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0818, -0.5581,  0.1907,  ..., -0.1746,  0.0046,  0.2035]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 14:02:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place trout lives in is called river
The place chimpanzee lives in is called grove
The place wolf lives in is called den
The place snake lives in is called nest
The place chinchilla lives in is called nest
The place rabbit lives in is called burrow
The place rat lives in is called nest
The place seal lives in is called
2024-07-09 14:02:47 root INFO     [order_1_approx] starting weight calculation for The place rat lives in is called nest
The place rabbit lives in is called burrow
The place trout lives in is called river
The place seal lives in is called den
The place wolf lives in is called den
The place chimpanzee lives in is called grove
The place snake lives in is called nest
The place chinchilla lives in is called
2024-07-09 14:02:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 14:06:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 14:10:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4199, -1.0996, -0.5146,  ..., -0.3384, -0.4519,  0.1081],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 1.3301, -1.0732, -0.4714,  ..., -0.2998, -0.4360,  0.0801],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1992, -2.8789, -2.9922,  ..., -3.5391,  2.1621, -0.2642],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0136, -0.0083, -0.0024,  ...,  0.0049, -0.0003, -0.0046],
        [-0.0007,  0.0115,  0.0177,  ...,  0.0055,  0.0143, -0.0082],
        [ 0.0100,  0.0048,  0.0180,  ..., -0.0080,  0.0157,  0.0247],
        ...,
        [ 0.0043, -0.0012, -0.0036,  ...,  0.0156, -0.0094,  0.0095],
        [-0.0033,  0.0013, -0.0090,  ...,  0.0008, -0.0007,  0.0092],
        [-0.0101, -0.0035, -0.0014,  ..., -0.0083,  0.0051, -0.0031]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 4.0197e-04,  8.5354e-04,  1.3065e-04,  ...,  1.2016e-03,
         -4.7731e-04, -1.1234e-03],
        [-4.2295e-04,  3.3116e-04,  1.1072e-03,  ..., -1.0151e-04,
          2.5558e-04,  5.7507e-04],
        [ 9.2030e-05,  8.6641e-04, -4.4823e-05,  ..., -1.3208e-04,
         -8.7261e-04, -4.4870e-04],
        ...,
        [ 8.9169e-04, -1.1435e-03,  1.8311e-03,  ..., -7.6723e-04,
          1.6272e-04,  1.0405e-03],
        [ 9.6703e-04,  7.0572e-04, -3.0613e-04,  ...,  1.8835e-03,
         -6.0940e-04, -1.2541e-03],
        [-2.0103e-03, -7.2575e-04,  3.2902e-04,  ...,  7.0429e-04,
          2.1343e-03,  4.9305e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.8154, -2.4688, -2.7715,  ..., -3.3281,  1.5117, -0.1172]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0012, -0.0049,  0.2157,  ..., -0.1848, -0.0277, -0.1202]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 14:10:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place rat lives in is called nest
The place rabbit lives in is called burrow
The place trout lives in is called river
The place seal lives in is called den
The place wolf lives in is called den
The place chimpanzee lives in is called grove
The place snake lives in is called nest
The place chinchilla lives in is called
2024-07-09 14:10:58 root INFO     [order_1_approx] starting weight calculation for The place rabbit lives in is called burrow
The place seal lives in is called den
The place trout lives in is called river
The place wolf lives in is called den
The place rat lives in is called nest
The place chimpanzee lives in is called grove
The place chinchilla lives in is called nest
The place snake lives in is called
2024-07-09 14:10:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 14:15:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 14:19:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6719,  0.0634, -0.7700,  ...,  0.9595,  0.1791,  0.0725],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.5967,  0.0392, -0.6709,  ...,  0.8257,  0.1377,  0.0408],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.9922, -1.2207,  0.2920,  ..., -1.6973,  3.8828,  0.5054],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0200, -0.0050, -0.0112,  ...,  0.0020,  0.0039, -0.0104],
        [-0.0073, -0.0030,  0.0228,  ...,  0.0096, -0.0109, -0.0075],
        [ 0.0088, -0.0015,  0.0097,  ..., -0.0166,  0.0201,  0.0085],
        ...,
        [-0.0072, -0.0149,  0.0165,  ...,  0.0020, -0.0096,  0.0143],
        [-0.0084, -0.0160,  0.0093,  ..., -0.0111,  0.0022,  0.0070],
        [ 0.0003, -0.0037,  0.0029,  ..., -0.0017,  0.0051,  0.0063]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.0431e-04, -3.9196e-04, -4.3440e-04,  ...,  5.9247e-05,
         -1.2779e-03, -1.3809e-03],
        [ 2.8825e-04,  7.0000e-04,  3.2444e-03,  ...,  5.8174e-04,
          1.0033e-03, -2.1610e-03],
        [ 2.1305e-03,  4.3440e-04, -3.4714e-03,  ..., -3.3627e-03,
          1.0519e-03, -3.0184e-04],
        ...,
        [ 1.6346e-03, -3.5334e-04,  2.9907e-03,  ..., -2.3346e-03,
          7.9775e-04, -9.6416e-04],
        [-7.0429e-04, -1.9515e-04, -2.2483e-04,  ..., -2.1706e-03,
         -5.5027e-04, -1.8692e-04],
        [ 4.4227e-04, -9.7847e-04, -1.9760e-03,  ..., -1.0605e-03,
          1.2934e-04,  5.9032e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.7617, -1.4746,  0.3379,  ..., -1.9023,  4.1758,  0.5352]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.2927, -0.4392,  0.2983,  ..., -0.2283,  0.1890,  0.4399]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 14:19:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place rabbit lives in is called burrow
The place seal lives in is called den
The place trout lives in is called river
The place wolf lives in is called den
The place rat lives in is called nest
The place chimpanzee lives in is called grove
The place chinchilla lives in is called nest
The place snake lives in is called
2024-07-09 14:19:05 root INFO     [order_1_approx] starting weight calculation for The place seal lives in is called den
The place snake lives in is called nest
The place rat lives in is called nest
The place wolf lives in is called den
The place rabbit lives in is called burrow
The place chinchilla lives in is called nest
The place chimpanzee lives in is called grove
The place trout lives in is called
2024-07-09 14:19:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 14:23:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 14:27:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5034,  0.9023, -0.1638,  ...,  0.4048, -0.6777,  0.7222],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.4175,  0.7578, -0.1450,  ...,  0.3276, -0.5825,  0.5933],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5439,  2.0000, -3.3027,  ...,  1.2891,  1.7207,  0.9600],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0085,  0.0002, -0.0150,  ...,  0.0014, -0.0086, -0.0007],
        [-0.0023, -0.0051,  0.0056,  ...,  0.0114, -0.0041, -0.0016],
        [ 0.0043,  0.0221,  0.0086,  ...,  0.0063,  0.0129,  0.0186],
        ...,
        [-0.0079, -0.0276, -0.0018,  ...,  0.0045, -0.0160,  0.0067],
        [-0.0095,  0.0106, -0.0007,  ..., -0.0152,  0.0011, -0.0038],
        [-0.0074, -0.0069, -0.0036,  ..., -0.0032,  0.0045,  0.0133]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.8892e-03,  7.8392e-04, -9.2983e-05,  ..., -4.6992e-04,
         -1.0815e-03, -2.4629e-04],
        [ 1.1063e-03, -1.3990e-03,  1.1854e-03,  ...,  8.1968e-04,
          2.2602e-03, -2.5806e-03],
        [-2.8086e-04,  3.3760e-04, -1.0777e-03,  ..., -1.5182e-03,
         -9.4414e-04,  4.3602e-03],
        ...,
        [ 3.1319e-03, -1.3628e-03,  1.1320e-03,  ..., -9.5177e-04,
          9.5367e-07, -1.7204e-03],
        [-4.4847e-04,  5.9938e-04, -1.4305e-06,  ..., -2.7828e-03,
          3.5763e-07, -1.8597e-03],
        [ 8.2135e-05,  7.4267e-05, -2.7561e-04,  ..., -1.5869e-03,
          8.7261e-04,  3.7360e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0879,  1.3125, -2.7637,  ...,  0.0029,  2.5762,  1.0547]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2172, -0.4224,  0.7256,  ..., -0.1985,  0.0466,  0.0193]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 14:27:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place seal lives in is called den
The place snake lives in is called nest
The place rat lives in is called nest
The place wolf lives in is called den
The place rabbit lives in is called burrow
The place chinchilla lives in is called nest
The place chimpanzee lives in is called grove
The place trout lives in is called
2024-07-09 14:27:14 root INFO     total operator prediction time: 3908.7341980934143 seconds
2024-07-09 14:27:14 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+ment_irreg
2024-07-09 14:27:16 root INFO     building operator verb+ment_irreg
2024-07-09 14:27:16 root INFO     [order_1_approx] starting weight calculation for To advertise results in a advertisement
To arrange results in a arrangement
To fulfill results in a fulfillment
To enforce results in a enforcement
To excite results in a excitement
To invest results in a investment
To enhance results in a enhancement
To embarrass results in a
2024-07-09 14:27:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 14:31:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 14:35:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0818, -0.2363, -0.2188,  ..., -0.0918,  0.1621, -0.0314],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.0837, -0.2422, -0.2057,  ..., -0.0812,  0.1204, -0.0585],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0318, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7793,  0.4712, -0.6094,  ...,  3.8887, -2.0742,  5.9766],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0043, -0.0204,  0.0016,  ...,  0.0059,  0.0072, -0.0173],
        [-0.0136, -0.0326,  0.0082,  ..., -0.0019, -0.0139, -0.0063],
        [ 0.0141,  0.0186,  0.0036,  ...,  0.0169,  0.0076,  0.0115],
        ...,
        [-0.0183, -0.0312, -0.0061,  ..., -0.0142, -0.0110, -0.0182],
        [ 0.0222,  0.0164,  0.0068,  ..., -0.0093, -0.0154,  0.0166],
        [-0.0129,  0.0175,  0.0024,  ..., -0.0063,  0.0151, -0.0247]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.5459e-03, -2.0084e-03, -1.9932e-03,  ..., -1.7586e-03,
         -8.3780e-04,  2.0561e-03],
        [ 2.0866e-03, -2.8396e-04, -1.9779e-03,  ...,  2.2268e-04,
          3.7909e-04,  2.0065e-03],
        [ 4.0674e-04,  1.5802e-03, -1.3199e-03,  ...,  6.3324e-04,
         -9.1791e-06, -9.1648e-04],
        ...,
        [-1.1044e-03,  5.3787e-04,  1.8940e-03,  ..., -3.4618e-03,
          5.6458e-04, -1.0424e-03],
        [ 7.0429e-04, -1.1120e-03,  1.2035e-03,  ..., -1.9622e-04,
         -2.1553e-03, -8.1015e-04],
        [ 2.3174e-03,  1.0166e-03, -1.0872e-04,  ..., -5.0163e-04,
          3.6001e-04, -3.4351e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0020,  1.4395, -1.0400,  ...,  3.7969, -0.8135,  6.2969]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2385,  0.3418,  0.2314,  ...,  0.0739,  0.0158, -0.1365]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 14:35:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To advertise results in a advertisement
To arrange results in a arrangement
To fulfill results in a fulfillment
To enforce results in a enforcement
To excite results in a excitement
To invest results in a investment
To enhance results in a enhancement
To embarrass results in a
2024-07-09 14:35:18 root INFO     [order_1_approx] starting weight calculation for To advertise results in a advertisement
To invest results in a investment
To excite results in a excitement
To embarrass results in a embarrassment
To enforce results in a enforcement
To arrange results in a arrangement
To enhance results in a enhancement
To fulfill results in a
2024-07-09 14:35:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 14:39:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 14:43:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0742,  0.7041,  1.9355,  ..., -0.2563, -0.2480, -0.6279],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-1.1035,  0.7139,  1.8662,  ..., -0.2500, -0.2715, -0.6802],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0318, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.9180,  2.0488, -1.0527,  ..., -2.0645,  2.6699,  6.4648],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-3.4363e-02, -9.0332e-03,  1.7883e-02,  ...,  6.3095e-03,
          5.6381e-03, -8.6594e-03],
        [ 1.8568e-03, -8.9874e-03, -4.0359e-03,  ...,  3.9902e-03,
          7.6294e-06, -1.1826e-02],
        [ 1.2848e-02,  4.8218e-03, -1.4191e-02,  ...,  3.9825e-03,
         -2.7752e-03, -5.7220e-06],
        ...,
        [-1.5762e-02, -6.5308e-03, -5.7068e-03,  ..., -1.3123e-02,
         -4.1466e-03,  2.9850e-03],
        [ 6.7902e-04, -5.1651e-03,  1.4296e-03,  ..., -1.8402e-02,
         -1.5793e-02,  7.0419e-03],
        [-5.6686e-03,  9.5520e-03,  5.8823e-03,  ...,  3.6354e-03,
          9.1248e-03,  2.2278e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-8.8072e-04, -1.6489e-03, -1.1282e-03,  ..., -5.0783e-05,
         -3.2902e-04, -2.2447e-04],
        [-3.8958e-04, -7.4720e-04, -6.6519e-04,  ...,  1.2541e-04,
          1.0958e-03, -7.2622e-04],
        [-2.0003e-04,  3.2597e-03, -1.0738e-03,  ...,  4.4942e-04,
         -5.0402e-04, -3.3569e-04],
        ...,
        [-2.3460e-03,  9.9659e-04,  1.3981e-03,  ..., -8.8787e-04,
          6.6280e-04, -3.1281e-04],
        [ 1.3561e-03,  2.9755e-04,  1.5144e-03,  ..., -2.3613e-03,
         -2.2545e-03, -2.7847e-03],
        [-7.9489e-04,  9.7513e-05,  1.8525e-04,  ...,  2.2774e-03,
         -3.5954e-04, -9.4271e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.2695,  2.5723, -0.5049,  ..., -1.9648,  2.3672,  6.1758]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1941,  0.0499,  0.2659,  ..., -0.1848, -0.1816,  0.0213]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 14:43:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To advertise results in a advertisement
To invest results in a investment
To excite results in a excitement
To embarrass results in a embarrassment
To enforce results in a enforcement
To arrange results in a arrangement
To enhance results in a enhancement
To fulfill results in a
2024-07-09 14:43:24 root INFO     [order_1_approx] starting weight calculation for To enhance results in a enhancement
To invest results in a investment
To arrange results in a arrangement
To advertise results in a advertisement
To fulfill results in a fulfillment
To embarrass results in a embarrassment
To excite results in a excitement
To enforce results in a
2024-07-09 14:43:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 14:47:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 14:51:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4885,  1.0957,  1.0000,  ...,  0.4731, -0.3833,  0.5039],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.5088,  1.1299,  0.9629,  ...,  0.4648, -0.4097,  0.4993],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0318, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0957, -2.0723, -2.1973,  ..., -2.9902, -0.2334,  7.6523],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0002, -0.0036, -0.0172,  ...,  0.0108,  0.0020, -0.0116],
        [ 0.0127, -0.0279,  0.0053,  ..., -0.0007, -0.0046, -0.0130],
        [ 0.0194, -0.0002, -0.0114,  ...,  0.0181,  0.0081,  0.0006],
        ...,
        [-0.0122, -0.0095, -0.0037,  ..., -0.0093,  0.0015, -0.0058],
        [-0.0047,  0.0170, -0.0091,  ...,  0.0080, -0.0173,  0.0028],
        [-0.0233,  0.0203,  0.0246,  ...,  0.0099,  0.0259, -0.0139]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 0.0008, -0.0015, -0.0010,  ...,  0.0014,  0.0002,  0.0008],
        [ 0.0004, -0.0024, -0.0012,  ...,  0.0022,  0.0013,  0.0001],
        [-0.0003,  0.0016, -0.0024,  ..., -0.0007, -0.0010, -0.0013],
        ...,
        [-0.0006,  0.0016,  0.0045,  ...,  0.0005,  0.0010, -0.0016],
        [-0.0013,  0.0017,  0.0009,  ...,  0.0018, -0.0020, -0.0012],
        [-0.0012, -0.0012,  0.0016,  ..., -0.0026, -0.0013,  0.0009]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0703, -2.3750, -1.2812,  ..., -2.6191,  0.0803,  7.7305]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.3467, -0.0358,  0.1371,  ...,  0.0100,  0.3330,  0.0112]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 14:51:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To enhance results in a enhancement
To invest results in a investment
To arrange results in a arrangement
To advertise results in a advertisement
To fulfill results in a fulfillment
To embarrass results in a embarrassment
To excite results in a excitement
To enforce results in a
2024-07-09 14:51:32 root INFO     [order_1_approx] starting weight calculation for To enhance results in a enhancement
To arrange results in a arrangement
To embarrass results in a embarrassment
To excite results in a excitement
To enforce results in a enforcement
To advertise results in a advertisement
To fulfill results in a fulfillment
To invest results in a
2024-07-09 14:51:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 14:55:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 14:59:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5713,  0.2251,  1.5664,  ...,  0.5332, -0.8311,  0.3545],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.5669,  0.2115,  1.4834,  ...,  0.5137, -0.8413,  0.3362],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0318, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8594,  3.0020, -2.3633,  ..., -0.7524,  2.0449,  5.2539],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.2373e-03, -2.4826e-02,  1.3947e-02,  ..., -8.6517e-03,
          5.4169e-03,  4.0855e-03],
        [-1.8005e-02,  2.9144e-03,  4.6768e-03,  ...,  2.0477e-02,
         -2.1362e-02,  6.8054e-03],
        [ 5.0163e-03,  2.8744e-03, -2.2232e-02,  ..., -2.2240e-03,
          2.6913e-03, -6.3515e-03],
        ...,
        [-3.5858e-03,  3.1509e-03, -1.1787e-03,  ..., -1.0193e-02,
         -2.9602e-03, -1.5045e-02],
        [ 1.9745e-02, -3.2177e-03, -2.9354e-03,  ..., -7.2479e-05,
         -1.4458e-02,  1.7395e-03],
        [-2.9259e-03, -2.7237e-03,  6.7215e-03,  ..., -9.8267e-03,
          1.7410e-02, -2.8458e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0010, -0.0012,  0.0012,  ..., -0.0001,  0.0015,  0.0010],
        [-0.0006, -0.0005,  0.0009,  ...,  0.0013,  0.0012, -0.0025],
        [-0.0011,  0.0011, -0.0018,  ...,  0.0003,  0.0002,  0.0005],
        ...,
        [-0.0013, -0.0003,  0.0010,  ...,  0.0003,  0.0022, -0.0024],
        [ 0.0019,  0.0002, -0.0006,  ..., -0.0001, -0.0013, -0.0013],
        [ 0.0006, -0.0002, -0.0007,  ..., -0.0011, -0.0010, -0.0004]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4688,  2.4141, -1.7998,  ..., -1.0840,  2.7969,  4.9922]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0460, -0.1306,  0.2197,  ..., -0.2805,  0.1348, -0.0029]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 14:59:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To enhance results in a enhancement
To arrange results in a arrangement
To embarrass results in a embarrassment
To excite results in a excitement
To enforce results in a enforcement
To advertise results in a advertisement
To fulfill results in a fulfillment
To invest results in a
2024-07-09 14:59:39 root INFO     [order_1_approx] starting weight calculation for To enhance results in a enhancement
To excite results in a excitement
To arrange results in a arrangement
To enforce results in a enforcement
To fulfill results in a fulfillment
To invest results in a investment
To embarrass results in a embarrassment
To advertise results in a
2024-07-09 14:59:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 15:03:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 15:07:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6279,  0.4736,  0.2070,  ...,  0.5176, -0.2603, -0.1843],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.6250,  0.4543,  0.1782,  ...,  0.4863, -0.2747, -0.2126],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0318, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.2666,  0.2449, -3.9336,  ..., -0.7817, -1.1113,  2.0352],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0028, -0.0171,  0.0156,  ...,  0.0036,  0.0107, -0.0016],
        [ 0.0022, -0.0159, -0.0061,  ...,  0.0095, -0.0139,  0.0050],
        [-0.0134, -0.0021, -0.0167,  ...,  0.0096,  0.0154,  0.0013],
        ...,
        [-0.0159, -0.0214,  0.0034,  ..., -0.0100,  0.0044,  0.0038],
        [ 0.0025,  0.0118,  0.0143,  ..., -0.0128, -0.0259,  0.0150],
        [-0.0273,  0.0120,  0.0101,  ...,  0.0231,  0.0102, -0.0128]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-4.1885e-03, -1.4162e-03, -8.1658e-05,  ...,  3.0732e-04,
         -1.2612e-04,  3.6883e-04],
        [ 2.0428e-03, -1.0691e-03, -2.6798e-04,  ...,  1.9264e-03,
          1.5535e-03, -1.9670e-05],
        [-2.7418e-04,  7.0810e-04, -1.5440e-03,  ...,  8.9931e-04,
          1.0562e-04,  2.4068e-04],
        ...,
        [-1.4524e-03, -1.6050e-03,  1.7672e-03,  ..., -2.0237e-03,
          3.1891e-03, -2.0254e-04],
        [ 5.8842e-04,  1.0586e-03,  5.8842e-04,  ..., -7.8821e-04,
         -2.8286e-03, -4.8494e-04],
        [-1.9407e-03,  2.7523e-03,  1.7710e-03,  ...,  2.5215e-03,
         -2.9254e-04, -7.2479e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8979, -0.3088, -4.2031,  ..., -1.3457, -0.3589,  1.2275]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1809, -0.0684,  0.1342,  ..., -0.1220,  0.2913,  0.1621]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 15:07:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To enhance results in a enhancement
To excite results in a excitement
To arrange results in a arrangement
To enforce results in a enforcement
To fulfill results in a fulfillment
To invest results in a investment
To embarrass results in a embarrassment
To advertise results in a
2024-07-09 15:07:47 root INFO     [order_1_approx] starting weight calculation for To arrange results in a arrangement
To advertise results in a advertisement
To excite results in a excitement
To enforce results in a enforcement
To invest results in a investment
To fulfill results in a fulfillment
To embarrass results in a embarrassment
To enhance results in a
2024-07-09 15:07:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 15:11:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 15:15:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3198,  0.1191,  1.3105,  ...,  0.2627, -0.1902, -0.7466],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3118,  0.1009,  1.2314,  ...,  0.2515, -0.2090, -0.7866],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0318, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3174, -0.1978,  2.2031,  ...,  0.1370, -2.0117,  8.9297],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0082, -0.0223,  0.0007,  ...,  0.0099, -0.0107,  0.0027],
        [-0.0098,  0.0013, -0.0017,  ...,  0.0094, -0.0076, -0.0080],
        [-0.0153,  0.0054, -0.0002,  ...,  0.0092,  0.0171,  0.0094],
        ...,
        [-0.0117, -0.0020,  0.0013,  ..., -0.0081, -0.0093, -0.0070],
        [ 0.0046,  0.0149,  0.0054,  ..., -0.0004, -0.0122, -0.0036],
        [-0.0048,  0.0021, -0.0004,  ...,  0.0165,  0.0199,  0.0051]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-4.0016e-03,  1.7729e-03,  1.5020e-04,  ...,  2.2793e-03,
         -1.7977e-03,  3.2501e-03],
        [ 8.7380e-05,  1.2321e-03, -3.5000e-04,  ...,  1.6708e-03,
          1.2760e-03,  7.7629e-04],
        [-2.7704e-04,  1.3390e-03, -1.0223e-03,  ...,  9.0361e-04,
          7.9632e-04, -7.3433e-04],
        ...,
        [-2.1935e-03,  2.1019e-03,  2.7924e-03,  ...,  6.3467e-04,
         -4.0960e-04,  1.7214e-03],
        [ 5.7316e-04,  2.9278e-04,  1.2102e-03,  ..., -8.8167e-04,
         -1.8253e-03,  1.2112e-03],
        [ 2.9850e-03, -5.4693e-04, -1.1845e-03,  ...,  6.2752e-04,
          2.3136e-03, -1.9369e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.6787e+00,  2.1973e-03,  2.3887e+00,  ...,  3.1616e-01,
         -1.7988e+00,  8.9688e+00]], device='cuda:1', dtype=torch.float16,
       grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.3853, -0.0146, -0.0904,  ...,  0.0657,  0.3242, -0.2585]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 15:15:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To arrange results in a arrangement
To advertise results in a advertisement
To excite results in a excitement
To enforce results in a enforcement
To invest results in a investment
To fulfill results in a fulfillment
To embarrass results in a embarrassment
To enhance results in a
2024-07-09 15:15:55 root INFO     [order_1_approx] starting weight calculation for To arrange results in a arrangement
To enhance results in a enhancement
To fulfill results in a fulfillment
To advertise results in a advertisement
To invest results in a investment
To embarrass results in a embarrassment
To enforce results in a enforcement
To excite results in a
2024-07-09 15:15:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 15:20:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 15:23:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5342, -0.4104,  1.6387,  ...,  0.8516,  0.3892,  0.6357],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.5215, -0.4275,  1.5215,  ...,  0.8047,  0.3586,  0.6133],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0318, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5396, -0.1060,  0.9731,  ..., -2.0879,  0.2134,  2.5078],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.6678e-03, -7.7934e-03, -2.6016e-03,  ..., -3.3112e-03,
         -6.5765e-03, -7.2861e-03],
        [-1.3664e-02,  1.0071e-02,  3.2730e-03,  ...,  6.7673e-03,
         -2.3285e-02,  5.9662e-03],
        [-2.3785e-03,  8.4076e-03, -4.7913e-03,  ...,  1.4114e-03,
         -7.1859e-04, -3.0289e-03],
        ...,
        [-5.4703e-03, -6.5193e-03, -5.4436e-03,  ..., -1.7090e-02,
          1.1131e-02,  2.0390e-03],
        [ 5.7716e-03,  3.6755e-03, -4.2915e-05,  ...,  1.4915e-03,
         -1.6541e-02, -7.2746e-03],
        [-5.3711e-03,  3.8223e-03,  1.2360e-02,  ...,  3.9291e-03,
          1.3229e-02, -2.5978e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.5430e-03,  3.5286e-04,  1.4925e-04,  ..., -7.8678e-04,
         -4.4584e-04,  5.2023e-04],
        [ 5.8651e-04,  8.6927e-04, -6.5947e-04,  ..., -8.0824e-05,
          1.0958e-03,  1.3208e-03],
        [-5.6362e-04,  1.3227e-03, -8.9931e-04,  ...,  5.0783e-04,
          7.8142e-05, -7.4625e-04],
        ...,
        [-9.9754e-04, -4.3917e-04,  1.3332e-03,  ..., -1.0757e-03,
          5.1546e-04,  2.6870e-04],
        [ 1.1587e-03,  3.2473e-04, -2.8968e-04,  ..., -9.9850e-04,
         -7.8678e-04, -9.5844e-05],
        [ 6.1846e-04, -1.7605e-03,  1.6651e-03,  ...,  1.9245e-03,
          8.9931e-04,  8.4996e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6855, -0.0656,  0.5684,  ..., -2.2402,  0.4863,  1.9922]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0906, -0.0506,  0.0497,  ..., -0.0905,  0.0359, -0.0225]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 15:23:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To arrange results in a arrangement
To enhance results in a enhancement
To fulfill results in a fulfillment
To advertise results in a advertisement
To invest results in a investment
To embarrass results in a embarrassment
To enforce results in a enforcement
To excite results in a
2024-07-09 15:23:59 root INFO     [order_1_approx] starting weight calculation for To fulfill results in a fulfillment
To enhance results in a enhancement
To excite results in a excitement
To invest results in a investment
To advertise results in a advertisement
To embarrass results in a embarrassment
To enforce results in a enforcement
To arrange results in a
2024-07-09 15:23:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 15:28:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 15:32:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.3611, 0.4685, 0.6416,  ..., 0.0077, 0.2705, 0.3137], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([0.3689, 0.4788, 0.6221,  ..., 0.0088, 0.2556, 0.3062], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0318, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5840, -0.7139,  0.5547,  ...,  2.4648,  1.0762,  6.0039],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0178, -0.0072,  0.0207,  ..., -0.0004,  0.0061, -0.0110],
        [-0.0040, -0.0116,  0.0048,  ...,  0.0057, -0.0080, -0.0001],
        [-0.0076,  0.0029,  0.0022,  ...,  0.0079,  0.0004,  0.0015],
        ...,
        [-0.0147, -0.0268, -0.0014,  ..., -0.0057, -0.0083,  0.0057],
        [ 0.0196,  0.0024,  0.0105,  ..., -0.0076, -0.0069, -0.0047],
        [ 0.0090,  0.0201, -0.0004,  ...,  0.0097,  0.0155, -0.0113]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-3.0499e-03, -5.2595e-04, -2.5296e-04,  ...,  5.5552e-05,
          1.9336e-04, -3.5191e-04],
        [ 2.0289e-04, -7.1096e-04, -2.4819e-04,  ...,  6.1941e-04,
          1.0576e-03, -7.4911e-04],
        [-1.0633e-03,  2.5387e-03, -4.4584e-04,  ..., -3.7789e-04,
          7.0810e-05,  8.7357e-04],
        ...,
        [-9.1648e-04,  7.4005e-04,  1.0662e-03,  ..., -5.6219e-04,
          9.7847e-04,  9.6178e-04],
        [ 2.5978e-03,  9.7036e-05, -1.3466e-03,  ..., -9.5654e-04,
         -6.9571e-04, -2.1877e-03],
        [ 7.9334e-05,  2.7037e-04,  6.6280e-05,  ...,  3.9768e-04,
         -2.6436e-03, -2.4939e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2275, -0.9639,  0.7583,  ...,  2.5703,  1.8828,  6.3867]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0099,  0.1479,  0.1511,  ..., -0.2146, -0.2229, -0.0162]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 15:32:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To fulfill results in a fulfillment
To enhance results in a enhancement
To excite results in a excitement
To invest results in a investment
To advertise results in a advertisement
To embarrass results in a embarrassment
To enforce results in a enforcement
To arrange results in a
2024-07-09 15:32:05 root INFO     total operator prediction time: 3888.4555201530457 seconds
2024-07-09 15:32:05 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on noun - plural_reg
2024-07-09 15:32:07 root INFO     building operator noun - plural_reg
2024-07-09 15:32:07 root INFO     [order_1_approx] starting weight calculation for The plural form of website is websites
The plural form of population is populations
The plural form of council is councils
The plural form of college is colleges
The plural form of street is streets
The plural form of customer is customers
The plural form of death is deaths
The plural form of week is
2024-07-09 15:32:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 15:36:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 15:40:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3604, -0.0363, -0.1533,  ...,  1.0039, -1.0801,  0.4927],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 1.3359, -0.0554, -0.1570,  ...,  0.9502, -1.0693,  0.4700],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8262, -2.8867,  0.9170,  ...,  0.5840,  3.5684,  3.1406],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0051, -0.0086,  0.0155,  ...,  0.0056,  0.0100, -0.0069],
        [-0.0063,  0.0251, -0.0085,  ..., -0.0032, -0.0084,  0.0027],
        [-0.0115, -0.0013, -0.0106,  ..., -0.0184,  0.0166, -0.0008],
        ...,
        [-0.0136, -0.0058, -0.0142,  ..., -0.0046,  0.0061, -0.0063],
        [ 0.0049, -0.0044,  0.0339,  ..., -0.0115, -0.0247,  0.0128],
        [-0.0018, -0.0015,  0.0084,  ..., -0.0042,  0.0183, -0.0028]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 4.1413e-04,  2.5797e-04,  1.3676e-03,  ..., -1.9321e-03,
          1.4925e-03, -3.8147e-05],
        [ 7.6342e-04, -5.1117e-04,  4.3178e-04,  ...,  1.8191e-04,
          4.7779e-04, -1.1425e-03],
        [-1.0271e-03,  6.0177e-04, -2.4366e-04,  ..., -5.8079e-04,
          1.0395e-03, -6.3610e-04],
        ...,
        [-1.0757e-03,  1.3189e-03, -1.0788e-04,  ..., -3.4213e-04,
          2.5249e-04, -2.2435e-04],
        [-1.9312e-03, -2.2011e-03,  2.0294e-03,  ..., -8.4305e-04,
         -1.4029e-03,  1.3332e-03],
        [-8.7166e-04,  6.2943e-04,  1.5068e-03,  ...,  3.2020e-04,
          1.0357e-03,  8.2588e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.1367, -2.1055,  1.5791,  ...,  0.1055,  3.3965,  3.3262]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.2881, -0.2220, -0.0580,  ..., -0.1582, -0.1964,  0.1409]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 15:40:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of website is websites
The plural form of population is populations
The plural form of council is councils
The plural form of college is colleges
The plural form of street is streets
The plural form of customer is customers
The plural form of death is deaths
The plural form of week is
2024-07-09 15:40:11 root INFO     [order_1_approx] starting weight calculation for The plural form of council is councils
The plural form of week is weeks
The plural form of street is streets
The plural form of death is deaths
The plural form of website is websites
The plural form of college is colleges
The plural form of population is populations
The plural form of customer is
2024-07-09 15:40:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 15:44:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 15:48:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5264, -0.5566,  0.3091,  ...,  2.1426, -0.5610,  0.7827],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.4888, -0.5576,  0.2629,  ...,  1.9336, -0.5425,  0.7246],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.7539,  2.5469,  1.5518,  ..., -5.3164, -2.4355, -0.7046],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.2436e-03, -7.2479e-03,  2.0798e-02,  ...,  5.4169e-03,
         -1.3969e-02, -7.2479e-03],
        [-2.4811e-02,  9.4757e-03,  1.2085e-02,  ...,  7.3090e-03,
         -1.7853e-02, -6.3324e-04],
        [ 2.0798e-02, -3.7518e-03, -1.5259e-05,  ..., -1.5001e-03,
          2.6810e-02,  9.6512e-03],
        ...,
        [ 2.1935e-03, -1.3962e-02, -3.3140e-04,  ..., -4.6997e-03,
          6.6910e-03,  4.6005e-03],
        [ 1.3542e-02, -9.4147e-03,  4.3030e-03,  ..., -6.1340e-03,
         -7.4577e-03,  3.5248e-03],
        [-2.3438e-02, -4.6635e-04, -8.2703e-03,  ...,  4.7302e-04,
          2.2964e-02, -1.9547e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.6298e-03,  7.1812e-04,  1.0681e-03,  ..., -1.9550e-03,
          1.9665e-03, -1.2803e-04],
        [-1.9035e-03, -1.3123e-03,  3.0398e-04,  ...,  9.0241e-05,
         -2.0199e-03, -1.0672e-03],
        [ 1.1712e-04, -1.1415e-03,  6.0225e-04,  ...,  2.4414e-04,
          1.9493e-03,  1.1282e-03],
        ...,
        [-4.1437e-04,  1.2665e-03, -3.6240e-04,  ..., -1.9779e-03,
          1.7176e-03,  5.7983e-04],
        [ 6.9380e-04,  1.8377e-03, -9.4414e-05,  ..., -3.7098e-04,
          9.3937e-04, -5.3883e-04],
        [-4.8599e-03,  3.5071e-04,  4.4107e-04,  ..., -1.1988e-03,
          1.6952e-04, -1.4515e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.9785,  2.8789,  1.4717,  ..., -5.3242, -2.5059, -0.9082]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2180, -0.0881,  0.1295,  ...,  0.3213,  0.3379,  0.2379]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 15:48:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of council is councils
The plural form of week is weeks
The plural form of street is streets
The plural form of death is deaths
The plural form of website is websites
The plural form of college is colleges
The plural form of population is populations
The plural form of customer is
2024-07-09 15:48:17 root INFO     [order_1_approx] starting weight calculation for The plural form of customer is customers
The plural form of death is deaths
The plural form of council is councils
The plural form of college is colleges
The plural form of website is websites
The plural form of week is weeks
The plural form of street is streets
The plural form of population is
2024-07-09 15:48:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 15:52:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 15:56:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5513,  0.2710, -0.2764,  ..., -0.0658, -0.1582, -0.1780],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.5464,  0.2576, -0.2795,  ..., -0.0630, -0.1788, -0.2098],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.4277, -1.3311,  1.0312,  ..., -0.2983, -1.6670,  2.3477],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0151, -0.0047,  0.0319,  ..., -0.0019, -0.0123, -0.0250],
        [-0.0159, -0.0143,  0.0070,  ...,  0.0160, -0.0150, -0.0152],
        [ 0.0168, -0.0040, -0.0116,  ..., -0.0044,  0.0287, -0.0009],
        ...,
        [-0.0138, -0.0073, -0.0121,  ..., -0.0061,  0.0115, -0.0179],
        [ 0.0075, -0.0176,  0.0103,  ..., -0.0126, -0.0105, -0.0114],
        [-0.0275,  0.0143,  0.0118,  ..., -0.0126,  0.0136,  0.0024]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.9658e-04,  1.5583e-03,  2.5826e-03,  ...,  3.0212e-03,
         -5.7793e-04,  2.3031e-04],
        [-1.0195e-03, -2.0599e-04,  1.4753e-03,  ...,  2.0351e-03,
         -7.4100e-04, -4.0588e-03],
        [-1.2875e-04,  3.4666e-04, -1.0529e-03,  ...,  1.3161e-04,
          1.3800e-03,  1.7586e-03],
        ...,
        [ 6.2656e-04, -8.3160e-04, -1.5364e-03,  ..., -1.8654e-03,
          4.2305e-03, -1.0929e-03],
        [-3.7909e-04,  2.1172e-03,  1.8196e-03,  ..., -2.5177e-04,
         -5.3263e-04, -1.9836e-03],
        [-1.9016e-03,  8.1301e-05,  1.5945e-03,  ...,  9.1934e-04,
          7.8201e-05,  1.4381e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9688, -0.7275,  0.4438,  ...,  0.1177, -1.4268,  1.9180]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1567, -0.3506, -0.1032,  ..., -0.1265, -0.0036, -0.1249]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 15:56:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of customer is customers
The plural form of death is deaths
The plural form of council is councils
The plural form of college is colleges
The plural form of website is websites
The plural form of week is weeks
The plural form of street is streets
The plural form of population is
2024-07-09 15:56:24 root INFO     [order_1_approx] starting weight calculation for The plural form of population is populations
The plural form of customer is customers
The plural form of street is streets
The plural form of website is websites
The plural form of college is colleges
The plural form of week is weeks
The plural form of death is deaths
The plural form of council is
2024-07-09 15:56:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 16:00:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 16:04:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1274, -0.0718, -0.9932,  ...,  0.7363, -0.2480, -0.4417],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.1182, -0.0905, -0.9336,  ...,  0.6875, -0.2581, -0.4634],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7734,  2.5098, -0.4941,  ..., -4.4922, -1.6309,  1.3105],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0191, -0.0135,  0.0200,  ..., -0.0042, -0.0096, -0.0108],
        [-0.0075, -0.0047,  0.0112,  ...,  0.0048, -0.0216,  0.0140],
        [ 0.0254, -0.0068, -0.0134,  ..., -0.0090,  0.0216, -0.0043],
        ...,
        [-0.0125, -0.0162, -0.0064,  ..., -0.0069,  0.0073,  0.0050],
        [-0.0196, -0.0117, -0.0122,  ..., -0.0148, -0.0134,  0.0078],
        [ 0.0071,  0.0085,  0.0133,  ...,  0.0107,  0.0179, -0.0241]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.1057e-03, -6.7425e-04,  2.0123e-03,  ..., -2.2278e-03,
         -1.5669e-03, -5.9009e-06],
        [-1.5173e-03, -8.5163e-04, -1.7834e-03,  ...,  1.5697e-03,
         -2.0523e-03,  6.3753e-04],
        [ 1.4057e-03,  3.0458e-05,  1.5426e-04,  ..., -1.7004e-03,
          1.6947e-03,  1.9798e-03],
        ...,
        [-1.4286e-03,  4.8542e-04,  9.5177e-04,  ..., -1.3523e-03,
          2.2354e-03, -5.0545e-04],
        [-9.2173e-04,  2.1381e-03,  1.4124e-03,  ..., -1.4973e-03,
         -7.8297e-04, -8.2064e-04],
        [-2.4986e-03, -3.1471e-04,  1.3332e-03,  ...,  7.1144e-04,
          1.9665e-03, -1.4925e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.0234,  2.3457, -0.4207,  ..., -4.1836, -1.2627,  1.0234]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1504, -0.2346, -0.0757,  ...,  0.1818, -0.0478,  0.1730]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 16:04:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of population is populations
The plural form of customer is customers
The plural form of street is streets
The plural form of website is websites
The plural form of college is colleges
The plural form of week is weeks
The plural form of death is deaths
The plural form of council is
2024-07-09 16:04:31 root INFO     [order_1_approx] starting weight calculation for The plural form of council is councils
The plural form of customer is customers
The plural form of week is weeks
The plural form of death is deaths
The plural form of population is populations
The plural form of street is streets
The plural form of website is websites
The plural form of college is
2024-07-09 16:04:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 16:08:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 16:12:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7764, -0.0166, -0.2278,  ...,  0.5630,  0.7490,  0.2197],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.7490, -0.0370, -0.2257,  ...,  0.5254,  0.6963,  0.1912],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.9414, -0.1177, -1.6074,  ..., -3.2207, -0.3042,  4.0742],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0042,  0.0039,  0.0148,  ..., -0.0053, -0.0023, -0.0170],
        [-0.0148,  0.0104, -0.0006,  ...,  0.0056, -0.0062, -0.0008],
        [ 0.0168, -0.0006, -0.0107,  ...,  0.0042,  0.0015,  0.0164],
        ...,
        [ 0.0142, -0.0159,  0.0045,  ..., -0.0049,  0.0093,  0.0062],
        [-0.0107, -0.0170,  0.0011,  ..., -0.0091, -0.0180,  0.0113],
        [-0.0381, -0.0177,  0.0121,  ..., -0.0153,  0.0178, -0.0439]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 2.8744e-03,  3.8171e-04, -8.9741e-04,  ..., -1.0357e-03,
          9.3746e-04, -6.7616e-04],
        [ 3.8838e-04,  1.0033e-03,  9.1600e-04,  ...,  1.2946e-04,
         -1.7822e-04,  2.5749e-04],
        [ 3.6168e-04, -1.9193e-04,  6.6519e-04,  ..., -5.2595e-04,
         -7.4244e-04,  5.4073e-04],
        ...,
        [ 1.0509e-03, -7.2718e-04,  1.2560e-03,  ..., -1.5831e-04,
          2.2755e-03, -1.5986e-04],
        [-6.2513e-04, -2.2869e-03, -5.7268e-04,  ..., -2.4662e-03,
         -1.9455e-04, -2.4261e-03],
        [-3.0212e-03, -6.4087e-04,  1.2493e-04,  ..., -1.9288e-04,
         -5.0545e-04, -7.1049e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-6.1641,  0.4575, -1.3047,  ..., -3.7344,  0.0200,  3.5664]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.2747,  0.0634,  0.1658,  ...,  0.1355, -0.1132,  0.1006]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 16:12:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of council is councils
The plural form of customer is customers
The plural form of week is weeks
The plural form of death is deaths
The plural form of population is populations
The plural form of street is streets
The plural form of website is websites
The plural form of college is
2024-07-09 16:12:37 root INFO     [order_1_approx] starting weight calculation for The plural form of week is weeks
The plural form of death is deaths
The plural form of population is populations
The plural form of website is websites
The plural form of council is councils
The plural form of college is colleges
The plural form of customer is customers
The plural form of street is
2024-07-09 16:12:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 16:16:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 16:20:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1709,  0.0150, -0.8701,  ...,  1.6094,  0.0713, -2.1758],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 1.1084, -0.0043, -0.8018,  ...,  1.4678,  0.0464, -2.1348],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5479, -0.6982, -1.0918,  ..., -4.6367,  2.2598,  2.1074],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0030, -0.0098,  0.0153,  ..., -0.0042, -0.0119, -0.0094],
        [-0.0053,  0.0152,  0.0035,  ..., -0.0042, -0.0382,  0.0026],
        [ 0.0070, -0.0072, -0.0007,  ..., -0.0133,  0.0278, -0.0001],
        ...,
        [ 0.0102, -0.0102,  0.0064,  ...,  0.0039,  0.0043, -0.0041],
        [-0.0147,  0.0019, -0.0046,  ..., -0.0027, -0.0330,  0.0158],
        [-0.0087,  0.0050, -0.0069,  ..., -0.0185,  0.0104, -0.0079]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0005, -0.0013,  0.0011,  ..., -0.0016,  0.0002, -0.0019],
        [-0.0011, -0.0012, -0.0013,  ..., -0.0003,  0.0004, -0.0040],
        [-0.0001,  0.0006,  0.0021,  ...,  0.0002,  0.0026,  0.0022],
        ...,
        [ 0.0019,  0.0004, -0.0014,  ..., -0.0017,  0.0006,  0.0010],
        [-0.0016,  0.0009, -0.0022,  ..., -0.0020, -0.0017, -0.0016],
        [-0.0023, -0.0003,  0.0014,  ...,  0.0008,  0.0013,  0.0007]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.4023, -0.6836, -1.1914,  ..., -4.8164,  1.7002,  2.1387]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0651, -0.1416, -0.2104,  ...,  0.0399,  0.2671, -0.0344]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 16:20:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of week is weeks
The plural form of death is deaths
The plural form of population is populations
The plural form of website is websites
The plural form of council is councils
The plural form of college is colleges
The plural form of customer is customers
The plural form of street is
2024-07-09 16:20:42 root INFO     [order_1_approx] starting weight calculation for The plural form of customer is customers
The plural form of street is streets
The plural form of council is councils
The plural form of population is populations
The plural form of college is colleges
The plural form of website is websites
The plural form of week is weeks
The plural form of death is
2024-07-09 16:20:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 16:24:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 16:28:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5996,  0.0620, -0.2152,  ...,  1.2510, -0.0303, -0.0049],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.5967,  0.0452, -0.2183,  ...,  1.2041, -0.0498, -0.0300],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.6172, -2.7656, -1.5820,  ..., -4.1211, -2.7520,  0.6553],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0391,  0.0095,  0.0256,  ...,  0.0017, -0.0234,  0.0202],
        [-0.0261, -0.0126, -0.0125,  ..., -0.0053, -0.0023,  0.0182],
        [ 0.0038,  0.0294, -0.0265,  ..., -0.0066,  0.0213, -0.0155],
        ...,
        [-0.0030, -0.0181, -0.0188,  ...,  0.0010,  0.0121, -0.0154],
        [ 0.0141,  0.0068,  0.0120,  ..., -0.0035, -0.0204, -0.0087],
        [-0.0046,  0.0101, -0.0024,  ..., -0.0034,  0.0036, -0.0318]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-5.6410e-04,  5.2595e-04,  5.5504e-04,  ..., -1.0052e-03,
          2.9993e-04, -1.4572e-03],
        [ 1.1444e-05,  1.4839e-03,  1.0624e-03,  ..., -3.4618e-04,
         -5.2977e-04, -1.8907e-04],
        [ 7.4863e-04,  1.2803e-04,  7.8917e-05,  ...,  9.4986e-04,
         -3.6550e-04, -8.3685e-04],
        ...,
        [-1.6689e-05, -3.1400e-04, -1.6022e-03,  ...,  2.3155e-03,
          3.1319e-03,  1.0223e-03],
        [ 5.7936e-04, -2.7132e-04,  1.1110e-03,  ..., -4.9353e-04,
         -2.5463e-04, -1.0414e-03],
        [ 1.5020e-04,  4.3631e-05,  1.8921e-03,  ...,  1.1234e-03,
         -6.4611e-05, -3.2616e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.3047, -1.8242, -3.0430,  ..., -4.1641, -2.1074, -0.0381]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.4534, -0.2554, -0.3279,  ...,  0.1993, -0.0011,  0.1840]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 16:28:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of customer is customers
The plural form of street is streets
The plural form of council is councils
The plural form of population is populations
The plural form of college is colleges
The plural form of website is websites
The plural form of week is weeks
The plural form of death is
2024-07-09 16:28:47 root INFO     [order_1_approx] starting weight calculation for The plural form of population is populations
The plural form of council is councils
The plural form of week is weeks
The plural form of street is streets
The plural form of college is colleges
The plural form of customer is customers
The plural form of death is deaths
The plural form of website is
2024-07-09 16:28:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 16:32:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 16:36:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9766,  0.8242, -0.2423,  ...,  0.7832,  0.7656,  0.3552],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.9297,  0.7886, -0.2350,  ...,  0.7202,  0.7012,  0.3208],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5938, -4.4844,  2.1855,  ..., -1.5039,  1.4883,  5.4453],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0190, -0.0044,  0.0082,  ..., -0.0029, -0.0171, -0.0075],
        [-0.0053,  0.0002, -0.0054,  ...,  0.0057,  0.0012, -0.0016],
        [ 0.0081, -0.0009, -0.0247,  ..., -0.0068,  0.0121,  0.0113],
        ...,
        [ 0.0018, -0.0121,  0.0129,  ..., -0.0178, -0.0039, -0.0044],
        [-0.0027, -0.0152,  0.0097,  ..., -0.0179, -0.0483,  0.0149],
        [-0.0112,  0.0208, -0.0079,  ..., -0.0032, -0.0021, -0.0152]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 1.1897e-04, -1.4267e-03,  1.6079e-03,  ..., -1.6670e-03,
         -2.2614e-04, -1.5855e-04],
        [-1.5697e-03, -9.6798e-04,  1.9150e-03,  ...,  5.4264e-04,
          7.4434e-04,  2.1381e-03],
        [-1.3971e-03, -6.9332e-04,  5.1975e-04,  ...,  2.8324e-03,
          7.9203e-04,  5.7554e-04],
        ...,
        [-8.3590e-04, -6.5041e-04,  1.7338e-03,  ..., -1.8034e-03,
          2.3155e-03, -2.6398e-03],
        [-1.4267e-03,  1.0557e-03,  1.0509e-03,  ..., -1.1721e-03,
         -1.2856e-03, -1.2941e-03],
        [-1.6241e-03, -3.1447e-04,  9.2220e-04,  ...,  2.2297e-03,
          8.3351e-04, -2.8610e-06]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5527, -4.2891,  1.6582,  ..., -1.5898,  1.6582,  4.7539]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.2463,  0.2267, -0.0910,  ..., -0.3047,  0.1746, -0.0959]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 16:36:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of population is populations
The plural form of council is councils
The plural form of week is weeks
The plural form of street is streets
The plural form of college is colleges
The plural form of customer is customers
The plural form of death is deaths
The plural form of website is
2024-07-09 16:36:54 root INFO     total operator prediction time: 3886.652464866638 seconds
2024-07-09 16:36:54 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on antonyms - binary
2024-07-09 16:36:56 root INFO     building operator antonyms - binary
2024-07-09 16:36:56 root INFO     [order_1_approx] starting weight calculation for The opposite of dead is alive
The opposite of off is on
The opposite of inbound is outbound
The opposite of backward is forward
The opposite of uphill is downhill
The opposite of beginning is end
The opposite of out is in
The opposite of southeast is
2024-07-09 16:36:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 16:41:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 16:45:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9150, -1.6416, -0.7725,  ..., -0.7329,  1.7676,  0.0950],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.8062, -1.4863, -0.6572,  ..., -0.6123,  1.4932,  0.0557],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.9941, -2.8789,  3.9199,  ..., -1.0703,  2.7246,  0.6016],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0296, -0.0026, -0.0029,  ..., -0.0234, -0.0091,  0.0076],
        [-0.0083, -0.0090, -0.0149,  ...,  0.0076, -0.0337,  0.0099],
        [-0.0048, -0.0047, -0.0378,  ...,  0.0029, -0.0247, -0.0239],
        ...,
        [ 0.0156, -0.0001, -0.0002,  ..., -0.0168,  0.0148, -0.0127],
        [ 0.0079, -0.0046,  0.0181,  ..., -0.0083,  0.0012, -0.0170],
        [-0.0093,  0.0060, -0.0127,  ..., -0.0022,  0.0147, -0.0061]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0007,  0.0022, -0.0006,  ..., -0.0014,  0.0013, -0.0001],
        [ 0.0008,  0.0013, -0.0005,  ..., -0.0006, -0.0009,  0.0017],
        [ 0.0010, -0.0008,  0.0008,  ...,  0.0017, -0.0015, -0.0039],
        ...,
        [ 0.0006, -0.0020, -0.0033,  ..., -0.0008,  0.0013, -0.0004],
        [ 0.0001,  0.0015, -0.0012,  ..., -0.0003, -0.0007,  0.0011],
        [ 0.0030, -0.0017, -0.0002,  ...,  0.0021, -0.0011, -0.0015]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.3906, -2.3359,  3.6992,  ..., -0.4253,  1.9297,  1.2949]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0535,  0.3511,  0.2866,  ..., -0.3564,  0.0353,  0.1127]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 16:45:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of dead is alive
The opposite of off is on
The opposite of inbound is outbound
The opposite of backward is forward
The opposite of uphill is downhill
The opposite of beginning is end
The opposite of out is in
The opposite of southeast is
2024-07-09 16:45:02 root INFO     [order_1_approx] starting weight calculation for The opposite of inbound is outbound
The opposite of beginning is end
The opposite of out is in
The opposite of backward is forward
The opposite of uphill is downhill
The opposite of southeast is southwest
The opposite of off is on
The opposite of dead is
2024-07-09 16:45:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 16:49:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-09 16:53:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0050, -1.0107, -0.0510,  ...,  0.2170, -0.5859, -0.5264],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-4.4060e-04, -1.0186e+00, -5.9631e-02,  ...,  2.0337e-01,
        -5.7764e-01, -5.4346e-01], device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9072, -3.7227, -0.8789,  ..., -5.2109,  0.0605, -1.5342],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0240, -0.0128,  0.0160,  ..., -0.0084, -0.0192,  0.0040],
        [ 0.0083, -0.0089, -0.0048,  ..., -0.0047,  0.0027, -0.0124],
        [ 0.0257, -0.0161,  0.0105,  ...,  0.0079,  0.0099,  0.0070],
        ...,
        [ 0.0173, -0.0102, -0.0022,  ...,  0.0115,  0.0060, -0.0209],
        [ 0.0188,  0.0123, -0.0058,  ..., -0.0119, -0.0158, -0.0283],
        [-0.0020, -0.0065, -0.0222,  ...,  0.0047, -0.0059, -0.0001]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 2.0256e-03,  1.1122e-04,  1.3504e-03,  ...,  9.8228e-04,
          6.9857e-04, -1.0395e-03],
        [-1.9665e-03,  3.4618e-04,  3.0708e-04,  ...,  5.3406e-05,
          2.6550e-03, -9.3699e-04],
        [ 1.0443e-03, -1.3838e-03,  1.6966e-03,  ...,  2.0580e-03,
         -4.8232e-04,  8.8406e-04],
        ...,
        [ 2.2182e-03,  6.1131e-04, -1.6584e-03,  ...,  3.8075e-04,
          8.1778e-04, -5.1451e-04],
        [ 2.8706e-03,  1.2665e-03, -2.2531e-04,  ...,  1.2150e-03,
         -1.5640e-03, -1.2436e-03],
        [ 5.3596e-04, -3.3569e-04, -2.9945e-04,  ..., -5.6028e-04,
         -1.5032e-04,  9.3746e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3389, -3.3945, -0.1738,  ..., -4.0117,  0.0160, -2.3633]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.3665,  0.1494,  0.0353,  ..., -0.0117, -0.0006,  0.0352]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-09 16:53:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of inbound is outbound
The opposite of beginning is end
The opposite of out is in
The opposite of backward is forward
The opposite of uphill is downhill
The opposite of southeast is southwest
The opposite of off is on
The opposite of dead is
2024-07-09 16:53:09 root INFO     [order_1_approx] starting weight calculation for The opposite of beginning is end
The opposite of out is in
The opposite of southeast is southwest
The opposite of backward is forward
The opposite of dead is alive
The opposite of inbound is outbound
The opposite of uphill is downhill
The opposite of off is
2024-07-09 16:53:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-09 16:57:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
