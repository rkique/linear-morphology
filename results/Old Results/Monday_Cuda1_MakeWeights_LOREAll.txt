2024-07-22 10:51:29 root INFO     loading model + tokenizer
2024-07-22 10:51:46 root INFO     model + tokenizer loaded
2024-07-22 10:51:46 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - part
2024-07-22 10:51:46 root INFO     building operator meronyms - part
2024-07-22 10:51:46 root INFO     total operator prediction time: 0.03546929359436035 seconds
2024-07-22 10:51:46 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on synonyms - exact
2024-07-22 10:51:46 root INFO     building operator synonyms - exact
2024-07-22 10:51:47 root INFO     [order_1_approx] starting weight calculation for Another word for auto is car
Another word for jewel is gem
Another word for incorrect is wrong
Another word for bicycle is bike
Another word for sofa is couch
Another word for mend is repair
Another word for rock is stone
Another word for airplane is
2024-07-22 10:51:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 10:55:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9453,  1.1113,  0.5840,  ..., -0.9033, -1.2295,  0.9248],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.8477,  0.0327,  1.6387,  ..., -4.4844,  0.2012, -2.8145],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0156,  0.0009,  0.0007,  ..., -0.0111, -0.0045,  0.0029],
        [ 0.0086, -0.0131,  0.0060,  ..., -0.0003, -0.0056,  0.0168],
        [-0.0050,  0.0038, -0.0185,  ..., -0.0117, -0.0006,  0.0212],
        ...,
        [-0.0100,  0.0003,  0.0180,  ..., -0.0068, -0.0192, -0.0152],
        [ 0.0074, -0.0127, -0.0114,  ...,  0.0095, -0.0113,  0.0067],
        [ 0.0150, -0.0129,  0.0018,  ...,  0.0038,  0.0033, -0.0032]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.7852, -0.0354,  1.6914,  ..., -3.6191,  0.5210, -2.3340]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 10:55:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for auto is car
Another word for jewel is gem
Another word for incorrect is wrong
Another word for bicycle is bike
Another word for sofa is couch
Another word for mend is repair
Another word for rock is stone
Another word for airplane is
2024-07-22 10:55:30 root INFO     [order_1_approx] starting weight calculation for Another word for jewel is gem
Another word for airplane is aeroplane
Another word for rock is stone
Another word for bicycle is bike
Another word for sofa is couch
Another word for mend is repair
Another word for incorrect is wrong
Another word for auto is
2024-07-22 10:55:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 10:59:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1523,  0.7085, -0.4919,  ...,  0.4731, -1.1338,  1.1113],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1582, -1.2041, -1.6016,  ..., -1.7236, -0.6128, -4.4922],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 9.4452e-03, -2.2400e-02,  4.6692e-03,  ...,  5.4817e-03,
         -7.7553e-03, -5.2490e-03],
        [-1.4153e-03,  1.1539e-03, -1.3176e-02,  ..., -6.9122e-03,
          2.8419e-03,  9.2773e-03],
        [ 3.6812e-03,  4.2496e-03, -4.3488e-03,  ..., -2.0523e-02,
         -1.6747e-03,  3.1769e-02],
        ...,
        [-1.8555e-02,  2.1667e-02,  1.1223e-02,  ...,  1.5717e-03,
         -1.4221e-02,  1.0773e-02],
        [ 1.2619e-02, -2.0416e-02, -1.8372e-02,  ..., -8.4152e-03,
          6.5446e-05, -1.5457e-02],
        [ 8.4305e-03, -2.8496e-03,  6.6910e-03,  ..., -4.5242e-03,
          6.9809e-03, -3.0365e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0615, -0.6353, -2.1426,  ..., -2.0996, -0.0767, -3.8438]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 10:59:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for jewel is gem
Another word for airplane is aeroplane
Another word for rock is stone
Another word for bicycle is bike
Another word for sofa is couch
Another word for mend is repair
Another word for incorrect is wrong
Another word for auto is
2024-07-22 10:59:13 root INFO     [order_1_approx] starting weight calculation for Another word for auto is car
Another word for mend is repair
Another word for incorrect is wrong
Another word for airplane is aeroplane
Another word for sofa is couch
Another word for rock is stone
Another word for jewel is gem
Another word for bicycle is
2024-07-22 10:59:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 11:02:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8477,  0.2629, -1.0254,  ..., -0.0814, -0.2358,  0.7734],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3042, -1.4688,  1.9316,  ..., -0.6807,  0.4746, -1.9121],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0152, -0.0054,  0.0100,  ..., -0.0008, -0.0052, -0.0125],
        [-0.0012,  0.0272,  0.0060,  ..., -0.0012,  0.0011, -0.0116],
        [ 0.0071,  0.0041, -0.0099,  ..., -0.0220, -0.0113, -0.0127],
        ...,
        [-0.0106, -0.0083, -0.0138,  ..., -0.0088, -0.0089,  0.0138],
        [ 0.0026, -0.0192, -0.0188,  ..., -0.0064, -0.0001,  0.0298],
        [ 0.0014, -0.0146,  0.0113,  ..., -0.0164,  0.0164, -0.0021]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.8276, -1.8887,  1.4453,  ..., -0.5771,  1.7002, -1.3574]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 11:02:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for auto is car
Another word for mend is repair
Another word for incorrect is wrong
Another word for airplane is aeroplane
Another word for sofa is couch
Another word for rock is stone
Another word for jewel is gem
Another word for bicycle is
2024-07-22 11:02:57 root INFO     [order_1_approx] starting weight calculation for Another word for mend is repair
Another word for jewel is gem
Another word for sofa is couch
Another word for auto is car
Another word for rock is stone
Another word for airplane is aeroplane
Another word for bicycle is bike
Another word for incorrect is
2024-07-22 11:02:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 11:06:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4883, -0.0493,  0.2124,  ...,  0.1803,  0.4456,  0.7422],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.0176,  0.9077, -0.9102,  ...,  3.7637,  6.4023, -0.2329],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0170,  0.0002,  0.0112,  ..., -0.0005,  0.0046, -0.0148],
        [ 0.0112, -0.0292,  0.0077,  ...,  0.0165, -0.0121,  0.0024],
        [-0.0154, -0.0036, -0.0045,  ...,  0.0041,  0.0075,  0.0193],
        ...,
        [-0.0126, -0.0129,  0.0151,  ..., -0.0022, -0.0083,  0.0004],
        [-0.0074, -0.0208, -0.0013,  ..., -0.0058, -0.0150,  0.0020],
        [ 0.0070, -0.0012,  0.0005,  ..., -0.0045,  0.0183,  0.0133]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.0352,  1.0576, -0.9844,  ...,  3.1270,  6.4922, -0.4448]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 11:06:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for mend is repair
Another word for jewel is gem
Another word for sofa is couch
Another word for auto is car
Another word for rock is stone
Another word for airplane is aeroplane
Another word for bicycle is bike
Another word for incorrect is
2024-07-22 11:06:40 root INFO     [order_1_approx] starting weight calculation for Another word for airplane is aeroplane
Another word for sofa is couch
Another word for auto is car
Another word for incorrect is wrong
Another word for mend is repair
Another word for rock is stone
Another word for bicycle is bike
Another word for jewel is
2024-07-22 11:06:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 11:10:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2405, -0.9180, -0.0588,  ...,  0.5801, -0.4644, -0.2485],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.5430, -0.1934,  0.1123,  ..., -1.7256,  0.4404,  3.9648],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.8095e-03, -1.4435e-02, -1.4305e-03,  ..., -1.7975e-02,
          1.7395e-03, -2.4094e-02],
        [ 9.6436e-03, -1.0590e-02, -1.4290e-02,  ...,  1.8520e-03,
          1.1429e-02,  5.3062e-03],
        [ 6.8970e-03,  1.2703e-02, -1.7654e-02,  ...,  8.3923e-03,
          1.0128e-03, -5.1880e-03],
        ...,
        [-6.7329e-03, -6.2561e-04,  1.1810e-02,  ..., -7.1106e-03,
         -1.7151e-02,  2.1935e-05],
        [ 1.6663e-02,  6.3515e-04,  5.1117e-04,  ..., -1.8158e-02,
         -1.7303e-02, -1.7441e-02],
        [-1.7792e-02, -1.2154e-02,  6.6662e-04,  ..., -5.9357e-03,
          1.3962e-02,  1.1963e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.3867,  0.0852,  0.0170,  ..., -1.7549,  0.7422,  4.7695]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 11:10:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for airplane is aeroplane
Another word for sofa is couch
Another word for auto is car
Another word for incorrect is wrong
Another word for mend is repair
Another word for rock is stone
Another word for bicycle is bike
Another word for jewel is
2024-07-22 11:10:25 root INFO     [order_1_approx] starting weight calculation for Another word for sofa is couch
Another word for airplane is aeroplane
Another word for jewel is gem
Another word for rock is stone
Another word for bicycle is bike
Another word for incorrect is wrong
Another word for auto is car
Another word for mend is
2024-07-22 11:10:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 11:14:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6455, -0.6875, -1.4229,  ...,  1.1143,  1.0000, -0.6221],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1431, -1.3936, -1.6660,  ...,  0.2090,  2.3066,  4.2422],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0032, -0.0046, -0.0004,  ...,  0.0101, -0.0053,  0.0014],
        [ 0.0011,  0.0026, -0.0059,  ...,  0.0040, -0.0019,  0.0063],
        [-0.0214,  0.0136, -0.0320,  ..., -0.0178, -0.0029,  0.0024],
        ...,
        [ 0.0133, -0.0025,  0.0162,  ..., -0.0130, -0.0008, -0.0119],
        [-0.0015, -0.0244, -0.0011,  ..., -0.0110, -0.0178,  0.0037],
        [ 0.0119, -0.0174,  0.0030,  ..., -0.0199,  0.0026, -0.0102]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0663, -0.7310, -0.4160,  ..., -0.6470,  1.6924,  4.0000]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 11:14:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for sofa is couch
Another word for airplane is aeroplane
Another word for jewel is gem
Another word for rock is stone
Another word for bicycle is bike
Another word for incorrect is wrong
Another word for auto is car
Another word for mend is
2024-07-22 11:14:11 root INFO     [order_1_approx] starting weight calculation for Another word for airplane is aeroplane
Another word for jewel is gem
Another word for auto is car
Another word for sofa is couch
Another word for mend is repair
Another word for bicycle is bike
Another word for incorrect is wrong
Another word for rock is
2024-07-22 11:14:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 11:17:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2040, -0.9150,  0.3013,  ...,  1.1406, -0.4124, -0.8555],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.8750,  0.9834, -2.8848,  ...,  1.4844,  0.6812,  0.3799],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0161, -0.0130, -0.0062,  ..., -0.0035, -0.0069,  0.0132],
        [ 0.0070,  0.0045,  0.0036,  ..., -0.0018,  0.0036, -0.0062],
        [-0.0135,  0.0033, -0.0078,  ..., -0.0142,  0.0086, -0.0202],
        ...,
        [ 0.0021,  0.0124,  0.0076,  ..., -0.0006,  0.0093,  0.0163],
        [-0.0014,  0.0038,  0.0013,  ..., -0.0125, -0.0064,  0.0066],
        [-0.0055, -0.0266, -0.0158,  ..., -0.0276, -0.0113,  0.0143]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.8047,  0.1396, -2.2129,  ...,  2.0488,  0.6016,  1.1641]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 11:17:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for airplane is aeroplane
Another word for jewel is gem
Another word for auto is car
Another word for sofa is couch
Another word for mend is repair
Another word for bicycle is bike
Another word for incorrect is wrong
Another word for rock is
2024-07-22 11:17:55 root INFO     [order_1_approx] starting weight calculation for Another word for mend is repair
Another word for incorrect is wrong
Another word for auto is car
Another word for rock is stone
Another word for airplane is aeroplane
Another word for bicycle is bike
Another word for jewel is gem
Another word for sofa is
2024-07-22 11:17:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 11:21:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7852, -0.4033, -0.8013,  ...,  0.8901, -1.0693,  0.4478],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3262,  1.3486,  0.9346,  ..., -2.6875, -2.4180,  2.3086],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0098, -0.0005,  0.0017,  ...,  0.0008,  0.0078, -0.0064],
        [ 0.0116,  0.0042, -0.0162,  ..., -0.0105, -0.0003,  0.0145],
        [-0.0136,  0.0100,  0.0026,  ..., -0.0226,  0.0066, -0.0155],
        ...,
        [ 0.0034, -0.0042,  0.0122,  ..., -0.0017, -0.0030,  0.0045],
        [-0.0072,  0.0023, -0.0078,  ...,  0.0040,  0.0043,  0.0129],
        [-0.0012,  0.0039,  0.0099,  ..., -0.0176,  0.0136,  0.0089]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0176,  1.3955,  1.6719,  ..., -3.0234, -1.1504,  2.9844]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 11:21:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for mend is repair
Another word for incorrect is wrong
Another word for auto is car
Another word for rock is stone
Another word for airplane is aeroplane
Another word for bicycle is bike
Another word for jewel is gem
Another word for sofa is
2024-07-22 11:21:39 root INFO     total operator prediction time: 1793.7006556987762 seconds
2024-07-22 11:21:39 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on hypernyms - misc
2024-07-22 11:21:39 root INFO     building operator hypernyms - misc
2024-07-22 11:21:40 root INFO     [order_1_approx] starting weight calculation for The grapefruit falls into the category of citrus
The skirt falls into the category of clothes
The sofa falls into the category of furniture
The wristband falls into the category of band
The fridge falls into the category of appliance
The brooch falls into the category of jewelry
The cake falls into the category of dessert
The blender falls into the category of
2024-07-22 11:21:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 11:25:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.3066,  0.1200, -1.7764,  ...,  0.1492, -0.3076,  0.5708],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.4902,  3.0117, -0.2117,  ...,  2.2617, -1.6621, -1.4834],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.0966e-02, -5.4436e-03, -4.9133e-03,  ...,  5.2223e-03,
          1.0101e-02,  8.0490e-04],
        [-1.1353e-02,  2.2644e-02,  3.2425e-04,  ..., -5.8746e-04,
         -4.7112e-03,  2.7866e-03],
        [-3.0098e-03,  8.3447e-04,  2.1637e-02,  ..., -8.2779e-04,
         -9.8038e-04,  9.5367e-04],
        ...,
        [ 3.7041e-03, -1.8570e-02,  5.5428e-03,  ...,  2.4704e-02,
         -1.9775e-02, -5.7220e-05],
        [-3.4256e-03,  3.3283e-03,  9.8877e-03,  ..., -4.1809e-03,
         -9.6130e-03,  2.2446e-02],
        [-2.7218e-03, -5.9242e-03, -7.7782e-03,  ...,  1.0605e-03,
          6.9771e-03,  2.2049e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.8867,  4.3125,  0.4690,  ...,  2.5664, -1.7061, -0.9521]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 11:25:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The grapefruit falls into the category of citrus
The skirt falls into the category of clothes
The sofa falls into the category of furniture
The wristband falls into the category of band
The fridge falls into the category of appliance
The brooch falls into the category of jewelry
The cake falls into the category of dessert
The blender falls into the category of
2024-07-22 11:25:25 root INFO     [order_1_approx] starting weight calculation for The sofa falls into the category of furniture
The grapefruit falls into the category of citrus
The fridge falls into the category of appliance
The blender falls into the category of appliance
The wristband falls into the category of band
The skirt falls into the category of clothes
The cake falls into the category of dessert
The brooch falls into the category of
2024-07-22 11:25:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 11:29:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8384, -0.4292,  0.3354,  ...,  0.3611, -0.4636,  0.0649],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.5039, -2.9434, -2.3164,  ..., -3.0508, -2.5078,  4.1992],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0114, -0.0036, -0.0046,  ..., -0.0050,  0.0033, -0.0057],
        [-0.0033,  0.0085,  0.0042,  ...,  0.0115,  0.0048,  0.0008],
        [-0.0030,  0.0016, -0.0017,  ...,  0.0081,  0.0044,  0.0020],
        ...,
        [ 0.0018,  0.0051, -0.0056,  ...,  0.0096, -0.0155,  0.0125],
        [ 0.0141,  0.0074, -0.0065,  ..., -0.0015,  0.0004,  0.0111],
        [-0.0075, -0.0032,  0.0007,  ...,  0.0014,  0.0171,  0.0108]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.6680, -2.2578, -2.3438,  ..., -2.8789, -2.2617,  5.0156]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 11:29:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sofa falls into the category of furniture
The grapefruit falls into the category of citrus
The fridge falls into the category of appliance
The blender falls into the category of appliance
The wristband falls into the category of band
The skirt falls into the category of clothes
The cake falls into the category of dessert
The brooch falls into the category of
2024-07-22 11:29:09 root INFO     [order_1_approx] starting weight calculation for The grapefruit falls into the category of citrus
The skirt falls into the category of clothes
The fridge falls into the category of appliance
The sofa falls into the category of furniture
The brooch falls into the category of jewelry
The blender falls into the category of appliance
The wristband falls into the category of band
The cake falls into the category of
2024-07-22 11:29:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 11:32:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.4385,  0.9766, -0.9629,  ..., -0.6914, -0.4675,  0.6338],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.1230,  0.5840, -2.4844,  ..., -1.0742, -0.3396,  0.5391],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0144,  0.0051, -0.0041,  ..., -0.0029,  0.0046,  0.0011],
        [-0.0118,  0.0165,  0.0032,  ...,  0.0095,  0.0007,  0.0071],
        [-0.0044,  0.0059, -0.0004,  ...,  0.0093,  0.0075,  0.0064],
        ...,
        [-0.0011,  0.0034,  0.0072,  ...,  0.0091, -0.0051,  0.0035],
        [ 0.0006,  0.0070,  0.0019,  ..., -0.0008,  0.0082,  0.0005],
        [-0.0113, -0.0002,  0.0046,  ...,  0.0047,  0.0037,  0.0124]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.0664,  1.2090, -2.6504,  ..., -1.7188, -0.9717,  0.0059]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 11:32:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The grapefruit falls into the category of citrus
The skirt falls into the category of clothes
The fridge falls into the category of appliance
The sofa falls into the category of furniture
The brooch falls into the category of jewelry
The blender falls into the category of appliance
The wristband falls into the category of band
The cake falls into the category of
2024-07-22 11:32:53 root INFO     [order_1_approx] starting weight calculation for The sofa falls into the category of furniture
The skirt falls into the category of clothes
The brooch falls into the category of jewelry
The blender falls into the category of appliance
The grapefruit falls into the category of citrus
The wristband falls into the category of band
The cake falls into the category of dessert
The fridge falls into the category of
2024-07-22 11:32:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 11:36:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0391, -0.2416, -1.3926,  ..., -1.0859,  0.5508,  1.1504],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.2383, -1.1465, -1.1172,  ..., -2.7539, -0.2290,  0.7803],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.6556e-02, -4.9324e-03, -1.3611e-02,  ...,  1.8969e-03,
         -7.0953e-03,  1.0063e-02],
        [-1.0277e-02,  1.5762e-02,  8.4305e-04,  ...,  6.3896e-05,
         -4.1351e-03,  2.1477e-03],
        [-1.0674e-02,  5.7526e-03,  6.3095e-03,  ...,  7.3128e-03,
          6.7329e-03,  2.6184e-02],
        ...,
        [-8.9111e-03, -1.0582e-02,  1.0872e-03,  ...,  1.7578e-02,
         -5.3825e-03,  6.5460e-03],
        [ 8.5449e-04,  2.7199e-03,  7.4997e-03,  ..., -1.2970e-03,
         -7.5111e-03,  8.4686e-03],
        [-9.5520e-03, -9.6970e-03, -2.9278e-03,  ..., -1.9703e-03,
          9.1629e-03,  1.0147e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.9375, -1.2949, -1.1055,  ..., -2.8906, -0.6279,  0.4600]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 11:36:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sofa falls into the category of furniture
The skirt falls into the category of clothes
The brooch falls into the category of jewelry
The blender falls into the category of appliance
The grapefruit falls into the category of citrus
The wristband falls into the category of band
The cake falls into the category of dessert
The fridge falls into the category of
2024-07-22 11:36:37 root INFO     [order_1_approx] starting weight calculation for The sofa falls into the category of furniture
The blender falls into the category of appliance
The brooch falls into the category of jewelry
The cake falls into the category of dessert
The skirt falls into the category of clothes
The wristband falls into the category of band
The fridge falls into the category of appliance
The grapefruit falls into the category of
2024-07-22 11:36:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 11:40:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5234, -1.4941,  0.8350,  ...,  0.4211,  0.0824,  0.1387],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.9375, -0.0366, -0.6416,  ..., -0.4448, -2.8125, -0.2646],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0245,  0.0006, -0.0069,  ..., -0.0042,  0.0099,  0.0011],
        [-0.0137,  0.0212, -0.0012,  ...,  0.0077,  0.0038,  0.0020],
        [ 0.0042, -0.0014, -0.0041,  ...,  0.0012,  0.0046, -0.0049],
        ...,
        [-0.0022, -0.0121,  0.0094,  ...,  0.0206, -0.0162, -0.0040],
        [ 0.0144,  0.0002, -0.0020,  ...,  0.0056,  0.0062,  0.0207],
        [-0.0090, -0.0065,  0.0103,  ..., -0.0045,  0.0092,  0.0093]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.0078,  0.4893, -1.3652,  ..., -0.5356, -4.1133, -0.5020]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 11:40:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sofa falls into the category of furniture
The blender falls into the category of appliance
The brooch falls into the category of jewelry
The cake falls into the category of dessert
The skirt falls into the category of clothes
The wristband falls into the category of band
The fridge falls into the category of appliance
The grapefruit falls into the category of
2024-07-22 11:40:23 root INFO     [order_1_approx] starting weight calculation for The cake falls into the category of dessert
The blender falls into the category of appliance
The brooch falls into the category of jewelry
The fridge falls into the category of appliance
The wristband falls into the category of band
The sofa falls into the category of furniture
The grapefruit falls into the category of citrus
The skirt falls into the category of
2024-07-22 11:40:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 11:44:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2460, -0.3611, -0.7046,  ..., -0.4854, -1.5684,  0.3511],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4893,  0.0350, -2.2656,  ..., -1.8047, -2.2637, -1.7363],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0274,  0.0026, -0.0145,  ..., -0.0017, -0.0050,  0.0037],
        [-0.0130,  0.0023,  0.0046,  ...,  0.0110,  0.0042, -0.0021],
        [-0.0007, -0.0017,  0.0022,  ...,  0.0060,  0.0002, -0.0021],
        ...,
        [-0.0075,  0.0027, -0.0004,  ...,  0.0054, -0.0034,  0.0055],
        [ 0.0101,  0.0016,  0.0003,  ...,  0.0138,  0.0048,  0.0124],
        [ 0.0014, -0.0044,  0.0024,  ..., -0.0002,  0.0167,  0.0264]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5566,  0.6826, -2.3984,  ..., -2.2012, -3.5039, -2.1270]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 11:44:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The cake falls into the category of dessert
The blender falls into the category of appliance
The brooch falls into the category of jewelry
The fridge falls into the category of appliance
The wristband falls into the category of band
The sofa falls into the category of furniture
The grapefruit falls into the category of citrus
The skirt falls into the category of
2024-07-22 11:44:09 root INFO     [order_1_approx] starting weight calculation for The cake falls into the category of dessert
The wristband falls into the category of band
The skirt falls into the category of clothes
The blender falls into the category of appliance
The grapefruit falls into the category of citrus
The brooch falls into the category of jewelry
The fridge falls into the category of appliance
The sofa falls into the category of
2024-07-22 11:44:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 11:47:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2720, -0.2229, -0.4624,  ..., -0.2222, -0.3545, -0.4297],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.9082, -1.5918, -1.9209,  ..., -2.4785, -2.9590, -2.7031],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.6993e-02, -1.6632e-03, -1.4069e-02,  ...,  1.1749e-03,
         -3.1891e-03,  6.7101e-03],
        [ 1.2589e-04,  1.5854e-02,  2.0599e-03,  ..., -2.1591e-03,
          1.9646e-03,  1.1663e-03],
        [-1.2751e-03,  3.0518e-03,  1.1501e-03,  ...,  4.0817e-03,
          5.5046e-03,  2.5558e-04],
        ...,
        [-4.1122e-03, -2.4490e-03,  1.4915e-03,  ...,  7.5073e-03,
         -3.6850e-03,  9.0714e-03],
        [-5.2261e-04, -1.4801e-03, -2.7447e-03,  ...,  2.2869e-03,
          3.4332e-05, -1.1330e-03],
        [ 5.8746e-03,  2.1896e-03,  5.9433e-03,  ..., -1.2589e-03,
          7.4654e-03,  1.3931e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.1680, -1.1055, -2.5762,  ..., -2.2168, -3.6055, -2.8223]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 11:47:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The cake falls into the category of dessert
The wristband falls into the category of band
The skirt falls into the category of clothes
The blender falls into the category of appliance
The grapefruit falls into the category of citrus
The brooch falls into the category of jewelry
The fridge falls into the category of appliance
The sofa falls into the category of
2024-07-22 11:47:54 root INFO     [order_1_approx] starting weight calculation for The blender falls into the category of appliance
The skirt falls into the category of clothes
The grapefruit falls into the category of citrus
The cake falls into the category of dessert
The brooch falls into the category of jewelry
The sofa falls into the category of furniture
The fridge falls into the category of appliance
The wristband falls into the category of
2024-07-22 11:47:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 11:51:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1689, -1.0908,  0.4456,  ..., -0.0684, -0.2072,  1.2188],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.6484, -1.5010, -1.3984,  ..., -5.3828, -0.3110,  1.9219],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0228, -0.0013, -0.0096,  ..., -0.0014, -0.0133, -0.0014],
        [-0.0142,  0.0122, -0.0014,  ...,  0.0054,  0.0113, -0.0008],
        [-0.0132,  0.0053,  0.0134,  ..., -0.0016,  0.0050, -0.0166],
        ...,
        [-0.0102, -0.0034, -0.0091,  ...,  0.0057, -0.0034,  0.0119],
        [ 0.0073, -0.0094, -0.0066,  ..., -0.0064, -0.0058,  0.0214],
        [ 0.0046, -0.0036,  0.0004,  ...,  0.0011, -0.0002,  0.0162]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.9375, -1.4814, -2.0586,  ..., -5.6914, -0.4473,  2.0508]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 11:51:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The blender falls into the category of appliance
The skirt falls into the category of clothes
The grapefruit falls into the category of citrus
The cake falls into the category of dessert
The brooch falls into the category of jewelry
The sofa falls into the category of furniture
The fridge falls into the category of appliance
The wristband falls into the category of
2024-07-22 11:51:37 root INFO     total operator prediction time: 1797.8651022911072 seconds
2024-07-22 11:51:37 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - substance
2024-07-22 11:51:37 root INFO     building operator meronyms - substance
2024-07-22 11:51:37 root INFO     [order_1_approx] starting weight calculation for A bread is made up of flour
A penny is made up of metal
A steel is made up of iron
A desk is made up of wood
A concrete is made up of silicon
A ocean is made up of water
A jeans is made up of fabric
A bottle is made up of
2024-07-22 11:51:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 11:55:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2411,  0.0103,  1.2559,  ...,  0.3223, -1.2949,  0.6387],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-5.1914,  0.5264,  2.2441,  ...,  1.6592, -0.4873,  0.9658],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0081, -0.0067,  0.0019,  ...,  0.0015, -0.0041,  0.0005],
        [ 0.0021, -0.0006,  0.0035,  ...,  0.0058,  0.0061,  0.0047],
        [ 0.0010, -0.0075,  0.0085,  ..., -0.0076,  0.0095, -0.0034],
        ...,
        [-0.0048, -0.0073,  0.0042,  ...,  0.0066, -0.0039,  0.0071],
        [-0.0075, -0.0081,  0.0062,  ..., -0.0006, -0.0032, -0.0059],
        [-0.0020,  0.0014,  0.0057,  ...,  0.0008,  0.0129,  0.0063]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.0547,  0.2471,  2.3008,  ...,  1.7617, -0.6357,  0.7646]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 11:55:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A bread is made up of flour
A penny is made up of metal
A steel is made up of iron
A desk is made up of wood
A concrete is made up of silicon
A ocean is made up of water
A jeans is made up of fabric
A bottle is made up of
2024-07-22 11:55:20 root INFO     [order_1_approx] starting weight calculation for A penny is made up of metal
A desk is made up of wood
A concrete is made up of silicon
A jeans is made up of fabric
A steel is made up of iron
A ocean is made up of water
A bottle is made up of glass
A bread is made up of
2024-07-22 11:55:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 11:59:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8438, -0.3306,  0.7002,  ...,  1.2383, -2.6738, -0.0566],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.5742, -0.7051, -0.0908,  ..., -0.4409, -2.1152, -1.6406],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0009, -0.0051, -0.0010,  ..., -0.0028, -0.0026, -0.0078],
        [-0.0108, -0.0056, -0.0008,  ...,  0.0130, -0.0036, -0.0077],
        [ 0.0018, -0.0091, -0.0097,  ..., -0.0043, -0.0064, -0.0049],
        ...,
        [ 0.0025, -0.0002, -0.0079,  ..., -0.0129,  0.0073,  0.0006],
        [ 0.0123, -0.0022,  0.0054,  ...,  0.0074,  0.0008, -0.0131],
        [ 0.0092, -0.0022, -0.0059,  ..., -0.0109,  0.0009,  0.0073]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.1836, -0.8926, -0.2773,  ...,  0.1519, -2.5586, -1.7207]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 11:59:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A penny is made up of metal
A desk is made up of wood
A concrete is made up of silicon
A jeans is made up of fabric
A steel is made up of iron
A ocean is made up of water
A bottle is made up of glass
A bread is made up of
2024-07-22 11:59:04 root INFO     [order_1_approx] starting weight calculation for A penny is made up of metal
A bread is made up of flour
A jeans is made up of fabric
A bottle is made up of glass
A steel is made up of iron
A desk is made up of wood
A ocean is made up of water
A concrete is made up of
2024-07-22 11:59:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 12:02:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0479,  0.2524, -0.9199,  ...,  0.3542,  0.6094, -0.9385],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4414,  0.3779, -0.9053,  ...,  0.5166, -1.8027,  1.2822],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0018, -0.0029,  0.0009,  ...,  0.0015,  0.0055, -0.0068],
        [-0.0003,  0.0019,  0.0022,  ...,  0.0013, -0.0016, -0.0033],
        [ 0.0072, -0.0097, -0.0100,  ..., -0.0078,  0.0125,  0.0038],
        ...,
        [ 0.0103, -0.0154, -0.0018,  ...,  0.0035, -0.0005,  0.0076],
        [ 0.0063, -0.0115,  0.0008,  ..., -0.0026, -0.0029, -0.0041],
        [ 0.0067, -0.0128, -0.0002,  ..., -0.0086,  0.0019,  0.0062]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1660,  0.1284, -1.3711,  ...,  0.6602, -2.0098,  0.9590]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 12:02:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A penny is made up of metal
A bread is made up of flour
A jeans is made up of fabric
A bottle is made up of glass
A steel is made up of iron
A desk is made up of wood
A ocean is made up of water
A concrete is made up of
2024-07-22 12:02:48 root INFO     [order_1_approx] starting weight calculation for A concrete is made up of silicon
A jeans is made up of fabric
A penny is made up of metal
A steel is made up of iron
A bread is made up of flour
A bottle is made up of glass
A ocean is made up of water
A desk is made up of
2024-07-22 12:02:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 12:06:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9829, -0.3660,  0.3115,  ...,  0.3069, -0.5342, -0.1345],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.6914, -0.2627, -0.9854,  ...,  4.0820, -3.1016, -0.9067],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0072, -0.0091, -0.0021,  ..., -0.0052, -0.0029,  0.0071],
        [-0.0019,  0.0160,  0.0097,  ...,  0.0129, -0.0042,  0.0049],
        [ 0.0077, -0.0047,  0.0123,  ...,  0.0023, -0.0007, -0.0064],
        ...,
        [ 0.0066, -0.0005, -0.0067,  ..., -0.0062, -0.0091,  0.0096],
        [ 0.0083, -0.0074,  0.0134,  ...,  0.0025,  0.0055, -0.0156],
        [-0.0019,  0.0007, -0.0073,  ..., -0.0124,  0.0077,  0.0225]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.3789, -0.1708, -1.3223,  ...,  3.7734, -2.8984, -0.4211]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 12:06:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A concrete is made up of silicon
A jeans is made up of fabric
A penny is made up of metal
A steel is made up of iron
A bread is made up of flour
A bottle is made up of glass
A ocean is made up of water
A desk is made up of
2024-07-22 12:06:31 root INFO     [order_1_approx] starting weight calculation for A bottle is made up of glass
A steel is made up of iron
A ocean is made up of water
A concrete is made up of silicon
A bread is made up of flour
A penny is made up of metal
A desk is made up of wood
A jeans is made up of
2024-07-22 12:06:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 12:10:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5117, -1.1924, -0.7056,  ...,  0.7139, -0.5093,  0.2700],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4824, -0.9409,  1.9570,  ..., -0.2056, -0.2878, -2.7539],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.2221e-04, -8.6365e-03,  3.9101e-05,  ...,  3.5458e-03,
         -1.7128e-03,  1.2760e-03],
        [-7.3242e-04, -5.9414e-04, -2.4929e-03,  ...,  1.0452e-02,
         -6.0387e-03, -3.8834e-03],
        [ 2.6131e-03,  3.0899e-04, -1.5884e-02,  ..., -5.4932e-03,
          3.0746e-03, -5.1880e-04],
        ...,
        [ 7.1182e-03,  5.7526e-03,  1.0595e-03,  ..., -1.7300e-03,
         -3.9177e-03,  9.1019e-03],
        [ 9.9640e-03,  9.2268e-05,  4.0817e-03,  ...,  1.3229e-02,
         -3.1662e-03, -7.9575e-03],
        [ 3.7079e-03,  3.9215e-03, -9.2163e-03,  ..., -6.9542e-03,
          7.5378e-03, -3.6259e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0146, -0.7266,  1.5566,  ..., -0.2030, -0.6377, -2.7461]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 12:10:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A bottle is made up of glass
A steel is made up of iron
A ocean is made up of water
A concrete is made up of silicon
A bread is made up of flour
A penny is made up of metal
A desk is made up of wood
A jeans is made up of
2024-07-22 12:10:15 root INFO     [order_1_approx] starting weight calculation for A desk is made up of wood
A concrete is made up of silicon
A jeans is made up of fabric
A bottle is made up of glass
A steel is made up of iron
A penny is made up of metal
A bread is made up of flour
A ocean is made up of
2024-07-22 12:10:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 12:13:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1597, -0.5737,  0.5933,  ..., -2.7031,  0.2491,  0.6196],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0234,  2.5918,  1.0820,  ...,  4.8867, -0.2637, -2.1387],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0016, -0.0176, -0.0010,  ...,  0.0003,  0.0024, -0.0090],
        [-0.0024, -0.0051,  0.0143,  ...,  0.0076, -0.0011, -0.0022],
        [-0.0027,  0.0047, -0.0062,  ..., -0.0035, -0.0041,  0.0018],
        ...,
        [-0.0010, -0.0043,  0.0145,  ..., -0.0025, -0.0091,  0.0032],
        [ 0.0051, -0.0072,  0.0028,  ..., -0.0005, -0.0025, -0.0091],
        [ 0.0099,  0.0016,  0.0030,  ..., -0.0047,  0.0042,  0.0050]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7559,  3.0547,  1.7793,  ...,  5.2852, -0.5146, -2.5312]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 12:13:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A desk is made up of wood
A concrete is made up of silicon
A jeans is made up of fabric
A bottle is made up of glass
A steel is made up of iron
A penny is made up of metal
A bread is made up of flour
A ocean is made up of
2024-07-22 12:13:59 root INFO     [order_1_approx] starting weight calculation for A bread is made up of flour
A jeans is made up of fabric
A bottle is made up of glass
A concrete is made up of silicon
A steel is made up of iron
A ocean is made up of water
A desk is made up of wood
A penny is made up of
2024-07-22 12:13:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 12:17:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5913,  0.1919, -0.9780,  ..., -0.5732, -0.4077,  2.3438],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.9336,  0.0498, -0.8066,  ..., -2.4355,  0.5806,  1.8291],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0204, -0.0160,  0.0070,  ...,  0.0009, -0.0005, -0.0145],
        [-0.0060,  0.0019,  0.0132,  ...,  0.0050, -0.0006, -0.0034],
        [ 0.0050, -0.0041, -0.0150,  ..., -0.0065, -0.0001,  0.0041],
        ...,
        [ 0.0099, -0.0130, -0.0105,  ..., -0.0124, -0.0063,  0.0139],
        [-0.0023,  0.0007,  0.0057,  ...,  0.0117, -0.0064, -0.0137],
        [-0.0039,  0.0098,  0.0152,  ...,  0.0049,  0.0091, -0.0123]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6699, -0.6880, -0.5098,  ..., -2.3828,  0.7910,  1.7949]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 12:17:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A bread is made up of flour
A jeans is made up of fabric
A bottle is made up of glass
A concrete is made up of silicon
A steel is made up of iron
A ocean is made up of water
A desk is made up of wood
A penny is made up of
2024-07-22 12:17:43 root INFO     [order_1_approx] starting weight calculation for A ocean is made up of water
A bottle is made up of glass
A penny is made up of metal
A jeans is made up of fabric
A concrete is made up of silicon
A bread is made up of flour
A desk is made up of wood
A steel is made up of
2024-07-22 12:17:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 12:21:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5791, -0.0253, -0.4597,  ...,  0.7524,  0.1887, -0.8252],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0723,  5.0000,  0.8604,  ..., -0.9565,  2.2266,  3.5703],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0108, -0.0049, -0.0018,  ...,  0.0032, -0.0024,  0.0031],
        [ 0.0025,  0.0063,  0.0069,  ...,  0.0113, -0.0066, -0.0090],
        [-0.0080, -0.0032, -0.0048,  ...,  0.0038,  0.0032,  0.0031],
        ...,
        [ 0.0084, -0.0013,  0.0027,  ..., -0.0037,  0.0056,  0.0125],
        [ 0.0024, -0.0105,  0.0057,  ...,  0.0043, -0.0040, -0.0075],
        [ 0.0038, -0.0166, -0.0057,  ...,  0.0003, -0.0009,  0.0101]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2952,  5.1914,  0.8867,  ..., -0.4827,  2.1914,  3.5449]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 12:21:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A ocean is made up of water
A bottle is made up of glass
A penny is made up of metal
A jeans is made up of fabric
A concrete is made up of silicon
A bread is made up of flour
A desk is made up of wood
A steel is made up of
2024-07-22 12:21:27 root INFO     total operator prediction time: 1789.7854738235474 seconds
2024-07-22 12:21:27 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on synonyms - intensity
2024-07-22 12:21:27 root INFO     building operator synonyms - intensity
2024-07-22 12:21:27 root INFO     [order_1_approx] starting weight calculation for A more intense word for bad is awful
A more intense word for soon is immediately
A more intense word for interesting is exciting
A more intense word for pony is horse
A more intense word for lake is sea
A more intense word for dislike is hate
A more intense word for chuckle is laugh
A more intense word for ask is
2024-07-22 12:21:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 12:25:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3633, -0.0287,  0.4490,  ...,  0.4150,  1.4766,  1.6562],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.9453, -3.3711, -5.3359,  ..., -0.9399,  2.0566,  3.5938],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0054, -0.0023, -0.0032,  ..., -0.0049, -0.0072,  0.0036],
        [-0.0023,  0.0139, -0.0005,  ..., -0.0094,  0.0006, -0.0232],
        [-0.0117,  0.0064,  0.0056,  ..., -0.0005, -0.0059,  0.0225],
        ...,
        [ 0.0112, -0.0208, -0.0027,  ..., -0.0110, -0.0046,  0.0142],
        [ 0.0107, -0.0099,  0.0002,  ..., -0.0166,  0.0317, -0.0076],
        [ 0.0141,  0.0081,  0.0044,  ..., -0.0127,  0.0097, -0.0014]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.1680, -3.8281, -5.0625,  ..., -0.5820,  2.0039,  4.3672]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 12:25:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for bad is awful
A more intense word for soon is immediately
A more intense word for interesting is exciting
A more intense word for pony is horse
A more intense word for lake is sea
A more intense word for dislike is hate
A more intense word for chuckle is laugh
A more intense word for ask is
2024-07-22 12:25:13 root INFO     [order_1_approx] starting weight calculation for A more intense word for dislike is hate
A more intense word for chuckle is laugh
A more intense word for interesting is exciting
A more intense word for pony is horse
A more intense word for ask is beg
A more intense word for lake is sea
A more intense word for soon is immediately
A more intense word for bad is
2024-07-22 12:25:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 12:28:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2939, -1.0938,  0.6709,  ...,  0.1194,  0.3408,  0.4678],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3945, -1.5098, -1.3818,  ...,  1.2070,  1.8066, -0.3965],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0044, -0.0055, -0.0086,  ...,  0.0035,  0.0048, -0.0034],
        [ 0.0191, -0.0127,  0.0030,  ...,  0.0044, -0.0077, -0.0119],
        [ 0.0055, -0.0110,  0.0102,  ..., -0.0116,  0.0137,  0.0029],
        ...,
        [ 0.0052, -0.0205,  0.0008,  ..., -0.0022, -0.0280, -0.0081],
        [-0.0007,  0.0009,  0.0229,  ..., -0.0023, -0.0057,  0.0108],
        [ 0.0148,  0.0008,  0.0187,  ..., -0.0003,  0.0080, -0.0077]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.8008, -1.7715, -0.6807,  ...,  1.0732,  1.3906, -0.3516]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 12:28:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for dislike is hate
A more intense word for chuckle is laugh
A more intense word for interesting is exciting
A more intense word for pony is horse
A more intense word for ask is beg
A more intense word for lake is sea
A more intense word for soon is immediately
A more intense word for bad is
2024-07-22 12:28:59 root INFO     [order_1_approx] starting weight calculation for A more intense word for ask is beg
A more intense word for soon is immediately
A more intense word for bad is awful
A more intense word for lake is sea
A more intense word for interesting is exciting
A more intense word for pony is horse
A more intense word for dislike is hate
A more intense word for chuckle is
2024-07-22 12:28:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 12:32:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4092, -0.6943,  0.0540,  ...,  2.3457,  0.4746,  1.4629],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.1758, -0.3926,  0.3398,  ...,  0.0469,  1.4473, -4.4805],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0069, -0.0135,  0.0135,  ...,  0.0174,  0.0024,  0.0053],
        [ 0.0045, -0.0044, -0.0104,  ...,  0.0163, -0.0211,  0.0008],
        [ 0.0050,  0.0031, -0.0281,  ..., -0.0042,  0.0124,  0.0274],
        ...,
        [-0.0203,  0.0041, -0.0110,  ..., -0.0121, -0.0271, -0.0107],
        [-0.0182,  0.0004, -0.0065,  ..., -0.0079, -0.0193,  0.0080],
        [-0.0059,  0.0050,  0.0061,  ...,  0.0045,  0.0173,  0.0224]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.3496, -1.3770,  1.1055,  ..., -0.0735,  2.5273, -5.1797]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 12:32:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for ask is beg
A more intense word for soon is immediately
A more intense word for bad is awful
A more intense word for lake is sea
A more intense word for interesting is exciting
A more intense word for pony is horse
A more intense word for dislike is hate
A more intense word for chuckle is
2024-07-22 12:32:43 root INFO     [order_1_approx] starting weight calculation for A more intense word for pony is horse
A more intense word for soon is immediately
A more intense word for lake is sea
A more intense word for interesting is exciting
A more intense word for ask is beg
A more intense word for bad is awful
A more intense word for chuckle is laugh
A more intense word for dislike is
2024-07-22 12:32:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 12:36:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5830, -0.9468,  0.8228,  ...,  0.4583,  0.2783,  0.8506],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8984, -0.1943,  2.3008,  ...,  3.7031,  1.8438,  1.7090],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.2878e-02, -3.2387e-03,  1.3054e-02,  ..., -2.9802e-05,
         -4.3144e-03, -5.1117e-03],
        [ 5.3177e-03, -1.2636e-03, -2.0050e-02,  ..., -4.2267e-03,
          8.2245e-03, -6.4240e-03],
        [ 3.1281e-02, -1.2711e-02, -2.0294e-03,  ...,  6.0577e-03,
          2.5673e-03,  1.4114e-02],
        ...,
        [-6.8665e-03, -1.4343e-02,  1.3123e-02,  ...,  1.3741e-02,
         -3.9307e-02, -1.0277e-02],
        [ 2.9907e-02, -4.8866e-03,  1.0193e-02,  ..., -1.5656e-02,
          1.4999e-02, -1.0361e-02],
        [-2.5116e-02,  1.1311e-03, -1.8295e-02,  ...,  2.1027e-02,
         -9.5749e-03,  9.2316e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.8105, -0.6514,  3.0215,  ...,  4.2383,  2.1582,  1.3857]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 12:36:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for pony is horse
A more intense word for soon is immediately
A more intense word for lake is sea
A more intense word for interesting is exciting
A more intense word for ask is beg
A more intense word for bad is awful
A more intense word for chuckle is laugh
A more intense word for dislike is
2024-07-22 12:36:27 root INFO     [order_1_approx] starting weight calculation for A more intense word for ask is beg
A more intense word for pony is horse
A more intense word for dislike is hate
A more intense word for lake is sea
A more intense word for chuckle is laugh
A more intense word for bad is awful
A more intense word for soon is immediately
A more intense word for interesting is
2024-07-22 12:36:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 12:40:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4082, -0.2554,  0.9570,  ...,  1.1602,  1.2002,  0.5977],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9268, -1.2910,  0.9922,  ..., -2.5508,  3.5078,  2.1719],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.6689e-04, -1.2062e-02,  1.7853e-02,  ...,  1.8196e-03,
         -4.0817e-03,  9.3689e-03],
        [-6.5308e-03,  5.7983e-03, -3.3989e-03,  ..., -7.3624e-03,
         -8.0109e-03,  1.6541e-02],
        [-3.6430e-03,  1.9684e-03, -1.2085e-02,  ...,  5.0049e-03,
          8.6823e-03,  1.1765e-02],
        ...,
        [-1.7914e-02, -1.3256e-03,  1.2192e-02,  ...,  7.7667e-03,
         -4.6616e-03, -7.2746e-03],
        [-7.6370e-03, -1.0292e-02, -3.0518e-05,  ..., -7.2670e-04,
         -2.8286e-03, -2.1267e-03],
        [-3.3531e-03,  3.4370e-03, -6.1417e-03,  ..., -1.7776e-03,
         -3.5858e-03,  1.9798e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9854, -1.4014,  0.4463,  ..., -1.6582,  3.5234,  2.1875]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 12:40:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for ask is beg
A more intense word for pony is horse
A more intense word for dislike is hate
A more intense word for lake is sea
A more intense word for chuckle is laugh
A more intense word for bad is awful
A more intense word for soon is immediately
A more intense word for interesting is
2024-07-22 12:40:12 root INFO     [order_1_approx] starting weight calculation for A more intense word for chuckle is laugh
A more intense word for ask is beg
A more intense word for pony is horse
A more intense word for dislike is hate
A more intense word for interesting is exciting
A more intense word for bad is awful
A more intense word for soon is immediately
A more intense word for lake is
2024-07-22 12:40:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 12:43:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([2.2617, 0.5010, 1.2910,  ..., 0.3013, 0.1877, 1.3125], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.3213,  3.2266, -3.2090,  ...,  2.1758, -0.5854, -1.7109],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.0208e-02,  5.2567e-03, -1.1597e-03,  ...,  8.3447e-07,
          6.9885e-03, -2.2812e-03],
        [ 8.0109e-04, -1.9531e-02, -2.5330e-03,  ...,  7.1411e-03,
         -4.5586e-04,  1.8978e-03],
        [ 1.0071e-02,  1.2459e-02, -5.2567e-03,  ..., -1.2169e-02,
         -8.2397e-04,  7.3509e-03],
        ...,
        [-8.6594e-03, -1.1879e-02,  7.0572e-03,  ...,  1.1505e-02,
         -2.9694e-02,  1.5411e-02],
        [ 2.0523e-02, -1.1612e-02,  6.4812e-03,  ..., -1.8661e-02,
          2.2278e-02, -1.2955e-02],
        [ 6.2904e-03,  4.8866e-03,  4.7493e-03,  ..., -5.2071e-03,
          1.6037e-02, -2.6703e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.1719,  3.6211, -2.3047,  ...,  2.8887, -1.2246, -0.8228]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 12:43:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for chuckle is laugh
A more intense word for ask is beg
A more intense word for pony is horse
A more intense word for dislike is hate
A more intense word for interesting is exciting
A more intense word for bad is awful
A more intense word for soon is immediately
A more intense word for lake is
2024-07-22 12:43:56 root INFO     [order_1_approx] starting weight calculation for A more intense word for ask is beg
A more intense word for dislike is hate
A more intense word for interesting is exciting
A more intense word for bad is awful
A more intense word for soon is immediately
A more intense word for chuckle is laugh
A more intense word for lake is sea
A more intense word for pony is
2024-07-22 12:43:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 12:47:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0996, -1.0693, -0.5752,  ...,  0.2844, -0.9824,  1.4873],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-7.3047e-01,  2.1973e-03, -9.5898e-01,  ..., -2.9199e+00,
         2.1836e+00, -7.1973e-01], device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0017, -0.0050,  0.0075,  ...,  0.0001, -0.0047, -0.0062],
        [-0.0010,  0.0068,  0.0103,  ..., -0.0016, -0.0053,  0.0012],
        [ 0.0003, -0.0023, -0.0074,  ..., -0.0107,  0.0049,  0.0113],
        ...,
        [-0.0215, -0.0045,  0.0032,  ...,  0.0003, -0.0013, -0.0052],
        [ 0.0152, -0.0036,  0.0069,  ...,  0.0031, -0.0048,  0.0073],
        [-0.0044, -0.0055,  0.0023,  ..., -0.0069,  0.0135, -0.0007]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7227, -0.2871, -0.9736,  ..., -1.2295,  2.0293, -0.2532]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 12:47:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for ask is beg
A more intense word for dislike is hate
A more intense word for interesting is exciting
A more intense word for bad is awful
A more intense word for soon is immediately
A more intense word for chuckle is laugh
A more intense word for lake is sea
A more intense word for pony is
2024-07-22 12:47:41 root INFO     [order_1_approx] starting weight calculation for A more intense word for interesting is exciting
A more intense word for pony is horse
A more intense word for bad is awful
A more intense word for chuckle is laugh
A more intense word for dislike is hate
A more intense word for lake is sea
A more intense word for ask is beg
A more intense word for soon is
2024-07-22 12:47:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 12:51:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2981,  0.5435, -0.5215,  ...,  1.2744,  0.2825,  0.7881],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.5352,  1.0762, -4.7969,  ..., -1.0498,  1.9756, -0.7441],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0130, -0.0142,  0.0220,  ..., -0.0192,  0.0039,  0.0115],
        [-0.0026, -0.0135,  0.0016,  ...,  0.0018, -0.0107,  0.0163],
        [-0.0090,  0.0039,  0.0087,  ..., -0.0026, -0.0041,  0.0160],
        ...,
        [-0.0334, -0.0304,  0.0146,  ..., -0.0086, -0.0228,  0.0195],
        [-0.0123,  0.0012,  0.0069,  ..., -0.0089, -0.0117,  0.0128],
        [ 0.0081,  0.0238, -0.0128,  ..., -0.0056, -0.0160, -0.0172]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2520,  1.5342, -4.7305,  ..., -0.6787,  1.4531, -0.6104]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 12:51:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for interesting is exciting
A more intense word for pony is horse
A more intense word for bad is awful
A more intense word for chuckle is laugh
A more intense word for dislike is hate
A more intense word for lake is sea
A more intense word for ask is beg
A more intense word for soon is
2024-07-22 12:51:25 root INFO     total operator prediction time: 1798.1527576446533 seconds
2024-07-22 12:51:25 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on hypernyms - animals
2024-07-22 12:51:25 root INFO     building operator hypernyms - animals
2024-07-22 12:51:25 root INFO     [order_1_approx] starting weight calculation for The jackal falls into the category of canine
The mouse falls into the category of rodent
The porcupine falls into the category of rodent
The triceratops falls into the category of dinosaur
The cow falls into the category of bovid
The pony falls into the category of bovid
The squirrel falls into the category of rodent
The allosaurus falls into the category of
2024-07-22 12:51:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 12:55:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6924,  0.4883, -1.2158,  ..., -1.2129,  0.3374,  1.1631],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3320, -3.7461,  4.0781,  ...,  2.9570, -3.4121,  1.8887],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0115,  0.0007,  0.0035,  ..., -0.0125,  0.0052, -0.0010],
        [ 0.0058, -0.0007,  0.0046,  ...,  0.0124,  0.0063,  0.0091],
        [-0.0016, -0.0186, -0.0055,  ...,  0.0058, -0.0119,  0.0083],
        ...,
        [-0.0094, -0.0036, -0.0101,  ...,  0.0208,  0.0052, -0.0033],
        [-0.0015,  0.0076, -0.0042,  ...,  0.0010, -0.0036,  0.0114],
        [-0.0061, -0.0105,  0.0007,  ..., -0.0051, -0.0017,  0.0127]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3027, -3.8418,  4.2031,  ...,  4.3047, -4.3945,  2.1328]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 12:55:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The jackal falls into the category of canine
The mouse falls into the category of rodent
The porcupine falls into the category of rodent
The triceratops falls into the category of dinosaur
The cow falls into the category of bovid
The pony falls into the category of bovid
The squirrel falls into the category of rodent
The allosaurus falls into the category of
2024-07-22 12:55:10 root INFO     [order_1_approx] starting weight calculation for The allosaurus falls into the category of dinosaur
The pony falls into the category of bovid
The triceratops falls into the category of dinosaur
The jackal falls into the category of canine
The mouse falls into the category of rodent
The squirrel falls into the category of rodent
The porcupine falls into the category of rodent
The cow falls into the category of
2024-07-22 12:55:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 12:58:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5117,  0.4346, -0.8916,  ..., -0.2947,  0.0610,  1.3750],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.9746, -0.1353, -0.4465,  ..., -0.2871, -2.0586,  0.4780],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.1387e-03, -4.5586e-03, -5.7678e-03,  ..., -1.0872e-03,
         -4.1199e-03,  5.0163e-04],
        [ 3.2482e-03,  3.8681e-03, -8.6498e-04,  ...,  2.1133e-03,
         -4.2686e-03, -5.5847e-03],
        [-3.5934e-03, -3.4409e-03, -1.9493e-03,  ..., -3.4809e-03,
          8.9312e-04, -2.5368e-04],
        ...,
        [-5.1308e-04, -7.2021e-03, -1.7424e-03,  ..., -4.7302e-03,
          3.8357e-03,  1.2787e-02],
        [-4.6349e-03, -8.4763e-03,  8.5354e-05,  ...,  6.1035e-04,
          3.7441e-03, -3.2806e-03],
        [-7.0724e-03, -1.4626e-02,  8.1940e-03,  ...,  1.9341e-03,
          6.8092e-03,  1.4999e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.4844,  0.7407, -0.9502,  ..., -0.0364, -2.3398,  0.7998]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 12:58:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The allosaurus falls into the category of dinosaur
The pony falls into the category of bovid
The triceratops falls into the category of dinosaur
The jackal falls into the category of canine
The mouse falls into the category of rodent
The squirrel falls into the category of rodent
The porcupine falls into the category of rodent
The cow falls into the category of
2024-07-22 12:58:55 root INFO     [order_1_approx] starting weight calculation for The mouse falls into the category of rodent
The allosaurus falls into the category of dinosaur
The squirrel falls into the category of rodent
The triceratops falls into the category of dinosaur
The cow falls into the category of bovid
The pony falls into the category of bovid
The porcupine falls into the category of rodent
The jackal falls into the category of
2024-07-22 12:58:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 13:02:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0039, -1.8184, -0.3103,  ...,  0.3560, -1.0547, -0.4087],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8203,  0.1990,  0.6426,  ...,  0.6060, -0.0791,  0.7275],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-6.9237e-04, -1.4359e-02, -9.5520e-03,  ..., -9.9792e-03,
         -4.3068e-03, -7.8430e-03],
        [ 1.2360e-02, -2.8229e-03,  1.0468e-02,  ...,  1.8997e-02,
          8.1711e-03,  2.2850e-03],
        [ 4.7188e-03, -2.6970e-03,  7.6294e-06,  ..., -2.2831e-03,
         -4.1542e-03, -8.9979e-04],
        ...,
        [ 3.6163e-03, -2.8954e-03,  6.5804e-03,  ...,  1.6205e-02,
         -3.3436e-03,  8.2550e-03],
        [-7.2937e-03, -8.4991e-03,  1.0338e-03,  ...,  1.0605e-03,
          6.7253e-03, -5.3902e-03],
        [-2.4605e-03, -5.2223e-03,  5.2567e-03,  ...,  2.6131e-03,
          3.9291e-04,  8.6060e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2671,  1.2637,  0.9922,  ...,  0.7158, -0.2773,  0.7988]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 13:02:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The mouse falls into the category of rodent
The allosaurus falls into the category of dinosaur
The squirrel falls into the category of rodent
The triceratops falls into the category of dinosaur
The cow falls into the category of bovid
The pony falls into the category of bovid
The porcupine falls into the category of rodent
The jackal falls into the category of
2024-07-22 13:02:41 root INFO     [order_1_approx] starting weight calculation for The squirrel falls into the category of rodent
The porcupine falls into the category of rodent
The jackal falls into the category of canine
The triceratops falls into the category of dinosaur
The cow falls into the category of bovid
The pony falls into the category of bovid
The allosaurus falls into the category of dinosaur
The mouse falls into the category of
2024-07-22 13:02:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 13:06:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9375, -0.1858,  0.4065,  ..., -0.2717, -0.6025,  1.3633],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.0664,  3.3906,  0.9795,  ..., -1.6162,  0.0781,  0.2615],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.2161e-02, -2.6016e-03,  1.8110e-03,  ...,  5.2032e-03,
          8.6365e-03,  7.7553e-03],
        [-7.3051e-03,  6.6681e-03,  7.1297e-03,  ...,  1.2337e-02,
          4.1389e-04, -2.2125e-03],
        [-2.3422e-03, -2.3071e-02, -2.0966e-02,  ...,  1.7891e-03,
          5.2032e-03, -4.7302e-04],
        ...,
        [-1.0345e-02, -1.5556e-02,  8.2550e-03,  ...,  1.5404e-02,
         -6.0120e-03,  2.3895e-02],
        [-7.4997e-03, -9.4299e-03,  3.6602e-03,  ...,  3.0518e-05,
         -2.3804e-03,  3.9749e-03],
        [ 1.2192e-02, -9.7427e-03, -5.6381e-03,  ..., -7.4654e-03,
         -2.4452e-03,  1.4709e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.0176,  3.5195,  0.4727,  ..., -2.0586,  0.0227,  0.5459]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 13:06:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The squirrel falls into the category of rodent
The porcupine falls into the category of rodent
The jackal falls into the category of canine
The triceratops falls into the category of dinosaur
The cow falls into the category of bovid
The pony falls into the category of bovid
The allosaurus falls into the category of dinosaur
The mouse falls into the category of
2024-07-22 13:06:27 root INFO     [order_1_approx] starting weight calculation for The porcupine falls into the category of rodent
The allosaurus falls into the category of dinosaur
The squirrel falls into the category of rodent
The cow falls into the category of bovid
The mouse falls into the category of rodent
The jackal falls into the category of canine
The triceratops falls into the category of dinosaur
The pony falls into the category of
2024-07-22 13:06:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 13:10:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0309, -1.6230, -1.4746,  ...,  0.1119, -0.9575,  0.3804],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6738, -0.5474, -1.5391,  ..., -0.9580,  0.1044,  0.9888],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0089, -0.0017,  0.0036,  ...,  0.0099,  0.0007, -0.0053],
        [ 0.0007,  0.0074, -0.0057,  ...,  0.0093,  0.0099, -0.0006],
        [ 0.0042, -0.0013,  0.0008,  ...,  0.0032,  0.0032,  0.0039],
        ...,
        [-0.0040, -0.0064, -0.0007,  ...,  0.0019, -0.0114,  0.0186],
        [-0.0016, -0.0110, -0.0060,  ..., -0.0062,  0.0123, -0.0059],
        [ 0.0087, -0.0233,  0.0040,  ..., -0.0050, -0.0060,  0.0044]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4824,  0.7417, -1.8379,  ..., -1.5547, -0.1910,  0.8296]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 13:10:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The porcupine falls into the category of rodent
The allosaurus falls into the category of dinosaur
The squirrel falls into the category of rodent
The cow falls into the category of bovid
The mouse falls into the category of rodent
The jackal falls into the category of canine
The triceratops falls into the category of dinosaur
The pony falls into the category of
2024-07-22 13:10:12 root INFO     [order_1_approx] starting weight calculation for The triceratops falls into the category of dinosaur
The cow falls into the category of bovid
The allosaurus falls into the category of dinosaur
The mouse falls into the category of rodent
The jackal falls into the category of canine
The squirrel falls into the category of rodent
The pony falls into the category of bovid
The porcupine falls into the category of
2024-07-22 13:10:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 13:13:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0089, -1.3975, -0.9663,  ...,  0.3887,  0.3242,  0.1716],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.1758,  0.2312, -3.8926,  ...,  2.2676, -0.6621,  3.0957],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 6.1646e-03, -2.1706e-03, -5.3930e-04,  ...,  5.3558e-03,
          1.2985e-02,  7.6294e-05],
        [ 1.3809e-03,  5.0430e-03,  9.1553e-03,  ...,  7.9498e-03,
          2.9907e-03, -5.5313e-04],
        [ 4.8866e-03, -6.9084e-03,  9.5673e-03,  ..., -6.9427e-04,
         -1.1425e-03, -8.0719e-03],
        ...,
        [-6.0577e-03, -8.8348e-03, -1.4114e-04,  ...,  9.8724e-03,
         -3.9558e-03,  1.6876e-02],
        [-1.1971e-02, -6.0844e-03, -7.5912e-04,  ..., -2.6131e-03,
          1.1055e-02,  1.7014e-03],
        [ 1.7357e-03,  2.5063e-03, -8.5211e-04,  ..., -4.2915e-03,
          5.5313e-03,  1.5526e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.0352,  0.5962, -3.6055,  ...,  2.3086, -1.0664,  3.2793]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 13:13:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The triceratops falls into the category of dinosaur
The cow falls into the category of bovid
The allosaurus falls into the category of dinosaur
The mouse falls into the category of rodent
The jackal falls into the category of canine
The squirrel falls into the category of rodent
The pony falls into the category of bovid
The porcupine falls into the category of
2024-07-22 13:13:56 root INFO     [order_1_approx] starting weight calculation for The porcupine falls into the category of rodent
The pony falls into the category of bovid
The mouse falls into the category of rodent
The jackal falls into the category of canine
The allosaurus falls into the category of dinosaur
The triceratops falls into the category of dinosaur
The cow falls into the category of bovid
The squirrel falls into the category of
2024-07-22 13:13:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 13:17:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0488, -0.7549, -1.7100,  ...,  0.5361,  0.7549, -0.1033],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([3.9922, 3.9902, 2.1836,  ..., 1.3125, 0.9326, 1.1543], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-3.0670e-03, -3.4504e-03,  9.3079e-04,  ...,  1.6880e-03,
          6.5994e-03, -1.2184e-02],
        [-4.5853e-03,  2.0050e-02, -3.4332e-05,  ...,  2.6703e-05,
         -6.0921e-03, -5.2490e-03],
        [ 5.7106e-03, -1.0139e-02, -1.0996e-03,  ...,  1.5793e-03,
          1.4219e-03,  1.4435e-02],
        ...,
        [-1.1047e-02, -2.3331e-02, -6.9427e-03,  ...,  1.5480e-02,
         -7.4806e-03,  1.7944e-02],
        [-1.5900e-02, -3.0403e-03,  1.3781e-04,  ...,  2.7180e-05,
          9.0561e-03,  5.1193e-03],
        [ 6.4087e-04, -7.7209e-03,  4.0436e-04,  ..., -1.0506e-02,
          1.0014e-03,  9.3689e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[3.5957, 3.9121, 1.6094,  ..., 1.3877, 0.9390, 1.3809]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 13:17:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The porcupine falls into the category of rodent
The pony falls into the category of bovid
The mouse falls into the category of rodent
The jackal falls into the category of canine
The allosaurus falls into the category of dinosaur
The triceratops falls into the category of dinosaur
The cow falls into the category of bovid
The squirrel falls into the category of
2024-07-22 13:17:41 root INFO     [order_1_approx] starting weight calculation for The porcupine falls into the category of rodent
The cow falls into the category of bovid
The mouse falls into the category of rodent
The allosaurus falls into the category of dinosaur
The pony falls into the category of bovid
The squirrel falls into the category of rodent
The jackal falls into the category of canine
The triceratops falls into the category of
2024-07-22 13:17:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 13:21:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9990,  1.7598, -1.7227,  ...,  0.2747, -1.1465,  1.6357],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.4844, -2.3574,  2.4062,  ...,  2.0449, -3.3027,  3.3691],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 6.4240e-03,  3.2921e-03, -1.1406e-03,  ..., -2.4853e-03,
          3.0556e-03,  3.3855e-04],
        [ 2.2011e-03, -1.6212e-05,  2.0313e-03,  ...,  1.1246e-02,
          4.3983e-03,  2.2106e-03],
        [-2.0962e-03, -8.7738e-05, -1.0290e-03,  ..., -6.4583e-03,
         -5.4016e-03,  3.3569e-03],
        ...,
        [-1.2596e-02, -4.8904e-03, -2.7409e-03,  ...,  5.3558e-03,
         -2.6436e-03,  1.1909e-02],
        [ 7.6218e-03,  1.0025e-02, -6.3553e-03,  ..., -1.3481e-02,
          5.2795e-03,  3.4599e-03],
        [-7.5684e-03, -9.8267e-03,  4.9362e-03,  ...,  7.0496e-03,
          2.4185e-03, -1.6623e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1523, -2.3242,  2.2910,  ...,  1.9580, -4.0430,  3.7031]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 13:21:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The porcupine falls into the category of rodent
The cow falls into the category of bovid
The mouse falls into the category of rodent
The allosaurus falls into the category of dinosaur
The pony falls into the category of bovid
The squirrel falls into the category of rodent
The jackal falls into the category of canine
The triceratops falls into the category of
2024-07-22 13:21:26 root INFO     total operator prediction time: 1800.814912557602 seconds
2024-07-22 13:21:26 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on hyponyms - misc
2024-07-22 13:21:26 root INFO     building operator hyponyms - misc
2024-07-22 13:21:26 root INFO     [order_1_approx] starting weight calculation for A more specific term for a painting is watercolor
A more specific term for a container is bag
A more specific term for a weekday is monday
A more specific term for a citrus is lemon
A more specific term for a dessert is cake
A more specific term for a cushion is pincushion
A more specific term for a dress is gown
A more specific term for a bed is
2024-07-22 13:21:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 13:25:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5562,  0.5278,  0.9111,  ...,  0.7695, -1.9805,  0.1636],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1543, -4.2930,  0.1016,  ..., -1.8848, -0.4287, -0.9966],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0076,  0.0039, -0.0092,  ..., -0.0001, -0.0004, -0.0064],
        [-0.0087,  0.0104,  0.0204,  ...,  0.0099,  0.0087, -0.0098],
        [-0.0090,  0.0185,  0.0061,  ...,  0.0043, -0.0002,  0.0062],
        ...,
        [ 0.0011, -0.0168,  0.0184,  ...,  0.0162, -0.0048,  0.0053],
        [-0.0190,  0.0003,  0.0050,  ...,  0.0043,  0.0103,  0.0002],
        [-0.0016, -0.0082,  0.0096,  ..., -0.0174,  0.0032,  0.0381]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4285, -3.6641,  0.5693,  ..., -1.6738,  0.0317, -0.6533]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 13:25:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a painting is watercolor
A more specific term for a container is bag
A more specific term for a weekday is monday
A more specific term for a citrus is lemon
A more specific term for a dessert is cake
A more specific term for a cushion is pincushion
A more specific term for a dress is gown
A more specific term for a bed is
2024-07-22 13:25:10 root INFO     [order_1_approx] starting weight calculation for A more specific term for a container is bag
A more specific term for a cushion is pincushion
A more specific term for a dessert is cake
A more specific term for a weekday is monday
A more specific term for a painting is watercolor
A more specific term for a bed is bunk
A more specific term for a dress is gown
A more specific term for a citrus is
2024-07-22 13:25:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 13:28:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7451, -0.9326,  1.7051,  ...,  0.5928, -1.1113,  1.0254],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4609, -1.6045, -2.5664,  ..., -1.0635,  2.6133, -1.3574],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0173, -0.0174, -0.0012,  ...,  0.0018, -0.0120,  0.0005],
        [-0.0059, -0.0148,  0.0115,  ...,  0.0014, -0.0053,  0.0204],
        [-0.0149,  0.0118,  0.0045,  ...,  0.0113, -0.0014,  0.0180],
        ...,
        [ 0.0056, -0.0161,  0.0165,  ...,  0.0193, -0.0042, -0.0042],
        [ 0.0142, -0.0045,  0.0063,  ...,  0.0191, -0.0040,  0.0038],
        [ 0.0021, -0.0019, -0.0032,  ..., -0.0016,  0.0186,  0.0165]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4355,  0.0303, -2.2070,  ..., -0.9663,  2.5332, -2.5156]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 13:28:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a container is bag
A more specific term for a cushion is pincushion
A more specific term for a dessert is cake
A more specific term for a weekday is monday
A more specific term for a painting is watercolor
A more specific term for a bed is bunk
A more specific term for a dress is gown
A more specific term for a citrus is
2024-07-22 13:28:54 root INFO     [order_1_approx] starting weight calculation for A more specific term for a cushion is pincushion
A more specific term for a bed is bunk
A more specific term for a citrus is lemon
A more specific term for a dress is gown
A more specific term for a dessert is cake
A more specific term for a painting is watercolor
A more specific term for a weekday is monday
A more specific term for a container is
2024-07-22 13:28:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 13:32:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3450,  0.3784,  0.7837,  ...,  0.9502, -0.9497,  0.8745],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.4795, -0.9702, -4.1094,  ...,  2.4609,  1.4297,  4.9219],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0185, -0.0068,  0.0009,  ..., -0.0011, -0.0104, -0.0116],
        [ 0.0117, -0.0007,  0.0053,  ...,  0.0049, -0.0126,  0.0070],
        [ 0.0056,  0.0144,  0.0191,  ..., -0.0165, -0.0002,  0.0046],
        ...,
        [ 0.0094, -0.0136,  0.0096,  ...,  0.0314,  0.0008, -0.0084],
        [ 0.0036, -0.0078,  0.0117,  ..., -0.0005,  0.0110,  0.0109],
        [ 0.0079, -0.0074,  0.0084,  ...,  0.0034,  0.0156,  0.0429]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.1191, -0.7686, -4.0195,  ...,  2.3164,  1.7539,  4.3711]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 13:32:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a cushion is pincushion
A more specific term for a bed is bunk
A more specific term for a citrus is lemon
A more specific term for a dress is gown
A more specific term for a dessert is cake
A more specific term for a painting is watercolor
A more specific term for a weekday is monday
A more specific term for a container is
2024-07-22 13:32:37 root INFO     [order_1_approx] starting weight calculation for A more specific term for a bed is bunk
A more specific term for a dessert is cake
A more specific term for a painting is watercolor
A more specific term for a weekday is monday
A more specific term for a citrus is lemon
A more specific term for a dress is gown
A more specific term for a container is bag
A more specific term for a cushion is
2024-07-22 13:32:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 13:36:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0598, -0.4370, -0.9580,  ...,  0.7461, -1.3281, -0.1797],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8076, -3.9297, -4.5000,  ...,  0.0000, -0.0416, -0.9053],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 8.5449e-03, -2.0126e-02,  6.7406e-03,  ...,  9.3842e-03,
         -4.6082e-03, -1.9455e-04],
        [ 8.2855e-03, -8.4534e-03,  1.2329e-02,  ..., -5.6028e-04,
         -2.0065e-03,  1.6289e-03],
        [ 1.3733e-03,  2.6917e-02, -1.0681e-04,  ..., -1.7105e-02,
          4.0970e-03,  1.5617e-02],
        ...,
        [-4.1246e-04, -2.8549e-02, -3.8624e-03,  ...,  1.1826e-02,
         -1.5480e-02,  1.4900e-02],
        [-8.6517e-03,  1.5259e-05,  1.0406e-02,  ...,  3.2806e-04,
          1.8635e-03,  5.0964e-03],
        [ 6.9962e-03, -1.0155e-02, -2.2926e-03,  ..., -9.2163e-03,
          1.5518e-02,  2.8473e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6035, -3.9863, -3.6641,  ...,  0.8301,  0.0266, -0.7461]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 13:36:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a bed is bunk
A more specific term for a dessert is cake
A more specific term for a painting is watercolor
A more specific term for a weekday is monday
A more specific term for a citrus is lemon
A more specific term for a dress is gown
A more specific term for a container is bag
A more specific term for a cushion is
2024-07-22 13:36:21 root INFO     [order_1_approx] starting weight calculation for A more specific term for a weekday is monday
A more specific term for a bed is bunk
A more specific term for a container is bag
A more specific term for a dress is gown
A more specific term for a cushion is pincushion
A more specific term for a painting is watercolor
A more specific term for a citrus is lemon
A more specific term for a dessert is
2024-07-22 13:36:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 13:40:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7090,  1.3223,  0.6021,  ...,  1.2891, -0.7144, -0.1406],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0928,  3.9395, -2.2578,  ...,  4.7344,  0.0457, -0.3281],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0091, -0.0161, -0.0016,  ..., -0.0020, -0.0017,  0.0138],
        [ 0.0048, -0.0189,  0.0060,  ..., -0.0009, -0.0125, -0.0051],
        [-0.0092, -0.0005,  0.0092,  ...,  0.0031,  0.0044,  0.0103],
        ...,
        [ 0.0058, -0.0181,  0.0073,  ...,  0.0230, -0.0172, -0.0052],
        [-0.0174, -0.0078, -0.0039,  ...,  0.0023,  0.0124,  0.0074],
        [-0.0042, -0.0105,  0.0036,  ...,  0.0026, -0.0022,  0.0169]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7754,  3.8145, -2.1367,  ...,  4.0547, -0.4832,  0.1140]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 13:40:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a weekday is monday
A more specific term for a bed is bunk
A more specific term for a container is bag
A more specific term for a dress is gown
A more specific term for a cushion is pincushion
A more specific term for a painting is watercolor
A more specific term for a citrus is lemon
A more specific term for a dessert is
2024-07-22 13:40:02 root INFO     [order_1_approx] starting weight calculation for A more specific term for a citrus is lemon
A more specific term for a cushion is pincushion
A more specific term for a container is bag
A more specific term for a painting is watercolor
A more specific term for a dessert is cake
A more specific term for a weekday is monday
A more specific term for a bed is bunk
A more specific term for a dress is
2024-07-22 13:40:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 13:43:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7627, -0.5708,  0.5762,  ...,  1.3672, -1.4609, -0.1372],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5552, -1.6504, -3.4609,  ...,  0.2031,  0.1074, -1.8359],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0124, -0.0110, -0.0069,  ..., -0.0036,  0.0028,  0.0015],
        [-0.0024,  0.0042, -0.0042,  ...,  0.0155, -0.0026, -0.0083],
        [-0.0121,  0.0032,  0.0110,  ..., -0.0093,  0.0046,  0.0229],
        ...,
        [-0.0118, -0.0072, -0.0001,  ...,  0.0132, -0.0065, -0.0081],
        [-0.0004, -0.0049,  0.0056,  ...,  0.0045,  0.0026, -0.0123],
        [-0.0092,  0.0062,  0.0097,  ..., -0.0014,  0.0171,  0.0205]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0769, -1.5107, -2.6074,  ...,  0.0488, -0.5317, -1.3242]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 13:43:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a citrus is lemon
A more specific term for a cushion is pincushion
A more specific term for a container is bag
A more specific term for a painting is watercolor
A more specific term for a dessert is cake
A more specific term for a weekday is monday
A more specific term for a bed is bunk
A more specific term for a dress is
2024-07-22 13:43:47 root INFO     [order_1_approx] starting weight calculation for A more specific term for a cushion is pincushion
A more specific term for a weekday is monday
A more specific term for a dress is gown
A more specific term for a dessert is cake
A more specific term for a container is bag
A more specific term for a bed is bunk
A more specific term for a citrus is lemon
A more specific term for a painting is
2024-07-22 13:43:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 13:47:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6641,  0.7383, -0.0356,  ..., -0.8882, -0.3662,  1.2324],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.4648, -2.9277, -2.9570,  ...,  0.2991, -0.7373,  2.2773],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.1362e-02, -7.4501e-03,  1.2863e-02,  ..., -1.6747e-03,
         -1.2573e-02,  2.8900e-02],
        [ 4.8561e-03,  3.8872e-03,  1.1147e-02,  ...,  1.0803e-02,
         -8.0109e-03,  4.4823e-05],
        [ 1.1604e-02,  2.1317e-02,  6.0234e-03,  ...,  3.6640e-03,
         -2.0008e-03,  1.8585e-02],
        ...,
        [-1.8120e-03, -1.4542e-02,  1.3641e-02,  ...,  2.1896e-02,
         -4.3182e-03,  6.9427e-04],
        [ 4.9133e-03,  6.0730e-03,  1.3466e-02,  ..., -3.4904e-03,
          3.1395e-03,  1.3863e-02],
        [ 7.5302e-03, -1.6296e-02, -5.1460e-03,  ..., -1.1963e-02,
          1.5945e-03,  2.4994e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.0117, -3.2715, -2.3281,  ...,  0.2018,  0.1157,  1.8848]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 13:47:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a cushion is pincushion
A more specific term for a weekday is monday
A more specific term for a dress is gown
A more specific term for a dessert is cake
A more specific term for a container is bag
A more specific term for a bed is bunk
A more specific term for a citrus is lemon
A more specific term for a painting is
2024-07-22 13:47:32 root INFO     [order_1_approx] starting weight calculation for A more specific term for a painting is watercolor
A more specific term for a dress is gown
A more specific term for a citrus is lemon
A more specific term for a bed is bunk
A more specific term for a cushion is pincushion
A more specific term for a dessert is cake
A more specific term for a container is bag
A more specific term for a weekday is
2024-07-22 13:47:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 13:51:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1299,  1.1729, -0.7275,  ...,  0.1816, -0.2532, -0.4395],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.4258, -3.4395, -5.1602,  ...,  0.9663,  3.3984, -5.3203],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0083, -0.0206, -0.0062,  ..., -0.0130, -0.0234,  0.0244],
        [ 0.0091, -0.0189,  0.0160,  ..., -0.0004, -0.0072, -0.0077],
        [-0.0229, -0.0085, -0.0065,  ..., -0.0179,  0.0173,  0.0198],
        ...,
        [-0.0054, -0.0168, -0.0025,  ..., -0.0046, -0.0145, -0.0129],
        [ 0.0004, -0.0066,  0.0052,  ...,  0.0085, -0.0061,  0.0042],
        [-0.0062, -0.0230,  0.0009,  ...,  0.0077,  0.0252,  0.0178]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.5312, -2.5430, -4.6328,  ...,  0.9844,  3.4219, -4.7031]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 13:51:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a painting is watercolor
A more specific term for a dress is gown
A more specific term for a citrus is lemon
A more specific term for a bed is bunk
A more specific term for a cushion is pincushion
A more specific term for a dessert is cake
A more specific term for a container is bag
A more specific term for a weekday is
2024-07-22 13:51:16 root INFO     total operator prediction time: 1790.1927409172058 seconds
2024-07-22 13:51:16 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on antonyms - binary
2024-07-22 13:51:16 root INFO     building operator antonyms - binary
2024-07-22 13:51:16 root INFO     [order_1_approx] starting weight calculation for The opposite of decrement is increment
The opposite of dynamic is static
The opposite of occupied is vacant
The opposite of backward is forward
The opposite of up is down
The opposite of climb is descend
The opposite of forget is remember
The opposite of after is
2024-07-22 13:51:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 13:55:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2820, -0.9585,  1.5010,  ...,  0.8423,  0.4470,  0.9731],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0361, -2.3887, -3.0488,  ..., -1.7852,  6.7422,  1.4600],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0240, -0.0179,  0.0182,  ...,  0.0010, -0.0218, -0.0095],
        [-0.0339,  0.0094, -0.0225,  ..., -0.0039,  0.0029, -0.0258],
        [ 0.0020,  0.0105, -0.0411,  ..., -0.0216,  0.0088,  0.0271],
        ...,
        [-0.0310, -0.0057, -0.0155,  ...,  0.0108, -0.0125, -0.0105],
        [ 0.0045, -0.0193,  0.0169,  ..., -0.0143, -0.0229, -0.0281],
        [ 0.0014,  0.0188, -0.0163,  ..., -0.0098,  0.0045, -0.0049]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0145, -3.3438, -2.2559,  ..., -2.1973,  6.8203,  2.4375]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 13:55:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of decrement is increment
The opposite of dynamic is static
The opposite of occupied is vacant
The opposite of backward is forward
The opposite of up is down
The opposite of climb is descend
The opposite of forget is remember
The opposite of after is
2024-07-22 13:55:02 root INFO     [order_1_approx] starting weight calculation for The opposite of forget is remember
The opposite of after is before
The opposite of up is down
The opposite of decrement is increment
The opposite of dynamic is static
The opposite of occupied is vacant
The opposite of climb is descend
The opposite of backward is
2024-07-22 13:55:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 13:58:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5723, -1.0391, -0.1523,  ..., -0.2188, -0.1478,  0.3452],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.2305,  0.6670,  0.0527,  ..., -1.8672,  4.4531,  1.5723],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0071, -0.0072,  0.0082,  ..., -0.0002,  0.0095,  0.0065],
        [-0.0085, -0.0046, -0.0266,  ...,  0.0019, -0.0098,  0.0188],
        [-0.0296, -0.0015, -0.0339,  ..., -0.0039, -0.0018,  0.0211],
        ...,
        [ 0.0118, -0.0215, -0.0300,  ..., -0.0135, -0.0051,  0.0110],
        [-0.0164, -0.0171,  0.0297,  ..., -0.0312,  0.0074,  0.0163],
        [ 0.0200, -0.0173, -0.0113,  ..., -0.0008,  0.0055, -0.0022]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.6045,  0.7231,  0.0450,  ..., -2.3711,  5.1953,  2.8594]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 13:58:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of forget is remember
The opposite of after is before
The opposite of up is down
The opposite of decrement is increment
The opposite of dynamic is static
The opposite of occupied is vacant
The opposite of climb is descend
The opposite of backward is
2024-07-22 13:58:46 root INFO     [order_1_approx] starting weight calculation for The opposite of occupied is vacant
The opposite of forget is remember
The opposite of dynamic is static
The opposite of up is down
The opposite of backward is forward
The opposite of decrement is increment
The opposite of after is before
The opposite of climb is
2024-07-22 13:58:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 14:02:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1161, -1.9707,  0.5078,  ...,  0.4734,  0.2039, -0.7891],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.2207, -4.5078,  0.9180,  ..., -2.1738,  1.2910, -0.9155],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0318, -0.0312, -0.0109,  ...,  0.0233, -0.0028,  0.0121],
        [-0.0008, -0.0076, -0.0223,  ..., -0.0119, -0.0082,  0.0057],
        [-0.0080, -0.0206, -0.0247,  ...,  0.0130, -0.0007,  0.0111],
        ...,
        [ 0.0119,  0.0002,  0.0249,  ..., -0.0181,  0.0108,  0.0004],
        [-0.0130,  0.0011,  0.0388,  ..., -0.0171, -0.0191, -0.0126],
        [ 0.0158,  0.0018, -0.0162,  ...,  0.0048,  0.0159, -0.0175]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7930, -4.3594,  1.5566,  ..., -1.4229,  1.7852, -0.7222]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 14:02:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of occupied is vacant
The opposite of forget is remember
The opposite of dynamic is static
The opposite of up is down
The opposite of backward is forward
The opposite of decrement is increment
The opposite of after is before
The opposite of climb is
2024-07-22 14:02:31 root INFO     [order_1_approx] starting weight calculation for The opposite of up is down
The opposite of dynamic is static
The opposite of backward is forward
The opposite of climb is descend
The opposite of forget is remember
The opposite of after is before
The opposite of occupied is vacant
The opposite of decrement is
2024-07-22 14:02:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 14:06:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0776, -0.9160,  0.2285,  ...,  0.4048,  1.6475,  0.5317],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4028, -4.1016, -0.9492,  ..., -1.1973,  2.5078,  0.3066],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0133, -0.0344, -0.0008,  ..., -0.0156, -0.0101, -0.0325],
        [-0.0058, -0.0085, -0.0128,  ...,  0.0030, -0.0188,  0.0100],
        [-0.0113, -0.0256, -0.0392,  ..., -0.0100,  0.0162,  0.0032],
        ...,
        [-0.0077, -0.0232, -0.0142,  ..., -0.0171,  0.0054,  0.0022],
        [-0.0246,  0.0244,  0.0142,  ..., -0.0284, -0.0175,  0.0021],
        [ 0.0216,  0.0320, -0.0089,  ...,  0.0075,  0.0161, -0.0190]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3679, -4.2812,  0.0820,  ..., -0.7026,  2.7637, -0.0640]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 14:06:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of up is down
The opposite of dynamic is static
The opposite of backward is forward
The opposite of climb is descend
The opposite of forget is remember
The opposite of after is before
The opposite of occupied is vacant
The opposite of decrement is
2024-07-22 14:06:16 root INFO     [order_1_approx] starting weight calculation for The opposite of forget is remember
The opposite of climb is descend
The opposite of backward is forward
The opposite of up is down
The opposite of after is before
The opposite of decrement is increment
The opposite of occupied is vacant
The opposite of dynamic is
2024-07-22 14:06:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 14:10:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2461, -1.4951,  0.7778,  ...,  0.0166, -0.1243, -0.0492],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6309, -0.3950, -1.0215,  ..., -3.5234,  5.2891, -1.5527],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0143, -0.0296, -0.0103,  ..., -0.0098, -0.0276, -0.0008],
        [-0.0042,  0.0037, -0.0244,  ..., -0.0250,  0.0150,  0.0135],
        [ 0.0067, -0.0133, -0.0374,  ..., -0.0237,  0.0016, -0.0014],
        ...,
        [ 0.0143, -0.0167, -0.0010,  ...,  0.0067, -0.0011,  0.0136],
        [-0.0152, -0.0162,  0.0029,  ..., -0.0040,  0.0040, -0.0106],
        [ 0.0229, -0.0206, -0.0053,  ..., -0.0013,  0.0057,  0.0058]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3242,  1.0137, -1.3906,  ..., -4.0898,  6.2891, -2.0195]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 14:10:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of forget is remember
The opposite of climb is descend
The opposite of backward is forward
The opposite of up is down
The opposite of after is before
The opposite of decrement is increment
The opposite of occupied is vacant
The opposite of dynamic is
2024-07-22 14:10:02 root INFO     [order_1_approx] starting weight calculation for The opposite of up is down
The opposite of after is before
The opposite of dynamic is static
The opposite of climb is descend
The opposite of decrement is increment
The opposite of occupied is vacant
The opposite of backward is forward
The opposite of forget is
2024-07-22 14:10:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 14:13:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1289, -0.6211,  0.8018,  ...,  1.7471,  1.0156,  0.6807],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.6758,  0.2344, -1.0703,  ..., -0.2588,  2.5488,  4.8516],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0051, -0.0051,  0.0038,  ..., -0.0022, -0.0174, -0.0014],
        [-0.0139, -0.0022, -0.0028,  ...,  0.0060,  0.0063, -0.0048],
        [ 0.0230,  0.0105, -0.0309,  ..., -0.0060,  0.0384,  0.0154],
        ...,
        [-0.0110, -0.0003, -0.0164,  ..., -0.0006, -0.0151,  0.0155],
        [ 0.0076,  0.0307, -0.0297,  ..., -0.0017,  0.0167, -0.0268],
        [-0.0145,  0.0121, -0.0073,  ..., -0.0245, -0.0027, -0.0033]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.8086, -0.6001, -1.1680,  ..., -0.8638,  2.7207,  5.1445]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 14:13:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of up is down
The opposite of after is before
The opposite of dynamic is static
The opposite of climb is descend
The opposite of decrement is increment
The opposite of occupied is vacant
The opposite of backward is forward
The opposite of forget is
2024-07-22 14:13:47 root INFO     [order_1_approx] starting weight calculation for The opposite of decrement is increment
The opposite of after is before
The opposite of up is down
The opposite of climb is descend
The opposite of dynamic is static
The opposite of forget is remember
The opposite of backward is forward
The opposite of occupied is
2024-07-22 14:13:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 14:17:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7729, -0.8027,  0.5264,  ...,  0.4668,  1.0781, -0.7891],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6631,  0.2695, -4.1641,  ..., -1.1582,  1.8691, -5.2891],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0261, -0.0135,  0.0018,  ..., -0.0157, -0.0172, -0.0077],
        [ 0.0034, -0.0049, -0.0211,  ...,  0.0004, -0.0089, -0.0025],
        [-0.0224,  0.0059, -0.0111,  ..., -0.0132, -0.0028, -0.0339],
        ...,
        [ 0.0093, -0.0030,  0.0176,  ..., -0.0068, -0.0160,  0.0039],
        [-0.0169, -0.0183,  0.0037,  ..., -0.0098, -0.0032, -0.0054],
        [ 0.0171,  0.0262, -0.0258,  ..., -0.0099,  0.0131,  0.0050]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1807,  1.1348, -3.7461,  ..., -1.5254,  1.1924, -5.2188]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 14:17:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of decrement is increment
The opposite of after is before
The opposite of up is down
The opposite of climb is descend
The opposite of dynamic is static
The opposite of forget is remember
The opposite of backward is forward
The opposite of occupied is
2024-07-22 14:17:32 root INFO     [order_1_approx] starting weight calculation for The opposite of after is before
The opposite of climb is descend
The opposite of forget is remember
The opposite of decrement is increment
The opposite of backward is forward
The opposite of dynamic is static
The opposite of occupied is vacant
The opposite of up is
2024-07-22 14:17:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 14:21:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6094, -0.8027, -0.5156,  ...,  0.6831, -0.0524,  1.2051],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8652, -3.6953,  1.9395,  ..., -0.3611,  6.2773,  0.3867],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.9745e-02, -8.4152e-03,  6.1035e-05,  ...,  4.7989e-03,
         -3.5896e-03,  2.2552e-02],
        [-3.7384e-03,  3.5591e-03, -1.4679e-02,  ...,  4.9477e-03,
         -1.4778e-02,  1.7044e-02],
        [-3.5915e-03, -3.1204e-03, -7.5874e-03,  ..., -7.8659e-03,
          8.9188e-03,  1.6968e-02],
        ...,
        [-1.6632e-02, -3.0384e-03,  1.4694e-02,  ..., -1.4793e-02,
         -7.5684e-03,  1.4389e-02],
        [-1.4221e-02,  1.5030e-02,  2.8046e-02,  ..., -2.7603e-02,
         -1.7746e-02, -1.9943e-02],
        [ 1.4275e-02, -1.8768e-03,  5.3864e-03,  ...,  5.1308e-03,
          8.1940e-03, -1.5121e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5723, -3.3203,  2.6660,  ..., -0.3916,  6.2461,  0.6196]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 14:21:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of after is before
The opposite of climb is descend
The opposite of forget is remember
The opposite of decrement is increment
The opposite of backward is forward
The opposite of dynamic is static
The opposite of occupied is vacant
The opposite of up is
2024-07-22 14:21:17 root INFO     total operator prediction time: 1800.3051640987396 seconds
2024-07-22 14:21:17 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - member
2024-07-22 14:21:17 root INFO     building operator meronyms - member
2024-07-22 14:21:17 root INFO     [order_1_approx] starting weight calculation for A player is a member of a team
A spouse is a member of a couple
A elephant is a member of a herd
A star is a member of a constellation
A wolf is a member of a pack
A page is a member of a book
A parishioner is a member of a parish
A college is a member of a
2024-07-22 14:21:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 14:24:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.5654, 0.3887, 0.0015,  ..., 0.7505, 1.3965, 0.8008], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.2549,  2.0762, -0.8457,  ...,  0.9780,  0.4019,  3.3281],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0158, -0.0180,  0.0014,  ...,  0.0004,  0.0023,  0.0005],
        [-0.0023, -0.0013,  0.0041,  ..., -0.0018, -0.0028, -0.0072],
        [ 0.0180, -0.0012, -0.0021,  ..., -0.0046, -0.0076, -0.0060],
        ...,
        [-0.0136,  0.0088, -0.0025,  ...,  0.0208, -0.0120,  0.0163],
        [ 0.0043, -0.0152,  0.0077,  ..., -0.0013, -0.0054, -0.0181],
        [-0.0037,  0.0058, -0.0028,  ..., -0.0049,  0.0150,  0.0055]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3848,  2.7500, -1.1211,  ...,  0.9043,  0.9307,  3.8125]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 14:24:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A player is a member of a team
A spouse is a member of a couple
A elephant is a member of a herd
A star is a member of a constellation
A wolf is a member of a pack
A page is a member of a book
A parishioner is a member of a parish
A college is a member of a
2024-07-22 14:24:59 root INFO     [order_1_approx] starting weight calculation for A page is a member of a book
A college is a member of a university
A wolf is a member of a pack
A star is a member of a constellation
A spouse is a member of a couple
A player is a member of a team
A parishioner is a member of a parish
A elephant is a member of a
2024-07-22 14:25:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 14:28:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0474, -0.7646, -0.2452,  ...,  1.2646,  0.0043,  0.6807],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7383,  3.6914,  0.7954,  ..., -1.5723, -2.5859,  2.9727],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 9.1324e-03, -1.1505e-02, -3.3951e-04,  ..., -2.4986e-03,
         -2.0981e-03, -5.5618e-03],
        [-3.6221e-03, -7.2746e-03,  1.2131e-02,  ...,  1.0193e-02,
         -3.8452e-03,  1.9073e-05],
        [ 1.5457e-02, -9.1095e-03, -6.8970e-03,  ..., -6.4545e-03,
          1.5366e-02, -1.2711e-02],
        ...,
        [-5.5809e-03, -1.2184e-02, -9.0332e-03,  ...,  9.1400e-03,
          3.5648e-03,  1.4099e-02],
        [ 2.5368e-03, -1.2039e-02,  9.5825e-03,  ..., -8.9874e-03,
         -2.1515e-02, -2.1545e-02],
        [ 8.9569e-03,  5.9204e-03, -9.9945e-04,  ..., -3.6926e-03,
          3.8910e-03,  5.4283e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.8984,  3.5977,  1.1074,  ..., -1.1543, -3.1523,  3.9844]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 14:28:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A page is a member of a book
A college is a member of a university
A wolf is a member of a pack
A star is a member of a constellation
A spouse is a member of a couple
A player is a member of a team
A parishioner is a member of a parish
A elephant is a member of a
2024-07-22 14:28:44 root INFO     [order_1_approx] starting weight calculation for A spouse is a member of a couple
A elephant is a member of a herd
A college is a member of a university
A player is a member of a team
A star is a member of a constellation
A wolf is a member of a pack
A parishioner is a member of a parish
A page is a member of a
2024-07-22 14:28:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 14:32:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3521, -0.3169, -0.7412,  ...,  1.3633,  0.3269, -0.2319],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([5.0898, 2.1973, 2.7129,  ..., 0.6797, 2.6641, 2.9121], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0182, -0.0038,  0.0172,  ..., -0.0025, -0.0065, -0.0154],
        [-0.0044, -0.0008,  0.0233,  ..., -0.0047,  0.0060, -0.0148],
        [-0.0182, -0.0012,  0.0102,  ...,  0.0080, -0.0150, -0.0105],
        ...,
        [-0.0100,  0.0035,  0.0104,  ..., -0.0030, -0.0062,  0.0158],
        [-0.0036, -0.0144,  0.0060,  ...,  0.0113, -0.0083, -0.0039],
        [ 0.0044,  0.0077,  0.0150,  ..., -0.0087,  0.0084,  0.0215]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[5.3164, 2.2695, 2.4766,  ..., 0.1025, 3.0059, 3.0078]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 14:32:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A spouse is a member of a couple
A elephant is a member of a herd
A college is a member of a university
A player is a member of a team
A star is a member of a constellation
A wolf is a member of a pack
A parishioner is a member of a parish
A page is a member of a
2024-07-22 14:32:28 root INFO     [order_1_approx] starting weight calculation for A college is a member of a university
A wolf is a member of a pack
A star is a member of a constellation
A page is a member of a book
A player is a member of a team
A spouse is a member of a couple
A elephant is a member of a herd
A parishioner is a member of a
2024-07-22 14:32:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 14:36:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1680, -1.3418,  1.4375,  ...,  1.5889,  0.5967,  0.2422],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([2.5352, 1.4336, 2.9375,  ..., 0.8770, 0.5581, 1.8750], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0021,  0.0046,  0.0031,  ..., -0.0038, -0.0016,  0.0045],
        [ 0.0053,  0.0013,  0.0059,  ..., -0.0007,  0.0055, -0.0016],
        [-0.0004,  0.0022,  0.0024,  ...,  0.0023,  0.0047,  0.0027],
        ...,
        [-0.0060, -0.0016,  0.0067,  ...,  0.0025, -0.0133,  0.0114],
        [-0.0046, -0.0093,  0.0109,  ...,  0.0108, -0.0022, -0.0034],
        [-0.0035, -0.0074, -0.0020,  ...,  0.0008,  0.0075,  0.0008]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[2.5391, 1.7002, 3.2852,  ..., 0.7310, 0.6772, 1.1895]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 14:36:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A college is a member of a university
A wolf is a member of a pack
A star is a member of a constellation
A page is a member of a book
A player is a member of a team
A spouse is a member of a couple
A elephant is a member of a herd
A parishioner is a member of a
2024-07-22 14:36:14 root INFO     [order_1_approx] starting weight calculation for A elephant is a member of a herd
A parishioner is a member of a parish
A college is a member of a university
A star is a member of a constellation
A page is a member of a book
A spouse is a member of a couple
A wolf is a member of a pack
A player is a member of a
2024-07-22 14:36:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 14:39:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1270, -0.1921,  0.7026,  ...,  1.5781,  0.3337, -0.1414],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8330,  2.9609,  1.8750,  ..., -0.1055,  4.2070,  4.9570],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.6663e-02,  3.3722e-03,  1.2947e-02,  ...,  1.0132e-02,
         -1.0834e-02, -1.7662e-03],
        [-3.7079e-03, -9.6741e-03,  2.4475e-02,  ...,  7.6599e-03,
         -2.5368e-03, -1.7578e-02],
        [ 6.1836e-03, -1.5945e-02,  8.1329e-03,  ..., -1.3100e-02,
          1.2756e-02,  9.7046e-03],
        ...,
        [-3.5782e-03,  1.0864e-02, -5.8594e-03,  ..., -2.6436e-03,
          2.8229e-04,  1.5717e-02],
        [ 1.9073e-05, -1.3130e-02, -1.1444e-04,  ..., -2.0065e-03,
         -2.0660e-02, -3.8147e-02],
        [-1.1314e-02, -2.0996e-02,  2.3327e-03,  ...,  2.3804e-03,
          1.4656e-02,  1.0658e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5957,  3.3613,  2.0293,  ...,  0.0282,  4.8633,  5.2891]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 14:39:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A elephant is a member of a herd
A parishioner is a member of a parish
A college is a member of a university
A star is a member of a constellation
A page is a member of a book
A spouse is a member of a couple
A wolf is a member of a pack
A player is a member of a
2024-07-22 14:40:00 root INFO     [order_1_approx] starting weight calculation for A player is a member of a team
A page is a member of a book
A parishioner is a member of a parish
A college is a member of a university
A star is a member of a constellation
A elephant is a member of a herd
A wolf is a member of a pack
A spouse is a member of a
2024-07-22 14:40:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 14:43:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.6709,  0.5112,  0.8652,  ...,  0.6758, -0.6279, -0.8784],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.8857,  0.0386,  2.2930,  ..., -2.8164,  2.0703,  3.5371],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.0014e-03,  1.3046e-02,  6.8626e-03,  ..., -1.1017e-02,
          1.7563e-02, -1.2421e-02],
        [-1.4725e-02, -4.5242e-03,  9.6893e-03,  ...,  2.0981e-05,
          1.9321e-03, -1.0155e-02],
        [ 5.9128e-04, -3.3646e-03, -1.0307e-02,  ...,  7.3204e-03,
         -4.8637e-03,  3.1090e-04],
        ...,
        [-2.7351e-03, -5.7678e-03, -3.6430e-04,  ..., -4.8141e-03,
         -1.1093e-02,  4.8676e-03],
        [-8.9264e-03,  4.9362e-03,  1.0811e-02,  ...,  8.9264e-03,
         -1.8234e-02,  3.8948e-03],
        [-1.1551e-02, -1.1330e-02, -8.6975e-03,  ...,  1.4038e-03,
          2.8076e-02,  5.2452e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.1035,  0.3206,  1.8789,  ..., -2.8555,  2.0879,  3.3926]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 14:43:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A player is a member of a team
A page is a member of a book
A parishioner is a member of a parish
A college is a member of a university
A star is a member of a constellation
A elephant is a member of a herd
A wolf is a member of a pack
A spouse is a member of a
2024-07-22 14:43:43 root INFO     [order_1_approx] starting weight calculation for A parishioner is a member of a parish
A player is a member of a team
A college is a member of a university
A wolf is a member of a pack
A spouse is a member of a couple
A elephant is a member of a herd
A page is a member of a book
A star is a member of a
2024-07-22 14:43:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 14:47:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2793,  1.1367,  0.0193,  ...,  0.2180, -0.2261, -0.1821],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.7402,  2.2852,  2.8145,  ..., -2.1680, -1.7598,  2.2148],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0099, -0.0251, -0.0157,  ..., -0.0180,  0.0006, -0.0034],
        [ 0.0072, -0.0004,  0.0099,  ..., -0.0123, -0.0114, -0.0080],
        [-0.0006,  0.0079,  0.0112,  ..., -0.0005,  0.0084, -0.0100],
        ...,
        [-0.0015,  0.0023, -0.0033,  ...,  0.0072, -0.0017,  0.0097],
        [ 0.0077,  0.0026, -0.0047,  ..., -0.0194,  0.0041,  0.0013],
        [ 0.0125, -0.0218, -0.0244,  ..., -0.0076,  0.0226,  0.0119]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.6035,  2.0410,  2.7188,  ..., -1.0723, -1.7236,  1.9336]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 14:47:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A parishioner is a member of a parish
A player is a member of a team
A college is a member of a university
A wolf is a member of a pack
A spouse is a member of a couple
A elephant is a member of a herd
A page is a member of a book
A star is a member of a
2024-07-22 14:47:28 root INFO     [order_1_approx] starting weight calculation for A page is a member of a book
A parishioner is a member of a parish
A college is a member of a university
A elephant is a member of a herd
A spouse is a member of a couple
A star is a member of a constellation
A player is a member of a team
A wolf is a member of a
2024-07-22 14:47:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 14:51:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9507, -1.0195, -0.7764,  ...,  1.0850, -0.4087,  0.0518],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.0742, -0.9785, -3.0332,  ...,  2.9316, -0.7563,  4.2930],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.8356e-02, -2.7756e-02, -9.5749e-03,  ...,  1.0406e-02,
         -1.1559e-02,  1.3702e-02],
        [-2.9945e-03,  4.0207e-03,  8.1482e-03,  ...,  1.0010e-02,
         -4.2038e-03, -9.7198e-03],
        [ 9.3307e-03, -1.4753e-03,  1.9638e-02,  ..., -3.6983e-03,
         -7.5493e-03, -5.7259e-03],
        ...,
        [-7.0686e-03,  9.9564e-04,  6.2828e-03,  ...,  2.2354e-03,
          7.6294e-05,  1.8570e-02],
        [ 3.6736e-03, -2.6306e-02,  4.2915e-03,  ..., -1.6718e-03,
         -1.1948e-02, -1.0941e-02],
        [ 6.0501e-03, -4.8523e-03, -7.8583e-04,  ..., -7.8583e-03,
          2.6512e-03,  7.8506e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.8662, -0.9399, -2.9512,  ...,  3.5801, -1.2188,  4.0547]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 14:51:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A page is a member of a book
A parishioner is a member of a parish
A college is a member of a university
A elephant is a member of a herd
A spouse is a member of a couple
A star is a member of a constellation
A player is a member of a team
A wolf is a member of a
2024-07-22 14:51:12 root INFO     total operator prediction time: 1795.1402099132538 seconds
2024-07-22 14:51:12 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on noun - plural_irreg
2024-07-22 14:51:12 root INFO     building operator noun - plural_irreg
2024-07-22 14:51:12 root INFO     [order_1_approx] starting weight calculation for The plural form of county is counties
The plural form of family is families
The plural form of opportunity is opportunities
The plural form of woman is women
The plural form of city is cities
The plural form of country is countries
The plural form of industry is industries
The plural form of basis is
2024-07-22 14:51:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 14:54:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5317,  0.9414,  0.4277,  ..., -0.2754,  0.2075,  0.7471],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3484,  0.3496, -6.9453,  ...,  7.1172,  3.8281,  6.7695],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.6654e-03, -6.8245e-03,  2.6001e-02,  ..., -1.5244e-02,
          3.3531e-03, -1.4084e-02],
        [ 9.9182e-05, -1.7303e-02,  4.5662e-03,  ...,  1.2260e-02,
          9.9258e-03,  1.4320e-02],
        [-1.4793e-02,  1.6571e-02, -1.8295e-02,  ..., -2.7180e-03,
          6.4201e-03, -7.5569e-03],
        ...,
        [-8.5831e-03, -1.6968e-02, -1.0880e-02,  ..., -2.8091e-02,
          4.1275e-03, -1.0269e-02],
        [ 2.0599e-03, -5.4398e-03,  1.4214e-02,  ..., -1.6693e-02,
         -3.1494e-02, -6.0310e-03],
        [-1.3390e-03,  1.1879e-02,  4.7188e-03,  ..., -2.1271e-02,
          9.3460e-04, -7.4005e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1755,  0.0327, -6.9648,  ...,  5.8555,  4.4023,  7.2812]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 14:54:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of county is counties
The plural form of family is families
The plural form of opportunity is opportunities
The plural form of woman is women
The plural form of city is cities
The plural form of country is countries
The plural form of industry is industries
The plural form of basis is
2024-07-22 14:54:56 root INFO     [order_1_approx] starting weight calculation for The plural form of woman is women
The plural form of basis is bases
The plural form of country is countries
The plural form of county is counties
The plural form of opportunity is opportunities
The plural form of family is families
The plural form of industry is industries
The plural form of city is
2024-07-22 14:54:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 14:58:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9053,  0.3643, -0.5078,  ...,  0.0276,  0.7100,  0.8984],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0752, -0.9395, -0.7695,  ..., -2.5684,  3.5977, -2.0078],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0205, -0.0235,  0.0052,  ...,  0.0061,  0.0017, -0.0077],
        [-0.0047, -0.0196, -0.0009,  ...,  0.0023, -0.0090,  0.0007],
        [ 0.0083,  0.0060, -0.0077,  ..., -0.0001,  0.0145, -0.0090],
        ...,
        [-0.0056,  0.0030, -0.0104,  ..., -0.0041, -0.0029, -0.0083],
        [-0.0040,  0.0017,  0.0111,  ..., -0.0106, -0.0285,  0.0050],
        [-0.0106,  0.0028, -0.0051,  ..., -0.0056,  0.0150, -0.0266]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6392, -1.0420, -0.3359,  ..., -2.9277,  3.5820, -2.2305]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 14:58:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of woman is women
The plural form of basis is bases
The plural form of country is countries
The plural form of county is counties
The plural form of opportunity is opportunities
The plural form of family is families
The plural form of industry is industries
The plural form of city is
2024-07-22 14:58:39 root INFO     [order_1_approx] starting weight calculation for The plural form of city is cities
The plural form of family is families
The plural form of opportunity is opportunities
The plural form of county is counties
The plural form of industry is industries
The plural form of basis is bases
The plural form of woman is women
The plural form of country is
2024-07-22 14:58:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 15:02:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.0765, 0.8442, 0.3425,  ..., 0.4380, 1.0186, 0.7734], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8975, -1.7471, -1.7090,  ..., -3.7617,  1.5684, -0.6875],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0120, -0.0132,  0.0082,  ..., -0.0036,  0.0025, -0.0234],
        [ 0.0022, -0.0169, -0.0041,  ...,  0.0024, -0.0075, -0.0025],
        [ 0.0239,  0.0054, -0.0091,  ..., -0.0085,  0.0207, -0.0056],
        ...,
        [ 0.0025, -0.0003, -0.0015,  ..., -0.0040, -0.0053, -0.0143],
        [ 0.0097, -0.0072,  0.0126,  ...,  0.0027, -0.0244,  0.0020],
        [-0.0075,  0.0074, -0.0060,  ..., -0.0129,  0.0191, -0.0096]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5293, -1.5859, -1.5439,  ..., -4.3438,  1.5391,  0.2866]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 15:02:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of city is cities
The plural form of family is families
The plural form of opportunity is opportunities
The plural form of county is counties
The plural form of industry is industries
The plural form of basis is bases
The plural form of woman is women
The plural form of country is
2024-07-22 15:02:22 root INFO     [order_1_approx] starting weight calculation for The plural form of basis is bases
The plural form of family is families
The plural form of industry is industries
The plural form of city is cities
The plural form of opportunity is opportunities
The plural form of country is countries
The plural form of woman is women
The plural form of county is
2024-07-22 15:02:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 15:06:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1415,  1.0635, -0.6797,  ...,  0.4019, -0.4316,  1.0625],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3601,  1.7148, -1.7344,  ..., -3.5586,  0.0684, -0.9819],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0163, -0.0142,  0.0181,  ...,  0.0042, -0.0121, -0.0136],
        [-0.0059, -0.0188, -0.0034,  ..., -0.0049, -0.0045,  0.0045],
        [ 0.0328,  0.0103,  0.0026,  ..., -0.0137,  0.0225, -0.0153],
        ...,
        [-0.0011, -0.0070, -0.0053,  ..., -0.0034,  0.0074, -0.0048],
        [ 0.0042, -0.0055,  0.0080,  ..., -0.0010, -0.0140, -0.0004],
        [-0.0008,  0.0117,  0.0201,  ..., -0.0088,  0.0079, -0.0321]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2927,  2.0918, -1.6572,  ..., -3.3438,  0.7627, -1.0693]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 15:06:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of basis is bases
The plural form of family is families
The plural form of industry is industries
The plural form of city is cities
The plural form of opportunity is opportunities
The plural form of country is countries
The plural form of woman is women
The plural form of county is
2024-07-22 15:06:05 root INFO     [order_1_approx] starting weight calculation for The plural form of county is counties
The plural form of city is cities
The plural form of opportunity is opportunities
The plural form of woman is women
The plural form of industry is industries
The plural form of basis is bases
The plural form of country is countries
The plural form of family is
2024-07-22 15:06:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 15:09:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1688,  0.6157,  0.7686,  ...,  0.2827,  0.6177,  0.7285],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8770,  0.3450, -1.9268,  ..., -3.5430,  1.8594,  3.9219],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0272, -0.0036,  0.0305,  ...,  0.0021,  0.0090, -0.0305],
        [ 0.0080, -0.0143, -0.0016,  ...,  0.0131,  0.0014, -0.0079],
        [ 0.0104,  0.0070, -0.0122,  ..., -0.0026,  0.0152, -0.0162],
        ...,
        [-0.0166, -0.0028, -0.0038,  ..., -0.0163,  0.0089, -0.0147],
        [-0.0067,  0.0023,  0.0246,  ..., -0.0088, -0.0301,  0.0099],
        [-0.0130, -0.0068, -0.0262,  ..., -0.0075,  0.0149, -0.0021]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0771,  0.5640, -1.7344,  ..., -4.2266,  1.1777,  4.5078]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 15:09:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of county is counties
The plural form of city is cities
The plural form of opportunity is opportunities
The plural form of woman is women
The plural form of industry is industries
The plural form of basis is bases
The plural form of country is countries
The plural form of family is
2024-07-22 15:09:48 root INFO     [order_1_approx] starting weight calculation for The plural form of basis is bases
The plural form of woman is women
The plural form of country is countries
The plural form of city is cities
The plural form of opportunity is opportunities
The plural form of county is counties
The plural form of family is families
The plural form of industry is
2024-07-22 15:09:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 15:13:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.6660, 0.5786, 0.5781,  ..., 0.2915, 1.0586, 0.7295], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.1250, -3.3340,  0.1938,  ...,  0.9014, -3.0273,  4.6523],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0233, -0.0159,  0.0089,  ..., -0.0002,  0.0063,  0.0034],
        [-0.0056, -0.0125, -0.0047,  ...,  0.0196, -0.0022, -0.0135],
        [ 0.0057,  0.0105, -0.0253,  ...,  0.0114,  0.0107, -0.0002],
        ...,
        [ 0.0076, -0.0061, -0.0015,  ..., -0.0083,  0.0063, -0.0128],
        [ 0.0006, -0.0175,  0.0154,  ..., -0.0118, -0.0221,  0.0097],
        [-0.0006,  0.0206, -0.0042,  ..., -0.0163,  0.0145, -0.0192]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.1270, -3.7305, -0.8911,  ...,  0.6816, -3.3379,  4.1250]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 15:13:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of basis is bases
The plural form of woman is women
The plural form of country is countries
The plural form of city is cities
The plural form of opportunity is opportunities
The plural form of county is counties
The plural form of family is families
The plural form of industry is
2024-07-22 15:13:28 root INFO     [order_1_approx] starting weight calculation for The plural form of family is families
The plural form of industry is industries
The plural form of woman is women
The plural form of city is cities
The plural form of country is countries
The plural form of basis is bases
The plural form of county is counties
The plural form of opportunity is
2024-07-22 15:13:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 15:16:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4023, -0.4592,  0.7856,  ...,  0.1018,  0.8364, -0.3508],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.4375,  0.8340, -0.5898,  ...,  1.0449, -0.3721,  2.7969],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0142, -0.0262,  0.0161,  ..., -0.0104, -0.0050, -0.0109],
        [-0.0061,  0.0049, -0.0193,  ...,  0.0115,  0.0086,  0.0100],
        [ 0.0080, -0.0103, -0.0109,  ...,  0.0004,  0.0127,  0.0037],
        ...,
        [-0.0017, -0.0240,  0.0066,  ..., -0.0244, -0.0124, -0.0069],
        [-0.0011,  0.0101,  0.0086,  ..., -0.0032, -0.0177,  0.0043],
        [ 0.0160, -0.0002,  0.0150,  ..., -0.0079,  0.0161, -0.0164]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.5371,  1.6797, -1.1426,  ...,  0.7671, -0.7930,  3.6211]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 15:16:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of family is families
The plural form of industry is industries
The plural form of woman is women
The plural form of city is cities
The plural form of country is countries
The plural form of basis is bases
The plural form of county is counties
The plural form of opportunity is
2024-07-22 15:16:58 root INFO     [order_1_approx] starting weight calculation for The plural form of basis is bases
The plural form of county is counties
The plural form of opportunity is opportunities
The plural form of family is families
The plural form of country is countries
The plural form of industry is industries
The plural form of city is cities
The plural form of woman is
2024-07-22 15:16:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 15:20:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.0781,  0.6748, -1.0967,  ...,  0.5752,  0.4766,  0.4312],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.8359, -1.2695,  0.9072,  ..., -0.7017,  3.0312, -0.1880],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.6052e-02, -6.3934e-03,  1.0590e-02,  ...,  5.5313e-03,
          3.0632e-03, -9.6588e-03],
        [-6.4163e-03, -1.7883e-02, -4.9400e-03,  ...,  6.6910e-03,
         -4.6577e-03,  1.2161e-02],
        [ 1.1780e-02, -9.3079e-03, -2.8671e-02,  ..., -9.4986e-03,
          6.4735e-03, -9.4299e-03],
        ...,
        [-1.1124e-02,  4.7684e-05,  3.8481e-04,  ..., -1.5457e-02,
         -2.6112e-03, -8.8196e-03],
        [ 6.0272e-03, -1.4282e-02,  5.5389e-03,  ..., -4.3449e-03,
         -2.8046e-02,  6.2332e-03],
        [-1.2314e-02, -6.7568e-04, -5.6648e-03,  ...,  6.9504e-03,
          7.9880e-03, -1.2634e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.4844, -0.8379,  0.4480,  ..., -0.9043,  3.1289,  1.2500]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 15:20:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of basis is bases
The plural form of county is counties
The plural form of opportunity is opportunities
The plural form of family is families
The plural form of country is countries
The plural form of industry is industries
The plural form of city is cities
The plural form of woman is
2024-07-22 15:20:37 root INFO     total operator prediction time: 1765.5175545215607 seconds
2024-07-22 15:20:37 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on Ving - verb_inf
2024-07-22 15:20:37 root INFO     building operator Ving - verb_inf
2024-07-22 15:20:37 root INFO     [order_1_approx] starting weight calculation for believing is the active form of believe
continuing is the active form of continue
seeming is the active form of seem
containing is the active form of contain
maintaining is the active form of maintain
asking is the active form of ask
involving is the active form of involve
appearing is the active form of
2024-07-22 15:20:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 15:24:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4973, -0.3831,  2.2266,  ...,  0.3452,  1.6777,  0.3757],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([2.9473, 0.1318, 1.6641,  ..., 0.9219, 0.4072, 0.5664], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0117, -0.0078,  0.0190,  ..., -0.0030, -0.0022, -0.0029],
        [-0.0069, -0.0158,  0.0117,  ..., -0.0042, -0.0060,  0.0164],
        [ 0.0130, -0.0104, -0.0140,  ...,  0.0066, -0.0016, -0.0075],
        ...,
        [-0.0202, -0.0038, -0.0072,  ..., -0.0117, -0.0112,  0.0030],
        [ 0.0007, -0.0094,  0.0077,  ..., -0.0283, -0.0187,  0.0148],
        [ 0.0144,  0.0060,  0.0102,  ..., -0.0094,  0.0238, -0.0257]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1328,  0.8501,  1.8955,  ...,  1.6309, -0.2114, -0.1323]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 15:24:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for believing is the active form of believe
continuing is the active form of continue
seeming is the active form of seem
containing is the active form of contain
maintaining is the active form of maintain
asking is the active form of ask
involving is the active form of involve
appearing is the active form of
2024-07-22 15:24:21 root INFO     [order_1_approx] starting weight calculation for seeming is the active form of seem
containing is the active form of contain
appearing is the active form of appear
involving is the active form of involve
maintaining is the active form of maintain
continuing is the active form of continue
believing is the active form of believe
asking is the active form of
2024-07-22 15:24:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 15:28:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.7656, -0.8315,  0.4597,  ...,  0.2593,  1.8594,  0.0137],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([3.4082, 0.4814, 2.5820,  ..., 1.4170, 2.5703, 2.4668], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0104,  0.0015,  0.0062,  ..., -0.0112, -0.0176,  0.0082],
        [ 0.0015, -0.0250,  0.0058,  ..., -0.0026, -0.0255, -0.0246],
        [ 0.0018, -0.0114, -0.0157,  ...,  0.0087, -0.0057, -0.0098],
        ...,
        [-0.0208, -0.0227,  0.0027,  ..., -0.0017,  0.0056, -0.0088],
        [-0.0063,  0.0021,  0.0012,  ...,  0.0061, -0.0277,  0.0016],
        [-0.0032, -0.0165,  0.0090,  ...,  0.0076,  0.0268,  0.0056]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[3.3672, 0.7061, 2.8965,  ..., 1.6113, 1.9570, 2.2129]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 15:28:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for seeming is the active form of seem
containing is the active form of contain
appearing is the active form of appear
involving is the active form of involve
maintaining is the active form of maintain
continuing is the active form of continue
believing is the active form of believe
asking is the active form of
2024-07-22 15:28:06 root INFO     [order_1_approx] starting weight calculation for involving is the active form of involve
asking is the active form of ask
maintaining is the active form of maintain
containing is the active form of contain
appearing is the active form of appear
seeming is the active form of seem
continuing is the active form of continue
believing is the active form of
2024-07-22 15:28:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 15:31:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.1992, -1.2783,  1.4346,  ...,  0.4990,  2.2090, -0.3474],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 6.4531,  1.1562, -0.2109,  ..., -3.4180, -2.8496,  2.2188],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.0986e-02, -2.2049e-03,  2.0020e-02,  ..., -2.8114e-03,
          5.0087e-03, -7.7019e-03],
        [ 3.3932e-03, -9.7275e-03,  1.0841e-02,  ..., -2.9411e-03,
         -5.7869e-03, -1.1711e-03],
        [-2.3556e-03, -3.8490e-03, -2.6276e-02,  ..., -1.6663e-02,
          5.1308e-03,  5.5962e-03],
        ...,
        [-1.4839e-02, -9.0027e-03, -3.2902e-05,  ..., -1.0643e-02,
         -7.7591e-03,  6.6452e-03],
        [-9.7504e-03,  7.9651e-03,  8.1177e-03,  ..., -1.4214e-02,
         -2.9404e-02,  7.0152e-03],
        [-1.0468e-02,  2.6123e-02,  6.3019e-03,  ...,  1.0971e-02,
          1.1414e-02, -2.0416e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.4219,  2.0625, -0.9419,  ..., -3.2188, -2.5820,  3.0195]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 15:31:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for involving is the active form of involve
asking is the active form of ask
maintaining is the active form of maintain
containing is the active form of contain
appearing is the active form of appear
seeming is the active form of seem
continuing is the active form of continue
believing is the active form of
2024-07-22 15:31:49 root INFO     [order_1_approx] starting weight calculation for appearing is the active form of appear
asking is the active form of ask
believing is the active form of believe
maintaining is the active form of maintain
seeming is the active form of seem
involving is the active form of involve
continuing is the active form of continue
containing is the active form of
2024-07-22 15:31:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 15:35:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0811, -0.5854,  0.5786,  ..., -0.2632,  1.2812,  0.5884],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.7109, -1.2422, -2.0527,  ..., -2.4336,  2.5273, -0.1836],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0177, -0.0048, -0.0037,  ..., -0.0099,  0.0144, -0.0087],
        [-0.0097, -0.0089,  0.0154,  ..., -0.0036, -0.0078, -0.0019],
        [ 0.0029, -0.0047,  0.0087,  ..., -0.0028,  0.0099, -0.0030],
        ...,
        [-0.0141, -0.0116, -0.0017,  ..., -0.0048,  0.0116, -0.0059],
        [-0.0210, -0.0053,  0.0065,  ..., -0.0169, -0.0126,  0.0055],
        [-0.0117,  0.0123,  0.0051,  ..., -0.0132,  0.0073, -0.0160]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.6367, -1.5342, -2.6914,  ..., -2.2832,  2.9180,  0.0636]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 15:35:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for appearing is the active form of appear
asking is the active form of ask
believing is the active form of believe
maintaining is the active form of maintain
seeming is the active form of seem
involving is the active form of involve
continuing is the active form of continue
containing is the active form of
2024-07-22 15:35:34 root INFO     [order_1_approx] starting weight calculation for seeming is the active form of seem
involving is the active form of involve
asking is the active form of ask
believing is the active form of believe
maintaining is the active form of maintain
appearing is the active form of appear
containing is the active form of contain
continuing is the active form of
2024-07-22 15:35:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 15:39:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2169, -1.6367,  2.2148,  ...,  0.5742,  1.7793,  0.9761],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.8281, -3.3770, -2.3770,  ...,  0.1873,  0.4390,  1.7373],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.5930e-02, -1.3718e-02,  5.5313e-04,  ..., -1.6037e-02,
          1.9112e-03,  7.0953e-04],
        [-3.0212e-03, -1.5068e-03, -2.3193e-03,  ...,  1.7529e-03,
         -1.0910e-03,  6.6471e-04],
        [ 1.4008e-02,  6.0844e-04, -1.0040e-02,  ...,  1.1620e-02,
          1.1959e-03,  5.9242e-03],
        ...,
        [-1.9592e-02, -1.4000e-02, -3.1471e-05,  ..., -1.0902e-02,
          7.5150e-03,  1.8158e-03],
        [-2.0027e-03,  4.8370e-03, -3.0327e-03,  ..., -4.0970e-03,
         -2.4857e-02,  1.1665e-02],
        [ 4.9744e-03,  2.4414e-02,  4.3945e-03,  ...,  2.1648e-03,
          2.0981e-03, -1.0735e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.5391, -2.5703, -2.3438,  ...,  0.3755,  0.0811,  1.5488]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 15:39:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for seeming is the active form of seem
involving is the active form of involve
asking is the active form of ask
believing is the active form of believe
maintaining is the active form of maintain
appearing is the active form of appear
containing is the active form of contain
continuing is the active form of
2024-07-22 15:39:19 root INFO     [order_1_approx] starting weight calculation for continuing is the active form of continue
maintaining is the active form of maintain
appearing is the active form of appear
believing is the active form of believe
seeming is the active form of seem
asking is the active form of ask
containing is the active form of contain
involving is the active form of
2024-07-22 15:39:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 15:43:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0547, -0.4446,  1.1914,  ...,  0.8560,  1.8945,  1.4902],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.2891, -1.9082, -1.7764,  ...,  0.9155,  1.5977,  1.9111],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0105, -0.0113,  0.0107,  ...,  0.0176, -0.0157, -0.0055],
        [-0.0146,  0.0078, -0.0062,  ...,  0.0057,  0.0103,  0.0172],
        [ 0.0137,  0.0120,  0.0050,  ...,  0.0117,  0.0032, -0.0079],
        ...,
        [ 0.0029, -0.0103,  0.0072,  ..., -0.0042,  0.0123, -0.0076],
        [-0.0065,  0.0208,  0.0171,  ..., -0.0007, -0.0257,  0.0109],
        [ 0.0057,  0.0042,  0.0173,  ..., -0.0073,  0.0184, -0.0176]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.1641, -1.3008, -0.8291,  ..., -0.2778,  2.2812,  1.2598]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 15:43:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for continuing is the active form of continue
maintaining is the active form of maintain
appearing is the active form of appear
believing is the active form of believe
seeming is the active form of seem
asking is the active form of ask
containing is the active form of contain
involving is the active form of
2024-07-22 15:43:01 root INFO     [order_1_approx] starting weight calculation for believing is the active form of believe
asking is the active form of ask
appearing is the active form of appear
seeming is the active form of seem
continuing is the active form of continue
containing is the active form of contain
involving is the active form of involve
maintaining is the active form of
2024-07-22 15:43:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 15:46:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1201, -0.9902,  1.4531,  ...,  0.5254,  0.5889,  0.6787],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.1719, -2.3672, -4.6641,  ...,  0.1428,  1.4443,  6.4219],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0090,  0.0041,  0.0166,  ..., -0.0097,  0.0084, -0.0116],
        [ 0.0048,  0.0141,  0.0054,  ..., -0.0075,  0.0054,  0.0063],
        [-0.0006,  0.0242,  0.0085,  ..., -0.0019,  0.0025, -0.0023],
        ...,
        [-0.0242, -0.0219, -0.0135,  ..., -0.0031,  0.0118,  0.0001],
        [ 0.0101,  0.0058,  0.0032,  ..., -0.0201, -0.0144,  0.0069],
        [ 0.0078,  0.0141,  0.0014,  ..., -0.0097,  0.0221, -0.0085]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.6250, -1.7988, -4.2773,  ...,  0.2891,  1.5908,  7.1289]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 15:46:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for believing is the active form of believe
asking is the active form of ask
appearing is the active form of appear
seeming is the active form of seem
continuing is the active form of continue
containing is the active form of contain
involving is the active form of involve
maintaining is the active form of
2024-07-22 15:46:45 root INFO     [order_1_approx] starting weight calculation for maintaining is the active form of maintain
asking is the active form of ask
continuing is the active form of continue
believing is the active form of believe
appearing is the active form of appear
containing is the active form of contain
involving is the active form of involve
seeming is the active form of
2024-07-22 15:46:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 15:50:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9092, -0.8257,  1.8281,  ...,  1.0723,  1.2812,  0.1880],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8984,  1.6797,  4.2930,  ..., -0.5552, -4.4688,  3.7617],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0001,  0.0049,  0.0239,  ..., -0.0035,  0.0077, -0.0052],
        [-0.0061, -0.0222, -0.0112,  ..., -0.0046, -0.0078,  0.0040],
        [ 0.0277, -0.0128, -0.0038,  ...,  0.0181, -0.0097,  0.0058],
        ...,
        [-0.0364, -0.0190, -0.0039,  ..., -0.0223, -0.0202, -0.0191],
        [ 0.0017,  0.0316,  0.0095,  ..., -0.0116, -0.0131,  0.0109],
        [-0.0028, -0.0019,  0.0177,  ..., -0.0272,  0.0186, -0.0211]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7788,  1.6240,  4.3516,  ..., -0.5625, -4.8672,  4.3281]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 15:50:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for maintaining is the active form of maintain
asking is the active form of ask
continuing is the active form of continue
believing is the active form of believe
appearing is the active form of appear
containing is the active form of contain
involving is the active form of involve
seeming is the active form of
2024-07-22 15:50:31 root INFO     total operator prediction time: 1793.480155467987 seconds
2024-07-22 15:50:31 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_Ving - Ved
2024-07-22 15:50:31 root INFO     building operator verb_Ving - Ved
2024-07-22 15:50:31 root INFO     [order_1_approx] starting weight calculation for After something is containing, it has contained
After something is describing, it has described
After something is introducing, it has introduced
After something is involving, it has involved
After something is including, it has included
After something is expecting, it has expected
After something is appearing, it has appeared
After something is allowing, it has
2024-07-22 15:50:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 15:54:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7295, -0.8955,  0.7524,  ...,  0.5420,  0.7378, -0.2251],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0924,  0.0940,  0.7344,  ...,  1.5713, -1.9639,  3.7246],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0149, -0.0154, -0.0023,  ...,  0.0014,  0.0068,  0.0011],
        [-0.0028, -0.0117,  0.0119,  ..., -0.0078,  0.0043,  0.0017],
        [-0.0108, -0.0158, -0.0146,  ..., -0.0124,  0.0056,  0.0082],
        ...,
        [-0.0215, -0.0264, -0.0059,  ..., -0.0084,  0.0013,  0.0065],
        [ 0.0054, -0.0122,  0.0202,  ..., -0.0261, -0.0181,  0.0120],
        [ 0.0104, -0.0017,  0.0047,  ..., -0.0003, -0.0056, -0.0135]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5142, -0.0366,  0.5557,  ...,  1.1582, -2.2422,  3.0625]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 15:54:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is containing, it has contained
After something is describing, it has described
After something is introducing, it has introduced
After something is involving, it has involved
After something is including, it has included
After something is expecting, it has expected
After something is appearing, it has appeared
After something is allowing, it has
2024-07-22 15:54:13 root INFO     [order_1_approx] starting weight calculation for After something is including, it has included
After something is expecting, it has expected
After something is containing, it has contained
After something is describing, it has described
After something is introducing, it has introduced
After something is involving, it has involved
After something is allowing, it has allowed
After something is appearing, it has
2024-07-22 15:54:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 15:57:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1836,  0.5928,  0.9912,  ..., -0.7124,  1.2461, -0.3574],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8340,  2.1211, -0.8203,  ...,  4.5078, -2.8379,  0.8994],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.6510e-02, -2.8946e-02, -6.8283e-03,  ...,  1.0162e-02,
         -3.8891e-03, -1.4046e-02],
        [-1.9913e-02, -2.5177e-02,  2.7943e-04,  ..., -1.6174e-03,
         -8.8654e-03,  1.4008e-02],
        [-3.4142e-03,  7.2441e-03, -2.1118e-02,  ...,  5.1689e-04,
         -4.0207e-03, -2.8248e-03],
        ...,
        [-9.1629e-03, -3.4027e-03, -1.2680e-02,  ..., -1.8524e-02,
         -1.8692e-02,  2.0767e-02],
        [-1.1673e-03, -1.1765e-02,  6.5727e-03,  ..., -2.3285e-02,
         -1.0750e-02,  7.6294e-05],
        [-3.5191e-03,  2.9510e-02,  5.4970e-03,  ...,  6.4545e-03,
          7.2174e-03, -2.0340e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4668,  2.9180, -0.8945,  ...,  5.5273, -3.9824,  0.4634]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 15:57:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is including, it has included
After something is expecting, it has expected
After something is containing, it has contained
After something is describing, it has described
After something is introducing, it has introduced
After something is involving, it has involved
After something is allowing, it has allowed
After something is appearing, it has
2024-07-22 15:57:59 root INFO     [order_1_approx] starting weight calculation for After something is describing, it has described
After something is appearing, it has appeared
After something is involving, it has involved
After something is including, it has included
After something is introducing, it has introduced
After something is allowing, it has allowed
After something is expecting, it has expected
After something is containing, it has
2024-07-22 15:57:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 16:01:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5312,  0.3174,  0.6797,  ...,  0.1907,  0.6323,  0.0710],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7988,  2.1699,  1.3105,  ..., -0.1294, -1.5518,  2.3457],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0092, -0.0130, -0.0014,  ..., -0.0031,  0.0020, -0.0106],
        [-0.0097, -0.0044, -0.0083,  ..., -0.0082, -0.0060,  0.0048],
        [-0.0118, -0.0017,  0.0021,  ...,  0.0010,  0.0079,  0.0079],
        ...,
        [-0.0152,  0.0061, -0.0082,  ..., -0.0186,  0.0009,  0.0189],
        [-0.0018, -0.0114,  0.0012,  ..., -0.0008, -0.0081,  0.0142],
        [-0.0086, -0.0065, -0.0077,  ..., -0.0227, -0.0055, -0.0055]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5088,  2.3477,  0.8838,  ..., -0.1215, -1.7842,  2.6914]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 16:01:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is describing, it has described
After something is appearing, it has appeared
After something is involving, it has involved
After something is including, it has included
After something is introducing, it has introduced
After something is allowing, it has allowed
After something is expecting, it has expected
After something is containing, it has
2024-07-22 16:01:44 root INFO     [order_1_approx] starting weight calculation for After something is appearing, it has appeared
After something is allowing, it has allowed
After something is expecting, it has expected
After something is including, it has included
After something is containing, it has contained
After something is introducing, it has introduced
After something is involving, it has involved
After something is describing, it has
2024-07-22 16:01:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 16:05:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1660,  0.2646,  1.1123,  ..., -0.6484,  0.3496, -1.4453],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0439,  0.3887,  2.6719,  ...,  1.9121, -2.7988,  2.6934],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.5085e-02, -1.0887e-02, -3.0861e-03,  ..., -4.4022e-03,
         -9.0714e-03, -1.5053e-02],
        [ 5.1117e-03, -9.6588e-03,  7.3624e-04,  ..., -4.6806e-03,
          1.8768e-03, -8.9798e-03],
        [ 2.7084e-03,  1.4297e-02, -1.5320e-02,  ..., -1.4442e-02,
         -2.2583e-03,  8.0109e-03],
        ...,
        [-2.8961e-02, -2.0493e-02,  3.9673e-03,  ..., -9.8343e-03,
          7.7057e-03,  4.6768e-03],
        [-8.3923e-05, -2.8992e-04,  1.1444e-02,  ..., -3.2471e-02,
         -4.4006e-02, -1.0887e-02],
        [ 1.3641e-02,  2.1484e-02,  1.3000e-02,  ..., -9.1858e-03,
          1.0155e-02, -1.9379e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.1006,  0.3533,  2.5703,  ...,  1.8965, -2.9297,  2.2969]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 16:05:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is appearing, it has appeared
After something is allowing, it has allowed
After something is expecting, it has expected
After something is including, it has included
After something is containing, it has contained
After something is introducing, it has introduced
After something is involving, it has involved
After something is describing, it has
2024-07-22 16:05:29 root INFO     [order_1_approx] starting weight calculation for After something is involving, it has involved
After something is allowing, it has allowed
After something is containing, it has contained
After something is introducing, it has introduced
After something is appearing, it has appeared
After something is describing, it has described
After something is including, it has included
After something is expecting, it has
2024-07-22 16:05:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 16:09:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2363,  0.4961, -0.0640,  ...,  0.7266,  1.9326,  1.3828],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3691,  1.4297,  1.1113,  ...,  1.8164, -4.3750,  3.3672],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.1520e-02, -2.1194e-02, -2.5101e-03,  ..., -3.7422e-03,
         -1.2482e-02,  1.7929e-03],
        [-2.6947e-02, -8.5831e-03, -1.5160e-02,  ..., -9.2468e-03,
         -9.6588e-03, -1.0529e-03],
        [ 8.2932e-03,  6.2561e-03, -1.0529e-02,  ...,  2.7542e-03,
         -7.9155e-05,  1.0864e-02],
        ...,
        [-1.3962e-02, -1.5610e-02,  2.3766e-03,  ..., -1.2787e-02,
         -1.4946e-02, -4.6616e-03],
        [ 5.4092e-03,  2.9049e-03,  3.3112e-03,  ..., -1.2688e-02,
         -3.1616e-02,  7.6294e-03],
        [-6.1188e-03,  3.1158e-02,  1.0612e-02,  ...,  2.9488e-03,
          3.4103e-03, -2.6688e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5303,  1.3672,  1.3896,  ...,  1.5566, -4.3438,  3.5195]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 16:09:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is involving, it has involved
After something is allowing, it has allowed
After something is containing, it has contained
After something is introducing, it has introduced
After something is appearing, it has appeared
After something is describing, it has described
After something is including, it has included
After something is expecting, it has
2024-07-22 16:09:12 root INFO     [order_1_approx] starting weight calculation for After something is expecting, it has expected
After something is allowing, it has allowed
After something is introducing, it has introduced
After something is involving, it has involved
After something is appearing, it has appeared
After something is containing, it has contained
After something is describing, it has described
After something is including, it has
2024-07-22 16:09:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 16:12:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0752, -0.8174,  1.5762,  ...,  1.0010,  0.6709,  0.8130],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0117, -0.0310,  0.8281,  ...,  2.9707, -2.6426,  1.8496],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0139, -0.0255,  0.0157,  ..., -0.0024,  0.0038, -0.0009],
        [-0.0192, -0.0188,  0.0210,  ..., -0.0044, -0.0195,  0.0095],
        [-0.0015, -0.0146,  0.0021,  ..., -0.0086, -0.0091,  0.0070],
        ...,
        [-0.0198,  0.0030, -0.0195,  ...,  0.0062, -0.0007, -0.0006],
        [ 0.0055,  0.0050,  0.0043,  ..., -0.0249, -0.0203,  0.0203],
        [-0.0112,  0.0209, -0.0104,  ..., -0.0193, -0.0044, -0.0341]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3643,  0.2010,  0.0732,  ...,  2.8203, -2.6836,  2.0820]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 16:12:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is expecting, it has expected
After something is allowing, it has allowed
After something is introducing, it has introduced
After something is involving, it has involved
After something is appearing, it has appeared
After something is containing, it has contained
After something is describing, it has described
After something is including, it has
2024-07-22 16:12:55 root INFO     [order_1_approx] starting weight calculation for After something is describing, it has described
After something is containing, it has contained
After something is allowing, it has allowed
After something is expecting, it has expected
After something is involving, it has involved
After something is including, it has included
After something is appearing, it has appeared
After something is introducing, it has
2024-07-22 16:12:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 16:16:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2368,  0.4600,  1.1719,  ...,  0.4895, -0.3506, -0.0444],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5703, -0.8516,  1.3125,  ...,  4.9336,  0.2490,  5.9727],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0048, -0.0206,  0.0135,  ..., -0.0064,  0.0091, -0.0134],
        [ 0.0012,  0.0036, -0.0015,  ..., -0.0133, -0.0124,  0.0141],
        [-0.0059, -0.0001, -0.0058,  ...,  0.0002, -0.0144,  0.0027],
        ...,
        [ 0.0061,  0.0027,  0.0002,  ...,  0.0104,  0.0106,  0.0149],
        [-0.0054, -0.0062, -0.0092,  ..., -0.0366, -0.0547,  0.0046],
        [ 0.0087,  0.0124,  0.0019,  ..., -0.0095, -0.0080, -0.0169]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8745, -0.5625,  1.1719,  ...,  4.7617,  0.2013,  5.6289]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 16:16:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is describing, it has described
After something is containing, it has contained
After something is allowing, it has allowed
After something is expecting, it has expected
After something is involving, it has involved
After something is including, it has included
After something is appearing, it has appeared
After something is introducing, it has
2024-07-22 16:16:40 root INFO     [order_1_approx] starting weight calculation for After something is expecting, it has expected
After something is containing, it has contained
After something is including, it has included
After something is allowing, it has allowed
After something is appearing, it has appeared
After something is introducing, it has introduced
After something is describing, it has described
After something is involving, it has
2024-07-22 16:16:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 16:20:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.5239, 0.5615, 0.6118,  ..., 1.0303, 0.9385, 0.9023], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1260,  0.2722,  0.9443,  ...,  1.8320,  0.5215,  3.3555],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0084, -0.0309,  0.0023,  ..., -0.0085,  0.0060,  0.0015],
        [-0.0139, -0.0039, -0.0037,  ..., -0.0098, -0.0114,  0.0165],
        [-0.0170, -0.0057, -0.0007,  ..., -0.0012, -0.0066,  0.0123],
        ...,
        [-0.0268, -0.0025,  0.0066,  ...,  0.0134,  0.0102,  0.0156],
        [-0.0105,  0.0027,  0.0159,  ..., -0.0144, -0.0169,  0.0032],
        [-0.0042,  0.0153, -0.0057,  ..., -0.0028,  0.0108, -0.0437]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4456,  0.6313,  1.5605,  ...,  2.6719,  0.8965,  2.8906]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 16:20:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is expecting, it has expected
After something is containing, it has contained
After something is including, it has included
After something is allowing, it has allowed
After something is appearing, it has appeared
After something is introducing, it has introduced
After something is describing, it has described
After something is involving, it has
2024-07-22 16:20:24 root INFO     total operator prediction time: 1793.5028653144836 seconds
2024-07-22 16:20:24 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_inf - Ved
2024-07-22 16:20:24 root INFO     building operator verb_inf - Ved
2024-07-22 16:20:24 root INFO     [order_1_approx] starting weight calculation for If the present form is ensure, the past form is ensured
If the present form is become, the past form is became
If the present form is lose, the past form is lost
If the present form is consider, the past form is considered
If the present form is seem, the past form is seemed
If the present form is involve, the past form is involved
If the present form is attend, the past form is attended
If the present form is accept, the past form is
2024-07-22 16:20:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 16:24:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8301,  0.3000,  2.0977,  ...,  1.7305,  0.4553,  1.2158],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.1523, -0.7153,  3.1484,  ...,  1.1758, -0.3489,  1.8691],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0262, -0.0049,  0.0055,  ...,  0.0134, -0.0055, -0.0169],
        [-0.0119, -0.0134,  0.0036,  ...,  0.0070, -0.0025, -0.0103],
        [-0.0124, -0.0136, -0.0169,  ...,  0.0091, -0.0023, -0.0045],
        ...,
        [-0.0038,  0.0117, -0.0074,  ..., -0.0101,  0.0157, -0.0070],
        [ 0.0022,  0.0074,  0.0181,  ..., -0.0210, -0.0209,  0.0059],
        [-0.0128,  0.0036,  0.0004,  ...,  0.0015, -0.0009, -0.0208]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.5859, -0.5024,  3.1074,  ...,  1.2500, -0.5898,  2.5430]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 16:24:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is ensure, the past form is ensured
If the present form is become, the past form is became
If the present form is lose, the past form is lost
If the present form is consider, the past form is considered
If the present form is seem, the past form is seemed
If the present form is involve, the past form is involved
If the present form is attend, the past form is attended
If the present form is accept, the past form is
2024-07-22 16:24:08 root INFO     [order_1_approx] starting weight calculation for If the present form is lose, the past form is lost
If the present form is accept, the past form is accepted
If the present form is consider, the past form is considered
If the present form is involve, the past form is involved
If the present form is seem, the past form is seemed
If the present form is become, the past form is became
If the present form is ensure, the past form is ensured
If the present form is attend, the past form is
2024-07-22 16:24:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 16:27:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4697,  1.8389,  1.2881,  ...,  1.3623,  0.2090, -0.6406],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1797,  2.2031,  2.3906,  ...,  2.2402,  0.8198,  1.2842],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-4.3564e-03, -1.1005e-03,  1.6708e-03,  ...,  1.8143e-02,
         -2.0790e-03, -1.0475e-02],
        [-6.5308e-03, -1.5045e-02,  3.5343e-03,  ...,  1.0864e-02,
          2.1667e-03, -1.5900e-02],
        [ 4.8676e-03, -3.3417e-03, -1.0345e-02,  ...,  1.8799e-02,
         -3.6011e-03, -1.6647e-02],
        ...,
        [-5.8212e-03,  1.1429e-02,  9.9792e-03,  ..., -1.7944e-02,
          6.3858e-03,  4.9095e-03],
        [ 2.5940e-03,  9.9487e-03,  1.0605e-02,  ..., -1.1108e-02,
         -2.3514e-02,  5.9509e-03],
        [-1.2550e-02,  7.0038e-03,  1.9798e-03,  ..., -9.2010e-03,
          4.7684e-05, -2.6093e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7529,  2.7520,  2.7090,  ...,  1.9189,  0.3921,  1.5391]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 16:27:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is lose, the past form is lost
If the present form is accept, the past form is accepted
If the present form is consider, the past form is considered
If the present form is involve, the past form is involved
If the present form is seem, the past form is seemed
If the present form is become, the past form is became
If the present form is ensure, the past form is ensured
If the present form is attend, the past form is
2024-07-22 16:27:52 root INFO     [order_1_approx] starting weight calculation for If the present form is seem, the past form is seemed
If the present form is involve, the past form is involved
If the present form is consider, the past form is considered
If the present form is lose, the past form is lost
If the present form is attend, the past form is attended
If the present form is accept, the past form is accepted
If the present form is ensure, the past form is ensured
If the present form is become, the past form is
2024-07-22 16:27:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 16:31:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.1880, 0.8862, 1.2402,  ..., 2.0859, 0.0704, 0.1799], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6426,  0.8203, -2.0859,  ..., -1.3066,  1.6494,  0.0605],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0367, -0.0380,  0.0152,  ..., -0.0079, -0.0011, -0.0196],
        [-0.0164, -0.0094, -0.0019,  ...,  0.0220, -0.0038, -0.0210],
        [ 0.0109,  0.0144, -0.0295,  ...,  0.0159,  0.0037,  0.0172],
        ...,
        [-0.0070, -0.0026,  0.0027,  ..., -0.0414, -0.0133,  0.0105],
        [-0.0112,  0.0039,  0.0068,  ..., -0.0308, -0.0132,  0.0049],
        [-0.0211,  0.0112, -0.0078,  ..., -0.0172,  0.0104, -0.0287]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1699,  1.0312, -1.8330,  ..., -1.3516,  1.2715,  0.4692]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 16:31:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is seem, the past form is seemed
If the present form is involve, the past form is involved
If the present form is consider, the past form is considered
If the present form is lose, the past form is lost
If the present form is attend, the past form is attended
If the present form is accept, the past form is accepted
If the present form is ensure, the past form is ensured
If the present form is become, the past form is
2024-07-22 16:31:36 root INFO     [order_1_approx] starting weight calculation for If the present form is lose, the past form is lost
If the present form is attend, the past form is attended
If the present form is accept, the past form is accepted
If the present form is seem, the past form is seemed
If the present form is ensure, the past form is ensured
If the present form is involve, the past form is involved
If the present form is become, the past form is became
If the present form is consider, the past form is
2024-07-22 16:31:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 16:35:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7427,  1.2480,  1.1572,  ...,  0.7671,  0.5073, -0.1497],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3047,  0.2140, -1.7656,  ...,  0.5664, -1.5332, -2.5879],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0172, -0.0002,  0.0018,  ...,  0.0080,  0.0007, -0.0148],
        [-0.0134, -0.0078,  0.0019,  ..., -0.0061,  0.0037, -0.0060],
        [-0.0130, -0.0080, -0.0110,  ..., -0.0043, -0.0007, -0.0061],
        ...,
        [-0.0155, -0.0057,  0.0005,  ..., -0.0211,  0.0024,  0.0017],
        [-0.0020,  0.0062,  0.0014,  ..., -0.0136, -0.0190, -0.0012],
        [-0.0009,  0.0142,  0.0111,  ...,  0.0073,  0.0170, -0.0207]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3398,  0.5293, -2.1680,  ...,  0.5391, -1.1621, -2.2070]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 16:35:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is lose, the past form is lost
If the present form is attend, the past form is attended
If the present form is accept, the past form is accepted
If the present form is seem, the past form is seemed
If the present form is ensure, the past form is ensured
If the present form is involve, the past form is involved
If the present form is become, the past form is became
If the present form is consider, the past form is
2024-07-22 16:35:19 root INFO     [order_1_approx] starting weight calculation for If the present form is consider, the past form is considered
If the present form is attend, the past form is attended
If the present form is accept, the past form is accepted
If the present form is involve, the past form is involved
If the present form is become, the past form is became
If the present form is lose, the past form is lost
If the present form is seem, the past form is seemed
If the present form is ensure, the past form is
2024-07-22 16:35:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 16:39:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7598, -0.0254,  1.1514,  ...,  2.2070,  0.2622, -0.1884],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2529,  3.2109,  1.5059,  ...,  0.1102, -0.4932,  0.6777],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0144, -0.0030,  0.0116,  ...,  0.0082, -0.0079, -0.0182],
        [-0.0172, -0.0210, -0.0147,  ...,  0.0009, -0.0034, -0.0063],
        [-0.0102,  0.0050, -0.0127,  ...,  0.0269,  0.0150, -0.0080],
        ...,
        [-0.0141,  0.0093,  0.0064,  ..., -0.0114,  0.0139, -0.0097],
        [-0.0054,  0.0176,  0.0020,  ..., -0.0336, -0.0257, -0.0003],
        [-0.0169,  0.0159,  0.0037,  ..., -0.0139,  0.0129, -0.0314]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4287,  2.6367,  2.4531,  ...,  0.2178, -1.6641,  0.3425]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 16:39:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is consider, the past form is considered
If the present form is attend, the past form is attended
If the present form is accept, the past form is accepted
If the present form is involve, the past form is involved
If the present form is become, the past form is became
If the present form is lose, the past form is lost
If the present form is seem, the past form is seemed
If the present form is ensure, the past form is
2024-07-22 16:39:03 root INFO     [order_1_approx] starting weight calculation for If the present form is attend, the past form is attended
If the present form is seem, the past form is seemed
If the present form is become, the past form is became
If the present form is accept, the past form is accepted
If the present form is ensure, the past form is ensured
If the present form is consider, the past form is considered
If the present form is lose, the past form is lost
If the present form is involve, the past form is
2024-07-22 16:39:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 16:42:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5503,  0.5400,  0.4568,  ...,  1.6094, -0.0776,  0.4131],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.3535,  0.5615, -1.3809,  ...,  2.2148,  2.1445,  0.3320],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0278, -0.0095, -0.0004,  ...,  0.0021,  0.0007, -0.0109],
        [-0.0133, -0.0013,  0.0043,  ..., -0.0021,  0.0069, -0.0032],
        [ 0.0084,  0.0033, -0.0107,  ...,  0.0023, -0.0025, -0.0069],
        ...,
        [-0.0198,  0.0032,  0.0018,  ..., -0.0075,  0.0028,  0.0030],
        [-0.0126,  0.0060,  0.0060,  ..., -0.0172, -0.0361, -0.0076],
        [ 0.0009,  0.0093,  0.0049,  ..., -0.0034,  0.0179, -0.0346]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.2012,  0.7178, -1.5928,  ...,  1.8525,  2.4121,  0.2474]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 16:42:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is attend, the past form is attended
If the present form is seem, the past form is seemed
If the present form is become, the past form is became
If the present form is accept, the past form is accepted
If the present form is ensure, the past form is ensured
If the present form is consider, the past form is considered
If the present form is lose, the past form is lost
If the present form is involve, the past form is
2024-07-22 16:42:46 root INFO     [order_1_approx] starting weight calculation for If the present form is accept, the past form is accepted
If the present form is attend, the past form is attended
If the present form is consider, the past form is considered
If the present form is become, the past form is became
If the present form is involve, the past form is involved
If the present form is seem, the past form is seemed
If the present form is ensure, the past form is ensured
If the present form is lose, the past form is
2024-07-22 16:42:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 16:46:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1619, -0.4448,  1.9609,  ...,  1.5176, -0.4668,  0.4497],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.4375, -1.2344, -3.2109,  ..., -0.5850, -0.7124,  1.9414],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0189, -0.0119,  0.0030,  ...,  0.0078,  0.0010, -0.0163],
        [ 0.0014, -0.0111,  0.0054,  ..., -0.0024, -0.0006, -0.0050],
        [-0.0116, -0.0089, -0.0152,  ..., -0.0017,  0.0051,  0.0067],
        ...,
        [-0.0090,  0.0080,  0.0010,  ..., -0.0104, -0.0023,  0.0076],
        [ 0.0049, -0.0051, -0.0024,  ..., -0.0072, -0.0179,  0.0058],
        [-0.0192,  0.0005, -0.0009,  ...,  0.0055,  0.0169, -0.0196]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.8047, -1.0840, -3.1836,  ..., -0.7598, -0.8018,  2.0801]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 16:46:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is accept, the past form is accepted
If the present form is attend, the past form is attended
If the present form is consider, the past form is considered
If the present form is become, the past form is became
If the present form is involve, the past form is involved
If the present form is seem, the past form is seemed
If the present form is ensure, the past form is ensured
If the present form is lose, the past form is
2024-07-22 16:46:25 root INFO     [order_1_approx] starting weight calculation for If the present form is ensure, the past form is ensured
If the present form is become, the past form is became
If the present form is lose, the past form is lost
If the present form is attend, the past form is attended
If the present form is involve, the past form is involved
If the present form is accept, the past form is accepted
If the present form is consider, the past form is considered
If the present form is seem, the past form is
2024-07-22 16:46:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 16:50:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1919,  0.8599,  0.7251,  ...,  0.5986, -0.1685,  0.4517],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0107,  2.5039,  0.1992,  ...,  1.3350, -6.8477,  3.6836],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-9.1629e-03, -1.2772e-02,  1.1047e-02,  ..., -1.5297e-03,
         -7.6294e-06, -9.1324e-03],
        [-3.4973e-02, -1.9150e-02, -1.7681e-03,  ..., -3.6240e-03,
          3.1624e-03, -3.0251e-03],
        [-5.7449e-03,  7.3700e-03, -2.8564e-02,  ...,  1.0292e-02,
         -1.1917e-02, -1.2283e-02],
        ...,
        [-1.8097e-02, -1.3527e-02,  9.9716e-03,  ..., -2.4353e-02,
          1.5793e-03, -1.6670e-03],
        [-2.6474e-03,  1.3443e-02, -3.5400e-03,  ..., -3.0197e-02,
         -2.3529e-02,  1.2779e-02],
        [-1.7990e-02, -6.4926e-03, -5.7907e-03,  ..., -2.4902e-02,
         -1.5778e-02, -3.8147e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5728,  2.8867,  1.2900,  ...,  0.5957, -6.9492,  4.1328]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 16:50:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is ensure, the past form is ensured
If the present form is become, the past form is became
If the present form is lose, the past form is lost
If the present form is attend, the past form is attended
If the present form is involve, the past form is involved
If the present form is accept, the past form is accepted
If the present form is consider, the past form is considered
If the present form is seem, the past form is
2024-07-22 16:50:07 root INFO     total operator prediction time: 1783.2010960578918 seconds
2024-07-22 16:50:07 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_inf - 3pSg
2024-07-22 16:50:07 root INFO     building operator verb_inf - 3pSg
2024-07-22 16:50:08 root INFO     [order_1_approx] starting weight calculation for I maintain, he maintains
I ensure, he ensures
I allow, he allows
I hear, he hears
I develop, he develops
I believe, he believes
I understand, he understands
I add, he
2024-07-22 16:50:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 16:53:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3420, -0.4451,  1.6865,  ..., -0.0636,  0.9536,  1.1143],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6777, -1.4961,  3.2422,  ..., -1.6768, -1.4893,  3.2852],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0089, -0.0068,  0.0166,  ...,  0.0083, -0.0244, -0.0227],
        [-0.0090, -0.0093,  0.0108,  ...,  0.0065, -0.0118, -0.0009],
        [-0.0127, -0.0134, -0.0312,  ...,  0.0050, -0.0079, -0.0071],
        ...,
        [-0.0100,  0.0022,  0.0082,  ...,  0.0010, -0.0002, -0.0032],
        [-0.0082,  0.0058,  0.0032,  ..., -0.0153, -0.0333,  0.0079],
        [-0.0086,  0.0269, -0.0037,  ..., -0.0156, -0.0091, -0.0338]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7319, -0.6431,  2.8281,  ..., -0.7710, -1.2451,  3.0273]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 16:53:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I maintain, he maintains
I ensure, he ensures
I allow, he allows
I hear, he hears
I develop, he develops
I believe, he believes
I understand, he understands
I add, he
2024-07-22 16:53:50 root INFO     [order_1_approx] starting weight calculation for I understand, he understands
I ensure, he ensures
I develop, he develops
I maintain, he maintains
I believe, he believes
I add, he adds
I hear, he hears
I allow, he
2024-07-22 16:53:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 16:57:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1316, -0.9668,  2.2539,  ..., -0.3169,  0.5791,  0.8511],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6235, -3.7754, -0.7471,  ...,  1.2344, -1.9307,  2.7539],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-5.3406e-03, -6.5956e-03, -3.8147e-06,  ...,  1.5091e-02,
          6.9733e-03, -3.4424e-02],
        [-5.2643e-04, -1.1253e-02,  4.0436e-04,  ..., -6.1417e-03,
         -1.5823e-02, -1.8387e-03],
        [-1.1406e-02,  4.0817e-04, -2.5497e-02,  ...,  5.2338e-03,
         -2.1191e-03, -3.2253e-03],
        ...,
        [-2.9663e-02, -1.4328e-02, -1.0529e-03,  ..., -1.0597e-02,
         -3.2787e-03, -1.0262e-03],
        [ 5.7907e-03, -1.4709e-02,  1.9577e-02,  ..., -1.6861e-02,
         -2.3422e-02,  2.7657e-03],
        [ 2.6703e-03,  1.4946e-02, -1.6830e-02,  ..., -1.3321e-02,
         -1.5045e-02, -2.8412e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3262, -3.4863, -0.2461,  ...,  1.1299, -0.3564,  2.2051]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 16:57:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I understand, he understands
I ensure, he ensures
I develop, he develops
I maintain, he maintains
I believe, he believes
I add, he adds
I hear, he hears
I allow, he
2024-07-22 16:57:35 root INFO     [order_1_approx] starting weight calculation for I ensure, he ensures
I hear, he hears
I understand, he understands
I add, he adds
I maintain, he maintains
I develop, he develops
I allow, he allows
I believe, he
2024-07-22 16:57:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 17:01:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1472, -0.9170,  0.2917,  ...,  0.0168,  1.6973,  0.2583],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7197, -1.2705, -0.6899,  ...,  0.9893, -3.6113,  3.2168],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0039, -0.0370,  0.0072,  ...,  0.0053,  0.0092, -0.0135],
        [-0.0251,  0.0028,  0.0246,  ..., -0.0042,  0.0122,  0.0051],
        [ 0.0055,  0.0057, -0.0350,  ...,  0.0109,  0.0074,  0.0013],
        ...,
        [ 0.0007, -0.0104, -0.0053,  ..., -0.0314,  0.0069,  0.0082],
        [-0.0065,  0.0104,  0.0221,  ..., -0.0192, -0.0299, -0.0042],
        [ 0.0040,  0.0194, -0.0083,  ..., -0.0047, -0.0112, -0.0249]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9316, -0.7969, -0.9160,  ...,  0.1421, -3.1699,  2.4902]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 17:01:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I ensure, he ensures
I hear, he hears
I understand, he understands
I add, he adds
I maintain, he maintains
I develop, he develops
I allow, he allows
I believe, he
2024-07-22 17:01:19 root INFO     [order_1_approx] starting weight calculation for I add, he adds
I hear, he hears
I maintain, he maintains
I believe, he believes
I understand, he understands
I ensure, he ensures
I allow, he allows
I develop, he
2024-07-22 17:01:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 17:05:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7812, -1.1270,  1.1719,  ..., -0.2649,  0.1379,  0.0264],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8413, -4.8320, -0.6953,  ..., -0.1890,  1.5254,  3.1016],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0251, -0.0071,  0.0189,  ..., -0.0124, -0.0069, -0.0122],
        [-0.0060, -0.0097,  0.0061,  ...,  0.0048, -0.0065,  0.0059],
        [-0.0180,  0.0094, -0.0312,  ..., -0.0138,  0.0051, -0.0050],
        ...,
        [ 0.0096,  0.0026, -0.0303,  ..., -0.0138, -0.0159, -0.0095],
        [-0.0056,  0.0141,  0.0125,  ..., -0.0038, -0.0263,  0.0082],
        [ 0.0074,  0.0256,  0.0078,  ..., -0.0033,  0.0072, -0.0063]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9189e-01, -4.3867e+00,  2.4414e-03,  ..., -8.0176e-01,
          2.0078e+00,  2.4824e+00]], device='cuda:1', dtype=torch.float16,
       grad_fn=<SubBackward0>) 

                    
2024-07-22 17:05:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I add, he adds
I hear, he hears
I maintain, he maintains
I believe, he believes
I understand, he understands
I ensure, he ensures
I allow, he allows
I develop, he
2024-07-22 17:05:02 root INFO     [order_1_approx] starting weight calculation for I develop, he develops
I add, he adds
I believe, he believes
I hear, he hears
I understand, he understands
I maintain, he maintains
I allow, he allows
I ensure, he
2024-07-22 17:05:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 17:08:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2031, -0.7461,  1.1758,  ...,  0.3291,  0.1340,  0.5420],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5801,  0.8701,  2.5508,  ...,  0.5815,  0.0151,  4.3320],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0137, -0.0069,  0.0071,  ..., -0.0048, -0.0020, -0.0280],
        [-0.0057, -0.0085, -0.0008,  ...,  0.0035,  0.0108,  0.0037],
        [-0.0158, -0.0001, -0.0265,  ...,  0.0138,  0.0184, -0.0111],
        ...,
        [-0.0133, -0.0203, -0.0204,  ..., -0.0155, -0.0026, -0.0111],
        [-0.0113,  0.0071,  0.0135,  ..., -0.0244, -0.0507,  0.0154],
        [ 0.0080,  0.0325,  0.0031,  ..., -0.0123, -0.0007, -0.0209]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5342,  0.4309,  2.1328,  ..., -0.8325,  0.5991,  3.8711]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 17:08:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I develop, he develops
I add, he adds
I believe, he believes
I hear, he hears
I understand, he understands
I maintain, he maintains
I allow, he allows
I ensure, he
2024-07-22 17:08:47 root INFO     [order_1_approx] starting weight calculation for I add, he adds
I develop, he develops
I believe, he believes
I ensure, he ensures
I understand, he understands
I allow, he allows
I maintain, he maintains
I hear, he
2024-07-22 17:08:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 17:12:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8540, -0.2288,  0.8696,  ..., -0.1252,  1.5039,  1.0254],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9575,  0.8506, -0.1572,  ..., -0.3733,  1.0117,  6.6016],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-9.5978e-03, -5.3329e-03, -6.9847e-03,  ...,  1.3489e-02,
         -1.0506e-02, -2.2751e-02],
        [ 3.7670e-03, -1.6541e-02,  1.2146e-02,  ..., -1.2375e-02,
          5.0316e-03,  1.6785e-02],
        [-2.0828e-02,  7.0496e-03, -1.4900e-02,  ...,  1.4374e-02,
          1.9073e-02,  4.8714e-03],
        ...,
        [-1.1841e-02, -1.0284e-02, -8.5831e-03,  ..., -1.8417e-02,
         -1.3685e-03, -2.3102e-02],
        [-8.4496e-04, -3.1204e-03,  9.5367e-03,  ..., -2.3132e-02,
         -2.4872e-02, -2.4681e-03],
        [ 1.8890e-02,  6.9618e-05, -1.6052e-02,  ..., -4.3602e-03,
          1.1124e-02, -1.2985e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8086,  1.0918, -0.3735,  ..., -1.2822,  1.4775,  7.0430]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 17:12:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I add, he adds
I develop, he develops
I believe, he believes
I ensure, he ensures
I understand, he understands
I allow, he allows
I maintain, he maintains
I hear, he
2024-07-22 17:12:29 root INFO     [order_1_approx] starting weight calculation for I understand, he understands
I add, he adds
I develop, he develops
I hear, he hears
I allow, he allows
I ensure, he ensures
I believe, he believes
I maintain, he
2024-07-22 17:12:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 17:16:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0649, -0.3711,  2.4082,  ..., -0.2788,  0.3740,  1.3662],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0469, -4.1133, -5.0195,  ...,  1.5342,  0.9863,  6.2383],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 8.4534e-03, -2.4185e-03, -1.0090e-03,  ...,  9.2316e-03,
          6.8512e-03, -7.9117e-03],
        [-4.3793e-03,  5.0240e-03,  1.1124e-02,  ...,  1.4362e-03,
          1.2672e-02, -4.3449e-03],
        [-1.4870e-02,  1.0689e-02, -2.1191e-03,  ...,  1.1185e-02,
          6.8207e-03, -5.1193e-03],
        ...,
        [-1.0742e-02, -1.2848e-02, -5.1956e-03,  ..., -1.7715e-02,
         -1.4824e-02,  6.5918e-03],
        [-7.6370e-03,  1.3412e-02,  1.7792e-02,  ..., -1.7242e-02,
         -1.9211e-02,  2.7420e-02],
        [-2.0905e-03,  1.0300e-02, -2.2087e-03,  ...,  6.4850e-05,
          6.3477e-03, -1.3794e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6992, -3.2949, -4.0586,  ...,  1.5225,  1.6133,  5.8711]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 17:16:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I understand, he understands
I add, he adds
I develop, he develops
I hear, he hears
I allow, he allows
I ensure, he ensures
I believe, he believes
I maintain, he
2024-07-22 17:16:15 root INFO     [order_1_approx] starting weight calculation for I hear, he hears
I ensure, he ensures
I add, he adds
I believe, he believes
I maintain, he maintains
I allow, he allows
I develop, he develops
I understand, he
2024-07-22 17:16:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 17:20:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9810, -1.4336,  0.7705,  ..., -0.5581,  1.0713,  0.2471],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4170, -2.2754,  0.7627,  ...,  0.7256, -0.1064,  3.3398],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0143, -0.0131,  0.0179,  ..., -0.0027, -0.0122, -0.0254],
        [-0.0184, -0.0178,  0.0216,  ...,  0.0068,  0.0013, -0.0222],
        [-0.0121,  0.0181, -0.0353,  ...,  0.0049,  0.0224,  0.0038],
        ...,
        [ 0.0087, -0.0063, -0.0012,  ..., -0.0195, -0.0226, -0.0108],
        [ 0.0045, -0.0126,  0.0173,  ..., -0.0188, -0.0424, -0.0030],
        [ 0.0125,  0.0334, -0.0228,  ..., -0.0049,  0.0267, -0.0349]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9531, -2.3145,  1.4141,  ...,  0.3081,  0.8887,  3.2598]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 17:20:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I hear, he hears
I ensure, he ensures
I add, he adds
I believe, he believes
I maintain, he maintains
I allow, he allows
I develop, he develops
I understand, he
2024-07-22 17:20:01 root INFO     total operator prediction time: 1793.08411860466 seconds
2024-07-22 17:20:01 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_Ving - 3pSg
2024-07-22 17:20:01 root INFO     building operator verb_Ving - 3pSg
2024-07-22 17:20:01 root INFO     [order_1_approx] starting weight calculation for When something is learning, it learns
When something is enabling, it enables
When something is allowing, it allows
When something is developing, it develops
When something is occurring, it occurs
When something is suggesting, it suggests
When something is understanding, it understands
When something is adding, it
2024-07-22 17:20:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 17:23:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3218, -0.3074,  0.4392,  ...,  0.2915,  0.2812, -0.0969],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0093,  0.6162,  4.4922,  ..., -0.7104, -2.0859,  2.9180],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0019, -0.0071,  0.0087,  ...,  0.0193, -0.0041, -0.0138],
        [-0.0016, -0.0047, -0.0045,  ..., -0.0122, -0.0019,  0.0058],
        [ 0.0063,  0.0017, -0.0174,  ...,  0.0077, -0.0012,  0.0013],
        ...,
        [-0.0075, -0.0137, -0.0093,  ...,  0.0109, -0.0023,  0.0038],
        [ 0.0060,  0.0058,  0.0126,  ..., -0.0107, -0.0223, -0.0105],
        [-0.0098,  0.0117, -0.0022,  ..., -0.0097, -0.0054, -0.0031]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3081,  0.8359,  4.0391,  ..., -0.7441, -2.5820,  2.0000]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 17:23:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is learning, it learns
When something is enabling, it enables
When something is allowing, it allows
When something is developing, it develops
When something is occurring, it occurs
When something is suggesting, it suggests
When something is understanding, it understands
When something is adding, it
2024-07-22 17:23:38 root INFO     [order_1_approx] starting weight calculation for When something is learning, it learns
When something is understanding, it understands
When something is adding, it adds
When something is enabling, it enables
When something is developing, it develops
When something is occurring, it occurs
When something is suggesting, it suggests
When something is allowing, it
2024-07-22 17:23:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 17:27:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5391, -0.5889,  0.2197,  ...,  0.8003,  0.7266, -0.4714],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2441, -1.5439,  1.0137,  ...,  0.0181, -0.8838,  2.9609],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-4.8103e-03, -2.6031e-02,  1.0162e-02,  ...,  1.5450e-02,
          8.7433e-03, -2.7008e-03],
        [ 1.2451e-02, -4.4136e-03,  1.5755e-03,  ..., -1.2421e-02,
         -1.3714e-03, -2.5921e-03],
        [-9.0942e-03,  1.1246e-02, -1.6998e-02,  ...,  6.9656e-03,
          2.5444e-03,  4.7340e-03],
        ...,
        [-1.2375e-02, -1.8921e-02, -2.0657e-03,  ..., -6.2866e-03,
         -4.0131e-03, -3.7117e-03],
        [-1.1215e-03, -3.0785e-03,  1.2482e-02,  ..., -1.1063e-02,
         -1.0239e-02, -9.2316e-03],
        [ 5.9242e-03, -5.9586e-03,  2.5253e-03,  ..., -3.2425e-05,
         -1.5251e-02,  2.1362e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2578, -0.7900,  1.5107,  ..., -0.4648, -0.1914,  2.5781]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 17:27:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is learning, it learns
When something is understanding, it understands
When something is adding, it adds
When something is enabling, it enables
When something is developing, it develops
When something is occurring, it occurs
When something is suggesting, it suggests
When something is allowing, it
2024-07-22 17:27:21 root INFO     [order_1_approx] starting weight calculation for When something is understanding, it understands
When something is enabling, it enables
When something is adding, it adds
When something is suggesting, it suggests
When something is allowing, it allows
When something is occurring, it occurs
When something is learning, it learns
When something is developing, it
2024-07-22 17:27:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 17:31:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4873, -1.1748,  1.0303,  ...,  0.2646, -0.1846, -0.2932],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0967, -2.3145,  1.6289,  ...,  0.0298,  2.0703, -0.2100],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 5.3940e-03, -1.3504e-02,  1.0178e-02,  ...,  1.3031e-02,
          5.9814e-03, -1.4709e-02],
        [-8.5068e-03, -4.3564e-03, -1.0109e-02,  ..., -4.4289e-03,
         -8.6517e-03,  5.2528e-03],
        [-3.4332e-05,  2.3632e-03, -2.1763e-03,  ..., -3.7804e-03,
          5.1346e-03,  3.2024e-03],
        ...,
        [-1.0170e-02,  4.5013e-03, -6.2599e-03,  ..., -1.8082e-03,
         -2.4231e-02, -7.6942e-03],
        [ 2.4815e-03, -1.2169e-03,  5.1956e-03,  ..., -1.4870e-02,
         -1.0773e-02,  6.0501e-03],
        [-3.7022e-03,  1.7700e-03,  8.4229e-03,  ..., -1.0422e-02,
          8.1482e-03, -8.9951e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4688, -1.3389,  1.2295,  ...,  0.2394,  2.2305, -0.8789]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 17:31:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is understanding, it understands
When something is enabling, it enables
When something is adding, it adds
When something is suggesting, it suggests
When something is allowing, it allows
When something is occurring, it occurs
When something is learning, it learns
When something is developing, it
2024-07-22 17:31:05 root INFO     [order_1_approx] starting weight calculation for When something is learning, it learns
When something is allowing, it allows
When something is developing, it develops
When something is occurring, it occurs
When something is suggesting, it suggests
When something is understanding, it understands
When something is adding, it adds
When something is enabling, it
2024-07-22 17:31:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 17:34:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6504,  0.0093,  0.3843,  ...,  0.3784,  0.8398, -0.4607],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7788,  0.1572,  2.2988,  ...,  0.3970, -2.7227,  5.2266],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0060, -0.0160,  0.0010,  ...,  0.0017,  0.0098, -0.0245],
        [ 0.0053,  0.0005, -0.0161,  ..., -0.0126, -0.0072, -0.0039],
        [ 0.0067,  0.0074, -0.0059,  ...,  0.0103,  0.0054,  0.0007],
        ...,
        [-0.0102, -0.0139, -0.0132,  ..., -0.0088, -0.0022, -0.0146],
        [ 0.0075, -0.0172,  0.0095,  ..., -0.0196, -0.0039, -0.0038],
        [-0.0019,  0.0083, -0.0091,  ..., -0.0035, -0.0019, -0.0008]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2700,  0.8423,  2.3555,  ...,  0.0828, -2.3398,  4.7656]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 17:34:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is learning, it learns
When something is allowing, it allows
When something is developing, it develops
When something is occurring, it occurs
When something is suggesting, it suggests
When something is understanding, it understands
When something is adding, it adds
When something is enabling, it
2024-07-22 17:34:49 root INFO     [order_1_approx] starting weight calculation for When something is occurring, it occurs
When something is suggesting, it suggests
When something is developing, it develops
When something is enabling, it enables
When something is adding, it adds
When something is understanding, it understands
When something is allowing, it allows
When something is learning, it
2024-07-22 17:34:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 17:38:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9839,  0.0480,  1.1973,  ..., -0.4883,  1.3516,  0.0708],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.3594,  3.2500,  0.9824,  ..., -0.2727, -1.6094,  2.5566],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0243, -0.0071,  0.0109,  ...,  0.0075, -0.0005, -0.0178],
        [-0.0078, -0.0043, -0.0138,  ..., -0.0051, -0.0019, -0.0026],
        [-0.0139, -0.0001, -0.0196,  ...,  0.0026,  0.0041, -0.0108],
        ...,
        [-0.0062,  0.0060, -0.0094,  ..., -0.0194, -0.0045, -0.0113],
        [ 0.0029, -0.0055,  0.0111,  ..., -0.0017, -0.0286,  0.0044],
        [-0.0154,  0.0055, -0.0062,  ..., -0.0086,  0.0117, -0.0336]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.4883,  3.5527,  0.5840,  ...,  0.0745, -2.0371,  2.5488]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 17:38:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is occurring, it occurs
When something is suggesting, it suggests
When something is developing, it develops
When something is enabling, it enables
When something is adding, it adds
When something is understanding, it understands
When something is allowing, it allows
When something is learning, it
2024-07-22 17:38:34 root INFO     [order_1_approx] starting weight calculation for When something is allowing, it allows
When something is enabling, it enables
When something is learning, it learns
When something is adding, it adds
When something is developing, it develops
When something is understanding, it understands
When something is suggesting, it suggests
When something is occurring, it
2024-07-22 17:38:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-22 17:42:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1396,  0.0841,  1.5498,  ...,  0.7715,  1.9170, -0.6494],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3750, -1.1797, -0.8804,  ..., -2.0820,  2.2676,  2.2773],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0113, -0.0079, -0.0030,  ...,  0.0190,  0.0156, -0.0013],
        [-0.0004, -0.0128, -0.0032,  ..., -0.0132, -0.0129,  0.0068],
        [-0.0236, -0.0002, -0.0455,  ..., -0.0035,  0.0060,  0.0211],
        ...,
        [-0.0210, -0.0180, -0.0055,  ..., -0.0463, -0.0135, -0.0224],
        [ 0.0008,  0.0224,  0.0070,  ..., -0.0079, -0.0294,  0.0055],
        [-0.0378,  0.0201,  0.0122,  ..., -0.0117,  0.0080, -0.0156]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7007, -0.7637, -1.5254,  ..., -2.6641,  1.7051,  2.3555]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 17:42:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is allowing, it allows
When something is enabling, it enables
When something is learning, it learns
When something is adding, it adds
When something is developing, it develops
When something is understanding, it understands
When something is suggesting, it suggests
When something is occurring, it
2024-07-22 17:42:18 root INFO     [order_1_approx] starting weight calculation for When something is understanding, it understands
When something is learning, it learns
When something is enabling, it enables
When something is adding, it adds
When something is developing, it develops
When something is allowing, it allows
When something is occurring, it occurs
When something is suggesting, it
2024-07-22 17:42:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
