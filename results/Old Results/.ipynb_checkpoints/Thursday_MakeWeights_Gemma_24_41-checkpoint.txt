2024-07-25 10:17:15 root INFO     loading model + tokenizer
2024-07-25 10:17:18 accelerate.big_modeling WARNING  Some parameters are on the meta device device because they were offloaded to the cpu.
2024-07-25 10:17:18 root INFO     model + tokenizer loaded
2024-07-25 10:17:18 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on meronyms - part
2024-07-25 10:17:18 root INFO     building operator meronyms - part
2024-07-25 10:17:43 root INFO     loading model + tokenizer
2024-07-25 10:17:51 root INFO     model + tokenizer loaded
2024-07-25 10:17:51 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on meronyms - part
2024-07-25 10:17:51 root INFO     building operator meronyms - part
2024-07-25 10:19:39 root INFO     loading model + tokenizer
2024-07-25 10:19:45 root INFO     model + tokenizer loaded
2024-07-25 10:19:45 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on meronyms - part
2024-07-25 10:19:45 root INFO     building operator meronyms - part
2024-07-25 10:19:46 root INFO     [order_1_approx] starting weight calculation for A part of a brush is a bristle
A part of a window is a pane
A part of a sword is a blade
A part of a apartment is a bedroom
A part of a gun is a trigger
A part of a shilling is a pence
A part of a gigabit is a megabit
A part of a gramm is a
2024-07-25 10:19:46 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 10:22:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0586,  0.5820,  2.9219,  ...,  0.0771, -1.9297, -1.7578],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([17.8750, -3.9375, -6.5625,  ...,  8.0625,  2.8438, -1.5938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.6855e-02,  4.7913e-03,  2.5024e-03,  ..., -9.5825e-03,
          2.7710e-02,  1.1230e-02],
        [ 4.6692e-03,  3.8605e-03, -2.8839e-03,  ..., -1.2207e-03,
         -9.2773e-03,  1.7548e-04],
        [ 3.1738e-03,  8.3008e-03,  1.5625e-01,  ..., -1.1719e-02,
         -2.3193e-03, -1.9897e-02],
        ...,
        [-7.6599e-03,  5.3711e-03, -2.1973e-02,  ...,  2.4658e-02,
          1.2695e-02,  2.5177e-04],
        [-7.0801e-03, -6.0730e-03, -1.1230e-02,  ..., -3.0823e-03,
          7.8613e-02,  9.6436e-03],
        [ 1.4343e-03, -1.2207e-04, -1.6174e-03,  ..., -2.6550e-03,
         -1.5015e-02,  3.2227e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[16.6719, -4.3359, -9.0469,  ...,  8.5547,  2.5098, -2.3574]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 10:22:34 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A part of a brush is a bristle
A part of a window is a pane
A part of a sword is a blade
A part of a apartment is a bedroom
A part of a gun is a trigger
A part of a shilling is a pence
A part of a gigabit is a megabit
A part of a gramm is a
2024-07-25 10:22:35 root INFO     [order_1_approx] starting weight calculation for A part of a gramm is a milligram
A part of a sword is a blade
A part of a apartment is a bedroom
A part of a window is a pane
A part of a brush is a bristle
A part of a shilling is a pence
A part of a gigabit is a megabit
A part of a gun is a
2024-07-25 10:22:35 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 10:25:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2500,  2.9062, -1.3516,  ..., -0.5039, -2.0938, -3.4688],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 7.6250,  4.1250,  6.8125,  ..., -6.0625,  6.5312, -3.2188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.8931e-02,  1.0681e-03,  2.2461e-02,  ..., -3.8147e-03,
          9.1553e-03,  8.4305e-04],
        [-9.1553e-05,  1.6235e-02, -9.4604e-03,  ..., -2.5177e-04,
         -4.6387e-03, -5.1117e-04],
        [ 4.5776e-04, -4.9438e-03,  1.3477e-01,  ..., -6.8970e-03,
          7.9346e-03,  4.1809e-03],
        ...,
        [-1.5869e-03,  2.2583e-03, -1.9226e-03,  ...,  2.0752e-02,
          3.5400e-03,  4.5776e-03],
        [-9.9182e-04,  1.1139e-03, -5.2185e-03,  ..., -4.4556e-03,
          4.0039e-02,  1.4954e-03],
        [ 1.3550e-02,  6.3324e-04,  9.7046e-03,  ...,  3.9673e-04,
          6.1798e-04,  3.9062e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.0391,  3.5977,  6.2969,  ..., -3.5801,  7.0859, -4.0938]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 10:25:21 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A part of a gramm is a milligram
A part of a sword is a blade
A part of a apartment is a bedroom
A part of a window is a pane
A part of a brush is a bristle
A part of a shilling is a pence
A part of a gigabit is a megabit
A part of a gun is a
2024-07-25 10:25:22 root INFO     [order_1_approx] starting weight calculation for A part of a window is a pane
A part of a brush is a bristle
A part of a gun is a trigger
A part of a apartment is a bedroom
A part of a shilling is a pence
A part of a gigabit is a megabit
A part of a gramm is a milligram
A part of a sword is a
2024-07-25 10:25:22 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 10:28:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4199,  2.0312, -4.2188,  ..., -0.5898, -2.5938, -3.1562],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 1.6719,  9.3750,  3.9375,  ..., 13.2500, -0.6875, -4.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0270,  0.0015,  0.0315,  ..., -0.0014,  0.0099, -0.0055],
        [ 0.0066,  0.0148, -0.0065,  ..., -0.0002, -0.0032,  0.0039],
        [-0.0084, -0.0009,  0.1309,  ..., -0.0135,  0.0004, -0.0027],
        ...,
        [-0.0014,  0.0029, -0.0168,  ...,  0.0281, -0.0005,  0.0099],
        [-0.0082,  0.0071, -0.0009,  ..., -0.0003,  0.0364,  0.0022],
        [ 0.0046, -0.0034,  0.0066,  ...,  0.0024, -0.0063,  0.0527]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7739,  8.9688,  4.9609,  ..., 13.1328,  0.8945, -4.5156]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 10:28:08 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A part of a window is a pane
A part of a brush is a bristle
A part of a gun is a trigger
A part of a apartment is a bedroom
A part of a shilling is a pence
A part of a gigabit is a megabit
A part of a gramm is a milligram
A part of a sword is a
2024-07-25 10:28:09 root INFO     [order_1_approx] starting weight calculation for A part of a gramm is a milligram
A part of a apartment is a bedroom
A part of a gun is a trigger
A part of a window is a pane
A part of a gigabit is a megabit
A part of a brush is a bristle
A part of a sword is a blade
A part of a shilling is a
2024-07-25 10:28:09 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 10:30:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3867,  0.8125, -3.1250,  ...,  0.9883,  0.3945, -0.8438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([15.0625,  2.4844,  4.9688,  ..., -7.6875, -6.0625, -7.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.8320e-02, -1.1597e-03,  3.7354e-02,  ..., -1.4038e-02,
          9.7656e-03, -4.8218e-03],
        [ 7.3242e-03, -2.5635e-03, -1.2939e-02,  ...,  6.9580e-03,
         -1.9775e-02, -5.0049e-03],
        [ 1.2024e-02, -3.5400e-03,  1.0791e-01,  ..., -1.5259e-03,
         -1.5640e-03,  5.6763e-03],
        ...,
        [-4.8523e-03, -5.7220e-06,  5.6152e-03,  ...,  1.5259e-02,
         -2.3346e-03,  7.5073e-03],
        [ 5.6763e-03,  3.8605e-03, -2.7832e-02,  ..., -9.0332e-03,
          3.8574e-02,  9.6436e-03],
        [-3.8757e-03,  7.8201e-04, -1.7090e-02,  ...,  1.0010e-02,
          2.6245e-03,  3.6133e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[14.3828,  2.6992,  3.0957,  ..., -7.8945, -5.9922, -8.3359]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 10:30:56 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A part of a gramm is a milligram
A part of a apartment is a bedroom
A part of a gun is a trigger
A part of a window is a pane
A part of a gigabit is a megabit
A part of a brush is a bristle
A part of a sword is a blade
A part of a shilling is a
2024-07-25 10:30:56 root INFO     [order_1_approx] starting weight calculation for A part of a sword is a blade
A part of a gun is a trigger
A part of a shilling is a pence
A part of a gramm is a milligram
A part of a gigabit is a megabit
A part of a brush is a bristle
A part of a apartment is a bedroom
A part of a window is a
2024-07-25 10:30:56 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 10:33:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3789,  0.6211,  0.9453,  ..., -2.5312, -2.9844, -2.8281],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 3.0469, -7.3750, -7.0625,  ...,  8.5000,  2.7969,  3.9688],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.4180e-02,  3.8147e-05,  1.1353e-02,  ...,  1.2970e-03,
         -1.6785e-03, -4.4861e-03],
        [-3.4027e-03,  2.0874e-02, -1.1902e-02,  ..., -6.0730e-03,
          6.0425e-03, -2.3193e-03],
        [-5.9204e-03, -1.7166e-03,  1.2598e-01,  ..., -1.9379e-03,
          5.7068e-03,  4.2725e-03],
        ...,
        [ 8.0566e-03,  6.7749e-03, -5.9814e-03,  ...,  2.2705e-02,
         -3.4027e-03, -2.1820e-03],
        [ 7.3242e-03,  7.5684e-03, -2.1057e-03,  ..., -2.9755e-03,
          4.4922e-02,  2.9144e-03],
        [ 2.7771e-03, -7.2021e-03, -2.5391e-02,  ...,  5.7983e-03,
          2.2316e-04,  4.2969e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7891, -7.1484, -8.5469,  ...,  9.0391,  2.9785,  4.1719]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 10:33:43 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A part of a sword is a blade
A part of a gun is a trigger
A part of a shilling is a pence
A part of a gramm is a milligram
A part of a gigabit is a megabit
A part of a brush is a bristle
A part of a apartment is a bedroom
A part of a window is a
2024-07-25 10:33:43 root INFO     [order_1_approx] starting weight calculation for A part of a gramm is a milligram
A part of a brush is a bristle
A part of a shilling is a pence
A part of a gun is a trigger
A part of a sword is a blade
A part of a window is a pane
A part of a gigabit is a megabit
A part of a apartment is a
2024-07-25 10:33:43 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 10:36:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8203,  3.3750, -3.2812,  ..., -0.2148, -0.7305, -3.8906],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([18.1250,  7.2500,  0.3125,  ..., -8.6250, -4.0000, -7.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0466, -0.0070,  0.0239,  ..., -0.0026, -0.0072, -0.0072],
        [-0.0051,  0.0208, -0.0244,  ...,  0.0022,  0.0028,  0.0057],
        [ 0.0013,  0.0030,  0.1279,  ..., -0.0070, -0.0076, -0.0033],
        ...,
        [ 0.0023,  0.0035, -0.0256,  ...,  0.0283,  0.0006, -0.0021],
        [-0.0024,  0.0036,  0.0160,  ..., -0.0047,  0.0393, -0.0002],
        [ 0.0042, -0.0013, -0.0293,  ...,  0.0020,  0.0002,  0.0386]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[18.6406,  6.3516,  1.9434,  ..., -6.6406, -3.5918, -5.8711]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 10:36:29 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A part of a gramm is a milligram
A part of a brush is a bristle
A part of a shilling is a pence
A part of a gun is a trigger
A part of a sword is a blade
A part of a window is a pane
A part of a gigabit is a megabit
A part of a apartment is a
2024-07-25 10:36:29 root INFO     [order_1_approx] starting weight calculation for A part of a window is a pane
A part of a sword is a blade
A part of a gun is a trigger
A part of a shilling is a pence
A part of a gramm is a milligram
A part of a apartment is a bedroom
A part of a brush is a bristle
A part of a gigabit is a
2024-07-25 10:36:29 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 10:39:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5039, -1.5156, -4.8438,  ...,  1.7422,  0.0195, -1.3359],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([25.6250,  1.6562, -6.1875,  ..., 21.1250, 14.5000, -5.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0231,  0.0066, -0.0085,  ...,  0.0002,  0.0007,  0.0160],
        [-0.0013,  0.0161,  0.0011,  ..., -0.0015, -0.0115, -0.0006],
        [ 0.0026,  0.0063,  0.1040,  ...,  0.0028,  0.0009,  0.0070],
        ...,
        [-0.0027,  0.0045, -0.0072,  ...,  0.0310,  0.0022,  0.0181],
        [-0.0026,  0.0056,  0.0187,  ...,  0.0068,  0.0562,  0.0098],
        [ 0.0087,  0.0012, -0.0162,  ...,  0.0002,  0.0026,  0.0449]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[25.8281,  1.0645, -3.8535,  ..., 20.6875, 15.7656, -5.5117]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 10:39:17 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A part of a window is a pane
A part of a sword is a blade
A part of a gun is a trigger
A part of a shilling is a pence
A part of a gramm is a milligram
A part of a apartment is a bedroom
A part of a brush is a bristle
A part of a gigabit is a
2024-07-25 10:39:17 root INFO     [order_1_approx] starting weight calculation for A part of a window is a pane
A part of a gramm is a milligram
A part of a sword is a blade
A part of a gigabit is a megabit
A part of a shilling is a pence
A part of a gun is a trigger
A part of a apartment is a bedroom
A part of a brush is a
2024-07-25 10:39:18 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 10:42:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2109,  0.6602,  1.5000,  ..., -1.5625, -3.1875, -3.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([25.0000, -3.5469, 15.8125,  ...,  5.5938,  0.3906, -6.0312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0454,  0.0036,  0.0430,  ...,  0.0017,  0.0011, -0.0021],
        [ 0.0082,  0.0287,  0.0131,  ..., -0.0024, -0.0016,  0.0016],
        [ 0.0048, -0.0055,  0.1621,  ...,  0.0011, -0.0083,  0.0007],
        ...,
        [ 0.0006,  0.0076, -0.0040,  ...,  0.0332,  0.0057, -0.0011],
        [-0.0054,  0.0042, -0.0093,  ..., -0.0012,  0.0420, -0.0021],
        [ 0.0028, -0.0018,  0.0049,  ..., -0.0019,  0.0019,  0.0557]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[25.2188, -2.0156, 14.2969,  ...,  5.5117,  1.2266, -5.9922]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 10:42:04 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A part of a window is a pane
A part of a gramm is a milligram
A part of a sword is a blade
A part of a gigabit is a megabit
A part of a shilling is a pence
A part of a gun is a trigger
A part of a apartment is a bedroom
A part of a brush is a
2024-07-25 10:42:04 root INFO     total operator prediction time: 1338.8528497219086 seconds
2024-07-25 10:42:04 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on synonyms - exact
2024-07-25 10:42:04 root INFO     building operator synonyms - exact
2024-07-25 10:42:04 root INFO     [order_1_approx] starting weight calculation for Another word for baby is infant
Another word for sofa is couch
Another word for mother is mom
Another word for portion is part
Another word for list is listing
Another word for hieroglyph is hieroglyphic
Another word for rock is stone
Another word for auto is
2024-07-25 10:42:05 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 10:44:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2188, -0.3359,  1.1406,  ..., -1.3438, -0.4766, -0.1348],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 0.0000, -4.0938, 10.0000,  ..., -2.7969,  2.0000, -3.9219],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0417,  0.0153,  0.0181,  ..., -0.0059,  0.0040,  0.0003],
        [ 0.0116,  0.0200,  0.0022,  ...,  0.0037, -0.0067,  0.0014],
        [-0.0036, -0.0038,  0.1572,  ..., -0.0074, -0.0015,  0.0021],
        ...,
        [-0.0016,  0.0018, -0.0082,  ...,  0.0303, -0.0098,  0.0067],
        [ 0.0058, -0.0037, -0.0037,  ...,  0.0022,  0.0571, -0.0045],
        [ 0.0012, -0.0063, -0.0261,  ...,  0.0010,  0.0050,  0.0542]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9458, -3.2891,  9.6797,  ..., -2.5488,  0.8271, -4.7578]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 10:44:53 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Another word for baby is infant
Another word for sofa is couch
Another word for mother is mom
Another word for portion is part
Another word for list is listing
Another word for hieroglyph is hieroglyphic
Another word for rock is stone
Another word for auto is
2024-07-25 10:44:53 root INFO     [order_1_approx] starting weight calculation for Another word for portion is part
Another word for auto is car
Another word for baby is infant
Another word for list is listing
Another word for rock is stone
Another word for mother is mom
Another word for hieroglyph is hieroglyphic
Another word for sofa is
2024-07-25 10:44:53 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 10:47:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1094, -2.4688, -6.1562,  ...,  0.1709, -1.1406, -2.9062],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 0.8984, -0.1270, -2.4375,  ...,  9.5625, -5.0000,  2.3281],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0254,  0.0044, -0.0036,  ...,  0.0014,  0.0033,  0.0033],
        [ 0.0035,  0.0227, -0.0035,  ..., -0.0053,  0.0108,  0.0020],
        [-0.0048, -0.0062,  0.1084,  ..., -0.0014, -0.0049, -0.0035],
        ...,
        [-0.0074, -0.0073,  0.0010,  ...,  0.0332, -0.0019,  0.0026],
        [ 0.0013,  0.0070, -0.0044,  ...,  0.0049,  0.0508, -0.0030],
        [ 0.0074,  0.0003, -0.0261,  ...,  0.0070,  0.0052,  0.0537]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0547, -0.8774, -2.2324,  ..., 10.4141, -3.7051,  1.2529]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 10:47:42 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Another word for portion is part
Another word for auto is car
Another word for baby is infant
Another word for list is listing
Another word for rock is stone
Another word for mother is mom
Another word for hieroglyph is hieroglyphic
Another word for sofa is
2024-07-25 10:47:42 root INFO     [order_1_approx] starting weight calculation for Another word for auto is car
Another word for baby is infant
Another word for sofa is couch
Another word for portion is part
Another word for hieroglyph is hieroglyphic
Another word for mother is mom
Another word for list is listing
Another word for rock is
2024-07-25 10:47:42 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 10:50:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.6016, -2.0938, -3.9062,  ..., -3.6094, -1.6484, -1.9609],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([11.5000, -6.3125, 14.0625,  ...,  9.8750,  2.5156, -6.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0376,  0.0051,  0.0143,  ...,  0.0020, -0.0055,  0.0068],
        [ 0.0002,  0.0267,  0.0066,  ..., -0.0027, -0.0088,  0.0258],
        [ 0.0049, -0.0096,  0.1445,  ..., -0.0050, -0.0089,  0.0056],
        ...,
        [ 0.0091,  0.0003, -0.0121,  ...,  0.0225, -0.0100, -0.0045],
        [ 0.0009,  0.0067,  0.0067,  ..., -0.0088,  0.0547, -0.0156],
        [ 0.0087, -0.0034,  0.0080,  ..., -0.0023, -0.0026,  0.0608]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[11.8359, -3.0996, 12.2109,  ...,  9.5469,  0.7598, -7.1484]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 10:50:30 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Another word for auto is car
Another word for baby is infant
Another word for sofa is couch
Another word for portion is part
Another word for hieroglyph is hieroglyphic
Another word for mother is mom
Another word for list is listing
Another word for rock is
2024-07-25 10:50:31 root INFO     [order_1_approx] starting weight calculation for Another word for mother is mom
Another word for list is listing
Another word for portion is part
Another word for rock is stone
Another word for sofa is couch
Another word for auto is car
Another word for baby is infant
Another word for hieroglyph is
2024-07-25 10:50:31 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 10:53:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4258, -1.5781, -2.5156,  ...,  1.1250,  1.2656, -0.7109],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  5.6250, -13.2500,   6.6250,  ...,  -3.2500,   5.6250,  -3.9844],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0103, -0.0005, -0.0021,  ...,  0.0005,  0.0024,  0.0038],
        [-0.0005,  0.0049, -0.0022,  ..., -0.0038,  0.0014,  0.0004],
        [-0.0004, -0.0018,  0.0508,  ..., -0.0028, -0.0002, -0.0029],
        ...,
        [ 0.0035, -0.0025, -0.0017,  ...,  0.0078,  0.0014, -0.0007],
        [-0.0023,  0.0019, -0.0073,  ..., -0.0017,  0.0172,  0.0020],
        [ 0.0008, -0.0030, -0.0011,  ...,  0.0018,  0.0038,  0.0131]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  5.4727, -13.4844,   6.8984,  ...,  -3.1562,   5.6289,  -3.3477]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 10:53:18 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Another word for mother is mom
Another word for list is listing
Another word for portion is part
Another word for rock is stone
Another word for sofa is couch
Another word for auto is car
Another word for baby is infant
Another word for hieroglyph is
2024-07-25 10:53:18 root INFO     [order_1_approx] starting weight calculation for Another word for mother is mom
Another word for sofa is couch
Another word for auto is car
Another word for hieroglyph is hieroglyphic
Another word for rock is stone
Another word for baby is infant
Another word for portion is part
Another word for list is
2024-07-25 10:53:18 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 10:56:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1094, -3.7188,  2.3281,  ..., -2.0469, -0.7148,  0.1094],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-3.3125, -0.8281,  5.8438,  ...,  6.2500,  6.0312, 11.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0449,  0.0057,  0.0009,  ..., -0.0021,  0.0003, -0.0046],
        [ 0.0002,  0.0194,  0.0067,  ...,  0.0017, -0.0097,  0.0084],
        [-0.0090, -0.0031,  0.1553,  ..., -0.0027, -0.0004, -0.0035],
        ...,
        [ 0.0101,  0.0002, -0.0058,  ...,  0.0245,  0.0092,  0.0070],
        [-0.0019,  0.0045,  0.0145,  ...,  0.0031,  0.0566, -0.0021],
        [-0.0020, -0.0005, -0.0025,  ..., -0.0011,  0.0002,  0.0562]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0078,  0.5176,  5.9883,  ...,  4.3125,  4.5547,  9.8828]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 10:56:06 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Another word for mother is mom
Another word for sofa is couch
Another word for auto is car
Another word for hieroglyph is hieroglyphic
Another word for rock is stone
Another word for baby is infant
Another word for portion is part
Another word for list is
2024-07-25 10:56:07 root INFO     [order_1_approx] starting weight calculation for Another word for sofa is couch
Another word for portion is part
Another word for auto is car
Another word for list is listing
Another word for baby is infant
Another word for hieroglyph is hieroglyphic
Another word for rock is stone
Another word for mother is
2024-07-25 10:56:07 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 10:58:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.3594,  0.8438, -0.3047,  ..., -2.5625, -2.3125, -1.8672],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 6.5000,  3.4531, 13.5625,  ..., -8.1875,  9.4375, -2.3281],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.9541e-02,  6.5308e-03,  1.4038e-02,  ...,  3.8147e-03,
          1.0834e-03, -2.4796e-04],
        [ 5.5237e-03,  1.4099e-02, -2.0264e-02,  ..., -5.3406e-05,
         -2.6245e-03, -2.1973e-03],
        [ 2.2736e-03,  4.4250e-03,  1.0400e-01,  ..., -2.1820e-03,
         -2.8076e-03,  1.5717e-03],
        ...,
        [-9.3842e-04,  4.8218e-03,  1.2512e-03,  ...,  2.5146e-02,
         -8.5449e-03,  7.9956e-03],
        [ 4.7302e-03,  2.0294e-03, -9.6436e-03,  ...,  2.4567e-03,
          3.7842e-02,  7.5912e-04],
        [-5.6458e-03, -4.8828e-03, -1.5259e-05,  ..., -1.6785e-04,
         -6.1035e-04,  4.1504e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 7.7344,  3.3457, 12.4062,  ..., -8.8906,  8.3828, -2.4648]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 10:58:56 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Another word for sofa is couch
Another word for portion is part
Another word for auto is car
Another word for list is listing
Another word for baby is infant
Another word for hieroglyph is hieroglyphic
Another word for rock is stone
Another word for mother is
2024-07-25 10:58:56 root INFO     [order_1_approx] starting weight calculation for Another word for list is listing
Another word for mother is mom
Another word for baby is infant
Another word for rock is stone
Another word for hieroglyph is hieroglyphic
Another word for sofa is couch
Another word for auto is car
Another word for portion is
2024-07-25 10:58:56 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 11:01:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0312, -3.3438, -1.6406,  ..., -0.7266, -0.7891,  0.2617],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  1.8750,   9.3750,  -4.2188,  ...,   6.4375,   4.8750, -13.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0493,  0.0018,  0.0228,  ..., -0.0013,  0.0080,  0.0097],
        [ 0.0009,  0.0337, -0.0087,  ..., -0.0017, -0.0009,  0.0046],
        [ 0.0016, -0.0034,  0.1875,  ..., -0.0036, -0.0036,  0.0126],
        ...,
        [-0.0063, -0.0066,  0.0008,  ...,  0.0420,  0.0053,  0.0057],
        [-0.0082,  0.0040,  0.0089,  ...,  0.0060,  0.0728, -0.0012],
        [-0.0009, -0.0079, -0.0277,  ..., -0.0031,  0.0108,  0.0718]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  3.5820,   7.4414,  -3.2012,  ...,   9.2969,   2.1875, -13.4453]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 11:01:52 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Another word for list is listing
Another word for mother is mom
Another word for baby is infant
Another word for rock is stone
Another word for hieroglyph is hieroglyphic
Another word for sofa is couch
Another word for auto is car
Another word for portion is
2024-07-25 11:01:52 root INFO     [order_1_approx] starting weight calculation for Another word for portion is part
Another word for sofa is couch
Another word for list is listing
Another word for rock is stone
Another word for hieroglyph is hieroglyphic
Another word for mother is mom
Another word for auto is car
Another word for baby is
2024-07-25 11:01:53 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 11:04:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2344, -2.5625, -1.4219,  ..., -1.5781, -0.7891,  0.2070],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  6.6562,  12.1875,   6.6875,  ...,  12.6875,   2.8438, -10.8125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0332,  0.0059,  0.0015,  ..., -0.0062,  0.0046, -0.0046],
        [ 0.0017,  0.0156, -0.0118,  ...,  0.0016, -0.0048,  0.0126],
        [-0.0004,  0.0019,  0.1040,  ..., -0.0060, -0.0019, -0.0010],
        ...,
        [ 0.0045, -0.0089, -0.0029,  ...,  0.0276, -0.0084,  0.0046],
        [-0.0089,  0.0059,  0.0019,  ...,  0.0017,  0.0405, -0.0055],
        [-0.0018, -0.0057, -0.0054,  ..., -0.0010, -0.0063,  0.0422]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  7.6914,  12.0000,   5.4375,  ...,  11.7500,   3.4492, -10.6797]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 11:04:41 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Another word for portion is part
Another word for sofa is couch
Another word for list is listing
Another word for rock is stone
Another word for hieroglyph is hieroglyphic
Another word for mother is mom
Another word for auto is car
Another word for baby is
2024-07-25 11:04:41 root INFO     total operator prediction time: 1356.628571987152 seconds
2024-07-25 11:04:41 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on hypernyms - misc
2024-07-25 11:04:41 root INFO     building operator hypernyms - misc
2024-07-25 11:04:41 root INFO     [order_1_approx] starting weight calculation for The photo falls into the category of picture
The diary falls into the category of journal
The vase falls into the category of jar
The deodorant falls into the category of toiletry
The brooch falls into the category of jewelry
The gasoline falls into the category of fuel
The stapler falls into the category of device
The sweater falls into the category of
2024-07-25 11:04:41 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 11:07:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.1094, -1.6641, -0.5508,  ...,  0.6250, -3.0156,  1.0703],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 2.8438, 11.5000, -4.1875,  ...,  3.7500,  3.2500, -6.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.7100e-02,  5.2490e-03,  4.4250e-03,  ...,  1.0498e-02,
          2.6398e-03, -2.0752e-03],
        [-3.5400e-03,  2.1729e-02,  9.0027e-04,  ..., -3.7537e-03,
          1.3885e-03,  5.6152e-03],
        [ 5.0659e-03, -4.4861e-03,  1.1084e-01,  ..., -4.6997e-03,
         -7.3853e-03,  7.6294e-06],
        ...,
        [ 1.9073e-03,  3.2616e-04,  7.8735e-03,  ...,  2.4170e-02,
         -4.9744e-03,  1.8463e-03],
        [ 5.5237e-03,  8.3618e-03,  6.3782e-03,  ...,  8.8501e-04,
          4.2725e-02,  4.1809e-03],
        [ 6.0120e-03, -3.9673e-03, -1.3611e-02,  ..., -4.2114e-03,
          7.1716e-03,  4.3457e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.3574, 10.5703, -1.9766,  ...,  3.1309,  5.3750, -6.0508]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 11:07:32 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The photo falls into the category of picture
The diary falls into the category of journal
The vase falls into the category of jar
The deodorant falls into the category of toiletry
The brooch falls into the category of jewelry
The gasoline falls into the category of fuel
The stapler falls into the category of device
The sweater falls into the category of
2024-07-25 11:07:32 root INFO     [order_1_approx] starting weight calculation for The stapler falls into the category of device
The diary falls into the category of journal
The deodorant falls into the category of toiletry
The gasoline falls into the category of fuel
The sweater falls into the category of clothes
The photo falls into the category of picture
The vase falls into the category of jar
The brooch falls into the category of
2024-07-25 11:07:32 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 11:10:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.7812, -2.3438, -0.1250,  ..., -1.0547, -2.9844,  0.0215],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-7.4375, -1.7188, -0.3750,  ...,  3.7812,  2.6875, -4.3125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0371,  0.0013,  0.0173,  ...,  0.0034, -0.0040, -0.0024],
        [-0.0001,  0.0208, -0.0007,  ..., -0.0034, -0.0070,  0.0046],
        [-0.0038, -0.0075,  0.1240,  ..., -0.0059,  0.0040, -0.0041],
        ...,
        [-0.0040,  0.0002, -0.0012,  ...,  0.0344, -0.0078,  0.0004],
        [ 0.0075,  0.0052,  0.0034,  ...,  0.0008,  0.0503,  0.0023],
        [-0.0010, -0.0027, -0.0032,  ..., -0.0040,  0.0080,  0.0498]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-7.8203, -1.4697, -0.1350,  ...,  4.2188,  2.0859, -5.4297]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 11:10:24 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The stapler falls into the category of device
The diary falls into the category of journal
The deodorant falls into the category of toiletry
The gasoline falls into the category of fuel
The sweater falls into the category of clothes
The photo falls into the category of picture
The vase falls into the category of jar
The brooch falls into the category of
2024-07-25 11:10:24 root INFO     [order_1_approx] starting weight calculation for The deodorant falls into the category of toiletry
The vase falls into the category of jar
The gasoline falls into the category of fuel
The photo falls into the category of picture
The sweater falls into the category of clothes
The diary falls into the category of journal
The brooch falls into the category of jewelry
The stapler falls into the category of
2024-07-25 11:10:24 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
