2024-07-12 16:55:13 root INFO     loading model + tokenizer
2024-07-12 16:55:33 root INFO     model + tokenizer loaded
2024-07-12 16:55:33 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - part
2024-07-12 16:55:36 root INFO     building operator meronyms - part
2024-07-12 16:55:36 root INFO     [order_1_approx] starting weight calculation for A part of a gigabit is a megabit
A part of a shilling is a pence
A part of a seafront is a harbor
A part of a academia is a college
A part of a deer is a antler
A part of a filename is a extension
A part of a orthography is a hyphenation
A part of a radio is a
2024-07-12 16:55:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 16:59:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 17:03:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0669,  0.8330,  0.4316,  ...,  0.4189, -0.5811,  0.8525],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.0711,  0.8418,  0.4033,  ...,  0.4082, -0.5947,  0.8516],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6719,  0.5518, -4.4844,  ..., -3.0762,  3.2578,  0.3389],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0033, -0.0164,  0.0040,  ...,  0.0062,  0.0050,  0.0102],
        [-0.0165,  0.0063, -0.0031,  ...,  0.0152, -0.0076, -0.0064],
        [ 0.0240,  0.0087,  0.0005,  ..., -0.0115,  0.0262,  0.0052],
        ...,
        [ 0.0017, -0.0021,  0.0074,  ...,  0.0047, -0.0158,  0.0147],
        [-0.0271,  0.0071, -0.0040,  ...,  0.0267, -0.0094,  0.0013],
        [ 0.0040, -0.0127, -0.0063,  ..., -0.0078,  0.0264,  0.0233]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0012,  0.0011, -0.0014,  ..., -0.0021, -0.0029,  0.0024],
        [-0.0009, -0.0005,  0.0012,  ..., -0.0008,  0.0008, -0.0014],
        [ 0.0028, -0.0004,  0.0006,  ...,  0.0005,  0.0002, -0.0006],
        ...,
        [ 0.0017, -0.0017,  0.0012,  ..., -0.0001,  0.0019, -0.0025],
        [ 0.0010, -0.0005,  0.0011,  ...,  0.0007,  0.0003, -0.0011],
        [-0.0017, -0.0019,  0.0007,  ...,  0.0002,  0.0010,  0.0014]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7949,  0.6836, -3.7773,  ..., -2.2207,  1.9639,  1.0391]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2340, -0.3035, -0.2094,  ..., -0.1025,  0.1378, -0.0703]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 17:03:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a gigabit is a megabit
A part of a shilling is a pence
A part of a seafront is a harbor
A part of a academia is a college
A part of a deer is a antler
A part of a filename is a extension
A part of a orthography is a hyphenation
A part of a radio is a
2024-07-12 17:03:32 root INFO     [order_1_approx] starting weight calculation for A part of a seafront is a harbor
A part of a orthography is a hyphenation
A part of a shilling is a pence
A part of a deer is a antler
A part of a radio is a receiver
A part of a gigabit is a megabit
A part of a academia is a college
A part of a filename is a
2024-07-12 17:03:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 17:07:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 17:11:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4741,  0.4504, -0.3994,  ...,  0.1704,  0.1587,  0.6094],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.4133,  0.3884, -0.3511,  ...,  0.1459,  0.1171,  0.5239],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6821,  3.5000, -1.5020,  ..., -0.6948, -1.5195,  1.3047],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-4.3144e-03, -1.8829e-02, -1.4563e-03,  ...,  2.6398e-03,
         -8.3847e-03, -8.8196e-03],
        [ 7.7591e-03, -7.3395e-03,  2.3560e-02,  ...,  1.4641e-02,
         -3.0270e-03, -1.8234e-02],
        [ 8.3351e-04,  8.8730e-03, -1.2047e-02,  ..., -9.1858e-03,
          4.4708e-03,  6.1035e-05],
        ...,
        [-3.7613e-03, -1.1055e-02, -1.9150e-03,  ..., -2.7943e-03,
         -9.8877e-03, -8.7738e-04],
        [ 9.7885e-03,  2.1210e-03,  1.6846e-02,  ..., -1.1795e-02,
         -6.8893e-03,  9.1934e-03],
        [-4.0703e-03,  1.2001e-02, -9.9716e-03,  ..., -2.8973e-03,
          1.7456e-02,  1.3474e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-4.3964e-04, -1.0853e-03,  5.4073e-04,  ...,  7.8773e-04,
          9.5844e-04,  4.7541e-04],
        [-1.6713e-04,  1.1034e-03,  2.9316e-03,  ...,  1.8520e-03,
         -6.1703e-04,  1.6749e-05],
        [ 1.2321e-03, -2.1267e-04,  2.8586e-04,  ..., -1.8969e-03,
          2.1267e-03, -5.0926e-04],
        ...,
        [ 5.4884e-04,  2.2411e-05, -2.0752e-03,  ..., -1.6603e-03,
         -3.5048e-05,  8.5211e-04],
        [ 1.6823e-03,  7.8154e-04,  9.1553e-05,  ..., -2.7695e-03,
         -6.8283e-04,  8.7118e-04],
        [-1.4706e-03,  4.5705e-04,  3.2616e-03,  ...,  2.4295e-04,
          1.1520e-03, -2.4271e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1001,  3.5391, -1.7861,  ..., -0.3135, -1.5928,  1.0332]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0325, -0.4890, -0.2136,  ...,  0.1470, -0.1932, -0.2058]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 17:11:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a seafront is a harbor
A part of a orthography is a hyphenation
A part of a shilling is a pence
A part of a deer is a antler
A part of a radio is a receiver
A part of a gigabit is a megabit
A part of a academia is a college
A part of a filename is a
2024-07-12 17:11:28 root INFO     [order_1_approx] starting weight calculation for A part of a orthography is a hyphenation
A part of a academia is a college
A part of a deer is a antler
A part of a filename is a extension
A part of a gigabit is a megabit
A part of a seafront is a harbor
A part of a radio is a receiver
A part of a shilling is a
2024-07-12 17:11:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 17:15:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 17:19:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5352, -0.9443, -0.7266,  ...,  0.9160, -0.1506,  0.9922],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.5527, -1.0078, -0.7207,  ...,  0.9004, -0.1714,  1.0107],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.4375,  0.3721, -3.9961,  ..., -0.1221, -2.1055,  1.1094],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0369, -0.0396,  0.0071,  ...,  0.0013, -0.0144, -0.0023],
        [-0.0006, -0.0012,  0.0036,  ..., -0.0147,  0.0142, -0.0102],
        [-0.0132, -0.0052, -0.0103,  ..., -0.0094,  0.0331,  0.0063],
        ...,
        [-0.0196, -0.0055, -0.0110,  ...,  0.0072,  0.0025, -0.0090],
        [ 0.0303, -0.0134,  0.0039,  ..., -0.0051,  0.0161, -0.0162],
        [-0.0048,  0.0126,  0.0053,  ..., -0.0242,  0.0190, -0.0105]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.3460e-03, -5.3883e-04,  1.4505e-03,  ...,  8.5115e-05,
          8.1825e-04, -2.2984e-04],
        [-1.5965e-03, -7.0095e-04,  4.2033e-04,  ...,  5.2166e-04,
         -1.8425e-03,  6.5136e-04],
        [ 9.2745e-04, -1.1563e-04,  1.2903e-03,  ...,  1.0567e-03,
          9.3937e-04, -1.3113e-04],
        ...,
        [-5.6314e-04,  9.9754e-04, -1.8177e-03,  ...,  9.7275e-05,
         -1.7154e-04, -1.7338e-03],
        [ 8.1635e-04,  1.4191e-03, -2.4376e-03,  ..., -2.5320e-04,
         -1.0204e-03,  5.5790e-04],
        [-5.3167e-04, -4.6921e-04, -1.4286e-03,  ..., -9.7466e-04,
          1.8539e-03,  1.0748e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6797,  0.1553, -4.0898,  ...,  0.9102, -1.2344,  0.7314]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0567,  0.1020, -0.0248,  ..., -0.2032, -0.0195, -0.0279]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 17:19:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a orthography is a hyphenation
A part of a academia is a college
A part of a deer is a antler
A part of a filename is a extension
A part of a gigabit is a megabit
A part of a seafront is a harbor
A part of a radio is a receiver
A part of a shilling is a
2024-07-12 17:19:25 root INFO     [order_1_approx] starting weight calculation for A part of a academia is a college
A part of a shilling is a pence
A part of a orthography is a hyphenation
A part of a radio is a receiver
A part of a deer is a antler
A part of a gigabit is a megabit
A part of a filename is a extension
A part of a seafront is a
2024-07-12 17:19:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 17:23:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 17:27:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.4082, -0.3330,  0.9854,  ..., -0.4656,  0.1775,  0.1226],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-1.2881, -0.3274,  0.8462,  ..., -0.4021,  0.1414,  0.0909],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 6.4297,  3.9688,  0.5117,  ...,  4.9180, -0.5444, -0.6377],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0092, -0.0126,  0.0191,  ..., -0.0027, -0.0084, -0.0038],
        [-0.0007,  0.0002,  0.0185,  ...,  0.0113, -0.0073, -0.0050],
        [-0.0055,  0.0090,  0.0014,  ..., -0.0089,  0.0005,  0.0017],
        ...,
        [-0.0130, -0.0110,  0.0284,  ...,  0.0191, -0.0183,  0.0110],
        [ 0.0017,  0.0011, -0.0047,  ...,  0.0037, -0.0011,  0.0036],
        [-0.0041, -0.0034, -0.0027,  ..., -0.0010,  0.0095,  0.0006]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-9.4795e-04,  4.6635e-04,  9.1934e-04,  ..., -3.9673e-04,
         -3.8815e-04,  9.7847e-04],
        [ 4.8351e-04, -6.4993e-04,  1.9479e-04,  ...,  1.1578e-03,
         -5.2977e-04, -1.5717e-03],
        [ 1.7424e-03,  6.9141e-05,  8.9836e-04,  ..., -5.0640e-04,
         -8.9049e-05,  2.8372e-04],
        ...,
        [ 1.9503e-04, -3.9816e-04,  3.1447e-04,  ...,  3.6359e-04,
         -7.4148e-04, -1.1311e-03],
        [-3.6335e-04, -6.5327e-04, -1.0023e-03,  ...,  6.2346e-05,
         -6.7091e-04, -6.1893e-04],
        [-1.7321e-04, -7.9298e-04, -3.9291e-04,  ...,  3.6812e-04,
          3.1376e-04, -2.9707e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.0898,  4.0195,  0.9028,  ...,  4.4609, -0.8398, -0.2102]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0324, -0.1094, -0.0524,  ..., -0.1952,  0.0330, -0.0756]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 17:27:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a academia is a college
A part of a shilling is a pence
A part of a orthography is a hyphenation
A part of a radio is a receiver
A part of a deer is a antler
A part of a gigabit is a megabit
A part of a filename is a extension
A part of a seafront is a
2024-07-12 17:27:20 root INFO     [order_1_approx] starting weight calculation for A part of a deer is a antler
A part of a shilling is a pence
A part of a radio is a receiver
A part of a seafront is a harbor
A part of a gigabit is a megabit
A part of a orthography is a hyphenation
A part of a filename is a extension
A part of a academia is a
2024-07-12 17:27:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 17:31:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 17:35:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2374,  1.0068,  0.2559,  ..., -0.3730, -0.2371,  0.1685],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.2155,  0.9468,  0.2142,  ..., -0.3350, -0.2413,  0.1338],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9912,  3.0293, -2.0527,  ...,  1.6611, -1.9580,  1.6455],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0007, -0.0032,  0.0024,  ...,  0.0028,  0.0047, -0.0082],
        [-0.0107,  0.0014,  0.0119,  ...,  0.0095, -0.0285, -0.0133],
        [ 0.0186,  0.0029, -0.0013,  ..., -0.0073,  0.0033,  0.0008],
        ...,
        [ 0.0062, -0.0078,  0.0100,  ..., -0.0152,  0.0050,  0.0147],
        [ 0.0219,  0.0133,  0.0086,  ..., -0.0187,  0.0065,  0.0248],
        [-0.0041, -0.0116, -0.0140,  ..., -0.0060,  0.0297, -0.0035]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-6.2466e-04,  6.3467e-04, -9.7847e-04,  ...,  2.2068e-03,
         -5.2452e-05, -9.9754e-04],
        [ 6.8283e-04,  4.6968e-04,  5.8985e-04,  ...,  2.5501e-03,
         -1.1530e-03,  5.2691e-05],
        [ 1.5593e-04, -1.4248e-03, -6.1893e-04,  ...,  6.4182e-04,
          2.5177e-03,  8.8358e-04],
        ...,
        [ 1.1921e-03,  5.8079e-04,  1.0914e-04,  ..., -3.0403e-03,
          1.4734e-03,  6.2990e-04],
        [ 3.1161e-04, -3.7360e-04, -5.4741e-04,  ..., -1.0262e-03,
         -4.1866e-04,  3.9554e-04],
        [-1.6842e-03, -2.4509e-03, -2.5296e-04,  ...,  5.6410e-04,
          4.5753e-04, -3.2568e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2324,  2.9453, -2.5625,  ...,  1.7832, -2.2383,  2.4199]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0823, -0.0183, -0.2622,  ..., -0.1754,  0.1367,  0.0840]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 17:35:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a deer is a antler
A part of a shilling is a pence
A part of a radio is a receiver
A part of a seafront is a harbor
A part of a gigabit is a megabit
A part of a orthography is a hyphenation
A part of a filename is a extension
A part of a academia is a
2024-07-12 17:35:13 root INFO     [order_1_approx] starting weight calculation for A part of a radio is a receiver
A part of a orthography is a hyphenation
A part of a shilling is a pence
A part of a deer is a antler
A part of a academia is a college
A part of a seafront is a harbor
A part of a filename is a extension
A part of a gigabit is a
2024-07-12 17:35:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 17:39:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 17:43:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2520, -0.8936,  0.4258,  ..., -0.7031,  0.6592,  0.2349],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-1.0840, -0.8062,  0.3403,  ..., -0.5767,  0.5430,  0.1842],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1152,  0.9985, -2.0586,  ...,  0.6328, -2.5664,  2.2383],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0111, -0.0168,  0.0006,  ...,  0.0069, -0.0196, -0.0067],
        [-0.0124,  0.0020, -0.0066,  ...,  0.0188, -0.0152, -0.0193],
        [-0.0044,  0.0027, -0.0048,  ..., -0.0163,  0.0157,  0.0217],
        ...,
        [-0.0061, -0.0086,  0.0158,  ..., -0.0030, -0.0124,  0.0053],
        [ 0.0144,  0.0022,  0.0010,  ..., -0.0068, -0.0176,  0.0035],
        [ 0.0111, -0.0004,  0.0123,  ..., -0.0140,  0.0102,  0.0121]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-5.3787e-04, -9.6607e-04, -1.0195e-03,  ...,  2.0874e-04,
         -2.1839e-03,  2.2354e-03],
        [-5.0068e-04, -5.6088e-05,  2.4343e-04,  ..., -4.1986e-04,
         -6.5327e-04, -1.6963e-04],
        [-3.3140e-04,  1.3237e-03,  2.2006e-04,  ..., -6.4492e-05,
          9.6178e-04,  4.7588e-04],
        ...,
        [ 1.0605e-03,  5.8603e-04,  2.8133e-05,  ..., -1.6623e-03,
         -6.3229e-04,  3.3307e-04],
        [-2.1458e-05, -2.9135e-04,  2.0146e-05,  ...,  1.8063e-03,
         -1.3113e-04,  4.3988e-05],
        [-8.3971e-04, -2.3675e-04,  1.0049e-04,  ..., -7.5150e-04,
         -9.5129e-05,  9.2220e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5225,  1.1006, -2.6875,  ...,  1.2051, -2.3652,  2.4219]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0368,  0.0641, -0.0106,  ..., -0.2219, -0.0227, -0.0167]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 17:43:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a radio is a receiver
A part of a orthography is a hyphenation
A part of a shilling is a pence
A part of a deer is a antler
A part of a academia is a college
A part of a seafront is a harbor
A part of a filename is a extension
A part of a gigabit is a
2024-07-12 17:43:09 root INFO     [order_1_approx] starting weight calculation for A part of a orthography is a hyphenation
A part of a shilling is a pence
A part of a radio is a receiver
A part of a seafront is a harbor
A part of a filename is a extension
A part of a academia is a college
A part of a gigabit is a megabit
A part of a deer is a
2024-07-12 17:43:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 17:47:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 17:51:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1611, -0.0298, -0.6069,  ...,  1.1846, -0.4504,  0.2993],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.1466, -0.0456, -0.5513,  ...,  1.0625, -0.4326,  0.2610],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6699,  1.5283, -6.4648,  ..., -3.6406,  0.0327,  0.5410],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0152, -0.0014,  0.0071,  ...,  0.0135,  0.0161, -0.0065],
        [-0.0406,  0.0156,  0.0034,  ...,  0.0247, -0.0064, -0.0093],
        [ 0.0146, -0.0066,  0.0043,  ..., -0.0243,  0.0134, -0.0121],
        ...,
        [ 0.0052, -0.0023,  0.0099,  ..., -0.0114, -0.0087,  0.0085],
        [ 0.0073, -0.0017,  0.0213,  ...,  0.0185,  0.0029,  0.0139],
        [-0.0081, -0.0041, -0.0077,  ..., -0.0082,  0.0066,  0.0164]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-8.0299e-04,  1.3580e-03, -1.3409e-03,  ..., -1.3618e-03,
          8.4639e-06,  1.2627e-03],
        [-3.5310e-04,  1.3244e-04,  6.5231e-04,  ...,  8.2207e-04,
         -2.0409e-03, -8.4686e-04],
        [ 9.4700e-04, -1.0891e-03,  1.1034e-03,  ..., -1.0662e-03,
         -3.7432e-05, -8.9979e-04],
        ...,
        [-1.5249e-03, -3.6049e-04,  1.5411e-03,  ...,  7.7820e-04,
          1.6689e-03,  5.8031e-04],
        [ 5.8270e-04,  3.9005e-04,  1.5235e-04,  ...,  4.9973e-04,
         -6.3801e-04,  8.2111e-04],
        [-4.5443e-04, -1.7729e-03, -7.9393e-05,  ...,  1.1234e-03,
          1.4172e-03,  8.6188e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2139,  2.2812, -5.8984,  ..., -3.5938, -0.6011,  1.3457]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1422, -0.0644, -0.3691,  ..., -0.0352,  0.1605, -0.0883]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 17:51:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a orthography is a hyphenation
A part of a shilling is a pence
A part of a radio is a receiver
A part of a seafront is a harbor
A part of a filename is a extension
A part of a academia is a college
A part of a gigabit is a megabit
A part of a deer is a
2024-07-12 17:51:05 root INFO     [order_1_approx] starting weight calculation for A part of a radio is a receiver
A part of a filename is a extension
A part of a gigabit is a megabit
A part of a academia is a college
A part of a deer is a antler
A part of a shilling is a pence
A part of a seafront is a harbor
A part of a orthography is a
2024-07-12 17:51:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 17:55:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 17:58:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2197,  0.6802,  0.0033,  ..., -1.0283, -0.1201,  1.3857],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 1.1309,  0.6304, -0.0088,  ..., -0.9126, -0.1290,  1.2900],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0771,  1.7734, -2.6172,  ...,  0.3018,  0.3223,  3.6172],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0025, -0.0123,  0.0008,  ..., -0.0055,  0.0018, -0.0013],
        [-0.0089, -0.0090,  0.0070,  ...,  0.0037, -0.0066, -0.0058],
        [ 0.0102,  0.0056, -0.0043,  ...,  0.0092,  0.0137,  0.0027],
        ...,
        [-0.0123, -0.0054, -0.0074,  ...,  0.0026,  0.0081,  0.0037],
        [ 0.0092,  0.0012,  0.0014,  ...,  0.0138, -0.0004,  0.0215],
        [-0.0197, -0.0092,  0.0185,  ..., -0.0105,  0.0152,  0.0000]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 3.1042e-04, -2.6274e-04,  8.8644e-04,  ..., -1.4267e-03,
         -9.2888e-04, -9.2506e-05],
        [-3.2043e-04, -3.8838e-04, -7.8499e-05,  ..., -2.4605e-04,
          3.8099e-04,  3.0804e-04],
        [ 1.5759e-04, -5.1451e-04,  1.6432e-03,  ...,  9.0313e-04,
          8.6403e-04, -7.1573e-04],
        ...,
        [-4.9591e-04, -2.6512e-04,  2.4509e-04,  ...,  4.5586e-04,
          1.8203e-04, -5.0211e-04],
        [-4.1890e-04,  8.2636e-04, -1.0033e-03,  ..., -2.4986e-04,
         -3.3998e-04,  1.3027e-03],
        [-4.6968e-04, -1.1749e-03,  1.4772e-03,  ..., -2.7561e-04,
          9.1171e-04, -5.3692e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2329,  1.6572, -2.6836,  ..., -0.4292,  0.4775,  2.4453]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0512, -0.0502, -0.1891,  ..., -0.0402, -0.0840,  0.0447]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 17:59:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a radio is a receiver
A part of a filename is a extension
A part of a gigabit is a megabit
A part of a academia is a college
A part of a deer is a antler
A part of a shilling is a pence
A part of a seafront is a harbor
A part of a orthography is a
2024-07-12 17:59:01 root INFO     total operator prediction time: 3804.574375152588 seconds
2024-07-12 17:59:01 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on synonyms - exact
2024-07-12 17:59:03 root INFO     building operator synonyms - exact
2024-07-12 17:59:03 root INFO     [order_1_approx] starting weight calculation for Another word for new is modern
Another word for harbor is seaport
Another word for emphasis is accent
Another word for market is marketplace
Another word for flower is blossom
Another word for dollars is bucks
Another word for honest is sincere
Another word for clothes is
2024-07-12 17:59:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 18:03:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 18:06:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1680,  0.3145, -0.5776,  ...,  1.1094, -0.1953,  0.0811],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 1.0742,  0.2781, -0.5225,  ...,  0.9839, -0.1987,  0.0503],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0345, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.6543,  0.2065, -3.8125,  ..., -4.3633,  0.0229, -0.1326],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0165, -0.0322,  0.0167,  ...,  0.0077,  0.0075, -0.0018],
        [ 0.0005,  0.0001,  0.0096,  ...,  0.0127, -0.0157, -0.0127],
        [ 0.0266, -0.0013,  0.0097,  ..., -0.0045, -0.0050,  0.0060],
        ...,
        [-0.0075,  0.0049,  0.0163,  ..., -0.0012, -0.0034,  0.0273],
        [ 0.0160,  0.0032, -0.0279,  ..., -0.0059,  0.0044,  0.0150],
        [-0.0235, -0.0063,  0.0116,  ..., -0.0051, -0.0131,  0.0087]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 1.6727e-03,  5.2261e-04, -1.5557e-05,  ...,  6.9380e-04,
         -1.7738e-04,  1.1406e-03],
        [-6.6280e-04, -3.0594e-03,  4.2534e-03,  ..., -1.6098e-03,
         -5.7650e-04,  8.5783e-04],
        [ 7.9274e-06, -1.2159e-03,  2.0218e-03,  ...,  2.1458e-03,
          2.5177e-03, -1.1992e-04],
        ...,
        [ 3.3340e-03,  1.5163e-03, -1.6699e-03,  ..., -1.3561e-03,
          2.7504e-03,  6.7234e-04],
        [ 4.3831e-03,  6.7139e-04, -3.5172e-03,  ..., -1.7605e-03,
          2.5392e-04,  1.7033e-03],
        [ 1.2560e-03, -7.6818e-04, -1.5011e-03,  ...,  1.3771e-03,
          2.2960e-04, -2.8038e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0752, -0.3315, -3.3320,  ..., -3.4375, -0.2168,  0.6758]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1611,  0.0638,  0.1519,  ...,  0.3960,  0.1689, -0.2739]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 18:06:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for new is modern
Another word for harbor is seaport
Another word for emphasis is accent
Another word for market is marketplace
Another word for flower is blossom
Another word for dollars is bucks
Another word for honest is sincere
Another word for clothes is
2024-07-12 18:06:58 root INFO     [order_1_approx] starting weight calculation for Another word for clothes is clothing
Another word for emphasis is accent
Another word for dollars is bucks
Another word for honest is sincere
Another word for harbor is seaport
Another word for market is marketplace
Another word for flower is blossom
Another word for new is
2024-07-12 18:06:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 18:10:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 18:14:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7993,  0.0818, -0.5835,  ..., -0.0750, -0.4668,  0.1587],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.8467,  0.0726, -0.6016,  ..., -0.0717, -0.5063,  0.1489],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0345, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.7520, -1.9766, -3.9902,  ..., -0.6338, -0.9756,  1.9170],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0010,  0.0033,  0.0058,  ..., -0.0067,  0.0108,  0.0030],
        [-0.0160, -0.0011, -0.0063,  ...,  0.0091, -0.0154, -0.0016],
        [-0.0112,  0.0197,  0.0027,  ...,  0.0065, -0.0153,  0.0006],
        ...,
        [ 0.0078, -0.0316,  0.0168,  ..., -0.0077, -0.0095,  0.0173],
        [ 0.0098, -0.0160,  0.0137,  ..., -0.0115,  0.0135,  0.0131],
        [-0.0051, -0.0051, -0.0108,  ..., -0.0023,  0.0137,  0.0007]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.3212e-03, -9.2554e-04,  8.2970e-04,  ..., -1.7138e-03,
          8.4400e-04,  2.1229e-03],
        [-1.3285e-03,  6.1226e-04,  9.0027e-04,  ...,  1.0786e-03,
          5.8270e-04, -2.5344e-04],
        [-3.8853e-03, -1.6689e-05,  9.5654e-04,  ..., -1.9722e-03,
         -1.8253e-03,  1.0538e-03],
        ...,
        [ 2.1935e-03, -2.6875e-03, -2.7418e-05,  ...,  1.4746e-04,
          2.2335e-03, -1.2264e-03],
        [ 2.6379e-03, -1.2722e-03,  2.3460e-04,  ...,  1.5907e-03,
          1.5841e-03, -1.2093e-03],
        [-1.1721e-03,  1.2398e-03,  2.1100e-05,  ...,  9.4223e-04,
         -3.6335e-03, -6.1464e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.7402, -0.8398, -4.6875,  ..., -0.3757, -0.1431,  1.3789]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2625,  0.0352,  0.2654,  ..., -0.0475, -0.0037, -0.1096]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 18:14:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for clothes is clothing
Another word for emphasis is accent
Another word for dollars is bucks
Another word for honest is sincere
Another word for harbor is seaport
Another word for market is marketplace
Another word for flower is blossom
Another word for new is
2024-07-12 18:14:51 root INFO     [order_1_approx] starting weight calculation for Another word for clothes is clothing
Another word for harbor is seaport
Another word for flower is blossom
Another word for new is modern
Another word for market is marketplace
Another word for dollars is bucks
Another word for emphasis is accent
Another word for honest is
2024-07-12 18:14:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 18:18:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 18:22:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4675, -1.0225, -1.3145,  ...,  0.7075,  0.4536,  0.7871],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.4253, -0.9863, -1.1709,  ...,  0.6270,  0.3911,  0.7144],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0345, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3364,  2.6523, -5.5703,  ..., -0.9165,  2.0859,  0.7109],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0066, -0.0065,  0.0072,  ...,  0.0020,  0.0097,  0.0054],
        [-0.0005,  0.0129, -0.0225,  ...,  0.0171, -0.0086,  0.0225],
        [ 0.0081, -0.0076, -0.0052,  ..., -0.0136,  0.0213,  0.0210],
        ...,
        [-0.0080,  0.0250,  0.0020,  ...,  0.0002, -0.0090,  0.0047],
        [-0.0423,  0.0128, -0.0129,  ..., -0.0046, -0.0049,  0.0199],
        [-0.0096, -0.0068, -0.0051,  ...,  0.0174, -0.0035, -0.0089]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-3.5305e-03, -1.1768e-03, -1.2960e-03,  ..., -5.2834e-04,
         -1.3285e-03,  1.7033e-03],
        [-1.4362e-03,  1.5163e-03, -3.5834e-04,  ..., -4.7040e-04,
         -1.7667e-04,  3.9101e-03],
        [-4.4823e-05, -1.8692e-03, -9.0599e-05,  ..., -9.5367e-04,
          1.5612e-03, -1.9217e-03],
        ...,
        [ 1.7767e-03,  2.3484e-04, -6.4945e-04,  ...,  8.6212e-04,
         -1.6689e-03, -1.9133e-04],
        [ 3.0398e-04,  6.7139e-04, -3.9434e-04,  ...,  1.6766e-03,
          2.9397e-04, -5.2547e-04],
        [ 2.1057e-03,  7.6914e-04, -5.7220e-06,  ...,  2.6207e-03,
         -8.0204e-04,  2.1248e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4153,  2.7012, -6.3555,  ..., -1.2305,  1.6143,  0.1133]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0138,  0.3118,  0.5767,  ..., -0.2081,  0.0975,  0.2045]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 18:22:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for clothes is clothing
Another word for harbor is seaport
Another word for flower is blossom
Another word for new is modern
Another word for market is marketplace
Another word for dollars is bucks
Another word for emphasis is accent
Another word for honest is
2024-07-12 18:22:45 root INFO     [order_1_approx] starting weight calculation for Another word for market is marketplace
Another word for harbor is seaport
Another word for flower is blossom
Another word for honest is sincere
Another word for dollars is bucks
Another word for new is modern
Another word for clothes is clothing
Another word for emphasis is
2024-07-12 18:22:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 18:26:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 18:30:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0676,  0.9800,  0.6270,  ..., -0.1547,  0.0186,  0.3418],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.0574,  0.9609,  0.5698,  ..., -0.1460, -0.0056,  0.3127],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0345, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.5986,  0.5425, -3.5664,  ..., -5.4844,  3.5469,  1.2676],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0152,  0.0128, -0.0089,  ...,  0.0036,  0.0094,  0.0173],
        [ 0.0004,  0.0261, -0.0282,  ...,  0.0053, -0.0031,  0.0179],
        [ 0.0159, -0.0009,  0.0184,  ..., -0.0106,  0.0237,  0.0063],
        ...,
        [-0.0194, -0.0114,  0.0035,  ...,  0.0034,  0.0010, -0.0081],
        [-0.0309, -0.0048,  0.0172,  ..., -0.0129, -0.0087,  0.0148],
        [-0.0190, -0.0067,  0.0242,  ...,  0.0107, -0.0008,  0.0431]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-9.2983e-04, -1.5154e-03,  1.0133e-06,  ...,  2.5451e-05,
         -2.6093e-03,  2.6188e-03],
        [ 2.5578e-03, -1.0443e-03, -8.6021e-04,  ...,  3.3689e-04,
          1.5688e-03,  1.9321e-03],
        [-2.0943e-03,  1.7757e-03, -2.1248e-03,  ..., -1.4229e-03,
          3.9177e-03, -1.1644e-03],
        ...,
        [-1.3447e-03,  1.5345e-03, -3.7518e-03,  ..., -3.7384e-03,
          3.0289e-03, -2.3766e-03],
        [-1.0576e-03, -2.9545e-03,  8.0490e-04,  ..., -2.8038e-03,
         -6.5088e-04, -1.2331e-03],
        [-4.7684e-04,  1.1806e-03, -5.2214e-04,  ...,  2.0218e-03,
         -1.3781e-03, -5.5265e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4668, -0.4966, -3.8320,  ..., -4.2070,  3.1250,  0.9141]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.4761,  0.1772,  0.2125,  ...,  0.1340, -0.0649,  0.3647]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 18:30:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for market is marketplace
Another word for harbor is seaport
Another word for flower is blossom
Another word for honest is sincere
Another word for dollars is bucks
Another word for new is modern
Another word for clothes is clothing
Another word for emphasis is
2024-07-12 18:30:39 root INFO     [order_1_approx] starting weight calculation for Another word for dollars is bucks
Another word for honest is sincere
Another word for harbor is seaport
Another word for clothes is clothing
Another word for new is modern
Another word for emphasis is accent
Another word for market is marketplace
Another word for flower is
2024-07-12 18:30:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 18:34:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 18:38:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6636,  0.2769, -0.3457,  ..., -0.6748, -0.2932,  0.9775],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.5933,  0.2367, -0.3103,  ..., -0.5806, -0.2808,  0.8735],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0346, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.3203,  2.1133, -1.6016,  ...,  0.9512, -0.4302, -0.1367],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0044,  0.0066,  0.0176,  ...,  0.0104, -0.0078, -0.0072],
        [-0.0192, -0.0153, -0.0142,  ...,  0.0100, -0.0330,  0.0079],
        [ 0.0271, -0.0041, -0.0148,  ..., -0.0066,  0.0225,  0.0083],
        ...,
        [ 0.0015, -0.0035,  0.0110,  ..., -0.0052, -0.0063,  0.0176],
        [-0.0022, -0.0012, -0.0129,  ...,  0.0108, -0.0032, -0.0009],
        [-0.0210, -0.0302,  0.0047,  ...,  0.0135, -0.0071,  0.0054]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-7.3481e-04, -1.2846e-03,  1.3914e-03,  ...,  1.9779e-03,
         -2.0008e-03,  2.7218e-03],
        [-8.7452e-04,  4.2653e-04, -1.8096e-04,  ...,  6.3181e-04,
          1.9035e-03, -1.1158e-03],
        [ 1.3018e-03, -3.6621e-04,  1.4305e-06,  ..., -5.7220e-04,
         -6.0415e-04, -6.0415e-04],
        ...,
        [ 1.2341e-03,  5.1498e-05, -6.9141e-04,  ..., -1.0433e-03,
          9.0027e-04, -4.6825e-04],
        [ 1.8871e-04, -1.3714e-03, -1.7271e-03,  ..., -1.0872e-03,
         -1.1787e-03, -1.6098e-03],
        [ 3.1257e-04,  1.4086e-03, -1.4782e-03,  ..., -2.2850e-03,
         -2.4986e-03,  1.1039e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6211,  2.5664, -2.0879,  ...,  1.0576,  0.1143,  0.9746]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.5435,  0.0592,  0.2822,  ...,  0.0997, -0.1635,  0.2573]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 18:38:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for dollars is bucks
Another word for honest is sincere
Another word for harbor is seaport
Another word for clothes is clothing
Another word for new is modern
Another word for emphasis is accent
Another word for market is marketplace
Another word for flower is
2024-07-12 18:38:33 root INFO     [order_1_approx] starting weight calculation for Another word for new is modern
Another word for honest is sincere
Another word for harbor is seaport
Another word for market is marketplace
Another word for flower is blossom
Another word for emphasis is accent
Another word for clothes is clothing
Another word for dollars is
2024-07-12 18:38:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 18:42:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 18:46:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0291, -1.4648, -0.2715,  ...,  0.7822, -0.2720,  1.1094],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.0343, -1.4199, -0.2559,  ...,  0.6997, -0.2717,  1.0283],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0345, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5781, -0.7515, -5.6211,  ..., -3.4395,  1.4307,  1.8301],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0319,  0.0023, -0.0164,  ...,  0.0004, -0.0011,  0.0036],
        [ 0.0080, -0.0082,  0.0066,  ...,  0.0263, -0.0007, -0.0164],
        [ 0.0033,  0.0048, -0.0251,  ...,  0.0186,  0.0295, -0.0074],
        ...,
        [ 0.0079,  0.0040,  0.0022,  ..., -0.0116,  0.0120,  0.0221],
        [-0.0122,  0.0224, -0.0032,  ...,  0.0162, -0.0072, -0.0069],
        [-0.0189,  0.0186,  0.0035,  ...,  0.0075,  0.0112, -0.0069]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0002, -0.0017, -0.0015,  ..., -0.0027, -0.0013,  0.0004],
        [ 0.0001,  0.0017,  0.0013,  ...,  0.0002,  0.0014,  0.0011],
        [-0.0013, -0.0011,  0.0009,  ..., -0.0017,  0.0019,  0.0003],
        ...,
        [ 0.0025,  0.0011, -0.0020,  ..., -0.0014,  0.0019,  0.0006],
        [-0.0038,  0.0012, -0.0016,  ...,  0.0026, -0.0032, -0.0002],
        [-0.0004, -0.0003,  0.0001,  ...,  0.0023, -0.0020, -0.0002]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0195, -1.3281, -5.5391,  ..., -2.8711,  0.3125,  1.7148]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2249,  0.1969,  0.0243,  ...,  0.4507, -0.3164, -0.0502]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 18:46:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for new is modern
Another word for honest is sincere
Another word for harbor is seaport
Another word for market is marketplace
Another word for flower is blossom
Another word for emphasis is accent
Another word for clothes is clothing
Another word for dollars is
2024-07-12 18:46:28 root INFO     [order_1_approx] starting weight calculation for Another word for market is marketplace
Another word for dollars is bucks
Another word for emphasis is accent
Another word for flower is blossom
Another word for honest is sincere
Another word for new is modern
Another word for clothes is clothing
Another word for harbor is
2024-07-12 18:46:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 18:50:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 18:54:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1421, -0.3882, -1.5547,  ...,  0.0979, -0.6392,  1.1523],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1277, -0.3608, -1.2891,  ...,  0.0834, -0.5620,  0.9868],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0346, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0400,  1.3877, -4.1406,  ...,  0.4297,  0.8320,  2.7793],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0224, -0.0185, -0.0004,  ...,  0.0148, -0.0020, -0.0038],
        [-0.0082, -0.0068,  0.0054,  ...,  0.0089, -0.0074, -0.0017],
        [ 0.0120,  0.0043, -0.0008,  ..., -0.0110, -0.0042,  0.0045],
        ...,
        [-0.0166, -0.0035,  0.0071,  ...,  0.0090, -0.0015,  0.0187],
        [-0.0009,  0.0088,  0.0076,  ..., -0.0048, -0.0112, -0.0023],
        [ 0.0066, -0.0209, -0.0020,  ..., -0.0048,  0.0043,  0.0190]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.1305e-03, -1.2016e-03,  1.7490e-03,  ...,  9.1076e-04,
         -1.8549e-03,  1.1196e-03],
        [ 1.8871e-04, -1.5335e-03,  2.8973e-03,  ...,  1.7433e-03,
          3.3736e-04,  5.3120e-04],
        [-1.9369e-03, -2.2721e-04,  1.7128e-03,  ..., -1.8721e-03,
         -1.9226e-03, -2.9039e-04],
        ...,
        [ 4.3774e-04, -2.3651e-04, -1.7452e-03,  ..., -3.5048e-05,
          9.5797e-04, -1.3351e-03],
        [ 7.0667e-04, -3.9172e-04, -1.4563e-03,  ...,  5.7507e-04,
          1.4067e-03, -2.1420e-03],
        [ 1.2722e-03, -2.4281e-03, -1.1253e-04,  ..., -1.4334e-03,
         -1.8415e-03, -2.7657e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3545,  1.1748, -4.0039,  ...,  0.3921,  0.7480,  2.6797]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2218,  0.1052,  0.2097,  ...,  0.4316, -0.0607,  0.1652]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 18:54:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for market is marketplace
Another word for dollars is bucks
Another word for emphasis is accent
Another word for flower is blossom
Another word for honest is sincere
Another word for new is modern
Another word for clothes is clothing
Another word for harbor is
2024-07-12 18:54:25 root INFO     [order_1_approx] starting weight calculation for Another word for new is modern
Another word for clothes is clothing
Another word for emphasis is accent
Another word for honest is sincere
Another word for flower is blossom
Another word for harbor is seaport
Another word for dollars is bucks
Another word for market is
2024-07-12 18:54:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 18:58:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 19:02:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2163, -0.7656,  0.5723,  ...,  0.7412,  0.0518,  0.2488],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.2074, -0.7896,  0.5234,  ...,  0.7002,  0.0300,  0.2242],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0345, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.2109, -2.6914, -6.2109,  ...,  0.6343,  1.3135,  4.0195],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0329,  0.0015,  0.0225,  ..., -0.0077,  0.0099,  0.0084],
        [ 0.0015,  0.0172,  0.0052,  ...,  0.0252, -0.0152, -0.0031],
        [ 0.0081, -0.0033,  0.0012,  ..., -0.0089,  0.0239,  0.0308],
        ...,
        [-0.0025, -0.0043,  0.0195,  ..., -0.0030,  0.0055,  0.0195],
        [ 0.0075, -0.0037, -0.0028,  ..., -0.0255,  0.0130,  0.0159],
        [-0.0224, -0.0123, -0.0020,  ..., -0.0065, -0.0059,  0.0197]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 1.3561e-03, -4.9543e-04, -2.2774e-03,  ...,  1.1683e-03,
         -3.9177e-03,  1.6651e-03],
        [ 7.2002e-04, -2.3460e-03,  3.0804e-03,  ...,  7.2670e-04,
          3.8314e-04,  1.5020e-03],
        [-8.8501e-04, -1.3466e-03,  1.9131e-03,  ...,  1.1444e-05,
         -2.9964e-03,  2.6817e-03],
        ...,
        [ 3.4904e-04,  3.3684e-03, -1.6556e-03,  ..., -2.4548e-03,
         -4.2629e-04,  7.2765e-04],
        [ 2.2392e-03,  3.7766e-03, -1.5850e-03,  ..., -3.0422e-03,
          1.1845e-03, -1.3018e-03],
        [-1.4048e-03,  5.4932e-04,  3.9220e-05,  ...,  6.3539e-05,
         -7.0286e-04, -2.3136e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.7031, -1.5332, -6.6641,  ...,  2.3691,  1.7803,  3.4824]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1021,  0.0064,  0.0742,  ...,  0.2598, -0.0996,  0.3135]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 19:02:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for new is modern
Another word for clothes is clothing
Another word for emphasis is accent
Another word for honest is sincere
Another word for flower is blossom
Another word for harbor is seaport
Another word for dollars is bucks
Another word for market is
2024-07-12 19:02:21 root INFO     total operator prediction time: 3797.831615924835 seconds
2024-07-12 19:02:21 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on hypernyms - misc
2024-07-12 19:02:23 root INFO     building operator hypernyms - misc
2024-07-12 19:02:23 root INFO     [order_1_approx] starting weight calculation for The cup falls into the category of tableware
The perfume falls into the category of toiletry
The blender falls into the category of appliance
The brooch falls into the category of jewelry
The skirt falls into the category of clothes
The hairpin falls into the category of pin
The sidewalk falls into the category of walk
The jacket falls into the category of
2024-07-12 19:02:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 19:06:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 19:10:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4836, -0.6528, -0.4966,  ..., -0.0358, -0.4421,  0.8418],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.4338, -0.6250, -0.4419,  ..., -0.0284, -0.4148,  0.7534],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5464,  0.3748,  1.1709,  ..., -2.7617, -0.7358, -1.7705],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.5513e-02, -1.1299e-02,  7.4310e-03,  ...,  3.5191e-03,
         -1.6739e-02,  7.0038e-03],
        [-1.9012e-02,  1.0399e-02,  5.2681e-03,  ...,  7.8583e-03,
          3.5515e-03, -5.1689e-03],
        [ 5.8250e-03,  8.4763e-03, -7.7515e-03,  ...,  1.3657e-03,
         -4.7493e-04, -1.0834e-03],
        ...,
        [ 1.8692e-03,  8.3771e-03,  3.7384e-03,  ...,  1.6022e-04,
         -1.0216e-02,  1.0910e-02],
        [ 5.7526e-03,  8.9645e-03,  2.5043e-03,  ..., -7.2479e-05,
          1.0548e-03, -2.2411e-05],
        [-1.7319e-03,  1.0101e-02,  5.5199e-03,  ..., -8.8835e-04,
          7.6294e-03,  1.2642e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 6.4611e-04, -1.0605e-03,  2.0103e-03,  ...,  5.9128e-04,
          9.1410e-04,  9.5367e-06],
        [-4.7064e-04, -5.8174e-04,  1.5087e-03,  ...,  8.4448e-04,
         -5.2357e-04, -1.5144e-03],
        [ 5.5170e-04,  1.1425e-03,  6.3896e-04,  ...,  1.2989e-03,
          9.8515e-04, -5.6148e-05],
        ...,
        [ 8.8692e-04,  1.0986e-03, -9.0122e-05,  ..., -1.2188e-03,
          1.7667e-04,  1.4114e-03],
        [ 2.4338e-03,  3.6955e-04, -1.1292e-03,  ..., -1.0395e-03,
         -4.3321e-04,  1.0672e-03],
        [ 5.4073e-04,  1.1892e-03,  2.1517e-05,  ..., -5.3978e-04,
          6.5422e-04,  5.1498e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4578, -0.5293,  0.8076,  ..., -3.0527, -1.5273, -1.5693]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0751, -0.1359,  0.1121,  ...,  0.1583,  0.0853, -0.0871]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 19:10:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The cup falls into the category of tableware
The perfume falls into the category of toiletry
The blender falls into the category of appliance
The brooch falls into the category of jewelry
The skirt falls into the category of clothes
The hairpin falls into the category of pin
The sidewalk falls into the category of walk
The jacket falls into the category of
2024-07-12 19:10:21 root INFO     [order_1_approx] starting weight calculation for The perfume falls into the category of toiletry
The sidewalk falls into the category of walk
The brooch falls into the category of jewelry
The blender falls into the category of appliance
The jacket falls into the category of clothes
The hairpin falls into the category of pin
The cup falls into the category of tableware
The skirt falls into the category of
2024-07-12 19:10:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 19:14:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 19:18:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6001, -0.6572, -0.2339,  ..., -0.1982, -1.1455,  0.6045],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.5312, -0.6196, -0.2115,  ..., -0.1667, -1.0273,  0.5259],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5830,  0.8965, -1.4941,  ..., -3.2148, -1.4229, -2.0898],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.7893e-02, -9.4376e-03, -5.1231e-03,  ..., -2.9678e-03,
         -2.2011e-03, -1.5373e-03],
        [-1.9226e-02,  4.9057e-03,  3.6259e-03,  ...,  8.2397e-03,
         -7.0114e-03, -5.3291e-03],
        [-6.3019e-03,  3.0708e-03, -2.5711e-03,  ...,  7.5684e-03,
         -2.9907e-03, -2.9373e-04],
        ...,
        [-5.7106e-03,  4.3678e-03, -2.9945e-04,  ...,  9.9182e-05,
          4.1313e-03,  7.2403e-03],
        [ 1.8673e-03,  5.4626e-03,  2.8610e-03,  ..., -8.7585e-03,
         -1.5020e-03,  1.5297e-03],
        [ 2.6875e-03,  7.0953e-03,  3.3264e-03,  ...,  9.7046e-03,
          1.2291e-02,  1.7685e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.6011e-04, -1.0300e-03,  1.2980e-03,  ...,  2.6417e-04,
          5.2261e-04, -1.9646e-03],
        [-1.4820e-03, -1.4553e-03,  2.3155e-03,  ...,  1.3742e-03,
         -2.8324e-04, -5.2738e-04],
        [-1.1482e-03,  1.4305e-03, -3.2663e-05,  ...,  1.8311e-04,
         -1.0782e-04,  4.6492e-04],
        ...,
        [ 6.7472e-04,  6.0749e-04,  1.4896e-03,  ..., -5.9748e-04,
          1.5955e-03,  1.1845e-03],
        [ 1.2741e-03,  3.9220e-04, -5.3072e-04,  ..., -1.5383e-03,
         -5.6744e-04,  7.1621e-04],
        [ 1.0967e-05,  1.8129e-03, -1.9825e-04,  ..., -7.0572e-04,
         -7.4387e-04,  7.1716e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9092,  0.9058, -1.2812,  ..., -3.9727, -1.4131, -1.9023]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0088, -0.2220,  0.3308,  ...,  0.0684,  0.1379,  0.2435]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 19:18:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The perfume falls into the category of toiletry
The sidewalk falls into the category of walk
The brooch falls into the category of jewelry
The blender falls into the category of appliance
The jacket falls into the category of clothes
The hairpin falls into the category of pin
The cup falls into the category of tableware
The skirt falls into the category of
2024-07-12 19:18:19 root INFO     [order_1_approx] starting weight calculation for The blender falls into the category of appliance
The brooch falls into the category of jewelry
The cup falls into the category of tableware
The jacket falls into the category of clothes
The hairpin falls into the category of pin
The sidewalk falls into the category of walk
The skirt falls into the category of clothes
The perfume falls into the category of
2024-07-12 19:18:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 19:22:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 19:26:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3040,  0.3928, -0.9365,  ..., -1.1641,  0.8711, -0.9863],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.2561,  0.3274, -0.7847,  ..., -0.9609,  0.7192, -0.8945],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.9844, -2.3262, -0.5913,  ..., -4.9766, -2.2266, -2.3477],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.7395e-02, -3.6240e-04, -1.5121e-02,  ..., -2.2774e-03,
          1.4481e-02,  1.8349e-03],
        [ 1.8826e-03, -3.0365e-03, -2.2697e-03,  ...,  5.6076e-03,
         -6.6853e-04,  5.1308e-04],
        [-2.8934e-03,  1.4816e-02, -6.6223e-03,  ...,  7.7362e-03,
          1.7204e-03,  6.6299e-03],
        ...,
        [ 4.6463e-03,  2.2011e-03, -1.1520e-02,  ...,  1.9569e-03,
          7.2174e-03,  7.9036e-05],
        [ 1.4648e-02,  2.7485e-03,  3.3283e-03,  ..., -6.9199e-03,
          1.1200e-02, -7.4081e-03],
        [-6.8665e-04, -1.5984e-03, -2.1744e-04,  ..., -1.5411e-02,
          1.4526e-02,  1.3412e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.2827e-03,  1.3285e-03,  2.1553e-03,  ..., -1.1311e-03,
         -1.5259e-03,  4.7255e-04],
        [-7.5626e-04, -1.8759e-03, -1.8048e-04,  ...,  5.5838e-04,
         -5.1260e-04, -1.0805e-03],
        [-8.6546e-04,  1.0443e-03, -1.8835e-05,  ...,  1.0128e-03,
         -1.1663e-03, -7.4530e-04],
        ...,
        [ 2.0051e-04,  8.2445e-04,  8.3733e-04,  ..., -1.5221e-03,
          1.1120e-03, -8.1253e-04],
        [ 1.5411e-03,  1.8692e-03,  7.5817e-05,  ..., -1.6403e-03,
         -9.1171e-04,  1.2836e-03],
        [ 3.4571e-04,  3.3684e-03,  6.9046e-04,  ..., -2.3136e-03,
         -8.2350e-04, -5.0831e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.6953, -1.8242, -0.6670,  ..., -4.7070, -3.1602, -2.6152]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.2103, -0.0822,  0.0235,  ..., -0.0956,  0.1359, -0.0040]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 19:26:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The blender falls into the category of appliance
The brooch falls into the category of jewelry
The cup falls into the category of tableware
The jacket falls into the category of clothes
The hairpin falls into the category of pin
The sidewalk falls into the category of walk
The skirt falls into the category of clothes
The perfume falls into the category of
2024-07-12 19:26:18 root INFO     [order_1_approx] starting weight calculation for The cup falls into the category of tableware
The sidewalk falls into the category of walk
The brooch falls into the category of jewelry
The jacket falls into the category of clothes
The perfume falls into the category of toiletry
The skirt falls into the category of clothes
The hairpin falls into the category of pin
The blender falls into the category of
2024-07-12 19:26:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 19:30:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 19:34:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.3184,  0.5439, -1.7061,  ...,  0.3020, -0.0422,  0.0739],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-1.1348,  0.4578, -1.4053,  ...,  0.2498, -0.0561,  0.0389],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.2227,  2.1621, -0.2490,  ...,  1.9336, -1.0430, -1.6719],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0322,  0.0102,  0.0049,  ..., -0.0050,  0.0002,  0.0151],
        [-0.0149,  0.0037, -0.0029,  ...,  0.0231, -0.0095, -0.0099],
        [-0.0021,  0.0113,  0.0045,  ..., -0.0036,  0.0131,  0.0061],
        ...,
        [ 0.0002, -0.0207, -0.0024,  ...,  0.0156, -0.0088, -0.0035],
        [-0.0043,  0.0223,  0.0129,  ..., -0.0038,  0.0103,  0.0075],
        [ 0.0018,  0.0056,  0.0022,  ..., -0.0040,  0.0068,  0.0099]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.6232e-03,  7.7200e-04,  1.8568e-03,  ..., -1.8749e-03,
          5.7697e-04,  2.2850e-03],
        [ 3.9959e-04, -1.9798e-03,  2.3150e-04,  ...,  2.3556e-03,
         -6.1989e-04, -1.1911e-03],
        [ 2.8825e-04, -5.2452e-06, -1.4496e-04,  ...,  9.1028e-04,
          3.7670e-04,  4.0293e-04],
        ...,
        [ 7.6962e-04, -1.7338e-03,  9.2220e-04,  ..., -8.3685e-04,
          1.2283e-03,  1.0633e-03],
        [ 1.4515e-03,  2.5311e-03, -1.0357e-03,  ..., -1.6174e-03,
          1.5545e-04,  6.7854e-04],
        [ 7.0906e-04,  9.1791e-04, -1.7524e-04,  ..., -1.3142e-03,
         -2.1553e-04,  6.1154e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.9004,  1.2832, -0.3923,  ...,  1.0020, -0.8882, -1.2344]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.2466, -0.0733, -0.1133,  ...,  0.0216,  0.1237,  0.0801]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 19:34:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The cup falls into the category of tableware
The sidewalk falls into the category of walk
The brooch falls into the category of jewelry
The jacket falls into the category of clothes
The perfume falls into the category of toiletry
The skirt falls into the category of clothes
The hairpin falls into the category of pin
The blender falls into the category of
2024-07-12 19:34:14 root INFO     [order_1_approx] starting weight calculation for The hairpin falls into the category of pin
The skirt falls into the category of clothes
The blender falls into the category of appliance
The jacket falls into the category of clothes
The cup falls into the category of tableware
The perfume falls into the category of toiletry
The brooch falls into the category of jewelry
The sidewalk falls into the category of
2024-07-12 19:34:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 19:38:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 19:42:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3660, -0.4990, -0.3987,  ...,  0.9604,  0.5811, -1.5957],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.3435, -0.4897, -0.3635,  ...,  0.8438,  0.5024, -1.5195],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5801, -3.8828, -0.4941,  ..., -3.1055, -0.5669,  2.1777],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0136,  0.0073, -0.0006,  ...,  0.0018, -0.0076,  0.0110],
        [-0.0078,  0.0056,  0.0025,  ...,  0.0053,  0.0043, -0.0011],
        [ 0.0007,  0.0105, -0.0082,  ...,  0.0037,  0.0032,  0.0083],
        ...,
        [-0.0078, -0.0069,  0.0004,  ...,  0.0085,  0.0033,  0.0049],
        [ 0.0006,  0.0013,  0.0068,  ...,  0.0069,  0.0039, -0.0087],
        [-0.0072,  0.0035,  0.0022,  ...,  0.0054,  0.0065,  0.0128]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-8.4114e-04,  8.6927e-04,  1.5402e-04,  ..., -1.1635e-03,
         -9.3555e-04,  1.8578e-03],
        [-5.0068e-05, -7.0333e-04,  1.2140e-03,  ...,  2.1172e-03,
         -1.7619e-04, -1.8511e-03],
        [-1.5569e-04,  4.3249e-04,  5.0783e-05,  ...,  5.7411e-04,
         -1.0509e-03,  1.1301e-04],
        ...,
        [-5.1928e-04, -5.5885e-04,  2.6560e-04,  ..., -9.0647e-04,
          1.3676e-03, -4.3690e-05],
        [-2.3198e-04,  3.2783e-04,  5.6171e-04,  ...,  1.2875e-04,
         -7.9727e-04, -2.7084e-04],
        [-1.4877e-04, -4.9448e-04,  2.9325e-04,  ...,  2.9802e-04,
         -3.6526e-04, -8.6689e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0273, -3.6504, -1.5498,  ..., -3.8242, -1.3516,  2.3027]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0453, -0.2214,  0.0435,  ...,  0.0448, -0.1114,  0.1332]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 19:42:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The hairpin falls into the category of pin
The skirt falls into the category of clothes
The blender falls into the category of appliance
The jacket falls into the category of clothes
The cup falls into the category of tableware
The perfume falls into the category of toiletry
The brooch falls into the category of jewelry
The sidewalk falls into the category of
2024-07-12 19:42:11 root INFO     [order_1_approx] starting weight calculation for The brooch falls into the category of jewelry
The jacket falls into the category of clothes
The sidewalk falls into the category of walk
The perfume falls into the category of toiletry
The cup falls into the category of tableware
The skirt falls into the category of clothes
The blender falls into the category of appliance
The hairpin falls into the category of
2024-07-12 19:42:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 19:46:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 19:50:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2307,  0.4612,  0.2505,  ...,  1.3281,  0.3369,  0.5967],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.2206,  0.4280,  0.2140,  ...,  1.2021,  0.2942,  0.5493],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.6621, -0.3635, -3.0566,  ..., -2.0723, -0.6982,  4.7422],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 7.6065e-03,  9.7275e-03,  5.7220e-04,  ...,  1.3781e-03,
         -2.5673e-03, -4.9095e-03],
        [-4.6310e-03,  1.5152e-02,  4.3488e-04,  ...,  1.1780e-02,
         -1.0315e-02,  1.5854e-02],
        [ 3.7022e-03, -1.8616e-03, -9.0408e-04,  ..., -6.4201e-03,
          1.4992e-03,  2.2209e-04],
        ...,
        [ 3.2921e-03, -5.9242e-03, -1.2657e-02,  ...,  1.1627e-02,
         -2.0782e-02,  6.0577e-03],
        [-2.7199e-03,  1.4076e-03,  4.1428e-03,  ..., -6.6795e-03,
         -3.6354e-03,  1.7891e-03],
        [-1.1673e-02, -5.5466e-03, -1.5259e-05,  ...,  7.1526e-05,
          4.1046e-03,  8.1787e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.0166e-03,  1.7843e-03,  1.2712e-03,  ...,  5.0068e-04,
          7.6103e-04,  1.1482e-03],
        [-1.9932e-04, -1.3847e-03,  1.5187e-04,  ...,  1.6203e-03,
         -4.6873e-04, -1.9693e-04],
        [-2.1696e-05, -1.1864e-03, -1.3895e-03,  ...,  1.0080e-03,
         -2.1529e-04, -4.9353e-04],
        ...,
        [ 3.0231e-04, -1.4992e-03, -1.1616e-03,  ...,  5.2834e-04,
         -7.1287e-04, -7.4005e-04],
        [ 3.2496e-04,  2.0428e-03, -1.1754e-04,  ...,  3.0327e-04,
         -1.6189e-04, -4.5061e-04],
        [-1.8892e-03,  1.2436e-03,  2.5010e-04,  ..., -3.4165e-04,
         -1.1921e-03, -1.0484e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.4746, -1.2861, -3.0781,  ..., -3.1367, -0.6201,  4.4141]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0323, -0.1018, -0.0773,  ..., -0.1169,  0.0278, -0.0195]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 19:50:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The brooch falls into the category of jewelry
The jacket falls into the category of clothes
The sidewalk falls into the category of walk
The perfume falls into the category of toiletry
The cup falls into the category of tableware
The skirt falls into the category of clothes
The blender falls into the category of appliance
The hairpin falls into the category of
2024-07-12 19:50:06 root INFO     [order_1_approx] starting weight calculation for The sidewalk falls into the category of walk
The jacket falls into the category of clothes
The blender falls into the category of appliance
The brooch falls into the category of jewelry
The hairpin falls into the category of pin
The skirt falls into the category of clothes
The perfume falls into the category of toiletry
The cup falls into the category of
2024-07-12 19:50:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 19:54:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 19:58:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2627, -0.7773, -0.6875,  ...,  0.5479, -0.2316,  0.8799],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.2386, -0.7305, -0.5986,  ...,  0.4766, -0.2222,  0.7837],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.2754e+00, -2.8320e-01, -2.5586e+00,  ..., -3.9258e-01,
         2.4414e-03,  1.3184e+00], device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.7323e-02,  1.1932e-02,  3.2692e-03,  ...,  3.4771e-03,
          1.3199e-03,  7.3013e-03],
        [-2.0828e-02,  7.4692e-03, -5.0888e-03,  ...,  7.0610e-03,
         -6.2332e-03, -2.1362e-04],
        [ 5.4626e-03,  7.1335e-03,  1.1848e-02,  ...,  6.7062e-03,
          8.5831e-05,  2.8534e-03],
        ...,
        [-1.6464e-02, -1.1902e-02,  8.7433e-03,  ..., -2.7733e-03,
         -1.7548e-02, -1.4915e-03],
        [ 9.6970e-03,  4.2877e-03,  7.5798e-03,  ...,  6.7177e-03,
         -2.7676e-03, -5.8289e-03],
        [-2.6474e-03, -8.2779e-04, -3.3360e-03,  ...,  5.8327e-03,
         -6.7520e-04,  5.8289e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0006,  0.0008,  0.0017,  ...,  0.0005,  0.0008,  0.0023],
        [ 0.0005, -0.0012,  0.0004,  ...,  0.0010, -0.0009, -0.0027],
        [ 0.0002,  0.0007, -0.0012,  ..., -0.0001, -0.0002,  0.0012],
        ...,
        [-0.0002, -0.0021,  0.0009,  ...,  0.0009,  0.0008, -0.0025],
        [ 0.0020,  0.0014, -0.0008,  ...,  0.0005, -0.0004, -0.0002],
        [-0.0003, -0.0001, -0.0007,  ..., -0.0008, -0.0006, -0.0013]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2852, -0.7129, -2.5879,  ..., -1.1113,  0.2646,  1.2090]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0573,  0.0342,  0.0803,  ..., -0.0996,  0.1729, -0.0364]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 19:58:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sidewalk falls into the category of walk
The jacket falls into the category of clothes
The blender falls into the category of appliance
The brooch falls into the category of jewelry
The hairpin falls into the category of pin
The skirt falls into the category of clothes
The perfume falls into the category of toiletry
The cup falls into the category of
2024-07-12 19:58:06 root INFO     [order_1_approx] starting weight calculation for The jacket falls into the category of clothes
The perfume falls into the category of toiletry
The cup falls into the category of tableware
The sidewalk falls into the category of walk
The blender falls into the category of appliance
The hairpin falls into the category of pin
The skirt falls into the category of clothes
The brooch falls into the category of
2024-07-12 19:58:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 20:02:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 20:06:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0923, -0.2559,  0.0878,  ...,  0.2891,  0.4338,  0.0717],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.0743, -0.2349,  0.0596,  ...,  0.2361,  0.3391,  0.0384],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.6680, -2.7637,  0.6201,  ..., -5.8164, -3.0195,  4.3984],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.2995e-02, -5.7144e-03,  5.5161e-03,  ..., -6.0959e-03,
          3.7842e-03,  2.5501e-03],
        [ 1.4763e-03,  1.3512e-02,  7.6599e-03,  ...,  8.9111e-03,
         -1.0881e-03,  9.2316e-03],
        [ 3.8605e-03, -1.5039e-03,  7.4348e-03,  ...,  3.6907e-03,
          2.4033e-03, -4.7302e-04],
        ...,
        [ 6.0081e-04, -4.2725e-03, -5.3101e-03,  ...,  5.0240e-03,
         -1.3504e-03,  6.9313e-03],
        [ 6.1722e-03, -1.8749e-03, -3.9101e-04,  ...,  5.8174e-05,
          4.2343e-04,  6.1722e-03],
        [-1.5198e-02, -3.5915e-03,  6.5460e-03,  ...,  2.0256e-03,
         -1.5450e-04,  5.1498e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.3514e-03,  3.4523e-04,  1.6174e-03,  ..., -7.0906e-04,
         -3.1257e-04,  7.7629e-04],
        [-1.7986e-03, -1.1368e-03,  1.5545e-03,  ...,  7.7724e-04,
          9.9754e-04, -7.2002e-04],
        [-6.4945e-04,  4.1485e-04,  4.3750e-04,  ...,  1.0252e-04,
         -4.2439e-04, -5.4359e-04],
        ...,
        [ 5.7316e-04, -4.3964e-04, -9.6035e-04,  ..., -1.4839e-03,
          1.0567e-03,  1.0252e-04],
        [ 1.2398e-03, -3.7241e-04, -3.8433e-04,  ...,  5.2977e-04,
         -4.0126e-04, -6.4850e-04],
        [-1.6451e-03, -1.1463e-03, -2.1410e-04,  ..., -1.3769e-05,
          4.8470e-04, -1.7242e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.9180, -3.4062,  1.2832,  ..., -5.5820, -3.1602,  3.9512]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-4.5593e-02, -3.7048e-02, -2.4384e-02,  ...,  6.7810e-02,
          4.2908e-02, -1.9073e-05]], device='cuda:1', dtype=torch.float16,
       grad_fn=<SubBackward0>)
                    
2024-07-12 20:06:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The jacket falls into the category of clothes
The perfume falls into the category of toiletry
The cup falls into the category of tableware
The sidewalk falls into the category of walk
The blender falls into the category of appliance
The hairpin falls into the category of pin
The skirt falls into the category of clothes
The brooch falls into the category of
2024-07-12 20:06:03 root INFO     total operator prediction time: 3820.477007627487 seconds
2024-07-12 20:06:03 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - substance
2024-07-12 20:06:06 root INFO     building operator meronyms - substance
2024-07-12 20:06:06 root INFO     [order_1_approx] starting weight calculation for A bronze is made up of copper
A ice is made up of water
A icicle is made up of ice
A boots is made up of leather
A spoon is made up of aluminium
A wig is made up of hair
A water is made up of oxygen
A pill is made up of
2024-07-12 20:06:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 20:10:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 20:13:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1438,  0.2012, -1.0898,  ..., -0.6284,  0.1951, -0.0444],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1328,  0.1683, -0.9448,  ..., -0.5366,  0.1544, -0.0640],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.6738, -1.7227,  2.5938,  ..., -4.8711,  0.9238,  2.7578],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.5182e-03,  4.0359e-03,  1.3062e-02,  ..., -1.0803e-02,
         -5.2185e-03, -1.3084e-02],
        [-8.5754e-03, -8.4229e-03,  1.2264e-03,  ...,  3.8147e-06,
         -2.9755e-03, -6.1321e-04],
        [-5.4741e-03, -8.5688e-04, -6.2256e-03,  ..., -1.1473e-03,
          8.1100e-03, -5.1212e-04],
        ...,
        [ 2.7084e-03,  3.3054e-03,  1.3268e-02,  ..., -1.1673e-02,
          2.7161e-03, -1.2238e-02],
        [ 3.4790e-03, -9.0866e-03,  1.4145e-02,  ..., -1.1520e-02,
         -1.3153e-02, -1.0826e-02],
        [-5.7678e-03, -5.1117e-03,  2.9659e-03,  ..., -1.5038e-02,
          2.1191e-03, -6.5804e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.7227e-04, -8.4734e-04,  2.2755e-03,  ...,  2.4843e-04,
         -1.4901e-06, -1.2140e-03],
        [-3.3236e-04, -1.0180e-04,  5.2357e-04,  ..., -4.0245e-04,
          1.1575e-04,  4.5156e-04],
        [ 3.6907e-04, -7.8058e-04, -1.6201e-04,  ...,  9.2316e-04,
          2.1935e-04, -4.4227e-05],
        ...,
        [-1.1368e-03,  1.5478e-03,  2.5578e-03,  ...,  6.1512e-04,
          7.1716e-04, -9.5892e-04],
        [ 1.5287e-03,  1.2946e-04,  1.0216e-04,  ..., -4.7588e-04,
          1.1196e-03,  9.9754e-04],
        [ 9.1851e-05,  4.4274e-04, -1.0252e-03,  ..., -3.0637e-05,
         -4.1294e-04,  7.7629e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.7500, -1.6201,  2.2441,  ..., -4.9609,  0.8813,  2.9570]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1990,  0.2062,  0.0570,  ..., -0.0094, -0.0431,  0.0994]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 20:14:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A bronze is made up of copper
A ice is made up of water
A icicle is made up of ice
A boots is made up of leather
A spoon is made up of aluminium
A wig is made up of hair
A water is made up of oxygen
A pill is made up of
2024-07-12 20:14:00 root INFO     [order_1_approx] starting weight calculation for A boots is made up of leather
A spoon is made up of aluminium
A pill is made up of medicine
A water is made up of oxygen
A wig is made up of hair
A icicle is made up of ice
A bronze is made up of copper
A ice is made up of
2024-07-12 20:14:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 20:18:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 20:21:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0736, -0.1917, -0.1665,  ..., -0.1011,  0.6729, -0.0017],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.0701, -0.2045, -0.1625,  ..., -0.0873,  0.6260, -0.0226],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3809,  0.5068,  2.1191,  ..., -2.6875,  0.1729, -1.2109],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.4902e-02, -1.3260e-02, -4.6387e-03,  ..., -6.4964e-03,
          8.3923e-05, -3.7117e-03],
        [-1.3512e-02,  9.2010e-03,  5.5847e-03,  ..., -2.3174e-04,
         -5.2071e-03,  2.4452e-03],
        [ 2.3132e-02, -1.2260e-02, -3.5439e-03,  ..., -8.6823e-03,
          1.0117e-02,  4.7302e-04],
        ...,
        [-3.3035e-03, -9.9640e-03, -1.4553e-03,  ..., -2.0161e-03,
         -6.6376e-04,  2.3956e-03],
        [ 1.2016e-03,  1.0023e-03,  6.7825e-03,  ..., -7.2708e-03,
         -1.2039e-02, -2.8801e-04],
        [-2.3937e-03, -1.0651e-02,  1.1139e-02,  ...,  8.9264e-04,
          7.4959e-03,  2.9259e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 2.1610e-03,  4.8923e-04,  3.8505e-04,  ..., -2.8086e-04,
         -2.2469e-03,  2.6703e-03],
        [-5.3310e-04,  2.1572e-03, -1.3828e-04,  ..., -6.2275e-04,
          1.6022e-03,  2.1100e-04],
        [ 9.2459e-04, -4.1342e-04,  1.2493e-04,  ...,  9.9182e-04,
         -1.7672e-03,  1.0710e-03],
        ...,
        [-1.4820e-03, -7.3051e-04, -1.2045e-03,  ..., -1.4362e-03,
          3.0231e-03, -1.0185e-03],
        [ 1.0262e-03,  1.3237e-03,  2.3580e-04,  ...,  6.0081e-04,
         -3.0460e-03,  3.2473e-04],
        [-6.5994e-04, -2.5234e-03,  2.8763e-03,  ..., -6.4039e-04,
          1.6336e-03, -9.2745e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.0332,  0.4438,  2.3105,  ..., -2.4375, -0.0286, -1.7148]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1959,  0.1394,  0.2028,  ..., -0.1685,  0.0665, -0.0313]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 20:21:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A boots is made up of leather
A spoon is made up of aluminium
A pill is made up of medicine
A water is made up of oxygen
A wig is made up of hair
A icicle is made up of ice
A bronze is made up of copper
A ice is made up of
2024-07-12 20:21:56 root INFO     [order_1_approx] starting weight calculation for A wig is made up of hair
A boots is made up of leather
A pill is made up of medicine
A icicle is made up of ice
A water is made up of oxygen
A spoon is made up of aluminium
A ice is made up of water
A bronze is made up of
2024-07-12 20:21:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 20:25:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 20:29:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9287, -0.3408, -0.4038,  ...,  0.3662, -0.6724,  0.0244],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.8716, -0.3440, -0.3716,  ...,  0.3291, -0.6372, -0.0021],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4727,  2.0879, -0.4673,  ..., -4.2266,  0.7529, -0.7915],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0018, -0.0055, -0.0128,  ..., -0.0024,  0.0055, -0.0139],
        [-0.0123,  0.0094,  0.0061,  ...,  0.0090, -0.0030,  0.0036],
        [ 0.0193,  0.0079,  0.0094,  ..., -0.0028,  0.0211, -0.0034],
        ...,
        [ 0.0059, -0.0013,  0.0125,  ..., -0.0007, -0.0078, -0.0014],
        [ 0.0061, -0.0092, -0.0047,  ..., -0.0062, -0.0095, -0.0105],
        [-0.0126,  0.0002,  0.0085,  ...,  0.0080,  0.0015, -0.0005]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-8.2731e-04, -6.7043e-04, -1.1292e-03,  ...,  1.8430e-04,
         -7.2908e-04, -1.0204e-03],
        [-4.2653e-04, -9.0265e-04,  9.1362e-04,  ..., -2.9993e-04,
          1.0777e-03, -1.2474e-03],
        [-1.0481e-03,  2.2650e-04,  3.6764e-04,  ...,  1.1206e-03,
          1.0288e-04, -8.7023e-04],
        ...,
        [ 1.2245e-03,  7.3576e-04,  3.3188e-04,  ..., -7.1096e-04,
          1.0815e-03, -9.9373e-04],
        [ 1.4246e-04,  1.5583e-03,  5.3763e-05,  ..., -8.8692e-04,
         -1.5221e-03,  8.9645e-04],
        [ 5.9128e-04,  5.4121e-04,  5.8651e-04,  ..., -1.0490e-03,
         -1.4238e-03,  4.6587e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5791,  1.3867,  0.4019,  ..., -3.7656,  1.4473, -1.0879]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0548,  0.0513, -0.0710,  ..., -0.1478, -0.0890,  0.0020]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 20:29:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A wig is made up of hair
A boots is made up of leather
A pill is made up of medicine
A icicle is made up of ice
A water is made up of oxygen
A spoon is made up of aluminium
A ice is made up of water
A bronze is made up of
2024-07-12 20:29:49 root INFO     [order_1_approx] starting weight calculation for A wig is made up of hair
A ice is made up of water
A spoon is made up of aluminium
A pill is made up of medicine
A bronze is made up of copper
A water is made up of oxygen
A boots is made up of leather
A icicle is made up of
2024-07-12 20:29:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 20:33:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 20:37:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4326,  0.6152, -1.7773,  ..., -0.1520,  0.8135,  1.7930],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3760,  0.5352, -1.4912,  ..., -0.1219,  0.6831,  1.5762],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([3.2285, 2.4082, 1.0557,  ..., 0.6392, 3.2227, 0.1455], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.8090e-03, -1.8520e-03,  2.6512e-03,  ...,  2.1591e-03,
          7.8583e-04, -3.0403e-03],
        [-1.6357e-02, -5.6410e-04, -6.9046e-03,  ...,  9.6130e-04,
          5.2910e-03, -2.5215e-03],
        [ 1.3435e-02,  5.5885e-03,  4.2801e-03,  ...,  4.6692e-03,
          5.4932e-03, -3.2444e-03],
        ...,
        [-3.9520e-03, -1.8311e-04,  6.8893e-03,  ...,  2.6073e-03,
         -8.1787e-03,  7.9422e-03],
        [-2.1553e-03,  5.6419e-03, -2.4796e-05,  ...,  4.4823e-03,
         -5.9128e-03, -5.8861e-03],
        [-1.1429e-02,  2.4586e-03,  3.9024e-03,  ..., -4.6654e-03,
          3.0518e-05, -2.6474e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-5.8365e-04,  4.0245e-04, -1.4114e-04,  ..., -8.8120e-04,
         -5.1117e-04,  6.1274e-04],
        [-1.2188e-03, -1.7226e-04, -1.9407e-04,  ...,  9.0790e-04,
         -1.0091e-04, -5.2595e-04],
        [ 1.6365e-03, -1.0347e-03,  8.1921e-04,  ...,  1.1215e-03,
         -6.7830e-05, -5.4932e-04],
        ...,
        [-7.5579e-04,  2.4724e-04,  1.9855e-03,  ..., -2.1386e-04,
         -2.7061e-05, -8.6641e-04],
        [-2.1362e-04,  1.3342e-03, -1.3947e-04,  ..., -3.5667e-04,
         -1.4887e-03,  7.0429e-04],
        [-1.5764e-03,  2.3818e-04,  1.0471e-03,  ..., -6.6817e-05,
         -3.9935e-05, -7.9274e-06]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[2.9688, 2.2344, 0.8584,  ..., 0.9385, 3.1289, 0.1487]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0391, -0.0479, -0.0573,  ..., -0.0978,  0.0508,  0.1153]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 20:37:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A wig is made up of hair
A ice is made up of water
A spoon is made up of aluminium
A pill is made up of medicine
A bronze is made up of copper
A water is made up of oxygen
A boots is made up of leather
A icicle is made up of
2024-07-12 20:37:45 root INFO     [order_1_approx] starting weight calculation for A bronze is made up of copper
A icicle is made up of ice
A pill is made up of medicine
A ice is made up of water
A wig is made up of hair
A spoon is made up of aluminium
A water is made up of oxygen
A boots is made up of
2024-07-12 20:37:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 20:41:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 20:45:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4985, -0.3618, -0.9453,  ...,  1.9551, -1.5420, -0.3787],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.4727, -0.3723, -0.8745,  ...,  1.7949, -1.4727, -0.3938],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9414,  1.3604, -0.9585,  ...,  0.6455, -3.1035, -4.7656],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0044, -0.0077, -0.0040,  ..., -0.0057,  0.0069, -0.0029],
        [-0.0171, -0.0091,  0.0092,  ..., -0.0025,  0.0156,  0.0080],
        [ 0.0274, -0.0020, -0.0036,  ...,  0.0040, -0.0041,  0.0127],
        ...,
        [-0.0002,  0.0096,  0.0036,  ..., -0.0101, -0.0118,  0.0164],
        [ 0.0164, -0.0039,  0.0053,  ...,  0.0016, -0.0196, -0.0017],
        [-0.0043, -0.0219, -0.0077,  ..., -0.0010,  0.0072, -0.0037]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.1052e-04,  2.1019e-03, -1.4830e-04,  ..., -4.0674e-04,
          2.2049e-03,  1.6060e-03],
        [-7.5293e-04, -2.3022e-03,  1.9341e-03,  ...,  8.1444e-04,
          3.6776e-05,  1.4329e-04],
        [ 1.0538e-03, -1.2803e-04, -1.3180e-03,  ...,  5.1975e-04,
         -4.8041e-04,  5.5838e-04],
        ...,
        [ 7.7724e-05,  1.0443e-04, -1.2159e-05,  ..., -8.1873e-04,
         -2.1935e-05,  4.0269e-04],
        [ 1.4162e-03, -1.1406e-03,  2.4152e-04,  ..., -1.5087e-03,
         -3.4065e-03, -3.1900e-04],
        [ 7.4530e-04, -5.8174e-04, -1.2245e-03,  ..., -1.4849e-03,
         -1.2016e-03,  3.2163e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1045,  1.5625, -1.0332,  ...,  0.3635, -3.4043, -4.6797]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0871,  0.0572,  0.2698,  ...,  0.0978,  0.0556,  0.0750]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 20:45:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A bronze is made up of copper
A icicle is made up of ice
A pill is made up of medicine
A ice is made up of water
A wig is made up of hair
A spoon is made up of aluminium
A water is made up of oxygen
A boots is made up of
2024-07-12 20:45:41 root INFO     [order_1_approx] starting weight calculation for A spoon is made up of aluminium
A pill is made up of medicine
A bronze is made up of copper
A ice is made up of water
A icicle is made up of ice
A water is made up of oxygen
A boots is made up of leather
A wig is made up of
2024-07-12 20:45:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 20:49:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 20:53:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2461, -0.4990, -1.0596,  ...,  1.0977, -0.8857,  1.1445],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-1.0732, -0.4548, -0.8774,  ...,  0.9067, -0.7666,  0.9785],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.2695, -0.8926, -0.6069,  ...,  2.7969, -0.7427, -0.8677],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0216, -0.0054, -0.0073,  ...,  0.0007,  0.0083, -0.0053],
        [-0.0126, -0.0052,  0.0166,  ...,  0.0051,  0.0045,  0.0095],
        [ 0.0130,  0.0025, -0.0048,  ...,  0.0104,  0.0102,  0.0069],
        ...,
        [-0.0041, -0.0010,  0.0001,  ...,  0.0013, -0.0072, -0.0010],
        [ 0.0062, -0.0054,  0.0025,  ...,  0.0072, -0.0039,  0.0024],
        [-0.0019,  0.0007,  0.0081,  ..., -0.0063,  0.0214,  0.0007]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 5.6744e-05,  1.3561e-03, -1.6556e-03,  ..., -9.8705e-04,
         -1.2169e-03, -9.7179e-04],
        [ 2.2554e-04, -1.3905e-03,  1.2646e-03,  ...,  2.4700e-04,
          8.5402e-04,  9.0790e-04],
        [-7.6294e-04,  1.8787e-04, -1.1215e-03,  ..., -2.4319e-05,
         -1.3237e-03, -2.7800e-04],
        ...,
        [-4.4584e-05, -1.0338e-03,  4.2439e-04,  ..., -7.0190e-04,
          1.0900e-03,  5.1451e-04],
        [ 1.4191e-03,  6.5851e-04, -3.2806e-04,  ...,  6.3419e-05,
         -4.5919e-04,  6.5994e-04],
        [-8.9025e-04,  5.7125e-04,  5.8937e-04,  ...,  2.2936e-04,
         -1.7405e-03,  2.4128e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.5684, -0.9888, -0.5171,  ...,  2.5215, -1.4434, -1.3594]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1163, -0.0612,  0.0108,  ..., -0.0978,  0.1859, -0.0148]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 20:53:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A spoon is made up of aluminium
A pill is made up of medicine
A bronze is made up of copper
A ice is made up of water
A icicle is made up of ice
A water is made up of oxygen
A boots is made up of leather
A wig is made up of
2024-07-12 20:53:36 root INFO     [order_1_approx] starting weight calculation for A pill is made up of medicine
A icicle is made up of ice
A ice is made up of water
A boots is made up of leather
A wig is made up of hair
A bronze is made up of copper
A water is made up of oxygen
A spoon is made up of
2024-07-12 20:53:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 20:57:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 21:01:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0219,  0.0128, -0.3096,  ...,  1.4775,  0.2002,  0.4829],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.0150, -0.0061, -0.2747,  ...,  1.2637,  0.1564,  0.4141],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.8828,  2.1211, -0.2144,  ...,  1.4375,  2.4453,  1.3525],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0217, -0.0004,  0.0011,  ..., -0.0137,  0.0036, -0.0095],
        [-0.0235,  0.0120, -0.0034,  ...,  0.0024, -0.0104,  0.0206],
        [ 0.0209, -0.0031,  0.0091,  ...,  0.0100,  0.0150, -0.0101],
        ...,
        [-0.0064, -0.0120,  0.0002,  ...,  0.0002, -0.0024,  0.0108],
        [ 0.0099, -0.0045,  0.0059,  ..., -0.0017, -0.0070, -0.0172],
        [ 0.0017, -0.0002, -0.0096,  ..., -0.0003,  0.0161, -0.0018]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.7214e-03,  2.1534e-03,  9.5463e-04,  ..., -8.9502e-04,
         -2.1946e-04,  1.1501e-03],
        [-5.3835e-04, -6.4135e-05, -9.2411e-04,  ..., -1.9121e-04,
          1.2035e-03, -5.8270e-04],
        [-2.7657e-04, -1.1730e-03, -9.2220e-04,  ...,  1.2274e-03,
         -1.0175e-04, -1.0309e-03],
        ...,
        [-6.1607e-04, -6.8188e-05, -4.6277e-04,  ...,  9.6512e-04,
          1.7757e-03,  2.8849e-05],
        [-3.8075e-04,  2.0828e-03,  2.0933e-04,  ..., -9.6560e-04,
         -1.6832e-03,  2.7847e-04],
        [-5.3596e-04,  5.6124e-04, -2.0885e-03,  ..., -9.6703e-04,
         -6.1607e-04,  1.0023e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.0723,  1.0107, -0.3657,  ...,  1.0371,  1.8340,  1.3613]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0465, -0.0012,  0.2644,  ..., -0.0595, -0.0765,  0.2074]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 21:01:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A pill is made up of medicine
A icicle is made up of ice
A ice is made up of water
A boots is made up of leather
A wig is made up of hair
A bronze is made up of copper
A water is made up of oxygen
A spoon is made up of
2024-07-12 21:01:32 root INFO     [order_1_approx] starting weight calculation for A pill is made up of medicine
A icicle is made up of ice
A wig is made up of hair
A bronze is made up of copper
A spoon is made up of aluminium
A ice is made up of water
A boots is made up of leather
A water is made up of
2024-07-12 21:01:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 21:05:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 21:09:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2012, -0.0444, -0.1259,  ..., -0.1033, -0.1567,  0.4121],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.2008, -0.0622, -0.1317,  ..., -0.0952, -0.1738,  0.4023],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.0625,  0.6289,  0.8760,  ..., -0.5068, -1.7500, -1.7100],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.0086e-02, -2.2163e-03, -9.2468e-03,  ..., -6.9351e-03,
          7.7171e-03, -3.8261e-03],
        [-2.6657e-02,  6.2790e-03,  4.3869e-03,  ...,  1.0986e-02,
         -2.9297e-03, -7.2098e-04],
        [ 2.1530e-02, -7.2403e-03, -1.6861e-03,  ...,  3.2501e-03,
          7.5645e-03, -3.4332e-04],
        ...,
        [-5.9586e-03, -1.2894e-03,  1.0719e-03,  ...,  4.4785e-03,
         -5.8975e-03,  4.7607e-03],
        [-5.9509e-04, -7.6294e-05,  1.0910e-02,  ...,  4.3488e-04,
         -3.5191e-03, -5.1537e-03],
        [ 3.7231e-03,  5.6267e-04,  1.2115e-02,  ..., -7.8506e-03,
         -1.0956e-02,  1.1978e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 8.8406e-04,  1.9588e-03, -8.5688e-04,  ...,  2.0409e-04,
         -2.4104e-04,  1.2970e-03],
        [-1.8492e-03, -8.5449e-04, -8.6689e-04,  ...,  1.1206e-05,
          1.1301e-03, -2.1515e-03],
        [-8.2111e-04, -4.3535e-04,  9.8705e-05,  ...,  1.7624e-03,
         -1.3752e-03, -7.2002e-04],
        ...,
        [-4.9877e-04, -1.3790e-03, -9.6750e-04,  ..., -4.5395e-04,
          1.8225e-03, -8.7500e-04],
        [-2.3556e-04,  4.3030e-03,  2.2411e-03,  ...,  1.1368e-03,
         -3.3498e-04, -1.3285e-03],
        [-8.1253e-04,  6.7377e-04,  2.5063e-03,  ...,  1.4305e-03,
         -2.2240e-03,  1.3123e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.9062,  1.1055,  0.8184,  ..., -0.6416, -1.8721, -1.5664]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2255,  0.0015,  0.1873,  ..., -0.1103,  0.0436,  0.1848]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 21:09:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A pill is made up of medicine
A icicle is made up of ice
A wig is made up of hair
A bronze is made up of copper
A spoon is made up of aluminium
A ice is made up of water
A boots is made up of leather
A water is made up of
2024-07-12 21:09:25 root INFO     total operator prediction time: 3799.1251318454742 seconds
2024-07-12 21:09:25 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on synonyms - intensity
2024-07-12 21:09:27 root INFO     building operator synonyms - intensity
2024-07-12 21:09:27 root INFO     [order_1_approx] starting weight calculation for A more intense word for doze is sleep
A more intense word for guilty is remorseful
A more intense word for love is adore
A more intense word for irritate is enrage
A more intense word for pony is horse
A more intense word for creative is ingenious
A more intense word for tasty is delicious
A more intense word for snack is
2024-07-12 21:09:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 21:13:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 21:17:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6914, -0.3132, -0.3914,  ..., -0.4854,  0.2395,  0.2076],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.6138, -0.3074, -0.3484,  ..., -0.4153,  0.1903,  0.1636],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.7520,  5.3438, -4.0195,  ..., -0.7583,  3.7109, -4.4883],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 8.6365e-03,  9.1553e-05,  9.4070e-03,  ..., -1.5457e-02,
          1.1620e-02, -8.7585e-03],
        [-2.2095e-02,  2.5482e-03, -4.3488e-04,  ...,  1.0887e-02,
         -4.9835e-02,  2.8137e-02],
        [ 5.0850e-03,  3.1738e-03, -1.4153e-03,  ..., -4.5776e-03,
          1.8158e-02, -1.1475e-02],
        ...,
        [ 1.0666e-02, -1.9150e-03,  6.5193e-03,  ..., -7.7095e-03,
          3.4332e-03,  6.0387e-03],
        [-5.8899e-03, -2.9049e-03,  8.2703e-03,  ...,  7.1487e-03,
          4.0436e-03,  1.1398e-02],
        [-9.0485e-03,  2.3232e-03, -1.5289e-02,  ..., -1.3519e-02,
         -2.3422e-03, -8.3923e-05]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0008, -0.0003, -0.0004,  ..., -0.0025, -0.0004,  0.0008],
        [-0.0003,  0.0007, -0.0015,  ...,  0.0009,  0.0013,  0.0018],
        [ 0.0017, -0.0002, -0.0016,  ..., -0.0044,  0.0010, -0.0012],
        ...,
        [ 0.0022,  0.0020, -0.0008,  ..., -0.0028, -0.0004,  0.0026],
        [-0.0019,  0.0001, -0.0004,  ...,  0.0005, -0.0010,  0.0009],
        [ 0.0020, -0.0004, -0.0007,  ..., -0.0001, -0.0002, -0.0015]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.2246,  3.8457, -2.0176,  ...,  1.3828,  2.9961, -3.9336]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0359,  0.2148,  0.5137,  ...,  0.2568, -0.1578,  0.0353]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 21:17:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for doze is sleep
A more intense word for guilty is remorseful
A more intense word for love is adore
A more intense word for irritate is enrage
A more intense word for pony is horse
A more intense word for creative is ingenious
A more intense word for tasty is delicious
A more intense word for snack is
2024-07-12 21:17:25 root INFO     [order_1_approx] starting weight calculation for A more intense word for guilty is remorseful
A more intense word for pony is horse
A more intense word for doze is sleep
A more intense word for irritate is enrage
A more intense word for love is adore
A more intense word for tasty is delicious
A more intense word for snack is meal
A more intense word for creative is
2024-07-12 21:17:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 21:21:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 21:25:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9390, -0.6768,  0.0939,  ...,  0.3872,  0.3477, -0.2573],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.8833, -0.6768,  0.0705,  ...,  0.3523,  0.3027, -0.2754],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5498, -4.3164, -4.4336,  ...,  0.0557,  0.2659, -1.7686],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0109, -0.0161,  0.0109,  ...,  0.0140, -0.0061, -0.0030],
        [-0.0022,  0.0075, -0.0132,  ...,  0.0104,  0.0003,  0.0031],
        [ 0.0006,  0.0224, -0.0042,  ..., -0.0178,  0.0018,  0.0138],
        ...,
        [-0.0074, -0.0026,  0.0283,  ..., -0.0046, -0.0047,  0.0195],
        [ 0.0004,  0.0132, -0.0101,  ...,  0.0104,  0.0040, -0.0019],
        [ 0.0012, -0.0020, -0.0067,  ..., -0.0113,  0.0124,  0.0118]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.3628e-03, -1.2560e-03, -1.3475e-03,  ...,  4.7016e-04,
         -7.1096e-04, -1.9741e-03],
        [ 2.7156e-04,  4.0531e-04,  2.4624e-03,  ...,  8.8072e-04,
          2.1362e-04,  3.6678e-03],
        [ 2.3794e-04, -2.5749e-05, -3.1209e-04,  ..., -1.5030e-03,
         -1.3351e-04,  1.5287e-03],
        ...,
        [ 9.8705e-04,  8.5735e-04, -7.8106e-04,  ..., -8.7070e-04,
         -5.6648e-04, -4.3130e-04],
        [-1.2474e-03,  8.0967e-04, -1.9064e-03,  ...,  7.3242e-04,
          8.9073e-04,  1.3781e-03],
        [-1.1206e-04,  9.5415e-04,  1.8797e-03,  ..., -7.7677e-04,
         -1.6737e-03,  3.1624e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3574, -4.8867, -4.5391,  ...,  0.9277,  0.5273, -2.6289]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.2009,  0.3430, -0.0920,  ...,  0.1714, -0.0201,  0.2539]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 21:25:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for guilty is remorseful
A more intense word for pony is horse
A more intense word for doze is sleep
A more intense word for irritate is enrage
A more intense word for love is adore
A more intense word for tasty is delicious
A more intense word for snack is meal
A more intense word for creative is
2024-07-12 21:25:24 root INFO     [order_1_approx] starting weight calculation for A more intense word for doze is sleep
A more intense word for irritate is enrage
A more intense word for creative is ingenious
A more intense word for love is adore
A more intense word for snack is meal
A more intense word for pony is horse
A more intense word for guilty is remorseful
A more intense word for tasty is
2024-07-12 21:25:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 21:29:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 21:33:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0737,  0.0151, -0.0297,  ..., -0.5962,  0.0786, -0.6855],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.0597, -0.0067, -0.0404,  ..., -0.5190,  0.0483, -0.6631],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.9082,  0.7637,  0.7109,  ..., -0.1045,  3.0312, -1.9551],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0136, -0.0176,  0.0208,  ...,  0.0039,  0.0117, -0.0172],
        [-0.0008, -0.0067, -0.0050,  ..., -0.0040, -0.0159,  0.0163],
        [-0.0229,  0.0064,  0.0176,  ..., -0.0155,  0.0006,  0.0249],
        ...,
        [-0.0385, -0.0202,  0.0341,  ...,  0.0110, -0.0039,  0.0213],
        [ 0.0027,  0.0082, -0.0076,  ...,  0.0014,  0.0019,  0.0023],
        [ 0.0210,  0.0092, -0.0092,  ..., -0.0132,  0.0298,  0.0058]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-3.0346e-03, -3.9577e-04,  1.9836e-04,  ..., -1.1377e-03,
         -3.2377e-04,  1.3542e-04],
        [-4.8876e-04, -2.6951e-03,  2.8534e-03,  ...,  1.4400e-03,
          3.1567e-04,  1.8883e-04],
        [-8.7929e-04, -5.5075e-05, -1.5030e-03,  ..., -2.9221e-03,
          6.0272e-04, -1.8644e-03],
        ...,
        [ 4.3392e-04, -2.9635e-04,  1.2054e-03,  ...,  6.7234e-05,
          5.7316e-04, -2.7418e-04],
        [-1.5392e-03,  1.4544e-03,  5.6648e-04,  ..., -9.4223e-04,
         -1.4954e-03, -9.5785e-05],
        [-5.7268e-04,  1.8072e-03,  2.2812e-03,  ..., -2.0337e-04,
          1.6975e-03,  4.7660e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0654, -1.1943,  0.5566,  ..., -0.0701,  2.3672, -1.2480]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.3228, -0.0633,  0.1985,  ...,  0.1569, -0.3550, -0.0519]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 21:33:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for doze is sleep
A more intense word for irritate is enrage
A more intense word for creative is ingenious
A more intense word for love is adore
A more intense word for snack is meal
A more intense word for pony is horse
A more intense word for guilty is remorseful
A more intense word for tasty is
2024-07-12 21:33:22 root INFO     [order_1_approx] starting weight calculation for A more intense word for pony is horse
A more intense word for snack is meal
A more intense word for irritate is enrage
A more intense word for creative is ingenious
A more intense word for doze is sleep
A more intense word for love is adore
A more intense word for tasty is delicious
A more intense word for guilty is
2024-07-12 21:33:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 21:37:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 21:41:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6562, -0.8003, -0.7246,  ...,  0.7119, -0.3757, -0.4907],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.5913, -0.7656, -0.6426,  ...,  0.6221, -0.3582, -0.4819],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0249,  1.5684, -3.1484,  ...,  1.7236, -2.1797, -0.0796],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0136,  0.0028, -0.0101,  ...,  0.0076, -0.0042, -0.0217],
        [ 0.0010,  0.0058, -0.0164,  ...,  0.0118, -0.0127, -0.0013],
        [ 0.0131,  0.0150,  0.0146,  ..., -0.0148,  0.0226,  0.0137],
        ...,
        [-0.0119, -0.0183,  0.0300,  ...,  0.0114, -0.0039,  0.0209],
        [-0.0147,  0.0010,  0.0044,  ..., -0.0026, -0.0004, -0.0089],
        [ 0.0100, -0.0058,  0.0115,  ...,  0.0098,  0.0205,  0.0111]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-7.5817e-04,  2.4815e-03,  7.6056e-05,  ...,  6.8378e-04,
         -6.4802e-04,  1.9722e-03],
        [ 2.7990e-04, -5.9986e-04,  3.5381e-04,  ...,  5.5313e-05,
         -5.2214e-05,  3.6602e-03],
        [-3.1090e-03, -1.1044e-03, -2.5539e-03,  ..., -3.5152e-03,
          5.4979e-04,  4.9114e-05],
        ...,
        [ 1.9512e-03, -7.8011e-04,  2.4772e-04,  ...,  1.9407e-03,
          2.6360e-03, -2.1706e-03],
        [-1.6975e-03,  1.4906e-03, -1.4963e-03,  ..., -3.2139e-04,
         -5.6028e-05, -1.7176e-03],
        [-3.2787e-03, -2.0447e-03,  2.1629e-03,  ..., -1.0166e-03,
          3.8052e-03,  3.1185e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.8193,  0.9302, -2.4785,  ...,  1.2715, -2.3105,  1.6738]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1299, -0.1316,  0.3977,  ..., -0.0489,  0.2325,  0.0053]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 21:41:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for pony is horse
A more intense word for snack is meal
A more intense word for irritate is enrage
A more intense word for creative is ingenious
A more intense word for doze is sleep
A more intense word for love is adore
A more intense word for tasty is delicious
A more intense word for guilty is
2024-07-12 21:41:21 root INFO     [order_1_approx] starting weight calculation for A more intense word for guilty is remorseful
A more intense word for love is adore
A more intense word for irritate is enrage
A more intense word for tasty is delicious
A more intense word for snack is meal
A more intense word for doze is sleep
A more intense word for creative is ingenious
A more intense word for pony is
2024-07-12 21:41:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 21:45:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 21:49:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7002, -1.1123, -0.8145,  ..., -0.3594, -0.4641,  0.9751],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.6636, -1.1074, -0.7549,  ..., -0.3267, -0.4573,  0.9243],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1504, -2.3438, -3.5840,  ..., -3.6328,  4.4492,  0.1030],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0063,  0.0039,  0.0029,  ...,  0.0084,  0.0010, -0.0216],
        [ 0.0048,  0.0071,  0.0092,  ...,  0.0131, -0.0061, -0.0007],
        [-0.0008,  0.0038,  0.0184,  ..., -0.0199,  0.0031,  0.0062],
        ...,
        [-0.0017, -0.0011,  0.0094,  ..., -0.0027, -0.0104,  0.0089],
        [-0.0178, -0.0065, -0.0037,  ...,  0.0102,  0.0203,  0.0204],
        [-0.0161,  0.0094, -0.0033,  ..., -0.0023,  0.0194,  0.0040]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.3003e-03,  3.9637e-05, -1.4696e-03,  ..., -2.2316e-03,
         -1.6289e-03,  4.1628e-04],
        [ 1.6344e-04,  2.9922e-04,  3.4657e-03,  ...,  1.1292e-03,
          1.0548e-03,  2.5673e-03],
        [-2.1458e-03, -7.2098e-04,  6.7711e-04,  ...,  5.2834e-04,
          6.8855e-04, -6.0558e-04],
        ...,
        [ 1.8177e-03,  2.0266e-05, -9.6083e-05,  ..., -1.3466e-03,
          1.2064e-03,  1.6956e-03],
        [-2.1076e-04,  1.0490e-05, -1.1730e-03,  ...,  7.8154e-04,
         -2.6150e-03, -2.2545e-03],
        [-5.2738e-04,  4.0483e-04, -5.9605e-05,  ...,  8.8549e-04,
          4.0555e-04,  5.7983e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2152, -2.2246, -3.4258,  ..., -4.1953,  4.2695,  0.7705]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0932, -0.0301,  0.1754,  ...,  0.1875, -0.2152, -0.2067]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 21:49:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for guilty is remorseful
A more intense word for love is adore
A more intense word for irritate is enrage
A more intense word for tasty is delicious
A more intense word for snack is meal
A more intense word for doze is sleep
A more intense word for creative is ingenious
A more intense word for pony is
2024-07-12 21:49:21 root INFO     [order_1_approx] starting weight calculation for A more intense word for pony is horse
A more intense word for tasty is delicious
A more intense word for creative is ingenious
A more intense word for guilty is remorseful
A more intense word for snack is meal
A more intense word for love is adore
A more intense word for doze is sleep
A more intense word for irritate is
2024-07-12 21:49:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 21:53:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 21:57:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2437, -0.7822,  0.5000,  ...,  0.9556, -0.9062,  1.0146],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.2388, -0.8101,  0.4622,  ...,  0.9121, -0.9023,  1.0029],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9863, -3.8965, -2.5566,  ...,  6.4258,  4.8125,  0.5752],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0037,  0.0016,  0.0210,  ...,  0.0079, -0.0007,  0.0018],
        [ 0.0098,  0.0008, -0.0074,  ..., -0.0061, -0.0018, -0.0089],
        [ 0.0007, -0.0029, -0.0009,  ..., -0.0046, -0.0124, -0.0134],
        ...,
        [-0.0195, -0.0157,  0.0007,  ...,  0.0063, -0.0165,  0.0242],
        [-0.0175,  0.0099,  0.0072,  ...,  0.0089, -0.0080,  0.0168],
        [ 0.0114, -0.0042, -0.0058,  ...,  0.0084,  0.0023, -0.0111]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.2326e-04, -2.1095e-03,  2.2221e-03,  ..., -8.1444e-04,
         -1.9817e-03, -4.0269e-04],
        [-8.5068e-04, -1.6232e-03,  3.0231e-04,  ...,  5.2977e-04,
         -6.6948e-04, -1.6727e-03],
        [-7.2241e-04, -6.3467e-04, -5.6505e-05,  ..., -7.6485e-04,
         -9.6226e-04,  5.1737e-04],
        ...,
        [ 9.2173e-04, -1.5717e-03,  6.1810e-05,  ...,  2.2221e-03,
         -8.5068e-04, -7.9966e-04],
        [ 7.6771e-05,  1.7509e-03,  5.2738e-04,  ..., -4.1151e-04,
          2.1851e-04,  1.8539e-03],
        [-1.1454e-03, -1.0147e-03, -1.8692e-04,  ...,  1.3409e-03,
         -1.2379e-03, -2.9278e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0547, -3.6855, -2.3320,  ...,  6.3242,  3.9316,  1.4443]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0047,  0.1122,  0.0419,  ...,  0.0771, -0.3550, -0.0029]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 21:57:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for pony is horse
A more intense word for tasty is delicious
A more intense word for creative is ingenious
A more intense word for guilty is remorseful
A more intense word for snack is meal
A more intense word for love is adore
A more intense word for doze is sleep
A more intense word for irritate is
2024-07-12 21:57:20 root INFO     [order_1_approx] starting weight calculation for A more intense word for guilty is remorseful
A more intense word for irritate is enrage
A more intense word for tasty is delicious
A more intense word for pony is horse
A more intense word for snack is meal
A more intense word for doze is sleep
A more intense word for creative is ingenious
A more intense word for love is
2024-07-12 21:57:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 22:01:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 22:05:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3027, -0.1875,  0.0505,  ...,  0.8970, -0.3154,  0.2949],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 1.2686, -0.2048,  0.0356,  ...,  0.8428, -0.3223,  0.2698],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.8594,  0.1670, -1.3789,  ..., -4.4688,  1.9395, -1.6152],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0140,  0.0134, -0.0036,  ..., -0.0014, -0.0203,  0.0168],
        [-0.0005, -0.0003, -0.0044,  ..., -0.0035,  0.0007, -0.0074],
        [ 0.0181, -0.0148, -0.0078,  ..., -0.0161, -0.0079,  0.0125],
        ...,
        [ 0.0044, -0.0240,  0.0367,  ..., -0.0062, -0.0060, -0.0040],
        [-0.0019,  0.0029,  0.0177,  ..., -0.0027, -0.0088,  0.0023],
        [ 0.0066, -0.0025,  0.0209,  ..., -0.0040,  0.0026,  0.0114]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 4.6635e-04, -2.7294e-03, -4.4060e-03,  ...,  2.1172e-03,
         -1.0319e-03,  1.5392e-03],
        [ 5.9319e-04,  1.6041e-03,  1.5926e-03,  ...,  1.9779e-03,
          2.4910e-03,  2.8133e-05],
        [ 9.3269e-04, -4.5090e-03, -5.3558e-03,  ..., -1.4954e-03,
         -3.2234e-03,  1.0185e-03],
        ...,
        [-3.3736e-05, -2.4605e-03,  1.5430e-03,  ..., -2.9564e-03,
          9.6989e-04, -4.1008e-03],
        [-1.7416e-04, -3.6550e-04,  1.8606e-03,  ...,  6.9809e-04,
          7.7009e-04,  6.9189e-04],
        [ 2.2449e-03, -1.7090e-03, -4.5128e-03,  ..., -3.2949e-04,
         -2.7065e-03,  3.2568e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5527, -0.0774, -1.9492,  ..., -4.5195,  1.8018, -1.2617]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.2996, -0.0606,  0.0997,  ...,  0.1215, -0.1384,  0.1398]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 22:05:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for guilty is remorseful
A more intense word for irritate is enrage
A more intense word for tasty is delicious
A more intense word for pony is horse
A more intense word for snack is meal
A more intense word for doze is sleep
A more intense word for creative is ingenious
A more intense word for love is
2024-07-12 22:05:15 root INFO     [order_1_approx] starting weight calculation for A more intense word for tasty is delicious
A more intense word for snack is meal
A more intense word for love is adore
A more intense word for creative is ingenious
A more intense word for pony is horse
A more intense word for irritate is enrage
A more intense word for guilty is remorseful
A more intense word for doze is
2024-07-12 22:05:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 22:09:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 22:13:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0750, -0.0890, -0.0889,  ...,  0.2729,  0.6924,  0.3682],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.0657, -0.0980, -0.0864,  ...,  0.2422,  0.6006,  0.3179],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.6328, -0.0542, -1.0518,  ..., -4.6758, -0.5137,  1.9092],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0058,  0.0031,  0.0160,  ..., -0.0008, -0.0204, -0.0003],
        [-0.0025,  0.0038,  0.0037,  ..., -0.0038,  0.0151, -0.0019],
        [-0.0169,  0.0092, -0.0108,  ...,  0.0042, -0.0007,  0.0067],
        ...,
        [-0.0020, -0.0272,  0.0035,  ..., -0.0108, -0.0055,  0.0039],
        [-0.0053,  0.0024,  0.0113,  ..., -0.0061,  0.0051, -0.0047],
        [-0.0051,  0.0137,  0.0008,  ..., -0.0100,  0.0173,  0.0178]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.9569e-03,  1.5745e-03,  1.5507e-03,  ..., -2.5558e-04,
         -1.5259e-03,  4.2915e-04],
        [ 8.1158e-04, -2.4967e-03,  3.1853e-04,  ..., -9.9087e-04,
          1.5125e-03, -2.3890e-04],
        [-7.5340e-05,  2.0504e-05,  3.8242e-04,  ..., -3.2592e-04,
         -4.2224e-04,  6.4898e-04],
        ...,
        [-2.2531e-04,  1.5898e-03, -3.1567e-04,  ..., -1.3530e-04,
         -1.0529e-03,  4.4298e-04],
        [-1.7595e-04, -7.3719e-04, -9.3174e-04,  ..., -1.0576e-03,
          2.7943e-04,  8.8930e-04],
        [-1.2503e-03,  8.5258e-04,  1.3332e-03,  ...,  1.0195e-03,
          8.9645e-04,  1.9813e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.1914, -0.7588, -0.8916,  ..., -3.1055, -0.0547,  2.6367]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0319, -0.1138, -0.1130,  ...,  0.1094,  0.1671,  0.1058]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 22:13:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for tasty is delicious
A more intense word for snack is meal
A more intense word for love is adore
A more intense word for creative is ingenious
A more intense word for pony is horse
A more intense word for irritate is enrage
A more intense word for guilty is remorseful
A more intense word for doze is
2024-07-12 22:13:13 root INFO     total operator prediction time: 3825.693719148636 seconds
2024-07-12 22:13:13 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on hypernyms - animals
2024-07-12 22:13:15 root INFO     building operator hypernyms - animals
2024-07-12 22:13:15 root INFO     [order_1_approx] starting weight calculation for The triceratops falls into the category of dinosaur
The jackal falls into the category of canine
The turkey falls into the category of fowl
The duck falls into the category of fowl
The viper falls into the category of snake
The falcon falls into the category of raptor
The vulture falls into the category of raptor
The mouse falls into the category of
2024-07-12 22:13:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 22:17:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 22:21:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0068,  0.0156, -0.1926,  ..., -0.2332, -0.1416,  1.5107],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.8765, -0.0027, -0.1711,  ..., -0.1907, -0.1399,  1.3223],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7422,  2.4180,  0.0908,  ..., -2.6016, -1.8545,  1.1680],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0071, -0.0037,  0.0022,  ...,  0.0030,  0.0081,  0.0025],
        [-0.0097, -0.0007,  0.0073,  ...,  0.0212, -0.0078,  0.0047],
        [ 0.0056,  0.0013, -0.0038,  ..., -0.0014, -0.0054,  0.0037],
        ...,
        [ 0.0043,  0.0043, -0.0012,  ...,  0.0088, -0.0055, -0.0040],
        [-0.0001, -0.0071,  0.0015,  ..., -0.0147, -0.0057, -0.0115],
        [-0.0034, -0.0030,  0.0072,  ..., -0.0055, -0.0055, -0.0029]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.2312e-03, -1.2999e-03,  2.0862e-04,  ...,  8.6451e-04,
          5.7030e-04, -2.1210e-03],
        [-6.5279e-04, -1.9550e-03,  1.5869e-03,  ...,  1.3628e-03,
          3.2902e-04, -3.3951e-04],
        [ 1.1539e-03,  1.4677e-03, -1.8682e-03,  ..., -1.3332e-03,
         -1.3971e-03,  1.2245e-03],
        ...,
        [ 6.6566e-04,  2.3499e-03, -8.1062e-05,  ..., -7.7438e-04,
         -2.0523e-03, -1.1292e-03],
        [ 1.3285e-03,  1.1806e-03, -2.8343e-03,  ..., -7.8821e-04,
         -1.8148e-03,  8.2922e-04],
        [ 5.1594e-04, -9.0075e-04,  4.4584e-04,  ...,  9.6703e-04,
          2.4390e-04,  4.9305e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.8633,  2.2402,  0.2520,  ..., -2.4922, -1.4404,  1.5205]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1989, -0.1056,  0.2157,  ...,  0.0040,  0.1086,  0.0218]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 22:21:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The triceratops falls into the category of dinosaur
The jackal falls into the category of canine
The turkey falls into the category of fowl
The duck falls into the category of fowl
The viper falls into the category of snake
The falcon falls into the category of raptor
The vulture falls into the category of raptor
The mouse falls into the category of
2024-07-12 22:21:13 root INFO     [order_1_approx] starting weight calculation for The jackal falls into the category of canine
The falcon falls into the category of raptor
The turkey falls into the category of fowl
The vulture falls into the category of raptor
The mouse falls into the category of rodent
The triceratops falls into the category of dinosaur
The viper falls into the category of snake
The duck falls into the category of
2024-07-12 22:21:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 22:25:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 22:29:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2991, -1.3604, -1.5059,  ...,  0.8735, -0.7236,  0.7183],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.2515, -1.2070, -1.2373,  ...,  0.7183, -0.6274,  0.6016],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5303,  1.3828, -4.1133,  ...,  1.8379,  1.9746,  0.8516],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0154, -0.0110, -0.0007,  ..., -0.0048,  0.0063, -0.0061],
        [-0.0138,  0.0107, -0.0062,  ...,  0.0147, -0.0166, -0.0013],
        [ 0.0175, -0.0027,  0.0085,  ..., -0.0031,  0.0127,  0.0078],
        ...,
        [-0.0073,  0.0004,  0.0031,  ...,  0.0067, -0.0039, -0.0026],
        [ 0.0183, -0.0085, -0.0031,  ..., -0.0031,  0.0040, -0.0052],
        [-0.0157,  0.0018,  0.0142,  ..., -0.0050, -0.0067,  0.0105]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.3132e-03, -2.5821e-04, -7.7724e-05,  ...,  6.7902e-04,
          1.0462e-03, -8.8406e-04],
        [-7.4100e-04,  9.1791e-04,  2.2964e-03,  ...,  1.5230e-03,
          8.8406e-04, -2.1744e-03],
        [ 1.3065e-03,  1.7204e-03, -1.7281e-03,  ..., -7.9155e-04,
         -4.7350e-04,  1.1158e-03],
        ...,
        [ 7.9298e-04,  1.4143e-03, -2.2697e-04,  ..., -3.6240e-05,
          2.5225e-04, -1.2674e-03],
        [ 1.4267e-03,  1.5903e-04, -1.8921e-03,  ..., -2.2674e-04,
         -1.0328e-03,  8.0967e-04],
        [ 2.3794e-04, -1.0128e-03,  8.0824e-04,  ..., -2.8205e-04,
          2.1410e-04, -5.0211e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6421,  1.2666, -4.2539,  ...,  1.5459,  1.7900,  0.6719]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0490, -0.0545,  0.1744,  ..., -0.1186,  0.0905, -0.0522]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 22:29:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The jackal falls into the category of canine
The falcon falls into the category of raptor
The turkey falls into the category of fowl
The vulture falls into the category of raptor
The mouse falls into the category of rodent
The triceratops falls into the category of dinosaur
The viper falls into the category of snake
The duck falls into the category of
2024-07-12 22:29:04 root INFO     [order_1_approx] starting weight calculation for The turkey falls into the category of fowl
The viper falls into the category of snake
The jackal falls into the category of canine
The triceratops falls into the category of dinosaur
The vulture falls into the category of raptor
The duck falls into the category of fowl
The mouse falls into the category of rodent
The falcon falls into the category of
2024-07-12 22:29:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 22:33:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 22:36:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9424, -1.2412, -1.2422,  ..., -0.8477,  0.7480,  0.4004],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.8384, -1.1475, -1.0645,  ..., -0.7173,  0.6396,  0.3420],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9141, -0.0924, -4.4922,  ..., -0.9834, -0.9189,  1.5166],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0113, -0.0037, -0.0018,  ..., -0.0045, -0.0002,  0.0003],
        [-0.0046,  0.0053,  0.0087,  ...,  0.0149,  0.0041, -0.0064],
        [ 0.0050,  0.0014,  0.0034,  ..., -0.0034,  0.0109,  0.0013],
        ...,
        [-0.0063, -0.0025,  0.0012,  ...,  0.0075, -0.0007,  0.0039],
        [-0.0015, -0.0133,  0.0020,  ..., -0.0085,  0.0027,  0.0071],
        [-0.0039, -0.0018, -0.0016,  ..., -0.0023,  0.0050,  0.0107]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.4305e-06,  3.8147e-06, -4.9353e-05,  ...,  7.7629e-04,
         -3.3832e-04, -4.4775e-04],
        [-3.6669e-04,  8.6069e-04,  1.2884e-03,  ...,  1.7099e-03,
          4.9400e-04, -2.4471e-03],
        [ 1.0910e-03,  9.8944e-05, -4.7588e-04,  ...,  1.5316e-03,
         -3.2854e-04,  5.3978e-04],
        ...,
        [ 2.9182e-04,  1.1616e-03,  1.0023e-03,  ..., -3.1066e-04,
         -5.6934e-04,  8.3733e-04],
        [ 6.4707e-04, -3.0971e-04,  6.9904e-04,  ...,  3.3092e-04,
         -2.0325e-04,  8.5258e-04],
        [ 1.3459e-04, -9.7561e-04,  5.0735e-04,  ...,  1.4448e-03,
          3.8207e-05, -1.4000e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6641, -1.1406, -3.9414,  ..., -1.3262, -0.6191,  1.4668]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0659, -0.1504, -0.0529,  ...,  0.0229, -0.0219, -0.1694]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 22:36:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The turkey falls into the category of fowl
The viper falls into the category of snake
The jackal falls into the category of canine
The triceratops falls into the category of dinosaur
The vulture falls into the category of raptor
The duck falls into the category of fowl
The mouse falls into the category of rodent
The falcon falls into the category of
2024-07-12 22:36:58 root INFO     [order_1_approx] starting weight calculation for The triceratops falls into the category of dinosaur
The vulture falls into the category of raptor
The viper falls into the category of snake
The jackal falls into the category of canine
The mouse falls into the category of rodent
The duck falls into the category of fowl
The falcon falls into the category of raptor
The turkey falls into the category of
2024-07-12 22:36:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 22:40:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 22:44:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0461, -0.8174, -0.1815,  ...,  0.7856, -0.5098,  1.3350],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.0462, -0.7573, -0.1655,  ...,  0.6665, -0.4634,  1.1758],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5742,  1.0615, -2.1992,  ..., -1.8916,  1.0195,  0.0503],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0113, -0.0036,  0.0131,  ..., -0.0134,  0.0081,  0.0132],
        [ 0.0020,  0.0018, -0.0012,  ...,  0.0179, -0.0035, -0.0070],
        [ 0.0122,  0.0136, -0.0035,  ...,  0.0054,  0.0106, -0.0060],
        ...,
        [-0.0124,  0.0064,  0.0018,  ...,  0.0092,  0.0104, -0.0050],
        [ 0.0002, -0.0062, -0.0019,  ...,  0.0010,  0.0031, -0.0004],
        [-0.0081, -0.0076,  0.0097,  ..., -0.0026,  0.0017,  0.0059]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-6.7329e-04, -6.7472e-04,  4.2629e-04,  ..., -8.3447e-04,
          6.3658e-04,  6.6328e-04],
        [-1.2465e-03, -1.9150e-03,  3.7994e-03,  ...,  2.0542e-03,
          1.5888e-03, -2.9278e-04],
        [ 1.4963e-03,  1.3237e-03, -1.4019e-03,  ..., -2.1439e-03,
          1.5497e-04,  7.8869e-04],
        ...,
        [-3.8147e-04,  1.2407e-03,  8.7452e-04,  ..., -2.2173e-04,
          3.7193e-05, -5.8365e-04],
        [ 1.8978e-03,  2.3880e-03, -1.9798e-03,  ..., -1.0033e-03,
         -2.8000e-03,  1.2569e-03],
        [ 3.9279e-05, -1.0653e-03,  7.0810e-04,  ...,  1.3380e-03,
         -4.9686e-04, -8.0109e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.6221, -0.0791, -2.8867,  ..., -2.2676,  0.3350,  0.5649]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1407, -0.4316,  0.0873,  ...,  0.0594,  0.1161,  0.0538]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 22:44:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The triceratops falls into the category of dinosaur
The vulture falls into the category of raptor
The viper falls into the category of snake
The jackal falls into the category of canine
The mouse falls into the category of rodent
The duck falls into the category of fowl
The falcon falls into the category of raptor
The turkey falls into the category of
2024-07-12 22:44:52 root INFO     [order_1_approx] starting weight calculation for The viper falls into the category of snake
The turkey falls into the category of fowl
The vulture falls into the category of raptor
The triceratops falls into the category of dinosaur
The falcon falls into the category of raptor
The mouse falls into the category of rodent
The duck falls into the category of fowl
The jackal falls into the category of
2024-07-12 22:44:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 22:48:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 22:52:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2820, -0.9888, -0.4866,  ...,  0.4468,  0.0889,  0.0295],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.2629, -0.9551, -0.4390,  ...,  0.4062,  0.0671,  0.0088],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7637, -1.9824,  0.8604,  ..., -2.0059, -2.0195, -0.0244],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0082, -0.0202,  0.0032,  ..., -0.0167, -0.0077, -0.0062],
        [-0.0131, -0.0050,  0.0153,  ...,  0.0262,  0.0092, -0.0018],
        [ 0.0072,  0.0026, -0.0026,  ..., -0.0060, -0.0029,  0.0041],
        ...,
        [-0.0019, -0.0085, -0.0004,  ...,  0.0187, -0.0006,  0.0053],
        [-0.0108, -0.0109, -0.0031,  ..., -0.0082,  0.0029,  0.0113],
        [-0.0044,  0.0003,  0.0172,  ...,  0.0088,  0.0068,  0.0062]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0006, -0.0013,  0.0008,  ...,  0.0002, -0.0009, -0.0003],
        [-0.0023, -0.0006,  0.0017,  ...,  0.0015,  0.0002,  0.0002],
        [ 0.0010, -0.0007, -0.0004,  ...,  0.0003, -0.0008,  0.0002],
        ...,
        [ 0.0005, -0.0006,  0.0004,  ...,  0.0001, -0.0008,  0.0002],
        [ 0.0012, -0.0002,  0.0003,  ...,  0.0006, -0.0008,  0.0018],
        [-0.0005, -0.0007,  0.0012,  ...,  0.0013,  0.0002, -0.0002]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2031, -2.9492,  1.0938,  ..., -2.5742, -1.4297, -0.1045]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0568, -0.0723,  0.0026,  ...,  0.0204, -0.0526,  0.0722]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 22:52:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The viper falls into the category of snake
The turkey falls into the category of fowl
The vulture falls into the category of raptor
The triceratops falls into the category of dinosaur
The falcon falls into the category of raptor
The mouse falls into the category of rodent
The duck falls into the category of fowl
The jackal falls into the category of
2024-07-12 22:52:49 root INFO     [order_1_approx] starting weight calculation for The falcon falls into the category of raptor
The viper falls into the category of snake
The vulture falls into the category of raptor
The jackal falls into the category of canine
The mouse falls into the category of rodent
The turkey falls into the category of fowl
The duck falls into the category of fowl
The triceratops falls into the category of
2024-07-12 22:52:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 22:56:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 23:00:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0283,  2.2188, -1.3926,  ...,  0.8564, -0.2059,  1.3643],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.8853,  1.9395, -1.1494,  ...,  0.7119, -0.1898,  1.1758],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.4492, -1.8877,  1.5830,  ...,  0.5688, -2.7539,  0.9458],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0020,  0.0049,  0.0048,  ..., -0.0074, -0.0061,  0.0016],
        [ 0.0008,  0.0043,  0.0050,  ...,  0.0064,  0.0028,  0.0050],
        [ 0.0009,  0.0106, -0.0034,  ..., -0.0082, -0.0042,  0.0054],
        ...,
        [-0.0088, -0.0064, -0.0048,  ...,  0.0124, -0.0062,  0.0049],
        [-0.0116,  0.0137,  0.0067,  ...,  0.0043, -0.0029,  0.0006],
        [-0.0015,  0.0052,  0.0043,  ..., -0.0029,  0.0034,  0.0054]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 6.7711e-04,  7.2002e-04,  1.1759e-03,  ...,  8.3923e-05,
         -6.7282e-04,  3.1805e-04],
        [-2.3723e-05, -1.7023e-04,  2.0504e-03,  ...,  1.3084e-03,
          2.2936e-04, -9.5749e-04],
        [ 5.1975e-05,  1.3504e-03, -8.5831e-04,  ..., -6.3658e-04,
          2.4056e-04,  1.2302e-03],
        ...,
        [ 5.0592e-04, -1.7941e-04, -1.9288e-04,  ...,  4.0650e-04,
          1.4818e-04, -2.7943e-04],
        [ 7.1907e-04,  1.8191e-04,  4.1223e-04,  ...,  7.3338e-04,
         -1.2994e-04, -4.3535e-04],
        [-4.2367e-04, -4.9400e-04,  6.5804e-04,  ...,  1.5593e-04,
          7.8630e-04,  1.3697e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.8203, -2.5098,  1.4473,  ..., -0.0669, -1.6152,  0.6621]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0092, -0.1069,  0.0503,  ..., -0.0267, -0.1231,  0.0315]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 23:00:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The falcon falls into the category of raptor
The viper falls into the category of snake
The vulture falls into the category of raptor
The jackal falls into the category of canine
The mouse falls into the category of rodent
The turkey falls into the category of fowl
The duck falls into the category of fowl
The triceratops falls into the category of
2024-07-12 23:00:46 root INFO     [order_1_approx] starting weight calculation for The viper falls into the category of snake
The mouse falls into the category of rodent
The turkey falls into the category of fowl
The falcon falls into the category of raptor
The duck falls into the category of fowl
The triceratops falls into the category of dinosaur
The jackal falls into the category of canine
The vulture falls into the category of
2024-07-12 23:00:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 23:04:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 23:08:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6206, -1.3369, -1.5137,  ...,  0.4854,  0.5249,  0.8486],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.5259, -1.1807, -1.2383,  ...,  0.4006,  0.4224,  0.7139],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0039,  1.2285, -2.4805,  ...,  1.1953, -1.5420, -2.3223],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0110,  0.0075,  0.0097,  ...,  0.0020,  0.0015, -0.0047],
        [-0.0096,  0.0049,  0.0120,  ...,  0.0140, -0.0008,  0.0029],
        [ 0.0061, -0.0064,  0.0061,  ..., -0.0045, -0.0056, -0.0109],
        ...,
        [-0.0210, -0.0161, -0.0029,  ...,  0.0239,  0.0039,  0.0015],
        [-0.0033, -0.0145, -0.0018,  ...,  0.0084, -0.0007,  0.0093],
        [-0.0105,  0.0058,  0.0061,  ..., -0.0040, -0.0040,  0.0193]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-6.5899e-04,  4.6468e-04, -1.0490e-03,  ...,  1.0195e-03,
          2.5225e-04, -4.8661e-04],
        [ 4.7159e-04, -3.7670e-04,  6.6185e-04,  ...,  2.0866e-03,
          1.1063e-03, -3.5596e-04],
        [ 2.4071e-03, -4.4346e-04, -1.7233e-03,  ..., -3.6669e-04,
         -1.5430e-03,  4.2367e-04],
        ...,
        [ 6.6376e-04,  5.2643e-04, -4.1056e-04,  ...,  8.5306e-04,
          7.3850e-05, -2.7180e-04],
        [-1.5950e-04,  1.2856e-03,  4.9543e-04,  ..., -1.4317e-04,
         -1.4362e-03,  1.4553e-03],
        [ 2.7227e-04, -1.0490e-03,  5.7268e-04,  ...,  4.2343e-04,
         -1.5783e-04, -1.8454e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1536,  1.7285, -2.5566,  ...,  0.5669, -1.2998, -1.2002]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1702, -0.2666,  0.0364,  ..., -0.1418,  0.0618, -0.0652]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 23:08:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The viper falls into the category of snake
The mouse falls into the category of rodent
The turkey falls into the category of fowl
The falcon falls into the category of raptor
The duck falls into the category of fowl
The triceratops falls into the category of dinosaur
The jackal falls into the category of canine
The vulture falls into the category of
2024-07-12 23:08:40 root INFO     [order_1_approx] starting weight calculation for The triceratops falls into the category of dinosaur
The vulture falls into the category of raptor
The mouse falls into the category of rodent
The falcon falls into the category of raptor
The jackal falls into the category of canine
The turkey falls into the category of fowl
The duck falls into the category of fowl
The viper falls into the category of
2024-07-12 23:08:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 23:12:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 23:16:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5864,  0.0547, -1.2803,  ...,  0.0958,  0.2852,  0.6602],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.5142,  0.0342, -1.0752,  ...,  0.0864,  0.2291,  0.5669],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.5098, -0.7090, -3.7852,  ...,  0.6890, -2.5137,  1.0762],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0271, -0.0081, -0.0063,  ..., -0.0013,  0.0151,  0.0007],
        [-0.0180,  0.0109,  0.0118,  ...,  0.0228,  0.0026, -0.0060],
        [ 0.0255, -0.0158, -0.0054,  ..., -0.0106, -0.0015, -0.0077],
        ...,
        [-0.0184, -0.0075,  0.0068,  ...,  0.0156, -0.0111,  0.0072],
        [ 0.0039, -0.0150,  0.0011,  ..., -0.0107,  0.0072,  0.0187],
        [-0.0262,  0.0079,  0.0027,  ...,  0.0057,  0.0009,  0.0107]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.0014e-03, -8.7404e-04, -3.0351e-04,  ...,  7.2670e-04,
          9.0361e-04, -5.1117e-04],
        [-6.0463e-04,  2.1725e-03,  1.5774e-03,  ...,  2.0218e-03,
          4.0245e-04, -2.7542e-03],
        [ 2.3079e-03, -1.0347e-03, -2.2888e-04,  ..., -2.9449e-03,
          4.6968e-04, -1.3459e-04],
        ...,
        [ 2.9588e-04, -4.9591e-05,  2.0802e-04,  ...,  8.8072e-04,
         -1.2243e-04, -1.5278e-03],
        [ 1.2922e-03, -3.0518e-04,  1.5926e-04,  ..., -9.7942e-04,
         -4.1628e-04,  1.0386e-03],
        [-9.4175e-05, -7.0667e-04,  1.2360e-03,  ...,  3.9005e-04,
          7.0632e-05,  1.6379e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6367, -0.9961, -2.8125,  ..., -0.5396, -2.4688,  0.9731]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1555, -0.2335,  0.1521,  ..., -0.0729,  0.0280,  0.1298]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 23:16:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The triceratops falls into the category of dinosaur
The vulture falls into the category of raptor
The mouse falls into the category of rodent
The falcon falls into the category of raptor
The jackal falls into the category of canine
The turkey falls into the category of fowl
The duck falls into the category of fowl
The viper falls into the category of
2024-07-12 23:16:38 root INFO     total operator prediction time: 3803.3332924842834 seconds
2024-07-12 23:16:38 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on hyponyms - misc
2024-07-12 23:16:40 root INFO     building operator hyponyms - misc
2024-07-12 23:16:40 root INFO     [order_1_approx] starting weight calculation for A more specific term for a weapon is gun
A more specific term for a guitar is ukulele
A more specific term for a church is chapel
A more specific term for a container is bag
A more specific term for a color is white
A more specific term for a bed is bunk
A more specific term for a tool is rake
A more specific term for a shoes is
2024-07-12 23:16:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 23:20:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 23:24:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9067, -0.2969, -0.3521,  ...,  1.6738, -0.6846,  1.0039],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.8403, -0.3008, -0.3242,  ...,  1.4961, -0.6470,  0.9287],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5103, -0.6875,  1.0146,  ..., -6.8086, -3.5762,  0.1223],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0040, -0.0162, -0.0018,  ...,  0.0110,  0.0165, -0.0091],
        [-0.0022,  0.0180,  0.0008,  ...,  0.0116, -0.0015, -0.0204],
        [ 0.0035, -0.0119, -0.0015,  ...,  0.0028, -0.0087,  0.0008],
        ...,
        [ 0.0089,  0.0029, -0.0005,  ...,  0.0064, -0.0127,  0.0308],
        [ 0.0180, -0.0054,  0.0127,  ...,  0.0137,  0.0171,  0.0077],
        [-0.0163, -0.0006,  0.0123,  ..., -0.0101,  0.0124,  0.0201]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.0695e-03,  1.1005e-03, -4.1556e-04,  ...,  4.3678e-04,
         -9.2793e-04, -3.3164e-04],
        [ 4.3249e-04, -3.0804e-04,  1.3828e-04,  ...,  2.5005e-03,
         -5.5408e-04, -1.9622e-04],
        [-3.5143e-04, -2.4819e-04, -6.6710e-04,  ...,  2.6550e-03,
          1.5755e-03, -1.8549e-03],
        ...,
        [ 2.9397e-04,  2.2011e-03, -2.2545e-03,  ..., -1.1749e-03,
          5.8293e-05,  3.4180e-03],
        [ 1.9503e-03,  1.2999e-03,  5.5218e-04,  ...,  2.3246e-04,
         -1.2493e-04, -6.2180e-04],
        [-8.1348e-04,  9.3460e-04, -2.0885e-04,  ...,  1.3399e-03,
         -1.4138e-04, -3.1490e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3574, -0.6543,  0.5630,  ..., -6.1953, -4.2812, -0.2493]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1479, -0.4670,  0.1509,  ...,  0.1127,  0.2007, -0.0315]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 23:24:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a weapon is gun
A more specific term for a guitar is ukulele
A more specific term for a church is chapel
A more specific term for a container is bag
A more specific term for a color is white
A more specific term for a bed is bunk
A more specific term for a tool is rake
A more specific term for a shoes is
2024-07-12 23:24:32 root INFO     [order_1_approx] starting weight calculation for A more specific term for a bed is bunk
A more specific term for a guitar is ukulele
A more specific term for a weapon is gun
A more specific term for a container is bag
A more specific term for a tool is rake
A more specific term for a shoes is sneakers
A more specific term for a color is white
A more specific term for a church is
2024-07-12 23:24:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 23:28:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 23:32:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6460, -0.9297, -0.3838,  ...,  0.8164, -0.1801,  0.5381],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.5859, -0.8872, -0.3462,  ...,  0.7178, -0.1812,  0.4771],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3652,  2.5039,  1.7666,  ..., -0.6582, -0.0410, -0.3096],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0095, -0.0156,  0.0105,  ...,  0.0005, -0.0228, -0.0204],
        [-0.0073, -0.0119,  0.0038,  ...,  0.0102, -0.0036, -0.0083],
        [-0.0090, -0.0053, -0.0067,  ..., -0.0012,  0.0172,  0.0009],
        ...,
        [ 0.0070, -0.0081,  0.0037,  ...,  0.0113,  0.0012,  0.0018],
        [ 0.0081,  0.0066, -0.0049,  ...,  0.0124,  0.0113,  0.0013],
        [-0.0127, -0.0072, -0.0054,  ..., -0.0032,  0.0154,  0.0081]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 1.3065e-03, -1.6680e-03,  1.5974e-05,  ...,  2.0242e-04,
         -2.4819e-04, -2.5177e-03],
        [ 1.8883e-03, -1.5774e-03, -3.8719e-04,  ...,  5.2452e-04,
         -1.3752e-03, -2.3365e-03],
        [-7.2241e-05,  2.3365e-04,  1.7071e-03,  ...,  3.3927e-04,
          7.7820e-04, -7.1049e-04],
        ...,
        [ 1.6937e-03,  2.4152e-04,  1.7309e-04,  ..., -1.3971e-04,
          2.8801e-03, -7.7438e-04],
        [-7.9989e-05,  1.0719e-03, -2.4986e-04,  ..., -7.6151e-04,
          9.1934e-04,  2.8210e-03],
        [-7.2122e-05, -2.1515e-03, -9.1362e-04,  ..., -2.8157e-04,
         -4.8494e-04,  9.9373e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1113,  1.4131,  1.5752,  ...,  0.2725, -0.8013, -0.5820]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0755, -0.1050,  0.1455,  ..., -0.2362, -0.1064,  0.1667]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 23:32:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a bed is bunk
A more specific term for a guitar is ukulele
A more specific term for a weapon is gun
A more specific term for a container is bag
A more specific term for a tool is rake
A more specific term for a shoes is sneakers
A more specific term for a color is white
A more specific term for a church is
2024-07-12 23:32:28 root INFO     [order_1_approx] starting weight calculation for A more specific term for a church is chapel
A more specific term for a weapon is gun
A more specific term for a tool is rake
A more specific term for a container is bag
A more specific term for a shoes is sneakers
A more specific term for a guitar is ukulele
A more specific term for a color is white
A more specific term for a bed is
2024-07-12 23:32:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 23:36:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 23:40:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6167, -0.5874,  0.8750,  ...,  0.7983, -0.4233,  0.6016],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.5898, -0.5933,  0.7910,  ...,  0.7393, -0.4175,  0.5669],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1016, -4.3789, -0.6152,  ..., -2.3984, -1.5664, -0.9604],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0241,  0.0093, -0.0058,  ...,  0.0092,  0.0003, -0.0100],
        [-0.0112, -0.0033, -0.0014,  ...,  0.0149,  0.0105, -0.0097],
        [-0.0164, -0.0035,  0.0118,  ..., -0.0152, -0.0047,  0.0071],
        ...,
        [ 0.0003, -0.0168,  0.0145,  ...,  0.0233, -0.0044,  0.0055],
        [ 0.0047, -0.0043,  0.0022,  ...,  0.0021,  0.0132,  0.0170],
        [-0.0042, -0.0046,  0.0081,  ..., -0.0172,  0.0037,  0.0437]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-6.5422e-04, -2.1305e-03,  1.9608e-03,  ...,  1.5287e-03,
          1.0509e-03, -1.5774e-03],
        [-2.9087e-03, -2.6894e-03, -2.1398e-05,  ...,  6.7902e-04,
         -1.0099e-03, -1.9932e-04],
        [ 1.6785e-03, -6.9809e-04,  5.0688e-04,  ..., -2.7161e-03,
         -2.9898e-04, -7.5293e-04],
        ...,
        [ 1.2264e-03,  4.7398e-04, -2.7514e-04,  ..., -1.0471e-03,
          1.5469e-03,  3.5629e-03],
        [ 1.1146e-05,  6.7282e-04, -1.8978e-03,  ..., -3.2234e-04,
         -9.5081e-04,  2.7895e-04],
        [-3.7708e-03,  2.6608e-04,  7.5340e-05,  ...,  3.3641e-04,
         -1.3041e-04,  2.7313e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.9395, -5.2500, -0.2559,  ..., -1.8398, -0.5762, -0.2705]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.2286, -0.1469,  0.2147,  ..., -0.1622,  0.2222, -0.0712]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 23:40:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a church is chapel
A more specific term for a weapon is gun
A more specific term for a tool is rake
A more specific term for a container is bag
A more specific term for a shoes is sneakers
A more specific term for a guitar is ukulele
A more specific term for a color is white
A more specific term for a bed is
2024-07-12 23:40:24 root INFO     [order_1_approx] starting weight calculation for A more specific term for a color is white
A more specific term for a bed is bunk
A more specific term for a tool is rake
A more specific term for a shoes is sneakers
A more specific term for a church is chapel
A more specific term for a guitar is ukulele
A more specific term for a weapon is gun
A more specific term for a container is
2024-07-12 23:40:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 23:44:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 23:48:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4224, -0.2089,  0.4087,  ...,  0.6797, -0.1438,  1.3379],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3955, -0.2213,  0.3569,  ...,  0.6201, -0.1542,  1.2686],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.4062, -2.3691, -2.1211,  ...,  1.7256,  1.0068,  3.6504],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0077,  0.0108,  0.0127,  ...,  0.0010,  0.0041,  0.0023],
        [-0.0009, -0.0094, -0.0028,  ...,  0.0107, -0.0066,  0.0019],
        [ 0.0069,  0.0097,  0.0124,  ..., -0.0085, -0.0060, -0.0019],
        ...,
        [ 0.0011, -0.0219, -0.0083,  ...,  0.0123,  0.0088, -0.0003],
        [ 0.0126, -0.0025,  0.0062,  ...,  0.0084,  0.0075,  0.0175],
        [-0.0097, -0.0094,  0.0107,  ..., -0.0226,  0.0202,  0.0170]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.8721e-03, -3.2768e-03,  2.5558e-03,  ...,  7.0286e-04,
         -3.3474e-04,  3.6860e-04],
        [-1.5628e-04, -1.5211e-04, -3.0017e-04,  ...,  1.2589e-03,
         -2.5272e-04,  1.6332e-05],
        [ 2.3746e-03, -1.7281e-03,  2.1667e-03,  ...,  8.5974e-04,
          1.0490e-03, -6.6662e-04],
        ...,
        [ 1.4000e-03,  1.1158e-03, -2.3308e-03,  ..., -1.6766e-03,
          2.0409e-03,  1.6127e-03],
        [ 1.1835e-03,  6.1607e-04, -3.1257e-04,  ..., -1.1902e-03,
          7.6294e-04, -4.1795e-04],
        [-5.1737e-04, -7.2289e-04, -4.8256e-04,  ..., -8.8358e-04,
          4.8351e-04, -8.9645e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.1875, -2.1777, -2.8398,  ...,  1.4424,  0.7842,  3.1719]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2290,  0.1278,  0.0939,  ...,  0.0535, -0.0107,  0.2015]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 23:48:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a color is white
A more specific term for a bed is bunk
A more specific term for a tool is rake
A more specific term for a shoes is sneakers
A more specific term for a church is chapel
A more specific term for a guitar is ukulele
A more specific term for a weapon is gun
A more specific term for a container is
2024-07-12 23:48:21 root INFO     [order_1_approx] starting weight calculation for A more specific term for a bed is bunk
A more specific term for a church is chapel
A more specific term for a shoes is sneakers
A more specific term for a color is white
A more specific term for a container is bag
A more specific term for a weapon is gun
A more specific term for a tool is rake
A more specific term for a guitar is
2024-07-12 23:48:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-12 23:52:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-12 23:56:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2422, -0.9453, -0.1923,  ...,  0.4468, -0.3477,  1.0938],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 1.1191, -0.8950, -0.1788,  ...,  0.3894, -0.3301,  0.9839],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7373, -5.1953,  0.3154,  ..., -2.7852,  3.3828, -5.0391],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0151,  0.0065, -0.0058,  ...,  0.0182,  0.0063, -0.0024],
        [-0.0211,  0.0097,  0.0084,  ..., -0.0060, -0.0065,  0.0003],
        [ 0.0007,  0.0234,  0.0138,  ..., -0.0088,  0.0051,  0.0004],
        ...,
        [-0.0075, -0.0103,  0.0075,  ...,  0.0040, -0.0040,  0.0098],
        [ 0.0063, -0.0032,  0.0068,  ...,  0.0099, -0.0139,  0.0163],
        [-0.0029,  0.0015,  0.0085,  ..., -0.0132,  0.0101,  0.0127]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.6289e-03,  6.5565e-04,  7.0190e-04,  ..., -1.2751e-03,
         -1.5831e-03, -1.7204e-03],
        [-6.9618e-04,  7.2956e-04,  1.3008e-03,  ...,  2.3880e-03,
         -1.3459e-04, -1.1212e-04],
        [ 1.3494e-03, -4.3201e-04,  5.5695e-04,  ..., -7.2098e-04,
          1.5011e-03, -1.4973e-03],
        ...,
        [ 3.2067e-05, -6.0415e-04,  1.7796e-03,  ..., -4.1199e-04,
          1.2932e-03,  5.8174e-04],
        [ 1.4114e-03,  1.2422e-04,  7.2145e-04,  ..., -3.4761e-04,
         -1.6844e-04,  7.0858e-04],
        [ 7.3791e-05, -1.0767e-03,  6.9046e-04,  ...,  1.0147e-03,
          1.7195e-03,  6.3419e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7314, -3.3516, -0.1919,  ..., -0.1895,  3.3418, -4.2070]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2556,  0.0617, -0.2825,  ..., -0.1126,  0.0019,  0.0111]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-12 23:56:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a bed is bunk
A more specific term for a church is chapel
A more specific term for a shoes is sneakers
A more specific term for a color is white
A more specific term for a container is bag
A more specific term for a weapon is gun
A more specific term for a tool is rake
A more specific term for a guitar is
2024-07-12 23:56:16 root INFO     [order_1_approx] starting weight calculation for A more specific term for a container is bag
A more specific term for a guitar is ukulele
A more specific term for a church is chapel
A more specific term for a shoes is sneakers
A more specific term for a tool is rake
A more specific term for a color is white
A more specific term for a bed is bunk
A more specific term for a weapon is
2024-07-12 23:56:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 00:00:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 00:04:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([1.1240, 0.0271, 0.4055,  ..., 0.3179, 0.0929, 1.0869], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([1.0225, 0.0079, 0.3413,  ..., 0.2815, 0.0646, 0.9878], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.4551,  0.5410, -2.6953,  ..., -1.0732,  2.9414, -0.9297],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.4992e-03, -2.5558e-03,  2.0767e-02,  ..., -6.0730e-03,
          1.5472e-02, -5.0640e-04],
        [-1.2924e-02,  1.8158e-02, -4.4174e-03,  ...,  1.6922e-02,
         -3.1567e-03, -4.1580e-03],
        [-9.6283e-03, -7.5912e-03,  2.6047e-02,  ..., -6.9199e-03,
          5.2795e-03, -3.3321e-03],
        ...,
        [-4.0932e-03, -1.1841e-02,  7.1487e-03,  ...,  1.7258e-02,
         -7.2327e-03,  2.1820e-03],
        [-2.2907e-03,  3.1757e-03,  1.0185e-02,  ..., -6.7234e-05,
          6.0692e-03,  2.5177e-03],
        [-1.0300e-04, -1.5144e-03, -5.4932e-03,  ..., -2.6093e-03,
          1.5213e-02,  1.0872e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.0002e-04, -2.9831e-03,  1.4915e-03,  ...,  3.4642e-04,
         -2.8300e-04,  2.1243e-04],
        [-8.9264e-04,  9.7370e-04,  2.1820e-03,  ...,  2.8458e-03,
          1.1307e-04,  3.5048e-04],
        [ 1.7796e-03, -1.9159e-03,  4.1723e-05,  ..., -1.0815e-03,
          6.6137e-04,  1.8525e-04],
        ...,
        [ 5.4359e-04,  1.7033e-03, -5.7173e-04,  ...,  1.0240e-04,
          5.9700e-04,  1.3618e-03],
        [-6.9809e-04,  3.9792e-04,  1.8682e-03,  ..., -5.2738e-04,
          8.7166e-04, -1.2732e-03],
        [ 6.7949e-04, -1.9150e-03, -3.5191e-04,  ...,  1.7204e-03,
         -1.3895e-03, -2.5215e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.9727,  0.9854, -2.9922,  ...,  0.1191,  2.5098, -0.9170]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1697,  0.1602,  0.0464,  ...,  0.0553, -0.2156,  0.0215]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 00:04:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a container is bag
A more specific term for a guitar is ukulele
A more specific term for a church is chapel
A more specific term for a shoes is sneakers
A more specific term for a tool is rake
A more specific term for a color is white
A more specific term for a bed is bunk
A more specific term for a weapon is
2024-07-13 00:04:14 root INFO     [order_1_approx] starting weight calculation for A more specific term for a container is bag
A more specific term for a weapon is gun
A more specific term for a bed is bunk
A more specific term for a shoes is sneakers
A more specific term for a guitar is ukulele
A more specific term for a church is chapel
A more specific term for a color is white
A more specific term for a tool is
2024-07-13 00:04:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 00:08:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 00:12:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6147, -0.2466, -0.1401,  ...,  0.8838, -0.2656,  0.9380],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.5903, -0.2595, -0.1392,  ...,  0.8208, -0.2700,  0.8994],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.2500, -0.7139, -0.6504,  ..., -1.5449, -2.0430,  2.3965],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0106, -0.0067,  0.0056,  ..., -0.0041,  0.0186,  0.0172],
        [-0.0140,  0.0065, -0.0073,  ...,  0.0145, -0.0144, -0.0065],
        [ 0.0044,  0.0056,  0.0226,  ..., -0.0155, -0.0047,  0.0164],
        ...,
        [-0.0101, -0.0105,  0.0036,  ...,  0.0284,  0.0090,  0.0126],
        [ 0.0035,  0.0030,  0.0022,  ..., -0.0050,  0.0238,  0.0005],
        [-0.0026, -0.0124, -0.0072,  ..., -0.0036, -0.0018,  0.0268]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.0242e-03, -2.3460e-03,  1.2350e-03,  ..., -8.7261e-04,
         -7.2956e-04, -4.8923e-04],
        [ 2.1667e-03, -2.1935e-04,  2.8839e-03,  ...,  2.3060e-03,
         -1.5125e-03, -1.1482e-03],
        [ 2.0256e-03, -4.3945e-03,  4.2319e-04,  ..., -1.3094e-03,
          6.5470e-04,  2.1744e-03],
        ...,
        [ 1.4172e-03,  3.7122e-04, -1.0204e-03,  ..., -1.6890e-03,
          2.5997e-03,  2.6569e-03],
        [-9.6226e-04,  2.4056e-04,  1.2922e-03,  ...,  7.5006e-04,
          1.1196e-03, -1.4544e-03],
        [-3.6836e-04, -9.8038e-04, -8.0395e-04,  ...,  7.6771e-05,
          2.6894e-04, -2.7599e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3887, -1.3008, -1.1816,  ..., -0.7368, -1.1738,  1.1748]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2301,  0.2072,  0.0081,  ..., -0.0629, -0.3013, -0.0119]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 00:12:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a container is bag
A more specific term for a weapon is gun
A more specific term for a bed is bunk
A more specific term for a shoes is sneakers
A more specific term for a guitar is ukulele
A more specific term for a church is chapel
A more specific term for a color is white
A more specific term for a tool is
2024-07-13 00:12:08 root INFO     [order_1_approx] starting weight calculation for A more specific term for a container is bag
A more specific term for a weapon is gun
A more specific term for a church is chapel
A more specific term for a tool is rake
A more specific term for a bed is bunk
A more specific term for a guitar is ukulele
A more specific term for a shoes is sneakers
A more specific term for a color is
2024-07-13 00:12:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 00:16:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 00:20:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5469,  0.4629,  0.1819,  ..., -0.4912,  1.3223,  0.4961],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.5269,  0.4424,  0.1573,  ..., -0.4507,  1.2432,  0.4668],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.2480, -4.2344, -2.4062,  ...,  0.2959,  2.9004,  1.0352],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.1261e-02, -1.4786e-02, -2.4986e-03,  ..., -2.6535e-02,
          3.2883e-03,  3.7727e-03],
        [-2.8858e-03, -6.0654e-03,  1.2238e-02,  ...,  2.4338e-03,
         -9.3307e-03, -5.8365e-03],
        [-7.3166e-03,  1.8356e-02,  1.4191e-02,  ..., -8.8959e-03,
          1.0986e-02,  1.5778e-02],
        ...,
        [-1.6022e-04, -7.5302e-03, -5.2757e-03,  ...,  1.7761e-02,
         -1.2115e-02,  3.4103e-03],
        [-1.5732e-02, -1.2695e-02,  6.6643e-03,  ...,  1.1055e-02,
         -1.7834e-03, -1.0490e-03],
        [ 1.5594e-02, -1.9547e-02, -3.3188e-03,  ...,  2.4750e-02,
          9.7275e-05,  1.1415e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.1782e-03, -9.6703e-04,  3.1233e-04,  ..., -8.1491e-04,
          1.2817e-03, -1.8196e-03],
        [ 3.0828e-04, -1.1244e-03,  1.4515e-03,  ...,  5.0163e-04,
         -1.6499e-03,  3.1948e-04],
        [ 1.7605e-03,  3.2330e-04, -1.4305e-06,  ...,  3.9458e-04,
          1.6689e-04,  1.3530e-04],
        ...,
        [ 1.6928e-03,  2.1935e-04,  8.4162e-04,  ...,  2.5749e-05,
         -4.4346e-04, -2.7275e-04],
        [ 6.3705e-04, -1.0319e-03,  2.3766e-03,  ..., -5.9032e-04,
         -1.8728e-04, -2.8658e-04],
        [-4.9114e-04, -3.2692e-03,  3.1710e-04,  ...,  2.4319e-03,
          6.3705e-04,  4.3297e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.6602, -4.5898, -1.3584,  ...,  0.7832,  2.0527, -0.3750]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1542,  0.2096,  0.4888,  ..., -0.0696,  0.0310,  0.2032]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 00:20:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a container is bag
A more specific term for a weapon is gun
A more specific term for a church is chapel
A more specific term for a tool is rake
A more specific term for a bed is bunk
A more specific term for a guitar is ukulele
A more specific term for a shoes is sneakers
A more specific term for a color is
2024-07-13 00:20:05 root INFO     total operator prediction time: 3804.360019683838 seconds
2024-07-13 00:20:05 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on antonyms - binary
2024-07-13 00:20:07 root INFO     building operator antonyms - binary
2024-07-13 00:20:07 root INFO     [order_1_approx] starting weight calculation for The opposite of anterior is posterior
The opposite of below is above
The opposite of internal is external
The opposite of descend is ascend
The opposite of true is false
The opposite of forward is backward
The opposite of uphill is downhill
The opposite of top is
2024-07-13 00:20:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 00:24:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 00:28:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7964, -0.6631,  0.1216,  ..., -0.3025,  0.9272, -0.5166],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.7886, -0.6860,  0.1022,  ..., -0.2817,  0.8794, -0.5425],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.8516,  0.0924, -2.5898,  ...,  0.5791,  4.0977, -0.0752],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0396,  0.0141, -0.0044,  ..., -0.0046, -0.0296,  0.0143],
        [-0.0069,  0.0181, -0.0027,  ..., -0.0164, -0.0223,  0.0247],
        [ 0.0008, -0.0255, -0.0104,  ...,  0.0167, -0.0061,  0.0244],
        ...,
        [ 0.0120,  0.0005, -0.0061,  ..., -0.0183,  0.0075, -0.0047],
        [ 0.0181, -0.0112,  0.0133,  ..., -0.0061, -0.0243,  0.0115],
        [-0.0039,  0.0028,  0.0071,  ...,  0.0076,  0.0007, -0.0066]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-5.0316e-03,  1.5211e-04,  3.7193e-04,  ..., -1.7147e-03,
          1.2636e-03, -7.6294e-04],
        [ 4.7445e-04,  8.6212e-04,  3.6507e-03,  ..., -1.1377e-03,
          3.0537e-03,  2.7275e-03],
        [ 3.1590e-04, -1.8091e-03, -2.4452e-03,  ...,  1.5640e-03,
         -1.2236e-03,  3.0065e-04],
        ...,
        [ 5.0068e-05,  2.1133e-03, -1.9951e-03,  ..., -6.9952e-04,
          1.4696e-03, -4.3640e-03],
        [-1.6344e-04,  4.1175e-04,  1.6046e-04,  ...,  8.1158e-04,
         -3.5744e-03, -1.0138e-03],
        [ 6.7139e-04,  4.2915e-06, -4.2439e-05,  ...,  5.6696e-04,
          1.2517e-04,  6.5136e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.0020, -1.1699, -3.2441,  ...,  0.7061,  5.6289, -0.7437]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.2937, -0.2708,  0.8442,  ...,  0.1494,  0.0081,  0.2087]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 00:28:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of anterior is posterior
The opposite of below is above
The opposite of internal is external
The opposite of descend is ascend
The opposite of true is false
The opposite of forward is backward
The opposite of uphill is downhill
The opposite of top is
2024-07-13 00:28:02 root INFO     [order_1_approx] starting weight calculation for The opposite of internal is external
The opposite of forward is backward
The opposite of below is above
The opposite of top is bottom
The opposite of anterior is posterior
The opposite of true is false
The opposite of uphill is downhill
The opposite of descend is
2024-07-13 00:28:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 00:32:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 00:35:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0868, -1.4863,  0.1499,  ..., -0.6821, -0.3828,  0.2764],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.0663, -1.3135,  0.1068,  ..., -0.5547, -0.3425,  0.2122],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0098, -3.1094,  3.4453,  ...,  1.5928,  1.8359,  1.3955],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.0630e-02,  1.3161e-02, -7.7209e-03,  ..., -7.3166e-03,
          4.0970e-03, -4.3030e-03],
        [-1.4763e-03, -1.3268e-02, -8.3160e-04,  ..., -3.3741e-03,
         -6.0234e-03, -4.4823e-03],
        [-7.2441e-03, -5.2872e-03, -7.9956e-03,  ..., -3.2654e-03,
         -6.7711e-05,  2.2125e-04],
        ...,
        [-3.2501e-03, -5.9166e-03,  1.7242e-02,  ...,  3.5629e-03,
         -4.0550e-03,  1.4343e-03],
        [ 1.0986e-03,  8.7662e-03,  7.2899e-03,  ...,  5.7297e-03,
         -2.6703e-02,  1.9791e-02],
        [ 6.0349e-03, -1.1154e-02, -1.8806e-03,  ..., -8.7585e-03,
         -2.3727e-03,  2.9793e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.8534e-03, -3.4690e-04,  1.2760e-03,  ..., -5.2929e-04,
          1.4248e-03, -2.5196e-03],
        [ 8.4305e-04, -1.3542e-03, -1.2541e-04,  ..., -7.9870e-06,
         -1.3618e-03,  3.4571e-04],
        [ 1.2398e-05, -2.5082e-04, -2.5129e-04,  ..., -1.1969e-03,
         -2.7323e-04,  9.6273e-04],
        ...,
        [ 6.6566e-04, -1.3151e-03,  1.6813e-03,  ..., -7.3051e-04,
         -8.9312e-04, -6.6471e-04],
        [ 2.0103e-03,  1.4429e-03, -5.4216e-04,  ..., -1.2808e-03,
         -1.3590e-03,  1.3008e-03],
        [-2.8253e-04, -1.0080e-03,  5.2404e-04,  ...,  6.4659e-04,
          3.9268e-04, -7.1526e-06]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3545, -4.6172,  1.8633,  ...,  1.8184,  3.5977,  0.5942]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1803,  0.1970,  0.1902,  ...,  0.0201, -0.0977, -0.0312]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 00:35:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of internal is external
The opposite of forward is backward
The opposite of below is above
The opposite of top is bottom
The opposite of anterior is posterior
The opposite of true is false
The opposite of uphill is downhill
The opposite of descend is
2024-07-13 00:35:59 root INFO     [order_1_approx] starting weight calculation for The opposite of internal is external
The opposite of descend is ascend
The opposite of top is bottom
The opposite of uphill is downhill
The opposite of true is false
The opposite of anterior is posterior
The opposite of below is above
The opposite of forward is
2024-07-13 00:35:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 00:40:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 00:43:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6367, -1.3770, -0.1068,  ..., -1.0713,  0.1722, -0.0173],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.6167, -1.3701, -0.1115,  ..., -0.9814,  0.1410, -0.0435],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.8496,  2.7910, -0.2422,  ...,  2.2246,  7.0859,  1.2080],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-4.9591e-05, -1.6846e-02, -4.4250e-03,  ...,  3.6697e-03,
          4.1466e-03,  1.0834e-02],
        [-2.6978e-02,  2.3956e-02, -8.0490e-03,  ..., -1.4343e-02,
         -2.9953e-02,  3.2043e-04],
        [-9.0256e-03, -3.3741e-03, -1.5976e-02,  ...,  4.2534e-03,
         -8.8501e-03, -9.7351e-03],
        ...,
        [-3.7212e-03, -1.7948e-03,  2.1317e-02,  ...,  8.1940e-03,
          7.2289e-03,  1.0452e-03],
        [-1.2863e-02,  1.5915e-02,  2.6230e-02,  ..., -1.9028e-02,
         -1.1948e-02,  2.4918e-02],
        [-1.3222e-02, -2.7496e-02, -5.0735e-04,  ...,  9.4604e-03,
          4.4632e-03, -5.4703e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-3.2082e-03, -4.6778e-04, -3.4752e-03,  ..., -1.2569e-03,
         -6.7282e-04, -3.4738e-04],
        [-2.1477e-03, -3.3665e-04,  1.4515e-03,  ..., -2.1439e-03,
         -6.1512e-05,  1.4381e-03],
        [-1.2388e-03,  6.9332e-04, -7.5769e-04,  ..., -1.9493e-03,
         -1.0738e-03, -2.4509e-03],
        ...,
        [ 1.8044e-03,  1.6575e-03, -9.8228e-04,  ..., -9.6655e-04,
          3.0398e-04, -2.6970e-03],
        [-4.4322e-04,  5.1270e-03,  1.3046e-03,  ..., -9.4652e-04,
         -1.5068e-03, -1.3447e-04],
        [-5.9271e-04, -2.3384e-03,  3.1424e-04,  ...,  3.1052e-03,
         -7.3528e-04, -1.1435e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.6445,  0.8555, -0.8760,  ...,  1.9609,  6.1914,  1.5430]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1426,  0.1271,  0.1432,  ..., -0.0512, -0.0528,  0.2395]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 00:43:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of internal is external
The opposite of descend is ascend
The opposite of top is bottom
The opposite of uphill is downhill
The opposite of true is false
The opposite of anterior is posterior
The opposite of below is above
The opposite of forward is
2024-07-13 00:43:56 root INFO     [order_1_approx] starting weight calculation for The opposite of internal is external
The opposite of uphill is downhill
The opposite of descend is ascend
The opposite of forward is backward
The opposite of top is bottom
The opposite of below is above
The opposite of true is false
The opposite of anterior is
2024-07-13 00:43:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 00:47:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 00:51:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4458, -1.5986, -0.3958,  ..., -0.6328,  1.2471,  0.6289],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.3960, -1.4463, -0.3435,  ..., -0.5273,  1.0479,  0.5312],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3750, -3.5449, -4.7031,  ...,  2.5156,  0.8828,  0.0137],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0163,  0.0002, -0.0135,  ..., -0.0077,  0.0198, -0.0014],
        [ 0.0213, -0.0267, -0.0102,  ...,  0.0118, -0.0331,  0.0065],
        [-0.0292, -0.0119, -0.0163,  ...,  0.0039,  0.0208, -0.0053],
        ...,
        [-0.0193,  0.0195,  0.0241,  ...,  0.0059,  0.0292,  0.0184],
        [-0.0034,  0.0023,  0.0107,  ...,  0.0024, -0.0031,  0.0011],
        [ 0.0006, -0.0107, -0.0122,  ..., -0.0169, -0.0011, -0.0106]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-6.0844e-04,  7.4434e-04, -1.9855e-03,  ...,  3.5214e-04,
          1.6718e-03, -2.6245e-03],
        [-8.0204e-04, -1.1969e-04,  1.6994e-03,  ...,  8.0168e-05,
         -3.6955e-04,  2.8915e-03],
        [ 8.5354e-04,  1.7557e-03,  5.9700e-04,  ...,  4.2610e-03,
          3.0785e-03,  9.0361e-04],
        ...,
        [ 1.3075e-03,  9.5749e-04,  8.8024e-04,  ..., -3.1519e-04,
          2.4242e-03, -1.0118e-03],
        [-1.0622e-04,  8.4400e-04,  7.7629e-04,  ...,  5.9462e-04,
         -1.2894e-03,  8.0633e-04],
        [ 1.8444e-03, -1.5521e-04, -7.0477e-04,  ...,  2.4223e-03,
         -1.6298e-03,  1.4210e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.2715, -3.3477, -4.7188,  ...,  2.5215,  1.6641,  0.0135]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0117, -0.0765,  0.7173,  ...,  0.0167,  0.0543,  0.2686]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 00:51:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of internal is external
The opposite of uphill is downhill
The opposite of descend is ascend
The opposite of forward is backward
The opposite of top is bottom
The opposite of below is above
The opposite of true is false
The opposite of anterior is
2024-07-13 00:51:52 root INFO     [order_1_approx] starting weight calculation for The opposite of true is false
The opposite of uphill is downhill
The opposite of forward is backward
The opposite of top is bottom
The opposite of anterior is posterior
The opposite of below is above
The opposite of descend is ascend
The opposite of internal is
2024-07-13 00:51:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 00:55:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 00:59:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2227, -0.2493,  0.0581,  ..., -0.0278,  0.7422,  0.4250],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-1.1553, -0.2593,  0.0385,  ..., -0.0234,  0.6655,  0.3809],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1599, -2.8828,  2.0391,  ..., -0.7168,  2.3164, -0.1460],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0156, -0.0219, -0.0059,  ...,  0.0106, -0.0087,  0.0046],
        [ 0.0043, -0.0146, -0.0126,  ...,  0.0076, -0.0270,  0.0028],
        [-0.0087, -0.0027, -0.0097,  ...,  0.0075, -0.0013,  0.0202],
        ...,
        [ 0.0050, -0.0147,  0.0265,  ...,  0.0190, -0.0080,  0.0111],
        [-0.0047, -0.0154, -0.0015,  ...,  0.0041, -0.0109,  0.0042],
        [ 0.0058,  0.0012,  0.0015,  ...,  0.0026,  0.0053,  0.0062]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 0.0005, -0.0004, -0.0009,  ..., -0.0006, -0.0014,  0.0002],
        [ 0.0006, -0.0026,  0.0013,  ...,  0.0014, -0.0015,  0.0018],
        [ 0.0001,  0.0011,  0.0017,  ...,  0.0010,  0.0006,  0.0003],
        ...,
        [-0.0002,  0.0003, -0.0011,  ..., -0.0017,  0.0008, -0.0012],
        [ 0.0008, -0.0001, -0.0020,  ...,  0.0014, -0.0022, -0.0011],
        [ 0.0012,  0.0003, -0.0009,  ...,  0.0012, -0.0006, -0.0006]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0781, -2.4707,  1.2852,  ..., -2.0020,  1.1475, -1.1230]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0116, -0.0936,  0.3718,  ..., -0.0906, -0.2312,  0.0929]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 00:59:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of true is false
The opposite of uphill is downhill
The opposite of forward is backward
The opposite of top is bottom
The opposite of anterior is posterior
The opposite of below is above
The opposite of descend is ascend
The opposite of internal is
2024-07-13 00:59:48 root INFO     [order_1_approx] starting weight calculation for The opposite of below is above
The opposite of top is bottom
The opposite of anterior is posterior
The opposite of true is false
The opposite of internal is external
The opposite of forward is backward
The opposite of descend is ascend
The opposite of uphill is
2024-07-13 00:59:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 01:03:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 01:07:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8892, -1.5088, -0.4854,  ..., -0.9775,  0.1355, -0.0803],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.7271, -1.2695, -0.3896,  ..., -0.7568,  0.0840, -0.0949],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8218, -0.1316,  2.9375,  ..., -2.4902,  2.3613,  2.1426],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0316, -0.0237,  0.0038,  ...,  0.0019, -0.0089,  0.0103],
        [ 0.0112, -0.0012, -0.0174,  ...,  0.0032, -0.0444,  0.0126],
        [-0.0346, -0.0098, -0.0249,  ..., -0.0058,  0.0077, -0.0180],
        ...,
        [ 0.0206, -0.0189, -0.0085,  ...,  0.0021, -0.0006, -0.0009],
        [-0.0284,  0.0011,  0.0251,  ..., -0.0126, -0.0092, -0.0161],
        [-0.0132, -0.0028,  0.0037,  ...,  0.0051, -0.0002, -0.0205]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-9.8801e-04, -1.0681e-03,  2.4748e-04,  ..., -1.7519e-03,
          7.9250e-04, -7.9012e-04],
        [-1.7481e-03, -1.9670e-04,  3.3989e-03,  ...,  1.2512e-03,
          3.9787e-03,  8.0204e-04],
        [-7.4482e-04, -3.1986e-03, -1.8597e-05,  ...,  4.0770e-05,
         -2.9397e-04, -2.5349e-03],
        ...,
        [ 2.9411e-03,  1.3933e-03,  1.2779e-03,  ...,  2.3155e-03,
          1.3685e-03,  2.0905e-03],
        [ 1.1377e-03,  9.2983e-05, -1.8921e-03,  ..., -1.3037e-03,
         -3.1757e-04, -1.7900e-03],
        [-3.7155e-03, -6.6710e-04,  2.3556e-03,  ...,  3.8910e-03,
          1.6046e-04, -3.5248e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0674, -1.6025,  3.3574,  ..., -2.5723,  2.1992,  2.1270]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1163, -0.1737,  0.2429,  ..., -0.1100, -0.0625,  0.0600]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 01:07:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of below is above
The opposite of top is bottom
The opposite of anterior is posterior
The opposite of true is false
The opposite of internal is external
The opposite of forward is backward
The opposite of descend is ascend
The opposite of uphill is
2024-07-13 01:07:45 root INFO     [order_1_approx] starting weight calculation for The opposite of top is bottom
The opposite of forward is backward
The opposite of anterior is posterior
The opposite of internal is external
The opposite of uphill is downhill
The opposite of descend is ascend
The opposite of true is false
The opposite of below is
2024-07-13 01:07:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 01:11:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 01:15:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3552, -1.8438, -0.4446,  ...,  0.3931,  0.3125, -0.4678],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.3323, -1.7529, -0.4026,  ...,  0.3477,  0.2620, -0.4639],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.0742, -6.6328,  3.7266,  ..., -0.7734,  4.7773,  1.3271],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0203, -0.0084, -0.0029,  ..., -0.0034, -0.0049, -0.0014],
        [ 0.0020, -0.0038, -0.0247,  ..., -0.0029,  0.0084, -0.0061],
        [ 0.0301, -0.0026, -0.0540,  ...,  0.0030,  0.0132,  0.0018],
        ...,
        [-0.0133,  0.0069,  0.0130,  ..., -0.0227,  0.0199,  0.0056],
        [-0.0086,  0.0089,  0.0263,  ..., -0.0309, -0.0046, -0.0099],
        [-0.0023, -0.0463,  0.0048,  ...,  0.0053,  0.0063, -0.0328]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0006,  0.0020,  0.0014,  ...,  0.0017,  0.0002, -0.0006],
        [ 0.0020,  0.0006,  0.0007,  ..., -0.0030,  0.0026,  0.0009],
        [ 0.0021,  0.0044, -0.0002,  ...,  0.0014, -0.0002, -0.0010],
        ...,
        [-0.0011,  0.0019,  0.0002,  ...,  0.0001,  0.0006, -0.0017],
        [ 0.0004, -0.0009,  0.0013,  ..., -0.0001,  0.0005, -0.0016],
        [-0.0026, -0.0012,  0.0002,  ...,  0.0019,  0.0011,  0.0019]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.1992, -5.8945,  5.9453,  ...,  0.9482,  4.1562,  3.1152]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0127, -0.0437,  0.1536,  ..., -0.1288, -0.0543,  0.2510]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 01:15:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of top is bottom
The opposite of forward is backward
The opposite of anterior is posterior
The opposite of internal is external
The opposite of uphill is downhill
The opposite of descend is ascend
The opposite of true is false
The opposite of below is
2024-07-13 01:15:42 root INFO     [order_1_approx] starting weight calculation for The opposite of below is above
The opposite of descend is ascend
The opposite of anterior is posterior
The opposite of uphill is downhill
The opposite of internal is external
The opposite of top is bottom
The opposite of forward is backward
The opposite of true is
2024-07-13 01:15:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 01:19:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 01:23:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7983, -1.0771, -0.3777,  ..., -0.1562,  0.1860, -0.5137],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.8638, -1.2207, -0.4058,  ..., -0.1614,  0.1786, -0.5967],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0067, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.7070, -1.2520,  0.9180,  ..., -0.5244,  2.1406, -0.8765],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.5742e-02, -9.7504e-03, -1.9226e-03,  ..., -5.9357e-03,
         -2.3499e-03, -2.0981e-03],
        [ 4.9591e-03, -7.6752e-03,  9.9277e-04,  ...,  3.0403e-03,
         -6.2180e-03, -1.1955e-02],
        [ 1.4313e-02,  1.9054e-03, -9.2773e-03,  ...,  1.8387e-02,
          1.8463e-02,  3.8086e-02],
        ...,
        [-6.6757e-05, -9.8419e-04,  8.2169e-03,  ..., -2.4139e-02,
          3.3020e-02, -1.0498e-02],
        [ 2.0676e-02, -5.5962e-03, -3.8433e-03,  ..., -3.1281e-02,
          2.2278e-02,  3.9902e-03],
        [ 3.0060e-02,  8.1024e-03,  1.2466e-02,  ..., -1.4267e-03,
          1.5511e-02,  1.8005e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-6.0463e-04, -2.2926e-03, -5.0735e-04,  ..., -5.4073e-04,
          3.9577e-04,  5.3692e-04],
        [-2.2812e-03, -8.1491e-04,  4.5929e-03,  ..., -6.7091e-04,
         -1.0643e-03,  1.9073e-05],
        [-5.4169e-04, -1.5712e-04, -2.3022e-03,  ...,  2.1610e-03,
          1.0777e-03,  3.3607e-03],
        ...,
        [ 3.4122e-03,  2.7180e-05, -2.9716e-03,  ..., -1.3142e-03,
          8.4066e-04, -2.4986e-03],
        [ 1.7500e-03, -1.3275e-03, -1.1196e-03,  ..., -1.4677e-03,
         -9.2554e-04, -5.6028e-04],
        [ 5.7602e-04,  2.5387e-03,  1.5898e-03,  ..., -1.5249e-03,
         -8.3208e-05,  8.7929e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.7109, -1.5586,  0.8120,  ...,  0.6602,  1.5762, -1.3887]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0798, -0.3569,  0.6704,  ...,  0.0214, -0.2223, -0.0030]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 01:23:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of below is above
The opposite of descend is ascend
The opposite of anterior is posterior
The opposite of uphill is downhill
The opposite of internal is external
The opposite of top is bottom
The opposite of forward is backward
The opposite of true is
2024-07-13 01:23:39 root INFO     total operator prediction time: 3812.551838874817 seconds
2024-07-13 01:23:39 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - member
2024-07-13 01:23:42 root INFO     building operator meronyms - member
2024-07-13 01:23:42 root INFO     [order_1_approx] starting weight calculation for A bee is a member of a swarm
A nomad is a member of a horde
A flower is a member of a bouquet
A fish is a member of a school
A state is a member of a country
A book is a member of a library
A photo is a member of a album
A secretary is a member of a
2024-07-13 01:23:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 01:27:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 01:31:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7422, -0.3167, -0.1660,  ...,  0.5332,  0.1997,  0.1556],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.7344, -0.3428, -0.1714,  ...,  0.5107,  0.1752,  0.1322],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2451, -0.5991,  1.6055,  ...,  2.8164,  2.4609,  1.7246],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0108, -0.0062, -0.0008,  ...,  0.0110, -0.0155,  0.0050],
        [-0.0074, -0.0058,  0.0030,  ...,  0.0033, -0.0023,  0.0119],
        [-0.0132, -0.0009, -0.0246,  ...,  0.0071, -0.0106, -0.0084],
        ...,
        [-0.0049, -0.0164, -0.0027,  ..., -0.0072,  0.0073,  0.0094],
        [ 0.0152,  0.0020,  0.0041,  ...,  0.0023, -0.0177,  0.0132],
        [-0.0021, -0.0023,  0.0057,  ...,  0.0022,  0.0144,  0.0171]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.9379e-03, -6.7997e-04,  3.0327e-04,  ..., -5.7650e-04,
         -2.8877e-03,  3.0499e-03],
        [ 1.8215e-04, -7.2479e-04,  4.8018e-04,  ..., -1.4210e-04,
         -3.3236e-04, -2.5101e-03],
        [ 1.1501e-03,  6.7425e-04, -1.8520e-03,  ...,  2.6169e-03,
          3.5629e-03,  6.2513e-04],
        ...,
        [ 1.5392e-03, -6.6853e-04, -2.7680e-04,  ..., -1.4229e-03,
          2.4586e-03, -7.5400e-05],
        [-2.7776e-04,  1.0843e-03,  1.8816e-03,  ..., -2.4629e-04,
         -8.1348e-04, -3.0220e-05],
        [-1.4334e-03,  7.4196e-04,  6.3324e-04,  ..., -2.0905e-03,
         -2.2459e-04, -7.9346e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1533, -0.2920,  1.3955,  ...,  2.7969,  2.0156,  1.3066]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.6323, -0.0551, -0.2969,  ...,  0.2085,  0.1660,  0.1050]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 01:31:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A bee is a member of a swarm
A nomad is a member of a horde
A flower is a member of a bouquet
A fish is a member of a school
A state is a member of a country
A book is a member of a library
A photo is a member of a album
A secretary is a member of a
2024-07-13 01:31:39 root INFO     [order_1_approx] starting weight calculation for A photo is a member of a album
A bee is a member of a swarm
A state is a member of a country
A secretary is a member of a staff
A flower is a member of a bouquet
A fish is a member of a school
A nomad is a member of a horde
A book is a member of a
2024-07-13 01:31:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 01:35:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 01:39:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8408, -0.8594,  0.0327,  ...,  0.5327, -0.0547,  0.2365],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.8232, -0.8804,  0.0202,  ...,  0.5059, -0.0706,  0.2140],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4241,  0.0842,  0.0166,  ...,  1.2129, -3.1270, -0.6729],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0049, -0.0072, -0.0073,  ..., -0.0088,  0.0008,  0.0161],
        [-0.0117, -0.0116, -0.0130,  ..., -0.0043,  0.0014, -0.0217],
        [-0.0003, -0.0037, -0.0052,  ...,  0.0050, -0.0076,  0.0022],
        ...,
        [-0.0077, -0.0072,  0.0054,  ..., -0.0221, -0.0002,  0.0135],
        [ 0.0384,  0.0039,  0.0053,  ...,  0.0002, -0.0042,  0.0068],
        [ 0.0065, -0.0076,  0.0065,  ...,  0.0009,  0.0154, -0.0067]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.4643e-03,  6.9475e-04,  1.7524e-05,  ...,  5.5170e-04,
         -8.5068e-04,  1.8826e-03],
        [ 1.6108e-03, -3.6488e-03,  1.2112e-03,  ...,  1.8282e-03,
          9.8705e-04, -5.7888e-04],
        [-1.0319e-03, -8.6021e-04, -1.8139e-03,  ...,  6.8998e-04,
         -2.3186e-04,  1.0500e-03],
        ...,
        [ 1.5392e-03, -7.7391e-04,  3.1128e-03,  ..., -2.8782e-03,
          2.5826e-03, -9.0265e-04],
        [ 1.3638e-04,  2.7370e-03, -2.1744e-03,  ...,  1.4186e-04,
         -1.5984e-03, -2.2812e-03],
        [-3.1967e-03,  7.5436e-04, -1.0532e-04,  ..., -2.3212e-03,
         -1.8911e-03, -3.5591e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0564, -0.4417,  0.0950,  ...,  1.0527, -1.7100,  0.0400]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2255, -0.3713,  0.0804,  ..., -0.0724, -0.1691,  0.1427]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 01:39:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A photo is a member of a album
A bee is a member of a swarm
A state is a member of a country
A secretary is a member of a staff
A flower is a member of a bouquet
A fish is a member of a school
A nomad is a member of a horde
A book is a member of a
2024-07-13 01:39:35 root INFO     [order_1_approx] starting weight calculation for A state is a member of a country
A fish is a member of a school
A secretary is a member of a staff
A nomad is a member of a horde
A photo is a member of a album
A book is a member of a library
A flower is a member of a bouquet
A bee is a member of a
2024-07-13 01:39:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 01:43:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 01:47:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8501,  0.0778, -0.5278,  ...,  0.9331, -0.3916,  1.1357],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.7178,  0.0518, -0.4360,  ...,  0.7622, -0.3438,  0.9585],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.2617,  2.3320, -1.4785,  ...,  1.9336,  1.9971,  2.4160],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0051, -0.0085,  0.0080,  ...,  0.0011, -0.0004, -0.0155],
        [-0.0193, -0.0065,  0.0162,  ...,  0.0100, -0.0036,  0.0132],
        [-0.0004,  0.0119, -0.0095,  ..., -0.0113,  0.0192, -0.0042],
        ...,
        [-0.0005, -0.0131, -0.0042,  ..., -0.0051, -0.0015, -0.0041],
        [ 0.0040, -0.0102,  0.0162,  ..., -0.0025, -0.0223, -0.0038],
        [-0.0088, -0.0089, -0.0098,  ..., -0.0089,  0.0007,  0.0063]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0019, -0.0013,  0.0006,  ...,  0.0006,  0.0013, -0.0030],
        [ 0.0020, -0.0007,  0.0033,  ...,  0.0004,  0.0011, -0.0024],
        [-0.0018,  0.0011,  0.0003,  ...,  0.0003, -0.0015,  0.0027],
        ...,
        [ 0.0008, -0.0010,  0.0013,  ..., -0.0040, -0.0002, -0.0022],
        [ 0.0002,  0.0008,  0.0005,  ...,  0.0008, -0.0015, -0.0007],
        [ 0.0005, -0.0011, -0.0003,  ..., -0.0021,  0.0009, -0.0009]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.4199,  1.3672, -0.8960,  ...,  0.9121,  2.7266,  2.3242]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0810, -0.2468,  0.2683,  ..., -0.5835, -0.1970, -0.2107]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 01:47:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A state is a member of a country
A fish is a member of a school
A secretary is a member of a staff
A nomad is a member of a horde
A photo is a member of a album
A book is a member of a library
A flower is a member of a bouquet
A bee is a member of a
2024-07-13 01:47:31 root INFO     [order_1_approx] starting weight calculation for A photo is a member of a album
A state is a member of a country
A secretary is a member of a staff
A bee is a member of a swarm
A flower is a member of a bouquet
A fish is a member of a school
A book is a member of a library
A nomad is a member of a
2024-07-13 01:47:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 01:51:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 01:55:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0845, -0.8491,  0.2510,  ..., -0.2397, -0.0386, -0.5547],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.0748, -0.7886,  0.2052,  ..., -0.1974, -0.0491, -0.5239],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.2422,  0.8154, -0.9116,  ..., -0.1509,  0.7188,  1.2285],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0088, -0.0017,  0.0039,  ..., -0.0088, -0.0065, -0.0164],
        [-0.0023, -0.0058,  0.0019,  ...,  0.0026,  0.0038, -0.0024],
        [-0.0043,  0.0087,  0.0093,  ...,  0.0035,  0.0087,  0.0022],
        ...,
        [-0.0018, -0.0022, -0.0047,  ...,  0.0113, -0.0038,  0.0118],
        [ 0.0093, -0.0023,  0.0052,  ...,  0.0020, -0.0035,  0.0020],
        [-0.0026, -0.0044, -0.0010,  ..., -0.0038,  0.0016,  0.0051]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-9.1362e-04, -2.0349e-04,  1.1177e-03,  ...,  7.8440e-05,
          1.5712e-04,  1.1140e-04],
        [ 9.8515e-04, -7.7057e-04,  3.8099e-04,  ..., -5.5838e-04,
          4.6444e-04,  4.3213e-05],
        [-1.2779e-03, -1.2970e-04,  1.1101e-03,  ...,  1.1730e-03,
         -5.4836e-04,  1.1654e-03],
        ...,
        [ 1.5059e-03, -4.6039e-04,  5.7793e-04,  ...,  1.4505e-03,
          7.6532e-04,  1.1806e-03],
        [ 9.6607e-04, -2.8133e-05,  3.6812e-04,  ...,  5.0449e-04,
         -1.0419e-04,  6.0654e-04],
        [-5.9223e-04,  1.1721e-03,  6.1417e-04,  ..., -1.0759e-04,
         -5.8746e-04, -2.3007e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.1445,  1.1904, -0.9634,  ...,  0.0715,  0.4685,  1.2334]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1169, -0.1013,  0.0175,  ...,  0.1064,  0.0523,  0.1666]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 01:55:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A photo is a member of a album
A state is a member of a country
A secretary is a member of a staff
A bee is a member of a swarm
A flower is a member of a bouquet
A fish is a member of a school
A book is a member of a library
A nomad is a member of a
2024-07-13 01:55:24 root INFO     [order_1_approx] starting weight calculation for A nomad is a member of a horde
A bee is a member of a swarm
A photo is a member of a album
A flower is a member of a bouquet
A fish is a member of a school
A book is a member of a library
A secretary is a member of a staff
A state is a member of a
2024-07-13 01:55:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 01:59:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 02:03:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4788, -0.5010, -0.3845,  ...,  0.5820,  0.6465, -0.6895],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.5122, -0.5664, -0.4036,  ...,  0.6040,  0.6670, -0.7773],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.9648, -3.2617,  0.1504,  ..., -3.1797,  1.7705,  1.5146],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.0406e-02, -1.0124e-02, -1.0101e-02,  ...,  3.1281e-03,
          8.9111e-03, -1.7029e-02],
        [ 5.3024e-03, -1.9226e-02,  2.3460e-03,  ...,  7.2632e-03,
         -9.3384e-03,  2.0351e-03],
        [ 2.9583e-03,  6.3019e-03, -1.4565e-02,  ...,  9.9945e-03,
          1.9196e-02, -1.2497e-02],
        ...,
        [ 8.5068e-03,  6.8665e-05,  5.1384e-03,  ..., -1.7761e-02,
          1.4381e-02,  7.0419e-03],
        [ 1.2222e-02, -1.9577e-02,  6.7368e-03,  ..., -5.2643e-03,
         -2.9694e-02, -7.3318e-03],
        [-2.1305e-03, -7.9956e-03, -4.5242e-03,  ...,  9.2468e-03,
          4.1771e-03,  5.7869e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 9.1434e-05, -3.8338e-04, -1.3380e-03,  ...,  1.6975e-03,
          1.7338e-03, -1.1120e-03],
        [-6.2370e-04, -4.4405e-05, -1.1120e-03,  ..., -9.9373e-04,
         -2.3136e-03,  2.4490e-03],
        [-2.1219e-04, -8.1253e-04, -2.1572e-03,  ...,  8.2552e-05,
          3.9406e-03, -1.0252e-03],
        ...,
        [ 2.2373e-03,  4.0483e-04,  9.6130e-04,  ..., -2.7142e-03,
          2.9297e-03, -1.1644e-03],
        [ 1.2350e-04,  1.0090e-03,  1.3494e-03,  ...,  1.1005e-03,
         -3.1891e-03, -1.6365e-03],
        [-1.2798e-03, -1.0657e-04,  6.3229e-04,  ..., -3.8314e-04,
          4.1413e-04, -9.5606e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4805, -3.3223, -0.5850,  ..., -3.2305,  2.4199,  1.6973]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0342,  0.3381, -0.5205,  ..., -0.3267,  0.4231, -0.1135]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 02:03:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A nomad is a member of a horde
A bee is a member of a swarm
A photo is a member of a album
A flower is a member of a bouquet
A fish is a member of a school
A book is a member of a library
A secretary is a member of a staff
A state is a member of a
2024-07-13 02:03:16 root INFO     [order_1_approx] starting weight calculation for A state is a member of a country
A secretary is a member of a staff
A book is a member of a library
A bee is a member of a swarm
A flower is a member of a bouquet
A photo is a member of a album
A nomad is a member of a horde
A fish is a member of a
2024-07-13 02:03:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 02:07:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 02:11:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0122, -0.0841,  0.0079,  ...,  0.4194, -0.6133,  0.6899],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.0129, -0.0936, -0.0024,  ...,  0.3728, -0.5684,  0.6211],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5859,  1.8691, -2.7812,  ...,  1.4785, -0.0127,  0.3701],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0005, -0.0028,  0.0114,  ...,  0.0024, -0.0098,  0.0010],
        [-0.0256,  0.0041, -0.0143,  ...,  0.0108, -0.0301, -0.0047],
        [ 0.0231, -0.0028, -0.0011,  ..., -0.0164,  0.0267, -0.0017],
        ...,
        [-0.0089,  0.0019,  0.0029,  ..., -0.0033, -0.0143,  0.0016],
        [ 0.0073, -0.0079,  0.0036,  ...,  0.0015,  0.0034, -0.0091],
        [ 0.0192, -0.0035,  0.0090,  ..., -0.0061,  0.0159,  0.0061]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-4.9114e-04, -6.2513e-04,  3.1452e-03,  ...,  1.9217e-03,
          9.2602e-04, -1.4219e-03],
        [ 3.9792e-04, -1.2569e-03, -3.1471e-05,  ...,  1.7948e-03,
         -7.1955e-04, -8.0442e-04],
        [ 1.8859e-04, -1.6870e-03, -1.4076e-03,  ...,  8.9216e-04,
          1.0595e-03, -1.4067e-04],
        ...,
        [ 1.8520e-03, -3.5501e-04,  3.7518e-03,  ..., -1.4906e-03,
          7.6199e-04, -1.3514e-03],
        [ 7.2002e-04, -5.3453e-04, -2.3823e-03,  ..., -5.0354e-04,
          1.4772e-03, -1.7385e-03],
        [-7.9823e-04,  1.3676e-03, -1.4572e-03,  ..., -5.7364e-04,
         -1.8227e-04,  1.4420e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.8066,  1.5449, -2.2930,  ...,  1.5088,  0.9604,  1.4629]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1406, -0.0746, -0.2681,  ...,  0.1724, -0.4324,  0.1356]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 02:11:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A state is a member of a country
A secretary is a member of a staff
A book is a member of a library
A bee is a member of a swarm
A flower is a member of a bouquet
A photo is a member of a album
A nomad is a member of a horde
A fish is a member of a
2024-07-13 02:11:13 root INFO     [order_1_approx] starting weight calculation for A nomad is a member of a horde
A secretary is a member of a staff
A book is a member of a library
A flower is a member of a bouquet
A state is a member of a country
A fish is a member of a school
A bee is a member of a swarm
A photo is a member of a
2024-07-13 02:11:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 02:15:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 02:19:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.0154, 0.3628, 0.9297,  ..., 0.0447, 0.0271, 0.0403], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([0.0116, 0.3193, 0.7896,  ..., 0.0440, 0.0070, 0.0150], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.2383, -1.9629,  0.3872,  ..., -0.3955, -5.1523,  1.1807],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0138, -0.0112,  0.0051,  ..., -0.0004,  0.0020, -0.0027],
        [-0.0069, -0.0092, -0.0089,  ...,  0.0022, -0.0065,  0.0071],
        [ 0.0076, -0.0033, -0.0158,  ..., -0.0153,  0.0040,  0.0080],
        ...,
        [-0.0206, -0.0127,  0.0058,  ..., -0.0005, -0.0089,  0.0028],
        [ 0.0260,  0.0129,  0.0085,  ..., -0.0050, -0.0038, -0.0029],
        [ 0.0006,  0.0097,  0.0046,  ..., -0.0147,  0.0152,  0.0011]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.8845e-03,  2.9564e-04,  1.9741e-04,  ...,  1.0366e-03,
          6.4135e-04,  2.5940e-04],
        [ 8.2684e-04, -2.0370e-03,  1.0433e-03,  ..., -9.9850e-04,
          3.4761e-04, -1.0042e-03],
        [ 8.6212e-04, -1.6146e-03, -2.4853e-03,  ..., -1.3514e-03,
         -3.7432e-04, -2.0924e-03],
        ...,
        [ 1.3838e-03, -8.0681e-04,  2.1725e-03,  ..., -9.6083e-04,
          1.1816e-03,  6.1893e-04],
        [ 7.5102e-04, -9.2328e-05,  1.4091e-04,  ...,  1.8768e-03,
         -6.9714e-04, -2.3041e-03],
        [-7.9823e-04, -2.0993e-04,  4.4870e-04,  ..., -7.7391e-04,
          8.1718e-05, -5.7364e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.9297, -2.0566,  0.3943,  ..., -0.2119, -4.8203,  0.9800]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0116, -0.2173, -0.2043,  ..., -0.0302, -0.4077,  0.0354]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 02:19:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A nomad is a member of a horde
A secretary is a member of a staff
A book is a member of a library
A flower is a member of a bouquet
A state is a member of a country
A fish is a member of a school
A bee is a member of a swarm
A photo is a member of a
2024-07-13 02:19:07 root INFO     [order_1_approx] starting weight calculation for A nomad is a member of a horde
A bee is a member of a swarm
A secretary is a member of a staff
A state is a member of a country
A photo is a member of a album
A book is a member of a library
A fish is a member of a school
A flower is a member of a
2024-07-13 02:19:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 02:23:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 02:27:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5820,  0.0097,  0.6831,  ..., -0.3604, -0.6016,  0.0660],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.5317, -0.0078,  0.5874,  ..., -0.3130, -0.5630,  0.0384],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3069,  1.0039,  0.0508,  ...,  1.4492,  0.2332, -1.1709],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0084, -0.0090,  0.0133,  ...,  0.0019,  0.0037, -0.0082],
        [-0.0205,  0.0040,  0.0070,  ..., -0.0019, -0.0228, -0.0020],
        [ 0.0042,  0.0117, -0.0048,  ..., -0.0118,  0.0079,  0.0137],
        ...,
        [ 0.0015, -0.0108,  0.0121,  ..., -0.0083, -0.0178,  0.0145],
        [ 0.0164, -0.0141, -0.0078,  ...,  0.0083,  0.0041, -0.0079],
        [ 0.0029, -0.0208, -0.0089,  ..., -0.0015,  0.0091,  0.0005]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.4992e-03, -2.0294e-03,  1.6394e-03,  ...,  2.2564e-03,
          8.4591e-04, -5.6076e-04],
        [ 2.2945e-03, -2.6817e-03, -1.6832e-04,  ...,  6.4707e-04,
          7.1526e-04, -1.2827e-03],
        [-7.0667e-04,  9.9778e-05, -1.8291e-03,  ..., -1.5478e-03,
          6.3181e-04,  1.4687e-03],
        ...,
        [ 2.1877e-03, -2.0714e-03,  2.6779e-03,  ...,  1.8728e-04,
          2.4090e-03,  3.9077e-04],
        [ 1.5373e-03, -4.2582e-04, -1.0643e-03,  ..., -5.5313e-05,
         -1.9245e-03, -8.0919e-04],
        [-2.9147e-05, -7.1049e-04, -6.6614e-04,  ..., -3.0060e-03,
         -1.9217e-03,  1.6470e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4312,  0.5752,  0.0351,  ...,  1.4385,  0.2402, -0.9409]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0227, -0.1870,  0.1516,  ...,  0.1823, -0.3315,  0.2903]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 02:27:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A nomad is a member of a horde
A bee is a member of a swarm
A secretary is a member of a staff
A state is a member of a country
A photo is a member of a album
A book is a member of a library
A fish is a member of a school
A flower is a member of a
2024-07-13 02:27:04 root INFO     total operator prediction time: 3802.209968805313 seconds
2024-07-13 02:27:04 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on name - nationality
2024-07-13 02:27:06 root INFO     building operator name - nationality
2024-07-13 02:27:06 root INFO     [order_1_approx] starting weight calculation for edison was american
rousseau was french
hume was scottish
lavoisier was french
darwin was english
euclid was greek
kepler was german
copernicus was
2024-07-13 02:27:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 02:31:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 02:34:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4041,  1.2666, -1.3184,  ..., -0.5249,  0.8052,  1.3184],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.3672,  1.1514, -1.1426,  ..., -0.4480,  0.6963,  1.1875],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0113, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.2559, -5.6172, -6.3359,  ..., -1.4150, -2.9160,  0.0000],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0030, -0.0132,  0.0179,  ..., -0.0113, -0.0050,  0.0063],
        [ 0.0046,  0.0037,  0.0087,  ..., -0.0023, -0.0016, -0.0152],
        [ 0.0199,  0.0110, -0.0094,  ..., -0.0169,  0.0086, -0.0109],
        ...,
        [-0.0124,  0.0168, -0.0049,  ..., -0.0018, -0.0018,  0.0173],
        [-0.0008,  0.0051, -0.0046,  ..., -0.0135,  0.0031,  0.0107],
        [ 0.0085, -0.0009, -0.0159,  ..., -0.0319,  0.0016, -0.0011]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 8.3160e-04, -7.1716e-04,  1.1444e-05,  ..., -1.9073e-03,
         -2.6226e-04,  7.2718e-04],
        [-6.5994e-04,  9.8324e-04,  3.0732e-04,  ...,  1.4067e-04,
          8.0538e-04, -1.7872e-03],
        [-6.3896e-04,  2.8973e-03,  4.1151e-04,  ..., -9.4080e-04,
          5.7364e-04,  9.6416e-04],
        ...,
        [ 7.7963e-04,  1.0920e-03,  5.3453e-04,  ..., -1.5278e-03,
         -1.1282e-03,  1.6823e-03],
        [-1.4076e-03,  1.6880e-03,  1.9455e-04,  ...,  1.2465e-03,
         -3.3712e-04,  1.6570e-05],
        [-2.7695e-03, -1.2407e-03, -2.7809e-03,  ..., -5.8365e-04,
          6.4468e-04, -1.4305e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1406, -6.1406, -5.8789,  ..., -0.8911, -2.6602,  1.0576]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1259, -0.0846,  0.0585,  ..., -0.2290,  0.0225,  0.0850]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 02:34:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for edison was american
rousseau was french
hume was scottish
lavoisier was french
darwin was english
euclid was greek
kepler was german
copernicus was
2024-07-13 02:34:58 root INFO     [order_1_approx] starting weight calculation for rousseau was french
darwin was english
kepler was german
hume was scottish
lavoisier was french
copernicus was polish
euclid was greek
edison was
2024-07-13 02:34:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 02:38:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 02:42:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4102,  0.8887,  0.9639,  ...,  1.4189, -0.1785,  0.4526],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.4062,  0.8804,  0.8950,  ...,  1.3428, -0.1899,  0.4324],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0275, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7861, -0.6147, -3.0449,  ..., -1.5938, -0.7275, -2.2949],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0101, -0.0075,  0.0083,  ..., -0.0173,  0.0012,  0.0046],
        [ 0.0087,  0.0110, -0.0061,  ...,  0.0020, -0.0051, -0.0160],
        [ 0.0049,  0.0204,  0.0110,  ...,  0.0100,  0.0071,  0.0054],
        ...,
        [-0.0115, -0.0050, -0.0108,  ...,  0.0016,  0.0087,  0.0061],
        [-0.0052, -0.0234, -0.0114,  ...,  0.0001, -0.0018,  0.0084],
        [-0.0013,  0.0107,  0.0076,  ..., -0.0139,  0.0070,  0.0118]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 1.6966e-03,  3.8409e-04,  1.6308e-03,  ...,  3.3760e-04,
         -2.1946e-04, -8.9073e-04],
        [ 1.8091e-03, -2.0862e-04,  1.2894e-03,  ..., -5.2166e-04,
          6.3753e-04, -1.6975e-03],
        [-3.0541e-04, -8.8549e-04, -6.2656e-04,  ..., -8.7738e-04,
          1.1539e-04,  1.8082e-03],
        ...,
        [ 2.6441e-04, -7.3957e-04, -7.9679e-04,  ..., -1.5068e-03,
          1.2913e-03, -1.9550e-04],
        [ 7.2622e-04,  3.4165e-04,  1.1702e-03,  ...,  8.1897e-05,
         -6.9618e-05,  1.1539e-03],
        [-1.8415e-03,  8.9359e-04, -5.3215e-04,  ...,  2.9964e-03,
         -8.4162e-04,  4.3392e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5322, -0.5869, -3.8633,  ..., -2.0137, -0.7046, -2.3887]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1600, -0.2201,  0.1356,  ...,  0.0172, -0.0153, -0.2744]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 02:42:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for rousseau was french
darwin was english
kepler was german
hume was scottish
lavoisier was french
copernicus was polish
euclid was greek
edison was
2024-07-13 02:42:49 root INFO     [order_1_approx] starting weight calculation for lavoisier was french
copernicus was polish
edison was american
darwin was english
euclid was greek
hume was scottish
rousseau was french
kepler was
2024-07-13 02:42:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 02:46:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 02:50:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7812, -0.0402, -0.5991,  ...,  1.3623, -0.0662,  0.4727],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.7935, -0.0579, -0.5879,  ...,  1.3232, -0.0837,  0.4644],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0018, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5381, -2.3535, -6.1758,  ..., -2.6191, -4.9180, -1.4629],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0103, -0.0244,  0.0036,  ..., -0.0022,  0.0153,  0.0058],
        [ 0.0121,  0.0283, -0.0013,  ..., -0.0023,  0.0174,  0.0062],
        [ 0.0272,  0.0136, -0.0168,  ..., -0.0130,  0.0042, -0.0143],
        ...,
        [-0.0100, -0.0257,  0.0045,  ...,  0.0180, -0.0041, -0.0222],
        [ 0.0145,  0.0058,  0.0118,  ..., -0.0183,  0.0173,  0.0174],
        [-0.0120,  0.0271, -0.0026,  ..., -0.0162,  0.0001,  0.0036]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 4.7624e-05, -1.5030e-03,  1.1559e-03,  ..., -1.4954e-03,
         -1.8215e-04,  1.6174e-03],
        [ 1.4648e-03,  2.1324e-03, -1.0977e-03,  ...,  1.2720e-04,
          8.3542e-04, -9.6703e-04],
        [ 1.6212e-03,  7.2193e-04, -1.4400e-04,  ...,  3.3379e-04,
          2.2430e-03,  7.2098e-04],
        ...,
        [-5.3310e-04,  1.0004e-03,  1.2426e-03,  ...,  2.3746e-04,
         -3.5477e-04, -9.0790e-04],
        [ 4.4465e-04, -2.2960e-04,  9.5034e-04,  ..., -4.5228e-04,
         -2.8381e-03, -1.9646e-04],
        [-2.6073e-03,  1.1778e-03,  8.9169e-04,  ..., -9.0301e-05,
          6.7759e-04,  9.6798e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3684, -1.4531, -5.3516,  ..., -3.0820, -4.7969, -0.7886]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0742,  0.0614,  0.3477,  ..., -0.1533,  0.0407,  0.0584]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 02:50:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for lavoisier was french
copernicus was polish
edison was american
darwin was english
euclid was greek
hume was scottish
rousseau was french
kepler was
2024-07-13 02:50:43 root INFO     [order_1_approx] starting weight calculation for darwin was english
hume was scottish
lavoisier was french
edison was american
copernicus was polish
euclid was greek
kepler was german
rousseau was
2024-07-13 02:50:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 02:54:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 02:58:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0974,  0.0396, -0.0164,  ...,  0.0664,  0.1055,  0.3965],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.1033,  0.0291, -0.0254,  ...,  0.0747,  0.0954,  0.4124],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0167, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.8203, -1.0615, -7.0664,  ..., -6.3438, -2.1191, -0.4561],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0038, -0.0112,  0.0224,  ...,  0.0006,  0.0088,  0.0105],
        [ 0.0172,  0.0126,  0.0116,  ...,  0.0059,  0.0051, -0.0053],
        [ 0.0154,  0.0065, -0.0254,  ..., -0.0119,  0.0211,  0.0090],
        ...,
        [ 0.0008,  0.0031, -0.0117,  ...,  0.0011,  0.0057,  0.0084],
        [ 0.0154,  0.0251, -0.0067,  ...,  0.0118,  0.0300,  0.0166],
        [-0.0062, -0.0083, -0.0056,  ..., -0.0067,  0.0157,  0.0036]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-6.2227e-04, -3.3426e-04, -1.5688e-04,  ..., -4.2439e-04,
         -1.4663e-04,  9.1314e-04],
        [ 2.1515e-03, -3.5691e-04,  1.4544e-03,  ...,  9.8705e-04,
          1.0147e-03, -2.2068e-03],
        [-1.6618e-04,  1.1921e-03, -1.3752e-03,  ...,  3.7622e-04,
          4.1151e-04, -5.7459e-04],
        ...,
        [-7.5531e-04,  7.0810e-04,  5.1117e-04,  ..., -4.5729e-04,
          2.8253e-04, -3.3426e-04],
        [ 9.9063e-05,  8.9073e-04, -4.7982e-05,  ..., -7.9489e-04,
          8.6069e-05, -4.5013e-04],
        [-1.9875e-03, -5.2881e-04, -7.5912e-04,  ..., -5.5122e-04,
          7.2908e-04, -3.2759e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.8477, -0.9307, -7.5117,  ..., -6.4219, -2.3027, -0.4915]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0800, -0.2764,  0.1350,  ..., -0.0836, -0.0504,  0.0488]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 02:58:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for darwin was english
hume was scottish
lavoisier was french
edison was american
copernicus was polish
euclid was greek
kepler was german
rousseau was
2024-07-13 02:58:38 root INFO     [order_1_approx] starting weight calculation for euclid was greek
edison was american
hume was scottish
kepler was german
darwin was english
rousseau was french
copernicus was polish
lavoisier was
2024-07-13 02:58:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 03:02:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 03:06:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1604,  0.1541,  0.6724,  ...,  0.9277,  0.0327,  1.8828],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1741,  0.1516,  0.6724,  ...,  0.9512,  0.0164,  2.0215],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0375, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9746, -0.0676, -5.9609,  ..., -4.2969, -4.2891, -1.9883],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0119, -0.0280,  0.0254,  ..., -0.0441,  0.0110,  0.0086],
        [ 0.0036,  0.0294,  0.0126,  ...,  0.0122, -0.0008,  0.0014],
        [ 0.0231,  0.0230, -0.0088,  ..., -0.0338,  0.0174,  0.0164],
        ...,
        [-0.0009,  0.0202, -0.0016,  ..., -0.0073, -0.0444,  0.0174],
        [ 0.0070,  0.0039,  0.0014,  ..., -0.0026, -0.0108, -0.0045],
        [-0.0145,  0.0374,  0.0157,  ..., -0.0235, -0.0043,  0.0185]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 1.1215e-03, -1.9503e-03,  2.0838e-04,  ...,  9.3889e-04,
          3.3188e-04,  2.1935e-03],
        [ 1.1930e-03, -8.6451e-04, -1.7166e-04,  ...,  9.5844e-04,
          1.5278e-03, -7.3433e-04],
        [-2.9230e-04,  2.1725e-03, -1.1692e-03,  ..., -1.3599e-03,
          1.1072e-03, -5.5122e-04],
        ...,
        [ 9.9087e-04,  2.9984e-03, -2.4910e-03,  ..., -7.6675e-04,
          8.3923e-04,  1.5640e-04],
        [-3.9244e-04,  1.3123e-03,  6.9284e-04,  ..., -8.1897e-05,
         -7.7963e-04, -5.9986e-04],
        [ 6.1750e-04,  1.4400e-03,  2.6226e-05,  ...,  1.8179e-04,
          5.7125e-04, -1.7366e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.6650, -0.2329, -5.2734,  ..., -3.6602, -3.9648, -1.4551]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.2153, -0.2646,  0.1299,  ..., -0.0025, -0.1055, -0.1661]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 03:06:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for euclid was greek
edison was american
hume was scottish
kepler was german
darwin was english
rousseau was french
copernicus was polish
lavoisier was
2024-07-13 03:06:33 root INFO     [order_1_approx] starting weight calculation for kepler was german
hume was scottish
edison was american
lavoisier was french
copernicus was polish
darwin was english
rousseau was french
euclid was
2024-07-13 03:06:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 03:10:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 03:14:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2336, -0.0164, -0.0208,  ..., -0.1979,  0.7285,  0.0054],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.2101, -0.0285, -0.0257,  ..., -0.1631,  0.6304, -0.0150],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0036, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4746,  0.0358, -3.3008,  ..., -1.4229, -4.3516, -2.8516],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0058,  0.0090, -0.0056,  ..., -0.0045,  0.0060, -0.0088],
        [-0.0006,  0.0238,  0.0053,  ..., -0.0144, -0.0134, -0.0050],
        [ 0.0002,  0.0034,  0.0083,  ..., -0.0033,  0.0262,  0.0012],
        ...,
        [-0.0022,  0.0041,  0.0079,  ...,  0.0217,  0.0045,  0.0112],
        [-0.0110,  0.0107, -0.0073,  ..., -0.0060,  0.0041,  0.0018],
        [ 0.0127,  0.0016, -0.0075,  ..., -0.0121,  0.0123,  0.0258]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.9188e-03,  1.4734e-03,  1.4172e-03,  ..., -1.0908e-04,
          2.0161e-03,  1.2722e-03],
        [ 2.4605e-03,  2.4796e-03, -6.3896e-05,  ...,  1.3247e-03,
         -8.0776e-04, -1.7548e-03],
        [-7.3719e-04, -1.7118e-04,  1.1377e-03,  ..., -1.4172e-03,
          1.3533e-03,  8.2302e-04],
        ...,
        [-1.7834e-04,  2.9583e-03,  7.5197e-04,  ...,  6.0034e-04,
          1.8339e-03, -2.0103e-03],
        [-5.7983e-04,  1.5936e-03,  7.5722e-04,  ...,  9.9945e-04,
         -1.3075e-03,  9.4795e-04],
        [-1.6861e-03,  1.6336e-03,  1.4544e-03,  ...,  1.1759e-03,
         -1.6098e-03,  7.5817e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0293, -0.9238, -2.7969,  ..., -1.6348, -4.5820, -2.3555]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2700, -0.1450, -0.1520,  ..., -0.1483, -0.0800,  0.0487]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 03:14:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for kepler was german
hume was scottish
edison was american
lavoisier was french
copernicus was polish
darwin was english
rousseau was french
euclid was
2024-07-13 03:14:26 root INFO     [order_1_approx] starting weight calculation for rousseau was french
kepler was german
hume was scottish
lavoisier was french
edison was american
euclid was greek
copernicus was polish
darwin was
2024-07-13 03:14:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 03:18:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 03:22:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3589, -0.6309, -0.0334,  ...,  0.4622,  0.0872,  0.4539],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3384, -0.6270, -0.0393,  ...,  0.4253,  0.0649,  0.4170],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0275, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7520, -1.5820, -3.4785,  ..., -2.3867, -1.7676, -4.5898],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0069, -0.0067, -0.0056,  ..., -0.0070, -0.0048, -0.0001],
        [ 0.0131,  0.0125,  0.0011,  ...,  0.0109,  0.0024,  0.0012],
        [ 0.0051,  0.0008, -0.0090,  ..., -0.0098,  0.0044, -0.0041],
        ...,
        [-0.0068,  0.0144,  0.0064,  ...,  0.0035,  0.0017,  0.0069],
        [-0.0063,  0.0059,  0.0209,  ...,  0.0045,  0.0124,  0.0160],
        [ 0.0018,  0.0155, -0.0010,  ..., -0.0160, -0.0101,  0.0028]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 5.5170e-04, -5.3501e-04,  1.8072e-03,  ..., -1.6379e-04,
          2.7609e-04,  1.3046e-03],
        [ 1.3638e-03,  3.4714e-04,  4.2486e-04,  ...,  1.4677e-03,
         -1.8191e-04, -2.0390e-03],
        [ 8.2016e-05, -8.3208e-05, -3.1586e-03,  ..., -1.9264e-03,
          2.2240e-03,  1.0204e-03],
        ...,
        [ 1.7428e-04,  1.8606e-03,  7.5006e-04,  ...,  3.2759e-04,
         -7.1239e-04, -7.6389e-04],
        [ 2.4796e-04,  7.7963e-04,  1.5373e-03,  ...,  2.7299e-04,
         -9.3699e-05,  1.0710e-03],
        [ 1.1940e-03,  3.3617e-04, -9.1887e-04,  ..., -1.4334e-03,
          4.2224e-04, -4.0030e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3184, -1.4150, -3.2930,  ..., -2.0117, -1.5059, -4.5859]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.2546, -0.2404,  0.0917,  ..., -0.1932,  0.0900, -0.1255]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 03:22:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for rousseau was french
kepler was german
hume was scottish
lavoisier was french
edison was american
euclid was greek
copernicus was polish
darwin was
2024-07-13 03:22:22 root INFO     [order_1_approx] starting weight calculation for euclid was greek
kepler was german
darwin was english
rousseau was french
lavoisier was french
edison was american
copernicus was polish
hume was
2024-07-13 03:22:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 03:26:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 03:30:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3062, -0.6279,  0.0400,  ...,  0.2954,  0.0024,  1.7129],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3057, -0.6626,  0.0283,  ...,  0.2898, -0.0149,  1.7354],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0375, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0059, -1.8662, -8.0312,  ..., -1.7637, -1.1875, -2.6328],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0325, -0.0249, -0.0014,  ..., -0.0054,  0.0154,  0.0148],
        [ 0.0225, -0.0146, -0.0052,  ...,  0.0025,  0.0125, -0.0008],
        [ 0.0482,  0.0231, -0.0263,  ...,  0.0049,  0.0278, -0.0104],
        ...,
        [-0.0058, -0.0011, -0.0122,  ...,  0.0172, -0.0035,  0.0016],
        [-0.0155, -0.0086,  0.0061,  ..., -0.0322,  0.0052,  0.0360],
        [-0.0029, -0.0007, -0.0012,  ..., -0.0125,  0.0005,  0.0210]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 1.8339e-03, -1.1034e-03,  9.0122e-04,  ...,  1.8902e-03,
          4.3488e-04,  1.1482e-03],
        [ 1.3180e-03, -1.2226e-03,  1.5831e-03,  ...,  9.4128e-04,
          1.4105e-03, -5.8174e-04],
        [ 5.0497e-04,  2.9316e-03, -2.0885e-03,  ..., -1.0300e-03,
          1.2722e-03,  3.0994e-06],
        ...,
        [ 1.6031e-03,  1.2007e-03,  5.1403e-04,  ...,  1.4153e-03,
          8.1778e-05, -1.5378e-05],
        [ 9.8801e-04,  2.0409e-04,  1.2264e-03,  ...,  5.7507e-04,
          5.4407e-04, -1.1787e-03],
        [-7.8678e-05, -2.1896e-03, -7.3624e-04,  ..., -3.2043e-04,
          4.2081e-04,  1.1082e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.1133, -2.4043, -7.9570,  ..., -1.9873, -0.3618, -2.6621]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1075,  0.0658,  0.1235,  ..., -0.2329, -0.0371, -0.0880]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 03:30:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for euclid was greek
kepler was german
darwin was english
rousseau was french
lavoisier was french
edison was american
copernicus was polish
hume was
2024-07-13 03:30:18 root INFO     total operator prediction time: 3792.3851051330566 seconds
2024-07-13 03:30:18 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on country - language
2024-07-13 03:30:20 root INFO     building operator country - language
2024-07-13 03:30:21 root INFO     [order_1_approx] starting weight calculation for The country of chile primarily speaks the language of spanish
The country of cuba primarily speaks the language of spanish
The country of australia primarily speaks the language of english
The country of ecuador primarily speaks the language of spanish
The country of norway primarily speaks the language of norwegian
The country of philippines primarily speaks the language of tagalog
The country of peru primarily speaks the language of spanish
The country of bahamas primarily speaks the language of
2024-07-13 03:30:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 03:34:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 03:38:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0410,  0.1703, -1.6143,  ...,  0.4963,  0.9189, -0.3896],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.9966,  0.1505, -1.4883,  ...,  0.4604,  0.8506, -0.4048],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2266, -4.6133, -7.3711,  ..., -1.6309,  1.7725,  1.8799],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0102, -0.0011,  0.0073,  ..., -0.0039, -0.0101, -0.0040],
        [-0.0060,  0.0033,  0.0025,  ...,  0.0190,  0.0083, -0.0005],
        [-0.0064,  0.0028,  0.0021,  ..., -0.0085, -0.0028, -0.0009],
        ...,
        [-0.0011, -0.0027, -0.0053,  ...,  0.0086, -0.0054, -0.0005],
        [-0.0017,  0.0022,  0.0027,  ..., -0.0008, -0.0082,  0.0052],
        [-0.0103,  0.0044, -0.0037,  ..., -0.0003,  0.0104,  0.0148]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-6.8903e-05, -4.8828e-04,  5.6505e-04,  ...,  1.1253e-03,
         -1.4472e-04, -1.1873e-03],
        [ 5.1498e-04, -1.4687e-03,  1.0023e-03,  ...,  5.6314e-04,
         -1.1647e-04, -2.8591e-03],
        [-6.3848e-04,  5.8270e-04, -5.6648e-04,  ...,  1.0586e-04,
          3.4976e-04,  1.0079e-04],
        ...,
        [ 8.3923e-04, -1.5907e-03, -3.2997e-04,  ...,  5.5075e-05,
          4.9400e-04, -4.4656e-04],
        [ 4.0197e-04, -2.0647e-04,  1.2910e-04,  ..., -6.5756e-04,
         -5.2404e-04,  4.0770e-04],
        [-1.9102e-03, -4.3130e-04, -1.1053e-03,  ..., -2.5630e-04,
         -5.4646e-04,  1.0672e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4180, -5.3359, -7.7617,  ..., -1.7559,  2.0996,  1.2539]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0179, -0.2451,  0.0062,  ..., -0.0944,  0.1271,  0.1086]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 03:38:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of chile primarily speaks the language of spanish
The country of cuba primarily speaks the language of spanish
The country of australia primarily speaks the language of english
The country of ecuador primarily speaks the language of spanish
The country of norway primarily speaks the language of norwegian
The country of philippines primarily speaks the language of tagalog
The country of peru primarily speaks the language of spanish
The country of bahamas primarily speaks the language of
2024-07-13 03:38:15 root INFO     [order_1_approx] starting weight calculation for The country of cuba primarily speaks the language of spanish
The country of philippines primarily speaks the language of tagalog
The country of australia primarily speaks the language of english
The country of norway primarily speaks the language of norwegian
The country of bahamas primarily speaks the language of english
The country of chile primarily speaks the language of spanish
The country of ecuador primarily speaks the language of spanish
The country of peru primarily speaks the language of
2024-07-13 03:38:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 03:42:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 03:46:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1399, -1.0645, -2.3047,  ..., -0.0906,  0.8975, -0.2698],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1191, -0.9399, -1.8760,  ..., -0.0662,  0.7368, -0.2537],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.5254, -0.5801, -8.2344,  ..., -1.8418,  3.3672, -2.0508],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 5.3787e-04, -8.7891e-03, -6.3591e-03,  ..., -4.0359e-03,
         -1.2093e-03,  6.4774e-03],
        [-7.3090e-03,  1.4740e-02, -5.2032e-03,  ...,  1.0662e-03,
         -4.1962e-05, -8.8654e-03],
        [ 1.2039e-02, -1.2917e-02, -2.1271e-02,  ..., -1.7761e-02,
         -2.8992e-04,  5.5313e-04],
        ...,
        [-6.3782e-03, -5.0850e-03,  1.7929e-03,  ..., -4.5052e-03,
          4.0436e-03,  1.0548e-03],
        [-4.2496e-03, -1.5282e-02,  6.0272e-04,  ...,  3.7498e-03,
         -7.8430e-03,  4.1504e-03],
        [-1.4923e-02,  2.1057e-03,  6.7787e-03,  ..., -7.9346e-03,
          6.3171e-03, -4.6921e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-6.9332e-04,  4.7398e-04,  1.0405e-03,  ...,  1.1549e-03,
         -3.1328e-04, -7.2002e-04],
        [ 5.1022e-04, -1.7726e-04,  8.7023e-04,  ..., -7.5340e-04,
         -7.9513e-05, -2.2507e-03],
        [-8.9741e-04,  9.3174e-04, -9.8324e-04,  ..., -8.6212e-04,
         -6.3992e-04,  1.4105e-03],
        ...,
        [ 1.7586e-03, -9.5510e-04, -1.6088e-03,  ...,  2.8324e-04,
          1.3990e-03, -7.7248e-04],
        [-1.2898e-04, -6.5136e-04, -5.4169e-04,  ..., -1.9093e-03,
          3.2759e-04,  2.3222e-04],
        [-1.6279e-03,  2.4433e-03, -6.9809e-04,  ...,  3.9816e-04,
          2.4438e-04,  1.4343e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.0293, -0.8735, -6.4922,  ..., -1.7676,  2.5605, -1.5459]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0486, -0.1382,  0.1627,  ..., -0.1705,  0.0379,  0.0954]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 03:46:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of cuba primarily speaks the language of spanish
The country of philippines primarily speaks the language of tagalog
The country of australia primarily speaks the language of english
The country of norway primarily speaks the language of norwegian
The country of bahamas primarily speaks the language of english
The country of chile primarily speaks the language of spanish
The country of ecuador primarily speaks the language of spanish
The country of peru primarily speaks the language of
2024-07-13 03:46:09 root INFO     [order_1_approx] starting weight calculation for The country of norway primarily speaks the language of norwegian
The country of australia primarily speaks the language of english
The country of chile primarily speaks the language of spanish
The country of bahamas primarily speaks the language of english
The country of ecuador primarily speaks the language of spanish
The country of cuba primarily speaks the language of spanish
The country of peru primarily speaks the language of spanish
The country of philippines primarily speaks the language of
2024-07-13 03:46:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 03:50:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 03:53:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6089, -0.2396, -0.4756,  ...,  1.2930,  1.0762, -0.3167],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.5825, -0.2500, -0.4421,  ...,  1.1816,  0.9893, -0.3303],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8086, -3.4512, -7.5195,  ..., -2.9648,  1.8604, -2.7188],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 5.3215e-03, -4.5738e-03,  9.9850e-04,  ...,  7.1335e-04,
          5.8746e-04,  4.1122e-03],
        [-3.5000e-03,  1.2848e-02,  1.5610e-02,  ..., -5.8174e-05,
          4.9591e-03, -4.1294e-04],
        [-2.9297e-03,  9.8038e-03, -4.9133e-03,  ..., -1.5678e-03,
         -2.5406e-03,  6.4468e-03],
        ...,
        [-1.3054e-02, -3.8338e-03, -1.8644e-04,  ...,  3.0251e-03,
         -6.7749e-03, -3.4637e-03],
        [-5.6992e-03, -5.0366e-05,  1.1307e-02,  ..., -9.9182e-03,
         -1.0529e-03, -7.2746e-03],
        [-5.5084e-03,  1.9379e-03,  1.0071e-03,  ..., -7.0724e-03,
         -1.5526e-03,  6.3705e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.0329e-04, -9.2030e-04,  3.3855e-04,  ...,  1.2217e-03,
         -5.0902e-05, -5.1212e-04],
        [ 5.2023e-04,  3.6478e-04,  1.5268e-03,  ...,  2.9635e-04,
         -1.0614e-03, -3.9935e-04],
        [-7.2098e-04,  1.3471e-04,  1.8158e-03,  ...,  1.2493e-03,
          1.8082e-03,  6.2323e-04],
        ...,
        [ 7.7057e-04, -2.4104e-04,  1.9145e-04,  ...,  1.6320e-04,
         -3.5048e-04, -1.4277e-03],
        [-3.8433e-04,  2.0981e-05,  1.3304e-03,  ...,  1.0958e-03,
          4.2295e-04, -2.2578e-04],
        [-8.5783e-04,  2.9135e-04, -1.0359e-04,  ...,  5.6362e-04,
         -8.9288e-05,  1.7052e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9746, -3.6387, -7.6719,  ..., -3.3008,  2.0352, -2.8301]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0163,  0.1090, -0.0885,  ..., -0.0331,  0.0011,  0.1226]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 03:54:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of norway primarily speaks the language of norwegian
The country of australia primarily speaks the language of english
The country of chile primarily speaks the language of spanish
The country of bahamas primarily speaks the language of english
The country of ecuador primarily speaks the language of spanish
The country of cuba primarily speaks the language of spanish
The country of peru primarily speaks the language of spanish
The country of philippines primarily speaks the language of
2024-07-13 03:54:01 root INFO     [order_1_approx] starting weight calculation for The country of australia primarily speaks the language of english
The country of cuba primarily speaks the language of spanish
The country of ecuador primarily speaks the language of spanish
The country of bahamas primarily speaks the language of english
The country of philippines primarily speaks the language of tagalog
The country of peru primarily speaks the language of spanish
The country of norway primarily speaks the language of norwegian
The country of chile primarily speaks the language of
2024-07-13 03:54:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 03:58:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 04:01:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0811, -0.4424, -1.0703,  ..., -0.0253,  1.8086, -0.6680],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.0714, -0.4277, -0.9380,  ..., -0.0167,  1.6035, -0.6416],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.5254, -1.5332, -4.1953,  ..., -1.8428,  2.0410, -2.9707],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0010, -0.0014, -0.0060,  ..., -0.0067,  0.0032,  0.0049],
        [-0.0047,  0.0109,  0.0094,  ...,  0.0125,  0.0120, -0.0236],
        [-0.0027,  0.0041, -0.0080,  ..., -0.0037, -0.0016,  0.0202],
        ...,
        [-0.0038, -0.0081,  0.0035,  ...,  0.0033,  0.0003, -0.0049],
        [ 0.0079,  0.0014, -0.0032,  ..., -0.0216, -0.0036,  0.0115],
        [-0.0083,  0.0019,  0.0056,  ..., -0.0092,  0.0060,  0.0084]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.8644e-03, -3.7336e-04,  2.8491e-04,  ...,  1.5802e-03,
         -4.5228e-04, -1.4248e-03],
        [-6.0511e-04,  2.6722e-03,  5.1832e-04,  ..., -1.2589e-03,
          1.9026e-04, -1.4019e-03],
        [ 4.1604e-04, -1.3428e-03, -2.3162e-04,  ..., -1.1473e-03,
          7.4100e-04,  9.5940e-04],
        ...,
        [ 8.4996e-05, -6.8617e-04, -5.6458e-04,  ..., -3.2949e-04,
          1.2422e-04, -1.4076e-03],
        [-1.0338e-03,  1.6270e-03,  1.8511e-03,  ...,  1.0782e-04,
         -1.2326e-04,  3.0947e-04],
        [-2.8062e-04,  3.8099e-04, -1.1358e-03,  ..., -2.1439e-03,
         -2.1577e-04,  1.4496e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7197, -2.1211, -4.1914,  ..., -2.1172,  2.4004, -2.6230]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0643,  0.1038,  0.1334,  ..., -0.0311, -0.2109,  0.4612]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 04:01:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of australia primarily speaks the language of english
The country of cuba primarily speaks the language of spanish
The country of ecuador primarily speaks the language of spanish
The country of bahamas primarily speaks the language of english
The country of philippines primarily speaks the language of tagalog
The country of peru primarily speaks the language of spanish
The country of norway primarily speaks the language of norwegian
The country of chile primarily speaks the language of
2024-07-13 04:01:55 root INFO     [order_1_approx] starting weight calculation for The country of cuba primarily speaks the language of spanish
The country of australia primarily speaks the language of english
The country of ecuador primarily speaks the language of spanish
The country of philippines primarily speaks the language of tagalog
The country of peru primarily speaks the language of spanish
The country of chile primarily speaks the language of spanish
The country of bahamas primarily speaks the language of english
The country of norway primarily speaks the language of
2024-07-13 04:01:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 04:05:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 04:09:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2881,  0.9072, -1.1289,  ..., -0.4460,  0.5059,  0.2429],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.2717,  0.8516, -1.0146,  ..., -0.3938,  0.4480,  0.2090],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9922, -2.8125, -3.6992,  ..., -5.8984, -3.2539,  0.2515],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0139, -0.0009, -0.0113,  ..., -0.0138, -0.0069, -0.0027],
        [ 0.0077,  0.0011,  0.0021,  ...,  0.0070,  0.0102, -0.0155],
        [-0.0108,  0.0122, -0.0017,  ..., -0.0017, -0.0029,  0.0164],
        ...,
        [ 0.0001, -0.0062,  0.0092,  ...,  0.0087, -0.0029,  0.0060],
        [ 0.0035,  0.0108,  0.0045,  ..., -0.0095, -0.0055,  0.0012],
        [-0.0012, -0.0028, -0.0046,  ..., -0.0054,  0.0081,  0.0112]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-7.8297e-04, -5.0735e-04, -3.2330e-04,  ..., -3.4285e-04,
          3.5620e-04, -1.4725e-03],
        [ 4.1032e-04, -1.3456e-03,  2.4662e-03,  ...,  3.6764e-04,
          1.2236e-03, -2.3708e-03],
        [ 1.1533e-04,  1.7662e-03, -8.4400e-04,  ..., -2.0447e-03,
          2.2650e-05,  1.9426e-03],
        ...,
        [ 9.4986e-04,  3.5691e-04,  3.3736e-05,  ...,  4.0197e-04,
          9.8991e-04,  2.6226e-04],
        [-2.8276e-04,  1.7509e-03,  7.9727e-04,  ...,  2.1839e-04,
          8.9824e-05,  8.8358e-04],
        [-2.5082e-04, -7.8011e-04, -2.7704e-04,  ..., -3.1066e-04,
          5.4979e-04,  9.2411e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5596, -2.9023, -3.2188,  ..., -5.4375, -2.5879,  0.3762]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0905, -0.2251,  0.2661,  ...,  0.0800,  0.1525,  0.0415]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 04:09:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of cuba primarily speaks the language of spanish
The country of australia primarily speaks the language of english
The country of ecuador primarily speaks the language of spanish
The country of philippines primarily speaks the language of tagalog
The country of peru primarily speaks the language of spanish
The country of chile primarily speaks the language of spanish
The country of bahamas primarily speaks the language of english
The country of norway primarily speaks the language of
2024-07-13 04:09:49 root INFO     [order_1_approx] starting weight calculation for The country of norway primarily speaks the language of norwegian
The country of philippines primarily speaks the language of tagalog
The country of bahamas primarily speaks the language of english
The country of cuba primarily speaks the language of spanish
The country of peru primarily speaks the language of spanish
The country of chile primarily speaks the language of spanish
The country of ecuador primarily speaks the language of spanish
The country of australia primarily speaks the language of
2024-07-13 04:09:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 04:13:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 04:17:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7700,  0.0283, -0.1203,  ..., -0.8213,  0.4165, -0.0698],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.7812,  0.0158, -0.1243,  ..., -0.7900,  0.4006, -0.0923],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0947, -3.5117, -5.5625,  ..., -2.8848,  3.6289, -5.8828],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0034, -0.0019, -0.0018,  ..., -0.0122, -0.0012,  0.0022],
        [-0.0012,  0.0020,  0.0171,  ...,  0.0151,  0.0029, -0.0114],
        [-0.0047,  0.0030,  0.0024,  ...,  0.0027, -0.0028,  0.0053],
        ...,
        [-0.0080,  0.0006,  0.0084,  ...,  0.0102, -0.0093,  0.0067],
        [-0.0004,  0.0053,  0.0118,  ...,  0.0029, -0.0075,  0.0012],
        [-0.0064,  0.0028,  0.0043,  ..., -0.0031,  0.0033,  0.0048]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.0986e-03,  1.7047e-05,  3.8528e-04,  ..., -2.3651e-04,
          6.6948e-04, -2.9302e-04],
        [ 9.9373e-04,  3.8242e-04,  1.9798e-03,  ...,  4.5204e-04,
          1.9741e-04, -1.2960e-03],
        [ 1.4782e-04, -9.8991e-04,  6.5041e-04,  ...,  7.4387e-04,
         -1.9169e-04,  4.9293e-05],
        ...,
        [ 3.3379e-04, -1.1104e-04,  5.3740e-04,  ...,  1.6773e-04,
          6.0272e-04, -6.7759e-04],
        [-7.5579e-05,  1.1988e-03,  8.8406e-04,  ...,  2.6894e-04,
          3.7479e-04,  4.3631e-04],
        [-7.4577e-04, -7.2122e-05,  9.6416e-04,  ...,  2.4056e-04,
         -2.5129e-04,  1.4458e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2239, -4.3906, -5.6133,  ..., -3.4023,  3.3438, -5.7656]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2090, -0.1553,  0.0060,  ..., -0.0449,  0.0047,  0.0955]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 04:17:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of norway primarily speaks the language of norwegian
The country of philippines primarily speaks the language of tagalog
The country of bahamas primarily speaks the language of english
The country of cuba primarily speaks the language of spanish
The country of peru primarily speaks the language of spanish
The country of chile primarily speaks the language of spanish
The country of ecuador primarily speaks the language of spanish
The country of australia primarily speaks the language of
2024-07-13 04:17:41 root INFO     [order_1_approx] starting weight calculation for The country of ecuador primarily speaks the language of spanish
The country of peru primarily speaks the language of spanish
The country of philippines primarily speaks the language of tagalog
The country of australia primarily speaks the language of english
The country of chile primarily speaks the language of spanish
The country of bahamas primarily speaks the language of english
The country of norway primarily speaks the language of norwegian
The country of cuba primarily speaks the language of
2024-07-13 04:17:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 04:21:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 04:25:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2446, -0.2031, -1.6055,  ...,  0.0425,  0.1240, -0.5547],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.2424, -0.2192, -1.5195,  ...,  0.0471,  0.1050, -0.5786],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.1445, -2.5703, -5.3281,  ..., -2.1660,  2.5723, -1.3037],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0024,  0.0003,  0.0017,  ...,  0.0084, -0.0052, -0.0124],
        [ 0.0054,  0.0319,  0.0177,  ...,  0.0183,  0.0263, -0.0116],
        [ 0.0123, -0.0453, -0.0339,  ..., -0.0370, -0.0236,  0.0233],
        ...,
        [-0.0078, -0.0094, -0.0084,  ...,  0.0221, -0.0006,  0.0040],
        [-0.0047, -0.0034, -0.0079,  ...,  0.0130, -0.0058,  0.0018],
        [-0.0192, -0.0050,  0.0121,  ...,  0.0046,  0.0154,  0.0127]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-6.1798e-04,  4.0531e-05,  3.1281e-04,  ...,  8.6606e-05,
         -2.3913e-04, -1.6994e-03],
        [ 6.1369e-04,  1.2589e-04,  4.0936e-04,  ...,  3.7766e-04,
          6.1893e-04, -1.3590e-03],
        [-6.3717e-05, -2.0103e-03,  2.0657e-03,  ...,  5.5361e-04,
         -2.4700e-04,  5.7220e-04],
        ...,
        [-7.1001e-04, -8.7786e-04, -2.3594e-03,  ..., -2.7466e-03,
          1.0557e-03,  4.4203e-04],
        [ 9.1648e-04,  1.4286e-03, -7.0190e-04,  ..., -4.3607e-04,
         -1.3132e-03,  3.4428e-04],
        [-1.5640e-03,  5.2738e-04, -6.9046e-04,  ..., -2.3460e-03,
         -2.5105e-04,  2.0313e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3477, -3.9727, -3.9980,  ..., -2.2461,  2.1211, -0.7139]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1351, -0.0009, -0.0500,  ...,  0.0134,  0.2062,  0.2847]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 04:25:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of ecuador primarily speaks the language of spanish
The country of peru primarily speaks the language of spanish
The country of philippines primarily speaks the language of tagalog
The country of australia primarily speaks the language of english
The country of chile primarily speaks the language of spanish
The country of bahamas primarily speaks the language of english
The country of norway primarily speaks the language of norwegian
The country of cuba primarily speaks the language of
2024-07-13 04:25:34 root INFO     [order_1_approx] starting weight calculation for The country of philippines primarily speaks the language of tagalog
The country of peru primarily speaks the language of spanish
The country of bahamas primarily speaks the language of english
The country of cuba primarily speaks the language of spanish
The country of australia primarily speaks the language of english
The country of chile primarily speaks the language of spanish
The country of norway primarily speaks the language of norwegian
The country of ecuador primarily speaks the language of
2024-07-13 04:25:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 04:29:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 04:33:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1931, -0.3638, -1.3408,  ..., -0.2227,  0.9004,  0.1936],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1797, -0.3540, -1.1611,  ..., -0.1897,  0.7754,  0.1514],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0146, -3.6250, -4.8789,  ...,  0.1206,  1.9922, -3.0996],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0070, -0.0156,  0.0026,  ..., -0.0042, -0.0074, -0.0045],
        [-0.0008,  0.0153,  0.0005,  ...,  0.0094, -0.0075, -0.0017],
        [-0.0021, -0.0040, -0.0059,  ...,  0.0002,  0.0028, -0.0060],
        ...,
        [-0.0017, -0.0012, -0.0032,  ...,  0.0048, -0.0009, -0.0012],
        [-0.0141, -0.0117,  0.0135,  ...,  0.0025, -0.0086,  0.0015],
        [-0.0109,  0.0024,  0.0120,  ..., -0.0067,  0.0061,  0.0040]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.7548e-03, -2.5406e-03,  1.7185e-03,  ..., -1.0061e-04,
          1.1148e-03,  2.0361e-04],
        [ 1.0300e-03, -9.3460e-05,  1.9493e-03,  ..., -7.6628e-04,
          3.4809e-05, -8.3160e-04],
        [-5.1594e-04, -8.5306e-04,  1.6413e-03,  ...,  1.5926e-03,
          5.8699e-04, -2.1970e-04],
        ...,
        [ 2.8992e-04, -1.4138e-04,  4.1771e-04,  ..., -1.2274e-03,
          9.1076e-04, -1.5283e-04],
        [ 1.4591e-04, -9.3174e-04,  1.3819e-03,  ...,  4.6396e-04,
         -1.3161e-03, -3.9959e-04],
        [ 1.5080e-04,  2.9230e-04, -6.1321e-04,  ..., -1.0643e-03,
          1.2741e-03,  2.3727e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6406, -4.6406, -4.5234,  ..., -0.2515,  1.3535, -3.1992]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0279, -0.0511, -0.3159,  ..., -0.0629, -0.0906,  0.2695]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 04:33:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of philippines primarily speaks the language of tagalog
The country of peru primarily speaks the language of spanish
The country of bahamas primarily speaks the language of english
The country of cuba primarily speaks the language of spanish
The country of australia primarily speaks the language of english
The country of chile primarily speaks the language of spanish
The country of norway primarily speaks the language of norwegian
The country of ecuador primarily speaks the language of
2024-07-13 04:33:26 root INFO     total operator prediction time: 3785.349459171295 seconds
2024-07-13 04:33:26 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on animal - shelter
2024-07-13 04:33:28 root INFO     building operator animal - shelter
2024-07-13 04:33:28 root INFO     [order_1_approx] starting weight calculation for The place ant lives in is called anthill
The place wasp lives in is called nest
The place fish lives in is called sea
The place chimpanzee lives in is called grove
The place bat lives in is called cave
The place hippopotamus lives in is called river
The place goldfish lives in is called pond
The place scorpion lives in is called
2024-07-13 04:33:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 04:37:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 04:41:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0721, -0.6152, -0.3506,  ...,  0.1619,  0.4197,  0.6558],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.0640, -0.5859, -0.3120,  ...,  0.1469,  0.3584,  0.5850],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0449, -5.0625, -0.8091,  ..., -5.1602,  2.0430,  0.7471],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.4403e-03,  4.6873e-04,  1.6174e-03,  ...,  1.8349e-03,
         -2.9373e-03, -6.4659e-03],
        [-4.6110e-04,  7.6904e-03,  9.4604e-03,  ...,  2.3575e-03,
          9.8114e-03, -6.8817e-03],
        [-3.8147e-04,  6.8665e-05, -4.3411e-03,  ..., -1.1200e-02,
          1.3647e-03, -6.0654e-04],
        ...,
        [ 1.9140e-03, -4.4708e-03,  2.0838e-04,  ...,  7.0038e-03,
         -3.4332e-03,  7.4539e-03],
        [-2.7561e-03,  5.1804e-03,  2.0714e-03,  ..., -3.7708e-03,
         -3.8338e-04,  4.5319e-03],
        [-3.0231e-04, -2.4261e-03, -1.0223e-03,  ..., -2.0008e-03,
          7.5302e-03,  1.8082e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-9.9540e-05,  6.4087e-04,  1.0128e-03,  ...,  4.0770e-05,
         -7.3552e-05, -8.8215e-04],
        [ 5.4121e-04,  2.9683e-04,  2.3766e-03,  ...,  1.5345e-03,
          1.9550e-03, -1.4858e-03],
        [-6.3944e-04,  1.6909e-03, -3.9148e-04,  ..., -3.2463e-03,
         -1.5945e-03,  4.9782e-04],
        ...,
        [ 1.3313e-03,  3.5477e-04,  5.1737e-04,  ..., -5.0688e-04,
          5.1117e-04, -5.7030e-04],
        [ 1.0738e-03,  1.1826e-04,  6.2370e-04,  ...,  3.3832e-04,
          4.6968e-04, -2.1152e-03],
        [-8.6069e-05, -7.7820e-04, -7.2193e-04,  ..., -5.0354e-04,
          2.9039e-04,  1.3447e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2803, -5.1016, -0.8286,  ..., -5.1172,  2.1973,  0.9800]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0173, -0.2328,  0.2378,  ..., -0.0126, -0.1998,  0.3145]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 04:41:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place ant lives in is called anthill
The place wasp lives in is called nest
The place fish lives in is called sea
The place chimpanzee lives in is called grove
The place bat lives in is called cave
The place hippopotamus lives in is called river
The place goldfish lives in is called pond
The place scorpion lives in is called
2024-07-13 04:41:24 root INFO     [order_1_approx] starting weight calculation for The place scorpion lives in is called nest
The place bat lives in is called cave
The place chimpanzee lives in is called grove
The place wasp lives in is called nest
The place ant lives in is called anthill
The place fish lives in is called sea
The place hippopotamus lives in is called river
The place goldfish lives in is called
2024-07-13 04:41:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 04:45:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 04:49:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2373,  0.2487,  0.1467,  ...,  0.1062,  0.3572,  0.5332],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.2083,  0.2032,  0.1100,  ...,  0.0922,  0.2856,  0.4460],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.4961,  0.2754, -2.0078,  ..., -4.1367, -2.2109, -0.8794],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0182, -0.0061, -0.0069,  ..., -0.0178,  0.0002,  0.0017],
        [-0.0151,  0.0147,  0.0075,  ...,  0.0138, -0.0092, -0.0141],
        [ 0.0120,  0.0017,  0.0094,  ..., -0.0260,  0.0106,  0.0066],
        ...,
        [-0.0002, -0.0007,  0.0063,  ...,  0.0093, -0.0161,  0.0128],
        [-0.0061, -0.0008,  0.0055,  ..., -0.0076, -0.0044,  0.0092],
        [ 0.0114, -0.0073,  0.0026,  ..., -0.0159,  0.0049,  0.0104]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 6.2943e-05,  2.8114e-03,  1.9073e-03,  ...,  3.2854e-04,
          6.5804e-05, -5.0068e-06],
        [-2.0504e-05,  4.4882e-05, -1.9765e-04,  ...,  7.9727e-04,
          9.2268e-04, -3.4332e-04],
        [-3.7956e-04,  1.2016e-03,  4.1938e-04,  ..., -5.1641e-04,
          1.1406e-03,  2.2011e-03],
        ...,
        [ 4.4417e-04,  1.4973e-03,  8.0490e-04,  ..., -9.6321e-04,
          7.6580e-04,  2.2030e-03],
        [ 3.7670e-04,  3.0184e-04, -1.2076e-04,  ..., -5.8937e-04,
          5.3549e-04, -3.8123e-04],
        [ 2.1791e-04, -7.8821e-04,  8.3208e-04,  ...,  7.7915e-04,
          4.9353e-04,  1.5879e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.8398,  0.3972, -2.1602,  ..., -4.1953, -2.5195, -1.0781]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1043, -0.1437,  0.3362,  ...,  0.0917, -0.0578,  0.0076]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 04:49:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place scorpion lives in is called nest
The place bat lives in is called cave
The place chimpanzee lives in is called grove
The place wasp lives in is called nest
The place ant lives in is called anthill
The place fish lives in is called sea
The place hippopotamus lives in is called river
The place goldfish lives in is called
2024-07-13 04:49:21 root INFO     [order_1_approx] starting weight calculation for The place scorpion lives in is called nest
The place chimpanzee lives in is called grove
The place fish lives in is called sea
The place hippopotamus lives in is called river
The place ant lives in is called anthill
The place bat lives in is called cave
The place goldfish lives in is called pond
The place wasp lives in is called
2024-07-13 04:49:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 04:53:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 04:57:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2026,  0.0474, -1.5859,  ...,  0.4165,  1.3887,  0.6274],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.1830,  0.0310, -1.3672,  ...,  0.3669,  1.2158,  0.5566],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1416, -3.5586,  0.3594,  ..., -3.6816,  4.2812,  1.8770],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.1742e-02, -5.7335e-03, -2.5940e-04,  ..., -1.3466e-03,
          9.2010e-03, -1.3039e-02],
        [-4.3564e-03,  1.6785e-03,  1.5144e-02,  ...,  1.0681e-02,
         -8.9645e-05, -1.0025e-02],
        [ 5.6953e-03,  9.3079e-03, -4.2114e-03,  ..., -7.3776e-03,
          1.9516e-02, -2.3613e-03],
        ...,
        [-4.8876e-05, -1.2543e-02,  9.1476e-03,  ...,  1.2009e-02,
         -8.3008e-03,  1.1368e-03],
        [-5.5695e-03, -9.4147e-03,  1.1948e-02,  ...,  5.5838e-04,
         -1.9035e-03,  9.4452e-03],
        [-1.2169e-02, -9.7961e-03,  1.1421e-02,  ..., -2.2659e-03,
          8.3771e-03,  9.4299e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-6.1607e-04, -1.2455e-03,  6.0272e-04,  ..., -6.0439e-05,
          1.6689e-04, -8.1110e-04],
        [-7.6294e-05,  5.1880e-04,  2.2125e-03,  ...,  2.5768e-03,
          1.9569e-03, -2.7714e-03],
        [ 1.4038e-03, -1.1349e-03,  8.3017e-04,  ..., -7.6580e-04,
          1.2131e-03,  7.8392e-04],
        ...,
        [ 1.5430e-03, -1.7567e-03,  1.1749e-03,  ..., -5.8746e-04,
          9.7466e-04, -1.3618e-03],
        [ 4.1342e-04, -2.2590e-04,  4.7398e-04,  ...,  1.1997e-03,
          2.7895e-05, -1.1997e-03],
        [-1.1711e-03, -8.6689e-04,  4.6062e-04,  ..., -8.2016e-04,
         -1.7524e-04,  1.0633e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9692, -3.8770,  0.2690,  ..., -3.6660,  4.5000,  2.0391]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0049, -0.4988, -0.1283,  ..., -0.3401, -0.1947,  0.1647]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 04:57:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place scorpion lives in is called nest
The place chimpanzee lives in is called grove
The place fish lives in is called sea
The place hippopotamus lives in is called river
The place ant lives in is called anthill
The place bat lives in is called cave
The place goldfish lives in is called pond
The place wasp lives in is called
2024-07-13 04:57:18 root INFO     [order_1_approx] starting weight calculation for The place bat lives in is called cave
The place scorpion lives in is called nest
The place hippopotamus lives in is called river
The place wasp lives in is called nest
The place chimpanzee lives in is called grove
The place ant lives in is called anthill
The place goldfish lives in is called pond
The place fish lives in is called
2024-07-13 04:57:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 05:01:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 05:05:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1379, -0.2412, -0.2079,  ...,  0.1707,  0.2891,  0.2571],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1324, -0.2477, -0.1963,  ...,  0.1578,  0.2489,  0.2228],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3174,  0.5859, -4.8516,  ..., -1.3945,  1.5967, -1.2490],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0003,  0.0101,  0.0034,  ..., -0.0041,  0.0042,  0.0026],
        [ 0.0026, -0.0052, -0.0064,  ...,  0.0086, -0.0063, -0.0150],
        [ 0.0015,  0.0122,  0.0196,  ..., -0.0082,  0.0216,  0.0095],
        ...,
        [ 0.0011, -0.0211,  0.0005,  ...,  0.0086, -0.0203, -0.0074],
        [-0.0075,  0.0003, -0.0062,  ..., -0.0076,  0.0030, -0.0023],
        [-0.0003,  0.0020,  0.0145,  ..., -0.0012,  0.0094,  0.0101]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0009,  0.0014,  0.0015,  ..., -0.0014, -0.0008,  0.0002],
        [ 0.0011, -0.0017,  0.0020,  ...,  0.0012,  0.0005, -0.0012],
        [-0.0011,  0.0019,  0.0003,  ...,  0.0005,  0.0017, -0.0026],
        ...,
        [ 0.0020, -0.0006,  0.0007,  ..., -0.0010,  0.0020,  0.0005],
        [ 0.0001,  0.0005, -0.0006,  ..., -0.0023, -0.0012, -0.0012],
        [ 0.0003,  0.0018, -0.0012,  ...,  0.0016, -0.0009, -0.0037]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1039,  0.1816, -4.6641,  ..., -1.8008,  1.9404, -1.3076]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1915, -0.3960,  0.1489,  ..., -0.2036, -0.2517, -0.6118]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 05:05:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place bat lives in is called cave
The place scorpion lives in is called nest
The place hippopotamus lives in is called river
The place wasp lives in is called nest
The place chimpanzee lives in is called grove
The place ant lives in is called anthill
The place goldfish lives in is called pond
The place fish lives in is called
2024-07-13 05:05:15 root INFO     [order_1_approx] starting weight calculation for The place fish lives in is called sea
The place ant lives in is called anthill
The place scorpion lives in is called nest
The place bat lives in is called cave
The place chimpanzee lives in is called grove
The place goldfish lives in is called pond
The place wasp lives in is called nest
The place hippopotamus lives in is called
2024-07-13 05:05:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 05:09:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 05:13:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3325, -0.6064,  0.2063,  ...,  0.1274, -0.5991,  1.5918],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.2966, -0.5640,  0.1627,  ...,  0.1121, -0.5381,  1.4082],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6392, -2.0625, -4.3320,  ..., -1.4912,  1.3066, -1.3213],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0102, -0.0015, -0.0082,  ..., -0.0137, -0.0053, -0.0008],
        [-0.0080,  0.0046,  0.0062,  ...,  0.0042, -0.0064, -0.0111],
        [ 0.0266,  0.0104, -0.0086,  ..., -0.0236,  0.0052, -0.0064],
        ...,
        [-0.0050,  0.0001, -0.0014,  ..., -0.0007, -0.0036,  0.0030],
        [-0.0033,  0.0019,  0.0024,  ...,  0.0045,  0.0005, -0.0030],
        [ 0.0034, -0.0050,  0.0020,  ..., -0.0024,  0.0116,  0.0113]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-4.2439e-04, -7.8392e-04,  5.1165e-04,  ...,  3.3236e-04,
          4.8089e-04,  4.8971e-04],
        [-3.3379e-06,  2.6131e-04,  2.0237e-03,  ...,  1.6432e-03,
          5.0879e-04, -7.2432e-04],
        [ 3.9673e-04, -1.1134e-04, -4.0102e-04,  ..., -4.8327e-04,
         -2.2578e-04,  6.4659e-04],
        ...,
        [ 8.4496e-04, -1.6737e-04,  1.3475e-03,  ...,  4.3511e-05,
          8.0705e-05, -8.3983e-05],
        [ 1.2846e-03,  6.0654e-04,  5.7411e-04,  ..., -6.3562e-04,
          2.4080e-05, -1.6279e-03],
        [-1.2106e-04, -6.9380e-04,  5.8222e-04,  ...,  1.0519e-03,
          2.4700e-04,  1.0529e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7363, -1.8926, -4.1758,  ..., -1.2793,  1.2256, -1.5596]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0623, -0.3984,  0.1230,  ..., -0.1224, -0.3162, -0.0374]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 05:13:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place fish lives in is called sea
The place ant lives in is called anthill
The place scorpion lives in is called nest
The place bat lives in is called cave
The place chimpanzee lives in is called grove
The place goldfish lives in is called pond
The place wasp lives in is called nest
The place hippopotamus lives in is called
2024-07-13 05:13:11 root INFO     [order_1_approx] starting weight calculation for The place scorpion lives in is called nest
The place ant lives in is called anthill
The place chimpanzee lives in is called grove
The place wasp lives in is called nest
The place goldfish lives in is called pond
The place hippopotamus lives in is called river
The place fish lives in is called sea
The place bat lives in is called
2024-07-13 05:13:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 05:17:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 05:21:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2012,  0.5249, -1.2012,  ..., -0.7529, -1.2246,  0.9785],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1812,  0.4614, -1.0264,  ..., -0.6348, -1.0879,  0.8633],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6943, -2.4238, -0.5889,  ..., -5.0078,  0.9883, -0.0211],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0086, -0.0074, -0.0033,  ...,  0.0034, -0.0088,  0.0220],
        [-0.0033,  0.0188,  0.0060,  ...,  0.0105, -0.0204,  0.0127],
        [-0.0021, -0.0005, -0.0002,  ...,  0.0025,  0.0102,  0.0069],
        ...,
        [-0.0010, -0.0056,  0.0014,  ...,  0.0021, -0.0003,  0.0038],
        [-0.0107, -0.0008, -0.0008,  ..., -0.0103,  0.0096, -0.0027],
        [ 0.0205, -0.0101, -0.0045,  ...,  0.0002,  0.0164, -0.0145]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-8.1491e-04, -2.1160e-04,  1.0910e-03,  ..., -8.5068e-04,
          3.7861e-04,  2.0599e-03],
        [ 2.1374e-04,  3.9983e-04, -6.6376e-04,  ...,  1.5240e-03,
          3.6216e-04, -1.0657e-04],
        [ 1.0471e-03, -2.3198e-04, -8.2970e-04,  ..., -2.1248e-03,
         -8.8453e-04,  1.4734e-03],
        ...,
        [ 2.2483e-04,  1.4076e-03, -4.8923e-04,  ..., -1.8015e-03,
         -5.0306e-04, -6.3896e-04],
        [-5.1022e-05, -2.8181e-04,  9.1457e-04,  ...,  7.5769e-04,
         -1.0700e-03, -2.6474e-03],
        [-2.6083e-04,  4.4179e-04, -8.6880e-04,  ..., -6.1512e-04,
          5.6922e-05, -6.7234e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5503, -2.5039, -1.0830,  ..., -4.5977,  1.4727,  0.4858]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.2751, -0.2124,  0.2399,  ..., -0.0596, -0.1576,  0.1656]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 05:21:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place scorpion lives in is called nest
The place ant lives in is called anthill
The place chimpanzee lives in is called grove
The place wasp lives in is called nest
The place goldfish lives in is called pond
The place hippopotamus lives in is called river
The place fish lives in is called sea
The place bat lives in is called
2024-07-13 05:21:08 root INFO     [order_1_approx] starting weight calculation for The place ant lives in is called anthill
The place fish lives in is called sea
The place bat lives in is called cave
The place hippopotamus lives in is called river
The place wasp lives in is called nest
The place goldfish lives in is called pond
The place scorpion lives in is called nest
The place chimpanzee lives in is called
2024-07-13 05:21:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 05:25:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 05:29:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3931, -0.5000, -0.4521,  ...,  0.1125, -0.0967,  0.9697],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.3484, -0.4768, -0.3984,  ...,  0.1000, -0.1046,  0.8628],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.6738, -3.0312, -1.5078,  ..., -5.8711,  1.3691, -0.5796],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0042, -0.0075, -0.0011,  ...,  0.0007, -0.0083, -0.0093],
        [-0.0088,  0.0073,  0.0090,  ...,  0.0060,  0.0050, -0.0041],
        [ 0.0107,  0.0009, -0.0014,  ...,  0.0005, -0.0065,  0.0011],
        ...,
        [ 0.0015,  0.0078, -0.0002,  ..., -0.0029,  0.0024,  0.0033],
        [ 0.0068,  0.0112,  0.0016,  ..., -0.0032, -0.0006,  0.0017],
        [ 0.0043, -0.0083,  0.0002,  ...,  0.0032, -0.0003,  0.0071]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-5.5647e-04, -3.0947e-04,  1.4610e-03,  ...,  1.1091e-03,
         -3.8743e-04, -4.7708e-04],
        [ 4.3201e-04,  1.2028e-04,  1.2627e-03,  ...,  1.1168e-03,
          2.5558e-04, -1.0281e-03],
        [-3.3116e-04, -3.9792e-04, -1.1702e-03,  ..., -6.0439e-05,
         -1.4219e-03,  2.5451e-05],
        ...,
        [ 6.3133e-04,  8.9598e-04,  9.1982e-04,  ..., -1.0366e-03,
         -4.1342e-04,  6.4969e-05],
        [-5.0306e-05,  5.4598e-04, -1.6284e-04,  ...,  1.1909e-04,
         -4.3201e-04, -2.5201e-04],
        [-4.6802e-04, -7.7248e-04, -6.2895e-04,  ...,  4.9448e-04,
          6.0844e-04,  1.2283e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.7109, -3.1484, -1.4238,  ..., -5.9297,  1.7852, -0.6704]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0425, -0.3433,  0.2607,  ..., -0.0316,  0.0369,  0.1566]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 05:29:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place ant lives in is called anthill
The place fish lives in is called sea
The place bat lives in is called cave
The place hippopotamus lives in is called river
The place wasp lives in is called nest
The place goldfish lives in is called pond
The place scorpion lives in is called nest
The place chimpanzee lives in is called
2024-07-13 05:29:05 root INFO     [order_1_approx] starting weight calculation for The place bat lives in is called cave
The place chimpanzee lives in is called grove
The place scorpion lives in is called nest
The place goldfish lives in is called pond
The place wasp lives in is called nest
The place hippopotamus lives in is called river
The place fish lives in is called sea
The place ant lives in is called
2024-07-13 05:29:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 05:33:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 05:36:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4937,  0.3796, -0.5830,  ..., -0.6465, -0.5508,  0.2261],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.4419,  0.3340, -0.5107,  ..., -0.5522, -0.5059,  0.1849],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8975, -4.6523, -0.6143,  ..., -6.7148,  6.2617,  1.6953],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0135, -0.0099,  0.0011,  ..., -0.0001, -0.0035,  0.0026],
        [-0.0082, -0.0025,  0.0055,  ...,  0.0116, -0.0036,  0.0046],
        [ 0.0102,  0.0101,  0.0068,  ..., -0.0078,  0.0172,  0.0091],
        ...,
        [-0.0063, -0.0080,  0.0117,  ...,  0.0038,  0.0054,  0.0024],
        [-0.0237,  0.0010,  0.0050,  ...,  0.0069, -0.0025,  0.0084],
        [ 0.0019, -0.0102,  0.0049,  ...,  0.0089, -0.0076,  0.0172]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.7834e-03, -4.7398e-04, -2.1935e-05,  ..., -1.6069e-04,
         -2.4204e-03,  6.1321e-04],
        [-4.0865e-04,  1.1611e-04,  2.7637e-03,  ...,  1.2827e-03,
         -4.2653e-04, -2.1896e-03],
        [ 2.7299e-04,  2.6283e-03, -2.4438e-04,  ..., -2.1667e-03,
         -1.5488e-03,  4.8103e-03],
        ...,
        [ 1.2980e-03,  8.2731e-04,  1.1721e-03,  ..., -2.7161e-03,
         -3.2783e-05, -3.0136e-04],
        [-1.1606e-03,  1.2398e-03,  1.4458e-03,  ..., -2.3031e-04,
         -9.9087e-04, -2.6913e-03],
        [-8.0347e-05, -2.1515e-03,  6.9380e-04,  ...,  1.1129e-03,
          6.6853e-04,  1.7710e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.8086, -4.7852,  0.1904,  ..., -6.0469,  5.9961,  1.0264]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1888, -0.2556,  0.4534,  ...,  0.1910, -0.1680,  0.0216]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 05:36:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place bat lives in is called cave
The place chimpanzee lives in is called grove
The place scorpion lives in is called nest
The place goldfish lives in is called pond
The place wasp lives in is called nest
The place hippopotamus lives in is called river
The place fish lives in is called sea
The place ant lives in is called
2024-07-13 05:36:59 root INFO     total operator prediction time: 3811.3272664546967 seconds
2024-07-13 05:36:59 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on male - female
2024-07-13 05:37:01 root INFO     building operator male - female
2024-07-13 05:37:02 root INFO     [order_1_approx] starting weight calculation for A female prince is known as a princess
A female poet is known as a poetess
A female man is known as a woman
A female sir is known as a madam
A female son is known as a daughter
A female manager is known as a manageress
A female emperor is known as a empress
A female boar is known as a
2024-07-13 05:37:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 05:41:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 05:44:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6943,  0.3459, -0.7051,  ...,  0.5513,  0.3403, -0.4109],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.6167,  0.3015, -0.6040,  ...,  0.4763,  0.2830, -0.3911],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6680,  0.3848, -3.2305,  ..., -4.7812,  2.5156, -1.0303],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.2276e-02, -8.4839e-03, -2.7657e-03,  ...,  1.6571e-02,
          7.5607e-03, -4.0588e-03],
        [-1.4786e-02, -9.2077e-04, -4.4861e-03,  ...,  2.1423e-02,
          1.1932e-02,  1.9913e-02],
        [ 4.2534e-03,  3.8147e-03, -7.1259e-03,  ..., -6.5384e-03,
          4.6387e-03, -1.4130e-02],
        ...,
        [-7.3624e-04, -1.1459e-02,  2.3937e-03,  ...,  3.1490e-03,
         -5.7335e-03, -4.7150e-03],
        [-4.9744e-03, -3.2520e-04,  7.2136e-03,  ...,  8.4152e-03,
         -1.1536e-02,  1.5884e-02],
        [-1.1215e-02, -6.0320e-05,  4.4212e-03,  ..., -9.5139e-03,
          1.3382e-02, -1.5930e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0007, -0.0007,  0.0015,  ...,  0.0006, -0.0011, -0.0003],
        [-0.0011, -0.0019,  0.0013,  ..., -0.0013, -0.0005,  0.0007],
        [ 0.0006,  0.0003, -0.0009,  ...,  0.0004,  0.0006, -0.0007],
        ...,
        [ 0.0008,  0.0036, -0.0014,  ...,  0.0006, -0.0003,  0.0002],
        [ 0.0004,  0.0004,  0.0011,  ...,  0.0011, -0.0011, -0.0002],
        [-0.0005,  0.0004,  0.0002,  ..., -0.0004,  0.0001, -0.0002]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1602,  0.6045, -3.8359,  ..., -3.9629,  2.1270, -0.1035]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0648,  0.1396,  0.0117,  ..., -0.0228, -0.1266,  0.0062]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 05:45:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female prince is known as a princess
A female poet is known as a poetess
A female man is known as a woman
A female sir is known as a madam
A female son is known as a daughter
A female manager is known as a manageress
A female emperor is known as a empress
A female boar is known as a
2024-07-13 05:45:00 root INFO     [order_1_approx] starting weight calculation for A female manager is known as a manageress
A female sir is known as a madam
A female man is known as a woman
A female son is known as a daughter
A female poet is known as a poetess
A female boar is known as a sow
A female prince is known as a princess
A female emperor is known as a
2024-07-13 05:45:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 05:48:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 05:52:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4104, -1.1387, -0.1068,  ..., -0.1332,  0.6431, -0.0621],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.3757, -1.0684, -0.1053,  ..., -0.1133,  0.5483, -0.0833],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.1719, -0.0336, -2.0156,  ..., -2.2090, -1.6016, -4.1406],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0004, -0.0052,  0.0044,  ...,  0.0160, -0.0024,  0.0074],
        [-0.0032, -0.0165, -0.0021,  ...,  0.0020, -0.0038,  0.0043],
        [ 0.0166,  0.0005, -0.0056,  ...,  0.0026,  0.0240, -0.0056],
        ...,
        [ 0.0061,  0.0008,  0.0027,  ...,  0.0072, -0.0156, -0.0087],
        [ 0.0149, -0.0030,  0.0023,  ..., -0.0080, -0.0127, -0.0051],
        [-0.0009, -0.0091,  0.0126,  ..., -0.0083,  0.0109,  0.0001]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.6918e-03, -1.5297e-03,  2.3842e-07,  ...,  8.3017e-04,
          5.2214e-05, -1.2913e-03],
        [ 1.3542e-03, -1.7834e-03,  2.6894e-04,  ...,  9.6512e-04,
          7.6628e-04,  8.5020e-04],
        [ 6.4564e-04,  5.3310e-04, -4.5872e-04,  ..., -3.3379e-06,
         -2.6941e-05, -1.5807e-04],
        ...,
        [ 1.8616e-03,  4.0650e-04, -2.5845e-03,  ..., -2.9430e-03,
          1.2884e-03, -3.8409e-04],
        [ 2.0218e-03,  4.6825e-04,  2.7537e-05,  ..., -1.2684e-04,
         -4.0960e-04, -5.8126e-04],
        [-8.6498e-04, -1.0312e-04, -2.4986e-04,  ...,  1.1139e-03,
         -3.0375e-04, -1.6155e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6543,  0.4795, -2.5430,  ..., -2.3379, -1.6738, -4.3398]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0354,  0.2390, -0.1187,  ..., -0.2456, -0.1302, -0.3555]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 05:52:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female manager is known as a manageress
A female sir is known as a madam
A female man is known as a woman
A female son is known as a daughter
A female poet is known as a poetess
A female boar is known as a sow
A female prince is known as a princess
A female emperor is known as a
2024-07-13 05:52:49 root INFO     [order_1_approx] starting weight calculation for A female emperor is known as a empress
A female man is known as a woman
A female poet is known as a poetess
A female manager is known as a manageress
A female boar is known as a sow
A female son is known as a daughter
A female prince is known as a princess
A female sir is known as a
2024-07-13 05:52:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 05:56:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 06:00:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5938,  0.1470, -0.2710,  ...,  0.6436,  0.7139,  0.6826],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.5933,  0.1340, -0.2712,  ...,  0.6235,  0.6880,  0.6758],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4414, -1.9355, -1.2031,  ..., -7.6133, -0.9619, -4.6211],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.8534e-03, -1.7242e-02,  2.3537e-03,  ..., -1.2787e-02,
         -4.2725e-03, -3.3539e-02],
        [ 4.4174e-03, -7.4043e-03, -6.0692e-03,  ..., -1.3931e-02,
         -1.3412e-02,  9.5062e-03],
        [ 3.1097e-02, -5.0697e-03, -1.4763e-03,  ...,  5.2299e-03,
          8.5144e-03, -3.6621e-04],
        ...,
        [ 1.1032e-02, -1.2268e-02, -7.8201e-04,  ..., -1.8097e-02,
         -2.6016e-03,  3.6201e-03],
        [ 3.4576e-02,  9.8038e-03,  1.6174e-02,  ..., -2.1667e-02,
         -1.3664e-02,  2.2095e-02],
        [-2.0180e-03, -1.6079e-03,  3.7403e-03,  ..., -2.0981e-05,
          2.2675e-02,  2.7054e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 3.8028e-04, -1.1754e-04,  1.0357e-03,  ..., -4.8018e-04,
         -2.5177e-03, -1.1702e-03],
        [ 1.0967e-03, -4.8828e-04, -2.8610e-06,  ..., -8.3351e-04,
          1.8358e-03, -5.5265e-04],
        [ 2.0409e-03,  9.4700e-04, -4.2772e-04,  ..., -1.6012e-03,
         -2.5320e-04,  4.4327e-03],
        ...,
        [ 6.2847e-04,  8.2970e-05,  6.4564e-04,  ..., -6.4898e-04,
          1.4477e-03, -4.9400e-04],
        [ 3.7460e-03,  1.7262e-03,  9.6846e-04,  ..., -1.3609e-03,
          9.5415e-04,  7.9298e-04],
        [-7.4482e-04,  7.9203e-04, -4.5848e-04,  ..., -2.2125e-04,
         -1.7166e-05,  1.6460e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.8320, -2.0000, -2.7988,  ..., -8.0078, -1.1777, -5.6836]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0753,  0.0065,  0.0794,  ...,  0.1835, -0.3647,  0.0453]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 06:00:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female emperor is known as a empress
A female man is known as a woman
A female poet is known as a poetess
A female manager is known as a manageress
A female boar is known as a sow
A female son is known as a daughter
A female prince is known as a princess
A female sir is known as a
2024-07-13 06:00:40 root INFO     [order_1_approx] starting weight calculation for A female manager is known as a manageress
A female sir is known as a madam
A female son is known as a daughter
A female prince is known as a princess
A female emperor is known as a empress
A female boar is known as a sow
A female poet is known as a poetess
A female man is known as a
2024-07-13 06:00:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 06:04:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 06:08:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6279, -0.3076, -0.4822,  ...,  0.7251,  0.4624,  1.2285],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.6343, -0.3342, -0.4751,  ...,  0.7090,  0.4446,  1.2471],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.7578, -1.5176, -5.4297,  ..., -5.3320,  2.1953,  0.8159],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0031, -0.0045,  0.0168,  ...,  0.0008,  0.0048,  0.0131],
        [ 0.0068, -0.0083, -0.0050,  ..., -0.0127,  0.0015, -0.0208],
        [ 0.0193, -0.0069, -0.0220,  ...,  0.0092,  0.0049, -0.0016],
        ...,
        [ 0.0028, -0.0239, -0.0078,  ..., -0.0129,  0.0049, -0.0134],
        [ 0.0202, -0.0168,  0.0157,  ...,  0.0055, -0.0221,  0.0081],
        [-0.0051, -0.0107,  0.0131,  ..., -0.0169,  0.0131,  0.0088]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 0.0017, -0.0008,  0.0022,  ..., -0.0006,  0.0009, -0.0010],
        [ 0.0017,  0.0018,  0.0020,  ..., -0.0015,  0.0011,  0.0002],
        [ 0.0013, -0.0006, -0.0043,  ...,  0.0011, -0.0012,  0.0002],
        ...,
        [-0.0022, -0.0023, -0.0017,  ..., -0.0002,  0.0002, -0.0027],
        [ 0.0019, -0.0014, -0.0025,  ..., -0.0014, -0.0023,  0.0007],
        [-0.0005,  0.0003,  0.0031,  ..., -0.0022,  0.0013,  0.0031]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.7031, -1.4248, -6.7148,  ..., -5.1641,  3.1484,  1.0771]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0869, -0.0483, -0.1515,  ..., -0.1263, -0.0548, -0.0718]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 06:08:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female manager is known as a manageress
A female sir is known as a madam
A female son is known as a daughter
A female prince is known as a princess
A female emperor is known as a empress
A female boar is known as a sow
A female poet is known as a poetess
A female man is known as a
2024-07-13 06:08:36 root INFO     [order_1_approx] starting weight calculation for A female man is known as a woman
A female prince is known as a princess
A female boar is known as a sow
A female manager is known as a manageress
A female poet is known as a poetess
A female sir is known as a madam
A female emperor is known as a empress
A female son is known as a
2024-07-13 06:08:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 06:12:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 06:16:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3223, -0.1672,  0.4231,  ...,  1.3779,  0.7983,  1.0410],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 1.3008, -0.1844,  0.3875,  ...,  1.3057,  0.7573,  1.0234],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.6133, -2.5684, -2.3457,  ..., -0.7056,  2.0723, -3.1094],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0114, -0.0005,  0.0113,  ...,  0.0120, -0.0176, -0.0176],
        [-0.0059,  0.0087,  0.0131,  ...,  0.0123, -0.0008, -0.0166],
        [-0.0009,  0.0125,  0.0179,  ...,  0.0110, -0.0102,  0.0124],
        ...,
        [ 0.0020, -0.0244, -0.0032,  ...,  0.0048, -0.0095, -0.0148],
        [ 0.0212,  0.0091, -0.0133,  ...,  0.0083, -0.0041, -0.0007],
        [ 0.0253, -0.0147,  0.0122,  ..., -0.0030,  0.0300,  0.0004]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-8.4639e-06, -1.5450e-03,  1.1301e-03,  ...,  4.1437e-04,
         -1.2569e-03, -2.1687e-03],
        [ 6.5184e-04, -6.3658e-04,  1.9875e-03,  ..., -3.6645e-04,
          8.3208e-04, -1.9407e-03],
        [-1.0118e-03,  3.0823e-03,  8.1539e-05,  ..., -2.2984e-03,
          1.3313e-03, -1.1177e-03],
        ...,
        [ 7.2336e-04, -3.2310e-03,  6.3896e-04,  ...,  5.6982e-04,
          1.8044e-03, -2.1496e-03],
        [ 6.0940e-04, -1.4806e-04, -1.6384e-03,  ...,  1.1158e-04,
         -9.2864e-05, -2.7657e-04],
        [ 1.2217e-03,  2.0695e-03, -2.9526e-03,  ...,  8.6689e-04,
          6.4468e-04, -7.3338e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1699, -2.4824, -1.5254,  ..., -0.7832,  1.5859, -2.9648]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.3269, -0.2825,  0.3506,  ..., -0.2153, -0.3213, -0.0936]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 06:16:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female man is known as a woman
A female prince is known as a princess
A female boar is known as a sow
A female manager is known as a manageress
A female poet is known as a poetess
A female sir is known as a madam
A female emperor is known as a empress
A female son is known as a
2024-07-13 06:16:32 root INFO     [order_1_approx] starting weight calculation for A female sir is known as a madam
A female emperor is known as a empress
A female boar is known as a sow
A female son is known as a daughter
A female prince is known as a princess
A female man is known as a woman
A female poet is known as a poetess
A female manager is known as a
2024-07-13 06:16:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 06:20:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 06:24:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8560,  0.4741, -0.0590,  ...,  0.4668,  0.0549,  0.7007],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.8198,  0.4490, -0.0668,  ...,  0.4331,  0.0320,  0.6631],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6953, -6.3008, -1.2881,  ..., -1.5488,  1.0225,  1.9072],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0171,  0.0129,  0.0057,  ...,  0.0097, -0.0106, -0.0031],
        [ 0.0032, -0.0063, -0.0011,  ..., -0.0043, -0.0045, -0.0023],
        [ 0.0038, -0.0080,  0.0046,  ...,  0.0053, -0.0049, -0.0061],
        ...,
        [-0.0068, -0.0233, -0.0055,  ..., -0.0153,  0.0072,  0.0050],
        [ 0.0249,  0.0135,  0.0092,  ...,  0.0056, -0.0171,  0.0119],
        [-0.0089,  0.0027, -0.0120,  ...,  0.0078,  0.0229,  0.0052]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.1148e-03, -1.2088e-04,  5.9652e-04,  ...,  1.5450e-04,
          1.8644e-03, -2.1973e-03],
        [ 2.2163e-03,  1.0338e-03,  2.5654e-04,  ...,  1.9426e-03,
         -1.5583e-03, -2.5921e-03],
        [ 1.6928e-03,  2.5902e-03,  1.2140e-03,  ..., -1.5306e-04,
          3.8624e-04,  6.5041e-04],
        ...,
        [ 1.4229e-03, -9.7942e-04,  1.1053e-03,  ..., -2.9354e-03,
          1.1044e-03, -1.5240e-03],
        [ 3.3779e-03,  7.6342e-04,  1.4877e-03,  ..., -2.4271e-04,
          8.3685e-05, -1.4973e-03],
        [-1.4563e-03, -2.1152e-03, -2.0199e-03,  ...,  1.1578e-03,
         -8.2684e-04,  1.9817e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5205, -5.6719, -1.0635,  ..., -1.1396,  1.0283,  0.5234]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0484, -0.5127, -0.1526,  ..., -0.1807, -0.4729,  0.0331]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 06:24:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female sir is known as a madam
A female emperor is known as a empress
A female boar is known as a sow
A female son is known as a daughter
A female prince is known as a princess
A female man is known as a woman
A female poet is known as a poetess
A female manager is known as a
2024-07-13 06:24:29 root INFO     [order_1_approx] starting weight calculation for A female son is known as a daughter
A female poet is known as a poetess
A female manager is known as a manageress
A female man is known as a woman
A female emperor is known as a empress
A female sir is known as a madam
A female boar is known as a sow
A female prince is known as a
2024-07-13 06:24:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 06:28:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 06:32:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1240, -0.3535, -0.4404,  ...,  0.4692,  0.7222,  0.1733],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1199, -0.3547, -0.4026,  ...,  0.4216,  0.6426,  0.1403],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6875, -1.7031, -1.0645,  ..., -3.1855, -3.2695, -3.5488],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0311, -0.0036, -0.0157,  ...,  0.0144, -0.0002, -0.0095],
        [-0.0042, -0.0149,  0.0096,  ...,  0.0059, -0.0130, -0.0086],
        [-0.0147,  0.0130,  0.0077,  ..., -0.0104,  0.0002,  0.0027],
        ...,
        [ 0.0269, -0.0204, -0.0265,  ..., -0.0243,  0.0099, -0.0065],
        [ 0.0227, -0.0085,  0.0086,  ..., -0.0096, -0.0068, -0.0164],
        [ 0.0157, -0.0081,  0.0015,  ..., -0.0039,  0.0267, -0.0125]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 0.0004, -0.0018, -0.0003,  ...,  0.0018, -0.0002, -0.0005],
        [-0.0008, -0.0023,  0.0028,  ...,  0.0008, -0.0003, -0.0003],
        [ 0.0009,  0.0006,  0.0005,  ..., -0.0010, -0.0003,  0.0015],
        ...,
        [ 0.0029,  0.0017, -0.0052,  ..., -0.0003,  0.0022, -0.0014],
        [-0.0007,  0.0028, -0.0011,  ...,  0.0005,  0.0005, -0.0001],
        [-0.0003,  0.0032, -0.0032,  ...,  0.0018,  0.0006, -0.0022]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7622, -1.2520, -1.0762,  ..., -3.2891, -2.6582, -4.2656]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1393,  0.0613,  0.1708,  ..., -0.2991,  0.0452, -0.2720]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 06:32:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female son is known as a daughter
A female poet is known as a poetess
A female manager is known as a manageress
A female man is known as a woman
A female emperor is known as a empress
A female sir is known as a madam
A female boar is known as a sow
A female prince is known as a
2024-07-13 06:32:25 root INFO     [order_1_approx] starting weight calculation for A female manager is known as a manageress
A female sir is known as a madam
A female emperor is known as a empress
A female prince is known as a princess
A female boar is known as a sow
A female man is known as a woman
A female son is known as a daughter
A female poet is known as a
2024-07-13 06:32:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 06:36:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 06:40:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1646, -0.4407,  0.1982,  ...,  0.7095,  0.1554,  0.3518],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.1396, -0.4099,  0.1534,  ...,  0.5957,  0.1147,  0.2888],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0146, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6484,  1.8135, -7.3359,  ..., -1.3086, -0.2517, -3.4570],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.6098e-03, -5.4016e-03, -1.4381e-03,  ...,  1.4526e-02,
         -1.2062e-02,  4.2458e-03],
        [-3.6240e-05, -7.9498e-03, -1.5625e-02,  ..., -3.7651e-03,
         -4.1199e-04,  1.3100e-02],
        [ 2.7023e-02,  3.0174e-03, -1.0277e-02,  ..., -5.2299e-03,
         -2.8992e-04, -2.6941e-05],
        ...,
        [ 1.0395e-03, -9.2468e-03,  1.2497e-02,  ..., -2.6825e-02,
         -3.7212e-03,  2.2736e-03],
        [ 8.4305e-03, -5.3902e-03,  1.6190e-02,  ...,  2.0142e-02,
         -1.8631e-02, -4.6349e-03],
        [-3.4561e-03, -1.3504e-03,  2.3956e-03,  ...,  1.7181e-02,
          1.5076e-02, -1.4763e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.7242e-03, -4.9210e-04,  1.5078e-03,  ..., -6.6376e-04,
         -1.8339e-03,  1.0290e-03],
        [ 5.1498e-04, -6.7377e-04, -4.1723e-04,  ..., -6.4433e-05,
         -3.3951e-04,  2.5082e-04],
        [ 2.2292e-04,  1.9522e-03, -2.5158e-03,  ..., -1.5182e-03,
          1.2026e-03, -1.3371e-03],
        ...,
        [ 7.9107e-04,  1.3103e-03, -4.0555e-04,  ..., -3.2177e-03,
          1.2493e-03, -2.0332e-03],
        [ 1.4391e-03, -1.6050e-03,  9.3079e-04,  ...,  3.2187e-04,
         -8.8310e-04,  1.2553e-04],
        [-1.5612e-03,  4.2701e-04,  5.0020e-04,  ...,  3.4761e-04,
          2.7418e-04, -1.6308e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6035,  1.9102, -6.9961,  ..., -1.4883, -0.1125, -2.3789]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1239, -0.0392, -0.0212,  ..., -0.2874, -0.0178,  0.1722]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 06:40:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female manager is known as a manageress
A female sir is known as a madam
A female emperor is known as a empress
A female prince is known as a princess
A female boar is known as a sow
A female man is known as a woman
A female son is known as a daughter
A female poet is known as a
2024-07-13 06:40:20 root INFO     total operator prediction time: 3798.1200845241547 seconds
2024-07-13 06:40:20 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on name - occupation
2024-07-13 06:40:22 root INFO     building operator name - occupation
2024-07-13 06:40:22 root INFO     [order_1_approx] starting weight calculation for dante was known for their work as a  poet
tolstoi was known for their work as a  novelist
newton was known for their work as a  scientist
pacino was known for their work as a  actor
hume was known for their work as a  philosopher
plato was known for their work as a  philosopher
picasso was known for their work as a  painter
strauss was known for their work as a 
2024-07-13 06:40:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 06:44:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 06:48:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4075, -0.1539, -0.1001,  ...,  1.4395,  0.8735, -0.0902],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.4114, -0.1736, -0.1054,  ...,  1.3906,  0.8472, -0.1141],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0030, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.8750, -4.0898,  0.6206,  ..., -6.9727,  1.2236,  3.5703],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0059, -0.0007,  0.0004,  ..., -0.0016,  0.0036, -0.0022],
        [-0.0143,  0.0112,  0.0132,  ...,  0.0077, -0.0037, -0.0070],
        [ 0.0044,  0.0014,  0.0035,  ...,  0.0014,  0.0175, -0.0083],
        ...,
        [-0.0096, -0.0042, -0.0063,  ...,  0.0087, -0.0010, -0.0046],
        [ 0.0053, -0.0114,  0.0031,  ...,  0.0033,  0.0047,  0.0095],
        [ 0.0052, -0.0186, -0.0123,  ..., -0.0091,  0.0138,  0.0188]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.5697e-03,  7.1764e-05, -5.4646e-04,  ..., -2.7585e-04,
         -1.3924e-04, -4.5323e-04],
        [-1.5945e-03, -4.1509e-04,  9.3555e-04,  ...,  6.7234e-04,
          4.2844e-04, -8.8274e-05],
        [-6.5708e-04, -2.7299e-05,  1.0395e-03,  ..., -1.4365e-04,
         -4.8733e-04, -8.0109e-04],
        ...,
        [ 8.7559e-05,  4.6015e-04, -6.2847e-04,  ...,  3.5906e-04,
          7.0429e-04,  2.2602e-03],
        [-1.8530e-03, -1.4639e-03, -2.7008e-03,  ..., -8.4162e-04,
          6.0463e-04,  3.8505e-04],
        [ 2.0754e-04, -2.1458e-03, -1.5802e-03,  ...,  6.9618e-04,
          3.3379e-04, -1.6177e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.5918, -4.6094, -0.2949,  ..., -6.9453,  1.7002,  3.4102]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0190, -0.0441, -0.2095,  ...,  0.1208,  0.1095,  0.0094]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 06:48:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for dante was known for their work as a  poet
tolstoi was known for their work as a  novelist
newton was known for their work as a  scientist
pacino was known for their work as a  actor
hume was known for their work as a  philosopher
plato was known for their work as a  philosopher
picasso was known for their work as a  painter
strauss was known for their work as a 
2024-07-13 06:48:15 root INFO     [order_1_approx] starting weight calculation for tolstoi was known for their work as a  novelist
plato was known for their work as a  philosopher
strauss was known for their work as a  composer
dante was known for their work as a  poet
hume was known for their work as a  philosopher
newton was known for their work as a  scientist
picasso was known for their work as a  painter
pacino was known for their work as a 
2024-07-13 06:48:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 06:52:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 06:56:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3962, -0.2925,  1.8965,  ...,  0.8545, -0.2197,  0.9277],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.4192, -0.3306,  1.8896,  ...,  0.8647, -0.2448,  0.9712],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0013, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3997, -1.3213,  0.7075,  ..., -4.8008,  2.3535,  3.1270],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0026,  0.0124, -0.0030,  ..., -0.0091,  0.0172, -0.0040],
        [-0.0131,  0.0071,  0.0139,  ...,  0.0401, -0.0293, -0.0327],
        [-0.0064,  0.0291,  0.0153,  ...,  0.0116,  0.0025, -0.0219],
        ...,
        [-0.0202, -0.0123, -0.0055,  ..., -0.0165,  0.0044, -0.0009],
        [ 0.0022,  0.0059,  0.0049,  ...,  0.0095,  0.0044, -0.0169],
        [-0.0019,  0.0031, -0.0092,  ..., -0.0085,  0.0306,  0.0103]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-5.3120e-04, -1.9102e-03,  1.2093e-03,  ..., -5.6410e-04,
         -6.9237e-04,  1.0557e-03],
        [ 1.3523e-03,  2.3425e-04, -1.8721e-03,  ...,  1.6155e-03,
          1.2045e-03, -2.3232e-03],
        [ 1.4610e-03,  3.9768e-04,  2.3174e-03,  ...,  2.4166e-03,
          1.9722e-03, -3.4599e-03],
        ...,
        [ 7.7009e-04, -1.0433e-03, -5.1594e-04,  ...,  2.7218e-03,
          1.3437e-03, -3.6502e-04],
        [ 3.7909e-04,  2.7103e-03, -2.0039e-04,  ...,  1.2474e-03,
          2.9683e-05, -2.4834e-03],
        [-1.6041e-03,  5.1737e-04, -7.5579e-05,  ..., -4.9973e-04,
          1.2531e-03, -3.7742e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5220, -0.9287,  0.3394,  ..., -4.5039,  1.9961,  2.8691]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0740, -0.2260, -0.2212,  ..., -0.1411, -0.0428,  0.0685]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 06:56:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for tolstoi was known for their work as a  novelist
plato was known for their work as a  philosopher
strauss was known for their work as a  composer
dante was known for their work as a  poet
hume was known for their work as a  philosopher
newton was known for their work as a  scientist
picasso was known for their work as a  painter
pacino was known for their work as a 
2024-07-13 06:56:09 root INFO     [order_1_approx] starting weight calculation for pacino was known for their work as a  actor
picasso was known for their work as a  painter
plato was known for their work as a  philosopher
hume was known for their work as a  philosopher
dante was known for their work as a  poet
strauss was known for their work as a  composer
newton was known for their work as a  scientist
tolstoi was known for their work as a 
2024-07-13 06:56:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 07:00:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 07:04:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1702,  0.1357,  1.3408,  ...,  1.1250,  0.4312, -0.4570],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1671,  0.1229,  1.2451,  ...,  1.0635,  0.4014, -0.4766],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0074, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.1973,  1.2773, -3.1328,  ..., -4.6523,  1.9102, -1.8994],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0013, -0.0017, -0.0040,  ..., -0.0033,  0.0032, -0.0096],
        [-0.0083,  0.0009,  0.0150,  ...,  0.0082, -0.0096, -0.0120],
        [ 0.0055,  0.0166,  0.0051,  ..., -0.0061,  0.0003, -0.0114],
        ...,
        [-0.0075, -0.0080,  0.0040,  ..., -0.0067, -0.0109,  0.0052],
        [ 0.0077,  0.0080, -0.0046,  ...,  0.0018, -0.0215,  0.0105],
        [-0.0129,  0.0055, -0.0071,  ..., -0.0081,  0.0099,  0.0024]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 4.6325e-04, -1.0500e-03, -3.8528e-04,  ...,  7.3051e-04,
          4.8637e-04, -1.5240e-03],
        [-1.8692e-04, -4.0865e-04,  7.2813e-04,  ...,  7.6675e-04,
          6.6423e-04,  9.0122e-05],
        [ 6.9904e-04,  6.4182e-04,  4.9400e-04,  ..., -1.8024e-04,
         -2.5249e-04,  1.5583e-03],
        ...,
        [ 4.6873e-04, -8.9169e-04, -3.9959e-04,  ...,  1.0357e-03,
          9.3651e-04,  1.8859e-04],
        [ 4.9829e-05,  1.6861e-03, -7.5817e-05,  ..., -5.7077e-04,
         -9.9373e-04, -9.8515e-04],
        [ 2.7752e-04,  2.1954e-03, -7.4244e-04,  ...,  6.3896e-04,
          5.8699e-04, -1.1568e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.9512,  1.1553, -3.1660,  ..., -5.1133,  2.5000, -1.9570]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0216, -0.2258, -0.0010,  ...,  0.0616,  0.0277,  0.2539]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 07:04:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for pacino was known for their work as a  actor
picasso was known for their work as a  painter
plato was known for their work as a  philosopher
hume was known for their work as a  philosopher
dante was known for their work as a  poet
strauss was known for their work as a  composer
newton was known for their work as a  scientist
tolstoi was known for their work as a 
2024-07-13 07:04:04 root INFO     [order_1_approx] starting weight calculation for picasso was known for their work as a  painter
tolstoi was known for their work as a  novelist
dante was known for their work as a  poet
newton was known for their work as a  scientist
strauss was known for their work as a  composer
plato was known for their work as a  philosopher
pacino was known for their work as a  actor
hume was known for their work as a 
2024-07-13 07:04:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 07:08:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 07:11:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2661, -0.7896,  0.5176,  ...,  0.3933,  0.2769,  1.2754],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.2625, -0.8154,  0.4800,  ...,  0.3796,  0.2539,  1.2676],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0047, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1992, -3.6387, -6.0195,  ..., -3.8867,  1.6875,  0.6787],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0049, -0.0081, -0.0037,  ...,  0.0067, -0.0053,  0.0346],
        [ 0.0009, -0.0072,  0.0035,  ...,  0.0107,  0.0039, -0.0095],
        [ 0.0072,  0.0049, -0.0144,  ...,  0.0030,  0.0017, -0.0110],
        ...,
        [-0.0061, -0.0081,  0.0020,  ...,  0.0004, -0.0072,  0.0142],
        [ 0.0024, -0.0048,  0.0064,  ...,  0.0054,  0.0049, -0.0047],
        [-0.0166,  0.0054,  0.0044,  ..., -0.0024, -0.0025,  0.0075]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 2.3186e-04, -1.5945e-03,  1.5640e-04,  ...,  2.1057e-03,
         -3.2568e-04,  1.5526e-03],
        [ 1.1387e-03, -1.1339e-03,  1.6155e-03,  ...,  1.0926e-04,
         -9.5415e-04, -1.0519e-03],
        [ 1.1196e-03, -2.3794e-04, -1.1950e-03,  ...,  1.1797e-03,
          9.3365e-04, -6.8521e-04],
        ...,
        [-1.4281e-04, -2.9778e-04, -2.7680e-04,  ...,  7.0333e-04,
         -1.8239e-04,  7.7295e-04],
        [-7.1430e-04,  1.2026e-03,  1.5507e-03,  ..., -1.4200e-03,
          2.0814e-04,  8.9550e-04],
        [-6.0034e-04, -4.0030e-04, -7.4196e-04,  ..., -5.2452e-06,
          2.9039e-04,  2.1439e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6865, -4.1602, -7.3750,  ..., -3.9434,  1.9990,  0.6606]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0277, -0.1105,  0.0345,  ...,  0.0107,  0.1501,  0.0742]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 07:11:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for picasso was known for their work as a  painter
tolstoi was known for their work as a  novelist
dante was known for their work as a  poet
newton was known for their work as a  scientist
strauss was known for their work as a  composer
plato was known for their work as a  philosopher
pacino was known for their work as a  actor
hume was known for their work as a 
2024-07-13 07:11:58 root INFO     [order_1_approx] starting weight calculation for tolstoi was known for their work as a  novelist
newton was known for their work as a  scientist
strauss was known for their work as a  composer
plato was known for their work as a  philosopher
pacino was known for their work as a  actor
dante was known for their work as a  poet
hume was known for their work as a  philosopher
picasso was known for their work as a 
2024-07-13 07:11:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 07:15:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 07:19:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8037, -0.6475, -0.6650,  ...,  1.6670,  0.5029, -0.1970],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.7158, -0.6099, -0.5781,  ...,  1.4326,  0.4248, -0.2030],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0013, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8174,  1.1152, -3.2773,  ..., -0.9707, -2.6738,  1.0000],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-8.0032e-03,  1.0357e-03, -9.1782e-03,  ..., -1.9424e-02,
          1.6312e-02, -1.3351e-04],
        [-3.6072e-02, -8.3923e-05,  2.8259e-02,  ...,  3.5782e-03,
          1.7654e-02, -5.8937e-04],
        [ 1.9394e-02,  1.6617e-02, -8.6746e-03,  ..., -1.1215e-03,
         -9.9335e-03, -4.4975e-03],
        ...,
        [ 1.2779e-03, -4.8256e-04, -2.3041e-03,  ...,  4.5013e-03,
         -1.4984e-02,  9.4299e-03],
        [ 1.1948e-02,  5.2071e-03, -1.4191e-03,  ...,  4.7455e-03,
         -1.8066e-02, -1.1425e-03],
        [-1.5472e-02, -1.6678e-02, -4.8542e-04,  ..., -2.8152e-03,
          1.8158e-03,  2.8000e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 2.8896e-04,  1.4877e-03, -3.0088e-04,  ..., -7.2956e-04,
         -1.3494e-04,  9.5558e-04],
        [-1.8859e-04,  1.1549e-03, -1.7571e-04,  ...,  5.0068e-06,
          3.7980e-04,  3.4046e-04],
        [ 1.5106e-03, -5.1785e-04,  5.8317e-04,  ...,  9.6464e-04,
          7.4816e-04, -1.8072e-04],
        ...,
        [ 1.1606e-03,  1.2722e-03, -1.1225e-03,  ...,  7.9536e-04,
          4.9686e-04,  1.6184e-03],
        [-1.1033e-04,  2.2221e-03,  1.0471e-03,  ..., -1.0128e-03,
         -2.0752e-03,  1.1139e-03],
        [-4.7779e-04, -6.8808e-04, -2.4939e-04,  ...,  5.3310e-04,
         -6.0225e-04, -6.6376e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7627,  2.0098, -3.5723,  ..., -0.5303, -3.0703,  0.5103]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0357,  0.0473, -0.0649,  ..., -0.0643, -0.0605,  0.0810]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 07:19:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for tolstoi was known for their work as a  novelist
newton was known for their work as a  scientist
strauss was known for their work as a  composer
plato was known for their work as a  philosopher
pacino was known for their work as a  actor
dante was known for their work as a  poet
hume was known for their work as a  philosopher
picasso was known for their work as a 
2024-07-13 07:19:52 root INFO     [order_1_approx] starting weight calculation for picasso was known for their work as a  painter
tolstoi was known for their work as a  novelist
newton was known for their work as a  scientist
pacino was known for their work as a  actor
plato was known for their work as a  philosopher
strauss was known for their work as a  composer
hume was known for their work as a  philosopher
dante was known for their work as a 
2024-07-13 07:19:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 07:23:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 07:27:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0750, -0.8408,  0.5918,  ...,  0.4829,  0.0830, -0.6621],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.0700, -0.7949,  0.5010,  ...,  0.4243,  0.0571, -0.6328],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0047, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1797,  2.1562, -6.7188,  ..., -4.2031,  3.5859,  0.2465],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0034, -0.0006,  0.0062,  ..., -0.0184,  0.0115,  0.0066],
        [ 0.0029,  0.0037,  0.0051,  ...,  0.0044,  0.0007,  0.0070],
        [-0.0054,  0.0005, -0.0050,  ..., -0.0028,  0.0067,  0.0034],
        ...,
        [-0.0011, -0.0015,  0.0080,  ..., -0.0136, -0.0017, -0.0020],
        [ 0.0029,  0.0054, -0.0048,  ..., -0.0017, -0.0071, -0.0007],
        [-0.0155, -0.0121,  0.0080,  ...,  0.0071,  0.0056,  0.0027]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-8.7070e-04, -9.6893e-04,  6.7663e-04,  ..., -3.1233e-04,
          1.3447e-03, -1.5354e-03],
        [-4.9305e-04, -1.7529e-03,  8.9216e-04,  ..., -1.5173e-03,
          8.1730e-04,  2.0695e-03],
        [ 6.1035e-05,  8.2541e-04,  1.0052e-03,  ...,  2.4080e-04,
          3.6049e-04,  1.1177e-03],
        ...,
        [-1.3752e-03,  6.5804e-04, -4.3797e-04,  ...,  3.5167e-05,
          7.7152e-04,  1.9569e-03],
        [ 1.7776e-03, -7.0524e-04, -2.0885e-03,  ..., -1.5914e-04,
         -2.0313e-03, -6.6662e-04],
        [-5.9128e-04,  9.2554e-04,  3.9768e-04,  ...,  7.7391e-04,
         -1.9908e-05, -3.9029e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0649,  1.4609, -6.4102,  ..., -4.6875,  3.8867,  0.0673]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1108, -0.0427, -0.0693,  ...,  0.0211, -0.1170,  0.2014]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 07:27:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for picasso was known for their work as a  painter
tolstoi was known for their work as a  novelist
newton was known for their work as a  scientist
pacino was known for their work as a  actor
plato was known for their work as a  philosopher
strauss was known for their work as a  composer
hume was known for their work as a  philosopher
dante was known for their work as a 
2024-07-13 07:27:42 root INFO     [order_1_approx] starting weight calculation for strauss was known for their work as a  composer
picasso was known for their work as a  painter
tolstoi was known for their work as a  novelist
hume was known for their work as a  philosopher
plato was known for their work as a  philosopher
pacino was known for their work as a  actor
dante was known for their work as a  poet
newton was known for their work as a 
2024-07-13 07:27:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 07:31:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 07:35:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0820,  0.3774,  0.1807,  ...,  0.5117, -0.5649,  1.0439],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.0755,  0.3411,  0.1501,  ...,  0.4570, -0.5254,  0.9565],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0106, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.0586, -3.0781, -2.0039,  ..., -0.5396, -3.1836,  0.8555],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.2115e-02, -7.1030e-03,  5.7983e-04,  ...,  6.2180e-03,
          5.0545e-03,  7.2403e-03],
        [-3.6507e-03, -2.4967e-03,  4.3106e-03,  ...,  8.4534e-03,
         -6.5231e-03, -3.9043e-03],
        [-7.9727e-04,  1.0605e-02, -4.8027e-03,  ...,  1.0666e-02,
         -2.1935e-03, -1.2619e-02],
        ...,
        [-9.0027e-04,  3.9062e-03, -1.6117e-03,  ..., -5.5695e-03,
         -8.4152e-03,  2.1935e-03],
        [ 3.0708e-03,  2.1439e-03,  4.6349e-04,  ...,  3.5152e-03,
          1.0071e-03,  4.6310e-03],
        [-1.6296e-02, -2.7351e-03,  5.0888e-03,  ..., -6.8207e-03,
         -9.1553e-05,  8.7738e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.0967e-03, -1.0681e-04, -1.2846e-03,  ...,  5.5408e-04,
         -1.6069e-04,  1.2131e-03],
        [-1.4305e-03,  3.9530e-04,  1.0872e-03,  ...,  1.0967e-03,
         -7.6771e-04,  5.3525e-05],
        [-1.2579e-03,  2.1517e-04, -6.5804e-04,  ..., -2.4581e-04,
         -3.6240e-05,  1.3065e-03],
        ...,
        [ 4.4584e-04,  5.4598e-04,  3.9387e-04,  ...,  2.4796e-04,
         -4.5133e-04,  1.5097e-03],
        [ 4.0174e-04,  5.0974e-04,  5.7316e-04,  ..., -1.5140e-04,
         -2.2125e-04,  1.4000e-03],
        [-4.6134e-05,  1.1005e-03, -4.3392e-04,  ..., -2.3210e-04,
         -5.0259e-04, -8.6975e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.3496, -3.0117, -2.1543,  ..., -0.2258, -2.6934,  0.5840]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0510, -0.0381,  0.0864,  ..., -0.1167,  0.0267,  0.0083]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 07:35:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for strauss was known for their work as a  composer
picasso was known for their work as a  painter
tolstoi was known for their work as a  novelist
hume was known for their work as a  philosopher
plato was known for their work as a  philosopher
pacino was known for their work as a  actor
dante was known for their work as a  poet
newton was known for their work as a 
2024-07-13 07:35:35 root INFO     [order_1_approx] starting weight calculation for hume was known for their work as a  philosopher
newton was known for their work as a  scientist
picasso was known for their work as a  painter
dante was known for their work as a  poet
strauss was known for their work as a  composer
pacino was known for their work as a  actor
tolstoi was known for their work as a  novelist
plato was known for their work as a 
2024-07-13 07:35:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 07:39:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 07:43:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7739,  0.4070,  1.0928,  ...,  0.3047,  1.0791,  0.7466],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.7422,  0.3855,  0.9902,  ...,  0.2866,  1.0029,  0.7090],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(-0.0137, device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8618, -0.6997, -0.4673,  ..., -5.4492,  0.1570,  0.1421],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-4.3907e-03, -5.6953e-03,  7.6637e-03,  ..., -7.7896e-03,
          1.6388e-02,  3.6964e-03],
        [-6.5994e-03,  8.2855e-03,  1.9577e-02,  ..., -4.1962e-05,
          4.0283e-03, -1.0025e-02],
        [ 1.2840e-02, -4.9686e-04, -2.5597e-03,  ..., -1.7441e-02,
          1.7914e-02, -8.9798e-03],
        ...,
        [ 4.5090e-03, -3.6659e-03, -2.0477e-02,  ..., -1.5747e-02,
         -1.9714e-02,  1.3298e-02],
        [ 5.2185e-03,  2.2869e-03, -1.3016e-02,  ...,  7.0343e-03,
         -2.2079e-02,  1.2894e-03],
        [-1.3855e-02,  7.1526e-04, -4.0436e-03,  ..., -3.7460e-03,
          5.3711e-03,  1.1604e-02]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.3422e-03,  5.5373e-05,  1.0252e-03,  ...,  1.5373e-03,
          1.1768e-03, -5.0354e-04],
        [-1.6317e-03, -1.4763e-03,  2.8152e-03,  ..., -3.2234e-04,
          1.1177e-03, -1.4067e-04],
        [-5.9652e-04, -3.1166e-03,  2.0256e-03,  ..., -2.0523e-03,
          3.1495e-04, -4.7779e-04],
        ...,
        [ 1.9855e-03,  2.4462e-04, -2.6512e-03,  ...,  1.8215e-03,
         -1.8644e-03,  9.4128e-04],
        [ 1.3561e-03,  1.5593e-03, -3.0422e-04,  ...,  1.2531e-03,
         -9.9754e-04,  1.0128e-03],
        [ 5.8270e-04, -2.6941e-04,  5.5599e-04,  ...,  1.4842e-04,
          7.3290e-04, -2.4140e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7109, -1.5146, -0.7500,  ..., -5.5898,  0.6021, -0.7510]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1000, -0.1073, -0.0194,  ..., -0.1265, -0.1632,  0.3325]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 07:43:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for hume was known for their work as a  philosopher
newton was known for their work as a  scientist
picasso was known for their work as a  painter
dante was known for their work as a  poet
strauss was known for their work as a  composer
pacino was known for their work as a  actor
tolstoi was known for their work as a  novelist
plato was known for their work as a 
2024-07-13 07:43:28 root INFO     total operator prediction time: 3786.639382123947 seconds
2024-07-13 07:43:28 __main__ INFO     starting test: <class 'lre.operators.JacobianIclMeanEstimator'> on country - capital
2024-07-13 07:43:31 root INFO     building operator country - capital
2024-07-13 07:43:31 root INFO     [order_1_approx] starting weight calculation for The country with taipei as its capital is known as taiwan
The country with tbilisi as its capital is known as georgia
The country with dublin as its capital is known as ireland
The country with london as its capital is known as england
The country with islamabad as its capital is known as pakistan
The country with abuja as its capital is known as nigeria
The country with manila as its capital is known as philippines
The country with conakry as its capital is known as
2024-07-13 07:43:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 07:47:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 07:51:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0217, -0.6982,  0.1484,  ...,  0.6509, -0.3484,  0.0974],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.0209, -0.7329,  0.1338,  ...,  0.6333, -0.3606,  0.0792],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.9766,  5.8281,  0.6758,  ..., -2.7617, -2.5645,  0.6738],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0091, -0.0087,  0.0011,  ...,  0.0115,  0.0145, -0.0210],
        [ 0.0049, -0.0286,  0.0005,  ...,  0.0007, -0.0058, -0.0305],
        [-0.0209,  0.0140, -0.0008,  ...,  0.0135, -0.0026, -0.0143],
        ...,
        [-0.0130,  0.0033,  0.0150,  ...,  0.0113,  0.0064,  0.0045],
        [-0.0033,  0.0225, -0.0196,  ...,  0.0260, -0.0387,  0.0309],
        [-0.0013, -0.0120,  0.0067,  ...,  0.0119,  0.0271, -0.0410]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-1.2732e-03, -8.0156e-04, -6.5613e-04,  ...,  1.3580e-03,
         -4.4012e-04, -2.0790e-03],
        [-1.6928e-05, -6.8665e-04, -2.1496e-03,  ...,  1.0185e-03,
          3.5667e-03, -9.2173e-04],
        [-3.9482e-04, -7.6866e-04,  7.6294e-04,  ...,  1.5259e-05,
         -4.6372e-04, -2.4867e-04],
        ...,
        [-5.0831e-04,  2.8038e-04,  1.2770e-03,  ...,  1.2302e-03,
          1.3208e-03, -1.6384e-03],
        [-4.6062e-04,  9.5367e-04, -5.6553e-04,  ...,  7.4005e-04,
         -2.9202e-03, -7.9918e-04],
        [-7.5102e-04, -8.5306e-04, -2.4776e-03,  ...,  2.2602e-04,
          9.5844e-04, -1.1845e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6484,  4.5391,  0.1826,  ..., -2.3652, -2.3398,  0.5684]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1857, -0.6030, -0.0415,  ..., -0.1583, -0.0486, -0.3796]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 07:51:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with taipei as its capital is known as taiwan
The country with tbilisi as its capital is known as georgia
The country with dublin as its capital is known as ireland
The country with london as its capital is known as england
The country with islamabad as its capital is known as pakistan
The country with abuja as its capital is known as nigeria
The country with manila as its capital is known as philippines
The country with conakry as its capital is known as
2024-07-13 07:51:24 root INFO     [order_1_approx] starting weight calculation for The country with conakry as its capital is known as guinea
The country with manila as its capital is known as philippines
The country with dublin as its capital is known as ireland
The country with taipei as its capital is known as taiwan
The country with abuja as its capital is known as nigeria
The country with london as its capital is known as england
The country with tbilisi as its capital is known as georgia
The country with islamabad as its capital is known as
2024-07-13 07:51:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 07:55:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 07:59:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6152, -0.6074,  0.1958,  ...,  0.4519, -0.8911,  0.2217],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.7324, -0.7534,  0.2078,  ...,  0.5137, -1.0566,  0.2417],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5146, -3.3828,  1.5098,  ..., -1.2393,  1.0303, -1.7266],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0035, -0.0032, -0.0073,  ...,  0.0137,  0.0021, -0.0187],
        [ 0.0084,  0.0055,  0.0258,  ..., -0.0110, -0.0147,  0.0029],
        [ 0.0025,  0.0140, -0.0023,  ...,  0.0023,  0.0072,  0.0145],
        ...,
        [-0.0088,  0.0022, -0.0050,  ...,  0.0090,  0.0012,  0.0017],
        [-0.0090, -0.0145, -0.0085,  ...,  0.0212, -0.0190,  0.0255],
        [ 0.0022, -0.0108, -0.0054,  ...,  0.0135,  0.0264, -0.0162]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 4.1986e-04, -7.7772e-04, -1.5535e-03,  ...,  1.3375e-04,
         -1.6861e-03, -2.4557e-04],
        [-6.9141e-05,  1.1959e-03,  1.6856e-04,  ...,  5.3287e-05,
         -1.3220e-04,  9.5940e-04],
        [-1.1063e-04,  1.4458e-03,  9.6893e-04,  ..., -9.1743e-04,
         -4.2439e-05, -3.0041e-05],
        ...,
        [ 1.1950e-03, -4.2796e-05,  7.1287e-04,  ...,  5.8079e-04,
         -1.0033e-03,  4.1819e-04],
        [-1.2217e-03, -9.6607e-04, -8.2397e-04,  ...,  1.1873e-03,
          6.8784e-05, -7.0572e-05],
        [-3.2592e-04, -8.7261e-04, -9.0790e-04,  ...,  1.5039e-03,
          2.9206e-04, -2.0046e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8940, -3.0098,  0.6865,  ..., -1.1348,  0.7236, -2.8164]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.0975,  0.0427, -0.1874,  ...,  0.2310,  0.0089, -0.1713]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 07:59:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with conakry as its capital is known as guinea
The country with manila as its capital is known as philippines
The country with dublin as its capital is known as ireland
The country with taipei as its capital is known as taiwan
The country with abuja as its capital is known as nigeria
The country with london as its capital is known as england
The country with tbilisi as its capital is known as georgia
The country with islamabad as its capital is known as
2024-07-13 07:59:19 root INFO     [order_1_approx] starting weight calculation for The country with london as its capital is known as england
The country with taipei as its capital is known as taiwan
The country with tbilisi as its capital is known as georgia
The country with abuja as its capital is known as nigeria
The country with dublin as its capital is known as ireland
The country with conakry as its capital is known as guinea
The country with islamabad as its capital is known as pakistan
The country with manila as its capital is known as
2024-07-13 07:59:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 08:03:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 08:07:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1465,  0.3618, -0.6138,  ...,  0.6567,  0.2246,  0.7080],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.1375,  0.3242, -0.5479,  ...,  0.5825,  0.1855,  0.6406],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.5020, -3.0879, -1.2393,  ..., -6.6094,  0.7354, -0.7651],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0021,  0.0059, -0.0165,  ...,  0.0102,  0.0098, -0.0030],
        [ 0.0013, -0.0112,  0.0077,  ...,  0.0125,  0.0016, -0.0158],
        [ 0.0026,  0.0071, -0.0051,  ..., -0.0049,  0.0003,  0.0010],
        ...,
        [-0.0042,  0.0070,  0.0038,  ..., -0.0029, -0.0107,  0.0040],
        [ 0.0037,  0.0144,  0.0014,  ..., -0.0162, -0.0082,  0.0059],
        [-0.0116, -0.0050,  0.0108,  ...,  0.0054,  0.0119, -0.0017]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-2.1114e-03,  6.4325e-04, -2.5520e-03,  ...,  3.9721e-04,
          1.1784e-04, -8.5306e-04],
        [ 2.6250e-04, -1.2159e-03,  1.3409e-03,  ..., -2.3627e-04,
          1.0908e-04, -1.4744e-03],
        [-4.1556e-04,  1.0042e-03,  1.6916e-04,  ..., -1.6928e-04,
          3.8433e-04,  3.6478e-04],
        ...,
        [-1.2884e-03,  1.4753e-03, -3.2544e-05,  ..., -8.2302e-04,
         -2.1315e-04,  1.5726e-03],
        [-2.0146e-04,  5.7125e-04,  4.7684e-07,  ...,  6.4135e-04,
          2.0552e-04, -4.0722e-04],
        [-1.1578e-03, -1.8253e-03,  3.0780e-04,  ...,  8.5115e-04,
         -9.5654e-04, -7.1764e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.6309, -3.7734, -1.4102,  ..., -5.9219,  1.3984, -1.3496]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.1045, -0.1716,  0.0492,  ...,  0.2091, -0.0580,  0.0437]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 08:07:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with london as its capital is known as england
The country with taipei as its capital is known as taiwan
The country with tbilisi as its capital is known as georgia
The country with abuja as its capital is known as nigeria
The country with dublin as its capital is known as ireland
The country with conakry as its capital is known as guinea
The country with islamabad as its capital is known as pakistan
The country with manila as its capital is known as
2024-07-13 08:07:14 root INFO     [order_1_approx] starting weight calculation for The country with abuja as its capital is known as nigeria
The country with manila as its capital is known as philippines
The country with islamabad as its capital is known as pakistan
The country with london as its capital is known as england
The country with taipei as its capital is known as taiwan
The country with dublin as its capital is known as ireland
The country with conakry as its capital is known as guinea
The country with tbilisi as its capital is known as
2024-07-13 08:07:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 08:11:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 08:15:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4580, -0.0652,  0.8057,  ...,  0.5654,  0.0311,  0.1403],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.4775, -0.0866,  0.7954,  ...,  0.5728,  0.0142,  0.1271],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1484,  2.4004,  5.9961,  ...,  0.4080, -1.0801, -1.8809],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0073, -0.0226, -0.0113,  ...,  0.0264, -0.0004, -0.0220],
        [-0.0004, -0.0239,  0.0021,  ...,  0.0124,  0.0035, -0.0008],
        [-0.0137, -0.0213,  0.0024,  ..., -0.0052,  0.0331, -0.0129],
        ...,
        [-0.0284,  0.0017,  0.0195,  ...,  0.0064, -0.0190,  0.0324],
        [-0.0136,  0.0005,  0.0094,  ..., -0.0053, -0.0257,  0.0070],
        [ 0.0262,  0.0012,  0.0012,  ..., -0.0234,  0.0200, -0.0244]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 1.7653e-03, -7.1764e-05, -3.4046e-04,  ...,  7.2300e-05,
         -1.2550e-03, -1.2054e-03],
        [-1.4277e-03,  2.8729e-04, -3.1137e-04,  ...,  1.2074e-03,
          3.7742e-04, -1.2846e-03],
        [-2.8477e-03, -1.8482e-03,  9.2506e-04,  ...,  1.8539e-03,
          1.3599e-03, -4.7731e-04],
        ...,
        [-8.3876e-04,  6.3896e-04,  9.3794e-04,  ...,  1.6842e-03,
         -1.1110e-04,  2.3746e-03],
        [ 5.9938e-04, -8.9884e-05,  1.6470e-03,  ...,  5.6887e-04,
         -6.5041e-04, -5.0902e-05],
        [ 6.0940e-04, -9.2888e-04,  7.4577e-04,  ...,  2.4681e-03,
          1.7710e-03, -9.6750e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3545,  2.1758,  4.0625,  ...,  1.0205, -0.0391, -1.8369]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[-0.1328, -0.1544, -0.3220,  ...,  0.1566, -0.2786, -0.1915]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 08:15:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with abuja as its capital is known as nigeria
The country with manila as its capital is known as philippines
The country with islamabad as its capital is known as pakistan
The country with london as its capital is known as england
The country with taipei as its capital is known as taiwan
The country with dublin as its capital is known as ireland
The country with conakry as its capital is known as guinea
The country with tbilisi as its capital is known as
2024-07-13 08:15:09 root INFO     [order_1_approx] starting weight calculation for The country with dublin as its capital is known as ireland
The country with islamabad as its capital is known as pakistan
The country with london as its capital is known as england
The country with tbilisi as its capital is known as georgia
The country with abuja as its capital is known as nigeria
The country with manila as its capital is known as philippines
The country with conakry as its capital is known as guinea
The country with taipei as its capital is known as
2024-07-13 08:15:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 08:19:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 08:23:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5444, -0.0454, -0.5728,  ..., -0.5347,  0.5635, -0.1932],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.5283, -0.0606, -0.5371,  ..., -0.4890,  0.5181, -0.2124],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7461, -0.9883, -1.7617,  ..., -5.6953,  0.4556, -1.8691],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0063, -0.0093, -0.0050,  ...,  0.0124,  0.0092, -0.0061],
        [-0.0095, -0.0096,  0.0177,  ...,  0.0233, -0.0041, -0.0097],
        [-0.0099,  0.0155,  0.0089,  ...,  0.0027, -0.0013, -0.0096],
        ...,
        [-0.0045,  0.0113, -0.0012,  ...,  0.0058, -0.0008,  0.0008],
        [ 0.0106,  0.0190, -0.0085,  ..., -0.0175, -0.0088,  0.0009],
        [ 0.0168,  0.0168,  0.0005,  ...,  0.0065,  0.0128, -0.0061]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[ 1.9550e-04, -1.5140e-04,  5.9271e-04,  ...,  1.9093e-03,
          2.6989e-04,  6.4075e-05],
        [-9.1553e-04,  5.6076e-04,  1.3771e-03,  ...,  1.5717e-03,
          2.6989e-04, -2.4376e-03],
        [-2.4624e-03,  7.4863e-04,  1.5697e-03,  ...,  1.2779e-03,
          1.3304e-03,  4.6158e-04],
        ...,
        [-1.6232e-03,  5.7030e-04,  1.0853e-03,  ...,  6.7472e-04,
          5.8651e-04, -2.7061e-05],
        [-5.1355e-04, -2.0671e-04,  2.5845e-04,  ..., -1.2255e-03,
         -1.6260e-03, -3.4237e-04],
        [ 2.6345e-05, -3.7308e-03,  1.5459e-03,  ...,  7.9966e-04,
         -7.4148e-04, -4.9829e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.4668, -1.2168, -2.1953,  ..., -4.6836,  1.9258, -0.7832]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0386, -0.2949, -0.1409,  ..., -0.0806,  0.0724, -0.1862]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 08:23:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with dublin as its capital is known as ireland
The country with islamabad as its capital is known as pakistan
The country with london as its capital is known as england
The country with tbilisi as its capital is known as georgia
The country with abuja as its capital is known as nigeria
The country with manila as its capital is known as philippines
The country with conakry as its capital is known as guinea
The country with taipei as its capital is known as
2024-07-13 08:23:05 root INFO     [order_1_approx] starting weight calculation for The country with manila as its capital is known as philippines
The country with conakry as its capital is known as guinea
The country with abuja as its capital is known as nigeria
The country with islamabad as its capital is known as pakistan
The country with dublin as its capital is known as ireland
The country with tbilisi as its capital is known as georgia
The country with taipei as its capital is known as taiwan
The country with london as its capital is known as
2024-07-13 08:23:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 08:27:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 08:30:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9961, -0.5479, -0.4507,  ...,  0.3813, -0.8022,  0.2028],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.9556, -0.5522, -0.4209,  ...,  0.3545, -0.7729,  0.1755],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5615,  3.5234, -0.3379,  ...,  0.4021,  2.3672, -5.2344],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 7.7896e-03,  3.5763e-03, -1.1307e-02,  ...,  1.5678e-03,
          7.2098e-03, -1.2650e-02],
        [-7.2327e-03, -6.4964e-03,  5.4626e-03,  ...,  1.2253e-02,
         -3.3550e-03,  3.9787e-03],
        [-4.4174e-03,  4.4174e-03, -2.1400e-03,  ..., -1.4706e-03,
          1.7300e-03, -2.9449e-03],
        ...,
        [-1.5778e-02, -1.9012e-02,  2.4662e-03,  ...,  8.9645e-05,
         -4.5586e-03,  7.9041e-03],
        [ 1.0452e-03,  9.8953e-03, -5.4131e-03,  ..., -8.0261e-03,
         -1.6289e-03,  5.9738e-03],
        [-3.4256e-03, -1.2451e-02, -1.0544e-02,  ..., -9.5291e-03,
          1.4328e-02, -6.5765e-03]], device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-7.2384e-04, -6.6090e-04,  1.3804e-04,  ..., -4.9448e-04,
         -2.5570e-05, -9.3889e-04],
        [ 1.4963e-03,  3.9458e-04,  4.5776e-04,  ...,  1.8749e-03,
         -9.4175e-05, -1.0853e-03],
        [-6.3324e-04, -7.8630e-04,  7.9870e-04,  ..., -4.5085e-04,
          1.2946e-04, -1.3027e-03],
        ...,
        [-1.7662e-03, -1.8854e-03,  8.2922e-04,  ...,  7.9346e-04,
          9.0694e-04, -1.1120e-03],
        [-1.3857e-03,  4.1795e-04, -1.3280e-04,  ...,  6.3610e-04,
          4.3559e-04, -1.4219e-03],
        [-1.0443e-03, -3.4695e-03,  3.3665e-04,  ..., -1.9312e-05,
         -3.4094e-04,  5.1022e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5098,  3.1328, -0.5986,  ..., -0.4260,  2.4980, -6.0859]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0635, -0.1749, -0.0434,  ..., -0.0276, -0.2988,  0.0947]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 08:30:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with manila as its capital is known as philippines
The country with conakry as its capital is known as guinea
The country with abuja as its capital is known as nigeria
The country with islamabad as its capital is known as pakistan
The country with dublin as its capital is known as ireland
The country with tbilisi as its capital is known as georgia
The country with taipei as its capital is known as taiwan
The country with london as its capital is known as
2024-07-13 08:30:59 root INFO     [order_1_approx] starting weight calculation for The country with taipei as its capital is known as taiwan
The country with islamabad as its capital is known as pakistan
The country with conakry as its capital is known as guinea
The country with manila as its capital is known as philippines
The country with tbilisi as its capital is known as georgia
The country with london as its capital is known as england
The country with abuja as its capital is known as nigeria
The country with dublin as its capital is known as
2024-07-13 08:30:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 08:34:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 08:38:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4766,  0.4424, -1.0020,  ..., -0.0139, -0.5352,  1.4688],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([-0.4795,  0.4368, -0.9653,  ..., -0.0084, -0.5444,  1.4746],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1172,  1.6992,  2.3477,  ..., -1.6074,  2.2188, -0.8730],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0048,  0.0010, -0.0070,  ...,  0.0062,  0.0009, -0.0034],
        [-0.0200, -0.0186,  0.0129,  ...,  0.0198, -0.0061,  0.0015],
        [-0.0138, -0.0065, -0.0095,  ...,  0.0084, -0.0020, -0.0072],
        ...,
        [ 0.0085,  0.0056, -0.0080,  ..., -0.0088, -0.0061,  0.0004],
        [ 0.0112,  0.0144, -0.0221,  ..., -0.0157,  0.0009, -0.0020],
        [-0.0245, -0.0046,  0.0003,  ..., -0.0010,  0.0098, -0.0071]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-9.0456e-04, -7.5102e-04, -8.3542e-04,  ..., -1.1501e-03,
         -1.4102e-04,  1.3437e-03],
        [-7.7820e-04, -8.8263e-04,  1.0214e-03,  ...,  1.4000e-03,
          4.8733e-04, -3.1815e-03],
        [-9.2125e-04, -7.3147e-04, -1.5659e-03,  ...,  6.8760e-04,
          6.5327e-04,  2.2864e-04],
        ...,
        [-1.0109e-03, -5.8365e-04,  1.2436e-03,  ...,  1.9956e-04,
          1.3828e-03, -1.6308e-03],
        [-1.1759e-03,  2.7156e-04,  5.9223e-04,  ..., -3.1114e-05,
          4.4274e-04, -8.4448e-04],
        [ 1.6928e-04, -1.3084e-03, -1.1177e-03,  ..., -3.1304e-04,
          4.1294e-04,  1.0943e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5371,  0.9214,  1.5098,  ..., -1.0020,  2.9355, -0.7773]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0302, -0.3088, -0.0406,  ..., -0.0465, -0.1070, -0.0533]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 08:38:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with taipei as its capital is known as taiwan
The country with islamabad as its capital is known as pakistan
The country with conakry as its capital is known as guinea
The country with manila as its capital is known as philippines
The country with tbilisi as its capital is known as georgia
The country with london as its capital is known as england
The country with abuja as its capital is known as nigeria
The country with dublin as its capital is known as
2024-07-13 08:38:53 root INFO     [order_1_approx] starting weight calculation for The country with london as its capital is known as england
The country with manila as its capital is known as philippines
The country with dublin as its capital is known as ireland
The country with taipei as its capital is known as taiwan
The country with tbilisi as its capital is known as georgia
The country with islamabad as its capital is known as pakistan
The country with conakry as its capital is known as guinea
The country with abuja as its capital is known as
2024-07-13 08:38:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-13 08:42:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27.ln_1 does not match transformer.h.6.ln_1
2024-07-13 08:46:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4399, -0.4919, -1.1338,  ..., -0.0591, -0.4414,  0.1187],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_s_j=tensor([ 0.4988, -0.5864, -1.2363,  ..., -0.0579, -0.5098,  0.1165],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        ln_o_j1=tensor(0.0066, device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.3633,  0.2686,  3.2031,  ...,  2.1035,  1.6826, -0.8794],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0097,  0.0100, -0.0298,  ...,  0.0133,  0.0026, -0.0228],
        [-0.0313,  0.0097,  0.0245,  ...,  0.0357,  0.0185, -0.0308],
        [-0.0141,  0.0081, -0.0271,  ..., -0.0219, -0.0140,  0.0155],
        ...,
        [-0.0084, -0.0096, -0.0109,  ...,  0.0341, -0.0319, -0.0241],
        [-0.0168,  0.0026,  0.0075,  ...,  0.0239,  0.0050,  0.0551],
        [-0.0240,  0.0198, -0.0127,  ..., -0.0071,  0.0037, -0.0086]],
       device='cuda:1', dtype=torch.float16) 

                        ln_s_o_weight: tensor([[-0.0006, -0.0004, -0.0019,  ...,  0.0013, -0.0004, -0.0005],
        [ 0.0005, -0.0003,  0.0003,  ..., -0.0002,  0.0008, -0.0013],
        [-0.0007,  0.0006,  0.0002,  ..., -0.0006, -0.0033,  0.0006],
        ...,
        [-0.0011,  0.0004, -0.0015,  ...,  0.0002,  0.0017, -0.0003],
        [-0.0003, -0.0003,  0.0012,  ...,  0.0024, -0.0013, -0.0003],
        [ 0.0009, -0.0021, -0.0016,  ..., -0.0001, -0.0009, -0.0005]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.8086, -1.2012,  3.0137,  ..., -0.5098,  3.3828, -2.0273]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                        ln_s_o_bias=tensor([[ 0.0069, -0.0493,  0.0389,  ...,  0.0047, -0.0548,  0.0927]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>)
                    
2024-07-13 08:46:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with london as its capital is known as england
The country with manila as its capital is known as philippines
The country with dublin as its capital is known as ireland
The country with taipei as its capital is known as taiwan
The country with tbilisi as its capital is known as georgia
The country with islamabad as its capital is known as pakistan
The country with conakry as its capital is known as guinea
The country with abuja as its capital is known as
2024-07-13 08:46:47 root INFO     total operator prediction time: 3795.8399982452393 seconds
