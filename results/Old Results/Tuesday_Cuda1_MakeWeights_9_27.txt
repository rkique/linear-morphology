2024-07-16 17:05:20 root INFO     loading model + tokenizer
2024-07-16 17:05:37 root INFO     model + tokenizer loaded
2024-07-16 17:05:37 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - part
2024-07-16 17:05:37 root INFO     building operator meronyms - part
2024-07-16 17:05:38 root INFO     [order_1_approx] starting weight calculation for A part of a dollar is a cent
A part of a poem is a stanza
A part of a seafront is a harbor
A part of a piano is a keyboard
A part of a byte is a bit
A part of a day is a hour
A part of a pub is a bar
A part of a dress is a
2024-07-16 17:05:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 17:09:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.6680, -0.2422,  0.4316,  ...,  1.4004, -1.7793, -0.3560],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0820,  0.1589, -4.0469,  ..., -1.9971, -3.0156,  0.1099],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0118, -0.0066,  0.0021,  ..., -0.0046, -0.0062, -0.0029],
        [ 0.0013, -0.0008,  0.0084,  ...,  0.0086,  0.0067, -0.0029],
        [ 0.0039, -0.0096, -0.0050,  ..., -0.0081,  0.0040,  0.0056],
        ...,
        [ 0.0022,  0.0108, -0.0089,  ...,  0.0072, -0.0056, -0.0064],
        [ 0.0022,  0.0032,  0.0029,  ..., -0.0026,  0.0024,  0.0121],
        [-0.0010,  0.0033,  0.0079,  ..., -0.0025,  0.0087,  0.0137]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8794, -0.1870, -4.4609,  ..., -1.7910, -2.9844,  0.2001]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 17:09:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a dollar is a cent
A part of a poem is a stanza
A part of a seafront is a harbor
A part of a piano is a keyboard
A part of a byte is a bit
A part of a day is a hour
A part of a pub is a bar
A part of a dress is a
2024-07-16 17:09:05 root INFO     [order_1_approx] starting weight calculation for A part of a byte is a bit
A part of a poem is a stanza
A part of a piano is a keyboard
A part of a dress is a sleeve
A part of a dollar is a cent
A part of a seafront is a harbor
A part of a day is a hour
A part of a pub is a
2024-07-16 17:09:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 17:12:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5747, -0.1367,  0.2734,  ...,  0.7900,  0.5640,  1.2051],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.9297, -0.5415, -1.3535,  ...,  0.9731, -0.0508, -3.6758],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0037, -0.0099,  0.0024,  ..., -0.0022, -0.0021,  0.0022],
        [ 0.0002,  0.0029,  0.0133,  ...,  0.0032,  0.0016,  0.0031],
        [-0.0043,  0.0008, -0.0009,  ..., -0.0042,  0.0075, -0.0056],
        ...,
        [-0.0032, -0.0069,  0.0026,  ...,  0.0026, -0.0077, -0.0087],
        [ 0.0038, -0.0033, -0.0079,  ...,  0.0085, -0.0051,  0.0012],
        [-0.0012,  0.0044, -0.0006,  ...,  0.0006, -0.0046,  0.0035]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.0898, -0.5215, -1.5186,  ...,  0.6562,  0.0317, -3.3047]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 17:12:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a byte is a bit
A part of a poem is a stanza
A part of a piano is a keyboard
A part of a dress is a sleeve
A part of a dollar is a cent
A part of a seafront is a harbor
A part of a day is a hour
A part of a pub is a
2024-07-16 17:12:32 root INFO     [order_1_approx] starting weight calculation for A part of a byte is a bit
A part of a poem is a stanza
A part of a pub is a bar
A part of a dollar is a cent
A part of a piano is a keyboard
A part of a dress is a sleeve
A part of a day is a hour
A part of a seafront is a
2024-07-16 17:12:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 17:15:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1475, -0.5083,  0.7646,  ..., -1.2070,  0.1008,  1.8066],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.1523,  4.4727, -1.9971,  ...,  1.8711, -3.1055,  0.6006],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0156, -0.0113,  0.0014,  ..., -0.0060, -0.0076, -0.0002],
        [-0.0004,  0.0053,  0.0044,  ...,  0.0049, -0.0020, -0.0020],
        [-0.0079,  0.0032,  0.0088,  ..., -0.0070,  0.0082, -0.0055],
        ...,
        [-0.0013, -0.0026,  0.0051,  ...,  0.0098, -0.0039,  0.0037],
        [ 0.0010,  0.0014,  0.0005,  ...,  0.0028,  0.0074,  0.0014],
        [ 0.0010,  0.0048,  0.0002,  ...,  0.0013,  0.0067,  0.0172]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.5234,  3.3672, -1.9961,  ...,  0.8252, -2.8379,  0.7646]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 17:16:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a byte is a bit
A part of a poem is a stanza
A part of a pub is a bar
A part of a dollar is a cent
A part of a piano is a keyboard
A part of a dress is a sleeve
A part of a day is a hour
A part of a seafront is a
2024-07-16 17:16:00 root INFO     [order_1_approx] starting weight calculation for A part of a byte is a bit
A part of a day is a hour
A part of a poem is a stanza
A part of a pub is a bar
A part of a seafront is a harbor
A part of a dress is a sleeve
A part of a dollar is a cent
A part of a piano is a
2024-07-16 17:16:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 17:19:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3062, -0.2087,  0.3037,  ..., -1.1543, -0.4644,  0.0442],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7886, -2.6660, -2.4336,  ..., -6.1211, -0.3062, -1.6191],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0084, -0.0182,  0.0014,  ..., -0.0026,  0.0068, -0.0188],
        [-0.0091,  0.0205,  0.0139,  ...,  0.0047, -0.0056, -0.0022],
        [-0.0128,  0.0019,  0.0019,  ..., -0.0021,  0.0161, -0.0034],
        ...,
        [-0.0179, -0.0047,  0.0006,  ...,  0.0052, -0.0134,  0.0111],
        [-0.0023,  0.0009, -0.0030,  ..., -0.0054, -0.0054, -0.0071],
        [ 0.0075,  0.0114,  0.0104,  ..., -0.0024, -0.0105,  0.0143]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.4111, -1.7910, -1.5391,  ..., -5.4922,  0.1201, -1.6992]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 17:19:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a byte is a bit
A part of a day is a hour
A part of a poem is a stanza
A part of a pub is a bar
A part of a seafront is a harbor
A part of a dress is a sleeve
A part of a dollar is a cent
A part of a piano is a
2024-07-16 17:19:27 root INFO     [order_1_approx] starting weight calculation for A part of a dollar is a cent
A part of a day is a hour
A part of a poem is a stanza
A part of a pub is a bar
A part of a seafront is a harbor
A part of a dress is a sleeve
A part of a piano is a keyboard
A part of a byte is a
2024-07-16 17:19:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 17:22:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4199, -0.3428, -0.6768,  ...,  0.2313, -2.5859,  2.7285],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4473, -3.5117,  2.0215,  ..., -4.4883, -6.1953,  3.1328],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0087, -0.0113, -0.0009,  ...,  0.0031, -0.0066, -0.0098],
        [-0.0069, -0.0184, -0.0023,  ..., -0.0009, -0.0170, -0.0150],
        [-0.0034, -0.0003, -0.0127,  ..., -0.0089,  0.0007,  0.0053],
        ...,
        [-0.0025, -0.0094,  0.0034,  ...,  0.0016, -0.0066,  0.0036],
        [ 0.0063,  0.0087, -0.0062,  ...,  0.0082, -0.0214,  0.0015],
        [ 0.0055,  0.0086, -0.0021,  ...,  0.0118,  0.0048, -0.0022]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4863, -3.6562,  0.5254,  ..., -4.9922, -6.0703,  3.1641]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 17:22:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a dollar is a cent
A part of a day is a hour
A part of a poem is a stanza
A part of a pub is a bar
A part of a seafront is a harbor
A part of a dress is a sleeve
A part of a piano is a keyboard
A part of a byte is a
2024-07-16 17:22:55 root INFO     [order_1_approx] starting weight calculation for A part of a seafront is a harbor
A part of a dollar is a cent
A part of a pub is a bar
A part of a dress is a sleeve
A part of a byte is a bit
A part of a day is a hour
A part of a piano is a keyboard
A part of a poem is a
2024-07-16 17:22:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 17:26:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4937, -0.6631,  0.7842,  ..., -1.5430, -1.1152,  0.2009],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8579,  1.9541, -3.9746,  ..., -4.2109, -2.0859, -4.5195],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0192, -0.0199,  0.0005,  ..., -0.0037,  0.0010, -0.0059],
        [-0.0049, -0.0019,  0.0143,  ...,  0.0067,  0.0007, -0.0136],
        [ 0.0036, -0.0031, -0.0065,  ..., -0.0102, -0.0091,  0.0037],
        ...,
        [ 0.0090, -0.0141, -0.0004,  ...,  0.0123,  0.0008,  0.0014],
        [ 0.0001,  0.0013,  0.0166,  ...,  0.0099, -0.0024,  0.0012],
        [-0.0008, -0.0006,  0.0058,  ...,  0.0010,  0.0019,  0.0116]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2778,  2.5391, -4.4727,  ..., -4.8867, -2.2559, -5.1367]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 17:26:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a seafront is a harbor
A part of a dollar is a cent
A part of a pub is a bar
A part of a dress is a sleeve
A part of a byte is a bit
A part of a day is a hour
A part of a piano is a keyboard
A part of a poem is a
2024-07-16 17:26:23 root INFO     [order_1_approx] starting weight calculation for A part of a poem is a stanza
A part of a dollar is a cent
A part of a byte is a bit
A part of a piano is a keyboard
A part of a pub is a bar
A part of a seafront is a harbor
A part of a dress is a sleeve
A part of a day is a
2024-07-16 17:26:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 17:29:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3032, -1.7900,  0.6982,  ..., -0.6689, -0.9756,  0.3042],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.6836,  1.9443, -1.1582,  ..., -3.2988,  0.1582,  0.4395],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0191, -0.0034,  0.0026,  ..., -0.0077,  0.0135, -0.0059],
        [-0.0065, -0.0002,  0.0045,  ...,  0.0002,  0.0051, -0.0112],
        [-0.0151, -0.0055,  0.0149,  ...,  0.0008, -0.0021, -0.0069],
        ...,
        [-0.0068, -0.0022,  0.0075,  ...,  0.0126,  0.0112,  0.0060],
        [ 0.0037, -0.0023,  0.0027,  ...,  0.0190,  0.0059,  0.0090],
        [ 0.0016,  0.0060,  0.0028,  ..., -0.0007,  0.0107,  0.0229]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.7305,  1.9844, -1.5283,  ..., -3.3496,  1.0020,  0.5376]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 17:29:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a poem is a stanza
A part of a dollar is a cent
A part of a byte is a bit
A part of a piano is a keyboard
A part of a pub is a bar
A part of a seafront is a harbor
A part of a dress is a sleeve
A part of a day is a
2024-07-16 17:29:50 root INFO     [order_1_approx] starting weight calculation for A part of a seafront is a harbor
A part of a piano is a keyboard
A part of a dress is a sleeve
A part of a byte is a bit
A part of a day is a hour
A part of a poem is a stanza
A part of a pub is a bar
A part of a dollar is a
2024-07-16 17:29:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 17:33:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2742, -1.8203, -0.5483,  ...,  1.1963, -0.7754,  2.5918],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 6.1758,  0.2568, -5.9297,  ..., -2.2305, -1.1748, -0.1768],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 7.4959e-03, -3.1494e-02,  1.0239e-02,  ..., -6.6986e-03,
          3.1757e-03, -1.3138e-02],
        [ 3.3531e-03,  5.1193e-03,  1.3550e-02,  ...,  4.6310e-03,
         -7.9803e-03, -4.7455e-03],
        [-9.0485e-03,  1.1810e-02,  7.6294e-06,  ..., -1.3351e-02,
         -5.0888e-03,  6.8512e-03],
        ...,
        [-5.6496e-03,  5.5885e-04,  1.4259e-02,  ...,  1.8494e-02,
         -1.2558e-02,  5.3291e-03],
        [ 9.3384e-03, -1.9577e-02,  2.7981e-03,  ...,  1.5366e-02,
          2.8896e-03,  1.8784e-02],
        [ 2.1420e-03,  8.6060e-03,  2.9182e-04,  ...,  2.4490e-03,
         -1.0017e-02,  1.4389e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.3711,  0.4419, -5.4102,  ..., -1.6934, -0.7275,  0.8179]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 17:33:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a seafront is a harbor
A part of a piano is a keyboard
A part of a dress is a sleeve
A part of a byte is a bit
A part of a day is a hour
A part of a poem is a stanza
A part of a pub is a bar
A part of a dollar is a
2024-07-16 17:33:19 root INFO     total operator prediction time: 1661.5090322494507 seconds
2024-07-16 17:33:19 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on synonyms - exact
2024-07-16 17:33:19 root INFO     building operator synonyms - exact
2024-07-16 17:33:19 root INFO     [order_1_approx] starting weight calculation for Another word for harbor is seaport
Another word for clothes is clothing
Another word for sofa is couch
Another word for new is modern
Another word for mother is mom
Another word for market is marketplace
Another word for mend is repair
Another word for portion is
2024-07-16 17:33:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 17:36:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7598, -0.1508, -1.5137,  ..., -1.2188, -0.3728,  2.0469],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.9590,  1.0293,  0.4648,  ..., -1.8535,  3.3789, -1.0967],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 7.0419e-03, -1.4236e-02,  8.8654e-03,  ..., -2.2537e-02,
         -1.2123e-02, -3.4332e-03],
        [-3.2234e-03, -4.7302e-04,  1.5244e-02,  ...,  2.1606e-02,
         -1.6418e-02, -1.4839e-03],
        [-2.4891e-03, -2.2736e-03,  2.1744e-04,  ...,  1.3611e-02,
         -7.9651e-03, -5.5733e-03],
        ...,
        [-7.9803e-03, -1.3504e-02,  2.1458e-04,  ...,  1.9379e-02,
         -5.1498e-04,  4.0741e-03],
        [ 6.4240e-03, -3.1166e-03, -2.8706e-04,  ..., -2.0386e-02,
         -1.2802e-02,  3.7384e-03],
        [-2.1553e-03,  4.0054e-05, -1.3672e-02,  ...,  8.1253e-03,
          7.1182e-03, -6.7177e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.9141, -0.2646, -0.0967,  ..., -1.2539,  2.6387, -0.8555]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 17:36:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for harbor is seaport
Another word for clothes is clothing
Another word for sofa is couch
Another word for new is modern
Another word for mother is mom
Another word for market is marketplace
Another word for mend is repair
Another word for portion is
2024-07-16 17:36:45 root INFO     [order_1_approx] starting weight calculation for Another word for market is marketplace
Another word for new is modern
Another word for portion is part
Another word for mend is repair
Another word for sofa is couch
Another word for mother is mom
Another word for clothes is clothing
Another word for harbor is
2024-07-16 17:36:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 17:40:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0537,  0.7334, -0.9307,  ..., -0.7803,  0.3110,  2.3750],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.4043,  1.4639, -1.9893,  ..., -0.9507,  2.3867,  3.6836],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 5.7220e-03, -8.5831e-03,  5.9319e-03,  ...,  1.1616e-03,
         -1.9623e-02,  1.5991e-02],
        [ 4.3602e-03, -6.3477e-03, -5.2490e-03,  ...,  1.4839e-02,
         -1.2321e-02,  8.0585e-04],
        [-1.2341e-03, -2.7084e-03, -6.2866e-03,  ..., -2.3727e-02,
          4.5242e-03,  1.1230e-02],
        ...,
        [ 3.4027e-03, -9.9564e-04,  3.0022e-03,  ...,  2.2751e-02,
         -8.5735e-04,  3.1624e-03],
        [ 8.4686e-03, -1.2085e-02, -4.1656e-03,  ...,  7.3290e-04,
         -7.6866e-03,  7.4120e-03],
        [-1.0368e-02, -4.0894e-03,  1.3115e-02,  ..., -5.3520e-03,
         -5.4836e-05,  1.8021e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6670,  1.4180, -1.0391,  ..., -1.7021,  2.7773,  4.5469]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 17:40:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for market is marketplace
Another word for new is modern
Another word for portion is part
Another word for mend is repair
Another word for sofa is couch
Another word for mother is mom
Another word for clothes is clothing
Another word for harbor is
2024-07-16 17:40:13 root INFO     [order_1_approx] starting weight calculation for Another word for portion is part
Another word for mother is mom
Another word for harbor is seaport
Another word for sofa is couch
Another word for mend is repair
Another word for market is marketplace
Another word for new is modern
Another word for clothes is
2024-07-16 17:40:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 17:43:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.7920,  1.2246, -0.7417,  ..., -0.0583, -0.6948, -0.1526],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.4609, -1.1074, -3.9023,  ..., -4.8555, -0.0780, -0.7817],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.2469e-03, -1.3596e-02,  7.0305e-03,  ..., -1.8265e-02,
         -1.2230e-02, -1.0399e-02],
        [ 4.8370e-03, -1.5106e-02, -3.6697e-03,  ...,  3.8643e-03,
          6.3171e-03,  2.8496e-03],
        [ 2.3766e-03, -7.4539e-03, -1.1078e-02,  ..., -1.0048e-02,
         -5.3482e-03,  1.3245e-02],
        ...,
        [ 4.5815e-03,  1.1124e-02,  2.7084e-03,  ...,  8.8654e-03,
         -3.8147e-03, -2.9945e-04],
        [ 1.3329e-02, -1.0818e-02, -5.8861e-03,  ...,  3.9711e-03,
         -3.4447e-03,  1.4954e-02],
        [ 2.8687e-03, -1.1444e-05,  4.6463e-03,  ...,  6.8092e-04,
          6.9475e-04,  5.2834e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.0117, -0.7183, -4.0625,  ..., -4.3789, -0.2227, -0.7285]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 17:43:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for portion is part
Another word for mother is mom
Another word for harbor is seaport
Another word for sofa is couch
Another word for mend is repair
Another word for market is marketplace
Another word for new is modern
Another word for clothes is
2024-07-16 17:43:40 root INFO     [order_1_approx] starting weight calculation for Another word for market is marketplace
Another word for harbor is seaport
Another word for portion is part
Another word for clothes is clothing
Another word for new is modern
Another word for mend is repair
Another word for sofa is couch
Another word for mother is
2024-07-16 17:43:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 17:47:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4082,  0.3081,  1.0996,  ..., -0.5806, -0.2383,  0.8394],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.5312, -4.4570, -0.6465,  ..., -3.4492,  5.1719, -4.3125],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.3695e-02, -1.0231e-02, -1.3866e-03,  ...,  3.8223e-03,
         -1.1208e-02,  9.8877e-03],
        [ 3.3188e-03, -2.8019e-03, -4.7913e-03,  ...,  1.2436e-02,
         -1.9531e-03,  1.0941e-02],
        [ 4.8828e-04, -7.1640e-03, -2.2984e-04,  ..., -1.2276e-02,
         -6.6605e-03,  1.1154e-02],
        ...,
        [-2.5253e-03,  9.3079e-04, -1.8902e-03,  ...,  4.9782e-03,
          5.4703e-03, -3.0270e-03],
        [ 1.7548e-02, -1.4366e-02,  4.6844e-03,  ...,  1.2146e-02,
         -1.1444e-03,  1.2398e-02],
        [ 2.6703e-03,  1.5182e-03,  1.7757e-03,  ..., -5.1498e-05,
          1.4153e-03, -4.6463e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.2266, -4.8125,  0.1060,  ..., -4.7109,  5.3125, -4.9844]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 17:47:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for market is marketplace
Another word for harbor is seaport
Another word for portion is part
Another word for clothes is clothing
Another word for new is modern
Another word for mend is repair
Another word for sofa is couch
Another word for mother is
2024-07-16 17:47:07 root INFO     [order_1_approx] starting weight calculation for Another word for clothes is clothing
Another word for new is modern
Another word for mother is mom
Another word for harbor is seaport
Another word for sofa is couch
Another word for mend is repair
Another word for portion is part
Another word for market is
2024-07-16 17:47:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 17:50:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5308,  0.1821,  0.9365,  ...,  0.0391, -0.0789,  0.8374],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.1211, -1.6289, -4.8906,  ..., -0.8179,  1.4160,  4.4492],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0172, -0.0010,  0.0148,  ..., -0.0024, -0.0007, -0.0032],
        [ 0.0077,  0.0021, -0.0100,  ...,  0.0219,  0.0066, -0.0044],
        [-0.0034,  0.0006, -0.0070,  ..., -0.0020,  0.0043,  0.0261],
        ...,
        [ 0.0060,  0.0038,  0.0119,  ...,  0.0124, -0.0116,  0.0010],
        [ 0.0027, -0.0006, -0.0114,  ..., -0.0104, -0.0018, -0.0104],
        [-0.0133, -0.0075,  0.0034,  ...,  0.0060, -0.0059,  0.0175]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.6328, -0.8369, -4.3164,  ..., -1.6777,  1.9639,  3.3438]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 17:50:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for clothes is clothing
Another word for new is modern
Another word for mother is mom
Another word for harbor is seaport
Another word for sofa is couch
Another word for mend is repair
Another word for portion is part
Another word for market is
2024-07-16 17:50:35 root INFO     [order_1_approx] starting weight calculation for Another word for market is marketplace
Another word for portion is part
Another word for sofa is couch
Another word for mother is mom
Another word for mend is repair
Another word for clothes is clothing
Another word for harbor is seaport
Another word for new is
2024-07-16 17:50:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 17:53:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5605,  1.1367, -0.0042,  ..., -1.4199, -0.3784,  0.4917],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.9922, -1.7852, -2.7012,  ...,  0.4485, -0.1851,  1.8008],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.3913e-03, -2.4414e-02,  1.1826e-02,  ..., -6.0310e-03,
         -4.6616e-03,  1.6144e-02],
        [-7.3776e-03, -2.8496e-03, -4.8981e-03,  ...,  1.0803e-02,
          4.9362e-03, -1.0345e-02],
        [-1.3123e-02, -2.1267e-03, -6.3629e-03,  ..., -5.5847e-03,
         -1.0025e-02,  2.2049e-02],
        ...,
        [ 7.9041e-03, -1.5030e-02,  1.2566e-02,  ..., -3.5248e-03,
          6.3934e-03, -3.1700e-03],
        [-8.3256e-04, -8.5602e-03, -1.4811e-03,  ...,  7.2136e-03,
         -1.9470e-02, -5.0507e-03],
        [-2.4414e-04,  1.9417e-03,  6.4850e-05,  ...,  6.6147e-03,
         -8.3160e-03, -8.5144e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.2266, -1.3281, -2.5547,  ...,  0.2499,  0.0214,  1.8271]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 17:54:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for market is marketplace
Another word for portion is part
Another word for sofa is couch
Another word for mother is mom
Another word for mend is repair
Another word for clothes is clothing
Another word for harbor is seaport
Another word for new is
2024-07-16 17:54:00 root INFO     [order_1_approx] starting weight calculation for Another word for mother is mom
Another word for clothes is clothing
Another word for mend is repair
Another word for portion is part
Another word for market is marketplace
Another word for harbor is seaport
Another word for new is modern
Another word for sofa is
2024-07-16 17:54:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 17:57:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2500,  0.4084, -1.1377,  ..., -0.1318, -0.6758,  1.4746],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4463,  2.1426, -0.5767,  ..., -3.0938, -1.9277,  2.2754],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0111, -0.0071,  0.0074,  ..., -0.0020, -0.0067, -0.0038],
        [ 0.0113,  0.0004, -0.0026,  ...,  0.0056,  0.0011,  0.0042],
        [ 0.0008,  0.0072,  0.0031,  ..., -0.0090,  0.0031,  0.0035],
        ...,
        [ 0.0084,  0.0014,  0.0070,  ...,  0.0087,  0.0014, -0.0026],
        [-0.0005,  0.0055, -0.0036,  ..., -0.0077, -0.0005, -0.0131],
        [-0.0029, -0.0014,  0.0089,  ..., -0.0085,  0.0013,  0.0123]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0010,  2.2969,  0.0801,  ..., -2.8945, -0.8926,  2.6523]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 17:57:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for mother is mom
Another word for clothes is clothing
Another word for mend is repair
Another word for portion is part
Another word for market is marketplace
Another word for harbor is seaport
Another word for new is modern
Another word for sofa is
2024-07-16 17:57:27 root INFO     [order_1_approx] starting weight calculation for Another word for mother is mom
Another word for clothes is clothing
Another word for sofa is couch
Another word for harbor is seaport
Another word for portion is part
Another word for new is modern
Another word for market is marketplace
Another word for mend is
2024-07-16 17:57:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 18:00:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1729, -1.1328, -1.8174,  ...,  0.8130,  0.5171, -0.9248],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3296, -1.0059, -4.1250,  ..., -0.7388,  1.8994,  6.3359],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0014, -0.0014,  0.0093,  ..., -0.0010, -0.0180,  0.0137],
        [-0.0056, -0.0005, -0.0079,  ...,  0.0111, -0.0032, -0.0035],
        [ 0.0038,  0.0068, -0.0083,  ..., -0.0145, -0.0084, -0.0047],
        ...,
        [ 0.0083, -0.0118,  0.0067,  ...,  0.0065,  0.0038, -0.0075],
        [-0.0064, -0.0120, -0.0043,  ...,  0.0168, -0.0006,  0.0020],
        [ 0.0054, -0.0192,  0.0116,  ...,  0.0157, -0.0110,  0.0230]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3804, -0.2954, -3.8867,  ..., -0.5869,  2.0938,  7.0273]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 18:00:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for mother is mom
Another word for clothes is clothing
Another word for sofa is couch
Another word for harbor is seaport
Another word for portion is part
Another word for new is modern
Another word for market is marketplace
Another word for mend is
2024-07-16 18:00:55 root INFO     total operator prediction time: 1655.977496623993 seconds
2024-07-16 18:00:55 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on hypernyms - misc
2024-07-16 18:00:55 root INFO     building operator hypernyms - misc
2024-07-16 18:00:55 root INFO     [order_1_approx] starting weight calculation for The plum falls into the category of fruit
The toaster falls into the category of appliance
The computer falls into the category of device
The hairnet falls into the category of net
The sidewalk falls into the category of walk
The cake falls into the category of dessert
The dishwasher falls into the category of appliance
The toothbrush falls into the category of
2024-07-16 18:00:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 18:04:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4336, -1.1113, -0.6060,  ..., -1.1270, -0.8369,  0.4668],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.6465,  1.7080, -1.1172,  ..., -0.4304, -3.7031, -0.0182],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0181, -0.0025, -0.0079,  ..., -0.0027,  0.0061,  0.0008],
        [-0.0025,  0.0086,  0.0095,  ...,  0.0097,  0.0027, -0.0026],
        [-0.0079,  0.0052,  0.0109,  ..., -0.0019, -0.0062,  0.0054],
        ...,
        [-0.0008, -0.0059, -0.0010,  ...,  0.0158, -0.0090,  0.0021],
        [-0.0004,  0.0032, -0.0075,  ..., -0.0018,  0.0038,  0.0126],
        [-0.0060, -0.0044,  0.0044,  ..., -0.0040,  0.0060,  0.0107]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.2852,  1.9160, -1.1572,  ..., -0.3767, -3.9961,  0.2233]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 18:04:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plum falls into the category of fruit
The toaster falls into the category of appliance
The computer falls into the category of device
The hairnet falls into the category of net
The sidewalk falls into the category of walk
The cake falls into the category of dessert
The dishwasher falls into the category of appliance
The toothbrush falls into the category of
2024-07-16 18:04:24 root INFO     [order_1_approx] starting weight calculation for The toaster falls into the category of appliance
The computer falls into the category of device
The dishwasher falls into the category of appliance
The cake falls into the category of dessert
The plum falls into the category of fruit
The toothbrush falls into the category of brush
The sidewalk falls into the category of walk
The hairnet falls into the category of
2024-07-16 18:04:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 18:07:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2104, -2.0332, -0.6904,  ...,  0.7500, -0.9404,  1.0371],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.7090,  1.3633, -3.7891,  ..., -2.6133, -2.0859,  0.9995],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0191,  0.0087, -0.0170,  ...,  0.0014, -0.0030, -0.0031],
        [-0.0062,  0.0106,  0.0028,  ...,  0.0117,  0.0007, -0.0009],
        [-0.0072,  0.0120,  0.0152,  ..., -0.0041, -0.0085,  0.0086],
        ...,
        [-0.0038, -0.0076,  0.0018,  ...,  0.0192, -0.0030,  0.0093],
        [ 0.0061,  0.0057, -0.0033,  ..., -0.0047,  0.0044,  0.0082],
        [ 0.0010, -0.0083, -0.0019,  ..., -0.0044,  0.0108,  0.0216]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.1641,  1.4492, -3.7773,  ..., -2.3301, -2.3418,  1.2471]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 18:07:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The toaster falls into the category of appliance
The computer falls into the category of device
The dishwasher falls into the category of appliance
The cake falls into the category of dessert
The plum falls into the category of fruit
The toothbrush falls into the category of brush
The sidewalk falls into the category of walk
The hairnet falls into the category of
2024-07-16 18:07:53 root INFO     [order_1_approx] starting weight calculation for The plum falls into the category of fruit
The toaster falls into the category of appliance
The hairnet falls into the category of net
The dishwasher falls into the category of appliance
The sidewalk falls into the category of walk
The toothbrush falls into the category of brush
The cake falls into the category of dessert
The computer falls into the category of
2024-07-16 18:07:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 18:11:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1509,  0.2925,  0.1646,  ..., -0.0560,  0.3501,  2.6289],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0645,  0.8540,  0.5488,  ..., -1.3203, -0.7900,  1.2812],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.8219e-02,  3.6240e-03, -1.0780e-02,  ...,  1.1272e-03,
          5.1575e-03,  7.5607e-03],
        [-7.0190e-03,  1.3275e-02,  5.0735e-03,  ...,  1.9073e-06,
         -2.5711e-03, -4.8599e-03],
        [-7.1564e-03,  2.7065e-03, -8.2397e-04,  ...,  1.5030e-03,
         -7.4463e-03,  2.0161e-03],
        ...,
        [-7.9155e-04,  4.4174e-03,  7.6065e-03,  ...,  1.4679e-02,
         -6.3629e-03,  5.7907e-03],
        [ 1.3672e-02, -7.7744e-03,  1.6804e-03,  ...,  3.7212e-03,
          1.1673e-03,  7.2403e-03],
        [-7.4234e-03, -4.4479e-03, -6.9046e-04,  ...,  6.0120e-03,
         -3.4885e-03,  1.1230e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6738,  1.5000,  0.7720,  ..., -1.4590, -0.9487,  1.2100]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 18:11:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plum falls into the category of fruit
The toaster falls into the category of appliance
The hairnet falls into the category of net
The dishwasher falls into the category of appliance
The sidewalk falls into the category of walk
The toothbrush falls into the category of brush
The cake falls into the category of dessert
The computer falls into the category of
2024-07-16 18:11:23 root INFO     [order_1_approx] starting weight calculation for The computer falls into the category of device
The plum falls into the category of fruit
The sidewalk falls into the category of walk
The cake falls into the category of dessert
The dishwasher falls into the category of appliance
The toothbrush falls into the category of brush
The hairnet falls into the category of net
The toaster falls into the category of
2024-07-16 18:11:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 18:14:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1855,  1.2188,  0.5039,  ...,  1.5898,  0.0635,  0.5371],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-5.1562,  3.1270,  2.5625,  ...,  1.7207, -1.3643, -0.9707],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0147, -0.0025, -0.0037,  ..., -0.0060,  0.0087,  0.0023],
        [-0.0061,  0.0145, -0.0080,  ...,  0.0042,  0.0049,  0.0027],
        [-0.0034,  0.0014,  0.0089,  ..., -0.0011,  0.0038, -0.0053],
        ...,
        [-0.0047, -0.0028, -0.0029,  ...,  0.0184, -0.0103,  0.0041],
        [-0.0032, -0.0002,  0.0042,  ..., -0.0024, -0.0008,  0.0042],
        [-0.0106, -0.0057,  0.0014,  ..., -0.0044,  0.0068,  0.0094]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.3438,  2.7695,  2.5527,  ...,  1.8125, -1.4844, -0.4780]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 18:14:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The computer falls into the category of device
The plum falls into the category of fruit
The sidewalk falls into the category of walk
The cake falls into the category of dessert
The dishwasher falls into the category of appliance
The toothbrush falls into the category of brush
The hairnet falls into the category of net
The toaster falls into the category of
2024-07-16 18:14:51 root INFO     [order_1_approx] starting weight calculation for The cake falls into the category of dessert
The hairnet falls into the category of net
The plum falls into the category of fruit
The computer falls into the category of device
The toaster falls into the category of appliance
The sidewalk falls into the category of walk
The toothbrush falls into the category of brush
The dishwasher falls into the category of
2024-07-16 18:14:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 18:18:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6914,  0.2407,  1.2471,  ...,  0.9326,  0.0493, -0.4363],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.8477, -1.0137, -0.4561,  ...,  0.3584, -1.0273,  3.1055],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0142,  0.0052, -0.0151,  ..., -0.0019,  0.0058,  0.0013],
        [-0.0029,  0.0097,  0.0047,  ...,  0.0044,  0.0034, -0.0031],
        [-0.0018,  0.0041,  0.0070,  ...,  0.0037, -0.0104,  0.0018],
        ...,
        [-0.0053, -0.0015,  0.0069,  ...,  0.0132, -0.0149,  0.0099],
        [ 0.0013, -0.0034,  0.0029,  ...,  0.0017,  0.0067,  0.0090],
        [-0.0059, -0.0044, -0.0017,  ..., -0.0051,  0.0054,  0.0095]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.5586, -0.6177, -0.6812,  ...,  0.4263, -0.7373,  3.3125]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 18:18:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The cake falls into the category of dessert
The hairnet falls into the category of net
The plum falls into the category of fruit
The computer falls into the category of device
The toaster falls into the category of appliance
The sidewalk falls into the category of walk
The toothbrush falls into the category of brush
The dishwasher falls into the category of
2024-07-16 18:18:19 root INFO     [order_1_approx] starting weight calculation for The computer falls into the category of device
The toaster falls into the category of appliance
The hairnet falls into the category of net
The sidewalk falls into the category of walk
The dishwasher falls into the category of appliance
The toothbrush falls into the category of brush
The plum falls into the category of fruit
The cake falls into the category of
2024-07-16 18:18:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 18:21:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.5244,  0.7515, -0.7021,  ..., -0.3813, -0.8784,  0.8838],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.0957,  0.9487, -1.6836,  ..., -0.4993,  0.2717,  0.6230],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0052, -0.0073, -0.0026,  ..., -0.0002,  0.0015,  0.0004],
        [-0.0026,  0.0099, -0.0017,  ...,  0.0040,  0.0072,  0.0031],
        [-0.0033,  0.0046, -0.0030,  ...,  0.0031, -0.0009, -0.0040],
        ...,
        [ 0.0006, -0.0037,  0.0094,  ...,  0.0077, -0.0060,  0.0060],
        [-0.0042,  0.0036,  0.0010,  ...,  0.0036,  0.0002,  0.0071],
        [-0.0108, -0.0038,  0.0046,  ...,  0.0051, -0.0025,  0.0109]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.8008,  0.9292, -2.0703,  ..., -0.5459,  0.2505,  0.8564]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 18:21:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The computer falls into the category of device
The toaster falls into the category of appliance
The hairnet falls into the category of net
The sidewalk falls into the category of walk
The dishwasher falls into the category of appliance
The toothbrush falls into the category of brush
The plum falls into the category of fruit
The cake falls into the category of
2024-07-16 18:21:48 root INFO     [order_1_approx] starting weight calculation for The toaster falls into the category of appliance
The cake falls into the category of dessert
The computer falls into the category of device
The hairnet falls into the category of net
The dishwasher falls into the category of appliance
The plum falls into the category of fruit
The toothbrush falls into the category of brush
The sidewalk falls into the category of
2024-07-16 18:21:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 18:25:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3481, -0.2441, -1.2080,  ...,  0.4871,  0.3569, -0.8213],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6738, -3.7969,  0.0659,  ..., -0.6206,  0.8848,  3.5332],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.0674e-02, -7.9498e-03, -8.7509e-03,  ..., -7.8087e-03,
         -2.3918e-03,  9.9716e-03],
        [-2.2926e-03,  1.5106e-02,  1.9398e-03,  ...,  4.5395e-03,
          4.2877e-03,  6.4015e-05],
        [ 2.2240e-03, -9.1314e-05,  7.3624e-04,  ..., -5.8365e-04,
         -1.7920e-03, -1.6766e-03],
        ...,
        [ 8.5068e-04, -5.8365e-03,  4.4060e-04,  ...,  4.3907e-03,
          6.7091e-04,  1.2619e-02],
        [-5.6362e-04, -1.9038e-04,  1.0796e-02,  ...,  6.6109e-03,
          7.1335e-04, -5.1689e-04],
        [-2.0714e-03, -7.1564e-03, -6.0081e-05,  ...,  8.7585e-03,
         -1.0548e-03,  1.0017e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7554, -3.4141, -0.3381,  ..., -0.6074,  0.4861,  3.4219]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 18:25:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The toaster falls into the category of appliance
The cake falls into the category of dessert
The computer falls into the category of device
The hairnet falls into the category of net
The dishwasher falls into the category of appliance
The plum falls into the category of fruit
The toothbrush falls into the category of brush
The sidewalk falls into the category of
2024-07-16 18:25:17 root INFO     [order_1_approx] starting weight calculation for The hairnet falls into the category of net
The computer falls into the category of device
The sidewalk falls into the category of walk
The toothbrush falls into the category of brush
The toaster falls into the category of appliance
The dishwasher falls into the category of appliance
The cake falls into the category of dessert
The plum falls into the category of
2024-07-16 18:25:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 18:28:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1565,  0.9414, -0.4084,  ..., -1.2646, -3.0840,  0.5835],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7441, -1.6211, -3.8242,  ...,  0.0576, -2.1719,  0.6475],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0130, -0.0080, -0.0005,  ..., -0.0057,  0.0003,  0.0017],
        [ 0.0019,  0.0128,  0.0042,  ...,  0.0028, -0.0013, -0.0012],
        [-0.0009,  0.0139,  0.0077,  ...,  0.0010, -0.0058, -0.0019],
        ...,
        [-0.0007, -0.0026,  0.0060,  ...,  0.0125, -0.0059,  0.0018],
        [ 0.0077, -0.0045, -0.0022,  ..., -0.0049,  0.0059,  0.0020],
        [-0.0044,  0.0078, -0.0010,  ...,  0.0085, -0.0010,  0.0064]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0020, -1.0078, -3.9121,  ...,  0.2576, -2.3848,  0.7456]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 18:28:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The hairnet falls into the category of net
The computer falls into the category of device
The sidewalk falls into the category of walk
The toothbrush falls into the category of brush
The toaster falls into the category of appliance
The dishwasher falls into the category of appliance
The cake falls into the category of dessert
The plum falls into the category of
2024-07-16 18:28:46 root INFO     total operator prediction time: 1671.0254788398743 seconds
2024-07-16 18:28:46 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - substance
2024-07-16 18:28:46 root INFO     building operator meronyms - substance
2024-07-16 18:28:46 root INFO     [order_1_approx] starting weight calculation for A doorknob is made up of metal
A omelette is made up of eggs
A money is made up of paper
A bag is made up of leather
A ocean is made up of water
A diamond is made up of carbon
A spoon is made up of aluminium
A bottle is made up of
2024-07-16 18:28:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 18:32:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4729, -0.9258,  1.3867,  ...,  0.1504, -2.0859,  0.6528],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.6445, -0.0232,  2.1406,  ...,  2.3750, -0.7944, -0.3391],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0096, -0.0053,  0.0065,  ...,  0.0012, -0.0044,  0.0010],
        [-0.0041,  0.0033, -0.0013,  ..., -0.0006,  0.0027,  0.0007],
        [ 0.0027, -0.0041,  0.0058,  ..., -0.0070,  0.0044,  0.0007],
        ...,
        [ 0.0018, -0.0052,  0.0011,  ...,  0.0079, -0.0057,  0.0076],
        [ 0.0017, -0.0033, -0.0022,  ...,  0.0004,  0.0002,  0.0043],
        [-0.0034, -0.0040,  0.0043,  ..., -0.0024,  0.0003,  0.0076]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.3750,  0.1910,  2.2285,  ...,  2.2617, -0.8818, -0.5610]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 18:32:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A doorknob is made up of metal
A omelette is made up of eggs
A money is made up of paper
A bag is made up of leather
A ocean is made up of water
A diamond is made up of carbon
A spoon is made up of aluminium
A bottle is made up of
2024-07-16 18:32:14 root INFO     [order_1_approx] starting weight calculation for A bag is made up of leather
A bottle is made up of glass
A ocean is made up of water
A doorknob is made up of metal
A spoon is made up of aluminium
A omelette is made up of eggs
A diamond is made up of carbon
A money is made up of
2024-07-16 18:32:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 18:35:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1288,  0.2194,  0.0071,  ...,  0.7222, -0.4790,  0.4495],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8604, -0.5981,  1.0088,  ..., -0.5537, -0.4746, -0.1467],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0012, -0.0272,  0.0048,  ..., -0.0006, -0.0008, -0.0028],
        [-0.0125, -0.0181,  0.0036,  ...,  0.0139, -0.0041, -0.0031],
        [-0.0011, -0.0140, -0.0043,  ...,  0.0019, -0.0014,  0.0068],
        ...,
        [ 0.0038,  0.0212, -0.0023,  ..., -0.0111, -0.0090,  0.0045],
        [ 0.0005, -0.0091, -0.0009,  ...,  0.0111, -0.0122,  0.0099],
        [-0.0073,  0.0027, -0.0003,  ...,  0.0042,  0.0024, -0.0019]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9683, -0.2742,  1.6660,  ..., -0.8276, -0.4072,  0.3064]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 18:35:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A bag is made up of leather
A bottle is made up of glass
A ocean is made up of water
A doorknob is made up of metal
A spoon is made up of aluminium
A omelette is made up of eggs
A diamond is made up of carbon
A money is made up of
2024-07-16 18:35:42 root INFO     [order_1_approx] starting weight calculation for A spoon is made up of aluminium
A omelette is made up of eggs
A bag is made up of leather
A money is made up of paper
A bottle is made up of glass
A ocean is made up of water
A doorknob is made up of metal
A diamond is made up of
2024-07-16 18:35:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 18:39:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.6465,  0.2627,  0.0072,  ..., -1.9473, -0.6855, -0.4353],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6240,  1.1416,  5.1367,  ...,  0.6177, -2.1074,  1.7100],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 8.6212e-03, -4.8370e-03,  1.7738e-04,  ..., -7.5912e-04,
         -1.5478e-03,  4.8943e-03],
        [ 1.4305e-06,  3.7346e-03, -1.8520e-03,  ...,  2.6817e-03,
          5.3253e-03, -8.1940e-03],
        [ 5.3444e-03, -1.9493e-03, -7.1106e-03,  ...,  2.2964e-03,
         -4.9210e-04, -4.8218e-03],
        ...,
        [ 7.7057e-03, -2.4033e-03,  2.3689e-03,  ..., -4.3755e-03,
         -8.1329e-03,  4.9820e-03],
        [ 5.6610e-03, -4.1122e-03, -4.1656e-03,  ...,  2.0714e-03,
         -3.2635e-03, -6.7482e-03],
        [-2.7895e-04,  1.6260e-03, -3.2578e-03,  ...,  7.2861e-03,
          7.6447e-03,  2.4338e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2012,  0.6133,  5.1758,  ...,  0.5713, -1.2520,  1.3164]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 18:39:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A spoon is made up of aluminium
A omelette is made up of eggs
A bag is made up of leather
A money is made up of paper
A bottle is made up of glass
A ocean is made up of water
A doorknob is made up of metal
A diamond is made up of
2024-07-16 18:39:12 root INFO     [order_1_approx] starting weight calculation for A bottle is made up of glass
A money is made up of paper
A omelette is made up of eggs
A doorknob is made up of metal
A ocean is made up of water
A diamond is made up of carbon
A bag is made up of leather
A spoon is made up of
2024-07-16 18:39:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 18:42:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8379,  0.2566, -0.1484,  ...,  1.2676, -1.5303,  0.3916],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-5.1250,  0.7305, -0.1318,  ...,  2.5371,  2.9688,  1.5986],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 5.8746e-03, -5.7793e-03,  9.0265e-04,  ..., -7.1373e-03,
         -1.1253e-03, -7.2098e-04],
        [-5.2147e-03,  9.5673e-03,  1.0338e-03,  ...,  5.7449e-03,
          3.7212e-03,  5.9891e-03],
        [ 3.8128e-03, -8.5983e-03,  2.8458e-03,  ..., -9.3365e-04,
         -4.8256e-03,  1.0300e-03],
        ...,
        [ 1.5259e-05, -1.4687e-03,  2.0943e-03,  ...,  8.6670e-03,
         -1.0376e-02,  4.0627e-03],
        [ 6.3705e-04,  6.3419e-04,  5.2757e-03,  ...,  5.1117e-04,
          2.6398e-03, -3.6774e-03],
        [-6.1073e-03,  2.6665e-03,  6.2675e-03,  ...,  6.1569e-03,
          7.5531e-04,  7.3586e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.9648,  0.7271,  0.3848,  ...,  2.6992,  2.8867,  1.7471]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 18:42:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A bottle is made up of glass
A money is made up of paper
A omelette is made up of eggs
A doorknob is made up of metal
A ocean is made up of water
A diamond is made up of carbon
A bag is made up of leather
A spoon is made up of
2024-07-16 18:42:40 root INFO     [order_1_approx] starting weight calculation for A money is made up of paper
A ocean is made up of water
A diamond is made up of carbon
A bottle is made up of glass
A bag is made up of leather
A spoon is made up of aluminium
A doorknob is made up of metal
A omelette is made up of
2024-07-16 18:42:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 18:46:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5732,  0.1466,  1.5732,  ...,  0.1958,  1.3086,  0.9277],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.5273, -1.8301, -2.1309,  ...,  1.2881, -2.5566, -4.4102],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0052,  0.0051,  0.0053,  ...,  0.0022, -0.0064, -0.0067],
        [ 0.0009, -0.0005,  0.0023,  ...,  0.0063,  0.0017,  0.0036],
        [ 0.0094, -0.0112,  0.0076,  ..., -0.0051, -0.0059,  0.0090],
        ...,
        [ 0.0050, -0.0027, -0.0023,  ...,  0.0078, -0.0019,  0.0043],
        [ 0.0061, -0.0028,  0.0023,  ...,  0.0003,  0.0083,  0.0092],
        [ 0.0077, -0.0022,  0.0030,  ..., -0.0067,  0.0029,  0.0088]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.9961, -1.6279, -1.7324,  ...,  1.7559, -2.8887, -4.0742]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 18:46:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A money is made up of paper
A ocean is made up of water
A diamond is made up of carbon
A bottle is made up of glass
A bag is made up of leather
A spoon is made up of aluminium
A doorknob is made up of metal
A omelette is made up of
2024-07-16 18:46:08 root INFO     [order_1_approx] starting weight calculation for A spoon is made up of aluminium
A bottle is made up of glass
A bag is made up of leather
A omelette is made up of eggs
A money is made up of paper
A diamond is made up of carbon
A doorknob is made up of metal
A ocean is made up of
2024-07-16 18:46:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 18:49:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5830, -1.4258,  0.4426,  ..., -3.2324,  0.1567,  0.9575],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7197,  1.8418,  1.8926,  ...,  4.8242, -1.4463, -2.2090],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0012, -0.0108,  0.0057,  ...,  0.0052, -0.0051, -0.0062],
        [-0.0090, -0.0134,  0.0015,  ...,  0.0125,  0.0027, -0.0079],
        [ 0.0006, -0.0035, -0.0168,  ..., -0.0030,  0.0030,  0.0044],
        ...,
        [-0.0066, -0.0005,  0.0084,  ..., -0.0109, -0.0083, -0.0087],
        [ 0.0025, -0.0016,  0.0022,  ...,  0.0051, -0.0059,  0.0011],
        [-0.0054,  0.0084,  0.0056,  ..., -0.0058, -0.0046, -0.0068]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.2617,  1.8604,  2.1035,  ...,  5.0586, -1.2998, -2.1465]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 18:49:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A spoon is made up of aluminium
A bottle is made up of glass
A bag is made up of leather
A omelette is made up of eggs
A money is made up of paper
A diamond is made up of carbon
A doorknob is made up of metal
A ocean is made up of
2024-07-16 18:49:37 root INFO     [order_1_approx] starting weight calculation for A spoon is made up of aluminium
A bag is made up of leather
A bottle is made up of glass
A money is made up of paper
A ocean is made up of water
A diamond is made up of carbon
A omelette is made up of eggs
A doorknob is made up of
2024-07-16 18:49:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 18:53:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.9219,  1.3242, -0.2891,  ..., -0.5088, -0.9912,  0.4329],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1504, -0.2910,  2.5059,  ..., -0.9336,  0.6304,  4.9258],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0097, -0.0050, -0.0025,  ..., -0.0050, -0.0127, -0.0070],
        [ 0.0011,  0.0089,  0.0027,  ...,  0.0094,  0.0142,  0.0042],
        [ 0.0043, -0.0048,  0.0096,  ...,  0.0040, -0.0032,  0.0028],
        ...,
        [-0.0039,  0.0029, -0.0069,  ...,  0.0045, -0.0109,  0.0057],
        [ 0.0049, -0.0070,  0.0034,  ..., -0.0019,  0.0101,  0.0056],
        [ 0.0006, -0.0030, -0.0012,  ...,  0.0009,  0.0131,  0.0115]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0371, -0.6797,  2.4531,  ..., -0.5459,  0.6914,  4.4570]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 18:53:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A spoon is made up of aluminium
A bag is made up of leather
A bottle is made up of glass
A money is made up of paper
A ocean is made up of water
A diamond is made up of carbon
A omelette is made up of eggs
A doorknob is made up of
2024-07-16 18:53:05 root INFO     [order_1_approx] starting weight calculation for A ocean is made up of water
A omelette is made up of eggs
A spoon is made up of aluminium
A doorknob is made up of metal
A bottle is made up of glass
A diamond is made up of carbon
A money is made up of paper
A bag is made up of
2024-07-16 18:53:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 18:56:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3516, -1.2012,  0.3655,  ...,  0.2192, -1.4268,  0.0676],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6689,  0.1262,  0.1416,  ...,  4.6172, -1.7148, -3.3320],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 8.9188e-03, -5.0430e-03,  5.8842e-04,  ...,  2.6016e-03,
         -1.0338e-03,  5.4207e-03],
        [-7.9956e-03,  3.7003e-04, -6.1874e-03,  ...,  4.5395e-04,
          1.0201e-02, -4.9210e-03],
        [-6.3400e-03, -5.8365e-03, -2.2697e-04,  ..., -5.6267e-04,
         -6.0310e-03,  8.1635e-03],
        ...,
        [ 5.5885e-04, -1.1330e-03,  6.8016e-03,  ...,  6.6071e-03,
         -1.3435e-02,  7.0572e-03],
        [ 7.8278e-03, -3.1223e-03,  2.1248e-03,  ..., -1.9360e-04,
          2.3022e-03, -1.8711e-03],
        [-5.6190e-03, -1.2642e-02,  4.9362e-03,  ..., -7.1564e-03,
         -4.2915e-05,  1.1612e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6602,  0.2405, -0.2986,  ...,  4.1758, -2.0820, -3.3242]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 18:56:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A ocean is made up of water
A omelette is made up of eggs
A spoon is made up of aluminium
A doorknob is made up of metal
A bottle is made up of glass
A diamond is made up of carbon
A money is made up of paper
A bag is made up of
2024-07-16 18:56:32 root INFO     total operator prediction time: 1666.396147966385 seconds
2024-07-16 18:56:32 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on synonyms - intensity
2024-07-16 18:56:32 root INFO     building operator synonyms - intensity
2024-07-16 18:56:32 root INFO     [order_1_approx] starting weight calculation for A more intense word for afraid is terrified
A more intense word for pony is horse
A more intense word for boring is tedious
A more intense word for want is crave
A more intense word for creative is ingenious
A more intense word for faith is fanatism
A more intense word for opposed is averse
A more intense word for guilty is
2024-07-16 18:56:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 18:59:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8916,  1.5518,  0.2805,  ..., -0.3662, -0.2236,  0.2778],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9131,  1.1387, -5.7891,  ...,  0.2236, -2.5645, -1.0156],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0021, -0.0017,  0.0065,  ..., -0.0095, -0.0016, -0.0112],
        [ 0.0066,  0.0006, -0.0146,  ...,  0.0067,  0.0224, -0.0083],
        [ 0.0099, -0.0027,  0.0057,  ..., -0.0100, -0.0140,  0.0168],
        ...,
        [ 0.0147, -0.0130,  0.0184,  ...,  0.0103, -0.0036,  0.0090],
        [ 0.0004,  0.0035,  0.0116,  ..., -0.0044, -0.0010, -0.0079],
        [ 0.0045, -0.0028,  0.0069,  ...,  0.0057, -0.0085,  0.0177]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5210,  1.5908, -6.9805,  ...,  0.7319, -2.2461, -1.0146]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 19:00:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for afraid is terrified
A more intense word for pony is horse
A more intense word for boring is tedious
A more intense word for want is crave
A more intense word for creative is ingenious
A more intense word for faith is fanatism
A more intense word for opposed is averse
A more intense word for guilty is
2024-07-16 19:00:00 root INFO     [order_1_approx] starting weight calculation for A more intense word for faith is fanatism
A more intense word for want is crave
A more intense word for afraid is terrified
A more intense word for opposed is averse
A more intense word for guilty is remorseful
A more intense word for creative is ingenious
A more intense word for boring is tedious
A more intense word for pony is
2024-07-16 19:00:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 19:03:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5913, -0.0210, -0.8008,  ..., -0.5513, -1.1152,  1.3770],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.2939, -1.0947, -1.9268,  ..., -2.8086,  2.8691,  2.3633],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0054,  0.0042, -0.0032,  ...,  0.0087, -0.0006, -0.0188],
        [-0.0063,  0.0100,  0.0077,  ...,  0.0139, -0.0011, -0.0083],
        [-0.0068, -0.0024,  0.0033,  ..., -0.0180, -0.0039,  0.0119],
        ...,
        [ 0.0048, -0.0158,  0.0109,  ...,  0.0128, -0.0051,  0.0075],
        [ 0.0038, -0.0184, -0.0033,  ...,  0.0147,  0.0126,  0.0080],
        [-0.0125,  0.0040,  0.0115,  ...,  0.0162,  0.0054,  0.0067]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.5977,  0.1855, -1.1211,  ..., -0.8096,  2.3965,  1.7266]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 19:03:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for faith is fanatism
A more intense word for want is crave
A more intense word for afraid is terrified
A more intense word for opposed is averse
A more intense word for guilty is remorseful
A more intense word for creative is ingenious
A more intense word for boring is tedious
A more intense word for pony is
2024-07-16 19:03:28 root INFO     [order_1_approx] starting weight calculation for A more intense word for guilty is remorseful
A more intense word for pony is horse
A more intense word for boring is tedious
A more intense word for afraid is terrified
A more intense word for want is crave
A more intense word for faith is fanatism
A more intense word for creative is ingenious
A more intense word for opposed is
2024-07-16 19:03:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 19:06:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.0234, -0.8218,  2.1758,  ..., -0.6611,  1.3633,  0.6309],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.6855, -2.3574, -2.8828,  ...,  3.3457,  9.7031,  2.7578],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.6283e-03, -3.0136e-04,  7.5531e-03,  ..., -5.4703e-03,
         -5.2452e-03, -7.7095e-03],
        [ 2.7885e-03, -3.8643e-03, -5.8174e-03,  ...,  7.9269e-03,
          7.1220e-03, -5.4932e-04],
        [-1.2749e-02,  1.1673e-03, -4.0207e-03,  ...,  3.0823e-03,
         -6.2103e-03, -1.5411e-03],
        ...,
        [-3.9444e-03, -1.5259e-05,  2.4414e-02,  ...,  9.0942e-03,
         -1.1234e-03,  4.6844e-03],
        [-7.0572e-04, -2.1637e-02,  6.4850e-03,  ..., -5.9414e-04,
          1.0090e-03, -3.2806e-03],
        [-1.2955e-02, -3.2101e-03,  2.7695e-03,  ..., -7.3395e-03,
         -9.4414e-04,  1.9646e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.1797, -1.8457, -2.1973,  ...,  4.3828,  8.9375,  2.8027]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 19:06:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for guilty is remorseful
A more intense word for pony is horse
A more intense word for boring is tedious
A more intense word for afraid is terrified
A more intense word for want is crave
A more intense word for faith is fanatism
A more intense word for creative is ingenious
A more intense word for opposed is
2024-07-16 19:06:56 root INFO     [order_1_approx] starting weight calculation for A more intense word for boring is tedious
A more intense word for creative is ingenious
A more intense word for guilty is remorseful
A more intense word for faith is fanatism
A more intense word for pony is horse
A more intense word for opposed is averse
A more intense word for want is crave
A more intense word for afraid is
2024-07-16 19:06:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 19:10:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.0397, 0.2192, 0.2588,  ..., 0.7061, 0.1826, 1.0498], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3125,  2.4102, -2.0898,  ...,  2.3574,  3.8301, -2.1992],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.0544e-02, -3.4332e-05,  2.2964e-03,  ..., -7.3051e-04,
         -8.5144e-03, -5.6686e-03],
        [ 2.6779e-03, -3.7766e-03, -2.8076e-03,  ...,  6.6299e-03,
          5.2185e-03, -8.2321e-03],
        [ 5.3253e-03, -2.0065e-03, -2.5444e-03,  ..., -1.6785e-03,
          3.4561e-03,  1.6464e-02],
        ...,
        [-2.3079e-03, -9.4147e-03,  1.9333e-02,  ..., -2.6112e-03,
          2.6321e-03,  1.4481e-02],
        [ 6.5422e-04, -3.2425e-04, -1.0010e-02,  ...,  5.5046e-03,
         -5.9032e-04, -5.8899e-03],
        [-3.2158e-03, -4.8485e-03, -3.0975e-03,  ..., -5.0888e-03,
         -4.0359e-03, -4.3259e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2683,  2.9258, -2.9160,  ...,  2.4727,  3.2480, -2.8496]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 19:10:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for boring is tedious
A more intense word for creative is ingenious
A more intense word for guilty is remorseful
A more intense word for faith is fanatism
A more intense word for pony is horse
A more intense word for opposed is averse
A more intense word for want is crave
A more intense word for afraid is
2024-07-16 19:10:26 root INFO     [order_1_approx] starting weight calculation for A more intense word for guilty is remorseful
A more intense word for afraid is terrified
A more intense word for opposed is averse
A more intense word for faith is fanatism
A more intense word for pony is horse
A more intense word for want is crave
A more intense word for boring is tedious
A more intense word for creative is
2024-07-16 19:10:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 19:13:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.9980, -0.7441,  1.2852,  ..., -0.2769,  0.3735,  1.3555],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.0781, -4.9023, -3.5625,  ..., -0.7803,  2.2090,  0.0386],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0131, -0.0061,  0.0069,  ..., -0.0033, -0.0065, -0.0045],
        [ 0.0015, -0.0086,  0.0012,  ...,  0.0044,  0.0017, -0.0032],
        [ 0.0027, -0.0032, -0.0106,  ..., -0.0009, -0.0159, -0.0025],
        ...,
        [-0.0058, -0.0066,  0.0174,  ...,  0.0076,  0.0110, -0.0132],
        [ 0.0099,  0.0027,  0.0039,  ...,  0.0174, -0.0019, -0.0001],
        [-0.0078, -0.0043,  0.0061,  ...,  0.0030, -0.0051,  0.0108]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.3145, -4.0430, -2.5234,  ...,  0.1973,  1.8418,  0.2876]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 19:13:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for guilty is remorseful
A more intense word for afraid is terrified
A more intense word for opposed is averse
A more intense word for faith is fanatism
A more intense word for pony is horse
A more intense word for want is crave
A more intense word for boring is tedious
A more intense word for creative is
2024-07-16 19:13:55 root INFO     [order_1_approx] starting weight calculation for A more intense word for boring is tedious
A more intense word for afraid is terrified
A more intense word for want is crave
A more intense word for creative is ingenious
A more intense word for opposed is averse
A more intense word for pony is horse
A more intense word for guilty is remorseful
A more intense word for faith is
2024-07-16 19:13:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 19:17:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.8057, -1.4297,  1.0791,  ...,  0.2842, -0.2778,  0.9897],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 6.0234,  0.2422, -2.1621,  ..., -0.7266,  1.7188,  0.4019],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0029, -0.0088, -0.0014,  ...,  0.0022, -0.0028, -0.0056],
        [ 0.0048, -0.0046, -0.0024,  ...,  0.0045, -0.0011, -0.0105],
        [-0.0028, -0.0103, -0.0130,  ..., -0.0071,  0.0043, -0.0082],
        ...,
        [ 0.0006, -0.0062,  0.0105,  ...,  0.0102, -0.0044,  0.0025],
        [ 0.0027, -0.0009, -0.0029,  ...,  0.0105, -0.0070, -0.0139],
        [-0.0080, -0.0034,  0.0020,  ..., -0.0002, -0.0095, -0.0036]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.9883,  0.9214, -2.1602,  ..., -1.2715,  2.2910,  1.0508]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 19:17:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for boring is tedious
A more intense word for afraid is terrified
A more intense word for want is crave
A more intense word for creative is ingenious
A more intense word for opposed is averse
A more intense word for pony is horse
A more intense word for guilty is remorseful
A more intense word for faith is
2024-07-16 19:17:23 root INFO     [order_1_approx] starting weight calculation for A more intense word for creative is ingenious
A more intense word for opposed is averse
A more intense word for guilty is remorseful
A more intense word for want is crave
A more intense word for afraid is terrified
A more intense word for pony is horse
A more intense word for faith is fanatism
A more intense word for boring is
2024-07-16 19:17:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 19:20:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4590,  0.9351,  0.2310,  ..., -1.2188,  0.5913,  1.1641],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6494,  1.2480, -2.1953,  ..., -1.3809,  1.5547,  2.6719],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.4391e-03, -5.4321e-03, -2.7351e-03,  ...,  8.2397e-03,
         -7.3776e-03, -6.4430e-03],
        [ 1.1070e-02, -5.1498e-05, -5.6686e-03,  ...,  7.9956e-03,
          4.0474e-03, -8.6060e-03],
        [-6.5269e-03, -1.1185e-02,  1.4057e-03,  ..., -6.7902e-03,
          8.2932e-03, -3.5820e-03],
        ...,
        [ 8.1024e-03, -9.0561e-03,  1.7357e-03,  ...,  1.7715e-02,
         -3.5954e-03, -8.2092e-03],
        [ 5.7716e-03,  1.8806e-03,  1.4362e-03,  ...,  6.8512e-03,
         -1.1276e-02, -8.1444e-04],
        [ 2.2659e-03, -1.2230e-02,  5.1346e-03,  ...,  1.0063e-02,
         -6.1760e-03,  9.5444e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7891,  1.5332, -2.3711,  ..., -1.2363,  1.7441,  2.6719]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 19:20:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for creative is ingenious
A more intense word for opposed is averse
A more intense word for guilty is remorseful
A more intense word for want is crave
A more intense word for afraid is terrified
A more intense word for pony is horse
A more intense word for faith is fanatism
A more intense word for boring is
2024-07-16 19:20:51 root INFO     [order_1_approx] starting weight calculation for A more intense word for faith is fanatism
A more intense word for creative is ingenious
A more intense word for pony is horse
A more intense word for boring is tedious
A more intense word for opposed is averse
A more intense word for guilty is remorseful
A more intense word for afraid is terrified
A more intense word for want is
2024-07-16 19:20:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 19:24:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.6445, -0.0996, -0.1393,  ..., -0.7695,  0.6655,  0.9526],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8672,  1.3633, -5.9102,  ..., -1.2578,  2.7422, -0.7695],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0059, -0.0122,  0.0043,  ..., -0.0043, -0.0042,  0.0013],
        [-0.0018, -0.0119,  0.0073,  ...,  0.0039, -0.0035,  0.0031],
        [-0.0076,  0.0085, -0.0090,  ...,  0.0022, -0.0193, -0.0066],
        ...,
        [ 0.0110,  0.0024,  0.0179,  ...,  0.0068, -0.0001,  0.0167],
        [ 0.0117, -0.0096,  0.0099,  ..., -0.0016,  0.0159, -0.0080],
        [-0.0025, -0.0105,  0.0229,  ...,  0.0028,  0.0107,  0.0110]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3840,  1.8223, -5.6562,  ..., -1.0391,  3.1523, -0.5312]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 19:24:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for faith is fanatism
A more intense word for creative is ingenious
A more intense word for pony is horse
A more intense word for boring is tedious
A more intense word for opposed is averse
A more intense word for guilty is remorseful
A more intense word for afraid is terrified
A more intense word for want is
2024-07-16 19:24:19 root INFO     total operator prediction time: 1666.7202231884003 seconds
2024-07-16 19:24:19 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on hypernyms - animals
2024-07-16 19:24:19 root INFO     building operator hypernyms - animals
2024-07-16 19:24:19 root INFO     [order_1_approx] starting weight calculation for The lion falls into the category of feline
The falcon falls into the category of raptor
The cat falls into the category of feline
The chinchilla falls into the category of rodent
The gorilla falls into the category of primate
The anaconda falls into the category of snake
The rattlesnake falls into the category of snake
The ant falls into the category of
2024-07-16 19:24:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 19:27:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5098,  0.3071, -0.3936,  ..., -1.0303, -0.8672,  0.6357],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.8633, -0.0376, -1.1445,  ..., -0.6162, -0.3203,  2.6152],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0134, -0.0021, -0.0022,  ...,  0.0069, -0.0030, -0.0101],
        [ 0.0026,  0.0046,  0.0041,  ...,  0.0092, -0.0042, -0.0098],
        [ 0.0039, -0.0009, -0.0002,  ..., -0.0027, -0.0019,  0.0063],
        ...,
        [-0.0032, -0.0037,  0.0034,  ...,  0.0158, -0.0058, -0.0007],
        [-0.0009,  0.0039,  0.0045,  ...,  0.0067,  0.0014, -0.0111],
        [-0.0059, -0.0040,  0.0011,  ...,  0.0021, -0.0067,  0.0144]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.8906, -0.0588, -1.2402,  ..., -0.5972, -0.4766,  3.0039]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 19:27:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The lion falls into the category of feline
The falcon falls into the category of raptor
The cat falls into the category of feline
The chinchilla falls into the category of rodent
The gorilla falls into the category of primate
The anaconda falls into the category of snake
The rattlesnake falls into the category of snake
The ant falls into the category of
2024-07-16 19:27:47 root INFO     [order_1_approx] starting weight calculation for The rattlesnake falls into the category of snake
The lion falls into the category of feline
The falcon falls into the category of raptor
The ant falls into the category of insect
The gorilla falls into the category of primate
The anaconda falls into the category of snake
The cat falls into the category of feline
The chinchilla falls into the category of
2024-07-16 19:27:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 19:31:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9600, -1.0889, -1.4736,  ...,  0.4404, -1.1738, -0.0266],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.0938, -0.4675, -0.0342,  ..., -1.1055,  0.1255,  1.2705],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0125, -0.0029,  0.0065,  ..., -0.0043, -0.0049, -0.0043],
        [-0.0042,  0.0091,  0.0105,  ...,  0.0226,  0.0035, -0.0061],
        [ 0.0014, -0.0069,  0.0188,  ..., -0.0054, -0.0003,  0.0104],
        ...,
        [-0.0056, -0.0078, -0.0097,  ...,  0.0197, -0.0036,  0.0172],
        [-0.0049, -0.0052, -0.0034,  ..., -0.0071,  0.0152,  0.0092],
        [ 0.0011,  0.0029,  0.0028,  ...,  0.0049, -0.0007,  0.0035]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.0898,  0.1418,  0.1620,  ..., -1.3906,  0.0701,  1.0889]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 19:31:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The rattlesnake falls into the category of snake
The lion falls into the category of feline
The falcon falls into the category of raptor
The ant falls into the category of insect
The gorilla falls into the category of primate
The anaconda falls into the category of snake
The cat falls into the category of feline
The chinchilla falls into the category of
2024-07-16 19:31:15 root INFO     [order_1_approx] starting weight calculation for The gorilla falls into the category of primate
The chinchilla falls into the category of rodent
The ant falls into the category of insect
The lion falls into the category of feline
The anaconda falls into the category of snake
The rattlesnake falls into the category of snake
The falcon falls into the category of raptor
The cat falls into the category of
2024-07-16 19:31:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 19:34:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3992,  1.2676,  1.3652,  ..., -2.4805,  0.2178, -0.8442],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1436,  0.2203,  2.5898,  ..., -1.0781, -4.0469,  1.1523],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 5.1842e-03, -3.1090e-04, -5.9738e-03,  ...,  1.2789e-03,
          6.1035e-04, -7.4387e-05],
        [ 8.0795e-03,  1.3933e-03,  5.4741e-03,  ...,  7.8735e-03,
         -1.6136e-03,  4.3869e-04],
        [ 3.4714e-04, -3.7670e-03,  4.3488e-04,  ..., -2.8648e-03,
         -1.6880e-03, -1.0595e-03],
        ...,
        [ 9.2697e-04, -2.0714e-03,  2.0695e-04,  ...,  8.5354e-05,
          1.8930e-04,  4.2629e-04],
        [ 2.8362e-03, -1.1549e-03,  1.9264e-03,  ..., -3.0117e-03,
          3.7193e-04, -4.6577e-03],
        [-3.6106e-03,  1.0777e-03, -1.4868e-03,  ...,  1.6508e-03,
         -2.6474e-03,  3.7422e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1348,  0.1819,  2.6660,  ..., -0.9521, -3.8652,  1.1709]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 19:34:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The gorilla falls into the category of primate
The chinchilla falls into the category of rodent
The ant falls into the category of insect
The lion falls into the category of feline
The anaconda falls into the category of snake
The rattlesnake falls into the category of snake
The falcon falls into the category of raptor
The cat falls into the category of
2024-07-16 19:34:43 root INFO     [order_1_approx] starting weight calculation for The gorilla falls into the category of primate
The cat falls into the category of feline
The ant falls into the category of insect
The rattlesnake falls into the category of snake
The falcon falls into the category of raptor
The chinchilla falls into the category of rodent
The anaconda falls into the category of snake
The lion falls into the category of
2024-07-16 19:34:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 19:38:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2729, -1.3291, -0.7095,  ...,  0.2130,  0.8486,  1.6357],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.5234,  0.8779,  2.3125,  ..., -1.4590, -8.0000,  2.6680],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0126, -0.0003, -0.0012,  ...,  0.0045, -0.0034, -0.0116],
        [ 0.0022,  0.0052,  0.0056,  ...,  0.0125, -0.0025, -0.0080],
        [-0.0023,  0.0046,  0.0071,  ...,  0.0051, -0.0028,  0.0070],
        ...,
        [ 0.0061, -0.0093, -0.0013,  ...,  0.0147, -0.0029,  0.0129],
        [ 0.0083, -0.0195, -0.0086,  ..., -0.0060, -0.0035,  0.0049],
        [-0.0060, -0.0022,  0.0035,  ...,  0.0099, -0.0008,  0.0110]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.2734,  1.4004,  2.3730,  ..., -1.0127, -8.3438,  2.9844]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 19:38:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The gorilla falls into the category of primate
The cat falls into the category of feline
The ant falls into the category of insect
The rattlesnake falls into the category of snake
The falcon falls into the category of raptor
The chinchilla falls into the category of rodent
The anaconda falls into the category of snake
The lion falls into the category of
2024-07-16 19:38:09 root INFO     [order_1_approx] starting weight calculation for The rattlesnake falls into the category of snake
The anaconda falls into the category of snake
The chinchilla falls into the category of rodent
The ant falls into the category of insect
The lion falls into the category of feline
The falcon falls into the category of raptor
The cat falls into the category of feline
The gorilla falls into the category of
2024-07-16 19:38:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 19:41:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3074, -2.3359, -1.8379,  ...,  0.5308, -1.1914,  1.8193],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 8.7344,  1.2158,  3.3555,  ..., -1.3691, -4.5781,  2.0527],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0054,  0.0025,  0.0059,  ...,  0.0148, -0.0055, -0.0163],
        [-0.0047,  0.0007,  0.0041,  ...,  0.0116, -0.0007, -0.0068],
        [-0.0051, -0.0006, -0.0006,  ...,  0.0006, -0.0069,  0.0097],
        ...,
        [-0.0074, -0.0034, -0.0046,  ...,  0.0100, -0.0044,  0.0078],
        [ 0.0149, -0.0132, -0.0135,  ...,  0.0050,  0.0098,  0.0090],
        [-0.0008,  0.0011,  0.0107,  ...,  0.0018,  0.0004,  0.0016]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 8.4141,  1.8848,  2.3906,  ..., -1.4277, -5.2188,  2.3398]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 19:41:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The rattlesnake falls into the category of snake
The anaconda falls into the category of snake
The chinchilla falls into the category of rodent
The ant falls into the category of insect
The lion falls into the category of feline
The falcon falls into the category of raptor
The cat falls into the category of feline
The gorilla falls into the category of
2024-07-16 19:41:37 root INFO     [order_1_approx] starting weight calculation for The chinchilla falls into the category of rodent
The gorilla falls into the category of primate
The rattlesnake falls into the category of snake
The lion falls into the category of feline
The ant falls into the category of insect
The anaconda falls into the category of snake
The cat falls into the category of feline
The falcon falls into the category of
2024-07-16 19:41:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 19:45:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3408, -0.1902, -1.3506,  ..., -1.0400, -0.3125,  0.2451],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-9.7656e-04, -2.9883e-01, -3.8906e+00,  ..., -4.5996e-01,
        -4.4434e-01,  1.4209e+00], device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 7.1335e-03, -8.4457e-03,  4.2534e-04,  ...,  1.0757e-03,
         -3.7251e-03,  4.1046e-03],
        [ 5.5161e-03,  8.4991e-03,  8.8043e-03,  ...,  8.4686e-03,
          7.5493e-03, -3.9825e-03],
        [ 2.2926e-03, -2.3956e-03,  1.5182e-03,  ..., -1.4061e-02,
          1.8921e-03,  4.9782e-03],
        ...,
        [-5.9962e-05, -6.8855e-03, -2.5082e-03,  ...,  8.7891e-03,
          5.7602e-04,  9.4376e-03],
        [ 6.1035e-04, -4.9858e-03, -2.1515e-03,  ..., -1.3371e-03,
          5.9013e-03,  6.2408e-03],
        [-3.4733e-03, -8.1024e-03, -2.0065e-03,  ...,  1.7748e-03,
          7.1564e-03,  9.1934e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0060, -0.2114, -3.7383,  ..., -0.6416, -0.2255,  1.7939]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 19:45:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The chinchilla falls into the category of rodent
The gorilla falls into the category of primate
The rattlesnake falls into the category of snake
The lion falls into the category of feline
The ant falls into the category of insect
The anaconda falls into the category of snake
The cat falls into the category of feline
The falcon falls into the category of
2024-07-16 19:45:06 root INFO     [order_1_approx] starting weight calculation for The ant falls into the category of insect
The rattlesnake falls into the category of snake
The chinchilla falls into the category of rodent
The falcon falls into the category of raptor
The lion falls into the category of feline
The cat falls into the category of feline
The gorilla falls into the category of primate
The anaconda falls into the category of
2024-07-16 19:45:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 19:48:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1758,  1.6367, -0.9883,  ...,  0.5674,  0.8369,  0.4912],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1855,  0.4902, -1.8623,  ...,  1.2627, -3.1953,  3.6445],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.2436e-02,  4.9629e-03,  4.8180e-03,  ...,  9.8953e-03,
          3.3970e-03, -3.6850e-03],
        [-9.4452e-03,  9.1934e-03, -3.4790e-03,  ...,  1.3329e-02,
          3.2444e-03, -2.3308e-03],
        [ 9.7198e-03,  4.9324e-03,  1.1993e-02,  ..., -4.5929e-03,
          5.9967e-03,  1.1482e-02],
        ...,
        [-8.6288e-03,  2.6321e-04, -1.1620e-02,  ...,  2.4506e-02,
         -5.1117e-04, -6.0196e-03],
        [ 9.6741e-03, -7.8430e-03, -3.5515e-03,  ..., -8.4457e-03,
          7.1106e-03,  1.5900e-02],
        [-1.3290e-02,  9.3460e-05, -7.2861e-04,  ...,  2.0866e-03,
          1.2894e-03,  9.3689e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1868,  1.2979, -2.6602,  ...,  1.6484, -3.7852,  4.5859]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 19:48:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The ant falls into the category of insect
The rattlesnake falls into the category of snake
The chinchilla falls into the category of rodent
The falcon falls into the category of raptor
The lion falls into the category of feline
The cat falls into the category of feline
The gorilla falls into the category of primate
The anaconda falls into the category of
2024-07-16 19:48:34 root INFO     [order_1_approx] starting weight calculation for The falcon falls into the category of raptor
The lion falls into the category of feline
The cat falls into the category of feline
The ant falls into the category of insect
The anaconda falls into the category of snake
The chinchilla falls into the category of rodent
The gorilla falls into the category of primate
The rattlesnake falls into the category of
2024-07-16 19:48:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 19:52:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7983,  1.1455, -2.7305,  ...,  0.5742,  1.2773, -0.1611],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.0977,  0.6855, -0.7412,  ..., -0.7490, -2.8828,  3.4375],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.5495e-02,  2.2850e-03, -2.5406e-03,  ...,  2.9278e-03,
         -2.3270e-03, -4.8256e-03],
        [-3.5229e-03,  2.6283e-03,  1.0178e-02,  ...,  8.1787e-03,
          1.0262e-03, -1.8902e-03],
        [ 3.8791e-04, -3.7994e-03,  2.1362e-04,  ...,  2.9507e-03,
          3.0136e-03,  5.2567e-03],
        ...,
        [-2.3136e-03,  3.0518e-05, -3.2387e-03,  ...,  1.7990e-02,
         -2.5482e-03,  5.9223e-04],
        [-5.4264e-04, -6.6833e-03, -2.6474e-03,  ..., -2.8992e-04,
          7.5645e-03,  1.6785e-02],
        [-8.3466e-03, -1.2321e-03,  3.7766e-03,  ...,  7.7782e-03,
          6.8283e-04,  1.0086e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.9609,  0.6196, -1.4805,  ..., -1.1016, -2.8770,  4.1094]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 19:52:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The falcon falls into the category of raptor
The lion falls into the category of feline
The cat falls into the category of feline
The ant falls into the category of insect
The anaconda falls into the category of snake
The chinchilla falls into the category of rodent
The gorilla falls into the category of primate
The rattlesnake falls into the category of
2024-07-16 19:52:02 root INFO     total operator prediction time: 1663.2412912845612 seconds
2024-07-16 19:52:02 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on hyponyms - misc
2024-07-16 19:52:02 root INFO     building operator hyponyms - misc
2024-07-16 19:52:02 root INFO     [order_1_approx] starting weight calculation for A more specific term for a brush is toothbrush
A more specific term for a bed is bunk
A more specific term for a shelf is bookshelf
A more specific term for a jewel is diamond
A more specific term for a oven is broiler
A more specific term for a weapon is gun
A more specific term for a weekday is monday
A more specific term for a cutlery is
2024-07-16 19:52:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 19:55:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6157,  1.2158,  1.7812,  ...,  1.2100, -2.0332,  1.2822],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-5.9648,  1.5605, -4.3516,  ...,  2.4922,  0.9751,  1.5967],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0060,  0.0092,  0.0010,  ...,  0.0068, -0.0044, -0.0149],
        [ 0.0051,  0.0001,  0.0073,  ...,  0.0113, -0.0024, -0.0025],
        [-0.0130, -0.0051,  0.0084,  ...,  0.0085, -0.0040,  0.0038],
        ...,
        [-0.0053, -0.0184,  0.0013,  ...,  0.0205, -0.0097,  0.0095],
        [ 0.0058, -0.0060,  0.0046,  ..., -0.0009,  0.0127,  0.0002],
        [-0.0060,  0.0022,  0.0025,  ..., -0.0030,  0.0115,  0.0173]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.0469,  1.2158, -4.0820,  ...,  3.0098,  0.9922,  1.1650]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 19:55:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a brush is toothbrush
A more specific term for a bed is bunk
A more specific term for a shelf is bookshelf
A more specific term for a jewel is diamond
A more specific term for a oven is broiler
A more specific term for a weapon is gun
A more specific term for a weekday is monday
A more specific term for a cutlery is
2024-07-16 19:55:28 root INFO     [order_1_approx] starting weight calculation for A more specific term for a weapon is gun
A more specific term for a weekday is monday
A more specific term for a oven is broiler
A more specific term for a brush is toothbrush
A more specific term for a jewel is diamond
A more specific term for a cutlery is knife
A more specific term for a bed is bunk
A more specific term for a shelf is
2024-07-16 19:55:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 19:58:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0713,  0.0101,  0.3464,  ...,  0.2285, -1.3516,  0.6045],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4517, -0.8447, -0.0586,  ..., -0.1357, -3.6504,  2.1895],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0209, -0.0076,  0.0029,  ..., -0.0014, -0.0085, -0.0075],
        [ 0.0030,  0.0065,  0.0048,  ...,  0.0051,  0.0003,  0.0030],
        [-0.0063, -0.0161,  0.0041,  ..., -0.0056, -0.0011,  0.0144],
        ...,
        [ 0.0067, -0.0050,  0.0062,  ...,  0.0205, -0.0068, -0.0048],
        [ 0.0092,  0.0021, -0.0020,  ..., -0.0053,  0.0194, -0.0009],
        [ 0.0020,  0.0039, -0.0011,  ..., -0.0163,  0.0106,  0.0306]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9873, -0.5078,  0.9341,  ..., -0.7451, -4.2148,  2.3223]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 19:58:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a weapon is gun
A more specific term for a weekday is monday
A more specific term for a oven is broiler
A more specific term for a brush is toothbrush
A more specific term for a jewel is diamond
A more specific term for a cutlery is knife
A more specific term for a bed is bunk
A more specific term for a shelf is
2024-07-16 19:58:55 root INFO     [order_1_approx] starting weight calculation for A more specific term for a shelf is bookshelf
A more specific term for a weapon is gun
A more specific term for a cutlery is knife
A more specific term for a brush is toothbrush
A more specific term for a bed is bunk
A more specific term for a jewel is diamond
A more specific term for a oven is broiler
A more specific term for a weekday is
2024-07-16 19:58:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 20:02:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2223,  1.0576, -0.0469,  ..., -1.1045, -0.7183, -0.2174],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.3711, -3.5059, -3.6289,  ..., -0.3477,  3.9551, -4.6641],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0112, -0.0112,  0.0005,  ..., -0.0075, -0.0216,  0.0081],
        [-0.0016,  0.0020,  0.0112,  ...,  0.0020, -0.0040, -0.0127],
        [-0.0151, -0.0042, -0.0006,  ..., -0.0020,  0.0098,  0.0094],
        ...,
        [ 0.0064, -0.0095,  0.0002,  ...,  0.0084, -0.0090,  0.0086],
        [ 0.0104, -0.0087,  0.0064,  ..., -0.0007,  0.0009,  0.0074],
        [-0.0078, -0.0002,  0.0126,  ...,  0.0036,  0.0154,  0.0131]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.3535, -3.0352, -3.3965,  ..., -0.1241,  3.7363, -4.7227]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 20:02:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a shelf is bookshelf
A more specific term for a weapon is gun
A more specific term for a cutlery is knife
A more specific term for a brush is toothbrush
A more specific term for a bed is bunk
A more specific term for a jewel is diamond
A more specific term for a oven is broiler
A more specific term for a weekday is
2024-07-16 20:02:21 root INFO     [order_1_approx] starting weight calculation for A more specific term for a jewel is diamond
A more specific term for a weekday is monday
A more specific term for a oven is broiler
A more specific term for a brush is toothbrush
A more specific term for a bed is bunk
A more specific term for a shelf is bookshelf
A more specific term for a cutlery is knife
A more specific term for a weapon is
2024-07-16 20:02:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 20:05:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5859,  0.1443,  1.2549,  ..., -0.6064, -1.0977,  0.4617],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0566,  0.0444, -1.0322,  ..., -2.0312,  3.3418, -0.3867],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-5.5161e-03,  2.0294e-03,  8.2092e-03,  ..., -1.4412e-02,
         -4.6387e-03,  2.5520e-03],
        [ 6.3248e-03, -4.1618e-03,  3.6259e-03,  ...,  1.7151e-02,
         -1.6384e-03, -9.5139e-03],
        [-9.4681e-03,  1.7853e-03, -2.6054e-03,  ..., -5.0659e-03,
         -1.5472e-02,  3.8795e-03],
        ...,
        [-1.7462e-03, -1.3399e-04,  1.5144e-03,  ...,  1.7212e-02,
         -5.7983e-04, -5.1270e-03],
        [-5.6648e-03,  9.8228e-05,  7.1335e-04,  ..., -7.1945e-03,
         -6.9847e-03, -2.4681e-03],
        [-1.3359e-02, -6.7329e-04,  3.5095e-03,  ...,  5.5790e-04,
          1.0281e-03,  8.3084e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2793, -0.1809, -0.4927,  ..., -1.9043,  3.4551, -0.3481]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 20:05:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a jewel is diamond
A more specific term for a weekday is monday
A more specific term for a oven is broiler
A more specific term for a brush is toothbrush
A more specific term for a bed is bunk
A more specific term for a shelf is bookshelf
A more specific term for a cutlery is knife
A more specific term for a weapon is
2024-07-16 20:05:49 root INFO     [order_1_approx] starting weight calculation for A more specific term for a cutlery is knife
A more specific term for a jewel is diamond
A more specific term for a bed is bunk
A more specific term for a weapon is gun
A more specific term for a weekday is monday
A more specific term for a shelf is bookshelf
A more specific term for a brush is toothbrush
A more specific term for a oven is
2024-07-16 20:05:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 20:09:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4314, -0.3660,  0.1768,  ..., -0.0720, -0.4417,  1.5928],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.0938,  2.2168, -1.7080,  ..., -0.6187,  0.4814,  1.5645],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0218,  0.0005, -0.0055,  ...,  0.0105,  0.0130, -0.0017],
        [-0.0037,  0.0132,  0.0079,  ...,  0.0186, -0.0076,  0.0087],
        [-0.0038, -0.0102,  0.0029,  ..., -0.0011,  0.0015, -0.0017],
        ...,
        [-0.0062,  0.0006,  0.0042,  ...,  0.0262, -0.0237, -0.0102],
        [-0.0081, -0.0156,  0.0127,  ...,  0.0039,  0.0151, -0.0006],
        [-0.0131, -0.0014, -0.0047,  ...,  0.0011,  0.0095,  0.0197]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.7656,  2.0000, -0.8438,  ..., -0.0112,  0.4578,  2.2402]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 20:09:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a cutlery is knife
A more specific term for a jewel is diamond
A more specific term for a bed is bunk
A more specific term for a weapon is gun
A more specific term for a weekday is monday
A more specific term for a shelf is bookshelf
A more specific term for a brush is toothbrush
A more specific term for a oven is
2024-07-16 20:09:16 root INFO     [order_1_approx] starting weight calculation for A more specific term for a oven is broiler
A more specific term for a bed is bunk
A more specific term for a cutlery is knife
A more specific term for a weapon is gun
A more specific term for a weekday is monday
A more specific term for a jewel is diamond
A more specific term for a shelf is bookshelf
A more specific term for a brush is
2024-07-16 20:09:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 20:12:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0931,  0.7246,  0.0430,  ...,  0.4451, -3.1094,  1.4375],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5190, -2.1445, -7.3906,  ..., -3.0312, -2.7500,  3.3164],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0148, -0.0043, -0.0048,  ...,  0.0014, -0.0022,  0.0011],
        [-0.0054,  0.0195,  0.0173,  ...,  0.0107,  0.0024,  0.0098],
        [-0.0060, -0.0103,  0.0161,  ..., -0.0095, -0.0162,  0.0249],
        ...,
        [ 0.0016,  0.0104,  0.0156,  ...,  0.0345, -0.0081, -0.0145],
        [ 0.0049, -0.0102,  0.0052,  ..., -0.0010,  0.0129,  0.0068],
        [ 0.0025, -0.0131,  0.0096,  ...,  0.0051,  0.0152,  0.0298]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6758, -1.4658, -6.9375,  ..., -3.3242, -2.5449,  3.5117]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 20:12:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a oven is broiler
A more specific term for a bed is bunk
A more specific term for a cutlery is knife
A more specific term for a weapon is gun
A more specific term for a weekday is monday
A more specific term for a jewel is diamond
A more specific term for a shelf is bookshelf
A more specific term for a brush is
2024-07-16 20:12:43 root INFO     [order_1_approx] starting weight calculation for A more specific term for a weekday is monday
A more specific term for a oven is broiler
A more specific term for a weapon is gun
A more specific term for a shelf is bookshelf
A more specific term for a brush is toothbrush
A more specific term for a cutlery is knife
A more specific term for a bed is bunk
A more specific term for a jewel is
2024-07-16 20:12:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 20:16:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1221, -0.1003, -0.3066,  ..., -0.2252, -1.1904, -0.3586],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.2871, -2.8457, -4.4219,  ..., -4.6211, -2.9766,  5.2031],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0058, -0.0037,  0.0020,  ..., -0.0118, -0.0002, -0.0145],
        [ 0.0107,  0.0009, -0.0005,  ...,  0.0057,  0.0177, -0.0001],
        [ 0.0157,  0.0050,  0.0051,  ...,  0.0023, -0.0108,  0.0033],
        ...,
        [-0.0045,  0.0072,  0.0004,  ...,  0.0187, -0.0128, -0.0008],
        [ 0.0003,  0.0139,  0.0021,  ...,  0.0004, -0.0105, -0.0136],
        [-0.0117,  0.0008,  0.0061,  ..., -0.0011,  0.0054,  0.0249]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.0176, -2.3398, -4.0273,  ..., -4.4492, -1.8877,  5.6094]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 20:16:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a weekday is monday
A more specific term for a oven is broiler
A more specific term for a weapon is gun
A more specific term for a shelf is bookshelf
A more specific term for a brush is toothbrush
A more specific term for a cutlery is knife
A more specific term for a bed is bunk
A more specific term for a jewel is
2024-07-16 20:16:09 root INFO     [order_1_approx] starting weight calculation for A more specific term for a jewel is diamond
A more specific term for a shelf is bookshelf
A more specific term for a brush is toothbrush
A more specific term for a weekday is monday
A more specific term for a cutlery is knife
A more specific term for a oven is broiler
A more specific term for a weapon is gun
A more specific term for a bed is
2024-07-16 20:16:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 20:19:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2124,  0.0349,  1.5684,  ..., -0.0200, -1.6348,  1.1357],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6992, -4.0078, -0.6377,  ..., -2.7930, -2.2520, -1.8066],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.4872e-02,  2.5349e-03, -1.5297e-03,  ...,  4.3716e-03,
         -1.8196e-03, -2.5749e-05],
        [ 8.9645e-04,  1.5160e-02,  8.3084e-03,  ...,  9.7504e-03,
          1.3542e-02, -1.2741e-03],
        [ 6.0225e-04,  6.4583e-03,  9.1934e-03,  ..., -3.9978e-03,
         -4.0627e-04, -9.7427e-03],
        ...,
        [ 1.0834e-02, -5.1880e-04,  3.2501e-03,  ...,  2.6520e-02,
         -1.2093e-02, -4.9515e-03],
        [-5.0774e-03, -2.0866e-03,  7.3128e-03,  ...,  7.9575e-03,
          1.4511e-02, -2.4319e-03],
        [-2.1591e-03, -6.1035e-05, -7.9575e-03,  ..., -5.9586e-03,
         -5.3024e-03,  3.0991e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.1895, -3.1680, -1.2109,  ..., -2.7871, -1.3027, -1.9248]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 20:19:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a jewel is diamond
A more specific term for a shelf is bookshelf
A more specific term for a brush is toothbrush
A more specific term for a weekday is monday
A more specific term for a cutlery is knife
A more specific term for a oven is broiler
A more specific term for a weapon is gun
A more specific term for a bed is
2024-07-16 20:19:35 root INFO     total operator prediction time: 1653.3366701602936 seconds
2024-07-16 20:19:35 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on antonyms - binary
2024-07-16 20:19:35 root INFO     building operator antonyms - binary
2024-07-16 20:19:36 root INFO     [order_1_approx] starting weight calculation for The opposite of under is over
The opposite of occupied is vacant
The opposite of before is after
The opposite of off is on
The opposite of ahead is behind
The opposite of below is above
The opposite of input is output
The opposite of outward is
2024-07-16 20:19:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 20:23:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9478,  0.4448, -0.1039,  ..., -2.0762,  1.2070,  1.4766],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.3008,  1.5098, -1.2324,  ..., -2.4766,  6.3711, -1.7783],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0017, -0.0290,  0.0024,  ...,  0.0054, -0.0122, -0.0057],
        [-0.0121,  0.0032, -0.0046,  ...,  0.0159,  0.0069,  0.0079],
        [-0.0108, -0.0025, -0.0077,  ..., -0.0016, -0.0012,  0.0145],
        ...,
        [ 0.0357, -0.0292, -0.0057,  ...,  0.0066,  0.0054, -0.0010],
        [-0.0066,  0.0054,  0.0097,  ..., -0.0167, -0.0072,  0.0049],
        [ 0.0003, -0.0105, -0.0026,  ...,  0.0108, -0.0091,  0.0123]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.2617,  1.4336, -2.4570,  ..., -2.5391,  7.6484, -1.3047]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 20:23:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of under is over
The opposite of occupied is vacant
The opposite of before is after
The opposite of off is on
The opposite of ahead is behind
The opposite of below is above
The opposite of input is output
The opposite of outward is
2024-07-16 20:23:03 root INFO     [order_1_approx] starting weight calculation for The opposite of before is after
The opposite of occupied is vacant
The opposite of input is output
The opposite of ahead is behind
The opposite of off is on
The opposite of below is above
The opposite of outward is upward
The opposite of under is
2024-07-16 20:23:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 20:26:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5337, -0.3960,  0.9199,  ..., -0.3938,  0.6489,  1.0977],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7817, -5.6680, -1.3906,  ..., -0.3962,  3.2598, -0.8203],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0134, -0.0023,  0.0055,  ..., -0.0181,  0.0027,  0.0010],
        [-0.0027,  0.0108, -0.0002,  ...,  0.0003, -0.0042,  0.0040],
        [ 0.0037, -0.0221, -0.0061,  ..., -0.0071, -0.0080,  0.0289],
        ...,
        [ 0.0005, -0.0053, -0.0016,  ...,  0.0052,  0.0053,  0.0052],
        [ 0.0134, -0.0030,  0.0050,  ..., -0.0018,  0.0029, -0.0033],
        [ 0.0049, -0.0067,  0.0034,  ..., -0.0070,  0.0090,  0.0130]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9199, -4.7617, -2.1992,  ..., -0.6553,  4.5430, -0.8091]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 20:26:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of before is after
The opposite of occupied is vacant
The opposite of input is output
The opposite of ahead is behind
The opposite of off is on
The opposite of below is above
The opposite of outward is upward
The opposite of under is
2024-07-16 20:26:31 root INFO     [order_1_approx] starting weight calculation for The opposite of ahead is behind
The opposite of outward is upward
The opposite of below is above
The opposite of under is over
The opposite of occupied is vacant
The opposite of before is after
The opposite of input is output
The opposite of off is
2024-07-16 20:26:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 20:29:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4746, -0.7998, -0.9253,  ...,  1.6016, -0.3970,  1.1934],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.8125, -4.4531, -0.8770,  ...,  0.0870,  1.1270,  0.1924],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0065,  0.0019,  0.0116,  ..., -0.0033, -0.0165, -0.0233],
        [-0.0135, -0.0165, -0.0034,  ..., -0.0041,  0.0050,  0.0167],
        [ 0.0101, -0.0170, -0.0268,  ..., -0.0169,  0.0034,  0.0222],
        ...,
        [ 0.0045,  0.0015, -0.0004,  ..., -0.0095,  0.0074, -0.0115],
        [ 0.0046,  0.0039,  0.0016,  ..., -0.0110, -0.0074, -0.0103],
        [ 0.0023, -0.0108,  0.0181,  ..., -0.0014, -0.0036,  0.0003]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.4502, -4.6719, -0.1035,  ..., -0.5503,  0.8682,  0.0316]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 20:29:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of ahead is behind
The opposite of outward is upward
The opposite of below is above
The opposite of under is over
The opposite of occupied is vacant
The opposite of before is after
The opposite of input is output
The opposite of off is
2024-07-16 20:29:59 root INFO     [order_1_approx] starting weight calculation for The opposite of before is after
The opposite of ahead is behind
The opposite of input is output
The opposite of outward is upward
The opposite of under is over
The opposite of occupied is vacant
The opposite of off is on
The opposite of below is
2024-07-16 20:29:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 20:33:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4189, -0.8779,  0.3672,  ..., -0.3560,  0.6816, -0.2737],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.3125, -5.0352,  2.0508,  ..., -0.6396,  4.4023,  2.0234],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0091, -0.0138,  0.0028,  ..., -0.0061, -0.0125, -0.0143],
        [-0.0034, -0.0077,  0.0018,  ...,  0.0067,  0.0044,  0.0054],
        [ 0.0001, -0.0068, -0.0191,  ..., -0.0017, -0.0046,  0.0102],
        ...,
        [ 0.0013, -0.0123,  0.0074,  ..., -0.0070, -0.0024,  0.0067],
        [-0.0003, -0.0143,  0.0022,  ..., -0.0115, -0.0035, -0.0056],
        [ 0.0011, -0.0062, -0.0004,  ..., -0.0023, -0.0007, -0.0046]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.8828, -4.8555,  1.2217,  ..., -0.0151,  5.0352,  1.7793]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 20:33:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of before is after
The opposite of ahead is behind
The opposite of input is output
The opposite of outward is upward
The opposite of under is over
The opposite of occupied is vacant
The opposite of off is on
The opposite of below is
2024-07-16 20:33:27 root INFO     [order_1_approx] starting weight calculation for The opposite of ahead is behind
The opposite of off is on
The opposite of outward is upward
The opposite of occupied is vacant
The opposite of input is output
The opposite of under is over
The opposite of below is above
The opposite of before is
2024-07-16 20:33:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 20:36:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.0186, 0.8213, 0.2590,  ..., 0.7173, 0.1094, 0.5889], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4922, -2.6719, -0.8750,  ...,  4.2812,  5.5742,  2.0430],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.7883e-02, -1.0689e-02,  8.6060e-03,  ...,  6.4850e-03,
         -9.0027e-03, -1.1436e-02],
        [-5.5313e-03,  9.4376e-03, -1.0399e-02,  ..., -5.6038e-03,
         -3.9291e-04,  8.4152e-03],
        [-2.4853e-03, -4.8256e-03, -1.1673e-02,  ..., -2.2202e-02,
         -9.3842e-03,  1.1581e-02],
        ...,
        [-1.3489e-02, -8.7547e-04,  9.0332e-03,  ...,  1.8631e-02,
         -1.0147e-03,  6.7291e-03],
        [ 1.1070e-02,  9.7733e-03,  1.4557e-02,  ..., -8.0109e-03,
         -1.2379e-03,  1.5259e-05],
        [-6.7062e-03, -1.2962e-02,  7.1754e-03,  ..., -1.2581e-02,
         -1.0132e-02,  1.2802e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7988, -2.9160, -0.9570,  ...,  3.8242,  6.4844,  1.7646]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 20:36:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of ahead is behind
The opposite of off is on
The opposite of outward is upward
The opposite of occupied is vacant
The opposite of input is output
The opposite of under is over
The opposite of below is above
The opposite of before is
2024-07-16 20:36:55 root INFO     [order_1_approx] starting weight calculation for The opposite of before is after
The opposite of ahead is behind
The opposite of below is above
The opposite of input is output
The opposite of outward is upward
The opposite of under is over
The opposite of off is on
The opposite of occupied is
2024-07-16 20:36:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 20:40:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1250, -0.9785,  0.6768,  ...,  0.5317,  0.4138, -0.5054],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9277,  1.4941, -3.7891,  ..., -0.3015,  0.9141, -4.1172],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0101, -0.0150,  0.0052,  ..., -0.0104, -0.0047, -0.0054],
        [ 0.0062,  0.0022, -0.0087,  ..., -0.0044,  0.0067,  0.0067],
        [-0.0007, -0.0035, -0.0185,  ..., -0.0068,  0.0069, -0.0108],
        ...,
        [ 0.0055, -0.0083, -0.0030,  ...,  0.0017,  0.0096,  0.0054],
        [ 0.0044, -0.0023, -0.0029,  ...,  0.0069, -0.0066,  0.0023],
        [-0.0009,  0.0114, -0.0136,  ...,  0.0096, -0.0027,  0.0023]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3984,  2.5195, -3.8320,  ..., -1.4336,  1.1660, -3.6914]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 20:40:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of before is after
The opposite of ahead is behind
The opposite of below is above
The opposite of input is output
The opposite of outward is upward
The opposite of under is over
The opposite of off is on
The opposite of occupied is
2024-07-16 20:40:23 root INFO     [order_1_approx] starting weight calculation for The opposite of occupied is vacant
The opposite of off is on
The opposite of below is above
The opposite of under is over
The opposite of outward is upward
The opposite of before is after
The opposite of input is output
The opposite of ahead is
2024-07-16 20:40:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 20:43:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5337, -0.2656, -0.6812,  ..., -0.9395, -0.5322,  0.1453],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0781, -0.0833, -2.7812,  ...,  3.6445,  7.0156,  0.1035],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0194, -0.0159,  0.0052,  ...,  0.0034,  0.0002,  0.0066],
        [-0.0089,  0.0014, -0.0113,  ..., -0.0148, -0.0016,  0.0075],
        [-0.0135, -0.0139, -0.0028,  ..., -0.0131,  0.0013,  0.0246],
        ...,
        [-0.0100, -0.0036, -0.0011,  ...,  0.0069, -0.0004, -0.0018],
        [-0.0066,  0.0051,  0.0111,  ..., -0.0168, -0.0032, -0.0082],
        [ 0.0035, -0.0061, -0.0110,  ..., -0.0032, -0.0046, -0.0019]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.2305,  0.6738, -3.3105,  ...,  4.1992,  7.3047, -0.1511]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 20:43:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of occupied is vacant
The opposite of off is on
The opposite of below is above
The opposite of under is over
The opposite of outward is upward
The opposite of before is after
The opposite of input is output
The opposite of ahead is
2024-07-16 20:43:51 root INFO     [order_1_approx] starting weight calculation for The opposite of off is on
The opposite of occupied is vacant
The opposite of outward is upward
The opposite of below is above
The opposite of ahead is behind
The opposite of under is over
The opposite of before is after
The opposite of input is
2024-07-16 20:43:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 20:47:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7925,  0.1985,  0.4016,  ..., -0.4309, -0.5962,  1.2021],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.2734, -3.1074,  0.9873,  ...,  0.9980,  4.5586, -0.0508],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0086, -0.0054, -0.0069,  ...,  0.0132, -0.0110,  0.0005],
        [ 0.0033,  0.0003,  0.0014,  ...,  0.0049, -0.0096, -0.0032],
        [-0.0076, -0.0013, -0.0239,  ..., -0.0131, -0.0069,  0.0182],
        ...,
        [ 0.0103, -0.0047,  0.0065,  ...,  0.0068, -0.0091,  0.0016],
        [ 0.0075,  0.0053, -0.0074,  ..., -0.0192,  0.0023, -0.0052],
        [ 0.0052,  0.0035,  0.0014,  ..., -0.0011, -0.0045,  0.0082]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.4961, -2.5527,  0.9121,  ...,  0.5068,  4.8516, -0.8120]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 20:47:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of off is on
The opposite of occupied is vacant
The opposite of outward is upward
The opposite of below is above
The opposite of ahead is behind
The opposite of under is over
The opposite of before is after
The opposite of input is
2024-07-16 20:47:18 root INFO     total operator prediction time: 1662.571487903595 seconds
2024-07-16 20:47:18 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - member
2024-07-16 20:47:18 root INFO     building operator meronyms - member
2024-07-16 20:47:18 root INFO     [order_1_approx] starting weight calculation for A person is a member of a society
A antelope is a member of a herd
A bee is a member of a swarm
A musician is a member of a orchestra
A wolf is a member of a pack
A senator is a member of a senate
A car is a member of a train
A shrub is a member of a
2024-07-16 20:47:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 20:50:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0332, -0.8857,  0.2103,  ..., -0.6924, -0.3445,  0.6641],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8613,  2.0547, -2.3809,  ...,  3.1562,  1.7480, -0.9810],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0104,  0.0006,  0.0073,  ..., -0.0005, -0.0051, -0.0091],
        [-0.0002, -0.0010,  0.0022,  ...,  0.0086, -0.0029, -0.0030],
        [-0.0083,  0.0038, -0.0057,  ..., -0.0007, -0.0053,  0.0033],
        ...,
        [-0.0017,  0.0055, -0.0036,  ...,  0.0068, -0.0074,  0.0016],
        [ 0.0055, -0.0047,  0.0018,  ...,  0.0056, -0.0056,  0.0015],
        [ 0.0027, -0.0099,  0.0019,  ..., -0.0058, -0.0014,  0.0013]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7236,  2.1719, -2.4004,  ...,  3.4785,  1.7070, -0.9624]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 20:50:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A person is a member of a society
A antelope is a member of a herd
A bee is a member of a swarm
A musician is a member of a orchestra
A wolf is a member of a pack
A senator is a member of a senate
A car is a member of a train
A shrub is a member of a
2024-07-16 20:50:45 root INFO     [order_1_approx] starting weight calculation for A musician is a member of a orchestra
A wolf is a member of a pack
A car is a member of a train
A senator is a member of a senate
A bee is a member of a swarm
A antelope is a member of a herd
A shrub is a member of a shrubbery
A person is a member of a
2024-07-16 20:50:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 20:54:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7998,  0.5625, -0.2854,  ...,  1.0234,  1.0693,  0.6787],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.9062, -2.8281, -0.6401,  ..., -1.9834,  1.2100,  1.0254],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0045, -0.0009,  0.0011,  ..., -0.0021, -0.0043, -0.0101],
        [-0.0100, -0.0015,  0.0109,  ...,  0.0058,  0.0033, -0.0069],
        [-0.0054,  0.0035, -0.0029,  ..., -0.0024, -0.0026,  0.0053],
        ...,
        [-0.0086,  0.0008,  0.0012,  ..., -0.0065, -0.0052, -0.0046],
        [ 0.0010, -0.0054,  0.0076,  ..., -0.0035, -0.0086,  0.0032],
        [-0.0078,  0.0090,  0.0079,  ...,  0.0053,  0.0015, -0.0045]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7129, -2.8926, -0.7310,  ..., -1.8564,  1.3223,  0.5337]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 20:54:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A musician is a member of a orchestra
A wolf is a member of a pack
A car is a member of a train
A senator is a member of a senate
A bee is a member of a swarm
A antelope is a member of a herd
A shrub is a member of a shrubbery
A person is a member of a
2024-07-16 20:54:13 root INFO     [order_1_approx] starting weight calculation for A senator is a member of a senate
A musician is a member of a orchestra
A wolf is a member of a pack
A bee is a member of a swarm
A person is a member of a society
A antelope is a member of a herd
A shrub is a member of a shrubbery
A car is a member of a
2024-07-16 20:54:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 20:57:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4424,  0.1105, -1.2295,  ...,  0.1479, -0.3293,  0.0933],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.6523, -0.8633, -2.5977,  ...,  2.2285, -0.5386,  0.6958],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0038, -0.0157, -0.0019,  ..., -0.0131, -0.0035, -0.0167],
        [ 0.0003,  0.0023,  0.0064,  ...,  0.0037,  0.0002, -0.0045],
        [ 0.0079,  0.0108, -0.0173,  ..., -0.0156,  0.0094, -0.0036],
        ...,
        [-0.0020, -0.0076, -0.0050,  ...,  0.0078, -0.0111, -0.0100],
        [ 0.0030, -0.0199,  0.0066,  ...,  0.0055, -0.0055, -0.0130],
        [-0.0066, -0.0077,  0.0015,  ..., -0.0102, -0.0009, -0.0082]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.7168, -0.8594, -3.5039,  ...,  2.8438, -0.3625,  0.3586]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 20:57:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A senator is a member of a senate
A musician is a member of a orchestra
A wolf is a member of a pack
A bee is a member of a swarm
A person is a member of a society
A antelope is a member of a herd
A shrub is a member of a shrubbery
A car is a member of a
2024-07-16 20:57:40 root INFO     [order_1_approx] starting weight calculation for A antelope is a member of a herd
A senator is a member of a senate
A car is a member of a train
A shrub is a member of a shrubbery
A person is a member of a society
A wolf is a member of a pack
A musician is a member of a orchestra
A bee is a member of a
2024-07-16 20:57:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 21:01:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9131,  1.1484, -1.5439,  ...,  0.4712, -1.4512,  1.5439],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.2148,  2.3945,  0.2837,  ...,  1.4424,  2.2500,  1.7832],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0005, -0.0110,  0.0043,  ...,  0.0008,  0.0053,  0.0013],
        [-0.0063, -0.0064,  0.0019,  ...,  0.0033,  0.0051,  0.0041],
        [ 0.0067, -0.0020, -0.0151,  ..., -0.0092,  0.0092, -0.0014],
        ...,
        [ 0.0110, -0.0017, -0.0053,  ..., -0.0060, -0.0006, -0.0094],
        [ 0.0059, -0.0107,  0.0001,  ...,  0.0017, -0.0034, -0.0059],
        [-0.0022,  0.0002,  0.0024,  ..., -0.0040,  0.0006, -0.0119]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.7930,  2.6250,  0.5146,  ...,  1.8301,  1.4229,  1.3809]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 21:01:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A antelope is a member of a herd
A senator is a member of a senate
A car is a member of a train
A shrub is a member of a shrubbery
A person is a member of a society
A wolf is a member of a pack
A musician is a member of a orchestra
A bee is a member of a
2024-07-16 21:01:09 root INFO     [order_1_approx] starting weight calculation for A car is a member of a train
A shrub is a member of a shrubbery
A bee is a member of a swarm
A antelope is a member of a herd
A musician is a member of a orchestra
A person is a member of a society
A wolf is a member of a pack
A senator is a member of a
2024-07-16 21:01:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 21:04:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.8740,  0.2297,  0.3889,  ...,  0.8076,  0.5137, -1.4570],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([0.9043, 5.5234, 0.1836,  ..., 2.4473, 1.1602, 2.4238], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.6327e-03, -4.5853e-03,  2.4910e-03,  ..., -4.4250e-03,
         -1.4133e-03,  7.4196e-04],
        [-8.8806e-03, -8.3389e-03,  2.0485e-03,  ...,  1.2070e-02,
         -2.6588e-03, -2.6817e-03],
        [-1.1635e-04, -2.0828e-03, -1.1841e-02,  ..., -6.8665e-05,
          4.0016e-03,  1.7338e-03],
        ...,
        [-2.5558e-03, -5.0774e-03, -7.0419e-03,  ...,  1.0612e-02,
         -3.8395e-03, -1.2474e-03],
        [ 3.4218e-03, -3.7670e-03,  9.4147e-03,  ...,  4.0703e-03,
         -1.1093e-02, -7.0648e-03],
        [-1.1368e-03,  8.7204e-03,  8.8425e-03,  ...,  1.0014e-04,
          4.7226e-03,  2.3079e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[1.1367, 5.6797, 0.3838,  ..., 1.9316, 0.8496, 2.5273]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 21:04:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A car is a member of a train
A shrub is a member of a shrubbery
A bee is a member of a swarm
A antelope is a member of a herd
A musician is a member of a orchestra
A person is a member of a society
A wolf is a member of a pack
A senator is a member of a
2024-07-16 21:04:37 root INFO     [order_1_approx] starting weight calculation for A musician is a member of a orchestra
A person is a member of a society
A car is a member of a train
A antelope is a member of a herd
A bee is a member of a swarm
A shrub is a member of a shrubbery
A senator is a member of a senate
A wolf is a member of a
2024-07-16 21:04:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 21:08:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1309, -0.7319, -1.2734,  ...,  0.2471, -1.2773, -0.0083],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.0566, -1.6689, -2.5723,  ...,  2.8711,  0.7617,  2.7285],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0054, -0.0115,  0.0080,  ...,  0.0010,  0.0004, -0.0030],
        [-0.0068,  0.0033,  0.0055,  ...,  0.0085, -0.0040, -0.0067],
        [-0.0066,  0.0091,  0.0072,  ..., -0.0048, -0.0084, -0.0045],
        ...,
        [-0.0050,  0.0010, -0.0057,  ...,  0.0088, -0.0019,  0.0047],
        [ 0.0189, -0.0208,  0.0069,  ...,  0.0074, -0.0083,  0.0013],
        [-0.0057,  0.0024,  0.0012,  ...,  0.0037, -0.0003, -0.0086]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.1758, -1.5957, -2.3457,  ...,  2.9434,  0.7080,  2.7578]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 21:08:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A musician is a member of a orchestra
A person is a member of a society
A car is a member of a train
A antelope is a member of a herd
A bee is a member of a swarm
A shrub is a member of a shrubbery
A senator is a member of a senate
A wolf is a member of a
2024-07-16 21:08:03 root INFO     [order_1_approx] starting weight calculation for A person is a member of a society
A antelope is a member of a herd
A shrub is a member of a shrubbery
A bee is a member of a swarm
A car is a member of a train
A wolf is a member of a pack
A senator is a member of a senate
A musician is a member of a
2024-07-16 21:08:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 21:11:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5215, -0.0828,  0.4487,  ..., -0.2383,  0.3572, -0.9844],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6514, -0.0156,  2.5801,  ...,  1.6514,  2.0020, -1.8232],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.0162e-02, -3.7479e-03,  2.9392e-03,  ..., -6.6280e-05,
          2.4033e-03, -6.7749e-03],
        [ 2.2087e-03, -5.7602e-03, -1.2665e-03,  ...,  1.2903e-03,
          2.0752e-03, -4.2992e-03],
        [-1.7309e-03,  8.2169e-03, -4.9591e-04,  ...,  8.5297e-03,
          9.5940e-04,  3.5477e-04],
        ...,
        [-8.5526e-03,  6.5689e-03, -5.0125e-03,  ...,  6.1951e-03,
         -9.7809e-03,  6.1035e-04],
        [ 1.2159e-03, -1.1253e-02,  2.7084e-03,  ..., -8.4839e-03,
         -1.1810e-02,  4.1847e-03],
        [-3.1185e-03,  4.1428e-03,  9.0408e-04,  ...,  2.5158e-03,
          1.2207e-02, -1.1505e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5693,  0.2316,  2.2266,  ...,  1.9277,  1.9600, -1.6650]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 21:11:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A person is a member of a society
A antelope is a member of a herd
A shrub is a member of a shrubbery
A bee is a member of a swarm
A car is a member of a train
A wolf is a member of a pack
A senator is a member of a senate
A musician is a member of a
2024-07-16 21:11:30 root INFO     [order_1_approx] starting weight calculation for A bee is a member of a swarm
A shrub is a member of a shrubbery
A car is a member of a train
A senator is a member of a senate
A wolf is a member of a pack
A person is a member of a society
A musician is a member of a orchestra
A antelope is a member of a
2024-07-16 21:11:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 21:14:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7866,  0.2311, -1.1777,  ..., -0.1396, -0.5479,  0.1042],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.2539,  3.3984, -2.2910,  ...,  0.8843,  1.5156,  0.2236],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0090,  0.0016,  0.0078,  ..., -0.0057,  0.0006,  0.0020],
        [-0.0009,  0.0088,  0.0038,  ...,  0.0157,  0.0055, -0.0019],
        [ 0.0009, -0.0046, -0.0025,  ..., -0.0066,  0.0019,  0.0016],
        ...,
        [-0.0043, -0.0059, -0.0041,  ...,  0.0106,  0.0054, -0.0018],
        [ 0.0032, -0.0152,  0.0022,  ...,  0.0010, -0.0019, -0.0015],
        [ 0.0041, -0.0008, -0.0003,  ..., -0.0059, -0.0013,  0.0024]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3525,  3.4629, -2.3164,  ...,  0.7998,  1.7227,  0.5435]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 21:14:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A bee is a member of a swarm
A shrub is a member of a shrubbery
A car is a member of a train
A senator is a member of a senate
A wolf is a member of a pack
A person is a member of a society
A musician is a member of a orchestra
A antelope is a member of a
2024-07-16 21:14:58 root INFO     total operator prediction time: 1660.3101406097412 seconds
2024-07-16 21:14:58 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on noun - plural_irreg
2024-07-16 21:14:58 root INFO     building operator noun - plural_irreg
2024-07-16 21:14:58 root INFO     [order_1_approx] starting weight calculation for The plural form of authority is authorities
The plural form of safety is safeties
The plural form of datum is data
The plural form of child is children
The plural form of majority is majorities
The plural form of story is stories
The plural form of policy is policies
The plural form of facility is
2024-07-16 21:14:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 21:18:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.1680,  0.5801, -1.1016,  ..., -0.6621,  0.4353,  0.9229],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5762,  3.7656,  0.9990,  ..., -2.0469,  0.4263,  5.4922],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0108, -0.0141,  0.0125,  ...,  0.0035,  0.0041, -0.0156],
        [-0.0030,  0.0016, -0.0007,  ...,  0.0137, -0.0009,  0.0028],
        [ 0.0013, -0.0099, -0.0016,  ..., -0.0083,  0.0243,  0.0047],
        ...,
        [ 0.0035, -0.0136, -0.0026,  ...,  0.0107,  0.0013,  0.0006],
        [ 0.0007, -0.0021,  0.0030,  ..., -0.0079, -0.0151,  0.0092],
        [-0.0160,  0.0126, -0.0002,  ...,  0.0018, -0.0106,  0.0128]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6064,  4.1914,  0.3130,  ..., -2.1367,  0.3921,  6.0820]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 21:18:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of authority is authorities
The plural form of safety is safeties
The plural form of datum is data
The plural form of child is children
The plural form of majority is majorities
The plural form of story is stories
The plural form of policy is policies
The plural form of facility is
2024-07-16 21:18:26 root INFO     [order_1_approx] starting weight calculation for The plural form of facility is facilities
The plural form of story is stories
The plural form of safety is safeties
The plural form of authority is authorities
The plural form of policy is policies
The plural form of child is children
The plural form of datum is data
The plural form of majority is
2024-07-16 21:18:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 21:21:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4492,  0.3477,  0.3479,  ..., -2.8262, -0.8926,  0.8457],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0371, -1.5791, -1.3848,  ...,  1.7441, -3.3809, -0.2432],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0035, -0.0116,  0.0129,  ..., -0.0090, -0.0048, -0.0113],
        [ 0.0081, -0.0020, -0.0008,  ...,  0.0123, -0.0026, -0.0068],
        [ 0.0073, -0.0006, -0.0244,  ...,  0.0082,  0.0107, -0.0033],
        ...,
        [-0.0074, -0.0075, -0.0015,  ..., -0.0032, -0.0019, -0.0006],
        [ 0.0061, -0.0115,  0.0132,  ..., -0.0073, -0.0072, -0.0101],
        [ 0.0030,  0.0153,  0.0118,  ..., -0.0007, -0.0026, -0.0108]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5967, -2.0293, -1.2432,  ...,  2.3457, -2.6621, -0.1416]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 21:21:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of facility is facilities
The plural form of story is stories
The plural form of safety is safeties
The plural form of authority is authorities
The plural form of policy is policies
The plural form of child is children
The plural form of datum is data
The plural form of majority is
2024-07-16 21:21:50 root INFO     [order_1_approx] starting weight calculation for The plural form of safety is safeties
The plural form of majority is majorities
The plural form of datum is data
The plural form of authority is authorities
The plural form of facility is facilities
The plural form of policy is policies
The plural form of child is children
The plural form of story is
2024-07-16 21:21:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 21:25:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7827,  0.4790,  1.4209,  ..., -0.3525,  0.7354,  0.5186],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9487,  2.0117,  1.3350,  ...,  1.9980, -6.0000,  1.1230],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0181, -0.0045,  0.0104,  ...,  0.0006, -0.0027, -0.0142],
        [-0.0014, -0.0121, -0.0023,  ...,  0.0105, -0.0103,  0.0011],
        [-0.0069, -0.0086, -0.0210,  ..., -0.0104,  0.0065,  0.0024],
        ...,
        [ 0.0043, -0.0131,  0.0059,  ..., -0.0144,  0.0026, -0.0065],
        [-0.0059,  0.0032,  0.0089,  ...,  0.0040, -0.0091, -0.0046],
        [-0.0021,  0.0130,  0.0097,  ..., -0.0138, -0.0053, -0.0054]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2734,  1.5957,  1.4150,  ...,  1.4785, -6.3359,  1.4473]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 21:25:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of safety is safeties
The plural form of majority is majorities
The plural form of datum is data
The plural form of authority is authorities
The plural form of facility is facilities
The plural form of policy is policies
The plural form of child is children
The plural form of story is
2024-07-16 21:25:15 root INFO     [order_1_approx] starting weight calculation for The plural form of facility is facilities
The plural form of authority is authorities
The plural form of story is stories
The plural form of majority is majorities
The plural form of policy is policies
The plural form of datum is data
The plural form of child is children
The plural form of safety is
2024-07-16 21:25:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 21:28:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0535,  1.3525, -0.0271,  ..., -0.6851,  0.0142, -0.2095],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.3828,  6.4375,  2.8008,  ..., -2.8398, -0.8579,  1.7100],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0188, -0.0193,  0.0073,  ..., -0.0065, -0.0125, -0.0252],
        [ 0.0055,  0.0088, -0.0078,  ...,  0.0100, -0.0024,  0.0104],
        [ 0.0033, -0.0048, -0.0132,  ..., -0.0126,  0.0111,  0.0088],
        ...,
        [-0.0005, -0.0233,  0.0027,  ...,  0.0049,  0.0190, -0.0153],
        [ 0.0103, -0.0071, -0.0018,  ...,  0.0099, -0.0013, -0.0106],
        [-0.0005,  0.0274,  0.0056,  ..., -0.0123, -0.0051, -0.0095]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.7930,  6.5977,  2.7109,  ..., -2.8066, -1.1904,  2.3594]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 21:28:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of facility is facilities
The plural form of authority is authorities
The plural form of story is stories
The plural form of majority is majorities
The plural form of policy is policies
The plural form of datum is data
The plural form of child is children
The plural form of safety is
2024-07-16 21:28:42 root INFO     [order_1_approx] starting weight calculation for The plural form of facility is facilities
The plural form of safety is safeties
The plural form of authority is authorities
The plural form of datum is data
The plural form of majority is majorities
The plural form of child is children
The plural form of story is stories
The plural form of policy is
2024-07-16 21:28:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 21:32:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([1.9395, 0.0811, 0.2158,  ..., 0.5405, 0.8262, 0.4336], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1996, -0.0312, -1.5566,  ...,  2.1914,  1.4756, -1.5625],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0147,  0.0028,  0.0180,  ...,  0.0040, -0.0143, -0.0166],
        [-0.0082, -0.0075,  0.0047,  ...,  0.0119,  0.0034, -0.0003],
        [ 0.0047, -0.0120, -0.0111,  ..., -0.0094,  0.0008,  0.0032],
        ...,
        [-0.0028, -0.0078, -0.0023,  ...,  0.0015, -0.0032, -0.0123],
        [-0.0016, -0.0006, -0.0040,  ..., -0.0064, -0.0306,  0.0098],
        [ 0.0009,  0.0055,  0.0107,  ..., -0.0123, -0.0002, -0.0140]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5537, -0.1147, -1.6826,  ...,  1.9570,  1.2686, -1.0127]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 21:32:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of facility is facilities
The plural form of safety is safeties
The plural form of authority is authorities
The plural form of datum is data
The plural form of majority is majorities
The plural form of child is children
The plural form of story is stories
The plural form of policy is
2024-07-16 21:32:07 root INFO     [order_1_approx] starting weight calculation for The plural form of majority is majorities
The plural form of facility is facilities
The plural form of safety is safeties
The plural form of child is children
The plural form of policy is policies
The plural form of datum is data
The plural form of story is stories
The plural form of authority is
2024-07-16 21:32:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 21:35:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7891, -0.8613,  1.2510,  ..., -1.1582, -1.1406,  0.9673],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.4434, -0.2949, -0.1006,  ..., -0.5000, -1.3145,  1.1445],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0021,  0.0124,  0.0093,  ..., -0.0025, -0.0034, -0.0135],
        [ 0.0096, -0.0108, -0.0024,  ...,  0.0107,  0.0042, -0.0070],
        [ 0.0037,  0.0021, -0.0116,  ..., -0.0046, -0.0044,  0.0066],
        ...,
        [ 0.0041, -0.0220, -0.0022,  ..., -0.0037,  0.0010, -0.0111],
        [ 0.0067, -0.0052,  0.0042,  ..., -0.0028, -0.0037,  0.0083],
        [-0.0065,  0.0126,  0.0109,  ..., -0.0110,  0.0028, -0.0008]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3613,  0.0120, -0.5068,  ..., -0.9434, -0.9829,  0.6152]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 21:35:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of majority is majorities
The plural form of facility is facilities
The plural form of safety is safeties
The plural form of child is children
The plural form of policy is policies
The plural form of datum is data
The plural form of story is stories
The plural form of authority is
2024-07-16 21:35:32 root INFO     [order_1_approx] starting weight calculation for The plural form of child is children
The plural form of authority is authorities
The plural form of safety is safeties
The plural form of story is stories
The plural form of majority is majorities
The plural form of policy is policies
The plural form of facility is facilities
The plural form of datum is
2024-07-16 21:35:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 21:38:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8105,  0.3667,  1.6416,  ..., -1.6787,  1.4561,  2.5605],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.3672, -0.2529, -1.8086,  ..., -2.8301,  0.0795,  8.7266],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0092, -0.0132,  0.0123,  ...,  0.0108,  0.0015, -0.0233],
        [-0.0075,  0.0114,  0.0102,  ...,  0.0127,  0.0183, -0.0021],
        [ 0.0098, -0.0064, -0.0159,  ..., -0.0179,  0.0031,  0.0093],
        ...,
        [-0.0003, -0.0038,  0.0035,  ...,  0.0056,  0.0073, -0.0074],
        [ 0.0135,  0.0024,  0.0045,  ...,  0.0036, -0.0192,  0.0059],
        [-0.0026,  0.0043,  0.0037,  ..., -0.0083,  0.0087,  0.0060]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5381, -0.2908, -1.5283,  ..., -3.0020,  0.1791,  8.6641]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 21:38:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of child is children
The plural form of authority is authorities
The plural form of safety is safeties
The plural form of story is stories
The plural form of majority is majorities
The plural form of policy is policies
The plural form of facility is facilities
The plural form of datum is
2024-07-16 21:38:57 root INFO     [order_1_approx] starting weight calculation for The plural form of datum is data
The plural form of policy is policies
The plural form of facility is facilities
The plural form of safety is safeties
The plural form of story is stories
The plural form of majority is majorities
The plural form of authority is authorities
The plural form of child is
2024-07-16 21:38:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 21:42:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.8242, -0.1750,  0.2954,  ..., -0.1747, -0.0447,  1.2402],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5508, -3.8770, -3.6602,  ..., -0.9434,  3.4316,  2.2480],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-9.5367e-03, -6.0081e-05,  1.8906e-02,  ...,  1.2131e-03,
         -1.4038e-03, -1.8417e-02],
        [ 1.0849e-02, -1.8600e-02, -8.6746e-03,  ...,  9.5978e-03,
          8.5526e-03, -1.1002e-02],
        [ 1.9806e-02, -1.0414e-03, -2.3193e-02,  ..., -1.8066e-02,
          7.3471e-03, -3.2921e-03],
        ...,
        [ 1.0689e-02,  6.0654e-04, -5.8136e-03,  ..., -4.1313e-03,
          2.0885e-04, -1.0193e-02],
        [-1.6846e-02,  2.1667e-03,  4.4022e-03,  ...,  5.1918e-03,
         -1.5228e-02, -9.6436e-03],
        [-6.6528e-03,  1.5381e-02, -9.6664e-03,  ..., -5.9128e-03,
         -4.6043e-03, -1.0223e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.4766, -3.5293, -4.0781,  ..., -1.2568,  2.7500,  2.1719]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 21:42:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of datum is data
The plural form of policy is policies
The plural form of facility is facilities
The plural form of safety is safeties
The plural form of story is stories
The plural form of majority is majorities
The plural form of authority is authorities
The plural form of child is
2024-07-16 21:42:22 root INFO     total operator prediction time: 1643.6071684360504 seconds
2024-07-16 21:42:22 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on Ving - verb_inf
2024-07-16 21:42:22 root INFO     building operator Ving - verb_inf
2024-07-16 21:42:22 root INFO     [order_1_approx] starting weight calculation for expecting is the active form of expect
maintaining is the active form of maintain
encouraging is the active form of encourage
performing is the active form of perform
considering is the active form of consider
developing is the active form of develop
preventing is the active form of prevent
operating is the active form of
2024-07-16 21:42:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 21:45:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6143, -0.6641,  1.2266,  ..., -0.5312,  1.4023,  0.7812],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.2031, -1.4111,  0.7656,  ..., -1.3242,  0.4092,  4.3555],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0136,  0.0003,  0.0084,  ...,  0.0066, -0.0108, -0.0109],
        [-0.0101, -0.0133, -0.0119,  ...,  0.0061, -0.0046, -0.0040],
        [-0.0055, -0.0051, -0.0225,  ..., -0.0017,  0.0083, -0.0178],
        ...,
        [-0.0116,  0.0062,  0.0058,  ..., -0.0057, -0.0036,  0.0013],
        [ 0.0017, -0.0041, -0.0002,  ..., -0.0069, -0.0276,  0.0085],
        [-0.0046,  0.0233,  0.0090,  ..., -0.0032,  0.0036, -0.0184]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.2227, -0.9111,  0.4451,  ..., -1.2500,  1.1650,  4.4766]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 21:45:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for expecting is the active form of expect
maintaining is the active form of maintain
encouraging is the active form of encourage
performing is the active form of perform
considering is the active form of consider
developing is the active form of develop
preventing is the active form of prevent
operating is the active form of
2024-07-16 21:45:51 root INFO     [order_1_approx] starting weight calculation for developing is the active form of develop
encouraging is the active form of encourage
operating is the active form of operate
maintaining is the active form of maintain
expecting is the active form of expect
preventing is the active form of prevent
performing is the active form of perform
considering is the active form of
2024-07-16 21:45:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 21:49:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.9961,  0.2644,  0.6938,  ..., -0.8506,  1.6270,  1.2100],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.1758,  0.3623, -0.2344,  ..., -3.8555, -0.6792, -3.0176],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0083, -0.0083,  0.0151,  ..., -0.0089, -0.0036, -0.0107],
        [-0.0028, -0.0036, -0.0039,  ..., -0.0009, -0.0027, -0.0060],
        [ 0.0094,  0.0092, -0.0068,  ..., -0.0049,  0.0065,  0.0009],
        ...,
        [-0.0025, -0.0146, -0.0052,  ..., -0.0125, -0.0090, -0.0010],
        [-0.0047,  0.0068,  0.0053,  ..., -0.0078, -0.0217,  0.0224],
        [-0.0027,  0.0357,  0.0199,  ..., -0.0036, -0.0126, -0.0218]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.0195,  0.4773, -0.1714,  ..., -3.4629, -0.4124, -2.7109]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 21:49:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for developing is the active form of develop
encouraging is the active form of encourage
operating is the active form of operate
maintaining is the active form of maintain
expecting is the active form of expect
preventing is the active form of prevent
performing is the active form of perform
considering is the active form of
2024-07-16 21:49:19 root INFO     [order_1_approx] starting weight calculation for maintaining is the active form of maintain
expecting is the active form of expect
operating is the active form of operate
considering is the active form of consider
developing is the active form of develop
preventing is the active form of prevent
performing is the active form of perform
encouraging is the active form of
2024-07-16 21:49:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 21:52:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([1.8086, 0.2703, 1.0898,  ..., 1.0732, 2.7695, 0.5020], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7158, -1.4277,  0.7539,  ..., -1.0869, -0.7549,  1.6289],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0101,  0.0016,  0.0275,  ..., -0.0132, -0.0228, -0.0123],
        [-0.0042, -0.0190, -0.0134,  ...,  0.0084,  0.0016,  0.0004],
        [-0.0099,  0.0005, -0.0262,  ...,  0.0053,  0.0183, -0.0019],
        ...,
        [-0.0133, -0.0288, -0.0121,  ..., -0.0150, -0.0002, -0.0108],
        [-0.0111,  0.0215,  0.0078,  ..., -0.0091, -0.0428,  0.0293],
        [-0.0019,  0.0188,  0.0229,  ...,  0.0016, -0.0060, -0.0271]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7798, -1.4570,  0.6641,  ..., -0.4746, -0.3115,  1.5566]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 21:52:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for maintaining is the active form of maintain
expecting is the active form of expect
operating is the active form of operate
considering is the active form of consider
developing is the active form of develop
preventing is the active form of prevent
performing is the active form of perform
encouraging is the active form of
2024-07-16 21:52:47 root INFO     [order_1_approx] starting weight calculation for encouraging is the active form of encourage
preventing is the active form of prevent
operating is the active form of operate
considering is the active form of consider
performing is the active form of perform
expecting is the active form of expect
developing is the active form of develop
maintaining is the active form of
2024-07-16 21:52:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 21:56:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1211, -0.5537,  1.1465,  ..., -0.6377,  0.9673,  0.1980],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.6523, -2.4824, -6.2812,  ..., -0.3223,  2.2656,  4.1055],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.9608e-03,  1.2375e-02,  1.4206e-02,  ..., -8.6060e-03,
         -7.3814e-04, -1.5869e-02],
        [-1.5764e-03,  5.3329e-03,  3.7918e-03,  ..., -3.9902e-03,
         -5.4932e-03,  5.1212e-04],
        [ 2.0542e-03,  1.9424e-02,  6.5918e-03,  ..., -9.9716e-03,
         -2.5249e-04,  3.2272e-03],
        ...,
        [-1.3741e-02, -7.0038e-03, -2.0523e-03,  ...,  4.6425e-03,
         -8.1711e-03, -1.3924e-03],
        [ 1.8702e-03, -2.4796e-05, -4.9820e-03,  ..., -8.9798e-03,
         -1.4038e-02,  9.3231e-03],
        [ 1.5945e-03,  1.7563e-02,  7.5111e-03,  ..., -9.8190e-03,
          2.3365e-05, -1.4496e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.3125, -2.6992, -7.0469,  ...,  0.1616,  2.3809,  4.2461]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 21:56:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for encouraging is the active form of encourage
preventing is the active form of prevent
operating is the active form of operate
considering is the active form of consider
performing is the active form of perform
expecting is the active form of expect
developing is the active form of develop
maintaining is the active form of
2024-07-16 21:56:15 root INFO     [order_1_approx] starting weight calculation for expecting is the active form of expect
encouraging is the active form of encourage
considering is the active form of consider
performing is the active form of perform
preventing is the active form of prevent
maintaining is the active form of maintain
operating is the active form of operate
developing is the active form of
2024-07-16 21:56:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 21:59:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.7930, -0.8613,  1.1836,  ...,  1.2363,  1.1963,  1.1045],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.1426, -2.3340,  0.7734,  ..., -3.1797,  2.3008, -1.2871],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0102, -0.0023,  0.0078,  ..., -0.0041, -0.0039, -0.0117],
        [-0.0096, -0.0129, -0.0106,  ...,  0.0046,  0.0043, -0.0094],
        [-0.0017,  0.0002, -0.0080,  ..., -0.0132,  0.0083, -0.0131],
        ...,
        [-0.0079,  0.0039,  0.0008,  ..., -0.0023, -0.0076,  0.0036],
        [-0.0014, -0.0044, -0.0009,  ...,  0.0017, -0.0203,  0.0079],
        [-0.0019,  0.0186,  0.0128,  ..., -0.0026, -0.0029, -0.0035]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1504, -2.2012,  1.6846,  ..., -3.0352,  2.5547, -1.4609]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 21:59:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for expecting is the active form of expect
encouraging is the active form of encourage
considering is the active form of consider
performing is the active form of perform
preventing is the active form of prevent
maintaining is the active form of maintain
operating is the active form of operate
developing is the active form of
2024-07-16 21:59:42 root INFO     [order_1_approx] starting weight calculation for considering is the active form of consider
encouraging is the active form of encourage
developing is the active form of develop
expecting is the active form of expect
maintaining is the active form of maintain
operating is the active form of operate
preventing is the active form of prevent
performing is the active form of
2024-07-16 21:59:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 22:03:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6606, -0.0212,  0.9688,  ..., -1.0039,  1.7695, -1.0195],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 6.1641, -1.7119,  0.8916,  ...,  0.6777,  1.3271,  0.6084],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0005, -0.0115,  0.0053,  ..., -0.0085, -0.0134, -0.0143],
        [-0.0080, -0.0057, -0.0074,  ...,  0.0131, -0.0021, -0.0098],
        [-0.0072,  0.0119, -0.0159,  ..., -0.0062, -0.0007, -0.0135],
        ...,
        [-0.0148,  0.0061, -0.0038,  ..., -0.0002, -0.0228, -0.0185],
        [-0.0166,  0.0100,  0.0013,  ..., -0.0077, -0.0206,  0.0094],
        [-0.0071,  0.0274,  0.0047,  ..., -0.0036, -0.0019, -0.0081]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.8984, -1.4492,  1.1641,  ...,  1.0293,  1.8301,  0.9697]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 22:03:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for considering is the active form of consider
encouraging is the active form of encourage
developing is the active form of develop
expecting is the active form of expect
maintaining is the active form of maintain
operating is the active form of operate
preventing is the active form of prevent
performing is the active form of
2024-07-16 22:03:10 root INFO     [order_1_approx] starting weight calculation for encouraging is the active form of encourage
preventing is the active form of prevent
performing is the active form of perform
considering is the active form of consider
operating is the active form of operate
developing is the active form of develop
maintaining is the active form of maintain
expecting is the active form of
2024-07-16 22:03:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 22:06:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.6680, -0.0371, -0.0649,  ..., -0.3599,  2.8926,  1.0488],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.6953, -1.2588,  0.6602,  ..., -1.7061,  0.1475,  2.2656],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0029, -0.0030,  0.0160,  ...,  0.0033, -0.0211, -0.0030],
        [-0.0046, -0.0059,  0.0015,  ...,  0.0108,  0.0017,  0.0009],
        [ 0.0049,  0.0121, -0.0131,  ..., -0.0066, -0.0105,  0.0061],
        ...,
        [-0.0155, -0.0040, -0.0046,  ..., -0.0010, -0.0164, -0.0011],
        [ 0.0058, -0.0010, -0.0036,  ..., -0.0128, -0.0268,  0.0224],
        [-0.0050,  0.0117,  0.0146,  ..., -0.0019, -0.0058, -0.0157]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.5273, -1.2949,  1.0967,  ..., -1.7178,  0.5010,  2.6621]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 22:06:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for encouraging is the active form of encourage
preventing is the active form of prevent
performing is the active form of perform
considering is the active form of consider
operating is the active form of operate
developing is the active form of develop
maintaining is the active form of maintain
expecting is the active form of
2024-07-16 22:06:39 root INFO     [order_1_approx] starting weight calculation for performing is the active form of perform
maintaining is the active form of maintain
encouraging is the active form of encourage
operating is the active form of operate
considering is the active form of consider
developing is the active form of develop
expecting is the active form of expect
preventing is the active form of
2024-07-16 22:06:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 22:10:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.3516,  1.0459,  0.5132,  ..., -0.1201,  2.0859,  0.2263],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.8301, -2.3398, -3.0605,  ...,  0.0425, -0.0254,  2.4688],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0008, -0.0032,  0.0156,  ..., -0.0098, -0.0130, -0.0012],
        [ 0.0035,  0.0033,  0.0042,  ..., -0.0029, -0.0038, -0.0019],
        [ 0.0014, -0.0005,  0.0075,  ..., -0.0205,  0.0076, -0.0034],
        ...,
        [-0.0122,  0.0035,  0.0004,  ..., -0.0102, -0.0010,  0.0023],
        [-0.0039,  0.0059, -0.0021,  ..., -0.0080, -0.0195,  0.0052],
        [-0.0073,  0.0161,  0.0121,  ..., -0.0150,  0.0044, -0.0164]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.0195, -2.8926, -2.8613,  ...,  0.2303,  0.1787,  3.1211]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 22:10:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for performing is the active form of perform
maintaining is the active form of maintain
encouraging is the active form of encourage
operating is the active form of operate
considering is the active form of consider
developing is the active form of develop
expecting is the active form of expect
preventing is the active form of
2024-07-16 22:10:07 root INFO     total operator prediction time: 1664.7061207294464 seconds
2024-07-16 22:10:07 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_Ving - Ved
2024-07-16 22:10:07 root INFO     building operator verb_Ving - Ved
2024-07-16 22:10:07 root INFO     [order_1_approx] starting weight calculation for After something is publishing, it has published
After something is reducing, it has reduced
After something is improving, it has improved
After something is involving, it has involved
After something is locating, it has located
After something is creating, it has created
After something is providing, it has provided
After something is including, it has
2024-07-16 22:10:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 22:13:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5361, -0.6167,  1.0684,  ...,  0.9272,  0.5146,  1.7168],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.5703,  1.5469,  0.1445,  ...,  3.0410, -1.1426,  0.3447],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0063, -0.0206,  0.0050,  ..., -0.0037,  0.0020, -0.0085],
        [-0.0022, -0.0022,  0.0030,  ...,  0.0133, -0.0047,  0.0021],
        [-0.0061, -0.0072,  0.0017,  ..., -0.0111, -0.0084,  0.0024],
        ...,
        [-0.0054, -0.0039, -0.0057,  ...,  0.0088, -0.0019, -0.0003],
        [-0.0037,  0.0128,  0.0004,  ..., -0.0053, -0.0170, -0.0019],
        [-0.0121,  0.0192, -0.0063,  ..., -0.0141, -0.0065, -0.0053]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7222,  1.3809, -0.0852,  ...,  3.4062, -1.3945,  1.4473]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 22:13:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is publishing, it has published
After something is reducing, it has reduced
After something is improving, it has improved
After something is involving, it has involved
After something is locating, it has located
After something is creating, it has created
After something is providing, it has provided
After something is including, it has
2024-07-16 22:13:36 root INFO     [order_1_approx] starting weight calculation for After something is improving, it has improved
After something is publishing, it has published
After something is creating, it has created
After something is reducing, it has reduced
After something is providing, it has provided
After something is locating, it has located
After something is including, it has included
After something is involving, it has
2024-07-16 22:13:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 22:17:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([1.5254, 0.0073, 1.0264,  ..., 0.3718, 1.0078, 1.3213], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([1.2148, 0.5767, 0.7188,  ..., 4.1367, 1.4893, 1.2832], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0130, -0.0166,  0.0180,  ..., -0.0048, -0.0015, -0.0111],
        [-0.0085, -0.0177, -0.0038,  ...,  0.0112, -0.0057,  0.0017],
        [-0.0018, -0.0062, -0.0171,  ..., -0.0133,  0.0038,  0.0109],
        ...,
        [-0.0097,  0.0008, -0.0055,  ...,  0.0099, -0.0139, -0.0043],
        [-0.0098,  0.0076,  0.0064,  ...,  0.0017, -0.0245,  0.0035],
        [-0.0023,  0.0218,  0.0097,  ..., -0.0165, -0.0009, -0.0265]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[1.3232, 0.0493, 0.9043,  ..., 5.0508, 1.3203, 0.7153]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 22:17:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is improving, it has improved
After something is publishing, it has published
After something is creating, it has created
After something is reducing, it has reduced
After something is providing, it has provided
After something is locating, it has located
After something is including, it has included
After something is involving, it has
2024-07-16 22:17:04 root INFO     [order_1_approx] starting weight calculation for After something is locating, it has located
After something is improving, it has improved
After something is involving, it has involved
After something is providing, it has provided
After something is including, it has included
After something is reducing, it has reduced
After something is publishing, it has published
After something is creating, it has
2024-07-16 22:17:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 22:20:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5742, -0.5479,  0.0770,  ...,  1.8125,  1.0020, -0.5508],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7236,  1.7861,  2.7109,  ...,  1.0928, -1.9844,  0.3076],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0048, -0.0077,  0.0045,  ..., -0.0062,  0.0046, -0.0213],
        [-0.0011, -0.0091, -0.0057,  ...,  0.0036, -0.0075, -0.0100],
        [-0.0040, -0.0018, -0.0081,  ...,  0.0010,  0.0082,  0.0036],
        ...,
        [-0.0057,  0.0172, -0.0031,  ..., -0.0038, -0.0012, -0.0075],
        [-0.0146,  0.0040, -0.0048,  ..., -0.0073, -0.0316,  0.0094],
        [-0.0162,  0.0163,  0.0009,  ..., -0.0058, -0.0085, -0.0295]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.3379,  1.9414,  2.5254,  ...,  1.5283, -1.9092,  0.3020]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 22:20:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is locating, it has located
After something is improving, it has improved
After something is involving, it has involved
After something is providing, it has provided
After something is including, it has included
After something is reducing, it has reduced
After something is publishing, it has published
After something is creating, it has
2024-07-16 22:20:33 root INFO     [order_1_approx] starting weight calculation for After something is including, it has included
After something is providing, it has provided
After something is reducing, it has reduced
After something is creating, it has created
After something is improving, it has improved
After something is involving, it has involved
After something is publishing, it has published
After something is locating, it has
2024-07-16 22:20:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 22:24:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3301, -0.1370, -0.1466,  ..., -0.0298,  0.3979, -0.1547],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([1.8340, 3.2578, 0.6943,  ..., 0.4404, 0.7261, 1.6123], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0128, -0.0227,  0.0083,  ...,  0.0035, -0.0046, -0.0223],
        [-0.0065, -0.0051, -0.0016,  ...,  0.0142,  0.0064, -0.0064],
        [-0.0080,  0.0018, -0.0042,  ..., -0.0045, -0.0012,  0.0042],
        ...,
        [-0.0143,  0.0041,  0.0039,  ..., -0.0020,  0.0049, -0.0010],
        [-0.0047, -0.0054, -0.0096,  ..., -0.0002, -0.0197,  0.0110],
        [-0.0066,  0.0067,  0.0030,  ..., -0.0106, -0.0013, -0.0171]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[1.2549, 3.1855, 1.1621,  ..., 0.5464, 0.6133, 2.1270]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 22:24:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is including, it has included
After something is providing, it has provided
After something is reducing, it has reduced
After something is creating, it has created
After something is improving, it has improved
After something is involving, it has involved
After something is publishing, it has published
After something is locating, it has
2024-07-16 22:24:03 root INFO     [order_1_approx] starting weight calculation for After something is providing, it has provided
After something is involving, it has involved
After something is publishing, it has published
After something is including, it has included
After something is improving, it has improved
After something is creating, it has created
After something is locating, it has located
After something is reducing, it has
2024-07-16 22:24:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 22:27:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2520, -1.5840,  0.4165,  ..., -1.0205,  0.3215,  0.4075],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0322,  2.5977,  1.8027,  ...,  2.0684, -0.2286,  0.8018],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0037, -0.0178,  0.0118,  ...,  0.0023, -0.0011, -0.0186],
        [ 0.0087, -0.0032, -0.0058,  ...,  0.0057,  0.0044,  0.0057],
        [-0.0057, -0.0013,  0.0002,  ..., -0.0089,  0.0015,  0.0078],
        ...,
        [-0.0055, -0.0027,  0.0084,  ..., -0.0014,  0.0024, -0.0097],
        [-0.0201, -0.0105, -0.0057,  ..., -0.0066, -0.0196,  0.0088],
        [-0.0042,  0.0049,  0.0042,  ..., -0.0036, -0.0072, -0.0039]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[0.2754, 2.6895, 1.5820,  ..., 2.6016, 0.1578, 1.0205]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 22:27:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is providing, it has provided
After something is involving, it has involved
After something is publishing, it has published
After something is including, it has included
After something is improving, it has improved
After something is creating, it has created
After something is locating, it has located
After something is reducing, it has
2024-07-16 22:27:31 root INFO     [order_1_approx] starting weight calculation for After something is reducing, it has reduced
After something is publishing, it has published
After something is improving, it has improved
After something is creating, it has created
After something is including, it has included
After something is involving, it has involved
After something is locating, it has located
After something is providing, it has
2024-07-16 22:27:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 22:30:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1230, -0.6411,  0.0380,  ..., -0.0991,  0.6006,  1.7559],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7383,  2.2500,  1.2246,  ...,  1.6416, -0.7354,  0.4707],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0009, -0.0149,  0.0065,  ..., -0.0037, -0.0069, -0.0182],
        [ 0.0010, -0.0037,  0.0037,  ...,  0.0052, -0.0087, -0.0059],
        [-0.0076, -0.0129,  0.0075,  ..., -0.0186,  0.0051,  0.0061],
        ...,
        [-0.0132, -0.0122, -0.0170,  ...,  0.0009, -0.0123, -0.0048],
        [-0.0015, -0.0100, -0.0004,  ..., -0.0047, -0.0244,  0.0194],
        [ 0.0019,  0.0058,  0.0014,  ..., -0.0069,  0.0049, -0.0036]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1113,  2.1523,  1.9082,  ...,  2.7656, -1.4385,  0.6187]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 22:30:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is reducing, it has reduced
After something is publishing, it has published
After something is improving, it has improved
After something is creating, it has created
After something is including, it has included
After something is involving, it has involved
After something is locating, it has located
After something is providing, it has
2024-07-16 22:30:58 root INFO     [order_1_approx] starting weight calculation for After something is involving, it has involved
After something is locating, it has located
After something is publishing, it has published
After something is reducing, it has reduced
After something is including, it has included
After something is creating, it has created
After something is providing, it has provided
After something is improving, it has
2024-07-16 22:30:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 22:34:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2642, -0.8916,  1.0547,  ..., -0.5195,  1.3926, -0.4810],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 6.0000,  0.3921,  2.3809,  ...,  4.3359, -2.1445,  1.1377],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0033, -0.0137,  0.0077,  ..., -0.0087, -0.0027, -0.0152],
        [ 0.0061, -0.0098, -0.0088,  ...,  0.0008, -0.0041, -0.0061],
        [-0.0071, -0.0074,  0.0029,  ..., -0.0058,  0.0063,  0.0081],
        ...,
        [-0.0031,  0.0059,  0.0135,  ...,  0.0041, -0.0055, -0.0039],
        [ 0.0004, -0.0062, -0.0060,  ..., -0.0125, -0.0240, -0.0028],
        [-0.0133,  0.0230,  0.0076,  ...,  0.0055, -0.0083, -0.0207]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.3281,  0.4023,  2.2070,  ...,  5.0469, -2.6016,  0.7705]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 22:34:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is involving, it has involved
After something is locating, it has located
After something is publishing, it has published
After something is reducing, it has reduced
After something is including, it has included
After something is creating, it has created
After something is providing, it has provided
After something is improving, it has
2024-07-16 22:34:26 root INFO     [order_1_approx] starting weight calculation for After something is reducing, it has reduced
After something is providing, it has provided
After something is involving, it has involved
After something is creating, it has created
After something is improving, it has improved
After something is including, it has included
After something is locating, it has located
After something is publishing, it has
2024-07-16 22:34:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 22:37:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.7383, -0.6685,  1.8672,  ...,  1.8291,  0.5156,  1.1738],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2493, -0.0474, -0.0479,  ...,  6.4414, -2.0098,  1.1367],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0050, -0.0108,  0.0187,  ...,  0.0010, -0.0064, -0.0106],
        [ 0.0029,  0.0029, -0.0106,  ...,  0.0087,  0.0009, -0.0079],
        [-0.0032, -0.0047, -0.0093,  ..., -0.0154,  0.0038,  0.0033],
        ...,
        [-0.0185, -0.0039, -0.0130,  ...,  0.0032, -0.0169, -0.0119],
        [-0.0199,  0.0052,  0.0035,  ..., -0.0008, -0.0206,  0.0156],
        [-0.0097,  0.0105, -0.0084,  ..., -0.0086,  0.0032, -0.0207]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9229, -0.0999,  0.0935,  ...,  6.7930, -1.9238,  1.1504]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 22:37:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is reducing, it has reduced
After something is providing, it has provided
After something is involving, it has involved
After something is creating, it has created
After something is improving, it has improved
After something is including, it has included
After something is locating, it has located
After something is publishing, it has
2024-07-16 22:37:55 root INFO     total operator prediction time: 1668.516954421997 seconds
2024-07-16 22:37:55 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_inf - Ved
2024-07-16 22:37:55 root INFO     building operator verb_inf - Ved
2024-07-16 22:37:55 root INFO     [order_1_approx] starting weight calculation for If the present form is receive, the past form is received
If the present form is consider, the past form is considered
If the present form is describe, the past form is described
If the present form is include, the past form is included
If the present form is achieve, the past form is achieved
If the present form is expect, the past form is expected
If the present form is apply, the past form is applied
If the present form is discover, the past form is
2024-07-16 22:37:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 22:41:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4648, -0.3501,  0.6172,  ...,  0.1284,  0.0757,  0.1375],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8242,  0.6338, -1.2676,  ..., -2.2422, -4.4023,  1.5928],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0067, -0.0089,  0.0035,  ..., -0.0011, -0.0032, -0.0074],
        [-0.0103, -0.0107,  0.0029,  ...,  0.0018,  0.0103,  0.0040],
        [-0.0060, -0.0103, -0.0205,  ..., -0.0030,  0.0058,  0.0007],
        ...,
        [-0.0046,  0.0076, -0.0059,  ..., -0.0004,  0.0045, -0.0041],
        [-0.0090, -0.0004,  0.0004,  ..., -0.0020, -0.0284, -0.0043],
        [-0.0105,  0.0041,  0.0068,  ..., -0.0082,  0.0080, -0.0204]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1260,  0.3401, -1.3252,  ..., -2.1680, -4.2695,  2.2148]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 22:41:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is receive, the past form is received
If the present form is consider, the past form is considered
If the present form is describe, the past form is described
If the present form is include, the past form is included
If the present form is achieve, the past form is achieved
If the present form is expect, the past form is expected
If the present form is apply, the past form is applied
If the present form is discover, the past form is
2024-07-16 22:41:22 root INFO     [order_1_approx] starting weight calculation for If the present form is expect, the past form is expected
If the present form is achieve, the past form is achieved
If the present form is consider, the past form is considered
If the present form is describe, the past form is described
If the present form is receive, the past form is received
If the present form is apply, the past form is applied
If the present form is discover, the past form is discovered
If the present form is include, the past form is
2024-07-16 22:41:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 22:44:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1953, -0.0952,  0.5522,  ...,  1.0410,  0.7764,  0.9844],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1875, -0.2383, -1.4697,  ...,  2.6699, -0.2891, -0.2634],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0086, -0.0009, -0.0025,  ...,  0.0009,  0.0075, -0.0011],
        [-0.0073, -0.0023,  0.0043,  ...,  0.0136,  0.0065,  0.0034],
        [-0.0023, -0.0120, -0.0066,  ..., -0.0101, -0.0089, -0.0083],
        ...,
        [-0.0011,  0.0024, -0.0089,  ...,  0.0007,  0.0007,  0.0080],
        [ 0.0048, -0.0023,  0.0013,  ..., -0.0015, -0.0167,  0.0016],
        [-0.0112,  0.0120, -0.0043,  ..., -0.0117,  0.0022, -0.0077]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3125, -0.1799, -1.6660,  ...,  2.7031, -1.2061, -0.2778]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 22:44:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is expect, the past form is expected
If the present form is achieve, the past form is achieved
If the present form is consider, the past form is considered
If the present form is describe, the past form is described
If the present form is receive, the past form is received
If the present form is apply, the past form is applied
If the present form is discover, the past form is discovered
If the present form is include, the past form is
2024-07-16 22:44:48 root INFO     [order_1_approx] starting weight calculation for If the present form is describe, the past form is described
If the present form is achieve, the past form is achieved
If the present form is discover, the past form is discovered
If the present form is apply, the past form is applied
If the present form is receive, the past form is received
If the present form is expect, the past form is expected
If the present form is include, the past form is included
If the present form is consider, the past form is
2024-07-16 22:44:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 22:48:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.0039,  0.3838,  0.6494,  ..., -0.8145,  0.6768, -0.0947],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9766,  1.2383, -1.3086,  ...,  0.2861, -1.9697, -2.4414],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0093, -0.0027,  0.0086,  ...,  0.0027, -0.0034, -0.0063],
        [-0.0029, -0.0097,  0.0030,  ...,  0.0069,  0.0061, -0.0073],
        [-0.0099, -0.0081, -0.0128,  ..., -0.0099,  0.0049, -0.0058],
        ...,
        [-0.0010, -0.0006, -0.0035,  ..., -0.0019, -0.0002,  0.0074],
        [ 0.0004,  0.0012,  0.0016,  ..., -0.0030, -0.0177,  0.0081],
        [ 0.0009,  0.0088,  0.0062,  ...,  0.0003, -0.0033, -0.0189]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.8242,  1.2012, -1.3955,  ...,  0.4009, -2.1797, -3.1016]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 22:48:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is describe, the past form is described
If the present form is achieve, the past form is achieved
If the present form is discover, the past form is discovered
If the present form is apply, the past form is applied
If the present form is receive, the past form is received
If the present form is expect, the past form is expected
If the present form is include, the past form is included
If the present form is consider, the past form is
2024-07-16 22:48:16 root INFO     [order_1_approx] starting weight calculation for If the present form is achieve, the past form is achieved
If the present form is receive, the past form is received
If the present form is include, the past form is included
If the present form is consider, the past form is considered
If the present form is expect, the past form is expected
If the present form is describe, the past form is described
If the present form is discover, the past form is discovered
If the present form is apply, the past form is
2024-07-16 22:48:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 22:51:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.4785, -0.7427,  1.3906,  ..., -0.1204, -0.3882, -0.5151],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5918,  2.6172, -0.9380,  ...,  1.0020, -0.5303, -0.4436],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0118, -0.0062,  0.0024,  ...,  0.0025,  0.0017, -0.0075],
        [-0.0050, -0.0088,  0.0048,  ...,  0.0108, -0.0067, -0.0042],
        [-0.0004,  0.0011, -0.0200,  ..., -0.0067, -0.0071, -0.0096],
        ...,
        [-0.0010,  0.0086, -0.0078,  ..., -0.0142,  0.0005, -0.0054],
        [ 0.0010, -0.0081, -0.0023,  ..., -0.0092, -0.0247,  0.0066],
        [-0.0083,  0.0219,  0.0017,  ..., -0.0038, -0.0094, -0.0192]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.1250,  2.4785, -1.6240,  ...,  0.4937, -1.0703, -0.7148]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 22:51:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is achieve, the past form is achieved
If the present form is receive, the past form is received
If the present form is include, the past form is included
If the present form is consider, the past form is considered
If the present form is expect, the past form is expected
If the present form is describe, the past form is described
If the present form is discover, the past form is discovered
If the present form is apply, the past form is
2024-07-16 22:51:41 root INFO     [order_1_approx] starting weight calculation for If the present form is receive, the past form is received
If the present form is apply, the past form is applied
If the present form is consider, the past form is considered
If the present form is describe, the past form is described
If the present form is discover, the past form is discovered
If the present form is achieve, the past form is achieved
If the present form is include, the past form is included
If the present form is expect, the past form is
2024-07-16 22:51:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 22:55:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.7373,  1.5215,  0.6172,  ...,  0.0361,  2.1406, -0.2383],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.2852, -0.2324, -1.2539,  ...,  0.7246, -1.7598,  1.2871],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0094, -0.0106,  0.0005,  ..., -0.0014, -0.0084, -0.0138],
        [-0.0077, -0.0200, -0.0029,  ...,  0.0113, -0.0042, -0.0052],
        [-0.0120, -0.0059, -0.0168,  ...,  0.0019, -0.0060, -0.0023],
        ...,
        [-0.0056,  0.0058, -0.0022,  ..., -0.0075,  0.0054, -0.0003],
        [ 0.0047,  0.0034, -0.0044,  ..., -0.0073, -0.0295, -0.0035],
        [-0.0053,  0.0195,  0.0010,  ..., -0.0127,  0.0077, -0.0176]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3262, -0.2209, -0.4521,  ...,  0.4355, -2.3301,  1.1621]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 22:55:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is receive, the past form is received
If the present form is apply, the past form is applied
If the present form is consider, the past form is considered
If the present form is describe, the past form is described
If the present form is discover, the past form is discovered
If the present form is achieve, the past form is achieved
If the present form is include, the past form is included
If the present form is expect, the past form is
2024-07-16 22:55:09 root INFO     [order_1_approx] starting weight calculation for If the present form is consider, the past form is considered
If the present form is discover, the past form is discovered
If the present form is include, the past form is included
If the present form is receive, the past form is received
If the present form is expect, the past form is expected
If the present form is achieve, the past form is achieved
If the present form is apply, the past form is applied
If the present form is describe, the past form is
2024-07-16 22:55:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 22:58:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.4453, -0.0571,  2.0098,  ..., -0.1602, -0.2859, -0.8467],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8564, -2.8125,  0.6807,  ...,  0.1455, -0.0762,  0.8301],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0134, -0.0089,  0.0010,  ..., -0.0057, -0.0038, -0.0006],
        [-0.0048, -0.0111,  0.0095,  ..., -0.0098, -0.0023, -0.0056],
        [-0.0104,  0.0012, -0.0131,  ..., -0.0117, -0.0028, -0.0004],
        ...,
        [-0.0094,  0.0022, -0.0013,  ..., -0.0099,  0.0086, -0.0043],
        [-0.0047, -0.0062,  0.0006,  ..., -0.0012, -0.0219, -0.0018],
        [-0.0115,  0.0072,  0.0104,  ..., -0.0010,  0.0010, -0.0109]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4150, -2.9668,  1.0059,  ..., -0.6284, -0.4236,  0.8320]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 22:58:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is consider, the past form is considered
If the present form is discover, the past form is discovered
If the present form is include, the past form is included
If the present form is receive, the past form is received
If the present form is expect, the past form is expected
If the present form is achieve, the past form is achieved
If the present form is apply, the past form is applied
If the present form is describe, the past form is
2024-07-16 22:58:36 root INFO     [order_1_approx] starting weight calculation for If the present form is receive, the past form is received
If the present form is include, the past form is included
If the present form is apply, the past form is applied
If the present form is discover, the past form is discovered
If the present form is consider, the past form is considered
If the present form is describe, the past form is described
If the present form is expect, the past form is expected
If the present form is achieve, the past form is
2024-07-16 22:58:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 23:02:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4736, -0.7256,  2.7012,  ..., -0.7500,  0.9077, -0.4849],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7695,  2.9844, -1.3574,  ..., -4.3203,  0.0234, -1.9863],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0176,  0.0074,  0.0053,  ..., -0.0056, -0.0028, -0.0195],
        [-0.0068, -0.0081, -0.0103,  ...,  0.0142,  0.0019, -0.0069],
        [-0.0142, -0.0093, -0.0226,  ..., -0.0015, -0.0098, -0.0048],
        ...,
        [ 0.0061,  0.0049, -0.0060,  ..., -0.0134,  0.0046, -0.0032],
        [-0.0016,  0.0004, -0.0061,  ..., -0.0027, -0.0203,  0.0028],
        [-0.0114,  0.0138,  0.0102,  ..., -0.0016,  0.0017, -0.0182]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.6768,  2.9902, -0.6411,  ..., -3.8672, -1.2031, -1.8291]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 23:02:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is receive, the past form is received
If the present form is include, the past form is included
If the present form is apply, the past form is applied
If the present form is discover, the past form is discovered
If the present form is consider, the past form is considered
If the present form is describe, the past form is described
If the present form is expect, the past form is expected
If the present form is achieve, the past form is
2024-07-16 23:02:04 root INFO     [order_1_approx] starting weight calculation for If the present form is apply, the past form is applied
If the present form is include, the past form is included
If the present form is expect, the past form is expected
If the present form is consider, the past form is considered
If the present form is describe, the past form is described
If the present form is discover, the past form is discovered
If the present form is achieve, the past form is achieved
If the present form is receive, the past form is
2024-07-16 23:02:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 23:05:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4492, -1.6455,  1.2227,  ...,  0.4153,  0.1846,  2.2383],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.5176,  0.6450, -1.0312,  ...,  1.6025,  0.7471,  2.8750],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0033, -0.0037, -0.0083,  ...,  0.0006,  0.0035, -0.0168],
        [-0.0110, -0.0159,  0.0045,  ...,  0.0013,  0.0038, -0.0013],
        [-0.0049, -0.0058, -0.0181,  ..., -0.0051, -0.0034, -0.0009],
        ...,
        [ 0.0037,  0.0132, -0.0166,  ..., -0.0126, -0.0003, -0.0023],
        [ 0.0028, -0.0062,  0.0018,  ..., -0.0107, -0.0294,  0.0052],
        [-0.0107,  0.0014, -0.0008,  ..., -0.0149,  0.0068, -0.0162]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3896,  0.9082, -1.1533,  ...,  0.7070, -0.9463,  2.4590]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 23:05:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is apply, the past form is applied
If the present form is include, the past form is included
If the present form is expect, the past form is expected
If the present form is consider, the past form is considered
If the present form is describe, the past form is described
If the present form is discover, the past form is discovered
If the present form is achieve, the past form is achieved
If the present form is receive, the past form is
2024-07-16 23:05:32 root INFO     total operator prediction time: 1656.4545214176178 seconds
2024-07-16 23:05:32 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_inf - 3pSg
2024-07-16 23:05:32 root INFO     building operator verb_inf - 3pSg
2024-07-16 23:05:32 root INFO     [order_1_approx] starting weight calculation for I hear, he hears
I ask, he asks
I describe, he describes
I identify, he identifies
I improve, he improves
I send, he sends
I become, he becomes
I represent, he
2024-07-16 23:05:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 23:08:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1895, -0.8257,  1.6367,  ..., -0.7896,  0.3281,  1.6152],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.5391, -3.2949, -0.2354,  ...,  1.6729, -0.0918,  4.2266],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0124, -0.0033,  0.0062,  ..., -0.0010, -0.0158, -0.0099],
        [-0.0086, -0.0142,  0.0056,  ...,  0.0047, -0.0037, -0.0034],
        [ 0.0002, -0.0053, -0.0225,  ..., -0.0022, -0.0045,  0.0011],
        ...,
        [ 0.0006,  0.0064, -0.0127,  ..., -0.0054, -0.0002, -0.0032],
        [-0.0061,  0.0055,  0.0127,  ..., -0.0092, -0.0288,  0.0015],
        [-0.0025,  0.0148,  0.0068,  ..., -0.0124,  0.0074, -0.0157]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.1602, -2.5469, -0.6250,  ...,  2.1367, -0.5811,  4.4375]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 23:09:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I hear, he hears
I ask, he asks
I describe, he describes
I identify, he identifies
I improve, he improves
I send, he sends
I become, he becomes
I represent, he
2024-07-16 23:09:00 root INFO     [order_1_approx] starting weight calculation for I improve, he improves
I ask, he asks
I hear, he hears
I become, he becomes
I identify, he identifies
I represent, he represents
I send, he sends
I describe, he
2024-07-16 23:09:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 23:12:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2500, -0.0669,  2.2227,  ..., -1.2109, -0.7954,  0.2026],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.9648, -3.8516, -0.5391,  ...,  1.6045, -0.7998,  1.9453],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0256, -0.0095,  0.0105,  ..., -0.0008, -0.0146,  0.0007],
        [ 0.0021, -0.0162,  0.0067,  ...,  0.0042, -0.0060,  0.0010],
        [-0.0017,  0.0046, -0.0272,  ..., -0.0098,  0.0080, -0.0150],
        ...,
        [-0.0180, -0.0150, -0.0070,  ..., -0.0044, -0.0040, -0.0115],
        [-0.0105,  0.0080,  0.0168,  ..., -0.0157, -0.0336,  0.0208],
        [-0.0021,  0.0292,  0.0131,  ..., -0.0130,  0.0074, -0.0213]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6084, -3.8984, -0.6333,  ...,  1.5254,  0.3359,  1.6582]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 23:12:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I improve, he improves
I ask, he asks
I hear, he hears
I become, he becomes
I identify, he identifies
I represent, he represents
I send, he sends
I describe, he
2024-07-16 23:12:29 root INFO     [order_1_approx] starting weight calculation for I improve, he improves
I send, he sends
I represent, he represents
I ask, he asks
I describe, he describes
I become, he becomes
I hear, he hears
I identify, he
2024-07-16 23:12:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 23:15:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9580,  0.4011,  0.4351,  ..., -0.9678, -0.3896,  0.5190],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.4043, -3.9590, -0.4290,  ...,  3.6973,  0.8105,  2.9492],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0224, -0.0113,  0.0170,  ..., -0.0010, -0.0103, -0.0007],
        [-0.0064,  0.0090,  0.0054,  ...,  0.0053, -0.0020, -0.0014],
        [-0.0245, -0.0016, -0.0106,  ..., -0.0016, -0.0090, -0.0121],
        ...,
        [-0.0145, -0.0029, -0.0197,  ...,  0.0033, -0.0184,  0.0047],
        [ 0.0040,  0.0020,  0.0094,  ..., -0.0092, -0.0208,  0.0018],
        [ 0.0043,  0.0065,  0.0082,  ..., -0.0146,  0.0058, -0.0175]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0400, -2.6641, -0.3750,  ...,  4.3828,  1.2178,  2.1094]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 23:15:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I improve, he improves
I send, he sends
I represent, he represents
I ask, he asks
I describe, he describes
I become, he becomes
I hear, he hears
I identify, he
2024-07-16 23:15:57 root INFO     [order_1_approx] starting weight calculation for I send, he sends
I represent, he represents
I hear, he hears
I ask, he asks
I identify, he identifies
I describe, he describes
I become, he becomes
I improve, he
2024-07-16 23:15:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 23:19:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4290, -0.6216,  2.4629,  ..., -1.3818,  0.7041, -0.2683],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 8.0234, -2.8633,  0.6523,  ...,  4.2070, -1.5234,  2.0312],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-3.4733e-03, -1.9485e-02, -1.5488e-03,  ..., -8.7433e-03,
         -7.9956e-03, -2.0569e-02],
        [-4.0092e-03, -9.3689e-03, -7.7744e-03,  ...,  8.0414e-03,
         -1.0689e-02, -7.4348e-03],
        [-1.2398e-02, -1.0773e-02, -1.1879e-02,  ..., -7.9966e-04,
          1.0994e-02, -1.1463e-03],
        ...,
        [-8.6899e-03,  5.2605e-03, -4.8752e-03,  ...,  5.2834e-03,
         -5.4359e-03,  1.2188e-03],
        [-5.1384e-03, -3.7594e-03,  5.3177e-03,  ..., -9.8190e-03,
         -2.0309e-02, -3.4027e-03],
        [ 2.3842e-05,  1.8509e-02,  3.4065e-03,  ..., -7.6599e-03,
          1.1131e-02, -3.8261e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 8.4219, -2.3320,  0.3169,  ...,  4.0938, -1.6094,  1.8613]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 23:19:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I send, he sends
I represent, he represents
I hear, he hears
I ask, he asks
I identify, he identifies
I describe, he describes
I become, he becomes
I improve, he
2024-07-16 23:19:25 root INFO     [order_1_approx] starting weight calculation for I identify, he identifies
I become, he becomes
I represent, he represents
I describe, he describes
I hear, he hears
I send, he sends
I improve, he improves
I ask, he
2024-07-16 23:19:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 23:22:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3486, -0.2974,  0.3018,  ..., -1.7285,  0.9707,  0.2820],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7026, -1.4990,  1.3008,  ...,  0.8184,  2.2969,  3.1855],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0123, -0.0085,  0.0094,  ..., -0.0016, -0.0206, -0.0023],
        [ 0.0037,  0.0053, -0.0056,  ...,  0.0055, -0.0069, -0.0041],
        [-0.0035,  0.0037, -0.0150,  ...,  0.0123, -0.0138,  0.0009],
        ...,
        [ 0.0048, -0.0038, -0.0042,  ..., -0.0073,  0.0061, -0.0035],
        [-0.0072,  0.0004, -0.0033,  ..., -0.0072, -0.0284, -0.0012],
        [-0.0007,  0.0077,  0.0108,  ..., -0.0069,  0.0076, -0.0175]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1455, -1.4238,  0.7241,  ...,  0.1001,  2.6348,  3.1973]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 23:22:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I identify, he identifies
I become, he becomes
I represent, he represents
I describe, he describes
I hear, he hears
I send, he sends
I improve, he improves
I ask, he
2024-07-16 23:22:53 root INFO     [order_1_approx] starting weight calculation for I ask, he asks
I hear, he hears
I describe, he describes
I improve, he improves
I identify, he identifies
I represent, he represents
I become, he becomes
I send, he
2024-07-16 23:22:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 23:26:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.9229, -0.1523,  1.5303,  ..., -0.1384, -0.4468,  1.3359],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.1035, -0.9551,  3.5312,  ...,  2.6855, -1.7793,  4.2539],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0138, -0.0007,  0.0092,  ..., -0.0028, -0.0156, -0.0199],
        [-0.0114, -0.0146,  0.0078,  ...,  0.0028, -0.0084,  0.0091],
        [ 0.0024, -0.0007, -0.0349,  ...,  0.0028, -0.0025, -0.0020],
        ...,
        [-0.0283, -0.0164, -0.0150,  ..., -0.0181, -0.0140, -0.0117],
        [-0.0107, -0.0121,  0.0066,  ..., -0.0071, -0.0231,  0.0119],
        [ 0.0067,  0.0183,  0.0035,  ..., -0.0149,  0.0028, -0.0322]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7871, -0.8311,  3.3555,  ...,  3.0254, -1.5820,  4.4258]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 23:26:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I ask, he asks
I hear, he hears
I describe, he describes
I improve, he improves
I identify, he identifies
I represent, he represents
I become, he becomes
I send, he
2024-07-16 23:26:21 root INFO     [order_1_approx] starting weight calculation for I represent, he represents
I describe, he describes
I improve, he improves
I ask, he asks
I become, he becomes
I send, he sends
I identify, he identifies
I hear, he
2024-07-16 23:26:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 23:29:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2930, -0.9404,  0.6040,  ..., -1.2754,  0.1171,  0.8340],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1617,  0.4268, -0.2100,  ..., -0.1274,  1.4746,  4.9688],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0106, -0.0100,  0.0023,  ...,  0.0090, -0.0127, -0.0211],
        [ 0.0009, -0.0045,  0.0069,  ...,  0.0015, -0.0008,  0.0048],
        [-0.0165, -0.0038, -0.0240,  ...,  0.0190,  0.0026, -0.0080],
        ...,
        [-0.0039,  0.0038, -0.0060,  ..., -0.0212,  0.0046, -0.0028],
        [-0.0021,  0.0049,  0.0145,  ..., -0.0147, -0.0199, -0.0058],
        [ 0.0162,  0.0005, -0.0025,  ..., -0.0208,  0.0101, -0.0211]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2350,  0.7856, -0.1857,  ..., -0.5166,  1.8242,  5.7500]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 23:29:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I represent, he represents
I describe, he describes
I improve, he improves
I ask, he asks
I become, he becomes
I send, he sends
I identify, he identifies
I hear, he
2024-07-16 23:29:48 root INFO     [order_1_approx] starting weight calculation for I ask, he asks
I represent, he represents
I identify, he identifies
I send, he sends
I improve, he improves
I describe, he describes
I hear, he hears
I become, he
2024-07-16 23:29:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 23:33:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3989, -0.0320,  2.7930,  ...,  0.0193,  0.6812,  0.7168],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4141, -1.9180,  0.9072,  ..., -0.5820,  1.9766,  1.2354],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0189, -0.0187,  0.0267,  ...,  0.0033, -0.0209, -0.0271],
        [-0.0162, -0.0057, -0.0159,  ...,  0.0114,  0.0010, -0.0066],
        [-0.0170, -0.0128, -0.0204,  ...,  0.0083, -0.0109,  0.0056],
        ...,
        [ 0.0010, -0.0080, -0.0182,  ..., -0.0233, -0.0144,  0.0056],
        [ 0.0009,  0.0084,  0.0176,  ..., -0.0029, -0.0102,  0.0068],
        [ 0.0106,  0.0026, -0.0019,  ...,  0.0038,  0.0110, -0.0323]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.2969, -2.1973,  0.5127,  ..., -0.5107,  2.0449,  1.3213]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 23:33:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I ask, he asks
I represent, he represents
I identify, he identifies
I send, he sends
I improve, he improves
I describe, he describes
I hear, he hears
I become, he
2024-07-16 23:33:16 root INFO     total operator prediction time: 1664.3200883865356 seconds
2024-07-16 23:33:16 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_Ving - 3pSg
2024-07-16 23:33:16 root INFO     building operator verb_Ving - 3pSg
2024-07-16 23:33:16 root INFO     [order_1_approx] starting weight calculation for When something is sitting, it sits
When something is becoming, it becomes
When something is involving, it involves
When something is receiving, it receives
When something is representing, it represents
When something is adding, it adds
When something is creating, it creates
When something is hearing, it
2024-07-16 23:33:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 23:36:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.8037, -0.6729,  0.3950,  ..., -0.7217,  1.2305,  0.9434],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.0581e-01,  3.8164e+00, -4.8828e-03,  ...,  3.8330e-01,
         3.3984e-01,  6.3320e+00], device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0025, -0.0157, -0.0075,  ...,  0.0092, -0.0095, -0.0071],
        [-0.0009,  0.0096, -0.0119,  ..., -0.0020,  0.0021, -0.0144],
        [-0.0079,  0.0036,  0.0031,  ..., -0.0059,  0.0016, -0.0005],
        ...,
        [-0.0082,  0.0069,  0.0047,  ..., -0.0029,  0.0020,  0.0096],
        [-0.0089, -0.0017,  0.0028,  ..., -0.0006, -0.0163, -0.0067],
        [-0.0036, -0.0039, -0.0146,  ..., -0.0104, -0.0037, -0.0237]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0714,  3.9922,  0.0137,  ...,  1.1660, -0.3379,  6.0586]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 23:36:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is sitting, it sits
When something is becoming, it becomes
When something is involving, it involves
When something is receiving, it receives
When something is representing, it represents
When something is adding, it adds
When something is creating, it creates
When something is hearing, it
2024-07-16 23:36:44 root INFO     [order_1_approx] starting weight calculation for When something is representing, it represents
When something is receiving, it receives
When something is hearing, it hears
When something is involving, it involves
When something is adding, it adds
When something is becoming, it becomes
When something is creating, it creates
When something is sitting, it
2024-07-16 23:36:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 23:40:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5254, -1.7520,  0.3833,  ..., -0.5190,  0.3647,  0.4189],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3438,  2.0859,  2.3145,  ...,  1.1094, -2.6953,  3.8340],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0132, -0.0116,  0.0055,  ..., -0.0063, -0.0084, -0.0134],
        [ 0.0057,  0.0066, -0.0115,  ...,  0.0056,  0.0062, -0.0146],
        [-0.0117,  0.0055,  0.0013,  ..., -0.0004, -0.0170,  0.0159],
        ...,
        [-0.0029,  0.0054, -0.0064,  ..., -0.0060,  0.0033,  0.0065],
        [-0.0045, -0.0029,  0.0153,  ...,  0.0003, -0.0277,  0.0006],
        [-0.0143,  0.0263, -0.0025,  ..., -0.0068,  0.0018, -0.0246]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3125,  1.9209,  2.6094,  ...,  1.1260, -2.5156,  3.6465]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 23:40:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is representing, it represents
When something is receiving, it receives
When something is hearing, it hears
When something is involving, it involves
When something is adding, it adds
When something is becoming, it becomes
When something is creating, it creates
When something is sitting, it
2024-07-16 23:40:12 root INFO     [order_1_approx] starting weight calculation for When something is becoming, it becomes
When something is creating, it creates
When something is receiving, it receives
When something is representing, it represents
When something is sitting, it sits
When something is hearing, it hears
When something is adding, it adds
When something is involving, it
2024-07-16 23:40:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 23:43:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1211, -0.0698,  0.2915,  ...,  0.2793,  1.3613,  0.9438],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([2.5684, 0.9785, 1.4629,  ..., 1.5186, 0.8916, 1.4258], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0076,  0.0019,  0.0046,  ..., -0.0025,  0.0013, -0.0131],
        [-0.0105,  0.0064,  0.0004,  ...,  0.0109, -0.0132, -0.0114],
        [-0.0022,  0.0086, -0.0175,  ..., -0.0105, -0.0149,  0.0101],
        ...,
        [-0.0107,  0.0096, -0.0258,  ...,  0.0214, -0.0205, -0.0122],
        [-0.0140,  0.0038,  0.0048,  ...,  0.0133, -0.0162, -0.0026],
        [-0.0034,  0.0253,  0.0058,  ..., -0.0096,  0.0036, -0.0208]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[2.5664, 0.8999, 1.7891,  ..., 1.3818, 1.2354, 1.7432]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 23:43:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is becoming, it becomes
When something is creating, it creates
When something is receiving, it receives
When something is representing, it represents
When something is sitting, it sits
When something is hearing, it hears
When something is adding, it adds
When something is involving, it
2024-07-16 23:43:39 root INFO     [order_1_approx] starting weight calculation for When something is involving, it involves
When something is representing, it represents
When something is creating, it creates
When something is hearing, it hears
When something is becoming, it becomes
When something is sitting, it sits
When something is adding, it adds
When something is receiving, it
2024-07-16 23:43:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 23:47:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2383, -1.7891,  0.8975,  ...,  0.0618,  1.3740,  2.2637],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([3.8496, 0.4102, 0.0898,  ..., 2.9141, 0.2419, 4.7070], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0026, -0.0117, -0.0141,  ..., -0.0035, -0.0012, -0.0190],
        [-0.0003, -0.0044,  0.0026,  ...,  0.0022, -0.0030, -0.0047],
        [-0.0012, -0.0002,  0.0074,  ..., -0.0099,  0.0013,  0.0045],
        ...,
        [-0.0107, -0.0013, -0.0065,  ..., -0.0073, -0.0048, -0.0053],
        [ 0.0007,  0.0029, -0.0012,  ...,  0.0039, -0.0127,  0.0080],
        [-0.0060,  0.0048, -0.0012,  ..., -0.0145,  0.0036, -0.0161]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.7812,  0.4229,  0.2534,  ...,  3.3594, -0.0513,  4.1953]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 23:47:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is involving, it involves
When something is representing, it represents
When something is creating, it creates
When something is hearing, it hears
When something is becoming, it becomes
When something is sitting, it sits
When something is adding, it adds
When something is receiving, it
2024-07-16 23:47:06 root INFO     [order_1_approx] starting weight calculation for When something is adding, it adds
When something is becoming, it becomes
When something is sitting, it sits
When something is receiving, it receives
When something is creating, it creates
When something is involving, it involves
When something is hearing, it hears
When something is representing, it
2024-07-16 23:47:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 23:50:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4746, -0.8555,  0.1689,  ..., -1.0566,  1.6875,  0.4922],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1279, -0.2002,  1.5449,  ...,  1.2070,  0.8438,  3.4082],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 5.9280e-03, -1.3344e-02,  3.6163e-03,  ...,  4.9210e-04,
         -7.7534e-04, -1.6113e-02],
        [-2.6131e-03, -2.9392e-03,  2.1858e-03,  ..., -3.6163e-03,
         -7.4463e-03, -5.1727e-03],
        [-5.3558e-03,  8.3017e-04, -6.7711e-05,  ..., -1.0567e-02,
          4.4441e-03,  3.4065e-03],
        ...,
        [-6.1111e-03, -7.5455e-03,  1.6594e-03,  ...,  6.0425e-03,
         -3.0403e-03, -8.7051e-03],
        [-4.6158e-03, -1.2512e-03, -1.6594e-04,  ..., -5.8899e-03,
         -1.1444e-02,  5.0507e-03],
        [-7.0000e-03,  1.5221e-02,  3.2558e-03,  ..., -1.1047e-02,
         -7.8964e-03, -4.0245e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7246,  0.2197,  1.3682,  ...,  1.4531,  0.7920,  3.8320]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 23:50:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is adding, it adds
When something is becoming, it becomes
When something is sitting, it sits
When something is receiving, it receives
When something is creating, it creates
When something is involving, it involves
When something is hearing, it hears
When something is representing, it
2024-07-16 23:50:33 root INFO     [order_1_approx] starting weight calculation for When something is representing, it represents
When something is becoming, it becomes
When something is receiving, it receives
When something is sitting, it sits
When something is hearing, it hears
When something is involving, it involves
When something is adding, it adds
When something is creating, it
2024-07-16 23:50:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 23:53:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5000, -0.4766,  0.3076,  ...,  1.5439,  1.3125, -0.3472],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1626,  0.3855,  0.0664,  ...,  0.8877, -0.4370, -0.0293],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0037, -0.0160,  0.0004,  ...,  0.0043, -0.0038, -0.0136],
        [-0.0056, -0.0177, -0.0011,  ..., -0.0043, -0.0087, -0.0084],
        [-0.0124, -0.0019, -0.0157,  ..., -0.0095,  0.0107, -0.0032],
        ...,
        [-0.0076, -0.0025, -0.0088,  ..., -0.0028, -0.0076, -0.0033],
        [-0.0071,  0.0068,  0.0078,  ..., -0.0008, -0.0179,  0.0174],
        [-0.0087,  0.0133, -0.0067,  ..., -0.0118,  0.0006, -0.0274]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6768,  0.8652,  0.2905,  ...,  0.7642, -0.3286, -0.1321]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 23:54:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is representing, it represents
When something is becoming, it becomes
When something is receiving, it receives
When something is sitting, it sits
When something is hearing, it hears
When something is involving, it involves
When something is adding, it adds
When something is creating, it
2024-07-16 23:54:00 root INFO     [order_1_approx] starting weight calculation for When something is sitting, it sits
When something is hearing, it hears
When something is creating, it creates
When something is receiving, it receives
When something is adding, it adds
When something is involving, it involves
When something is representing, it represents
When something is becoming, it
2024-07-16 23:54:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-16 23:57:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2080,  0.5552,  0.6445,  ...,  0.2524, -0.2634,  0.6670],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2891,  0.8887,  1.2080,  ..., -0.1821,  0.9038,  1.3145],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.7023e-02, -2.1515e-02,  7.6294e-04,  ..., -1.1845e-03,
          8.5640e-04, -1.5381e-02],
        [-1.7548e-02, -2.1477e-03, -2.0813e-02,  ...,  6.0749e-04,
         -9.2468e-03, -1.9928e-02],
        [-1.9180e-02, -1.1177e-02, -2.0721e-02,  ..., -4.9248e-03,
          8.9645e-05,  1.2352e-02],
        ...,
        [-4.4708e-03,  2.0065e-02,  6.8283e-04,  ..., -1.7334e-02,
         -1.0406e-02,  6.7520e-03],
        [-2.0096e-02, -9.0637e-03,  7.8201e-03,  ...,  6.3400e-03,
         -2.7695e-02,  1.4091e-02],
        [-3.9043e-03,  1.4359e-02, -7.1144e-04,  ..., -3.2272e-03,
         -2.6245e-03, -3.1219e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2793,  0.9116,  1.1016,  ..., -0.1989,  1.4590,  1.4023]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 23:57:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is sitting, it sits
When something is hearing, it hears
When something is creating, it creates
When something is receiving, it receives
When something is adding, it adds
When something is involving, it involves
When something is representing, it represents
When something is becoming, it
2024-07-16 23:57:28 root INFO     [order_1_approx] starting weight calculation for When something is receiving, it receives
When something is hearing, it hears
When something is becoming, it becomes
When something is representing, it represents
When something is involving, it involves
When something is creating, it creates
When something is sitting, it sits
When something is adding, it
2024-07-16 23:57:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 00:00:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4131,  0.1106,  0.8643,  ..., -0.0945,  0.2817,  0.1541],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9902,  0.6523,  4.2188,  ...,  0.5918, -1.6152,  2.2461],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0118, -0.0079,  0.0091,  ...,  0.0079, -0.0024, -0.0052],
        [ 0.0101, -0.0002, -0.0023,  ...,  0.0055,  0.0032, -0.0093],
        [ 0.0032,  0.0067, -0.0006,  ..., -0.0054,  0.0076, -0.0040],
        ...,
        [-0.0015, -0.0073, -0.0057,  ...,  0.0071,  0.0058, -0.0019],
        [-0.0087,  0.0030,  0.0084,  ..., -0.0081, -0.0194,  0.0002],
        [-0.0104,  0.0151,  0.0051,  ..., -0.0095, -0.0004, -0.0059]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3418,  0.1738,  3.3184,  ...,  0.5044, -1.8359,  1.8711]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 00:00:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is receiving, it receives
When something is hearing, it hears
When something is becoming, it becomes
When something is representing, it represents
When something is involving, it involves
When something is creating, it creates
When something is sitting, it sits
When something is adding, it
2024-07-17 00:00:56 root INFO     total operator prediction time: 1659.687467098236 seconds
2024-07-17 00:00:56 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on noun - plural_reg
2024-07-17 00:00:56 root INFO     building operator noun - plural_reg
2024-07-17 00:00:56 root INFO     [order_1_approx] starting weight calculation for The plural form of week is weeks
The plural form of role is roles
The plural form of period is periods
The plural form of website is websites
The plural form of system is systems
The plural form of law is laws
The plural form of council is councils
The plural form of area is
2024-07-17 00:00:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 00:04:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3604, -0.0063, -1.8154,  ..., -1.2695,  0.9756,  0.4651],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8462, -0.3262,  1.5840,  ..., -4.0117,  3.4727,  2.7676],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0213, -0.0113,  0.0141,  ..., -0.0038, -0.0091, -0.0084],
        [-0.0069, -0.0077,  0.0003,  ...,  0.0128,  0.0020, -0.0054],
        [ 0.0011, -0.0017, -0.0062,  ..., -0.0035,  0.0056,  0.0097],
        ...,
        [-0.0028, -0.0028,  0.0003,  ...,  0.0040,  0.0032, -0.0045],
        [-0.0046, -0.0097,  0.0098,  ..., -0.0093, -0.0130,  0.0047],
        [-0.0033,  0.0117,  0.0042,  ..., -0.0154, -0.0065, -0.0006]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9038, -0.2756,  1.3906,  ..., -3.6289,  4.3867,  2.9746]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 00:04:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of week is weeks
The plural form of role is roles
The plural form of period is periods
The plural form of website is websites
The plural form of system is systems
The plural form of law is laws
The plural form of council is councils
The plural form of area is
2024-07-17 00:04:22 root INFO     [order_1_approx] starting weight calculation for The plural form of law is laws
The plural form of system is systems
The plural form of week is weeks
The plural form of role is roles
The plural form of council is councils
The plural form of period is periods
The plural form of area is areas
The plural form of website is
2024-07-17 00:04:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 00:07:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4902,  1.7705, -0.5327,  ..., -0.3645,  0.6177,  2.7910],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1494, -4.3281,  1.9805,  ..., -1.7910,  1.6768,  4.9766],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0152, -0.0064,  0.0152,  ..., -0.0024,  0.0031, -0.0118],
        [-0.0116, -0.0059, -0.0008,  ...,  0.0111, -0.0021, -0.0024],
        [ 0.0045, -0.0189, -0.0288,  ..., -0.0121,  0.0125,  0.0102],
        ...,
        [-0.0033,  0.0003,  0.0038,  ..., -0.0191, -0.0016, -0.0040],
        [-0.0010, -0.0213,  0.0078,  ..., -0.0043, -0.0337,  0.0083],
        [-0.0146,  0.0244, -0.0079,  ...,  0.0040, -0.0052,  0.0021]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4299, -4.4844,  1.7637,  ..., -1.6279,  1.6514,  5.0352]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 00:07:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of law is laws
The plural form of system is systems
The plural form of week is weeks
The plural form of role is roles
The plural form of council is councils
The plural form of period is periods
The plural form of area is areas
The plural form of website is
2024-07-17 00:07:51 root INFO     [order_1_approx] starting weight calculation for The plural form of period is periods
The plural form of law is laws
The plural form of area is areas
The plural form of role is roles
The plural form of week is weeks
The plural form of system is systems
The plural form of website is websites
The plural form of council is
2024-07-17 00:07:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 00:11:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5762, -0.3596, -0.3545,  ..., -0.3831,  0.2161,  1.2520],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4683,  2.4453, -1.2871,  ..., -4.3594, -1.4883,  1.7529],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0071, -0.0071,  0.0060,  ..., -0.0025, -0.0063, -0.0189],
        [-0.0082, -0.0151, -0.0025,  ...,  0.0132,  0.0090,  0.0043],
        [ 0.0042, -0.0251, -0.0077,  ..., -0.0130,  0.0042,  0.0051],
        ...,
        [ 0.0104, -0.0035, -0.0059,  ...,  0.0072,  0.0125,  0.0015],
        [-0.0047, -0.0102, -0.0003,  ..., -0.0008, -0.0228,  0.0079],
        [ 0.0033,  0.0235,  0.0155,  ..., -0.0030, -0.0009, -0.0141]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4958,  1.8926, -1.0293,  ..., -4.6289, -1.6641,  2.1133]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 00:11:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of period is periods
The plural form of law is laws
The plural form of area is areas
The plural form of role is roles
The plural form of week is weeks
The plural form of system is systems
The plural form of website is websites
The plural form of council is
2024-07-17 00:11:18 root INFO     [order_1_approx] starting weight calculation for The plural form of council is councils
The plural form of system is systems
The plural form of period is periods
The plural form of website is websites
The plural form of role is roles
The plural form of week is weeks
The plural form of area is areas
The plural form of law is
2024-07-17 00:11:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 00:14:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4043,  0.1029,  1.8018,  ..., -0.3149, -0.7075,  0.9170],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9204, -0.3083, -3.8750,  ..., -1.6719,  2.1426,  1.8154],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.1667e-03,  6.7291e-03,  1.5717e-02,  ..., -9.1553e-03,
         -1.0391e-02, -2.2385e-02],
        [ 1.7519e-03, -1.2665e-02, -2.8229e-03,  ...,  9.0179e-03,
          4.6539e-03, -9.0332e-03],
        [-1.5078e-03, -1.1581e-02, -8.1253e-04,  ..., -5.3635e-03,
         -7.6294e-06,  6.7711e-03],
        ...,
        [ 1.2970e-02, -7.6752e-03,  1.4172e-03,  ...,  2.0142e-02,
          1.1993e-02, -1.6880e-04],
        [-2.2907e-03, -2.0752e-03,  2.7847e-04,  ..., -7.8506e-03,
         -2.4689e-02, -2.5063e-03],
        [-5.2643e-03,  6.7406e-03,  1.8234e-02,  ..., -9.9869e-03,
         -9.9411e-03,  5.4588e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0737, -0.7798, -3.7773,  ..., -2.3398,  2.1914,  2.5801]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 00:14:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of council is councils
The plural form of system is systems
The plural form of period is periods
The plural form of website is websites
The plural form of role is roles
The plural form of week is weeks
The plural form of area is areas
The plural form of law is
2024-07-17 00:14:45 root INFO     [order_1_approx] starting weight calculation for The plural form of law is laws
The plural form of system is systems
The plural form of week is weeks
The plural form of role is roles
The plural form of council is councils
The plural form of website is websites
The plural form of area is areas
The plural form of period is
2024-07-17 00:14:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 00:18:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9126,  0.3611, -0.3699,  ..., -1.7070,  0.1289,  1.4922],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6245, -0.9668, -1.8398,  ...,  3.0801,  2.1367,  2.3789],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0130, -0.0050,  0.0206,  ..., -0.0060, -0.0161, -0.0115],
        [-0.0099, -0.0131, -0.0071,  ...,  0.0185, -0.0083,  0.0052],
        [ 0.0061, -0.0049, -0.0158,  ..., -0.0152,  0.0035,  0.0022],
        ...,
        [ 0.0071, -0.0015,  0.0082,  ...,  0.0011,  0.0022, -0.0022],
        [-0.0009, -0.0096,  0.0009,  ..., -0.0028, -0.0200,  0.0035],
        [-0.0019,  0.0138,  0.0026,  ..., -0.0006,  0.0036, -0.0053]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0234, -0.9512, -1.5869,  ...,  2.2617,  1.7246,  3.1465]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 00:18:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of law is laws
The plural form of system is systems
The plural form of week is weeks
The plural form of role is roles
The plural form of council is councils
The plural form of website is websites
The plural form of area is areas
The plural form of period is
2024-07-17 00:18:13 root INFO     [order_1_approx] starting weight calculation for The plural form of council is councils
The plural form of law is laws
The plural form of week is weeks
The plural form of system is systems
The plural form of period is periods
The plural form of website is websites
The plural form of area is areas
The plural form of role is
2024-07-17 00:18:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 00:21:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7339, -0.4651,  0.7373,  ...,  0.0731, -0.5449, -0.1238],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-5.0781,  0.2012,  1.8789,  ...,  2.0117,  1.6396,  3.8789],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0019,  0.0031,  0.0147,  ..., -0.0029, -0.0164, -0.0168],
        [ 0.0015, -0.0122,  0.0012,  ...,  0.0120,  0.0041, -0.0080],
        [ 0.0045, -0.0128, -0.0171,  ..., -0.0021, -0.0021,  0.0029],
        ...,
        [-0.0005, -0.0073,  0.0029,  ...,  0.0024,  0.0040, -0.0036],
        [ 0.0056,  0.0144,  0.0060,  ..., -0.0051, -0.0132,  0.0047],
        [-0.0087,  0.0142,  0.0054,  ..., -0.0088,  0.0077,  0.0043]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.1328,  0.1301,  1.9736,  ...,  1.3633,  1.8135,  4.5703]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 00:21:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of council is councils
The plural form of law is laws
The plural form of week is weeks
The plural form of system is systems
The plural form of period is periods
The plural form of website is websites
The plural form of area is areas
The plural form of role is
2024-07-17 00:21:41 root INFO     [order_1_approx] starting weight calculation for The plural form of area is areas
The plural form of period is periods
The plural form of week is weeks
The plural form of website is websites
The plural form of law is laws
The plural form of role is roles
The plural form of council is councils
The plural form of system is
2024-07-17 00:21:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 00:25:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4834,  0.2302,  0.3540,  ..., -0.9922,  1.0605,  0.5688],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.1543,  0.2781,  4.2852,  ...,  0.8311, -2.1250,  4.4727],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.2400e-02,  9.0179e-03,  7.7438e-03,  ..., -6.5498e-03,
          1.1635e-02, -2.5116e-02],
        [-4.4785e-03, -8.4686e-03, -8.4305e-04,  ...,  1.1848e-02,
         -6.6833e-03,  5.3139e-03],
        [ 2.7752e-04, -1.2817e-02, -1.4084e-02,  ..., -1.3535e-02,
          2.4338e-03,  8.9264e-03],
        ...,
        [-4.5853e-03, -6.9199e-03, -1.3504e-03,  ..., -4.8904e-03,
          4.3640e-03, -8.1482e-03],
        [-3.9139e-03, -2.9106e-03,  9.8114e-03,  ..., -7.8125e-03,
          4.3869e-04,  7.8735e-03],
        [-1.9440e-02,  2.7634e-02,  2.5177e-03,  ..., -4.4975e-03,
         -9.3536e-03,  9.3460e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.4219, -0.2178,  4.7578,  ...,  0.0645, -2.4824,  4.8398]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 00:25:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of area is areas
The plural form of period is periods
The plural form of week is weeks
The plural form of website is websites
The plural form of law is laws
The plural form of role is roles
The plural form of council is councils
The plural form of system is
2024-07-17 00:25:09 root INFO     [order_1_approx] starting weight calculation for The plural form of role is roles
The plural form of law is laws
The plural form of area is areas
The plural form of website is websites
The plural form of period is periods
The plural form of council is councils
The plural form of system is systems
The plural form of week is
2024-07-17 00:25:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 00:28:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.0586, -0.3948, -1.1660,  ..., -0.8208, -1.7773,  2.2383],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1465, -3.0391,  0.8271,  ...,  1.1260,  4.7734,  2.9121],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0076, -0.0061,  0.0118,  ..., -0.0087,  0.0020, -0.0186],
        [-0.0041, -0.0015, -0.0025,  ...,  0.0067, -0.0060, -0.0042],
        [ 0.0077, -0.0127, -0.0267,  ..., -0.0162,  0.0102,  0.0052],
        ...,
        [ 0.0113, -0.0104,  0.0059,  ...,  0.0012, -0.0115, -0.0044],
        [ 0.0013, -0.0113,  0.0107,  ..., -0.0113, -0.0214,  0.0071],
        [-0.0011,  0.0171,  0.0059,  ..., -0.0002,  0.0082, -0.0073]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2944, -2.6934,  1.2402,  ...,  1.0977,  5.4297,  3.0508]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 00:28:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of role is roles
The plural form of law is laws
The plural form of area is areas
The plural form of website is websites
The plural form of period is periods
The plural form of council is councils
The plural form of system is systems
The plural form of week is
2024-07-17 00:28:36 root INFO     total operator prediction time: 1660.163901090622 seconds
2024-07-17 00:28:36 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_3pSg - Ved
2024-07-17 00:28:36 root INFO     building operator verb_3pSg - Ved
2024-07-17 00:28:36 root INFO     [order_1_approx] starting weight calculation for When he fails something, something has been failed
When he requires something, something has been required
When he seems something, something has been seemed
When he provides something, something has been provided
When he manages something, something has been managed
When he tells something, something has been told
When he relates something, something has been related
When he receives something, something has been
2024-07-17 00:28:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 00:32:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0732, -0.7041,  0.4561,  ..., -0.3545,  0.5942,  1.0918],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([1.6133, 2.1641, 0.3633,  ..., 0.2505, 2.1094, 1.7021], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0054, -0.0055, -0.0025,  ..., -0.0005,  0.0028, -0.0035],
        [ 0.0011,  0.0035, -0.0005,  ..., -0.0064, -0.0059,  0.0036],
        [ 0.0077, -0.0081,  0.0042,  ..., -0.0045,  0.0048,  0.0032],
        ...,
        [-0.0046,  0.0064,  0.0015,  ..., -0.0045, -0.0053, -0.0015],
        [ 0.0072,  0.0003, -0.0062,  ..., -0.0033, -0.0115,  0.0132],
        [-0.0060,  0.0062,  0.0051,  ..., -0.0095, -0.0031, -0.0124]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[1.8359, 1.8730, 0.1168,  ..., 0.3413, 1.8809, 1.6963]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 00:32:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he fails something, something has been failed
When he requires something, something has been required
When he seems something, something has been seemed
When he provides something, something has been provided
When he manages something, something has been managed
When he tells something, something has been told
When he relates something, something has been related
When he receives something, something has been
2024-07-17 00:32:05 root INFO     [order_1_approx] starting weight calculation for When he requires something, something has been required
When he tells something, something has been told
When he seems something, something has been seemed
When he fails something, something has been failed
When he receives something, something has been received
When he manages something, something has been managed
When he provides something, something has been provided
When he relates something, something has been
2024-07-17 00:32:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 00:35:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4019,  0.4736,  1.7451,  ..., -0.5210,  0.5156, -0.3721],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9097,  1.6680, -0.0664,  ..., -2.7520,  2.0469,  2.8145],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-6.2637e-03,  3.0022e-03,  1.0849e-02,  ..., -2.9159e-04,
          9.2793e-04, -8.6060e-03],
        [ 1.0805e-03, -3.4714e-03, -5.6410e-04,  ...,  5.0392e-03,
          2.1019e-03,  2.6531e-03],
        [ 5.1804e-03,  4.1962e-03, -1.1620e-02,  ...,  1.8883e-04,
          3.0022e-03,  5.5695e-03],
        ...,
        [-2.9640e-03, -6.0349e-03, -6.6996e-04,  ...,  1.7166e-03,
          4.2763e-03, -1.1719e-02],
        [-4.8256e-04,  9.3460e-03, -2.3174e-03,  ..., -1.0475e-02,
         -1.3397e-02, -1.8234e-03],
        [-1.0506e-02,  1.7334e-02,  5.0621e-03,  ..., -6.7596e-03,
          6.6757e-05, -1.0727e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8281,  1.6465, -0.1965,  ..., -3.1895,  2.0449,  3.6953]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 00:35:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he requires something, something has been required
When he tells something, something has been told
When he seems something, something has been seemed
When he fails something, something has been failed
When he receives something, something has been received
When he manages something, something has been managed
When he provides something, something has been provided
When he relates something, something has been
2024-07-17 00:35:34 root INFO     [order_1_approx] starting weight calculation for When he tells something, something has been told
When he fails something, something has been failed
When he manages something, something has been managed
When he relates something, something has been related
When he requires something, something has been required
When he receives something, something has been received
When he seems something, something has been seemed
When he provides something, something has been
2024-07-17 00:35:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 00:39:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1757,  0.7285,  0.1576,  ..., -0.1179,  0.4980,  0.7305],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2871,  1.7461,  0.3037,  ..., -1.0957, -1.0820,  0.0859],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-5.4169e-04, -6.2637e-03,  1.5480e-02,  ...,  7.2174e-03,
          3.3665e-03, -1.2589e-02],
        [ 1.1604e-02, -8.0490e-04,  2.2278e-03,  ..., -6.4430e-03,
         -1.2009e-02,  9.6893e-03],
        [ 1.1444e-05,  5.2719e-03, -1.3981e-03,  ...,  1.6403e-04,
         -4.7684e-04,  4.5700e-03],
        ...,
        [-8.7814e-03, -1.0178e-02, -5.5962e-03,  ...,  6.1493e-03,
         -6.6605e-03, -1.3634e-02],
        [ 7.7362e-03,  8.0109e-03,  7.9346e-04,  ..., -4.8904e-03,
         -9.9640e-03,  1.3779e-02],
        [-3.4027e-03,  2.0035e-02, -8.6784e-04,  ..., -1.6422e-03,
          1.3489e-02, -1.0445e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0889,  1.6650,  0.4219,  ..., -1.5586, -1.4062,  0.3687]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 00:39:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he tells something, something has been told
When he fails something, something has been failed
When he manages something, something has been managed
When he relates something, something has been related
When he requires something, something has been required
When he receives something, something has been received
When he seems something, something has been seemed
When he provides something, something has been
2024-07-17 00:39:02 root INFO     [order_1_approx] starting weight calculation for When he receives something, something has been received
When he seems something, something has been seemed
When he provides something, something has been provided
When he fails something, something has been failed
When he tells something, something has been told
When he requires something, something has been required
When he relates something, something has been related
When he manages something, something has been
2024-07-17 00:39:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 00:42:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.0786, 0.9238, 0.2014,  ..., 0.2742, 0.1023, 0.0161], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0789, -0.8062, -2.4453,  ...,  0.5137,  0.5269,  0.1914],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0060, -0.0067,  0.0051,  ...,  0.0001, -0.0014, -0.0025],
        [-0.0068,  0.0034, -0.0031,  ...,  0.0019, -0.0018,  0.0011],
        [-0.0002,  0.0049, -0.0112,  ..., -0.0035,  0.0036, -0.0025],
        ...,
        [-0.0039, -0.0122, -0.0053,  ..., -0.0048, -0.0020, -0.0069],
        [-0.0025, -0.0075,  0.0059,  ..., -0.0069, -0.0069,  0.0006],
        [ 0.0009,  0.0215,  0.0050,  ...,  0.0022,  0.0019, -0.0065]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1971, -0.7544, -2.4395,  ...,  0.0833,  0.5317,  0.2681]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 00:42:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he receives something, something has been received
When he seems something, something has been seemed
When he provides something, something has been provided
When he fails something, something has been failed
When he tells something, something has been told
When he requires something, something has been required
When he relates something, something has been related
When he manages something, something has been
2024-07-17 00:42:30 root INFO     [order_1_approx] starting weight calculation for When he relates something, something has been related
When he provides something, something has been provided
When he seems something, something has been seemed
When he tells something, something has been told
When he manages something, something has been managed
When he receives something, something has been received
When he fails something, something has been failed
When he requires something, something has been
2024-07-17 00:42:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 00:45:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4336,  0.0980,  0.1250,  ..., -0.0352,  0.4775,  1.1270],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7988,  0.1641, -0.4932,  ..., -1.1055, -0.2281,  2.7422],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0037, -0.0063,  0.0092,  ..., -0.0043, -0.0026, -0.0077],
        [ 0.0030,  0.0114, -0.0043,  ..., -0.0060, -0.0164,  0.0031],
        [ 0.0069,  0.0019, -0.0013,  ...,  0.0026,  0.0064,  0.0030],
        ...,
        [-0.0019, -0.0023, -0.0041,  ...,  0.0049,  0.0040, -0.0063],
        [ 0.0005, -0.0096,  0.0090,  ..., -0.0045, -0.0089,  0.0047],
        [-0.0058,  0.0115,  0.0078,  ...,  0.0094, -0.0046, -0.0045]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5430,  0.2844, -0.5073,  ..., -1.3750, -0.0953,  2.9316]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 00:45:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he relates something, something has been related
When he provides something, something has been provided
When he seems something, something has been seemed
When he tells something, something has been told
When he manages something, something has been managed
When he receives something, something has been received
When he fails something, something has been failed
When he requires something, something has been
2024-07-17 00:45:59 root INFO     [order_1_approx] starting weight calculation for When he manages something, something has been managed
When he provides something, something has been provided
When he seems something, something has been seemed
When he receives something, something has been received
When he relates something, something has been related
When he fails something, something has been failed
When he requires something, something has been required
When he tells something, something has been
2024-07-17 00:45:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 00:49:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.2048, 0.6543, 0.9409,  ..., 0.2590, 0.3564, 0.3179], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([3.5977, 1.1094, 2.5703,  ..., 0.8555, 2.2891, 1.4795], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0003, -0.0112,  0.0061,  ..., -0.0041, -0.0018, -0.0071],
        [-0.0062,  0.0008,  0.0043,  ...,  0.0069,  0.0037,  0.0110],
        [-0.0003, -0.0066, -0.0046,  ..., -0.0057,  0.0105,  0.0018],
        ...,
        [-0.0024, -0.0102, -0.0097,  ..., -0.0054,  0.0039, -0.0040],
        [ 0.0034,  0.0084,  0.0037,  ..., -0.0013, -0.0166, -0.0090],
        [-0.0093,  0.0222,  0.0165,  ...,  0.0021,  0.0071, -0.0122]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[3.7891, 1.2041, 2.3438,  ..., 0.3350, 2.3398, 1.9453]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 00:49:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he manages something, something has been managed
When he provides something, something has been provided
When he seems something, something has been seemed
When he receives something, something has been received
When he relates something, something has been related
When he fails something, something has been failed
When he requires something, something has been required
When he tells something, something has been
2024-07-17 00:49:27 root INFO     [order_1_approx] starting weight calculation for When he receives something, something has been received
When he provides something, something has been provided
When he tells something, something has been told
When he relates something, something has been related
When he requires something, something has been required
When he manages something, something has been managed
When he seems something, something has been seemed
When he fails something, something has been
2024-07-17 00:49:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 00:52:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6777,  0.4619,  0.6797,  ...,  0.5581,  1.2002, -0.5410],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0127,  1.6289,  3.0293,  ..., -1.3857, -2.1152, -0.6318],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0120, -0.0019, -0.0011,  ...,  0.0037,  0.0008, -0.0036],
        [-0.0040, -0.0082, -0.0014,  ...,  0.0035, -0.0022, -0.0067],
        [ 0.0026, -0.0004, -0.0125,  ..., -0.0054,  0.0064, -0.0002],
        ...,
        [-0.0046, -0.0073, -0.0080,  ..., -0.0106,  0.0076, -0.0059],
        [ 0.0024, -0.0043,  0.0034,  ..., -0.0064, -0.0050,  0.0016],
        [-0.0102,  0.0130,  0.0068,  ..., -0.0006, -0.0086, -0.0112]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2688,  2.1309,  3.3711,  ..., -1.5117, -2.2422, -0.1875]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 00:52:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he receives something, something has been received
When he provides something, something has been provided
When he tells something, something has been told
When he relates something, something has been related
When he requires something, something has been required
When he manages something, something has been managed
When he seems something, something has been seemed
When he fails something, something has been
2024-07-17 00:52:54 root INFO     [order_1_approx] starting weight calculation for When he fails something, something has been failed
When he manages something, something has been managed
When he relates something, something has been related
When he receives something, something has been received
When he tells something, something has been told
When he requires something, something has been required
When he provides something, something has been provided
When he seems something, something has been
2024-07-17 00:52:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 00:56:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3376,  0.9102,  0.0625,  ...,  0.2241,  0.8213, -0.9546],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.1992,  5.7070,  3.3672,  ...,  0.4741, -8.4141, -0.2910],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0012, -0.0162,  0.0089,  ...,  0.0045,  0.0076, -0.0035],
        [-0.0086, -0.0055, -0.0005,  ...,  0.0063, -0.0077, -0.0061],
        [-0.0009, -0.0032, -0.0011,  ..., -0.0006,  0.0061, -0.0006],
        ...,
        [-0.0018, -0.0016, -0.0007,  ...,  0.0050, -0.0052, -0.0051],
        [-0.0070,  0.0130,  0.0066,  ..., -0.0112, -0.0102,  0.0114],
        [-0.0012,  0.0156,  0.0125,  ..., -0.0085,  0.0026, -0.0108]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4609,  5.5352,  3.2480,  ...,  0.3003, -7.8203, -0.7021]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 00:56:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he fails something, something has been failed
When he manages something, something has been managed
When he relates something, something has been related
When he receives something, something has been received
When he tells something, something has been told
When he requires something, something has been required
When he provides something, something has been provided
When he seems something, something has been
2024-07-17 00:56:22 root INFO     total operator prediction time: 1666.0458884239197 seconds
2024-07-17 00:56:22 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on adj - superlative
2024-07-17 00:56:22 root INFO     building operator adj - superlative
2024-07-17 00:56:22 root INFO     [order_1_approx] starting weight calculation for If something is the most harsh, it is harshest
If something is the most angry, it is angriest
If something is the most hungry, it is hungriest
If something is the most merry, it is merriest
If something is the most scary, it is scariest
If something is the most tiny, it is tiniest
If something is the most hot, it is hottest
If something is the most lengthy, it is
2024-07-17 00:56:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 00:59:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6895,  1.0430, -0.0254,  ..., -0.7720,  0.1207,  0.1899],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9990, -1.2285, -2.0352,  ...,  3.4316, -1.8193,  6.3086],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.1177e-03, -2.1790e-02,  4.4365e-03,  ...,  8.5258e-04,
         -4.4098e-03, -6.8741e-03],
        [ 7.4387e-04,  6.5193e-03,  1.1444e-03,  ...,  3.6087e-03,
         -4.0512e-03, -3.2444e-03],
        [-5.6839e-03,  8.8654e-03,  1.4381e-03,  ..., -1.4811e-03,
         -6.4659e-03,  4.3831e-03],
        ...,
        [ 3.6430e-03,  2.8496e-03,  2.0027e-05,  ..., -4.0932e-03,
         -1.4181e-03, -6.1073e-03],
        [-6.0177e-04, -7.1945e-03, -2.9869e-03,  ...,  3.6373e-03,
         -1.4725e-02,  1.5472e-02],
        [-6.0081e-03,  1.0780e-02,  5.5008e-03,  ..., -6.9046e-03,
         -5.5542e-03, -3.9291e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4961, -1.3652, -1.9180,  ...,  2.9688, -2.0039,  6.6016]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 00:59:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most harsh, it is harshest
If something is the most angry, it is angriest
If something is the most hungry, it is hungriest
If something is the most merry, it is merriest
If something is the most scary, it is scariest
If something is the most tiny, it is tiniest
If something is the most hot, it is hottest
If something is the most lengthy, it is
2024-07-17 00:59:50 root INFO     [order_1_approx] starting weight calculation for If something is the most scary, it is scariest
If something is the most angry, it is angriest
If something is the most hot, it is hottest
If something is the most hungry, it is hungriest
If something is the most lengthy, it is lengthiest
If something is the most harsh, it is harshest
If something is the most tiny, it is tiniest
If something is the most merry, it is
2024-07-17 00:59:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 01:03:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2500,  1.2383,  0.2991,  ...,  0.9351,  1.7090,  0.5596],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.9424, -0.2754, -4.5117,  ...,  1.5938,  3.8438,  1.9434],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0058, -0.0112,  0.0092,  ..., -0.0003,  0.0030,  0.0029],
        [-0.0046, -0.0024,  0.0007,  ...,  0.0065, -0.0056,  0.0002],
        [-0.0013, -0.0027,  0.0012,  ..., -0.0163,  0.0078,  0.0072],
        ...,
        [ 0.0023, -0.0050, -0.0028,  ...,  0.0057,  0.0050,  0.0060],
        [ 0.0045,  0.0017, -0.0014,  ...,  0.0004, -0.0078,  0.0044],
        [-0.0049,  0.0054,  0.0055,  ...,  0.0040, -0.0129,  0.0005]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.1348, -0.6387, -4.9102,  ...,  1.3467,  3.4707,  2.4922]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 01:03:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most scary, it is scariest
If something is the most angry, it is angriest
If something is the most hot, it is hottest
If something is the most hungry, it is hungriest
If something is the most lengthy, it is lengthiest
If something is the most harsh, it is harshest
If something is the most tiny, it is tiniest
If something is the most merry, it is
2024-07-17 01:03:17 root INFO     [order_1_approx] starting weight calculation for If something is the most hungry, it is hungriest
If something is the most tiny, it is tiniest
If something is the most angry, it is angriest
If something is the most harsh, it is harshest
If something is the most scary, it is scariest
If something is the most merry, it is merriest
If something is the most lengthy, it is lengthiest
If something is the most hot, it is
2024-07-17 01:03:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 01:06:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7500,  1.1348,  0.9570,  ..., -1.0068,  0.6885, -0.5220],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6074, -0.6743, -0.7476,  ...,  0.4995,  2.0723,  6.0859],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0074, -0.0126,  0.0085,  ...,  0.0077, -0.0027, -0.0047],
        [-0.0072,  0.0014,  0.0039,  ...,  0.0140, -0.0044, -0.0047],
        [-0.0051,  0.0080,  0.0017,  ..., -0.0066,  0.0054, -0.0006],
        ...,
        [ 0.0045,  0.0076,  0.0087,  ..., -0.0034,  0.0004,  0.0045],
        [-0.0044, -0.0018, -0.0048,  ..., -0.0015, -0.0150,  0.0029],
        [-0.0101, -0.0045,  0.0099,  ...,  0.0009, -0.0039, -0.0180]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.8232, -0.5527, -1.0537,  ...,  0.8125,  1.6953,  6.5430]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 01:06:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most hungry, it is hungriest
If something is the most tiny, it is tiniest
If something is the most angry, it is angriest
If something is the most harsh, it is harshest
If something is the most scary, it is scariest
If something is the most merry, it is merriest
If something is the most lengthy, it is lengthiest
If something is the most hot, it is
2024-07-17 01:06:45 root INFO     [order_1_approx] starting weight calculation for If something is the most hot, it is hottest
If something is the most angry, it is angriest
If something is the most merry, it is merriest
If something is the most tiny, it is tiniest
If something is the most harsh, it is harshest
If something is the most scary, it is scariest
If something is the most lengthy, it is lengthiest
If something is the most hungry, it is
2024-07-17 01:06:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 01:10:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2490,  0.0701, -0.5366,  ...,  0.5488,  1.7744,  0.3167],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.8477,  0.1201, -3.8301,  ...,  1.8301,  1.0371,  2.2227],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0030, -0.0181,  0.0055,  ..., -0.0006,  0.0012, -0.0053],
        [-0.0083, -0.0024, -0.0077,  ...,  0.0075, -0.0043,  0.0003],
        [-0.0056, -0.0028,  0.0011,  ..., -0.0042, -0.0062,  0.0073],
        ...,
        [-0.0051, -0.0001, -0.0017,  ...,  0.0120,  0.0092,  0.0087],
        [-0.0077, -0.0049, -0.0065,  ..., -0.0052, -0.0091,  0.0036],
        [-0.0140,  0.0012, -0.0004,  ..., -0.0066,  0.0016, -0.0131]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7598,  0.3560, -4.4023,  ...,  1.0918,  0.6938,  2.5625]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 01:10:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most hot, it is hottest
If something is the most angry, it is angriest
If something is the most merry, it is merriest
If something is the most tiny, it is tiniest
If something is the most harsh, it is harshest
If something is the most scary, it is scariest
If something is the most lengthy, it is lengthiest
If something is the most hungry, it is
2024-07-17 01:10:11 root INFO     [order_1_approx] starting weight calculation for If something is the most hungry, it is hungriest
If something is the most tiny, it is tiniest
If something is the most hot, it is hottest
If something is the most scary, it is scariest
If something is the most angry, it is angriest
If something is the most merry, it is merriest
If something is the most lengthy, it is lengthiest
If something is the most harsh, it is
2024-07-17 01:10:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 01:13:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2783, -2.2109, -0.8882,  ..., -1.5215,  1.3447,  0.3586],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.4336, -0.1873, -1.4756,  ...,  3.4961,  0.3096,  5.1641],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0042, -0.0148,  0.0006,  ...,  0.0080, -0.0067, -0.0075],
        [ 0.0022, -0.0114, -0.0021,  ..., -0.0019, -0.0129, -0.0008],
        [-0.0038,  0.0011, -0.0040,  ..., -0.0011,  0.0027, -0.0030],
        ...,
        [-0.0027,  0.0004, -0.0019,  ...,  0.0016,  0.0041,  0.0052],
        [-0.0059, -0.0062, -0.0026,  ..., -0.0063, -0.0088,  0.0033],
        [-0.0087,  0.0166,  0.0053,  ..., -0.0124, -0.0030, -0.0083]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.0312, -0.3716, -1.0693,  ...,  3.8203,  0.3567,  5.1211]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 01:13:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most hungry, it is hungriest
If something is the most tiny, it is tiniest
If something is the most hot, it is hottest
If something is the most scary, it is scariest
If something is the most angry, it is angriest
If something is the most merry, it is merriest
If something is the most lengthy, it is lengthiest
If something is the most harsh, it is
2024-07-17 01:13:40 root INFO     [order_1_approx] starting weight calculation for If something is the most scary, it is scariest
If something is the most hot, it is hottest
If something is the most angry, it is angriest
If something is the most lengthy, it is lengthiest
If something is the most merry, it is merriest
If something is the most harsh, it is harshest
If something is the most hungry, it is hungriest
If something is the most tiny, it is
2024-07-17 01:13:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 01:17:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1074,  0.9619,  0.3425,  ..., -1.8154,  0.7788, -0.1952],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4570, -0.0928,  1.4131,  ...,  2.0000, -1.4766,  3.2227],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0046, -0.0122,  0.0102,  ...,  0.0013, -0.0105, -0.0100],
        [ 0.0069, -0.0069,  0.0017,  ...,  0.0081, -0.0105,  0.0044],
        [-0.0036, -0.0003, -0.0026,  ..., -0.0007, -0.0060,  0.0009],
        ...,
        [-0.0069, -0.0147, -0.0003,  ..., -0.0017,  0.0005, -0.0090],
        [-0.0086, -0.0051,  0.0009,  ..., -0.0035, -0.0179, -0.0047],
        [-0.0026,  0.0075,  0.0014,  ..., -0.0154, -0.0004, -0.0249]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6680,  0.0176,  1.2480,  ...,  2.1836, -1.4170,  3.7012]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 01:17:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most scary, it is scariest
If something is the most hot, it is hottest
If something is the most angry, it is angriest
If something is the most lengthy, it is lengthiest
If something is the most merry, it is merriest
If something is the most harsh, it is harshest
If something is the most hungry, it is hungriest
If something is the most tiny, it is
2024-07-17 01:17:06 root INFO     [order_1_approx] starting weight calculation for If something is the most tiny, it is tiniest
If something is the most harsh, it is harshest
If something is the most angry, it is angriest
If something is the most hungry, it is hungriest
If something is the most hot, it is hottest
If something is the most merry, it is merriest
If something is the most lengthy, it is lengthiest
If something is the most scary, it is
2024-07-17 01:17:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 01:20:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9585,  0.5420,  0.1606,  ..., -0.1152,  1.5322,  0.2297],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.7891,  1.1289,  1.0078,  ...,  3.8516, -0.8511,  0.0420],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-6.9351e-03, -1.0071e-02,  3.5629e-03,  ..., -8.4639e-05,
         -2.9335e-03, -6.9389e-03],
        [-5.1842e-03, -7.4959e-03,  5.8365e-03,  ...,  5.4703e-03,
         -1.2764e-02, -5.9586e-03],
        [-8.5907e-03, -6.8169e-03, -8.4534e-03,  ..., -3.2024e-03,
          3.5076e-03,  7.3051e-04],
        ...,
        [ 2.4509e-03, -3.5782e-03,  3.1719e-03,  ..., -1.9121e-03,
         -2.5368e-03, -7.6790e-03],
        [-6.3744e-03,  1.0757e-03,  6.3705e-03,  ...,  1.4172e-03,
         -1.1299e-02,  5.5161e-03],
        [-6.2370e-03,  1.6968e-02, -2.2125e-03,  ..., -1.1612e-02,
         -2.0237e-03, -2.0477e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.5898,  1.3926,  0.9561,  ...,  3.8613, -1.0898,  0.2085]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 01:20:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most tiny, it is tiniest
If something is the most harsh, it is harshest
If something is the most angry, it is angriest
If something is the most hungry, it is hungriest
If something is the most hot, it is hottest
If something is the most merry, it is merriest
If something is the most lengthy, it is lengthiest
If something is the most scary, it is
2024-07-17 01:20:33 root INFO     [order_1_approx] starting weight calculation for If something is the most tiny, it is tiniest
If something is the most merry, it is merriest
If something is the most scary, it is scariest
If something is the most hungry, it is hungriest
If something is the most hot, it is hottest
If something is the most lengthy, it is lengthiest
If something is the most harsh, it is harshest
If something is the most angry, it is
2024-07-17 01:20:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 01:23:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5859,  0.4434,  1.1777,  ..., -0.6675,  1.2734,  1.0264],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.0371,  0.4609, -3.7676,  ...,  4.1602,  2.4258,  3.3477],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.8853e-03, -1.2085e-02,  6.6757e-03,  ...,  4.3583e-04,
          1.1406e-03, -6.9122e-03],
        [-9.4299e-03,  2.9564e-04,  3.8452e-03,  ...,  1.3981e-03,
         -3.9368e-03, -2.4014e-03],
        [ 1.2951e-03,  3.6106e-03, -6.5575e-03,  ..., -7.2556e-03,
         -4.8103e-03,  9.9182e-03],
        ...,
        [ 1.0628e-02,  3.3188e-03, -5.6686e-03,  ...,  2.1248e-03,
          2.9430e-03, -7.7724e-05],
        [ 4.2200e-04, -4.2915e-03,  2.9469e-03,  ..., -2.2564e-03,
         -1.2177e-02,  5.9052e-03],
        [ 2.0752e-03,  9.6054e-03,  1.2169e-02,  ...,  5.2414e-03,
          5.3062e-03, -1.5793e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.2363,  0.2759, -3.9258,  ...,  4.3477,  2.1328,  3.4355]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 01:24:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most tiny, it is tiniest
If something is the most merry, it is merriest
If something is the most scary, it is scariest
If something is the most hungry, it is hungriest
If something is the most hot, it is hottest
If something is the most lengthy, it is lengthiest
If something is the most harsh, it is harshest
If something is the most angry, it is
2024-07-17 01:24:00 root INFO     total operator prediction time: 1657.9451067447662 seconds
2024-07-17 01:24:00 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+er_irreg
2024-07-17 01:24:00 root INFO     building operator verb+er_irreg
2024-07-17 01:24:00 root INFO     [order_1_approx] starting weight calculation for If you begin something, you are a beginner
If you skydive something, you are a skydiver
If you borrow something, you are a borrower
If you interpret something, you are a interpreter
If you compose something, you are a composer
If you tell something, you are a teller
If you destroy something, you are a destroyer
If you perform something, you are a
2024-07-17 01:24:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 01:27:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.4482, -0.6992,  0.5703,  ..., -1.8770,  0.2079, -0.4268],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.3477, -1.0391, -1.7920,  ..., -3.2148,  2.3867,  3.4766],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.5013e-03,  2.3499e-03, -3.0136e-03,  ..., -2.7771e-03,
         -4.3602e-03, -4.9973e-03],
        [-4.9438e-03,  6.8054e-03, -1.0643e-03,  ..., -2.0790e-04,
         -4.6158e-03, -4.7989e-03],
        [ 1.9798e-03,  8.3542e-03, -8.3542e-03,  ..., -8.9455e-04,
          2.2049e-03, -1.1734e-02],
        ...,
        [-1.2344e-02,  2.2469e-03,  8.6594e-03,  ...,  3.7327e-03,
         -4.2915e-03, -1.0109e-02],
        [ 3.6659e-03, -1.0166e-03, -4.5776e-05,  ..., -5.3329e-03,
         -2.2011e-03,  1.1948e-02],
        [-1.0376e-02,  1.3901e-02,  4.5242e-03,  ..., -9.1019e-03,
         -1.0162e-02,  2.9259e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.0391, -1.1445, -2.3047,  ..., -3.0469,  2.1055,  4.2539]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 01:27:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you begin something, you are a beginner
If you skydive something, you are a skydiver
If you borrow something, you are a borrower
If you interpret something, you are a interpreter
If you compose something, you are a composer
If you tell something, you are a teller
If you destroy something, you are a destroyer
If you perform something, you are a
2024-07-17 01:27:26 root INFO     [order_1_approx] starting weight calculation for If you borrow something, you are a borrower
If you begin something, you are a beginner
If you compose something, you are a composer
If you destroy something, you are a destroyer
If you tell something, you are a teller
If you perform something, you are a performer
If you interpret something, you are a interpreter
If you skydive something, you are a
2024-07-17 01:27:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 01:30:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3564, -0.4329, -1.7285,  ..., -1.1211,  1.8564,  0.9834],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6299, -1.2188,  0.9648,  ..., -7.7305, -5.0703, -0.8472],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0044,  0.0029, -0.0010,  ..., -0.0030, -0.0018, -0.0004],
        [ 0.0052,  0.0084,  0.0017,  ...,  0.0005,  0.0019,  0.0046],
        [-0.0017,  0.0030,  0.0035,  ...,  0.0018, -0.0014, -0.0026],
        ...,
        [-0.0015, -0.0049, -0.0015,  ...,  0.0006, -0.0021,  0.0051],
        [ 0.0065,  0.0011, -0.0023,  ..., -0.0027,  0.0011,  0.0038],
        [-0.0021,  0.0063,  0.0014,  ..., -0.0038, -0.0038,  0.0079]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3872, -1.2900,  0.4763,  ..., -7.8711, -4.8125, -0.9258]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 01:30:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you borrow something, you are a borrower
If you begin something, you are a beginner
If you compose something, you are a composer
If you destroy something, you are a destroyer
If you tell something, you are a teller
If you perform something, you are a performer
If you interpret something, you are a interpreter
If you skydive something, you are a
2024-07-17 01:30:53 root INFO     [order_1_approx] starting weight calculation for If you perform something, you are a performer
If you skydive something, you are a skydiver
If you destroy something, you are a destroyer
If you tell something, you are a teller
If you interpret something, you are a interpreter
If you borrow something, you are a borrower
If you compose something, you are a composer
If you begin something, you are a
2024-07-17 01:30:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 01:34:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.9658, -0.1105,  0.4380,  ..., -0.0889, -0.7373,  0.0893],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.4961,  0.2385, -4.5938,  ..., -1.4121,  5.3164,  4.1953],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0036, -0.0033,  0.0030,  ..., -0.0046, -0.0061, -0.0168],
        [ 0.0075,  0.0146, -0.0079,  ..., -0.0026,  0.0014,  0.0016],
        [ 0.0031,  0.0063, -0.0090,  ..., -0.0019,  0.0085, -0.0025],
        ...,
        [-0.0008, -0.0030, -0.0003,  ...,  0.0037, -0.0019,  0.0025],
        [-0.0062, -0.0037,  0.0078,  ..., -0.0041, -0.0020,  0.0040],
        [-0.0065,  0.0057,  0.0035,  ..., -0.0155,  0.0043, -0.0101]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.9590,  0.5107, -4.6328,  ..., -1.3223,  5.3867,  3.6719]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 01:34:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you perform something, you are a performer
If you skydive something, you are a skydiver
If you destroy something, you are a destroyer
If you tell something, you are a teller
If you interpret something, you are a interpreter
If you borrow something, you are a borrower
If you compose something, you are a composer
If you begin something, you are a
2024-07-17 01:34:19 root INFO     [order_1_approx] starting weight calculation for If you compose something, you are a composer
If you begin something, you are a beginner
If you tell something, you are a teller
If you destroy something, you are a destroyer
If you perform something, you are a performer
If you skydive something, you are a skydiver
If you interpret something, you are a interpreter
If you borrow something, you are a
2024-07-17 01:34:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 01:37:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7422, -2.6367,  0.8511,  ..., -0.7344, -1.4971,  1.5684],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.1875, -1.7578, -0.9263,  ..., -1.3574, -0.5483,  5.5430],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0009,  0.0040,  0.0024,  ...,  0.0007, -0.0100, -0.0045],
        [-0.0006,  0.0065, -0.0078,  ..., -0.0025,  0.0055,  0.0032],
        [ 0.0015,  0.0054,  0.0030,  ..., -0.0098, -0.0090,  0.0056],
        ...,
        [ 0.0028, -0.0098, -0.0029,  ...,  0.0084,  0.0035, -0.0020],
        [-0.0050, -0.0159,  0.0031,  ...,  0.0013,  0.0043,  0.0073],
        [-0.0075,  0.0033,  0.0065,  ..., -0.0001,  0.0092,  0.0004]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.2891, -1.2588, -0.6304,  ..., -2.0664, -0.8643,  5.7070]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 01:37:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you compose something, you are a composer
If you begin something, you are a beginner
If you tell something, you are a teller
If you destroy something, you are a destroyer
If you perform something, you are a performer
If you skydive something, you are a skydiver
If you interpret something, you are a interpreter
If you borrow something, you are a
2024-07-17 01:37:45 root INFO     [order_1_approx] starting weight calculation for If you begin something, you are a beginner
If you skydive something, you are a skydiver
If you borrow something, you are a borrower
If you interpret something, you are a interpreter
If you compose something, you are a composer
If you destroy something, you are a destroyer
If you perform something, you are a performer
If you tell something, you are a
2024-07-17 01:37:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 01:41:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0110,  0.6602,  0.2754,  ...,  0.3750, -0.1001,  0.5552],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6758,  1.7031, -3.6816,  ..., -1.5420,  2.1230,  3.0117],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0046,  0.0026,  0.0012,  ..., -0.0115, -0.0123,  0.0004],
        [-0.0049,  0.0092,  0.0089,  ...,  0.0040, -0.0020,  0.0003],
        [ 0.0004, -0.0044,  0.0024,  ..., -0.0062,  0.0021,  0.0020],
        ...,
        [-0.0028, -0.0073, -0.0051,  ..., -0.0013,  0.0046, -0.0025],
        [ 0.0018,  0.0022,  0.0016,  ...,  0.0080, -0.0024, -0.0016],
        [-0.0083,  0.0082,  0.0029,  ..., -0.0099, -0.0015,  0.0052]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0098,  1.8447, -4.4844,  ..., -0.6040,  1.1465,  3.6289]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 01:41:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you begin something, you are a beginner
If you skydive something, you are a skydiver
If you borrow something, you are a borrower
If you interpret something, you are a interpreter
If you compose something, you are a composer
If you destroy something, you are a destroyer
If you perform something, you are a performer
If you tell something, you are a
2024-07-17 01:41:13 root INFO     [order_1_approx] starting weight calculation for If you begin something, you are a beginner
If you destroy something, you are a destroyer
If you tell something, you are a teller
If you borrow something, you are a borrower
If you skydive something, you are a skydiver
If you perform something, you are a performer
If you compose something, you are a composer
If you interpret something, you are a
2024-07-17 01:41:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 01:44:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5244, -0.4258,  0.0701,  ...,  0.4504,  0.8550,  0.1836],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.2256,  1.5547, -1.1387,  ..., -1.0010,  3.1211,  5.8984],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.2314e-02, -4.1962e-03,  3.9062e-03,  ..., -5.5008e-03,
         -7.7820e-03, -4.7684e-03],
        [-7.2479e-05,  6.1340e-03,  8.4381e-03,  ..., -2.7771e-03,
         -6.2218e-03, -2.7084e-04],
        [ 3.9406e-03, -5.1575e-03,  3.5133e-03,  ...,  2.0561e-03,
          2.4586e-03, -2.4471e-03],
        ...,
        [-5.3406e-03, -4.4327e-03, -5.3101e-03,  ...,  1.4725e-02,
          6.2180e-04, -1.0063e-02],
        [-1.0078e-02, -3.0613e-03,  8.2321e-03,  ...,  1.5106e-02,
          5.8126e-04,  4.4632e-03],
        [ 2.4185e-03,  6.6071e-03, -3.6125e-03,  ..., -1.6281e-02,
         -4.0016e-03,  9.3155e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8232,  2.1055, -1.5840,  ..., -1.2021,  3.1816,  5.8633]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 01:44:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you begin something, you are a beginner
If you destroy something, you are a destroyer
If you tell something, you are a teller
If you borrow something, you are a borrower
If you skydive something, you are a skydiver
If you perform something, you are a performer
If you compose something, you are a composer
If you interpret something, you are a
2024-07-17 01:44:39 root INFO     [order_1_approx] starting weight calculation for If you tell something, you are a teller
If you perform something, you are a performer
If you interpret something, you are a interpreter
If you begin something, you are a beginner
If you destroy something, you are a destroyer
If you skydive something, you are a skydiver
If you borrow something, you are a borrower
If you compose something, you are a
2024-07-17 01:44:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 01:48:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4397, -0.1143, -0.6343,  ..., -1.0957, -0.2935,  0.4705],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7305,  0.6738, -4.3359,  ...,  0.1836, -0.1019,  1.3877],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 6.7749e-03, -6.9046e-03,  4.0283e-03,  ..., -1.7891e-03,
         -6.5689e-03, -3.7651e-03],
        [ 2.0523e-03,  5.5962e-03,  5.0468e-03,  ...,  2.0313e-03,
         -1.0252e-03,  1.4734e-03],
        [ 8.5220e-03,  3.3512e-03, -1.6422e-03,  ...,  7.9012e-04,
         -2.5883e-03,  2.6245e-03],
        ...,
        [-6.9351e-03, -8.0414e-03, -5.2528e-03,  ...,  4.1656e-03,
         -9.3269e-04, -3.9673e-04],
        [ 4.6844e-03, -5.3406e-05,  3.3569e-03,  ...,  2.6627e-03,
          4.9629e-03,  8.1787e-03],
        [-1.2680e-02,  8.0643e-03,  3.5992e-03,  ..., -6.5651e-03,
          2.0638e-03,  3.3627e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.9199,  1.1904, -4.8867,  ...,  0.5811, -0.5396,  1.5898]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 01:48:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you tell something, you are a teller
If you perform something, you are a performer
If you interpret something, you are a interpreter
If you begin something, you are a beginner
If you destroy something, you are a destroyer
If you skydive something, you are a skydiver
If you borrow something, you are a borrower
If you compose something, you are a
2024-07-17 01:48:06 root INFO     [order_1_approx] starting weight calculation for If you begin something, you are a beginner
If you compose something, you are a composer
If you skydive something, you are a skydiver
If you tell something, you are a teller
If you borrow something, you are a borrower
If you interpret something, you are a interpreter
If you perform something, you are a performer
If you destroy something, you are a
2024-07-17 01:48:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 01:51:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5439, -0.9644,  0.5728,  ..., -0.1670, -0.7461, -0.1072],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.8828, -2.2031, -3.2012,  ..., -1.9521,  3.2500,  5.1641],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.7035e-03, -4.7493e-04,  4.0741e-03,  ..., -4.6387e-03,
         -9.2850e-03, -3.8509e-03],
        [ 3.0689e-03,  8.5678e-03, -1.5497e-05,  ...,  1.8454e-03,
         -9.9468e-04,  3.1590e-04],
        [-2.9640e-03,  4.6310e-03,  6.0577e-03,  ..., -2.0180e-03,
         -4.5967e-04,  2.2621e-03],
        ...,
        [-4.5586e-03, -1.0567e-02, -4.2152e-03,  ...,  5.3864e-03,
         -3.5076e-03,  3.5210e-03],
        [ 3.0365e-03, -5.4169e-03,  2.0409e-04,  ..., -3.7518e-03,
          1.4591e-03,  3.5706e-03],
        [-5.4474e-03, -4.0817e-03,  6.6452e-03,  ..., -7.0267e-03,
         -3.9062e-03,  6.5689e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.8633, -2.6348, -3.7461,  ..., -1.7285,  2.5977,  6.0430]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 01:51:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you begin something, you are a beginner
If you compose something, you are a composer
If you skydive something, you are a skydiver
If you tell something, you are a teller
If you borrow something, you are a borrower
If you interpret something, you are a interpreter
If you perform something, you are a performer
If you destroy something, you are a
2024-07-17 01:51:34 root INFO     total operator prediction time: 1654.3838391304016 seconds
2024-07-17 01:51:34 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on over+adj_reg
2024-07-17 01:51:34 root INFO     building operator over+adj_reg
2024-07-17 01:51:34 root INFO     [order_1_approx] starting weight calculation for If something is too heated, it is overheated
If something is too filled, it is overfilled
If something is too sized, it is oversized
If something is too enthusiastic, it is overenthusiastic
If something is too protective, it is overprotective
If something is too qualified, it is overqualified
If something is too turned, it is overturned
If something is too stretched, it is
2024-07-17 01:51:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 01:55:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6084,  1.1680,  0.6509,  ...,  0.0093,  1.5830, -1.6045],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1016, -0.0642, -2.1133,  ...,  1.1367,  0.0549,  0.2959],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0075, -0.0250, -0.0110,  ..., -0.0033, -0.0072, -0.0024],
        [-0.0042,  0.0131, -0.0007,  ...,  0.0097, -0.0034, -0.0045],
        [-0.0073,  0.0050, -0.0027,  ..., -0.0064, -0.0052, -0.0030],
        ...,
        [-0.0010, -0.0097, -0.0028,  ...,  0.0021,  0.0031,  0.0004],
        [ 0.0034,  0.0082,  0.0024,  ...,  0.0054, -0.0089, -0.0012],
        [-0.0104,  0.0067,  0.0074,  ..., -0.0062,  0.0079,  0.0051]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5547,  0.4524, -2.5508,  ...,  0.4561, -0.1980,  0.4404]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 01:55:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too heated, it is overheated
If something is too filled, it is overfilled
If something is too sized, it is oversized
If something is too enthusiastic, it is overenthusiastic
If something is too protective, it is overprotective
If something is too qualified, it is overqualified
If something is too turned, it is overturned
If something is too stretched, it is
2024-07-17 01:55:02 root INFO     [order_1_approx] starting weight calculation for If something is too heated, it is overheated
If something is too turned, it is overturned
If something is too filled, it is overfilled
If something is too qualified, it is overqualified
If something is too stretched, it is overstretched
If something is too enthusiastic, it is overenthusiastic
If something is too sized, it is oversized
If something is too protective, it is
2024-07-17 01:55:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 01:58:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.7969,  0.9751,  0.7627,  ..., -0.8672,  1.5117,  0.8735],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4834,  1.0176, -0.2432,  ..., -0.4639,  1.5371, -0.5845],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0053, -0.0134,  0.0078,  ...,  0.0008, -0.0095, -0.0149],
        [-0.0047,  0.0024,  0.0004,  ...,  0.0015, -0.0041, -0.0017],
        [-0.0051, -0.0018, -0.0018,  ..., -0.0031,  0.0005,  0.0050],
        ...,
        [-0.0094, -0.0004, -0.0095,  ...,  0.0110, -0.0149, -0.0032],
        [-0.0036,  0.0010,  0.0057,  ...,  0.0037,  0.0081,  0.0016],
        [-0.0064,  0.0071,  0.0079,  ..., -0.0106, -0.0010,  0.0028]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1179,  1.3584, -0.1727,  ..., -0.4966,  1.4268, -0.8213]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 01:58:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too heated, it is overheated
If something is too turned, it is overturned
If something is too filled, it is overfilled
If something is too qualified, it is overqualified
If something is too stretched, it is overstretched
If something is too enthusiastic, it is overenthusiastic
If something is too sized, it is oversized
If something is too protective, it is
2024-07-17 01:58:30 root INFO     [order_1_approx] starting weight calculation for If something is too stretched, it is overstretched
If something is too filled, it is overfilled
If something is too enthusiastic, it is overenthusiastic
If something is too protective, it is overprotective
If something is too heated, it is overheated
If something is too qualified, it is overqualified
If something is too turned, it is overturned
If something is too sized, it is
2024-07-17 01:58:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 02:01:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2100,  1.8262, -1.0244,  ..., -1.2402,  2.6328, -0.2366],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6895, -1.6152,  1.5996,  ..., -3.5078,  3.0938, -0.2168],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0013, -0.0195, -0.0042,  ..., -0.0007, -0.0047, -0.0083],
        [ 0.0034,  0.0053,  0.0029,  ...,  0.0076, -0.0040,  0.0004],
        [-0.0072,  0.0022, -0.0027,  ..., -0.0052, -0.0120,  0.0007],
        ...,
        [-0.0089, -0.0108,  0.0045,  ...,  0.0116,  0.0090, -0.0019],
        [ 0.0015, -0.0075, -0.0020,  ..., -0.0020,  0.0036,  0.0039],
        [-0.0003,  0.0090,  0.0082,  ..., -0.0156,  0.0091,  0.0073]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6377, -1.4287,  2.2070,  ..., -4.0430,  2.8496,  0.0305]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 02:01:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too stretched, it is overstretched
If something is too filled, it is overfilled
If something is too enthusiastic, it is overenthusiastic
If something is too protective, it is overprotective
If something is too heated, it is overheated
If something is too qualified, it is overqualified
If something is too turned, it is overturned
If something is too sized, it is
2024-07-17 02:01:57 root INFO     [order_1_approx] starting weight calculation for If something is too protective, it is overprotective
If something is too enthusiastic, it is overenthusiastic
If something is too stretched, it is overstretched
If something is too sized, it is oversized
If something is too heated, it is overheated
If something is too qualified, it is overqualified
If something is too filled, it is overfilled
If something is too turned, it is
2024-07-17 02:01:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 02:05:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4639,  1.4590,  0.7715,  ...,  0.5703,  1.0908, -0.4277],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4492, -1.5225, -0.6006,  ..., -1.2197,  3.5371,  1.2383],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0138, -0.0072,  0.0048,  ..., -0.0010, -0.0209, -0.0090],
        [-0.0028,  0.0086,  0.0011,  ...,  0.0047,  0.0051, -0.0074],
        [-0.0144,  0.0018, -0.0092,  ..., -0.0195,  0.0061,  0.0040],
        ...,
        [ 0.0069, -0.0054, -0.0003,  ...,  0.0001, -0.0041, -0.0038],
        [-0.0218, -0.0095, -0.0012,  ..., -0.0083, -0.0120, -0.0045],
        [ 0.0006,  0.0116,  0.0132,  ...,  0.0061,  0.0033,  0.0002]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7847, -0.6621, -0.5088,  ..., -1.9570,  4.7852,  1.2871]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 02:05:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too protective, it is overprotective
If something is too enthusiastic, it is overenthusiastic
If something is too stretched, it is overstretched
If something is too sized, it is oversized
If something is too heated, it is overheated
If something is too qualified, it is overqualified
If something is too filled, it is overfilled
If something is too turned, it is
2024-07-17 02:05:25 root INFO     [order_1_approx] starting weight calculation for If something is too stretched, it is overstretched
If something is too qualified, it is overqualified
If something is too sized, it is oversized
If something is too turned, it is overturned
If something is too filled, it is overfilled
If something is too enthusiastic, it is overenthusiastic
If something is too protective, it is overprotective
If something is too heated, it is
2024-07-17 02:05:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 02:08:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.8115,  0.4321,  0.0474,  ..., -1.7354,  2.3262,  0.1506],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.1191, -0.1185, -1.4492,  ..., -0.9961,  3.3926,  2.5781],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0049, -0.0160, -0.0017,  ..., -0.0017, -0.0111, -0.0075],
        [-0.0003,  0.0059, -0.0042,  ...,  0.0153, -0.0021,  0.0027],
        [-0.0097, -0.0024, -0.0027,  ..., -0.0018,  0.0106,  0.0027],
        ...,
        [-0.0093, -0.0006, -0.0005,  ...,  0.0014,  0.0054, -0.0027],
        [-0.0027, -0.0049, -0.0051,  ..., -0.0024, -0.0115, -0.0014],
        [-0.0054,  0.0021, -0.0033,  ..., -0.0054, -0.0032, -0.0039]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.9648,  0.0781, -1.5166,  ..., -1.5049,  3.3086,  2.2246]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 02:08:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too stretched, it is overstretched
If something is too qualified, it is overqualified
If something is too sized, it is oversized
If something is too turned, it is overturned
If something is too filled, it is overfilled
If something is too enthusiastic, it is overenthusiastic
If something is too protective, it is overprotective
If something is too heated, it is
2024-07-17 02:08:51 root INFO     [order_1_approx] starting weight calculation for If something is too sized, it is oversized
If something is too turned, it is overturned
If something is too heated, it is overheated
If something is too enthusiastic, it is overenthusiastic
If something is too protective, it is overprotective
If something is too stretched, it is overstretched
If something is too filled, it is overfilled
If something is too qualified, it is
2024-07-17 02:08:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 02:12:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7158, -0.2583,  0.9639,  ..., -1.6348,  2.1738,  0.4973],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([1.4678, 0.1582, 0.2539,  ..., 1.7471, 3.6953, 0.7139], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0129, -0.0289,  0.0130,  ..., -0.0029,  0.0007, -0.0152],
        [-0.0033,  0.0026,  0.0044,  ...,  0.0221, -0.0154, -0.0036],
        [-0.0028, -0.0094, -0.0144,  ..., -0.0017, -0.0042,  0.0034],
        ...,
        [-0.0100, -0.0091, -0.0014,  ..., -0.0042, -0.0063, -0.0092],
        [-0.0007, -0.0130,  0.0127,  ..., -0.0004, -0.0103,  0.0066],
        [-0.0011,  0.0129,  0.0063,  ..., -0.0013,  0.0172, -0.0138]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[1.7627, 0.9497, 0.2632,  ..., 1.5391, 3.5820, 0.7251]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 02:12:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too sized, it is oversized
If something is too turned, it is overturned
If something is too heated, it is overheated
If something is too enthusiastic, it is overenthusiastic
If something is too protective, it is overprotective
If something is too stretched, it is overstretched
If something is too filled, it is overfilled
If something is too qualified, it is
2024-07-17 02:12:18 root INFO     [order_1_approx] starting weight calculation for If something is too turned, it is overturned
If something is too sized, it is oversized
If something is too stretched, it is overstretched
If something is too heated, it is overheated
If something is too filled, it is overfilled
If something is too protective, it is overprotective
If something is too qualified, it is overqualified
If something is too enthusiastic, it is
2024-07-17 02:12:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 02:15:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.7109,  0.2043,  0.7559,  ..., -1.0430,  2.3652,  0.1205],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5684, -1.7393,  1.2139,  ...,  0.4775,  6.4062,  1.1172],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0002, -0.0120,  0.0083,  ..., -0.0047, -0.0101, -0.0114],
        [-0.0033, -0.0037, -0.0026,  ...,  0.0067, -0.0019, -0.0053],
        [-0.0040, -0.0037, -0.0075,  ..., -0.0050,  0.0122, -0.0001],
        ...,
        [-0.0088, -0.0082, -0.0025,  ...,  0.0045,  0.0061, -0.0039],
        [ 0.0008, -0.0036,  0.0010,  ...,  0.0092, -0.0205,  0.0098],
        [-0.0065, -0.0010,  0.0059,  ...,  0.0041,  0.0026, -0.0052]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.8301, -1.9697,  1.1826,  ...,  0.4966,  6.6992,  1.2305]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 02:15:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too turned, it is overturned
If something is too sized, it is oversized
If something is too stretched, it is overstretched
If something is too heated, it is overheated
If something is too filled, it is overfilled
If something is too protective, it is overprotective
If something is too qualified, it is overqualified
If something is too enthusiastic, it is
2024-07-17 02:15:45 root INFO     [order_1_approx] starting weight calculation for If something is too stretched, it is overstretched
If something is too turned, it is overturned
If something is too heated, it is overheated
If something is too enthusiastic, it is overenthusiastic
If something is too sized, it is oversized
If something is too qualified, it is overqualified
If something is too protective, it is overprotective
If something is too filled, it is
2024-07-17 02:15:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 02:19:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5181,  0.8970,  0.8271,  ..., -0.7891,  1.4033, -1.1768],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3164,  0.9170, -1.4482,  ...,  1.7793,  1.2559, -0.1196],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0012, -0.0145, -0.0078,  ...,  0.0090, -0.0160, -0.0222],
        [-0.0047,  0.0043, -0.0015,  ...,  0.0023, -0.0026, -0.0080],
        [-0.0004, -0.0043, -0.0146,  ..., -0.0055,  0.0019,  0.0031],
        ...,
        [ 0.0056, -0.0052, -0.0024,  ...,  0.0098, -0.0167,  0.0002],
        [-0.0139, -0.0049,  0.0008,  ..., -0.0016, -0.0021,  0.0013],
        [-0.0073,  0.0025,  0.0039,  ..., -0.0042,  0.0045,  0.0013]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2842,  1.3848, -1.4805,  ...,  2.1016,  1.4268,  0.5239]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 02:19:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too stretched, it is overstretched
If something is too turned, it is overturned
If something is too heated, it is overheated
If something is too enthusiastic, it is overenthusiastic
If something is too sized, it is oversized
If something is too qualified, it is overqualified
If something is too protective, it is overprotective
If something is too filled, it is
2024-07-17 02:19:13 root INFO     total operator prediction time: 1658.4343593120575 seconds
2024-07-17 02:19:13 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on adj+ly_reg
2024-07-17 02:19:13 root INFO     building operator adj+ly_reg
2024-07-17 02:19:13 root INFO     [order_1_approx] starting weight calculation for The adjective form of physical is physically
The adjective form of creative is creatively
The adjective form of additional is additionally
The adjective form of internal is internally
The adjective form of strong is strongly
The adjective form of financial is financially
The adjective form of according is accordingly
The adjective form of political is
2024-07-17 02:19:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 02:22:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.7158,  0.3179,  0.6348,  ...,  0.0071, -0.1450, -0.1196],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.0469, -1.9658,  1.5527,  ..., -2.9902, -1.9785, -0.1924],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-3.2177e-03, -4.8065e-03,  1.5236e-02,  ..., -1.0704e-02,
         -2.3251e-03, -1.4748e-02],
        [ 3.4142e-03,  2.7142e-03,  1.1772e-02,  ...,  8.9264e-03,
         -8.4229e-03, -4.4708e-03],
        [ 9.0981e-04, -1.8265e-02, -5.7182e-03,  ..., -1.8768e-03,
         -7.9727e-03,  1.0300e-02],
        ...,
        [-4.8180e-03, -1.1635e-04,  1.7929e-04,  ...,  4.3411e-03,
          4.5357e-03,  1.9073e-05],
        [-2.9583e-03, -2.3460e-03, -1.9989e-02,  ..., -1.0696e-02,
         -6.9695e-03,  1.8082e-03],
        [-1.0368e-02,  5.2185e-03, -5.3253e-03,  ..., -7.5264e-03,
          3.3054e-03, -1.2024e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.3047, -2.0430,  1.5449,  ..., -2.8125, -2.4414, -0.1810]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 02:22:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of physical is physically
The adjective form of creative is creatively
The adjective form of additional is additionally
The adjective form of internal is internally
The adjective form of strong is strongly
The adjective form of financial is financially
The adjective form of according is accordingly
The adjective form of political is
2024-07-17 02:22:41 root INFO     [order_1_approx] starting weight calculation for The adjective form of physical is physically
The adjective form of creative is creatively
The adjective form of additional is additionally
The adjective form of according is accordingly
The adjective form of strong is strongly
The adjective form of political is politically
The adjective form of financial is financially
The adjective form of internal is
2024-07-17 02:22:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 02:26:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4741,  0.0637, -0.2350,  ...,  0.2632,  0.2390,  1.6055],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3047,  1.8730, -1.3965,  ...,  1.9014, -2.0977,  0.9883],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.1398e-02,  2.5902e-03,  1.4061e-02,  ..., -9.5673e-03,
          1.5282e-02, -1.8234e-03],
        [ 1.7357e-04,  2.1591e-03,  8.3160e-03,  ...,  1.4458e-02,
         -4.9019e-03, -2.9964e-03],
        [ 4.9019e-03, -8.7204e-03, -6.8092e-03,  ..., -8.7204e-03,
          9.9869e-03,  8.2397e-03],
        ...,
        [ 5.6839e-03, -6.2447e-03, -2.0504e-04,  ...,  4.1733e-03,
         -6.3896e-03, -7.9193e-03],
        [-5.3749e-03, -1.8326e-02,  1.7872e-03,  ..., -1.0437e-02,
         -1.5900e-02, -9.7427e-03],
        [-8.4229e-03,  3.3455e-03, -2.3422e-03,  ..., -1.0277e-02,
         -9.4414e-05, -1.8204e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7422,  1.5898, -1.3447,  ...,  2.0918, -1.2549,  1.1152]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 02:26:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of physical is physically
The adjective form of creative is creatively
The adjective form of additional is additionally
The adjective form of according is accordingly
The adjective form of strong is strongly
The adjective form of political is politically
The adjective form of financial is financially
The adjective form of internal is
2024-07-17 02:26:08 root INFO     [order_1_approx] starting weight calculation for The adjective form of strong is strongly
The adjective form of additional is additionally
The adjective form of physical is physically
The adjective form of according is accordingly
The adjective form of internal is internally
The adjective form of political is politically
The adjective form of financial is financially
The adjective form of creative is
2024-07-17 02:26:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 02:29:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.6621, -0.1338,  0.0234,  ...,  0.1726,  0.4001,  0.5107],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.0781, -0.1165,  0.3047,  ...,  0.8594, -1.0312, -2.7852],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.6245e-03, -8.1940e-03,  6.5041e-03,  ..., -9.3689e-03,
         -1.5259e-05, -1.2085e-02],
        [-6.7596e-03,  2.2545e-03,  9.4910e-03,  ..., -2.0599e-03,
         -2.9106e-03, -9.0561e-03],
        [-4.4441e-03, -1.4099e-02, -1.0529e-02,  ..., -3.1853e-03,
          2.9774e-03,  3.6469e-03],
        ...,
        [-4.8971e-04,  5.3711e-03, -9.1743e-04,  ...,  7.0419e-03,
          4.0436e-04, -1.3268e-02],
        [-3.4981e-03, -5.4436e-03, -4.3373e-03,  ..., -4.4327e-03,
         -2.0485e-03,  1.6441e-03],
        [-9.9030e-03,  1.0498e-02, -5.2452e-05,  ..., -1.0132e-02,
         -2.1219e-04, -1.1917e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.8750, -0.0297,  1.3291,  ...,  0.1333, -1.3848, -2.7305]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 02:29:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of strong is strongly
The adjective form of additional is additionally
The adjective form of physical is physically
The adjective form of according is accordingly
The adjective form of internal is internally
The adjective form of political is politically
The adjective form of financial is financially
The adjective form of creative is
2024-07-17 02:29:35 root INFO     [order_1_approx] starting weight calculation for The adjective form of financial is financially
The adjective form of strong is strongly
The adjective form of political is politically
The adjective form of physical is physically
The adjective form of creative is creatively
The adjective form of according is accordingly
The adjective form of internal is internally
The adjective form of additional is
2024-07-17 02:29:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 02:33:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0074,  0.7969,  1.3340,  ..., -1.2451,  0.5698,  1.0996],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0557,  0.2646,  3.2930,  ..., -1.8516,  1.6328,  3.1504],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0103, -0.0040,  0.0165,  ..., -0.0037, -0.0036, -0.0183],
        [ 0.0065, -0.0095,  0.0071,  ...,  0.0129, -0.0019,  0.0022],
        [-0.0060,  0.0061, -0.0079,  ...,  0.0044, -0.0046,  0.0043],
        ...,
        [ 0.0006, -0.0031,  0.0024,  ..., -0.0071, -0.0025, -0.0062],
        [ 0.0022, -0.0125,  0.0101,  ..., -0.0078, -0.0151, -0.0014],
        [-0.0050,  0.0140,  0.0083,  ..., -0.0110,  0.0009, -0.0041]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2144,  1.0547,  3.0020,  ..., -2.0039,  2.3477,  3.4668]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 02:33:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of financial is financially
The adjective form of strong is strongly
The adjective form of political is politically
The adjective form of physical is physically
The adjective form of creative is creatively
The adjective form of according is accordingly
The adjective form of internal is internally
The adjective form of additional is
2024-07-17 02:33:03 root INFO     [order_1_approx] starting weight calculation for The adjective form of additional is additionally
The adjective form of political is politically
The adjective form of internal is internally
The adjective form of strong is strongly
The adjective form of creative is creatively
The adjective form of physical is physically
The adjective form of according is accordingly
The adjective form of financial is
2024-07-17 02:33:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 02:36:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1201,  0.3689,  0.5493,  ..., -0.1924,  0.8799,  0.6338],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3318,  2.1328, -1.6836,  ..., -1.9434,  2.4375,  2.1641],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0129, -0.0134,  0.0077,  ..., -0.0167,  0.0016, -0.0200],
        [ 0.0055, -0.0037,  0.0103,  ...,  0.0213, -0.0154,  0.0055],
        [-0.0059, -0.0063, -0.0049,  ..., -0.0142,  0.0098,  0.0125],
        ...,
        [ 0.0053,  0.0005,  0.0023,  ...,  0.0029,  0.0015, -0.0093],
        [ 0.0081, -0.0107, -0.0079,  ..., -0.0071, -0.0097, -0.0112],
        [-0.0180,  0.0082,  0.0056,  ..., -0.0044,  0.0004, -0.0059]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0972,  2.1738, -1.4219,  ..., -2.6367,  1.9414,  2.3418]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 02:36:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of additional is additionally
The adjective form of political is politically
The adjective form of internal is internally
The adjective form of strong is strongly
The adjective form of creative is creatively
The adjective form of physical is physically
The adjective form of according is accordingly
The adjective form of financial is
2024-07-17 02:36:30 root INFO     [order_1_approx] starting weight calculation for The adjective form of strong is strongly
The adjective form of internal is internally
The adjective form of creative is creatively
The adjective form of physical is physically
The adjective form of financial is financially
The adjective form of political is politically
The adjective form of additional is additionally
The adjective form of according is
2024-07-17 02:36:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 02:39:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.2637,  0.3892,  0.6514,  ..., -0.5371,  1.7695,  1.3359],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.7109,  1.0127,  1.6602,  ..., -2.5898, -0.0439, -2.2812],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.5930e-02, -1.9989e-02,  1.0437e-02,  ..., -1.9562e-02,
         -2.1545e-02, -2.2827e-02],
        [-2.6226e-05, -1.0742e-02,  1.3077e-02,  ...,  2.1774e-02,
         -1.5869e-03, -1.7532e-02],
        [-1.1902e-02,  6.1378e-03, -9.4528e-03,  ..., -5.5122e-03,
         -2.0233e-02,  8.2092e-03],
        ...,
        [-1.1459e-02, -1.2428e-02, -3.0365e-03,  ..., -2.8229e-03,
         -8.0185e-03, -2.8748e-02],
        [ 2.0996e-02,  2.1515e-03,  5.8212e-03,  ..., -1.5259e-04,
         -1.8524e-02,  1.7586e-03],
        [-1.4145e-02,  2.1179e-02,  1.8692e-03,  ..., -5.3482e-03,
         -5.0430e-03, -1.0933e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.3281,  0.9995,  1.7568,  ..., -3.6582,  1.3203, -2.0371]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 02:39:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of strong is strongly
The adjective form of internal is internally
The adjective form of creative is creatively
The adjective form of physical is physically
The adjective form of financial is financially
The adjective form of political is politically
The adjective form of additional is additionally
The adjective form of according is
2024-07-17 02:39:57 root INFO     [order_1_approx] starting weight calculation for The adjective form of physical is physically
The adjective form of financial is financially
The adjective form of additional is additionally
The adjective form of according is accordingly
The adjective form of creative is creatively
The adjective form of internal is internally
The adjective form of political is politically
The adjective form of strong is
2024-07-17 02:39:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 02:43:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0977,  0.4204,  0.8501,  ..., -0.2954,  0.6416, -0.8652],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.2988, -0.5654,  0.5684,  ...,  1.2480, -1.4805, -0.1152],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0064, -0.0026,  0.0146,  ..., -0.0123, -0.0024, -0.0203],
        [ 0.0072,  0.0067, -0.0018,  ...,  0.0100, -0.0042,  0.0034],
        [-0.0023, -0.0018,  0.0016,  ..., -0.0054,  0.0064,  0.0053],
        ...,
        [ 0.0023,  0.0009,  0.0092,  ...,  0.0186, -0.0050, -0.0063],
        [-0.0154, -0.0196, -0.0039,  ..., -0.0046,  0.0083,  0.0035],
        [-0.0053,  0.0068,  0.0037,  ..., -0.0024,  0.0044, -0.0186]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1230, -0.2373,  1.2988,  ...,  1.0215, -1.4854, -0.3306]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 02:43:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of physical is physically
The adjective form of financial is financially
The adjective form of additional is additionally
The adjective form of according is accordingly
The adjective form of creative is creatively
The adjective form of internal is internally
The adjective form of political is politically
The adjective form of strong is
2024-07-17 02:43:24 root INFO     [order_1_approx] starting weight calculation for The adjective form of internal is internally
The adjective form of political is politically
The adjective form of creative is creatively
The adjective form of according is accordingly
The adjective form of additional is additionally
The adjective form of strong is strongly
The adjective form of financial is financially
The adjective form of physical is
2024-07-17 02:43:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 02:46:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8774,  1.3496,  0.9565,  ...,  0.4578,  0.4910, -1.6230],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6914, -0.5654,  1.7900,  ..., -1.5459, -2.1094, -0.4551],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0204, -0.0125,  0.0175,  ..., -0.0158, -0.0042, -0.0164],
        [-0.0026,  0.0011,  0.0023,  ...,  0.0090, -0.0113,  0.0001],
        [-0.0148, -0.0088, -0.0023,  ..., -0.0068, -0.0003, -0.0009],
        ...,
        [-0.0040, -0.0107, -0.0009,  ..., -0.0003,  0.0043, -0.0122],
        [-0.0034, -0.0089, -0.0024,  ..., -0.0138, -0.0119, -0.0023],
        [-0.0116,  0.0059, -0.0066,  ..., -0.0107, -0.0068, -0.0261]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9243, -0.6348,  2.1367,  ..., -1.3369, -1.4482, -0.1213]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 02:46:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of internal is internally
The adjective form of political is politically
The adjective form of creative is creatively
The adjective form of according is accordingly
The adjective form of additional is additionally
The adjective form of strong is strongly
The adjective form of financial is financially
The adjective form of physical is
2024-07-17 02:46:52 root INFO     total operator prediction time: 1659.263752937317 seconds
2024-07-17 02:46:52 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+tion_irreg
2024-07-17 02:46:52 root INFO     building operator verb+tion_irreg
2024-07-17 02:46:52 root INFO     [order_1_approx] starting weight calculation for To occupy results in occupation
To expire results in expiration
To randomize results in randomization
To oblige results in obligation
To customize results in customization
To examine results in examination
To admire results in admiration
To organize results in
2024-07-17 02:46:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 02:50:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9253,  0.5918,  0.0422,  ..., -0.4573,  0.6934,  0.7466],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0303, -0.5122, -1.1465,  ...,  1.0225, -1.4678,  5.3711],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0084, -0.0169, -0.0006,  ..., -0.0140, -0.0048, -0.0216],
        [-0.0043, -0.0090, -0.0034,  ...,  0.0002,  0.0061,  0.0002],
        [-0.0034, -0.0045,  0.0057,  ..., -0.0051,  0.0059, -0.0090],
        ...,
        [-0.0054, -0.0165,  0.0060,  ...,  0.0079, -0.0148, -0.0104],
        [-0.0001,  0.0060, -0.0076,  ..., -0.0008,  0.0006,  0.0019],
        [-0.0073,  0.0197,  0.0094,  ..., -0.0041,  0.0047,  0.0005]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1149, -0.4744, -1.0508,  ...,  1.4971, -1.8555,  5.8125]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 02:50:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To occupy results in occupation
To expire results in expiration
To randomize results in randomization
To oblige results in obligation
To customize results in customization
To examine results in examination
To admire results in admiration
To organize results in
2024-07-17 02:50:20 root INFO     [order_1_approx] starting weight calculation for To occupy results in occupation
To organize results in organization
To expire results in expiration
To randomize results in randomization
To customize results in customization
To admire results in admiration
To oblige results in obligation
To examine results in
2024-07-17 02:50:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 02:53:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3848,  0.4854,  0.9150,  ..., -1.1768, -0.1414,  0.5322],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.0078,  1.8184, -4.7812,  ...,  2.9219, -3.4336,  2.6875],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0039, -0.0088, -0.0031,  ..., -0.0164, -0.0150, -0.0099],
        [-0.0085, -0.0107, -0.0056,  ...,  0.0013, -0.0033, -0.0069],
        [ 0.0211,  0.0075,  0.0167,  ..., -0.0083, -0.0060,  0.0014],
        ...,
        [-0.0177, -0.0066, -0.0006,  ...,  0.0067, -0.0088, -0.0145],
        [ 0.0092,  0.0059,  0.0024,  ...,  0.0046, -0.0042,  0.0056],
        [ 0.0095,  0.0175,  0.0140,  ..., -0.0035, -0.0064,  0.0165]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.5234,  1.3086, -4.8047,  ...,  2.8984, -3.5801,  3.0273]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 02:53:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To occupy results in occupation
To organize results in organization
To expire results in expiration
To randomize results in randomization
To customize results in customization
To admire results in admiration
To oblige results in obligation
To examine results in
2024-07-17 02:53:48 root INFO     [order_1_approx] starting weight calculation for To customize results in customization
To occupy results in occupation
To organize results in organization
To randomize results in randomization
To expire results in expiration
To examine results in examination
To admire results in admiration
To oblige results in
2024-07-17 02:53:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 02:57:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6826, -0.0465, -0.4753,  ..., -0.2009,  0.3997,  1.1123],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.9922, -1.4707, -4.9922,  ...,  1.3916, -2.5898,  6.9609],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0108,  0.0056,  0.0094,  ..., -0.0062, -0.0079, -0.0137],
        [ 0.0005, -0.0057, -0.0015,  ...,  0.0067,  0.0005, -0.0059],
        [ 0.0047,  0.0035, -0.0046,  ..., -0.0085, -0.0025,  0.0040],
        ...,
        [-0.0121, -0.0157, -0.0042,  ...,  0.0014,  0.0022, -0.0090],
        [-0.0050,  0.0158,  0.0012,  ..., -0.0125, -0.0144,  0.0070],
        [-0.0105,  0.0049,  0.0114,  ...,  0.0025,  0.0031,  0.0011]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.3984, -1.6045, -4.9141,  ...,  1.2021, -2.8750,  7.2539]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 02:57:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To customize results in customization
To occupy results in occupation
To organize results in organization
To randomize results in randomization
To expire results in expiration
To examine results in examination
To admire results in admiration
To oblige results in
2024-07-17 02:57:16 root INFO     [order_1_approx] starting weight calculation for To admire results in admiration
To expire results in expiration
To oblige results in obligation
To examine results in examination
To customize results in customization
To organize results in organization
To occupy results in occupation
To randomize results in
2024-07-17 02:57:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 03:00:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6553, -0.4395, -0.0969,  ..., -0.5918, -0.4316,  0.4211],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5732,  2.0723, -2.4336,  ...,  2.0352,  4.1406,  6.9414],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.2772e-02, -7.5340e-03,  1.6165e-03,  ..., -3.4294e-03,
         -3.1490e-03, -1.2146e-02],
        [ 3.4714e-04,  7.5226e-03, -7.9422e-03,  ...,  1.1505e-02,
          8.0109e-03, -2.0866e-03],
        [-8.5831e-06,  2.0676e-03,  7.1297e-03,  ..., -6.7215e-03,
          1.3676e-03, -6.2065e-03],
        ...,
        [-2.2751e-02, -1.1139e-02, -1.0864e-02,  ...,  1.9974e-02,
         -1.3790e-03,  4.3945e-03],
        [ 6.9656e-03, -4.2267e-03,  9.4461e-04,  ...,  3.7441e-03,
          5.9929e-03,  1.1971e-02],
        [-1.1902e-02, -2.4643e-03, -3.6144e-03,  ..., -4.5700e-03,
          8.7204e-03,  1.5839e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2969,  2.1699, -2.4980,  ...,  1.8613,  4.1953,  6.7969]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 03:00:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To admire results in admiration
To expire results in expiration
To oblige results in obligation
To examine results in examination
To customize results in customization
To organize results in organization
To occupy results in occupation
To randomize results in
2024-07-17 03:00:43 root INFO     [order_1_approx] starting weight calculation for To oblige results in obligation
To expire results in expiration
To organize results in organization
To occupy results in occupation
To randomize results in randomization
To customize results in customization
To examine results in examination
To admire results in
2024-07-17 03:00:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 03:04:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3481,  0.2915,  1.0078,  ..., -0.9805,  0.2478, -0.9453],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.1484,  3.2285, -2.4199,  ..., -0.7578, -3.4375,  4.5469],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 5.3482e-03, -1.2894e-02,  2.4700e-03,  ..., -1.9073e-06,
          9.8038e-04, -1.7868e-02],
        [ 3.2482e-03,  8.3923e-03, -8.5068e-03,  ...,  5.3482e-03,
         -1.3893e-02,  1.4763e-03],
        [-1.1505e-02,  1.6031e-03,  4.6997e-03,  ..., -4.1122e-03,
          1.5167e-02,  9.5825e-03],
        ...,
        [-2.3956e-02,  5.1117e-04,  4.8447e-03,  ..., -5.8174e-05,
         -1.9398e-03, -1.0292e-02],
        [ 7.8888e-03,  8.6212e-03, -1.0300e-02,  ...,  1.2589e-04,
         -1.6876e-02,  1.3229e-02],
        [-4.5776e-03,  1.5839e-02,  1.7410e-02,  ..., -1.3504e-02,
          6.2790e-03,  1.2558e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.6152,  3.2871, -1.9795,  ..., -0.3813, -3.6074,  5.0625]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 03:04:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To oblige results in obligation
To expire results in expiration
To organize results in organization
To occupy results in occupation
To randomize results in randomization
To customize results in customization
To examine results in examination
To admire results in
2024-07-17 03:04:12 root INFO     [order_1_approx] starting weight calculation for To examine results in examination
To occupy results in occupation
To organize results in organization
To randomize results in randomization
To admire results in admiration
To oblige results in obligation
To customize results in customization
To expire results in
2024-07-17 03:04:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 03:07:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.8145, -0.4849,  0.5303,  ...,  0.0391,  1.3506,  1.1025],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.8965,  1.5996, -2.3750,  ..., -0.0560, -1.8496,  1.3398],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0015, -0.0031,  0.0060,  ..., -0.0096, -0.0166, -0.0213],
        [-0.0064,  0.0055,  0.0030,  ...,  0.0124, -0.0016,  0.0031],
        [-0.0011, -0.0088,  0.0038,  ..., -0.0011, -0.0022,  0.0078],
        ...,
        [-0.0020, -0.0061, -0.0010,  ..., -0.0027, -0.0150, -0.0018],
        [-0.0046,  0.0151,  0.0087,  ...,  0.0127, -0.0119,  0.0062],
        [ 0.0005,  0.0205,  0.0125,  ..., -0.0126, -0.0069, -0.0018]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5449,  1.6211, -1.9150,  ..., -0.7349, -1.7500,  1.4727]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 03:07:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To examine results in examination
To occupy results in occupation
To organize results in organization
To randomize results in randomization
To admire results in admiration
To oblige results in obligation
To customize results in customization
To expire results in
2024-07-17 03:07:39 root INFO     [order_1_approx] starting weight calculation for To admire results in admiration
To oblige results in obligation
To randomize results in randomization
To organize results in organization
To customize results in customization
To examine results in examination
To expire results in expiration
To occupy results in
2024-07-17 03:07:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 03:11:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1031, -0.3682,  0.5337,  ...,  0.3232, -1.0117,  0.2803],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.9453,  2.6172, -2.0059,  ..., -0.8867,  1.2793,  5.6523],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.0544e-02, -1.6575e-03, -4.5013e-03,  ..., -4.3755e-03,
         -1.4786e-02, -2.1866e-02],
        [ 3.4523e-03, -1.5839e-02,  3.9406e-03,  ...,  3.1166e-03,
         -2.7828e-03,  1.4105e-03],
        [-8.0719e-03, -1.2428e-02, -1.7288e-02,  ..., -5.1003e-03,
         -9.7275e-04, -3.5515e-03],
        ...,
        [-7.4425e-03, -1.2360e-02, -5.2147e-03,  ...,  8.5068e-04,
         -1.0368e-02, -1.0017e-02],
        [-2.4643e-03,  1.7731e-02, -6.0158e-03,  ...,  5.0697e-03,
         -2.6581e-02,  3.8490e-03],
        [-8.7585e-03,  8.6670e-03,  5.8784e-03,  ..., -5.3024e-03,
         -5.8441e-03, -6.4850e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5879,  2.6348, -2.2363,  ..., -1.6689,  1.1484,  5.5781]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 03:11:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To admire results in admiration
To oblige results in obligation
To randomize results in randomization
To organize results in organization
To customize results in customization
To examine results in examination
To expire results in expiration
To occupy results in
2024-07-17 03:11:06 root INFO     [order_1_approx] starting weight calculation for To examine results in examination
To oblige results in obligation
To randomize results in randomization
To occupy results in occupation
To admire results in admiration
To organize results in organization
To expire results in expiration
To customize results in
2024-07-17 03:11:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 03:14:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.5000, -0.4580, -0.4089,  ...,  0.6177, -0.9082,  0.3462],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2524,  0.4446, -2.8164,  ..., -3.0469, -1.3574,  3.3926],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0054, -0.0120,  0.0024,  ..., -0.0150,  0.0030, -0.0119],
        [-0.0037, -0.0028,  0.0020,  ...,  0.0084, -0.0071,  0.0077],
        [-0.0010, -0.0054, -0.0066,  ..., -0.0017,  0.0153, -0.0050],
        ...,
        [-0.0138, -0.0064, -0.0061,  ...,  0.0123, -0.0074,  0.0003],
        [ 0.0007,  0.0087, -0.0015,  ..., -0.0003, -0.0063, -0.0046],
        [-0.0027,  0.0090,  0.0049,  ..., -0.0117,  0.0091, -0.0008]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0214,  1.5293, -2.9160,  ..., -2.9512, -2.4102,  1.9209]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 03:14:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To examine results in examination
To oblige results in obligation
To randomize results in randomization
To occupy results in occupation
To admire results in admiration
To organize results in organization
To expire results in expiration
To customize results in
2024-07-17 03:14:34 root INFO     total operator prediction time: 1661.8797371387482 seconds
2024-07-17 03:14:34 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+able_reg
2024-07-17 03:14:34 root INFO     building operator verb+able_reg
2024-07-17 03:14:34 root INFO     [order_1_approx] starting weight calculation for If you can recognize something, that thing is recognizable
If you can perform something, that thing is performable
If you can learn something, that thing is learnable
If you can afford something, that thing is affordable
If you can avoid something, that thing is avoidable
If you can observe something, that thing is observable
If you can prevent something, that thing is preventable
If you can protect something, that thing is
2024-07-17 03:14:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 03:18:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.4414, 1.0479, 0.7480,  ..., 1.0107, 0.6592, 1.0215], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.6719,  4.5586, -2.2031,  ..., -2.7344,  1.3652,  2.2109],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-5.6877e-03, -6.9122e-03,  1.0841e-02,  ..., -4.7398e-04,
          6.0844e-03, -1.4343e-03],
        [-5.1956e-03,  1.0124e-02, -3.8147e-05,  ...,  3.2082e-03,
         -8.6060e-03, -3.7632e-03],
        [ 6.5918e-03, -8.8882e-04, -1.3611e-02,  ..., -1.3870e-02,
          1.1238e-02, -8.6060e-03],
        ...,
        [-1.0292e-02, -4.0436e-03, -1.5854e-02,  ..., -9.0027e-04,
          6.4926e-03, -1.4908e-02],
        [ 8.5831e-05, -6.3324e-03, -7.1716e-04,  ..., -6.9523e-04,
         -1.1322e-02,  2.2568e-02],
        [-6.1035e-05,  2.0584e-02, -5.8746e-03,  ..., -6.8245e-03,
          6.6376e-03, -1.6037e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.2500,  4.7852, -2.4395,  ..., -3.1855,  1.3799,  2.0254]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 03:18:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can recognize something, that thing is recognizable
If you can perform something, that thing is performable
If you can learn something, that thing is learnable
If you can afford something, that thing is affordable
If you can avoid something, that thing is avoidable
If you can observe something, that thing is observable
If you can prevent something, that thing is preventable
If you can protect something, that thing is
2024-07-17 03:18:02 root INFO     [order_1_approx] starting weight calculation for If you can recognize something, that thing is recognizable
If you can perform something, that thing is performable
If you can afford something, that thing is affordable
If you can learn something, that thing is learnable
If you can protect something, that thing is protectable
If you can prevent something, that thing is preventable
If you can avoid something, that thing is avoidable
If you can observe something, that thing is
2024-07-17 03:18:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 03:21:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0869,  0.3655,  0.7183,  ..., -1.2490, -0.0529,  2.1738],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.5234,  5.7578, -3.7305,  ..., -0.9277, -0.0194,  0.4155],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0022, -0.0070,  0.0042,  ..., -0.0034,  0.0061, -0.0104],
        [-0.0036, -0.0012,  0.0059,  ...,  0.0148, -0.0105,  0.0044],
        [-0.0029,  0.0078, -0.0103,  ...,  0.0001,  0.0069, -0.0021],
        ...,
        [-0.0012, -0.0039, -0.0008,  ...,  0.0037,  0.0057, -0.0079],
        [-0.0055, -0.0090,  0.0058,  ..., -0.0143, -0.0126,  0.0083],
        [-0.0142,  0.0177,  0.0047,  ..., -0.0007, -0.0021, -0.0030]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.3145,  6.2109, -3.8750,  ..., -0.7891,  0.2367,  0.0359]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 03:21:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can recognize something, that thing is recognizable
If you can perform something, that thing is performable
If you can afford something, that thing is affordable
If you can learn something, that thing is learnable
If you can protect something, that thing is protectable
If you can prevent something, that thing is preventable
If you can avoid something, that thing is avoidable
If you can observe something, that thing is
2024-07-17 03:21:28 root INFO     [order_1_approx] starting weight calculation for If you can avoid something, that thing is avoidable
If you can afford something, that thing is affordable
If you can prevent something, that thing is preventable
If you can recognize something, that thing is recognizable
If you can learn something, that thing is learnable
If you can observe something, that thing is observable
If you can protect something, that thing is protectable
If you can perform something, that thing is
2024-07-17 03:21:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 03:24:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2256,  0.6157,  0.5742,  ..., -1.5596,  1.1045, -0.5063],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.9297e-03,  2.8926e+00, -3.5332e+00,  ..., -3.2227e+00,
         3.6367e+00,  1.0127e+00], device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-6.0158e-03, -1.3031e-02,  1.5011e-03,  ..., -7.8735e-03,
          3.4866e-03, -4.7607e-03],
        [ 1.5831e-04,  4.1122e-03,  7.4959e-04,  ...,  2.4071e-03,
         -3.5553e-03, -1.8196e-03],
        [ 6.9160e-03, -1.1826e-04, -1.1330e-02,  ..., -8.5678e-03,
          1.2672e-02, -7.1411e-03],
        ...,
        [-2.3251e-03, -5.3024e-03, -7.9803e-03,  ...,  1.5678e-03,
          1.1024e-02, -1.1208e-02],
        [-8.3466e-03, -7.7858e-03, -8.5754e-03,  ..., -1.4549e-02,
         -4.3144e-03,  1.1017e-02],
        [-1.9321e-03,  1.8372e-02, -3.7460e-03,  ..., -7.2365e-03,
          3.2959e-03,  1.9073e-06]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1088,  2.9961, -3.6719,  ..., -3.3809,  3.9980,  1.5273]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 03:24:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can avoid something, that thing is avoidable
If you can afford something, that thing is affordable
If you can prevent something, that thing is preventable
If you can recognize something, that thing is recognizable
If you can learn something, that thing is learnable
If you can observe something, that thing is observable
If you can protect something, that thing is protectable
If you can perform something, that thing is
2024-07-17 03:24:54 root INFO     [order_1_approx] starting weight calculation for If you can avoid something, that thing is avoidable
If you can learn something, that thing is learnable
If you can protect something, that thing is protectable
If you can recognize something, that thing is recognizable
If you can observe something, that thing is observable
If you can afford something, that thing is affordable
If you can perform something, that thing is performable
If you can prevent something, that thing is
2024-07-17 03:24:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 03:28:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3975,  2.3867,  0.7109,  ..., -0.2749,  1.3936, -0.2693],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7715,  1.5723, -4.4102,  ...,  0.3926,  5.5391,  4.5000],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0057, -0.0093,  0.0166,  ..., -0.0053,  0.0023, -0.0078],
        [-0.0087,  0.0035,  0.0044,  ..., -0.0006, -0.0137, -0.0010],
        [-0.0012, -0.0097, -0.0115,  ..., -0.0156,  0.0046, -0.0023],
        ...,
        [-0.0109, -0.0119, -0.0077,  ...,  0.0032,  0.0100, -0.0135],
        [-0.0014, -0.0020, -0.0032,  ..., -0.0190, -0.0142, -0.0024],
        [-0.0077,  0.0261,  0.0011,  ..., -0.0166,  0.0001, -0.0302]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1104,  1.6553, -4.5391,  ...,  0.3481,  5.5703,  4.5547]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 03:28:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can avoid something, that thing is avoidable
If you can learn something, that thing is learnable
If you can protect something, that thing is protectable
If you can recognize something, that thing is recognizable
If you can observe something, that thing is observable
If you can afford something, that thing is affordable
If you can perform something, that thing is performable
If you can prevent something, that thing is
2024-07-17 03:28:20 root INFO     [order_1_approx] starting weight calculation for If you can observe something, that thing is observable
If you can afford something, that thing is affordable
If you can perform something, that thing is performable
If you can recognize something, that thing is recognizable
If you can prevent something, that thing is preventable
If you can learn something, that thing is learnable
If you can protect something, that thing is protectable
If you can avoid something, that thing is
2024-07-17 03:28:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 03:31:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5654,  0.9399, -0.9785,  ..., -0.1421,  0.1536,  0.1666],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.9121,  4.3242, -2.2969,  ...,  0.6172,  6.8359,  1.3037],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0079, -0.0076,  0.0164,  ...,  0.0099,  0.0043, -0.0121],
        [-0.0101,  0.0055,  0.0038,  ...,  0.0081,  0.0026, -0.0108],
        [ 0.0053, -0.0057, -0.0206,  ..., -0.0094,  0.0017, -0.0073],
        ...,
        [-0.0028,  0.0027, -0.0175,  ..., -0.0098,  0.0021, -0.0151],
        [-0.0090,  0.0015, -0.0061,  ..., -0.0122, -0.0120,  0.0091],
        [-0.0036,  0.0186,  0.0020,  ..., -0.0101,  0.0066, -0.0220]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5234,  5.0078, -2.3047,  ...,  0.2917,  7.3477,  1.2012]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 03:31:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can observe something, that thing is observable
If you can afford something, that thing is affordable
If you can perform something, that thing is performable
If you can recognize something, that thing is recognizable
If you can prevent something, that thing is preventable
If you can learn something, that thing is learnable
If you can protect something, that thing is protectable
If you can avoid something, that thing is
2024-07-17 03:31:46 root INFO     [order_1_approx] starting weight calculation for If you can observe something, that thing is observable
If you can afford something, that thing is affordable
If you can perform something, that thing is performable
If you can prevent something, that thing is preventable
If you can protect something, that thing is protectable
If you can recognize something, that thing is recognizable
If you can avoid something, that thing is avoidable
If you can learn something, that thing is
2024-07-17 03:31:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 03:35:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1152,  0.7798,  1.2598,  ..., -0.7070,  1.9297,  0.6958],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.1836,  5.8125, -3.9531,  ..., -0.7217,  0.5146,  0.3965],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-9.0942e-03, -2.1820e-03,  9.0561e-03,  ...,  6.0043e-03,
          6.4468e-04,  4.3297e-04],
        [ 9.2697e-04, -1.0300e-02,  2.2125e-03,  ...,  6.4812e-03,
          1.6756e-03,  9.9792e-03],
        [ 3.4084e-03,  1.0208e-02, -7.2937e-03,  ..., -6.1607e-03,
          9.4604e-03, -7.2403e-03],
        ...,
        [ 1.2054e-03,  3.3200e-05, -3.9673e-03,  ...,  1.1520e-03,
         -5.7983e-04, -1.1955e-02],
        [-3.9978e-03, -1.0939e-03, -4.5624e-03,  ..., -9.4147e-03,
         -1.1917e-02,  9.5215e-03],
        [-4.8208e-04,  1.3275e-02,  1.1845e-03,  ..., -4.9515e-03,
          4.0627e-03, -2.0187e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.9180,  5.8438, -4.0273,  ..., -0.4871,  0.3994,  0.4988]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 03:35:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can observe something, that thing is observable
If you can afford something, that thing is affordable
If you can perform something, that thing is performable
If you can prevent something, that thing is preventable
If you can protect something, that thing is protectable
If you can recognize something, that thing is recognizable
If you can avoid something, that thing is avoidable
If you can learn something, that thing is
2024-07-17 03:35:12 root INFO     [order_1_approx] starting weight calculation for If you can prevent something, that thing is preventable
If you can protect something, that thing is protectable
If you can recognize something, that thing is recognizable
If you can observe something, that thing is observable
If you can learn something, that thing is learnable
If you can perform something, that thing is performable
If you can avoid something, that thing is avoidable
If you can afford something, that thing is
2024-07-17 03:35:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 03:38:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4377, -0.0928,  0.0593,  ...,  0.5322,  2.0840, -0.4258],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.1641,  4.3281, -2.7695,  ..., -4.0742,  1.9482,  2.9160],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0114, -0.0016,  0.0143,  ..., -0.0030, -0.0031, -0.0080],
        [ 0.0072, -0.0100, -0.0004,  ...,  0.0046, -0.0045, -0.0013],
        [-0.0045, -0.0010, -0.0091,  ...,  0.0025,  0.0038, -0.0080],
        ...,
        [ 0.0015, -0.0069, -0.0067,  ..., -0.0076,  0.0095, -0.0099],
        [-0.0004, -0.0085,  0.0007,  ..., -0.0087, -0.0130,  0.0001],
        [ 0.0020,  0.0180, -0.0051,  ..., -0.0071, -0.0037, -0.0134]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.7578,  4.2695, -2.4785,  ..., -4.3125,  1.5742,  3.0117]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 03:38:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can prevent something, that thing is preventable
If you can protect something, that thing is protectable
If you can recognize something, that thing is recognizable
If you can observe something, that thing is observable
If you can learn something, that thing is learnable
If you can perform something, that thing is performable
If you can avoid something, that thing is avoidable
If you can afford something, that thing is
2024-07-17 03:38:38 root INFO     [order_1_approx] starting weight calculation for If you can perform something, that thing is performable
If you can prevent something, that thing is preventable
If you can learn something, that thing is learnable
If you can observe something, that thing is observable
If you can afford something, that thing is affordable
If you can avoid something, that thing is avoidable
If you can protect something, that thing is protectable
If you can recognize something, that thing is
2024-07-17 03:38:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 03:42:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2034,  1.4004, -0.2202,  ..., -0.6362,  0.9141,  0.4658],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.6172,  2.4590, -2.5488,  ..., -1.4248,  0.8135,  0.9316],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0064, -0.0200,  0.0015,  ..., -0.0001, -0.0037, -0.0056],
        [-0.0076,  0.0036, -0.0018,  ...,  0.0059,  0.0103, -0.0005],
        [ 0.0059, -0.0020, -0.0033,  ..., -0.0009,  0.0022, -0.0007],
        ...,
        [ 0.0067, -0.0069,  0.0017,  ...,  0.0080, -0.0009, -0.0084],
        [-0.0061, -0.0025,  0.0016,  ..., -0.0070, -0.0132,  0.0117],
        [-0.0058,  0.0084,  0.0028,  ..., -0.0058,  0.0110, -0.0104]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.5371,  2.5723, -2.2676,  ..., -1.1631,  0.8188,  1.6621]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 03:42:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can perform something, that thing is performable
If you can prevent something, that thing is preventable
If you can learn something, that thing is learnable
If you can observe something, that thing is observable
If you can afford something, that thing is affordable
If you can avoid something, that thing is avoidable
If you can protect something, that thing is protectable
If you can recognize something, that thing is
2024-07-17 03:42:03 root INFO     total operator prediction time: 1649.5694034099579 seconds
2024-07-17 03:42:03 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on un+adj_reg
2024-07-17 03:42:03 root INFO     building operator un+adj_reg
2024-07-17 03:42:04 root INFO     [order_1_approx] starting weight calculation for The opposite of lucky is unlucky
The opposite of expected is unexpected
The opposite of suitable is unsuitable
The opposite of reasonable is unreasonable
The opposite of published is unpublished
The opposite of resolved is unresolved
The opposite of predictable is unpredictable
The opposite of pleasant is
2024-07-17 03:42:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 03:45:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8794,  0.0862, -2.7422,  ..., -1.5156,  1.9668,  0.6602],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.1875, -1.6162, -1.6992,  ...,  3.2773, -0.1289,  1.2109],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0002, -0.0082,  0.0050,  ..., -0.0043,  0.0026, -0.0120],
        [ 0.0067, -0.0079,  0.0112,  ..., -0.0109,  0.0026, -0.0074],
        [ 0.0018, -0.0069, -0.0237,  ..., -0.0042,  0.0055,  0.0122],
        ...,
        [ 0.0024, -0.0062,  0.0100,  ...,  0.0068, -0.0085, -0.0007],
        [-0.0133, -0.0024,  0.0010,  ..., -0.0054, -0.0230, -0.0096],
        [-0.0014,  0.0109,  0.0060,  ...,  0.0057,  0.0051, -0.0036]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.3438, -1.4375, -2.0449,  ...,  2.9512,  0.2993,  1.6328]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 03:45:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of lucky is unlucky
The opposite of expected is unexpected
The opposite of suitable is unsuitable
The opposite of reasonable is unreasonable
The opposite of published is unpublished
The opposite of resolved is unresolved
The opposite of predictable is unpredictable
The opposite of pleasant is
2024-07-17 03:45:32 root INFO     [order_1_approx] starting weight calculation for The opposite of lucky is unlucky
The opposite of expected is unexpected
The opposite of predictable is unpredictable
The opposite of published is unpublished
The opposite of pleasant is unpleasant
The opposite of resolved is unresolved
The opposite of reasonable is unreasonable
The opposite of suitable is
2024-07-17 03:45:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 03:49:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.6641,  0.6108,  0.2605,  ..., -0.1300,  2.4453,  0.9756],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1875,  0.0850, -0.4980,  ..., -0.2764,  2.0117,  2.7578],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0107, -0.0215,  0.0104,  ..., -0.0068, -0.0271, -0.0237],
        [ 0.0133, -0.0104, -0.0092,  ..., -0.0194, -0.0162, -0.0004],
        [ 0.0063, -0.0037, -0.0219,  ..., -0.0238, -0.0192,  0.0026],
        ...,
        [ 0.0057, -0.0079, -0.0086,  ..., -0.0136,  0.0066, -0.0040],
        [-0.0104, -0.0205,  0.0113,  ..., -0.0042, -0.0062, -0.0086],
        [-0.0146,  0.0042, -0.0010,  ..., -0.0085,  0.0002,  0.0069]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5752,  0.5527, -0.2307,  ..., -0.2367,  2.7207,  2.7109]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 03:49:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of lucky is unlucky
The opposite of expected is unexpected
The opposite of predictable is unpredictable
The opposite of published is unpublished
The opposite of pleasant is unpleasant
The opposite of resolved is unresolved
The opposite of reasonable is unreasonable
The opposite of suitable is
2024-07-17 03:49:01 root INFO     [order_1_approx] starting weight calculation for The opposite of resolved is unresolved
The opposite of lucky is unlucky
The opposite of pleasant is unpleasant
The opposite of suitable is unsuitable
The opposite of reasonable is unreasonable
The opposite of published is unpublished
The opposite of predictable is unpredictable
The opposite of expected is
2024-07-17 03:49:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 03:52:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4102,  1.0156,  0.4934,  ..., -0.0625,  1.9707,  0.5024],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.5234, -1.2920, -0.3184,  ...,  1.9668,  3.2441,  3.4219],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0029, -0.0089,  0.0183,  ..., -0.0135, -0.0152, -0.0104],
        [ 0.0008, -0.0002, -0.0027,  ..., -0.0044, -0.0116,  0.0024],
        [-0.0033, -0.0093, -0.0028,  ..., -0.0178, -0.0028,  0.0103],
        ...,
        [-0.0062,  0.0010, -0.0032,  ...,  0.0164,  0.0122, -0.0117],
        [ 0.0024, -0.0173,  0.0112,  ..., -0.0132, -0.0024, -0.0035],
        [ 0.0015,  0.0034,  0.0148,  ..., -0.0005,  0.0046, -0.0041]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2207, -0.8643, -0.1550,  ...,  1.6240,  3.3262,  2.9727]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 03:52:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of resolved is unresolved
The opposite of lucky is unlucky
The opposite of pleasant is unpleasant
The opposite of suitable is unsuitable
The opposite of reasonable is unreasonable
The opposite of published is unpublished
The opposite of predictable is unpredictable
The opposite of expected is
2024-07-17 03:52:28 root INFO     [order_1_approx] starting weight calculation for The opposite of published is unpublished
The opposite of pleasant is unpleasant
The opposite of lucky is unlucky
The opposite of resolved is unresolved
The opposite of suitable is unsuitable
The opposite of reasonable is unreasonable
The opposite of expected is unexpected
The opposite of predictable is
2024-07-17 03:52:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 03:55:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0713, -0.4915,  1.2812,  ..., -0.7285,  3.1523,  0.4910],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.9248, -3.1289, -2.2520,  ...,  2.4609,  3.9746,  4.9102],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0023, -0.0172,  0.0036,  ..., -0.0061,  0.0039, -0.0068],
        [-0.0026,  0.0048, -0.0069,  ..., -0.0067, -0.0093,  0.0021],
        [-0.0046, -0.0110, -0.0080,  ..., -0.0094, -0.0043,  0.0057],
        ...,
        [-0.0117, -0.0088,  0.0042,  ...,  0.0076,  0.0100, -0.0073],
        [-0.0030, -0.0031, -0.0037,  ..., -0.0049, -0.0198,  0.0039],
        [ 0.0013,  0.0044,  0.0030,  ..., -0.0039, -0.0014, -0.0008]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.1094, -2.9512, -2.0938,  ...,  2.4297,  3.8418,  4.5898]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 03:55:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of published is unpublished
The opposite of pleasant is unpleasant
The opposite of lucky is unlucky
The opposite of resolved is unresolved
The opposite of suitable is unsuitable
The opposite of reasonable is unreasonable
The opposite of expected is unexpected
The opposite of predictable is
2024-07-17 03:55:56 root INFO     [order_1_approx] starting weight calculation for The opposite of expected is unexpected
The opposite of reasonable is unreasonable
The opposite of published is unpublished
The opposite of suitable is unsuitable
The opposite of pleasant is unpleasant
The opposite of resolved is unresolved
The opposite of predictable is unpredictable
The opposite of lucky is
2024-07-17 03:55:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 03:59:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.7891, 2.1250, 0.0280,  ..., 1.0527, 1.8389, 0.0333], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.8984,  2.9141, -1.8066,  ...,  3.0352, -1.9609,  3.4102],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0051, -0.0194, -0.0014,  ..., -0.0082, -0.0074, -0.0282],
        [ 0.0087, -0.0029,  0.0003,  ...,  0.0112, -0.0008,  0.0190],
        [-0.0065, -0.0093, -0.0029,  ..., -0.0061,  0.0026,  0.0123],
        ...,
        [ 0.0035, -0.0142,  0.0029,  ..., -0.0129,  0.0047, -0.0083],
        [-0.0160, -0.0088, -0.0104,  ..., -0.0151, -0.0173, -0.0370],
        [-0.0030,  0.0185,  0.0031,  ...,  0.0063, -0.0012, -0.0086]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.4297,  3.4238, -2.0371,  ...,  2.4414, -1.8652,  3.6094]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 03:59:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of expected is unexpected
The opposite of reasonable is unreasonable
The opposite of published is unpublished
The opposite of suitable is unsuitable
The opposite of pleasant is unpleasant
The opposite of resolved is unresolved
The opposite of predictable is unpredictable
The opposite of lucky is
2024-07-17 03:59:24 root INFO     [order_1_approx] starting weight calculation for The opposite of suitable is unsuitable
The opposite of predictable is unpredictable
The opposite of pleasant is unpleasant
The opposite of resolved is unresolved
The opposite of expected is unexpected
The opposite of published is unpublished
The opposite of lucky is unlucky
The opposite of reasonable is
2024-07-17 03:59:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 04:02:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7119, -0.2568,  0.7812,  ..., -1.1289,  2.1426,  0.0684],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9619, -4.2539, -0.9365,  ...,  1.1963,  4.9180,  1.2178],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0029, -0.0125,  0.0080,  ..., -0.0083, -0.0212, -0.0160],
        [-0.0033,  0.0080, -0.0063,  ...,  0.0110,  0.0027, -0.0026],
        [-0.0002, -0.0128, -0.0005,  ..., -0.0196, -0.0023,  0.0060],
        ...,
        [-0.0008, -0.0104, -0.0010,  ...,  0.0078,  0.0067, -0.0098],
        [-0.0138, -0.0080,  0.0083,  ...,  0.0046, -0.0173,  0.0068],
        [-0.0114,  0.0090,  0.0063,  ..., -0.0010, -0.0022, -0.0007]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1055, -3.7754, -1.0762,  ...,  1.1387,  5.3047,  1.7812]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 04:02:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of suitable is unsuitable
The opposite of predictable is unpredictable
The opposite of pleasant is unpleasant
The opposite of resolved is unresolved
The opposite of expected is unexpected
The opposite of published is unpublished
The opposite of lucky is unlucky
The opposite of reasonable is
2024-07-17 04:02:53 root INFO     [order_1_approx] starting weight calculation for The opposite of lucky is unlucky
The opposite of predictable is unpredictable
The opposite of reasonable is unreasonable
The opposite of expected is unexpected
The opposite of pleasant is unpleasant
The opposite of suitable is unsuitable
The opposite of resolved is unresolved
The opposite of published is
2024-07-17 04:02:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 04:06:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.6792, 0.7515, 1.6826,  ..., 1.0986, 0.7266, 1.6230], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9121, -4.0430,  1.7168,  ...,  2.9258,  2.7227, -2.9160],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0055, -0.0158,  0.0026,  ..., -0.0051, -0.0011, -0.0081],
        [-0.0100,  0.0059,  0.0031,  ...,  0.0101, -0.0041, -0.0023],
        [ 0.0032, -0.0067,  0.0012,  ..., -0.0042,  0.0078,  0.0111],
        ...,
        [-0.0097, -0.0003, -0.0001,  ...,  0.0065, -0.0058, -0.0072],
        [-0.0156,  0.0057, -0.0035,  ...,  0.0079, -0.0097, -0.0002],
        [ 0.0077,  0.0011, -0.0011,  ..., -0.0047,  0.0074, -0.0095]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1758, -3.3848,  0.8931,  ...,  3.2090,  2.9688, -3.3164]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 04:06:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of lucky is unlucky
The opposite of predictable is unpredictable
The opposite of reasonable is unreasonable
The opposite of expected is unexpected
The opposite of pleasant is unpleasant
The opposite of suitable is unsuitable
The opposite of resolved is unresolved
The opposite of published is
2024-07-17 04:06:20 root INFO     [order_1_approx] starting weight calculation for The opposite of predictable is unpredictable
The opposite of reasonable is unreasonable
The opposite of lucky is unlucky
The opposite of published is unpublished
The opposite of pleasant is unpleasant
The opposite of suitable is unsuitable
The opposite of expected is unexpected
The opposite of resolved is
2024-07-17 04:06:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 04:09:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3679,  1.0850,  0.3301,  ...,  0.8477,  1.4209, -0.1041],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0254, -3.8457, -3.9688,  ...,  5.0195,  4.4375, -1.2793],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0094, -0.0197,  0.0026,  ..., -0.0110, -0.0003, -0.0281],
        [-0.0083, -0.0082, -0.0022,  ..., -0.0130,  0.0003, -0.0011],
        [-0.0003, -0.0199, -0.0006,  ..., -0.0189, -0.0054, -0.0059],
        ...,
        [-0.0009, -0.0195,  0.0031,  ...,  0.0012, -0.0148,  0.0094],
        [-0.0121, -0.0046, -0.0038,  ...,  0.0192, -0.0147, -0.0016],
        [-0.0161,  0.0093,  0.0147,  ..., -0.0055, -0.0061, -0.0040]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.4766, -3.4473, -2.9922,  ...,  5.0039,  5.1758, -1.7793]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 04:09:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of predictable is unpredictable
The opposite of reasonable is unreasonable
The opposite of lucky is unlucky
The opposite of published is unpublished
The opposite of pleasant is unpleasant
The opposite of suitable is unsuitable
The opposite of expected is unexpected
The opposite of resolved is
2024-07-17 04:09:49 root INFO     total operator prediction time: 1665.343220949173 seconds
2024-07-17 04:09:49 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on re+verb_reg
2024-07-17 04:09:49 root INFO     building operator re+verb_reg
2024-07-17 04:09:49 root INFO     [order_1_approx] starting weight calculation for To consider again is to reconsider
To adjust again is to readjust
To grow again is to regrow
To write again is to rewrite
To establish again is to reestablish
To commend again is to recommend
To apply again is to reapply
To organize again is to
2024-07-17 04:09:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 04:13:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0527,  0.3953,  1.1523,  ..., -0.9419,  0.5156,  1.7227],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3271, -2.0156, -4.3984,  ...,  1.0889,  0.0496,  1.0820],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 8.5602e-03, -2.4605e-03,  1.1516e-04,  ...,  1.8778e-03,
         -5.7316e-04, -4.6349e-03],
        [ 1.0319e-03, -3.7813e-04,  3.6068e-03,  ...,  3.5191e-04,
          1.6298e-03,  5.6458e-04],
        [-3.3112e-03, -6.0730e-03, -9.0027e-03,  ..., -2.1954e-03,
          3.5629e-03, -7.0076e-03],
        ...,
        [-5.9700e-04,  8.5449e-04,  4.4289e-03,  ...,  1.1721e-03,
         -5.0278e-03,  7.9155e-05],
        [-2.4185e-03, -7.3099e-04,  2.3479e-03,  ...,  4.2419e-03,
          5.4245e-03, -9.9182e-05],
        [-3.0632e-03,  7.7019e-03,  2.6016e-03,  ...,  2.6550e-03,
          2.6283e-03,  1.5678e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2148, -1.6064, -4.7422,  ...,  1.7227, -0.3230,  1.5625]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 04:13:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To consider again is to reconsider
To adjust again is to readjust
To grow again is to regrow
To write again is to rewrite
To establish again is to reestablish
To commend again is to recommend
To apply again is to reapply
To organize again is to
2024-07-17 04:13:16 root INFO     [order_1_approx] starting weight calculation for To organize again is to reorganize
To establish again is to reestablish
To consider again is to reconsider
To apply again is to reapply
To write again is to rewrite
To grow again is to regrow
To adjust again is to readjust
To commend again is to
2024-07-17 04:13:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 04:16:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2061, -0.0249,  0.9028,  ..., -0.6177,  0.0176,  0.2656],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.2461, -0.0361, -2.0000,  ..., -1.0830, -3.2168,  0.7295],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0026, -0.0081, -0.0007,  ...,  0.0014, -0.0091, -0.0072],
        [ 0.0018, -0.0101,  0.0041,  ..., -0.0026,  0.0118, -0.0001],
        [-0.0087,  0.0020,  0.0002,  ..., -0.0091,  0.0007,  0.0020],
        ...,
        [-0.0115, -0.0090,  0.0035,  ..., -0.0009, -0.0073, -0.0003],
        [ 0.0068,  0.0096,  0.0074,  ..., -0.0045, -0.0113,  0.0027],
        [-0.0049,  0.0003, -0.0038,  ...,  0.0025, -0.0018, -0.0060]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1660,  0.2207, -1.5449,  ..., -1.2100, -3.1777,  1.2695]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 04:16:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To organize again is to reorganize
To establish again is to reestablish
To consider again is to reconsider
To apply again is to reapply
To write again is to rewrite
To grow again is to regrow
To adjust again is to readjust
To commend again is to
2024-07-17 04:16:44 root INFO     [order_1_approx] starting weight calculation for To adjust again is to readjust
To organize again is to reorganize
To consider again is to reconsider
To commend again is to recommend
To apply again is to reapply
To write again is to rewrite
To establish again is to reestablish
To grow again is to
2024-07-17 04:16:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 04:20:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([1.4492, 0.5459, 1.2666,  ..., 0.6450, 0.1304, 1.1895], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9277,  0.5557, -3.6250,  ..., -0.2573,  2.2695, -2.4883],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0020, -0.0042,  0.0001,  ...,  0.0005, -0.0070, -0.0049],
        [ 0.0022,  0.0003,  0.0057,  ...,  0.0064,  0.0079, -0.0044],
        [ 0.0005,  0.0058, -0.0035,  ..., -0.0064, -0.0024, -0.0036],
        ...,
        [-0.0014,  0.0018,  0.0009,  ...,  0.0048, -0.0053,  0.0004],
        [-0.0079, -0.0098, -0.0047,  ...,  0.0042, -0.0022, -0.0026],
        [ 0.0043, -0.0016,  0.0099,  ...,  0.0008, -0.0029,  0.0010]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3359,  1.3232, -4.2539,  ..., -0.3196,  2.1914, -1.7227]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 04:20:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To adjust again is to readjust
To organize again is to reorganize
To consider again is to reconsider
To commend again is to recommend
To apply again is to reapply
To write again is to rewrite
To establish again is to reestablish
To grow again is to
2024-07-17 04:20:10 root INFO     [order_1_approx] starting weight calculation for To write again is to rewrite
To apply again is to reapply
To grow again is to regrow
To adjust again is to readjust
To consider again is to reconsider
To organize again is to reorganize
To commend again is to recommend
To establish again is to
2024-07-17 04:20:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 04:23:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0325,  0.0079,  0.2766,  ...,  0.3757,  0.4858,  0.5591],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.8516, -1.5234, -2.7070,  ...,  0.2754, -0.0801,  4.4023],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0131, -0.0097,  0.0064,  ..., -0.0011, -0.0134, -0.0045],
        [ 0.0042, -0.0002,  0.0039,  ..., -0.0030,  0.0010,  0.0011],
        [ 0.0054, -0.0023, -0.0001,  ..., -0.0018, -0.0059, -0.0039],
        ...,
        [-0.0051, -0.0099,  0.0034,  ...,  0.0012, -0.0062,  0.0051],
        [-0.0070, -0.0046,  0.0037,  ...,  0.0058,  0.0051,  0.0027],
        [ 0.0019,  0.0021,  0.0062,  ...,  0.0001,  0.0050,  0.0075]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.3574, -1.5693, -3.3945,  ...,  0.3953,  0.7222,  4.5703]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 04:23:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To write again is to rewrite
To apply again is to reapply
To grow again is to regrow
To adjust again is to readjust
To consider again is to reconsider
To organize again is to reorganize
To commend again is to recommend
To establish again is to
2024-07-17 04:23:36 root INFO     [order_1_approx] starting weight calculation for To write again is to rewrite
To organize again is to reorganize
To consider again is to reconsider
To grow again is to regrow
To establish again is to reestablish
To adjust again is to readjust
To commend again is to recommend
To apply again is to
2024-07-17 04:23:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 04:27:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3818, -0.6689,  1.3574,  ..., -0.9512, -0.0635,  1.0498],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4072,  0.7295, -2.7539,  ..., -0.3403, -2.4668,  7.2734],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.1629e-03, -4.4670e-03,  4.2076e-03,  ...,  1.8148e-03,
         -1.2932e-02,  9.8133e-04],
        [ 6.6757e-03, -6.3324e-04,  2.4395e-03,  ..., -5.3835e-04,
          2.7657e-05, -1.7624e-03],
        [ 1.9350e-03, -7.2908e-04, -9.8648e-03,  ..., -1.2199e-02,
         -8.5907e-03, -6.9733e-03],
        ...,
        [-1.3542e-03,  1.1530e-03, -1.3628e-03,  ..., -5.3368e-03,
         -6.3934e-03, -2.4529e-03],
        [-5.8136e-03,  4.8256e-04,  1.2169e-03,  ..., -5.4207e-03,
         -7.8678e-04,  9.4757e-03],
        [ 7.0915e-03,  8.2855e-03,  1.6006e-02,  ...,  4.0436e-03,
          3.8452e-03, -6.1951e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1238,  0.6675, -3.0625,  ..., -0.1976, -2.2871,  7.5195]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 04:27:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To write again is to rewrite
To organize again is to reorganize
To consider again is to reconsider
To grow again is to regrow
To establish again is to reestablish
To adjust again is to readjust
To commend again is to recommend
To apply again is to
2024-07-17 04:27:04 root INFO     [order_1_approx] starting weight calculation for To organize again is to reorganize
To write again is to rewrite
To apply again is to reapply
To commend again is to recommend
To establish again is to reestablish
To grow again is to regrow
To consider again is to reconsider
To adjust again is to
2024-07-17 04:27:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 04:30:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4531,  0.0546,  0.5518,  ..., -0.4778,  0.6191,  0.3733],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3574,  1.1719, -7.2266,  ..., -3.5352,  0.2422,  4.1602],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0075, -0.0025,  0.0036,  ..., -0.0014, -0.0017, -0.0039],
        [ 0.0002,  0.0050, -0.0015,  ...,  0.0053, -0.0049,  0.0017],
        [-0.0012, -0.0069, -0.0009,  ..., -0.0070, -0.0063, -0.0130],
        ...,
        [-0.0043, -0.0051, -0.0006,  ..., -0.0049, -0.0039, -0.0025],
        [-0.0029, -0.0036, -0.0055,  ..., -0.0013, -0.0104,  0.0008],
        [ 0.0012,  0.0110,  0.0022,  ...,  0.0080, -0.0049, -0.0005]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1230,  1.4795, -7.9609,  ..., -3.5098,  0.1340,  4.5742]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 04:30:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To organize again is to reorganize
To write again is to rewrite
To apply again is to reapply
To commend again is to recommend
To establish again is to reestablish
To grow again is to regrow
To consider again is to reconsider
To adjust again is to
2024-07-17 04:30:33 root INFO     [order_1_approx] starting weight calculation for To adjust again is to readjust
To apply again is to reapply
To write again is to rewrite
To commend again is to recommend
To grow again is to regrow
To establish again is to reestablish
To organize again is to reorganize
To consider again is to
2024-07-17 04:30:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 04:33:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.8887,  0.6460,  0.0604,  ..., -1.2988, -0.0630,  1.6777],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.8359,  3.0957, -4.3516,  ..., -1.0254, -1.4346,  1.1416],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0030, -0.0017,  0.0069,  ..., -0.0045, -0.0094,  0.0003],
        [ 0.0034,  0.0008,  0.0195,  ...,  0.0033,  0.0032,  0.0002],
        [-0.0051,  0.0038, -0.0061,  ...,  0.0032, -0.0088,  0.0045],
        ...,
        [-0.0039, -0.0003,  0.0056,  ..., -0.0048, -0.0131,  0.0037],
        [-0.0020, -0.0062, -0.0053,  ..., -0.0109, -0.0124,  0.0080],
        [-0.0005,  0.0120,  0.0086,  ...,  0.0015, -0.0005, -0.0055]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.5508,  3.3105, -4.2344,  ..., -0.8145, -1.0889,  1.1055]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 04:33:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To adjust again is to readjust
To apply again is to reapply
To write again is to rewrite
To commend again is to recommend
To grow again is to regrow
To establish again is to reestablish
To organize again is to reorganize
To consider again is to
2024-07-17 04:34:00 root INFO     [order_1_approx] starting weight calculation for To commend again is to recommend
To organize again is to reorganize
To establish again is to reestablish
To consider again is to reconsider
To apply again is to reapply
To adjust again is to readjust
To grow again is to regrow
To write again is to
2024-07-17 04:34:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 04:37:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.6465,  0.8730,  0.7822,  ..., -0.4285, -0.3003,  1.2021],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2324,  0.0293, -4.7773,  ...,  2.6387,  1.2012, -0.5576],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.2114e-03, -7.1068e-03, -6.5994e-03,  ..., -5.7068e-03,
         -1.2283e-03, -3.6697e-03],
        [ 1.7910e-03,  2.8839e-03,  3.5667e-03,  ...,  5.9586e-03,
         -8.8215e-06,  2.3575e-03],
        [ 2.3003e-03,  1.5545e-03, -6.2485e-03,  ..., -7.7133e-03,
         -1.1055e-02, -6.3553e-03],
        ...,
        [-4.7302e-03,  2.9755e-03,  2.9678e-03,  ..., -4.6921e-03,
         -1.0010e-02,  2.7542e-03],
        [ 9.1553e-05, -2.2774e-03, -1.1835e-03,  ...,  1.3828e-03,
         -1.1765e-02,  8.0948e-03],
        [-1.9760e-03,  4.1084e-03,  4.6997e-03,  ..., -3.1471e-03,
         -6.4850e-03, -3.7117e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3984,  0.1539, -4.6797,  ...,  2.1543,  0.7964, -0.2644]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 04:37:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To commend again is to recommend
To organize again is to reorganize
To establish again is to reestablish
To consider again is to reconsider
To apply again is to reapply
To adjust again is to readjust
To grow again is to regrow
To write again is to
2024-07-17 04:37:28 root INFO     total operator prediction time: 1658.8794178962708 seconds
2024-07-17 04:37:28 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on adj+ness_reg
2024-07-17 04:37:28 root INFO     building operator adj+ness_reg
2024-07-17 04:37:28 root INFO     [order_1_approx] starting weight calculation for The state of being reasonable is reasonableness
The state of being impressive is impressiveness
The state of being hidden is hiddenness
The state of being same is sameness
The state of being connected is connectedness
The state of being serious is seriousness
The state of being strange is strangeness
The state of being related is
2024-07-17 04:37:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 04:40:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0546, -0.0833,  1.5977,  ..., -0.6641,  1.3066, -0.3303],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.0293, -0.6328,  1.3955,  ..., -2.8848,  0.0189,  4.9766],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0024, -0.0084,  0.0241,  ...,  0.0022,  0.0045, -0.0250],
        [-0.0016, -0.0141,  0.0007,  ...,  0.0116, -0.0006, -0.0020],
        [-0.0032, -0.0075, -0.0160,  ..., -0.0083, -0.0036,  0.0084],
        ...,
        [-0.0058, -0.0008, -0.0065,  ..., -0.0088, -0.0087, -0.0035],
        [-0.0032, -0.0057, -0.0014,  ..., -0.0096, -0.0073, -0.0015],
        [-0.0001,  0.0111,  0.0066,  ..., -0.0112,  0.0001, -0.0113]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.3926, -0.5933,  1.7324,  ..., -3.0312,  0.2390,  5.0312]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 04:40:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being reasonable is reasonableness
The state of being impressive is impressiveness
The state of being hidden is hiddenness
The state of being same is sameness
The state of being connected is connectedness
The state of being serious is seriousness
The state of being strange is strangeness
The state of being related is
2024-07-17 04:40:56 root INFO     [order_1_approx] starting weight calculation for The state of being impressive is impressiveness
The state of being strange is strangeness
The state of being reasonable is reasonableness
The state of being related is relatedness
The state of being serious is seriousness
The state of being same is sameness
The state of being hidden is hiddenness
The state of being connected is
2024-07-17 04:40:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 04:44:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([1.0029, 0.3237, 1.1885,  ..., 1.3955, 1.3535, 1.1719], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.4180, -0.1244, -0.3325,  ..., -3.8867, -2.2148,  2.7734],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0056, -0.0050,  0.0168,  ..., -0.0046,  0.0033, -0.0124],
        [-0.0040, -0.0109, -0.0028,  ...,  0.0091, -0.0012, -0.0107],
        [-0.0066, -0.0073, -0.0115,  ..., -0.0122, -0.0063,  0.0095],
        ...,
        [ 0.0029, -0.0031, -0.0032,  ..., -0.0044,  0.0037, -0.0044],
        [ 0.0019,  0.0010,  0.0071,  ..., -0.0079, -0.0153,  0.0019],
        [-0.0012,  0.0200,  0.0019,  ..., -0.0138,  0.0064, -0.0126]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.4473, -0.0395, -0.0525,  ..., -4.1680, -2.6543,  2.3008]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 04:44:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being impressive is impressiveness
The state of being strange is strangeness
The state of being reasonable is reasonableness
The state of being related is relatedness
The state of being serious is seriousness
The state of being same is sameness
The state of being hidden is hiddenness
The state of being connected is
2024-07-17 04:44:25 root INFO     [order_1_approx] starting weight calculation for The state of being reasonable is reasonableness
The state of being strange is strangeness
The state of being same is sameness
The state of being serious is seriousness
The state of being impressive is impressiveness
The state of being related is relatedness
The state of being connected is connectedness
The state of being hidden is
2024-07-17 04:44:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 04:47:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6465, -0.2612,  1.3115,  ...,  1.7695,  0.6650,  1.1719],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9473,  3.9473,  0.0869,  ..., -0.7607, -1.4570,  5.3945],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0019, -0.0162,  0.0153,  ..., -0.0021, -0.0007, -0.0173],
        [ 0.0047, -0.0035,  0.0025,  ...,  0.0056, -0.0040, -0.0085],
        [-0.0056, -0.0055, -0.0082,  ..., -0.0102, -0.0036,  0.0012],
        ...,
        [-0.0030, -0.0078, -0.0006,  ..., -0.0030, -0.0015, -0.0026],
        [-0.0004, -0.0022,  0.0042,  ..., -0.0104, -0.0214, -0.0031],
        [-0.0024,  0.0030, -0.0064,  ..., -0.0205, -0.0005, -0.0151]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8301,  4.2695,  0.0412,  ..., -0.8799, -1.4521,  6.1445]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 04:47:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being reasonable is reasonableness
The state of being strange is strangeness
The state of being same is sameness
The state of being serious is seriousness
The state of being impressive is impressiveness
The state of being related is relatedness
The state of being connected is connectedness
The state of being hidden is
2024-07-17 04:47:53 root INFO     [order_1_approx] starting weight calculation for The state of being serious is seriousness
The state of being connected is connectedness
The state of being hidden is hiddenness
The state of being strange is strangeness
The state of being impressive is impressiveness
The state of being related is relatedness
The state of being same is sameness
The state of being reasonable is
2024-07-17 04:47:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 04:51:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2250,  0.2717,  1.6094,  ..., -0.6768,  1.8652,  0.4639],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8701,  3.5391, -1.2939,  ..., -0.6743,  3.0469,  3.9746],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0048, -0.0120,  0.0167,  ..., -0.0046, -0.0054, -0.0146],
        [-0.0001,  0.0012, -0.0059,  ...,  0.0061, -0.0009,  0.0012],
        [-0.0048, -0.0119, -0.0052,  ..., -0.0172,  0.0069,  0.0145],
        ...,
        [-0.0023, -0.0054, -0.0019,  ...,  0.0059,  0.0063, -0.0029],
        [-0.0033, -0.0045,  0.0029,  ..., -0.0046, -0.0146, -0.0016],
        [-0.0065,  0.0122,  0.0130,  ..., -0.0082, -0.0076, -0.0072]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1914,  3.7500, -1.3809,  ..., -0.7183,  3.5293,  3.9375]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 04:51:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being serious is seriousness
The state of being connected is connectedness
The state of being hidden is hiddenness
The state of being strange is strangeness
The state of being impressive is impressiveness
The state of being related is relatedness
The state of being same is sameness
The state of being reasonable is
2024-07-17 04:51:22 root INFO     [order_1_approx] starting weight calculation for The state of being hidden is hiddenness
The state of being same is sameness
The state of being related is relatedness
The state of being serious is seriousness
The state of being connected is connectedness
The state of being strange is strangeness
The state of being reasonable is reasonableness
The state of being impressive is
2024-07-17 04:51:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 04:54:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3127,  1.3779,  1.0840,  ...,  0.9819,  1.9570, -0.0546],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 6.1523,  0.5703, -0.8804,  ...,  0.5361, -2.3496,  1.9805],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-4.3144e-03, -3.7861e-03,  1.4122e-02,  ..., -1.1612e-02,
         -4.7607e-03, -6.8245e-03],
        [ 5.4665e-03, -4.9591e-03,  4.8065e-03,  ...,  2.4204e-03,
         -5.2185e-03,  2.0771e-03],
        [-8.7891e-03, -1.4709e-02, -5.8594e-03,  ..., -1.5053e-02,
          1.9360e-03,  6.0501e-03],
        ...,
        [-7.6981e-03, -4.5547e-03,  4.7646e-03,  ..., -1.5450e-03,
         -6.2904e-03,  4.5433e-03],
        [ 7.4387e-05, -9.3231e-03,  9.3002e-03,  ..., -1.0300e-03,
         -1.1322e-02,  2.2101e-04],
        [-1.1322e-02,  9.5520e-03,  5.3978e-04,  ..., -9.9411e-03,
         -1.7563e-02,  1.1063e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.3906,  0.7666, -0.9829,  ...,  0.3022, -2.5957,  2.3984]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 04:54:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being hidden is hiddenness
The state of being same is sameness
The state of being related is relatedness
The state of being serious is seriousness
The state of being connected is connectedness
The state of being strange is strangeness
The state of being reasonable is reasonableness
The state of being impressive is
2024-07-17 04:54:49 root INFO     [order_1_approx] starting weight calculation for The state of being related is relatedness
The state of being serious is seriousness
The state of being connected is connectedness
The state of being hidden is hiddenness
The state of being impressive is impressiveness
The state of being strange is strangeness
The state of being reasonable is reasonableness
The state of being same is
2024-07-17 04:54:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 04:58:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0225,  1.3896,  1.5293,  ..., -1.2881,  0.2725,  0.1377],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2334,  2.9922, -3.0195,  ...,  0.1317,  1.6816,  1.9277],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0117, -0.0007,  0.0093,  ..., -0.0142, -0.0130, -0.0283],
        [ 0.0003, -0.0121,  0.0067,  ...,  0.0130, -0.0095, -0.0008],
        [-0.0072, -0.0109, -0.0174,  ..., -0.0131, -0.0073,  0.0056],
        ...,
        [-0.0018,  0.0021, -0.0001,  ...,  0.0038,  0.0030, -0.0098],
        [-0.0072,  0.0033,  0.0054,  ..., -0.0060, -0.0108,  0.0056],
        [-0.0107,  0.0143,  0.0025,  ..., -0.0149, -0.0032, -0.0198]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1836,  3.4219, -3.5176,  ..., -0.2035,  1.5576,  2.2129]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 04:58:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being related is relatedness
The state of being serious is seriousness
The state of being connected is connectedness
The state of being hidden is hiddenness
The state of being impressive is impressiveness
The state of being strange is strangeness
The state of being reasonable is reasonableness
The state of being same is
2024-07-17 04:58:17 root INFO     [order_1_approx] starting weight calculation for The state of being connected is connectedness
The state of being impressive is impressiveness
The state of being same is sameness
The state of being related is relatedness
The state of being serious is seriousness
The state of being hidden is hiddenness
The state of being reasonable is reasonableness
The state of being strange is
2024-07-17 04:58:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 05:01:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8877,  0.4521, -0.4771,  ...,  1.5068,  1.5234, -0.2426],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.7500, -0.3330,  0.2588,  ...,  1.7754,  1.1514,  7.0625],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.6785e-03, -9.2773e-03,  1.2291e-02,  ..., -7.0992e-03,
          5.8441e-03, -9.7122e-03],
        [ 6.9427e-04, -5.3444e-03,  4.2725e-03,  ...,  2.6684e-03,
         -1.5091e-02, -6.3019e-03],
        [ 7.3166e-03, -2.0828e-03, -1.8494e-02,  ..., -3.8052e-03,
         -9.4757e-03, -2.0981e-05],
        ...,
        [-4.2648e-03, -1.0498e-02,  7.7209e-03,  ..., -9.5139e-03,
         -4.5738e-03, -9.6970e-03],
        [-4.4785e-03,  3.9177e-03,  7.2098e-03,  ..., -5.7220e-03,
         -1.4450e-02, -2.5539e-03],
        [-1.0178e-02,  1.1002e-02,  3.6106e-03,  ..., -6.3782e-03,
         -1.2619e-02, -1.0132e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.3926, -0.2301,  0.4077,  ...,  1.6055,  1.1162,  7.3750]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 05:01:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being connected is connectedness
The state of being impressive is impressiveness
The state of being same is sameness
The state of being related is relatedness
The state of being serious is seriousness
The state of being hidden is hiddenness
The state of being reasonable is reasonableness
The state of being strange is
2024-07-17 05:01:45 root INFO     [order_1_approx] starting weight calculation for The state of being same is sameness
The state of being impressive is impressiveness
The state of being related is relatedness
The state of being connected is connectedness
The state of being hidden is hiddenness
The state of being reasonable is reasonableness
The state of being strange is strangeness
The state of being serious is
2024-07-17 05:01:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 05:05:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4670,  2.3438,  1.5234,  ..., -0.0843,  2.5781,  0.7529],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0117,  4.0703,  3.6426,  ..., -3.0293, -1.2285,  1.6777],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0060, -0.0061,  0.0184,  ..., -0.0038, -0.0150, -0.0084],
        [ 0.0055, -0.0094,  0.0028,  ...,  0.0072, -0.0062, -0.0113],
        [-0.0055, -0.0060, -0.0146,  ..., -0.0121,  0.0033,  0.0104],
        ...,
        [-0.0103, -0.0112,  0.0059,  ..., -0.0052,  0.0106, -0.0082],
        [-0.0098, -0.0035,  0.0043,  ...,  0.0008, -0.0182,  0.0004],
        [-0.0102,  0.0155,  0.0051,  ..., -0.0081,  0.0055, -0.0140]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5137,  4.4688,  3.8438,  ..., -3.0703, -1.0176,  1.3223]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 05:05:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being same is sameness
The state of being impressive is impressiveness
The state of being related is relatedness
The state of being connected is connectedness
The state of being hidden is hiddenness
The state of being reasonable is reasonableness
The state of being strange is strangeness
The state of being serious is
2024-07-17 05:05:12 root INFO     total operator prediction time: 1664.0681431293488 seconds
2024-07-17 05:05:12 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on noun+less_reg
2024-07-17 05:05:12 root INFO     building operator noun+less_reg
2024-07-17 05:05:12 root INFO     [order_1_approx] starting weight calculation for Something without home is homeless
Something without ruth is ruthless
Something without gender is genderless
Something without death is deathless
Something without error is errorless
Something without speech is speechless
Something without hair is hairless
Something without life is
2024-07-17 05:05:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 05:08:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0029,  1.2129,  0.6836,  ..., -0.4993,  1.0029,  1.3203],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.3828, -1.6836,  1.0391,  ..., -2.2754,  2.2930, -0.1611],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0012, -0.0259,  0.0106,  ..., -0.0034, -0.0034, -0.0137],
        [-0.0035, -0.0075, -0.0149,  ...,  0.0107,  0.0099, -0.0123],
        [-0.0033, -0.0015, -0.0136,  ..., -0.0168,  0.0010, -0.0032],
        ...,
        [ 0.0026, -0.0007,  0.0101,  ..., -0.0051, -0.0076, -0.0005],
        [-0.0065, -0.0115,  0.0094,  ..., -0.0048, -0.0049,  0.0044],
        [-0.0083,  0.0088,  0.0134,  ...,  0.0002, -0.0034, -0.0130]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.9512, -1.8877,  1.2139,  ..., -2.3906,  2.2031, -0.3965]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 05:08:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without home is homeless
Something without ruth is ruthless
Something without gender is genderless
Something without death is deathless
Something without error is errorless
Something without speech is speechless
Something without hair is hairless
Something without life is
2024-07-17 05:08:39 root INFO     [order_1_approx] starting weight calculation for Something without life is lifeless
Something without home is homeless
Something without hair is hairless
Something without speech is speechless
Something without error is errorless
Something without gender is genderless
Something without ruth is ruthless
Something without death is
2024-07-17 05:08:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 05:12:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4106,  1.6592, -0.4805,  ..., -0.6387,  0.8965,  0.3809],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.4609, -1.9961,  0.4023,  ..., -3.6992, -0.2218, -0.1006],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0055, -0.0214,  0.0094,  ..., -0.0131, -0.0025, -0.0079],
        [-0.0075, -0.0134,  0.0037,  ...,  0.0102,  0.0057, -0.0050],
        [-0.0038,  0.0054, -0.0119,  ..., -0.0150, -0.0014,  0.0015],
        ...,
        [ 0.0005,  0.0054, -0.0018,  ..., -0.0146,  0.0082, -0.0005],
        [-0.0065, -0.0049,  0.0026,  ...,  0.0002, -0.0074,  0.0087],
        [-0.0128,  0.0111,  0.0218,  ..., -0.0030, -0.0049, -0.0082]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.3398, -1.5576,  0.4639,  ..., -4.5508, -0.2954, -0.5229]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 05:12:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without life is lifeless
Something without home is homeless
Something without hair is hairless
Something without speech is speechless
Something without error is errorless
Something without gender is genderless
Something without ruth is ruthless
Something without death is
2024-07-17 05:12:08 root INFO     [order_1_approx] starting weight calculation for Something without life is lifeless
Something without speech is speechless
Something without ruth is ruthless
Something without hair is hairless
Something without error is errorless
Something without gender is genderless
Something without death is deathless
Something without home is
2024-07-17 05:12:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 05:15:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.0527, 0.6709, 0.1616,  ..., 0.0845, 0.8652, 1.8447], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6650, -0.7959, -4.6484,  ..., -4.3398,  2.3496,  4.4688],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0083, -0.0158,  0.0107,  ..., -0.0060,  0.0027, -0.0067],
        [-0.0053, -0.0092,  0.0012,  ...,  0.0073,  0.0012, -0.0027],
        [-0.0067,  0.0050, -0.0219,  ..., -0.0122, -0.0021, -0.0106],
        ...,
        [-0.0007, -0.0104,  0.0025,  ..., -0.0075,  0.0037, -0.0050],
        [-0.0042, -0.0073,  0.0107,  ...,  0.0063, -0.0145,  0.0085],
        [-0.0052,  0.0233,  0.0044,  ..., -0.0015, -0.0041, -0.0093]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9526,  0.2090, -3.7227,  ..., -4.3711,  2.2207,  4.1562]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 05:15:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without life is lifeless
Something without speech is speechless
Something without ruth is ruthless
Something without hair is hairless
Something without error is errorless
Something without gender is genderless
Something without death is deathless
Something without home is
2024-07-17 05:15:35 root INFO     [order_1_approx] starting weight calculation for Something without home is homeless
Something without error is errorless
Something without speech is speechless
Something without hair is hairless
Something without life is lifeless
Something without gender is genderless
Something without death is deathless
Something without ruth is
2024-07-17 05:15:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 05:19:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5811,  0.5981,  0.1044,  ...,  1.7246, -1.1133,  1.2363],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.1719, -3.5469, -6.4531,  ...,  2.6836, -3.0723,  4.0781],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0087, -0.0233,  0.0131,  ..., -0.0141, -0.0096, -0.0140],
        [-0.0060, -0.0124, -0.0075,  ..., -0.0012,  0.0059,  0.0049],
        [-0.0131,  0.0046, -0.0202,  ...,  0.0061, -0.0051, -0.0040],
        ...,
        [-0.0070, -0.0036,  0.0025,  ..., -0.0080,  0.0004,  0.0013],
        [-0.0041, -0.0101,  0.0110,  ...,  0.0082, -0.0052,  0.0091],
        [-0.0196,  0.0084,  0.0101,  ..., -0.0019,  0.0028, -0.0208]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.8145, -3.3984, -6.3594,  ...,  1.8311, -3.1621,  4.2930]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 05:19:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without home is homeless
Something without error is errorless
Something without speech is speechless
Something without hair is hairless
Something without life is lifeless
Something without gender is genderless
Something without death is deathless
Something without ruth is
2024-07-17 05:19:03 root INFO     [order_1_approx] starting weight calculation for Something without home is homeless
Something without death is deathless
Something without gender is genderless
Something without error is errorless
Something without hair is hairless
Something without ruth is ruthless
Something without life is lifeless
Something without speech is
2024-07-17 05:19:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 05:22:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3535,  0.9297, -0.2646,  ...,  0.0342,  0.7969,  0.8037],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.2852,  2.3926, -4.5547,  ..., -1.2246, -0.8804, -1.3916],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0075, -0.0156,  0.0085,  ..., -0.0074, -0.0089, -0.0013],
        [-0.0128, -0.0013, -0.0066,  ...,  0.0055,  0.0027,  0.0051],
        [-0.0024,  0.0124, -0.0051,  ..., -0.0188, -0.0028, -0.0004],
        ...,
        [-0.0030, -0.0081,  0.0070,  ...,  0.0028,  0.0077, -0.0013],
        [-0.0019, -0.0197,  0.0089,  ..., -0.0162, -0.0105, -0.0019],
        [-0.0121,  0.0155,  0.0102,  ..., -0.0032,  0.0015, -0.0053]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4980,  2.3086, -4.1133,  ..., -1.2012, -0.8545, -1.4766]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 05:22:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without home is homeless
Something without death is deathless
Something without gender is genderless
Something without error is errorless
Something without hair is hairless
Something without ruth is ruthless
Something without life is lifeless
Something without speech is
2024-07-17 05:22:30 root INFO     [order_1_approx] starting weight calculation for Something without death is deathless
Something without speech is speechless
Something without ruth is ruthless
Something without home is homeless
Something without hair is hairless
Something without gender is genderless
Something without life is lifeless
Something without error is
2024-07-17 05:22:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 05:25:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1084,  0.7979,  0.7441,  ..., -0.4854, -1.2637,  1.2998],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7285,  0.8120,  1.9658,  ..., -1.8848, -0.0096,  1.0605],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0157, -0.0109,  0.0131,  ..., -0.0032,  0.0046, -0.0023],
        [ 0.0015, -0.0185,  0.0012,  ..., -0.0039, -0.0083, -0.0070],
        [ 0.0052,  0.0033, -0.0156,  ..., -0.0062,  0.0122, -0.0100],
        ...,
        [ 0.0114, -0.0092,  0.0064,  ...,  0.0007,  0.0025, -0.0006],
        [-0.0126, -0.0120,  0.0004,  ..., -0.0024, -0.0191,  0.0066],
        [-0.0023,  0.0109, -0.0001,  ...,  0.0020,  0.0067, -0.0113]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5312,  1.2031,  1.6074,  ..., -2.3789, -0.0141,  0.4033]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 05:25:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without death is deathless
Something without speech is speechless
Something without ruth is ruthless
Something without home is homeless
Something without hair is hairless
Something without gender is genderless
Something without life is lifeless
Something without error is
2024-07-17 05:25:59 root INFO     [order_1_approx] starting weight calculation for Something without hair is hairless
Something without death is deathless
Something without error is errorless
Something without ruth is ruthless
Something without speech is speechless
Something without life is lifeless
Something without home is homeless
Something without gender is
2024-07-17 05:25:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 05:29:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9561,  0.1826, -1.1543,  ..., -1.7314,  0.7559,  2.2031],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.7617,  1.4160, -6.8633,  ..., -1.1094,  4.8203,  2.2559],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0108, -0.0161,  0.0001,  ..., -0.0037, -0.0049, -0.0118],
        [ 0.0076, -0.0098,  0.0026,  ..., -0.0029, -0.0040, -0.0006],
        [ 0.0005,  0.0107, -0.0172,  ..., -0.0063, -0.0104,  0.0017],
        ...,
        [ 0.0106, -0.0054,  0.0020,  ...,  0.0026, -0.0024, -0.0075],
        [-0.0186, -0.0135,  0.0086,  ...,  0.0047, -0.0049,  0.0055],
        [-0.0044,  0.0145,  0.0098,  ...,  0.0005,  0.0089, -0.0134]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.8242,  1.7500, -6.5938,  ..., -1.0225,  4.5469,  2.5039]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 05:29:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without hair is hairless
Something without death is deathless
Something without error is errorless
Something without ruth is ruthless
Something without speech is speechless
Something without life is lifeless
Something without home is homeless
Something without gender is
2024-07-17 05:29:28 root INFO     [order_1_approx] starting weight calculation for Something without speech is speechless
Something without home is homeless
Something without death is deathless
Something without life is lifeless
Something without error is errorless
Something without ruth is ruthless
Something without gender is genderless
Something without hair is
2024-07-17 05:29:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 05:32:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3389,  1.6113, -0.7402,  ...,  1.3281, -0.8164,  1.7578],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.0469, -2.0293, -5.1797,  ...,  1.3164, -1.1943,  0.1035],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0137, -0.0100,  0.0015,  ...,  0.0022,  0.0015, -0.0056],
        [ 0.0038, -0.0177,  0.0077,  ..., -0.0078, -0.0006,  0.0031],
        [ 0.0015,  0.0133, -0.0144,  ..., -0.0132,  0.0059,  0.0014],
        ...,
        [-0.0059, -0.0014,  0.0098,  ...,  0.0027,  0.0003, -0.0099],
        [-0.0098, -0.0144,  0.0084,  ...,  0.0079, -0.0122, -0.0011],
        [-0.0054,  0.0030,  0.0120,  ..., -0.0016, -0.0070, -0.0285]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.0586, -1.9893, -5.5273,  ...,  0.2969, -0.8203,  0.5605]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 05:32:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without speech is speechless
Something without home is homeless
Something without death is deathless
Something without life is lifeless
Something without error is errorless
Something without ruth is ruthless
Something without gender is genderless
Something without hair is
2024-07-17 05:32:56 root INFO     total operator prediction time: 1663.8505129814148 seconds
2024-07-17 05:32:56 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+ment_irreg
2024-07-17 05:32:56 root INFO     building operator verb+ment_irreg
2024-07-17 05:32:56 root INFO     [order_1_approx] starting weight calculation for To redevelop results in a redevelopment
To encourage results in a encouragement
To resent results in a resentment
To disagree results in a disagreement
To entitle results in a entitlement
To reimburse results in a reimbursement
To fulfil results in a fulfilment
To enjoy results in a
2024-07-17 05:32:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 05:36:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1875,  0.7021,  1.1416,  ..., -0.5684,  0.0735,  0.5142],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3184,  4.6836,  2.8301,  ..., -0.4219, -0.0903,  4.6055],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0074, -0.0137,  0.0126,  ..., -0.0088,  0.0013, -0.0114],
        [-0.0068, -0.0122,  0.0009,  ..., -0.0054, -0.0012, -0.0126],
        [ 0.0060,  0.0082, -0.0009,  ..., -0.0002,  0.0042, -0.0045],
        ...,
        [-0.0052, -0.0067,  0.0030,  ..., -0.0069, -0.0085, -0.0095],
        [ 0.0052,  0.0137, -0.0106,  ...,  0.0067, -0.0063,  0.0135],
        [-0.0043,  0.0272,  0.0076,  ..., -0.0099,  0.0036, -0.0118]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0809,  4.4492,  2.0371,  ..., -0.7822,  0.3684,  5.2617]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 05:36:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To redevelop results in a redevelopment
To encourage results in a encouragement
To resent results in a resentment
To disagree results in a disagreement
To entitle results in a entitlement
To reimburse results in a reimbursement
To fulfil results in a fulfilment
To enjoy results in a
2024-07-17 05:36:24 root INFO     [order_1_approx] starting weight calculation for To redevelop results in a redevelopment
To entitle results in a entitlement
To resent results in a resentment
To fulfil results in a fulfilment
To reimburse results in a reimbursement
To disagree results in a disagreement
To enjoy results in a enjoyment
To encourage results in a
2024-07-17 05:36:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 05:39:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6772,  0.0149,  0.0913,  ..., -0.4363,  1.5371,  1.3652],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0859, -1.7197,  0.5322,  ...,  0.3992, -2.5137,  2.2812],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.6616e-03, -8.4152e-03,  1.0513e-02,  ..., -9.9716e-03,
         -1.2901e-02, -4.5013e-04],
        [-8.5907e-03, -6.1417e-04, -4.7684e-03,  ..., -4.0817e-03,
          8.0032e-03, -4.6997e-03],
        [-5.8174e-03, -6.6071e-03, -5.3787e-04,  ..., -6.8665e-05,
          1.4763e-02, -6.3934e-03],
        ...,
        [-3.8033e-03, -2.4307e-02,  5.8022e-03,  ..., -2.8763e-03,
          3.4256e-03, -1.1124e-02],
        [ 1.0468e-02,  1.3657e-02, -1.5686e-02,  ..., -3.5934e-03,
         -1.5411e-02,  1.6037e-02],
        [-1.6205e-02,  3.1952e-02,  9.7885e-03,  ...,  6.4392e-03,
         -8.5449e-03, -6.5155e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1113, -2.0391,  0.6797,  ...,  0.7559, -2.2402,  2.4434]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 05:39:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To redevelop results in a redevelopment
To entitle results in a entitlement
To resent results in a resentment
To fulfil results in a fulfilment
To reimburse results in a reimbursement
To disagree results in a disagreement
To enjoy results in a enjoyment
To encourage results in a
2024-07-17 05:39:52 root INFO     [order_1_approx] starting weight calculation for To enjoy results in a enjoyment
To encourage results in a encouragement
To fulfil results in a fulfilment
To entitle results in a entitlement
To disagree results in a disagreement
To reimburse results in a reimbursement
To resent results in a resentment
To redevelop results in a
2024-07-17 05:39:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 05:43:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2947,  0.1946,  0.2732,  ..., -0.7769, -0.1960,  0.7485],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0723, -0.5615,  0.4648,  ..., -2.5977, -1.5557,  5.4492],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0074, -0.0084,  0.0051,  ..., -0.0039, -0.0038, -0.0139],
        [-0.0085,  0.0005, -0.0005,  ..., -0.0006,  0.0019,  0.0028],
        [-0.0153, -0.0026, -0.0077,  ..., -0.0071,  0.0139, -0.0099],
        ...,
        [-0.0098, -0.0073, -0.0013,  ..., -0.0113,  0.0005, -0.0032],
        [-0.0027,  0.0037,  0.0079,  ...,  0.0019, -0.0153,  0.0149],
        [-0.0112,  0.0128, -0.0029,  ..., -0.0014,  0.0081, -0.0098]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6245, -0.7153,  0.3140,  ..., -2.9570, -1.7461,  5.4609]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 05:43:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To enjoy results in a enjoyment
To encourage results in a encouragement
To fulfil results in a fulfilment
To entitle results in a entitlement
To disagree results in a disagreement
To reimburse results in a reimbursement
To resent results in a resentment
To redevelop results in a
2024-07-17 05:43:20 root INFO     [order_1_approx] starting weight calculation for To redevelop results in a redevelopment
To resent results in a resentment
To fulfil results in a fulfilment
To entitle results in a entitlement
To enjoy results in a enjoyment
To encourage results in a encouragement
To reimburse results in a reimbursement
To disagree results in a
2024-07-17 05:43:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 05:46:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4561, -0.9580,  1.9336,  ..., -0.5068,  0.2644,  0.4363],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7695, -1.3662, -0.8599,  ...,  2.2285,  3.2305,  1.8457],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0040, -0.0105,  0.0045,  ..., -0.0023, -0.0102, -0.0045],
        [-0.0103, -0.0022, -0.0025,  ..., -0.0155,  0.0008, -0.0026],
        [-0.0029, -0.0042, -0.0037,  ..., -0.0094,  0.0075, -0.0090],
        ...,
        [-0.0065,  0.0005,  0.0042,  ...,  0.0095,  0.0111, -0.0043],
        [-0.0003,  0.0045, -0.0021,  ..., -0.0046, -0.0226,  0.0062],
        [-0.0055,  0.0262,  0.0081,  ..., -0.0175, -0.0087, -0.0136]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5381, -1.2471, -1.5957,  ...,  2.4023,  3.7207,  1.1973]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 05:46:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To redevelop results in a redevelopment
To resent results in a resentment
To fulfil results in a fulfilment
To entitle results in a entitlement
To enjoy results in a enjoyment
To encourage results in a encouragement
To reimburse results in a reimbursement
To disagree results in a
2024-07-17 05:46:48 root INFO     [order_1_approx] starting weight calculation for To encourage results in a encouragement
To resent results in a resentment
To disagree results in a disagreement
To entitle results in a entitlement
To redevelop results in a redevelopment
To fulfil results in a fulfilment
To enjoy results in a enjoyment
To reimburse results in a
2024-07-17 05:46:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 05:50:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0342, -0.2754,  1.7148,  ..., -1.1855,  0.5518,  0.1494],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.3086,  3.8105, -5.8633,  ...,  4.9492, -0.3994,  5.7656],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0079, -0.0111,  0.0088,  ..., -0.0080,  0.0031, -0.0055],
        [-0.0035, -0.0080,  0.0024,  ...,  0.0023, -0.0047, -0.0029],
        [-0.0096, -0.0029,  0.0077,  ..., -0.0084, -0.0046,  0.0071],
        ...,
        [-0.0038, -0.0063, -0.0052,  ...,  0.0042,  0.0048, -0.0161],
        [-0.0027,  0.0078, -0.0033,  ..., -0.0043, -0.0113,  0.0189],
        [-0.0135,  0.0092,  0.0085,  ..., -0.0034, -0.0082, -0.0028]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.0391,  3.1328, -5.5312,  ...,  4.7227, -0.2417,  6.5195]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 05:50:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To encourage results in a encouragement
To resent results in a resentment
To disagree results in a disagreement
To entitle results in a entitlement
To redevelop results in a redevelopment
To fulfil results in a fulfilment
To enjoy results in a enjoyment
To reimburse results in a
2024-07-17 05:50:16 root INFO     [order_1_approx] starting weight calculation for To redevelop results in a redevelopment
To enjoy results in a enjoyment
To disagree results in a disagreement
To entitle results in a entitlement
To encourage results in a encouragement
To fulfil results in a fulfilment
To reimburse results in a reimbursement
To resent results in a
2024-07-17 05:50:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 05:53:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.5078,  0.1328,  1.0234,  ...,  0.3125, -0.0347, -0.3049],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.9531,  5.7422,  0.7451,  ...,  6.1328, -3.9551,  3.9883],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-8.9645e-03,  2.6703e-05,  6.4049e-03,  ..., -3.2806e-03,
         -6.1073e-03,  1.8473e-03],
        [-8.9264e-03, -2.4033e-02,  2.8076e-03,  ..., -8.1024e-03,
         -3.7003e-03, -1.0323e-02],
        [ 4.7150e-03, -7.2136e-03, -1.1765e-02,  ...,  3.6621e-03,
          1.0864e-02, -7.4692e-03],
        ...,
        [-1.9623e-02, -1.3168e-02, -2.7695e-03,  ..., -1.4496e-04,
         -7.3853e-03, -7.1945e-03],
        [ 4.2953e-03,  1.5114e-02, -1.5068e-03,  ..., -1.6422e-03,
         -2.1393e-02,  2.0386e-02],
        [-1.0811e-02,  2.1500e-02,  5.2567e-03,  ..., -1.0872e-02,
         -1.4908e-02, -1.4725e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.6211,  5.7188,  0.2981,  ...,  5.9688, -3.8711,  4.6445]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 05:53:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To redevelop results in a redevelopment
To enjoy results in a enjoyment
To disagree results in a disagreement
To entitle results in a entitlement
To encourage results in a encouragement
To fulfil results in a fulfilment
To reimburse results in a reimbursement
To resent results in a
2024-07-17 05:53:44 root INFO     [order_1_approx] starting weight calculation for To encourage results in a encouragement
To redevelop results in a redevelopment
To resent results in a resentment
To disagree results in a disagreement
To enjoy results in a enjoyment
To reimburse results in a reimbursement
To fulfil results in a fulfilment
To entitle results in a
2024-07-17 05:53:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 05:57:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0166,  0.0242,  1.8955,  ...,  0.0836, -0.3564,  0.4905],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1948,  2.4219,  1.1797,  ...,  5.1992, -1.1680,  1.8789],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0010, -0.0148,  0.0069,  ..., -0.0077, -0.0077, -0.0072],
        [-0.0147, -0.0031,  0.0008,  ..., -0.0005, -0.0005, -0.0082],
        [ 0.0020, -0.0089, -0.0050,  ...,  0.0115, -0.0142,  0.0026],
        ...,
        [-0.0110, -0.0066, -0.0131,  ...,  0.0036, -0.0050, -0.0130],
        [-0.0015,  0.0081, -0.0088,  ..., -0.0094, -0.0096,  0.0165],
        [-0.0116,  0.0341, -0.0012,  ..., -0.0035,  0.0036, -0.0113]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0338,  2.0000,  0.3472,  ...,  4.6836, -1.0742,  2.1719]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 05:57:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To encourage results in a encouragement
To redevelop results in a redevelopment
To resent results in a resentment
To disagree results in a disagreement
To enjoy results in a enjoyment
To reimburse results in a reimbursement
To fulfil results in a fulfilment
To entitle results in a
2024-07-17 05:57:14 root INFO     [order_1_approx] starting weight calculation for To redevelop results in a redevelopment
To entitle results in a entitlement
To disagree results in a disagreement
To encourage results in a encouragement
To reimburse results in a reimbursement
To enjoy results in a enjoyment
To resent results in a resentment
To fulfil results in a
2024-07-17 05:57:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 06:00:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.4766,  0.1362,  2.6895,  ..., -0.6338, -0.7041, -0.3948],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.8447,  2.4941, -1.0605,  ..., -1.0889,  1.5283,  4.1055],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0038, -0.0134,  0.0084,  ..., -0.0111,  0.0032, -0.0085],
        [-0.0044, -0.0048, -0.0031,  ..., -0.0031,  0.0036, -0.0010],
        [-0.0042,  0.0006,  0.0037,  ..., -0.0080,  0.0105, -0.0006],
        ...,
        [-0.0102, -0.0035,  0.0058,  ...,  0.0008, -0.0061,  0.0016],
        [ 0.0018, -0.0006, -0.0043,  ..., -0.0039, -0.0083,  0.0046],
        [-0.0059,  0.0302, -0.0020,  ...,  0.0040,  0.0060, -0.0075]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.1250,  2.2891, -1.2061,  ..., -0.7905,  1.5586,  4.6562]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 06:00:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To redevelop results in a redevelopment
To entitle results in a entitlement
To disagree results in a disagreement
To encourage results in a encouragement
To reimburse results in a reimbursement
To enjoy results in a enjoyment
To resent results in a resentment
To fulfil results in a
2024-07-17 06:00:42 root INFO     total operator prediction time: 1666.7356352806091 seconds
2024-07-17 06:00:42 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on name - nationality
2024-07-17 06:00:42 root INFO     building operator name - nationality
2024-07-17 06:00:43 root INFO     [order_1_approx] starting weight calculation for depp was american
gorbachev was soviet
strauss was austrian
plato was greek
galilei was italian
lincoln was american
tolstoi was russian
machiavelli was
2024-07-17 06:00:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 06:04:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6328, -1.1182,  0.0968,  ...,  0.3486,  0.4146,  1.2988],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3359, -3.4219, -7.6016,  ..., -1.2715, -3.2188, -2.5332],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0258, -0.0088,  0.0008,  ..., -0.0078,  0.0041,  0.0041],
        [ 0.0038,  0.0087,  0.0021,  ...,  0.0071,  0.0047, -0.0034],
        [-0.0027, -0.0071,  0.0148,  ..., -0.0063, -0.0004, -0.0115],
        ...,
        [-0.0084,  0.0080,  0.0070,  ...,  0.0270, -0.0046,  0.0033],
        [-0.0027, -0.0026,  0.0039,  ...,  0.0031,  0.0136,  0.0026],
        [ 0.0012,  0.0026,  0.0060,  ..., -0.0123,  0.0108,  0.0179]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1738, -3.3965, -7.4297,  ..., -2.0020, -2.7773, -2.1680]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 06:04:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for depp was american
gorbachev was soviet
strauss was austrian
plato was greek
galilei was italian
lincoln was american
tolstoi was russian
machiavelli was
2024-07-17 06:04:10 root INFO     [order_1_approx] starting weight calculation for strauss was austrian
tolstoi was russian
machiavelli was italian
depp was american
lincoln was american
galilei was italian
plato was greek
gorbachev was
2024-07-17 06:04:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 06:07:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3264, -0.3599, -0.5815,  ...,  0.1267,  1.2832,  1.8760],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2559, -1.3574, -2.5449,  ..., -6.4531, -1.6016,  0.0623],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0234, -0.0110,  0.0004,  ..., -0.0087,  0.0020, -0.0027],
        [ 0.0104,  0.0060, -0.0029,  ...,  0.0136, -0.0056, -0.0057],
        [ 0.0026, -0.0095,  0.0009,  ..., -0.0064, -0.0024, -0.0016],
        ...,
        [-0.0045,  0.0081, -0.0020,  ...,  0.0198, -0.0055,  0.0041],
        [ 0.0058, -0.0099,  0.0104,  ...,  0.0009,  0.0206,  0.0067],
        [-0.0037, -0.0085,  0.0111,  ..., -0.0191, -0.0080,  0.0068]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.8911, -1.7930, -2.9512,  ..., -6.2500, -1.8193, -0.3472]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 06:07:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for strauss was austrian
tolstoi was russian
machiavelli was italian
depp was american
lincoln was american
galilei was italian
plato was greek
gorbachev was
2024-07-17 06:07:38 root INFO     [order_1_approx] starting weight calculation for depp was american
tolstoi was russian
gorbachev was soviet
machiavelli was italian
strauss was austrian
plato was greek
lincoln was american
galilei was
2024-07-17 06:07:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 06:11:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3701,  0.1465,  1.0059,  ...,  0.4917, -0.4497,  0.7422],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.0625, -4.0078, -4.4883,  ..., -1.6338, -4.9609, -3.7422],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0240, -0.0106,  0.0072,  ..., -0.0006, -0.0027,  0.0014],
        [-0.0002,  0.0110,  0.0090,  ...,  0.0081, -0.0082,  0.0186],
        [ 0.0025, -0.0043,  0.0149,  ..., -0.0071, -0.0211, -0.0006],
        ...,
        [-0.0182,  0.0036, -0.0047,  ...,  0.0253, -0.0117, -0.0034],
        [-0.0057, -0.0048,  0.0062,  ..., -0.0068,  0.0015,  0.0092],
        [-0.0090,  0.0123,  0.0030,  ..., -0.0223, -0.0115,  0.0179]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1230, -3.8887, -4.2109,  ..., -1.5518, -4.3477, -3.6660]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 06:11:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for depp was american
tolstoi was russian
gorbachev was soviet
machiavelli was italian
strauss was austrian
plato was greek
lincoln was american
galilei was
2024-07-17 06:11:05 root INFO     [order_1_approx] starting weight calculation for plato was greek
depp was american
tolstoi was russian
galilei was italian
gorbachev was soviet
lincoln was american
machiavelli was italian
strauss was
2024-07-17 06:11:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 06:14:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0994,  0.3030,  0.7295,  ...,  1.0254, -0.1006, -0.0244],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2949, -3.4180, -7.2148,  ..., -7.1562, -2.0117, -3.2031],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0071, -0.0031, -0.0090,  ...,  0.0009,  0.0030,  0.0032],
        [ 0.0119,  0.0060,  0.0047,  ...,  0.0062,  0.0014, -0.0051],
        [ 0.0039, -0.0095, -0.0003,  ..., -0.0062,  0.0038,  0.0005],
        ...,
        [-0.0022, -0.0027,  0.0060,  ...,  0.0099, -0.0220, -0.0012],
        [-0.0018, -0.0028,  0.0050,  ...,  0.0030,  0.0066,  0.0045],
        [-0.0124, -0.0009,  0.0015,  ..., -0.0084, -0.0018,  0.0106]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0624, -3.7773, -6.9297,  ..., -5.8633, -1.8076, -2.7383]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 06:14:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for plato was greek
depp was american
tolstoi was russian
galilei was italian
gorbachev was soviet
lincoln was american
machiavelli was italian
strauss was
2024-07-17 06:14:32 root INFO     [order_1_approx] starting weight calculation for gorbachev was soviet
machiavelli was italian
galilei was italian
plato was greek
tolstoi was russian
strauss was austrian
depp was american
lincoln was
2024-07-17 06:14:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 06:17:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2754, -0.3584, -0.5273,  ...,  0.8389, -1.9727,  1.2646],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0508, -1.1406, -5.9766,  ..., -0.0962,  0.4807, -2.5566],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 9.1629e-03, -5.0850e-03, -2.7943e-03,  ..., -3.2539e-03,
         -4.5776e-05,  7.0953e-03],
        [ 4.4022e-03,  1.6747e-03,  1.1330e-02,  ...,  1.4145e-02,
         -8.1482e-03,  5.1117e-03],
        [ 7.6256e-03, -5.7602e-04, -3.3951e-03,  ..., -8.7280e-03,
         -8.6784e-04,  2.6131e-03],
        ...,
        [-1.1997e-03,  7.8125e-03,  4.4746e-03,  ...,  1.0086e-02,
         -3.8376e-03,  1.8978e-04],
        [-5.0735e-04, -3.5229e-03,  8.7509e-03,  ...,  2.1572e-03,
          1.0986e-02,  7.8964e-04],
        [ 1.9855e-03,  3.7384e-04, -9.6893e-04,  ..., -1.7538e-03,
          1.9951e-03,  4.2953e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0430, -0.9619, -5.8516,  ...,  0.7119,  0.8242, -1.7109]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 06:17:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for gorbachev was soviet
machiavelli was italian
galilei was italian
plato was greek
tolstoi was russian
strauss was austrian
depp was american
lincoln was
2024-07-17 06:17:59 root INFO     [order_1_approx] starting weight calculation for strauss was austrian
tolstoi was russian
machiavelli was italian
lincoln was american
plato was greek
galilei was italian
gorbachev was soviet
depp was
2024-07-17 06:18:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 06:21:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.8320,  0.8711,  0.1210,  ...,  0.7656, -1.2354, -0.9697],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0234, -1.1426, -5.2188,  ..., -2.9570, -1.1523, -3.5449],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0153, -0.0036, -0.0078,  ..., -0.0033, -0.0029,  0.0095],
        [ 0.0115,  0.0060, -0.0069,  ...,  0.0026,  0.0060,  0.0107],
        [ 0.0055, -0.0055, -0.0055,  ..., -0.0008,  0.0059, -0.0035],
        ...,
        [ 0.0003, -0.0009, -0.0025,  ...,  0.0217, -0.0026,  0.0092],
        [-0.0036,  0.0057,  0.0112,  ...,  0.0024,  0.0131, -0.0101],
        [ 0.0113, -0.0068,  0.0057,  ..., -0.0159,  0.0093,  0.0276]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4844, -0.8516, -5.4258,  ..., -3.1699, -1.1113, -4.5586]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 06:21:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for strauss was austrian
tolstoi was russian
machiavelli was italian
lincoln was american
plato was greek
galilei was italian
gorbachev was soviet
depp was
2024-07-17 06:21:27 root INFO     [order_1_approx] starting weight calculation for plato was greek
machiavelli was italian
galilei was italian
depp was american
strauss was austrian
lincoln was american
gorbachev was soviet
tolstoi was
2024-07-17 06:21:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 06:24:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1055,  0.8140,  1.7656,  ...,  0.0601, -0.1572,  0.9365],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.5039, -1.1475, -5.1211,  ..., -4.3125, -1.8066, -5.4336],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0298, -0.0114, -0.0101,  ..., -0.0073,  0.0154,  0.0008],
        [ 0.0068,  0.0150,  0.0157,  ...,  0.0138,  0.0097,  0.0057],
        [-0.0074,  0.0069,  0.0053,  ..., -0.0145, -0.0044,  0.0042],
        ...,
        [-0.0114,  0.0147,  0.0007,  ...,  0.0212, -0.0211,  0.0063],
        [ 0.0025, -0.0201, -0.0020,  ...,  0.0068,  0.0273,  0.0067],
        [-0.0069,  0.0184, -0.0033,  ..., -0.0244,  0.0037,  0.0155]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.2422, -1.2812, -4.7852,  ..., -3.9375, -1.3389, -4.4688]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 06:24:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for plato was greek
machiavelli was italian
galilei was italian
depp was american
strauss was austrian
lincoln was american
gorbachev was soviet
tolstoi was
2024-07-17 06:24:55 root INFO     [order_1_approx] starting weight calculation for depp was american
galilei was italian
tolstoi was russian
strauss was austrian
machiavelli was italian
lincoln was american
gorbachev was soviet
plato was
2024-07-17 06:24:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 06:28:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3569,  0.6743,  0.7891,  ...,  1.3164,  0.2661,  1.1562],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1328, -0.3987, -2.2988,  ...,  0.8501, -5.1484, -2.7305],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0427, -0.0152,  0.0022,  ..., -0.0040,  0.0076,  0.0051],
        [ 0.0027,  0.0096,  0.0011,  ...,  0.0093,  0.0011,  0.0020],
        [ 0.0030, -0.0124,  0.0039,  ..., -0.0055,  0.0150, -0.0060],
        ...,
        [-0.0063,  0.0109, -0.0056,  ...,  0.0224, -0.0039, -0.0040],
        [ 0.0103,  0.0017, -0.0031,  ...,  0.0085, -0.0013,  0.0030],
        [-0.0067,  0.0044,  0.0032,  ..., -0.0164,  0.0095,  0.0138]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6523, -1.1270, -2.4531,  ...,  1.3936, -5.6797, -3.2578]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 06:28:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for depp was american
galilei was italian
tolstoi was russian
strauss was austrian
machiavelli was italian
lincoln was american
gorbachev was soviet
plato was
2024-07-17 06:28:23 root INFO     total operator prediction time: 1660.4076600074768 seconds
2024-07-17 06:28:23 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on country - language
2024-07-17 06:28:23 root INFO     building operator country - language
2024-07-17 06:28:23 root INFO     [order_1_approx] starting weight calculation for The country of bahamas primarily speaks the language of english
The country of cyprus primarily speaks the language of greek
The country of morocco primarily speaks the language of berber
The country of austria primarily speaks the language of german
The country of colombia primarily speaks the language of spanish
The country of cuba primarily speaks the language of spanish
The country of mexico primarily speaks the language of spanish
The country of belize primarily speaks the language of
2024-07-17 06:28:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 06:31:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1836, -0.2605, -1.5801,  ...,  0.3154,  0.3577, -0.0837],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2783, -4.3125, -7.6289,  ...,  0.7451,  2.1797, -1.0176],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 6.9160e-03, -8.7500e-04,  1.5068e-04,  ...,  4.3259e-03,
         -1.5235e-04, -4.3774e-04],
        [ 3.2330e-03,  5.0507e-03, -5.5733e-03,  ...,  9.2773e-03,
          5.7220e-04, -6.2943e-05],
        [ 9.3460e-04,  6.8550e-03, -3.2616e-04,  ..., -5.8594e-03,
         -5.0497e-04, -3.0766e-03],
        ...,
        [-9.5444e-03, -4.8523e-03,  3.6469e-03,  ...,  9.6817e-03,
         -2.0599e-03, -4.4441e-03],
        [-5.9223e-04, -6.4774e-03, -2.6970e-03,  ...,  7.4310e-03,
          1.0681e-03, -5.1498e-05],
        [-6.3858e-03,  1.5421e-03,  1.0328e-03,  ..., -1.8215e-03,
          8.4076e-03,  3.0537e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9980, -3.2734, -7.4883,  ...,  0.4465,  2.1465, -0.8433]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 06:31:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of bahamas primarily speaks the language of english
The country of cyprus primarily speaks the language of greek
The country of morocco primarily speaks the language of berber
The country of austria primarily speaks the language of german
The country of colombia primarily speaks the language of spanish
The country of cuba primarily speaks the language of spanish
The country of mexico primarily speaks the language of spanish
The country of belize primarily speaks the language of
2024-07-17 06:31:49 root INFO     [order_1_approx] starting weight calculation for The country of colombia primarily speaks the language of spanish
The country of morocco primarily speaks the language of berber
The country of belize primarily speaks the language of english
The country of cuba primarily speaks the language of spanish
The country of mexico primarily speaks the language of spanish
The country of bahamas primarily speaks the language of english
The country of austria primarily speaks the language of german
The country of cyprus primarily speaks the language of
2024-07-17 06:31:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 06:35:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2666, -1.0576, -2.6758,  ...,  1.3545,  0.1106, -0.4380],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9316, -5.6953, -1.3535,  ..., -2.2734, -0.2391, -2.5352],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.0849e-02, -2.8839e-03,  6.1150e-03,  ..., -8.1253e-04,
          4.8733e-04, -9.8343e-03],
        [ 2.8782e-03,  7.6752e-03, -2.7237e-03,  ...,  4.8103e-03,
         -2.4319e-04,  5.8823e-03],
        [ 5.0449e-04,  3.4580e-03, -1.6623e-03,  ..., -2.0103e-03,
          2.6875e-03, -4.6272e-03],
        ...,
        [-5.9433e-03, -4.5776e-05,  1.7548e-03,  ...,  8.3618e-03,
         -1.1513e-02, -3.7432e-04],
        [-2.7161e-03, -3.9139e-03,  5.1069e-04,  ..., -2.3727e-03,
          2.1019e-03,  8.2550e-03],
        [-4.2534e-03, -4.2725e-04,  4.4556e-03,  ..., -1.2589e-04,
          4.5624e-03,  2.8439e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1143, -5.2852, -1.6602,  ..., -3.2695, -1.0039, -2.1738]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 06:35:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of colombia primarily speaks the language of spanish
The country of morocco primarily speaks the language of berber
The country of belize primarily speaks the language of english
The country of cuba primarily speaks the language of spanish
The country of mexico primarily speaks the language of spanish
The country of bahamas primarily speaks the language of english
The country of austria primarily speaks the language of german
The country of cyprus primarily speaks the language of
2024-07-17 06:35:15 root INFO     [order_1_approx] starting weight calculation for The country of morocco primarily speaks the language of berber
The country of cyprus primarily speaks the language of greek
The country of austria primarily speaks the language of german
The country of belize primarily speaks the language of english
The country of bahamas primarily speaks the language of english
The country of colombia primarily speaks the language of spanish
The country of mexico primarily speaks the language of spanish
The country of cuba primarily speaks the language of
2024-07-17 06:35:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 06:38:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1035, -0.7480, -1.4238,  ...,  0.2891, -0.4043, -0.4329],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.8281, -2.6660, -4.8359,  ..., -1.7900,  2.4863, -0.7383],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0072,  0.0008,  0.0020,  ...,  0.0065,  0.0051, -0.0024],
        [-0.0011,  0.0114, -0.0028,  ...,  0.0080, -0.0031,  0.0067],
        [ 0.0007,  0.0032, -0.0014,  ..., -0.0057, -0.0030,  0.0067],
        ...,
        [-0.0069, -0.0058, -0.0029,  ...,  0.0108,  0.0018,  0.0083],
        [-0.0036,  0.0034,  0.0003,  ...,  0.0069,  0.0070,  0.0024],
        [-0.0029,  0.0067,  0.0141,  ...,  0.0059,  0.0168, -0.0130]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6406, -2.7129, -4.9141,  ..., -2.1641,  1.6738, -0.3479]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 06:38:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of morocco primarily speaks the language of berber
The country of cyprus primarily speaks the language of greek
The country of austria primarily speaks the language of german
The country of belize primarily speaks the language of english
The country of bahamas primarily speaks the language of english
The country of colombia primarily speaks the language of spanish
The country of mexico primarily speaks the language of spanish
The country of cuba primarily speaks the language of
2024-07-17 06:38:43 root INFO     [order_1_approx] starting weight calculation for The country of bahamas primarily speaks the language of english
The country of cuba primarily speaks the language of spanish
The country of morocco primarily speaks the language of berber
The country of belize primarily speaks the language of english
The country of cyprus primarily speaks the language of greek
The country of colombia primarily speaks the language of spanish
The country of austria primarily speaks the language of german
The country of mexico primarily speaks the language of
2024-07-17 06:38:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 06:42:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9375,  0.3237,  1.0225,  ..., -0.8984,  1.6602,  0.8604],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2471, -1.9072, -3.1680,  ..., -3.6445, -0.5156, -1.0508],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 8.6441e-03, -1.0624e-03, -6.2485e-03,  ..., -2.7466e-04,
         -1.8339e-03, -7.0381e-04],
        [ 4.5547e-03,  2.7580e-03,  5.7220e-05,  ...,  1.1292e-02,
          3.9749e-03,  2.1858e-03],
        [-2.2087e-03,  6.5422e-04, -2.5558e-03,  ...,  1.1969e-03,
         -1.0052e-03,  9.1858e-03],
        ...,
        [-2.1515e-03,  4.2000e-03,  2.7256e-03,  ...,  1.2100e-02,
         -5.9166e-03,  6.4754e-04],
        [ 1.3208e-03,  8.1635e-04, -2.6779e-03,  ...,  1.5831e-03,
          1.2207e-04, -5.2929e-04],
        [-8.4448e-04, -3.1128e-03,  8.6594e-04,  ...,  2.1801e-03,
          6.0234e-03, -9.1743e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1244, -1.6250, -2.9199,  ..., -3.9727, -0.7095, -0.5376]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 06:42:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of bahamas primarily speaks the language of english
The country of cuba primarily speaks the language of spanish
The country of morocco primarily speaks the language of berber
The country of belize primarily speaks the language of english
The country of cyprus primarily speaks the language of greek
The country of colombia primarily speaks the language of spanish
The country of austria primarily speaks the language of german
The country of mexico primarily speaks the language of
2024-07-17 06:42:11 root INFO     [order_1_approx] starting weight calculation for The country of morocco primarily speaks the language of berber
The country of mexico primarily speaks the language of spanish
The country of bahamas primarily speaks the language of english
The country of colombia primarily speaks the language of spanish
The country of belize primarily speaks the language of english
The country of cyprus primarily speaks the language of greek
The country of cuba primarily speaks the language of spanish
The country of austria primarily speaks the language of
2024-07-17 06:42:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 06:45:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1699,  0.8369, -0.4253,  ..., -1.3867,  0.2083,  2.0508],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3862, -3.2168, -4.0234,  ..., -5.3555,  0.7017, -4.4688],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0135, -0.0075, -0.0066,  ..., -0.0038, -0.0073, -0.0070],
        [ 0.0175,  0.0015,  0.0004,  ...,  0.0106, -0.0003, -0.0094],
        [-0.0054,  0.0183, -0.0003,  ..., -0.0059, -0.0115,  0.0099],
        ...,
        [-0.0135, -0.0093, -0.0029,  ...,  0.0173, -0.0072,  0.0098],
        [ 0.0020,  0.0009, -0.0004,  ...,  0.0024,  0.0014,  0.0111],
        [ 0.0028,  0.0077, -0.0104,  ..., -0.0078,  0.0068,  0.0034]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7417, -3.1523, -3.8574,  ..., -5.5039, -0.3208, -3.7461]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 06:45:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of morocco primarily speaks the language of berber
The country of mexico primarily speaks the language of spanish
The country of bahamas primarily speaks the language of english
The country of colombia primarily speaks the language of spanish
The country of belize primarily speaks the language of english
The country of cyprus primarily speaks the language of greek
The country of cuba primarily speaks the language of spanish
The country of austria primarily speaks the language of
2024-07-17 06:45:38 root INFO     [order_1_approx] starting weight calculation for The country of mexico primarily speaks the language of spanish
The country of colombia primarily speaks the language of spanish
The country of belize primarily speaks the language of english
The country of austria primarily speaks the language of german
The country of cyprus primarily speaks the language of greek
The country of morocco primarily speaks the language of berber
The country of cuba primarily speaks the language of spanish
The country of bahamas primarily speaks the language of
2024-07-17 06:45:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 06:49:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7168,  0.4375, -0.4172,  ..., -0.6758,  0.8218, -0.2607],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6094, -4.1445, -8.3359,  ..., -1.5117,  2.3340,  1.7129],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0169, -0.0041,  0.0089,  ...,  0.0005, -0.0019, -0.0031],
        [-0.0091,  0.0123,  0.0015,  ...,  0.0012,  0.0076,  0.0021],
        [-0.0044,  0.0063,  0.0067,  ..., -0.0026, -0.0002,  0.0008],
        ...,
        [-0.0080,  0.0024,  0.0002,  ...,  0.0129, -0.0032, -0.0009],
        [ 0.0021, -0.0050,  0.0003,  ...,  0.0110,  0.0054,  0.0044],
        [-0.0010,  0.0058,  0.0062,  ...,  0.0006,  0.0080,  0.0053]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2834, -4.5039, -8.5625,  ..., -1.5879,  2.4199,  1.6289]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 06:49:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of mexico primarily speaks the language of spanish
The country of colombia primarily speaks the language of spanish
The country of belize primarily speaks the language of english
The country of austria primarily speaks the language of german
The country of cyprus primarily speaks the language of greek
The country of morocco primarily speaks the language of berber
The country of cuba primarily speaks the language of spanish
The country of bahamas primarily speaks the language of
2024-07-17 06:49:04 root INFO     [order_1_approx] starting weight calculation for The country of austria primarily speaks the language of german
The country of cuba primarily speaks the language of spanish
The country of cyprus primarily speaks the language of greek
The country of morocco primarily speaks the language of berber
The country of belize primarily speaks the language of english
The country of bahamas primarily speaks the language of english
The country of mexico primarily speaks the language of spanish
The country of colombia primarily speaks the language of
2024-07-17 06:49:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 06:52:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7139,  0.3711, -0.6782,  ...,  1.5918,  0.9082,  0.3828],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7637, -3.2988, -4.1758,  ..., -5.7969, -0.2988, -1.9785],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 8.2169e-03, -6.6681e-03, -2.4147e-03,  ..., -2.2774e-03,
          2.1267e-03,  3.0975e-03],
        [-1.8692e-04, -1.0834e-03,  7.0190e-04,  ...,  9.8801e-03,
         -1.6861e-03,  9.1934e-04],
        [ 3.7079e-03,  2.8458e-03, -2.7580e-03,  ..., -6.6261e-03,
          3.4409e-03,  5.0888e-03],
        ...,
        [-8.5602e-03, -4.5395e-03,  5.2872e-03,  ...,  6.4011e-03,
         -4.7150e-03,  2.8229e-03],
        [ 1.4572e-03, -2.1667e-03, -3.9177e-03,  ...,  5.4779e-03,
         -1.2993e-02,  7.6675e-04],
        [-9.0637e-03,  1.1883e-03,  3.9139e-03,  ..., -5.4016e-03,
          1.4782e-05, -5.6000e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 8.1885e-01, -2.8555e+00, -4.5664e+00,  ..., -5.7930e+00,
          4.6387e-03, -1.7344e+00]], device='cuda:1', dtype=torch.float16,
       grad_fn=<SubBackward0>) 

                    
2024-07-17 06:52:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of austria primarily speaks the language of german
The country of cuba primarily speaks the language of spanish
The country of cyprus primarily speaks the language of greek
The country of morocco primarily speaks the language of berber
The country of belize primarily speaks the language of english
The country of bahamas primarily speaks the language of english
The country of mexico primarily speaks the language of spanish
The country of colombia primarily speaks the language of
2024-07-17 06:52:30 root INFO     [order_1_approx] starting weight calculation for The country of belize primarily speaks the language of english
The country of austria primarily speaks the language of german
The country of colombia primarily speaks the language of spanish
The country of mexico primarily speaks the language of spanish
The country of bahamas primarily speaks the language of english
The country of cyprus primarily speaks the language of greek
The country of cuba primarily speaks the language of spanish
The country of morocco primarily speaks the language of
2024-07-17 06:52:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 06:55:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.3906, -0.9873,  0.8394,  ...,  0.3625, -0.4009,  1.5547],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4121, -3.7773, -4.9062,  ..., -3.8047, -3.1387, -1.0879],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0097, -0.0071,  0.0026,  ...,  0.0003, -0.0001,  0.0054],
        [ 0.0016,  0.0053, -0.0014,  ...,  0.0016,  0.0099, -0.0051],
        [-0.0062,  0.0060,  0.0012,  ..., -0.0013, -0.0132,  0.0136],
        ...,
        [-0.0028,  0.0013,  0.0067,  ...,  0.0069, -0.0084,  0.0094],
        [-0.0025, -0.0002, -0.0009,  ..., -0.0043, -0.0140,  0.0226],
        [-0.0096, -0.0011,  0.0102,  ..., -0.0005,  0.0053,  0.0023]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2705, -3.6621, -5.6328,  ..., -4.0195, -3.3691, -1.3105]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 06:55:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of belize primarily speaks the language of english
The country of austria primarily speaks the language of german
The country of colombia primarily speaks the language of spanish
The country of mexico primarily speaks the language of spanish
The country of bahamas primarily speaks the language of english
The country of cyprus primarily speaks the language of greek
The country of cuba primarily speaks the language of spanish
The country of morocco primarily speaks the language of
2024-07-17 06:55:56 root INFO     total operator prediction time: 1652.8310449123383 seconds
2024-07-17 06:55:56 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on animal - shelter
2024-07-17 06:55:56 root INFO     building operator animal - shelter
2024-07-17 06:55:56 root INFO     [order_1_approx] starting weight calculation for The place seal lives in is called den
The place hippopotamus lives in is called river
The place scorpion lives in is called nest
The place bear lives in is called den
The place dolphin lives in is called sea
The place wasp lives in is called nest
The place woodchuck lives in is called hole
The place lion lives in is called
2024-07-17 06:55:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 06:59:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3003, -0.5562, -0.0327,  ...,  0.1035, -0.4233,  1.9434],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9590, -3.2109, -0.1279,  ..., -4.9414,  0.2563,  1.2266],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.0063e-02,  2.9316e-03, -8.9169e-04,  ...,  7.8583e-03,
         -4.2200e-04, -2.9659e-03],
        [ 1.3056e-03,  5.7716e-03,  1.1826e-03,  ...,  8.4686e-03,
          9.1705e-03, -3.1338e-03],
        [-8.3923e-03,  8.0795e-03, -9.5367e-05,  ..., -6.0883e-03,
          1.6947e-03,  7.4081e-03],
        ...,
        [ 3.9444e-03,  4.7112e-03, -2.6093e-03,  ...,  8.4305e-03,
         -2.9583e-03,  8.8577e-03],
        [-2.5082e-03, -6.9237e-03,  2.8000e-03,  ..., -9.5673e-03,
         -2.5330e-03,  1.5593e-03],
        [ 1.6766e-03, -8.9340e-03,  2.7485e-03,  ..., -1.3466e-03,
         -2.1515e-03,  1.6235e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4453, -2.9766, -0.4988,  ..., -4.6602,  0.2954,  1.0684]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 06:59:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place seal lives in is called den
The place hippopotamus lives in is called river
The place scorpion lives in is called nest
The place bear lives in is called den
The place dolphin lives in is called sea
The place wasp lives in is called nest
The place woodchuck lives in is called hole
The place lion lives in is called
2024-07-17 06:59:26 root INFO     [order_1_approx] starting weight calculation for The place scorpion lives in is called nest
The place hippopotamus lives in is called river
The place dolphin lives in is called sea
The place woodchuck lives in is called hole
The place lion lives in is called den
The place bear lives in is called den
The place wasp lives in is called nest
The place seal lives in is called
2024-07-17 06:59:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 07:02:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6919,  1.1846, -1.0557,  ..., -0.0325, -0.8242,  0.6963],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3569,  1.0215, -4.1328,  ..., -1.7842,  4.2578,  2.0508],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0145, -0.0101, -0.0036,  ...,  0.0066, -0.0030, -0.0035],
        [-0.0078,  0.0253,  0.0082,  ..., -0.0001,  0.0128, -0.0076],
        [-0.0057, -0.0086, -0.0012,  ..., -0.0113, -0.0040,  0.0197],
        ...,
        [-0.0073, -0.0033, -0.0069,  ...,  0.0159,  0.0001,  0.0125],
        [-0.0016, -0.0100,  0.0070,  ...,  0.0003,  0.0204,  0.0025],
        [ 0.0045, -0.0056,  0.0015,  ..., -0.0052,  0.0109,  0.0177]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3560,  1.7900, -4.5039,  ..., -2.1543,  3.0625,  2.1582]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 07:02:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place scorpion lives in is called nest
The place hippopotamus lives in is called river
The place dolphin lives in is called sea
The place woodchuck lives in is called hole
The place lion lives in is called den
The place bear lives in is called den
The place wasp lives in is called nest
The place seal lives in is called
2024-07-17 07:02:55 root INFO     [order_1_approx] starting weight calculation for The place hippopotamus lives in is called river
The place wasp lives in is called nest
The place bear lives in is called den
The place seal lives in is called den
The place dolphin lives in is called sea
The place woodchuck lives in is called hole
The place lion lives in is called den
The place scorpion lives in is called
2024-07-17 07:02:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 07:06:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6426, -0.2378, -0.1127,  ..., -0.5068,  0.0161,  0.4504],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.3877, -4.4180, -0.2158,  ..., -4.0352,  2.3398,  1.9824],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 6.8588e-03,  5.2452e-05,  3.0632e-03,  ...,  4.6234e-03,
          2.0027e-04, -3.6011e-03],
        [ 4.5052e-03,  1.2589e-03,  7.5645e-03,  ...,  6.5002e-03,
          5.3902e-03, -4.5662e-03],
        [-2.0523e-03,  1.0109e-04,  4.5013e-04,  ..., -4.4250e-03,
         -9.5940e-04,  5.3482e-03],
        ...,
        [ 3.3150e-03, -4.9362e-03, -1.1396e-03,  ...,  7.6904e-03,
         -4.8027e-03,  2.3003e-03],
        [-4.4022e-03,  2.0962e-03, -2.7847e-04,  ..., -5.2490e-03,
          8.6784e-04,  5.4092e-03],
        [-1.8396e-03, -4.6005e-03,  1.5602e-03,  ..., -1.3552e-03,
          4.3983e-03,  2.4052e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2891, -4.2070, -0.7822,  ..., -4.2461,  2.2207,  2.0137]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 07:06:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place hippopotamus lives in is called river
The place wasp lives in is called nest
The place bear lives in is called den
The place seal lives in is called den
The place dolphin lives in is called sea
The place woodchuck lives in is called hole
The place lion lives in is called den
The place scorpion lives in is called
2024-07-17 07:06:23 root INFO     [order_1_approx] starting weight calculation for The place scorpion lives in is called nest
The place dolphin lives in is called sea
The place hippopotamus lives in is called river
The place wasp lives in is called nest
The place bear lives in is called den
The place seal lives in is called den
The place lion lives in is called den
The place woodchuck lives in is called
2024-07-17 07:06:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 07:09:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8618,  0.9404, -0.8086,  ..., -0.6738, -0.3999,  0.4141],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6152, -2.1484, -2.6289,  ..., -5.5039,  1.7471,  1.6943],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 9.5520e-03, -3.0251e-03,  3.8643e-03,  ..., -2.1152e-03,
          3.7155e-03,  3.6621e-04],
        [-4.5776e-05,  6.1417e-03,  2.0237e-03,  ...,  1.0536e-02,
          8.5297e-03, -3.8414e-03],
        [ 1.3094e-03,  1.2417e-03,  8.6441e-03,  ..., -8.4991e-03,
          5.1651e-03,  6.9733e-03],
        ...,
        [-1.6851e-03,  3.0251e-03,  3.1967e-03,  ...,  1.1337e-02,
         -9.6035e-04,  5.9128e-03],
        [-3.2806e-04, -2.9869e-03, -2.3651e-04,  ..., -3.7346e-03,
          9.2468e-03,  4.5967e-03],
        [-1.7452e-03,  1.6966e-03,  3.5686e-03,  ...,  1.0309e-03,
          4.2191e-03,  3.6640e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9131, -2.6367, -2.5547,  ..., -5.2188,  1.4121,  1.6221]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 07:09:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place scorpion lives in is called nest
The place dolphin lives in is called sea
The place hippopotamus lives in is called river
The place wasp lives in is called nest
The place bear lives in is called den
The place seal lives in is called den
The place lion lives in is called den
The place woodchuck lives in is called
2024-07-17 07:09:51 root INFO     [order_1_approx] starting weight calculation for The place lion lives in is called den
The place woodchuck lives in is called hole
The place scorpion lives in is called nest
The place hippopotamus lives in is called river
The place wasp lives in is called nest
The place seal lives in is called den
The place bear lives in is called den
The place dolphin lives in is called
2024-07-17 07:09:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 07:13:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2959,  0.5420, -1.8115,  ..., -1.3057, -1.3936,  0.8179],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4043, -2.4023, -4.3594,  ..., -4.5078,  2.8535,  2.0156],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0109, -0.0118, -0.0043,  ..., -0.0044, -0.0060, -0.0005],
        [ 0.0073,  0.0142,  0.0033,  ...,  0.0074, -0.0068,  0.0044],
        [-0.0028,  0.0016, -0.0052,  ..., -0.0092, -0.0023,  0.0136],
        ...,
        [-0.0118,  0.0030, -0.0034,  ...,  0.0164, -0.0048,  0.0102],
        [ 0.0057,  0.0061,  0.0086,  ...,  0.0052,  0.0041, -0.0099],
        [-0.0079, -0.0031, -0.0072,  ...,  0.0023,  0.0111,  0.0087]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2903, -1.9629, -4.4414,  ..., -3.6543,  2.5684,  1.8955]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 07:13:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place lion lives in is called den
The place woodchuck lives in is called hole
The place scorpion lives in is called nest
The place hippopotamus lives in is called river
The place wasp lives in is called nest
The place seal lives in is called den
The place bear lives in is called den
The place dolphin lives in is called
2024-07-17 07:13:19 root INFO     [order_1_approx] starting weight calculation for The place seal lives in is called den
The place wasp lives in is called nest
The place bear lives in is called den
The place dolphin lives in is called sea
The place lion lives in is called den
The place woodchuck lives in is called hole
The place scorpion lives in is called nest
The place hippopotamus lives in is called
2024-07-17 07:13:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 07:16:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2314, -0.6768,  1.1045,  ...,  0.7417, -0.7285,  2.7578],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0840, -0.8086, -3.9414,  ..., -1.3691,  1.9727, -0.4399],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 7.8964e-03, -2.2240e-03,  7.8583e-04,  ...,  1.7176e-03,
         -7.9060e-04, -3.9673e-03],
        [-4.4060e-03,  2.4452e-03, -5.5313e-05,  ...,  3.1929e-03,
          5.5351e-03, -9.3174e-04],
        [-4.0359e-03,  7.2479e-04,  4.2076e-03,  ..., -5.2261e-03,
          1.1196e-03,  1.2131e-02],
        ...,
        [-4.5624e-03, -1.6680e-03, -5.1346e-03,  ...,  9.2239e-03,
          2.2640e-03,  8.7738e-03],
        [-3.3340e-03, -2.1706e-03,  2.0790e-04,  ..., -8.0204e-04,
          8.7433e-03,  4.2419e-03],
        [-4.4746e-03,  1.9054e-03,  3.2673e-03,  ..., -2.1553e-03,
          9.1629e-03,  3.5801e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2002, -0.6611, -4.3047,  ..., -1.1436,  2.3496, -0.9282]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 07:16:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place seal lives in is called den
The place wasp lives in is called nest
The place bear lives in is called den
The place dolphin lives in is called sea
The place lion lives in is called den
The place woodchuck lives in is called hole
The place scorpion lives in is called nest
The place hippopotamus lives in is called
2024-07-17 07:16:48 root INFO     [order_1_approx] starting weight calculation for The place lion lives in is called den
The place bear lives in is called den
The place seal lives in is called den
The place scorpion lives in is called nest
The place hippopotamus lives in is called river
The place dolphin lives in is called sea
The place woodchuck lives in is called hole
The place wasp lives in is called
2024-07-17 07:16:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 07:20:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2366,  1.2920, -1.4873,  ...,  0.8916,  0.5034,  1.2852],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0776, -3.6191,  1.4570,  ..., -2.9453,  6.2891,  2.0332],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.4967e-03, -1.0078e-02,  8.3923e-04,  ...,  5.0201e-03,
          5.1498e-05, -6.4011e-03],
        [ 4.0131e-03,  3.1128e-03,  8.5144e-03,  ...,  5.6648e-03,
          1.7853e-03, -8.9340e-03],
        [-3.7518e-03,  5.1880e-04, -9.7809e-03,  ..., -9.9716e-03,
          4.7913e-03,  1.5213e-02],
        ...,
        [ 1.4744e-03, -7.7667e-03, -9.6512e-04,  ...,  5.6953e-03,
          4.8447e-04, -8.2779e-04],
        [ 2.0409e-03, -1.5381e-02,  2.7809e-03,  ...,  3.0975e-03,
         -1.2169e-03, -5.6381e-03],
        [-1.2352e-02, -2.0981e-04,  7.8506e-03,  ..., -8.8978e-04,
          7.3853e-03,  6.4621e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2715, -4.3906,  1.8672,  ..., -2.2930,  6.0312,  2.0664]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 07:20:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place lion lives in is called den
The place bear lives in is called den
The place seal lives in is called den
The place scorpion lives in is called nest
The place hippopotamus lives in is called river
The place dolphin lives in is called sea
The place woodchuck lives in is called hole
The place wasp lives in is called
2024-07-17 07:20:17 root INFO     [order_1_approx] starting weight calculation for The place scorpion lives in is called nest
The place hippopotamus lives in is called river
The place dolphin lives in is called sea
The place seal lives in is called den
The place lion lives in is called den
The place woodchuck lives in is called hole
The place wasp lives in is called nest
The place bear lives in is called
2024-07-17 07:20:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 07:23:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6631,  0.2100, -1.1475,  ...,  0.6938, -0.5200,  0.9482],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2686, -1.8789, -4.6836,  ..., -5.8594,  1.0586,  5.3164],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0159, -0.0160,  0.0059,  ...,  0.0116, -0.0031, -0.0013],
        [-0.0002,  0.0129,  0.0107,  ...,  0.0150,  0.0239,  0.0021],
        [-0.0098, -0.0006, -0.0077,  ..., -0.0136,  0.0003,  0.0054],
        ...,
        [-0.0005, -0.0006, -0.0050,  ...,  0.0134, -0.0016,  0.0090],
        [-0.0031, -0.0064,  0.0021,  ..., -0.0014,  0.0141,  0.0036],
        [-0.0057, -0.0024,  0.0052,  ..., -0.0039,  0.0039,  0.0051]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1572, -1.5752, -4.8867,  ..., -5.7266,  0.7393,  5.3672]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 07:23:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place scorpion lives in is called nest
The place hippopotamus lives in is called river
The place dolphin lives in is called sea
The place seal lives in is called den
The place lion lives in is called den
The place woodchuck lives in is called hole
The place wasp lives in is called nest
The place bear lives in is called
2024-07-17 07:23:46 root INFO     total operator prediction time: 1669.9668517112732 seconds
2024-07-17 07:23:46 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on male - female
2024-07-17 07:23:46 root INFO     building operator male - female
2024-07-17 07:23:46 root INFO     [order_1_approx] starting weight calculation for A female man is known as a woman
A female groom is known as a bride
A female nephew is known as a niece
A female gentleman is known as a lady
A female rooster is known as a hen
A female batman is known as a batwoman
A female mister is known as a miss
A female boy is known as a
2024-07-17 07:23:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 07:27:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1289, -0.0701, -0.4854,  ..., -0.1713,  0.6958, -0.7715],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.1719, -2.8262, -5.9375,  ...,  0.6064,  3.0762, -1.8955],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0020, -0.0065, -0.0008,  ...,  0.0109, -0.0028, -0.0095],
        [-0.0049, -0.0078,  0.0009,  ..., -0.0040, -0.0014, -0.0044],
        [ 0.0127, -0.0050, -0.0015,  ..., -0.0068, -0.0072,  0.0086],
        ...,
        [-0.0075, -0.0037, -0.0011,  ..., -0.0003, -0.0107, -0.0030],
        [ 0.0100, -0.0035,  0.0017,  ..., -0.0002, -0.0007, -0.0016],
        [-0.0017, -0.0022,  0.0007,  ..., -0.0040,  0.0054, -0.0026]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.7949, -2.5078, -5.7344,  ..., -0.0225,  3.5508, -2.6777]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 07:27:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female man is known as a woman
A female groom is known as a bride
A female nephew is known as a niece
A female gentleman is known as a lady
A female rooster is known as a hen
A female batman is known as a batwoman
A female mister is known as a miss
A female boy is known as a
2024-07-17 07:27:15 root INFO     [order_1_approx] starting weight calculation for A female boy is known as a girl
A female nephew is known as a niece
A female rooster is known as a hen
A female man is known as a woman
A female groom is known as a bride
A female gentleman is known as a lady
A female batman is known as a batwoman
A female mister is known as a
2024-07-17 07:27:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 07:30:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.8496, -0.0198, -0.6572,  ...,  1.7510, -1.0586, -0.4521],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9043, -2.6348, -4.2656,  ..., -2.3047,  0.5508, -2.4238],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0079, -0.0084,  0.0043,  ..., -0.0071, -0.0037, -0.0310],
        [ 0.0101, -0.0218,  0.0119,  ..., -0.0100,  0.0094,  0.0099],
        [ 0.0032, -0.0157, -0.0120,  ...,  0.0078, -0.0089, -0.0176],
        ...,
        [-0.0106, -0.0111, -0.0077,  ..., -0.0040,  0.0033, -0.0135],
        [ 0.0053, -0.0103,  0.0131,  ...,  0.0017, -0.0159,  0.0151],
        [ 0.0016,  0.0015,  0.0007,  ..., -0.0024,  0.0127, -0.0168]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1543, -3.3555, -3.6973,  ..., -2.1875,  0.8481, -1.9336]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 07:30:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female boy is known as a girl
A female nephew is known as a niece
A female rooster is known as a hen
A female man is known as a woman
A female groom is known as a bride
A female gentleman is known as a lady
A female batman is known as a batwoman
A female mister is known as a
2024-07-17 07:30:43 root INFO     [order_1_approx] starting weight calculation for A female man is known as a woman
A female boy is known as a girl
A female batman is known as a batwoman
A female mister is known as a miss
A female groom is known as a bride
A female rooster is known as a hen
A female nephew is known as a niece
A female gentleman is known as a
2024-07-17 07:30:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 07:34:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0103, -0.1833,  0.0488,  ..., -0.2329, -0.4082, -0.2010],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8477, -2.2363, -1.7910,  ..., -1.3223,  0.9971, -3.4746],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0099,  0.0104,  0.0014,  ...,  0.0040,  0.0092, -0.0185],
        [ 0.0085, -0.0090, -0.0028,  ..., -0.0072,  0.0007,  0.0076],
        [ 0.0071, -0.0071, -0.0120,  ...,  0.0067, -0.0043,  0.0028],
        ...,
        [-0.0030,  0.0071,  0.0053,  ..., -0.0058, -0.0177,  0.0024],
        [ 0.0072, -0.0039,  0.0016,  ...,  0.0073, -0.0055,  0.0014],
        [ 0.0042, -0.0010,  0.0006,  ...,  0.0047,  0.0006,  0.0033]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9824, -2.5469, -1.7070,  ..., -1.3789,  1.3301, -3.1914]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 07:34:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female man is known as a woman
A female boy is known as a girl
A female batman is known as a batwoman
A female mister is known as a miss
A female groom is known as a bride
A female rooster is known as a hen
A female nephew is known as a niece
A female gentleman is known as a
2024-07-17 07:34:12 root INFO     [order_1_approx] starting weight calculation for A female groom is known as a bride
A female batman is known as a batwoman
A female mister is known as a miss
A female gentleman is known as a lady
A female nephew is known as a niece
A female boy is known as a girl
A female man is known as a woman
A female rooster is known as a
2024-07-17 07:34:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 07:37:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0479, -0.0167, -1.4541,  ...,  1.0801, -0.0518,  0.4114],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8770,  1.7070, -0.8550,  ...,  0.1807,  3.4453, -1.4492],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.8311e-04, -2.9259e-03, -7.9632e-05,  ...,  9.5558e-04,
          1.4925e-03, -4.0245e-03],
        [-2.7275e-03,  1.2875e-03,  1.8501e-03,  ...,  1.0109e-02,
          6.7482e-03,  4.7913e-03],
        [-5.7335e-03, -5.4169e-03, -7.5912e-03,  ..., -8.0719e-03,
          1.0391e-02,  2.8896e-03],
        ...,
        [-1.9608e-03, -5.5695e-04, -2.1553e-03,  ...,  4.5967e-03,
          4.1580e-03, -1.0128e-03],
        [-7.8869e-04, -8.5678e-03,  4.6539e-03,  ...,  2.5482e-03,
          6.0844e-03,  4.4098e-03],
        [-5.5733e-03, -8.8196e-03, -3.5739e-04,  ..., -2.1515e-03,
          1.2115e-02,  2.5864e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9570,  1.3193, -0.5127,  ...,  0.1791,  3.3359, -1.4365]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 07:37:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female groom is known as a bride
A female batman is known as a batwoman
A female mister is known as a miss
A female gentleman is known as a lady
A female nephew is known as a niece
A female boy is known as a girl
A female man is known as a woman
A female rooster is known as a
2024-07-17 07:37:40 root INFO     [order_1_approx] starting weight calculation for A female batman is known as a batwoman
A female mister is known as a miss
A female boy is known as a girl
A female gentleman is known as a lady
A female nephew is known as a niece
A female man is known as a woman
A female rooster is known as a hen
A female groom is known as a
2024-07-17 07:37:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 07:41:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1808,  0.1313,  0.3091,  ...,  0.9199, -2.1387, -0.0618],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1113, -1.7324, -2.4727,  ..., -2.4570, -2.3125,  0.9146],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0089, -0.0074, -0.0003,  ..., -0.0056, -0.0008, -0.0074],
        [-0.0099, -0.0054,  0.0114,  ...,  0.0070,  0.0040, -0.0010],
        [-0.0024, -0.0046, -0.0078,  ..., -0.0068,  0.0073, -0.0043],
        ...,
        [-0.0124, -0.0055, -0.0086,  ...,  0.0071, -0.0022,  0.0031],
        [ 0.0062, -0.0049,  0.0107,  ..., -0.0001,  0.0047,  0.0059],
        [-0.0063, -0.0123,  0.0106,  ...,  0.0028,  0.0008,  0.0058]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3125, -1.3984, -2.6699,  ..., -2.4531, -1.4473,  1.1943]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 07:41:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female batman is known as a batwoman
A female mister is known as a miss
A female boy is known as a girl
A female gentleman is known as a lady
A female nephew is known as a niece
A female man is known as a woman
A female rooster is known as a hen
A female groom is known as a
2024-07-17 07:41:09 root INFO     [order_1_approx] starting weight calculation for A female rooster is known as a hen
A female man is known as a woman
A female boy is known as a girl
A female groom is known as a bride
A female batman is known as a batwoman
A female mister is known as a miss
A female gentleman is known as a lady
A female nephew is known as a
2024-07-17 07:41:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 07:44:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.9453, -1.1865, -0.1062,  ...,  0.4053, -0.2319, -1.7539],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.5186, -3.3438, -1.7998,  ..., -1.3916, -0.2373, -0.6660],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0122,  0.0041, -0.0003,  ..., -0.0018, -0.0043, -0.0035],
        [ 0.0089, -0.0005,  0.0072,  ..., -0.0052,  0.0029,  0.0004],
        [ 0.0079, -0.0030, -0.0106,  ..., -0.0026,  0.0039,  0.0042],
        ...,
        [ 0.0029,  0.0011, -0.0025,  ..., -0.0095,  0.0087, -0.0156],
        [ 0.0043, -0.0003,  0.0006,  ..., -0.0039, -0.0197,  0.0030],
        [-0.0056,  0.0121,  0.0076,  ...,  0.0090,  0.0015, -0.0113]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4121, -3.6836, -1.7920,  ..., -1.5459,  0.0647, -1.2012]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 07:44:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female rooster is known as a hen
A female man is known as a woman
A female boy is known as a girl
A female groom is known as a bride
A female batman is known as a batwoman
A female mister is known as a miss
A female gentleman is known as a lady
A female nephew is known as a
2024-07-17 07:44:39 root INFO     [order_1_approx] starting weight calculation for A female nephew is known as a niece
A female mister is known as a miss
A female boy is known as a girl
A female groom is known as a bride
A female gentleman is known as a lady
A female rooster is known as a hen
A female batman is known as a batwoman
A female man is known as a
2024-07-17 07:44:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 07:48:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6846, -0.1512,  0.6606,  ..., -0.0928,  0.3760,  0.6602],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.4844, -2.2168, -4.4766,  ..., -5.3633,  2.8984, -0.9507],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0116, -0.0137, -0.0019,  ...,  0.0054,  0.0061, -0.0027],
        [ 0.0078, -0.0072,  0.0040,  ..., -0.0120,  0.0101,  0.0038],
        [-0.0051, -0.0139, -0.0136,  ...,  0.0132, -0.0192, -0.0179],
        ...,
        [-0.0109, -0.0011, -0.0082,  ...,  0.0063, -0.0180, -0.0044],
        [ 0.0081,  0.0013,  0.0029,  ...,  0.0033, -0.0020, -0.0029],
        [ 0.0104, -0.0017,  0.0069,  ..., -0.0022,  0.0099,  0.0160]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.2891, -1.9336, -2.7070,  ..., -4.1719,  3.0801, -2.2891]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 07:48:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female nephew is known as a niece
A female mister is known as a miss
A female boy is known as a girl
A female groom is known as a bride
A female gentleman is known as a lady
A female rooster is known as a hen
A female batman is known as a batwoman
A female man is known as a
2024-07-17 07:48:07 root INFO     [order_1_approx] starting weight calculation for A female mister is known as a miss
A female rooster is known as a hen
A female groom is known as a bride
A female gentleman is known as a lady
A female nephew is known as a niece
A female man is known as a woman
A female boy is known as a girl
A female batman is known as a
2024-07-17 07:48:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 07:51:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1348, -0.0134,  0.6665,  ..., -0.9297, -1.9570,  0.7402],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7373, -0.6450, -3.4004,  ..., -2.7441,  0.4131,  0.3621],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 8.7891e-03, -8.0490e-03,  2.4796e-05,  ..., -4.9629e-03,
         -4.6463e-03, -1.1276e-02],
        [ 3.0441e-03, -9.9182e-05,  2.5558e-03,  ..., -1.5545e-04,
          1.4925e-03, -5.8365e-03],
        [-6.8245e-03,  1.4639e-03, -6.7787e-03,  ...,  1.9455e-04,
         -1.1234e-03, -1.7414e-03],
        ...,
        [-9.7046e-03,  2.3422e-03,  1.7662e-03,  ...,  1.7738e-03,
         -2.5501e-03, -7.0534e-03],
        [-2.4223e-04,  4.3869e-03, -2.3460e-03,  ...,  8.8577e-03,
          5.6229e-03, -1.2913e-03],
        [-6.2256e-03,  4.9438e-03,  3.5858e-03,  ...,  1.1261e-02,
          1.0910e-02,  4.1046e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2744, -0.8389, -3.3477,  ..., -2.4688,  0.2617,  0.3081]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 07:51:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female mister is known as a miss
A female rooster is known as a hen
A female groom is known as a bride
A female gentleman is known as a lady
A female nephew is known as a niece
A female man is known as a woman
A female boy is known as a girl
A female batman is known as a
2024-07-17 07:51:36 root INFO     total operator prediction time: 1670.6871926784515 seconds
2024-07-17 07:51:36 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on name - occupation
2024-07-17 07:51:36 root INFO     building operator name - occupation
2024-07-17 07:51:36 root INFO     [order_1_approx] starting weight calculation for hegel was known for their work as a  philosopher
spinoza was known for their work as a  philosopher
wittgenstein was known for their work as a  philosopher
plato was known for their work as a  philosopher
wagner was known for their work as a  composer
descartes was known for their work as a  mathematician
balzac was known for their work as a  novelist
caesar was known for their work as a 
2024-07-17 07:51:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 07:55:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1582, -1.0059, -1.4795,  ...,  1.6367, -0.4624,  1.5371],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4490, -0.5791, -4.7227,  ..., -3.7168,  0.0247, -1.5234],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.1436e-02, -6.5231e-04, -3.6812e-03,  ..., -3.3951e-03,
          1.3290e-02, -8.6136e-03],
        [-1.6296e-02,  8.6288e-03,  3.5191e-03,  ...,  9.5978e-03,
         -4.9820e-03,  1.1444e-05],
        [-4.5166e-03,  8.8501e-04, -3.1815e-03,  ..., -1.8921e-02,
          9.4528e-03,  1.6113e-02],
        ...,
        [ 1.7891e-03,  4.8828e-03, -2.3861e-03,  ...,  1.1253e-03,
         -8.1940e-03,  5.5161e-03],
        [ 1.8244e-03, -4.8676e-03,  2.0103e-03,  ...,  8.7357e-04,
         -1.5202e-03,  5.4665e-03],
        [-1.1749e-02, -7.4539e-03,  2.0523e-03,  ..., -2.7962e-03,
          8.0185e-03, -1.0681e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1074, -0.1555, -4.4453,  ..., -4.2422,  0.2291, -1.1016]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 07:55:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for hegel was known for their work as a  philosopher
spinoza was known for their work as a  philosopher
wittgenstein was known for their work as a  philosopher
plato was known for their work as a  philosopher
wagner was known for their work as a  composer
descartes was known for their work as a  mathematician
balzac was known for their work as a  novelist
caesar was known for their work as a 
2024-07-17 07:55:04 root INFO     [order_1_approx] starting weight calculation for descartes was known for their work as a  mathematician
plato was known for their work as a  philosopher
caesar was known for their work as a  emperor
balzac was known for their work as a  novelist
spinoza was known for their work as a  philosopher
wittgenstein was known for their work as a  philosopher
hegel was known for their work as a  philosopher
wagner was known for their work as a 
2024-07-17 07:55:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 07:58:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6602,  0.6577,  0.3489,  ...,  0.4180, -0.7026, -0.3630],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.5391,  0.1465, -3.1992,  ..., -0.7656,  2.9023, -3.3867],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.7161e-03, -1.2894e-03, -4.7112e-04,  ..., -4.9896e-03,
          2.0081e-02, -7.3395e-03],
        [ 2.2736e-03,  1.5917e-03, -6.0310e-03,  ...,  1.2848e-02,
          1.8120e-05, -6.1340e-03],
        [ 3.6926e-03,  4.1389e-04, -6.3133e-03,  ...,  7.9346e-04,
          2.2449e-03, -5.0049e-03],
        ...,
        [-9.3994e-03,  3.6983e-03, -8.0919e-04,  ...,  7.3090e-03,
         -6.2485e-03, -1.7729e-03],
        [ 2.3270e-04, -8.8501e-03, -2.9678e-03,  ...,  9.2316e-03,
         -9.2926e-03, -6.1798e-03],
        [-2.4834e-03, -4.5776e-03,  6.2027e-03,  ..., -3.6373e-03,
          3.3875e-03,  1.8196e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.3828, -0.0968, -3.4473,  ..., -1.4160,  2.4805, -2.9785]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 07:58:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for descartes was known for their work as a  mathematician
plato was known for their work as a  philosopher
caesar was known for their work as a  emperor
balzac was known for their work as a  novelist
spinoza was known for their work as a  philosopher
wittgenstein was known for their work as a  philosopher
hegel was known for their work as a  philosopher
wagner was known for their work as a 
2024-07-17 07:58:32 root INFO     [order_1_approx] starting weight calculation for caesar was known for their work as a  emperor
plato was known for their work as a  philosopher
spinoza was known for their work as a  philosopher
wagner was known for their work as a  composer
descartes was known for their work as a  mathematician
hegel was known for their work as a  philosopher
balzac was known for their work as a  novelist
wittgenstein was known for their work as a 
2024-07-17 07:58:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 08:01:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8818, -0.5244,  0.2195,  ..., -0.6621,  0.1743,  1.2617],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1592, -2.4688, -3.6191,  ..., -0.4746,  1.6074, -2.7109],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0037,  0.0104,  0.0010,  ...,  0.0040,  0.0076, -0.0045],
        [ 0.0035,  0.0088,  0.0073,  ...,  0.0062,  0.0002, -0.0052],
        [ 0.0131,  0.0040,  0.0005,  ..., -0.0012,  0.0036, -0.0066],
        ...,
        [-0.0190, -0.0071,  0.0036,  ...,  0.0165, -0.0053, -0.0109],
        [-0.0055, -0.0088,  0.0053,  ...,  0.0097, -0.0070,  0.0034],
        [-0.0049,  0.0010,  0.0072,  ..., -0.0080,  0.0123,  0.0030]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1936, -2.0371, -3.3047,  ..., -1.3145,  1.5859, -2.8887]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 08:01:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for caesar was known for their work as a  emperor
plato was known for their work as a  philosopher
spinoza was known for their work as a  philosopher
wagner was known for their work as a  composer
descartes was known for their work as a  mathematician
hegel was known for their work as a  philosopher
balzac was known for their work as a  novelist
wittgenstein was known for their work as a 
2024-07-17 08:01:59 root INFO     [order_1_approx] starting weight calculation for wagner was known for their work as a  composer
balzac was known for their work as a  novelist
plato was known for their work as a  philosopher
descartes was known for their work as a  mathematician
wittgenstein was known for their work as a  philosopher
hegel was known for their work as a  philosopher
caesar was known for their work as a  emperor
spinoza was known for their work as a 
2024-07-17 08:01:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 08:05:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1938, -1.3447, -1.6885,  ..., -0.1095, -0.5234, -0.1929],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2520, -6.0078, -4.1484,  ..., -4.6523, -1.4434,  0.1859],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0113, -0.0113,  0.0043,  ..., -0.0026,  0.0149,  0.0092],
        [ 0.0065,  0.0157,  0.0002,  ...,  0.0093,  0.0062, -0.0050],
        [-0.0082,  0.0062,  0.0007,  ..., -0.0078, -0.0032, -0.0005],
        ...,
        [-0.0135, -0.0068, -0.0037,  ...,  0.0177, -0.0158,  0.0009],
        [-0.0086, -0.0128,  0.0076,  ..., -0.0029, -0.0039,  0.0269],
        [-0.0068,  0.0026, -0.0003,  ...,  0.0031,  0.0029,  0.0153]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9297, -6.0352, -5.0977,  ..., -3.9785, -0.6860, -0.7520]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 08:05:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for wagner was known for their work as a  composer
balzac was known for their work as a  novelist
plato was known for their work as a  philosopher
descartes was known for their work as a  mathematician
wittgenstein was known for their work as a  philosopher
hegel was known for their work as a  philosopher
caesar was known for their work as a  emperor
spinoza was known for their work as a 
2024-07-17 08:05:26 root INFO     [order_1_approx] starting weight calculation for plato was known for their work as a  philosopher
caesar was known for their work as a  emperor
wagner was known for their work as a  composer
spinoza was known for their work as a  philosopher
descartes was known for their work as a  mathematician
wittgenstein was known for their work as a  philosopher
balzac was known for their work as a  novelist
hegel was known for their work as a 
2024-07-17 08:05:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 08:08:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.6592,  0.0885, -1.0684,  ...,  1.2285,  0.0532,  1.4062],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.4629, -4.2422, -1.8223,  ..., -6.9062, -0.7412, -1.7051],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0134,  0.0067, -0.0066,  ...,  0.0030,  0.0167, -0.0103],
        [ 0.0152,  0.0098, -0.0056,  ...,  0.0116, -0.0037, -0.0035],
        [-0.0035, -0.0037, -0.0109,  ..., -0.0058,  0.0049,  0.0006],
        ...,
        [-0.0007,  0.0057, -0.0059,  ...,  0.0038, -0.0095,  0.0033],
        [ 0.0007, -0.0017, -0.0005,  ...,  0.0069, -0.0120,  0.0019],
        [-0.0098, -0.0029,  0.0014,  ..., -0.0008,  0.0005,  0.0142]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6514, -4.4727, -1.4287,  ..., -6.6016, -0.5039, -1.9824]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 08:08:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for plato was known for their work as a  philosopher
caesar was known for their work as a  emperor
wagner was known for their work as a  composer
spinoza was known for their work as a  philosopher
descartes was known for their work as a  mathematician
wittgenstein was known for their work as a  philosopher
balzac was known for their work as a  novelist
hegel was known for their work as a 
2024-07-17 08:08:52 root INFO     [order_1_approx] starting weight calculation for descartes was known for their work as a  mathematician
wagner was known for their work as a  composer
wittgenstein was known for their work as a  philosopher
plato was known for their work as a  philosopher
spinoza was known for their work as a  philosopher
caesar was known for their work as a  emperor
hegel was known for their work as a  philosopher
balzac was known for their work as a 
2024-07-17 08:08:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 08:12:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.6680,  1.2412, -1.2227,  ..., -0.6973, -0.4153,  2.0430],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8203, -1.5996, -1.8994,  ..., -0.7695, -3.2852, -2.4004],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0101,  0.0036, -0.0035,  ..., -0.0008,  0.0143, -0.0217],
        [ 0.0005,  0.0211,  0.0075,  ...,  0.0077, -0.0070, -0.0116],
        [-0.0007,  0.0007,  0.0020,  ..., -0.0024,  0.0039, -0.0043],
        ...,
        [-0.0063, -0.0040, -0.0029,  ...,  0.0203, -0.0165, -0.0078],
        [-0.0077, -0.0107,  0.0111,  ..., -0.0057,  0.0025,  0.0081],
        [-0.0084, -0.0110,  0.0078,  ..., -0.0073,  0.0153,  0.0184]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1348, -1.9131, -2.0566,  ..., -1.3926, -2.5781, -1.9609]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 08:12:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for descartes was known for their work as a  mathematician
wagner was known for their work as a  composer
wittgenstein was known for their work as a  philosopher
plato was known for their work as a  philosopher
spinoza was known for their work as a  philosopher
caesar was known for their work as a  emperor
hegel was known for their work as a  philosopher
balzac was known for their work as a 
2024-07-17 08:12:19 root INFO     [order_1_approx] starting weight calculation for hegel was known for their work as a  philosopher
caesar was known for their work as a  emperor
descartes was known for their work as a  mathematician
balzac was known for their work as a  novelist
wagner was known for their work as a  composer
spinoza was known for their work as a  philosopher
wittgenstein was known for their work as a  philosopher
plato was known for their work as a 
2024-07-17 08:12:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 08:15:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1511,  0.4272, -0.4531,  ...,  1.1641, -0.1929,  1.5254],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0635,  0.5068, -0.0186,  ..., -4.4727, -0.3953, -0.4663],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0109, -0.0040,  0.0017,  ...,  0.0008,  0.0145, -0.0058],
        [-0.0029,  0.0102,  0.0021,  ...,  0.0081, -0.0011, -0.0031],
        [-0.0035,  0.0012, -0.0088,  ..., -0.0055, -0.0006, -0.0047],
        ...,
        [-0.0078,  0.0034,  0.0064,  ..., -0.0036, -0.0106, -0.0035],
        [ 0.0163, -0.0072,  0.0056,  ...,  0.0069, -0.0020,  0.0010],
        [-0.0108, -0.0015, -0.0002,  ...,  0.0045, -0.0036,  0.0055]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.4014,  0.1768, -0.2013,  ..., -4.5391, -0.9219, -0.6143]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 08:15:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for hegel was known for their work as a  philosopher
caesar was known for their work as a  emperor
descartes was known for their work as a  mathematician
balzac was known for their work as a  novelist
wagner was known for their work as a  composer
spinoza was known for their work as a  philosopher
wittgenstein was known for their work as a  philosopher
plato was known for their work as a 
2024-07-17 08:15:46 root INFO     [order_1_approx] starting weight calculation for wagner was known for their work as a  composer
balzac was known for their work as a  novelist
hegel was known for their work as a  philosopher
spinoza was known for their work as a  philosopher
caesar was known for their work as a  emperor
plato was known for their work as a  philosopher
wittgenstein was known for their work as a  philosopher
descartes was known for their work as a 
2024-07-17 08:15:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 08:19:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6494, -0.4229, -0.1641,  ...,  1.3643, -1.7148,  0.3425],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.9707, -3.2441, -1.8896,  ..., -3.4629, -0.9751, -0.9961],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0033,  0.0064, -0.0023,  ...,  0.0056,  0.0092,  0.0054],
        [-0.0022,  0.0099,  0.0110,  ...,  0.0078,  0.0062, -0.0041],
        [ 0.0026,  0.0032, -0.0037,  ..., -0.0005, -0.0016,  0.0067],
        ...,
        [-0.0044,  0.0009, -0.0010,  ...,  0.0014, -0.0073,  0.0148],
        [-0.0048, -0.0034,  0.0103,  ..., -0.0042, -0.0069,  0.0067],
        [-0.0020,  0.0017, -0.0037,  ...,  0.0049, -0.0014,  0.0132]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.3438, -3.2559, -2.2578,  ..., -3.0645, -0.7393, -0.9829]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 08:19:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for wagner was known for their work as a  composer
balzac was known for their work as a  novelist
hegel was known for their work as a  philosopher
spinoza was known for their work as a  philosopher
caesar was known for their work as a  emperor
plato was known for their work as a  philosopher
wittgenstein was known for their work as a  philosopher
descartes was known for their work as a 
2024-07-17 08:19:12 root INFO     total operator prediction time: 1656.06147813797 seconds
2024-07-17 08:19:12 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on country - capital
2024-07-17 08:19:12 root INFO     building operator country - capital
2024-07-17 08:19:13 root INFO     [order_1_approx] starting weight calculation for The country with damascus as its capital is known as syria
The country with manila as its capital is known as philippines
The country with oslo as its capital is known as norway
The country with moscow as its capital is known as russia
The country with islamabad as its capital is known as pakistan
The country with tbilisi as its capital is known as georgia
The country with rome as its capital is known as italy
The country with beijing as its capital is known as
2024-07-17 08:19:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 08:22:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.3857, -0.7827,  0.8740,  ..., -1.5391, -0.3242,  0.6064],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.0977, -3.5762,  0.0361,  ..., -5.7969,  0.3977, -2.7773],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.2215e-02,  5.0964e-03, -4.7684e-07,  ...,  6.8665e-05,
          4.4670e-03, -3.7003e-03],
        [ 5.0926e-03, -9.0790e-03,  1.3695e-03,  ...,  1.1452e-02,
          4.9438e-03, -7.5684e-03],
        [ 2.1164e-02, -8.5449e-03, -1.9104e-02,  ..., -3.9749e-03,
          2.5616e-03, -2.8305e-03],
        ...,
        [-1.1871e-02,  6.9733e-03, -1.7395e-02,  ..., -5.2986e-03,
          7.6294e-06,  1.6174e-02],
        [-1.9436e-03,  5.4626e-03, -7.8278e-03,  ..., -1.0040e-02,
         -1.5427e-02,  1.3649e-02],
        [-9.0790e-03,  2.3842e-03, -3.9062e-03,  ..., -4.1122e-03,
          1.4488e-02, -8.6670e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6836, -2.8320,  1.1338,  ..., -6.1328,  0.7969, -3.1172]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 08:22:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with damascus as its capital is known as syria
The country with manila as its capital is known as philippines
The country with oslo as its capital is known as norway
The country with moscow as its capital is known as russia
The country with islamabad as its capital is known as pakistan
The country with tbilisi as its capital is known as georgia
The country with rome as its capital is known as italy
The country with beijing as its capital is known as
2024-07-17 08:22:36 root INFO     [order_1_approx] starting weight calculation for The country with beijing as its capital is known as china
The country with damascus as its capital is known as syria
The country with manila as its capital is known as philippines
The country with rome as its capital is known as italy
The country with oslo as its capital is known as norway
The country with islamabad as its capital is known as pakistan
The country with moscow as its capital is known as russia
The country with tbilisi as its capital is known as
2024-07-17 08:22:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 08:26:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1875, -0.2993,  0.6748,  ...,  0.9653,  0.1616,  0.8242],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6328,  1.5117,  7.0195,  ...,  1.4678, -1.6611, -2.7656],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0052, -0.0108,  0.0055,  ..., -0.0008,  0.0074, -0.0132],
        [ 0.0024, -0.0133,  0.0112,  ...,  0.0126,  0.0081, -0.0016],
        [-0.0006, -0.0010, -0.0085,  ..., -0.0083,  0.0093, -0.0099],
        ...,
        [-0.0103,  0.0021,  0.0082,  ..., -0.0034, -0.0024,  0.0075],
        [-0.0113,  0.0026, -0.0101,  ..., -0.0126, -0.0316,  0.0128],
        [ 0.0015,  0.0037, -0.0023,  ..., -0.0018,  0.0047, -0.0045]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0898,  2.2188,  7.2344,  ...,  2.0254, -2.8027, -4.0195]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 08:26:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with beijing as its capital is known as china
The country with damascus as its capital is known as syria
The country with manila as its capital is known as philippines
The country with rome as its capital is known as italy
The country with oslo as its capital is known as norway
The country with islamabad as its capital is known as pakistan
The country with moscow as its capital is known as russia
The country with tbilisi as its capital is known as
2024-07-17 08:26:04 root INFO     [order_1_approx] starting weight calculation for The country with manila as its capital is known as philippines
The country with moscow as its capital is known as russia
The country with tbilisi as its capital is known as georgia
The country with beijing as its capital is known as china
The country with oslo as its capital is known as norway
The country with rome as its capital is known as italy
The country with islamabad as its capital is known as pakistan
The country with damascus as its capital is known as
2024-07-17 08:26:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 08:29:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2197, -1.0547, -0.6768,  ..., -0.3701, -0.9609,  1.0957],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8203,  0.3691,  1.0605,  ..., -5.9766,  0.7739, -2.9238],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0030, -0.0053,  0.0062,  ...,  0.0004,  0.0057, -0.0077],
        [ 0.0035, -0.0042,  0.0054,  ...,  0.0060,  0.0040, -0.0038],
        [-0.0018, -0.0042, -0.0050,  ..., -0.0027, -0.0015,  0.0060],
        ...,
        [-0.0058,  0.0018, -0.0005,  ...,  0.0066, -0.0115,  0.0061],
        [-0.0107, -0.0056, -0.0084,  ..., -0.0065, -0.0048,  0.0125],
        [-0.0069, -0.0004,  0.0069,  ...,  0.0028, -0.0088,  0.0051]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8442,  0.5010,  1.0361,  ..., -5.6953,  0.5498, -3.3242]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 08:29:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with manila as its capital is known as philippines
The country with moscow as its capital is known as russia
The country with tbilisi as its capital is known as georgia
The country with beijing as its capital is known as china
The country with oslo as its capital is known as norway
The country with rome as its capital is known as italy
The country with islamabad as its capital is known as pakistan
The country with damascus as its capital is known as
2024-07-17 08:29:31 root INFO     [order_1_approx] starting weight calculation for The country with islamabad as its capital is known as pakistan
The country with manila as its capital is known as philippines
The country with oslo as its capital is known as norway
The country with damascus as its capital is known as syria
The country with moscow as its capital is known as russia
The country with tbilisi as its capital is known as georgia
The country with beijing as its capital is known as china
The country with rome as its capital is known as
2024-07-17 08:29:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 08:32:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0400, -0.5459,  0.3311,  ...,  1.7227,  0.2294, -0.0865],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3555,  1.5811, -0.1055,  ..., -1.7324, -1.7598, -3.7227],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-4.6253e-05, -6.2180e-04, -5.5008e-03,  ...,  4.3564e-03,
          3.0556e-03, -9.9487e-03],
        [-1.3332e-03, -6.1836e-03,  6.1417e-04,  ...,  1.7822e-02,
         -3.7575e-03, -6.4850e-03],
        [-1.6117e-03, -1.0452e-03, -1.8860e-02,  ..., -9.4299e-03,
          9.7046e-03, -4.2953e-03],
        ...,
        [-3.5362e-03,  1.6499e-03,  1.0406e-02,  ...,  7.0038e-03,
         -1.2894e-02, -9.0790e-03],
        [-6.6071e-03, -3.9978e-03, -5.2948e-03,  ..., -1.5545e-03,
         -5.5313e-03,  2.4166e-03],
        [-2.1423e-02,  4.2801e-03, -1.6632e-03,  ...,  7.1955e-04,
          6.7863e-03, -4.2267e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.2871,  2.6445, -0.1630,  ..., -1.6846, -2.5898, -4.4805]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 08:32:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with islamabad as its capital is known as pakistan
The country with manila as its capital is known as philippines
The country with oslo as its capital is known as norway
The country with damascus as its capital is known as syria
The country with moscow as its capital is known as russia
The country with tbilisi as its capital is known as georgia
The country with beijing as its capital is known as china
The country with rome as its capital is known as
2024-07-17 08:32:58 root INFO     [order_1_approx] starting weight calculation for The country with tbilisi as its capital is known as georgia
The country with beijing as its capital is known as china
The country with manila as its capital is known as philippines
The country with rome as its capital is known as italy
The country with damascus as its capital is known as syria
The country with oslo as its capital is known as norway
The country with islamabad as its capital is known as pakistan
The country with moscow as its capital is known as
2024-07-17 08:32:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 08:36:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9771, -0.7251,  0.9639,  ..., -0.5376, -0.0996,  2.9336],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2441, -2.9531,  1.7236,  ..., -1.1621,  0.0857,  1.5664],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0108, -0.0097,  0.0039,  ..., -0.0015,  0.0060, -0.0055],
        [ 0.0066, -0.0046,  0.0021,  ...,  0.0067,  0.0094, -0.0107],
        [-0.0058,  0.0081, -0.0065,  ..., -0.0082,  0.0014, -0.0079],
        ...,
        [ 0.0005,  0.0082,  0.0018,  ...,  0.0099, -0.0052,  0.0062],
        [ 0.0046, -0.0048, -0.0078,  ...,  0.0009, -0.0244,  0.0218],
        [-0.0046, -0.0059,  0.0050,  ...,  0.0063,  0.0015, -0.0091]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1102, -2.0762,  1.7168,  ..., -0.1123, -0.7773,  2.4316]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 08:36:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with tbilisi as its capital is known as georgia
The country with beijing as its capital is known as china
The country with manila as its capital is known as philippines
The country with rome as its capital is known as italy
The country with damascus as its capital is known as syria
The country with oslo as its capital is known as norway
The country with islamabad as its capital is known as pakistan
The country with moscow as its capital is known as
2024-07-17 08:36:26 root INFO     [order_1_approx] starting weight calculation for The country with moscow as its capital is known as russia
The country with damascus as its capital is known as syria
The country with manila as its capital is known as philippines
The country with islamabad as its capital is known as pakistan
The country with tbilisi as its capital is known as georgia
The country with rome as its capital is known as italy
The country with beijing as its capital is known as china
The country with oslo as its capital is known as
2024-07-17 08:36:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 08:39:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9780,  1.4102,  1.5303,  ...,  0.3665, -0.5781,  0.2927],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.1328,  0.3057,  0.9360,  ..., -6.0625, -3.6875, -0.8501],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0040, -0.0011,  0.0089,  ...,  0.0103,  0.0009, -0.0029],
        [ 0.0036,  0.0014, -0.0094,  ...,  0.0121,  0.0090, -0.0125],
        [ 0.0076,  0.0092, -0.0050,  ...,  0.0063,  0.0063, -0.0003],
        ...,
        [-0.0027,  0.0020, -0.0068,  ..., -0.0055,  0.0036, -0.0045],
        [-0.0017, -0.0095, -0.0056,  ...,  0.0035, -0.0165,  0.0167],
        [-0.0050,  0.0086, -0.0024,  ...,  0.0034, -0.0001, -0.0124]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.8965,  0.2620,  1.3320,  ..., -5.9219, -3.1445, -1.3447]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 08:39:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with moscow as its capital is known as russia
The country with damascus as its capital is known as syria
The country with manila as its capital is known as philippines
The country with islamabad as its capital is known as pakistan
The country with tbilisi as its capital is known as georgia
The country with rome as its capital is known as italy
The country with beijing as its capital is known as china
The country with oslo as its capital is known as
2024-07-17 08:39:53 root INFO     [order_1_approx] starting weight calculation for The country with rome as its capital is known as italy
The country with oslo as its capital is known as norway
The country with tbilisi as its capital is known as georgia
The country with islamabad as its capital is known as pakistan
The country with moscow as its capital is known as russia
The country with damascus as its capital is known as syria
The country with beijing as its capital is known as china
The country with manila as its capital is known as
2024-07-17 08:39:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 08:43:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6343, -0.2336,  0.8560,  ...,  1.3496,  0.3118,  0.6294],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1963, -2.7754, -1.2236,  ..., -5.7266,  0.6079, -0.9131],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0001, -0.0038,  0.0025,  ..., -0.0009,  0.0075, -0.0035],
        [ 0.0074, -0.0022, -0.0025,  ...,  0.0101,  0.0017, -0.0026],
        [ 0.0003,  0.0015,  0.0007,  ..., -0.0001, -0.0025, -0.0035],
        ...,
        [ 0.0013, -0.0017, -0.0006,  ...,  0.0022, -0.0042,  0.0032],
        [-0.0044, -0.0048, -0.0058,  ..., -0.0007, -0.0030,  0.0042],
        [-0.0042, -0.0009,  0.0019,  ..., -0.0040,  0.0070, -0.0008]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.4883, -2.3906, -0.8857,  ..., -5.2188,  0.3838, -1.0938]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 08:43:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with rome as its capital is known as italy
The country with oslo as its capital is known as norway
The country with tbilisi as its capital is known as georgia
The country with islamabad as its capital is known as pakistan
The country with moscow as its capital is known as russia
The country with damascus as its capital is known as syria
The country with beijing as its capital is known as china
The country with manila as its capital is known as
2024-07-17 08:43:21 root INFO     [order_1_approx] starting weight calculation for The country with moscow as its capital is known as russia
The country with rome as its capital is known as italy
The country with manila as its capital is known as philippines
The country with oslo as its capital is known as norway
The country with tbilisi as its capital is known as georgia
The country with damascus as its capital is known as syria
The country with beijing as its capital is known as china
The country with islamabad as its capital is known as
2024-07-17 08:43:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.9
2024-07-17 08:46:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3591, -1.7910,  0.6284,  ...,  1.0254, -0.5420,  0.8735],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.2041, -3.6875,  2.2734,  ..., -0.8750,  1.7090,  0.0737],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 7.3929e-03,  1.3018e-03,  5.3978e-04,  ...,  2.0676e-03,
          8.4915e-03, -5.1270e-03],
        [-1.3094e-03, -9.4681e-03, -1.2522e-03,  ...,  1.2236e-03,
          6.3400e-03, -1.8291e-03],
        [-7.2823e-03, -1.4791e-03, -4.3106e-03,  ..., -5.7487e-03,
         -1.1005e-03,  3.9482e-03],
        ...,
        [-6.9809e-03,  2.7351e-03,  2.8763e-03,  ...,  4.5662e-03,
         -1.3123e-02, -4.9744e-03],
        [-6.6528e-03, -6.0387e-03, -2.7409e-03,  ..., -2.7523e-03,
         -6.4850e-05,  7.4005e-03],
        [-4.3488e-04, -1.3046e-03, -8.3542e-04,  ...,  3.1471e-03,
         -2.9545e-03, -1.2550e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4062, -3.3164,  2.1074,  ..., -0.8281,  1.5117, -0.6133]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 08:46:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with moscow as its capital is known as russia
The country with rome as its capital is known as italy
The country with manila as its capital is known as philippines
The country with oslo as its capital is known as norway
The country with tbilisi as its capital is known as georgia
The country with damascus as its capital is known as syria
The country with beijing as its capital is known as china
The country with islamabad as its capital is known as
2024-07-17 08:46:49 root INFO     total operator prediction time: 1656.7704479694366 seconds
