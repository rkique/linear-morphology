2024-07-22 17:43:26 root INFO     loading model + tokenizer
2024-07-22 17:43:44 root INFO     model + tokenizer loaded
2024-07-22 17:43:44 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - part
2024-07-22 17:43:44 root INFO     building operator meronyms - part
2024-07-22 17:43:44 root INFO     total operator prediction time: 0.06370401382446289 seconds
2024-07-22 17:43:44 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on synonyms - exact
2024-07-22 17:43:44 root INFO     building operator synonyms - exact
2024-07-22 17:43:45 root INFO     [order_1_approx] starting weight calculation for Another word for incorrect is wrong
Another word for rock is stone
Another word for auto is car
Another word for bicycle is bike
Another word for sofa is couch
Another word for jewel is gem
Another word for mend is repair
Another word for airplane is
2024-07-22 17:43:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 17:48:55 root INFO     loading model + tokenizer
2024-07-22 17:49:13 root INFO     model + tokenizer loaded
2024-07-22 17:49:13 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - part
2024-07-22 17:49:13 root INFO     building operator meronyms - part
2024-07-22 17:49:13 root INFO     total operator prediction time: 0.08361482620239258 seconds
2024-07-22 17:49:13 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on synonyms - exact
2024-07-22 17:49:13 root INFO     building operator synonyms - exact
2024-07-22 17:50:07 root INFO     loading model + tokenizer
2024-07-22 17:50:25 root INFO     model + tokenizer loaded
2024-07-22 17:50:25 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - part
2024-07-22 17:50:25 root INFO     building operator meronyms - part
2024-07-22 17:50:25 root INFO     total operator prediction time: 0.0001277923583984375 seconds
2024-07-22 17:50:25 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on synonyms - exact
2024-07-22 17:50:25 root INFO     building operator synonyms - exact
2024-07-22 17:50:25 root INFO     [order_1_approx] starting weight calculation for Another word for jewel is gem
Another word for sofa is couch
Another word for mend is repair
Another word for rock is stone
Another word for auto is car
Another word for bicycle is bike
Another word for incorrect is wrong
Another word for airplane is
2024-07-22 17:50:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-22 17:51:43 root INFO     loading model + tokenizer
2024-07-22 17:51:56 root INFO     loading model + tokenizer
2024-07-22 17:52:13 root INFO     model + tokenizer loaded
2024-07-22 17:52:13 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - part
2024-07-22 17:52:13 root INFO     building operator meronyms - part
2024-07-22 17:52:14 root INFO     [order_1_approx] starting weight calculation for A part of a torso is a chest
A part of a womb is a cervix
A part of a filename is a extension
A part of a brush is a bristle
A part of a deer is a antler
A part of a shilling is a pence
A part of a academia is a college
A part of a tonne is a
2024-07-22 17:52:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 17:56:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6021, -0.3782,  0.9932,  ..., -0.2454,  0.3584,  0.0337],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.5312, -3.3320, -4.4492,  ..., -1.3721, -4.1602,  2.0469],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0038,  0.0014, -0.0016,  ...,  0.0182, -0.0042, -0.0016],
        [ 0.0002, -0.0012,  0.0152,  ..., -0.0159, -0.0106,  0.0011],
        [-0.0196, -0.0018, -0.0047,  ..., -0.0174,  0.0031, -0.0020],
        ...,
        [-0.0065,  0.0006,  0.0014,  ...,  0.0003, -0.0116,  0.0238],
        [-0.0064,  0.0105,  0.0081,  ..., -0.0059, -0.0016, -0.0012],
        [-0.0055, -0.0173,  0.0129,  ..., -0.0136,  0.0054,  0.0037]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.1367, -3.2090, -3.4375,  ..., -1.4922, -3.7031,  1.7559]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 17:56:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a torso is a chest
A part of a womb is a cervix
A part of a filename is a extension
A part of a brush is a bristle
A part of a deer is a antler
A part of a shilling is a pence
A part of a academia is a college
A part of a tonne is a
2024-07-22 17:56:12 root INFO     [order_1_approx] starting weight calculation for A part of a tonne is a kilogram
A part of a shilling is a pence
A part of a deer is a antler
A part of a brush is a bristle
A part of a filename is a extension
A part of a torso is a chest
A part of a academia is a college
A part of a womb is a
2024-07-22 17:56:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 18:00:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1611, -0.1064, -0.8657,  ...,  0.6562, -0.3005,  0.5566],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5586,  0.2074, -4.5859,  ...,  0.5020,  1.4521,  0.3330],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.4328e-02,  6.9427e-03, -7.6294e-06,  ..., -5.5466e-03,
          5.0354e-03, -5.8670e-03],
        [ 4.1466e-03,  3.5477e-04, -8.6594e-04,  ...,  7.2784e-03,
          1.9569e-03,  2.5482e-03],
        [ 1.3638e-03, -1.0834e-03, -9.1553e-04,  ..., -1.7365e-02,
          6.3782e-03,  1.6785e-02],
        ...,
        [-8.8120e-03,  9.6817e-03,  1.5274e-02,  ...,  8.8120e-04,
         -8.4763e-03, -4.0550e-03],
        [ 4.1122e-03,  9.9258e-03,  5.5428e-03,  ..., -5.1498e-04,
          3.1967e-03, -9.2926e-03],
        [ 4.4823e-04, -1.1047e-02, -4.7951e-03,  ..., -2.3346e-02,
          1.8097e-02,  1.1871e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0117,  1.3730, -3.8984,  ..., -0.1196,  1.4238,  0.6919]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 18:00:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a tonne is a kilogram
A part of a shilling is a pence
A part of a deer is a antler
A part of a brush is a bristle
A part of a filename is a extension
A part of a torso is a chest
A part of a academia is a college
A part of a womb is a
2024-07-22 18:00:13 root INFO     [order_1_approx] starting weight calculation for A part of a deer is a antler
A part of a academia is a college
A part of a womb is a cervix
A part of a shilling is a pence
A part of a tonne is a kilogram
A part of a torso is a chest
A part of a filename is a extension
A part of a brush is a
2024-07-22 18:00:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 18:04:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8398, -0.5591,  0.8652,  ...,  0.9707, -0.7695,  0.3459],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9297, -0.0715, -6.3047,  ..., -1.5547,  0.6963,  2.5391],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0036,  0.0153, -0.0045,  ...,  0.0058, -0.0010,  0.0032],
        [ 0.0030, -0.0094,  0.0242,  ...,  0.0192, -0.0282,  0.0115],
        [ 0.0180,  0.0038,  0.0126,  ...,  0.0124,  0.0025,  0.0162],
        ...,
        [-0.0015, -0.0291, -0.0099,  ...,  0.0237, -0.0120,  0.0122],
        [ 0.0058, -0.0230, -0.0075,  ..., -0.0149,  0.0042, -0.0005],
        [-0.0086,  0.0163, -0.0212,  ..., -0.0203,  0.0273,  0.0241]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3281,  1.4424, -6.6172,  ..., -1.0801,  0.9570,  2.0801]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 18:04:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a deer is a antler
A part of a academia is a college
A part of a womb is a cervix
A part of a shilling is a pence
A part of a tonne is a kilogram
A part of a torso is a chest
A part of a filename is a extension
A part of a brush is a
2024-07-22 18:04:12 root INFO     [order_1_approx] starting weight calculation for A part of a womb is a cervix
A part of a filename is a extension
A part of a deer is a antler
A part of a academia is a college
A part of a shilling is a pence
A part of a tonne is a kilogram
A part of a brush is a bristle
A part of a torso is a
2024-07-22 18:04:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 18:08:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0723,  0.6816,  0.2605,  ...,  1.4717, -0.3401,  0.4419],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.8789,  1.0459, -0.3838,  ...,  1.8643,  0.4814, -1.9414],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0029, -0.0068,  0.0019,  ..., -0.0014,  0.0047, -0.0164],
        [-0.0030,  0.0017,  0.0081,  ..., -0.0052,  0.0024, -0.0137],
        [ 0.0103,  0.0176,  0.0231,  ...,  0.0108,  0.0003, -0.0026],
        ...,
        [-0.0090,  0.0096, -0.0139,  ...,  0.0121, -0.0073,  0.0021],
        [ 0.0007, -0.0043, -0.0117,  ..., -0.0089, -0.0018, -0.0009],
        [-0.0162,  0.0144, -0.0068,  ..., -0.0064,  0.0037,  0.0239]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7910,  1.5596,  0.1724,  ...,  1.9746,  0.7173, -1.1289]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 18:08:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a womb is a cervix
A part of a filename is a extension
A part of a deer is a antler
A part of a academia is a college
A part of a shilling is a pence
A part of a tonne is a kilogram
A part of a brush is a bristle
A part of a torso is a
2024-07-22 18:08:11 root INFO     [order_1_approx] starting weight calculation for A part of a shilling is a pence
A part of a womb is a cervix
A part of a torso is a chest
A part of a academia is a college
A part of a brush is a bristle
A part of a filename is a extension
A part of a tonne is a kilogram
A part of a deer is a
2024-07-22 18:08:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 18:12:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9521, -0.4453, -0.3423,  ...,  1.0107,  0.1925,  0.2712],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.2773,  1.4395, -6.7461,  ..., -2.3516, -1.5352,  1.3525],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0104, -0.0265,  0.0085,  ...,  0.0039,  0.0106, -0.0208],
        [ 0.0005,  0.0138,  0.0078,  ...,  0.0192, -0.0040, -0.0204],
        [-0.0131,  0.0015,  0.0165,  ..., -0.0145,  0.0031,  0.0249],
        ...,
        [-0.0056,  0.0028, -0.0168,  ..., -0.0011, -0.0093,  0.0257],
        [ 0.0240, -0.0011, -0.0116,  ...,  0.0302, -0.0084,  0.0279],
        [-0.0013, -0.0040, -0.0161,  ..., -0.0033,  0.0331,  0.0295]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1660,  1.9424, -6.3086,  ..., -3.2246, -1.3545, -0.1846]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 18:12:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a shilling is a pence
A part of a womb is a cervix
A part of a torso is a chest
A part of a academia is a college
A part of a brush is a bristle
A part of a filename is a extension
A part of a tonne is a kilogram
A part of a deer is a
2024-07-22 18:12:10 root INFO     [order_1_approx] starting weight calculation for A part of a torso is a chest
A part of a filename is a extension
A part of a tonne is a kilogram
A part of a womb is a cervix
A part of a deer is a antler
A part of a academia is a college
A part of a brush is a bristle
A part of a shilling is a
2024-07-22 18:12:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 18:16:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4329, -1.2598, -0.2273,  ...,  0.8325, -0.2112,  1.4766],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.6367, -0.0615, -5.0820,  ..., -0.7451, -2.7676, -0.9453],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0272, -0.0460,  0.0033,  ..., -0.0082, -0.0063, -0.0121],
        [ 0.0101,  0.0131, -0.0019,  ...,  0.0023,  0.0131, -0.0126],
        [-0.0099, -0.0062, -0.0256,  ...,  0.0048,  0.0231,  0.0033],
        ...,
        [-0.0248, -0.0087, -0.0100,  ..., -0.0036,  0.0188, -0.0124],
        [-0.0092, -0.0376,  0.0210,  ...,  0.0069,  0.0014, -0.0117],
        [-0.0042,  0.0319,  0.0143,  ..., -0.0335, -0.0037, -0.0242]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.9492,  0.0950, -5.1328,  ...,  0.3779, -2.4785, -0.3926]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 18:16:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a torso is a chest
A part of a filename is a extension
A part of a tonne is a kilogram
A part of a womb is a cervix
A part of a deer is a antler
A part of a academia is a college
A part of a brush is a bristle
A part of a shilling is a
2024-07-22 18:16:09 root INFO     [order_1_approx] starting weight calculation for A part of a tonne is a kilogram
A part of a shilling is a pence
A part of a academia is a college
A part of a deer is a antler
A part of a womb is a cervix
A part of a brush is a bristle
A part of a torso is a chest
A part of a filename is a
2024-07-22 18:16:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 18:20:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2213,  0.1814,  0.4165,  ...,  0.1343,  0.6357,  0.5420],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4629,  2.6094, -0.6377,  ..., -1.2852, -3.2773,  0.4863],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.0040e-02, -8.1253e-04, -2.0599e-03,  ...,  5.9280e-03,
         -1.6800e-02, -8.6517e-03],
        [ 1.2924e-02, -1.3405e-02,  9.0637e-03,  ...,  1.6983e-02,
         -4.6005e-03, -4.5776e-05],
        [ 8.4152e-03,  1.0345e-02, -7.9498e-03,  ...,  4.4556e-03,
         -9.7198e-03,  2.7771e-03],
        ...,
        [-1.1742e-02, -1.6510e-02, -2.3346e-03,  ..., -1.9302e-03,
         -1.0651e-02, -2.7893e-02],
        [ 9.7656e-03, -1.6464e-02, -3.6678e-03,  ..., -6.9351e-03,
         -4.9400e-03, -3.2806e-04],
        [ 1.5137e-02,  3.7720e-02, -8.9264e-03,  ..., -2.7103e-03,
          5.9204e-03,  2.2888e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1338,  2.7480, -1.6094,  ..., -0.8096, -3.5996,  0.7266]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 18:20:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a tonne is a kilogram
A part of a shilling is a pence
A part of a academia is a college
A part of a deer is a antler
A part of a womb is a cervix
A part of a brush is a bristle
A part of a torso is a chest
A part of a filename is a
2024-07-22 18:20:10 root INFO     [order_1_approx] starting weight calculation for A part of a filename is a extension
A part of a womb is a cervix
A part of a deer is a antler
A part of a tonne is a kilogram
A part of a brush is a bristle
A part of a torso is a chest
A part of a shilling is a pence
A part of a academia is a
2024-07-22 18:20:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 18:24:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6699,  1.4658,  0.5732,  ..., -0.5127,  0.5205,  1.1719],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7622,  2.3496, -0.4629,  ...,  1.0938,  0.0698,  1.2451],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0101, -0.0172, -0.0087,  ..., -0.0086, -0.0041, -0.0122],
        [ 0.0151,  0.0013,  0.0066,  ..., -0.0027, -0.0131, -0.0254],
        [ 0.0069, -0.0031, -0.0046,  ..., -0.0044, -0.0084, -0.0033],
        ...,
        [-0.0074, -0.0111,  0.0037,  ..., -0.0052,  0.0147,  0.0119],
        [ 0.0204, -0.0032,  0.0081,  ..., -0.0050, -0.0071,  0.0150],
        [-0.0090,  0.0088,  0.0015,  ..., -0.0039,  0.0079,  0.0045]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.8740,  3.3633, -0.2710,  ...,  0.1353, -0.6597,  1.9961]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 18:24:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a filename is a extension
A part of a womb is a cervix
A part of a deer is a antler
A part of a tonne is a kilogram
A part of a brush is a bristle
A part of a torso is a chest
A part of a shilling is a pence
A part of a academia is a
2024-07-22 18:24:10 root INFO     total operator prediction time: 1917.208237171173 seconds
2024-07-22 18:24:10 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on synonyms - exact
2024-07-22 18:24:10 root INFO     building operator synonyms - exact
2024-07-22 18:24:10 root INFO     [order_1_approx] starting weight calculation for Another word for market is marketplace
Another word for help is aid
Another word for harbor is seaport
Another word for incorrect is wrong
Another word for spouse is partner
Another word for monument is memorial
Another word for style is manner
Another word for hieroglyph is
2024-07-22 18:24:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 18:28:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9185, -0.5879,  1.0635,  ...,  0.1027, -0.8262,  0.0495],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.4531, -2.0742, -2.2422,  ..., -5.3438, -2.1602,  3.0488],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0108,  0.0026,  0.0032,  ..., -0.0008, -0.0003, -0.0105],
        [ 0.0007, -0.0061, -0.0016,  ...,  0.0027, -0.0080,  0.0053],
        [ 0.0038,  0.0025,  0.0089,  ..., -0.0016,  0.0041,  0.0003],
        ...,
        [-0.0161, -0.0061, -0.0012,  ...,  0.0012,  0.0055, -0.0089],
        [-0.0011,  0.0040, -0.0061,  ..., -0.0041, -0.0021,  0.0156],
        [-0.0100,  0.0013,  0.0010,  ..., -0.0053,  0.0017,  0.0106]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.7480, -2.3301, -2.5078,  ..., -4.5469, -1.6768,  2.8633]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 18:28:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for market is marketplace
Another word for help is aid
Another word for harbor is seaport
Another word for incorrect is wrong
Another word for spouse is partner
Another word for monument is memorial
Another word for style is manner
Another word for hieroglyph is
2024-07-22 18:28:09 root INFO     [order_1_approx] starting weight calculation for Another word for spouse is partner
Another word for incorrect is wrong
Another word for hieroglyph is hieroglyphic
Another word for harbor is seaport
Another word for market is marketplace
Another word for style is manner
Another word for help is aid
Another word for monument is
2024-07-22 18:28:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 18:32:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3291, -0.7573,  0.3179,  ..., -0.1938,  0.4819,  1.3682],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6445, -0.6890, -2.0527,  ..., -1.8281,  3.1230,  2.1270],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0085, -0.0046, -0.0035,  ...,  0.0048,  0.0013, -0.0053],
        [-0.0007, -0.0043, -0.0163,  ...,  0.0016, -0.0147, -0.0044],
        [ 0.0037,  0.0251,  0.0092,  ..., -0.0190,  0.0020,  0.0085],
        ...,
        [-0.0154,  0.0060,  0.0113,  ...,  0.0002, -0.0028, -0.0018],
        [-0.0203,  0.0247,  0.0034,  ..., -0.0146, -0.0067,  0.0031],
        [-0.0364, -0.0443, -0.0059,  ...,  0.0172,  0.0016,  0.0151]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.5664,  0.0830, -1.8809,  ..., -1.3672,  2.5957,  2.7090]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 18:32:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for spouse is partner
Another word for incorrect is wrong
Another word for hieroglyph is hieroglyphic
Another word for harbor is seaport
Another word for market is marketplace
Another word for style is manner
Another word for help is aid
Another word for monument is
2024-07-22 18:32:07 root INFO     [order_1_approx] starting weight calculation for Another word for style is manner
Another word for monument is memorial
Another word for incorrect is wrong
Another word for help is aid
Another word for spouse is partner
Another word for hieroglyph is hieroglyphic
Another word for harbor is seaport
Another word for market is
2024-07-22 18:32:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 18:36:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2170, -0.6221,  0.3164,  ...,  0.7183, -0.2019,  0.4353],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.5781, -2.0000, -6.3711,  ...,  0.1987,  2.1602,  3.6797],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.8198e-02,  1.3779e-02, -3.5248e-03,  ..., -1.4679e-02,
          4.2114e-03, -1.6113e-02],
        [ 8.0643e-03,  2.0538e-02, -2.0996e-02,  ...,  4.2114e-03,
         -1.9226e-02,  1.4450e-02],
        [ 6.6223e-03,  1.1375e-02, -2.6093e-03,  ...,  3.7842e-03,
          7.3090e-03,  3.0823e-02],
        ...,
        [-1.9653e-02, -7.2956e-04,  2.2705e-02,  ...,  2.3117e-02,
         -8.6212e-03,  4.7531e-03],
        [ 1.7214e-03,  2.8229e-03, -2.7895e-05,  ..., -2.2964e-03,
          1.5175e-02, -2.0199e-03],
        [-1.9180e-02, -3.7170e-02,  3.2997e-03,  ...,  6.9427e-03,
          1.2894e-03,  2.3773e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.6992, -1.0254, -6.6562,  ..., -0.0397,  1.7031,  3.8164]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 18:36:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for style is manner
Another word for monument is memorial
Another word for incorrect is wrong
Another word for help is aid
Another word for spouse is partner
Another word for hieroglyph is hieroglyphic
Another word for harbor is seaport
Another word for market is
2024-07-22 18:36:05 root INFO     [order_1_approx] starting weight calculation for Another word for style is manner
Another word for hieroglyph is hieroglyphic
Another word for spouse is partner
Another word for incorrect is wrong
Another word for harbor is seaport
Another word for market is marketplace
Another word for monument is memorial
Another word for help is
2024-07-22 18:36:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 18:40:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2979,  0.7817, -1.1934,  ...,  0.1595,  0.4541,  1.4590],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4741, -1.2676, -2.8008,  ..., -5.9062,  2.1562,  3.7305],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0136,  0.0069,  0.0021,  ..., -0.0054, -0.0182,  0.0143],
        [ 0.0075, -0.0182,  0.0009,  ..., -0.0181, -0.0028, -0.0085],
        [-0.0101,  0.0380,  0.0031,  ...,  0.0110,  0.0063,  0.0166],
        ...,
        [-0.0148, -0.0036,  0.0019,  ..., -0.0093,  0.0004, -0.0031],
        [ 0.0083, -0.0029, -0.0088,  ..., -0.0126,  0.0198, -0.0024],
        [ 0.0009, -0.0071, -0.0072,  ...,  0.0027,  0.0259,  0.0250]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2849, -1.4561, -2.3105,  ..., -5.3477,  1.5703,  3.6016]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 18:40:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for style is manner
Another word for hieroglyph is hieroglyphic
Another word for spouse is partner
Another word for incorrect is wrong
Another word for harbor is seaport
Another word for market is marketplace
Another word for monument is memorial
Another word for help is
2024-07-22 18:40:02 root INFO     [order_1_approx] starting weight calculation for Another word for style is manner
Another word for spouse is partner
Another word for hieroglyph is hieroglyphic
Another word for incorrect is wrong
Another word for monument is memorial
Another word for market is marketplace
Another word for help is aid
Another word for harbor is
2024-07-22 18:40:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 18:44:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3667, -0.2039, -1.5215,  ..., -0.2559, -0.1846,  1.5488],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5703,  0.1235, -4.0391,  ...,  1.2793,  2.0938,  3.3652],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0015, -0.0089,  0.0032,  ..., -0.0065, -0.0163,  0.0080],
        [-0.0035, -0.0098, -0.0122,  ...,  0.0023, -0.0176,  0.0062],
        [-0.0021,  0.0065, -0.0141,  ..., -0.0238, -0.0045,  0.0056],
        ...,
        [-0.0159, -0.0271, -0.0023,  ...,  0.0213, -0.0025,  0.0061],
        [ 0.0034, -0.0092, -0.0044,  ..., -0.0079, -0.0091, -0.0113],
        [ 0.0059, -0.0210,  0.0047,  ...,  0.0008, -0.0077,  0.0059]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1780,  0.3481, -3.6914,  ...,  1.0283,  1.6953,  3.9551]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 18:44:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for style is manner
Another word for spouse is partner
Another word for hieroglyph is hieroglyphic
Another word for incorrect is wrong
Another word for monument is memorial
Another word for market is marketplace
Another word for help is aid
Another word for harbor is
2024-07-22 18:44:01 root INFO     [order_1_approx] starting weight calculation for Another word for hieroglyph is hieroglyphic
Another word for harbor is seaport
Another word for style is manner
Another word for market is marketplace
Another word for spouse is partner
Another word for monument is memorial
Another word for help is aid
Another word for incorrect is
2024-07-22 18:44:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 18:47:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2266, -0.4246,  0.2086,  ...,  0.1639,  0.5435,  1.2881],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5908,  1.6641, -2.5176,  ..., -0.4238,  6.7344, -0.0898],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0265,  0.0253,  0.0288,  ..., -0.0027, -0.0042,  0.0025],
        [ 0.0228, -0.0377, -0.0007,  ...,  0.0234, -0.0148, -0.0134],
        [-0.0063,  0.0012, -0.0054,  ...,  0.0115,  0.0068,  0.0269],
        ...,
        [-0.0031, -0.0320,  0.0126,  ..., -0.0299, -0.0131, -0.0007],
        [-0.0059, -0.0269, -0.0097,  ..., -0.0132,  0.0019, -0.0030],
        [ 0.0126,  0.0107,  0.0013,  ...,  0.0170,  0.0123, -0.0201]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3936,  2.1035, -1.9902,  ..., -0.8281,  7.0742,  1.2773]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 18:47:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for hieroglyph is hieroglyphic
Another word for harbor is seaport
Another word for style is manner
Another word for market is marketplace
Another word for spouse is partner
Another word for monument is memorial
Another word for help is aid
Another word for incorrect is
2024-07-22 18:47:58 root INFO     [order_1_approx] starting weight calculation for Another word for harbor is seaport
Another word for monument is memorial
Another word for help is aid
Another word for incorrect is wrong
Another word for spouse is partner
Another word for market is marketplace
Another word for hieroglyph is hieroglyphic
Another word for style is
2024-07-22 18:47:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 18:51:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.1270, -0.5283,  0.1204,  ...,  0.9175, -0.1321,  0.7476],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6426, -3.3652, -3.9414,  ..., -0.8555, -0.7998,  2.4160],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0121,  0.0038,  0.0201,  ...,  0.0004, -0.0095,  0.0004],
        [ 0.0015, -0.0222, -0.0063,  ..., -0.0002, -0.0257, -0.0009],
        [ 0.0018, -0.0215, -0.0047,  ...,  0.0074, -0.0147, -0.0025],
        ...,
        [-0.0012, -0.0011,  0.0007,  ..., -0.0025,  0.0058,  0.0131],
        [-0.0096,  0.0060,  0.0077,  ...,  0.0018, -0.0212,  0.0057],
        [ 0.0143, -0.0061,  0.0034,  ...,  0.0210,  0.0022,  0.0176]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.9102, -3.2773, -4.6797,  ..., -1.1621, -0.5410,  2.0996]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 18:51:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for harbor is seaport
Another word for monument is memorial
Another word for help is aid
Another word for incorrect is wrong
Another word for spouse is partner
Another word for market is marketplace
Another word for hieroglyph is hieroglyphic
Another word for style is
2024-07-22 18:51:57 root INFO     [order_1_approx] starting weight calculation for Another word for monument is memorial
Another word for harbor is seaport
Another word for style is manner
Another word for market is marketplace
Another word for help is aid
Another word for incorrect is wrong
Another word for hieroglyph is hieroglyphic
Another word for spouse is
2024-07-22 18:51:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 18:55:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.8076,  0.4390,  0.0803,  ...,  0.6807, -0.2600,  0.6763],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3169, -0.8105, -3.2891,  ..., -6.9258, -0.0781,  3.2031],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0229,  0.0148, -0.0029,  ..., -0.0089,  0.0033,  0.0126],
        [-0.0117, -0.0097, -0.0047,  ...,  0.0075, -0.0118,  0.0154],
        [-0.0097, -0.0100, -0.0245,  ..., -0.0054, -0.0096,  0.0453],
        ...,
        [ 0.0009, -0.0041,  0.0068,  ..., -0.0109, -0.0279,  0.0281],
        [-0.0096,  0.0037,  0.0005,  ..., -0.0075, -0.0057,  0.0158],
        [-0.0126, -0.0217, -0.0204,  ...,  0.0099,  0.0182,  0.0049]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.6660, -1.2744, -2.6504,  ..., -6.6992, -0.1217,  4.3125]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 18:55:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for monument is memorial
Another word for harbor is seaport
Another word for style is manner
Another word for market is marketplace
Another word for help is aid
Another word for incorrect is wrong
Another word for hieroglyph is hieroglyphic
Another word for spouse is
2024-07-22 18:55:55 root INFO     total operator prediction time: 1904.8797142505646 seconds
2024-07-22 18:55:55 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on hypernyms - misc
2024-07-22 18:55:55 root INFO     building operator hypernyms - misc
2024-07-22 18:55:55 root INFO     [order_1_approx] starting weight calculation for The shirt falls into the category of clothes
The cake falls into the category of dessert
The peach falls into the category of fruit
The mascara falls into the category of makeup
The dishwasher falls into the category of appliance
The juicer falls into the category of utensil
The tub falls into the category of container
The jeans falls into the category of
2024-07-22 18:55:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 18:59:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9150, -0.8076, -0.3569,  ..., -0.5967,  0.4399,  0.4849],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5347, -1.5215,  0.1016,  ..., -5.7109, -1.6260, -2.3984],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0006, -0.0156, -0.0037,  ...,  0.0077, -0.0163, -0.0008],
        [-0.0144,  0.0105,  0.0043,  ...,  0.0093, -0.0014,  0.0132],
        [-0.0013, -0.0073, -0.0003,  ..., -0.0055, -0.0020, -0.0046],
        ...,
        [-0.0068, -0.0098,  0.0069,  ...,  0.0010, -0.0041,  0.0144],
        [ 0.0047, -0.0070,  0.0003,  ...,  0.0105, -0.0081,  0.0023],
        [-0.0115, -0.0160,  0.0041,  ...,  0.0014,  0.0203,  0.0073]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3003, -2.0703, -0.7686,  ..., -6.0156, -1.7910, -2.8320]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 18:59:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The shirt falls into the category of clothes
The cake falls into the category of dessert
The peach falls into the category of fruit
The mascara falls into the category of makeup
The dishwasher falls into the category of appliance
The juicer falls into the category of utensil
The tub falls into the category of container
The jeans falls into the category of
2024-07-22 18:59:54 root INFO     [order_1_approx] starting weight calculation for The juicer falls into the category of utensil
The jeans falls into the category of trousers
The dishwasher falls into the category of appliance
The cake falls into the category of dessert
The tub falls into the category of container
The shirt falls into the category of clothes
The peach falls into the category of fruit
The mascara falls into the category of
2024-07-22 18:59:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 19:03:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0186,  0.2976,  0.4780,  ..., -0.6890, -0.1002,  0.8828],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-7.1289, -3.0117, -0.7378,  ..., -1.5430, -6.9258,  3.1953],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0104,  0.0063,  0.0041,  ..., -0.0043, -0.0207,  0.0072],
        [-0.0184,  0.0060, -0.0077,  ...,  0.0039, -0.0083,  0.0143],
        [ 0.0018, -0.0022,  0.0052,  ...,  0.0022,  0.0087,  0.0149],
        ...,
        [-0.0139,  0.0104,  0.0028,  ...,  0.0180, -0.0159,  0.0005],
        [-0.0043, -0.0064,  0.0106,  ..., -0.0075, -0.0267,  0.0125],
        [ 0.0023, -0.0168,  0.0103,  ...,  0.0059,  0.0229,  0.0058]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-7.8125, -2.9160, -0.6240,  ..., -1.9551, -7.6328,  2.9512]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 19:03:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The juicer falls into the category of utensil
The jeans falls into the category of trousers
The dishwasher falls into the category of appliance
The cake falls into the category of dessert
The tub falls into the category of container
The shirt falls into the category of clothes
The peach falls into the category of fruit
The mascara falls into the category of
2024-07-22 19:03:53 root INFO     [order_1_approx] starting weight calculation for The mascara falls into the category of makeup
The tub falls into the category of container
The peach falls into the category of fruit
The shirt falls into the category of clothes
The juicer falls into the category of utensil
The jeans falls into the category of trousers
The cake falls into the category of dessert
The dishwasher falls into the category of
2024-07-22 19:03:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 19:07:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1182, -0.7920,  1.3223,  ...,  0.5908,  0.5210, -0.6382],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.4727, -1.2910, -0.8311,  ..., -0.6230, -2.6914,  2.9766],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0183,  0.0139,  0.0008,  ..., -0.0060, -0.0101,  0.0135],
        [-0.0124,  0.0144,  0.0047,  ...,  0.0069, -0.0005, -0.0062],
        [-0.0039, -0.0010,  0.0031,  ...,  0.0028, -0.0075,  0.0138],
        ...,
        [-0.0048, -0.0037, -0.0084,  ...,  0.0234, -0.0092,  0.0025],
        [-0.0062,  0.0057,  0.0049,  ..., -0.0022,  0.0003,  0.0040],
        [-0.0013, -0.0157,  0.0038,  ...,  0.0015,  0.0116,  0.0038]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.8047, -0.4194, -1.2070,  ..., -0.8608, -3.0977,  2.9844]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 19:07:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The mascara falls into the category of makeup
The tub falls into the category of container
The peach falls into the category of fruit
The shirt falls into the category of clothes
The juicer falls into the category of utensil
The jeans falls into the category of trousers
The cake falls into the category of dessert
The dishwasher falls into the category of
2024-07-22 19:07:52 root INFO     [order_1_approx] starting weight calculation for The shirt falls into the category of clothes
The cake falls into the category of dessert
The peach falls into the category of fruit
The jeans falls into the category of trousers
The mascara falls into the category of makeup
The tub falls into the category of container
The dishwasher falls into the category of appliance
The juicer falls into the category of
2024-07-22 19:07:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 19:11:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4722, -1.6289, -0.3501,  ...,  0.2817,  1.1455,  1.3311],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.1250,  2.3867,  1.4639,  ...,  0.3484, -0.9824,  0.2354],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0289,  0.0023,  0.0042,  ..., -0.0050, -0.0031, -0.0054],
        [-0.0093,  0.0144, -0.0064,  ...,  0.0031,  0.0045, -0.0046],
        [ 0.0003, -0.0051,  0.0076,  ...,  0.0060,  0.0123,  0.0135],
        ...,
        [-0.0065, -0.0016, -0.0077,  ...,  0.0151, -0.0196,  0.0100],
        [ 0.0067,  0.0088,  0.0095,  ..., -0.0092, -0.0003,  0.0144],
        [-0.0053, -0.0147, -0.0079,  ...,  0.0034,  0.0149,  0.0177]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.9062,  2.9238,  1.7617,  ...,  0.0247, -1.2783,  0.2051]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 19:11:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The shirt falls into the category of clothes
The cake falls into the category of dessert
The peach falls into the category of fruit
The jeans falls into the category of trousers
The mascara falls into the category of makeup
The tub falls into the category of container
The dishwasher falls into the category of appliance
The juicer falls into the category of
2024-07-22 19:11:51 root INFO     [order_1_approx] starting weight calculation for The mascara falls into the category of makeup
The juicer falls into the category of utensil
The shirt falls into the category of clothes
The peach falls into the category of fruit
The tub falls into the category of container
The jeans falls into the category of trousers
The dishwasher falls into the category of appliance
The cake falls into the category of
2024-07-22 19:11:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 19:15:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7598,  0.5098, -0.5332,  ..., -0.2324,  0.2202,  1.7432],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.3066, -0.2637, -2.6875,  ..., -0.5942, -1.3828,  1.2686],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0191, -0.0020,  0.0093,  ..., -0.0021,  0.0022, -0.0023],
        [-0.0039,  0.0140,  0.0013,  ...,  0.0081, -0.0052,  0.0038],
        [ 0.0016,  0.0021, -0.0057,  ...,  0.0073,  0.0035,  0.0098],
        ...,
        [-0.0008, -0.0006,  0.0048,  ...,  0.0057, -0.0066,  0.0021],
        [-0.0049,  0.0013,  0.0100,  ...,  0.0071,  0.0137, -0.0095],
        [-0.0143, -0.0013, -0.0040,  ...,  0.0044,  0.0059,  0.0135]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.3867,  0.9062, -2.5195,  ..., -0.6558, -2.0000,  0.5845]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 19:15:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The mascara falls into the category of makeup
The juicer falls into the category of utensil
The shirt falls into the category of clothes
The peach falls into the category of fruit
The tub falls into the category of container
The jeans falls into the category of trousers
The dishwasher falls into the category of appliance
The cake falls into the category of
2024-07-22 19:15:50 root INFO     [order_1_approx] starting weight calculation for The shirt falls into the category of clothes
The juicer falls into the category of utensil
The peach falls into the category of fruit
The jeans falls into the category of trousers
The mascara falls into the category of makeup
The cake falls into the category of dessert
The dishwasher falls into the category of appliance
The tub falls into the category of
2024-07-22 19:15:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 19:19:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0178,  0.7764,  0.0328,  ..., -0.0183,  0.0792, -0.2454],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.6309,  1.5527, -1.7412,  ..., -0.3767, -0.5166,  0.8174],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0265,  0.0273,  0.0002,  ...,  0.0107,  0.0099, -0.0070],
        [-0.0068,  0.0136, -0.0006,  ..., -0.0087, -0.0015,  0.0114],
        [ 0.0009,  0.0169,  0.0010,  ...,  0.0052,  0.0097,  0.0083],
        ...,
        [-0.0104,  0.0097, -0.0014,  ...,  0.0087, -0.0006,  0.0019],
        [-0.0105,  0.0207,  0.0209,  ...,  0.0024,  0.0049,  0.0014],
        [ 0.0102,  0.0034,  0.0082,  ...,  0.0229,  0.0113,  0.0043]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.4062,  2.4512, -3.0469,  ..., -0.0886, -1.2812,  0.9502]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 19:19:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The shirt falls into the category of clothes
The juicer falls into the category of utensil
The peach falls into the category of fruit
The jeans falls into the category of trousers
The mascara falls into the category of makeup
The cake falls into the category of dessert
The dishwasher falls into the category of appliance
The tub falls into the category of
2024-07-22 19:19:49 root INFO     [order_1_approx] starting weight calculation for The cake falls into the category of dessert
The peach falls into the category of fruit
The dishwasher falls into the category of appliance
The jeans falls into the category of trousers
The juicer falls into the category of utensil
The mascara falls into the category of makeup
The tub falls into the category of container
The shirt falls into the category of
2024-07-22 19:19:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 19:23:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2195, -0.4019,  0.3533,  ..., -0.2065, -0.0266,  1.3867],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0088, -0.2725,  0.0068,  ..., -4.5664, -0.0857, -2.8848],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0092,  0.0022, -0.0058,  ..., -0.0060, -0.0166,  0.0088],
        [-0.0174,  0.0041,  0.0040,  ...,  0.0054, -0.0092, -0.0043],
        [-0.0029,  0.0027, -0.0040,  ...,  0.0033,  0.0085,  0.0052],
        ...,
        [ 0.0011, -0.0001,  0.0061,  ...,  0.0147, -0.0062,  0.0119],
        [-0.0013,  0.0034, -0.0049,  ..., -0.0022, -0.0027,  0.0030],
        [-0.0118, -0.0073,  0.0041,  ..., -0.0098,  0.0188,  0.0187]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6040, -0.0294, -1.0361,  ..., -4.5352, -0.4333, -3.4453]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 19:23:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The cake falls into the category of dessert
The peach falls into the category of fruit
The dishwasher falls into the category of appliance
The jeans falls into the category of trousers
The juicer falls into the category of utensil
The mascara falls into the category of makeup
The tub falls into the category of container
The shirt falls into the category of
2024-07-22 19:23:48 root INFO     [order_1_approx] starting weight calculation for The juicer falls into the category of utensil
The cake falls into the category of dessert
The jeans falls into the category of trousers
The mascara falls into the category of makeup
The shirt falls into the category of clothes
The dishwasher falls into the category of appliance
The tub falls into the category of container
The peach falls into the category of
2024-07-22 19:23:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 19:27:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4131, -0.7080,  0.0513,  ..., -1.4385, -0.2178,  0.5645],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.9209, -0.6504, -6.0781,  ..., -2.1973, -1.4316,  1.1855],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0167,  0.0044, -0.0024,  ..., -0.0084, -0.0053,  0.0051],
        [-0.0038,  0.0022,  0.0043,  ...,  0.0048, -0.0008,  0.0046],
        [ 0.0066, -0.0003,  0.0009,  ...,  0.0165,  0.0032,  0.0064],
        ...,
        [-0.0031, -0.0053,  0.0042,  ...,  0.0082, -0.0041,  0.0097],
        [ 0.0196,  0.0060,  0.0002,  ...,  0.0096, -0.0035, -0.0036],
        [-0.0203, -0.0047,  0.0034,  ..., -0.0062,  0.0109,  0.0071]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.6250, -0.1396, -5.7773,  ..., -2.5762, -1.1426,  0.1621]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 19:27:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The juicer falls into the category of utensil
The cake falls into the category of dessert
The jeans falls into the category of trousers
The mascara falls into the category of makeup
The shirt falls into the category of clothes
The dishwasher falls into the category of appliance
The tub falls into the category of container
The peach falls into the category of
2024-07-22 19:27:47 root INFO     total operator prediction time: 1911.8189709186554 seconds
2024-07-22 19:27:47 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - substance
2024-07-22 19:27:47 root INFO     building operator meronyms - substance
2024-07-22 19:27:47 root INFO     [order_1_approx] starting weight calculation for A desk is made up of wood
A concrete is made up of silicon
A beard is made up of hair
A house is made up of bricks
A pastry is made up of flour
A box is made up of cardboard
A beach is made up of sand
A roof is made up of
2024-07-22 19:27:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 19:31:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5977,  0.0752,  1.4219,  ...,  1.1211, -0.4280,  0.4434],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.0625,  2.1914, -0.3398,  ...,  2.3027,  1.0244, -2.3203],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-6.2752e-04,  6.7902e-04,  2.3384e-03,  ..., -2.2316e-03,
          7.7667e-03, -5.5161e-03],
        [-2.9716e-03,  9.2163e-03,  2.7275e-03,  ...,  1.2398e-04,
          1.2192e-02,  7.6294e-06],
        [ 3.8986e-03,  1.1871e-02,  2.4605e-04,  ..., -4.1962e-04,
          1.3283e-02,  2.8553e-03],
        ...,
        [-7.7820e-03, -1.1234e-03, -1.7719e-03,  ..., -2.9335e-03,
         -7.5989e-03, -1.1082e-03],
        [ 1.6251e-03,  5.7068e-03, -3.0861e-03,  ...,  2.0599e-04,
         -7.3204e-03, -3.8338e-03],
        [-5.9509e-04,  5.1727e-03,  7.1945e-03,  ..., -7.5722e-03,
          4.5815e-03,  1.1856e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.3281,  1.9414, -0.5708,  ...,  2.7969,  0.7002, -2.7344]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 19:31:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A desk is made up of wood
A concrete is made up of silicon
A beard is made up of hair
A house is made up of bricks
A pastry is made up of flour
A box is made up of cardboard
A beach is made up of sand
A roof is made up of
2024-07-22 19:31:45 root INFO     [order_1_approx] starting weight calculation for A pastry is made up of flour
A house is made up of bricks
A box is made up of cardboard
A concrete is made up of silicon
A beach is made up of sand
A desk is made up of wood
A roof is made up of shingles
A beard is made up of
2024-07-22 19:31:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 19:35:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3037, -0.3530, -0.1118,  ..., -0.0288, -0.4653,  3.1211],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7539, -0.6445, -2.6152,  ...,  2.5234, -2.9199, -0.6890],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0171, -0.0124, -0.0098,  ...,  0.0085,  0.0155, -0.0208],
        [-0.0119, -0.0012,  0.0050,  ...,  0.0201,  0.0223, -0.0106],
        [ 0.0014, -0.0015, -0.0061,  ..., -0.0043,  0.0047,  0.0068],
        ...,
        [ 0.0026, -0.0182, -0.0183,  ...,  0.0216, -0.0006,  0.0087],
        [ 0.0098,  0.0023, -0.0152,  ...,  0.0087, -0.0185,  0.0082],
        [-0.0050, -0.0011,  0.0002,  ..., -0.0119,  0.0075, -0.0019]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2930, -0.9766, -3.1621,  ...,  2.5898, -3.2871, -0.0352]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 19:35:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A pastry is made up of flour
A house is made up of bricks
A box is made up of cardboard
A concrete is made up of silicon
A beach is made up of sand
A desk is made up of wood
A roof is made up of shingles
A beard is made up of
2024-07-22 19:35:41 root INFO     [order_1_approx] starting weight calculation for A house is made up of bricks
A concrete is made up of silicon
A roof is made up of shingles
A beach is made up of sand
A pastry is made up of flour
A box is made up of cardboard
A beard is made up of hair
A desk is made up of
2024-07-22 19:35:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 19:39:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3093, -0.5078,  0.8770,  ...,  0.4353,  0.1923,  0.6758],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 7.1172,  0.1162, -1.0918,  ...,  3.9121, -3.9062, -0.1973],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0183, -0.0139,  0.0050,  ..., -0.0065, -0.0023, -0.0065],
        [-0.0006,  0.0212,  0.0263,  ...,  0.0167, -0.0168, -0.0003],
        [ 0.0203,  0.0072,  0.0243,  ..., -0.0006, -0.0146, -0.0082],
        ...,
        [ 0.0089, -0.0027, -0.0096,  ..., -0.0030, -0.0085,  0.0152],
        [ 0.0232,  0.0004,  0.0040,  ...,  0.0060, -0.0053,  0.0019],
        [ 0.0070, -0.0105,  0.0239,  ..., -0.0116,  0.0111,  0.0357]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.5000, -0.1572, -1.4795,  ...,  4.5898, -3.4297, -0.2993]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 19:39:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A house is made up of bricks
A concrete is made up of silicon
A roof is made up of shingles
A beach is made up of sand
A pastry is made up of flour
A box is made up of cardboard
A beard is made up of hair
A desk is made up of
2024-07-22 19:39:37 root INFO     [order_1_approx] starting weight calculation for A roof is made up of shingles
A box is made up of cardboard
A beach is made up of sand
A house is made up of bricks
A desk is made up of wood
A pastry is made up of flour
A beard is made up of hair
A concrete is made up of
2024-07-22 19:39:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 19:43:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2583,  0.3440, -0.3923,  ...,  0.4875,  1.1621, -0.2598],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.5410, -0.3086, -1.2715,  ...,  1.8691, -2.6758,  2.1191],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-9.6817e-03, -4.0550e-03, -9.7275e-04,  ...,  4.0283e-03,
          9.0714e-03, -3.0441e-03],
        [-9.3079e-04, -5.1346e-03,  1.1162e-02,  ...,  5.9547e-03,
          3.0136e-03, -7.7133e-03],
        [-3.0136e-04, -1.3847e-02, -5.2643e-03,  ...,  1.7662e-03,
         -1.4915e-03, -1.7014e-02],
        ...,
        [ 1.1398e-02, -7.7972e-03,  6.0997e-03,  ...,  9.4604e-04,
          1.6403e-04,  3.0396e-02],
        [ 4.8676e-03, -2.1637e-02, -4.3564e-03,  ..., -1.6251e-03,
         -9.1934e-04, -9.8228e-05],
        [ 1.7815e-03, -6.0501e-03,  1.0681e-04,  ..., -2.9888e-03,
          6.5231e-03,  8.7967e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.0293, -0.2747, -2.2520,  ...,  2.4863, -3.0215,  2.0586]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 19:43:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A roof is made up of shingles
A box is made up of cardboard
A beach is made up of sand
A house is made up of bricks
A desk is made up of wood
A pastry is made up of flour
A beard is made up of hair
A concrete is made up of
2024-07-22 19:43:34 root INFO     [order_1_approx] starting weight calculation for A beard is made up of hair
A pastry is made up of flour
A concrete is made up of silicon
A box is made up of cardboard
A house is made up of bricks
A roof is made up of shingles
A desk is made up of wood
A beach is made up of
2024-07-22 19:43:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 19:47:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1602,  0.7275,  0.6284,  ..., -0.5381,  0.0938, -0.5996],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6260,  3.4277, -1.8887,  ...,  4.6797, -2.2715, -0.2983],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.2125e-02, -1.1703e-02,  2.3880e-03,  ...,  5.1193e-03,
          2.2488e-03, -1.1169e-02],
        [ 2.7523e-03, -8.7357e-04,  6.3019e-03,  ...,  6.3782e-03,
         -5.8327e-03,  7.1945e-03],
        [ 7.6981e-03,  2.2697e-03, -6.2027e-03,  ..., -9.9182e-05,
          1.5900e-02,  4.2915e-03],
        ...,
        [ 9.7198e-03, -7.5836e-03,  4.4403e-03,  ...,  2.5272e-03,
         -4.3564e-03,  1.2817e-03],
        [-2.9564e-03,  6.2637e-03, -7.4310e-03,  ...,  1.9474e-03,
         -1.3405e-02, -1.6632e-03],
        [-1.0433e-03,  5.1155e-03,  8.9340e-03,  ..., -3.2177e-03,
          1.0307e-02,  8.2016e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9492,  2.9316, -1.7949,  ...,  4.7578, -1.9629,  0.0232]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 19:47:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A beard is made up of hair
A pastry is made up of flour
A concrete is made up of silicon
A box is made up of cardboard
A house is made up of bricks
A roof is made up of shingles
A desk is made up of wood
A beach is made up of
2024-07-22 19:47:31 root INFO     [order_1_approx] starting weight calculation for A roof is made up of shingles
A concrete is made up of silicon
A beach is made up of sand
A desk is made up of wood
A beard is made up of hair
A box is made up of cardboard
A pastry is made up of flour
A house is made up of
2024-07-22 19:47:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 19:51:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0931,  0.2766, -0.2402,  ...,  0.6675, -0.4338,  0.1348],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.3867, -1.1758,  0.8672,  ..., -0.5000, -2.2012, -1.0547],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0016, -0.0086, -0.0103,  ..., -0.0014, -0.0054, -0.0009],
        [-0.0009, -0.0045,  0.0046,  ...,  0.0077,  0.0026,  0.0038],
        [ 0.0132,  0.0022, -0.0035,  ..., -0.0012, -0.0049,  0.0065],
        ...,
        [-0.0017,  0.0050, -0.0019,  ...,  0.0126,  0.0022,  0.0172],
        [ 0.0024,  0.0025,  0.0097,  ...,  0.0026, -0.0013,  0.0131],
        [ 0.0107, -0.0038,  0.0062,  ...,  0.0008,  0.0217,  0.0199]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.0137, -0.6396,  0.5918,  ..., -0.3511, -2.3008, -1.5195]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 19:51:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A roof is made up of shingles
A concrete is made up of silicon
A beach is made up of sand
A desk is made up of wood
A beard is made up of hair
A box is made up of cardboard
A pastry is made up of flour
A house is made up of
2024-07-22 19:51:27 root INFO     [order_1_approx] starting weight calculation for A concrete is made up of silicon
A house is made up of bricks
A desk is made up of wood
A beard is made up of hair
A beach is made up of sand
A roof is made up of shingles
A pastry is made up of flour
A box is made up of
2024-07-22 19:51:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 19:55:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2144, -0.5576,  0.3098,  ...,  0.1147, -0.5210,  1.0635],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.4766,  0.9263, -2.9531,  ...,  5.4414, -4.5000, -0.5664],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 6.8665e-04, -2.4750e-02,  1.0651e-02,  ..., -6.0005e-03,
         -2.5291e-03,  3.8433e-03],
        [-4.9438e-03,  1.5671e-02, -3.7289e-03,  ...,  3.5095e-04,
         -1.8024e-03,  3.3493e-03],
        [ 6.0043e-03, -9.2087e-03,  2.7649e-02,  ...,  2.1744e-02,
         -2.0599e-04, -3.5725e-03],
        ...,
        [-3.0460e-03, -2.1408e-02, -4.2305e-03,  ..., -2.7466e-04,
         -4.0398e-03,  2.2675e-02],
        [ 1.3504e-02, -1.1993e-02,  1.3832e-02,  ..., -4.1962e-05,
         -4.5776e-03,  2.2774e-03],
        [ 7.5836e-03,  5.2757e-03,  2.1301e-02,  ...,  1.3702e-02,
          1.7792e-02,  2.3514e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1172,  0.9512, -3.0332,  ...,  5.8008, -4.1641, -0.3740]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 19:55:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A concrete is made up of silicon
A house is made up of bricks
A desk is made up of wood
A beard is made up of hair
A beach is made up of sand
A roof is made up of shingles
A pastry is made up of flour
A box is made up of
2024-07-22 19:55:22 root INFO     [order_1_approx] starting weight calculation for A box is made up of cardboard
A desk is made up of wood
A beard is made up of hair
A roof is made up of shingles
A beach is made up of sand
A concrete is made up of silicon
A house is made up of bricks
A pastry is made up of
2024-07-22 19:55:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 19:59:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3428, -0.3735,  0.1833,  ...,  0.9834, -0.3232,  1.1191],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.2500, -1.5254,  2.4062,  ...,  3.0879, -3.3945,  1.5020],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0015, -0.0076,  0.0018,  ..., -0.0068,  0.0025, -0.0132],
        [-0.0066, -0.0082, -0.0060,  ...,  0.0030, -0.0006,  0.0099],
        [ 0.0032,  0.0084, -0.0035,  ...,  0.0071,  0.0039,  0.0023],
        ...,
        [ 0.0039, -0.0146, -0.0020,  ..., -0.0080,  0.0044, -0.0012],
        [ 0.0087,  0.0093,  0.0033,  ...,  0.0085, -0.0045,  0.0020],
        [ 0.0043,  0.0036,  0.0028,  ...,  0.0020,  0.0120,  0.0009]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.6250, -1.4297,  2.1152,  ...,  2.8613, -3.1699,  1.3418]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 19:59:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A box is made up of cardboard
A desk is made up of wood
A beard is made up of hair
A roof is made up of shingles
A beach is made up of sand
A concrete is made up of silicon
A house is made up of bricks
A pastry is made up of
2024-07-22 19:59:17 root INFO     total operator prediction time: 1890.353499174118 seconds
2024-07-22 19:59:17 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on synonyms - intensity
2024-07-22 19:59:17 root INFO     building operator synonyms - intensity
2024-07-22 19:59:17 root INFO     [order_1_approx] starting weight calculation for A more intense word for tasty is delicious
A more intense word for unfortunate is tragic
A more intense word for lake is sea
A more intense word for creative is ingenious
A more intense word for soon is immediately
A more intense word for like is love
A more intense word for faith is fanatism
A more intense word for sad is
2024-07-22 19:59:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 20:03:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3320, -0.2656, -0.0923,  ...,  1.5137,  1.0127,  1.4033],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7227,  1.4893, -4.7930,  ...,  1.3926,  1.5537, -0.9570],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0286,  0.0168,  0.0036,  ..., -0.0208, -0.0227, -0.0116],
        [ 0.0099,  0.0154,  0.0007,  ...,  0.0168, -0.0004, -0.0012],
        [ 0.0080,  0.0109, -0.0258,  ..., -0.0070, -0.0045,  0.0076],
        ...,
        [-0.0194, -0.0153,  0.0065,  ..., -0.0020, -0.0113,  0.0028],
        [-0.0043, -0.0121,  0.0197,  ...,  0.0023,  0.0021,  0.0021],
        [ 0.0094,  0.0053, -0.0203,  ..., -0.0085,  0.0116,  0.0027]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0352,  1.0537, -4.5703,  ...,  2.7852,  1.2471, -2.1562]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 20:03:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for tasty is delicious
A more intense word for unfortunate is tragic
A more intense word for lake is sea
A more intense word for creative is ingenious
A more intense word for soon is immediately
A more intense word for like is love
A more intense word for faith is fanatism
A more intense word for sad is
2024-07-22 20:03:16 root INFO     [order_1_approx] starting weight calculation for A more intense word for sad is desparate
A more intense word for tasty is delicious
A more intense word for unfortunate is tragic
A more intense word for soon is immediately
A more intense word for like is love
A more intense word for lake is sea
A more intense word for creative is ingenious
A more intense word for faith is
2024-07-22 20:03:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 20:07:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.8770, -1.3145,  0.0571,  ...,  0.4207, -0.8862,  1.0967],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.2344,  0.4316, -0.8828,  ..., -1.2305,  1.7500,  0.1191],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0001, -0.0106, -0.0024,  ..., -0.0024, -0.0079, -0.0029],
        [ 0.0144, -0.0104,  0.0101,  ...,  0.0061, -0.0052, -0.0334],
        [-0.0007,  0.0166, -0.0233,  ..., -0.0026, -0.0031,  0.0168],
        ...,
        [-0.0179, -0.0107,  0.0139,  ..., -0.0110, -0.0265,  0.0221],
        [-0.0013, -0.0055,  0.0120,  ...,  0.0041, -0.0043, -0.0003],
        [-0.0096, -0.0117, -0.0039,  ...,  0.0004,  0.0003, -0.0011]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.9922,  0.5439,  0.0737,  ..., -0.4546,  1.0684,  1.3467]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 20:07:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for sad is desparate
A more intense word for tasty is delicious
A more intense word for unfortunate is tragic
A more intense word for soon is immediately
A more intense word for like is love
A more intense word for lake is sea
A more intense word for creative is ingenious
A more intense word for faith is
2024-07-22 20:07:15 root INFO     [order_1_approx] starting weight calculation for A more intense word for creative is ingenious
A more intense word for sad is desparate
A more intense word for faith is fanatism
A more intense word for soon is immediately
A more intense word for unfortunate is tragic
A more intense word for tasty is delicious
A more intense word for lake is sea
A more intense word for like is
2024-07-22 20:07:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 20:11:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2490, -0.4934,  0.8276,  ...,  0.3682, -0.4189,  1.0059],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.7891, -0.7783,  1.7002,  ..., -1.8281,  5.0703, -2.5391],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0101,  0.0143,  0.0241,  ...,  0.0029,  0.0032,  0.0178],
        [-0.0096,  0.0040, -0.0027,  ...,  0.0107, -0.0009,  0.0061],
        [-0.0123, -0.0093,  0.0154,  ...,  0.0190, -0.0100,  0.0005],
        ...,
        [-0.0188,  0.0032,  0.0035,  ..., -0.0179,  0.0009, -0.0343],
        [ 0.0147,  0.0143, -0.0007,  ..., -0.0082, -0.0021, -0.0054],
        [ 0.0224,  0.0147, -0.0187,  ..., -0.0105,  0.0295, -0.0070]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.6406, -1.1387,  2.6250,  ..., -0.2295,  5.2188, -1.4189]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 20:11:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for creative is ingenious
A more intense word for sad is desparate
A more intense word for faith is fanatism
A more intense word for soon is immediately
A more intense word for unfortunate is tragic
A more intense word for tasty is delicious
A more intense word for lake is sea
A more intense word for like is
2024-07-22 20:11:16 root INFO     [order_1_approx] starting weight calculation for A more intense word for unfortunate is tragic
A more intense word for like is love
A more intense word for sad is desparate
A more intense word for tasty is delicious
A more intense word for faith is fanatism
A more intense word for soon is immediately
A more intense word for lake is sea
A more intense word for creative is
2024-07-22 20:11:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 20:15:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4805, -0.6084,  0.3379,  ...,  0.9771,  0.3281,  0.6113],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.0664, -3.0078, -3.7266,  ..., -0.8364,  1.3994, -1.6406],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0233, -0.0195,  0.0155,  ...,  0.0021, -0.0158, -0.0033],
        [ 0.0118, -0.0049, -0.0008,  ..., -0.0059, -0.0122, -0.0130],
        [ 0.0133,  0.0130, -0.0157,  ..., -0.0002,  0.0091,  0.0124],
        ...,
        [-0.0328, -0.0231,  0.0296,  ..., -0.0053,  0.0097, -0.0011],
        [-0.0115,  0.0038, -0.0010,  ...,  0.0002, -0.0023,  0.0075],
        [-0.0126,  0.0162, -0.0082,  ..., -0.0037,  0.0019, -0.0194]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.6836, -2.6680, -3.9961,  ..., -0.5615,  1.7715, -0.0400]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 20:15:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for unfortunate is tragic
A more intense word for like is love
A more intense word for sad is desparate
A more intense word for tasty is delicious
A more intense word for faith is fanatism
A more intense word for soon is immediately
A more intense word for lake is sea
A more intense word for creative is
2024-07-22 20:15:17 root INFO     [order_1_approx] starting weight calculation for A more intense word for faith is fanatism
A more intense word for like is love
A more intense word for lake is sea
A more intense word for creative is ingenious
A more intense word for sad is desparate
A more intense word for tasty is delicious
A more intense word for unfortunate is tragic
A more intense word for soon is
2024-07-22 20:15:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 20:19:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4492,  0.4673, -0.7344,  ...,  0.7964,  0.0537,  0.6143],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.1367,  1.7217, -4.3047,  ..., -1.5049,  0.5356, -2.1973],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0039,  0.0099,  0.0138,  ..., -0.0118, -0.0098,  0.0091],
        [ 0.0046, -0.0111, -0.0073,  ...,  0.0076, -0.0102,  0.0021],
        [-0.0116,  0.0043,  0.0027,  ..., -0.0026,  0.0053,  0.0079],
        ...,
        [-0.0042, -0.0110, -0.0069,  ..., -0.0088, -0.0055,  0.0226],
        [-0.0043,  0.0025,  0.0262,  ..., -0.0096,  0.0023,  0.0080],
        [ 0.0178,  0.0083,  0.0046,  ...,  0.0123,  0.0046, -0.0093]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7988,  1.9766, -3.7988,  ..., -0.9448,  0.2314, -1.9473]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 20:19:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for faith is fanatism
A more intense word for like is love
A more intense word for lake is sea
A more intense word for creative is ingenious
A more intense word for sad is desparate
A more intense word for tasty is delicious
A more intense word for unfortunate is tragic
A more intense word for soon is
2024-07-22 20:19:17 root INFO     [order_1_approx] starting weight calculation for A more intense word for unfortunate is tragic
A more intense word for lake is sea
A more intense word for soon is immediately
A more intense word for sad is desparate
A more intense word for faith is fanatism
A more intense word for like is love
A more intense word for creative is ingenious
A more intense word for tasty is
2024-07-22 20:19:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 20:23:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7749,  0.0630, -0.0959,  ..., -0.0332,  0.1208,  0.2461],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.8867, -1.0859,  0.9268,  ..., -0.2305,  3.4980, -0.6299],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0110,  0.0044,  0.0217,  ..., -0.0100, -0.0060, -0.0010],
        [ 0.0047,  0.0121, -0.0128,  ..., -0.0121, -0.0125,  0.0305],
        [-0.0020,  0.0108,  0.0143,  ..., -0.0055,  0.0066,  0.0173],
        ...,
        [-0.0169, -0.0089,  0.0059,  ...,  0.0059, -0.0135,  0.0232],
        [-0.0113,  0.0075, -0.0005,  ...,  0.0017, -0.0117,  0.0051],
        [-0.0101, -0.0061,  0.0020,  ...,  0.0085,  0.0056, -0.0046]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.5430, -0.9551,  1.3096,  ...,  0.8535,  3.4473, -1.1533]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 20:23:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for unfortunate is tragic
A more intense word for lake is sea
A more intense word for soon is immediately
A more intense word for sad is desparate
A more intense word for faith is fanatism
A more intense word for like is love
A more intense word for creative is ingenious
A more intense word for tasty is
2024-07-22 20:23:18 root INFO     [order_1_approx] starting weight calculation for A more intense word for like is love
A more intense word for faith is fanatism
A more intense word for unfortunate is tragic
A more intense word for sad is desparate
A more intense word for soon is immediately
A more intense word for creative is ingenious
A more intense word for tasty is delicious
A more intense word for lake is
2024-07-22 20:23:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 20:27:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5586, -0.1443,  0.6440,  ...,  0.2260, -0.1455,  0.9150],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4702,  3.1406, -4.2773,  ...,  3.9043, -0.0781, -2.6250],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0030,  0.0058,  0.0031,  ..., -0.0047,  0.0076,  0.0027],
        [ 0.0087, -0.0205, -0.0175,  ...,  0.0058,  0.0064,  0.0058],
        [-0.0099,  0.0375, -0.0096,  ..., -0.0038, -0.0072,  0.0227],
        ...,
        [-0.0208, -0.0139,  0.0094,  ..., -0.0090, -0.0002,  0.0052],
        [ 0.0017,  0.0087, -0.0046,  ..., -0.0158,  0.0046,  0.0037],
        [ 0.0063,  0.0061,  0.0109,  ..., -0.0066, -0.0094, -0.0056]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2944,  4.4453, -2.9023,  ...,  3.4844,  1.0420, -2.5254]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 20:27:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for like is love
A more intense word for faith is fanatism
A more intense word for unfortunate is tragic
A more intense word for sad is desparate
A more intense word for soon is immediately
A more intense word for creative is ingenious
A more intense word for tasty is delicious
A more intense word for lake is
2024-07-22 20:27:18 root INFO     [order_1_approx] starting weight calculation for A more intense word for like is love
A more intense word for tasty is delicious
A more intense word for faith is fanatism
A more intense word for sad is desparate
A more intense word for soon is immediately
A more intense word for lake is sea
A more intense word for creative is ingenious
A more intense word for unfortunate is
2024-07-22 20:27:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 20:31:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9810, -0.5156, -1.0703,  ...,  0.6816,  1.6133,  0.9688],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.3984,  0.2695, -6.3516,  ..., -1.0352, -3.0723,  3.4102],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0012, -0.0123,  0.0126,  ..., -0.0079,  0.0054,  0.0073],
        [ 0.0130, -0.0046, -0.0019,  ...,  0.0159, -0.0140, -0.0104],
        [-0.0101,  0.0245, -0.0059,  ..., -0.0099, -0.0048,  0.0013],
        ...,
        [-0.0255, -0.0094,  0.0071,  ..., -0.0249, -0.0062,  0.0145],
        [ 0.0054,  0.0095,  0.0107,  ..., -0.0233,  0.0088, -0.0116],
        [-0.0154, -0.0122,  0.0031,  ..., -0.0069,  0.0214,  0.0112]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.4414,  0.4551, -5.6250,  ..., -0.9995, -1.3965,  3.0703]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 20:31:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for like is love
A more intense word for tasty is delicious
A more intense word for faith is fanatism
A more intense word for sad is desparate
A more intense word for soon is immediately
A more intense word for lake is sea
A more intense word for creative is ingenious
A more intense word for unfortunate is
2024-07-22 20:31:19 root INFO     total operator prediction time: 1921.862063884735 seconds
2024-07-22 20:31:19 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on hypernyms - animals
2024-07-22 20:31:19 root INFO     building operator hypernyms - animals
2024-07-22 20:31:19 root INFO     [order_1_approx] starting weight calculation for The tiger falls into the category of feline
The buffalo falls into the category of bovid
The fox falls into the category of canine
The hawk falls into the category of raptor
The chinchilla falls into the category of rodent
The chicken falls into the category of fowl
The porcupine falls into the category of rodent
The chimpanzee falls into the category of
2024-07-22 20:31:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 20:35:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2681, -0.9214, -0.3354,  ..., -0.0815,  0.0859,  0.7871],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.0508, -0.5269,  4.0664,  ..., -1.4014, -1.8594, -0.6172],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0011, -0.0128, -0.0005,  ..., -0.0023, -0.0039, -0.0047],
        [ 0.0017, -0.0064,  0.0085,  ...,  0.0056,  0.0025, -0.0043],
        [-0.0022, -0.0099,  0.0052,  ..., -0.0071,  0.0039,  0.0021],
        ...,
        [-0.0092, -0.0028, -0.0023,  ...,  0.0021, -0.0007,  0.0100],
        [-0.0059, -0.0026, -0.0122,  ...,  0.0042,  0.0087,  0.0036],
        [ 0.0031, -0.0098,  0.0015,  ...,  0.0028,  0.0016, -0.0003]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.1953,  0.2959,  4.3945,  ..., -0.9521, -2.3203, -0.2368]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 20:35:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The tiger falls into the category of feline
The buffalo falls into the category of bovid
The fox falls into the category of canine
The hawk falls into the category of raptor
The chinchilla falls into the category of rodent
The chicken falls into the category of fowl
The porcupine falls into the category of rodent
The chimpanzee falls into the category of
2024-07-22 20:35:17 root INFO     [order_1_approx] starting weight calculation for The porcupine falls into the category of rodent
The tiger falls into the category of feline
The chinchilla falls into the category of rodent
The hawk falls into the category of raptor
The buffalo falls into the category of bovid
The fox falls into the category of canine
The chimpanzee falls into the category of primate
The chicken falls into the category of
2024-07-22 20:35:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 20:39:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5493, -0.2568,  0.2842,  ...,  0.7432, -1.3770,  1.2090],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7676, -2.8594, -0.2915,  ...,  1.8750,  1.1523,  0.7793],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0060, -0.0012,  0.0119,  ...,  0.0073,  0.0045,  0.0067],
        [ 0.0051, -0.0018, -0.0045,  ...,  0.0123,  0.0166, -0.0033],
        [-0.0175,  0.0040, -0.0151,  ..., -0.0026,  0.0040,  0.0004],
        ...,
        [-0.0312,  0.0002, -0.0159,  ..., -0.0051, -0.0175,  0.0247],
        [-0.0005,  0.0060, -0.0009,  ..., -0.0020,  0.0039, -0.0001],
        [-0.0051, -0.0090, -0.0041,  ..., -0.0160,  0.0013,  0.0068]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3154, -2.6094, -0.2871,  ...,  2.3496,  1.2051,  0.8081]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 20:39:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The porcupine falls into the category of rodent
The tiger falls into the category of feline
The chinchilla falls into the category of rodent
The hawk falls into the category of raptor
The buffalo falls into the category of bovid
The fox falls into the category of canine
The chimpanzee falls into the category of primate
The chicken falls into the category of
2024-07-22 20:39:17 root INFO     [order_1_approx] starting weight calculation for The fox falls into the category of canine
The chicken falls into the category of fowl
The chimpanzee falls into the category of primate
The chinchilla falls into the category of rodent
The tiger falls into the category of feline
The buffalo falls into the category of bovid
The hawk falls into the category of raptor
The porcupine falls into the category of
2024-07-22 20:39:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 20:43:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0496, -1.5439, -0.7266,  ...,  0.5400,  0.6338,  0.3960],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.6484,  0.8145, -2.7344,  ...,  2.0645, -1.4199,  3.6621],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0064, -0.0118,  0.0066,  ...,  0.0063,  0.0034, -0.0044],
        [-0.0083,  0.0018,  0.0124,  ...,  0.0049, -0.0002, -0.0070],
        [ 0.0032, -0.0011,  0.0056,  ...,  0.0015,  0.0085, -0.0050],
        ...,
        [-0.0094, -0.0068, -0.0045,  ...,  0.0034, -0.0075,  0.0085],
        [-0.0086, -0.0002, -0.0041,  ..., -0.0058,  0.0020, -0.0004],
        [-0.0016,  0.0043, -0.0039,  ..., -0.0034,  0.0086,  0.0042]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.0781,  1.5791, -2.6660,  ...,  2.2949, -2.0645,  3.9746]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 20:43:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The fox falls into the category of canine
The chicken falls into the category of fowl
The chimpanzee falls into the category of primate
The chinchilla falls into the category of rodent
The tiger falls into the category of feline
The buffalo falls into the category of bovid
The hawk falls into the category of raptor
The porcupine falls into the category of
2024-07-22 20:43:18 root INFO     [order_1_approx] starting weight calculation for The buffalo falls into the category of bovid
The chinchilla falls into the category of rodent
The chicken falls into the category of fowl
The tiger falls into the category of feline
The hawk falls into the category of raptor
The chimpanzee falls into the category of primate
The porcupine falls into the category of rodent
The fox falls into the category of
2024-07-22 20:43:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 20:47:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6030, -0.8843, -1.0693,  ...,  0.2241, -0.2766,  0.5210],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1680, -0.4456,  0.8291,  ..., -2.3340,  0.8887,  2.8574],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0039, -0.0151, -0.0057,  ...,  0.0047, -0.0029, -0.0010],
        [-0.0012,  0.0103,  0.0089,  ...,  0.0121,  0.0004,  0.0030],
        [ 0.0082, -0.0048,  0.0043,  ..., -0.0050, -0.0053,  0.0038],
        ...,
        [-0.0052, -0.0054,  0.0154,  ...,  0.0240, -0.0082,  0.0156],
        [-0.0088, -0.0093, -0.0045,  ...,  0.0009,  0.0111, -0.0013],
        [ 0.0077,  0.0055,  0.0029,  ..., -0.0096,  0.0159,  0.0154]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0303,  0.3572,  0.8135,  ..., -1.6543,  1.1680,  2.6270]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 20:47:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The buffalo falls into the category of bovid
The chinchilla falls into the category of rodent
The chicken falls into the category of fowl
The tiger falls into the category of feline
The hawk falls into the category of raptor
The chimpanzee falls into the category of primate
The porcupine falls into the category of rodent
The fox falls into the category of
2024-07-22 20:47:19 root INFO     [order_1_approx] starting weight calculation for The chinchilla falls into the category of rodent
The tiger falls into the category of feline
The buffalo falls into the category of bovid
The chimpanzee falls into the category of primate
The fox falls into the category of canine
The porcupine falls into the category of rodent
The chicken falls into the category of fowl
The hawk falls into the category of
2024-07-22 20:47:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 20:51:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1670, -0.7222, -2.0254,  ...,  0.4207, -0.4761,  0.1355],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1709,  0.8813, -4.1719,  ...,  1.5723, -1.3281,  0.5166],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.0658e-02, -9.0790e-03,  7.4005e-04,  ...,  4.4899e-03,
          3.0212e-03,  1.0468e-02],
        [ 6.1493e-03,  4.0436e-03,  1.0841e-02,  ...,  3.7079e-03,
          5.8670e-03,  2.7428e-03],
        [-3.5267e-03, -1.1993e-02,  5.5313e-03,  ..., -2.3918e-03,
         -3.0785e-03,  6.3934e-03],
        ...,
        [-4.6272e-03, -1.2749e-02, -1.2360e-03,  ...,  7.8812e-03,
         -4.8370e-03,  3.4275e-03],
        [-9.9182e-03, -8.5907e-03,  1.8330e-03,  ...,  7.3700e-03,
          1.2755e-05,  6.6948e-04],
        [ 6.9046e-04, -4.1580e-03,  1.3618e-03,  ..., -1.2695e-02,
          3.4103e-03,  1.6174e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6387,  1.6816, -5.1484,  ...,  1.9053, -1.3604,  0.6372]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 20:51:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The chinchilla falls into the category of rodent
The tiger falls into the category of feline
The buffalo falls into the category of bovid
The chimpanzee falls into the category of primate
The fox falls into the category of canine
The porcupine falls into the category of rodent
The chicken falls into the category of fowl
The hawk falls into the category of
2024-07-22 20:51:17 root INFO     [order_1_approx] starting weight calculation for The chicken falls into the category of fowl
The porcupine falls into the category of rodent
The chimpanzee falls into the category of primate
The fox falls into the category of canine
The hawk falls into the category of raptor
The tiger falls into the category of feline
The buffalo falls into the category of bovid
The chinchilla falls into the category of
2024-07-22 20:51:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 20:55:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2803, -1.1133, -0.7295,  ...,  0.0308, -0.6152,  0.1174],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.5703, -2.6895,  0.4258,  ...,  2.2148,  0.2942,  1.2148],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0048, -0.0145,  0.0097,  ...,  0.0127, -0.0129, -0.0060],
        [-0.0065,  0.0005,  0.0087,  ...,  0.0145,  0.0045, -0.0051],
        [-0.0068,  0.0044, -0.0068,  ..., -0.0020,  0.0060,  0.0087],
        ...,
        [-0.0117, -0.0106, -0.0088,  ...,  0.0103,  0.0014,  0.0274],
        [-0.0235, -0.0041, -0.0149,  ..., -0.0045,  0.0135,  0.0169],
        [ 0.0133,  0.0009, -0.0023,  ..., -0.0015,  0.0038,  0.0023]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.6953, -2.5547,  0.0776,  ...,  1.9629, -0.3982,  2.2129]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 20:55:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The chicken falls into the category of fowl
The porcupine falls into the category of rodent
The chimpanzee falls into the category of primate
The fox falls into the category of canine
The hawk falls into the category of raptor
The tiger falls into the category of feline
The buffalo falls into the category of bovid
The chinchilla falls into the category of
2024-07-22 20:55:16 root INFO     [order_1_approx] starting weight calculation for The chimpanzee falls into the category of primate
The porcupine falls into the category of rodent
The hawk falls into the category of raptor
The tiger falls into the category of feline
The chinchilla falls into the category of rodent
The chicken falls into the category of fowl
The fox falls into the category of canine
The buffalo falls into the category of
2024-07-22 20:55:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 20:59:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9360, -1.1836, -0.4551,  ...,  1.4570, -0.5205,  0.4265],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.0234, -0.2272, -1.5342,  ..., -2.1992, -0.1545,  1.5254],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0022, -0.0154, -0.0004,  ...,  0.0004, -0.0013,  0.0039],
        [-0.0065,  0.0040,  0.0015,  ...,  0.0110,  0.0124, -0.0135],
        [ 0.0079, -0.0069,  0.0033,  ...,  0.0005,  0.0062, -0.0019],
        ...,
        [-0.0098, -0.0119, -0.0130,  ..., -0.0016, -0.0013,  0.0041],
        [-0.0126, -0.0156, -0.0127,  ..., -0.0026,  0.0127, -0.0043],
        [ 0.0006,  0.0015,  0.0071,  ..., -0.0097, -0.0023,  0.0058]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.1211, -0.1016, -2.1934,  ..., -2.2402, -0.5049,  1.2656]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 20:59:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The chimpanzee falls into the category of primate
The porcupine falls into the category of rodent
The hawk falls into the category of raptor
The tiger falls into the category of feline
The chinchilla falls into the category of rodent
The chicken falls into the category of fowl
The fox falls into the category of canine
The buffalo falls into the category of
2024-07-22 20:59:15 root INFO     [order_1_approx] starting weight calculation for The buffalo falls into the category of bovid
The chicken falls into the category of fowl
The chimpanzee falls into the category of primate
The porcupine falls into the category of rodent
The fox falls into the category of canine
The hawk falls into the category of raptor
The chinchilla falls into the category of rodent
The tiger falls into the category of
2024-07-22 20:59:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 21:03:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4724, -0.4702,  0.2991,  ...,  1.2637,  0.4109, -0.5083],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4883,  1.4141,  0.9375,  ..., -1.7793, -5.0312,  2.0469],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.6373e-02, -1.7883e-02,  1.7319e-03,  ..., -2.2297e-03,
          9.1400e-03, -6.6109e-03],
        [-1.6212e-03,  7.6294e-06,  1.0651e-02,  ...,  6.6223e-03,
         -1.1398e-02, -1.1749e-03],
        [ 9.3307e-03, -9.6664e-03,  2.5787e-03,  ..., -7.1716e-03,
         -5.4741e-04,  1.3290e-02],
        ...,
        [ 2.9716e-03, -1.0818e-02, -4.0016e-03,  ...,  1.2657e-02,
         -1.4458e-02,  1.6296e-02],
        [ 7.7486e-05, -2.4433e-03, -3.4523e-03,  ...,  2.3270e-04,
          7.9041e-03, -4.4556e-03],
        [ 7.1945e-03, -3.7594e-03,  8.5831e-03,  ..., -9.2163e-03,
          4.6921e-03,  1.8295e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7871,  2.1797,  0.1665,  ..., -1.5234, -5.8125,  1.9238]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 21:03:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The buffalo falls into the category of bovid
The chicken falls into the category of fowl
The chimpanzee falls into the category of primate
The porcupine falls into the category of rodent
The fox falls into the category of canine
The hawk falls into the category of raptor
The chinchilla falls into the category of rodent
The tiger falls into the category of
2024-07-22 21:03:14 root INFO     total operator prediction time: 1915.2973489761353 seconds
2024-07-22 21:03:14 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on hyponyms - misc
2024-07-22 21:03:14 root INFO     building operator hyponyms - misc
2024-07-22 21:03:15 root INFO     [order_1_approx] starting weight calculation for A more specific term for a dessert is cake
A more specific term for a car is limousine
A more specific term for a dress is gown
A more specific term for a church is chapel
A more specific term for a computer is laptop
A more specific term for a color is white
A more specific term for a poem is haiku
A more specific term for a candy is
2024-07-22 21:03:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 21:07:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6357,  0.2361,  0.1719,  ...,  0.4807,  0.0513,  0.5972],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.4219,  3.6445, -2.3633,  ..., -2.6641,  2.3652,  1.4395],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0128, -0.0098, -0.0152,  ...,  0.0022,  0.0080, -0.0051],
        [-0.0030, -0.0131, -0.0124,  ...,  0.0098, -0.0033,  0.0185],
        [-0.0038, -0.0031,  0.0079,  ..., -0.0054,  0.0209,  0.0089],
        ...,
        [ 0.0165, -0.0200, -0.0037,  ...,  0.0106, -0.0126,  0.0037],
        [-0.0065,  0.0133,  0.0050,  ...,  0.0034,  0.0179, -0.0033],
        [-0.0013,  0.0098,  0.0017,  ...,  0.0060,  0.0044,  0.0114]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.4062,  2.6074, -2.7812,  ..., -2.5000,  1.6152,  0.9619]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 21:07:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a dessert is cake
A more specific term for a car is limousine
A more specific term for a dress is gown
A more specific term for a church is chapel
A more specific term for a computer is laptop
A more specific term for a color is white
A more specific term for a poem is haiku
A more specific term for a candy is
2024-07-22 21:07:14 root INFO     [order_1_approx] starting weight calculation for A more specific term for a candy is lollipop
A more specific term for a church is chapel
A more specific term for a dress is gown
A more specific term for a car is limousine
A more specific term for a color is white
A more specific term for a computer is laptop
A more specific term for a dessert is cake
A more specific term for a poem is
2024-07-22 21:07:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 21:11:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6841, -0.5605,  0.4146,  ..., -0.4377,  0.1418,  1.1699],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7871,  0.4963, -3.9922,  ..., -5.3750,  3.6113, -7.5547],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-9.9373e-04,  8.6975e-03,  7.1869e-03,  ...,  5.1994e-03,
         -1.1948e-02, -7.8125e-03],
        [ 5.6381e-03,  4.5929e-03,  1.1581e-02,  ...,  6.5002e-03,
         -1.7639e-02,  6.2027e-03],
        [ 1.6846e-02,  3.4752e-03, -1.1765e-02,  ...,  2.9144e-03,
          1.9951e-03,  1.2993e-02],
        ...,
        [ 2.8839e-02,  2.2369e-02, -1.0513e-02,  ..., -9.1171e-04,
         -3.2806e-03,  1.3542e-02],
        [-1.5945e-02,  1.6235e-02,  1.8005e-02,  ...,  3.3779e-03,
         -2.3285e-02, -1.1139e-02],
        [-4.0245e-03, -2.1667e-03, -3.9597e-03,  ..., -8.7738e-05,
          8.8577e-03,  8.5754e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.2812,  0.5581, -4.4023,  ..., -4.9531,  3.4258, -7.3672]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 21:11:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a candy is lollipop
A more specific term for a church is chapel
A more specific term for a dress is gown
A more specific term for a car is limousine
A more specific term for a color is white
A more specific term for a computer is laptop
A more specific term for a dessert is cake
A more specific term for a poem is
2024-07-22 21:11:14 root INFO     [order_1_approx] starting weight calculation for A more specific term for a dress is gown
A more specific term for a computer is laptop
A more specific term for a car is limousine
A more specific term for a color is white
A more specific term for a poem is haiku
A more specific term for a church is chapel
A more specific term for a candy is lollipop
A more specific term for a dessert is
2024-07-22 21:11:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 21:15:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2847,  0.9614,  1.1211,  ...,  0.9424, -0.7456,  0.6318],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.2910,  2.9805, -0.7651,  ...,  3.3867,  0.7915, -0.1758],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.4923e-02, -1.1841e-02,  2.4452e-03,  ..., -2.8305e-03,
          6.6681e-03,  2.1591e-03],
        [-8.8959e-03,  5.4359e-03,  1.3428e-02,  ...,  1.0078e-02,
         -1.1047e-02,  1.7517e-02],
        [-6.4659e-04, -4.9896e-03,  1.5587e-02,  ...,  1.5182e-02,
         -9.3460e-03,  7.1907e-03],
        ...,
        [ 1.8616e-02, -1.0704e-02,  8.3389e-03,  ...,  1.3992e-02,
         -1.8707e-02, -2.0828e-03],
        [-1.7090e-02,  3.7403e-03,  1.3351e-05,  ...,  2.8152e-03,
          1.0223e-02,  2.9335e-03],
        [-3.0228e-02, -1.2817e-03,  5.7068e-03,  ...,  1.8578e-03,
          1.5793e-02,  2.7206e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.8555,  3.2129, -0.7852,  ...,  2.0977,  1.2305, -0.0679]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 21:15:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a dress is gown
A more specific term for a computer is laptop
A more specific term for a car is limousine
A more specific term for a color is white
A more specific term for a poem is haiku
A more specific term for a church is chapel
A more specific term for a candy is lollipop
A more specific term for a dessert is
2024-07-22 21:15:11 root INFO     [order_1_approx] starting weight calculation for A more specific term for a dessert is cake
A more specific term for a computer is laptop
A more specific term for a poem is haiku
A more specific term for a dress is gown
A more specific term for a candy is lollipop
A more specific term for a car is limousine
A more specific term for a church is chapel
A more specific term for a color is
2024-07-22 21:15:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 21:19:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0137,  0.8252,  0.8828,  ..., -0.4014,  0.8750,  1.0117],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.6094, -2.4844, -1.8330,  ...,  0.5366,  3.4824,  0.4155],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-3.2135e-02, -2.8564e-02,  2.0370e-03,  ..., -4.4983e-02,
         -1.5205e-02, -4.6158e-04],
        [ 3.2806e-03,  1.0559e-02,  3.2539e-03,  ...,  1.3771e-02,
          3.0518e-05,  4.6120e-03],
        [-1.3779e-02,  2.5040e-02,  3.1433e-03,  ..., -3.6469e-03,
          1.9836e-03,  2.0416e-02],
        ...,
        [-1.5488e-03, -2.5909e-02, -4.7836e-03,  ...,  8.6365e-03,
         -9.1095e-03,  5.0812e-03],
        [-7.2403e-03, -7.5836e-03, -2.8763e-03,  ...,  3.2806e-03,
         -2.2064e-02,  1.2672e-02],
        [ 3.3203e-02, -1.7136e-02,  3.8815e-03,  ...,  2.9541e-02,
          1.2848e-02,  2.8076e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.0234, -1.9922, -0.5830,  ...,  0.4666,  3.4043,  0.6758]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 21:19:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a dessert is cake
A more specific term for a computer is laptop
A more specific term for a poem is haiku
A more specific term for a dress is gown
A more specific term for a candy is lollipop
A more specific term for a car is limousine
A more specific term for a church is chapel
A more specific term for a color is
2024-07-22 21:19:09 root INFO     [order_1_approx] starting weight calculation for A more specific term for a color is white
A more specific term for a candy is lollipop
A more specific term for a dessert is cake
A more specific term for a computer is laptop
A more specific term for a car is limousine
A more specific term for a church is chapel
A more specific term for a poem is haiku
A more specific term for a dress is
2024-07-22 21:19:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 21:23:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1702, -0.3838,  0.2578,  ...,  0.9072, -0.4333,  0.5303],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5396, -1.4932, -3.2285,  ...,  1.1514, -0.5420, -0.8091],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0185, -0.0087, -0.0089,  ...,  0.0078, -0.0225,  0.0033],
        [-0.0057,  0.0019, -0.0038,  ...,  0.0074,  0.0058,  0.0011],
        [-0.0042,  0.0022,  0.0219,  ...,  0.0079, -0.0112,  0.0210],
        ...,
        [-0.0202,  0.0004,  0.0112,  ...,  0.0330,  0.0130, -0.0003],
        [ 0.0116,  0.0210, -0.0006,  ...,  0.0050, -0.0082,  0.0102],
        [-0.0226, -0.0039,  0.0219,  ...,  0.0130,  0.0278,  0.0247]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.4629, -1.3135, -3.3320,  ...,  1.6074, -2.1562, -0.6743]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 21:23:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a color is white
A more specific term for a candy is lollipop
A more specific term for a dessert is cake
A more specific term for a computer is laptop
A more specific term for a car is limousine
A more specific term for a church is chapel
A more specific term for a poem is haiku
A more specific term for a dress is
2024-07-22 21:23:06 root INFO     [order_1_approx] starting weight calculation for A more specific term for a candy is lollipop
A more specific term for a dessert is cake
A more specific term for a dress is gown
A more specific term for a poem is haiku
A more specific term for a church is chapel
A more specific term for a computer is laptop
A more specific term for a color is white
A more specific term for a car is
2024-07-22 21:23:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 21:27:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6221, -0.1310, -0.6436,  ..., -0.0132, -0.3896,  0.7866],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-5.8984, -2.8242,  0.1162,  ...,  1.6426,  1.2334, -1.6719],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0077,  0.0002, -0.0114,  ..., -0.0027, -0.0043,  0.0010],
        [ 0.0024,  0.0167, -0.0076,  ...,  0.0019, -0.0066, -0.0027],
        [-0.0141, -0.0026, -0.0110,  ..., -0.0137, -0.0025,  0.0216],
        ...,
        [-0.0017, -0.0209,  0.0014,  ...,  0.0023, -0.0020, -0.0009],
        [-0.0052,  0.0048, -0.0198,  ..., -0.0055, -0.0084,  0.0267],
        [-0.0091,  0.0054, -0.0061,  ..., -0.0198,  0.0049,  0.0221]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-6.6016, -2.1875,  0.5156,  ...,  1.4219,  1.2832, -1.5039]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 21:27:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a candy is lollipop
A more specific term for a dessert is cake
A more specific term for a dress is gown
A more specific term for a poem is haiku
A more specific term for a church is chapel
A more specific term for a computer is laptop
A more specific term for a color is white
A more specific term for a car is
2024-07-22 21:27:06 root INFO     [order_1_approx] starting weight calculation for A more specific term for a church is chapel
A more specific term for a dress is gown
A more specific term for a color is white
A more specific term for a poem is haiku
A more specific term for a dessert is cake
A more specific term for a car is limousine
A more specific term for a candy is lollipop
A more specific term for a computer is
2024-07-22 21:27:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 21:31:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0903, -0.1041,  0.1064,  ...,  0.8359,  0.5205,  1.7461],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3945, -1.3848, -0.6729,  ...,  0.8818,  3.2617,  3.0254],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0128, -0.0038, -0.0057,  ..., -0.0001, -0.0128,  0.0176],
        [-0.0043,  0.0159, -0.0064,  ...,  0.0026, -0.0095, -0.0155],
        [-0.0025,  0.0171, -0.0062,  ...,  0.0049, -0.0151, -0.0004],
        ...,
        [-0.0122,  0.0001,  0.0037,  ...,  0.0091, -0.0081, -0.0003],
        [ 0.0054, -0.0201, -0.0046,  ...,  0.0105, -0.0079,  0.0056],
        [-0.0151,  0.0103,  0.0085,  ..., -0.0184,  0.0111,  0.0109]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7432, -1.7441, -0.7510,  ...,  0.9639,  3.4844,  2.5000]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 21:31:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a church is chapel
A more specific term for a dress is gown
A more specific term for a color is white
A more specific term for a poem is haiku
A more specific term for a dessert is cake
A more specific term for a car is limousine
A more specific term for a candy is lollipop
A more specific term for a computer is
2024-07-22 21:31:03 root INFO     [order_1_approx] starting weight calculation for A more specific term for a dessert is cake
A more specific term for a poem is haiku
A more specific term for a color is white
A more specific term for a dress is gown
A more specific term for a computer is laptop
A more specific term for a candy is lollipop
A more specific term for a car is limousine
A more specific term for a church is
2024-07-22 21:31:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 21:34:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5786, -0.8086, -0.2605,  ...,  0.8672, -0.3540,  0.7954],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7324,  1.6191,  1.2559,  ...,  0.0767,  0.7129, -0.3457],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0015, -0.0169,  0.0164,  ...,  0.0006, -0.0217, -0.0093],
        [ 0.0182,  0.0117,  0.0075,  ...,  0.0185,  0.0006, -0.0038],
        [-0.0091, -0.0175,  0.0003,  ...,  0.0234, -0.0015,  0.0146],
        ...,
        [-0.0082, -0.0090,  0.0062,  ...,  0.0159, -0.0133, -0.0023],
        [ 0.0073,  0.0114, -0.0040,  ...,  0.0208, -0.0043, -0.0028],
        [-0.0008,  0.0177, -0.0073,  ...,  0.0044, -0.0071,  0.0148]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5498,  1.5420,  1.4014,  ..., -0.2295,  0.5981, -0.1454]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 21:34:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a dessert is cake
A more specific term for a poem is haiku
A more specific term for a color is white
A more specific term for a dress is gown
A more specific term for a computer is laptop
A more specific term for a candy is lollipop
A more specific term for a car is limousine
A more specific term for a church is
2024-07-22 21:34:59 root INFO     total operator prediction time: 1904.7990202903748 seconds
2024-07-22 21:34:59 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on antonyms - binary
2024-07-22 21:34:59 root INFO     building operator antonyms - binary
2024-07-22 21:34:59 root INFO     [order_1_approx] starting weight calculation for The opposite of dive is emerge
The opposite of dynamic is static
The opposite of under is over
The opposite of front is back
The opposite of drop is lift
The opposite of west is east
The opposite of below is above
The opposite of previously is
2024-07-22 21:34:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 21:38:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6602,  0.6655,  1.2236,  ..., -0.2703,  0.1736, -0.2365],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9023, -2.6992,  1.6816,  ...,  3.8223,  2.8594,  0.8730],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0087,  0.0035, -0.0067,  ..., -0.0094,  0.0041,  0.0065],
        [-0.0178, -0.0012, -0.0312,  ..., -0.0132,  0.0256,  0.0069],
        [-0.0103, -0.0101, -0.0304,  ...,  0.0229,  0.0097, -0.0099],
        ...,
        [-0.0377, -0.0199,  0.0092,  ..., -0.0015, -0.0092, -0.0006],
        [-0.0200, -0.0134,  0.0250,  ..., -0.0221, -0.0251,  0.0141],
        [ 0.0116, -0.0036,  0.0090,  ...,  0.0087,  0.0219, -0.0355]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5840, -2.6348,  1.0869,  ...,  3.7148,  2.7852,  0.4668]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 21:38:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of dive is emerge
The opposite of dynamic is static
The opposite of under is over
The opposite of front is back
The opposite of drop is lift
The opposite of west is east
The opposite of below is above
The opposite of previously is
2024-07-22 21:38:59 root INFO     [order_1_approx] starting weight calculation for The opposite of previously is subsequently
The opposite of below is above
The opposite of front is back
The opposite of west is east
The opposite of under is over
The opposite of dynamic is static
The opposite of dive is emerge
The opposite of drop is
2024-07-22 21:38:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 21:42:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0476, -0.0093,  1.4912,  ...,  0.2651,  0.1055,  0.4380],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0723, -4.0977, -0.0088,  ...,  0.4563, -0.6143,  3.7188],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-6.1035e-05, -1.9577e-02,  3.9711e-03,  ...,  1.2695e-02,
          4.5891e-03, -1.3191e-02],
        [-4.6425e-03, -7.8011e-04,  3.1738e-03,  ..., -1.2177e-02,
         -2.0584e-02,  1.9135e-02],
        [-1.3107e-02,  1.1284e-02, -8.8654e-03,  ..., -1.8738e-02,
          1.2817e-03,  5.1727e-03],
        ...,
        [ 6.1340e-03, -8.2245e-03, -1.6815e-02,  ..., -1.5974e-03,
         -5.1651e-03,  8.8577e-03],
        [ 1.7738e-03, -5.5923e-03,  1.9379e-03,  ...,  6.4697e-03,
          1.3969e-02, -1.2589e-02],
        [-9.1019e-03,  3.3569e-03,  5.7564e-03,  ...,  6.1951e-03,
         -9.0561e-03, -2.7828e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1758, -3.5840,  0.2561,  ...,  1.7383, -0.6313,  3.9707]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 21:42:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of previously is subsequently
The opposite of below is above
The opposite of front is back
The opposite of west is east
The opposite of under is over
The opposite of dynamic is static
The opposite of dive is emerge
The opposite of drop is
2024-07-22 21:42:58 root INFO     [order_1_approx] starting weight calculation for The opposite of previously is subsequently
The opposite of below is above
The opposite of dynamic is static
The opposite of drop is lift
The opposite of west is east
The opposite of dive is emerge
The opposite of front is back
The opposite of under is
2024-07-22 21:42:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 21:46:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0590, -0.5332,  0.6235,  ...,  0.5864,  0.3254,  0.5000],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4954, -6.6953,  0.1895,  ...,  0.5396,  3.9785, -1.2188],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0063, -0.0150, -0.0107,  ..., -0.0001, -0.0139, -0.0058],
        [-0.0259,  0.0106, -0.0145,  ..., -0.0005,  0.0018,  0.0251],
        [ 0.0150, -0.0240, -0.0325,  ...,  0.0125, -0.0112,  0.0219],
        ...,
        [-0.0074, -0.0195,  0.0046,  ..., -0.0015,  0.0156,  0.0029],
        [-0.0212, -0.0062,  0.0266,  ..., -0.0256,  0.0012, -0.0065],
        [-0.0148, -0.0010, -0.0214,  ..., -0.0334,  0.0045, -0.0264]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0273, -7.1172,  0.3203,  ...,  0.1228,  4.6172, -0.6904]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 21:46:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of previously is subsequently
The opposite of below is above
The opposite of dynamic is static
The opposite of drop is lift
The opposite of west is east
The opposite of dive is emerge
The opposite of front is back
The opposite of under is
2024-07-22 21:46:57 root INFO     [order_1_approx] starting weight calculation for The opposite of under is over
The opposite of drop is lift
The opposite of dynamic is static
The opposite of front is back
The opposite of below is above
The opposite of west is east
The opposite of previously is subsequently
The opposite of dive is
2024-07-22 21:46:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 21:50:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9209, -1.5713,  1.0840,  ..., -0.3198, -0.6348, -0.2063],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0818, -2.4902,  4.2344,  ..., -3.0234, -1.7520, -1.7803],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0363, -0.0147, -0.0369,  ...,  0.0132,  0.0050, -0.0004],
        [-0.0230,  0.0256,  0.0017,  ..., -0.0044, -0.0101,  0.0147],
        [-0.0178, -0.0163, -0.0443,  ..., -0.0110, -0.0146,  0.0088],
        ...,
        [ 0.0073, -0.0053, -0.0141,  ...,  0.0033,  0.0066,  0.0076],
        [-0.0147, -0.0084,  0.0113,  ..., -0.0072, -0.0276, -0.0186],
        [ 0.0226,  0.0227, -0.0060,  ...,  0.0066,  0.0035, -0.0003]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6758, -3.1602,  4.3789,  ..., -2.3730, -0.9297, -2.0078]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 21:50:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of under is over
The opposite of drop is lift
The opposite of dynamic is static
The opposite of front is back
The opposite of below is above
The opposite of west is east
The opposite of previously is subsequently
The opposite of dive is
2024-07-22 21:50:57 root INFO     [order_1_approx] starting weight calculation for The opposite of west is east
The opposite of below is above
The opposite of drop is lift
The opposite of dive is emerge
The opposite of under is over
The opposite of dynamic is static
The opposite of previously is subsequently
The opposite of front is
2024-07-22 21:50:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 21:54:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0347, -0.5195,  0.3276,  ...,  0.6328,  0.4265,  1.0215],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7681, -0.6553, -0.9131,  ...,  3.5664,  3.4121,  1.4619],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0065, -0.0003, -0.0048,  ..., -0.0025, -0.0152, -0.0173],
        [-0.0050,  0.0091, -0.0195,  ..., -0.0042, -0.0146,  0.0065],
        [-0.0263, -0.0099, -0.0069,  ..., -0.0114, -0.0062,  0.0180],
        ...,
        [-0.0127,  0.0054,  0.0084,  ...,  0.0147,  0.0144,  0.0150],
        [-0.0199, -0.0126,  0.0267,  ..., -0.0048, -0.0143, -0.0150],
        [-0.0154, -0.0284, -0.0017,  ..., -0.0218,  0.0161,  0.0109]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5615, -1.5635, -0.2896,  ...,  2.3906,  2.7266,  0.8066]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 21:54:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of west is east
The opposite of below is above
The opposite of drop is lift
The opposite of dive is emerge
The opposite of under is over
The opposite of dynamic is static
The opposite of previously is subsequently
The opposite of front is
2024-07-22 21:54:55 root INFO     [order_1_approx] starting weight calculation for The opposite of below is above
The opposite of under is over
The opposite of front is back
The opposite of drop is lift
The opposite of west is east
The opposite of previously is subsequently
The opposite of dive is emerge
The opposite of dynamic is
2024-07-22 21:54:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 21:58:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2627, -0.8662,  1.1172,  ..., -0.4573, -0.1450, -0.6465],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9468, -0.2192, -2.9336,  ..., -1.9648,  4.4844, -1.4551],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0093, -0.0153,  0.0006,  ...,  0.0018, -0.0286,  0.0107],
        [ 0.0096,  0.0029, -0.0157,  ..., -0.0091, -0.0013,  0.0061],
        [ 0.0045, -0.0022, -0.0400,  ..., -0.0139, -0.0021, -0.0100],
        ...,
        [-0.0003, -0.0109,  0.0036,  ...,  0.0173, -0.0020,  0.0127],
        [-0.0002, -0.0220, -0.0011,  ...,  0.0006, -0.0063, -0.0145],
        [ 0.0150,  0.0005, -0.0093,  ...,  0.0079,  0.0084,  0.0224]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1196,  1.3105, -3.0254,  ..., -2.0996,  4.7812, -1.6660]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 21:58:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of below is above
The opposite of under is over
The opposite of front is back
The opposite of drop is lift
The opposite of west is east
The opposite of previously is subsequently
The opposite of dive is emerge
The opposite of dynamic is
2024-07-22 21:58:54 root INFO     [order_1_approx] starting weight calculation for The opposite of drop is lift
The opposite of previously is subsequently
The opposite of under is over
The opposite of front is back
The opposite of below is above
The opposite of dynamic is static
The opposite of dive is emerge
The opposite of west is
2024-07-22 21:58:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 22:02:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2551, -0.5781, -0.8735,  ..., -0.7090,  1.4473,  0.4878],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.9570, -2.6230, -0.2578,  ..., -2.6973,  1.2598, -1.4512],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0094, -0.0026,  0.0030,  ...,  0.0062, -0.0157,  0.0147],
        [-0.0145, -0.0038, -0.0039,  ...,  0.0191,  0.0002, -0.0009],
        [ 0.0023,  0.0033, -0.0058,  ..., -0.0182, -0.0047,  0.0217],
        ...,
        [ 0.0086, -0.0099, -0.0069,  ..., -0.0114, -0.0008,  0.0095],
        [ 0.0169,  0.0013,  0.0156,  ...,  0.0033, -0.0039,  0.0015],
        [-0.0027,  0.0009, -0.0054,  ...,  0.0080,  0.0197, -0.0066]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.6113, -2.5840,  1.2666,  ..., -2.5645,  1.7988, -0.5962]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 22:02:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of drop is lift
The opposite of previously is subsequently
The opposite of under is over
The opposite of front is back
The opposite of below is above
The opposite of dynamic is static
The opposite of dive is emerge
The opposite of west is
2024-07-22 22:02:48 root INFO     [order_1_approx] starting weight calculation for The opposite of drop is lift
The opposite of under is over
The opposite of dynamic is static
The opposite of previously is subsequently
The opposite of west is east
The opposite of dive is emerge
The opposite of front is back
The opposite of below is
2024-07-22 22:02:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 22:06:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1304, -0.9980,  0.6372,  ..., -0.0248,  0.7690, -0.1633],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.2148, -5.3281,  1.5176,  ...,  0.0406,  5.9141,  2.0078],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0335, -0.0109, -0.0114,  ...,  0.0088,  0.0061, -0.0266],
        [-0.0311, -0.0175, -0.0131,  ...,  0.0141,  0.0029,  0.0132],
        [-0.0009,  0.0127, -0.0325,  ..., -0.0006, -0.0085,  0.0301],
        ...,
        [-0.0186, -0.0041,  0.0087,  ..., -0.0016,  0.0170,  0.0156],
        [-0.0080, -0.0011,  0.0332,  ..., -0.0336, -0.0231,  0.0024],
        [-0.0204, -0.0083, -0.0169,  ..., -0.0054,  0.0025, -0.0157]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.1914, -5.0742,  2.9414,  ...,  1.0303,  6.7461,  2.5469]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 22:06:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of drop is lift
The opposite of under is over
The opposite of dynamic is static
The opposite of previously is subsequently
The opposite of west is east
The opposite of dive is emerge
The opposite of front is back
The opposite of below is
2024-07-22 22:06:46 root INFO     total operator prediction time: 1906.526626110077 seconds
2024-07-22 22:06:46 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - member
2024-07-22 22:06:46 root INFO     building operator meronyms - member
2024-07-22 22:06:46 root INFO     [order_1_approx] starting weight calculation for A member is a member of a club
A letter is a member of a alphabet
A person is a member of a society
A christian is a member of a congregation
A parishioner is a member of a parish
A book is a member of a library
A college is a member of a university
A car is a member of a
2024-07-22 22:06:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 22:10:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4636, -0.0698, -0.9482,  ...,  0.1650, -0.0447,  0.1884],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.0547, -2.0703, -3.7695,  ...,  1.0107,  0.9624,  0.7871],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0173, -0.0118,  0.0019,  ..., -0.0088, -0.0082, -0.0056],
        [-0.0070,  0.0006,  0.0064,  ...,  0.0039, -0.0016,  0.0034],
        [ 0.0045,  0.0140, -0.0084,  ..., -0.0110, -0.0025, -0.0064],
        ...,
        [-0.0199, -0.0036, -0.0087,  ..., -0.0090, -0.0096,  0.0027],
        [-0.0002, -0.0130,  0.0023,  ..., -0.0063, -0.0114, -0.0136],
        [ 0.0011, -0.0055, -0.0044,  ..., -0.0027,  0.0138, -0.0099]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.2461, -2.1777, -4.4023,  ...,  1.7754,  0.7686,  0.2554]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 22:10:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A member is a member of a club
A letter is a member of a alphabet
A person is a member of a society
A christian is a member of a congregation
A parishioner is a member of a parish
A book is a member of a library
A college is a member of a university
A car is a member of a
2024-07-22 22:10:43 root INFO     [order_1_approx] starting weight calculation for A person is a member of a society
A letter is a member of a alphabet
A parishioner is a member of a parish
A college is a member of a university
A christian is a member of a congregation
A car is a member of a train
A book is a member of a library
A member is a member of a
2024-07-22 22:10:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 22:14:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6006, -0.1619,  1.5820,  ..., -0.8354,  0.2764,  0.4902],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7168, -0.9722, -0.1445,  ..., -1.4541,  1.6768,  2.3828],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0024, -0.0049,  0.0010,  ...,  0.0008, -0.0029,  0.0020],
        [-0.0041, -0.0013, -0.0002,  ...,  0.0011,  0.0012, -0.0045],
        [ 0.0004, -0.0032,  0.0007,  ...,  0.0022, -0.0047, -0.0004],
        ...,
        [-0.0010, -0.0019,  0.0024,  ..., -0.0021, -0.0003, -0.0021],
        [ 0.0019, -0.0022,  0.0025,  ...,  0.0013, -0.0006, -0.0026],
        [ 0.0060,  0.0039, -0.0010,  ..., -0.0042,  0.0019,  0.0018]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7236, -1.1865,  0.1357,  ..., -1.2852,  1.9355,  2.1973]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 22:14:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A person is a member of a society
A letter is a member of a alphabet
A parishioner is a member of a parish
A college is a member of a university
A christian is a member of a congregation
A car is a member of a train
A book is a member of a library
A member is a member of a
2024-07-22 22:14:43 root INFO     [order_1_approx] starting weight calculation for A member is a member of a club
A college is a member of a university
A parishioner is a member of a parish
A car is a member of a train
A person is a member of a society
A christian is a member of a congregation
A letter is a member of a alphabet
A book is a member of a
2024-07-22 22:14:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 22:18:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0820, -0.1409,  0.0588,  ...,  0.5005,  0.1663, -0.1213],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8169,  0.5620, -0.3896,  ...,  2.4316, -1.0400,  0.6182],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0155, -0.0135, -0.0024,  ..., -0.0057,  0.0129,  0.0068],
        [ 0.0130,  0.0140,  0.0120,  ...,  0.0007,  0.0077, -0.0101],
        [ 0.0161, -0.0025,  0.0090,  ...,  0.0020, -0.0143,  0.0018],
        ...,
        [-0.0075, -0.0176,  0.0013,  ..., -0.0003, -0.0131,  0.0178],
        [ 0.0216, -0.0077, -0.0015,  ...,  0.0176, -0.0015, -0.0021],
        [ 0.0094, -0.0069,  0.0100,  ...,  0.0085,  0.0056, -0.0175]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0156,  0.5786, -0.3765,  ...,  2.6055, -0.9404,  0.1089]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 22:18:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A member is a member of a club
A college is a member of a university
A parishioner is a member of a parish
A car is a member of a train
A person is a member of a society
A christian is a member of a congregation
A letter is a member of a alphabet
A book is a member of a
2024-07-22 22:18:43 root INFO     [order_1_approx] starting weight calculation for A letter is a member of a alphabet
A person is a member of a society
A member is a member of a club
A book is a member of a library
A parishioner is a member of a parish
A car is a member of a train
A christian is a member of a congregation
A college is a member of a
2024-07-22 22:18:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 22:22:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1165,  0.5601,  0.3467,  ...,  0.3364,  1.1787,  0.3625],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7554,  0.4028, -0.3589,  ..., -0.2461,  1.6611,  0.8320],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0031, -0.0114,  0.0050,  ..., -0.0050, -0.0018,  0.0019],
        [ 0.0072, -0.0011,  0.0025,  ..., -0.0001,  0.0064, -0.0143],
        [ 0.0159,  0.0027, -0.0132,  ...,  0.0091, -0.0106, -0.0050],
        ...,
        [-0.0089, -0.0060, -0.0008,  ...,  0.0079, -0.0055,  0.0087],
        [ 0.0035, -0.0155,  0.0018,  ...,  0.0068, -0.0067, -0.0023],
        [-0.0122,  0.0056, -0.0090,  ...,  0.0044,  0.0166, -0.0094]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.4004,  1.0586, -1.1211,  ..., -0.7046,  1.6953,  1.0391]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 22:22:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A letter is a member of a alphabet
A person is a member of a society
A member is a member of a club
A book is a member of a library
A parishioner is a member of a parish
A car is a member of a train
A christian is a member of a congregation
A college is a member of a
2024-07-22 22:22:44 root INFO     [order_1_approx] starting weight calculation for A letter is a member of a alphabet
A parishioner is a member of a parish
A christian is a member of a congregation
A member is a member of a club
A college is a member of a university
A book is a member of a library
A car is a member of a train
A person is a member of a
2024-07-22 22:22:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 22:26:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.0055, 0.6709, 0.7725,  ..., 1.1318, 1.1309, 0.2698], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.5957, -1.9072, -1.4893,  ..., -3.5527,  1.9531,  2.3281],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0087, -0.0109,  0.0010,  ...,  0.0025, -0.0069,  0.0033],
        [-0.0133,  0.0085,  0.0070,  ..., -0.0060, -0.0052, -0.0057],
        [ 0.0009,  0.0046, -0.0104,  ..., -0.0010,  0.0036,  0.0104],
        ...,
        [-0.0035,  0.0014,  0.0039,  ..., -0.0022, -0.0086, -0.0046],
        [ 0.0004, -0.0058,  0.0047,  ...,  0.0002, -0.0150, -0.0080],
        [-0.0114,  0.0019, -0.0049,  ...,  0.0121,  0.0064, -0.0049]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0762, -1.9648, -1.6387,  ..., -3.3145,  2.4355,  1.9990]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 22:26:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A letter is a member of a alphabet
A parishioner is a member of a parish
A christian is a member of a congregation
A member is a member of a club
A college is a member of a university
A book is a member of a library
A car is a member of a train
A person is a member of a
2024-07-22 22:26:44 root INFO     [order_1_approx] starting weight calculation for A person is a member of a society
A parishioner is a member of a parish
A car is a member of a train
A member is a member of a club
A college is a member of a university
A book is a member of a library
A letter is a member of a alphabet
A christian is a member of a
2024-07-22 22:26:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 22:30:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1058, -0.9941, -0.5752,  ...,  0.1245, -0.8281,  0.0386],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4248, -0.3118, -1.8896,  ...,  0.7334, -1.2656, -2.3945],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0017,  0.0008,  0.0026,  ..., -0.0009,  0.0002, -0.0079],
        [-0.0055, -0.0074, -0.0012,  ..., -0.0017,  0.0118, -0.0115],
        [ 0.0029,  0.0093,  0.0033,  ...,  0.0011,  0.0043, -0.0031],
        ...,
        [-0.0013, -0.0048,  0.0065,  ...,  0.0041, -0.0107,  0.0003],
        [-0.0012, -0.0051, -0.0018,  ...,  0.0041, -0.0184, -0.0033],
        [-0.0074, -0.0041, -0.0005,  ...,  0.0006, -0.0005, -0.0074]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5405, -0.1492, -2.5859,  ...,  1.4355, -1.6738, -2.5859]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 22:30:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A person is a member of a society
A parishioner is a member of a parish
A car is a member of a train
A member is a member of a club
A college is a member of a university
A book is a member of a library
A letter is a member of a alphabet
A christian is a member of a
2024-07-22 22:30:45 root INFO     [order_1_approx] starting weight calculation for A book is a member of a library
A christian is a member of a congregation
A car is a member of a train
A person is a member of a society
A member is a member of a club
A letter is a member of a alphabet
A college is a member of a university
A parishioner is a member of a
2024-07-22 22:30:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 22:34:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.6494, 0.0000, 1.5674,  ..., 1.2812, 0.2983, 0.9248], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.0703,  1.1816,  3.2852,  ..., -0.6152,  0.7246,  1.4609],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.1433e-03,  4.6005e-03,  2.4166e-03,  ...,  3.8185e-03,
         -6.4087e-03,  3.0422e-04],
        [-6.4011e-03,  1.6556e-02, -6.4850e-04,  ...,  1.0139e-02,
          2.5520e-03,  5.0468e-03],
        [ 9.6283e-03, -5.5313e-03,  5.6877e-03,  ...,  9.9945e-04,
          7.7248e-03, -1.0281e-03],
        ...,
        [-1.2390e-02,  2.9831e-03,  8.3637e-04,  ..., -3.8147e-04,
         -6.2103e-03,  1.9028e-02],
        [ 5.5313e-04, -1.0536e-02,  8.7433e-03,  ...,  4.6234e-03,
         -2.8992e-04, -1.1963e-02],
        [-1.4439e-03, -7.7515e-03,  4.9591e-05,  ...,  8.1787e-03,
          1.2985e-02, -1.2955e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6875,  0.6440,  3.4023,  ..., -0.1790,  0.8906,  1.2725]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 22:34:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A book is a member of a library
A christian is a member of a congregation
A car is a member of a train
A person is a member of a society
A member is a member of a club
A letter is a member of a alphabet
A college is a member of a university
A parishioner is a member of a
2024-07-22 22:34:42 root INFO     [order_1_approx] starting weight calculation for A book is a member of a library
A christian is a member of a congregation
A member is a member of a club
A college is a member of a university
A person is a member of a society
A car is a member of a train
A parishioner is a member of a parish
A letter is a member of a
2024-07-22 22:34:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 22:38:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8745,  0.7227,  0.2664,  ..., -0.3125, -0.2214,  0.5947],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7896,  1.9209,  0.6104,  ..., -0.6113,  3.9199,  3.1797],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.3813e-02, -8.5144e-03, -4.6272e-03,  ...,  3.3932e-03,
          7.2899e-03, -3.8147e-06],
        [-1.8723e-02,  2.1759e-02,  2.2964e-03,  ...,  1.1787e-02,
         -2.8458e-03,  3.1204e-03],
        [ 1.1124e-02, -1.4481e-02,  2.2888e-03,  ...,  6.4468e-03,
          2.6703e-03,  1.6586e-02],
        ...,
        [ 5.0354e-03, -1.8921e-02, -8.9188e-03,  ...,  7.0763e-03,
         -1.1871e-02,  5.6953e-03],
        [-4.7226e-03, -2.0737e-02, -4.4022e-03,  ..., -5.1765e-03,
          1.2268e-02, -1.1932e-02],
        [-1.1909e-02, -1.7868e-02, -3.8605e-03,  ...,  7.6447e-03,
          5.5695e-03, -8.6060e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5596,  2.6738,  0.1343,  ..., -1.1816,  3.3457,  2.7949]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 22:38:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A book is a member of a library
A christian is a member of a congregation
A member is a member of a club
A college is a member of a university
A person is a member of a society
A car is a member of a train
A parishioner is a member of a parish
A letter is a member of a
2024-07-22 22:38:42 root INFO     total operator prediction time: 1916.3185546398163 seconds
2024-07-22 22:38:42 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on noun - plural_irreg
2024-07-22 22:38:42 root INFO     building operator noun - plural_irreg
2024-07-22 22:38:42 root INFO     [order_1_approx] starting weight calculation for The plural form of secretary is secretaries
The plural form of army is armies
The plural form of country is countries
The plural form of century is centuries
The plural form of loss is losses
The plural form of security is securities
The plural form of analysis is analyses
The plural form of safety is
2024-07-22 22:38:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 22:42:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5537,  0.8066,  0.7046,  ..., -0.3633,  0.3313,  0.5410],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.2930,  7.4492,  5.9688,  ..., -2.9844, -2.2246,  0.9756],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0323, -0.0150,  0.0107,  ..., -0.0169, -0.0180, -0.0146],
        [ 0.0223, -0.0038, -0.0177,  ..., -0.0078,  0.0047,  0.0171],
        [ 0.0059, -0.0141, -0.0093,  ..., -0.0088,  0.0206,  0.0140],
        ...,
        [-0.0230, -0.0292,  0.0017,  ..., -0.0003,  0.0135, -0.0381],
        [-0.0145, -0.0211,  0.0005,  ..., -0.0018, -0.0239, -0.0103],
        [-0.0133,  0.0082,  0.0082,  ..., -0.0049, -0.0018, -0.0120]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.7539,  7.1758,  5.2031,  ..., -2.8867, -2.3301,  0.2812]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 22:42:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of secretary is secretaries
The plural form of army is armies
The plural form of country is countries
The plural form of century is centuries
The plural form of loss is losses
The plural form of security is securities
The plural form of analysis is analyses
The plural form of safety is
2024-07-22 22:42:41 root INFO     [order_1_approx] starting weight calculation for The plural form of army is armies
The plural form of safety is safeties
The plural form of security is securities
The plural form of loss is losses
The plural form of analysis is analyses
The plural form of century is centuries
The plural form of country is countries
The plural form of secretary is
2024-07-22 22:42:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 22:46:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.8604,  0.1219, -0.7178,  ...,  0.9634,  0.2512,  0.8076],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-5.8008,  4.5508,  0.8037,  ...,  1.4092, -3.2812,  4.0977],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0229, -0.0107,  0.0175,  ..., -0.0125, -0.0063, -0.0236],
        [-0.0085, -0.0207, -0.0175,  ..., -0.0144, -0.0157,  0.0029],
        [ 0.0147,  0.0083,  0.0018,  ...,  0.0013,  0.0097,  0.0110],
        ...,
        [-0.0181, -0.0255,  0.0053,  ..., -0.0115,  0.0108, -0.0198],
        [-0.0020, -0.0314,  0.0151,  ...,  0.0083, -0.0255,  0.0313],
        [ 0.0204,  0.0186, -0.0088,  ..., -0.0079,  0.0254,  0.0107]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-6.0742,  5.4297,  1.2510,  ...,  1.6396, -3.3164,  3.9688]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 22:46:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of army is armies
The plural form of safety is safeties
The plural form of security is securities
The plural form of loss is losses
The plural form of analysis is analyses
The plural form of century is centuries
The plural form of country is countries
The plural form of secretary is
2024-07-22 22:46:40 root INFO     [order_1_approx] starting weight calculation for The plural form of century is centuries
The plural form of army is armies
The plural form of safety is safeties
The plural form of security is securities
The plural form of secretary is secretaries
The plural form of loss is losses
The plural form of analysis is analyses
The plural form of country is
2024-07-22 22:46:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 22:50:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0880,  0.0546, -0.5166,  ..., -0.5259,  0.6079,  0.7739],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0020, -1.6436, -3.7773,  ..., -3.1465,  1.1289, -0.7773],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0222, -0.0092,  0.0059,  ..., -0.0070, -0.0098, -0.0194],
        [-0.0293, -0.0151,  0.0021,  ...,  0.0080,  0.0142, -0.0043],
        [ 0.0285,  0.0092, -0.0117,  ..., -0.0037, -0.0105, -0.0072],
        ...,
        [-0.0077, -0.0083,  0.0056,  ..., -0.0106,  0.0101,  0.0021],
        [-0.0001, -0.0078,  0.0056,  ...,  0.0044, -0.0352,  0.0351],
        [-0.0105,  0.0191, -0.0059,  ..., -0.0005,  0.0184, -0.0544]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5420, -0.7441, -3.5664,  ..., -3.8848,  0.1025,  0.1143]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 22:50:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of century is centuries
The plural form of army is armies
The plural form of safety is safeties
The plural form of security is securities
The plural form of secretary is secretaries
The plural form of loss is losses
The plural form of analysis is analyses
The plural form of country is
2024-07-22 22:50:37 root INFO     [order_1_approx] starting weight calculation for The plural form of country is countries
The plural form of analysis is analyses
The plural form of safety is safeties
The plural form of century is centuries
The plural form of secretary is secretaries
The plural form of loss is losses
The plural form of army is armies
The plural form of security is
2024-07-22 22:50:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 22:54:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.8555, 0.3928, 0.6523,  ..., 0.2344, 0.2681, 0.8320], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.3555,  3.4824,  2.7051,  ..., -1.2520, -1.3789, -0.6523],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-4.6936e-02, -1.9302e-02,  1.5434e-02,  ..., -1.0551e-02,
         -2.4506e-02, -2.8809e-02],
        [-4.2847e-02, -1.2001e-02, -1.4053e-02,  ...,  1.2405e-02,
         -3.6316e-03,  1.9730e-02],
        [ 2.3499e-03, -7.8583e-03,  1.2444e-02,  ..., -1.2875e-04,
          1.6220e-02,  1.1368e-02],
        ...,
        [-1.2466e-02, -2.8244e-02,  5.5542e-03,  ...,  8.1940e-03,
          1.9791e-02, -1.3535e-02],
        [ 9.4833e-03, -3.8605e-03,  2.3499e-03,  ..., -7.7972e-03,
         -1.9897e-02, -3.0518e-05],
        [ 1.1185e-02,  2.2247e-02, -9.1791e-04,  ..., -1.7059e-02,
          2.4872e-03, -6.2180e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3613,  4.9453,  2.1250,  ..., -1.7773, -2.1953, -1.1689]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 22:54:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of country is countries
The plural form of analysis is analyses
The plural form of safety is safeties
The plural form of century is centuries
The plural form of secretary is secretaries
The plural form of loss is losses
The plural form of army is armies
The plural form of security is
2024-07-22 22:54:36 root INFO     [order_1_approx] starting weight calculation for The plural form of loss is losses
The plural form of analysis is analyses
The plural form of security is securities
The plural form of century is centuries
The plural form of country is countries
The plural form of safety is safeties
The plural form of secretary is secretaries
The plural form of army is
2024-07-22 22:54:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 22:58:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7998,  0.9160, -0.0189,  ..., -0.8857, -0.4238,  1.3447],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4617,  3.0527, -1.5352,  ..., -3.6074, -0.3086,  2.1406],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0059, -0.0034,  0.0036,  ...,  0.0096, -0.0113, -0.0236],
        [-0.0302, -0.0290, -0.0095,  ...,  0.0041, -0.0168,  0.0010],
        [ 0.0137,  0.0045,  0.0183,  ..., -0.0121,  0.0171,  0.0044],
        ...,
        [ 0.0040, -0.0068,  0.0009,  ..., -0.0082, -0.0096,  0.0025],
        [ 0.0077, -0.0030,  0.0061,  ..., -0.0032, -0.0070,  0.0166],
        [ 0.0028,  0.0020, -0.0013,  ..., -0.0194,  0.0120, -0.0047]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1331,  2.6426, -2.6465,  ..., -3.6895, -0.4319,  2.0957]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 22:58:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of loss is losses
The plural form of analysis is analyses
The plural form of security is securities
The plural form of century is centuries
The plural form of country is countries
The plural form of safety is safeties
The plural form of secretary is secretaries
The plural form of army is
2024-07-22 22:58:34 root INFO     [order_1_approx] starting weight calculation for The plural form of army is armies
The plural form of secretary is secretaries
The plural form of safety is safeties
The plural form of loss is losses
The plural form of analysis is analyses
The plural form of country is countries
The plural form of security is securities
The plural form of century is
2024-07-22 22:58:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 23:02:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0894, -0.4268, -0.5894,  ..., -0.6611, -0.0610,  1.0039],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.8516, -1.5498, -3.3555,  ..., -1.1611, -1.4658,  3.3906],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0195, -0.0486,  0.0396,  ...,  0.0102,  0.0047, -0.0299],
        [-0.0214, -0.0062, -0.0100,  ...,  0.0026,  0.0044,  0.0073],
        [-0.0066,  0.0050, -0.0312,  ..., -0.0143,  0.0054, -0.0078],
        ...,
        [-0.0194, -0.0194,  0.0017,  ..., -0.0107,  0.0182,  0.0031],
        [ 0.0194, -0.0079,  0.0075,  ..., -0.0161, -0.0222,  0.0152],
        [-0.0201,  0.0023,  0.0265,  ...,  0.0033,  0.0044, -0.0319]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.1426, -1.4736, -2.7852,  ..., -0.5142, -2.0664,  3.7285]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 23:02:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of army is armies
The plural form of secretary is secretaries
The plural form of safety is safeties
The plural form of loss is losses
The plural form of analysis is analyses
The plural form of country is countries
The plural form of security is securities
The plural form of century is
2024-07-22 23:02:33 root INFO     [order_1_approx] starting weight calculation for The plural form of safety is safeties
The plural form of loss is losses
The plural form of security is securities
The plural form of army is armies
The plural form of century is centuries
The plural form of secretary is secretaries
The plural form of country is countries
The plural form of analysis is
2024-07-22 23:02:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 23:06:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9629,  0.0366, -0.3621,  ..., -0.2347,  0.2162,  1.0664],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8813,  1.1621, -0.6885,  ..., -0.2207,  1.7930,  3.2441],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0239, -0.0196,  0.0146,  ..., -0.0218,  0.0037, -0.0306],
        [-0.0275, -0.0372, -0.0005,  ...,  0.0024, -0.0125,  0.0120],
        [-0.0077,  0.0133, -0.0161,  ...,  0.0066,  0.0160,  0.0157],
        ...,
        [-0.0179,  0.0043, -0.0068,  ..., -0.0316,  0.0148, -0.0004],
        [-0.0143,  0.0017, -0.0037,  ..., -0.0051, -0.0389,  0.0162],
        [ 0.0130,  0.0414,  0.0140,  ...,  0.0088,  0.0004, -0.0228]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[0.4263, 1.0615, 0.3760,  ..., 0.1189, 1.3184, 3.8965]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 23:06:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of safety is safeties
The plural form of loss is losses
The plural form of security is securities
The plural form of army is armies
The plural form of century is centuries
The plural form of secretary is secretaries
The plural form of country is countries
The plural form of analysis is
2024-07-22 23:06:32 root INFO     [order_1_approx] starting weight calculation for The plural form of security is securities
The plural form of century is centuries
The plural form of country is countries
The plural form of army is armies
The plural form of safety is safeties
The plural form of secretary is secretaries
The plural form of analysis is analyses
The plural form of loss is
2024-07-22 23:06:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 23:10:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1592, -0.3975,  0.5737,  ...,  0.3853, -0.2566,  0.8750],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.3613,  1.0127, -1.0605,  ..., -0.0801,  1.4971,  2.9141],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0021,  0.0072,  0.0144,  ..., -0.0089, -0.0052, -0.0227],
        [-0.0222, -0.0338,  0.0011,  ...,  0.0069,  0.0057,  0.0004],
        [ 0.0009,  0.0090, -0.0241,  ..., -0.0002,  0.0156,  0.0123],
        ...,
        [ 0.0165, -0.0013, -0.0002,  ..., -0.0164,  0.0046, -0.0027],
        [ 0.0028,  0.0022,  0.0025,  ..., -0.0068, -0.0269,  0.0184],
        [-0.0007,  0.0021,  0.0049,  ...,  0.0028,  0.0179, -0.0161]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5400,  1.5215, -0.2573,  ...,  0.0171,  2.1758,  3.2695]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 23:10:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of security is securities
The plural form of century is centuries
The plural form of country is countries
The plural form of army is armies
The plural form of safety is safeties
The plural form of secretary is secretaries
The plural form of analysis is analyses
The plural form of loss is
2024-07-22 23:10:27 root INFO     total operator prediction time: 1905.2955923080444 seconds
2024-07-22 23:10:27 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on Ving - verb_inf
2024-07-22 23:10:27 root INFO     building operator Ving - verb_inf
2024-07-22 23:10:28 root INFO     [order_1_approx] starting weight calculation for applying is the active form of apply
remembering is the active form of remember
operating is the active form of operate
happening is the active form of happen
managing is the active form of manage
ensuring is the active form of ensure
creating is the active form of create
including is the active form of
2024-07-22 23:10:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 23:14:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9004, -0.7793,  0.6299,  ..., -0.1924,  0.8154,  0.9229],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.0547,  0.1719, -3.3750,  ..., -1.1035,  0.2050, -0.7129],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0147,  0.0030,  0.0136,  ..., -0.0057,  0.0017, -0.0187],
        [ 0.0034, -0.0147, -0.0047,  ...,  0.0219, -0.0043, -0.0092],
        [ 0.0006,  0.0263, -0.0068,  ..., -0.0114, -0.0234, -0.0207],
        ...,
        [-0.0055, -0.0148,  0.0216,  ..., -0.0324, -0.0005, -0.0235],
        [-0.0222, -0.0091,  0.0037,  ..., -0.0163, -0.0408,  0.0211],
        [-0.0033,  0.0305,  0.0067,  ..., -0.0100,  0.0091, -0.0018]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5195, -0.2864, -2.4492,  ..., -1.0469, -0.7725, -1.8281]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 23:14:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for applying is the active form of apply
remembering is the active form of remember
operating is the active form of operate
happening is the active form of happen
managing is the active form of manage
ensuring is the active form of ensure
creating is the active form of create
including is the active form of
2024-07-22 23:14:27 root INFO     [order_1_approx] starting weight calculation for operating is the active form of operate
creating is the active form of create
managing is the active form of manage
applying is the active form of apply
including is the active form of include
ensuring is the active form of ensure
remembering is the active form of remember
happening is the active form of
2024-07-22 23:14:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 23:18:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2881, -0.0569,  1.4102,  ..., -0.4761,  1.8984,  0.0417],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.9648, -1.3379, -5.2109,  ..., -3.2188,  2.7246,  5.4414],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0149, -0.0210,  0.0119,  ...,  0.0006, -0.0063,  0.0089],
        [-0.0092, -0.0034,  0.0035,  ...,  0.0149,  0.0071, -0.0031],
        [-0.0045,  0.0147, -0.0109,  ..., -0.0058, -0.0081, -0.0003],
        ...,
        [-0.0216,  0.0112, -0.0079,  ..., -0.0186, -0.0060, -0.0110],
        [-0.0181,  0.0076, -0.0002,  ...,  0.0031, -0.0210,  0.0170],
        [-0.0114,  0.0050, -0.0010,  ..., -0.0011, -0.0125, -0.0018]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.9473, -1.3389, -5.4219,  ..., -3.4707,  3.5547,  4.9648]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 23:18:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for operating is the active form of operate
creating is the active form of create
managing is the active form of manage
applying is the active form of apply
including is the active form of include
ensuring is the active form of ensure
remembering is the active form of remember
happening is the active form of
2024-07-22 23:18:26 root INFO     [order_1_approx] starting weight calculation for happening is the active form of happen
operating is the active form of operate
including is the active form of include
remembering is the active form of remember
creating is the active form of create
applying is the active form of apply
managing is the active form of manage
ensuring is the active form of
2024-07-22 23:18:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 23:22:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9111, -0.1029,  0.1968,  ..., -0.3674, -0.1748,  0.2837],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7129,  0.5264,  0.8442,  ..., -0.2998,  0.6895,  2.3457],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0067, -0.0056,  0.0218,  ...,  0.0023, -0.0099, -0.0173],
        [ 0.0105, -0.0057,  0.0022,  ...,  0.0095,  0.0026,  0.0068],
        [ 0.0056, -0.0019, -0.0223,  ...,  0.0120,  0.0207,  0.0019],
        ...,
        [-0.0252, -0.0066, -0.0028,  ..., -0.0094,  0.0261, -0.0016],
        [ 0.0068,  0.0187,  0.0164,  ..., -0.0235, -0.0207,  0.0265],
        [ 0.0261,  0.0132,  0.0180,  ...,  0.0140,  0.0057, -0.0075]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1523,  0.4719,  0.7344,  ..., -0.2218,  0.7026,  3.0469]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 23:22:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for happening is the active form of happen
operating is the active form of operate
including is the active form of include
remembering is the active form of remember
creating is the active form of create
applying is the active form of apply
managing is the active form of manage
ensuring is the active form of
2024-07-22 23:22:25 root INFO     [order_1_approx] starting weight calculation for ensuring is the active form of ensure
creating is the active form of create
happening is the active form of happen
operating is the active form of operate
including is the active form of include
managing is the active form of manage
applying is the active form of apply
remembering is the active form of
2024-07-22 23:22:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 23:26:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.8638, 0.3899, 0.2324,  ..., 0.9067, 2.1758, 0.6709], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1145,  3.8730, -1.7607,  ..., -1.6660, -1.0518,  4.5742],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0133, -0.0058,  0.0256,  ...,  0.0058, -0.0022,  0.0043],
        [ 0.0089, -0.0084,  0.0027,  ...,  0.0077, -0.0163,  0.0125],
        [ 0.0065,  0.0018, -0.0133,  ..., -0.0062, -0.0005,  0.0048],
        ...,
        [-0.0199, -0.0081,  0.0073,  ..., -0.0082,  0.0104, -0.0018],
        [ 0.0045,  0.0157, -0.0003,  ..., -0.0086, -0.0071,  0.0065],
        [ 0.0023, -0.0049, -0.0044,  ..., -0.0127, -0.0111,  0.0025]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6895,  4.2578, -2.5020,  ..., -0.4424, -1.5781,  4.1250]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 23:26:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for ensuring is the active form of ensure
creating is the active form of create
happening is the active form of happen
operating is the active form of operate
including is the active form of include
managing is the active form of manage
applying is the active form of apply
remembering is the active form of
2024-07-22 23:26:24 root INFO     [order_1_approx] starting weight calculation for ensuring is the active form of ensure
creating is the active form of create
remembering is the active form of remember
happening is the active form of happen
applying is the active form of apply
including is the active form of include
operating is the active form of operate
managing is the active form of
2024-07-22 23:26:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 23:30:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5518, -0.5400,  0.5278,  ..., -1.0361,  1.2363, -1.0352],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7559, -2.1816, -2.9102,  ...,  0.6006,  0.1816,  2.4746],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-6.1646e-03, -2.8400e-03,  3.3203e-02,  ..., -6.5804e-04,
          4.4060e-04, -4.1580e-04],
        [ 6.1989e-03, -1.2100e-02, -7.4482e-04,  ...,  6.0425e-03,
         -7.6065e-03,  6.1035e-05],
        [ 1.4404e-02,  2.0123e-03, -1.4191e-02,  ..., -4.9286e-03,
          1.3306e-02,  1.1536e-02],
        ...,
        [-3.7292e-02, -9.6588e-03, -3.3092e-03,  ..., -2.4384e-02,
          1.5182e-02, -1.2047e-02],
        [-9.6436e-03,  1.3779e-02,  8.3389e-03,  ..., -1.2451e-02,
         -2.2903e-02,  1.8036e-02],
        [ 2.0645e-02, -1.1414e-02,  9.6893e-03,  ...,  1.0429e-02,
          8.8196e-03, -1.2344e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5654, -2.4199, -3.4258,  ...,  0.7139,  0.7319,  2.7168]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 23:30:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for ensuring is the active form of ensure
creating is the active form of create
remembering is the active form of remember
happening is the active form of happen
applying is the active form of apply
including is the active form of include
operating is the active form of operate
managing is the active form of
2024-07-22 23:30:24 root INFO     [order_1_approx] starting weight calculation for ensuring is the active form of ensure
applying is the active form of apply
remembering is the active form of remember
creating is the active form of create
happening is the active form of happen
including is the active form of include
managing is the active form of manage
operating is the active form of
2024-07-22 23:30:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 23:34:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1072, -0.5854,  1.4668,  ..., -0.2642,  1.7461, -0.0781],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 6.7969, -1.3789, -0.5977,  ..., -1.2734, -1.9609,  4.3789],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 8.0261e-03, -2.8610e-04,  1.2184e-02,  ...,  2.9564e-03,
         -1.3275e-02, -6.5536e-03],
        [ 7.0572e-05, -1.1642e-02,  1.1230e-02,  ...,  1.2405e-02,
         -2.1420e-03,  7.2403e-03],
        [ 8.7128e-03, -1.5053e-02,  5.4474e-03,  ..., -5.6763e-03,
          2.1133e-03,  7.4310e-03],
        ...,
        [-2.7405e-02,  1.9436e-03, -1.0559e-02,  ..., -1.6541e-02,
          6.8130e-03,  2.6073e-03],
        [-4.5013e-03,  7.4959e-03,  1.6327e-02,  ..., -5.8594e-03,
         -1.3290e-02,  8.7585e-03],
        [ 7.4768e-03,  4.0359e-03, -5.1498e-03,  ..., -3.4356e-04,
         -1.4877e-04, -1.9867e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 7.5156, -0.8496, -0.1785,  ..., -1.6348, -2.0020,  4.2148]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 23:34:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for ensuring is the active form of ensure
applying is the active form of apply
remembering is the active form of remember
creating is the active form of create
happening is the active form of happen
including is the active form of include
managing is the active form of manage
operating is the active form of
2024-07-22 23:34:23 root INFO     [order_1_approx] starting weight calculation for operating is the active form of operate
remembering is the active form of remember
creating is the active form of create
happening is the active form of happen
managing is the active form of manage
including is the active form of include
ensuring is the active form of ensure
applying is the active form of
2024-07-22 23:34:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 23:38:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2949, -0.9438,  0.6680,  ..., -0.5303,  0.6577,  0.7778],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.5293,  0.4341, -1.3691,  ..., -1.2393, -3.4062,  2.4883],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.0651e-02,  2.0580e-03,  1.6220e-02,  ...,  4.5509e-03,
         -1.6663e-02,  5.3864e-03],
        [ 5.9471e-03, -1.6388e-02, -1.0757e-02,  ...,  6.2904e-03,
         -1.8463e-02,  3.5477e-04],
        [ 5.7526e-03,  5.6572e-03, -5.7106e-03,  ...,  1.3283e-02,
          2.8362e-03, -7.6904e-03],
        ...,
        [-1.4526e-02,  1.1444e-04,  1.8950e-03,  ..., -2.3727e-02,
          1.1024e-03, -1.9760e-02],
        [-1.3741e-02,  6.7902e-03,  1.8433e-02,  ..., -1.0628e-02,
         -2.0950e-02,  4.0588e-03],
        [ 3.9139e-03,  2.0599e-02, -4.9706e-03,  ...,  3.9139e-03,
          7.2937e-03,  9.5367e-07]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.2969,  0.6943, -2.1738,  ..., -1.3936, -3.8105,  2.1543]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 23:38:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for operating is the active form of operate
remembering is the active form of remember
creating is the active form of create
happening is the active form of happen
managing is the active form of manage
including is the active form of include
ensuring is the active form of ensure
applying is the active form of
2024-07-22 23:38:23 root INFO     [order_1_approx] starting weight calculation for operating is the active form of operate
including is the active form of include
happening is the active form of happen
managing is the active form of manage
remembering is the active form of remember
applying is the active form of apply
ensuring is the active form of ensure
creating is the active form of
2024-07-22 23:38:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 23:42:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4023, -1.1035,  0.0303,  ...,  0.4517,  1.3066, -0.7612],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0156, -2.3770, -0.4883,  ..., -2.2383, -1.0195, -2.1914],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.7319e-02, -8.9951e-03,  5.9280e-03,  ...,  1.0696e-02,
         -9.1171e-03, -4.0359e-03],
        [-1.7090e-03, -1.1467e-02, -1.9474e-03,  ..., -4.0970e-03,
         -1.0529e-02,  1.1551e-02],
        [ 5.0735e-03, -4.7531e-03, -6.4163e-03,  ..., -3.7594e-03,
         -8.6823e-03,  2.7981e-03],
        ...,
        [-2.5208e-02, -4.3564e-03,  1.5045e-02,  ...,  9.6130e-04,
         -2.0828e-03, -6.7902e-04],
        [-1.3771e-02,  1.1139e-02,  6.6795e-03,  ...,  1.7719e-03,
         -1.1559e-02,  1.5610e-02],
        [-3.7422e-03,  1.1911e-03,  4.1962e-03,  ..., -1.6571e-02,
          1.9318e-02, -1.3351e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.6895, -2.1465, -0.2544,  ..., -1.9297, -0.8701, -2.8047]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 23:42:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for operating is the active form of operate
including is the active form of include
happening is the active form of happen
managing is the active form of manage
remembering is the active form of remember
applying is the active form of apply
ensuring is the active form of ensure
creating is the active form of
2024-07-22 23:42:22 root INFO     total operator prediction time: 1914.8094651699066 seconds
2024-07-22 23:42:22 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_Ving - Ved
2024-07-22 23:42:22 root INFO     building operator verb_Ving - Ved
2024-07-22 23:42:22 root INFO     [order_1_approx] starting weight calculation for After something is suffering, it has suffered
After something is developing, it has developed
After something is replacing, it has replaced
After something is following, it has followed
After something is operating, it has operated
After something is performing, it has performed
After something is spending, it has spent
After something is appointing, it has
2024-07-22 23:42:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 23:46:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5781, -0.7773,  0.5039,  ...,  0.4797, -0.1302,  0.7358],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.6055,  1.1836,  0.4531,  ...,  5.0273,  0.7168,  2.7930],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0177, -0.0305, -0.0083,  ...,  0.0101, -0.0316, -0.0213],
        [ 0.0020, -0.0166, -0.0187,  ..., -0.0216, -0.0006, -0.0104],
        [ 0.0052,  0.0204,  0.0061,  ...,  0.0147, -0.0036,  0.0106],
        ...,
        [-0.0549, -0.0171, -0.0458,  ..., -0.0245,  0.0196,  0.0057],
        [-0.0032, -0.0014,  0.0185,  ..., -0.0009, -0.0337,  0.0188],
        [-0.0028,  0.0087, -0.0186,  ..., -0.0027,  0.0064, -0.0495]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.1055,  1.3984,  0.5273,  ...,  5.6016,  0.5664,  3.2500]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 23:46:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is suffering, it has suffered
After something is developing, it has developed
After something is replacing, it has replaced
After something is following, it has followed
After something is operating, it has operated
After something is performing, it has performed
After something is spending, it has spent
After something is appointing, it has
2024-07-22 23:46:21 root INFO     [order_1_approx] starting weight calculation for After something is operating, it has operated
After something is suffering, it has suffered
After something is appointing, it has appointed
After something is spending, it has spent
After something is developing, it has developed
After something is following, it has followed
After something is performing, it has performed
After something is replacing, it has
2024-07-22 23:46:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 23:50:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3496,  0.1500,  0.7363,  ..., -0.0673,  0.2192, -0.7588],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4453,  3.2402,  0.2520,  ...,  5.2148, -0.5303,  6.0273],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0201, -0.0049,  0.0033,  ...,  0.0021, -0.0149, -0.0207],
        [-0.0071, -0.0154, -0.0120,  ..., -0.0102,  0.0079, -0.0008],
        [ 0.0164,  0.0045, -0.0140,  ...,  0.0024, -0.0102, -0.0004],
        ...,
        [-0.0302, -0.0145, -0.0229,  ..., -0.0167,  0.0293,  0.0051],
        [-0.0179,  0.0013, -0.0095,  ..., -0.0045, -0.0442,  0.0136],
        [ 0.0134,  0.0170, -0.0268,  ..., -0.0193, -0.0125, -0.0391]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.8125,  3.4258,  0.5420,  ...,  4.6562, -0.1060,  5.8555]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 23:50:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is operating, it has operated
After something is suffering, it has suffered
After something is appointing, it has appointed
After something is spending, it has spent
After something is developing, it has developed
After something is following, it has followed
After something is performing, it has performed
After something is replacing, it has
2024-07-22 23:50:20 root INFO     [order_1_approx] starting weight calculation for After something is replacing, it has replaced
After something is spending, it has spent
After something is performing, it has performed
After something is appointing, it has appointed
After something is suffering, it has suffered
After something is operating, it has operated
After something is developing, it has developed
After something is following, it has
2024-07-22 23:50:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 23:54:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9995, -0.5703,  1.5752,  ...,  0.7246, -0.3142,  1.4443],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8872,  0.9043, -0.6250,  ..., -0.1616,  0.5586,  2.4727],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0121, -0.0102,  0.0102,  ...,  0.0021, -0.0351, -0.0203],
        [ 0.0063, -0.0167,  0.0093,  ..., -0.0047, -0.0154, -0.0110],
        [-0.0050, -0.0004, -0.0026,  ...,  0.0034, -0.0185,  0.0043],
        ...,
        [-0.0241, -0.0090, -0.0087,  ..., -0.0120,  0.0014, -0.0006],
        [-0.0036, -0.0018,  0.0022,  ...,  0.0044, -0.0169,  0.0073],
        [-0.0002,  0.0078, -0.0113,  ..., -0.0080,  0.0217, -0.0209]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4551,  0.9424, -0.8501,  ..., -0.2837,  1.1426,  1.7656]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 23:54:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is replacing, it has replaced
After something is spending, it has spent
After something is performing, it has performed
After something is appointing, it has appointed
After something is suffering, it has suffered
After something is operating, it has operated
After something is developing, it has developed
After something is following, it has
2024-07-22 23:54:19 root INFO     [order_1_approx] starting weight calculation for After something is suffering, it has suffered
After something is following, it has followed
After something is performing, it has performed
After something is appointing, it has appointed
After something is operating, it has operated
After something is spending, it has spent
After something is replacing, it has replaced
After something is developing, it has
2024-07-22 23:54:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-22 23:58:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0986, -0.2886,  1.4004,  ...,  0.2047,  0.2573,  0.6436],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8711, -0.6973,  2.9238,  ...,  1.0059, -1.3984, -0.1377],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0005, -0.0137,  0.0068,  ...,  0.0110, -0.0132, -0.0190],
        [-0.0031, -0.0182,  0.0033,  ..., -0.0092, -0.0160,  0.0026],
        [-0.0044,  0.0103, -0.0096,  ...,  0.0013, -0.0021, -0.0127],
        ...,
        [-0.0197,  0.0062, -0.0127,  ..., -0.0389,  0.0043, -0.0052],
        [ 0.0067,  0.0042,  0.0057,  ..., -0.0092, -0.0431,  0.0264],
        [ 0.0130, -0.0028, -0.0139,  ...,  0.0100,  0.0047, -0.0499]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1680, -1.5781,  2.0742,  ...,  1.4893, -1.2891, -1.1680]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-22 23:58:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is suffering, it has suffered
After something is following, it has followed
After something is performing, it has performed
After something is appointing, it has appointed
After something is operating, it has operated
After something is spending, it has spent
After something is replacing, it has replaced
After something is developing, it has
2024-07-22 23:58:19 root INFO     [order_1_approx] starting weight calculation for After something is following, it has followed
After something is operating, it has operated
After something is spending, it has spent
After something is appointing, it has appointed
After something is suffering, it has suffered
After something is developing, it has developed
After something is replacing, it has replaced
After something is performing, it has
2024-07-22 23:58:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 00:02:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3906,  1.1357,  0.1907,  ..., -0.5322,  0.8896, -0.1541],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.2520,  1.2676,  1.1797,  ...,  5.9961, -0.8633,  2.0371],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0045, -0.0254, -0.0125,  ...,  0.0025, -0.0136, -0.0226],
        [-0.0084, -0.0238, -0.0010,  ..., -0.0202,  0.0043, -0.0056],
        [ 0.0057, -0.0137, -0.0152,  ...,  0.0120,  0.0015, -0.0008],
        ...,
        [-0.0284, -0.0212, -0.0006,  ..., -0.0259,  0.0064,  0.0129],
        [-0.0075,  0.0043, -0.0008,  ..., -0.0096, -0.0356,  0.0183],
        [-0.0083, -0.0046, -0.0046,  ..., -0.0075, -0.0025, -0.0263]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.4258,  1.6816,  0.6094,  ...,  7.1680, -0.8081,  1.4209]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 00:02:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is following, it has followed
After something is operating, it has operated
After something is spending, it has spent
After something is appointing, it has appointed
After something is suffering, it has suffered
After something is developing, it has developed
After something is replacing, it has replaced
After something is performing, it has
2024-07-23 00:02:19 root INFO     [order_1_approx] starting weight calculation for After something is replacing, it has replaced
After something is performing, it has performed
After something is appointing, it has appointed
After something is following, it has followed
After something is suffering, it has suffered
After something is developing, it has developed
After something is operating, it has operated
After something is spending, it has
2024-07-23 00:02:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 00:06:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9351,  1.0059, -0.2465,  ...,  0.1543,  0.2435,  0.2876],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9570,  4.1836,  2.8496,  ...,  3.5898, -1.9541,  3.8809],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0198, -0.0308,  0.0258,  ...,  0.0114, -0.0165, -0.0178],
        [-0.0015, -0.0253, -0.0077,  ..., -0.0051, -0.0057, -0.0164],
        [-0.0016, -0.0033, -0.0277,  ..., -0.0091, -0.0070, -0.0016],
        ...,
        [-0.0024, -0.0170, -0.0179,  ..., -0.0447, -0.0085, -0.0119],
        [-0.0224, -0.0089,  0.0090,  ...,  0.0008, -0.0342,  0.0007],
        [-0.0133,  0.0098, -0.0051,  ..., -0.0033,  0.0063, -0.0284]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5488,  3.8828,  2.7930,  ...,  3.7637, -2.0527,  4.0156]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 00:06:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is replacing, it has replaced
After something is performing, it has performed
After something is appointing, it has appointed
After something is following, it has followed
After something is suffering, it has suffered
After something is developing, it has developed
After something is operating, it has operated
After something is spending, it has
2024-07-23 00:06:18 root INFO     [order_1_approx] starting weight calculation for After something is developing, it has developed
After something is suffering, it has suffered
After something is appointing, it has appointed
After something is spending, it has spent
After something is performing, it has performed
After something is replacing, it has replaced
After something is following, it has followed
After something is operating, it has
2024-07-23 00:06:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 00:10:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3411, -0.0879,  0.2249,  ..., -0.1565,  0.4922,  0.1171],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1313, -0.5215,  2.6680,  ...,  1.8320,  0.2144,  3.2207],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.7395e-03, -4.5052e-03,  3.1738e-03,  ...,  7.4768e-03,
         -1.4511e-02, -3.1891e-03],
        [-2.2034e-02, -7.6065e-03, -2.0123e-03,  ..., -2.1477e-03,
         -8.0490e-03,  7.8812e-03],
        [ 1.0437e-02,  9.5749e-04, -9.5215e-03,  ...,  1.4908e-02,
          2.7275e-03,  7.2479e-04],
        ...,
        [-1.7471e-02, -9.6817e-03, -1.2886e-02,  ..., -4.2236e-02,
          6.8588e-03, -6.1035e-05],
        [ 2.3251e-03,  4.3831e-03, -1.0941e-02,  ...,  6.4468e-04,
         -3.6194e-02,  2.5116e-02],
        [ 8.4305e-04,  6.4087e-03, -5.3177e-03,  ..., -8.7128e-03,
         -6.1951e-03, -3.7659e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5576, -0.9248,  2.4492,  ...,  2.3379, -0.0239,  1.9932]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 00:10:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is developing, it has developed
After something is suffering, it has suffered
After something is appointing, it has appointed
After something is spending, it has spent
After something is performing, it has performed
After something is replacing, it has replaced
After something is following, it has followed
After something is operating, it has
2024-07-23 00:10:18 root INFO     [order_1_approx] starting weight calculation for After something is spending, it has spent
After something is performing, it has performed
After something is operating, it has operated
After something is following, it has followed
After something is replacing, it has replaced
After something is appointing, it has appointed
After something is developing, it has developed
After something is suffering, it has
2024-07-23 00:10:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 00:14:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0198, -0.4863,  0.0043,  ...,  0.7983,  0.7739,  0.4392],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9121,  2.9199,  0.0127,  ...,  0.0894, -1.9443,  2.6758],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0179,  0.0048,  0.0114,  ...,  0.0028, -0.0083, -0.0355],
        [-0.0041, -0.0244, -0.0124,  ..., -0.0068, -0.0138, -0.0137],
        [ 0.0101,  0.0061, -0.0308,  ...,  0.0025,  0.0084,  0.0088],
        ...,
        [-0.0187, -0.0149, -0.0029,  ..., -0.0501,  0.0060,  0.0071],
        [-0.0042, -0.0017, -0.0090,  ..., -0.0032, -0.0268,  0.0239],
        [-0.0012,  0.0039,  0.0043,  ..., -0.0033,  0.0209, -0.0562]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2910,  3.0566, -0.0669,  ...,  1.3418, -1.5684,  3.2891]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 00:14:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is spending, it has spent
After something is performing, it has performed
After something is operating, it has operated
After something is following, it has followed
After something is replacing, it has replaced
After something is appointing, it has appointed
After something is developing, it has developed
After something is suffering, it has
2024-07-23 00:14:17 root INFO     total operator prediction time: 1914.9333157539368 seconds
2024-07-23 00:14:17 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_inf - Ved
2024-07-23 00:14:17 root INFO     building operator verb_inf - Ved
2024-07-23 00:14:17 root INFO     [order_1_approx] starting weight calculation for If the present form is relate, the past form is related
If the present form is enjoy, the past form is enjoyed
If the present form is include, the past form is included
If the present form is locate, the past form is located
If the present form is establish, the past form is established
If the present form is believe, the past form is believed
If the present form is send, the past form is sent
If the present form is introduce, the past form is
2024-07-23 00:14:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 00:18:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4473,  0.0339,  0.9326,  ...,  0.4578, -0.9082,  0.1199],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3276, -1.2725, -0.1387,  ...,  3.5332, -0.3472,  4.3984],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0181, -0.0084, -0.0011,  ...,  0.0008, -0.0054, -0.0015],
        [-0.0198, -0.0250,  0.0000,  ...,  0.0156, -0.0072, -0.0120],
        [-0.0091, -0.0196, -0.0257,  ..., -0.0094, -0.0115, -0.0134],
        ...,
        [ 0.0086,  0.0133, -0.0113,  ..., -0.0176, -0.0041,  0.0109],
        [ 0.0073,  0.0152,  0.0090,  ..., -0.0065, -0.0287,  0.0096],
        [-0.0114,  0.0046,  0.0069,  ..., -0.0127,  0.0265, -0.0151]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6162, -1.5518, -0.0229,  ...,  2.9082, -0.2247,  4.2031]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 00:18:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is relate, the past form is related
If the present form is enjoy, the past form is enjoyed
If the present form is include, the past form is included
If the present form is locate, the past form is located
If the present form is establish, the past form is established
If the present form is believe, the past form is believed
If the present form is send, the past form is sent
If the present form is introduce, the past form is
2024-07-23 00:18:15 root INFO     [order_1_approx] starting weight calculation for If the present form is include, the past form is included
If the present form is introduce, the past form is introduced
If the present form is locate, the past form is located
If the present form is enjoy, the past form is enjoyed
If the present form is establish, the past form is established
If the present form is relate, the past form is related
If the present form is believe, the past form is believed
If the present form is send, the past form is
2024-07-23 00:18:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 00:22:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([1.1230, 0.5107, 1.3848,  ..., 0.1787, 0.0950, 0.2830], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.9785,  0.9873,  2.5508,  ...,  3.7402, -1.8955,  2.4141],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0309, -0.0098, -0.0018,  ...,  0.0087, -0.0013, -0.0001],
        [-0.0063, -0.0344,  0.0032,  ...,  0.0061, -0.0103, -0.0053],
        [ 0.0082, -0.0052, -0.0182,  ..., -0.0063, -0.0056, -0.0084],
        ...,
        [-0.0367,  0.0013, -0.0206,  ..., -0.0231, -0.0081,  0.0046],
        [ 0.0041,  0.0013,  0.0133,  ..., -0.0022, -0.0162,  0.0058],
        [-0.0037, -0.0022, -0.0065,  ..., -0.0200,  0.0052, -0.0199]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.1270,  0.8789,  2.5801,  ...,  3.8047, -2.5234,  2.4121]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 00:22:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is include, the past form is included
If the present form is introduce, the past form is introduced
If the present form is locate, the past form is located
If the present form is enjoy, the past form is enjoyed
If the present form is establish, the past form is established
If the present form is relate, the past form is related
If the present form is believe, the past form is believed
If the present form is send, the past form is
2024-07-23 00:22:11 root INFO     [order_1_approx] starting weight calculation for If the present form is relate, the past form is related
If the present form is locate, the past form is located
If the present form is send, the past form is sent
If the present form is include, the past form is included
If the present form is believe, the past form is believed
If the present form is establish, the past form is established
If the present form is introduce, the past form is introduced
If the present form is enjoy, the past form is
2024-07-23 00:22:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 00:26:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.3467, 0.9526, 0.8896,  ..., 0.0455, 0.4702, 0.2698], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7080,  1.6270,  2.6523,  ..., -1.3301, -1.5186,  4.3711],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0315, -0.0089,  0.0080,  ..., -0.0035,  0.0032, -0.0186],
        [-0.0062, -0.0353, -0.0085,  ...,  0.0023,  0.0020, -0.0066],
        [-0.0045, -0.0050, -0.0306,  ..., -0.0007, -0.0031, -0.0164],
        ...,
        [-0.0244,  0.0103, -0.0044,  ..., -0.0282,  0.0012,  0.0090],
        [-0.0028,  0.0184,  0.0031,  ..., -0.0050, -0.0281,  0.0139],
        [-0.0057,  0.0022,  0.0007,  ..., -0.0120,  0.0209, -0.0336]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2383,  1.3828,  2.4297,  ..., -1.3779, -1.9199,  4.8281]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 00:26:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is relate, the past form is related
If the present form is locate, the past form is located
If the present form is send, the past form is sent
If the present form is include, the past form is included
If the present form is believe, the past form is believed
If the present form is establish, the past form is established
If the present form is introduce, the past form is introduced
If the present form is enjoy, the past form is
2024-07-23 00:26:07 root INFO     [order_1_approx] starting weight calculation for If the present form is believe, the past form is believed
If the present form is introduce, the past form is introduced
If the present form is enjoy, the past form is enjoyed
If the present form is establish, the past form is established
If the present form is send, the past form is sent
If the present form is relate, the past form is related
If the present form is include, the past form is included
If the present form is locate, the past form is
2024-07-23 00:26:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 00:30:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2529, -0.3743, -0.0303,  ...,  0.0664,  0.0399, -0.1755],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.1230,  1.2939, -3.2109,  ..., -1.1074,  1.9023,  0.6865],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0221, -0.0169, -0.0069,  ...,  0.0042,  0.0050, -0.0075],
        [-0.0159, -0.0139,  0.0016,  ...,  0.0209,  0.0174, -0.0113],
        [ 0.0043,  0.0058, -0.0115,  ...,  0.0219,  0.0040, -0.0122],
        ...,
        [-0.0242,  0.0003, -0.0104,  ..., -0.0365, -0.0023,  0.0063],
        [ 0.0067,  0.0038, -0.0002,  ..., -0.0320, -0.0166,  0.0098],
        [-0.0173, -0.0147,  0.0153,  ..., -0.0006, -0.0094, -0.0224]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5078,  0.9512, -3.1523,  ..., -0.5776,  1.6406,  0.5732]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 00:30:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is believe, the past form is believed
If the present form is introduce, the past form is introduced
If the present form is enjoy, the past form is enjoyed
If the present form is establish, the past form is established
If the present form is send, the past form is sent
If the present form is relate, the past form is related
If the present form is include, the past form is included
If the present form is locate, the past form is
2024-07-23 00:30:02 root INFO     [order_1_approx] starting weight calculation for If the present form is include, the past form is included
If the present form is introduce, the past form is introduced
If the present form is believe, the past form is believed
If the present form is establish, the past form is established
If the present form is send, the past form is sent
If the present form is enjoy, the past form is enjoyed
If the present form is locate, the past form is located
If the present form is relate, the past form is
2024-07-23 00:30:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 00:33:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6069,  0.3589,  1.5254,  ..., -0.3879,  0.0388, -0.3296],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.4795,  0.1602, -0.2529,  ..., -0.3464,  1.4648,  2.7891],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0199, -0.0157,  0.0091,  ...,  0.0039, -0.0182, -0.0082],
        [-0.0014, -0.0208, -0.0116,  ...,  0.0119,  0.0116, -0.0123],
        [-0.0037, -0.0146, -0.0204,  ...,  0.0034, -0.0050, -0.0051],
        ...,
        [-0.0341,  0.0058, -0.0155,  ..., -0.0251, -0.0021, -0.0045],
        [ 0.0078,  0.0064, -0.0121,  ..., -0.0116, -0.0162,  0.0018],
        [-0.0132,  0.0181,  0.0019,  ..., -0.0085,  0.0128, -0.0236]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6699,  0.7129,  0.1355,  ...,  0.1760,  2.1172,  2.4707]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 00:33:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is include, the past form is included
If the present form is introduce, the past form is introduced
If the present form is believe, the past form is believed
If the present form is establish, the past form is established
If the present form is send, the past form is sent
If the present form is enjoy, the past form is enjoyed
If the present form is locate, the past form is located
If the present form is relate, the past form is
2024-07-23 00:33:58 root INFO     [order_1_approx] starting weight calculation for If the present form is send, the past form is sent
If the present form is locate, the past form is located
If the present form is relate, the past form is related
If the present form is establish, the past form is established
If the present form is enjoy, the past form is enjoyed
If the present form is introduce, the past form is introduced
If the present form is believe, the past form is believed
If the present form is include, the past form is
2024-07-23 00:33:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 00:37:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.7021, 0.6128, 1.0254,  ..., 0.5972, 0.3677, 0.8564], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0684,  0.8545, -1.9980,  ...,  2.7910,  0.9238, -1.3086],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0200, -0.0089, -0.0155,  ...,  0.0098,  0.0083,  0.0046],
        [-0.0076, -0.0202,  0.0041,  ...,  0.0021, -0.0016, -0.0019],
        [-0.0097,  0.0024, -0.0131,  ...,  0.0071, -0.0120, -0.0192],
        ...,
        [-0.0263,  0.0168, -0.0067,  ..., -0.0099, -0.0014,  0.0123],
        [ 0.0017,  0.0152,  0.0179,  ..., -0.0051, -0.0312,  0.0105],
        [-0.0095,  0.0218,  0.0002,  ..., -0.0124,  0.0004, -0.0172]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7842,  0.7871, -2.1250,  ...,  3.5938,  0.4700, -1.1553]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 00:37:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is send, the past form is sent
If the present form is locate, the past form is located
If the present form is relate, the past form is related
If the present form is establish, the past form is established
If the present form is enjoy, the past form is enjoyed
If the present form is introduce, the past form is introduced
If the present form is believe, the past form is believed
If the present form is include, the past form is
2024-07-23 00:37:55 root INFO     [order_1_approx] starting weight calculation for If the present form is relate, the past form is related
If the present form is include, the past form is included
If the present form is introduce, the past form is introduced
If the present form is send, the past form is sent
If the present form is believe, the past form is believed
If the present form is enjoy, the past form is enjoyed
If the present form is locate, the past form is located
If the present form is establish, the past form is
2024-07-23 00:37:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 00:41:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1879,  0.0281,  0.7695,  ...,  0.6841, -0.7632, -0.1245],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7109,  0.0352, -0.3730,  ...,  1.2451,  0.5820,  2.1719],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0138, -0.0207, -0.0022,  ...,  0.0023, -0.0078, -0.0103],
        [-0.0084, -0.0350, -0.0111,  ...,  0.0157,  0.0035, -0.0235],
        [ 0.0006,  0.0088, -0.0190,  ...,  0.0095, -0.0097,  0.0022],
        ...,
        [-0.0175,  0.0179,  0.0033,  ..., -0.0181,  0.0083,  0.0079],
        [ 0.0042,  0.0088,  0.0043,  ..., -0.0099, -0.0315,  0.0235],
        [-0.0165, -0.0008,  0.0023,  ..., -0.0090,  0.0105, -0.0286]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5361,  0.5215, -0.0920,  ...,  1.8926,  0.2329,  1.9932]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 00:41:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is relate, the past form is related
If the present form is include, the past form is included
If the present form is introduce, the past form is introduced
If the present form is send, the past form is sent
If the present form is believe, the past form is believed
If the present form is enjoy, the past form is enjoyed
If the present form is locate, the past form is located
If the present form is establish, the past form is
2024-07-23 00:41:50 root INFO     [order_1_approx] starting weight calculation for If the present form is relate, the past form is related
If the present form is establish, the past form is established
If the present form is include, the past form is included
If the present form is introduce, the past form is introduced
If the present form is locate, the past form is located
If the present form is send, the past form is sent
If the present form is enjoy, the past form is enjoyed
If the present form is believe, the past form is
2024-07-23 00:41:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 00:45:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9336,  0.2705,  0.3994,  ...,  0.7651,  0.2484, -0.0710],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0566,  0.5103,  0.3203,  ...,  0.5337, -4.5820,  1.7344],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0173, -0.0300,  0.0051,  ..., -0.0086,  0.0011, -0.0085],
        [-0.0087, -0.0320, -0.0055,  ..., -0.0025,  0.0151, -0.0062],
        [-0.0005, -0.0033, -0.0077,  ...,  0.0067, -0.0002, -0.0089],
        ...,
        [-0.0148, -0.0044, -0.0020,  ..., -0.0119, -0.0081,  0.0040],
        [-0.0003,  0.0146,  0.0126,  ..., -0.0249, -0.0371,  0.0099],
        [-0.0013,  0.0244, -0.0053,  ...,  0.0088, -0.0015, -0.0453]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0820,  0.1096,  0.0347,  ...,  0.9346, -3.8145,  2.5352]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 00:45:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is relate, the past form is related
If the present form is establish, the past form is established
If the present form is include, the past form is included
If the present form is introduce, the past form is introduced
If the present form is locate, the past form is located
If the present form is send, the past form is sent
If the present form is enjoy, the past form is enjoyed
If the present form is believe, the past form is
2024-07-23 00:45:46 root INFO     total operator prediction time: 1889.2086188793182 seconds
2024-07-23 00:45:46 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_inf - 3pSg
2024-07-23 00:45:46 root INFO     building operator verb_inf - 3pSg
2024-07-23 00:45:46 root INFO     [order_1_approx] starting weight calculation for I refer, he refers
I remain, he remains
I become, he becomes
I apply, he applies
I remember, he remembers
I create, he creates
I send, he sends
I happen, he
2024-07-23 00:45:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 00:49:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2148, -0.1814,  1.0146,  ..., -0.1550,  0.6797,  0.1086],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0967, -0.3428, -5.3594,  ...,  0.7085,  2.9473,  9.1797],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0110, -0.0153,  0.0072,  ...,  0.0201, -0.0023, -0.0195],
        [-0.0131, -0.0255,  0.0036,  ..., -0.0030,  0.0060,  0.0132],
        [-0.0004, -0.0028, -0.0280,  ...,  0.0061, -0.0178,  0.0079],
        ...,
        [ 0.0063,  0.0082,  0.0059,  ..., -0.0365, -0.0133,  0.0106],
        [ 0.0181,  0.0129,  0.0088,  ..., -0.0076, -0.0338, -0.0068],
        [-0.0102, -0.0047, -0.0039,  ..., -0.0068, -0.0016, -0.0422]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0236,  0.3457, -6.3047,  ...,  1.0342,  3.0137,  9.2500]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 00:49:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I refer, he refers
I remain, he remains
I become, he becomes
I apply, he applies
I remember, he remembers
I create, he creates
I send, he sends
I happen, he
2024-07-23 00:49:46 root INFO     [order_1_approx] starting weight calculation for I create, he creates
I apply, he applies
I send, he sends
I remember, he remembers
I remain, he remains
I refer, he refers
I happen, he happens
I become, he
2024-07-23 00:49:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 00:53:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7866, -0.0591,  1.7227,  ...,  0.8525, -0.3276,  0.3972],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2834, -2.0879,  0.0518,  ...,  0.8623,  1.0518,  1.4248],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0218, -0.0232,  0.0204,  ..., -0.0126, -0.0126, -0.0238],
        [-0.0135, -0.0276, -0.0162,  ...,  0.0233,  0.0123, -0.0021],
        [-0.0080,  0.0049, -0.0393,  ...,  0.0222, -0.0157,  0.0152],
        ...,
        [-0.0007, -0.0113, -0.0143,  ..., -0.0544, -0.0110,  0.0121],
        [-0.0039, -0.0021,  0.0089,  ..., -0.0308, -0.0167,  0.0064],
        [-0.0092,  0.0014,  0.0064,  ...,  0.0041, -0.0044, -0.0358]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.8203, -1.7832,  1.1143,  ...,  1.6426,  1.0479,  1.2480]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 00:53:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I create, he creates
I apply, he applies
I send, he sends
I remember, he remembers
I remain, he remains
I refer, he refers
I happen, he happens
I become, he
2024-07-23 00:53:44 root INFO     [order_1_approx] starting weight calculation for I apply, he applies
I remember, he remembers
I happen, he happens
I send, he sends
I remain, he remains
I become, he becomes
I refer, he refers
I create, he
2024-07-23 00:53:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 00:57:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3191, -1.1396,  0.2734,  ...,  0.7231, -0.2764, -0.0676],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.3096, -2.9043,  0.3613,  ...,  1.6035, -1.5312,  0.4639],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0044, -0.0088, -0.0020,  ...,  0.0015, -0.0181, -0.0337],
        [-0.0182, -0.0380,  0.0067,  ...,  0.0073,  0.0122, -0.0164],
        [-0.0110, -0.0017, -0.0362,  ...,  0.0174, -0.0009,  0.0034],
        ...,
        [-0.0243, -0.0015, -0.0017,  ..., -0.0220, -0.0166,  0.0079],
        [-0.0167,  0.0106,  0.0090,  ..., -0.0153, -0.0294,  0.0168],
        [-0.0041,  0.0025,  0.0057,  ..., -0.0111,  0.0022, -0.0269]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.4434, -2.8008, -0.1533,  ...,  1.3379, -0.9990, -0.2891]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 00:57:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I apply, he applies
I remember, he remembers
I happen, he happens
I send, he sends
I remain, he remains
I become, he becomes
I refer, he refers
I create, he
2024-07-23 00:57:43 root INFO     [order_1_approx] starting weight calculation for I become, he becomes
I remember, he remembers
I send, he sends
I apply, he applies
I refer, he refers
I create, he creates
I happen, he happens
I remain, he
2024-07-23 00:57:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 01:01:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6216, -0.1208,  1.8857,  ...,  0.2505, -0.4604,  0.5562],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.2891, -0.7324, -4.8477,  ...,  2.5293, -1.4219,  6.7031],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0134, -0.0282, -0.0009,  ...,  0.0216, -0.0033, -0.0061],
        [-0.0014, -0.0228,  0.0100,  ...,  0.0093,  0.0066,  0.0123],
        [-0.0067,  0.0039, -0.0306,  ...,  0.0087, -0.0207,  0.0055],
        ...,
        [-0.0010,  0.0130, -0.0110,  ..., -0.0335, -0.0222,  0.0041],
        [-0.0060,  0.0035,  0.0173,  ..., -0.0235, -0.0392,  0.0075],
        [-0.0175, -0.0008, -0.0034,  ...,  0.0033, -0.0021, -0.0393]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.2773, -0.2139, -4.5312,  ...,  3.6250, -1.2910,  7.3242]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 01:01:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I become, he becomes
I remember, he remembers
I send, he sends
I apply, he applies
I refer, he refers
I create, he creates
I happen, he happens
I remain, he
2024-07-23 01:01:42 root INFO     [order_1_approx] starting weight calculation for I send, he sends
I apply, he applies
I become, he becomes
I happen, he happens
I create, he creates
I remain, he remains
I refer, he refers
I remember, he
2024-07-23 01:01:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 01:05:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3159, -0.1438,  0.4653,  ..., -0.4463, -0.1895,  0.5830],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.5801, -0.7012, -3.1797,  ...,  1.2549, -2.4844,  5.4531],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0207, -0.0062,  0.0162,  ...,  0.0106, -0.0044, -0.0140],
        [-0.0036, -0.0198,  0.0054,  ..., -0.0019,  0.0145, -0.0034],
        [-0.0090,  0.0101, -0.0286,  ...,  0.0304, -0.0094, -0.0144],
        ...,
        [-0.0005, -0.0160, -0.0010,  ..., -0.0357, -0.0113,  0.0120],
        [-0.0022,  0.0140,  0.0204,  ..., -0.0187, -0.0361,  0.0090],
        [-0.0068,  0.0087,  0.0045,  ..., -0.0034,  0.0147, -0.0339]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.6484, -0.5059, -3.1172,  ...,  2.0039, -2.0391,  4.8984]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 01:05:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I send, he sends
I apply, he applies
I become, he becomes
I happen, he happens
I create, he creates
I remain, he remains
I refer, he refers
I remember, he
2024-07-23 01:05:42 root INFO     [order_1_approx] starting weight calculation for I create, he creates
I remain, he remains
I become, he becomes
I remember, he remembers
I happen, he happens
I refer, he refers
I apply, he applies
I send, he
2024-07-23 01:05:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 01:09:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3711, -0.6514,  1.4834,  ...,  0.4131,  0.2117,  0.7007],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.8242, -0.7153,  3.8516,  ...,  3.2617, -1.6270,  5.6250],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0149,  0.0021, -0.0023,  ..., -0.0045, -0.0033, -0.0156],
        [-0.0086, -0.0226,  0.0016,  ...,  0.0077,  0.0034,  0.0026],
        [-0.0036, -0.0112, -0.0252,  ...,  0.0056, -0.0055,  0.0028],
        ...,
        [-0.0276, -0.0174,  0.0004,  ..., -0.0323, -0.0060, -0.0051],
        [ 0.0068, -0.0013, -0.0003,  ..., -0.0125, -0.0260,  0.0039],
        [ 0.0002,  0.0010, -0.0002,  ..., -0.0100,  0.0056, -0.0317]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.0586, -0.4341,  3.2734,  ...,  2.4180, -1.9375,  4.9297]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 01:09:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I create, he creates
I remain, he remains
I become, he becomes
I remember, he remembers
I happen, he happens
I refer, he refers
I apply, he applies
I send, he
2024-07-23 01:09:40 root INFO     [order_1_approx] starting weight calculation for I become, he becomes
I create, he creates
I remain, he remains
I remember, he remembers
I send, he sends
I happen, he happens
I apply, he applies
I refer, he
2024-07-23 01:09:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 01:13:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6577, -0.4395,  0.8682,  ..., -0.2593,  0.7388,  0.5894],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.8984,  0.4829,  0.9922,  ...,  6.4141,  1.4346,  3.4453],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0179, -0.0248,  0.0126,  ...,  0.0060, -0.0028, -0.0123],
        [-0.0137, -0.0129, -0.0103,  ...,  0.0004,  0.0093, -0.0004],
        [-0.0124, -0.0104, -0.0290,  ...,  0.0183, -0.0071, -0.0008],
        ...,
        [-0.0107, -0.0182, -0.0112,  ..., -0.0403, -0.0020, -0.0195],
        [ 0.0127,  0.0028,  0.0220,  ..., -0.0177, -0.0397,  0.0140],
        [-0.0024,  0.0199,  0.0109,  ..., -0.0021, -0.0019, -0.0363]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5811,  0.9307,  0.1797,  ...,  6.0664,  1.9062,  3.7441]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 01:13:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I become, he becomes
I create, he creates
I remain, he remains
I remember, he remembers
I send, he sends
I happen, he happens
I apply, he applies
I refer, he
2024-07-23 01:13:38 root INFO     [order_1_approx] starting weight calculation for I become, he becomes
I refer, he refers
I happen, he happens
I remember, he remembers
I remain, he remains
I create, he creates
I send, he sends
I apply, he
2024-07-23 01:13:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 01:17:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8574, -0.3157,  1.4258,  ..., -0.0977, -0.2236,  1.1172],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.2070,  0.7432, -0.2461,  ...,  2.6738, -2.9023,  5.8594],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0087, -0.0077,  0.0048,  ...,  0.0077, -0.0049, -0.0089],
        [ 0.0038, -0.0104,  0.0032,  ...,  0.0026, -0.0051,  0.0092],
        [ 0.0093,  0.0004, -0.0201,  ...,  0.0190, -0.0063, -0.0116],
        ...,
        [-0.0076, -0.0039, -0.0055,  ..., -0.0164,  0.0004,  0.0102],
        [-0.0004,  0.0056,  0.0191,  ..., -0.0210, -0.0168,  0.0027],
        [ 0.0085,  0.0136,  0.0028,  ..., -0.0008,  0.0042, -0.0192]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.0039,  0.5845, -1.7129,  ...,  3.1055, -2.4258,  5.6133]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 01:17:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I become, he becomes
I refer, he refers
I happen, he happens
I remember, he remembers
I remain, he remains
I create, he creates
I send, he sends
I apply, he
2024-07-23 01:17:36 root INFO     total operator prediction time: 1909.9490258693695 seconds
2024-07-23 01:17:36 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_Ving - 3pSg
2024-07-23 01:17:36 root INFO     building operator verb_Ving - 3pSg
2024-07-23 01:17:36 root INFO     [order_1_approx] starting weight calculation for When something is promoting, it promotes
When something is continuing, it continues
When something is following, it follows
When something is operating, it operates
When something is occurring, it occurs
When something is applying, it applies
When something is happening, it happens
When something is remaining, it
2024-07-23 01:17:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 01:21:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2062,  0.6558,  1.1270,  ...,  0.2383,  0.8389,  0.2773],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.5664,  2.4180, -3.0781,  ...,  3.5391, -0.8965,  4.5078],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0037, -0.0231,  0.0031,  ...,  0.0103, -0.0116, -0.0084],
        [-0.0060, -0.0022, -0.0188,  ..., -0.0100, -0.0014, -0.0096],
        [ 0.0133,  0.0088, -0.0230,  ...,  0.0182, -0.0089,  0.0103],
        ...,
        [-0.0042, -0.0076, -0.0109,  ..., -0.0112, -0.0001, -0.0182],
        [ 0.0012, -0.0031,  0.0086,  ...,  0.0036, -0.0238,  0.0099],
        [-0.0056, -0.0028, -0.0229,  ...,  0.0062,  0.0121, -0.0368]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.2305,  3.2832, -3.0801,  ...,  3.3438, -0.7227,  4.0117]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 01:21:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is promoting, it promotes
When something is continuing, it continues
When something is following, it follows
When something is operating, it operates
When something is occurring, it occurs
When something is applying, it applies
When something is happening, it happens
When something is remaining, it
2024-07-23 01:21:34 root INFO     [order_1_approx] starting weight calculation for When something is promoting, it promotes
When something is operating, it operates
When something is occurring, it occurs
When something is continuing, it continues
When something is happening, it happens
When something is applying, it applies
When something is remaining, it remains
When something is following, it
2024-07-23 01:21:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 01:25:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5474, -0.5044,  1.9805,  ...,  0.2729, -0.0419,  1.1387],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8516, -1.0254,  0.3174,  ...,  0.2778,  0.4092,  3.3125],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0017,  0.0044,  0.0009,  ...,  0.0065, -0.0242, -0.0160],
        [ 0.0043, -0.0131, -0.0161,  ..., -0.0116, -0.0136, -0.0072],
        [ 0.0103,  0.0006,  0.0070,  ...,  0.0085, -0.0188,  0.0054],
        ...,
        [-0.0093, -0.0069,  0.0013,  ..., -0.0173, -0.0033, -0.0033],
        [-0.0061,  0.0187,  0.0192,  ...,  0.0070, -0.0362,  0.0126],
        [-0.0151,  0.0003,  0.0008,  ..., -0.0104,  0.0120, -0.0219]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9443, -1.2471,  0.2610,  ...,  0.6870,  1.3369,  2.2266]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 01:25:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is promoting, it promotes
When something is operating, it operates
When something is occurring, it occurs
When something is continuing, it continues
When something is happening, it happens
When something is applying, it applies
When something is remaining, it remains
When something is following, it
2024-07-23 01:25:33 root INFO     [order_1_approx] starting weight calculation for When something is promoting, it promotes
When something is happening, it happens
When something is remaining, it remains
When something is operating, it operates
When something is following, it follows
When something is continuing, it continues
When something is occurring, it occurs
When something is applying, it
2024-07-23 01:25:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 01:29:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2764, -0.8418,  0.5835,  ..., -0.0923,  0.5107,  0.4065],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.4033,  0.6445,  0.7432,  ...,  4.1250, -2.8672,  3.1152],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.1444e-02, -4.7646e-03,  1.2131e-02,  ...,  8.0795e-03,
         -3.9139e-03, -3.7479e-03],
        [-5.2071e-03,  3.0403e-03,  2.5940e-04,  ..., -7.0724e-03,
         -1.1276e-02, -5.2681e-03],
        [ 1.3023e-02,  2.3529e-02, -1.3588e-02,  ...,  3.0457e-02,
         -2.8763e-03, -3.2196e-03],
        ...,
        [-8.8577e-03, -7.6370e-03, -8.8730e-03,  ..., -2.8809e-02,
          1.0635e-02,  5.1155e-03],
        [ 1.0124e-02,  8.0109e-05,  2.1729e-02,  ...,  8.0872e-04,
         -8.8348e-03, -4.8828e-04],
        [-5.5885e-04, -3.8300e-03,  4.8447e-04,  ...,  4.4327e-03,
          1.7456e-02, -8.5144e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6572,  1.2266,  0.1616,  ...,  4.1484, -2.8379,  2.5156]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 01:29:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is promoting, it promotes
When something is happening, it happens
When something is remaining, it remains
When something is operating, it operates
When something is following, it follows
When something is continuing, it continues
When something is occurring, it occurs
When something is applying, it
2024-07-23 01:29:32 root INFO     [order_1_approx] starting weight calculation for When something is following, it follows
When something is continuing, it continues
When something is applying, it applies
When something is promoting, it promotes
When something is happening, it happens
When something is remaining, it remains
When something is occurring, it occurs
When something is operating, it
2024-07-23 01:29:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 01:33:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4402, -0.7695,  0.4849,  ..., -0.5283,  0.7944,  0.2124],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1602, -1.6016,  0.1201,  ...,  1.7432,  1.2197,  1.9697],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 8.6594e-04,  6.6719e-03,  4.3373e-03,  ...,  1.6861e-02,
         -3.6106e-03, -6.7062e-03],
        [ 6.5041e-04, -4.6844e-03,  4.9591e-05,  ..., -7.0152e-03,
         -7.4158e-03, -3.2043e-03],
        [ 1.1345e-02,  9.3613e-03, -8.1635e-03,  ...,  2.7283e-02,
          8.1635e-03,  2.4223e-03],
        ...,
        [-1.0071e-02, -3.9253e-03, -2.0966e-02,  ..., -2.3453e-02,
         -9.8419e-03,  2.0676e-03],
        [ 7.5340e-03, -3.3569e-04, -7.4005e-04,  ...,  1.8387e-03,
         -2.2949e-02,  8.1329e-03],
        [ 9.0637e-03,  4.2572e-03,  1.0201e-02,  ..., -8.6823e-03,
          1.8234e-02, -3.5522e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9282, -1.0703,  0.1274,  ...,  2.0508,  0.4126,  1.5078]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 01:33:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is following, it follows
When something is continuing, it continues
When something is applying, it applies
When something is promoting, it promotes
When something is happening, it happens
When something is remaining, it remains
When something is occurring, it occurs
When something is operating, it
2024-07-23 01:33:31 root INFO     [order_1_approx] starting weight calculation for When something is promoting, it promotes
When something is happening, it happens
When something is remaining, it remains
When something is applying, it applies
When something is following, it follows
When something is operating, it operates
When something is occurring, it occurs
When something is continuing, it
2024-07-23 01:33:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 01:37:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5225,  0.1621,  0.7969,  ..., -0.2261,  1.0576,  0.4028],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1211, -1.6436, -2.1465,  ...,  2.9258, -1.4473,  1.6494],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-8.6441e-03, -1.6647e-02,  1.0996e-03,  ...,  4.6997e-03,
         -7.3586e-03, -1.4328e-02],
        [ 7.8201e-05, -1.3756e-02, -8.9951e-03,  ..., -8.6975e-03,
         -3.0289e-03, -8.1253e-03],
        [ 1.4954e-02,  3.2120e-03, -2.4292e-02,  ...,  2.0157e-02,
          1.3170e-03,  1.0529e-03],
        ...,
        [-3.1189e-02,  1.0056e-02, -1.5823e-02,  ..., -1.1375e-02,
         -2.5177e-03, -1.9646e-03],
        [ 1.2405e-02,  1.8158e-03,  6.7825e-03,  ...,  1.0971e-02,
         -2.2461e-02,  1.6830e-02],
        [-8.0872e-03, -2.3117e-03, -5.8250e-03,  ..., -4.0894e-03,
          7.6294e-03, -4.4525e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.4121, -1.4932, -2.0703,  ...,  3.8965, -2.0703,  1.3320]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 01:37:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is promoting, it promotes
When something is happening, it happens
When something is remaining, it remains
When something is applying, it applies
When something is following, it follows
When something is operating, it operates
When something is occurring, it occurs
When something is continuing, it
2024-07-23 01:37:30 root INFO     [order_1_approx] starting weight calculation for When something is operating, it operates
When something is occurring, it occurs
When something is applying, it applies
When something is continuing, it continues
When something is promoting, it promotes
When something is remaining, it remains
When something is following, it follows
When something is happening, it
2024-07-23 01:37:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 01:41:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4751,  0.4097,  1.6152,  ..., -0.2354,  1.2070, -0.4197],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7725, -0.7256, -1.9297,  ..., -1.0176,  4.2578,  2.5801],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0028, -0.0384,  0.0088,  ...,  0.0168, -0.0112, -0.0006],
        [-0.0029,  0.0141, -0.0090,  ..., -0.0048, -0.0079, -0.0042],
        [ 0.0059,  0.0173, -0.0332,  ...,  0.0025,  0.0163,  0.0016],
        ...,
        [-0.0178, -0.0191, -0.0216,  ..., -0.0216, -0.0113, -0.0066],
        [ 0.0111,  0.0177,  0.0047,  ..., -0.0019, -0.0399,  0.0044],
        [-0.0437, -0.0077,  0.0113,  ...,  0.0017,  0.0049, -0.0164]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9790,  0.2793, -3.3535,  ..., -1.6943,  3.4492,  3.1562]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 01:41:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is operating, it operates
When something is occurring, it occurs
When something is applying, it applies
When something is continuing, it continues
When something is promoting, it promotes
When something is remaining, it remains
When something is following, it follows
When something is happening, it
2024-07-23 01:41:29 root INFO     [order_1_approx] starting weight calculation for When something is operating, it operates
When something is remaining, it remains
When something is applying, it applies
When something is following, it follows
When something is continuing, it continues
When something is happening, it happens
When something is occurring, it occurs
When something is promoting, it
2024-07-23 01:41:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 01:45:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5049, -0.5688,  0.6855,  ...,  0.3853,  0.3406,  0.1155],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8994, -0.5146,  1.5000,  ...,  3.7188, -0.5127,  2.3477],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0145, -0.0336,  0.0041,  ..., -0.0094, -0.0139, -0.0153],
        [-0.0092,  0.0029,  0.0061,  ..., -0.0014, -0.0004, -0.0102],
        [ 0.0168,  0.0108, -0.0165,  ...,  0.0077,  0.0055, -0.0057],
        ...,
        [ 0.0052, -0.0055, -0.0005,  ..., -0.0100, -0.0056, -0.0141],
        [-0.0026,  0.0074, -0.0015,  ..., -0.0201, -0.0380,  0.0137],
        [ 0.0039, -0.0197, -0.0036,  ..., -0.0008,  0.0138,  0.0016]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4531,  0.2173,  1.8350,  ...,  3.4023, -1.5830,  2.3457]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 01:45:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is operating, it operates
When something is remaining, it remains
When something is applying, it applies
When something is following, it follows
When something is continuing, it continues
When something is happening, it happens
When something is occurring, it occurs
When something is promoting, it
2024-07-23 01:45:28 root INFO     [order_1_approx] starting weight calculation for When something is continuing, it continues
When something is promoting, it promotes
When something is operating, it operates
When something is remaining, it remains
When something is following, it follows
When something is applying, it applies
When something is happening, it happens
When something is occurring, it
2024-07-23 01:45:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 01:49:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4751, -0.0898,  2.2031,  ...,  0.2534,  1.2158, -0.1112],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.5557, -1.0703, -1.1660,  ..., -0.6846,  2.7090,  1.4570],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.2278e-03, -2.2812e-02, -1.2123e-02,  ..., -1.2207e-04,
          1.0666e-02, -8.8730e-03],
        [-9.9182e-03, -3.2623e-02, -1.3489e-02,  ..., -5.3368e-03,
         -1.5854e-02, -6.3477e-03],
        [ 1.9592e-02, -1.8066e-02, -3.4851e-02,  ...,  7.8125e-03,
         -2.7054e-02,  4.4937e-03],
        ...,
        [-3.5980e-02, -3.0930e-02, -8.5983e-03,  ..., -4.7638e-02,
         -5.3406e-05, -1.0864e-02],
        [-1.2062e-02,  1.1490e-02,  7.8430e-03,  ...,  4.3640e-03,
         -4.1718e-02,  2.8290e-02],
        [-1.1040e-02, -1.2741e-03,  2.9633e-02,  ..., -3.0792e-02,
          1.9348e-02, -4.5929e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4463, -1.1309, -1.4727,  ...,  0.0142,  0.7188,  1.7090]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 01:49:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is continuing, it continues
When something is promoting, it promotes
When something is operating, it operates
When something is remaining, it remains
When something is following, it follows
When something is applying, it applies
When something is happening, it happens
When something is occurring, it
2024-07-23 01:49:27 root INFO     total operator prediction time: 1910.630627155304 seconds
2024-07-23 01:49:27 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on noun - plural_reg
2024-07-23 01:49:27 root INFO     building operator noun - plural_reg
2024-07-23 01:49:27 root INFO     [order_1_approx] starting weight calculation for The plural form of problem is problems
The plural form of week is weeks
The plural form of village is villages
The plural form of department is departments
The plural form of role is roles
The plural form of product is products
The plural form of example is examples
The plural form of god is
2024-07-23 01:49:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 01:53:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5254, -0.1997, -0.5762,  ..., -0.4272, -0.5034, -0.4795],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.4629, -0.7393, -0.2012,  ...,  0.6016, -0.1973,  1.2529],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0155, -0.0171,  0.0193,  ...,  0.0085,  0.0071, -0.0106],
        [-0.0020, -0.0247, -0.0028,  ...,  0.0007, -0.0006, -0.0020],
        [ 0.0141, -0.0056, -0.0218,  ...,  0.0036, -0.0120, -0.0060],
        ...,
        [-0.0125,  0.0078, -0.0133,  ..., -0.0299,  0.0181, -0.0016],
        [-0.0028, -0.0051, -0.0014,  ..., -0.0206, -0.0352,  0.0096],
        [-0.0076,  0.0054, -0.0038,  ..., -0.0028,  0.0066, -0.0174]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.0273, -0.6733, -0.0957,  ...,  0.0254, -0.6782,  1.3682]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 01:53:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of problem is problems
The plural form of week is weeks
The plural form of village is villages
The plural form of department is departments
The plural form of role is roles
The plural form of product is products
The plural form of example is examples
The plural form of god is
2024-07-23 01:53:27 root INFO     [order_1_approx] starting weight calculation for The plural form of god is gods
The plural form of week is weeks
The plural form of example is examples
The plural form of problem is problems
The plural form of department is departments
The plural form of role is roles
The plural form of product is products
The plural form of village is
2024-07-23 01:53:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 01:57:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3154, -0.5039, -0.0374,  ..., -0.3862, -0.1786,  0.7354],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.3477,  0.5366, -1.5605,  ..., -1.6396,  3.7051, -0.8154],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0183, -0.0165,  0.0210,  ..., -0.0006,  0.0087, -0.0151],
        [-0.0117, -0.0076, -0.0027,  ...,  0.0086,  0.0024, -0.0046],
        [ 0.0153, -0.0026, -0.0130,  ..., -0.0153, -0.0003, -0.0128],
        ...,
        [ 0.0026, -0.0100, -0.0018,  ..., -0.0156,  0.0178, -0.0072],
        [-0.0099,  0.0043, -0.0060,  ..., -0.0048, -0.0064, -0.0060],
        [-0.0161,  0.0218,  0.0017,  ..., -0.0067, -0.0014, -0.0116]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.7988,  0.7354, -1.6113,  ..., -1.7021,  3.3867, -1.0322]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 01:57:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of god is gods
The plural form of week is weeks
The plural form of example is examples
The plural form of problem is problems
The plural form of department is departments
The plural form of role is roles
The plural form of product is products
The plural form of village is
2024-07-23 01:57:27 root INFO     [order_1_approx] starting weight calculation for The plural form of role is roles
The plural form of village is villages
The plural form of example is examples
The plural form of department is departments
The plural form of god is gods
The plural form of problem is problems
The plural form of product is products
The plural form of week is
2024-07-23 01:57:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 02:01:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.0039,  0.1206, -0.8916,  ...,  0.1904, -0.9727,  0.6226],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1504, -2.9258,  1.1201,  ...,  1.6172,  3.6465,  3.1270],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0166, -0.0111,  0.0155,  ...,  0.0108,  0.0035, -0.0116],
        [-0.0095,  0.0024,  0.0003,  ..., -0.0057,  0.0037, -0.0070],
        [ 0.0051, -0.0138, -0.0294,  ..., -0.0085,  0.0167, -0.0003],
        ...,
        [-0.0149, -0.0177, -0.0063,  ..., -0.0096, -0.0034, -0.0076],
        [ 0.0127, -0.0066,  0.0233,  ..., -0.0142, -0.0408,  0.0263],
        [-0.0079,  0.0061,  0.0067,  ..., -0.0096,  0.0284, -0.0267]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3220, -2.5254,  1.8848,  ...,  1.9932,  3.7500,  4.2930]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 02:01:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of role is roles
The plural form of village is villages
The plural form of example is examples
The plural form of department is departments
The plural form of god is gods
The plural form of problem is problems
The plural form of product is products
The plural form of week is
2024-07-23 02:01:26 root INFO     [order_1_approx] starting weight calculation for The plural form of village is villages
The plural form of role is roles
The plural form of example is examples
The plural form of problem is problems
The plural form of week is weeks
The plural form of product is products
The plural form of god is gods
The plural form of department is
2024-07-23 02:01:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 02:05:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4453,  0.3301, -0.6211,  ...,  0.2148,  0.2451,  0.3979],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7666, -2.3438,  2.2070,  ..., -2.0117,  0.3672,  3.2559],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0173, -0.0041, -0.0027,  ..., -0.0113, -0.0038, -0.0050],
        [-0.0077, -0.0176,  0.0115,  ..., -0.0087,  0.0005,  0.0004],
        [ 0.0061, -0.0051, -0.0085,  ...,  0.0011,  0.0039, -0.0032],
        ...,
        [-0.0187, -0.0244, -0.0085,  ..., -0.0046,  0.0166,  0.0013],
        [ 0.0107, -0.0128,  0.0238,  ..., -0.0060, -0.0237,  0.0186],
        [-0.0046,  0.0079, -0.0146,  ...,  0.0114,  0.0123, -0.0073]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3435, -2.3359,  2.1855,  ..., -2.1973,  0.3320,  3.7871]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 02:05:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of village is villages
The plural form of role is roles
The plural form of example is examples
The plural form of problem is problems
The plural form of week is weeks
The plural form of product is products
The plural form of god is gods
The plural form of department is
2024-07-23 02:05:25 root INFO     [order_1_approx] starting weight calculation for The plural form of product is products
The plural form of role is roles
The plural form of problem is problems
The plural form of god is gods
The plural form of department is departments
The plural form of week is weeks
The plural form of village is villages
The plural form of example is
2024-07-23 02:05:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 02:09:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.9990,  2.3945,  0.6455,  ..., -1.2031,  0.7500,  0.2832],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5674, -1.4121,  0.9990,  ..., -1.4160,  2.2266,  6.2500],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0155, -0.0015,  0.0248,  ..., -0.0123,  0.0030, -0.0409],
        [ 0.0046, -0.0049, -0.0004,  ..., -0.0099, -0.0013, -0.0024],
        [ 0.0024, -0.0211, -0.0300,  ...,  0.0140,  0.0093,  0.0044],
        ...,
        [-0.0085, -0.0027,  0.0122,  ..., -0.0149,  0.0169, -0.0069],
        [ 0.0161,  0.0156,  0.0093,  ..., -0.0024, -0.0315,  0.0088],
        [-0.0003, -0.0048,  0.0007,  ..., -0.0080,  0.0188, -0.0070]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4861, -1.3887,  1.8975,  ..., -1.5791,  2.0527,  6.7500]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 02:09:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of product is products
The plural form of role is roles
The plural form of problem is problems
The plural form of god is gods
The plural form of department is departments
The plural form of week is weeks
The plural form of village is villages
The plural form of example is
2024-07-23 02:09:21 root INFO     [order_1_approx] starting weight calculation for The plural form of role is roles
The plural form of village is villages
The plural form of department is departments
The plural form of god is gods
The plural form of example is examples
The plural form of week is weeks
The plural form of product is products
The plural form of problem is
2024-07-23 02:09:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 02:13:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9258,  1.2500,  0.0099,  ..., -0.9971,  1.9121, -0.0148],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8560,  0.5410,  3.0781,  ...,  0.6040, -0.4561,  3.4453],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0165,  0.0093,  0.0247,  ...,  0.0057, -0.0104, -0.0276],
        [-0.0009, -0.0060, -0.0164,  ..., -0.0109, -0.0007, -0.0111],
        [ 0.0094, -0.0062, -0.0254,  ...,  0.0153,  0.0001,  0.0185],
        ...,
        [-0.0035, -0.0136,  0.0144,  ..., -0.0263,  0.0238, -0.0173],
        [ 0.0080,  0.0257,  0.0226,  ..., -0.0129, -0.0352,  0.0157],
        [-0.0145, -0.0015, -0.0008,  ..., -0.0049,  0.0249, -0.0164]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3301,  0.6460,  2.3613,  ...,  1.1670, -0.1948,  3.6934]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 02:13:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of role is roles
The plural form of village is villages
The plural form of department is departments
The plural form of god is gods
The plural form of example is examples
The plural form of week is weeks
The plural form of product is products
The plural form of problem is
2024-07-23 02:13:15 root INFO     [order_1_approx] starting weight calculation for The plural form of problem is problems
The plural form of god is gods
The plural form of product is products
The plural form of week is weeks
The plural form of village is villages
The plural form of department is departments
The plural form of example is examples
The plural form of role is
2024-07-23 02:13:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 02:17:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3691,  0.3860,  0.3225,  ...,  0.4204, -0.1941, -0.4280],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.8320, -1.0566,  1.5273,  ...,  1.8623,  1.9688,  3.4082],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0165,  0.0061,  0.0230,  ...,  0.0020,  0.0031, -0.0160],
        [-0.0056, -0.0285,  0.0019,  ..., -0.0027,  0.0084, -0.0017],
        [ 0.0077, -0.0091, -0.0262,  ...,  0.0027,  0.0013,  0.0045],
        ...,
        [-0.0266, -0.0099, -0.0177,  ..., -0.0217,  0.0168,  0.0078],
        [-0.0039,  0.0255, -0.0069,  ..., -0.0248, -0.0221,  0.0250],
        [-0.0102,  0.0199, -0.0051,  ..., -0.0067,  0.0272, -0.0072]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.3125, -0.8896,  1.8965,  ...,  1.3760,  1.5146,  4.2891]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 02:17:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of problem is problems
The plural form of god is gods
The plural form of product is products
The plural form of week is weeks
The plural form of village is villages
The plural form of department is departments
The plural form of example is examples
The plural form of role is
2024-07-23 02:17:13 root INFO     [order_1_approx] starting weight calculation for The plural form of problem is problems
The plural form of department is departments
The plural form of role is roles
The plural form of god is gods
The plural form of week is weeks
The plural form of example is examples
The plural form of village is villages
The plural form of product is
2024-07-23 02:17:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 02:21:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3242,  1.0713,  0.5137,  ...,  0.1055,  0.2561, -0.3230],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.2148, -3.2637,  1.2227,  ...,  1.4385, -0.8125,  4.0430],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0169,  0.0033,  0.0190,  ...,  0.0088, -0.0104, -0.0190],
        [ 0.0007, -0.0141,  0.0005,  ...,  0.0086,  0.0021, -0.0118],
        [ 0.0134, -0.0020, -0.0214,  ..., -0.0020, -0.0014,  0.0090],
        ...,
        [-0.0042, -0.0152, -0.0090,  ..., -0.0205,  0.0119,  0.0098],
        [ 0.0004,  0.0070,  0.0018,  ..., -0.0012, -0.0335,  0.0018],
        [-0.0132, -0.0013, -0.0007,  ..., -0.0114,  0.0226, -0.0208]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.5059, -3.1543,  1.4941,  ...,  0.7402, -0.5381,  4.3828]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 02:21:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of problem is problems
The plural form of department is departments
The plural form of role is roles
The plural form of god is gods
The plural form of week is weeks
The plural form of example is examples
The plural form of village is villages
The plural form of product is
2024-07-23 02:21:12 root INFO     total operator prediction time: 1904.98725771904 seconds
2024-07-23 02:21:12 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_3pSg - Ved
2024-07-23 02:21:12 root INFO     building operator verb_3pSg - Ved
2024-07-23 02:21:12 root INFO     [order_1_approx] starting weight calculation for When he expects something, something has been expected
When he becomes something, something has been became
When he consists something, something has been consisted
When he operates something, something has been operated
When he believes something, something has been believed
When he appears something, something has been appeared
When he replaces something, something has been replaced
When he provides something, something has been
2024-07-23 02:21:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 02:25:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4961,  0.7217, -0.1182,  ..., -0.2004,  0.1447,  0.4368],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8350,  0.4663, -0.8350,  ..., -0.2622, -0.5693,  0.4150],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0035,  0.0174,  0.0125,  ...,  0.0147, -0.0033, -0.0265],
        [ 0.0131, -0.0124, -0.0062,  ..., -0.0015, -0.0085,  0.0048],
        [ 0.0100, -0.0006, -0.0176,  ...,  0.0058, -0.0082,  0.0031],
        ...,
        [-0.0243,  0.0042, -0.0103,  ..., -0.0040, -0.0077, -0.0016],
        [ 0.0219,  0.0105,  0.0172,  ..., -0.0157, -0.0316,  0.0074],
        [-0.0006,  0.0073, -0.0041,  ...,  0.0062,  0.0251, -0.0343]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.8604,  0.7012, -0.1289,  ..., -0.1711, -0.5771, -0.8223]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 02:25:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he expects something, something has been expected
When he becomes something, something has been became
When he consists something, something has been consisted
When he operates something, something has been operated
When he believes something, something has been believed
When he appears something, something has been appeared
When he replaces something, something has been replaced
When he provides something, something has been
2024-07-23 02:25:11 root INFO     [order_1_approx] starting weight calculation for When he expects something, something has been expected
When he consists something, something has been consisted
When he provides something, something has been provided
When he operates something, something has been operated
When he replaces something, something has been replaced
When he appears something, something has been appeared
When he believes something, something has been believed
When he becomes something, something has been
2024-07-23 02:25:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 02:29:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1367,  0.7261,  1.3662,  ...,  0.7373,  0.4941, -0.1450],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.1445,  0.1425,  0.5859,  ..., -0.2461,  1.2871, -0.2773],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0258, -0.0264,  0.0201,  ..., -0.0016, -0.0010, -0.0339],
        [-0.0099, -0.0056, -0.0043,  ..., -0.0097,  0.0078, -0.0078],
        [ 0.0167,  0.0055, -0.0289,  ...,  0.0175, -0.0112,  0.0086],
        ...,
        [-0.0136, -0.0204,  0.0009,  ..., -0.0200, -0.0007,  0.0206],
        [ 0.0153,  0.0006, -0.0006,  ..., -0.0286, -0.0291,  0.0007],
        [-0.0016,  0.0236, -0.0108,  ..., -0.0055,  0.0036, -0.0524]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4785, -0.2079,  1.1328,  ..., -0.0691,  1.7256,  0.7402]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 02:29:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he expects something, something has been expected
When he consists something, something has been consisted
When he provides something, something has been provided
When he operates something, something has been operated
When he replaces something, something has been replaced
When he appears something, something has been appeared
When he believes something, something has been believed
When he becomes something, something has been
2024-07-23 02:29:10 root INFO     [order_1_approx] starting weight calculation for When he expects something, something has been expected
When he becomes something, something has been became
When he believes something, something has been believed
When he operates something, something has been operated
When he consists something, something has been consisted
When he appears something, something has been appeared
When he provides something, something has been provided
When he replaces something, something has been
2024-07-23 02:29:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 02:33:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2498,  0.2856,  0.5449,  ...,  0.1401, -0.0073, -0.2622],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.0137,  1.8262, -0.2432,  ...,  2.3496,  1.8281,  4.5625],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.4236e-02,  8.4639e-05,  1.0956e-02,  ..., -1.8539e-02,
         -1.0147e-02, -1.0902e-02],
        [-1.5305e-02, -1.1887e-02, -2.2545e-03,  ...,  1.0452e-02,
          2.2182e-03,  5.8060e-03],
        [ 5.0430e-03,  2.7733e-03, -1.7319e-02,  ...,  1.0437e-02,
         -4.3449e-03, -2.8038e-04],
        ...,
        [-2.6703e-02, -1.3496e-02, -1.0567e-02,  ..., -5.0697e-03,
         -4.3449e-03, -4.7989e-03],
        [-1.1978e-03,  1.4221e-02, -3.6087e-03,  ..., -5.4436e-03,
         -2.7939e-02, -1.7281e-03],
        [ 2.0218e-03,  3.8700e-03, -6.1417e-03,  ..., -1.0155e-02,
          1.3504e-03, -5.3375e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[2.7988, 2.5020, 0.0515,  ..., 1.7070, 2.3613, 3.8320]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 02:33:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he expects something, something has been expected
When he becomes something, something has been became
When he believes something, something has been believed
When he operates something, something has been operated
When he consists something, something has been consisted
When he appears something, something has been appeared
When he provides something, something has been provided
When he replaces something, something has been
2024-07-23 02:33:10 root INFO     [order_1_approx] starting weight calculation for When he provides something, something has been provided
When he replaces something, something has been replaced
When he becomes something, something has been became
When he operates something, something has been operated
When he believes something, something has been believed
When he consists something, something has been consisted
When he appears something, something has been appeared
When he expects something, something has been
2024-07-23 02:33:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 02:37:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0928,  0.8901, -0.7368,  ...,  0.2920,  1.5732,  0.7979],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.9023, -0.0811,  0.8354,  ...,  0.4507, -1.0371,  0.8691],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0203, -0.0157,  0.0221,  ...,  0.0091, -0.0203, -0.0325],
        [-0.0167, -0.0028, -0.0082,  ...,  0.0072,  0.0036,  0.0131],
        [ 0.0103,  0.0123, -0.0150,  ...,  0.0172, -0.0148,  0.0139],
        ...,
        [-0.0102, -0.0101, -0.0118,  ..., -0.0043, -0.0063, -0.0011],
        [ 0.0067, -0.0014, -0.0106,  ..., -0.0145, -0.0382,  0.0022],
        [-0.0208,  0.0039,  0.0006,  ..., -0.0090,  0.0181, -0.0440]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.8066,  0.5977,  1.3887,  ...,  0.0054, -0.8105,  0.2754]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 02:37:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he provides something, something has been provided
When he replaces something, something has been replaced
When he becomes something, something has been became
When he operates something, something has been operated
When he believes something, something has been believed
When he consists something, something has been consisted
When he appears something, something has been appeared
When he expects something, something has been
2024-07-23 02:37:10 root INFO     [order_1_approx] starting weight calculation for When he believes something, something has been believed
When he replaces something, something has been replaced
When he appears something, something has been appeared
When he becomes something, something has been became
When he expects something, something has been expected
When he operates something, something has been operated
When he provides something, something has been provided
When he consists something, something has been
2024-07-23 02:37:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 02:41:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0352,  0.5303,  0.4460,  ...,  0.6875,  0.5527,  0.8408],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9854,  2.4141, -1.0205,  ...,  1.9043, -5.9570,  1.8506],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.6459e-02,  5.9814e-03, -3.6564e-03,  ...,  1.3969e-02,
         -9.5825e-03, -2.3514e-02],
        [-1.4519e-02,  4.5776e-05,  2.6230e-02,  ..., -1.4153e-02,
         -1.8860e-02,  3.3951e-03],
        [ 2.3994e-03, -6.6681e-03, -2.5024e-03,  ..., -1.4221e-02,
          5.4436e-03,  3.2501e-03],
        ...,
        [-3.4027e-02, -1.1765e-02, -3.0041e-03,  ..., -2.4628e-02,
          1.3786e-02,  2.4353e-02],
        [-1.0117e-02,  1.1139e-03,  6.4087e-03,  ..., -1.5022e-02,
         -1.3397e-02, -8.6212e-03],
        [-1.6403e-02,  1.6159e-02, -2.2186e-02,  ...,  1.1139e-03,
          5.1689e-03, -3.8330e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1309,  2.2734, -0.6279,  ...,  1.4336, -4.8047,  1.4697]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 02:41:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he believes something, something has been believed
When he replaces something, something has been replaced
When he appears something, something has been appeared
When he becomes something, something has been became
When he expects something, something has been expected
When he operates something, something has been operated
When he provides something, something has been provided
When he consists something, something has been
2024-07-23 02:41:10 root INFO     [order_1_approx] starting weight calculation for When he provides something, something has been provided
When he believes something, something has been believed
When he appears something, something has been appeared
When he becomes something, something has been became
When he expects something, something has been expected
When he consists something, something has been consisted
When he replaces something, something has been replaced
When he operates something, something has been
2024-07-23 02:41:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 02:45:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2139,  0.7832, -0.1929,  ...,  0.1841,  0.4138,  0.5791],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3899, -2.3984,  0.3633,  ..., -0.8945,  2.0684,  0.0156],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0057, -0.0031, -0.0004,  ...,  0.0107, -0.0170, -0.0108],
        [-0.0088,  0.0015, -0.0026,  ...,  0.0044, -0.0017,  0.0095],
        [ 0.0019,  0.0128, -0.0220,  ..., -0.0023,  0.0073, -0.0110],
        ...,
        [-0.0276, -0.0016, -0.0113,  ..., -0.0259, -0.0023,  0.0099],
        [-0.0071, -0.0095, -0.0061,  ..., -0.0187, -0.0238, -0.0043],
        [ 0.0008,  0.0174,  0.0015,  ..., -0.0051,  0.0113, -0.0303]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6055, -2.0684,  1.1621,  ..., -1.2656,  2.3926, -1.0967]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 02:45:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he provides something, something has been provided
When he believes something, something has been believed
When he appears something, something has been appeared
When he becomes something, something has been became
When he expects something, something has been expected
When he consists something, something has been consisted
When he replaces something, something has been replaced
When he operates something, something has been
2024-07-23 02:45:09 root INFO     [order_1_approx] starting weight calculation for When he expects something, something has been expected
When he becomes something, something has been became
When he provides something, something has been provided
When he replaces something, something has been replaced
When he consists something, something has been consisted
When he believes something, something has been believed
When he operates something, something has been operated
When he appears something, something has been
2024-07-23 02:45:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 02:49:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1699,  0.4761,  0.9121,  ..., -0.1620,  0.2234,  0.2612],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0312,  0.8447,  1.5273,  ...,  0.1426, -3.8496, -2.9199],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0014, -0.0009,  0.0012,  ...,  0.0331, -0.0058, -0.0162],
        [-0.0118, -0.0020,  0.0074,  ..., -0.0090, -0.0120,  0.0074],
        [ 0.0194, -0.0058, -0.0164,  ...,  0.0112, -0.0180, -0.0019],
        ...,
        [-0.0078, -0.0009, -0.0047,  ..., -0.0064,  0.0070, -0.0085],
        [ 0.0127, -0.0020,  0.0021,  ..., -0.0090, -0.0167, -0.0047],
        [-0.0215,  0.0065, -0.0003,  ...,  0.0186, -0.0149, -0.0201]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.2969,  1.8232,  0.9424,  ..., -0.2786, -3.8047, -4.0781]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 02:49:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he expects something, something has been expected
When he becomes something, something has been became
When he provides something, something has been provided
When he replaces something, something has been replaced
When he consists something, something has been consisted
When he believes something, something has been believed
When he operates something, something has been operated
When he appears something, something has been
2024-07-23 02:49:09 root INFO     [order_1_approx] starting weight calculation for When he expects something, something has been expected
When he appears something, something has been appeared
When he operates something, something has been operated
When he provides something, something has been provided
When he consists something, something has been consisted
When he becomes something, something has been became
When he replaces something, something has been replaced
When he believes something, something has been
2024-07-23 02:49:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 02:53:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5693,  0.8457, -0.3198,  ...,  0.0181,  1.1553,  0.1187],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3242, -1.5205,  0.6406,  ...,  0.9844, -4.9297,  1.0293],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0358, -0.0163,  0.0190,  ...,  0.0043,  0.0043, -0.0317],
        [-0.0005, -0.0064,  0.0065,  ...,  0.0023,  0.0141,  0.0038],
        [ 0.0181,  0.0153, -0.0037,  ...,  0.0101,  0.0056,  0.0136],
        ...,
        [-0.0358, -0.0056, -0.0001,  ..., -0.0152, -0.0067,  0.0055],
        [-0.0140,  0.0113,  0.0004,  ..., -0.0324, -0.0361, -0.0027],
        [-0.0102,  0.0204, -0.0100,  ...,  0.0131,  0.0169, -0.0436]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3779, -1.3750,  0.9258,  ...,  1.6250, -4.7812,  1.4648]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 02:53:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he expects something, something has been expected
When he appears something, something has been appeared
When he operates something, something has been operated
When he provides something, something has been provided
When he consists something, something has been consisted
When he becomes something, something has been became
When he replaces something, something has been replaced
When he believes something, something has been
2024-07-23 02:53:09 root INFO     total operator prediction time: 1917.1561875343323 seconds
2024-07-23 02:53:09 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on adj - superlative
2024-07-23 02:53:09 root INFO     building operator adj - superlative
2024-07-23 02:53:09 root INFO     [order_1_approx] starting weight calculation for If something is the most shiny, it is shiniest
If something is the most noisy, it is noisiest
If something is the most harsh, it is harshest
If something is the most sunny, it is sunniest
If something is the most wealthy, it is wealthiest
If something is the most neat, it is neatest
If something is the most strange, it is strangest
If something is the most mild, it is
2024-07-23 02:53:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 02:57:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4048, -0.0551,  1.1680,  ...,  0.1807, -0.2517,  1.0449],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.0352,  0.9336, -5.2227,  ..., -0.8916, -1.3955,  3.7773],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0007, -0.0156,  0.0239,  ..., -0.0141, -0.0109, -0.0099],
        [-0.0198, -0.0157, -0.0043,  ..., -0.0119,  0.0034, -0.0055],
        [-0.0056,  0.0119, -0.0144,  ...,  0.0174, -0.0054,  0.0052],
        ...,
        [-0.0135, -0.0207,  0.0141,  ..., -0.0242, -0.0079,  0.0041],
        [ 0.0064,  0.0109,  0.0142,  ..., -0.0124, -0.0445,  0.0183],
        [-0.0180, -0.0236, -0.0093,  ..., -0.0096,  0.0130, -0.0461]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.4297,  1.0820, -5.4023,  ..., -1.1074, -1.1670,  3.3320]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 02:57:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most shiny, it is shiniest
If something is the most noisy, it is noisiest
If something is the most harsh, it is harshest
If something is the most sunny, it is sunniest
If something is the most wealthy, it is wealthiest
If something is the most neat, it is neatest
If something is the most strange, it is strangest
If something is the most mild, it is
2024-07-23 02:57:08 root INFO     [order_1_approx] starting weight calculation for If something is the most neat, it is neatest
If something is the most harsh, it is harshest
If something is the most mild, it is mildest
If something is the most shiny, it is shiniest
If something is the most noisy, it is noisiest
If something is the most wealthy, it is wealthiest
If something is the most sunny, it is sunniest
If something is the most strange, it is
2024-07-23 02:57:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 03:01:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8730, -0.2095,  0.2788,  ..., -0.2258,  1.2354, -0.1523],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.3750, -0.6729, -0.5864,  ...,  6.6445,  0.6699,  5.3594],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0090, -0.0234,  0.0038,  ..., -0.0134,  0.0001, -0.0300],
        [-0.0134,  0.0048, -0.0008,  ...,  0.0089, -0.0118, -0.0039],
        [-0.0034,  0.0042, -0.0154,  ...,  0.0137, -0.0114, -0.0144],
        ...,
        [-0.0046, -0.0218,  0.0042,  ..., -0.0246,  0.0126, -0.0304],
        [ 0.0087,  0.0036,  0.0229,  ..., -0.0056, -0.0424,  0.0109],
        [-0.0244,  0.0128, -0.0098,  ..., -0.0215,  0.0074, -0.0465]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.2148, -0.5361, -0.5078,  ...,  5.6367,  0.1421,  5.0703]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 03:01:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most neat, it is neatest
If something is the most harsh, it is harshest
If something is the most mild, it is mildest
If something is the most shiny, it is shiniest
If something is the most noisy, it is noisiest
If something is the most wealthy, it is wealthiest
If something is the most sunny, it is sunniest
If something is the most strange, it is
2024-07-23 03:01:07 root INFO     [order_1_approx] starting weight calculation for If something is the most mild, it is mildest
If something is the most harsh, it is harshest
If something is the most neat, it is neatest
If something is the most shiny, it is shiniest
If something is the most strange, it is strangest
If something is the most sunny, it is sunniest
If something is the most wealthy, it is wealthiest
If something is the most noisy, it is
2024-07-23 03:01:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 03:05:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0571,  0.3806,  1.0723,  ..., -0.1304,  0.6157,  0.3208],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3452, -2.6543, -2.0215,  ...,  3.6523,  2.9258,  4.5312],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0007, -0.0205, -0.0008,  ...,  0.0128, -0.0123, -0.0090],
        [-0.0056,  0.0002,  0.0056,  ...,  0.0044, -0.0153, -0.0053],
        [-0.0037,  0.0227, -0.0122,  ..., -0.0047,  0.0189,  0.0045],
        ...,
        [-0.0028, -0.0147, -0.0048,  ..., -0.0284, -0.0104, -0.0140],
        [ 0.0073,  0.0035,  0.0099,  ..., -0.0024, -0.0434,  0.0084],
        [-0.0132, -0.0187, -0.0034,  ..., -0.0317, -0.0017, -0.0390]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.8394, -1.6846, -2.3066,  ...,  3.5078,  2.2852,  4.2656]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 03:05:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most mild, it is mildest
If something is the most harsh, it is harshest
If something is the most neat, it is neatest
If something is the most shiny, it is shiniest
If something is the most strange, it is strangest
If something is the most sunny, it is sunniest
If something is the most wealthy, it is wealthiest
If something is the most noisy, it is
2024-07-23 03:05:06 root INFO     [order_1_approx] starting weight calculation for If something is the most harsh, it is harshest
If something is the most mild, it is mildest
If something is the most neat, it is neatest
If something is the most noisy, it is noisiest
If something is the most sunny, it is sunniest
If something is the most strange, it is strangest
If something is the most shiny, it is shiniest
If something is the most wealthy, it is
2024-07-23 03:05:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 03:09:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.2925, 0.7480, 0.9102,  ..., 0.3372, 1.3242, 0.4619], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.1602, -0.7275, -1.3369,  ..., -2.8203, -1.1992,  2.0938],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0071, -0.0180,  0.0051,  ..., -0.0243, -0.0066, -0.0026],
        [-0.0048,  0.0088, -0.0027,  ...,  0.0066, -0.0029, -0.0103],
        [ 0.0050,  0.0129, -0.0225,  ...,  0.0106,  0.0010,  0.0032],
        ...,
        [ 0.0007, -0.0114, -0.0085,  ..., -0.0253, -0.0052, -0.0055],
        [ 0.0018, -0.0220,  0.0077,  ..., -0.0272, -0.0749,  0.0276],
        [-0.0321,  0.0093, -0.0007,  ...,  0.0218,  0.0306, -0.0501]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.2852, -0.2861, -1.2910,  ..., -3.4453, -2.3594,  1.3750]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 03:09:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most harsh, it is harshest
If something is the most mild, it is mildest
If something is the most neat, it is neatest
If something is the most noisy, it is noisiest
If something is the most sunny, it is sunniest
If something is the most strange, it is strangest
If something is the most shiny, it is shiniest
If something is the most wealthy, it is
2024-07-23 03:09:03 root INFO     [order_1_approx] starting weight calculation for If something is the most sunny, it is sunniest
If something is the most mild, it is mildest
If something is the most noisy, it is noisiest
If something is the most harsh, it is harshest
If something is the most wealthy, it is wealthiest
If something is the most shiny, it is shiniest
If something is the most strange, it is strangest
If something is the most neat, it is
2024-07-23 03:09:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 03:12:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4248, -0.7969,  0.5933,  ..., -0.1465, -0.1294,  0.7734],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5371,  0.2278, -0.9854,  ...,  2.8047, -3.6914,  5.5430],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0076, -0.0345,  0.0124,  ...,  0.0173, -0.0217, -0.0233],
        [-0.0060, -0.0228, -0.0026,  ...,  0.0048,  0.0110, -0.0072],
        [ 0.0107,  0.0088, -0.0124,  ...,  0.0122,  0.0028, -0.0104],
        ...,
        [ 0.0013, -0.0289, -0.0016,  ..., -0.0237, -0.0209,  0.0133],
        [ 0.0155,  0.0010, -0.0016,  ..., -0.0105, -0.0367,  0.0219],
        [-0.0323, -0.0091, -0.0133,  ..., -0.0217,  0.0006, -0.0450]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2500,  0.1184, -2.1426,  ...,  2.6758, -3.5664,  5.3672]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 03:13:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most sunny, it is sunniest
If something is the most mild, it is mildest
If something is the most noisy, it is noisiest
If something is the most harsh, it is harshest
If something is the most wealthy, it is wealthiest
If something is the most shiny, it is shiniest
If something is the most strange, it is strangest
If something is the most neat, it is
2024-07-23 03:13:00 root INFO     [order_1_approx] starting weight calculation for If something is the most sunny, it is sunniest
If something is the most mild, it is mildest
If something is the most noisy, it is noisiest
If something is the most wealthy, it is wealthiest
If something is the most shiny, it is shiniest
If something is the most neat, it is neatest
If something is the most strange, it is strangest
If something is the most harsh, it is
2024-07-23 03:13:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 03:16:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7139, -1.1094,  0.2744,  ..., -0.1340,  0.3540,  0.3721],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-6.3125, -1.2188, -2.2617,  ...,  2.2559,  1.4551,  6.5898],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0040, -0.0037,  0.0075,  ...,  0.0053, -0.0317, -0.0057],
        [-0.0033, -0.0191, -0.0030,  ..., -0.0188, -0.0179, -0.0116],
        [-0.0029,  0.0054, -0.0176,  ...,  0.0283,  0.0092,  0.0037],
        ...,
        [-0.0148, -0.0114,  0.0039,  ..., -0.0275, -0.0011,  0.0023],
        [-0.0041, -0.0037,  0.0253,  ..., -0.0187, -0.0252,  0.0062],
        [-0.0227,  0.0089, -0.0120,  ..., -0.0085, -0.0025, -0.0534]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-6.4297, -1.2598, -2.0508,  ...,  1.9502,  0.6646,  6.9844]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 03:16:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most sunny, it is sunniest
If something is the most mild, it is mildest
If something is the most noisy, it is noisiest
If something is the most wealthy, it is wealthiest
If something is the most shiny, it is shiniest
If something is the most neat, it is neatest
If something is the most strange, it is strangest
If something is the most harsh, it is
2024-07-23 03:16:58 root INFO     [order_1_approx] starting weight calculation for If something is the most noisy, it is noisiest
If something is the most neat, it is neatest
If something is the most wealthy, it is wealthiest
If something is the most mild, it is mildest
If something is the most shiny, it is shiniest
If something is the most harsh, it is harshest
If something is the most strange, it is strangest
If something is the most sunny, it is
2024-07-23 03:16:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 03:20:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4829,  0.0958,  0.7490,  ..., -0.2091,  0.4221,  0.3740],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7266,  0.4541, -1.2070,  ..., -2.0625, -1.1035,  2.3105],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0088, -0.0134, -0.0016,  ...,  0.0028, -0.0095, -0.0174],
        [-0.0101, -0.0023, -0.0181,  ..., -0.0061, -0.0230, -0.0008],
        [ 0.0105,  0.0134, -0.0034,  ...,  0.0101,  0.0092, -0.0085],
        ...,
        [-0.0087, -0.0036, -0.0042,  ..., -0.0029,  0.0029, -0.0061],
        [-0.0019, -0.0066,  0.0169,  ..., -0.0093, -0.0357,  0.0204],
        [-0.0132,  0.0072,  0.0100,  ..., -0.0157,  0.0067, -0.0296]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0601,  1.4531, -1.4062,  ..., -2.8770, -1.4375,  2.0254]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 03:20:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most noisy, it is noisiest
If something is the most neat, it is neatest
If something is the most wealthy, it is wealthiest
If something is the most mild, it is mildest
If something is the most shiny, it is shiniest
If something is the most harsh, it is harshest
If something is the most strange, it is strangest
If something is the most sunny, it is
2024-07-23 03:20:55 root INFO     [order_1_approx] starting weight calculation for If something is the most mild, it is mildest
If something is the most strange, it is strangest
If something is the most neat, it is neatest
If something is the most harsh, it is harshest
If something is the most wealthy, it is wealthiest
If something is the most sunny, it is sunniest
If something is the most noisy, it is noisiest
If something is the most shiny, it is
2024-07-23 03:20:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 03:24:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4773,  0.0754,  1.3027,  ..., -0.8154,  1.4531,  0.6421],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6660,  1.4082, -1.2080,  ...,  5.7969, -6.4531,  3.4688],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0104, -0.0286,  0.0027,  ...,  0.0053,  0.0027, -0.0080],
        [ 0.0029, -0.0027,  0.0066,  ..., -0.0074, -0.0143, -0.0023],
        [-0.0129, -0.0069, -0.0145,  ...,  0.0049,  0.0072,  0.0006],
        ...,
        [-0.0148, -0.0120, -0.0071,  ..., -0.0194, -0.0197, -0.0168],
        [ 0.0117, -0.0022,  0.0048,  ..., -0.0074, -0.0294,  0.0155],
        [-0.0285, -0.0050, -0.0023,  ..., -0.0037,  0.0253, -0.0159]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8491,  1.4395, -0.5928,  ...,  6.2969, -6.4727,  3.9727]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 03:24:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most mild, it is mildest
If something is the most strange, it is strangest
If something is the most neat, it is neatest
If something is the most harsh, it is harshest
If something is the most wealthy, it is wealthiest
If something is the most sunny, it is sunniest
If something is the most noisy, it is noisiest
If something is the most shiny, it is
2024-07-23 03:24:53 root INFO     total operator prediction time: 1904.1102080345154 seconds
2024-07-23 03:24:53 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+er_irreg
2024-07-23 03:24:53 root INFO     building operator verb+er_irreg
2024-07-23 03:24:53 root INFO     [order_1_approx] starting weight calculation for If you advertise something, you are a advertiser
If you deliver something, you are a deliverer
If you organise something, you are a organiser
If you bake something, you are a baker
If you subscribe something, you are a subscriber
If you announce something, you are a announcer
If you manage something, you are a manager
If you destroy something, you are a
2024-07-23 03:24:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 03:28:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2991, -0.1272,  1.1982,  ...,  0.2805, -0.4458, -0.0322],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.5156, -2.2500, -3.7461,  ..., -1.5693,  4.0117,  3.7500],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0082,  0.0145,  0.0140,  ...,  0.0059, -0.0118, -0.0069],
        [ 0.0057, -0.0029, -0.0044,  ...,  0.0064, -0.0055, -0.0010],
        [-0.0040,  0.0275,  0.0075,  ..., -0.0041, -0.0109,  0.0125],
        ...,
        [-0.0216, -0.0195,  0.0012,  ...,  0.0002,  0.0068, -0.0049],
        [ 0.0018, -0.0142,  0.0004,  ..., -0.0043, -0.0091,  0.0035],
        [-0.0098, -0.0068,  0.0073,  ..., -0.0142,  0.0149,  0.0140]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.7422, -1.8926, -3.7715,  ..., -1.1602,  4.1602,  3.6230]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 03:28:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you advertise something, you are a advertiser
If you deliver something, you are a deliverer
If you organise something, you are a organiser
If you bake something, you are a baker
If you subscribe something, you are a subscriber
If you announce something, you are a announcer
If you manage something, you are a manager
If you destroy something, you are a
2024-07-23 03:28:54 root INFO     [order_1_approx] starting weight calculation for If you bake something, you are a baker
If you destroy something, you are a destroyer
If you manage something, you are a manager
If you organise something, you are a organiser
If you subscribe something, you are a subscriber
If you deliver something, you are a deliverer
If you announce something, you are a announcer
If you advertise something, you are a
2024-07-23 03:28:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 03:32:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.7520,  0.7783,  0.5186,  ..., -0.2227,  0.2047, -0.0841],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7480,  2.1602, -6.9180,  ..., -2.5371,  1.5449,  1.3730],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0011,  0.0091,  0.0177,  ...,  0.0080, -0.0007, -0.0093],
        [-0.0074, -0.0244,  0.0052,  ...,  0.0004,  0.0018, -0.0027],
        [ 0.0143,  0.0067,  0.0020,  ...,  0.0029,  0.0132,  0.0077],
        ...,
        [-0.0080, -0.0082, -0.0033,  ..., -0.0067,  0.0017, -0.0001],
        [-0.0048, -0.0011, -0.0140,  ..., -0.0118, -0.0116,  0.0062],
        [ 0.0064,  0.0008, -0.0007,  ..., -0.0116,  0.0162, -0.0051]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7705,  2.6348, -6.5508,  ..., -1.8926,  1.5166,  1.5469]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 03:32:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you bake something, you are a baker
If you destroy something, you are a destroyer
If you manage something, you are a manager
If you organise something, you are a organiser
If you subscribe something, you are a subscriber
If you deliver something, you are a deliverer
If you announce something, you are a announcer
If you advertise something, you are a
2024-07-23 03:32:54 root INFO     [order_1_approx] starting weight calculation for If you manage something, you are a manager
If you deliver something, you are a deliverer
If you advertise something, you are a advertiser
If you announce something, you are a announcer
If you destroy something, you are a destroyer
If you bake something, you are a baker
If you subscribe something, you are a subscriber
If you organise something, you are a
2024-07-23 03:32:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 03:36:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9141,  0.5449, -0.0624,  ...,  0.2900, -0.3325,  0.4067],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8467,  0.1492, -0.7363,  ...,  0.0334,  1.9824,  4.9102],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0104, -0.0070, -0.0006,  ...,  0.0059,  0.0034, -0.0107],
        [ 0.0032, -0.0147, -0.0054,  ...,  0.0092,  0.0079, -0.0086],
        [ 0.0014, -0.0060,  0.0080,  ..., -0.0069, -0.0006, -0.0021],
        ...,
        [-0.0079,  0.0023,  0.0029,  ..., -0.0080,  0.0103,  0.0145],
        [ 0.0041, -0.0043,  0.0036,  ...,  0.0011, -0.0093,  0.0192],
        [ 0.0024, -0.0054,  0.0029,  ..., -0.0077,  0.0129,  0.0100]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3955,  0.4717, -0.3965,  ...,  0.0635,  1.4980,  4.8594]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 03:36:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you manage something, you are a manager
If you deliver something, you are a deliverer
If you advertise something, you are a advertiser
If you announce something, you are a announcer
If you destroy something, you are a destroyer
If you bake something, you are a baker
If you subscribe something, you are a subscriber
If you organise something, you are a
2024-07-23 03:36:54 root INFO     [order_1_approx] starting weight calculation for If you manage something, you are a manager
If you advertise something, you are a advertiser
If you subscribe something, you are a subscriber
If you destroy something, you are a destroyer
If you organise something, you are a organiser
If you announce something, you are a announcer
If you deliver something, you are a deliverer
If you bake something, you are a
2024-07-23 03:36:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 03:40:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0017,  0.2620, -0.3987,  ..., -0.8608, -0.1025,  0.1133],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0801,  5.8906, -3.2773,  ..., -2.9824,  0.7354,  3.3887],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 9.2621e-03,  3.2730e-03,  1.0956e-02,  ...,  9.1553e-05,
         -3.1700e-03, -8.2626e-03],
        [ 9.2545e-03, -1.9318e-02, -5.5161e-03,  ..., -6.3591e-03,
          7.4234e-03, -5.8899e-03],
        [-8.5373e-03,  2.4078e-02,  1.1673e-02,  ..., -4.5738e-03,
         -1.1075e-04,  1.2001e-02],
        ...,
        [ 7.2861e-03, -4.9591e-03, -2.9182e-03,  ...,  3.4447e-03,
          5.9586e-03,  5.6610e-03],
        [ 5.3368e-03, -5.4455e-04,  9.9640e-03,  ..., -2.7657e-03,
         -1.3138e-02,  1.1551e-02],
        [ 5.1651e-03, -5.4398e-03,  1.2085e-02,  ...,  2.4796e-03,
          5.7526e-03,  2.9373e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9258,  6.2266, -2.4902,  ..., -2.7305,  0.4509,  3.6562]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 03:40:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you manage something, you are a manager
If you advertise something, you are a advertiser
If you subscribe something, you are a subscriber
If you destroy something, you are a destroyer
If you organise something, you are a organiser
If you announce something, you are a announcer
If you deliver something, you are a deliverer
If you bake something, you are a
2024-07-23 03:40:55 root INFO     [order_1_approx] starting weight calculation for If you subscribe something, you are a subscriber
If you organise something, you are a organiser
If you destroy something, you are a destroyer
If you bake something, you are a baker
If you announce something, you are a announcer
If you deliver something, you are a deliverer
If you advertise something, you are a advertiser
If you manage something, you are a
2024-07-23 03:40:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 03:44:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0596,  0.6084,  0.1869,  ..., -0.0266, -1.2168,  0.0903],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3730, -1.6494, -2.0703,  ...,  1.1436,  1.0889,  4.7227],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.1730e-03,  7.5760e-03, -1.4744e-03,  ...,  5.9967e-03,
          3.9673e-03, -1.1383e-02],
        [-1.4515e-03, -1.0780e-02,  6.6185e-03,  ...,  4.3869e-04,
         -3.0899e-04, -1.1292e-03],
        [-5.7297e-03, -1.0777e-03, -3.6049e-04,  ..., -9.5367e-05,
          5.4855e-03, -5.3368e-03],
        ...,
        [-1.7700e-02, -9.3918e-03, -5.2414e-03,  ..., -2.7122e-03,
          2.3155e-03,  1.4977e-02],
        [ 1.5419e-02, -1.2726e-02,  8.1062e-04,  ..., -9.0332e-03,
         -1.3924e-04,  4.0970e-03],
        [-7.2060e-03, -2.2469e-03, -1.0155e-02,  ..., -4.3449e-03,
          1.5076e-02,  3.0499e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.8979, -1.6387, -1.8242,  ...,  1.1553,  1.2539,  4.6602]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 03:44:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you subscribe something, you are a subscriber
If you organise something, you are a organiser
If you destroy something, you are a destroyer
If you bake something, you are a baker
If you announce something, you are a announcer
If you deliver something, you are a deliverer
If you advertise something, you are a advertiser
If you manage something, you are a
2024-07-23 03:44:55 root INFO     [order_1_approx] starting weight calculation for If you organise something, you are a organiser
If you deliver something, you are a deliverer
If you bake something, you are a baker
If you destroy something, you are a destroyer
If you subscribe something, you are a subscriber
If you manage something, you are a manager
If you advertise something, you are a advertiser
If you announce something, you are a
2024-07-23 03:44:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 03:48:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.7861,  0.2996,  1.2285,  ...,  0.4817, -0.6670,  0.9531],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5352,  0.7949, -5.7500,  ...,  1.9941,  6.7461,  7.0039],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0069,  0.0061,  0.0133,  ...,  0.0110, -0.0073, -0.0160],
        [-0.0161, -0.0059,  0.0081,  ...,  0.0104, -0.0028, -0.0031],
        [ 0.0104,  0.0421, -0.0027,  ...,  0.0296,  0.0009,  0.0163],
        ...,
        [-0.0134, -0.0199,  0.0055,  ..., -0.0109,  0.0117, -0.0064],
        [ 0.0173, -0.0013, -0.0037,  ..., -0.0024, -0.0224,  0.0099],
        [ 0.0058, -0.0024,  0.0305,  ..., -0.0134,  0.0317,  0.0020]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3477,  0.4792, -4.8281,  ...,  2.4492,  7.7578,  7.2148]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 03:48:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you organise something, you are a organiser
If you deliver something, you are a deliverer
If you bake something, you are a baker
If you destroy something, you are a destroyer
If you subscribe something, you are a subscriber
If you manage something, you are a manager
If you advertise something, you are a advertiser
If you announce something, you are a
2024-07-23 03:48:55 root INFO     [order_1_approx] starting weight calculation for If you destroy something, you are a destroyer
If you subscribe something, you are a subscriber
If you advertise something, you are a advertiser
If you announce something, you are a announcer
If you bake something, you are a baker
If you organise something, you are a organiser
If you manage something, you are a manager
If you deliver something, you are a
2024-07-23 03:48:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 03:52:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1538, -0.2111, -0.1659,  ...,  0.4072, -1.3535, -0.0101],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.9668, -0.6191, -3.5039,  ..., -5.1055,  1.2139,  3.4629],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-6.0196e-03,  1.3290e-02,  3.5744e-03,  ..., -5.5084e-03,
         -3.3436e-03, -4.7569e-03],
        [ 4.3716e-03, -4.8447e-03,  6.4850e-03,  ..., -2.9755e-03,
          4.7760e-03, -7.8735e-03],
        [-5.4169e-04, -1.2016e-03,  7.4310e-03,  ..., -3.8147e-05,
          3.7804e-03, -7.3509e-03],
        ...,
        [-7.4615e-03,  9.3079e-04,  5.5771e-03,  ..., -7.6332e-03,
          8.4534e-03, -6.4316e-03],
        [-2.0924e-03,  5.9090e-03,  2.0714e-03,  ..., -2.1362e-03,
         -1.7273e-02,  1.1230e-02],
        [ 4.7493e-03,  6.5613e-03,  9.7961e-03,  ..., -1.0872e-02,
          1.4679e-02,  1.6928e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.0137, -0.6729, -3.3418,  ..., -4.5312,  1.0625,  2.7930]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 03:52:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you destroy something, you are a destroyer
If you subscribe something, you are a subscriber
If you advertise something, you are a advertiser
If you announce something, you are a announcer
If you bake something, you are a baker
If you organise something, you are a organiser
If you manage something, you are a manager
If you deliver something, you are a
2024-07-23 03:52:54 root INFO     [order_1_approx] starting weight calculation for If you manage something, you are a manager
If you destroy something, you are a destroyer
If you advertise something, you are a advertiser
If you organise something, you are a organiser
If you bake something, you are a baker
If you deliver something, you are a deliverer
If you announce something, you are a announcer
If you subscribe something, you are a
2024-07-23 03:52:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 03:56:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.4834,  0.0298, -0.4221,  ...,  1.5508, -0.8105,  1.1846],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.2695,  2.3789, -3.6523,  ..., -2.5078,  0.1064,  1.0840],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0049, -0.0182,  0.0127,  ...,  0.0018, -0.0071, -0.0011],
        [-0.0028, -0.0146,  0.0032,  ...,  0.0043, -0.0085, -0.0137],
        [-0.0049,  0.0035, -0.0099,  ..., -0.0021, -0.0102,  0.0143],
        ...,
        [-0.0196, -0.0072, -0.0022,  ..., -0.0094, -0.0024, -0.0026],
        [ 0.0163, -0.0012, -0.0081,  ...,  0.0092, -0.0097,  0.0098],
        [ 0.0076,  0.0016, -0.0079,  ..., -0.0129,  0.0143,  0.0007]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.9551,  2.9375, -3.3770,  ..., -2.5645, -0.7842,  0.7490]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 03:56:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you manage something, you are a manager
If you destroy something, you are a destroyer
If you advertise something, you are a advertiser
If you organise something, you are a organiser
If you bake something, you are a baker
If you deliver something, you are a deliverer
If you announce something, you are a announcer
If you subscribe something, you are a
2024-07-23 03:56:53 root INFO     total operator prediction time: 1920.1693069934845 seconds
2024-07-23 03:56:53 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on over+adj_reg
2024-07-23 03:56:53 root INFO     building operator over+adj_reg
2024-07-23 03:56:54 root INFO     [order_1_approx] starting weight calculation for If something is too saturated, it is oversaturated
If something is too developed, it is overdeveloped
If something is too dressed, it is overdressed
If something is too confident, it is overconfident
If something is too sized, it is oversized
If something is too stocked, it is overstocked
If something is too crowded, it is overcrowded
If something is too populated, it is
2024-07-23 03:56:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 04:00:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.3389,  0.4355,  1.5830,  ..., -0.7012,  2.0469, -0.7227],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4414,  0.9497,  1.3115,  ...,  0.9473, -0.7192,  1.7471],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-8.4763e-03, -4.7516e-02,  1.7227e-02,  ..., -1.0406e-02,
         -9.2545e-03, -1.9760e-02],
        [-1.9119e-02,  7.6294e-05,  3.7689e-03,  ...,  7.8964e-03,
          9.0179e-03, -2.1179e-02],
        [ 2.7710e-02, -1.0452e-02, -2.4662e-03,  ...,  7.5912e-03,
          9.9564e-04,  8.9417e-03],
        ...,
        [-2.2308e-02, -1.8097e-02, -2.0172e-02,  ..., -2.4048e-02,
         -1.3969e-02, -1.0994e-02],
        [-1.2741e-02,  1.8482e-03,  1.1871e-02,  ...,  4.9629e-03,
         -3.2410e-02,  3.3569e-03],
        [ 6.2256e-03, -1.0368e-02,  8.0109e-03,  ..., -2.7695e-02,
          1.1627e-02, -1.8021e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.3789,  1.3125,  1.3711,  ...,  0.5430, -0.7573,  2.8281]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 04:00:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too saturated, it is oversaturated
If something is too developed, it is overdeveloped
If something is too dressed, it is overdressed
If something is too confident, it is overconfident
If something is too sized, it is oversized
If something is too stocked, it is overstocked
If something is too crowded, it is overcrowded
If something is too populated, it is
2024-07-23 04:00:53 root INFO     [order_1_approx] starting weight calculation for If something is too crowded, it is overcrowded
If something is too dressed, it is overdressed
If something is too confident, it is overconfident
If something is too developed, it is overdeveloped
If something is too populated, it is overpopulated
If something is too sized, it is oversized
If something is too saturated, it is oversaturated
If something is too stocked, it is
2024-07-23 04:00:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 04:04:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3132,  1.2480,  1.1699,  ..., -0.2981,  0.7490, -0.2544],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.0898,  2.2891, -0.1934,  ..., -1.7061,  0.6748,  6.1875],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0190, -0.0349,  0.0059,  ...,  0.0048, -0.0306, -0.0360],
        [-0.0038, -0.0061,  0.0022,  ...,  0.0048, -0.0114, -0.0328],
        [ 0.0067, -0.0161,  0.0038,  ..., -0.0036,  0.0198, -0.0002],
        ...,
        [-0.0300, -0.0143,  0.0027,  ..., -0.0031, -0.0170, -0.0210],
        [-0.0100, -0.0002, -0.0040,  ..., -0.0068, -0.0140, -0.0026],
        [-0.0131,  0.0323,  0.0066,  ..., -0.0164,  0.0104, -0.0016]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-9.2188e-01,  2.0605e+00, -4.8828e-03,  ..., -2.2773e+00,
          7.1631e-01,  6.2578e+00]], device='cuda:1', dtype=torch.float16,
       grad_fn=<SubBackward0>) 

                    
2024-07-23 04:04:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too crowded, it is overcrowded
If something is too dressed, it is overdressed
If something is too confident, it is overconfident
If something is too developed, it is overdeveloped
If something is too populated, it is overpopulated
If something is too sized, it is oversized
If something is too saturated, it is oversaturated
If something is too stocked, it is
2024-07-23 04:04:52 root INFO     [order_1_approx] starting weight calculation for If something is too sized, it is oversized
If something is too dressed, it is overdressed
If something is too crowded, it is overcrowded
If something is too developed, it is overdeveloped
If something is too stocked, it is overstocked
If something is too populated, it is overpopulated
If something is too confident, it is overconfident
If something is too saturated, it is
2024-07-23 04:04:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 04:08:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6777,  0.2474,  2.1309,  ..., -1.2686,  0.6650, -0.2634],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.7383,  0.2661,  0.6963,  ...,  1.0977,  2.1289,  2.8203],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0058, -0.0270,  0.0047,  ..., -0.0172, -0.0103, -0.0282],
        [-0.0195, -0.0090,  0.0104,  ..., -0.0057, -0.0328,  0.0030],
        [-0.0056,  0.0083, -0.0105,  ..., -0.0100,  0.0118,  0.0114],
        ...,
        [-0.0321, -0.0129, -0.0037,  ..., -0.0128,  0.0040, -0.0061],
        [-0.0048, -0.0040, -0.0059,  ..., -0.0037, -0.0412,  0.0077],
        [-0.0062, -0.0022,  0.0187,  ..., -0.0258,  0.0144, -0.0266]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.4590,  0.2830,  0.3833,  ...,  1.0879,  1.9990,  3.2617]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 04:08:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too sized, it is oversized
If something is too dressed, it is overdressed
If something is too crowded, it is overcrowded
If something is too developed, it is overdeveloped
If something is too stocked, it is overstocked
If something is too populated, it is overpopulated
If something is too confident, it is overconfident
If something is too saturated, it is
2024-07-23 04:08:51 root INFO     [order_1_approx] starting weight calculation for If something is too confident, it is overconfident
If something is too dressed, it is overdressed
If something is too stocked, it is overstocked
If something is too populated, it is overpopulated
If something is too developed, it is overdeveloped
If something is too saturated, it is oversaturated
If something is too crowded, it is overcrowded
If something is too sized, it is
2024-07-23 04:08:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 04:12:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4585,  0.7578,  0.2036,  ..., -0.6157,  0.9380,  0.1311],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0391, -1.4043,  3.2715,  ..., -2.1523,  3.7969,  0.5508],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0255, -0.0298,  0.0035,  ...,  0.0009, -0.0302, -0.0341],
        [ 0.0055, -0.0057, -0.0052,  ...,  0.0008, -0.0103, -0.0004],
        [-0.0110, -0.0164, -0.0005,  ...,  0.0098, -0.0051,  0.0032],
        ...,
        [-0.0340, -0.0009, -0.0025,  ..., -0.0118, -0.0038, -0.0242],
        [ 0.0152,  0.0030, -0.0126,  ..., -0.0059, -0.0482,  0.0315],
        [ 0.0028,  0.0216,  0.0152,  ..., -0.0188,  0.0157, -0.0232]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4604, -1.2783,  4.1523,  ..., -2.9941,  3.2305,  0.6963]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 04:12:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too confident, it is overconfident
If something is too dressed, it is overdressed
If something is too stocked, it is overstocked
If something is too populated, it is overpopulated
If something is too developed, it is overdeveloped
If something is too saturated, it is oversaturated
If something is too crowded, it is overcrowded
If something is too sized, it is
2024-07-23 04:12:50 root INFO     [order_1_approx] starting weight calculation for If something is too saturated, it is oversaturated
If something is too stocked, it is overstocked
If something is too developed, it is overdeveloped
If something is too populated, it is overpopulated
If something is too crowded, it is overcrowded
If something is too confident, it is overconfident
If something is too sized, it is oversized
If something is too dressed, it is
2024-07-23 04:12:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 04:16:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5835,  0.1411,  1.8311,  ...,  0.6689,  0.4573,  0.3394],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6094, -1.6123, -1.1641,  ..., -3.9941,  1.3125,  2.7324],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0111, -0.0274,  0.0052,  ..., -0.0068,  0.0026,  0.0027],
        [-0.0021, -0.0192,  0.0084,  ...,  0.0194,  0.0022, -0.0150],
        [ 0.0181, -0.0029,  0.0019,  ...,  0.0041,  0.0177,  0.0018],
        ...,
        [-0.0082, -0.0242, -0.0016,  ..., -0.0087, -0.0234,  0.0096],
        [-0.0136,  0.0053,  0.0022,  ..., -0.0039, -0.0141,  0.0187],
        [-0.0237,  0.0013,  0.0039,  ..., -0.0013, -0.0031, -0.0126]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.4414e-03, -8.4131e-01, -2.0898e+00,  ..., -3.4414e+00,
          3.6670e-01,  2.4746e+00]], device='cuda:1', dtype=torch.float16,
       grad_fn=<SubBackward0>) 

                    
2024-07-23 04:16:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too saturated, it is oversaturated
If something is too stocked, it is overstocked
If something is too developed, it is overdeveloped
If something is too populated, it is overpopulated
If something is too crowded, it is overcrowded
If something is too confident, it is overconfident
If something is too sized, it is oversized
If something is too dressed, it is
2024-07-23 04:16:49 root INFO     [order_1_approx] starting weight calculation for If something is too populated, it is overpopulated
If something is too crowded, it is overcrowded
If something is too saturated, it is oversaturated
If something is too sized, it is oversized
If something is too dressed, it is overdressed
If something is too developed, it is overdeveloped
If something is too stocked, it is overstocked
If something is too confident, it is
2024-07-23 04:16:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 04:20:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0394, -0.3674, -0.6709,  ..., -0.4160,  0.5317, -0.3545],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6992, -1.6387, -1.3867,  ..., -1.4688,  1.3730,  0.3018],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0100, -0.0165,  0.0279,  ...,  0.0006,  0.0008, -0.0126],
        [ 0.0042, -0.0124,  0.0068,  ..., -0.0034,  0.0133, -0.0019],
        [-0.0052,  0.0018, -0.0009,  ...,  0.0084,  0.0023,  0.0004],
        ...,
        [-0.0045, -0.0179, -0.0079,  ..., -0.0032, -0.0097, -0.0022],
        [ 0.0064, -0.0018, -0.0027,  ..., -0.0114, -0.0320,  0.0217],
        [ 0.0028, -0.0072,  0.0061,  ..., -0.0076, -0.0004, -0.0223]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0312, -1.9863, -1.3701,  ..., -1.0762,  0.3213,  0.2837]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 04:20:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too populated, it is overpopulated
If something is too crowded, it is overcrowded
If something is too saturated, it is oversaturated
If something is too sized, it is oversized
If something is too dressed, it is overdressed
If something is too developed, it is overdeveloped
If something is too stocked, it is overstocked
If something is too confident, it is
2024-07-23 04:20:49 root INFO     [order_1_approx] starting weight calculation for If something is too sized, it is oversized
If something is too saturated, it is oversaturated
If something is too developed, it is overdeveloped
If something is too dressed, it is overdressed
If something is too stocked, it is overstocked
If something is too populated, it is overpopulated
If something is too confident, it is overconfident
If something is too crowded, it is
2024-07-23 04:20:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 04:24:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9697,  0.2668,  1.0029,  ..., -1.5156,  1.4717, -0.8779],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5469, -0.3945, -2.5449,  ...,  1.3428,  1.3789,  0.0649],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.4069e-02, -2.1378e-02, -2.2964e-03,  ..., -1.8372e-02,
         -9.2239e-03, -3.2928e-02],
        [-2.5482e-02, -2.7466e-04,  2.2400e-02,  ...,  1.5259e-03,
         -3.8605e-03, -2.5452e-02],
        [ 1.6241e-03, -2.0657e-03, -1.0483e-02,  ...,  1.0574e-02,
          2.4963e-02,  3.1738e-03],
        ...,
        [-3.1219e-02, -2.5452e-02, -4.6082e-03,  ..., -1.5900e-02,
          3.0899e-03, -1.5274e-02],
        [-1.5686e-02,  3.1616e-02, -1.3351e-05,  ..., -8.5678e-03,
         -4.4159e-02,  1.4969e-02],
        [-1.5350e-02,  6.0081e-03,  6.4545e-03,  ..., -7.4158e-03,
          2.1790e-02, -4.6600e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6543,  0.5249, -3.3359,  ...,  0.3550,  1.9600,  0.8994]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 04:24:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too sized, it is oversized
If something is too saturated, it is oversaturated
If something is too developed, it is overdeveloped
If something is too dressed, it is overdressed
If something is too stocked, it is overstocked
If something is too populated, it is overpopulated
If something is too confident, it is overconfident
If something is too crowded, it is
2024-07-23 04:24:48 root INFO     [order_1_approx] starting weight calculation for If something is too confident, it is overconfident
If something is too sized, it is oversized
If something is too stocked, it is overstocked
If something is too crowded, it is overcrowded
If something is too dressed, it is overdressed
If something is too saturated, it is oversaturated
If something is too populated, it is overpopulated
If something is too developed, it is
2024-07-23 04:24:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 04:28:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2500, -0.0347,  2.4727,  ..., -0.0342,  0.8877, -0.0705],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1426, -1.2012,  2.5137,  ..., -2.5996,  1.1953,  0.2766],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0036, -0.0405,  0.0105,  ..., -0.0154,  0.0002, -0.0152],
        [-0.0050,  0.0072,  0.0013,  ...,  0.0022,  0.0058,  0.0073],
        [-0.0196, -0.0084, -0.0101,  ..., -0.0022,  0.0223, -0.0043],
        ...,
        [-0.0119,  0.0105, -0.0116,  ...,  0.0098, -0.0259, -0.0089],
        [-0.0162, -0.0092,  0.0084,  ..., -0.0049, -0.0149,  0.0147],
        [-0.0041,  0.0044,  0.0068,  ..., -0.0037,  0.0038,  0.0083]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7769, -1.3936,  2.7344,  ..., -2.3027,  1.1787, -0.4348]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 04:28:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too confident, it is overconfident
If something is too sized, it is oversized
If something is too stocked, it is overstocked
If something is too crowded, it is overcrowded
If something is too dressed, it is overdressed
If something is too saturated, it is oversaturated
If something is too populated, it is overpopulated
If something is too developed, it is
2024-07-23 04:28:47 root INFO     total operator prediction time: 1913.4634385108948 seconds
2024-07-23 04:28:47 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on adj+ly_reg
2024-07-23 04:28:47 root INFO     building operator adj+ly_reg
2024-07-23 04:28:47 root INFO     [order_1_approx] starting weight calculation for The adjective form of strong is strongly
The adjective form of internal is internally
The adjective form of unique is uniquely
The adjective form of interesting is interestingly
The adjective form of typical is typically
The adjective form of subsequent is subsequently
The adjective form of legal is legally
The adjective form of apparent is
2024-07-23 04:28:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 04:32:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4304, -0.0327,  0.2114,  ...,  0.3218,  0.4834, -0.1299],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.7598,  2.0703, -0.2021,  ...,  1.4883, -2.1953, -0.1680],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0190, -0.0235, -0.0055,  ..., -0.0180,  0.0011, -0.0105],
        [-0.0061, -0.0113,  0.0034,  ..., -0.0047, -0.0246, -0.0027],
        [-0.0098,  0.0038, -0.0335,  ...,  0.0273,  0.0023,  0.0249],
        ...,
        [-0.0368, -0.0284, -0.0170,  ..., -0.0304,  0.0005,  0.0015],
        [ 0.0061,  0.0083,  0.0139,  ..., -0.0043, -0.0025, -0.0024],
        [ 0.0206,  0.0148, -0.0040,  ...,  0.0025,  0.0143, -0.0196]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.4902,  2.9434, -1.1406,  ...,  1.1973, -2.0488, -1.0684]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 04:32:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of strong is strongly
The adjective form of internal is internally
The adjective form of unique is uniquely
The adjective form of interesting is interestingly
The adjective form of typical is typically
The adjective form of subsequent is subsequently
The adjective form of legal is legally
The adjective form of apparent is
2024-07-23 04:32:46 root INFO     [order_1_approx] starting weight calculation for The adjective form of legal is legally
The adjective form of subsequent is subsequently
The adjective form of apparent is apparently
The adjective form of unique is uniquely
The adjective form of internal is internally
The adjective form of interesting is interestingly
The adjective form of strong is strongly
The adjective form of typical is
2024-07-23 04:32:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 04:36:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.7891,  1.7539, -0.0441,  ..., -0.1018,  1.0859,  0.2793],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3545,  1.5889,  2.8555,  ...,  3.1113, -1.5186,  1.5117],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0113, -0.0201, -0.0180,  ..., -0.0100,  0.0051, -0.0117],
        [-0.0258, -0.0141,  0.0205,  ..., -0.0003, -0.0239,  0.0165],
        [-0.0191, -0.0173, -0.0094,  ...,  0.0196, -0.0039,  0.0304],
        ...,
        [-0.0089,  0.0050,  0.0049,  ..., -0.0172,  0.0124,  0.0112],
        [ 0.0186,  0.0108,  0.0105,  ..., -0.0042,  0.0002, -0.0035],
        [ 0.0105,  0.0129,  0.0035,  ..., -0.0020,  0.0147, -0.0253]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1442,  1.9160,  1.9395,  ...,  2.8242, -1.4111,  1.0430]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 04:36:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of legal is legally
The adjective form of subsequent is subsequently
The adjective form of apparent is apparently
The adjective form of unique is uniquely
The adjective form of internal is internally
The adjective form of interesting is interestingly
The adjective form of strong is strongly
The adjective form of typical is
2024-07-23 04:36:45 root INFO     [order_1_approx] starting weight calculation for The adjective form of typical is typically
The adjective form of apparent is apparently
The adjective form of interesting is interestingly
The adjective form of strong is strongly
The adjective form of subsequent is subsequently
The adjective form of legal is legally
The adjective form of internal is internally
The adjective form of unique is
2024-07-23 04:36:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 04:40:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2666,  0.3071, -0.0347,  ...,  0.1409,  1.2656,  0.1665],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([2.5117, 0.9805, 2.2109,  ..., 0.6143, 3.3086, 3.0898], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0315, -0.0157,  0.0068,  ..., -0.0100, -0.0190,  0.0106],
        [-0.0071, -0.0177,  0.0111,  ...,  0.0308, -0.0257,  0.0122],
        [-0.0035, -0.0130, -0.0004,  ...,  0.0135,  0.0091,  0.0129],
        ...,
        [ 0.0009,  0.0022,  0.0066,  ..., -0.0002, -0.0083, -0.0013],
        [-0.0153, -0.0063,  0.0064,  ..., -0.0121, -0.0126, -0.0108],
        [-0.0092,  0.0194,  0.0015,  ..., -0.0024,  0.0193, -0.0371]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[2.3574, 1.1338, 1.4238,  ..., 0.6982, 3.2109, 3.5273]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 04:40:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of typical is typically
The adjective form of apparent is apparently
The adjective form of interesting is interestingly
The adjective form of strong is strongly
The adjective form of subsequent is subsequently
The adjective form of legal is legally
The adjective form of internal is internally
The adjective form of unique is
2024-07-23 04:40:44 root INFO     [order_1_approx] starting weight calculation for The adjective form of apparent is apparently
The adjective form of typical is typically
The adjective form of unique is uniquely
The adjective form of interesting is interestingly
The adjective form of legal is legally
The adjective form of internal is internally
The adjective form of subsequent is subsequently
The adjective form of strong is
2024-07-23 04:40:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 04:44:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8452, -0.5312,  0.3542,  ...,  0.3579,  0.5742, -0.1479],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.8965, -0.6260,  0.5303,  ...,  2.1934, -1.5879, -1.0234],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0124, -0.0096,  0.0089,  ..., -0.0074, -0.0085, -0.0017],
        [ 0.0058, -0.0182,  0.0160,  ...,  0.0032, -0.0175,  0.0047],
        [-0.0048, -0.0146,  0.0251,  ..., -0.0024,  0.0217,  0.0222],
        ...,
        [-0.0118, -0.0164,  0.0197,  ..., -0.0028, -0.0150,  0.0067],
        [-0.0046, -0.0013, -0.0050,  ..., -0.0017,  0.0102,  0.0028],
        [ 0.0015,  0.0196,  0.0104,  ..., -0.0160,  0.0225, -0.0162]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5625, -0.5288,  0.2686,  ...,  2.2461, -1.0801, -1.2627]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 04:44:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of apparent is apparently
The adjective form of typical is typically
The adjective form of unique is uniquely
The adjective form of interesting is interestingly
The adjective form of legal is legally
The adjective form of internal is internally
The adjective form of subsequent is subsequently
The adjective form of strong is
2024-07-23 04:44:43 root INFO     [order_1_approx] starting weight calculation for The adjective form of typical is typically
The adjective form of unique is uniquely
The adjective form of legal is legally
The adjective form of subsequent is subsequently
The adjective form of apparent is apparently
The adjective form of internal is internally
The adjective form of strong is strongly
The adjective form of interesting is
2024-07-23 04:44:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 04:48:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([1.3896, 0.6016, 0.4626,  ..., 0.6011, 0.9790, 0.5190], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1250,  2.2402, -1.3867,  ...,  2.4551,  2.3574,  2.8652],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0041, -0.0050,  0.0132,  ..., -0.0175, -0.0036,  0.0034],
        [-0.0101, -0.0152,  0.0139,  ...,  0.0199, -0.0170,  0.0103],
        [-0.0031, -0.0081, -0.0050,  ...,  0.0090, -0.0030,  0.0051],
        ...,
        [-0.0135,  0.0041, -0.0055,  ..., -0.0094, -0.0137, -0.0266],
        [-0.0201,  0.0083,  0.0074,  ...,  0.0021, -0.0180,  0.0123],
        [ 0.0066,  0.0372, -0.0178,  ..., -0.0101, -0.0048, -0.0200]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2578,  1.5840, -1.7461,  ...,  3.3672,  3.1172,  3.5625]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 04:48:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of typical is typically
The adjective form of unique is uniquely
The adjective form of legal is legally
The adjective form of subsequent is subsequently
The adjective form of apparent is apparently
The adjective form of internal is internally
The adjective form of strong is strongly
The adjective form of interesting is
2024-07-23 04:48:43 root INFO     [order_1_approx] starting weight calculation for The adjective form of unique is uniquely
The adjective form of apparent is apparently
The adjective form of interesting is interestingly
The adjective form of typical is typically
The adjective form of strong is strongly
The adjective form of subsequent is subsequently
The adjective form of internal is internally
The adjective form of legal is
2024-07-23 04:48:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 04:52:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0986,  0.1431,  0.3899,  ..., -0.1765,  0.2837,  0.6523],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2319,  1.7988, -0.7788,  ..., -2.3164,  4.2812,  1.2500],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0066, -0.0034,  0.0040,  ..., -0.0026, -0.0029,  0.0151],
        [-0.0033, -0.0072,  0.0128,  ...,  0.0099,  0.0008,  0.0067],
        [-0.0347, -0.0228,  0.0028,  ...,  0.0021,  0.0036,  0.0295],
        ...,
        [-0.0176, -0.0081,  0.0026,  ..., -0.0131,  0.0004, -0.0060],
        [ 0.0059,  0.0037, -0.0192,  ..., -0.0022, -0.0220,  0.0014],
        [-0.0005,  0.0084,  0.0145,  ..., -0.0025,  0.0210, -0.0216]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0536,  2.0371, -1.5049,  ..., -2.3594,  4.3086,  1.1328]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 04:52:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of unique is uniquely
The adjective form of apparent is apparently
The adjective form of interesting is interestingly
The adjective form of typical is typically
The adjective form of strong is strongly
The adjective form of subsequent is subsequently
The adjective form of internal is internally
The adjective form of legal is
2024-07-23 04:52:42 root INFO     [order_1_approx] starting weight calculation for The adjective form of typical is typically
The adjective form of unique is uniquely
The adjective form of interesting is interestingly
The adjective form of legal is legally
The adjective form of subsequent is subsequently
The adjective form of strong is strongly
The adjective form of apparent is apparently
The adjective form of internal is
2024-07-23 04:52:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 04:56:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5762,  1.1309,  0.0393,  ...,  0.2686,  0.6289,  1.4766],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.1660,  1.2656, -3.5352,  ..., -0.5122, -0.6250,  0.3467],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0286,  0.0044, -0.0043,  ..., -0.0279,  0.0156,  0.0103],
        [-0.0255, -0.0131,  0.0077,  ..., -0.0003,  0.0060,  0.0046],
        [-0.0192, -0.0278, -0.0041,  ...,  0.0291, -0.0188,  0.0020],
        ...,
        [-0.0146, -0.0165, -0.0015,  ..., -0.0112, -0.0112, -0.0125],
        [ 0.0154,  0.0075,  0.0035,  ..., -0.0191, -0.0293,  0.0114],
        [-0.0202,  0.0042,  0.0124,  ...,  0.0006,  0.0151, -0.0206]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9482,  0.9834, -2.9922,  ..., -0.3350,  0.0112,  0.2507]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 04:56:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of typical is typically
The adjective form of unique is uniquely
The adjective form of interesting is interestingly
The adjective form of legal is legally
The adjective form of subsequent is subsequently
The adjective form of strong is strongly
The adjective form of apparent is apparently
The adjective form of internal is
2024-07-23 04:56:41 root INFO     [order_1_approx] starting weight calculation for The adjective form of internal is internally
The adjective form of legal is legally
The adjective form of unique is uniquely
The adjective form of apparent is apparently
The adjective form of interesting is interestingly
The adjective form of typical is typically
The adjective form of strong is strongly
The adjective form of subsequent is
2024-07-23 04:56:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 05:00:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6919,  0.8643,  0.9810,  ..., -0.1145,  0.2505,  0.5381],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1909,  1.1982, -2.4883,  ...,  4.7891,  0.2197, -0.2080],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0285, -0.0174,  0.0011,  ..., -0.0325, -0.0211, -0.0322],
        [-0.0243, -0.0184, -0.0009,  ...,  0.0068, -0.0108,  0.0142],
        [-0.0182, -0.0279, -0.0372,  ..., -0.0016, -0.0170,  0.0151],
        ...,
        [-0.0108, -0.0425, -0.0216,  ..., -0.0137, -0.0223,  0.0202],
        [-0.0144,  0.0148, -0.0118,  ..., -0.0178, -0.0014,  0.0261],
        [-0.0004,  0.0119,  0.0273,  ...,  0.0070,  0.0381, -0.0648]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4209,  0.9966, -1.9268,  ...,  5.1797,  0.4883, -0.8042]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 05:00:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of internal is internally
The adjective form of legal is legally
The adjective form of unique is uniquely
The adjective form of apparent is apparently
The adjective form of interesting is interestingly
The adjective form of typical is typically
The adjective form of strong is strongly
The adjective form of subsequent is
2024-07-23 05:00:39 root INFO     total operator prediction time: 1912.4544701576233 seconds
2024-07-23 05:00:39 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+tion_irreg
2024-07-23 05:00:39 root INFO     building operator verb+tion_irreg
2024-07-23 05:00:39 root INFO     [order_1_approx] starting weight calculation for To improvize results in improvization
To consult results in consulation
To derive results in derivation
To configure results in configuration
To specialize results in specialization
To declare results in declaration
To observe results in observation
To minimize results in
2024-07-23 05:00:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 05:04:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3416, -0.9658,  0.2236,  ..., -1.5488, -1.1318, -0.4670],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.2812,  1.1357, -5.8789,  ...,  3.5059,  0.7168,  4.7695],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0223, -0.0086, -0.0013,  ...,  0.0078, -0.0003, -0.0227],
        [-0.0089, -0.0071, -0.0075,  ..., -0.0019, -0.0137,  0.0198],
        [-0.0029,  0.0032,  0.0032,  ...,  0.0014, -0.0135, -0.0022],
        ...,
        [-0.0248, -0.0054,  0.0039,  ..., -0.0009,  0.0083, -0.0177],
        [ 0.0196, -0.0034,  0.0248,  ..., -0.0219,  0.0029,  0.0178],
        [ 0.0006,  0.0062, -0.0019,  ..., -0.0116,  0.0086, -0.0016]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.8340,  1.0947, -6.6641,  ...,  3.3809,  0.4055,  5.0625]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 05:04:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To improvize results in improvization
To consult results in consulation
To derive results in derivation
To configure results in configuration
To specialize results in specialization
To declare results in declaration
To observe results in observation
To minimize results in
2024-07-23 05:04:40 root INFO     [order_1_approx] starting weight calculation for To minimize results in minimization
To improvize results in improvization
To configure results in configuration
To specialize results in specialization
To consult results in consulation
To derive results in derivation
To declare results in declaration
To observe results in
2024-07-23 05:04:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 05:08:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8799, -0.1946, -0.3008,  ..., -0.8848, -0.6738,  0.7544],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 6.1094,  0.6880, -4.1172,  ...,  0.7090,  0.2051,  4.2031],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0079, -0.0057, -0.0039,  ...,  0.0005,  0.0069, -0.0232],
        [ 0.0047, -0.0118, -0.0034,  ...,  0.0034, -0.0150, -0.0017],
        [-0.0089,  0.0077, -0.0074,  ..., -0.0002, -0.0084, -0.0087],
        ...,
        [-0.0211, -0.0234, -0.0119,  ..., -0.0151, -0.0148, -0.0104],
        [-0.0052,  0.0043, -0.0033,  ...,  0.0027, -0.0165,  0.0039],
        [ 0.0017,  0.0108,  0.0031,  ..., -0.0056,  0.0208, -0.0165]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.7734,  1.2949, -4.4141,  ...,  1.1885, -0.4180,  4.5000]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 05:08:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To minimize results in minimization
To improvize results in improvization
To configure results in configuration
To specialize results in specialization
To consult results in consulation
To derive results in derivation
To declare results in declaration
To observe results in
2024-07-23 05:08:40 root INFO     [order_1_approx] starting weight calculation for To declare results in declaration
To specialize results in specialization
To observe results in observation
To derive results in derivation
To minimize results in minimization
To consult results in consulation
To improvize results in improvization
To configure results in
2024-07-23 05:08:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 05:12:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6851,  1.0215, -0.3569,  ...,  0.1997, -0.1282, -0.9380],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7676,  1.2744, -4.5469,  ..., -1.4316,  0.6187,  2.7051],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-3.4088e-02, -2.0599e-02,  5.8365e-03,  ..., -4.6654e-03,
          1.2207e-04, -1.5381e-02],
        [-2.1000e-03, -3.7098e-03, -5.3883e-05,  ...,  4.4060e-04,
         -1.3885e-02,  5.9052e-03],
        [ 1.1902e-02, -1.2894e-03, -1.2268e-02,  ...,  7.0419e-03,
         -8.9951e-03, -1.5259e-03],
        ...,
        [-2.0905e-02, -2.3743e-02, -3.9673e-03,  ...,  4.0588e-03,
         -2.1286e-03,  1.1459e-02],
        [-8.8577e-03,  2.1667e-03,  1.8101e-03,  ..., -1.3504e-02,
          5.3596e-04, -2.2182e-03],
        [ 8.9264e-03,  4.9133e-03, -8.0490e-03,  ..., -3.1204e-03,
          1.7426e-02, -2.3849e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2422,  1.3223, -4.4375,  ..., -0.8066,  0.5439,  2.7891]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 05:12:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To declare results in declaration
To specialize results in specialization
To observe results in observation
To derive results in derivation
To minimize results in minimization
To consult results in consulation
To improvize results in improvization
To configure results in
2024-07-23 05:12:41 root INFO     [order_1_approx] starting weight calculation for To observe results in observation
To improvize results in improvization
To specialize results in specialization
To derive results in derivation
To declare results in declaration
To minimize results in minimization
To configure results in configuration
To consult results in
2024-07-23 05:12:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 05:16:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2402, -0.2986,  0.2122,  ..., -0.0797, -0.0320,  0.5195],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1523,  0.9375, -4.6797,  ..., -1.2178, -0.1831,  1.5947],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0050, -0.0137,  0.0070,  ..., -0.0173,  0.0044, -0.0075],
        [-0.0165,  0.0073, -0.0064,  ..., -0.0036,  0.0085,  0.0114],
        [-0.0139, -0.0080, -0.0039,  ..., -0.0027, -0.0010,  0.0093],
        ...,
        [-0.0275, -0.0109, -0.0134,  ..., -0.0027, -0.0133,  0.0054],
        [ 0.0065,  0.0262,  0.0312,  ...,  0.0108, -0.0203,  0.0193],
        [ 0.0057,  0.0137, -0.0059,  ...,  0.0074,  0.0099, -0.0019]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1768,  0.4602, -6.4492,  ..., -0.4360, -0.2377,  1.9658]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 05:16:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To observe results in observation
To improvize results in improvization
To specialize results in specialization
To derive results in derivation
To declare results in declaration
To minimize results in minimization
To configure results in configuration
To consult results in
2024-07-23 05:16:41 root INFO     [order_1_approx] starting weight calculation for To minimize results in minimization
To specialize results in specialization
To observe results in observation
To consult results in consulation
To improvize results in improvization
To declare results in declaration
To configure results in configuration
To derive results in
2024-07-23 05:16:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 05:20:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3792, -1.0967,  0.0553,  ..., -1.1973, -0.6313,  1.3125],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4971,  3.5977,  1.4629,  ..., -3.1328,  0.3525,  2.8281],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0248, -0.0038,  0.0064,  ..., -0.0032,  0.0007, -0.0133],
        [-0.0028, -0.0233,  0.0047,  ...,  0.0146,  0.0079,  0.0048],
        [ 0.0127,  0.0106, -0.0201,  ...,  0.0033, -0.0211, -0.0069],
        ...,
        [-0.0148, -0.0108, -0.0152,  ...,  0.0132, -0.0163,  0.0141],
        [ 0.0021, -0.0038,  0.0128,  ..., -0.0038, -0.0044, -0.0085],
        [-0.0031,  0.0125, -0.0037,  ..., -0.0072,  0.0276, -0.0192]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6670,  3.4336,  0.8120,  ..., -2.8496,  0.5869,  2.9492]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 05:20:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To minimize results in minimization
To specialize results in specialization
To observe results in observation
To consult results in consulation
To improvize results in improvization
To declare results in declaration
To configure results in configuration
To derive results in
2024-07-23 05:20:41 root INFO     [order_1_approx] starting weight calculation for To derive results in derivation
To consult results in consulation
To specialize results in specialization
To observe results in observation
To configure results in configuration
To declare results in declaration
To minimize results in minimization
To improvize results in
2024-07-23 05:20:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 05:24:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2944, -1.0293, -0.4812,  ..., -1.0752, -0.0883, -0.3306],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.8184,  0.8428, -6.3906,  ...,  3.3008,  0.2935,  3.3047],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0055, -0.0177,  0.0119,  ..., -0.0032,  0.0009, -0.0002],
        [ 0.0038, -0.0047,  0.0087,  ...,  0.0011, -0.0036, -0.0123],
        [ 0.0100, -0.0089, -0.0240,  ...,  0.0009,  0.0015,  0.0107],
        ...,
        [-0.0141, -0.0146, -0.0200,  ...,  0.0055, -0.0061,  0.0062],
        [ 0.0062,  0.0030,  0.0140,  ...,  0.0112, -0.0109, -0.0040],
        [ 0.0090, -0.0013, -0.0184,  ..., -0.0021,  0.0094,  0.0168]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1758,  0.3027, -7.2031,  ...,  3.1465,  0.0985,  3.1699]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 05:24:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To derive results in derivation
To consult results in consulation
To specialize results in specialization
To observe results in observation
To configure results in configuration
To declare results in declaration
To minimize results in minimization
To improvize results in
2024-07-23 05:24:41 root INFO     [order_1_approx] starting weight calculation for To derive results in derivation
To configure results in configuration
To observe results in observation
To improvize results in improvization
To declare results in declaration
To minimize results in minimization
To consult results in consulation
To specialize results in
2024-07-23 05:24:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 05:28:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7910,  0.4253, -0.3601,  ..., -0.7427,  0.7441,  1.0781],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.5508, -0.7490,  0.4128,  ...,  2.9805, -2.8438,  1.1406],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.8341e-02, -1.2054e-02,  1.5259e-02,  ..., -1.5182e-02,
          2.9663e-02, -1.3687e-02],
        [ 1.8402e-02, -1.3252e-02,  1.9272e-02,  ...,  9.3765e-03,
         -3.2104e-02,  9.6054e-03],
        [ 1.2024e-02, -3.8528e-03, -2.2598e-02,  ..., -4.2992e-03,
          7.9193e-03,  6.7482e-03],
        ...,
        [-1.7883e-02, -8.5907e-03,  1.4648e-03,  ...,  4.4022e-03,
         -2.2537e-02, -6.7368e-03],
        [-6.4468e-03,  1.6571e-02, -1.7624e-03,  ...,  3.5915e-03,
          7.8735e-03, -9.5367e-05],
        [-4.0207e-03,  7.1259e-03,  8.0109e-05,  ..., -9.8877e-03,
          4.4365e-03,  4.8981e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.1328, -0.1035, -0.4778,  ...,  3.3926, -3.0742,  0.5234]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 05:28:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To derive results in derivation
To configure results in configuration
To observe results in observation
To improvize results in improvization
To declare results in declaration
To minimize results in minimization
To consult results in consulation
To specialize results in
2024-07-23 05:28:41 root INFO     [order_1_approx] starting weight calculation for To configure results in configuration
To observe results in observation
To derive results in derivation
To minimize results in minimization
To consult results in consulation
To specialize results in specialization
To improvize results in improvization
To declare results in
2024-07-23 05:28:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 05:32:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6055,  0.0352, -0.6943,  ...,  0.0820, -0.7358,  1.2129],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8916, -3.1328, -4.1055,  ..., -0.3638,  3.2852,  1.2324],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0274, -0.0103,  0.0077,  ..., -0.0053,  0.0136, -0.0223],
        [ 0.0156, -0.0080,  0.0206,  ...,  0.0158, -0.0102,  0.0055],
        [ 0.0003, -0.0076, -0.0123,  ...,  0.0087, -0.0083, -0.0126],
        ...,
        [-0.0179, -0.0057, -0.0005,  ..., -0.0035, -0.0081,  0.0153],
        [ 0.0038, -0.0136,  0.0031,  ...,  0.0036, -0.0079, -0.0086],
        [ 0.0097,  0.0123, -0.0072,  ..., -0.0107,  0.0109, -0.0146]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0938, -3.3906, -4.5078,  ..., -0.0347,  3.6992,  1.5215]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 05:32:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To configure results in configuration
To observe results in observation
To derive results in derivation
To minimize results in minimization
To consult results in consulation
To specialize results in specialization
To improvize results in improvization
To declare results in
2024-07-23 05:32:41 root INFO     total operator prediction time: 1921.815746307373 seconds
2024-07-23 05:32:41 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+able_reg
2024-07-23 05:32:41 root INFO     building operator verb+able_reg
2024-07-23 05:32:41 root INFO     [order_1_approx] starting weight calculation for If you can consider something, that thing is considerable
If you can replace something, that thing is replaceable
If you can rely something, that thing is reliable
If you can download something, that thing is downloadable
If you can deliver something, that thing is deliverable
If you can foresee something, that thing is foreseeable
If you can adjust something, that thing is adjustable
If you can protect something, that thing is
2024-07-23 05:32:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 05:36:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1882,  0.6211,  0.1404,  ...,  0.2654, -0.1477,  0.1048],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9395,  4.3281, -3.7344,  ..., -2.8555,  1.4053,  0.5391],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0010,  0.0079, -0.0036,  ...,  0.0101,  0.0006,  0.0087],
        [ 0.0019, -0.0167, -0.0178,  ...,  0.0021,  0.0025, -0.0061],
        [ 0.0235,  0.0054, -0.0131,  ...,  0.0058,  0.0198,  0.0036],
        ...,
        [-0.0201,  0.0059, -0.0108,  ..., -0.0194, -0.0020, -0.0125],
        [-0.0009, -0.0084, -0.0012,  ..., -0.0312, -0.0326, -0.0002],
        [-0.0072, -0.0053, -0.0010,  ..., -0.0106,  0.0156, -0.0341]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5869,  4.8516, -4.3203,  ..., -3.1875,  1.5518, -0.2935]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 05:36:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can consider something, that thing is considerable
If you can replace something, that thing is replaceable
If you can rely something, that thing is reliable
If you can download something, that thing is downloadable
If you can deliver something, that thing is deliverable
If you can foresee something, that thing is foreseeable
If you can adjust something, that thing is adjustable
If you can protect something, that thing is
2024-07-23 05:36:40 root INFO     [order_1_approx] starting weight calculation for If you can deliver something, that thing is deliverable
If you can foresee something, that thing is foreseeable
If you can protect something, that thing is protectable
If you can adjust something, that thing is adjustable
If you can replace something, that thing is replaceable
If you can consider something, that thing is considerable
If you can rely something, that thing is reliable
If you can download something, that thing is
2024-07-23 05:36:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 05:40:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3108,  0.9277, -0.1526,  ..., -0.0137,  0.8574,  0.5942],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.4551,  2.5078, -3.3828,  ..., -7.5156,  3.2656,  0.1299],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 6.0120e-03, -2.7847e-03, -1.9394e-02,  ...,  4.8447e-03,
         -5.8365e-03,  8.1100e-03],
        [ 6.1035e-05, -4.7779e-04,  1.1032e-02,  ...,  1.0099e-03,
         -4.0474e-03, -3.8986e-03],
        [-2.5673e-03,  4.7493e-03,  4.7874e-04,  ..., -8.0490e-04,
         -4.5547e-03, -9.1248e-03],
        ...,
        [-9.7427e-03, -9.3460e-03, -6.4278e-03,  ..., -6.5842e-03,
         -1.8311e-04,  7.0572e-03],
        [ 1.1581e-02,  1.6775e-03, -1.3840e-02,  ..., -1.2215e-02,
         -3.8055e-02, -1.2329e-02],
        [ 2.0561e-03,  1.9894e-03,  2.4185e-03,  ...,  1.9455e-04,
          4.2114e-03, -1.7075e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.4648,  2.1094, -3.9062,  ..., -7.5469,  2.6055, -0.7939]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 05:40:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can deliver something, that thing is deliverable
If you can foresee something, that thing is foreseeable
If you can protect something, that thing is protectable
If you can adjust something, that thing is adjustable
If you can replace something, that thing is replaceable
If you can consider something, that thing is considerable
If you can rely something, that thing is reliable
If you can download something, that thing is
2024-07-23 05:40:39 root INFO     [order_1_approx] starting weight calculation for If you can download something, that thing is downloadable
If you can protect something, that thing is protectable
If you can consider something, that thing is considerable
If you can adjust something, that thing is adjustable
If you can rely something, that thing is reliable
If you can deliver something, that thing is deliverable
If you can foresee something, that thing is foreseeable
If you can replace something, that thing is
2024-07-23 05:40:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 05:44:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4482,  0.9746,  0.0549,  ..., -0.4727,  0.3984, -0.8389],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1099,  4.0938, -4.5508,  ..., -0.4912,  2.2910,  3.4883],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0111,  0.0023, -0.0028,  ...,  0.0047, -0.0106,  0.0078],
        [-0.0059, -0.0074,  0.0045,  ...,  0.0016,  0.0032, -0.0114],
        [ 0.0113, -0.0053, -0.0067,  ...,  0.0117,  0.0070, -0.0095],
        ...,
        [-0.0144, -0.0143, -0.0140,  ...,  0.0040,  0.0097, -0.0045],
        [-0.0006,  0.0076,  0.0072,  ..., -0.0185, -0.0180,  0.0109],
        [-0.0063, -0.0090,  0.0133,  ..., -0.0177,  0.0186, -0.0374]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2512,  3.8203, -4.8750,  ...,  0.1265,  2.5566,  3.1504]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 05:44:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can download something, that thing is downloadable
If you can protect something, that thing is protectable
If you can consider something, that thing is considerable
If you can adjust something, that thing is adjustable
If you can rely something, that thing is reliable
If you can deliver something, that thing is deliverable
If you can foresee something, that thing is foreseeable
If you can replace something, that thing is
2024-07-23 05:44:37 root INFO     [order_1_approx] starting weight calculation for If you can consider something, that thing is considerable
If you can foresee something, that thing is foreseeable
If you can rely something, that thing is reliable
If you can protect something, that thing is protectable
If you can adjust something, that thing is adjustable
If you can download something, that thing is downloadable
If you can replace something, that thing is replaceable
If you can deliver something, that thing is
2024-07-23 05:44:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 05:48:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5054,  0.2488, -0.2693,  ..., -0.5029,  0.1104, -0.3579],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2786,  2.4238, -5.0195,  ..., -3.9219,  4.2266,  2.8945],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0129, -0.0013, -0.0063,  ...,  0.0186, -0.0073,  0.0112],
        [-0.0033, -0.0007, -0.0015,  ..., -0.0108,  0.0032,  0.0076],
        [ 0.0104,  0.0097, -0.0072,  ...,  0.0020,  0.0186, -0.0046],
        ...,
        [-0.0246,  0.0133,  0.0024,  ..., -0.0064,  0.0113,  0.0151],
        [-0.0024,  0.0048, -0.0030,  ..., -0.0081, -0.0184,  0.0052],
        [ 0.0005, -0.0093, -0.0009,  ..., -0.0303,  0.0095, -0.0524]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5674,  2.7695, -5.0820,  ..., -3.8594,  4.4219,  3.3574]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 05:48:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can consider something, that thing is considerable
If you can foresee something, that thing is foreseeable
If you can rely something, that thing is reliable
If you can protect something, that thing is protectable
If you can adjust something, that thing is adjustable
If you can download something, that thing is downloadable
If you can replace something, that thing is replaceable
If you can deliver something, that thing is
2024-07-23 05:48:36 root INFO     [order_1_approx] starting weight calculation for If you can consider something, that thing is considerable
If you can protect something, that thing is protectable
If you can deliver something, that thing is deliverable
If you can rely something, that thing is reliable
If you can download something, that thing is downloadable
If you can foresee something, that thing is foreseeable
If you can replace something, that thing is replaceable
If you can adjust something, that thing is
2024-07-23 05:48:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 05:52:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4878,  0.9590,  0.1689,  ..., -0.1357,  0.7422, -0.5225],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6240,  6.6055, -4.6680,  ..., -2.4199,  2.8125,  0.4902],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0106,  0.0086, -0.0072,  ...,  0.0148, -0.0046,  0.0134],
        [-0.0034, -0.0020, -0.0100,  ...,  0.0074,  0.0048, -0.0151],
        [ 0.0051,  0.0082, -0.0165,  ...,  0.0358, -0.0020, -0.0259],
        ...,
        [-0.0240, -0.0075, -0.0057,  ..., -0.0121,  0.0050,  0.0114],
        [-0.0029, -0.0077,  0.0036,  ..., -0.0222, -0.0264,  0.0093],
        [-0.0183, -0.0021,  0.0026,  ...,  0.0073,  0.0017, -0.0305]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0254,  7.1250, -4.8477,  ..., -1.8857,  3.0000, -0.2017]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 05:52:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can consider something, that thing is considerable
If you can protect something, that thing is protectable
If you can deliver something, that thing is deliverable
If you can rely something, that thing is reliable
If you can download something, that thing is downloadable
If you can foresee something, that thing is foreseeable
If you can replace something, that thing is replaceable
If you can adjust something, that thing is
2024-07-23 05:52:35 root INFO     [order_1_approx] starting weight calculation for If you can adjust something, that thing is adjustable
If you can replace something, that thing is replaceable
If you can foresee something, that thing is foreseeable
If you can protect something, that thing is protectable
If you can download something, that thing is downloadable
If you can rely something, that thing is reliable
If you can deliver something, that thing is deliverable
If you can consider something, that thing is
2024-07-23 05:52:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 05:56:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1968,  0.1719, -0.7534,  ..., -0.2140,  0.3826,  1.0029],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0144,  4.8398, -4.9492,  ..., -1.3174,  2.5020, -2.2715],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0073,  0.0086, -0.0064,  ...,  0.0108, -0.0155, -0.0072],
        [-0.0028, -0.0067,  0.0081,  ..., -0.0021, -0.0127, -0.0076],
        [-0.0085,  0.0115, -0.0083,  ...,  0.0179,  0.0043, -0.0042],
        ...,
        [-0.0129, -0.0101, -0.0058,  ..., -0.0057,  0.0070,  0.0139],
        [ 0.0115, -0.0177,  0.0064,  ..., -0.0204, -0.0176,  0.0107],
        [-0.0093, -0.0096, -0.0042,  ..., -0.0065,  0.0283, -0.0085]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1232,  4.1133, -5.2422,  ..., -0.1338,  3.3047, -2.7578]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 05:56:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can adjust something, that thing is adjustable
If you can replace something, that thing is replaceable
If you can foresee something, that thing is foreseeable
If you can protect something, that thing is protectable
If you can download something, that thing is downloadable
If you can rely something, that thing is reliable
If you can deliver something, that thing is deliverable
If you can consider something, that thing is
2024-07-23 05:56:33 root INFO     [order_1_approx] starting weight calculation for If you can download something, that thing is downloadable
If you can foresee something, that thing is foreseeable
If you can adjust something, that thing is adjustable
If you can consider something, that thing is considerable
If you can replace something, that thing is replaceable
If you can protect something, that thing is protectable
If you can deliver something, that thing is deliverable
If you can rely something, that thing is
2024-07-23 05:56:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 06:00:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3474, -0.0078, -0.0267,  ...,  0.1484,  0.9883,  0.1643],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6748,  3.0000, -4.2422,  ..., -2.0684,  2.1348,  4.5859],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.8677e-02, -1.7609e-02,  4.5738e-03,  ...,  1.3672e-02,
         -2.0004e-02, -1.9623e-02],
        [-2.8648e-03, -5.9280e-03, -5.1460e-03,  ..., -2.1935e-05,
          2.6817e-03,  8.4610e-03],
        [-4.9362e-03,  1.7792e-02, -4.9400e-03,  ...,  1.0353e-02,
          2.3224e-02, -1.2924e-02],
        ...,
        [ 8.6784e-04, -2.9633e-02, -2.0355e-02,  ..., -4.1382e-02,
          2.4910e-03, -2.2324e-02],
        [ 9.8724e-03, -1.6098e-02,  4.5013e-03,  ..., -2.2003e-02,
         -4.7760e-02,  1.3657e-02],
        [-1.1005e-03, -3.5248e-03,  1.3733e-02,  ..., -3.0731e-02,
          1.5610e-02, -6.5002e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4124,  2.3223, -4.6133,  ..., -1.8691,  2.1621,  4.6875]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 06:00:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can download something, that thing is downloadable
If you can foresee something, that thing is foreseeable
If you can adjust something, that thing is adjustable
If you can consider something, that thing is considerable
If you can replace something, that thing is replaceable
If you can protect something, that thing is protectable
If you can deliver something, that thing is deliverable
If you can rely something, that thing is
2024-07-23 06:00:31 root INFO     [order_1_approx] starting weight calculation for If you can rely something, that thing is reliable
If you can adjust something, that thing is adjustable
If you can download something, that thing is downloadable
If you can replace something, that thing is replaceable
If you can protect something, that thing is protectable
If you can consider something, that thing is considerable
If you can deliver something, that thing is deliverable
If you can foresee something, that thing is
2024-07-23 06:00:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 06:04:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3828,  0.7529, -1.1777,  ..., -0.0032,  0.4497,  0.8447],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6270,  3.0312, -6.9805,  ..., -6.0898, -1.3398, -0.1182],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0195,  0.0145, -0.0016,  ...,  0.0047, -0.0049,  0.0125],
        [-0.0061, -0.0071,  0.0082,  ..., -0.0096,  0.0006, -0.0058],
        [ 0.0061,  0.0035, -0.0108,  ...,  0.0083,  0.0176, -0.0034],
        ...,
        [-0.0141, -0.0161, -0.0165,  ..., -0.0104,  0.0134,  0.0079],
        [-0.0020, -0.0009,  0.0088,  ..., -0.0142, -0.0523,  0.0139],
        [-0.0225, -0.0075,  0.0001,  ..., -0.0328,  0.0176, -0.0326]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3652,  3.9062, -6.4648,  ..., -6.9531, -1.2295,  0.1968]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 06:04:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can rely something, that thing is reliable
If you can adjust something, that thing is adjustable
If you can download something, that thing is downloadable
If you can replace something, that thing is replaceable
If you can protect something, that thing is protectable
If you can consider something, that thing is considerable
If you can deliver something, that thing is deliverable
If you can foresee something, that thing is
2024-07-23 06:04:30 root INFO     total operator prediction time: 1909.0666563510895 seconds
2024-07-23 06:04:30 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on un+adj_reg
2024-07-23 06:04:30 root INFO     building operator un+adj_reg
2024-07-23 06:04:30 root INFO     [order_1_approx] starting weight calculation for The opposite of specified is unspecified
The opposite of pleasant is unpleasant
The opposite of suitable is unsuitable
The opposite of happy is unhappy
The opposite of identified is unidentified
The opposite of comfortable is uncomfortable
The opposite of fortunate is unfortunate
The opposite of restricted is
2024-07-23 06:04:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 06:08:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3643, -0.4668, -0.0234,  ..., -0.5210,  1.8145, -0.2031],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.0469, -0.9844, -6.3086,  ...,  3.5508,  1.5449, -2.6816],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0008, -0.0004, -0.0047,  ..., -0.0045, -0.0327, -0.0201],
        [-0.0020, -0.0236, -0.0127,  ...,  0.0097, -0.0162,  0.0112],
        [ 0.0027,  0.0083, -0.0276,  ...,  0.0108, -0.0214, -0.0019],
        ...,
        [-0.0236, -0.0142,  0.0190,  ..., -0.0088,  0.0186,  0.0281],
        [ 0.0082, -0.0018,  0.0211,  ...,  0.0001,  0.0041,  0.0005],
        [-0.0003, -0.0027, -0.0407,  ..., -0.0087, -0.0052, -0.0228]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.7559, -0.2847, -5.5742,  ...,  3.1504,  1.4561, -1.9482]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 06:08:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of specified is unspecified
The opposite of pleasant is unpleasant
The opposite of suitable is unsuitable
The opposite of happy is unhappy
The opposite of identified is unidentified
The opposite of comfortable is uncomfortable
The opposite of fortunate is unfortunate
The opposite of restricted is
2024-07-23 06:08:30 root INFO     [order_1_approx] starting weight calculation for The opposite of pleasant is unpleasant
The opposite of fortunate is unfortunate
The opposite of suitable is unsuitable
The opposite of comfortable is uncomfortable
The opposite of identified is unidentified
The opposite of restricted is unrestricted
The opposite of happy is unhappy
The opposite of specified is
2024-07-23 06:08:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 06:12:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1517,  1.9131, -0.1042,  ..., -0.2412,  1.9336, -0.5884],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1250, -2.4727, -3.0156,  ...,  3.7246,  2.0977, -2.0156],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0067, -0.0150, -0.0023,  ..., -0.0329, -0.0148, -0.0028],
        [-0.0195, -0.0262, -0.0215,  ..., -0.0133, -0.0093, -0.0085],
        [-0.0062, -0.0026, -0.0011,  ...,  0.0259, -0.0187,  0.0191],
        ...,
        [-0.0315,  0.0193,  0.0014,  ..., -0.0153,  0.0023,  0.0364],
        [ 0.0045, -0.0090,  0.0137,  ..., -0.0005, -0.0130, -0.0111],
        [ 0.0069,  0.0093, -0.0128,  ..., -0.0053,  0.0113, -0.0031]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1927, -2.2148, -2.5762,  ...,  2.9688,  3.0781, -1.8965]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 06:12:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of pleasant is unpleasant
The opposite of fortunate is unfortunate
The opposite of suitable is unsuitable
The opposite of comfortable is uncomfortable
The opposite of identified is unidentified
The opposite of restricted is unrestricted
The opposite of happy is unhappy
The opposite of specified is
2024-07-23 06:12:29 root INFO     [order_1_approx] starting weight calculation for The opposite of fortunate is unfortunate
The opposite of happy is unhappy
The opposite of suitable is unsuitable
The opposite of comfortable is uncomfortable
The opposite of restricted is unrestricted
The opposite of pleasant is unpleasant
The opposite of specified is unspecified
The opposite of identified is
2024-07-23 06:12:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 06:16:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7148,  1.0215, -1.4326,  ..., -0.0215,  1.1631, -0.1055],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.0215, -2.0664, -1.6484,  ...,  3.5820,  2.4062,  0.8203],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0107, -0.0057,  0.0006,  ..., -0.0033,  0.0051, -0.0172],
        [-0.0109, -0.0197, -0.0061,  ..., -0.0132, -0.0099, -0.0081],
        [-0.0106, -0.0144, -0.0221,  ...,  0.0106, -0.0109, -0.0017],
        ...,
        [-0.0290, -0.0231,  0.0148,  ..., -0.0159,  0.0070,  0.0251],
        [-0.0043, -0.0175,  0.0116,  ...,  0.0047, -0.0047, -0.0002],
        [ 0.0309,  0.0022, -0.0010,  ..., -0.0002,  0.0047,  0.0152]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.0293, -2.1777, -1.5547,  ...,  4.3867,  3.3828,  1.0254]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 06:16:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of fortunate is unfortunate
The opposite of happy is unhappy
The opposite of suitable is unsuitable
The opposite of comfortable is uncomfortable
The opposite of restricted is unrestricted
The opposite of pleasant is unpleasant
The opposite of specified is unspecified
The opposite of identified is
2024-07-23 06:16:26 root INFO     [order_1_approx] starting weight calculation for The opposite of identified is unidentified
The opposite of comfortable is uncomfortable
The opposite of happy is unhappy
The opposite of suitable is unsuitable
The opposite of pleasant is unpleasant
The opposite of restricted is unrestricted
The opposite of specified is unspecified
The opposite of fortunate is
2024-07-23 06:16:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 06:20:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.9800, 1.3105, 0.3652,  ..., 0.2191, 1.9453, 0.6006], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.9844,  4.6562, -4.1133,  ...,  0.8096, -2.5859,  2.2344],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0034, -0.0259,  0.0082,  ..., -0.0059, -0.0089, -0.0200],
        [ 0.0093, -0.0088, -0.0052,  ..., -0.0084,  0.0054, -0.0027],
        [ 0.0125,  0.0051, -0.0352,  ...,  0.0061, -0.0109,  0.0059],
        ...,
        [-0.0152, -0.0138, -0.0156,  ..., -0.0583, -0.0216, -0.0003],
        [-0.0131, -0.0069, -0.0043,  ..., -0.0247, -0.0357, -0.0324],
        [-0.0139,  0.0190,  0.0060,  ..., -0.0014,  0.0263, -0.0218]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.8555,  4.5391, -3.6602,  ..., -0.6562, -3.9316,  2.4258]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 06:20:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of identified is unidentified
The opposite of comfortable is uncomfortable
The opposite of happy is unhappy
The opposite of suitable is unsuitable
The opposite of pleasant is unpleasant
The opposite of restricted is unrestricted
The opposite of specified is unspecified
The opposite of fortunate is
2024-07-23 06:20:24 root INFO     [order_1_approx] starting weight calculation for The opposite of identified is unidentified
The opposite of happy is unhappy
The opposite of restricted is unrestricted
The opposite of comfortable is uncomfortable
The opposite of fortunate is unfortunate
The opposite of specified is unspecified
The opposite of suitable is unsuitable
The opposite of pleasant is
2024-07-23 06:20:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 06:24:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2148,  0.0684, -0.5034,  ..., -1.4082,  2.2715,  0.3999],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.7422, -0.5674, -2.1758,  ...,  3.8926,  0.5420,  0.5625],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.3275e-02,  9.7656e-04, -1.3382e-02,  ..., -1.1482e-03,
         -6.3591e-03, -2.2491e-02],
        [ 6.5231e-04, -1.2405e-02, -1.3947e-02,  ..., -1.5625e-02,
         -9.2163e-03, -8.0566e-03],
        [ 8.1635e-03,  3.0441e-03, -3.4943e-02,  ..., -2.1305e-03,
         -1.2001e-02,  3.2471e-02],
        ...,
        [-1.9821e-02, -1.7868e-02,  6.7978e-03,  ..., -1.3351e-04,
         -8.6899e-03,  8.6594e-03],
        [-4.5395e-03, -9.5825e-03,  4.6310e-03,  ..., -8.4381e-03,
         -2.2797e-02,  1.6975e-03],
        [-4.5433e-03,  5.4932e-04,  2.2888e-05,  ...,  1.7433e-03,
          2.1301e-02, -2.3972e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.2266,  0.2144, -2.4297,  ...,  2.4336,  0.0959,  0.9326]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 06:24:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of identified is unidentified
The opposite of happy is unhappy
The opposite of restricted is unrestricted
The opposite of comfortable is uncomfortable
The opposite of fortunate is unfortunate
The opposite of specified is unspecified
The opposite of suitable is unsuitable
The opposite of pleasant is
2024-07-23 06:24:24 root INFO     [order_1_approx] starting weight calculation for The opposite of pleasant is unpleasant
The opposite of restricted is unrestricted
The opposite of specified is unspecified
The opposite of identified is unidentified
The opposite of suitable is unsuitable
The opposite of comfortable is uncomfortable
The opposite of fortunate is unfortunate
The opposite of happy is
2024-07-23 06:24:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 06:28:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6514,  0.5439, -0.0142,  ...,  0.7451,  2.0566,  0.1431],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.2012, -0.9468, -3.3750,  ...,  3.9648,  1.4404, -0.8633],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0034, -0.0187, -0.0096,  ..., -0.0002, -0.0011, -0.0019],
        [ 0.0036, -0.0087,  0.0088,  ..., -0.0017, -0.0060, -0.0135],
        [ 0.0065,  0.0022, -0.0412,  ...,  0.0039,  0.0185,  0.0266],
        ...,
        [-0.0199, -0.0147, -0.0020,  ..., -0.0040, -0.0074,  0.0090],
        [-0.0175, -0.0104,  0.0231,  ..., -0.0005, -0.0370, -0.0198],
        [-0.0034, -0.0139,  0.0002,  ..., -0.0016,  0.0159, -0.0243]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.8926, -1.4561, -3.6152,  ...,  3.6328,  3.0938, -0.8608]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 06:28:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of pleasant is unpleasant
The opposite of restricted is unrestricted
The opposite of specified is unspecified
The opposite of identified is unidentified
The opposite of suitable is unsuitable
The opposite of comfortable is uncomfortable
The opposite of fortunate is unfortunate
The opposite of happy is
2024-07-23 06:28:23 root INFO     [order_1_approx] starting weight calculation for The opposite of identified is unidentified
The opposite of fortunate is unfortunate
The opposite of pleasant is unpleasant
The opposite of restricted is unrestricted
The opposite of comfortable is uncomfortable
The opposite of specified is unspecified
The opposite of happy is unhappy
The opposite of suitable is
2024-07-23 06:28:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 06:32:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([1.7959, 0.1685, 0.1631,  ..., 0.1353, 1.2021, 0.5737], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9814, -0.0977, -1.0547,  ..., -0.5005,  2.3750,  2.5273],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.1086e-02, -1.0994e-02,  1.3107e-02,  ..., -2.6379e-03,
         -2.7115e-02, -1.5259e-02],
        [ 1.4313e-02, -5.1231e-03, -3.0182e-02,  ..., -1.1070e-02,
         -1.7395e-02, -1.6861e-02],
        [ 4.1046e-03, -4.9515e-03, -1.3687e-02,  ..., -9.6283e-03,
         -1.2802e-02,  5.0354e-03],
        ...,
        [-2.3087e-02, -2.4261e-02, -2.6703e-05,  ..., -9.9335e-03,
          1.4305e-02,  1.0582e-02],
        [-2.7588e-02, -1.6495e-02,  3.3508e-02,  ..., -1.4420e-03,
         -4.8370e-03,  2.5463e-03],
        [ 7.4997e-03,  3.5370e-02, -1.6251e-02,  ..., -3.9001e-02,
          3.6194e-02, -1.0452e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7554, -0.3232, -0.8760,  ...,  0.1694,  1.7148,  1.8301]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 06:32:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of identified is unidentified
The opposite of fortunate is unfortunate
The opposite of pleasant is unpleasant
The opposite of restricted is unrestricted
The opposite of comfortable is uncomfortable
The opposite of specified is unspecified
The opposite of happy is unhappy
The opposite of suitable is
2024-07-23 06:32:22 root INFO     [order_1_approx] starting weight calculation for The opposite of restricted is unrestricted
The opposite of fortunate is unfortunate
The opposite of specified is unspecified
The opposite of identified is unidentified
The opposite of happy is unhappy
The opposite of pleasant is unpleasant
The opposite of suitable is unsuitable
The opposite of comfortable is
2024-07-23 06:32:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 06:36:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1309,  0.5381, -0.7363,  ..., -0.3687,  1.9531,  0.4741],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.9570,  0.8066, -3.3945,  ..., -1.9199,  2.8164,  4.2500],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0062, -0.0220, -0.0094,  ..., -0.0038, -0.0087, -0.0210],
        [-0.0095, -0.0006, -0.0079,  ..., -0.0130, -0.0031, -0.0112],
        [ 0.0086, -0.0006, -0.0267,  ...,  0.0257,  0.0025,  0.0152],
        ...,
        [-0.0213, -0.0125, -0.0131,  ...,  0.0005, -0.0133,  0.0137],
        [-0.0151,  0.0154,  0.0295,  ..., -0.0010,  0.0069,  0.0006],
        [ 0.0030, -0.0008, -0.0024,  ...,  0.0056,  0.0054, -0.0125]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.0039,  0.8042, -2.9434,  ..., -2.5312,  2.8730,  4.5547]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 06:36:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of restricted is unrestricted
The opposite of fortunate is unfortunate
The opposite of specified is unspecified
The opposite of identified is unidentified
The opposite of happy is unhappy
The opposite of pleasant is unpleasant
The opposite of suitable is unsuitable
The opposite of comfortable is
2024-07-23 06:36:22 root INFO     total operator prediction time: 1911.4595613479614 seconds
2024-07-23 06:36:22 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on re+verb_reg
2024-07-23 06:36:22 root INFO     building operator re+verb_reg
2024-07-23 06:36:22 root INFO     [order_1_approx] starting weight calculation for To examine again is to reexamine
To distribute again is to redistribute
To generate again is to regenerate
To engage again is to reengage
To connect again is to reconnect
To write again is to rewrite
To assure again is to reassure
To learn again is to
2024-07-23 06:36:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 06:40:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4529,  0.3960,  1.6211,  ..., -0.4648,  0.8359,  0.5322],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.5840, -0.3032, -7.3828,  ..., -0.0391, -3.3750, -1.8203],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.0208e-02, -1.6594e-04, -8.3923e-05,  ..., -1.6136e-03,
          4.0665e-03, -1.4397e-02],
        [-1.2558e-02, -8.2779e-03,  2.7466e-04,  ..., -1.1757e-02,
          7.6103e-03, -1.6975e-03],
        [ 1.8448e-02,  2.3727e-03, -2.8954e-03,  ...,  6.3362e-03,
          5.4703e-03, -1.5427e-02],
        ...,
        [-1.5991e-02, -6.9542e-03,  3.1815e-03,  ..., -2.2614e-02,
         -1.5327e-02,  1.8295e-02],
        [ 2.0462e-02, -5.7755e-03,  1.2245e-03,  ..., -1.0742e-02,
         -1.2054e-02, -4.8714e-03],
        [ 2.3392e-02, -9.3842e-04, -3.5477e-03,  ..., -1.6846e-02,
          6.6147e-03, -1.8463e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.8906, -0.2122, -8.3516,  ...,  0.2917, -2.9648, -1.8125]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 06:40:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To examine again is to reexamine
To distribute again is to redistribute
To generate again is to regenerate
To engage again is to reengage
To connect again is to reconnect
To write again is to rewrite
To assure again is to reassure
To learn again is to
2024-07-23 06:40:21 root INFO     [order_1_approx] starting weight calculation for To connect again is to reconnect
To learn again is to relearn
To write again is to rewrite
To generate again is to regenerate
To distribute again is to redistribute
To examine again is to reexamine
To engage again is to reengage
To assure again is to
2024-07-23 06:40:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 06:44:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7827, -0.5962, -0.3782,  ...,  0.0967, -0.5112,  0.2651],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.2148,  3.5117, -4.7070,  ..., -0.6333,  0.0078, -2.0293],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0046, -0.0042,  0.0131,  ...,  0.0045, -0.0025, -0.0167],
        [ 0.0147, -0.0036,  0.0194,  ..., -0.0086,  0.0129,  0.0018],
        [ 0.0260,  0.0273,  0.0034,  ..., -0.0037, -0.0038,  0.0108],
        ...,
        [-0.0273, -0.0003, -0.0010,  ..., -0.0035, -0.0083,  0.0101],
        [ 0.0140, -0.0144, -0.0098,  ...,  0.0034, -0.0143, -0.0033],
        [ 0.0104,  0.0149,  0.0203,  ...,  0.0111, -0.0128,  0.0003]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6582,  3.9570, -5.7812,  ..., -0.9893, -0.0433, -1.8877]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 06:44:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To connect again is to reconnect
To learn again is to relearn
To write again is to rewrite
To generate again is to regenerate
To distribute again is to redistribute
To examine again is to reexamine
To engage again is to reengage
To assure again is to
2024-07-23 06:44:17 root INFO     [order_1_approx] starting weight calculation for To distribute again is to redistribute
To engage again is to reengage
To assure again is to reassure
To learn again is to relearn
To connect again is to reconnect
To examine again is to reexamine
To generate again is to regenerate
To write again is to
2024-07-23 06:44:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 06:48:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.9346, -0.5269,  1.3047,  ...,  0.6641, -0.0580,  0.8047],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.5381, -0.5850, -5.0742,  ...,  2.4297, -1.0742, -1.5752],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0238, -0.0028,  0.0071,  ...,  0.0014, -0.0091, -0.0196],
        [ 0.0150, -0.0102, -0.0038,  ..., -0.0022,  0.0017, -0.0016],
        [ 0.0118,  0.0200, -0.0005,  ...,  0.0056, -0.0085, -0.0097],
        ...,
        [-0.0167,  0.0107,  0.0046,  ..., -0.0290, -0.0112,  0.0060],
        [ 0.0056, -0.0054,  0.0125,  ..., -0.0041, -0.0102,  0.0035],
        [ 0.0027,  0.0112,  0.0047,  ..., -0.0189,  0.0100, -0.0054]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0117, -0.2856, -4.4297,  ...,  1.9600, -0.6562, -1.7305]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 06:48:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To distribute again is to redistribute
To engage again is to reengage
To assure again is to reassure
To learn again is to relearn
To connect again is to reconnect
To examine again is to reexamine
To generate again is to regenerate
To write again is to
2024-07-23 06:48:13 root INFO     [order_1_approx] starting weight calculation for To write again is to rewrite
To examine again is to reexamine
To distribute again is to redistribute
To assure again is to reassure
To engage again is to reengage
To learn again is to relearn
To generate again is to regenerate
To connect again is to
2024-07-23 06:48:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 06:52:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2690, -0.0337,  0.7686,  ...,  0.7690,  0.2283,  1.2773],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1226, -0.8525, -4.3047,  ...,  0.8623, -5.5781,  2.0859],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0135, -0.0025,  0.0093,  ...,  0.0126, -0.0162, -0.0238],
        [ 0.0067, -0.0033,  0.0080,  ...,  0.0068,  0.0070,  0.0008],
        [ 0.0095,  0.0115,  0.0051,  ...,  0.0177, -0.0047, -0.0027],
        ...,
        [-0.0290, -0.0125, -0.0073,  ..., -0.0058, -0.0116,  0.0034],
        [ 0.0041,  0.0161, -0.0026,  ..., -0.0034, -0.0158,  0.0057],
        [ 0.0152,  0.0077,  0.0061,  ..., -0.0038,  0.0009, -0.0029]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5542, -0.8945, -5.8320,  ...,  1.4922, -6.1914,  1.4043]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 06:52:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To write again is to rewrite
To examine again is to reexamine
To distribute again is to redistribute
To assure again is to reassure
To engage again is to reengage
To learn again is to relearn
To generate again is to regenerate
To connect again is to
2024-07-23 06:52:10 root INFO     [order_1_approx] starting weight calculation for To distribute again is to redistribute
To generate again is to regenerate
To connect again is to reconnect
To learn again is to relearn
To write again is to rewrite
To engage again is to reengage
To assure again is to reassure
To examine again is to
2024-07-23 06:52:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 06:56:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3096, -0.2563,  0.2766,  ..., -0.7905,  0.1597,  0.6021],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.0430,  1.8760, -5.2812,  ..., -0.4043, -3.6953, -1.9902],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0078, -0.0019,  0.0083,  ..., -0.0091, -0.0047, -0.0122],
        [-0.0198, -0.0051,  0.0076,  ..., -0.0077,  0.0072, -0.0070],
        [ 0.0549,  0.0494, -0.0124,  ..., -0.0038,  0.0029, -0.0039],
        ...,
        [-0.0280, -0.0046,  0.0074,  ..., -0.0196, -0.0038,  0.0056],
        [ 0.0008, -0.0193,  0.0173,  ..., -0.0040, -0.0112, -0.0105],
        [ 0.0150,  0.0223,  0.0055,  ..., -0.0017,  0.0227, -0.0094]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.4395,  2.8672, -6.4102,  ...,  0.0640, -4.4180, -2.2578]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 06:56:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To distribute again is to redistribute
To generate again is to regenerate
To connect again is to reconnect
To learn again is to relearn
To write again is to rewrite
To engage again is to reengage
To assure again is to reassure
To examine again is to
2024-07-23 06:56:09 root INFO     [order_1_approx] starting weight calculation for To examine again is to reexamine
To distribute again is to redistribute
To learn again is to relearn
To write again is to rewrite
To engage again is to reengage
To assure again is to reassure
To connect again is to reconnect
To generate again is to
2024-07-23 06:56:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 07:00:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3545, -0.1465,  0.1672,  ...,  0.6992, -0.8379,  0.5669],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5293, -0.2368, -4.3242,  ...,  4.5547,  0.4995, -1.9473],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0279, -0.0009,  0.0125,  ...,  0.0009,  0.0044, -0.0188],
        [ 0.0005,  0.0117,  0.0028,  ...,  0.0120,  0.0048, -0.0072],
        [ 0.0210,  0.0113, -0.0081,  ...,  0.0106,  0.0146,  0.0011],
        ...,
        [-0.0268, -0.0077, -0.0129,  ..., -0.0060, -0.0258,  0.0015],
        [-0.0094, -0.0129,  0.0020,  ..., -0.0027, -0.0219, -0.0050],
        [ 0.0155,  0.0339,  0.0094,  ...,  0.0062,  0.0159,  0.0083]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.4521,  0.3745, -5.1602,  ...,  5.0508,  1.2227, -1.9492]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 07:00:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To examine again is to reexamine
To distribute again is to redistribute
To learn again is to relearn
To write again is to rewrite
To engage again is to reengage
To assure again is to reassure
To connect again is to reconnect
To generate again is to
2024-07-23 07:00:07 root INFO     [order_1_approx] starting weight calculation for To generate again is to regenerate
To examine again is to reexamine
To write again is to rewrite
To assure again is to reassure
To engage again is to reengage
To connect again is to reconnect
To learn again is to relearn
To distribute again is to
2024-07-23 07:00:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 07:04:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4045,  0.3799,  0.8984,  ...,  0.5107, -0.4658,  0.7871],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.1152,  1.6797, -5.3125,  ..., -1.3154,  1.1055, -1.3584],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0146, -0.0025,  0.0107,  ...,  0.0106, -0.0066, -0.0105],
        [-0.0025,  0.0059,  0.0206,  ...,  0.0010,  0.0140, -0.0107],
        [ 0.0228,  0.0010, -0.0001,  ...,  0.0056, -0.0011,  0.0117],
        ...,
        [-0.0407, -0.0088, -0.0140,  ..., -0.0152, -0.0036,  0.0004],
        [ 0.0016,  0.0004,  0.0018,  ..., -0.0136, -0.0057,  0.0122],
        [ 0.0214, -0.0009, -0.0029,  ..., -0.0182,  0.0127,  0.0096]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.8047,  2.3945, -6.4609,  ..., -0.7314,  0.7900, -2.5234]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 07:04:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To generate again is to regenerate
To examine again is to reexamine
To write again is to rewrite
To assure again is to reassure
To engage again is to reengage
To connect again is to reconnect
To learn again is to relearn
To distribute again is to
2024-07-23 07:04:06 root INFO     [order_1_approx] starting weight calculation for To distribute again is to redistribute
To examine again is to reexamine
To connect again is to reconnect
To assure again is to reassure
To generate again is to regenerate
To write again is to rewrite
To learn again is to relearn
To engage again is to
2024-07-23 07:04:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 07:08:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2471, -0.0508,  0.6846,  ...,  0.5195, -0.9702,  0.9141],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2080, -0.8672, -2.9102,  ..., -0.7041, -1.1377,  1.0078],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.1927e-02, -2.0874e-02,  4.9706e-03,  ...,  1.8044e-03,
          3.3951e-04, -8.9951e-03],
        [ 1.4862e-02,  7.1716e-03, -9.8419e-04,  ...,  7.4005e-04,
          2.4796e-05, -8.8806e-03],
        [ 5.5428e-03,  1.7757e-03,  2.3766e-03,  ..., -8.5983e-03,
          4.5624e-03,  9.7885e-03],
        ...,
        [-1.0056e-02, -1.4067e-04, -6.1302e-03,  ...,  4.6158e-04,
         -2.8896e-03, -2.6340e-03],
        [ 9.2316e-03, -2.4704e-02,  1.5545e-04,  ...,  5.6000e-03,
         -5.1994e-03,  8.5449e-03],
        [-1.0586e-03,  2.9640e-03,  4.0855e-03,  ..., -5.1193e-03,
          7.1869e-03,  1.6365e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1096, -1.8760, -4.4297,  ..., -1.0020, -1.5664,  0.6455]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 07:08:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To distribute again is to redistribute
To examine again is to reexamine
To connect again is to reconnect
To assure again is to reassure
To generate again is to regenerate
To write again is to rewrite
To learn again is to relearn
To engage again is to
2024-07-23 07:08:05 root INFO     total operator prediction time: 1903.1378798484802 seconds
2024-07-23 07:08:05 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on adj+ness_reg
2024-07-23 07:08:05 root INFO     building operator adj+ness_reg
2024-07-23 07:08:05 root INFO     [order_1_approx] starting weight calculation for The state of being marked is markedness
The state of being aware is awareness
The state of being careful is carefulness
The state of being devoted is devotedness
The state of being odd is oddness
The state of being broken is brokenness
The state of being happy is happiness
The state of being distinct is
2024-07-23 07:08:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 07:12:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8203,  0.3188,  1.3848,  ...,  0.4189,  0.6709, -1.3076],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.8711,  1.9189,  0.1426,  ..., -3.5117, -3.7754,  1.4590],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0120, -0.0181,  0.0201,  ...,  0.0003, -0.0028, -0.0179],
        [-0.0055, -0.0078, -0.0071,  ..., -0.0056,  0.0036, -0.0084],
        [ 0.0090, -0.0188, -0.0320,  ...,  0.0075, -0.0085,  0.0050],
        ...,
        [-0.0078, -0.0026,  0.0069,  ..., -0.0106,  0.0118, -0.0107],
        [ 0.0224,  0.0188,  0.0037,  ..., -0.0126, -0.0261,  0.0105],
        [-0.0057,  0.0020,  0.0015,  ..., -0.0042,  0.0139, -0.0130]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.8906,  2.1484,  0.2214,  ..., -3.8164, -4.3086,  1.2773]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 07:12:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being marked is markedness
The state of being aware is awareness
The state of being careful is carefulness
The state of being devoted is devotedness
The state of being odd is oddness
The state of being broken is brokenness
The state of being happy is happiness
The state of being distinct is
2024-07-23 07:12:05 root INFO     [order_1_approx] starting weight calculation for The state of being happy is happiness
The state of being marked is markedness
The state of being aware is awareness
The state of being distinct is distinctness
The state of being odd is oddness
The state of being careful is carefulness
The state of being devoted is devotedness
The state of being broken is
2024-07-23 07:12:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 07:16:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5586, -0.6631,  0.8628,  ...,  0.2256,  0.6465, -0.6699],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.4082, -0.0801, -5.2109,  ..., -3.6953, -0.8662,  5.4297],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0196, -0.0016,  0.0207,  ..., -0.0055, -0.0112, -0.0016],
        [-0.0063, -0.0213, -0.0093,  ..., -0.0197, -0.0070, -0.0171],
        [ 0.0008,  0.0185, -0.0042,  ..., -0.0089, -0.0092,  0.0068],
        ...,
        [-0.0044, -0.0169, -0.0118,  ..., -0.0327, -0.0057, -0.0161],
        [-0.0069,  0.0004,  0.0085,  ..., -0.0023, -0.0172,  0.0082],
        [ 0.0017,  0.0019,  0.0075,  ...,  0.0028,  0.0164, -0.0385]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2178, -0.5098, -5.0117,  ..., -3.9844, -0.6279,  4.6250]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 07:16:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being happy is happiness
The state of being marked is markedness
The state of being aware is awareness
The state of being distinct is distinctness
The state of being odd is oddness
The state of being careful is carefulness
The state of being devoted is devotedness
The state of being broken is
2024-07-23 07:16:04 root INFO     [order_1_approx] starting weight calculation for The state of being devoted is devotedness
The state of being broken is brokenness
The state of being marked is markedness
The state of being distinct is distinctness
The state of being odd is oddness
The state of being aware is awareness
The state of being happy is happiness
The state of being careful is
2024-07-23 07:16:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 07:20:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9785, -0.3577,  0.3179,  ...,  0.8569, -0.4170,  0.0994],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.4922,  2.5195,  3.3613,  ..., -1.1270, -1.6523, -0.3110],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0182, -0.0232,  0.0139,  ..., -0.0008, -0.0233, -0.0069],
        [-0.0069, -0.0117, -0.0047,  ..., -0.0075, -0.0013, -0.0080],
        [ 0.0024, -0.0026, -0.0156,  ..., -0.0024,  0.0136,  0.0039],
        ...,
        [-0.0200, -0.0136, -0.0069,  ..., -0.0221,  0.0161, -0.0104],
        [ 0.0026,  0.0258,  0.0056,  ..., -0.0323, -0.0383,  0.0025],
        [-0.0126,  0.0133,  0.0011,  ..., -0.0133,  0.0167, -0.0240]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.0684,  2.6719,  3.3320,  ..., -0.6011, -1.8369,  0.0085]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 07:20:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being devoted is devotedness
The state of being broken is brokenness
The state of being marked is markedness
The state of being distinct is distinctness
The state of being odd is oddness
The state of being aware is awareness
The state of being happy is happiness
The state of being careful is
2024-07-23 07:20:04 root INFO     [order_1_approx] starting weight calculation for The state of being happy is happiness
The state of being marked is markedness
The state of being aware is awareness
The state of being devoted is devotedness
The state of being distinct is distinctness
The state of being careful is carefulness
The state of being broken is brokenness
The state of being odd is
2024-07-23 07:20:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 07:24:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5562, -0.5146,  0.0942,  ..., -0.7441,  1.2109,  0.3230],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.4883,  0.4888, -0.3223,  ..., -3.8457,  0.8643,  6.0938],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 9.9411e-03, -1.5244e-02,  1.6388e-02,  ..., -3.4332e-05,
         -1.0017e-02, -1.1635e-02],
        [ 5.8212e-03, -1.4870e-02, -1.3504e-02,  ...,  3.3493e-03,
          2.9716e-03,  7.9346e-04],
        [ 7.9041e-03,  2.3956e-03, -6.9199e-03,  ..., -1.1343e-04,
         -9.0256e-03,  2.5463e-03],
        ...,
        [-3.3512e-03, -4.5967e-04, -1.0956e-02,  ..., -7.8812e-03,
         -1.6541e-02, -1.3000e-02],
        [ 2.1553e-04,  1.8768e-02,  1.5778e-02,  ..., -1.1520e-02,
         -2.6810e-02, -3.7041e-03],
        [-2.5299e-02, -9.5367e-03, -1.9318e-02,  ...,  1.3428e-03,
          8.9417e-03, -2.6291e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.8984,  0.6191, -0.3574,  ..., -4.1914,  0.9248,  6.6055]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 07:24:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being happy is happiness
The state of being marked is markedness
The state of being aware is awareness
The state of being devoted is devotedness
The state of being distinct is distinctness
The state of being careful is carefulness
The state of being broken is brokenness
The state of being odd is
2024-07-23 07:24:02 root INFO     [order_1_approx] starting weight calculation for The state of being marked is markedness
The state of being odd is oddness
The state of being distinct is distinctness
The state of being broken is brokenness
The state of being devoted is devotedness
The state of being happy is happiness
The state of being careful is carefulness
The state of being aware is
2024-07-23 07:24:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 07:27:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0049,  0.2471,  0.1481,  ...,  0.0959,  0.5757, -0.3777],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.8535,  5.1562,  0.4492,  ..., -4.6836, -2.5684,  0.0752],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0255, -0.0145,  0.0045,  ..., -0.0033, -0.0074, -0.0223],
        [-0.0023, -0.0200, -0.0063,  ...,  0.0013, -0.0017, -0.0032],
        [ 0.0110,  0.0018, -0.0150,  ...,  0.0097,  0.0009, -0.0050],
        ...,
        [-0.0217, -0.0107, -0.0118,  ..., -0.0232, -0.0071, -0.0010],
        [-0.0047,  0.0067,  0.0138,  ..., -0.0177, -0.0083, -0.0012],
        [ 0.0027,  0.0013,  0.0024,  ..., -0.0060,  0.0135, -0.0301]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.6328,  5.3281,  0.1904,  ..., -4.2188, -2.3359,  0.3813]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 07:28:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being marked is markedness
The state of being odd is oddness
The state of being distinct is distinctness
The state of being broken is brokenness
The state of being devoted is devotedness
The state of being happy is happiness
The state of being careful is carefulness
The state of being aware is
2024-07-23 07:28:00 root INFO     [order_1_approx] starting weight calculation for The state of being distinct is distinctness
The state of being odd is oddness
The state of being aware is awareness
The state of being broken is brokenness
The state of being careful is carefulness
The state of being devoted is devotedness
The state of being marked is markedness
The state of being happy is
2024-07-23 07:28:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 07:31:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3352,  0.4897,  0.9385,  ..., -0.1653,  1.4414,  0.1689],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0215,  1.4131, -0.6699,  ..., -0.9990,  4.3906,  4.2500],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0146, -0.0298,  0.0061,  ...,  0.0027, -0.0160, -0.0302],
        [-0.0052, -0.0184, -0.0007,  ..., -0.0113,  0.0061, -0.0148],
        [-0.0054,  0.0023, -0.0243,  ..., -0.0036,  0.0054,  0.0023],
        ...,
        [-0.0126, -0.0114, -0.0049,  ..., -0.0245, -0.0098, -0.0208],
        [-0.0070,  0.0055,  0.0053,  ..., -0.0143, -0.0356, -0.0103],
        [-0.0171, -0.0142,  0.0006,  ...,  0.0024,  0.0108, -0.0392]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9043,  1.6152, -0.6284,  ..., -0.5171,  4.2383,  4.6367]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 07:32:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being distinct is distinctness
The state of being odd is oddness
The state of being aware is awareness
The state of being broken is brokenness
The state of being careful is carefulness
The state of being devoted is devotedness
The state of being marked is markedness
The state of being happy is
2024-07-23 07:32:00 root INFO     [order_1_approx] starting weight calculation for The state of being devoted is devotedness
The state of being odd is oddness
The state of being careful is carefulness
The state of being aware is awareness
The state of being happy is happiness
The state of being distinct is distinctness
The state of being broken is brokenness
The state of being marked is
2024-07-23 07:32:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 07:35:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3721,  0.2954,  1.5215,  ...,  1.0059,  0.1973, -0.0380],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.7012,  0.7642, -0.3672,  ..., -1.1240,  0.8701,  6.7461],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0030,  0.0005,  0.0096,  ...,  0.0052, -0.0058, -0.0178],
        [-0.0049, -0.0066, -0.0060,  ..., -0.0186,  0.0068, -0.0126],
        [ 0.0162,  0.0048, -0.0022,  ...,  0.0046, -0.0241,  0.0094],
        ...,
        [-0.0199, -0.0217, -0.0003,  ..., -0.0151, -0.0037, -0.0171],
        [-0.0057,  0.0148,  0.0083,  ...,  0.0079, -0.0049,  0.0124],
        [-0.0110, -0.0176, -0.0035,  ..., -0.0163,  0.0079, -0.0248]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.7539,  1.1094,  0.3804,  ..., -0.6318,  1.7402,  6.5078]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 07:36:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being devoted is devotedness
The state of being odd is oddness
The state of being careful is carefulness
The state of being aware is awareness
The state of being happy is happiness
The state of being distinct is distinctness
The state of being broken is brokenness
The state of being marked is
2024-07-23 07:36:00 root INFO     [order_1_approx] starting weight calculation for The state of being happy is happiness
The state of being aware is awareness
The state of being broken is brokenness
The state of being distinct is distinctness
The state of being odd is oddness
The state of being careful is carefulness
The state of being marked is markedness
The state of being devoted is
2024-07-23 07:36:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 07:39:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2649, -0.0750, -0.1182,  ...,  0.0405,  0.8623, -0.7524],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7451,  4.1172,  4.8359,  ..., -3.7324, -0.6631,  5.0977],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0092,  0.0005,  0.0187,  ..., -0.0125, -0.0422, -0.0146],
        [-0.0086, -0.0268, -0.0236,  ..., -0.0089,  0.0321,  0.0179],
        [ 0.0093,  0.0247, -0.0340,  ...,  0.0067, -0.0019,  0.0095],
        ...,
        [-0.0293, -0.0193,  0.0154,  ..., -0.0441,  0.0012,  0.0028],
        [-0.0186, -0.0058,  0.0079,  ..., -0.0060, -0.0389, -0.0065],
        [-0.0152,  0.0050,  0.0037,  ...,  0.0002, -0.0060, -0.0288]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7002,  3.7070,  5.6641,  ..., -2.8750, -0.1785,  5.4844]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 07:39:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being happy is happiness
The state of being aware is awareness
The state of being broken is brokenness
The state of being distinct is distinctness
The state of being odd is oddness
The state of being careful is carefulness
The state of being marked is markedness
The state of being devoted is
2024-07-23 07:39:59 root INFO     total operator prediction time: 1914.6933240890503 seconds
2024-07-23 07:39:59 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on noun+less_reg
2024-07-23 07:39:59 root INFO     building operator noun+less_reg
2024-07-23 07:40:00 root INFO     [order_1_approx] starting weight calculation for Something without heir is heirless
Something without arm is armless
Something without leg is legless
Something without faith is faithless
Something without art is artless
Something without carbon is carbonless
Something without death is deathless
Something without expression is
2024-07-23 07:40:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 07:43:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7500, -0.2588,  1.2500,  ..., -0.5259,  0.6670,  0.8789],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6133, -0.0713, -0.5596,  ...,  0.3018,  0.7412, -0.1348],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0038, -0.0124, -0.0113,  ..., -0.0015, -0.0265, -0.0017],
        [-0.0143, -0.0393,  0.0024,  ...,  0.0025,  0.0025,  0.0018],
        [ 0.0039,  0.0196, -0.0419,  ...,  0.0083, -0.0052, -0.0044],
        ...,
        [-0.0110,  0.0035,  0.0148,  ..., -0.0194,  0.0151,  0.0113],
        [-0.0130, -0.0089,  0.0282,  ..., -0.0105, -0.0262,  0.0092],
        [ 0.0145,  0.0117, -0.0048,  ..., -0.0162,  0.0049, -0.0180]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1375,  0.2693, -0.8721,  ...,  1.0732,  0.5400, -0.9604]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 07:43:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without heir is heirless
Something without arm is armless
Something without leg is legless
Something without faith is faithless
Something without art is artless
Something without carbon is carbonless
Something without death is deathless
Something without expression is
2024-07-23 07:44:00 root INFO     [order_1_approx] starting weight calculation for Something without leg is legless
Something without art is artless
Something without death is deathless
Something without expression is expressionless
Something without heir is heirless
Something without arm is armless
Something without carbon is carbonless
Something without faith is
2024-07-23 07:44:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 07:47:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5068, -1.3438,  0.1699,  ...,  0.2454,  0.9673,  0.5537],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.3105,  2.6094, -0.8047,  ...,  1.9707,  3.3438,  0.3496],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0271, -0.0094,  0.0135,  ..., -0.0068, -0.0364, -0.0264],
        [-0.0043, -0.0348, -0.0046,  ...,  0.0079,  0.0113,  0.0021],
        [ 0.0051, -0.0037, -0.0268,  ...,  0.0235,  0.0190,  0.0038],
        ...,
        [ 0.0176, -0.0067, -0.0136,  ..., -0.0488, -0.0035,  0.0066],
        [-0.0009, -0.0168,  0.0191,  ...,  0.0168, -0.0486,  0.0107],
        [ 0.0009,  0.0241,  0.0083,  ...,  0.0117, -0.0121, -0.0605]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-7.9688e-01,  3.6289e+00, -1.0693e+00,  ...,  2.1309e+00,
          1.9141e+00,  7.3242e-04]], device='cuda:1', dtype=torch.float16,
       grad_fn=<SubBackward0>) 

                    
2024-07-23 07:47:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without leg is legless
Something without art is artless
Something without death is deathless
Something without expression is expressionless
Something without heir is heirless
Something without arm is armless
Something without carbon is carbonless
Something without faith is
2024-07-23 07:47:59 root INFO     [order_1_approx] starting weight calculation for Something without death is deathless
Something without leg is legless
Something without expression is expressionless
Something without heir is heirless
Something without art is artless
Something without faith is faithless
Something without arm is armless
Something without carbon is
2024-07-23 07:47:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 07:51:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5957,  0.7979, -0.9038,  ..., -0.0149,  0.4541,  0.9204],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.0000, -1.2207, -0.3008,  ..., -0.8047, -0.7334,  2.0469],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0032, -0.0057, -0.0076,  ..., -0.0022, -0.0161, -0.0192],
        [-0.0087, -0.0117,  0.0068,  ..., -0.0067, -0.0101, -0.0013],
        [-0.0135, -0.0046, -0.0226,  ...,  0.0016,  0.0020,  0.0262],
        ...,
        [-0.0068,  0.0036,  0.0149,  ..., -0.0151,  0.0138,  0.0010],
        [-0.0027, -0.0027,  0.0218,  ...,  0.0080, -0.0146,  0.0023],
        [-0.0021,  0.0044,  0.0018,  ..., -0.0147,  0.0004, -0.0092]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.4355, -0.8638, -0.4868,  ..., -1.0068, -0.0933,  2.7227]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 07:51:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without death is deathless
Something without leg is legless
Something without expression is expressionless
Something without heir is heirless
Something without art is artless
Something without faith is faithless
Something without arm is armless
Something without carbon is
2024-07-23 07:51:58 root INFO     [order_1_approx] starting weight calculation for Something without faith is faithless
Something without arm is armless
Something without heir is heirless
Something without art is artless
Something without carbon is carbonless
Something without expression is expressionless
Something without death is deathless
Something without leg is
2024-07-23 07:51:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 07:55:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1963,  0.5957,  0.3328,  ...,  0.4792, -0.3945,  1.0947],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.7344, -1.5322, -2.3203,  ..., -3.7363,  1.6387,  3.5469],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0195, -0.0257,  0.0050,  ..., -0.0178, -0.0074,  0.0177],
        [-0.0035,  0.0064, -0.0123,  ..., -0.0153, -0.0059, -0.0213],
        [-0.0038,  0.0086, -0.0349,  ...,  0.0064,  0.0029,  0.0232],
        ...,
        [-0.0044,  0.0097,  0.0053,  ..., -0.0233, -0.0139, -0.0118],
        [-0.0046,  0.0108, -0.0015,  ..., -0.0042, -0.0215, -0.0169],
        [-0.0124, -0.0093,  0.0087,  ..., -0.0063,  0.0146, -0.0508]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.8438, -1.0049, -2.4746,  ..., -3.1250,  1.8389,  3.5098]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 07:55:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without faith is faithless
Something without arm is armless
Something without heir is heirless
Something without art is artless
Something without carbon is carbonless
Something without expression is expressionless
Something without death is deathless
Something without leg is
2024-07-23 07:55:58 root INFO     [order_1_approx] starting weight calculation for Something without expression is expressionless
Something without art is artless
Something without leg is legless
Something without heir is heirless
Something without carbon is carbonless
Something without faith is faithless
Something without death is deathless
Something without arm is
2024-07-23 07:55:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 07:59:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([1.2793, 0.8730, 0.2520,  ..., 0.8008, 0.0364, 0.6211], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3867,  0.8760, -0.6914,  ..., -2.8223,  2.4219,  3.2754],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.9745e-02,  4.2953e-03,  1.4236e-02,  ...,  2.2278e-03,
         -3.7842e-02,  3.2654e-03],
        [-8.0566e-03, -1.8112e-02, -1.3290e-02,  ...,  2.1858e-03,
          5.0430e-03, -2.4368e-02],
        [ 1.2619e-02, -7.4387e-05, -4.6661e-02,  ...,  1.6937e-02,
          5.4207e-03,  2.4796e-03],
        ...,
        [-1.5106e-03,  4.6806e-03,  1.1520e-02,  ..., -1.3412e-02,
         -1.2756e-02, -7.6599e-03],
        [ 3.0766e-03, -2.3422e-03,  1.9653e-02,  ..., -5.9052e-03,
         -1.5472e-02, -2.4986e-04],
        [-5.3482e-03,  2.8496e-03, -8.7357e-03,  ..., -3.1662e-04,
         -7.9422e-03, -5.2673e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1152,  1.0391, -0.8496,  ..., -2.2402,  2.7793,  4.0156]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 07:59:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without expression is expressionless
Something without art is artless
Something without leg is legless
Something without heir is heirless
Something without carbon is carbonless
Something without faith is faithless
Something without death is deathless
Something without arm is
2024-07-23 07:59:58 root INFO     [order_1_approx] starting weight calculation for Something without expression is expressionless
Something without faith is faithless
Something without death is deathless
Something without arm is armless
Something without art is artless
Something without leg is legless
Something without carbon is carbonless
Something without heir is
2024-07-23 07:59:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 08:03:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.1921, 0.5449, 1.0137,  ..., 1.2139, 0.3030, 1.4355], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.6250, -2.4102, -3.6152,  ..., -1.6992, -1.9619,  3.3457],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0214,  0.0021, -0.0096,  ...,  0.0093, -0.0137, -0.0104],
        [-0.0002, -0.0134, -0.0321,  ...,  0.0105, -0.0022, -0.0120],
        [ 0.0151,  0.0123, -0.0276,  ..., -0.0010,  0.0023,  0.0155],
        ...,
        [ 0.0031, -0.0120, -0.0187,  ..., -0.0369, -0.0121, -0.0152],
        [ 0.0126,  0.0063, -0.0015,  ...,  0.0167, -0.0459,  0.0080],
        [-0.0104,  0.0320, -0.0124,  ..., -0.0145,  0.0099, -0.0339]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.9805, -1.6133, -3.8301,  ..., -1.3027, -1.0605,  4.2812]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 08:03:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without expression is expressionless
Something without faith is faithless
Something without death is deathless
Something without arm is armless
Something without art is artless
Something without leg is legless
Something without carbon is carbonless
Something without heir is
2024-07-23 08:03:57 root INFO     [order_1_approx] starting weight calculation for Something without art is artless
Something without expression is expressionless
Something without heir is heirless
Something without faith is faithless
Something without arm is armless
Something without leg is legless
Something without carbon is carbonless
Something without death is
2024-07-23 08:03:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 08:07:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5039, -0.2119, -0.0083,  ..., -0.4431,  0.8174, -0.0829],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.5039, -2.7891,  0.6758,  ..., -4.3242,  0.6162,  0.4258],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0230, -0.0032,  0.0188,  ..., -0.0150, -0.0063, -0.0061],
        [-0.0118, -0.0232,  0.0050,  ..., -0.0142,  0.0091, -0.0089],
        [-0.0051,  0.0095, -0.0339,  ..., -0.0119,  0.0041, -0.0002],
        ...,
        [-0.0013,  0.0021, -0.0117,  ..., -0.0173, -0.0062, -0.0020],
        [-0.0019, -0.0051,  0.0032,  ...,  0.0125, -0.0222,  0.0145],
        [-0.0030,  0.0022,  0.0062,  ..., -0.0127, -0.0023, -0.0299]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.4922, -2.4883,  1.1172,  ..., -3.7441,  1.0469,  0.1125]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 08:07:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without art is artless
Something without expression is expressionless
Something without heir is heirless
Something without faith is faithless
Something without arm is armless
Something without leg is legless
Something without carbon is carbonless
Something without death is
2024-07-23 08:07:57 root INFO     [order_1_approx] starting weight calculation for Something without faith is faithless
Something without death is deathless
Something without carbon is carbonless
Something without heir is heirless
Something without leg is legless
Something without arm is armless
Something without expression is expressionless
Something without art is
2024-07-23 08:07:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 08:11:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4961, -0.1309,  0.2366,  ..., -0.1421,  0.5967,  0.8076],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5156,  1.6758, -1.4082,  ..., -0.9438,  1.6025,  0.5107],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0212, -0.0117, -0.0023,  ...,  0.0066, -0.0327, -0.0056],
        [-0.0122, -0.0183,  0.0011,  ..., -0.0068, -0.0165, -0.0131],
        [ 0.0009,  0.0054, -0.0370,  ..., -0.0083,  0.0115, -0.0033],
        ...,
        [ 0.0101,  0.0163, -0.0122,  ..., -0.0295,  0.0138, -0.0044],
        [-0.0135, -0.0180,  0.0132,  ...,  0.0017, -0.0315,  0.0138],
        [-0.0128,  0.0182,  0.0042,  ..., -0.0021,  0.0068, -0.0306]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9053,  1.5547, -0.6533,  ..., -0.6206,  1.6484,  0.1885]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 08:11:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without faith is faithless
Something without death is deathless
Something without carbon is carbonless
Something without heir is heirless
Something without leg is legless
Something without arm is armless
Something without expression is expressionless
Something without art is
2024-07-23 08:11:56 root INFO     total operator prediction time: 1916.550844669342 seconds
2024-07-23 08:11:56 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+ment_irreg
2024-07-23 08:11:56 root INFO     building operator verb+ment_irreg
2024-07-23 08:11:56 root INFO     [order_1_approx] starting weight calculation for To harass results in a harassment
To amuse results in a amusement
To enlighten results in a enlightenment
To agree results in a agreement
To assign results in a assignment
To impair results in a impairment
To amend results in a amendment
To enjoy results in a
2024-07-23 08:11:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 08:15:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2490,  0.3271,  1.1377,  ..., -0.2568,  0.0505,  0.6460],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3091,  3.0664,  2.5410,  ..., -2.0625,  1.7275,  5.7969],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0037,  0.0042,  0.0016,  ..., -0.0038,  0.0086, -0.0233],
        [-0.0115, -0.0345,  0.0146,  ..., -0.0016, -0.0071, -0.0070],
        [ 0.0066,  0.0064, -0.0133,  ...,  0.0006,  0.0164, -0.0062],
        ...,
        [-0.0156, -0.0045, -0.0064,  ..., -0.0195,  0.0106, -0.0036],
        [ 0.0167,  0.0224, -0.0202,  ...,  0.0167,  0.0011,  0.0061],
        [ 0.0021,  0.0335, -0.0028,  ..., -0.0036,  0.0109, -0.0114]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4438,  3.2012,  2.4062,  ..., -2.6406,  1.4707,  6.3984]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 08:15:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To harass results in a harassment
To amuse results in a amusement
To enlighten results in a enlightenment
To agree results in a agreement
To assign results in a assignment
To impair results in a impairment
To amend results in a amendment
To enjoy results in a
2024-07-23 08:15:56 root INFO     [order_1_approx] starting weight calculation for To enlighten results in a enlightenment
To amend results in a amendment
To agree results in a agreement
To harass results in a harassment
To assign results in a assignment
To amuse results in a amusement
To enjoy results in a enjoyment
To impair results in a
2024-07-23 08:15:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 08:19:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4521, -0.4419,  0.3103,  ...,  0.5518, -0.5195, -0.3530],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6719,  0.0903, -2.5605,  ...,  3.9551, -1.4453,  3.3125],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.2234e-04, -1.3428e-02,  5.2681e-03,  ..., -3.5324e-03,
         -5.9357e-03,  4.0970e-03],
        [ 3.3569e-04, -1.8036e-02,  1.8730e-03,  ..., -1.3229e-02,
         -1.0956e-02,  3.5706e-03],
        [-1.4221e-02, -6.6948e-03, -5.7716e-03,  ...,  6.2561e-03,
         -1.0099e-03, -2.0935e-02],
        ...,
        [-1.8631e-02, -8.0032e-03, -1.4877e-04,  ..., -9.1553e-05,
          4.2496e-03, -5.6801e-03],
        [-3.9978e-03, -1.0422e-02,  7.2632e-03,  ..., -8.0261e-03,
         -7.2098e-03,  9.5749e-03],
        [-6.0272e-04,  2.2797e-02, -3.2063e-03,  ..., -6.7596e-03,
          1.4542e-02, -2.4796e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3633,  0.9722, -2.9082,  ...,  4.0664, -1.8555,  3.2051]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 08:19:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To enlighten results in a enlightenment
To amend results in a amendment
To agree results in a agreement
To harass results in a harassment
To assign results in a assignment
To amuse results in a amusement
To enjoy results in a enjoyment
To impair results in a
2024-07-23 08:19:57 root INFO     [order_1_approx] starting weight calculation for To amuse results in a amusement
To harass results in a harassment
To impair results in a impairment
To enjoy results in a enjoyment
To assign results in a assignment
To agree results in a agreement
To enlighten results in a enlightenment
To amend results in a
2024-07-23 08:19:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 08:23:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1501, -0.9878,  0.0240,  ...,  0.8506,  0.5762, -0.5811],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9707,  1.8652, -1.1143,  ...,  2.2812, -2.4492,  7.0430],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0011, -0.0131, -0.0012,  ...,  0.0132, -0.0063, -0.0073],
        [ 0.0088, -0.0250, -0.0128,  ..., -0.0073, -0.0139, -0.0043],
        [ 0.0040, -0.0096, -0.0161,  ..., -0.0008,  0.0195,  0.0165],
        ...,
        [-0.0220, -0.0218,  0.0023,  ..., -0.0212, -0.0167,  0.0103],
        [-0.0088,  0.0097,  0.0094,  ..., -0.0021, -0.0211,  0.0087],
        [-0.0050,  0.0184, -0.0029,  ...,  0.0068,  0.0019, -0.0067]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7627,  2.5820, -1.8027,  ...,  3.0117, -2.6914,  7.0117]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 08:23:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To amuse results in a amusement
To harass results in a harassment
To impair results in a impairment
To enjoy results in a enjoyment
To assign results in a assignment
To agree results in a agreement
To enlighten results in a enlightenment
To amend results in a
2024-07-23 08:23:57 root INFO     [order_1_approx] starting weight calculation for To agree results in a agreement
To enlighten results in a enlightenment
To impair results in a impairment
To amend results in a amendment
To assign results in a assignment
To amuse results in a amusement
To enjoy results in a enjoyment
To harass results in a
2024-07-23 08:23:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 08:27:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6763, -0.0707,  0.4895,  ...,  0.5449, -0.5967,  1.0312],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0576,  2.0820, -0.4629,  ...,  0.6211,  1.5039,  8.3359],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0013, -0.0067,  0.0023,  ...,  0.0063, -0.0002, -0.0057],
        [-0.0237, -0.0386,  0.0017,  ..., -0.0242, -0.0105, -0.0100],
        [-0.0154, -0.0096, -0.0108,  ...,  0.0006, -0.0002, -0.0024],
        ...,
        [-0.0275, -0.0164, -0.0103,  ..., -0.0068,  0.0038,  0.0038],
        [-0.0078,  0.0049,  0.0087,  ..., -0.0064, -0.0190,  0.0035],
        [-0.0047,  0.0285, -0.0091,  ..., -0.0085,  0.0088, -0.0261]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3555,  2.5176, -1.1348,  ...,  0.3206,  1.8301,  9.0391]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 08:27:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To agree results in a agreement
To enlighten results in a enlightenment
To impair results in a impairment
To amend results in a amendment
To assign results in a assignment
To amuse results in a amusement
To enjoy results in a enjoyment
To harass results in a
2024-07-23 08:27:57 root INFO     [order_1_approx] starting weight calculation for To impair results in a impairment
To enlighten results in a enlightenment
To harass results in a harassment
To agree results in a agreement
To amend results in a amendment
To assign results in a assignment
To enjoy results in a enjoyment
To amuse results in a
2024-07-23 08:27:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 08:31:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([1.3750, 0.6792, 0.6646,  ..., 0.3499, 0.4417, 1.3164], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9170,  2.9043, -1.5195,  ...,  1.0654, -1.0938,  8.7969],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0048, -0.0189,  0.0103,  ...,  0.0075, -0.0088, -0.0298],
        [ 0.0091, -0.0073, -0.0021,  ..., -0.0164, -0.0080, -0.0126],
        [-0.0045,  0.0012, -0.0440,  ..., -0.0014,  0.0182, -0.0032],
        ...,
        [-0.0286, -0.0306, -0.0132,  ..., -0.0201, -0.0117, -0.0065],
        [ 0.0089,  0.0147, -0.0062,  ...,  0.0163, -0.0133, -0.0055],
        [-0.0225,  0.0239, -0.0120,  ...,  0.0013,  0.0064, -0.0375]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0049,  2.8652, -2.6289,  ...,  0.8647, -0.5796,  9.3203]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 08:31:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To impair results in a impairment
To enlighten results in a enlightenment
To harass results in a harassment
To agree results in a agreement
To amend results in a amendment
To assign results in a assignment
To enjoy results in a enjoyment
To amuse results in a
2024-07-23 08:31:57 root INFO     [order_1_approx] starting weight calculation for To assign results in a assignment
To amuse results in a amusement
To agree results in a agreement
To enjoy results in a enjoyment
To harass results in a harassment
To amend results in a amendment
To impair results in a impairment
To enlighten results in a
2024-07-23 08:31:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 08:35:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1816,  0.4238,  0.7119,  ...,  0.4556,  0.3530, -0.1809],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.5352,  4.1797, -3.2773,  ..., -0.5171, -3.1250,  6.5547],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0122,  0.0003, -0.0025,  ..., -0.0037,  0.0010, -0.0170],
        [-0.0178, -0.0158,  0.0005,  ..., -0.0154, -0.0019, -0.0074],
        [ 0.0086,  0.0241, -0.0114,  ...,  0.0138,  0.0150,  0.0033],
        ...,
        [-0.0056, -0.0016, -0.0047,  ..., -0.0042,  0.0052,  0.0102],
        [ 0.0103,  0.0046,  0.0068,  ...,  0.0105, -0.0178, -0.0007],
        [ 0.0003,  0.0084, -0.0012,  ..., -0.0042,  0.0102,  0.0009]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5205,  5.1367, -4.1484,  ..., -0.9731, -3.8105,  6.6602]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 08:35:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To assign results in a assignment
To amuse results in a amusement
To agree results in a agreement
To enjoy results in a enjoyment
To harass results in a harassment
To amend results in a amendment
To impair results in a impairment
To enlighten results in a
2024-07-23 08:35:56 root INFO     [order_1_approx] starting weight calculation for To impair results in a impairment
To enjoy results in a enjoyment
To agree results in a agreement
To harass results in a harassment
To enlighten results in a enlightenment
To amend results in a amendment
To amuse results in a amusement
To assign results in a
2024-07-23 08:35:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 08:39:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6953, -0.0566,  0.6660,  ...,  0.4775, -0.4229,  0.1716],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.4258,  2.4629, -1.0918,  ...,  1.8096, -1.2832,  4.3594],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0017,  0.0088,  0.0084,  ...,  0.0031, -0.0088,  0.0013],
        [-0.0012, -0.0363, -0.0006,  ..., -0.0023, -0.0039, -0.0108],
        [-0.0068,  0.0063, -0.0193,  ...,  0.0187, -0.0048, -0.0153],
        ...,
        [-0.0138, -0.0154,  0.0064,  ..., -0.0063,  0.0090, -0.0014],
        [-0.0107,  0.0119,  0.0013,  ..., -0.0030, -0.0155, -0.0138],
        [ 0.0194,  0.0086,  0.0028,  ..., -0.0055,  0.0067, -0.0314]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5420,  2.6133, -1.2305,  ...,  2.3242, -1.1641,  4.1289]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 08:39:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To impair results in a impairment
To enjoy results in a enjoyment
To agree results in a agreement
To harass results in a harassment
To enlighten results in a enlightenment
To amend results in a amendment
To amuse results in a amusement
To assign results in a
2024-07-23 08:39:55 root INFO     [order_1_approx] starting weight calculation for To amend results in a amendment
To harass results in a harassment
To enjoy results in a enjoyment
To assign results in a assignment
To impair results in a impairment
To enlighten results in a enlightenment
To amuse results in a amusement
To agree results in a
2024-07-23 08:39:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 08:43:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1304,  0.2720,  0.4106,  ..., -0.0913, -0.6572,  0.1089],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1853,  0.9346, -0.4263,  ..., -0.6040, -2.7402,  2.1133],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0029,  0.0077,  0.0097,  ...,  0.0109, -0.0008, -0.0187],
        [-0.0030, -0.0245,  0.0069,  ...,  0.0001, -0.0032, -0.0014],
        [-0.0013, -0.0050, -0.0051,  ...,  0.0042, -0.0030, -0.0060],
        ...,
        [-0.0040, -0.0088,  0.0032,  ..., -0.0039, -0.0095,  0.0062],
        [ 0.0032, -0.0007, -0.0082,  ...,  0.0004, -0.0187, -0.0016],
        [-0.0029,  0.0212,  0.0124,  ..., -0.0179,  0.0115, -0.0097]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0994,  1.0908, -0.9121,  ..., -0.3032, -2.7090,  2.7090]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 08:43:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To amend results in a amendment
To harass results in a harassment
To enjoy results in a enjoyment
To assign results in a assignment
To impair results in a impairment
To enlighten results in a enlightenment
To amuse results in a amusement
To agree results in a
2024-07-23 08:43:55 root INFO     total operator prediction time: 1919.352549314499 seconds
2024-07-23 08:43:55 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on name - nationality
2024-07-23 08:43:55 root INFO     building operator name - nationality
2024-07-23 08:43:56 root INFO     [order_1_approx] starting weight calculation for lenin was soviet
plato was greek
maxwell was scottish
locke was english
mozart was german
newton was english
rousseau was french
euclid was
2024-07-23 08:43:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 08:47:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4006,  0.4458,  0.1996,  ..., -0.0708,  1.0020,  0.4141],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8232, -0.5254, -2.9648,  ..., -0.3181, -4.4297, -2.7812],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0128, -0.0035, -0.0010,  ..., -0.0114,  0.0060, -0.0127],
        [-0.0042,  0.0199,  0.0066,  ...,  0.0057,  0.0024,  0.0002],
        [-0.0054, -0.0042,  0.0068,  ..., -0.0031,  0.0083,  0.0038],
        ...,
        [-0.0221, -0.0041,  0.0095,  ...,  0.0253, -0.0077,  0.0107],
        [-0.0025,  0.0083, -0.0095,  ..., -0.0245,  0.0276,  0.0051],
        [-0.0017,  0.0041,  0.0089,  ..., -0.0085,  0.0116,  0.0114]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0723, -0.7153, -3.1934,  ..., -0.4741, -4.7188, -3.8398]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 08:47:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for lenin was soviet
plato was greek
maxwell was scottish
locke was english
mozart was german
newton was english
rousseau was french
euclid was
2024-07-23 08:47:56 root INFO     [order_1_approx] starting weight calculation for maxwell was scottish
rousseau was french
newton was english
mozart was german
plato was greek
euclid was greek
lenin was soviet
locke was
2024-07-23 08:47:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 08:51:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2413, -0.4170, -0.0659,  ...,  0.6699, -0.9526,  1.3613],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.8770, -2.1016, -7.0547,  ..., -5.4258, -1.7021, -3.7754],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0053, -0.0046,  0.0072,  ..., -0.0137, -0.0045,  0.0041],
        [ 0.0033,  0.0101,  0.0040,  ...,  0.0162,  0.0052, -0.0109],
        [ 0.0195, -0.0018, -0.0096,  ..., -0.0022,  0.0017, -0.0085],
        ...,
        [-0.0066, -0.0166, -0.0022,  ...,  0.0202, -0.0056,  0.0002],
        [-0.0075, -0.0070, -0.0027,  ...,  0.0060,  0.0007,  0.0137],
        [ 0.0053, -0.0163, -0.0071,  ..., -0.0059, -0.0033, -0.0036]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.3379, -1.6064, -6.7539,  ..., -6.1328, -1.7617, -4.4062]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 08:51:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for maxwell was scottish
rousseau was french
newton was english
mozart was german
plato was greek
euclid was greek
lenin was soviet
locke was
2024-07-23 08:51:55 root INFO     [order_1_approx] starting weight calculation for newton was english
maxwell was scottish
mozart was german
euclid was greek
lenin was soviet
locke was english
plato was greek
rousseau was
2024-07-23 08:51:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 08:55:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4409, -0.2939, -0.9658,  ...,  0.4314, -0.0104,  0.9922],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.3477, -0.4780, -7.4062,  ..., -5.5195, -2.5566, -0.8838],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0111, -0.0101,  0.0016,  ..., -0.0050,  0.0207,  0.0251],
        [ 0.0214,  0.0081,  0.0031,  ...,  0.0116,  0.0078, -0.0085],
        [-0.0107, -0.0025, -0.0042,  ..., -0.0014, -0.0043, -0.0211],
        ...,
        [-0.0115,  0.0175, -0.0022,  ...,  0.0191, -0.0072,  0.0039],
        [ 0.0044, -0.0099, -0.0028,  ..., -0.0023,  0.0053,  0.0056],
        [-0.0116,  0.0018,  0.0019,  ..., -0.0242, -0.0007,  0.0204]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.2344, -0.5234, -7.9258,  ..., -5.9648, -2.5449, -1.1201]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 08:55:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for newton was english
maxwell was scottish
mozart was german
euclid was greek
lenin was soviet
locke was english
plato was greek
rousseau was
2024-07-23 08:55:54 root INFO     [order_1_approx] starting weight calculation for locke was english
euclid was greek
plato was greek
maxwell was scottish
lenin was soviet
newton was english
rousseau was french
mozart was
2024-07-23 08:55:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 08:59:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2573, -0.1227,  0.0516,  ...,  0.1584,  0.5356,  0.3887],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.0781, -0.2031, -5.5156,  ..., -0.4448, -4.7695, -5.4609],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0053, -0.0101, -0.0038,  ...,  0.0007,  0.0017, -0.0012],
        [-0.0008,  0.0146,  0.0008,  ...,  0.0119,  0.0085, -0.0040],
        [ 0.0039, -0.0132,  0.0010,  ..., -0.0159, -0.0026,  0.0105],
        ...,
        [-0.0164,  0.0109,  0.0096,  ...,  0.0084, -0.0019,  0.0250],
        [ 0.0106, -0.0130,  0.0041,  ...,  0.0055, -0.0131,  0.0093],
        [-0.0043, -0.0132, -0.0043,  ..., -0.0035, -0.0105, -0.0111]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.1445,  0.0510, -5.6133,  ...,  0.0967, -5.5000, -7.7188]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 08:59:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for locke was english
euclid was greek
plato was greek
maxwell was scottish
lenin was soviet
newton was english
rousseau was french
mozart was
2024-07-23 08:59:55 root INFO     [order_1_approx] starting weight calculation for lenin was soviet
maxwell was scottish
mozart was german
euclid was greek
rousseau was french
newton was english
locke was english
plato was
2024-07-23 08:59:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 09:03:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2949,  1.2578,  1.3867,  ...,  0.4480,  0.5659,  0.6528],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6309, -0.2375, -3.3691,  ..., -0.4146, -5.4688, -3.4492],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0038, -0.0071,  0.0099,  ..., -0.0331,  0.0203, -0.0033],
        [ 0.0041,  0.0008,  0.0124,  ...,  0.0016, -0.0172, -0.0116],
        [-0.0209, -0.0060,  0.0010,  ..., -0.0107,  0.0042,  0.0192],
        ...,
        [-0.0311,  0.0197,  0.0091,  ...,  0.0137, -0.0038,  0.0053],
        [ 0.0117,  0.0101, -0.0032,  ..., -0.0122, -0.0085,  0.0163],
        [-0.0238,  0.0047,  0.0126,  ..., -0.0266,  0.0214,  0.0064]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.2188, -0.6328, -4.1484,  ..., -0.5151, -5.3203, -4.5156]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 09:03:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for lenin was soviet
maxwell was scottish
mozart was german
euclid was greek
rousseau was french
newton was english
locke was english
plato was
2024-07-23 09:03:56 root INFO     [order_1_approx] starting weight calculation for newton was english
rousseau was french
lenin was soviet
euclid was greek
plato was greek
mozart was german
locke was english
maxwell was
2024-07-23 09:03:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 09:07:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3638,  0.2983,  0.2229,  ..., -0.6743, -0.5645, -0.2186],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0098, -0.1924, -4.5781,  ..., -1.8740, -1.0654, -1.2969],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0252, -0.0154,  0.0098,  ..., -0.0033,  0.0111, -0.0021],
        [ 0.0013,  0.0167,  0.0041,  ...,  0.0039, -0.0036,  0.0153],
        [ 0.0063, -0.0017,  0.0177,  ...,  0.0005, -0.0034, -0.0104],
        ...,
        [-0.0145, -0.0069,  0.0097,  ...,  0.0240,  0.0116,  0.0119],
        [-0.0159, -0.0138,  0.0200,  ..., -0.0194,  0.0459,  0.0180],
        [ 0.0078, -0.0158,  0.0135,  ..., -0.0302,  0.0111,  0.0341]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6514, -0.2778, -5.4102,  ..., -2.0176, -0.7871, -1.1514]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 09:07:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for newton was english
rousseau was french
lenin was soviet
euclid was greek
plato was greek
mozart was german
locke was english
maxwell was
2024-07-23 09:07:54 root INFO     [order_1_approx] starting weight calculation for euclid was greek
maxwell was scottish
mozart was german
rousseau was french
plato was greek
newton was english
locke was english
lenin was
2024-07-23 09:07:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 09:11:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6982,  1.6748, -0.5239,  ..., -1.4258, -0.7998,  0.3735],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.9180, -1.8623, -4.4180,  ..., -8.6719, -4.6641, -0.6777],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0003, -0.0018,  0.0136,  ..., -0.0164,  0.0226, -0.0012],
        [ 0.0168, -0.0007,  0.0156,  ...,  0.0229, -0.0030, -0.0048],
        [ 0.0187,  0.0009, -0.0028,  ...,  0.0076, -0.0267, -0.0122],
        ...,
        [-0.0077, -0.0105, -0.0053,  ...,  0.0136, -0.0247,  0.0049],
        [ 0.0055, -0.0068,  0.0017,  ..., -0.0148,  0.0016,  0.0051],
        [-0.0139,  0.0213,  0.0116,  ..., -0.0182,  0.0255,  0.0051]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6445, -2.0430, -5.0742,  ..., -8.8594, -3.9375, -0.9624]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 09:11:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for euclid was greek
maxwell was scottish
mozart was german
rousseau was french
plato was greek
newton was english
locke was english
lenin was
2024-07-23 09:11:52 root INFO     [order_1_approx] starting weight calculation for lenin was soviet
locke was english
maxwell was scottish
euclid was greek
plato was greek
mozart was german
rousseau was french
newton was
2024-07-23 09:11:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 09:15:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4885,  0.7007,  0.2563,  ..., -0.2861, -1.0371,  0.9614],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1084, -1.0391, -5.0078,  ..., -1.5811, -1.4775, -1.6973],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0123, -0.0142, -0.0048,  ..., -0.0071, -0.0091, -0.0160],
        [ 0.0134,  0.0101, -0.0005,  ...,  0.0023, -0.0147, -0.0119],
        [ 0.0083, -0.0032, -0.0094,  ..., -0.0085,  0.0004, -0.0059],
        ...,
        [-0.0100,  0.0089,  0.0056,  ...,  0.0123,  0.0235,  0.0154],
        [ 0.0005,  0.0029,  0.0077,  ..., -0.0073,  0.0088,  0.0032],
        [-0.0046,  0.0137,  0.0031,  ..., -0.0171, -0.0041, -0.0067]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5537, -1.5371, -5.8945,  ..., -1.2148,  0.3232, -2.3359]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 09:15:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for lenin was soviet
locke was english
maxwell was scottish
euclid was greek
plato was greek
mozart was german
rousseau was french
newton was
2024-07-23 09:15:51 root INFO     total operator prediction time: 1916.0178129673004 seconds
2024-07-23 09:15:51 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on country - language
2024-07-23 09:15:51 root INFO     building operator country - language
2024-07-23 09:15:52 root INFO     [order_1_approx] starting weight calculation for The country of andorra primarily speaks the language of catalan
The country of brazil primarily speaks the language of portuguese
The country of syria primarily speaks the language of arabic
The country of barbados primarily speaks the language of english
The country of ecuador primarily speaks the language of spanish
The country of nicaragua primarily speaks the language of spanish
The country of kosovo primarily speaks the language of albanian
The country of fiji primarily speaks the language of
2024-07-23 09:15:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 09:19:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6562, -0.2957, -1.9561,  ..., -0.8306, -1.3340,  1.1299],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.9238, -5.2969, -6.1562,  ..., -1.0801,  1.6348, -1.6025],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0075, -0.0147, -0.0023,  ..., -0.0111, -0.0007, -0.0059],
        [-0.0047,  0.0042, -0.0023,  ...,  0.0030,  0.0094, -0.0033],
        [ 0.0210,  0.0349, -0.0010,  ...,  0.0145, -0.0047, -0.0014],
        ...,
        [-0.0044, -0.0057, -0.0030,  ..., -0.0006, -0.0053, -0.0033],
        [-0.0096, -0.0078, -0.0025,  ...,  0.0009, -0.0032,  0.0022],
        [ 0.0053,  0.0031, -0.0098,  ...,  0.0049,  0.0122,  0.0091]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.3672, -4.5898, -6.5820,  ..., -1.3457,  1.7441, -2.1035]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 09:19:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of andorra primarily speaks the language of catalan
The country of brazil primarily speaks the language of portuguese
The country of syria primarily speaks the language of arabic
The country of barbados primarily speaks the language of english
The country of ecuador primarily speaks the language of spanish
The country of nicaragua primarily speaks the language of spanish
The country of kosovo primarily speaks the language of albanian
The country of fiji primarily speaks the language of
2024-07-23 09:19:50 root INFO     [order_1_approx] starting weight calculation for The country of fiji primarily speaks the language of english
The country of andorra primarily speaks the language of catalan
The country of barbados primarily speaks the language of english
The country of nicaragua primarily speaks the language of spanish
The country of brazil primarily speaks the language of portuguese
The country of ecuador primarily speaks the language of spanish
The country of kosovo primarily speaks the language of albanian
The country of syria primarily speaks the language of
2024-07-23 09:19:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 09:23:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1465, -0.5186,  0.0851,  ..., -0.1162, -1.1963, -0.5273],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.0000, -4.9766, -1.4336,  ..., -8.4219, -0.2119, -0.3608],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0101, -0.0299, -0.0195,  ..., -0.0100,  0.0025, -0.0202],
        [ 0.0023, -0.0032, -0.0087,  ..., -0.0032,  0.0037, -0.0015],
        [-0.0037,  0.0132, -0.0102,  ...,  0.0103, -0.0099, -0.0038],
        ...,
        [ 0.0048, -0.0012, -0.0027,  ...,  0.0068, -0.0230, -0.0097],
        [-0.0162,  0.0039, -0.0093,  ...,  0.0092, -0.0110,  0.0088],
        [ 0.0115, -0.0179,  0.0081,  ..., -0.0045,  0.0276, -0.0009]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.9922, -4.4844, -2.4844,  ..., -8.7344, -0.7026, -1.0820]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 09:23:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of fiji primarily speaks the language of english
The country of andorra primarily speaks the language of catalan
The country of barbados primarily speaks the language of english
The country of nicaragua primarily speaks the language of spanish
The country of brazil primarily speaks the language of portuguese
The country of ecuador primarily speaks the language of spanish
The country of kosovo primarily speaks the language of albanian
The country of syria primarily speaks the language of
2024-07-23 09:23:49 root INFO     [order_1_approx] starting weight calculation for The country of syria primarily speaks the language of arabic
The country of ecuador primarily speaks the language of spanish
The country of nicaragua primarily speaks the language of spanish
The country of fiji primarily speaks the language of english
The country of andorra primarily speaks the language of catalan
The country of kosovo primarily speaks the language of albanian
The country of barbados primarily speaks the language of english
The country of brazil primarily speaks the language of
2024-07-23 09:23:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 09:27:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.3096,  0.0087, -0.1029,  ..., -0.7339, -0.1018, -1.0566],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.2500, -2.5195, -5.3398,  ..., -2.7480,  0.3899, -2.7520],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0017, -0.0082, -0.0047,  ..., -0.0047, -0.0096,  0.0016],
        [-0.0014,  0.0075,  0.0024,  ...,  0.0081,  0.0228,  0.0031],
        [-0.0073,  0.0149, -0.0256,  ...,  0.0041, -0.0093, -0.0085],
        ...,
        [-0.0053, -0.0025, -0.0045,  ...,  0.0078, -0.0105, -0.0145],
        [ 0.0112,  0.0041, -0.0076,  ..., -0.0013, -0.0337, -0.0073],
        [-0.0104,  0.0039, -0.0018,  ...,  0.0051, -0.0039, -0.0074]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.5195, -2.2891, -6.2891,  ..., -2.9492,  0.3564, -3.1992]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 09:27:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of syria primarily speaks the language of arabic
The country of ecuador primarily speaks the language of spanish
The country of nicaragua primarily speaks the language of spanish
The country of fiji primarily speaks the language of english
The country of andorra primarily speaks the language of catalan
The country of kosovo primarily speaks the language of albanian
The country of barbados primarily speaks the language of english
The country of brazil primarily speaks the language of
2024-07-23 09:27:45 root INFO     [order_1_approx] starting weight calculation for The country of syria primarily speaks the language of arabic
The country of barbados primarily speaks the language of english
The country of fiji primarily speaks the language of english
The country of brazil primarily speaks the language of portuguese
The country of andorra primarily speaks the language of catalan
The country of nicaragua primarily speaks the language of spanish
The country of kosovo primarily speaks the language of albanian
The country of ecuador primarily speaks the language of
2024-07-23 09:27:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 09:31:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7007, -0.4385, -1.1885,  ..., -0.0032,  0.4841,  0.5918],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0322, -2.9180, -4.4531,  ..., -0.0088,  3.8066, -3.3340],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-3.2806e-03, -2.2354e-02,  4.3640e-03,  ..., -2.3403e-03,
          5.8632e-03, -3.5572e-03],
        [-2.1858e-03,  6.9389e-03, -5.2986e-03,  ...,  3.4180e-03,
          1.2489e-02,  2.6169e-03],
        [-7.5684e-03,  1.4305e-02, -2.2797e-02,  ...,  1.2436e-02,
         -2.9602e-03, -1.0017e-02],
        ...,
        [-1.3046e-03,  1.9989e-03, -1.5326e-03,  ...,  8.9188e-03,
          3.2959e-03, -8.2550e-03],
        [-1.3283e-02, -1.7242e-02, -5.5656e-03,  ..., -2.5253e-03,
         -1.5656e-02,  5.6076e-04],
        [ 1.2291e-02, -4.1351e-03,  5.8174e-05,  ..., -1.9989e-03,
          1.3176e-02, -2.2324e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0493, -1.9961, -5.4492,  ..., -0.7339,  3.3633, -4.4336]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 09:31:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of syria primarily speaks the language of arabic
The country of barbados primarily speaks the language of english
The country of fiji primarily speaks the language of english
The country of brazil primarily speaks the language of portuguese
The country of andorra primarily speaks the language of catalan
The country of nicaragua primarily speaks the language of spanish
The country of kosovo primarily speaks the language of albanian
The country of ecuador primarily speaks the language of
2024-07-23 09:31:43 root INFO     [order_1_approx] starting weight calculation for The country of nicaragua primarily speaks the language of spanish
The country of syria primarily speaks the language of arabic
The country of andorra primarily speaks the language of catalan
The country of brazil primarily speaks the language of portuguese
The country of barbados primarily speaks the language of english
The country of ecuador primarily speaks the language of spanish
The country of fiji primarily speaks the language of english
The country of kosovo primarily speaks the language of
2024-07-23 09:31:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 09:35:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.3345, 1.4180, 0.2842,  ..., 0.6924, 0.3071, 1.0654], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9214, -5.4102, -1.7939,  ..., -0.7979, -2.6875, -6.3516],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 8.5678e-03, -2.8992e-03,  4.3259e-03,  ...,  8.8043e-03,
         -5.7297e-03, -8.1100e-03],
        [ 3.9825e-03,  3.6621e-03,  2.5654e-03,  ..., -4.5776e-03,
          5.1193e-03,  3.3951e-03],
        [ 6.0654e-04,  2.0309e-02, -1.1497e-02,  ..., -2.8419e-03,
         -2.9266e-02, -2.3079e-03],
        ...,
        [ 1.8520e-03, -7.8049e-03, -9.2268e-04,  ...,  8.8654e-03,
         -1.7410e-02, -3.3054e-03],
        [ 4.6883e-03, -1.5030e-03, -2.9125e-03,  ...,  7.8812e-03,
         -9.0942e-03,  5.1346e-03],
        [ 9.7656e-03, -6.0272e-04,  4.3869e-05,  ..., -5.1117e-03,
          3.3836e-03, -4.4022e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5771, -4.9766, -2.2246,  ..., -0.9609, -3.1562, -6.6445]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 09:35:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of nicaragua primarily speaks the language of spanish
The country of syria primarily speaks the language of arabic
The country of andorra primarily speaks the language of catalan
The country of brazil primarily speaks the language of portuguese
The country of barbados primarily speaks the language of english
The country of ecuador primarily speaks the language of spanish
The country of fiji primarily speaks the language of english
The country of kosovo primarily speaks the language of
2024-07-23 09:35:39 root INFO     [order_1_approx] starting weight calculation for The country of fiji primarily speaks the language of english
The country of kosovo primarily speaks the language of albanian
The country of brazil primarily speaks the language of portuguese
The country of andorra primarily speaks the language of catalan
The country of ecuador primarily speaks the language of spanish
The country of syria primarily speaks the language of arabic
The country of barbados primarily speaks the language of english
The country of nicaragua primarily speaks the language of
2024-07-23 09:35:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 09:39:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6929, -1.1367, -2.0469,  ...,  0.1367,  0.3926,  0.0103],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1406, -0.1309, -4.4453,  ..., -3.2676, -0.4170,  0.7373],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0042, -0.0074, -0.0028,  ..., -0.0035, -0.0035,  0.0003],
        [-0.0030, -0.0045, -0.0063,  ...,  0.0093,  0.0125,  0.0020],
        [ 0.0017,  0.0225, -0.0229,  ..., -0.0111, -0.0121, -0.0107],
        ...,
        [-0.0076, -0.0031, -0.0044,  ..., -0.0024, -0.0085,  0.0042],
        [-0.0074, -0.0092, -0.0008,  ...,  0.0120, -0.0015, -0.0017],
        [ 0.0004,  0.0033, -0.0014,  ...,  0.0081,  0.0054,  0.0006]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9478,  0.2114, -3.9453,  ..., -3.8613, -0.4119,  0.7583]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 09:39:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of fiji primarily speaks the language of english
The country of kosovo primarily speaks the language of albanian
The country of brazil primarily speaks the language of portuguese
The country of andorra primarily speaks the language of catalan
The country of ecuador primarily speaks the language of spanish
The country of syria primarily speaks the language of arabic
The country of barbados primarily speaks the language of english
The country of nicaragua primarily speaks the language of
2024-07-23 09:39:37 root INFO     [order_1_approx] starting weight calculation for The country of nicaragua primarily speaks the language of spanish
The country of andorra primarily speaks the language of catalan
The country of brazil primarily speaks the language of portuguese
The country of kosovo primarily speaks the language of albanian
The country of fiji primarily speaks the language of english
The country of syria primarily speaks the language of arabic
The country of ecuador primarily speaks the language of spanish
The country of barbados primarily speaks the language of
2024-07-23 09:39:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 09:43:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5078, -1.1221, -0.7256,  ...,  1.3096,  0.2290, -0.0767],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3477, -4.0273, -9.2734,  ..., -0.1895,  2.0430, -0.2402],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0064, -0.0073,  0.0071,  ..., -0.0059, -0.0056,  0.0094],
        [-0.0182, -0.0065,  0.0114,  ...,  0.0077,  0.0043, -0.0002],
        [ 0.0166,  0.0093, -0.0264,  ..., -0.0033, -0.0040, -0.0084],
        ...,
        [-0.0115, -0.0115,  0.0069,  ..., -0.0008, -0.0024,  0.0090],
        [-0.0052, -0.0143, -0.0037,  ...,  0.0041, -0.0044, -0.0036],
        [ 0.0046, -0.0077, -0.0161,  ..., -0.0107,  0.0027,  0.0066]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.2500, -4.4023, -9.2891,  ..., -0.3540,  1.7988, -0.5337]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 09:43:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of nicaragua primarily speaks the language of spanish
The country of andorra primarily speaks the language of catalan
The country of brazil primarily speaks the language of portuguese
The country of kosovo primarily speaks the language of albanian
The country of fiji primarily speaks the language of english
The country of syria primarily speaks the language of arabic
The country of ecuador primarily speaks the language of spanish
The country of barbados primarily speaks the language of
2024-07-23 09:43:35 root INFO     [order_1_approx] starting weight calculation for The country of nicaragua primarily speaks the language of spanish
The country of brazil primarily speaks the language of portuguese
The country of barbados primarily speaks the language of english
The country of fiji primarily speaks the language of english
The country of syria primarily speaks the language of arabic
The country of ecuador primarily speaks the language of spanish
The country of kosovo primarily speaks the language of albanian
The country of andorra primarily speaks the language of
2024-07-23 09:43:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 09:47:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6240, -0.3274, -0.2568,  ...,  0.2477,  0.5649,  0.9512],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6748, -4.2852, -4.4531,  ..., -3.2383, -1.3223, -4.0195],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.0676e-02, -1.1688e-02,  6.0387e-03,  ..., -1.0727e-02,
         -5.6152e-03, -2.6894e-04],
        [-4.8714e-03,  8.8501e-03, -5.8975e-03,  ..., -4.9362e-03,
          1.0544e-02, -2.9106e-03],
        [-1.7185e-03,  1.7090e-02, -1.2268e-02,  ..., -8.9569e-03,
         -7.7438e-03, -2.3376e-02],
        ...,
        [-1.6510e-02, -6.5918e-03,  2.3041e-03,  ...,  1.3237e-03,
         -2.3613e-03,  7.4463e-03],
        [-1.3256e-03,  1.8444e-03, -1.4008e-02,  ..., -1.9791e-02,
          6.5346e-03, -7.4768e-03],
        [ 1.4771e-02, -1.9913e-03,  5.9128e-05,  ...,  2.8753e-04,
          1.7487e-02, -3.0975e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.4258, -3.9219, -3.3125,  ..., -3.8594, -0.8027, -3.7305]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 09:47:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of nicaragua primarily speaks the language of spanish
The country of brazil primarily speaks the language of portuguese
The country of barbados primarily speaks the language of english
The country of fiji primarily speaks the language of english
The country of syria primarily speaks the language of arabic
The country of ecuador primarily speaks the language of spanish
The country of kosovo primarily speaks the language of albanian
The country of andorra primarily speaks the language of
2024-07-23 09:47:30 root INFO     total operator prediction time: 1898.4554438591003 seconds
2024-07-23 09:47:30 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on animal - shelter
2024-07-23 09:47:30 root INFO     building operator animal - shelter
2024-07-23 09:47:30 root INFO     [order_1_approx] starting weight calculation for The place monkey lives in is called tree
The place hippopotamus lives in is called river
The place seal lives in is called den
The place rat lives in is called nest
The place raven lives in is called nest
The place cricket lives in is called nest
The place ant lives in is called anthill
The place fish lives in is called
2024-07-23 09:47:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 09:51:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0867,  0.2095,  0.7153,  ...,  0.0789, -0.5967,  0.3281],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1650,  0.2363, -4.1172,  ..., -1.1777,  1.3525, -1.0957],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0074,  0.0013, -0.0122,  ..., -0.0013,  0.0070, -0.0028],
        [-0.0095,  0.0081, -0.0008,  ...,  0.0101, -0.0049,  0.0070],
        [-0.0131,  0.0221,  0.0080,  ..., -0.0070,  0.0189,  0.0049],
        ...,
        [-0.0143, -0.0185, -0.0055,  ...,  0.0109, -0.0192, -0.0072],
        [-0.0002,  0.0053, -0.0064,  ..., -0.0007,  0.0011, -0.0090],
        [-0.0021, -0.0054,  0.0107,  ..., -0.0131,  0.0103,  0.0081]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1877,  0.7686, -3.9531,  ..., -0.5112,  1.5273, -0.4590]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 09:51:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place monkey lives in is called tree
The place hippopotamus lives in is called river
The place seal lives in is called den
The place rat lives in is called nest
The place raven lives in is called nest
The place cricket lives in is called nest
The place ant lives in is called anthill
The place fish lives in is called
2024-07-23 09:51:31 root INFO     [order_1_approx] starting weight calculation for The place seal lives in is called den
The place raven lives in is called nest
The place rat lives in is called nest
The place fish lives in is called sea
The place monkey lives in is called tree
The place cricket lives in is called nest
The place ant lives in is called anthill
The place hippopotamus lives in is called
2024-07-23 09:51:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 09:55:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4829, -0.9307,  0.5249,  ...,  0.3721, -0.6367,  1.8955],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2783, -3.2480, -3.7422,  ..., -2.9414,  1.3398, -0.3623],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0060, -0.0001, -0.0032,  ..., -0.0024, -0.0076, -0.0080],
        [-0.0013, -0.0025,  0.0005,  ...,  0.0065,  0.0042, -0.0022],
        [ 0.0039,  0.0140,  0.0043,  ..., -0.0020,  0.0025,  0.0085],
        ...,
        [-0.0097,  0.0028, -0.0058,  ...,  0.0041, -0.0043,  0.0033],
        [-0.0034,  0.0019,  0.0022,  ..., -0.0011,  0.0043,  0.0046],
        [-0.0053, -0.0059,  0.0071,  ...,  0.0031,  0.0072,  0.0077]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5430, -3.0371, -4.5898,  ..., -2.9531,  1.4395, -0.3479]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 09:55:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place seal lives in is called den
The place raven lives in is called nest
The place rat lives in is called nest
The place fish lives in is called sea
The place monkey lives in is called tree
The place cricket lives in is called nest
The place ant lives in is called anthill
The place hippopotamus lives in is called
2024-07-23 09:55:32 root INFO     [order_1_approx] starting weight calculation for The place hippopotamus lives in is called river
The place cricket lives in is called nest
The place raven lives in is called nest
The place fish lives in is called sea
The place monkey lives in is called tree
The place rat lives in is called nest
The place seal lives in is called den
The place ant lives in is called
2024-07-23 09:55:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 09:59:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6318,  0.1853,  0.4644,  ..., -0.8101, -1.2344,  0.7246],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9292, -3.5703, -0.8164,  ..., -6.5742,  5.6758,  1.5010],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0116, -0.0028,  0.0028,  ...,  0.0184, -0.0076,  0.0016],
        [-0.0077, -0.0048, -0.0019,  ...,  0.0141,  0.0027, -0.0064],
        [ 0.0086,  0.0182, -0.0165,  ...,  0.0022,  0.0122,  0.0095],
        ...,
        [ 0.0033, -0.0040,  0.0004,  ..., -0.0017, -0.0007, -0.0027],
        [-0.0137, -0.0148,  0.0038,  ...,  0.0075,  0.0064,  0.0071],
        [-0.0040, -0.0068, -0.0052,  ..., -0.0113,  0.0030,  0.0089]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7070, -3.7676, -1.1719,  ..., -6.1445,  5.4102,  1.5820]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 09:59:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place hippopotamus lives in is called river
The place cricket lives in is called nest
The place raven lives in is called nest
The place fish lives in is called sea
The place monkey lives in is called tree
The place rat lives in is called nest
The place seal lives in is called den
The place ant lives in is called
2024-07-23 09:59:31 root INFO     [order_1_approx] starting weight calculation for The place cricket lives in is called nest
The place hippopotamus lives in is called river
The place rat lives in is called nest
The place fish lives in is called sea
The place ant lives in is called anthill
The place monkey lives in is called tree
The place seal lives in is called den
The place raven lives in is called
2024-07-23 09:59:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 10:03:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5605, -1.3818, -0.3857,  ...,  0.4346, -0.3430, -0.3062],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.2461, -1.7480, -0.6665,  ..., -2.9219,  0.4548, -2.2773],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0114, -0.0206, -0.0144,  ...,  0.0166,  0.0120, -0.0026],
        [-0.0014,  0.0236,  0.0179,  ...,  0.0163,  0.0064,  0.0035],
        [-0.0004,  0.0124,  0.0040,  ...,  0.0174,  0.0081, -0.0031],
        ...,
        [-0.0121, -0.0085,  0.0133,  ...,  0.0248,  0.0032,  0.0047],
        [-0.0360, -0.0128, -0.0105,  ...,  0.0166, -0.0004,  0.0165],
        [-0.0068, -0.0142,  0.0063,  ..., -0.0044,  0.0050,  0.0109]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.0977, -1.7441, -1.4717,  ..., -2.9238,  1.2139, -2.5938]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 10:03:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place cricket lives in is called nest
The place hippopotamus lives in is called river
The place rat lives in is called nest
The place fish lives in is called sea
The place ant lives in is called anthill
The place monkey lives in is called tree
The place seal lives in is called den
The place raven lives in is called
2024-07-23 10:03:31 root INFO     [order_1_approx] starting weight calculation for The place ant lives in is called anthill
The place rat lives in is called nest
The place hippopotamus lives in is called river
The place monkey lives in is called tree
The place fish lives in is called sea
The place raven lives in is called nest
The place seal lives in is called den
The place cricket lives in is called
2024-07-23 10:03:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 10:07:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3164,  0.1494, -0.3789,  ...,  0.5234, -0.3730,  1.1484],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.8828, -3.7676, -0.6992,  ..., -3.7910,  3.0391, -0.9551],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.6159e-02, -1.2833e-02,  8.3389e-03,  ...,  2.0081e-02,
          5.1003e-03, -3.0937e-03],
        [-9.5673e-03,  1.6022e-03,  5.6267e-03,  ...,  9.4223e-03,
         -6.4163e-03,  3.3302e-03],
        [-8.4076e-03,  1.3077e-02, -4.1199e-03,  ..., -8.1177e-03,
          1.6159e-02,  1.1154e-02],
        ...,
        [-8.9073e-04, -1.5545e-03, -3.5782e-03,  ...,  5.6839e-03,
          3.4523e-03,  3.6354e-03],
        [-1.2115e-02, -3.5324e-03, -3.6469e-03,  ..., -1.1749e-02,
          4.3755e-03,  5.3825e-03],
        [ 5.7220e-05, -8.4076e-03,  1.1757e-02,  ..., -1.2421e-02,
          2.6093e-03,  4.0741e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2793, -3.9375, -1.0684,  ..., -3.8730,  3.4238, -1.0918]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 10:07:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place ant lives in is called anthill
The place rat lives in is called nest
The place hippopotamus lives in is called river
The place monkey lives in is called tree
The place fish lives in is called sea
The place raven lives in is called nest
The place seal lives in is called den
The place cricket lives in is called
2024-07-23 10:07:31 root INFO     [order_1_approx] starting weight calculation for The place cricket lives in is called nest
The place seal lives in is called den
The place fish lives in is called sea
The place hippopotamus lives in is called river
The place ant lives in is called anthill
The place raven lives in is called nest
The place rat lives in is called nest
The place monkey lives in is called
2024-07-23 10:07:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 10:11:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2839, -1.9131, -0.0269,  ...,  0.8057, -0.3970, -0.1766],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.2988, -2.2949, -1.1543,  ..., -2.1797,  0.8521, -0.5898],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0007, -0.0144, -0.0019,  ...,  0.0084,  0.0052,  0.0146],
        [-0.0099,  0.0063,  0.0089,  ...,  0.0129,  0.0089, -0.0029],
        [-0.0010,  0.0243, -0.0217,  ..., -0.0083,  0.0134,  0.0072],
        ...,
        [-0.0105,  0.0011,  0.0018,  ...,  0.0076, -0.0038,  0.0025],
        [-0.0099,  0.0044, -0.0098,  ..., -0.0033, -0.0038,  0.0078],
        [-0.0080, -0.0044, -0.0098,  ..., -0.0036, -0.0033,  0.0012]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7500, -1.9512, -1.6953,  ..., -2.1484,  0.8228, -0.1738]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 10:11:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place cricket lives in is called nest
The place seal lives in is called den
The place fish lives in is called sea
The place hippopotamus lives in is called river
The place ant lives in is called anthill
The place raven lives in is called nest
The place rat lives in is called nest
The place monkey lives in is called
2024-07-23 10:11:32 root INFO     [order_1_approx] starting weight calculation for The place rat lives in is called nest
The place fish lives in is called sea
The place hippopotamus lives in is called river
The place cricket lives in is called nest
The place ant lives in is called anthill
The place monkey lives in is called tree
The place raven lives in is called nest
The place seal lives in is called
2024-07-23 10:11:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 10:15:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5703,  1.1465,  0.2290,  ...,  0.7603, -0.7744,  0.8999],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9800,  0.8149, -3.4746,  ..., -1.9912,  3.4492, -0.3296],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0030, -0.0118, -0.0029,  ...,  0.0140, -0.0058, -0.0040],
        [-0.0108,  0.0138, -0.0007,  ...,  0.0080,  0.0133,  0.0047],
        [ 0.0120,  0.0142, -0.0044,  ...,  0.0117, -0.0042,  0.0004],
        ...,
        [-0.0100,  0.0049, -0.0074,  ...,  0.0163,  0.0069,  0.0122],
        [-0.0053, -0.0043,  0.0085,  ..., -0.0003,  0.0155,  0.0117],
        [ 0.0059, -0.0057,  0.0051,  ..., -0.0103,  0.0161,  0.0164]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6895,  2.2461, -4.7500,  ..., -2.3418,  3.8691, -0.2349]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 10:15:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place rat lives in is called nest
The place fish lives in is called sea
The place hippopotamus lives in is called river
The place cricket lives in is called nest
The place ant lives in is called anthill
The place monkey lives in is called tree
The place raven lives in is called nest
The place seal lives in is called
2024-07-23 10:15:33 root INFO     [order_1_approx] starting weight calculation for The place monkey lives in is called tree
The place fish lives in is called sea
The place cricket lives in is called nest
The place seal lives in is called den
The place raven lives in is called nest
The place ant lives in is called anthill
The place hippopotamus lives in is called river
The place rat lives in is called
2024-07-23 10:15:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 10:19:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2454,  0.1002,  0.4463,  ...,  0.1022, -1.3740,  0.6782],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.0039, -2.1914, -0.0752,  ..., -5.7891, -1.3223, -0.5854],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.7090e-02, -3.5858e-03,  6.9618e-03,  ...,  1.6129e-02,
         -9.1553e-05,  1.2283e-02],
        [-3.5896e-03,  5.2338e-03, -4.4250e-04,  ...,  1.1559e-02,
          7.8278e-03, -1.9970e-03],
        [-4.2877e-03,  1.9363e-02, -7.5760e-03,  ...,  1.2299e-02,
          2.6455e-03,  6.0539e-03],
        ...,
        [ 2.7733e-03, -4.1275e-03, -1.3161e-03,  ...,  4.9133e-03,
         -1.0757e-02, -1.0824e-03],
        [-1.1368e-03,  5.1498e-03,  2.3651e-04,  ...,  2.1286e-03,
         -2.7657e-04,  5.0850e-03],
        [ 8.7433e-03,  4.6310e-03, -3.9787e-03,  ..., -1.3382e-02,
         -1.1520e-03,  1.3447e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.9229, -2.6309, -0.5508,  ..., -5.6367, -0.7998, -0.5273]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 10:19:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place monkey lives in is called tree
The place fish lives in is called sea
The place cricket lives in is called nest
The place seal lives in is called den
The place raven lives in is called nest
The place ant lives in is called anthill
The place hippopotamus lives in is called river
The place rat lives in is called
2024-07-23 10:19:34 root INFO     total operator prediction time: 1924.0232200622559 seconds
2024-07-23 10:19:34 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on male - female
2024-07-23 10:19:34 root INFO     building operator male - female
2024-07-23 10:19:34 root INFO     [order_1_approx] starting weight calculation for A female webmaster is known as a webmistress
A female headmaster is known as a headmistress
A female boy is known as a girl
A female actor is known as a actress
A female boar is known as a sow
A female waiter is known as a waitress
A female son is known as a daughter
A female lion is known as a
2024-07-23 10:19:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 10:23:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0332, -1.2070,  0.9805,  ...,  1.3418,  0.3564,  0.4917],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.6523, -0.7070,  0.0605,  ..., -2.9395, -1.6162, -0.6245],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0211, -0.0153,  0.0005,  ...,  0.0129,  0.0037, -0.0176],
        [-0.0010, -0.0171, -0.0187,  ...,  0.0255,  0.0166,  0.0052],
        [ 0.0232,  0.0093, -0.0026,  ..., -0.0146,  0.0074,  0.0283],
        ...,
        [ 0.0017, -0.0118, -0.0061,  ..., -0.0045, -0.0091, -0.0102],
        [ 0.0228, -0.0144,  0.0043,  ..., -0.0025, -0.0321,  0.0044],
        [-0.0135,  0.0005, -0.0163,  ...,  0.0123,  0.0232, -0.0084]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7754,  0.2041, -0.4751,  ..., -2.2129, -2.0840,  0.7251]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 10:23:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female webmaster is known as a webmistress
A female headmaster is known as a headmistress
A female boy is known as a girl
A female actor is known as a actress
A female boar is known as a sow
A female waiter is known as a waitress
A female son is known as a daughter
A female lion is known as a
2024-07-23 10:23:35 root INFO     [order_1_approx] starting weight calculation for A female lion is known as a lioness
A female headmaster is known as a headmistress
A female actor is known as a actress
A female webmaster is known as a webmistress
A female son is known as a daughter
A female boy is known as a girl
A female boar is known as a sow
A female waiter is known as a
2024-07-23 10:23:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 10:27:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6406, -0.9473,  0.6899,  ...,  0.5195,  0.4709,  0.4358],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1484,  1.5479, -0.9492,  ...,  2.0078,  1.5820, -0.3574],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 6.6032e-03, -2.4776e-03,  1.5549e-02,  ...,  7.7667e-03,
         -9.0790e-04, -1.7273e-02],
        [-1.8311e-02,  9.3384e-03, -1.9913e-02,  ..., -2.5921e-03,
         -8.0109e-05,  9.9869e-03],
        [ 1.9836e-04, -1.1604e-02, -2.1866e-02,  ...,  1.9470e-02,
          2.5177e-03,  1.2062e-02],
        ...,
        [-3.1982e-02, -1.3161e-02,  5.2414e-03,  ..., -3.6621e-04,
         -2.1774e-02, -9.1553e-03],
        [ 6.1302e-03, -2.1362e-03, -1.9531e-03,  ...,  7.0724e-03,
         -1.7273e-02,  1.7395e-02],
        [-7.4120e-03, -5.2185e-03,  1.3222e-02,  ...,  2.8057e-03,
          2.0981e-03,  1.2764e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6240,  2.6914, -0.7446,  ...,  2.3828,  1.9277, -0.4482]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 10:27:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female lion is known as a lioness
A female headmaster is known as a headmistress
A female actor is known as a actress
A female webmaster is known as a webmistress
A female son is known as a daughter
A female boy is known as a girl
A female boar is known as a sow
A female waiter is known as a
2024-07-23 10:27:33 root INFO     [order_1_approx] starting weight calculation for A female waiter is known as a waitress
A female webmaster is known as a webmistress
A female lion is known as a lioness
A female son is known as a daughter
A female boar is known as a sow
A female boy is known as a girl
A female headmaster is known as a headmistress
A female actor is known as a
2024-07-23 10:27:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 10:31:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1621, -0.0430,  0.3149,  ..., -0.0342, -0.2469, -0.2397],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4961,  0.0894, -0.9277,  ..., -1.9688, -0.0740, -2.4609],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0168,  0.0008,  0.0111,  ...,  0.0224,  0.0026, -0.0122],
        [-0.0084, -0.0219, -0.0122,  ...,  0.0148, -0.0088, -0.0075],
        [ 0.0208,  0.0080, -0.0075,  ...,  0.0228,  0.0083,  0.0022],
        ...,
        [-0.0188,  0.0049,  0.0087,  ..., -0.0205, -0.0244, -0.0109],
        [ 0.0064,  0.0183,  0.0072,  ..., -0.0065, -0.0497,  0.0033],
        [-0.0044, -0.0058, -0.0055,  ..., -0.0064, -0.0050, -0.0230]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9150,  1.0742, -2.0430,  ..., -1.8379,  0.2581, -3.0527]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 10:31:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female waiter is known as a waitress
A female webmaster is known as a webmistress
A female lion is known as a lioness
A female son is known as a daughter
A female boar is known as a sow
A female boy is known as a girl
A female headmaster is known as a headmistress
A female actor is known as a
2024-07-23 10:31:33 root INFO     [order_1_approx] starting weight calculation for A female lion is known as a lioness
A female boy is known as a girl
A female webmaster is known as a webmistress
A female headmaster is known as a headmistress
A female waiter is known as a waitress
A female son is known as a daughter
A female actor is known as a actress
A female boar is known as a
2024-07-23 10:31:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 10:35:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4932, -0.1162, -0.9238,  ...,  0.5337,  0.6934, -0.2939],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.1973, -0.0225, -5.3906,  ..., -5.1094,  1.2822, -0.9912],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0246, -0.0037,  0.0040,  ...,  0.0112,  0.0028, -0.0238],
        [-0.0184, -0.0151, -0.0116,  ...,  0.0198,  0.0122,  0.0082],
        [ 0.0257,  0.0082, -0.0157,  ..., -0.0005,  0.0039,  0.0112],
        ...,
        [ 0.0048, -0.0178,  0.0034,  ..., -0.0148, -0.0253, -0.0147],
        [-0.0121, -0.0181,  0.0128,  ...,  0.0013, -0.0241,  0.0066],
        [-0.0062,  0.0064, -0.0058,  ..., -0.0122,  0.0126, -0.0260]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7266,  0.2412, -6.2656,  ..., -4.2422,  1.0557, -1.1855]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 10:35:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female lion is known as a lioness
A female boy is known as a girl
A female webmaster is known as a webmistress
A female headmaster is known as a headmistress
A female waiter is known as a waitress
A female son is known as a daughter
A female actor is known as a actress
A female boar is known as a
2024-07-23 10:35:31 root INFO     [order_1_approx] starting weight calculation for A female boar is known as a sow
A female waiter is known as a waitress
A female boy is known as a girl
A female headmaster is known as a headmistress
A female actor is known as a actress
A female webmaster is known as a webmistress
A female lion is known as a lioness
A female son is known as a
2024-07-23 10:35:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 10:39:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4624,  0.2485, -0.4417,  ...,  0.8477,  0.5371,  1.3750],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7949, -2.9297, -2.6328,  ..., -0.1714,  3.3242, -1.7441],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0048,  0.0055,  0.0261,  ...,  0.0026, -0.0040, -0.0163],
        [ 0.0013,  0.0015, -0.0109,  ..., -0.0132,  0.0055, -0.0067],
        [ 0.0126, -0.0176,  0.0042,  ...,  0.0195,  0.0093,  0.0067],
        ...,
        [-0.0084, -0.0195,  0.0062,  ...,  0.0042, -0.0135,  0.0042],
        [ 0.0036,  0.0062,  0.0008,  ...,  0.0052, -0.0284,  0.0025],
        [ 0.0046,  0.0038,  0.0141,  ...,  0.0103,  0.0079, -0.0148]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5352, -3.0195, -1.6523,  ..., -0.0034,  3.0430, -1.8086]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 10:39:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female boar is known as a sow
A female waiter is known as a waitress
A female boy is known as a girl
A female headmaster is known as a headmistress
A female actor is known as a actress
A female webmaster is known as a webmistress
A female lion is known as a lioness
A female son is known as a
2024-07-23 10:39:30 root INFO     [order_1_approx] starting weight calculation for A female actor is known as a actress
A female waiter is known as a waitress
A female boy is known as a girl
A female boar is known as a sow
A female son is known as a daughter
A female lion is known as a lioness
A female headmaster is known as a headmistress
A female webmaster is known as a
2024-07-23 10:39:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 10:43:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5693,  1.1865,  1.5029,  ..., -0.3667,  0.7539,  0.9551],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.7734, -3.3125, -2.4492,  ..., -0.8516,  1.6592,  0.5869],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0081,  0.0067, -0.0132,  ..., -0.0045, -0.0088,  0.0029],
        [ 0.0148, -0.0071, -0.0067,  ...,  0.0051, -0.0056, -0.0010],
        [ 0.0283,  0.0329, -0.0189,  ..., -0.0103,  0.0043,  0.0222],
        ...,
        [-0.0004,  0.0096,  0.0015,  ..., -0.0125,  0.0036, -0.0070],
        [ 0.0157,  0.0047,  0.0104,  ...,  0.0003, -0.0134,  0.0341],
        [-0.0186,  0.0082, -0.0165,  ..., -0.0001,  0.0016, -0.0237]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.6914, -3.5098, -3.4727,  ..., -0.7764,  1.0078,  0.4414]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 10:43:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female actor is known as a actress
A female waiter is known as a waitress
A female boy is known as a girl
A female boar is known as a sow
A female son is known as a daughter
A female lion is known as a lioness
A female headmaster is known as a headmistress
A female webmaster is known as a
2024-07-23 10:43:28 root INFO     [order_1_approx] starting weight calculation for A female webmaster is known as a webmistress
A female son is known as a daughter
A female boar is known as a sow
A female waiter is known as a waitress
A female actor is known as a actress
A female lion is known as a lioness
A female boy is known as a girl
A female headmaster is known as a
2024-07-23 10:43:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 10:47:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7783,  0.6021,  0.7891,  ...,  0.1787,  0.6030,  1.3750],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.2441, -4.9961, -2.4492,  ..., -3.7734,  2.6914,  0.8115],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 5.7220e-04, -1.0406e-02,  9.2621e-03,  ...,  4.4098e-03,
         -7.4463e-03, -1.4923e-02],
        [ 9.1553e-05, -4.2572e-03, -3.2616e-04,  ..., -1.2009e-02,
          1.3626e-02,  2.9221e-03],
        [ 1.8631e-02,  1.3504e-02, -2.8824e-02,  ..., -5.4398e-03,
         -1.4442e-02,  2.3438e-02],
        ...,
        [-8.3847e-03, -3.1204e-03, -2.5940e-02,  ..., -3.6316e-03,
         -9.2621e-03,  2.6825e-02],
        [ 1.5594e-02,  2.3331e-02,  5.6610e-03,  ..., -6.8235e-04,
         -2.7252e-02,  2.9785e-02],
        [ 1.8654e-03,  6.1989e-03, -7.8583e-04,  ...,  8.0948e-03,
          1.2840e-02, -5.6648e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1895, -4.2266, -3.3301,  ..., -3.6094,  1.3809,  0.3457]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 10:47:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female webmaster is known as a webmistress
A female son is known as a daughter
A female boar is known as a sow
A female waiter is known as a waitress
A female actor is known as a actress
A female lion is known as a lioness
A female boy is known as a girl
A female headmaster is known as a
2024-07-23 10:47:25 root INFO     [order_1_approx] starting weight calculation for A female lion is known as a lioness
A female actor is known as a actress
A female waiter is known as a waitress
A female webmaster is known as a webmistress
A female boar is known as a sow
A female son is known as a daughter
A female headmaster is known as a headmistress
A female boy is known as a
2024-07-23 10:47:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 10:51:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5225, -0.1965, -0.2861,  ...,  0.4492,  0.8945,  0.2822],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.0469, -2.2656, -4.2148,  ...,  0.0562,  2.0625, -0.1821],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0031,  0.0115,  0.0100,  ..., -0.0137,  0.0004, -0.0146],
        [ 0.0086, -0.0140, -0.0026,  ..., -0.0036,  0.0046, -0.0111],
        [ 0.0035, -0.0120,  0.0048,  ...,  0.0171,  0.0056,  0.0018],
        ...,
        [-0.0168, -0.0217,  0.0153,  ..., -0.0019, -0.0138,  0.0112],
        [ 0.0053, -0.0067,  0.0075,  ...,  0.0006, -0.0110, -0.0042],
        [ 0.0028,  0.0158,  0.0084,  ..., -0.0056,  0.0125,  0.0024]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4980, -1.8164, -6.4141,  ..., -0.1819,  2.3516,  0.1240]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 10:51:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female lion is known as a lioness
A female actor is known as a actress
A female waiter is known as a waitress
A female webmaster is known as a webmistress
A female boar is known as a sow
A female son is known as a daughter
A female headmaster is known as a headmistress
A female boy is known as a
2024-07-23 10:51:23 root INFO     total operator prediction time: 1908.9974455833435 seconds
2024-07-23 10:51:23 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on name - occupation
2024-07-23 10:51:23 root INFO     building operator name - occupation
2024-07-23 10:51:23 root INFO     [order_1_approx] starting weight calculation for hitler was known for their work as a  dictator
truman was known for their work as a  president
haydn was known for their work as a  composer
newton was known for their work as a  scientist
locke was known for their work as a  philosopher
mozart was known for their work as a  composer
confucius was known for their work as a  philosopher
hawking was known for their work as a 
2024-07-23 10:51:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 10:55:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0957,  0.7686, -0.9155,  ...,  0.5806,  0.3403,  0.3989],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0342, -0.4182, -2.6426,  ..., -1.3945,  0.4624, -0.1816],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-7.4425e-03,  3.1166e-03,  2.1133e-02,  ..., -2.6749e-02,
          2.7161e-02, -1.4915e-03],
        [ 1.9064e-03,  1.3794e-02,  9.2697e-03,  ...,  2.8290e-02,
          7.0877e-03, -3.4428e-03],
        [ 2.2888e-02,  4.6387e-03, -6.0081e-03,  ...,  1.4755e-02,
          1.4069e-02, -7.5455e-03],
        ...,
        [-5.3253e-03,  3.0975e-03, -4.6120e-03,  ..., -1.1841e-02,
         -2.5604e-02, -6.7215e-03],
        [ 6.7520e-03, -2.5589e-02, -3.1471e-05,  ...,  2.2781e-02,
         -3.7766e-03,  2.2415e-02],
        [-1.1444e-02, -8.6517e-03, -1.3573e-02,  ..., -5.3024e-03,
          1.3687e-02,  3.6964e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0435,  0.0168, -3.2012,  ..., -1.4502,  0.3687, -1.1123]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 10:55:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for hitler was known for their work as a  dictator
truman was known for their work as a  president
haydn was known for their work as a  composer
newton was known for their work as a  scientist
locke was known for their work as a  philosopher
mozart was known for their work as a  composer
confucius was known for their work as a  philosopher
hawking was known for their work as a 
2024-07-23 10:55:19 root INFO     [order_1_approx] starting weight calculation for confucius was known for their work as a  philosopher
truman was known for their work as a  president
newton was known for their work as a  scientist
locke was known for their work as a  philosopher
hawking was known for their work as a  physicist
mozart was known for their work as a  composer
haydn was known for their work as a  composer
hitler was known for their work as a 
2024-07-23 10:55:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 10:59:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2878,  0.5010, -0.1033,  ..., -0.0771, -0.8667, -0.7129],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9600, -2.5879, -5.3125,  ..., -1.4141, -1.5723, -2.0508],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0070, -0.0166, -0.0086,  ..., -0.0087,  0.0054, -0.0026],
        [-0.0049,  0.0082, -0.0077,  ...,  0.0059,  0.0030, -0.0116],
        [-0.0017,  0.0012, -0.0088,  ...,  0.0045,  0.0010, -0.0119],
        ...,
        [-0.0028,  0.0060,  0.0047,  ...,  0.0032,  0.0015,  0.0088],
        [ 0.0071,  0.0012, -0.0026,  ..., -0.0073, -0.0146, -0.0077],
        [ 0.0033, -0.0047,  0.0103,  ..., -0.0121, -0.0039,  0.0018]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.2031, -2.3984, -5.1445,  ..., -2.3184, -1.7734, -1.8027]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 10:59:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for confucius was known for their work as a  philosopher
truman was known for their work as a  president
newton was known for their work as a  scientist
locke was known for their work as a  philosopher
hawking was known for their work as a  physicist
mozart was known for their work as a  composer
haydn was known for their work as a  composer
hitler was known for their work as a 
2024-07-23 10:59:17 root INFO     [order_1_approx] starting weight calculation for haydn was known for their work as a  composer
confucius was known for their work as a  philosopher
truman was known for their work as a  president
locke was known for their work as a  philosopher
mozart was known for their work as a  composer
hawking was known for their work as a  physicist
hitler was known for their work as a  dictator
newton was known for their work as a 
2024-07-23 10:59:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 11:03:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8809,  0.7080, -0.0845,  ...,  0.1006, -1.6875,  0.8057],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.1328, -2.5449, -0.8086,  ..., -0.0910, -3.8750,  0.3708],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0188, -0.0084,  0.0029,  ..., -0.0023,  0.0111, -0.0007],
        [-0.0077, -0.0045, -0.0036,  ...,  0.0098,  0.0127, -0.0099],
        [ 0.0085,  0.0285, -0.0107,  ...,  0.0075, -0.0031, -0.0193],
        ...,
        [-0.0110,  0.0065,  0.0026,  ..., -0.0158,  0.0003,  0.0173],
        [ 0.0129,  0.0089, -0.0019,  ...,  0.0128, -0.0166, -0.0048],
        [-0.0125,  0.0029,  0.0004,  ..., -0.0054,  0.0007, -0.0028]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.7930, -1.6289, -1.0430,  ...,  0.0544, -3.4980,  0.2629]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 11:03:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for haydn was known for their work as a  composer
confucius was known for their work as a  philosopher
truman was known for their work as a  president
locke was known for their work as a  philosopher
mozart was known for their work as a  composer
hawking was known for their work as a  physicist
hitler was known for their work as a  dictator
newton was known for their work as a 
2024-07-23 11:03:15 root INFO     [order_1_approx] starting weight calculation for confucius was known for their work as a  philosopher
mozart was known for their work as a  composer
truman was known for their work as a  president
hitler was known for their work as a  dictator
haydn was known for their work as a  composer
hawking was known for their work as a  physicist
newton was known for their work as a  scientist
locke was known for their work as a 
2024-07-23 11:03:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 11:07:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2236, -0.1211, -0.0591,  ...,  0.4419, -1.1113,  1.3115],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2314, -3.5176, -3.7930,  ..., -4.3555,  2.3555,  1.2500],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.3417e-03,  3.5534e-03,  5.7487e-03,  ..., -6.6528e-03,
          9.4070e-03, -1.0536e-02],
        [ 7.1449e-03,  4.9591e-05,  4.3945e-03,  ...,  1.4442e-02,
          6.1874e-03, -7.8354e-03],
        [ 1.5976e-02,  1.5793e-03, -1.4267e-02,  ...,  1.2650e-02,
         -4.0436e-04, -4.4708e-03],
        ...,
        [ 6.8588e-03,  2.3365e-04,  1.3443e-02,  ..., -5.3558e-03,
         -4.6196e-03, -1.4286e-03],
        [-2.4323e-02, -9.6817e-03,  4.6883e-03,  ...,  4.4823e-03,
         -2.6550e-03,  1.1749e-03],
        [-1.2573e-02,  5.9509e-03,  8.1482e-03,  ..., -2.6367e-02,
          7.2708e-03, -6.8932e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1042, -3.0391, -3.5820,  ..., -4.8047,  2.1660,  0.7070]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 11:07:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for confucius was known for their work as a  philosopher
mozart was known for their work as a  composer
truman was known for their work as a  president
hitler was known for their work as a  dictator
haydn was known for their work as a  composer
hawking was known for their work as a  physicist
newton was known for their work as a  scientist
locke was known for their work as a 
2024-07-23 11:07:14 root INFO     [order_1_approx] starting weight calculation for hawking was known for their work as a  physicist
mozart was known for their work as a  composer
truman was known for their work as a  president
confucius was known for their work as a  philosopher
hitler was known for their work as a  dictator
newton was known for their work as a  scientist
locke was known for their work as a  philosopher
haydn was known for their work as a 
2024-07-23 11:07:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 11:11:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4268,  0.0090, -0.3977,  ...,  1.0469, -0.4373,  1.3125],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1548,  0.2217, -0.7192,  ..., -2.7988,  2.3730,  2.9336],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0007, -0.0004, -0.0040,  ..., -0.0094,  0.0107, -0.0181],
        [-0.0028,  0.0127,  0.0129,  ...,  0.0069,  0.0070, -0.0006],
        [ 0.0023,  0.0044, -0.0003,  ...,  0.0120, -0.0068, -0.0121],
        ...,
        [ 0.0057, -0.0016, -0.0055,  ..., -0.0025, -0.0016, -0.0033],
        [-0.0013, -0.0058, -0.0017,  ...,  0.0035, -0.0122,  0.0134],
        [ 0.0043,  0.0103,  0.0011,  ...,  0.0096,  0.0056,  0.0107]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2930,  0.1484, -0.6787,  ..., -3.0234,  2.4629,  3.2598]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 11:11:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for hawking was known for their work as a  physicist
mozart was known for their work as a  composer
truman was known for their work as a  president
confucius was known for their work as a  philosopher
hitler was known for their work as a  dictator
newton was known for their work as a  scientist
locke was known for their work as a  philosopher
haydn was known for their work as a 
2024-07-23 11:11:12 root INFO     [order_1_approx] starting weight calculation for newton was known for their work as a  scientist
hawking was known for their work as a  physicist
locke was known for their work as a  philosopher
confucius was known for their work as a  philosopher
haydn was known for their work as a  composer
truman was known for their work as a  president
hitler was known for their work as a  dictator
mozart was known for their work as a 
2024-07-23 11:11:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 11:15:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3501,  0.1072, -0.0085,  ...,  0.1145,  0.4570, -0.1976],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8066, -0.9434, -2.8672,  ..., -0.6826, -0.8145, -1.2070],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0206, -0.0080, -0.0014,  ...,  0.0036,  0.0182,  0.0080],
        [-0.0006,  0.0026,  0.0039,  ...,  0.0157, -0.0045, -0.0029],
        [ 0.0094,  0.0100, -0.0007,  ...,  0.0054,  0.0004, -0.0136],
        ...,
        [-0.0107,  0.0187, -0.0214,  ..., -0.0178, -0.0315,  0.0149],
        [ 0.0097, -0.0109,  0.0018,  ...,  0.0071, -0.0272, -0.0147],
        [-0.0082, -0.0219, -0.0052,  ...,  0.0056,  0.0086, -0.0156]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5586, -1.0078, -2.9199,  ..., -0.1855, -0.8691, -1.3037]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 11:15:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for newton was known for their work as a  scientist
hawking was known for their work as a  physicist
locke was known for their work as a  philosopher
confucius was known for their work as a  philosopher
haydn was known for their work as a  composer
truman was known for their work as a  president
hitler was known for their work as a  dictator
mozart was known for their work as a 
2024-07-23 11:15:09 root INFO     [order_1_approx] starting weight calculation for newton was known for their work as a  scientist
locke was known for their work as a  philosopher
hitler was known for their work as a  dictator
mozart was known for their work as a  composer
hawking was known for their work as a  physicist
haydn was known for their work as a  composer
confucius was known for their work as a  philosopher
truman was known for their work as a 
2024-07-23 11:15:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 11:19:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2500,  0.0090, -0.0083,  ...,  0.3975, -0.7402, -0.1848],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3425, -1.1895, -0.2217,  ..., -3.3594,  1.8047, -1.0000],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0055,  0.0059,  0.0030,  ..., -0.0080,  0.0216, -0.0059],
        [-0.0181, -0.0148, -0.0015,  ...,  0.0074, -0.0038, -0.0153],
        [-0.0008,  0.0064,  0.0016,  ...,  0.0024,  0.0018, -0.0039],
        ...,
        [-0.0041,  0.0010,  0.0043,  ..., -0.0055, -0.0138,  0.0083],
        [ 0.0092,  0.0114,  0.0005,  ...,  0.0056, -0.0139,  0.0032],
        [-0.0115,  0.0070,  0.0025,  ..., -0.0015,  0.0163,  0.0069]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9492, -0.7754, -0.3704,  ..., -3.5703,  1.9004, -1.6328]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 11:19:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for newton was known for their work as a  scientist
locke was known for their work as a  philosopher
hitler was known for their work as a  dictator
mozart was known for their work as a  composer
hawking was known for their work as a  physicist
haydn was known for their work as a  composer
confucius was known for their work as a  philosopher
truman was known for their work as a 
2024-07-23 11:19:08 root INFO     [order_1_approx] starting weight calculation for haydn was known for their work as a  composer
hitler was known for their work as a  dictator
newton was known for their work as a  scientist
hawking was known for their work as a  physicist
mozart was known for their work as a  composer
truman was known for their work as a  president
locke was known for their work as a  philosopher
confucius was known for their work as a 
2024-07-23 11:19:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 11:23:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2207, -0.7705, -1.6904,  ..., -0.5239, -0.1587,  0.8433],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.2031, -3.9297, -5.6758,  ..., -2.6445,  1.2305, -3.3516],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0152, -0.0024, -0.0053,  ..., -0.0049,  0.0112, -0.0161],
        [-0.0087,  0.0063, -0.0013,  ..., -0.0028,  0.0051, -0.0095],
        [ 0.0218, -0.0053, -0.0041,  ..., -0.0004, -0.0051, -0.0115],
        ...,
        [-0.0035, -0.0024, -0.0049,  ..., -0.0005, -0.0048, -0.0016],
        [ 0.0159, -0.0013,  0.0020,  ...,  0.0092, -0.0213,  0.0066],
        [ 0.0009,  0.0024,  0.0043,  ..., -0.0010, -0.0020, -0.0021]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.0625, -4.4766, -5.6328,  ..., -2.6406,  1.3037, -3.8926]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 11:23:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for haydn was known for their work as a  composer
hitler was known for their work as a  dictator
newton was known for their work as a  scientist
hawking was known for their work as a  physicist
mozart was known for their work as a  composer
truman was known for their work as a  president
locke was known for their work as a  philosopher
confucius was known for their work as a 
2024-07-23 11:23:06 root INFO     total operator prediction time: 1902.6445379257202 seconds
2024-07-23 11:23:06 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on country - capital
2024-07-23 11:23:06 root INFO     building operator country - capital
2024-07-23 11:23:06 root INFO     [order_1_approx] starting weight calculation for The country with bangkok as its capital is known as thailand
The country with havana as its capital is known as cuba
The country with ankara as its capital is known as turkey
The country with jakarta as its capital is known as indonesia
The country with madrid as its capital is known as spain
The country with budapest as its capital is known as hungary
The country with stockholm as its capital is known as sweden
The country with dublin as its capital is known as
2024-07-23 11:23:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 11:27:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5205,  0.2700, -0.5244,  ...,  0.0718, -0.6914,  0.7510],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7437,  0.4775,  2.1172,  ..., -2.7246,  1.1797, -0.6396],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0026,  0.0053, -0.0100,  ...,  0.0120,  0.0102, -0.0022],
        [ 0.0101, -0.0282,  0.0068,  ...,  0.0024, -0.0053,  0.0045],
        [-0.0075,  0.0048, -0.0077,  ...,  0.0094,  0.0003, -0.0135],
        ...,
        [ 0.0023,  0.0088, -0.0116,  ..., -0.0024, -0.0096,  0.0022],
        [-0.0122,  0.0213, -0.0063,  ..., -0.0075, -0.0031,  0.0106],
        [-0.0062,  0.0010, -0.0099,  ..., -0.0055,  0.0077, -0.0045]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2498,  0.6504,  1.8125,  ..., -3.1582, -0.0996,  0.4746]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 11:27:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with bangkok as its capital is known as thailand
The country with havana as its capital is known as cuba
The country with ankara as its capital is known as turkey
The country with jakarta as its capital is known as indonesia
The country with madrid as its capital is known as spain
The country with budapest as its capital is known as hungary
The country with stockholm as its capital is known as sweden
The country with dublin as its capital is known as
2024-07-23 11:27:03 root INFO     [order_1_approx] starting weight calculation for The country with madrid as its capital is known as spain
The country with dublin as its capital is known as ireland
The country with jakarta as its capital is known as indonesia
The country with ankara as its capital is known as turkey
The country with stockholm as its capital is known as sweden
The country with havana as its capital is known as cuba
The country with bangkok as its capital is known as thailand
The country with budapest as its capital is known as
2024-07-23 11:27:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 11:30:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.5430, -0.6641,  0.3975,  ...,  0.3821,  0.2466, -0.1842],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8320,  0.4971,  4.2773,  ...,  3.5586, -3.9336,  1.4023],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-4.5776e-02, -2.3468e-02,  6.2675e-03,  ...,  1.8448e-02,
          6.6147e-03, -3.8738e-03],
        [-1.0574e-02, -3.0182e-02,  2.3468e-02,  ...,  1.8387e-02,
          1.7273e-02, -8.1482e-03],
        [-2.0996e-02, -3.5309e-02, -9.7504e-03,  ...,  1.0681e-03,
         -1.9180e-02, -2.4628e-02],
        ...,
        [-5.1727e-02, -1.3062e-02,  8.5297e-03,  ..., -7.5340e-03,
         -8.7738e-04,  5.7220e-06],
        [ 4.8218e-03,  2.5894e-02, -3.0426e-02,  ..., -1.7883e-02,
         -3.6682e-02,  7.1716e-03],
        [-3.2990e-02, -3.1860e-02, -1.3733e-03,  ...,  3.4866e-03,
          1.5976e-02,  2.2964e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0156,  1.1309,  4.7070,  ...,  3.0742, -3.6484,  1.8672]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 11:31:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with madrid as its capital is known as spain
The country with dublin as its capital is known as ireland
The country with jakarta as its capital is known as indonesia
The country with ankara as its capital is known as turkey
The country with stockholm as its capital is known as sweden
The country with havana as its capital is known as cuba
The country with bangkok as its capital is known as thailand
The country with budapest as its capital is known as
2024-07-23 11:31:00 root INFO     [order_1_approx] starting weight calculation for The country with madrid as its capital is known as spain
The country with stockholm as its capital is known as sweden
The country with bangkok as its capital is known as thailand
The country with ankara as its capital is known as turkey
The country with jakarta as its capital is known as indonesia
The country with budapest as its capital is known as hungary
The country with dublin as its capital is known as ireland
The country with havana as its capital is known as
2024-07-23 11:31:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 11:34:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9946,  0.3987, -0.2739,  ..., -0.7480, -1.0967,  0.7275],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1953,  0.7529,  2.0664,  ..., -2.5000,  2.8516,  0.3401],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0346, -0.0097,  0.0111,  ...,  0.0170,  0.0029, -0.0457],
        [-0.0104, -0.0141,  0.0029,  ...,  0.0331, -0.0088, -0.0216],
        [ 0.0197,  0.0142, -0.0410,  ..., -0.0359,  0.0126,  0.0109],
        ...,
        [-0.0175,  0.0320, -0.0006,  ...,  0.0140,  0.0074,  0.0141],
        [-0.0063, -0.0407, -0.0051,  ...,  0.0151, -0.0191, -0.0116],
        [-0.0064, -0.0308,  0.0067,  ...,  0.0281, -0.0115, -0.0238]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3438,  0.0298,  1.9375,  ..., -2.1406,  3.0430,  1.1035]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 11:34:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with madrid as its capital is known as spain
The country with stockholm as its capital is known as sweden
The country with bangkok as its capital is known as thailand
The country with ankara as its capital is known as turkey
The country with jakarta as its capital is known as indonesia
The country with budapest as its capital is known as hungary
The country with dublin as its capital is known as ireland
The country with havana as its capital is known as
2024-07-23 11:34:59 root INFO     [order_1_approx] starting weight calculation for The country with dublin as its capital is known as ireland
The country with jakarta as its capital is known as indonesia
The country with stockholm as its capital is known as sweden
The country with budapest as its capital is known as hungary
The country with bangkok as its capital is known as thailand
The country with ankara as its capital is known as turkey
The country with havana as its capital is known as cuba
The country with madrid as its capital is known as
2024-07-23 11:34:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 11:38:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0664,  0.6270,  0.9106,  ..., -0.5176,  0.4341, -0.2983],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2949,  1.3203,  2.9258,  ..., -1.5596, -4.3242, -0.2686],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0015,  0.0084, -0.0187,  ...,  0.0066,  0.0019, -0.0055],
        [-0.0003, -0.0208, -0.0031,  ..., -0.0057,  0.0125, -0.0141],
        [-0.0219,  0.0077, -0.0228,  ..., -0.0044, -0.0050, -0.0051],
        ...,
        [-0.0209, -0.0040,  0.0036,  ..., -0.0001, -0.0005,  0.0111],
        [ 0.0114,  0.0198, -0.0112,  ..., -0.0171, -0.0265, -0.0037],
        [ 0.0036, -0.0156,  0.0003,  ..., -0.0023,  0.0262, -0.0160]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4702,  1.9219,  2.7773,  ..., -1.1113, -4.7773,  0.0369]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 11:38:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with dublin as its capital is known as ireland
The country with jakarta as its capital is known as indonesia
The country with stockholm as its capital is known as sweden
The country with budapest as its capital is known as hungary
The country with bangkok as its capital is known as thailand
The country with ankara as its capital is known as turkey
The country with havana as its capital is known as cuba
The country with madrid as its capital is known as
2024-07-23 11:38:57 root INFO     [order_1_approx] starting weight calculation for The country with havana as its capital is known as cuba
The country with bangkok as its capital is known as thailand
The country with budapest as its capital is known as hungary
The country with madrid as its capital is known as spain
The country with ankara as its capital is known as turkey
The country with jakarta as its capital is known as indonesia
The country with dublin as its capital is known as ireland
The country with stockholm as its capital is known as
2024-07-23 11:38:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 11:42:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2793,  1.2324,  1.0684,  ..., -0.4636,  1.1016, -0.4141],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-6.3633, -0.6782, -1.6719,  ..., -3.0742, -1.1064, -5.9258],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0380,  0.0182, -0.0155,  ...,  0.0076, -0.0045, -0.0069],
        [-0.0020, -0.0096,  0.0109,  ...,  0.0017,  0.0069,  0.0003],
        [ 0.0292,  0.0472, -0.0121,  ...,  0.0008, -0.0148, -0.0067],
        ...,
        [-0.0177,  0.0143,  0.0008,  ...,  0.0159, -0.0075, -0.0011],
        [-0.0046,  0.0101, -0.0066,  ..., -0.0081, -0.0090,  0.0243],
        [ 0.0301,  0.0257, -0.0283,  ..., -0.0091, -0.0005, -0.0029]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.8867, -0.6104, -1.7949,  ..., -3.7188, -0.6201, -5.4414]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 11:42:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with havana as its capital is known as cuba
The country with bangkok as its capital is known as thailand
The country with budapest as its capital is known as hungary
The country with madrid as its capital is known as spain
The country with ankara as its capital is known as turkey
The country with jakarta as its capital is known as indonesia
The country with dublin as its capital is known as ireland
The country with stockholm as its capital is known as
2024-07-23 11:42:55 root INFO     [order_1_approx] starting weight calculation for The country with dublin as its capital is known as ireland
The country with jakarta as its capital is known as indonesia
The country with stockholm as its capital is known as sweden
The country with budapest as its capital is known as hungary
The country with madrid as its capital is known as spain
The country with ankara as its capital is known as turkey
The country with havana as its capital is known as cuba
The country with bangkok as its capital is known as
2024-07-23 11:42:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 11:46:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5010,  0.1753,  0.0737,  ...,  0.7852,  0.2039, -1.0957],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3242,  1.9141, -2.4102,  ...,  0.8154, -2.3320, -4.7344],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0106, -0.0159,  0.0039,  ...,  0.0049,  0.0036, -0.0146],
        [ 0.0107,  0.0024, -0.0068,  ..., -0.0065,  0.0089,  0.0113],
        [ 0.0181,  0.0403,  0.0067,  ..., -0.0101,  0.0146, -0.0007],
        ...,
        [-0.0565, -0.0062, -0.0093,  ...,  0.0224, -0.0177,  0.0172],
        [ 0.0138,  0.0228,  0.0096,  ..., -0.0031, -0.0274, -0.0011],
        [-0.0200,  0.0068, -0.0140,  ...,  0.0073, -0.0043,  0.0006]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5869,  2.6406, -2.9922,  ...,  1.2188, -0.9824, -3.3359]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 11:46:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with dublin as its capital is known as ireland
The country with jakarta as its capital is known as indonesia
The country with stockholm as its capital is known as sweden
The country with budapest as its capital is known as hungary
The country with madrid as its capital is known as spain
The country with ankara as its capital is known as turkey
The country with havana as its capital is known as cuba
The country with bangkok as its capital is known as
2024-07-23 11:46:53 root INFO     [order_1_approx] starting weight calculation for The country with dublin as its capital is known as ireland
The country with budapest as its capital is known as hungary
The country with madrid as its capital is known as spain
The country with stockholm as its capital is known as sweden
The country with ankara as its capital is known as turkey
The country with havana as its capital is known as cuba
The country with bangkok as its capital is known as thailand
The country with jakarta as its capital is known as
2024-07-23 11:46:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 11:50:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6631, -0.0598,  0.8682,  ..., -0.5278, -0.8232,  0.0372],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3672, -2.3945, -2.0996,  ..., -5.7578, -0.8867,  0.2441],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0069,  0.0070, -0.0029,  ..., -0.0042,  0.0098, -0.0055],
        [ 0.0098, -0.0087,  0.0166,  ...,  0.0051,  0.0196, -0.0146],
        [ 0.0134,  0.0234, -0.0167,  ...,  0.0033,  0.0048, -0.0121],
        ...,
        [-0.0078, -0.0106, -0.0062,  ...,  0.0117, -0.0160,  0.0230],
        [ 0.0079, -0.0009, -0.0165,  ...,  0.0087, -0.0179,  0.0098],
        [-0.0116, -0.0075,  0.0117,  ...,  0.0012,  0.0085, -0.0038]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5176, -1.7275, -2.6777,  ..., -5.4766, -1.3809,  0.8359]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 11:50:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with dublin as its capital is known as ireland
The country with budapest as its capital is known as hungary
The country with madrid as its capital is known as spain
The country with stockholm as its capital is known as sweden
The country with ankara as its capital is known as turkey
The country with havana as its capital is known as cuba
The country with bangkok as its capital is known as thailand
The country with jakarta as its capital is known as
2024-07-23 11:50:53 root INFO     [order_1_approx] starting weight calculation for The country with stockholm as its capital is known as sweden
The country with jakarta as its capital is known as indonesia
The country with dublin as its capital is known as ireland
The country with budapest as its capital is known as hungary
The country with bangkok as its capital is known as thailand
The country with madrid as its capital is known as spain
The country with havana as its capital is known as cuba
The country with ankara as its capital is known as
2024-07-23 11:50:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 11:54:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1238, -0.0349,  1.3652,  ..., -0.4946, -0.5142,  0.2986],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.0996,  1.1201,  2.9082,  ...,  2.2031, -2.6777, -3.3594],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0312, -0.0143, -0.0061,  ..., -0.0077,  0.0291, -0.0505],
        [ 0.0386, -0.0073, -0.0062,  ...,  0.0114,  0.0135, -0.0189],
        [ 0.0050,  0.0088, -0.0412,  ...,  0.0306, -0.0177,  0.0130],
        ...,
        [-0.0497,  0.0049,  0.0124,  ...,  0.0706, -0.0123,  0.0005],
        [-0.0350,  0.0050, -0.0249,  ...,  0.0317, -0.0245,  0.0475],
        [-0.0060,  0.0435, -0.0224,  ...,  0.0336,  0.0115,  0.0311]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.4629,  1.9277,  2.1074,  ...,  3.1328, -1.9316, -2.4824]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 11:54:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with stockholm as its capital is known as sweden
The country with jakarta as its capital is known as indonesia
The country with dublin as its capital is known as ireland
The country with budapest as its capital is known as hungary
The country with bangkok as its capital is known as thailand
The country with madrid as its capital is known as spain
The country with havana as its capital is known as cuba
The country with ankara as its capital is known as
2024-07-23 11:54:49 root INFO     total operator prediction time: 1903.1022744178772 seconds
2024-07-23 11:54:49 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on things - color
2024-07-23 11:54:49 root INFO     building operator things - color
2024-07-23 11:54:49 root INFO     [order_1_approx] starting weight calculation for The emerald is colored green
The frog is colored green
The pepper is colored black
The cranberry is colored red
The blackboard is colored black
The grass is colored green
The tea is colored black
The blueberry is colored
2024-07-23 11:54:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 11:58:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1162,  0.2634,  1.2148,  ..., -1.6270, -0.3367,  0.6914],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4580,  3.3965, -1.5605,  ...,  0.0156,  4.8789, -0.6235],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0096, -0.0055, -0.0021,  ...,  0.0010, -0.0035, -0.0043],
        [-0.0082, -0.0021,  0.0031,  ..., -0.0007,  0.0078, -0.0006],
        [-0.0018, -0.0042, -0.0016,  ..., -0.0036, -0.0084, -0.0038],
        ...,
        [-0.0008, -0.0122, -0.0042,  ...,  0.0080, -0.0049,  0.0039],
        [-0.0003,  0.0063,  0.0010,  ...,  0.0028,  0.0038,  0.0053],
        [ 0.0021,  0.0035,  0.0072,  ..., -0.0025,  0.0081,  0.0076]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6787,  3.1523, -1.2617,  ...,  0.1572,  4.6797, -0.6934]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 11:58:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The emerald is colored green
The frog is colored green
The pepper is colored black
The cranberry is colored red
The blackboard is colored black
The grass is colored green
The tea is colored black
The blueberry is colored
2024-07-23 11:58:49 root INFO     [order_1_approx] starting weight calculation for The grass is colored green
The tea is colored black
The cranberry is colored red
The pepper is colored black
The emerald is colored green
The frog is colored green
The blueberry is colored blue
The blackboard is colored
2024-07-23 11:58:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 12:02:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1523, -0.6592,  0.8906,  ...,  0.7285,  1.7754,  0.4126],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0781,  2.1602, -2.9336,  ...,  1.5928,  4.5742, -2.7656],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.2100e-02, -5.5428e-03, -3.9291e-04,  ..., -2.4414e-03,
          2.0561e-03,  1.4324e-03],
        [ 3.1204e-03, -4.6921e-03, -1.2665e-03,  ...,  9.5596e-03,
         -6.4850e-04, -3.8109e-03],
        [-7.0572e-03,  7.3662e-03, -3.5896e-03,  ...,  6.2180e-03,
          2.5749e-03,  8.5526e-03],
        ...,
        [-1.0254e-02, -1.2733e-02,  1.1635e-04,  ...,  7.9041e-03,
         -3.6221e-03,  2.0294e-03],
        [-9.2468e-03,  8.2245e-03,  8.7280e-03,  ...,  5.6725e-03,
          1.1330e-02,  4.4899e-03],
        [ 7.6294e-05, -1.1253e-03,  5.4970e-03,  ..., -1.4954e-03,
          9.7504e-03,  1.0254e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7988,  1.8828, -2.9121,  ...,  1.0859,  4.3438, -3.1777]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 12:02:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The grass is colored green
The tea is colored black
The cranberry is colored red
The pepper is colored black
The emerald is colored green
The frog is colored green
The blueberry is colored blue
The blackboard is colored
2024-07-23 12:02:50 root INFO     [order_1_approx] starting weight calculation for The pepper is colored black
The frog is colored green
The blueberry is colored blue
The grass is colored green
The cranberry is colored red
The blackboard is colored black
The tea is colored black
The emerald is colored
2024-07-23 12:02:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 12:06:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6904, -0.6035,  0.3633,  ..., -0.0894,  1.8154, -0.8975],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6562,  3.3418,  0.8477,  ..., -1.8506,  1.9248,  0.1260],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0006, -0.0064,  0.0015,  ..., -0.0036, -0.0057,  0.0016],
        [-0.0055, -0.0041,  0.0019,  ...,  0.0026,  0.0029, -0.0043],
        [-0.0070,  0.0005, -0.0016,  ..., -0.0063,  0.0047,  0.0065],
        ...,
        [-0.0056,  0.0013, -0.0041,  ...,  0.0090, -0.0010,  0.0098],
        [ 0.0034,  0.0037, -0.0035,  ..., -0.0114, -0.0051,  0.0061],
        [-0.0062,  0.0004,  0.0076,  ..., -0.0059,  0.0035, -0.0106]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5918,  2.9219,  0.9062,  ..., -1.7363,  1.4258,  0.1600]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 12:06:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The pepper is colored black
The frog is colored green
The blueberry is colored blue
The grass is colored green
The cranberry is colored red
The blackboard is colored black
The tea is colored black
The emerald is colored
2024-07-23 12:06:48 root INFO     [order_1_approx] starting weight calculation for The blackboard is colored black
The emerald is colored green
The tea is colored black
The pepper is colored black
The blueberry is colored blue
The grass is colored green
The cranberry is colored red
The frog is colored
2024-07-23 12:06:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 12:10:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9897,  0.2827, -0.4592,  ...,  0.1240, -0.6035,  0.4766],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0000,  4.0977,  0.8398,  ..., -2.0020,  3.8633, -1.1699],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0153, -0.0122, -0.0033,  ...,  0.0115, -0.0011,  0.0031],
        [-0.0029,  0.0014,  0.0056,  ..., -0.0004,  0.0104, -0.0089],
        [-0.0013, -0.0015,  0.0048,  ..., -0.0044, -0.0057,  0.0026],
        ...,
        [ 0.0012, -0.0120, -0.0115,  ...,  0.0090,  0.0021,  0.0166],
        [-0.0039,  0.0035, -0.0011,  ...,  0.0036,  0.0034,  0.0055],
        [-0.0072, -0.0081,  0.0012,  ..., -0.0105,  0.0009, -0.0031]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9644,  4.2188,  0.4644,  ..., -1.4062,  3.7109, -1.4766]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 12:10:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The blackboard is colored black
The emerald is colored green
The tea is colored black
The pepper is colored black
The blueberry is colored blue
The grass is colored green
The cranberry is colored red
The frog is colored
2024-07-23 12:10:47 root INFO     [order_1_approx] starting weight calculation for The pepper is colored black
The grass is colored green
The frog is colored green
The blackboard is colored black
The emerald is colored green
The blueberry is colored blue
The cranberry is colored red
The tea is colored
2024-07-23 12:10:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 12:14:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0020, -0.3174,  0.5898,  ..., -0.0479, -0.1721, -0.3186],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.0469,  3.4395,  0.1191,  ...,  1.1836,  2.9824, -1.9941],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.8324e-04,  4.8637e-04, -1.6022e-02,  ...,  6.2943e-05,
         -5.7144e-03, -4.6310e-03],
        [ 3.5362e-03,  1.4753e-03,  4.0207e-03,  ...,  2.4872e-03,
          6.3286e-03, -4.8676e-03],
        [ 2.5330e-03,  1.2474e-03,  3.4409e-03,  ...,  3.6354e-03,
         -2.0275e-03,  8.8272e-03],
        ...,
        [-5.0430e-03, -2.0752e-03, -3.2558e-03,  ...,  5.8861e-03,
         -5.6839e-03,  9.4604e-03],
        [ 4.8065e-03, -4.2343e-03, -3.9101e-04,  ..., -9.3460e-03,
         -4.8828e-04, -1.2054e-02],
        [-1.4009e-03,  1.4610e-03,  5.3787e-03,  ...,  1.8167e-03,
          2.9731e-04,  2.6913e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5234,  3.6328,  1.2842,  ...,  1.2988,  3.1602, -2.1348]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 12:14:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The pepper is colored black
The grass is colored green
The frog is colored green
The blackboard is colored black
The emerald is colored green
The blueberry is colored blue
The cranberry is colored red
The tea is colored
2024-07-23 12:14:47 root INFO     [order_1_approx] starting weight calculation for The tea is colored black
The blueberry is colored blue
The cranberry is colored red
The emerald is colored green
The frog is colored green
The blackboard is colored black
The pepper is colored black
The grass is colored
2024-07-23 12:14:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 12:18:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0360, -0.0708, -0.2739,  ..., -0.1384, -0.2654, -0.2817],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.8828,  5.1250, -0.6250,  ...,  3.4297,  4.3125, -4.3750],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 9.3155e-03, -8.8120e-03,  5.2719e-03,  ...,  7.4081e-03,
         -4.1275e-03,  1.4124e-03],
        [-1.4542e-02, -4.8141e-03, -1.2589e-04,  ..., -1.3412e-02,
          5.3482e-03, -5.3406e-03],
        [-1.0941e-02, -4.2915e-05,  3.6659e-03,  ...,  6.1874e-03,
         -3.5954e-04,  8.8348e-03],
        ...,
        [-1.0651e-02, -1.5686e-02, -1.9255e-03,  ...,  1.3550e-02,
          1.3580e-03,  3.6049e-03],
        [ 1.3458e-02, -2.6398e-03,  4.1733e-03,  ...,  9.4910e-03,
          1.6022e-03, -4.4746e-03],
        [-3.8719e-04, -5.2986e-03,  3.0022e-03,  ..., -6.9122e-03,
          9.7961e-03,  2.5253e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5488,  4.5742, -0.5371,  ...,  3.3164,  3.9570, -4.2930]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 12:18:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The tea is colored black
The blueberry is colored blue
The cranberry is colored red
The emerald is colored green
The frog is colored green
The blackboard is colored black
The pepper is colored black
The grass is colored
2024-07-23 12:18:46 root INFO     [order_1_approx] starting weight calculation for The pepper is colored black
The frog is colored green
The emerald is colored green
The grass is colored green
The blueberry is colored blue
The blackboard is colored black
The tea is colored black
The cranberry is colored
2024-07-23 12:18:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 12:22:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2891,  0.4497,  1.0039,  ..., -1.4727, -0.3459,  0.0289],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.2422,  3.8066, -1.2168,  ..., -1.1016,  5.6250,  0.7334],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0042, -0.0048, -0.0007,  ...,  0.0012,  0.0013, -0.0011],
        [-0.0022, -0.0012,  0.0052,  ...,  0.0024,  0.0115,  0.0004],
        [ 0.0010,  0.0065,  0.0019,  ..., -0.0002, -0.0051, -0.0022],
        ...,
        [ 0.0090, -0.0030, -0.0043,  ...,  0.0042, -0.0101,  0.0031],
        [ 0.0051, -0.0015, -0.0038,  ...,  0.0069, -0.0005,  0.0019],
        [-0.0105,  0.0071,  0.0006,  ..., -0.0014, -0.0034, -0.0026]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4512,  3.9180, -1.3809,  ..., -1.4238,  5.6758,  0.4863]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 12:22:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The pepper is colored black
The frog is colored green
The emerald is colored green
The grass is colored green
The blueberry is colored blue
The blackboard is colored black
The tea is colored black
The cranberry is colored
2024-07-23 12:22:46 root INFO     [order_1_approx] starting weight calculation for The tea is colored black
The blackboard is colored black
The blueberry is colored blue
The grass is colored green
The frog is colored green
The emerald is colored green
The cranberry is colored red
The pepper is colored
2024-07-23 12:22:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 12:26:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2681,  1.0059,  1.0156,  ...,  0.2988, -0.3975,  0.5469],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4668,  2.5820,  1.5566,  ..., -0.3921,  6.5977, -0.7266],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 9.7733e-03, -5.8784e-03, -4.5776e-03,  ...,  6.4898e-04,
          2.0256e-03, -5.1613e-03],
        [-7.6008e-04,  5.9624e-03, -1.6327e-03,  ...,  4.0970e-03,
          1.6699e-03, -4.8676e-03],
        [-5.5618e-03,  9.0714e-03, -7.9193e-03,  ..., -5.1804e-03,
         -5.3406e-05,  9.8801e-03],
        ...,
        [-2.6703e-05, -9.1553e-03, -6.0349e-03,  ...,  7.6065e-03,
         -1.7471e-02,  8.9645e-05],
        [-4.2801e-03, -2.0142e-03,  1.0941e-02,  ..., -4.3106e-04,
          2.2793e-04, -5.0774e-03],
        [ 1.6823e-03,  1.2703e-02,  3.6240e-05,  ..., -9.5978e-03,
          1.9131e-03, -2.2335e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6494,  2.9102,  1.9727,  ..., -0.3462,  6.2930, -0.8979]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 12:26:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The tea is colored black
The blackboard is colored black
The blueberry is colored blue
The grass is colored green
The frog is colored green
The emerald is colored green
The cranberry is colored red
The pepper is colored
2024-07-23 12:26:45 root INFO     total operator prediction time: 1915.9055933952332 seconds
2024-07-23 12:26:45 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on animal - sound
2024-07-23 12:26:45 root INFO     building operator animal - sound
2024-07-23 12:26:45 root INFO     [order_1_approx] starting weight calculation for The sound that a duck makes is called a quack
The sound that a fox makes is called a howl
The sound that a cattle makes is called a moo
The sound that a wasp makes is called a buzz
The sound that a elk makes is called a bellow
The sound that a fly makes is called a buzz
The sound that a dog makes is called a bark
The sound that a rat makes is called a
2024-07-23 12:26:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 12:30:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1655, -0.0791,  0.0098,  ...,  0.1611, -0.8105,  0.6396],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.2344, -0.0498,  1.0889,  ..., -0.2102, -2.8008, -7.0117],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0093,  0.0009,  0.0114,  ...,  0.0108, -0.0006,  0.0087],
        [-0.0110, -0.0047, -0.0071,  ...,  0.0072,  0.0065, -0.0142],
        [-0.0099,  0.0118,  0.0140,  ...,  0.0131,  0.0215,  0.0081],
        ...,
        [-0.0055, -0.0186,  0.0003,  ...,  0.0132, -0.0175,  0.0117],
        [-0.0036, -0.0101, -0.0048,  ..., -0.0194,  0.0023,  0.0065],
        [ 0.0035, -0.0060, -0.0092,  ...,  0.0100,  0.0081, -0.0129]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6055, -0.8364,  0.0000,  ...,  0.1130, -2.1016, -7.2344]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 12:30:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a duck makes is called a quack
The sound that a fox makes is called a howl
The sound that a cattle makes is called a moo
The sound that a wasp makes is called a buzz
The sound that a elk makes is called a bellow
The sound that a fly makes is called a buzz
The sound that a dog makes is called a bark
The sound that a rat makes is called a
2024-07-23 12:30:41 root INFO     [order_1_approx] starting weight calculation for The sound that a dog makes is called a bark
The sound that a wasp makes is called a buzz
The sound that a fox makes is called a howl
The sound that a elk makes is called a bellow
The sound that a duck makes is called a quack
The sound that a cattle makes is called a moo
The sound that a rat makes is called a squeak
The sound that a fly makes is called a
2024-07-23 12:30:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 12:34:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7212,  0.0964,  1.1074,  ..., -0.5356, -0.7549,  1.5781],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 7.6211,  1.0967,  4.6406,  ...,  2.3828, -2.2129, -5.1992],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0067, -0.0004,  0.0051,  ...,  0.0033,  0.0002,  0.0060],
        [-0.0053, -0.0086,  0.0018,  ...,  0.0045, -0.0074,  0.0044],
        [-0.0149, -0.0006,  0.0090,  ...,  0.0037, -0.0071, -0.0067],
        ...,
        [ 0.0010,  0.0004, -0.0013,  ...,  0.0126, -0.0096,  0.0032],
        [-0.0040, -0.0098, -0.0085,  ..., -0.0022,  0.0034,  0.0105],
        [-0.0030,  0.0115, -0.0012,  ..., -0.0061,  0.0011,  0.0231]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 7.1367,  0.5039,  4.5898,  ...,  3.0000, -2.0039, -4.4844]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 12:34:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a dog makes is called a bark
The sound that a wasp makes is called a buzz
The sound that a fox makes is called a howl
The sound that a elk makes is called a bellow
The sound that a duck makes is called a quack
The sound that a cattle makes is called a moo
The sound that a rat makes is called a squeak
The sound that a fly makes is called a
2024-07-23 12:34:38 root INFO     [order_1_approx] starting weight calculation for The sound that a fox makes is called a howl
The sound that a fly makes is called a buzz
The sound that a cattle makes is called a moo
The sound that a rat makes is called a squeak
The sound that a dog makes is called a bark
The sound that a elk makes is called a bellow
The sound that a wasp makes is called a buzz
The sound that a duck makes is called a
2024-07-23 12:34:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 12:38:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4502, -1.0029, -0.1299,  ...,  1.0674, -0.8789,  0.7690],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.0957,  4.0781,  0.2749,  ...,  2.9531, -1.3730, -5.2422],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0068, -0.0084,  0.0020,  ...,  0.0090,  0.0069, -0.0042],
        [-0.0040,  0.0087,  0.0105,  ...,  0.0020, -0.0029, -0.0103],
        [-0.0242, -0.0051,  0.0125,  ...,  0.0001,  0.0030,  0.0028],
        ...,
        [-0.0058, -0.0112, -0.0073,  ..., -0.0033, -0.0208,  0.0119],
        [-0.0137, -0.0204, -0.0068,  ..., -0.0023,  0.0043, -0.0078],
        [ 0.0033, -0.0058,  0.0203,  ...,  0.0098,  0.0004, -0.0095]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.9160,  3.9316,  1.1152,  ...,  3.8477, -1.6348, -4.5547]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 12:38:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a fox makes is called a howl
The sound that a fly makes is called a buzz
The sound that a cattle makes is called a moo
The sound that a rat makes is called a squeak
The sound that a dog makes is called a bark
The sound that a elk makes is called a bellow
The sound that a wasp makes is called a buzz
The sound that a duck makes is called a
2024-07-23 12:38:35 root INFO     [order_1_approx] starting weight calculation for The sound that a dog makes is called a bark
The sound that a duck makes is called a quack
The sound that a cattle makes is called a moo
The sound that a rat makes is called a squeak
The sound that a fox makes is called a howl
The sound that a elk makes is called a bellow
The sound that a fly makes is called a buzz
The sound that a wasp makes is called a
2024-07-23 12:38:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 12:42:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0447,  0.6978, -1.3740,  ...,  0.1604,  0.5508,  1.3711],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.6973, -2.4688,  2.8906,  ...,  1.7773,  1.6914, -3.1719],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.8127e-02,  3.3684e-03, -2.8076e-03,  ..., -7.3242e-03,
         -1.3130e-02, -9.8724e-03],
        [-1.1162e-02,  4.4708e-03,  1.7529e-03,  ..., -5.8517e-03,
         -3.7041e-03,  4.6158e-03],
        [ 1.2367e-02,  3.6201e-03, -6.0272e-04,  ..., -1.5993e-03,
          8.0261e-03,  4.8828e-03],
        ...,
        [ 4.1771e-04, -1.1459e-02, -8.9874e-03,  ...,  1.4954e-02,
          9.2983e-04, -1.8555e-02],
        [-2.4307e-02, -3.4454e-02, -1.8806e-03,  ...,  8.2855e-03,
          2.7573e-02,  3.2501e-02],
        [-3.1250e-02,  1.2299e-02, -2.0981e-05,  ..., -9.3079e-03,
          6.6757e-03,  1.6403e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.8398, -3.1289,  3.6270,  ...,  1.7295,  1.0117, -3.6660]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 12:42:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a dog makes is called a bark
The sound that a duck makes is called a quack
The sound that a cattle makes is called a moo
The sound that a rat makes is called a squeak
The sound that a fox makes is called a howl
The sound that a elk makes is called a bellow
The sound that a fly makes is called a buzz
The sound that a wasp makes is called a
2024-07-23 12:42:33 root INFO     [order_1_approx] starting weight calculation for The sound that a elk makes is called a bellow
The sound that a fly makes is called a buzz
The sound that a duck makes is called a quack
The sound that a rat makes is called a squeak
The sound that a fox makes is called a howl
The sound that a wasp makes is called a buzz
The sound that a dog makes is called a bark
The sound that a cattle makes is called a
2024-07-23 12:42:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 12:46:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0225,  0.4800,  0.6904,  ...,  1.0312,  0.4487,  1.4980],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.2734,  1.9902, -4.7773,  ..., -1.7129, -1.0820, -2.5977],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-3.7460e-03,  6.0158e-03, -8.2245e-03,  ...,  5.1117e-03,
         -4.8637e-04, -7.1716e-04],
        [ 5.5580e-03,  1.1391e-02, -9.0313e-04,  ...,  3.3245e-03,
          9.9335e-03, -3.0670e-03],
        [-1.0094e-02, -1.1749e-02,  3.3302e-03,  ..., -9.5673e-03,
          4.5357e-03,  8.1329e-03],
        ...,
        [ 1.9379e-03,  5.1651e-03, -7.9041e-03,  ...,  2.2144e-03,
          3.2177e-03,  1.4809e-02],
        [ 6.1607e-03,  4.3106e-03, -2.0523e-02,  ..., -8.8806e-03,
          1.8677e-02,  1.3412e-02],
        [ 1.4877e-02,  6.8665e-04, -7.5645e-03,  ..., -3.1185e-03,
         -9.9182e-05,  4.5776e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1328,  1.8486, -4.8711,  ..., -1.3115, -0.7427, -2.7812]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 12:46:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a elk makes is called a bellow
The sound that a fly makes is called a buzz
The sound that a duck makes is called a quack
The sound that a rat makes is called a squeak
The sound that a fox makes is called a howl
The sound that a wasp makes is called a buzz
The sound that a dog makes is called a bark
The sound that a cattle makes is called a
2024-07-23 12:46:30 root INFO     [order_1_approx] starting weight calculation for The sound that a dog makes is called a bark
The sound that a fly makes is called a buzz
The sound that a wasp makes is called a buzz
The sound that a duck makes is called a quack
The sound that a rat makes is called a squeak
The sound that a fox makes is called a howl
The sound that a cattle makes is called a moo
The sound that a elk makes is called a
2024-07-23 12:46:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 12:50:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.4932, 0.0996, 0.6089,  ..., 0.4517, 0.6460, 1.4004], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.8906,  0.9893, -4.7930,  ..., -2.2227, -2.0859,  0.5386],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0153, -0.0176,  0.0113,  ...,  0.0105,  0.0026, -0.0102],
        [-0.0101,  0.0133, -0.0082,  ...,  0.0071, -0.0012, -0.0062],
        [ 0.0020, -0.0037,  0.0144,  ...,  0.0006,  0.0001,  0.0141],
        ...,
        [ 0.0037, -0.0136, -0.0026,  ...,  0.0081, -0.0061,  0.0137],
        [-0.0143, -0.0099, -0.0111,  ...,  0.0031,  0.0053,  0.0113],
        [-0.0013,  0.0072, -0.0121,  ..., -0.0027,  0.0027, -0.0154]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5859,  0.1997, -4.2812,  ..., -1.3965, -1.5273,  1.5664]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 12:50:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a dog makes is called a bark
The sound that a fly makes is called a buzz
The sound that a wasp makes is called a buzz
The sound that a duck makes is called a quack
The sound that a rat makes is called a squeak
The sound that a fox makes is called a howl
The sound that a cattle makes is called a moo
The sound that a elk makes is called a
2024-07-23 12:50:25 root INFO     [order_1_approx] starting weight calculation for The sound that a rat makes is called a squeak
The sound that a cattle makes is called a moo
The sound that a fly makes is called a buzz
The sound that a dog makes is called a bark
The sound that a elk makes is called a bellow
The sound that a wasp makes is called a buzz
The sound that a duck makes is called a quack
The sound that a fox makes is called a
2024-07-23 12:50:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 12:54:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4412, -0.5249, -0.2549,  ...,  0.5430,  0.0544,  1.2080],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0505,  1.1309, -1.9688,  ..., -4.7461, -0.3135,  0.0583],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0018, -0.0016,  0.0063,  ...,  0.0012,  0.0107, -0.0094],
        [-0.0102,  0.0195, -0.0066,  ..., -0.0058, -0.0067, -0.0104],
        [ 0.0004, -0.0036,  0.0158,  ..., -0.0060, -0.0133, -0.0043],
        ...,
        [ 0.0137, -0.0108,  0.0113,  ...,  0.0161, -0.0359,  0.0065],
        [-0.0107, -0.0167, -0.0194,  ...,  0.0028,  0.0114,  0.0051],
        [-0.0007,  0.0022, -0.0212,  ..., -0.0149,  0.0169, -0.0016]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2307,  0.6699, -1.0391,  ..., -3.1289, -1.4639, -0.5859]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 12:54:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a rat makes is called a squeak
The sound that a cattle makes is called a moo
The sound that a fly makes is called a buzz
The sound that a dog makes is called a bark
The sound that a elk makes is called a bellow
The sound that a wasp makes is called a buzz
The sound that a duck makes is called a quack
The sound that a fox makes is called a
2024-07-23 12:54:21 root INFO     [order_1_approx] starting weight calculation for The sound that a cattle makes is called a moo
The sound that a fly makes is called a buzz
The sound that a elk makes is called a bellow
The sound that a fox makes is called a howl
The sound that a duck makes is called a quack
The sound that a wasp makes is called a buzz
The sound that a rat makes is called a squeak
The sound that a dog makes is called a
2024-07-23 12:54:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 12:58:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1104, -0.1956,  0.0935,  ...,  0.0852,  0.1443,  1.1895],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3379, -0.9536, -4.0859,  ..., -2.7930,  3.6602, -2.8789],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0224, -0.0062, -0.0025,  ..., -0.0083,  0.0011,  0.0099],
        [ 0.0171,  0.0189, -0.0003,  ..., -0.0044, -0.0018, -0.0108],
        [-0.0204, -0.0042, -0.0026,  ...,  0.0023,  0.0065,  0.0001],
        ...,
        [ 0.0111,  0.0029,  0.0006,  ...,  0.0051, -0.0045,  0.0075],
        [-0.0110, -0.0098,  0.0069,  ...,  0.0001, -0.0063,  0.0151],
        [ 0.0137,  0.0012, -0.0085,  ...,  0.0050, -0.0107, -0.0015]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4414, -1.4336, -3.7617,  ..., -2.6016,  4.3438, -2.4785]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 12:58:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a cattle makes is called a moo
The sound that a fly makes is called a buzz
The sound that a elk makes is called a bellow
The sound that a fox makes is called a howl
The sound that a duck makes is called a quack
The sound that a wasp makes is called a buzz
The sound that a rat makes is called a squeak
The sound that a dog makes is called a
2024-07-23 12:58:16 root INFO     total operator prediction time: 1891.0694756507874 seconds
2024-07-23 12:58:16 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on animal - youth
2024-07-23 12:58:16 root INFO     building operator animal - youth
2024-07-23 12:58:16 root INFO     [order_1_approx] starting weight calculation for The offspring of a sheep is referred to as a lamb
The offspring of a skunk is referred to as a kit
The offspring of a badger is referred to as a kit
The offspring of a fish is referred to as a fingerling
The offspring of a beaver is referred to as a kit
The offspring of a salmon is referred to as a smolt
The offspring of a dog is referred to as a puppy
The offspring of a bear is referred to as a
2024-07-23 12:58:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 13:02:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0332, -0.3120, -0.8203,  ...,  1.2705,  0.0664,  0.1288],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9131, -1.2754, -6.1055,  ...,  1.5293, -0.5293,  3.0312],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0010, -0.0133, -0.0015,  ...,  0.0114, -0.0149, -0.0097],
        [-0.0180, -0.0077,  0.0238,  ..., -0.0011,  0.0096, -0.0059],
        [-0.0097, -0.0050,  0.0132,  ..., -0.0040,  0.0069, -0.0144],
        ...,
        [ 0.0104, -0.0065,  0.0008,  ...,  0.0186, -0.0230,  0.0007],
        [ 0.0142, -0.0099, -0.0329,  ...,  0.0116, -0.0009, -0.0172],
        [-0.0016,  0.0054, -0.0086,  ..., -0.0040,  0.0072, -0.0146]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6543, -0.9487, -6.5508,  ...,  1.5234, -1.3262,  2.9727]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 13:02:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a sheep is referred to as a lamb
The offspring of a skunk is referred to as a kit
The offspring of a badger is referred to as a kit
The offspring of a fish is referred to as a fingerling
The offspring of a beaver is referred to as a kit
The offspring of a salmon is referred to as a smolt
The offspring of a dog is referred to as a puppy
The offspring of a bear is referred to as a
2024-07-23 13:02:15 root INFO     [order_1_approx] starting weight calculation for The offspring of a bear is referred to as a cub
The offspring of a skunk is referred to as a kit
The offspring of a fish is referred to as a fingerling
The offspring of a beaver is referred to as a kit
The offspring of a badger is referred to as a kit
The offspring of a salmon is referred to as a smolt
The offspring of a dog is referred to as a puppy
The offspring of a sheep is referred to as a
2024-07-23 13:02:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 13:06:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3911, -0.2347,  0.0439,  ...,  1.1328, -0.1033,  0.9741],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.1777,  0.6436, -8.9219,  ...,  1.7588,  1.5869,  2.2930],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0315, -0.0086,  0.0051,  ...,  0.0067, -0.0208, -0.0142],
        [-0.0051,  0.0156, -0.0133,  ..., -0.0083, -0.0019, -0.0207],
        [-0.0233,  0.0078, -0.0053,  ...,  0.0072,  0.0145, -0.0010],
        ...,
        [-0.0033,  0.0177, -0.0067,  ..., -0.0012, -0.0015,  0.0083],
        [-0.0023,  0.0129, -0.0203,  ..., -0.0040, -0.0125,  0.0018],
        [-0.0212,  0.0162, -0.0088,  ...,  0.0176,  0.0162, -0.0061]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7383,  0.0757, -8.7500,  ...,  0.7295,  1.7207,  2.1465]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 13:06:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a bear is referred to as a cub
The offspring of a skunk is referred to as a kit
The offspring of a fish is referred to as a fingerling
The offspring of a beaver is referred to as a kit
The offspring of a badger is referred to as a kit
The offspring of a salmon is referred to as a smolt
The offspring of a dog is referred to as a puppy
The offspring of a sheep is referred to as a
2024-07-23 13:06:12 root INFO     [order_1_approx] starting weight calculation for The offspring of a salmon is referred to as a smolt
The offspring of a fish is referred to as a fingerling
The offspring of a dog is referred to as a puppy
The offspring of a sheep is referred to as a lamb
The offspring of a bear is referred to as a cub
The offspring of a skunk is referred to as a kit
The offspring of a badger is referred to as a kit
The offspring of a beaver is referred to as a
2024-07-23 13:06:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 13:10:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8901, -0.6523, -1.4473,  ...,  1.9717,  0.0374,  1.2129],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2783,  0.2759, -6.0273,  ...,  1.9033,  0.6201,  3.6465],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0009, -0.0127,  0.0258,  ...,  0.0174,  0.0017, -0.0007],
        [-0.0166, -0.0130,  0.0288,  ..., -0.0020,  0.0152, -0.0173],
        [-0.0190,  0.0191,  0.0022,  ..., -0.0295,  0.0199, -0.0030],
        ...,
        [-0.0054, -0.0142, -0.0090,  ..., -0.0033, -0.0109, -0.0024],
        [-0.0174, -0.0159,  0.0003,  ..., -0.0129, -0.0221,  0.0100],
        [ 0.0154,  0.0163, -0.0209,  ..., -0.0005,  0.0153, -0.0126]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0156,  0.5117, -5.2734,  ...,  2.1953,  1.0352,  3.3320]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 13:10:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a salmon is referred to as a smolt
The offspring of a fish is referred to as a fingerling
The offspring of a dog is referred to as a puppy
The offspring of a sheep is referred to as a lamb
The offspring of a bear is referred to as a cub
The offspring of a skunk is referred to as a kit
The offspring of a badger is referred to as a kit
The offspring of a beaver is referred to as a
2024-07-23 13:10:10 root INFO     [order_1_approx] starting weight calculation for The offspring of a sheep is referred to as a lamb
The offspring of a skunk is referred to as a kit
The offspring of a dog is referred to as a puppy
The offspring of a salmon is referred to as a smolt
The offspring of a fish is referred to as a fingerling
The offspring of a bear is referred to as a cub
The offspring of a beaver is referred to as a kit
The offspring of a badger is referred to as a
2024-07-23 13:10:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 13:14:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8218, -1.4434, -0.1541,  ...,  1.1211, -1.0176,  1.8838],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3887, -3.1484, -3.7188,  ..., -1.4258,  1.4521,  1.8281],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0065, -0.0014,  0.0145,  ...,  0.0013, -0.0047, -0.0133],
        [ 0.0045, -0.0059,  0.0301,  ...,  0.0085,  0.0169, -0.0145],
        [-0.0137,  0.0191,  0.0005,  ..., -0.0120,  0.0259,  0.0045],
        ...,
        [ 0.0133,  0.0006, -0.0122,  ...,  0.0175, -0.0177,  0.0009],
        [ 0.0066, -0.0170, -0.0122,  ..., -0.0016, -0.0243, -0.0159],
        [ 0.0047, -0.0086,  0.0013,  ...,  0.0047,  0.0105, -0.0069]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3066, -3.0859, -4.0039,  ..., -1.0430,  2.2773,  1.1699]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 13:14:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a sheep is referred to as a lamb
The offspring of a skunk is referred to as a kit
The offspring of a dog is referred to as a puppy
The offspring of a salmon is referred to as a smolt
The offspring of a fish is referred to as a fingerling
The offspring of a bear is referred to as a cub
The offspring of a beaver is referred to as a kit
The offspring of a badger is referred to as a
2024-07-23 13:14:09 root INFO     [order_1_approx] starting weight calculation for The offspring of a salmon is referred to as a smolt
The offspring of a sheep is referred to as a lamb
The offspring of a bear is referred to as a cub
The offspring of a beaver is referred to as a kit
The offspring of a dog is referred to as a puppy
The offspring of a badger is referred to as a kit
The offspring of a fish is referred to as a fingerling
The offspring of a skunk is referred to as a
2024-07-23 13:14:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 13:18:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.8936,  0.8101, -1.6836,  ...,  1.6025,  0.3899,  1.3760],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6035, -1.2451, -2.0703,  ...,  0.2661, -1.9258,  3.9473],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 5.4779e-03, -1.1932e-02,  1.0094e-02,  ...,  2.1408e-02,
         -2.1713e-02,  6.5613e-03],
        [ 1.0025e-02, -1.2131e-02,  2.9785e-02,  ...,  1.2527e-02,
         -4.4823e-05, -1.7471e-03],
        [-1.2932e-02,  1.6602e-02,  6.7596e-03,  ...,  8.3847e-03,
          2.0782e-02,  6.4125e-03],
        ...,
        [ 5.0964e-03, -2.5208e-02, -9.7504e-03,  ...,  1.4763e-03,
         -7.5798e-03, -8.1711e-03],
        [ 3.0937e-03, -4.9438e-03, -2.0199e-03,  ..., -3.0670e-03,
          7.8106e-04,  9.2773e-03],
        [ 2.5520e-03, -7.4196e-04, -6.2103e-03,  ...,  3.1223e-03,
          1.0284e-02, -9.6283e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5811, -0.6846, -2.9805,  ...,  0.6074, -1.4473,  4.1133]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 13:18:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a salmon is referred to as a smolt
The offspring of a sheep is referred to as a lamb
The offspring of a bear is referred to as a cub
The offspring of a beaver is referred to as a kit
The offspring of a dog is referred to as a puppy
The offspring of a badger is referred to as a kit
The offspring of a fish is referred to as a fingerling
The offspring of a skunk is referred to as a
2024-07-23 13:18:09 root INFO     [order_1_approx] starting weight calculation for The offspring of a dog is referred to as a puppy
The offspring of a skunk is referred to as a kit
The offspring of a beaver is referred to as a kit
The offspring of a fish is referred to as a fingerling
The offspring of a bear is referred to as a cub
The offspring of a sheep is referred to as a lamb
The offspring of a badger is referred to as a kit
The offspring of a salmon is referred to as a
2024-07-23 13:18:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 13:22:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 9.2725e-01, -4.5557e-01,  1.0986e-03,  ...,  2.2876e-01,
         2.9785e-01,  1.4814e+00], device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5898, -2.7480, -0.2363,  ...,  1.1514,  1.3623,  1.7090],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0210, -0.0078,  0.0197,  ..., -0.0015, -0.0005, -0.0062],
        [ 0.0013,  0.0070, -0.0051,  ..., -0.0034,  0.0210, -0.0101],
        [-0.0036,  0.0154, -0.0149,  ..., -0.0080,  0.0046,  0.0050],
        ...,
        [ 0.0201,  0.0075, -0.0136,  ...,  0.0102, -0.0362, -0.0056],
        [ 0.0028,  0.0069,  0.0004,  ..., -0.0101,  0.0148, -0.0074],
        [-0.0134, -0.0082, -0.0014,  ...,  0.0094,  0.0124, -0.0002]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3213, -1.9541, -0.5361,  ...,  1.6318,  0.9355,  0.8965]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 13:22:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a dog is referred to as a puppy
The offspring of a skunk is referred to as a kit
The offspring of a beaver is referred to as a kit
The offspring of a fish is referred to as a fingerling
The offspring of a bear is referred to as a cub
The offspring of a sheep is referred to as a lamb
The offspring of a badger is referred to as a kit
The offspring of a salmon is referred to as a
2024-07-23 13:22:08 root INFO     [order_1_approx] starting weight calculation for The offspring of a badger is referred to as a kit
The offspring of a sheep is referred to as a lamb
The offspring of a beaver is referred to as a kit
The offspring of a fish is referred to as a fingerling
The offspring of a skunk is referred to as a kit
The offspring of a salmon is referred to as a smolt
The offspring of a bear is referred to as a cub
The offspring of a dog is referred to as a
2024-07-23 13:22:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 13:26:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9355, -0.9097, -0.4927,  ...,  0.5825, -0.4094,  1.0967],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4824, -3.7773, -3.0742,  ...,  0.5972,  4.4805, -0.9780],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0062, -0.0140, -0.0012,  ...,  0.0051, -0.0127, -0.0112],
        [ 0.0066,  0.0013,  0.0272,  ...,  0.0054,  0.0059, -0.0039],
        [ 0.0025,  0.0043, -0.0043,  ..., -0.0019,  0.0069,  0.0158],
        ...,
        [-0.0083, -0.0087,  0.0028,  ...,  0.0139, -0.0194,  0.0113],
        [ 0.0073,  0.0214, -0.0048,  ..., -0.0091, -0.0228,  0.0120],
        [ 0.0007, -0.0078,  0.0060,  ...,  0.0183, -0.0013, -0.0118]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5527, -3.6777, -3.0156,  ...,  0.5117,  5.3359, -0.5049]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 13:26:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a badger is referred to as a kit
The offspring of a sheep is referred to as a lamb
The offspring of a beaver is referred to as a kit
The offspring of a fish is referred to as a fingerling
The offspring of a skunk is referred to as a kit
The offspring of a salmon is referred to as a smolt
The offspring of a bear is referred to as a cub
The offspring of a dog is referred to as a
2024-07-23 13:26:07 root INFO     [order_1_approx] starting weight calculation for The offspring of a dog is referred to as a puppy
The offspring of a badger is referred to as a kit
The offspring of a beaver is referred to as a kit
The offspring of a salmon is referred to as a smolt
The offspring of a bear is referred to as a cub
The offspring of a skunk is referred to as a kit
The offspring of a sheep is referred to as a lamb
The offspring of a fish is referred to as a
2024-07-23 13:26:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 13:30:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2174,  0.4131,  0.3779,  ..., -0.3394, -0.3384,  1.0176],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2568, -0.4419, -1.2432,  ...,  1.3311,  0.1064,  1.2578],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0076, -0.0061, -0.0026,  ...,  0.0073,  0.0102,  0.0051],
        [-0.0093, -0.0113,  0.0018,  ..., -0.0101, -0.0169, -0.0096],
        [-0.0023,  0.0002, -0.0052,  ..., -0.0143,  0.0060,  0.0177],
        ...,
        [ 0.0036, -0.0032, -0.0075,  ...,  0.0121, -0.0158,  0.0010],
        [ 0.0031,  0.0216,  0.0047,  ..., -0.0041,  0.0099,  0.0048],
        [-0.0078, -0.0128,  0.0081,  ..., -0.0027,  0.0039,  0.0077]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6924,  0.1802, -1.2461,  ...,  1.5176, -0.6440,  1.0576]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 13:30:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a dog is referred to as a puppy
The offspring of a badger is referred to as a kit
The offspring of a beaver is referred to as a kit
The offspring of a salmon is referred to as a smolt
The offspring of a bear is referred to as a cub
The offspring of a skunk is referred to as a kit
The offspring of a sheep is referred to as a lamb
The offspring of a fish is referred to as a
2024-07-23 13:30:06 root INFO     total operator prediction time: 1910.1324679851532 seconds
2024-07-23 13:30:06 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on UK_city - county
2024-07-23 13:30:06 root INFO     building operator UK_city - county
2024-07-23 13:30:06 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of southampton is in the county of hampshire
In the United Kingdom, the city of bradford is in the county of yorkshire
In the United Kingdom, the city of liverpool is in the county of lancashire
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of exeter is in the county of devon
In the United Kingdom, the city of inverness is in the county of highlands
In the United Kingdom, the city of york is in the county of yorkshire
In the United Kingdom, the city of birmingham is in the county of
2024-07-23 13:30:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 13:34:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8564, -0.4031,  0.4551,  ...,  0.4236, -0.3462,  0.1377],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4546, -5.2188, -7.1758,  ...,  0.3618,  1.2881, -1.6797],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.2476e-02, -4.0131e-03,  1.9646e-04,  ...,  1.3634e-02,
          1.1169e-02, -5.3864e-03],
        [ 1.2756e-02, -2.6073e-03, -2.4475e-02,  ..., -2.6560e-04,
         -1.2985e-02,  7.9155e-05],
        [ 3.0273e-02,  4.2358e-02, -7.5722e-03,  ..., -8.1635e-03,
         -1.9562e-02,  6.2561e-03],
        ...,
        [-1.2732e-03,  7.9575e-03, -5.0812e-03,  ..., -4.6158e-03,
          6.4621e-03, -2.8763e-03],
        [ 1.5778e-02,  8.3160e-03, -1.4641e-02,  ..., -1.0117e-02,
          7.7248e-03,  1.6594e-03],
        [-1.4824e-02, -2.9236e-02,  1.2045e-03,  ...,  1.4984e-02,
          1.0330e-02, -1.2314e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3215, -4.5078, -6.4180,  ..., -0.1938,  1.6162, -2.3223]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 13:34:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of southampton is in the county of hampshire
In the United Kingdom, the city of bradford is in the county of yorkshire
In the United Kingdom, the city of liverpool is in the county of lancashire
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of exeter is in the county of devon
In the United Kingdom, the city of inverness is in the county of highlands
In the United Kingdom, the city of york is in the county of yorkshire
In the United Kingdom, the city of birmingham is in the county of
2024-07-23 13:34:05 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of southampton is in the county of hampshire
In the United Kingdom, the city of york is in the county of yorkshire
In the United Kingdom, the city of liverpool is in the county of lancashire
In the United Kingdom, the city of bradford is in the county of yorkshire
In the United Kingdom, the city of exeter is in the county of devon
In the United Kingdom, the city of birmingham is in the county of midlands
In the United Kingdom, the city of inverness is in the county of
2024-07-23 13:34:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 13:37:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.1357, 0.2749, 0.3503,  ..., 1.2598, 1.0928, 0.6094], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.4023, -0.2422, -7.4453,  ...,  2.0977,  1.2334, -0.6748],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0030, -0.0143, -0.0006,  ..., -0.0073,  0.0179, -0.0173],
        [ 0.0074, -0.0196, -0.0177,  ...,  0.0121,  0.0079, -0.0069],
        [ 0.0072,  0.0267, -0.0280,  ...,  0.0067,  0.0009,  0.0108],
        ...,
        [-0.0015, -0.0108,  0.0030,  ...,  0.0007, -0.0122,  0.0075],
        [ 0.0197,  0.0130, -0.0069,  ..., -0.0145, -0.0131,  0.0193],
        [ 0.0053, -0.0072, -0.0060,  ...,  0.0114,  0.0204, -0.0272]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.5977, -0.2229, -7.8086,  ...,  2.2578,  1.2158, -0.1416]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 13:37:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of southampton is in the county of hampshire
In the United Kingdom, the city of york is in the county of yorkshire
In the United Kingdom, the city of liverpool is in the county of lancashire
In the United Kingdom, the city of bradford is in the county of yorkshire
In the United Kingdom, the city of exeter is in the county of devon
In the United Kingdom, the city of birmingham is in the county of midlands
In the United Kingdom, the city of inverness is in the county of
2024-07-23 13:37:59 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of bradford is in the county of yorkshire
In the United Kingdom, the city of inverness is in the county of highlands
In the United Kingdom, the city of southampton is in the county of hampshire
In the United Kingdom, the city of york is in the county of yorkshire
In the United Kingdom, the city of birmingham is in the county of midlands
In the United Kingdom, the city of liverpool is in the county of lancashire
In the United Kingdom, the city of exeter is in the county of devon
In the United Kingdom, the city of sheffield is in the county of
2024-07-23 13:37:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 13:41:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9189,  0.5737,  1.4219,  ...,  0.9961,  0.4150,  0.9160],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3281, -3.3242, -5.0859,  ...,  3.4414,  3.0566,  1.3164],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.6688e-02, -1.1581e-02,  3.2349e-03,  ...,  1.4099e-02,
          9.1324e-03,  2.4796e-04],
        [-3.4714e-03,  1.8406e-03, -9.6130e-03,  ..., -3.8815e-03,
          2.5749e-05, -5.4588e-03],
        [ 3.0807e-02,  2.9083e-02, -6.1874e-03,  ..., -9.2010e-03,
         -1.9745e-02, -1.2123e-02],
        ...,
        [-2.3834e-02, -1.1757e-02,  2.0203e-02,  ...,  1.2161e-02,
         -9.6588e-03, -6.9733e-03],
        [-1.3819e-03,  2.1400e-03, -2.2507e-04,  ..., -1.0231e-02,
          1.9547e-02, -9.7961e-03],
        [-1.8173e-02, -2.1927e-02, -4.3030e-03,  ...,  9.3918e-03,
         -9.9411e-03, -8.6517e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9775, -3.1250, -4.6250,  ...,  3.2246,  3.0957,  1.3525]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 13:41:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of bradford is in the county of yorkshire
In the United Kingdom, the city of inverness is in the county of highlands
In the United Kingdom, the city of southampton is in the county of hampshire
In the United Kingdom, the city of york is in the county of yorkshire
In the United Kingdom, the city of birmingham is in the county of midlands
In the United Kingdom, the city of liverpool is in the county of lancashire
In the United Kingdom, the city of exeter is in the county of devon
In the United Kingdom, the city of sheffield is in the county of
2024-07-23 13:42:00 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of inverness is in the county of highlands
In the United Kingdom, the city of southampton is in the county of hampshire
In the United Kingdom, the city of bradford is in the county of yorkshire
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of york is in the county of yorkshire
In the United Kingdom, the city of exeter is in the county of devon
In the United Kingdom, the city of birmingham is in the county of midlands
In the United Kingdom, the city of liverpool is in the county of
2024-07-23 13:42:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 13:45:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.3174,  0.0909,  0.8364,  ..., -0.1653, -0.9468,  0.0259],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0610, -2.8672, -6.3203,  ..., -0.4443,  4.2930, -3.5449],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0257, -0.0088, -0.0013,  ...,  0.0143,  0.0089, -0.0052],
        [ 0.0045, -0.0102, -0.0130,  ..., -0.0021, -0.0182,  0.0015],
        [ 0.0041,  0.0408,  0.0009,  ...,  0.0116,  0.0070,  0.0019],
        ...,
        [-0.0039, -0.0020, -0.0027,  ..., -0.0128, -0.0086,  0.0071],
        [ 0.0084, -0.0107, -0.0060,  ..., -0.0177,  0.0093,  0.0039],
        [-0.0170, -0.0296,  0.0074,  ..., -0.0024, -0.0138, -0.0108]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1641, -3.1934, -5.9141,  ..., -0.5137,  4.0352, -3.8535]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 13:45:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of inverness is in the county of highlands
In the United Kingdom, the city of southampton is in the county of hampshire
In the United Kingdom, the city of bradford is in the county of yorkshire
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of york is in the county of yorkshire
In the United Kingdom, the city of exeter is in the county of devon
In the United Kingdom, the city of birmingham is in the county of midlands
In the United Kingdom, the city of liverpool is in the county of
2024-07-23 13:45:59 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of birmingham is in the county of midlands
In the United Kingdom, the city of exeter is in the county of devon
In the United Kingdom, the city of liverpool is in the county of lancashire
In the United Kingdom, the city of inverness is in the county of highlands
In the United Kingdom, the city of york is in the county of yorkshire
In the United Kingdom, the city of southampton is in the county of hampshire
In the United Kingdom, the city of bradford is in the county of
2024-07-23 13:45:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 13:49:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2302, -0.1066,  0.9644,  ...,  0.8491,  0.9692,  0.0225],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.2285, -4.0938, -6.4648,  ...,  0.5767,  4.7812, -3.0078],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0256, -0.0064,  0.0101,  ..., -0.0030,  0.0117, -0.0099],
        [ 0.0305, -0.0058, -0.0155,  ...,  0.0311, -0.0152,  0.0032],
        [ 0.0139,  0.0296, -0.0036,  ...,  0.0026, -0.0067,  0.0080],
        ...,
        [-0.0440, -0.0081,  0.0285,  ...,  0.0188,  0.0210,  0.0192],
        [-0.0034, -0.0045, -0.0086,  ..., -0.0073, -0.0059,  0.0121],
        [-0.0083, -0.0236, -0.0303,  ...,  0.0343, -0.0197, -0.0033]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3125, -2.8066, -5.3477,  ..., -0.1831,  5.0586, -3.8926]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 13:49:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of birmingham is in the county of midlands
In the United Kingdom, the city of exeter is in the county of devon
In the United Kingdom, the city of liverpool is in the county of lancashire
In the United Kingdom, the city of inverness is in the county of highlands
In the United Kingdom, the city of york is in the county of yorkshire
In the United Kingdom, the city of southampton is in the county of hampshire
In the United Kingdom, the city of bradford is in the county of
2024-07-23 13:49:58 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of birmingham is in the county of midlands
In the United Kingdom, the city of exeter is in the county of devon
In the United Kingdom, the city of inverness is in the county of highlands
In the United Kingdom, the city of york is in the county of yorkshire
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of liverpool is in the county of lancashire
In the United Kingdom, the city of bradford is in the county of yorkshire
In the United Kingdom, the city of southampton is in the county of
2024-07-23 13:49:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 13:53:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1311, -0.2576,  0.5938,  ..., -0.2720, -0.9443,  0.7568],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0928, -2.6504, -7.5195,  ..., -0.7109,  5.8320,  4.0781],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0245, -0.0165, -0.0065,  ...,  0.0016, -0.0058, -0.0164],
        [ 0.0070, -0.0124, -0.0100,  ...,  0.0195, -0.0092, -0.0102],
        [ 0.0241,  0.0295, -0.0242,  ..., -0.0079, -0.0202, -0.0010],
        ...,
        [-0.0206, -0.0012,  0.0159,  ..., -0.0245, -0.0141,  0.0143],
        [ 0.0072,  0.0077,  0.0061,  ...,  0.0121,  0.0127,  0.0225],
        [-0.0326, -0.0099,  0.0007,  ...,  0.0056,  0.0233, -0.0240]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2109, -1.4717, -7.3594,  ..., -1.3496,  5.5273,  3.6719]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 13:53:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of birmingham is in the county of midlands
In the United Kingdom, the city of exeter is in the county of devon
In the United Kingdom, the city of inverness is in the county of highlands
In the United Kingdom, the city of york is in the county of yorkshire
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of liverpool is in the county of lancashire
In the United Kingdom, the city of bradford is in the county of yorkshire
In the United Kingdom, the city of southampton is in the county of
2024-07-23 13:53:57 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of inverness is in the county of highlands
In the United Kingdom, the city of liverpool is in the county of lancashire
In the United Kingdom, the city of york is in the county of yorkshire
In the United Kingdom, the city of birmingham is in the county of midlands
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of bradford is in the county of yorkshire
In the United Kingdom, the city of southampton is in the county of hampshire
In the United Kingdom, the city of exeter is in the county of
2024-07-23 13:53:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 13:57:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3071,  0.1676, -0.0942,  ...,  0.8955,  0.1079,  0.9043],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7822,  0.8936, -7.6641,  ..., -2.1250,  4.4883, -0.5161],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-4.1321e-02, -1.3535e-02, -5.1155e-03,  ..., -1.8005e-02,
          9.1553e-05,  5.7983e-03],
        [-3.5400e-03, -8.5678e-03, -1.3725e-02,  ..., -1.7181e-02,
         -3.7384e-04,  7.8964e-03],
        [ 3.3264e-03, -1.3145e-02, -3.3894e-03,  ...,  2.9984e-03,
         -4.1695e-03, -1.0361e-02],
        ...,
        [-1.5778e-02,  5.1880e-04, -2.6321e-02,  ..., -2.7222e-02,
         -3.2867e-02, -3.4237e-03],
        [-2.5650e-02,  7.9498e-03, -7.0114e-03,  ..., -1.1688e-02,
         -9.3079e-03,  1.4404e-02],
        [ 7.1182e-03, -3.7613e-03, -6.6299e-03,  ..., -7.2327e-03,
         -4.0321e-03, -1.5266e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7588,  1.8359, -7.9180,  ..., -2.0449,  5.0547, -1.2432]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 13:57:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of inverness is in the county of highlands
In the United Kingdom, the city of liverpool is in the county of lancashire
In the United Kingdom, the city of york is in the county of yorkshire
In the United Kingdom, the city of birmingham is in the county of midlands
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of bradford is in the county of yorkshire
In the United Kingdom, the city of southampton is in the county of hampshire
In the United Kingdom, the city of exeter is in the county of
2024-07-23 13:57:55 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of inverness is in the county of highlands
In the United Kingdom, the city of exeter is in the county of devon
In the United Kingdom, the city of birmingham is in the county of midlands
In the United Kingdom, the city of bradford is in the county of yorkshire
In the United Kingdom, the city of southampton is in the county of hampshire
In the United Kingdom, the city of liverpool is in the county of lancashire
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of york is in the county of
2024-07-23 13:57:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 14:01:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6709, -1.0703,  0.8740,  ...,  0.2786, -0.7285,  0.7227],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6719, -2.8730, -6.3008,  ...,  3.9141,  3.1367,  0.1461],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0273, -0.0125,  0.0078,  ...,  0.0087,  0.0145, -0.0193],
        [-0.0048, -0.0076,  0.0043,  ...,  0.0015,  0.0066, -0.0284],
        [ 0.0408,  0.0022, -0.0182,  ...,  0.0023, -0.0034,  0.0124],
        ...,
        [-0.0171, -0.0076,  0.0115,  ...,  0.0157, -0.0092, -0.0034],
        [ 0.0118,  0.0081, -0.0010,  ..., -0.0089,  0.0017, -0.0023],
        [ 0.0037, -0.0160, -0.0132,  ...,  0.0105, -0.0056, -0.0018]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5234, -1.5898, -6.4219,  ...,  3.9414,  2.9277,  0.0155]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 14:01:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of inverness is in the county of highlands
In the United Kingdom, the city of exeter is in the county of devon
In the United Kingdom, the city of birmingham is in the county of midlands
In the United Kingdom, the city of bradford is in the county of yorkshire
In the United Kingdom, the city of southampton is in the county of hampshire
In the United Kingdom, the city of liverpool is in the county of lancashire
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of york is in the county of
2024-07-23 14:01:49 root INFO     total operator prediction time: 1902.8373923301697 seconds
