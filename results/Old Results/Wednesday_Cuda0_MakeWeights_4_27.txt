2024-07-17 17:09:09 root INFO     loading model + tokenizer
2024-07-17 17:09:26 root INFO     model + tokenizer loaded
2024-07-17 17:09:26 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - part
2024-07-17 17:09:26 root INFO     building operator meronyms - part
2024-07-17 17:09:27 root INFO     [order_1_approx] starting weight calculation for A part of a litre is a millilitre
A part of a piano is a keyboard
A part of a academia is a college
A part of a tonne is a kilogram
A part of a church is a altar
A part of a typewriter is a keyboard
A part of a railcar is a suspension
A part of a bus is a
2024-07-17 17:09:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 17:13:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1538,  0.6094, -0.6128,  ...,  1.4170,  0.2842, -0.1536],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.0781, -2.5527, -0.2764,  ..., -2.2168,  1.2793, -0.7134],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0125, -0.0119,  0.0390,  ..., -0.0389, -0.0230, -0.0099],
        [-0.0040,  0.0062,  0.0056,  ...,  0.0182, -0.0174, -0.0140],
        [-0.0141,  0.0213, -0.0015,  ..., -0.0210,  0.0186,  0.0048],
        ...,
        [ 0.0061, -0.0118, -0.0114,  ...,  0.0123, -0.0088,  0.0042],
        [-0.0035, -0.0001, -0.0189,  ...,  0.0163, -0.0063,  0.0017],
        [ 0.0105, -0.0144,  0.0119,  ..., -0.0206, -0.0129, -0.0067]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5566, -1.8652, -0.5332,  ..., -2.3672,  0.5571,  0.1802]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 17:13:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a litre is a millilitre
A part of a piano is a keyboard
A part of a academia is a college
A part of a tonne is a kilogram
A part of a church is a altar
A part of a typewriter is a keyboard
A part of a railcar is a suspension
A part of a bus is a
2024-07-17 17:13:35 root INFO     [order_1_approx] starting weight calculation for A part of a piano is a keyboard
A part of a church is a altar
A part of a railcar is a suspension
A part of a academia is a college
A part of a bus is a seats
A part of a typewriter is a keyboard
A part of a litre is a millilitre
A part of a tonne is a
2024-07-17 17:13:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 17:17:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5576,  0.1383,  0.0568,  ..., -0.4778, -0.0835, -0.6777],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.8203, -2.9199, -4.3281,  ..., -1.7773, -4.2773,  2.0430],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0086, -0.0173, -0.0073,  ...,  0.0026,  0.0005, -0.0087],
        [ 0.0041, -0.0066,  0.0138,  ..., -0.0117, -0.0045, -0.0017],
        [-0.0099,  0.0097,  0.0056,  ...,  0.0121,  0.0055, -0.0043],
        ...,
        [ 0.0046, -0.0122,  0.0110,  ...,  0.0096, -0.0041,  0.0192],
        [-0.0050, -0.0002, -0.0130,  ...,  0.0099,  0.0139,  0.0076],
        [ 0.0030, -0.0240,  0.0118,  ..., -0.0084,  0.0077,  0.0104]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.9082, -4.0156, -4.8008,  ..., -2.2949, -3.9609,  1.9023]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 17:17:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a piano is a keyboard
A part of a church is a altar
A part of a railcar is a suspension
A part of a academia is a college
A part of a bus is a seats
A part of a typewriter is a keyboard
A part of a litre is a millilitre
A part of a tonne is a
2024-07-17 17:17:44 root INFO     [order_1_approx] starting weight calculation for A part of a typewriter is a keyboard
A part of a litre is a millilitre
A part of a bus is a seats
A part of a academia is a college
A part of a piano is a keyboard
A part of a railcar is a suspension
A part of a tonne is a kilogram
A part of a church is a
2024-07-17 17:17:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 17:21:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1548, -0.3970,  0.0471,  ...,  1.0039, -0.5396, -0.5254],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1621, -0.8311,  4.0859,  ..., -2.0039, -1.8965, -0.7104],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-6.6872e-03, -1.2894e-02,  1.3046e-03,  ...,  2.5711e-03,
         -8.4457e-03, -2.5604e-02],
        [ 1.3290e-02, -2.7618e-03, -2.8477e-03,  ...,  1.2764e-02,
         -7.5264e-03, -1.2756e-02],
        [-1.1047e-02,  1.3710e-02, -6.0272e-03,  ...,  3.6621e-04,
          3.6316e-02,  1.6296e-02],
        ...,
        [-9.9792e-03, -7.8888e-03, -3.3951e-04,  ...,  3.3875e-03,
         -9.9487e-03, -2.1439e-03],
        [ 2.1835e-02, -2.6817e-03, -8.0109e-05,  ...,  1.8173e-02,
         -3.9444e-03,  2.7580e-03],
        [-1.2520e-02, -7.4692e-03, -3.7155e-03,  ..., -6.8398e-03,
          1.8101e-03,  9.1553e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6763, -0.0801,  2.7266,  ..., -2.1660, -2.4219, -0.0142]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 17:21:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a typewriter is a keyboard
A part of a litre is a millilitre
A part of a bus is a seats
A part of a academia is a college
A part of a piano is a keyboard
A part of a railcar is a suspension
A part of a tonne is a kilogram
A part of a church is a
2024-07-17 17:21:51 root INFO     [order_1_approx] starting weight calculation for A part of a tonne is a kilogram
A part of a litre is a millilitre
A part of a bus is a seats
A part of a typewriter is a keyboard
A part of a piano is a keyboard
A part of a academia is a college
A part of a church is a altar
A part of a railcar is a
2024-07-17 17:21:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 17:25:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1582, -0.5508, -0.1589,  ...,  0.3020, -0.5596,  0.2495],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.6621, -0.0345,  0.1533,  ..., -0.1963,  2.3594,  0.0210],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0031, -0.0261,  0.0085,  ..., -0.0002,  0.0036, -0.0256],
        [ 0.0101,  0.0165,  0.0119,  ..., -0.0030,  0.0043, -0.0066],
        [-0.0065, -0.0127,  0.0226,  ..., -0.0209,  0.0032,  0.0233],
        ...,
        [ 0.0043, -0.0084,  0.0122,  ...,  0.0096,  0.0087, -0.0074],
        [ 0.0147, -0.0172,  0.0029,  ...,  0.0037,  0.0120,  0.0018],
        [-0.0169, -0.0109,  0.0236,  ..., -0.0098,  0.0028,  0.0267]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.9619, -0.3516,  0.4253,  ...,  0.6987,  3.5039,  0.9761]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 17:25:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a tonne is a kilogram
A part of a litre is a millilitre
A part of a bus is a seats
A part of a typewriter is a keyboard
A part of a piano is a keyboard
A part of a academia is a college
A part of a church is a altar
A part of a railcar is a
2024-07-17 17:25:58 root INFO     [order_1_approx] starting weight calculation for A part of a litre is a millilitre
A part of a piano is a keyboard
A part of a bus is a seats
A part of a railcar is a suspension
A part of a church is a altar
A part of a tonne is a kilogram
A part of a typewriter is a keyboard
A part of a academia is a
2024-07-17 17:25:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 17:30:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0120,  1.2383,  0.4492,  ..., -0.4961, -0.4690,  0.0508],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1025,  1.2998,  0.5703,  ...,  0.3604, -1.1934,  1.0293],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0108, -0.0143,  0.0027,  ..., -0.0078, -0.0048, -0.0108],
        [-0.0043, -0.0060,  0.0099,  ...,  0.0167, -0.0257, -0.0063],
        [ 0.0105,  0.0007, -0.0033,  ..., -0.0081,  0.0010,  0.0085],
        ...,
        [ 0.0017, -0.0264, -0.0044,  ..., -0.0105,  0.0077,  0.0062],
        [ 0.0292,  0.0146,  0.0053,  ...,  0.0062,  0.0053,  0.0063],
        [-0.0192, -0.0092, -0.0068,  ..., -0.0095,  0.0298, -0.0048]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0205,  1.1367, -0.4854,  ...,  0.6221, -1.5625,  1.8184]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 17:30:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a litre is a millilitre
A part of a piano is a keyboard
A part of a bus is a seats
A part of a railcar is a suspension
A part of a church is a altar
A part of a tonne is a kilogram
A part of a typewriter is a keyboard
A part of a academia is a
2024-07-17 17:30:05 root INFO     [order_1_approx] starting weight calculation for A part of a railcar is a suspension
A part of a tonne is a kilogram
A part of a typewriter is a keyboard
A part of a bus is a seats
A part of a piano is a keyboard
A part of a academia is a college
A part of a church is a altar
A part of a litre is a
2024-07-17 17:30:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 17:34:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4028,  0.2698, -0.7036,  ...,  0.2551,  0.1748,  0.3164],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9805,  1.7207, -1.0254,  ..., -0.1680, -3.1914,  1.6475],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0086, -0.0101, -0.0080,  ..., -0.0013,  0.0130,  0.0141],
        [-0.0172, -0.0080, -0.0083,  ..., -0.0076, -0.0046, -0.0036],
        [ 0.0090, -0.0144, -0.0058,  ...,  0.0108,  0.0048,  0.0110],
        ...,
        [-0.0216, -0.0033,  0.0085,  ...,  0.0059, -0.0255,  0.0147],
        [ 0.0193, -0.0105,  0.0152,  ...,  0.0006,  0.0182,  0.0043],
        [-0.0030,  0.0096,  0.0127,  ..., -0.0044,  0.0265,  0.0002]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1758,  2.0078, -1.0391,  ...,  1.1299, -2.7734,  1.5928]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 17:34:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a railcar is a suspension
A part of a tonne is a kilogram
A part of a typewriter is a keyboard
A part of a bus is a seats
A part of a piano is a keyboard
A part of a academia is a college
A part of a church is a altar
A part of a litre is a
2024-07-17 17:34:13 root INFO     [order_1_approx] starting weight calculation for A part of a railcar is a suspension
A part of a church is a altar
A part of a litre is a millilitre
A part of a academia is a college
A part of a bus is a seats
A part of a tonne is a kilogram
A part of a typewriter is a keyboard
A part of a piano is a
2024-07-17 17:34:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 17:38:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4084,  0.0563,  0.6196,  ...,  0.3054, -0.0083, -0.4773],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5469, -1.4375, -1.2568,  ..., -4.2930,  0.5977, -3.3555],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0004, -0.0395,  0.0128,  ...,  0.0087, -0.0073, -0.0124],
        [ 0.0026,  0.0014,  0.0121,  ..., -0.0028, -0.0245, -0.0053],
        [ 0.0014,  0.0008,  0.0094,  ..., -0.0244,  0.0268, -0.0221],
        ...,
        [-0.0034,  0.0116,  0.0039,  ..., -0.0007, -0.0251,  0.0159],
        [-0.0163, -0.0179, -0.0048,  ..., -0.0004,  0.0035,  0.0271],
        [-0.0092, -0.0039,  0.0224,  ..., -0.0146,  0.0271,  0.0210]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5498, -0.0342, -1.2188,  ..., -2.7891, -0.3311, -3.8262]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 17:38:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a railcar is a suspension
A part of a church is a altar
A part of a litre is a millilitre
A part of a academia is a college
A part of a bus is a seats
A part of a tonne is a kilogram
A part of a typewriter is a keyboard
A part of a piano is a
2024-07-17 17:38:21 root INFO     [order_1_approx] starting weight calculation for A part of a academia is a college
A part of a tonne is a kilogram
A part of a church is a altar
A part of a bus is a seats
A part of a piano is a keyboard
A part of a railcar is a suspension
A part of a litre is a millilitre
A part of a typewriter is a
2024-07-17 17:38:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 17:42:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3918, -0.5635,  0.3464,  ...,  1.3066, -0.6641,  0.3464],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4727,  0.8711,  0.4248,  ...,  1.3496,  0.5806,  2.1973],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0073, -0.0071,  0.0035,  ...,  0.0023, -0.0045,  0.0036],
        [ 0.0009, -0.0059,  0.0040,  ...,  0.0004,  0.0003,  0.0064],
        [ 0.0019, -0.0068, -0.0099,  ..., -0.0015,  0.0006, -0.0078],
        ...,
        [-0.0117,  0.0032,  0.0035,  ...,  0.0062,  0.0068,  0.0242],
        [-0.0004,  0.0007,  0.0074,  ..., -0.0028, -0.0058,  0.0090],
        [-0.0099, -0.0017,  0.0137,  ..., -0.0029,  0.0092,  0.0139]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3862,  0.7715,  1.2695,  ...,  1.8408,  0.9507,  1.3047]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 17:42:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a academia is a college
A part of a tonne is a kilogram
A part of a church is a altar
A part of a bus is a seats
A part of a piano is a keyboard
A part of a railcar is a suspension
A part of a litre is a millilitre
A part of a typewriter is a
2024-07-17 17:42:28 root INFO     total operator prediction time: 1981.9014866352081 seconds
2024-07-17 17:42:28 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on synonyms - exact
2024-07-17 17:42:28 root INFO     building operator synonyms - exact
2024-07-17 17:42:28 root INFO     [order_1_approx] starting weight calculation for Another word for obsolete is outdated
Another word for new is modern
Another word for mend is repair
Another word for harbor is seaport
Another word for loyal is faithful
Another word for cloth is fabric
Another word for portion is part
Another word for flower is
2024-07-17 17:42:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 17:46:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6406,  0.5820, -0.0383,  ..., -0.5557, -0.5273,  0.0681],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.6211,  3.3086, -0.6191,  ...,  1.2520, -0.3716,  0.1582],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0174,  0.0122,  0.0121,  ...,  0.0118, -0.0044, -0.0099],
        [ 0.0040, -0.0159, -0.0164,  ...,  0.0046, -0.0071,  0.0110],
        [ 0.0267,  0.0070,  0.0066,  ..., -0.0085,  0.0130, -0.0075],
        ...,
        [ 0.0010, -0.0139,  0.0067,  ...,  0.0015, -0.0020,  0.0135],
        [-0.0082,  0.0137, -0.0028,  ...,  0.0151, -0.0005, -0.0066],
        [-0.0330, -0.0351,  0.0064,  ...,  0.0117,  0.0009,  0.0151]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.2539,  2.7773, -0.1343,  ...,  1.5928,  0.5645,  1.7871]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 17:46:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for obsolete is outdated
Another word for new is modern
Another word for mend is repair
Another word for harbor is seaport
Another word for loyal is faithful
Another word for cloth is fabric
Another word for portion is part
Another word for flower is
2024-07-17 17:46:35 root INFO     [order_1_approx] starting weight calculation for Another word for mend is repair
Another word for new is modern
Another word for harbor is seaport
Another word for obsolete is outdated
Another word for flower is blossom
Another word for portion is part
Another word for loyal is faithful
Another word for cloth is
2024-07-17 17:46:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 17:50:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2480,  0.4453, -1.6807,  ...,  2.5781,  0.1464, -0.2048],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6953,  1.4717, -0.7456,  ..., -2.1055,  2.8477, -2.6562],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0185,  0.0173, -0.0062,  ...,  0.0169,  0.0018, -0.0017],
        [-0.0131, -0.0010,  0.0041,  ...,  0.0318, -0.0177,  0.0081],
        [ 0.0036,  0.0119, -0.0203,  ..., -0.0051, -0.0129,  0.0176],
        ...,
        [ 0.0040, -0.0060, -0.0103,  ...,  0.0117,  0.0002,  0.0225],
        [-0.0071, -0.0262, -0.0071,  ..., -0.0094, -0.0006,  0.0174],
        [ 0.0037, -0.0104,  0.0150,  ..., -0.0157,  0.0127,  0.0191]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7949,  2.1641, -0.6230,  ..., -2.0391,  1.8584, -2.3203]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 17:50:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for mend is repair
Another word for new is modern
Another word for harbor is seaport
Another word for obsolete is outdated
Another word for flower is blossom
Another word for portion is part
Another word for loyal is faithful
Another word for cloth is
2024-07-17 17:50:42 root INFO     [order_1_approx] starting weight calculation for Another word for mend is repair
Another word for harbor is seaport
Another word for new is modern
Another word for flower is blossom
Another word for loyal is faithful
Another word for cloth is fabric
Another word for obsolete is outdated
Another word for portion is
2024-07-17 17:50:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 17:54:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0261, -0.5630, -0.9126,  ...,  0.0468, -0.3647,  1.5215],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.8223,  0.3057, -1.7148,  ..., -1.8672,  1.4521,  1.1484],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0043,  0.0363,  0.0013,  ..., -0.0101, -0.0269, -0.0013],
        [-0.0188,  0.0028,  0.0051,  ...,  0.0229, -0.0431, -0.0026],
        [ 0.0271,  0.0097,  0.0038,  ...,  0.0290, -0.0105,  0.0028],
        ...,
        [-0.0067,  0.0162,  0.0052,  ...,  0.0392,  0.0066,  0.0072],
        [-0.0135, -0.0117, -0.0055,  ..., -0.0002, -0.0016, -0.0257],
        [-0.0160, -0.0177, -0.0275,  ..., -0.0095,  0.0006,  0.0235]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.7734,  0.8823, -1.6016,  ..., -1.7900,  2.0469, -0.7676]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 17:54:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for mend is repair
Another word for harbor is seaport
Another word for new is modern
Another word for flower is blossom
Another word for loyal is faithful
Another word for cloth is fabric
Another word for obsolete is outdated
Another word for portion is
2024-07-17 17:54:50 root INFO     [order_1_approx] starting weight calculation for Another word for obsolete is outdated
Another word for portion is part
Another word for flower is blossom
Another word for loyal is faithful
Another word for cloth is fabric
Another word for mend is repair
Another word for new is modern
Another word for harbor is
2024-07-17 17:54:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 17:58:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4155,  0.1461, -1.5869,  ...,  0.0817, -1.1885,  0.9878],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8672,  1.3252, -2.8516,  ...,  1.9043,  0.7266,  3.0156],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0084, -0.0193,  0.0041,  ...,  0.0253, -0.0063,  0.0004],
        [ 0.0031, -0.0014,  0.0053,  ...,  0.0045, -0.0111,  0.0031],
        [ 0.0010,  0.0141, -0.0100,  ...,  0.0036, -0.0009, -0.0027],
        ...,
        [-0.0089, -0.0055,  0.0105,  ...,  0.0018,  0.0003,  0.0334],
        [-0.0090,  0.0144,  0.0007,  ...,  0.0102,  0.0043, -0.0101],
        [ 0.0085, -0.0239, -0.0053,  ...,  0.0097, -0.0004,  0.0216]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.9297e-03,  1.7686e+00, -3.2832e+00,  ...,  3.0586e+00,
          4.1992e-02,  3.5059e+00]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SubBackward0>) 

                    
2024-07-17 17:58:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for obsolete is outdated
Another word for portion is part
Another word for flower is blossom
Another word for loyal is faithful
Another word for cloth is fabric
Another word for mend is repair
Another word for new is modern
Another word for harbor is
2024-07-17 17:58:59 root INFO     [order_1_approx] starting weight calculation for Another word for portion is part
Another word for mend is repair
Another word for cloth is fabric
Another word for loyal is faithful
Another word for harbor is seaport
Another word for new is modern
Another word for flower is blossom
Another word for obsolete is
2024-07-17 17:58:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 18:03:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2598,  0.3677,  0.4680,  ..., -1.5771, -0.0442,  0.6201],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1416, -2.1191, -2.0938,  ..., -2.6543,  0.6357,  2.0117],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0157, -0.0133,  0.0042,  ...,  0.0058, -0.0040,  0.0042],
        [-0.0066, -0.0076, -0.0081,  ..., -0.0094, -0.0172,  0.0141],
        [ 0.0012,  0.0119,  0.0083,  ...,  0.0006, -0.0075, -0.0008],
        ...,
        [-0.0016, -0.0206,  0.0211,  ..., -0.0119, -0.0048,  0.0109],
        [ 0.0085, -0.0082,  0.0026,  ...,  0.0193,  0.0088, -0.0078],
        [ 0.0022, -0.0147,  0.0037,  ...,  0.0157,  0.0130,  0.0243]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7314, -0.8877, -2.1113,  ..., -2.1660,  0.5239,  2.1406]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 18:03:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for portion is part
Another word for mend is repair
Another word for cloth is fabric
Another word for loyal is faithful
Another word for harbor is seaport
Another word for new is modern
Another word for flower is blossom
Another word for obsolete is
2024-07-17 18:03:07 root INFO     [order_1_approx] starting weight calculation for Another word for portion is part
Another word for obsolete is outdated
Another word for cloth is fabric
Another word for mend is repair
Another word for loyal is faithful
Another word for harbor is seaport
Another word for flower is blossom
Another word for new is
2024-07-17 18:03:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 18:07:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3120,  0.4646, -0.8701,  ..., -0.4089, -0.5903,  0.4338],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7188, -3.1680, -2.9023,  ..., -0.8696, -0.6943,  2.3281],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0049,  0.0003, -0.0029,  ..., -0.0040,  0.0035,  0.0127],
        [ 0.0060,  0.0007, -0.0063,  ...,  0.0152, -0.0116, -0.0029],
        [-0.0041,  0.0075, -0.0065,  ...,  0.0039, -0.0038,  0.0086],
        ...,
        [ 0.0055, -0.0119, -0.0017,  ..., -0.0037, -0.0026,  0.0239],
        [ 0.0032,  0.0143,  0.0051,  ..., -0.0135,  0.0051,  0.0062],
        [-0.0100, -0.0212, -0.0067,  ...,  0.0038, -0.0018,  0.0028]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3320, -2.4453, -3.6504,  ..., -0.6514,  0.3154,  1.5439]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 18:07:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for portion is part
Another word for obsolete is outdated
Another word for cloth is fabric
Another word for mend is repair
Another word for loyal is faithful
Another word for harbor is seaport
Another word for flower is blossom
Another word for new is
2024-07-17 18:07:13 root INFO     [order_1_approx] starting weight calculation for Another word for mend is repair
Another word for flower is blossom
Another word for portion is part
Another word for new is modern
Another word for obsolete is outdated
Another word for harbor is seaport
Another word for cloth is fabric
Another word for loyal is
2024-07-17 18:07:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 18:11:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0949,  0.3210, -0.3762,  ...,  0.0614, -0.6357, -0.1976],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0410,  2.6328,  0.0898,  ..., -1.7656,  3.1250,  4.6445],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0221, -0.0077,  0.0221,  ...,  0.0026, -0.0002,  0.0063],
        [-0.0011,  0.0175, -0.0341,  ...,  0.0181,  0.0005,  0.0009],
        [ 0.0112, -0.0104, -0.0050,  ..., -0.0168, -0.0098,  0.0111],
        ...,
        [-0.0135,  0.0024, -0.0026,  ..., -0.0146, -0.0032, -0.0027],
        [-0.0246,  0.0066,  0.0010,  ..., -0.0015,  0.0062, -0.0066],
        [ 0.0026, -0.0113, -0.0219,  ...,  0.0068,  0.0223,  0.0117]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1680,  2.5039,  0.0799,  ..., -2.4453,  3.6504,  4.4375]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 18:11:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for mend is repair
Another word for flower is blossom
Another word for portion is part
Another word for new is modern
Another word for obsolete is outdated
Another word for harbor is seaport
Another word for cloth is fabric
Another word for loyal is
2024-07-17 18:11:19 root INFO     [order_1_approx] starting weight calculation for Another word for cloth is fabric
Another word for harbor is seaport
Another word for new is modern
Another word for flower is blossom
Another word for obsolete is outdated
Another word for portion is part
Another word for loyal is faithful
Another word for mend is
2024-07-17 18:11:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 18:15:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5459, -0.2556, -1.4180,  ...,  0.3425,  0.6709, -0.9551],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9819, -0.9336, -4.2148,  ..., -1.3291,  1.8564,  6.1641],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0135,  0.0181,  0.0155,  ...,  0.0059, -0.0103, -0.0012],
        [-0.0027,  0.0196, -0.0069,  ...,  0.0068, -0.0258, -0.0008],
        [ 0.0065,  0.0146,  0.0050,  ...,  0.0018,  0.0002, -0.0068],
        ...,
        [-0.0065, -0.0108,  0.0094,  ..., -0.0087,  0.0030,  0.0080],
        [ 0.0016, -0.0013,  0.0144,  ...,  0.0172, -0.0185, -0.0040],
        [ 0.0051,  0.0135, -0.0075,  ...,  0.0052,  0.0134,  0.0176]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.8770, -0.6968, -2.3281,  ..., -1.3350, -0.2393,  4.1875]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 18:15:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for cloth is fabric
Another word for harbor is seaport
Another word for new is modern
Another word for flower is blossom
Another word for obsolete is outdated
Another word for portion is part
Another word for loyal is faithful
Another word for mend is
2024-07-17 18:15:29 root INFO     total operator prediction time: 1980.906730413437 seconds
2024-07-17 18:15:29 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on hypernyms - misc
2024-07-17 18:15:29 root INFO     building operator hypernyms - misc
2024-07-17 18:15:29 root INFO     [order_1_approx] starting weight calculation for The pastry falls into the category of food
The croissant falls into the category of pastry
The sunscreen falls into the category of cream
The lemon falls into the category of citrus
The tub falls into the category of container
The toaster falls into the category of appliance
The shelf falls into the category of furniture
The wristband falls into the category of
2024-07-17 18:15:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 18:19:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2281, -0.2661,  0.2556,  ..., -0.0486, -0.4888,  0.9839],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.6211, -3.9258, -0.8535,  ..., -5.6758, -0.2734,  1.6377],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.4332e-02,  5.1575e-03, -7.0038e-03,  ..., -7.6294e-04,
         -1.6190e-02, -3.6983e-03],
        [-6.3400e-03, -5.3139e-03,  1.9379e-03,  ...,  6.2790e-03,
          3.3226e-03,  6.0196e-03],
        [ 1.2589e-03, -1.2608e-03, -4.0588e-03,  ...,  1.3580e-02,
          5.3062e-03,  7.4158e-03],
        ...,
        [-5.0211e-04, -5.1918e-03, -3.1452e-03,  ..., -5.5313e-04,
          9.2850e-03,  5.4131e-03],
        [ 3.9490e-02,  5.2071e-04,  8.5068e-03,  ..., -4.5776e-05,
         -2.9831e-03, -1.7128e-03],
        [-1.5068e-02, -5.6076e-03,  9.3460e-03,  ...,  1.0033e-02,
          8.8348e-03,  4.9934e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.4648, -3.6113, -1.7266,  ..., -6.7773, -1.1953,  2.2832]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 18:19:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The pastry falls into the category of food
The croissant falls into the category of pastry
The sunscreen falls into the category of cream
The lemon falls into the category of citrus
The tub falls into the category of container
The toaster falls into the category of appliance
The shelf falls into the category of furniture
The wristband falls into the category of
2024-07-17 18:19:37 root INFO     [order_1_approx] starting weight calculation for The croissant falls into the category of pastry
The tub falls into the category of container
The shelf falls into the category of furniture
The wristband falls into the category of band
The pastry falls into the category of food
The sunscreen falls into the category of cream
The lemon falls into the category of citrus
The toaster falls into the category of
2024-07-17 18:19:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 18:23:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8232,  1.1299, -0.0239,  ...,  0.8999,  0.2383,  1.9805],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-6.0977,  0.6519,  1.9385,  ...,  1.3291, -2.3301,  0.0605],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0187,  0.0164,  0.0144,  ...,  0.0065, -0.0120,  0.0207],
        [-0.0069,  0.0098, -0.0059,  ..., -0.0038,  0.0108,  0.0012],
        [-0.0010,  0.0028,  0.0197,  ...,  0.0125,  0.0006,  0.0132],
        ...,
        [-0.0091, -0.0039, -0.0135,  ...,  0.0191,  0.0033, -0.0088],
        [ 0.0048,  0.0036,  0.0145,  ...,  0.0070,  0.0040, -0.0026],
        [-0.0059, -0.0040, -0.0034,  ..., -0.0008,  0.0116,  0.0029]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.9648,  0.6631,  1.4482,  ...,  0.5723, -2.5137,  0.2238]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 18:23:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The croissant falls into the category of pastry
The tub falls into the category of container
The shelf falls into the category of furniture
The wristband falls into the category of band
The pastry falls into the category of food
The sunscreen falls into the category of cream
The lemon falls into the category of citrus
The toaster falls into the category of
2024-07-17 18:23:45 root INFO     [order_1_approx] starting weight calculation for The croissant falls into the category of pastry
The pastry falls into the category of food
The shelf falls into the category of furniture
The wristband falls into the category of band
The tub falls into the category of container
The toaster falls into the category of appliance
The sunscreen falls into the category of cream
The lemon falls into the category of
2024-07-17 18:23:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 18:27:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0254, -0.6289,  0.8394,  ...,  0.5522, -0.0718,  0.5669],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0791,  2.0664,  0.9316,  ...,  1.1719, -2.3555, -2.0801],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0080, -0.0002,  0.0003,  ...,  0.0001, -0.0080,  0.0013],
        [-0.0081,  0.0122,  0.0051,  ...,  0.0034, -0.0014, -0.0004],
        [ 0.0027,  0.0090, -0.0028,  ...,  0.0086,  0.0064,  0.0015],
        ...,
        [-0.0022,  0.0036,  0.0102,  ...,  0.0176, -0.0275,  0.0048],
        [-0.0086,  0.0078,  0.0007,  ...,  0.0103,  0.0001,  0.0051],
        [ 0.0134, -0.0003, -0.0095,  ...,  0.0158,  0.0089,  0.0146]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2661,  2.5684,  1.3145,  ..., -0.2529, -2.5039, -2.6758]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 18:27:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The croissant falls into the category of pastry
The pastry falls into the category of food
The shelf falls into the category of furniture
The wristband falls into the category of band
The tub falls into the category of container
The toaster falls into the category of appliance
The sunscreen falls into the category of cream
The lemon falls into the category of
2024-07-17 18:27:53 root INFO     [order_1_approx] starting weight calculation for The wristband falls into the category of band
The croissant falls into the category of pastry
The sunscreen falls into the category of cream
The pastry falls into the category of food
The lemon falls into the category of citrus
The tub falls into the category of container
The toaster falls into the category of appliance
The shelf falls into the category of
2024-07-17 18:27:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 18:32:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1335,  0.0786,  0.4094,  ..., -0.0562, -0.4497, -0.4285],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5986, -2.5781,  0.8574,  ..., -0.1021, -0.1587,  1.9863],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.2034e-02,  1.8661e-02,  7.0305e-03,  ..., -3.2120e-03,
         -8.0643e-03,  6.6185e-04],
        [-1.1513e-02, -4.4174e-03, -4.7760e-03,  ...,  7.8888e-03,
          4.7569e-03, -7.7128e-05],
        [-1.4130e-02,  5.7411e-03, -6.1226e-03,  ...,  5.0850e-03,
         -7.2098e-03,  1.7090e-02],
        ...,
        [-5.3406e-03, -1.3039e-02,  5.2948e-03,  ...,  9.9564e-03,
          1.7548e-04, -4.0855e-03],
        [-3.3913e-03,  6.2408e-03, -4.2686e-03,  ...,  1.1826e-02,
         -1.0635e-02,  1.8372e-02],
        [-6.6223e-03, -5.4932e-03, -2.2182e-03,  ..., -7.6828e-03,
          1.0025e-02,  6.2714e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6279, -2.1621,  0.2417,  ..., -0.7656, -0.5430,  1.6367]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 18:32:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The wristband falls into the category of band
The croissant falls into the category of pastry
The sunscreen falls into the category of cream
The pastry falls into the category of food
The lemon falls into the category of citrus
The tub falls into the category of container
The toaster falls into the category of appliance
The shelf falls into the category of
2024-07-17 18:32:01 root INFO     [order_1_approx] starting weight calculation for The lemon falls into the category of citrus
The wristband falls into the category of band
The pastry falls into the category of food
The croissant falls into the category of pastry
The tub falls into the category of container
The toaster falls into the category of appliance
The shelf falls into the category of furniture
The sunscreen falls into the category of
2024-07-17 18:32:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 18:36:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0776,  0.1365, -1.2529,  ..., -1.0029,  0.7305, -0.0046],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.4297,  0.3123, -2.8730,  ..., -4.5312,  0.6982,  1.1758],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0123, -0.0037,  0.0010,  ...,  0.0060, -0.0053, -0.0011],
        [ 0.0011,  0.0136, -0.0088,  ...,  0.0036, -0.0138,  0.0048],
        [ 0.0003,  0.0142,  0.0037,  ...,  0.0126,  0.0127,  0.0131],
        ...,
        [-0.0132,  0.0027, -0.0023,  ...,  0.0152,  0.0060,  0.0105],
        [ 0.0035,  0.0087,  0.0105,  ...,  0.0135,  0.0097, -0.0081],
        [-0.0127, -0.0083, -0.0072,  ...,  0.0087,  0.0154,  0.0120]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.9316,  0.7266, -2.5742,  ..., -4.4219, -0.2642,  0.7852]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 18:36:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The lemon falls into the category of citrus
The wristband falls into the category of band
The pastry falls into the category of food
The croissant falls into the category of pastry
The tub falls into the category of container
The toaster falls into the category of appliance
The shelf falls into the category of furniture
The sunscreen falls into the category of
2024-07-17 18:36:08 root INFO     [order_1_approx] starting weight calculation for The croissant falls into the category of pastry
The wristband falls into the category of band
The shelf falls into the category of furniture
The toaster falls into the category of appliance
The tub falls into the category of container
The sunscreen falls into the category of cream
The lemon falls into the category of citrus
The pastry falls into the category of
2024-07-17 18:36:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 18:40:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2063,  0.1666,  0.0828,  ...,  0.0205,  0.3760, -0.3501],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.3477, -0.9746, -1.2754,  ...,  0.4670, -1.6699, -0.1836],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0471,  0.0052,  0.0061,  ..., -0.0147,  0.0026,  0.0151],
        [-0.0174,  0.0150, -0.0173,  ...,  0.0132, -0.0023,  0.0064],
        [ 0.0142, -0.0025,  0.0101,  ..., -0.0097,  0.0015, -0.0065],
        ...,
        [-0.0013, -0.0148,  0.0213,  ..., -0.0187, -0.0046, -0.0014],
        [ 0.0370,  0.0213,  0.0201,  ..., -0.0163,  0.0087,  0.0088],
        [ 0.0039, -0.0164,  0.0096,  ...,  0.0060,  0.0124,  0.0204]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9531, -2.1016,  0.3945,  ...,  1.2197, -0.5234,  0.5532]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 18:40:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The croissant falls into the category of pastry
The wristband falls into the category of band
The shelf falls into the category of furniture
The toaster falls into the category of appliance
The tub falls into the category of container
The sunscreen falls into the category of cream
The lemon falls into the category of citrus
The pastry falls into the category of
2024-07-17 18:40:17 root INFO     [order_1_approx] starting weight calculation for The croissant falls into the category of pastry
The lemon falls into the category of citrus
The wristband falls into the category of band
The sunscreen falls into the category of cream
The shelf falls into the category of furniture
The pastry falls into the category of food
The toaster falls into the category of appliance
The tub falls into the category of
2024-07-17 18:40:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 18:44:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0281,  0.7559, -0.1271,  ..., -0.3330, -0.9268, -0.2112],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.6016, -0.1516, -1.7949,  ...,  0.0226, -0.7461,  0.5605],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.2614e-02,  1.2878e-02,  7.4921e-03,  ...,  1.7014e-03,
         -2.7618e-03,  2.2400e-02],
        [-2.4506e-02,  1.3565e-02, -5.8174e-03,  ...,  1.4030e-02,
         -7.7057e-04, -1.0040e-02],
        [ 4.1962e-05,  2.2278e-02,  1.5404e-02,  ...,  1.2680e-02,
          4.9820e-03,  2.2888e-02],
        ...,
        [-1.3931e-02, -1.8988e-03,  3.8767e-04,  ...,  1.6342e-02,
          2.2888e-03,  5.4855e-03],
        [-2.9144e-03,  1.6968e-02,  1.0551e-02,  ...,  5.3482e-03,
          2.0065e-02,  3.1128e-03],
        [-4.6959e-03, -6.8703e-03, -6.4240e-03,  ...,  2.4918e-02,
         -4.8065e-04,  2.5768e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7139, -1.5557, -0.8052,  ..., -1.0781, -1.1270,  0.2239]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 18:44:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The croissant falls into the category of pastry
The lemon falls into the category of citrus
The wristband falls into the category of band
The sunscreen falls into the category of cream
The shelf falls into the category of furniture
The pastry falls into the category of food
The toaster falls into the category of appliance
The tub falls into the category of
2024-07-17 18:44:25 root INFO     [order_1_approx] starting weight calculation for The tub falls into the category of container
The shelf falls into the category of furniture
The toaster falls into the category of appliance
The pastry falls into the category of food
The sunscreen falls into the category of cream
The wristband falls into the category of band
The lemon falls into the category of citrus
The croissant falls into the category of
2024-07-17 18:44:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 18:48:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2874, -0.9692, -0.9766,  ..., -0.2219,  0.3135,  0.7822],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-5.3203,  1.1680,  1.9062,  ...,  1.5049, -1.7354,  1.1406],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.5269e-02, -8.4305e-03,  1.2344e-02,  ...,  1.2375e-02,
         -6.0616e-03,  6.9809e-04],
        [-1.5488e-02,  1.1795e-02,  1.9493e-03,  ..., -7.6904e-03,
          1.4709e-02,  1.0498e-02],
        [ 4.5395e-03,  2.3613e-03, -2.3060e-03,  ...,  1.8997e-03,
          3.2291e-03,  1.1726e-02],
        ...,
        [-6.3629e-03, -5.0735e-03, -2.0618e-03,  ...,  4.2725e-03,
          3.5706e-03, -5.7697e-04],
        [ 1.7181e-02, -5.8060e-03,  1.0376e-02,  ...,  1.3550e-02,
         -1.0719e-02,  5.6801e-03],
        [-1.4145e-02,  1.3733e-03,  7.6294e-05,  ...,  1.0033e-03,
          1.4420e-02,  1.0818e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.7305,  0.5010,  1.4766,  ...,  1.4912, -1.5381,  0.5430]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 18:48:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The tub falls into the category of container
The shelf falls into the category of furniture
The toaster falls into the category of appliance
The pastry falls into the category of food
The sunscreen falls into the category of cream
The wristband falls into the category of band
The lemon falls into the category of citrus
The croissant falls into the category of
2024-07-17 18:48:33 root INFO     total operator prediction time: 1984.2505600452423 seconds
2024-07-17 18:48:33 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - substance
2024-07-17 18:48:33 root INFO     building operator meronyms - substance
2024-07-17 18:48:33 root INFO     [order_1_approx] starting weight calculation for A wine is made up of grapes
A body is made up of flesh
A wig is made up of hair
A icicle is made up of ice
A beach is made up of sand
A pill is made up of medicine
A glass is made up of silicone
A clothing is made up of
2024-07-17 18:48:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 18:52:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6543, -0.0427, -0.0806,  ...,  0.4878,  0.2717,  0.3823],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0918, -3.1348, -1.1387,  ...,  0.9468, -1.6348, -3.2402],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0003, -0.0311, -0.0137,  ...,  0.0019,  0.0023,  0.0023],
        [ 0.0010, -0.0118,  0.0080,  ...,  0.0079, -0.0106,  0.0113],
        [ 0.0128, -0.0112, -0.0293,  ..., -0.0007,  0.0101,  0.0141],
        ...,
        [-0.0043,  0.0092,  0.0178,  ..., -0.0065, -0.0065, -0.0028],
        [-0.0015,  0.0016, -0.0026,  ...,  0.0042, -0.0251,  0.0037],
        [-0.0237, -0.0109, -0.0036,  ...,  0.0019,  0.0013,  0.0018]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7686, -2.6328, -1.8965,  ...,  1.5156, -1.2451, -4.1914]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 18:52:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A wine is made up of grapes
A body is made up of flesh
A wig is made up of hair
A icicle is made up of ice
A beach is made up of sand
A pill is made up of medicine
A glass is made up of silicone
A clothing is made up of
2024-07-17 18:52:41 root INFO     [order_1_approx] starting weight calculation for A icicle is made up of ice
A clothing is made up of fabric
A pill is made up of medicine
A beach is made up of sand
A wig is made up of hair
A wine is made up of grapes
A glass is made up of silicone
A body is made up of
2024-07-17 18:52:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 18:56:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1143,  0.4089,  0.2742,  ...,  0.6245, -0.5010, -0.6157],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0811, -1.4209,  2.1133,  ...,  1.0400, -4.4062, -1.8809],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0059,  0.0072,  0.0110,  ...,  0.0079, -0.0107,  0.0026],
        [-0.0074, -0.0046,  0.0093,  ...,  0.0004, -0.0050,  0.0160],
        [-0.0012,  0.0116, -0.0151,  ...,  0.0097,  0.0339, -0.0034],
        ...,
        [-0.0013,  0.0018,  0.0062,  ..., -0.0063, -0.0196, -0.0015],
        [ 0.0059,  0.0213, -0.0072,  ...,  0.0077,  0.0089, -0.0195],
        [-0.0197, -0.0118,  0.0029,  ...,  0.0155,  0.0051, -0.0014]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2314, -0.6206,  1.6738,  ...,  1.4668, -3.9863, -2.5527]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 18:56:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A icicle is made up of ice
A clothing is made up of fabric
A pill is made up of medicine
A beach is made up of sand
A wig is made up of hair
A wine is made up of grapes
A glass is made up of silicone
A body is made up of
2024-07-17 18:56:48 root INFO     [order_1_approx] starting weight calculation for A beach is made up of sand
A body is made up of flesh
A clothing is made up of fabric
A wine is made up of grapes
A glass is made up of silicone
A wig is made up of hair
A icicle is made up of ice
A pill is made up of
2024-07-17 18:56:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 19:00:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0627,  0.0620, -0.8359,  ..., -0.3887, -0.3008, -0.5127],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.1660, -2.2910,  1.6367,  ..., -5.0352,  0.7573,  3.5742],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0002,  0.0026, -0.0002,  ..., -0.0037, -0.0094, -0.0082],
        [-0.0013, -0.0080, -0.0008,  ...,  0.0012,  0.0052,  0.0154],
        [-0.0057, -0.0018, -0.0023,  ...,  0.0010,  0.0249, -0.0034],
        ...,
        [ 0.0080,  0.0073,  0.0126,  ..., -0.0070, -0.0133, -0.0281],
        [-0.0059, -0.0048,  0.0023,  ..., -0.0075, -0.0114,  0.0051],
        [ 0.0018, -0.0221,  0.0124,  ..., -0.0076,  0.0100,  0.0078]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.7363, -2.0195,  0.8901,  ..., -4.3008,  0.4302,  3.2773]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 19:00:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A beach is made up of sand
A body is made up of flesh
A clothing is made up of fabric
A wine is made up of grapes
A glass is made up of silicone
A wig is made up of hair
A icicle is made up of ice
A pill is made up of
2024-07-17 19:00:55 root INFO     [order_1_approx] starting weight calculation for A wig is made up of hair
A body is made up of flesh
A clothing is made up of fabric
A wine is made up of grapes
A glass is made up of silicone
A pill is made up of medicine
A beach is made up of sand
A icicle is made up of
2024-07-17 19:00:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 19:05:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6216,  0.9951, -0.9507,  ...,  0.2705, -0.0129,  1.2539],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([3.7207, 1.6680, 0.4429,  ..., 1.8398, 2.3496, 2.3320], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 6.9523e-04, -2.1610e-03,  3.6430e-03,  ...,  1.4257e-03,
          1.1206e-03, -4.1313e-03],
        [-1.5793e-02,  2.1362e-03,  1.5793e-03,  ..., -3.9673e-04,
          6.0043e-03, -4.2725e-04],
        [-1.5259e-05, -5.7831e-03,  1.1467e-02,  ...,  3.8528e-04,
          1.1200e-02, -5.1880e-03],
        ...,
        [-3.9101e-03,  2.7466e-04,  7.4120e-03,  ...,  8.7814e-03,
         -2.0325e-02,  1.0653e-03],
        [-4.3869e-04,  9.7275e-04,  8.9111e-03,  ...,  1.4267e-02,
          5.1880e-04,  2.6512e-03],
        [-1.2154e-02,  7.2174e-03,  4.0665e-03,  ..., -5.0888e-03,
         -3.7651e-03, -7.5951e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.2793,  1.7168, -0.2744,  ...,  1.6543,  2.0176,  2.0137]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 19:05:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A wig is made up of hair
A body is made up of flesh
A clothing is made up of fabric
A wine is made up of grapes
A glass is made up of silicone
A pill is made up of medicine
A beach is made up of sand
A icicle is made up of
2024-07-17 19:05:01 root INFO     [order_1_approx] starting weight calculation for A clothing is made up of fabric
A beach is made up of sand
A wig is made up of hair
A glass is made up of silicone
A body is made up of flesh
A icicle is made up of ice
A pill is made up of medicine
A wine is made up of
2024-07-17 19:05:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 19:09:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0703, -0.1539,  0.4258,  ..., -0.3945, -0.3064,  0.8086],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.8262, -4.6289, -0.3389,  ...,  2.9414, -0.0674, -1.3857],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0013,  0.0019,  0.0055,  ...,  0.0040, -0.0088, -0.0074],
        [-0.0061,  0.0173,  0.0226,  ...,  0.0051, -0.0070, -0.0205],
        [ 0.0030, -0.0004, -0.0081,  ...,  0.0025,  0.0148, -0.0013],
        ...,
        [-0.0063, -0.0147, -0.0111,  ..., -0.0299,  0.0055,  0.0173],
        [ 0.0016,  0.0003,  0.0020,  ...,  0.0177, -0.0052, -0.0205],
        [-0.0186, -0.0002,  0.0131,  ..., -0.0122, -0.0016, -0.0158]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2754, -3.7715, -0.4568,  ...,  3.8340,  0.0978, -0.7314]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 19:09:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A clothing is made up of fabric
A beach is made up of sand
A wig is made up of hair
A glass is made up of silicone
A body is made up of flesh
A icicle is made up of ice
A pill is made up of medicine
A wine is made up of
2024-07-17 19:09:09 root INFO     [order_1_approx] starting weight calculation for A clothing is made up of fabric
A wig is made up of hair
A pill is made up of medicine
A wine is made up of grapes
A body is made up of flesh
A glass is made up of silicone
A icicle is made up of ice
A beach is made up of
2024-07-17 19:09:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 19:13:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9341,  0.7383,  0.4866,  ..., -0.2798,  0.0048, -0.9307],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4973,  1.5586, -0.6416,  ...,  3.7363, -2.0918, -1.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0175, -0.0136, -0.0086,  ...,  0.0013, -0.0074, -0.0092],
        [-0.0202,  0.0010,  0.0154,  ...,  0.0082, -0.0100, -0.0014],
        [-0.0026,  0.0015, -0.0079,  ..., -0.0090,  0.0116,  0.0011],
        ...,
        [ 0.0110, -0.0167,  0.0108,  ..., -0.0028, -0.0212, -0.0042],
        [-0.0063,  0.0042, -0.0034,  ..., -0.0091, -0.0026,  0.0050],
        [-0.0173, -0.0016,  0.0169,  ...,  0.0025, -0.0005,  0.0033]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1877,  1.7686, -1.1719,  ...,  3.8145, -2.7012, -1.0254]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 19:13:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A clothing is made up of fabric
A wig is made up of hair
A pill is made up of medicine
A wine is made up of grapes
A body is made up of flesh
A glass is made up of silicone
A icicle is made up of ice
A beach is made up of
2024-07-17 19:13:14 root INFO     [order_1_approx] starting weight calculation for A icicle is made up of ice
A body is made up of flesh
A wine is made up of grapes
A clothing is made up of fabric
A glass is made up of silicone
A beach is made up of sand
A pill is made up of medicine
A wig is made up of
2024-07-17 19:13:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 19:17:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0889,  0.2925, -1.0225,  ...,  0.5474, -1.0918,  0.7393],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.6953, -2.0137, -1.4971,  ...,  3.3066, -0.1855, -0.2986],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0054, -0.0089, -0.0043,  ..., -0.0060, -0.0033, -0.0062],
        [-0.0103, -0.0046,  0.0057,  ..., -0.0005, -0.0013,  0.0202],
        [ 0.0086,  0.0031,  0.0007,  ...,  0.0097,  0.0130,  0.0105],
        ...,
        [ 0.0066, -0.0188, -0.0013,  ...,  0.0005, -0.0085, -0.0120],
        [ 0.0075, -0.0046, -0.0065,  ...,  0.0059, -0.0144, -0.0017],
        [-0.0146,  0.0013,  0.0036,  ..., -0.0087,  0.0227,  0.0114]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.7676, -0.9922, -1.3369,  ...,  3.2754, -0.1077, -0.6836]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 19:17:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A icicle is made up of ice
A body is made up of flesh
A wine is made up of grapes
A clothing is made up of fabric
A glass is made up of silicone
A beach is made up of sand
A pill is made up of medicine
A wig is made up of
2024-07-17 19:17:22 root INFO     [order_1_approx] starting weight calculation for A beach is made up of sand
A wig is made up of hair
A icicle is made up of ice
A clothing is made up of fabric
A body is made up of flesh
A wine is made up of grapes
A pill is made up of medicine
A glass is made up of
2024-07-17 19:17:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 19:21:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3286, -1.1055, -0.0911,  ...,  0.0425, -0.1133,  0.4836],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3643,  1.0010, -0.2173,  ...,  0.8672, -5.1875,  0.8862],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0237, -0.0105,  0.0016,  ..., -0.0102,  0.0018, -0.0106],
        [-0.0175,  0.0002,  0.0083,  ...,  0.0035, -0.0051,  0.0218],
        [ 0.0073, -0.0079,  0.0080,  ...,  0.0145,  0.0264, -0.0078],
        ...,
        [ 0.0151, -0.0197,  0.0094,  ...,  0.0124, -0.0204,  0.0132],
        [ 0.0035,  0.0047,  0.0200,  ..., -0.0095,  0.0064, -0.0100],
        [-0.0177, -0.0053,  0.0057,  ..., -0.0027,  0.0117,  0.0005]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6133,  0.2261,  0.2495,  ...,  1.8193, -3.6465,  0.2441]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 19:21:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A beach is made up of sand
A wig is made up of hair
A icicle is made up of ice
A clothing is made up of fabric
A body is made up of flesh
A wine is made up of grapes
A pill is made up of medicine
A glass is made up of
2024-07-17 19:21:30 root INFO     total operator prediction time: 1976.6250195503235 seconds
2024-07-17 19:21:30 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on synonyms - intensity
2024-07-17 19:21:30 root INFO     building operator synonyms - intensity
2024-07-17 19:21:30 root INFO     [order_1_approx] starting weight calculation for A more intense word for opposed is averse
A more intense word for monkey is gorilla
A more intense word for strong is powerful
A more intense word for love is adore
A more intense word for tired is exhausted
A more intense word for want is crave
A more intense word for unfortunate is tragic
A more intense word for warm is
2024-07-17 19:21:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 19:25:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3071,  0.2952, -0.0930,  ...,  0.4014,  0.7578,  0.4534],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9678,  1.1807, -1.3936,  ..., -3.0430,  1.8516,  3.0723],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.3297e-03,  1.4771e-02,  9.6130e-04,  ...,  6.1607e-03,
          1.2627e-02,  7.1430e-04],
        [-7.2784e-03, -9.4757e-03,  4.2305e-03,  ...,  2.5558e-03,
         -1.8864e-03, -5.7144e-03],
        [-5.7831e-03,  3.1452e-03,  2.4376e-03,  ..., -1.8982e-02,
          5.2719e-03,  3.2272e-03],
        ...,
        [ 1.5007e-02, -1.0780e-02,  9.1553e-05,  ...,  1.4832e-02,
         -8.7738e-03, -1.3527e-02],
        [-9.8724e-03, -1.0300e-02,  3.6068e-03,  ..., -1.0559e-02,
          1.4297e-02, -1.5373e-02],
        [-1.4038e-02, -2.5146e-02, -2.5101e-03,  ..., -1.0078e-02,
          5.4169e-04,  5.4474e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1230,  0.7627, -1.7910,  ..., -2.8672,  2.1484,  2.6309]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 19:25:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for opposed is averse
A more intense word for monkey is gorilla
A more intense word for strong is powerful
A more intense word for love is adore
A more intense word for tired is exhausted
A more intense word for want is crave
A more intense word for unfortunate is tragic
A more intense word for warm is
2024-07-17 19:25:38 root INFO     [order_1_approx] starting weight calculation for A more intense word for want is crave
A more intense word for warm is hot
A more intense word for opposed is averse
A more intense word for tired is exhausted
A more intense word for monkey is gorilla
A more intense word for love is adore
A more intense word for strong is powerful
A more intense word for unfortunate is
2024-07-17 19:25:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 19:29:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5605,  0.1285, -0.6221,  ...,  0.5698,  0.4395,  0.7891],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.4062, -0.2598, -6.3203,  ..., -2.1367, -1.5342,  3.1445],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0064,  0.0239,  0.0017,  ...,  0.0025,  0.0142,  0.0034],
        [-0.0039, -0.0105, -0.0067,  ..., -0.0055, -0.0183, -0.0014],
        [ 0.0220,  0.0605, -0.0170,  ...,  0.0116,  0.0225, -0.0103],
        ...,
        [-0.0040, -0.0005,  0.0140,  ...,  0.0013,  0.0028,  0.0030],
        [-0.0064,  0.0267, -0.0018,  ...,  0.0040,  0.0092, -0.0172],
        [-0.0042, -0.0090, -0.0012,  ...,  0.0176, -0.0071,  0.0185]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.1992, -1.8906, -4.7969,  ..., -1.3555, -0.4180,  3.0527]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 19:29:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for want is crave
A more intense word for warm is hot
A more intense word for opposed is averse
A more intense word for tired is exhausted
A more intense word for monkey is gorilla
A more intense word for love is adore
A more intense word for strong is powerful
A more intense word for unfortunate is
2024-07-17 19:29:44 root INFO     [order_1_approx] starting weight calculation for A more intense word for tired is exhausted
A more intense word for opposed is averse
A more intense word for warm is hot
A more intense word for strong is powerful
A more intense word for monkey is gorilla
A more intense word for want is crave
A more intense word for unfortunate is tragic
A more intense word for love is
2024-07-17 19:29:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 19:33:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3047e+00,  2.1533e-01,  1.7407e-01,  ...,  6.1816e-01,
        -4.4189e-01, -9.7656e-04], device='cuda:0', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.1836,  0.0967, -0.9775,  ..., -4.5391,  3.0000, -2.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0132,  0.0055, -0.0050,  ..., -0.0018,  0.0057,  0.0014],
        [ 0.0042, -0.0117,  0.0023,  ...,  0.0123,  0.0057, -0.0197],
        [ 0.0026,  0.0038, -0.0002,  ..., -0.0240, -0.0054, -0.0084],
        ...,
        [-0.0091, -0.0154,  0.0074,  ..., -0.0001, -0.0055, -0.0120],
        [-0.0105,  0.0055, -0.0010,  ...,  0.0107, -0.0141, -0.0087],
        [-0.0091, -0.0044,  0.0121,  ...,  0.0097,  0.0197, -0.0044]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.2578,  0.2549, -1.1338,  ..., -4.8086,  2.7578, -1.0391]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 19:33:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for tired is exhausted
A more intense word for opposed is averse
A more intense word for warm is hot
A more intense word for strong is powerful
A more intense word for monkey is gorilla
A more intense word for want is crave
A more intense word for unfortunate is tragic
A more intense word for love is
2024-07-17 19:33:52 root INFO     [order_1_approx] starting weight calculation for A more intense word for love is adore
A more intense word for opposed is averse
A more intense word for unfortunate is tragic
A more intense word for strong is powerful
A more intense word for tired is exhausted
A more intense word for warm is hot
A more intense word for want is crave
A more intense word for monkey is
2024-07-17 19:33:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 19:37:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7383, -1.1689, -0.9883,  ...,  0.5815, -0.4189,  0.4731],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.9297, -2.6250, -0.8979,  ...,  0.1428,  1.8438, -0.2073],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0148, -0.0055, -0.0083,  ...,  0.0016, -0.0110,  0.0060],
        [ 0.0073,  0.0155, -0.0021,  ...,  0.0233,  0.0172, -0.0037],
        [-0.0218, -0.0014, -0.0032,  ..., -0.0099,  0.0060,  0.0199],
        ...,
        [-0.0086, -0.0052,  0.0013,  ..., -0.0055, -0.0150,  0.0129],
        [-0.0194, -0.0107, -0.0048,  ...,  0.0112,  0.0073, -0.0020],
        [ 0.0001,  0.0039, -0.0177,  ...,  0.0014,  0.0007, -0.0178]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.7363, -1.5205, -1.1836,  ...,  0.9502,  2.0156, -0.5229]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 19:37:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for love is adore
A more intense word for opposed is averse
A more intense word for unfortunate is tragic
A more intense word for strong is powerful
A more intense word for tired is exhausted
A more intense word for warm is hot
A more intense word for want is crave
A more intense word for monkey is
2024-07-17 19:37:59 root INFO     [order_1_approx] starting weight calculation for A more intense word for love is adore
A more intense word for opposed is averse
A more intense word for monkey is gorilla
A more intense word for want is crave
A more intense word for warm is hot
A more intense word for unfortunate is tragic
A more intense word for strong is powerful
A more intense word for tired is
2024-07-17 19:37:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 19:42:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4207, -0.1078, -0.5024,  ...,  0.6006, -1.0166, -0.4341],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5547, -1.2539, -1.5596,  ..., -9.1875,  2.8223,  0.7520],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0184,  0.0044,  0.0088,  ...,  0.0075, -0.0003, -0.0213],
        [ 0.0031,  0.0355,  0.0045,  ...,  0.0079, -0.0020, -0.0057],
        [ 0.0039,  0.0156,  0.0049,  ...,  0.0122, -0.0009,  0.0044],
        ...,
        [ 0.0002,  0.0011, -0.0197,  ...,  0.0133,  0.0207,  0.0052],
        [-0.0102, -0.0195,  0.0023,  ..., -0.0095,  0.0350,  0.0115],
        [-0.0049,  0.0040,  0.0096,  ...,  0.0158,  0.0069,  0.0364]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6992, -1.2148, -0.7827,  ..., -7.8555,  2.7871,  0.5547]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 19:42:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for love is adore
A more intense word for opposed is averse
A more intense word for monkey is gorilla
A more intense word for want is crave
A more intense word for warm is hot
A more intense word for unfortunate is tragic
A more intense word for strong is powerful
A more intense word for tired is
2024-07-17 19:42:07 root INFO     [order_1_approx] starting weight calculation for A more intense word for unfortunate is tragic
A more intense word for monkey is gorilla
A more intense word for want is crave
A more intense word for tired is exhausted
A more intense word for love is adore
A more intense word for opposed is averse
A more intense word for warm is hot
A more intense word for strong is
2024-07-17 19:42:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 19:46:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6816, -0.2949,  0.1353,  ...,  0.7412, -0.0150, -0.2374],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5615, -3.7910, -1.9551,  ..., -3.9102,  0.9209, -2.3633],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.4719e-03,  2.8744e-03,  1.7517e-02,  ...,  1.0124e-02,
          5.1613e-03,  5.9280e-03],
        [ 1.1810e-02,  8.7509e-03,  2.1881e-02,  ...,  2.1713e-02,
          1.3145e-02, -3.0289e-02],
        [-7.6246e-04, -1.1902e-02,  7.9575e-03,  ...,  5.3024e-04,
          1.0880e-02, -2.9236e-02],
        ...,
        [ 5.3635e-03, -1.5755e-03,  8.6975e-03,  ...,  1.8806e-03,
         -1.9073e-05, -5.1842e-03],
        [-1.2276e-02, -1.4343e-02, -1.0391e-02,  ...,  5.5389e-03,
          1.2573e-02,  1.0818e-02],
        [-2.2186e-02, -9.6130e-03,  6.0730e-03,  ...,  6.1264e-03,
          1.1879e-02,  1.2482e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5283, -3.2363, -2.2031,  ..., -3.5742,  1.7070, -2.5508]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 19:46:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for unfortunate is tragic
A more intense word for monkey is gorilla
A more intense word for want is crave
A more intense word for tired is exhausted
A more intense word for love is adore
A more intense word for opposed is averse
A more intense word for warm is hot
A more intense word for strong is
2024-07-17 19:46:14 root INFO     [order_1_approx] starting weight calculation for A more intense word for warm is hot
A more intense word for monkey is gorilla
A more intense word for tired is exhausted
A more intense word for unfortunate is tragic
A more intense word for strong is powerful
A more intense word for opposed is averse
A more intense word for love is adore
A more intense word for want is
2024-07-17 19:46:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 19:50:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5186,  0.2778, -1.2051,  ..., -0.2443, -0.0515, -0.1111],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2959,  1.2100, -4.2578,  ..., -2.5918,  4.6484, -1.2617],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0202,  0.0185,  0.0173,  ..., -0.0175,  0.0101,  0.0159],
        [-0.0069,  0.0116,  0.0235,  ...,  0.0184, -0.0068,  0.0050],
        [ 0.0606,  0.0140, -0.0165,  ...,  0.0244,  0.0066, -0.0188],
        ...,
        [-0.0109, -0.0191, -0.0145,  ..., -0.0045,  0.0041, -0.0032],
        [-0.0047, -0.0070,  0.0249,  ...,  0.0168,  0.0141, -0.0007],
        [-0.0172,  0.0016,  0.0230,  ..., -0.0091,  0.0157,  0.0209]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2068,  1.5938, -4.6562,  ..., -2.9102,  4.1250, -1.4717]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 19:50:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for warm is hot
A more intense word for monkey is gorilla
A more intense word for tired is exhausted
A more intense word for unfortunate is tragic
A more intense word for strong is powerful
A more intense word for opposed is averse
A more intense word for love is adore
A more intense word for want is
2024-07-17 19:50:22 root INFO     [order_1_approx] starting weight calculation for A more intense word for tired is exhausted
A more intense word for unfortunate is tragic
A more intense word for monkey is gorilla
A more intense word for strong is powerful
A more intense word for warm is hot
A more intense word for love is adore
A more intense word for want is crave
A more intense word for opposed is
2024-07-17 19:50:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 19:54:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2590, -0.2372,  0.6304,  ..., -0.1515, -0.7861, -0.1660],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.1465, -2.3555, -1.0723,  ...,  4.0273, 10.1406,  3.5879],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.9049e-03, -2.5101e-03,  8.3084e-03,  ..., -1.6769e-02,
          8.0490e-04, -2.4765e-02],
        [-5.0583e-03,  1.3138e-02, -8.0338e-03,  ...,  7.7057e-03,
          1.7197e-02,  4.0131e-03],
        [ 1.0941e-02,  1.5762e-02,  3.8147e-05,  ...,  6.1607e-03,
         -3.0632e-03, -9.3918e-03],
        ...,
        [-7.0763e-03, -2.9892e-02,  2.7451e-02,  ...,  6.7062e-03,
         -9.1705e-03,  1.0185e-02],
        [ 8.1482e-03, -3.1494e-02,  9.1400e-03,  ..., -1.2360e-03,
          9.1553e-03,  4.7424e-02],
        [ 1.8196e-03,  1.2497e-02,  8.6517e-03,  ...,  5.5580e-03,
         -5.3368e-03,  1.1147e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6699, -1.3242, -0.4800,  ...,  4.2422, 11.3438,  4.5625]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 19:54:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for tired is exhausted
A more intense word for unfortunate is tragic
A more intense word for monkey is gorilla
A more intense word for strong is powerful
A more intense word for warm is hot
A more intense word for love is adore
A more intense word for want is crave
A more intense word for opposed is
2024-07-17 19:54:29 root INFO     total operator prediction time: 1979.8196346759796 seconds
2024-07-17 19:54:30 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on hypernyms - animals
2024-07-17 19:54:30 root INFO     building operator hypernyms - animals
2024-07-17 19:54:30 root INFO     [order_1_approx] starting weight calculation for The cow falls into the category of bovid
The tyrannosaurus falls into the category of dinosaur
The coyote falls into the category of canine
The cobra falls into the category of snake
The jaguar falls into the category of feline
The wolf falls into the category of canine
The porcupine falls into the category of rodent
The bee falls into the category of
2024-07-17 19:54:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 19:58:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2842, -0.5078, -1.1309,  ...,  0.2281, -0.0833,  1.0859],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.1367, -0.7383, -1.5410,  ...,  1.8779, -1.1523,  1.0801],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0049, -0.0015, -0.0036,  ...,  0.0036, -0.0043, -0.0082],
        [ 0.0055, -0.0009, -0.0078,  ...,  0.0138,  0.0073,  0.0072],
        [ 0.0038, -0.0088,  0.0098,  ..., -0.0023,  0.0037, -0.0056],
        ...,
        [ 0.0033, -0.0063,  0.0110,  ...,  0.0007,  0.0009,  0.0027],
        [ 0.0058, -0.0086,  0.0085,  ..., -0.0042, -0.0067,  0.0105],
        [-0.0108, -0.0130,  0.0037,  ...,  0.0004, -0.0012,  0.0201]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.8125, -0.2017, -1.9336,  ...,  1.5811, -1.2607,  0.9644]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 19:58:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The cow falls into the category of bovid
The tyrannosaurus falls into the category of dinosaur
The coyote falls into the category of canine
The cobra falls into the category of snake
The jaguar falls into the category of feline
The wolf falls into the category of canine
The porcupine falls into the category of rodent
The bee falls into the category of
2024-07-17 19:58:38 root INFO     [order_1_approx] starting weight calculation for The wolf falls into the category of canine
The porcupine falls into the category of rodent
The jaguar falls into the category of feline
The bee falls into the category of insect
The cobra falls into the category of snake
The cow falls into the category of bovid
The tyrannosaurus falls into the category of dinosaur
The coyote falls into the category of
2024-07-17 19:58:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 20:02:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9697, -1.1289, -1.0430,  ...,  1.0537,  0.5024,  0.6094],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3574, -0.7012,  2.0488,  ..., -2.5020,  1.8203,  1.9219],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0060, -0.0317,  0.0124,  ..., -0.0016, -0.0129, -0.0111],
        [-0.0098,  0.0084,  0.0163,  ...,  0.0093,  0.0223,  0.0054],
        [-0.0077,  0.0032,  0.0056,  ...,  0.0016,  0.0078,  0.0013],
        ...,
        [-0.0014, -0.0105,  0.0226,  ...,  0.0040, -0.0089,  0.0026],
        [-0.0083, -0.0075, -0.0126,  ...,  0.0115,  0.0137,  0.0167],
        [-0.0039, -0.0054, -0.0041,  ...,  0.0046,  0.0045,  0.0030]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1404, -0.9409,  2.3516,  ..., -1.7285,  1.2168,  2.0996]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 20:02:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The wolf falls into the category of canine
The porcupine falls into the category of rodent
The jaguar falls into the category of feline
The bee falls into the category of insect
The cobra falls into the category of snake
The cow falls into the category of bovid
The tyrannosaurus falls into the category of dinosaur
The coyote falls into the category of
2024-07-17 20:02:45 root INFO     [order_1_approx] starting weight calculation for The cow falls into the category of bovid
The porcupine falls into the category of rodent
The bee falls into the category of insect
The jaguar falls into the category of feline
The tyrannosaurus falls into the category of dinosaur
The wolf falls into the category of canine
The coyote falls into the category of canine
The cobra falls into the category of
2024-07-17 20:02:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 20:06:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0596, -0.8613, -0.9736,  ...,  0.4053, -0.0747, -0.3606],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0527, -1.8213, -1.3320,  ...,  0.3945, -2.3262,  2.7031],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0040, -0.0038, -0.0089,  ...,  0.0075, -0.0067, -0.0167],
        [ 0.0184,  0.0006,  0.0273,  ...,  0.0160,  0.0287,  0.0184],
        [-0.0170,  0.0015,  0.0008,  ..., -0.0025, -0.0258, -0.0032],
        ...,
        [ 0.0025, -0.0038,  0.0093,  ...,  0.0126, -0.0129,  0.0067],
        [ 0.0125,  0.0020, -0.0050,  ...,  0.0052,  0.0186,  0.0022],
        [ 0.0004, -0.0115,  0.0075,  ..., -0.0025,  0.0331,  0.0328]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1741, -2.3164, -1.3320,  ...,  0.5547, -2.8828,  2.4805]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 20:06:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The cow falls into the category of bovid
The porcupine falls into the category of rodent
The bee falls into the category of insect
The jaguar falls into the category of feline
The tyrannosaurus falls into the category of dinosaur
The wolf falls into the category of canine
The coyote falls into the category of canine
The cobra falls into the category of
2024-07-17 20:06:52 root INFO     [order_1_approx] starting weight calculation for The coyote falls into the category of canine
The jaguar falls into the category of feline
The tyrannosaurus falls into the category of dinosaur
The bee falls into the category of insect
The cow falls into the category of bovid
The wolf falls into the category of canine
The cobra falls into the category of snake
The porcupine falls into the category of
2024-07-17 20:06:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 20:10:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3218, -1.1250, -0.7158,  ...,  0.8892,  0.8872,  0.2432],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.6875, -0.1318, -4.2852,  ...,  1.2412, -0.8081,  4.4922],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0033, -0.0126,  0.0005,  ...,  0.0028, -0.0030, -0.0031],
        [-0.0183,  0.0073,  0.0101,  ...,  0.0097,  0.0029,  0.0035],
        [ 0.0145, -0.0030,  0.0068,  ...,  0.0067,  0.0073, -0.0006],
        ...,
        [-0.0121, -0.0148,  0.0073,  ...,  0.0131,  0.0010,  0.0030],
        [-0.0070, -0.0086, -0.0029,  ..., -0.0110,  0.0101, -0.0004],
        [-0.0314,  0.0059, -0.0050,  ..., -0.0109,  0.0136,  0.0231]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.5273, -0.2710, -4.4492,  ...,  0.6763, -1.7188,  4.0273]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 20:10:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The coyote falls into the category of canine
The jaguar falls into the category of feline
The tyrannosaurus falls into the category of dinosaur
The bee falls into the category of insect
The cow falls into the category of bovid
The wolf falls into the category of canine
The cobra falls into the category of snake
The porcupine falls into the category of
2024-07-17 20:11:00 root INFO     [order_1_approx] starting weight calculation for The coyote falls into the category of canine
The jaguar falls into the category of feline
The porcupine falls into the category of rodent
The wolf falls into the category of canine
The bee falls into the category of insect
The cow falls into the category of bovid
The cobra falls into the category of snake
The tyrannosaurus falls into the category of
2024-07-17 20:11:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 20:15:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.8711,  0.5635, -0.6787,  ...,  0.3276,  0.0532,  1.3857],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9434, -2.3164,  3.7109,  ...,  2.5820, -2.8184,  1.8281],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0005, -0.0032,  0.0016,  ..., -0.0051, -0.0093, -0.0047],
        [ 0.0078, -0.0065,  0.0048,  ...,  0.0089,  0.0090,  0.0070],
        [-0.0062,  0.0037, -0.0112,  ...,  0.0028,  0.0055,  0.0081],
        ...,
        [-0.0132, -0.0017, -0.0033,  ...,  0.0047, -0.0031,  0.0020],
        [ 0.0006,  0.0008,  0.0173,  ...,  0.0023, -0.0019,  0.0070],
        [-0.0055, -0.0025,  0.0006,  ...,  0.0002,  0.0100,  0.0056]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1934, -2.5195,  3.5938,  ...,  2.3594, -2.8359,  1.3604]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 20:15:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The coyote falls into the category of canine
The jaguar falls into the category of feline
The porcupine falls into the category of rodent
The wolf falls into the category of canine
The bee falls into the category of insect
The cow falls into the category of bovid
The cobra falls into the category of snake
The tyrannosaurus falls into the category of
2024-07-17 20:15:07 root INFO     [order_1_approx] starting weight calculation for The cow falls into the category of bovid
The cobra falls into the category of snake
The porcupine falls into the category of rodent
The coyote falls into the category of canine
The tyrannosaurus falls into the category of dinosaur
The bee falls into the category of insect
The wolf falls into the category of canine
The jaguar falls into the category of
2024-07-17 20:15:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 20:19:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2051, -0.8579,  0.1287,  ..., -0.3833,  0.2778,  0.3137],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3057, -1.1279, -0.1064,  ..., -0.6084, -4.3281,  2.2988],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-4.9973e-04, -1.5175e-02,  6.4125e-03,  ..., -5.0545e-03,
         -7.8964e-04, -3.0327e-03],
        [-8.8959e-03, -1.0033e-03,  1.1940e-02,  ...,  1.6418e-02,
          7.5150e-03, -3.1624e-03],
        [-2.5063e-03,  1.4687e-03, -5.1651e-03,  ...,  4.4556e-03,
         -4.3983e-03, -4.8370e-03],
        ...,
        [-1.1276e-02,  3.5248e-03,  1.7044e-02,  ...,  8.2169e-03,
         -1.0483e-02,  4.2763e-03],
        [ 1.7071e-03, -2.0218e-02, -1.7147e-03,  ..., -3.8071e-03,
          1.2527e-02,  7.9346e-03],
        [-9.6130e-03, -6.8588e-03,  6.8970e-03,  ...,  4.2915e-05,
          5.3902e-03,  1.3153e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.8633, -1.2480, -0.6016,  ...,  0.0239, -5.5625,  2.7598]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 20:19:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The cow falls into the category of bovid
The cobra falls into the category of snake
The porcupine falls into the category of rodent
The coyote falls into the category of canine
The tyrannosaurus falls into the category of dinosaur
The bee falls into the category of insect
The wolf falls into the category of canine
The jaguar falls into the category of
2024-07-17 20:19:14 root INFO     [order_1_approx] starting weight calculation for The tyrannosaurus falls into the category of dinosaur
The coyote falls into the category of canine
The cobra falls into the category of snake
The jaguar falls into the category of feline
The porcupine falls into the category of rodent
The cow falls into the category of bovid
The bee falls into the category of insect
The wolf falls into the category of
2024-07-17 20:19:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 20:23:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0889, -0.7998, -0.9023,  ...,  0.6665, -0.1826,  0.3667],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2227, -1.4580,  0.6758,  ..., -1.2363,  0.5610,  2.1953],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0089, -0.0159,  0.0027,  ...,  0.0095, -0.0025,  0.0008],
        [-0.0067,  0.0055,  0.0056,  ...,  0.0157, -0.0014, -0.0067],
        [-0.0046, -0.0063, -0.0027,  ...,  0.0040, -0.0002, -0.0123],
        ...,
        [-0.0094,  0.0062,  0.0115,  ...,  0.0074, -0.0059,  0.0096],
        [-0.0040, -0.0027, -0.0072,  ...,  0.0005,  0.0080,  0.0110],
        [-0.0028, -0.0022, -0.0020,  ..., -0.0008, -0.0096,  0.0121]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1855, -1.4561,  0.4766,  ..., -1.2275,  0.0469,  2.5156]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 20:23:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The tyrannosaurus falls into the category of dinosaur
The coyote falls into the category of canine
The cobra falls into the category of snake
The jaguar falls into the category of feline
The porcupine falls into the category of rodent
The cow falls into the category of bovid
The bee falls into the category of insect
The wolf falls into the category of
2024-07-17 20:23:21 root INFO     [order_1_approx] starting weight calculation for The jaguar falls into the category of feline
The porcupine falls into the category of rodent
The wolf falls into the category of canine
The coyote falls into the category of canine
The bee falls into the category of insect
The cobra falls into the category of snake
The tyrannosaurus falls into the category of dinosaur
The cow falls into the category of
2024-07-17 20:23:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 20:27:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1416, -0.2457, -0.7153,  ...,  0.6431,  0.3442,  1.6035],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.1562,  0.1753, -0.0430,  ..., -1.8330, -1.7314,  0.8857],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0045, -0.0080, -0.0009,  ..., -0.0030, -0.0079,  0.0012],
        [-0.0027,  0.0041, -0.0141,  ...,  0.0022,  0.0021, -0.0021],
        [-0.0038, -0.0060, -0.0265,  ...,  0.0027,  0.0084, -0.0152],
        ...,
        [-0.0035, -0.0079, -0.0069,  ..., -0.0046, -0.0050,  0.0056],
        [-0.0217, -0.0041, -0.0007,  ...,  0.0049, -0.0034, -0.0002],
        [-0.0071, -0.0068,  0.0112,  ..., -0.0022, -0.0025,  0.0073]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.0234, -0.3042, -1.2715,  ..., -2.5938, -2.7812,  1.0381]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 20:27:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The jaguar falls into the category of feline
The porcupine falls into the category of rodent
The wolf falls into the category of canine
The coyote falls into the category of canine
The bee falls into the category of insect
The cobra falls into the category of snake
The tyrannosaurus falls into the category of dinosaur
The cow falls into the category of
2024-07-17 20:27:28 root INFO     total operator prediction time: 1978.9234211444855 seconds
2024-07-17 20:27:28 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on hyponyms - misc
2024-07-17 20:27:28 root INFO     building operator hyponyms - misc
2024-07-17 20:27:29 root INFO     [order_1_approx] starting weight calculation for A more specific term for a month is january
A more specific term for a oven is broiler
A more specific term for a church is chapel
A more specific term for a candy is lollipop
A more specific term for a sofa is divan
A more specific term for a dress is gown
A more specific term for a spice is pepper
A more specific term for a painting is
2024-07-17 20:27:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 20:31:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3491,  0.8213,  0.0461,  ..., -0.1133,  0.3081,  0.4868],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.5469, -2.7402, -4.1445,  ..., -1.3613, -1.7422,  1.2285],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0145, -0.0002,  0.0100,  ...,  0.0009,  0.0080, -0.0068],
        [-0.0003,  0.0125,  0.0025,  ...,  0.0223, -0.0251,  0.0012],
        [-0.0119,  0.0539,  0.0117,  ..., -0.0052,  0.0208, -0.0188],
        ...,
        [ 0.0143,  0.0100,  0.0226,  ...,  0.0265,  0.0062,  0.0064],
        [-0.0194,  0.0391,  0.0074,  ..., -0.0127,  0.0433, -0.0304],
        [-0.0264, -0.0129, -0.0073,  ..., -0.0117,  0.0047,  0.0015]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.4883, -2.0820, -2.6016,  ..., -0.9375, -0.1309, -0.3760]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 20:31:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a month is january
A more specific term for a oven is broiler
A more specific term for a church is chapel
A more specific term for a candy is lollipop
A more specific term for a sofa is divan
A more specific term for a dress is gown
A more specific term for a spice is pepper
A more specific term for a painting is
2024-07-17 20:31:34 root INFO     [order_1_approx] starting weight calculation for A more specific term for a painting is watercolor
A more specific term for a candy is lollipop
A more specific term for a month is january
A more specific term for a church is chapel
A more specific term for a oven is broiler
A more specific term for a sofa is divan
A more specific term for a dress is gown
A more specific term for a spice is
2024-07-17 20:31:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 20:35:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5820,  0.4058, -0.1282,  ..., -0.7856, -0.1720, -0.2695],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.3076,  2.0918, -0.2417,  ...,  1.4287,  3.4980, -1.4258],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0034,  0.0076,  0.0031,  ...,  0.0079,  0.0062, -0.0029],
        [-0.0149,  0.0069, -0.0077,  ..., -0.0055, -0.0194,  0.0159],
        [-0.0080,  0.0174,  0.0204,  ...,  0.0072,  0.0143, -0.0167],
        ...,
        [-0.0006,  0.0037, -0.0136,  ...,  0.0316,  0.0118,  0.0003],
        [-0.0283, -0.0179,  0.0170,  ..., -0.0003,  0.0122,  0.0047],
        [-0.0139,  0.0308, -0.0124,  ..., -0.0087,  0.0219,  0.0095]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6782,  2.1230, -0.6196,  ...,  1.5674,  3.0840, -0.7012]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 20:35:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a painting is watercolor
A more specific term for a candy is lollipop
A more specific term for a month is january
A more specific term for a church is chapel
A more specific term for a oven is broiler
A more specific term for a sofa is divan
A more specific term for a dress is gown
A more specific term for a spice is
2024-07-17 20:35:41 root INFO     [order_1_approx] starting weight calculation for A more specific term for a candy is lollipop
A more specific term for a dress is gown
A more specific term for a spice is pepper
A more specific term for a painting is watercolor
A more specific term for a church is chapel
A more specific term for a oven is broiler
A more specific term for a sofa is divan
A more specific term for a month is
2024-07-17 20:35:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 20:39:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3164, -0.0968,  0.2830,  ...,  0.5225, -0.7705, -0.0186],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.5430, -1.3037, -7.9297,  ...,  0.7227,  2.9785, -1.7783],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-7.9956e-03, -7.7744e-03, -5.9700e-03,  ...,  6.6338e-03,
          6.0043e-03, -1.7090e-02],
        [-1.1511e-03, -2.0752e-03,  1.3039e-02,  ...,  1.5228e-02,
         -1.1261e-02, -1.9226e-03],
        [ 1.4961e-02,  2.2079e-02, -1.0147e-02,  ..., -1.0025e-02,
          2.3041e-03, -1.9653e-02],
        ...,
        [-1.5091e-02,  2.2858e-02, -1.6327e-03,  ..., -3.1700e-03,
         -2.2621e-03,  2.0050e-02],
        [-3.3188e-04, -3.7155e-03, -1.3245e-02,  ..., -1.1887e-02,
         -3.0785e-03,  6.8893e-03],
        [ 1.3695e-03, -3.6240e-05, -1.1627e-02,  ...,  4.6539e-03,
          1.9852e-02,  5.3749e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.7969, -1.2588, -7.9570,  ...,  1.9229,  2.5430, -2.6875]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 20:39:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a candy is lollipop
A more specific term for a dress is gown
A more specific term for a spice is pepper
A more specific term for a painting is watercolor
A more specific term for a church is chapel
A more specific term for a oven is broiler
A more specific term for a sofa is divan
A more specific term for a month is
2024-07-17 20:39:46 root INFO     [order_1_approx] starting weight calculation for A more specific term for a month is january
A more specific term for a dress is gown
A more specific term for a painting is watercolor
A more specific term for a oven is broiler
A more specific term for a candy is lollipop
A more specific term for a sofa is divan
A more specific term for a spice is pepper
A more specific term for a church is
2024-07-17 20:39:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 20:43:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3953, -0.0968, -0.2490,  ...,  0.3445, -0.4651,  0.0138],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0430,  2.0859,  0.5625,  ..., -0.3613, -0.5908, -0.7744],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0065, -0.0240,  0.0258,  ..., -0.0008, -0.0085, -0.0169],
        [ 0.0043, -0.0097,  0.0078,  ...,  0.0229, -0.0168, -0.0116],
        [-0.0226, -0.0015, -0.0072,  ..., -0.0008,  0.0162, -0.0069],
        ...,
        [-0.0035, -0.0179,  0.0121,  ...,  0.0113, -0.0093,  0.0065],
        [-0.0058,  0.0066, -0.0059,  ...,  0.0070,  0.0207,  0.0015],
        [-0.0061, -0.0117,  0.0068,  ..., -0.0051,  0.0023,  0.0209]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9111,  1.4033, -0.3330,  ...,  0.5786, -1.5508, -0.7510]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 20:43:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a month is january
A more specific term for a dress is gown
A more specific term for a painting is watercolor
A more specific term for a oven is broiler
A more specific term for a candy is lollipop
A more specific term for a sofa is divan
A more specific term for a spice is pepper
A more specific term for a church is
2024-07-17 20:43:53 root INFO     [order_1_approx] starting weight calculation for A more specific term for a dress is gown
A more specific term for a spice is pepper
A more specific term for a candy is lollipop
A more specific term for a oven is broiler
A more specific term for a month is january
A more specific term for a church is chapel
A more specific term for a painting is watercolor
A more specific term for a sofa is
2024-07-17 20:43:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 20:47:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0264,  0.0039, -0.7412,  ...,  0.5117, -0.6470,  0.1809],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4824, -1.7393, -1.7314,  ..., -3.0820, -0.4734,  2.1172],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0048, -0.0117,  0.0080,  ..., -0.0016,  0.0181, -0.0110],
        [ 0.0021,  0.0051,  0.0038,  ...,  0.0071, -0.0068,  0.0019],
        [-0.0382,  0.0077,  0.0049,  ..., -0.0125,  0.0078, -0.0034],
        ...,
        [-0.0086,  0.0087,  0.0334,  ...,  0.0247, -0.0032,  0.0012],
        [-0.0038,  0.0108,  0.0134,  ...,  0.0140,  0.0225,  0.0024],
        [-0.0088, -0.0084,  0.0073,  ..., -0.0134, -0.0020,  0.0285]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.8115, -2.5039, -1.5615,  ..., -1.8115,  0.4070,  0.2695]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 20:47:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a dress is gown
A more specific term for a spice is pepper
A more specific term for a candy is lollipop
A more specific term for a oven is broiler
A more specific term for a month is january
A more specific term for a church is chapel
A more specific term for a painting is watercolor
A more specific term for a sofa is
2024-07-17 20:47:58 root INFO     [order_1_approx] starting weight calculation for A more specific term for a oven is broiler
A more specific term for a sofa is divan
A more specific term for a church is chapel
A more specific term for a painting is watercolor
A more specific term for a spice is pepper
A more specific term for a candy is lollipop
A more specific term for a month is january
A more specific term for a dress is
2024-07-17 20:47:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 20:52:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5586,  0.0923,  0.0832,  ...,  0.4370, -0.3303, -0.1754],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2520, -1.8281, -2.7070,  ...,  0.0303, -0.0923, -1.8018],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0110, -0.0003,  0.0124,  ...,  0.0215,  0.0040,  0.0038],
        [ 0.0017,  0.0160,  0.0075,  ...,  0.0145, -0.0023, -0.0172],
        [-0.0028,  0.0303,  0.0298,  ...,  0.0002,  0.0143,  0.0228],
        ...,
        [-0.0093,  0.0063, -0.0026,  ...,  0.0179, -0.0125,  0.0092],
        [ 0.0346,  0.0157,  0.0071,  ..., -0.0060,  0.0175, -0.0113],
        [-0.0059,  0.0044,  0.0182,  ...,  0.0014,  0.0237,  0.0228]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3354, -0.9307, -2.2305,  ..., -0.6362,  0.5371, -2.1758]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 20:52:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a oven is broiler
A more specific term for a sofa is divan
A more specific term for a church is chapel
A more specific term for a painting is watercolor
A more specific term for a spice is pepper
A more specific term for a candy is lollipop
A more specific term for a month is january
A more specific term for a dress is
2024-07-17 20:52:04 root INFO     [order_1_approx] starting weight calculation for A more specific term for a dress is gown
A more specific term for a spice is pepper
A more specific term for a church is chapel
A more specific term for a oven is broiler
A more specific term for a sofa is divan
A more specific term for a painting is watercolor
A more specific term for a month is january
A more specific term for a candy is
2024-07-17 20:52:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 20:56:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1674,  0.6313,  0.3594,  ..., -0.1924, -0.0383,  0.5264],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.2607,  3.5234, -2.0371,  ..., -2.8398,  1.7207,  0.8081],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0072, -0.0021, -0.0190,  ...,  0.0023,  0.0038,  0.0013],
        [-0.0081, -0.0046, -0.0156,  ...,  0.0061, -0.0257,  0.0047],
        [-0.0046,  0.0267, -0.0054,  ..., -0.0117,  0.0197, -0.0083],
        ...,
        [ 0.0116, -0.0037,  0.0043,  ..., -0.0024, -0.0126,  0.0017],
        [ 0.0066, -0.0038,  0.0033,  ..., -0.0163,  0.0174,  0.0046],
        [ 0.0029,  0.0177,  0.0021,  ..., -0.0117,  0.0130, -0.0089]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0791,  2.9746, -2.1074,  ..., -2.8809,  0.8652,  0.9019]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 20:56:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a dress is gown
A more specific term for a spice is pepper
A more specific term for a church is chapel
A more specific term for a oven is broiler
A more specific term for a sofa is divan
A more specific term for a painting is watercolor
A more specific term for a month is january
A more specific term for a candy is
2024-07-17 20:56:11 root INFO     [order_1_approx] starting weight calculation for A more specific term for a month is january
A more specific term for a painting is watercolor
A more specific term for a church is chapel
A more specific term for a dress is gown
A more specific term for a candy is lollipop
A more specific term for a spice is pepper
A more specific term for a sofa is divan
A more specific term for a oven is
2024-07-17 20:56:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 21:00:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1976,  0.3643, -0.3572,  ...,  0.1978, -0.3228,  0.3896],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.8184,  2.1992, -0.2505,  ..., -2.2480,  1.1953,  2.3711],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.2837e-02, -7.6332e-03, -3.3226e-03,  ...,  1.1139e-02,
          2.3758e-02, -3.2684e-02],
        [ 3.3894e-03,  9.2545e-03,  8.0109e-04,  ...,  4.2381e-03,
         -1.7166e-02,  1.5198e-02],
        [-2.4414e-02,  1.4290e-02,  1.1612e-02,  ..., -8.8577e-03,
         -4.8637e-03, -2.2144e-03],
        ...,
        [-7.9803e-03,  1.0231e-02, -5.3291e-03,  ...,  2.2247e-02,
         -1.1444e-02,  2.3270e-02],
        [-8.5497e-04,  5.0430e-03, -8.3923e-05,  ..., -1.5984e-03,
          2.5299e-02, -5.4741e-03],
        [ 2.3041e-03, -1.0963e-02, -2.1420e-03,  ..., -1.2680e-02,
          3.0533e-02, -3.6240e-04]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.5000,  2.6191, -0.7563,  ..., -2.6387,  2.1250,  1.0117]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 21:00:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a month is january
A more specific term for a painting is watercolor
A more specific term for a church is chapel
A more specific term for a dress is gown
A more specific term for a candy is lollipop
A more specific term for a spice is pepper
A more specific term for a sofa is divan
A more specific term for a oven is
2024-07-17 21:00:17 root INFO     total operator prediction time: 1968.3146419525146 seconds
2024-07-17 21:00:17 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on antonyms - binary
2024-07-17 21:00:17 root INFO     building operator antonyms - binary
2024-07-17 21:00:17 root INFO     [order_1_approx] starting weight calculation for The opposite of forget is remember
The opposite of internal is external
The opposite of mortal is immortal
The opposite of off is on
The opposite of downslope is upslope
The opposite of employ is dismiss
The opposite of previously is subsequently
The opposite of occupied is
2024-07-17 21:00:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 21:04:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5625, -0.1918,  0.5889,  ..., -0.4146, -0.3926, -0.5254],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4736,  1.1836, -5.9062,  ...,  0.4033,  1.6748, -4.3438],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0285, -0.0195, -0.0153,  ...,  0.0055, -0.0260, -0.0056],
        [-0.0200, -0.0132, -0.0316,  ..., -0.0234,  0.0064, -0.0174],
        [-0.0028,  0.0029, -0.0171,  ...,  0.0267, -0.0253, -0.0294],
        ...,
        [ 0.0045, -0.0284, -0.0099,  ..., -0.0062,  0.0011,  0.0201],
        [-0.0022,  0.0057,  0.0190,  ...,  0.0199, -0.0291,  0.0146],
        [-0.0044,  0.0093, -0.0244,  ..., -0.0051,  0.0059, -0.0086]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.2207,  1.6855, -5.8906,  ...,  0.5586,  1.6299, -3.5566]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 21:04:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of forget is remember
The opposite of internal is external
The opposite of mortal is immortal
The opposite of off is on
The opposite of downslope is upslope
The opposite of employ is dismiss
The opposite of previously is subsequently
The opposite of occupied is
2024-07-17 21:04:24 root INFO     [order_1_approx] starting weight calculation for The opposite of mortal is immortal
The opposite of previously is subsequently
The opposite of forget is remember
The opposite of occupied is vacant
The opposite of internal is external
The opposite of off is on
The opposite of downslope is upslope
The opposite of employ is
2024-07-17 21:04:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 21:08:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6528,  0.0204,  0.2776,  ...,  0.9814, -0.9814, -0.1182],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9897,  0.6255, -3.2305,  ...,  0.9277,  0.8838,  0.1338],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0208,  0.0019, -0.0111,  ...,  0.0051,  0.0015,  0.0032],
        [ 0.0100,  0.0027,  0.0029,  ...,  0.0191, -0.0284,  0.0014],
        [ 0.0118, -0.0273, -0.0215,  ..., -0.0214, -0.0189, -0.0109],
        ...,
        [-0.0055, -0.0216, -0.0269,  ..., -0.0157, -0.0130, -0.0139],
        [-0.0107,  0.0130,  0.0100,  ...,  0.0019,  0.0010, -0.0015],
        [-0.0076,  0.0024,  0.0208,  ..., -0.0273, -0.0007, -0.0113]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5215,  0.5850, -1.7559,  ...,  1.2793,  0.6079,  0.9019]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 21:08:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of mortal is immortal
The opposite of previously is subsequently
The opposite of forget is remember
The opposite of occupied is vacant
The opposite of internal is external
The opposite of off is on
The opposite of downslope is upslope
The opposite of employ is
2024-07-17 21:08:31 root INFO     [order_1_approx] starting weight calculation for The opposite of employ is dismiss
The opposite of off is on
The opposite of mortal is immortal
The opposite of internal is external
The opposite of forget is remember
The opposite of occupied is vacant
The opposite of downslope is upslope
The opposite of previously is
2024-07-17 21:08:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 21:12:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3997,  0.8696,  0.4771,  ...,  0.0187, -0.5332, -0.0959],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7773, -2.1953, -0.3857,  ...,  3.0293,  1.5645,  0.3428],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0119,  0.0005, -0.0018,  ...,  0.0146, -0.0113, -0.0024],
        [-0.0252,  0.0021, -0.0246,  ...,  0.0052,  0.0044,  0.0118],
        [ 0.0144,  0.0031, -0.0165,  ..., -0.0066,  0.0128, -0.0324],
        ...,
        [ 0.0057, -0.0156, -0.0189,  ..., -0.0243, -0.0257,  0.0122],
        [-0.0072,  0.0122,  0.0172,  ..., -0.0323, -0.0220,  0.0113],
        [ 0.0068, -0.0136,  0.0191,  ...,  0.0109, -0.0026, -0.0002]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4512, -4.3750, -2.4219,  ...,  4.3828,  1.0137,  1.6162]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 21:12:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of employ is dismiss
The opposite of off is on
The opposite of mortal is immortal
The opposite of internal is external
The opposite of forget is remember
The opposite of occupied is vacant
The opposite of downslope is upslope
The opposite of previously is
2024-07-17 21:12:39 root INFO     [order_1_approx] starting weight calculation for The opposite of forget is remember
The opposite of downslope is upslope
The opposite of employ is dismiss
The opposite of mortal is immortal
The opposite of previously is subsequently
The opposite of occupied is vacant
The opposite of off is on
The opposite of internal is
2024-07-17 21:12:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 21:16:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5381,  0.8252, -0.0512,  ...,  0.3022, -0.2046,  0.5420],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1201, -1.6260, -1.9512,  ..., -2.4141,  2.3691,  0.7236],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 9.1553e-05, -1.8173e-02,  1.6907e-02,  ...,  2.0264e-02,
         -2.5360e-02,  7.6599e-03],
        [-1.0399e-02,  6.3667e-03, -1.2932e-02,  ...,  1.3992e-02,
         -1.5465e-02, -4.3182e-03],
        [-1.0910e-02,  9.4147e-03, -3.5095e-02,  ...,  9.9106e-03,
         -3.2349e-03,  4.7684e-03],
        ...,
        [-9.0485e-03,  5.8060e-03,  6.8359e-03,  ...,  2.6764e-02,
         -1.7120e-02,  1.6388e-02],
        [-1.2970e-02,  1.0345e-02,  4.6692e-03,  ...,  1.5173e-03,
         -8.9645e-03,  1.4877e-02],
        [-1.3962e-02, -3.5210e-03, -1.1192e-02,  ...,  2.6123e-02,
          3.2768e-03,  7.0953e-04]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0536, -0.5117, -1.2852,  ..., -2.9727,  1.0986, -0.1235]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 21:16:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of forget is remember
The opposite of downslope is upslope
The opposite of employ is dismiss
The opposite of mortal is immortal
The opposite of previously is subsequently
The opposite of occupied is vacant
The opposite of off is on
The opposite of internal is
2024-07-17 21:16:47 root INFO     [order_1_approx] starting weight calculation for The opposite of occupied is vacant
The opposite of previously is subsequently
The opposite of employ is dismiss
The opposite of off is on
The opposite of internal is external
The opposite of mortal is immortal
The opposite of downslope is upslope
The opposite of forget is
2024-07-17 21:16:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 21:20:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6787,  0.4185,  0.2284,  ...,  0.5195, -0.3574, -0.0092],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0703, -1.1865, -4.9844,  ...,  0.3838,  3.8516,  5.1992],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0116,  0.0156,  0.0020,  ...,  0.0079, -0.0204, -0.0144],
        [-0.0127, -0.0289, -0.0091,  ..., -0.0039, -0.0032,  0.0038],
        [ 0.0117, -0.0005, -0.0281,  ...,  0.0271,  0.0003, -0.0307],
        ...,
        [-0.0086, -0.0017, -0.0100,  ..., -0.0113, -0.0213, -0.0012],
        [ 0.0014,  0.0144, -0.0113,  ..., -0.0067,  0.0182, -0.0118],
        [-0.0043,  0.0254,  0.0118,  ..., -0.0030,  0.0061, -0.0069]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8003, -2.9375, -3.7148,  ...,  0.2559,  2.5195,  6.9062]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 21:20:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of occupied is vacant
The opposite of previously is subsequently
The opposite of employ is dismiss
The opposite of off is on
The opposite of internal is external
The opposite of mortal is immortal
The opposite of downslope is upslope
The opposite of forget is
2024-07-17 21:20:50 root INFO     [order_1_approx] starting weight calculation for The opposite of mortal is immortal
The opposite of employ is dismiss
The opposite of internal is external
The opposite of off is on
The opposite of forget is remember
The opposite of occupied is vacant
The opposite of previously is subsequently
The opposite of downslope is
2024-07-17 21:20:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 21:24:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1914,  0.3101,  0.2764,  ..., -0.9419, -0.1689, -0.3394],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7480, -3.0176,  2.3066,  ...,  1.2715,  3.5586, -0.6387],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0146,  0.0059, -0.0196,  ..., -0.0065, -0.0195, -0.0076],
        [ 0.0046, -0.0157, -0.0031,  ..., -0.0172,  0.0017,  0.0120],
        [-0.0025, -0.0012, -0.0056,  ..., -0.0226,  0.0070, -0.0139],
        ...,
        [-0.0157, -0.0116, -0.0113,  ..., -0.0078, -0.0264, -0.0033],
        [-0.0065,  0.0167,  0.0013,  ...,  0.0037, -0.0192,  0.0030],
        [-0.0005,  0.0022, -0.0050,  ...,  0.0038, -0.0045, -0.0165]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.1016, -2.8965,  2.3789,  ...,  1.1201,  3.2891, -0.7505]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 21:24:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of mortal is immortal
The opposite of employ is dismiss
The opposite of internal is external
The opposite of off is on
The opposite of forget is remember
The opposite of occupied is vacant
The opposite of previously is subsequently
The opposite of downslope is
2024-07-17 21:24:55 root INFO     [order_1_approx] starting weight calculation for The opposite of forget is remember
The opposite of downslope is upslope
The opposite of occupied is vacant
The opposite of employ is dismiss
The opposite of internal is external
The opposite of mortal is immortal
The opposite of previously is subsequently
The opposite of off is
2024-07-17 21:24:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 21:29:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1064, -0.0330, -0.3877,  ...,  0.4800, -1.2637, -0.1002],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7700, -3.7852, -0.8379,  ..., -0.9556,  2.5273,  1.3281],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0079, -0.0123, -0.0036,  ...,  0.0043, -0.0214,  0.0067],
        [-0.0056,  0.0012, -0.0025,  ...,  0.0264, -0.0079,  0.0151],
        [ 0.0008,  0.0109, -0.0280,  ..., -0.0308,  0.0110,  0.0083],
        ...,
        [ 0.0117,  0.0056, -0.0118,  ...,  0.0199, -0.0062,  0.0071],
        [-0.0137, -0.0027,  0.0015,  ..., -0.0321, -0.0051, -0.0094],
        [-0.0264, -0.0132, -0.0034,  ..., -0.0060,  0.0406,  0.0219]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.0762e-02, -5.1484e+00, -1.4648e-03,  ..., -9.7705e-01,
          1.6875e+00,  9.4727e-01]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SubBackward0>) 

                    
2024-07-17 21:29:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of forget is remember
The opposite of downslope is upslope
The opposite of occupied is vacant
The opposite of employ is dismiss
The opposite of internal is external
The opposite of mortal is immortal
The opposite of previously is subsequently
The opposite of off is
2024-07-17 21:29:02 root INFO     [order_1_approx] starting weight calculation for The opposite of employ is dismiss
The opposite of internal is external
The opposite of downslope is upslope
The opposite of off is on
The opposite of occupied is vacant
The opposite of previously is subsequently
The opposite of forget is remember
The opposite of mortal is
2024-07-17 21:29:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 21:33:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3984,  0.9761, -0.6230,  ..., -0.2588, -1.0703, -0.6611],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6221,  1.2148, -1.0811,  ..., -3.4375,  1.5869,  4.8984],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0031, -0.0270, -0.0222,  ...,  0.0039, -0.0199,  0.0103],
        [-0.0100, -0.0012, -0.0142,  ...,  0.0029, -0.0122,  0.0125],
        [-0.0143, -0.0011, -0.0466,  ...,  0.0196, -0.0184, -0.0138],
        ...,
        [-0.0017, -0.0384, -0.0012,  ..., -0.0017,  0.0083,  0.0038],
        [ 0.0089, -0.0078,  0.0291,  ...,  0.0274, -0.0263, -0.0281],
        [-0.0021, -0.0302, -0.0071,  ..., -0.0158,  0.0224, -0.0021]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1738,  0.5918,  0.2715,  ..., -2.3828,  2.1055,  4.1797]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 21:33:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of employ is dismiss
The opposite of internal is external
The opposite of downslope is upslope
The opposite of off is on
The opposite of occupied is vacant
The opposite of previously is subsequently
The opposite of forget is remember
The opposite of mortal is
2024-07-17 21:33:09 root INFO     total operator prediction time: 1972.6614081859589 seconds
2024-07-17 21:33:09 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - member
2024-07-17 21:33:09 root INFO     building operator meronyms - member
2024-07-17 21:33:10 root INFO     [order_1_approx] starting weight calculation for A student is a member of a class
A christian is a member of a congregation
A crow is a member of a murder
A calf is a member of a cattle
A song is a member of a album
A kitten is a member of a litter
A lion is a member of a pride
A galaxy is a member of a
2024-07-17 21:33:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 21:37:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3267, -0.0085, -0.2351,  ..., -0.7163, -0.1162,  0.3633],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.2109, -1.7656,  4.0859,  ...,  0.3354, -3.8320,  2.6797],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0267,  0.0014, -0.0080,  ...,  0.0134, -0.0079, -0.0041],
        [ 0.0103,  0.0017,  0.0052,  ...,  0.0132, -0.0061, -0.0234],
        [-0.0086,  0.0068,  0.0073,  ..., -0.0266, -0.0052,  0.0049],
        ...,
        [ 0.0014,  0.0176,  0.0081,  ..., -0.0087, -0.0041, -0.0091],
        [ 0.0210,  0.0038,  0.0216,  ...,  0.0058,  0.0069, -0.0016],
        [-0.0123,  0.0097, -0.0110,  ...,  0.0017,  0.0183,  0.0145]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.8555, -2.0684,  4.0898,  ...,  0.7988, -2.9180,  3.2227]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 21:37:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A student is a member of a class
A christian is a member of a congregation
A crow is a member of a murder
A calf is a member of a cattle
A song is a member of a album
A kitten is a member of a litter
A lion is a member of a pride
A galaxy is a member of a
2024-07-17 21:37:18 root INFO     [order_1_approx] starting weight calculation for A song is a member of a album
A crow is a member of a murder
A calf is a member of a cattle
A kitten is a member of a litter
A galaxy is a member of a universe
A lion is a member of a pride
A christian is a member of a congregation
A student is a member of a
2024-07-17 21:37:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 21:41:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5322,  0.7129,  1.1660,  ...,  0.8027, -0.2996,  0.2734],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8667,  2.5957, -1.1426,  ...,  0.9902,  2.5605,  2.0762],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0046,  0.0011,  0.0044,  ..., -0.0054, -0.0049, -0.0038],
        [ 0.0084,  0.0031, -0.0041,  ..., -0.0144,  0.0037, -0.0081],
        [ 0.0036,  0.0197, -0.0025,  ...,  0.0027,  0.0030, -0.0023],
        ...,
        [ 0.0002, -0.0028, -0.0021,  ..., -0.0019, -0.0114, -0.0009],
        [ 0.0005, -0.0138,  0.0069,  ...,  0.0012, -0.0136,  0.0157],
        [-0.0035,  0.0014, -0.0107,  ...,  0.0022, -0.0002, -0.0041]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5327,  2.3535, -0.3047,  ...,  1.0518,  3.0156,  2.3379]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 21:41:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A song is a member of a album
A crow is a member of a murder
A calf is a member of a cattle
A kitten is a member of a litter
A galaxy is a member of a universe
A lion is a member of a pride
A christian is a member of a congregation
A student is a member of a
2024-07-17 21:41:27 root INFO     [order_1_approx] starting weight calculation for A christian is a member of a congregation
A calf is a member of a cattle
A galaxy is a member of a universe
A student is a member of a class
A song is a member of a album
A lion is a member of a pride
A kitten is a member of a litter
A crow is a member of a
2024-07-17 21:41:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 21:45:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3809, -1.3184, -0.2739,  ...,  0.8286, -0.4392,  0.2539],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6787,  0.7285,  0.1323,  ...,  2.3145,  0.3418,  2.9453],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0003, -0.0028, -0.0010,  ...,  0.0169, -0.0289, -0.0197],
        [-0.0125,  0.0229,  0.0183,  ...,  0.0104,  0.0022,  0.0182],
        [-0.0010, -0.0086,  0.0030,  ...,  0.0040,  0.0013,  0.0011],
        ...,
        [-0.0077, -0.0064, -0.0075,  ...,  0.0051,  0.0052, -0.0148],
        [ 0.0106, -0.0040, -0.0050,  ...,  0.0175, -0.0045, -0.0013],
        [-0.0009, -0.0096, -0.0054,  ...,  0.0022, -0.0080,  0.0100]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2988, -0.1904,  1.0234,  ...,  2.7148,  1.6348,  3.8887]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 21:45:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A christian is a member of a congregation
A calf is a member of a cattle
A galaxy is a member of a universe
A student is a member of a class
A song is a member of a album
A lion is a member of a pride
A kitten is a member of a litter
A crow is a member of a
2024-07-17 21:45:34 root INFO     [order_1_approx] starting weight calculation for A song is a member of a album
A lion is a member of a pride
A christian is a member of a congregation
A galaxy is a member of a universe
A kitten is a member of a litter
A student is a member of a class
A crow is a member of a murder
A calf is a member of a
2024-07-17 21:45:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 21:49:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5547, -0.0703, -0.8203,  ...,  0.2313, -0.8125, -0.0668],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4805,  3.1484, -4.3789,  ..., -2.5039, -0.7139,  1.7969],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0142, -0.0075, -0.0067,  ..., -0.0011, -0.0005, -0.0004],
        [-0.0068,  0.0203, -0.0061,  ...,  0.0248, -0.0256, -0.0014],
        [-0.0273,  0.0075,  0.0137,  ...,  0.0050,  0.0242, -0.0161],
        ...,
        [ 0.0030,  0.0006, -0.0141,  ..., -0.0101, -0.0103, -0.0068],
        [ 0.0211,  0.0014,  0.0277,  ..., -0.0069,  0.0097, -0.0050],
        [-0.0112,  0.0030, -0.0202,  ..., -0.0038, -0.0024, -0.0023]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.9902,  2.4922, -3.0859,  ..., -2.0547, -0.2278,  3.5703]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 21:49:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A song is a member of a album
A lion is a member of a pride
A christian is a member of a congregation
A galaxy is a member of a universe
A kitten is a member of a litter
A student is a member of a class
A crow is a member of a murder
A calf is a member of a
2024-07-17 21:49:41 root INFO     [order_1_approx] starting weight calculation for A student is a member of a class
A lion is a member of a pride
A song is a member of a album
A galaxy is a member of a universe
A crow is a member of a murder
A christian is a member of a congregation
A calf is a member of a cattle
A kitten is a member of a
2024-07-17 21:49:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 21:53:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2104, -0.5586, -0.9717,  ...,  0.5547, -0.4795,  0.0273],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7168,  1.1416, -1.6348,  ..., -0.3027, -0.3391,  1.6553],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0055, -0.0168, -0.0048,  ...,  0.0177, -0.0132, -0.0176],
        [ 0.0106,  0.0008,  0.0064,  ..., -0.0062,  0.0102,  0.0101],
        [-0.0157,  0.0017, -0.0101,  ..., -0.0024,  0.0046,  0.0047],
        ...,
        [-0.0009, -0.0045,  0.0042,  ...,  0.0143, -0.0190, -0.0020],
        [ 0.0232,  0.0064,  0.0171,  ..., -0.0109, -0.0092, -0.0042],
        [ 0.0008, -0.0198, -0.0135,  ..., -0.0094,  0.0128, -0.0073]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1553,  1.6582, -0.4863,  ..., -0.3823,  0.2288,  2.2422]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 21:53:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A student is a member of a class
A lion is a member of a pride
A song is a member of a album
A galaxy is a member of a universe
A crow is a member of a murder
A christian is a member of a congregation
A calf is a member of a cattle
A kitten is a member of a
2024-07-17 21:53:48 root INFO     [order_1_approx] starting weight calculation for A kitten is a member of a litter
A song is a member of a album
A lion is a member of a pride
A galaxy is a member of a universe
A student is a member of a class
A calf is a member of a cattle
A crow is a member of a murder
A christian is a member of a
2024-07-17 21:53:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 21:57:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0913, -1.1396, -0.4424,  ...,  0.4814, -0.6333,  0.2605],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4500, -1.3418, -3.4668,  ...,  0.3506, -1.3613, -3.5371],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0022,  0.0011,  0.0004,  ...,  0.0021,  0.0089, -0.0037],
        [ 0.0117, -0.0126, -0.0093,  ...,  0.0089,  0.0074,  0.0027],
        [-0.0031,  0.0059,  0.0139,  ..., -0.0063,  0.0026, -0.0034],
        ...,
        [ 0.0018,  0.0038, -0.0014,  ...,  0.0080,  0.0034,  0.0096],
        [ 0.0016, -0.0100, -0.0013,  ..., -0.0026, -0.0170, -0.0095],
        [-0.0049, -0.0042, -0.0057,  ...,  0.0040, -0.0014, -0.0072]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2568, -0.9429, -3.7188,  ...,  1.2617, -1.5732, -3.3281]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 21:57:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A kitten is a member of a litter
A song is a member of a album
A lion is a member of a pride
A galaxy is a member of a universe
A student is a member of a class
A calf is a member of a cattle
A crow is a member of a murder
A christian is a member of a
2024-07-17 21:57:56 root INFO     [order_1_approx] starting weight calculation for A galaxy is a member of a universe
A crow is a member of a murder
A calf is a member of a cattle
A lion is a member of a pride
A christian is a member of a congregation
A kitten is a member of a litter
A student is a member of a class
A song is a member of a
2024-07-17 21:57:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 22:02:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3013, -0.3779, -0.3088,  ...,  0.5020,  0.1155, -0.9702],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1855,  2.9395, -0.4419,  ...,  1.6855,  4.5000, -4.4219],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0031,  0.0039, -0.0092,  ...,  0.0073,  0.0087, -0.0080],
        [-0.0190,  0.0041,  0.0047,  ...,  0.0100, -0.0024, -0.0074],
        [ 0.0134,  0.0229, -0.0275,  ..., -0.0026, -0.0003,  0.0095],
        ...,
        [-0.0016,  0.0011, -0.0033,  ...,  0.0137, -0.0118,  0.0094],
        [-0.0069, -0.0045,  0.0020,  ...,  0.0001, -0.0022,  0.0049],
        [ 0.0165,  0.0145, -0.0017,  ...,  0.0076,  0.0336, -0.0070]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2839,  2.5020, -0.5991,  ...,  1.3691,  4.0820, -3.5898]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 22:02:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A galaxy is a member of a universe
A crow is a member of a murder
A calf is a member of a cattle
A lion is a member of a pride
A christian is a member of a congregation
A kitten is a member of a litter
A student is a member of a class
A song is a member of a
2024-07-17 22:02:04 root INFO     [order_1_approx] starting weight calculation for A galaxy is a member of a universe
A song is a member of a album
A calf is a member of a cattle
A kitten is a member of a litter
A crow is a member of a murder
A christian is a member of a congregation
A student is a member of a class
A lion is a member of a
2024-07-17 22:02:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 22:06:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2825, -0.8105, -0.0240,  ...,  1.1592, -0.2651, -0.2495],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.9395,  1.9678,  0.5581,  ...,  0.7407, -3.7168,  2.9668],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0157, -0.0114, -0.0095,  ..., -0.0024, -0.0086, -0.0113],
        [ 0.0027, -0.0228, -0.0138,  ...,  0.0081,  0.0046,  0.0013],
        [-0.0065,  0.0036,  0.0114,  ..., -0.0077, -0.0087,  0.0069],
        ...,
        [ 0.0013, -0.0257, -0.0125,  ...,  0.0130, -0.0062,  0.0096],
        [ 0.0281, -0.0057,  0.0169,  ..., -0.0128, -0.0145,  0.0067],
        [-0.0056,  0.0046,  0.0021,  ...,  0.0037,  0.0088,  0.0195]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.6016,  2.0938,  1.3936,  ..., -0.0273, -3.0059,  3.4375]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 22:06:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A galaxy is a member of a universe
A song is a member of a album
A calf is a member of a cattle
A kitten is a member of a litter
A crow is a member of a murder
A christian is a member of a congregation
A student is a member of a class
A lion is a member of a
2024-07-17 22:06:11 root INFO     total operator prediction time: 1981.85440325737 seconds
2024-07-17 22:06:11 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on noun - plural_irreg
2024-07-17 22:06:11 root INFO     building operator noun - plural_irreg
2024-07-17 22:06:12 root INFO     [order_1_approx] starting weight calculation for The plural form of child is children
The plural form of facility is facilities
The plural form of majority is majorities
The plural form of memory is memories
The plural form of community is communities
The plural form of duty is duties
The plural form of authority is authorities
The plural form of security is
2024-07-17 22:06:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 22:10:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1196, -0.2773,  0.0380,  ...,  0.2603, -0.4727,  0.4634],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.8223,  3.3555,  1.9189,  ..., -2.0508, -2.1875, -0.4727],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.7197e-02, -1.6037e-02,  2.3575e-02,  ...,  4.9591e-04,
         -5.6982e-04, -2.7924e-03],
        [-7.5989e-03, -2.2858e-02, -3.0518e-03,  ..., -9.1553e-04,
          1.2238e-02,  2.3556e-03],
        [ 1.2100e-02,  1.5965e-03, -1.7586e-03,  ..., -7.7744e-03,
          4.9591e-05,  2.0676e-03],
        ...,
        [-1.6357e-02, -1.7731e-02, -2.8858e-03,  ...,  8.3008e-03,
          1.2878e-02, -1.1894e-02],
        [-1.5411e-03, -2.1362e-02, -2.0256e-03,  ..., -1.4725e-03,
         -1.3817e-02,  1.4313e-02],
        [-5.9891e-03,  1.5991e-02,  7.0877e-03,  ..., -6.8893e-03,
          8.0872e-04, -3.3295e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6318,  3.3809,  1.2969,  ..., -1.6836, -2.6562, -1.1211]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 22:10:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of child is children
The plural form of facility is facilities
The plural form of majority is majorities
The plural form of memory is memories
The plural form of community is communities
The plural form of duty is duties
The plural form of authority is authorities
The plural form of security is
2024-07-17 22:10:19 root INFO     [order_1_approx] starting weight calculation for The plural form of community is communities
The plural form of majority is majorities
The plural form of security is securities
The plural form of facility is facilities
The plural form of authority is authorities
The plural form of duty is duties
The plural form of memory is memories
The plural form of child is
2024-07-17 22:10:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 22:14:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8965, -0.2725, -0.1147,  ...,  0.3940, -0.7056,  0.6299],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7559, -3.3984, -3.3730,  ...,  0.1436,  3.4688,  1.1064],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0214, -0.0059,  0.0205,  ...,  0.0076, -0.0033, -0.0177],
        [-0.0023, -0.0060, -0.0043,  ...,  0.0048,  0.0024, -0.0061],
        [ 0.0243,  0.0195, -0.0246,  ..., -0.0134,  0.0069,  0.0038],
        ...,
        [ 0.0021,  0.0030, -0.0094,  ..., -0.0134, -0.0056,  0.0096],
        [ 0.0056,  0.0025,  0.0043,  ..., -0.0013, -0.0190,  0.0166],
        [-0.0066, -0.0054, -0.0090,  ..., -0.0145,  0.0037, -0.0322]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.1758, -2.8359, -2.9609,  ...,  0.5215,  3.0371,  0.9883]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 22:14:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of community is communities
The plural form of majority is majorities
The plural form of security is securities
The plural form of facility is facilities
The plural form of authority is authorities
The plural form of duty is duties
The plural form of memory is memories
The plural form of child is
2024-07-17 22:14:26 root INFO     [order_1_approx] starting weight calculation for The plural form of duty is duties
The plural form of majority is majorities
The plural form of child is children
The plural form of security is securities
The plural form of memory is memories
The plural form of community is communities
The plural form of authority is authorities
The plural form of facility is
2024-07-17 22:14:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 22:18:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5596, -0.3923, -1.2803,  ..., -0.2764, -0.8770, -0.3052],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1973,  4.0039,  0.8486,  ..., -1.5566,  1.3652,  4.6914],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0173, -0.0019,  0.0197,  ...,  0.0015,  0.0147, -0.0128],
        [-0.0194,  0.0094, -0.0101,  ..., -0.0087,  0.0018,  0.0205],
        [ 0.0116,  0.0103, -0.0150,  ...,  0.0119,  0.0175,  0.0031],
        ...,
        [-0.0037, -0.0084, -0.0109,  ..., -0.0025, -0.0155,  0.0130],
        [ 0.0287, -0.0233,  0.0073,  ..., -0.0058, -0.0213, -0.0010],
        [ 0.0031,  0.0097, -0.0042,  ...,  0.0022, -0.0047, -0.0244]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.9854,  4.3047,  1.5898,  ..., -1.8223,  1.2520,  4.9727]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 22:18:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of duty is duties
The plural form of majority is majorities
The plural form of child is children
The plural form of security is securities
The plural form of memory is memories
The plural form of community is communities
The plural form of authority is authorities
The plural form of facility is
2024-07-17 22:18:33 root INFO     [order_1_approx] starting weight calculation for The plural form of duty is duties
The plural form of child is children
The plural form of facility is facilities
The plural form of security is securities
The plural form of community is communities
The plural form of memory is memories
The plural form of majority is majorities
The plural form of authority is
2024-07-17 22:18:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 22:22:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7588, -0.7021, -0.1029,  ..., -0.5024, -0.3984,  0.4541],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7695,  0.1553,  0.6133,  ..., -0.4785,  0.7100, -1.5195],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0020, -0.0166,  0.0038,  ...,  0.0008, -0.0067, -0.0079],
        [ 0.0031, -0.0036, -0.0056,  ...,  0.0183,  0.0075, -0.0198],
        [ 0.0187,  0.0302,  0.0001,  ..., -0.0019,  0.0033, -0.0051],
        ...,
        [-0.0209, -0.0296,  0.0050,  ..., -0.0103,  0.0099,  0.0220],
        [ 0.0067, -0.0068, -0.0026,  ..., -0.0119, -0.0292,  0.0091],
        [ 0.0051,  0.0159,  0.0199,  ...,  0.0170,  0.0082,  0.0015]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.2656, -0.1985,  1.5908,  ...,  0.1626, -0.2319, -1.4102]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 22:22:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of duty is duties
The plural form of child is children
The plural form of facility is facilities
The plural form of security is securities
The plural form of community is communities
The plural form of memory is memories
The plural form of majority is majorities
The plural form of authority is
2024-07-17 22:22:40 root INFO     [order_1_approx] starting weight calculation for The plural form of facility is facilities
The plural form of security is securities
The plural form of authority is authorities
The plural form of child is children
The plural form of majority is majorities
The plural form of duty is duties
The plural form of memory is memories
The plural form of community is
2024-07-17 22:22:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 22:26:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2002, -0.3196, -0.0928,  ...,  0.5400, -0.6162,  0.1777],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1152, -0.2642, -1.2686,  ..., -4.8047, -1.1201,  0.9561],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0086, -0.0082,  0.0174,  ..., -0.0091, -0.0213, -0.0212],
        [-0.0105, -0.0060, -0.0119,  ...,  0.0171, -0.0070,  0.0043],
        [ 0.0191,  0.0131, -0.0068,  ..., -0.0101,  0.0024,  0.0045],
        ...,
        [-0.0046, -0.0012, -0.0134,  ..., -0.0035, -0.0004,  0.0150],
        [ 0.0122,  0.0080, -0.0174,  ..., -0.0110, -0.0209,  0.0221],
        [-0.0117,  0.0033,  0.0072,  ..., -0.0086, -0.0014, -0.0243]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7666, -0.3191, -1.7344,  ..., -4.2812, -1.0205,  0.4336]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 22:26:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of facility is facilities
The plural form of security is securities
The plural form of authority is authorities
The plural form of child is children
The plural form of majority is majorities
The plural form of duty is duties
The plural form of memory is memories
The plural form of community is
2024-07-17 22:26:47 root INFO     [order_1_approx] starting weight calculation for The plural form of memory is memories
The plural form of majority is majorities
The plural form of child is children
The plural form of authority is authorities
The plural form of facility is facilities
The plural form of security is securities
The plural form of community is communities
The plural form of duty is
2024-07-17 22:26:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 22:30:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1472,  0.7979,  0.2585,  ..., -0.4436, -1.6270,  0.8726],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.9277,  0.9580, -1.2393,  ..., -2.7578,  3.2988,  0.4785],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0021, -0.0091,  0.0071,  ...,  0.0055, -0.0199, -0.0203],
        [-0.0163,  0.0087, -0.0021,  ...,  0.0013,  0.0019,  0.0062],
        [ 0.0253,  0.0156, -0.0170,  ...,  0.0073,  0.0011, -0.0008],
        ...,
        [-0.0148, -0.0081, -0.0027,  ..., -0.0180,  0.0049,  0.0174],
        [-0.0137,  0.0009, -0.0125,  ..., -0.0017, -0.0106,  0.0064],
        [-0.0133,  0.0075, -0.0125,  ...,  0.0125,  0.0011, -0.0073]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.5195,  1.6211, -1.2061,  ..., -2.4414,  3.3477,  0.6152]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 22:30:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of memory is memories
The plural form of majority is majorities
The plural form of child is children
The plural form of authority is authorities
The plural form of facility is facilities
The plural form of security is securities
The plural form of community is communities
The plural form of duty is
2024-07-17 22:30:53 root INFO     [order_1_approx] starting weight calculation for The plural form of security is securities
The plural form of authority is authorities
The plural form of community is communities
The plural form of majority is majorities
The plural form of child is children
The plural form of duty is duties
The plural form of facility is facilities
The plural form of memory is
2024-07-17 22:30:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 22:34:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1621, -0.0132, -0.1512,  ...,  0.0466, -0.8154,  0.4529],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.8398,  0.9331, -3.6250,  ...,  1.2227, -1.9277,  6.3320],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0140, -0.0232,  0.0038,  ...,  0.0021,  0.0017, -0.0235],
        [-0.0093,  0.0092, -0.0057,  ...,  0.0164, -0.0056,  0.0325],
        [ 0.0060,  0.0163, -0.0044,  ...,  0.0004,  0.0254, -0.0128],
        ...,
        [-0.0067, -0.0031,  0.0154,  ..., -0.0039, -0.0006,  0.0013],
        [ 0.0106,  0.0042, -0.0144,  ...,  0.0029,  0.0047, -0.0176],
        [-0.0163,  0.0016,  0.0164,  ..., -0.0026, -0.0211, -0.0039]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.4023,  1.8447, -3.8770,  ...,  0.9614, -2.4688,  6.0039]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 22:35:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of security is securities
The plural form of authority is authorities
The plural form of community is communities
The plural form of majority is majorities
The plural form of child is children
The plural form of duty is duties
The plural form of facility is facilities
The plural form of memory is
2024-07-17 22:35:00 root INFO     [order_1_approx] starting weight calculation for The plural form of duty is duties
The plural form of child is children
The plural form of facility is facilities
The plural form of memory is memories
The plural form of security is securities
The plural form of community is communities
The plural form of authority is authorities
The plural form of majority is
2024-07-17 22:35:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 22:39:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6304,  0.1084,  0.1899,  ..., -1.5488, -0.6821,  0.0138],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.2520, -1.5078, -1.6602,  ...,  2.5293, -2.8281, -0.8135],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0188, -0.0282,  0.0113,  ...,  0.0032,  0.0052, -0.0190],
        [ 0.0128, -0.0101, -0.0090,  ...,  0.0150,  0.0020, -0.0034],
        [-0.0046, -0.0088, -0.0202,  ..., -0.0049,  0.0151,  0.0133],
        ...,
        [ 0.0043, -0.0132, -0.0246,  ..., -0.0095,  0.0073,  0.0057],
        [-0.0085, -0.0065, -0.0013,  ..., -0.0090, -0.0187, -0.0087],
        [-0.0141,  0.0054,  0.0113,  ..., -0.0005, -0.0032, -0.0243]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0156, -1.3418, -1.2822,  ...,  1.8984, -3.3086, -1.1279]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 22:39:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of duty is duties
The plural form of child is children
The plural form of facility is facilities
The plural form of memory is memories
The plural form of security is securities
The plural form of community is communities
The plural form of authority is authorities
The plural form of majority is
2024-07-17 22:39:07 root INFO     total operator prediction time: 1975.7368123531342 seconds
2024-07-17 22:39:07 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on Ving - verb_inf
2024-07-17 22:39:07 root INFO     building operator Ving - verb_inf
2024-07-17 22:39:07 root INFO     [order_1_approx] starting weight calculation for developing is the active form of develop
creating is the active form of create
preventing is the active form of prevent
applying is the active form of apply
requiring is the active form of require
attending is the active form of attend
protecting is the active form of protect
becoming is the active form of
2024-07-17 22:39:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 22:43:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2410, -0.1958,  0.3701,  ...,  0.4736, -0.3623, -0.3584],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 6.1719, -0.6523, -1.8369,  ...,  0.2109,  1.5918,  0.5752],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0103, -0.0273, -0.0059,  ...,  0.0020, -0.0074, -0.0043],
        [-0.0119, -0.0074, -0.0102,  ...,  0.0008,  0.0056, -0.0071],
        [ 0.0118, -0.0013, -0.0065,  ...,  0.0098, -0.0020, -0.0218],
        ...,
        [ 0.0002, -0.0183, -0.0020,  ..., -0.0305, -0.0213, -0.0170],
        [-0.0157,  0.0050, -0.0003,  ..., -0.0131, -0.0043,  0.0100],
        [ 0.0011,  0.0003,  0.0111,  ..., -0.0011,  0.0295, -0.0134]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.8320, -0.5273, -1.6533,  ..., -0.1538,  2.1387,  0.5034]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 22:43:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for developing is the active form of develop
creating is the active form of create
preventing is the active form of prevent
applying is the active form of apply
requiring is the active form of require
attending is the active form of attend
protecting is the active form of protect
becoming is the active form of
2024-07-17 22:43:16 root INFO     [order_1_approx] starting weight calculation for creating is the active form of create
applying is the active form of apply
developing is the active form of develop
requiring is the active form of require
becoming is the active form of become
protecting is the active form of protect
attending is the active form of attend
preventing is the active form of
2024-07-17 22:43:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 22:47:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1001,  0.0723,  0.5117,  ...,  0.1973, -0.2998, -0.4272],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.9141, -1.9766, -1.8086,  ...,  0.7563, -0.5068,  3.6016],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0118, -0.0051,  0.0171,  ..., -0.0095, -0.0046,  0.0007],
        [-0.0024,  0.0050, -0.0034,  ...,  0.0021, -0.0052,  0.0022],
        [ 0.0221,  0.0040, -0.0115,  ..., -0.0255, -0.0002, -0.0074],
        ...,
        [-0.0117,  0.0065,  0.0009,  ..., -0.0200,  0.0046, -0.0100],
        [-0.0374,  0.0180,  0.0014,  ..., -0.0098, -0.0189,  0.0163],
        [ 0.0108,  0.0168,  0.0001,  ..., -0.0133, -0.0212, -0.0325]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.1523, -1.4121, -3.0703,  ...,  0.9028, -0.4883,  3.2363]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 22:47:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for creating is the active form of create
applying is the active form of apply
developing is the active form of develop
requiring is the active form of require
becoming is the active form of become
protecting is the active form of protect
attending is the active form of attend
preventing is the active form of
2024-07-17 22:47:25 root INFO     [order_1_approx] starting weight calculation for attending is the active form of attend
becoming is the active form of become
creating is the active form of create
applying is the active form of apply
developing is the active form of develop
preventing is the active form of prevent
requiring is the active form of require
protecting is the active form of
2024-07-17 22:47:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 22:51:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0466,  0.2808,  0.8691,  ...,  0.2480, -0.5630,  0.0928],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.1230, -0.5723, -0.2500,  ...,  0.4810, -3.0371,  4.9258],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.9062e-03,  5.6152e-03,  2.1271e-02,  ..., -1.5726e-03,
         -1.1024e-02, -1.0757e-02],
        [-8.3923e-05,  2.6741e-03,  8.1787e-03,  ..., -8.1177e-03,
         -8.2932e-03,  1.6342e-02],
        [ 2.4872e-02, -1.1673e-02,  1.1086e-02,  ..., -7.1106e-03,
         -1.9516e-02,  1.1597e-03],
        ...,
        [-1.7166e-02,  2.4681e-03, -1.1452e-02,  ..., -2.6031e-02,
         -1.1988e-03,  5.5656e-03],
        [-2.3651e-02,  5.6381e-03,  1.5373e-02,  ..., -1.4542e-02,
         -2.3102e-02,  4.9591e-03],
        [-1.8585e-02,  1.3397e-02,  2.1027e-02,  ..., -2.0554e-02,
         -1.1642e-02, -3.0884e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6211,  0.1060, -1.1133,  ...,  0.3154, -2.9609,  4.7227]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 22:51:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for attending is the active form of attend
becoming is the active form of become
creating is the active form of create
applying is the active form of apply
developing is the active form of develop
preventing is the active form of prevent
requiring is the active form of require
protecting is the active form of
2024-07-17 22:51:34 root INFO     [order_1_approx] starting weight calculation for applying is the active form of apply
requiring is the active form of require
becoming is the active form of become
attending is the active form of attend
developing is the active form of develop
protecting is the active form of protect
preventing is the active form of prevent
creating is the active form of
2024-07-17 22:51:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 22:55:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2683, -0.4883, -0.1199,  ...,  0.5117,  0.3948, -0.7021],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1279, -2.1348, -0.0586,  ..., -2.2500, -1.2324, -1.7510],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.6016e-03,  1.3260e-02,  1.5396e-02,  ...,  6.4850e-04,
          3.7766e-04, -2.8427e-02],
        [-4.1046e-03, -1.4099e-02,  7.8659e-03,  ...,  8.5068e-03,
         -1.7075e-02, -8.2550e-03],
        [ 1.4481e-02, -1.2238e-02, -2.6733e-02,  ...,  6.3248e-03,
          7.8888e-03, -3.4332e-05],
        ...,
        [-3.7689e-03, -2.6321e-03, -2.2469e-03,  ..., -1.2604e-02,
         -1.3611e-02, -3.6392e-03],
        [-1.4130e-02,  1.1642e-02,  8.3466e-03,  ..., -1.2894e-02,
         -2.3026e-02,  1.6388e-02],
        [-1.9440e-02,  1.4847e-02,  3.6736e-03,  ..., -1.8280e-02,
          7.8049e-03,  4.7760e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7002, -2.0840, -0.6182,  ..., -2.9141, -0.9424, -3.0195]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 22:55:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for applying is the active form of apply
requiring is the active form of require
becoming is the active form of become
attending is the active form of attend
developing is the active form of develop
protecting is the active form of protect
preventing is the active form of prevent
creating is the active form of
2024-07-17 22:55:43 root INFO     [order_1_approx] starting weight calculation for developing is the active form of develop
attending is the active form of attend
protecting is the active form of protect
applying is the active form of apply
creating is the active form of create
becoming is the active form of become
preventing is the active form of prevent
requiring is the active form of
2024-07-17 22:55:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 22:59:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1201,  0.0031,  0.0478,  ..., -0.1558, -0.1570,  0.1669],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.6055, -2.2578, -1.7539,  ..., -0.1548, -0.0234,  3.0586],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0075,  0.0051,  0.0186,  ..., -0.0095, -0.0110,  0.0243],
        [-0.0108,  0.0070, -0.0088,  ...,  0.0166, -0.0007,  0.0133],
        [ 0.0181,  0.0014, -0.0004,  ...,  0.0023, -0.0141, -0.0013],
        ...,
        [-0.0087, -0.0154, -0.0048,  ...,  0.0009,  0.0117,  0.0008],
        [-0.0184,  0.0105,  0.0043,  ..., -0.0185, -0.0269,  0.0138],
        [-0.0160,  0.0308,  0.0204,  ..., -0.0333,  0.0186, -0.0059]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.7734, -1.2041, -1.6953,  ..., -0.1436, -0.1108,  2.6426]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 22:59:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for developing is the active form of develop
attending is the active form of attend
protecting is the active form of protect
applying is the active form of apply
creating is the active form of create
becoming is the active form of become
preventing is the active form of prevent
requiring is the active form of
2024-07-17 22:59:51 root INFO     [order_1_approx] starting weight calculation for preventing is the active form of prevent
becoming is the active form of become
creating is the active form of create
applying is the active form of apply
attending is the active form of attend
protecting is the active form of protect
requiring is the active form of require
developing is the active form of
2024-07-17 22:59:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 23:03:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7539, -0.4712,  0.4534,  ...,  0.3804,  0.0071, -0.0444],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.5625, -0.6841,  0.5098,  ..., -3.9805,  1.9883, -1.1084],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0119, -0.0111,  0.0170,  ...,  0.0010, -0.0066,  0.0033],
        [-0.0043, -0.0011,  0.0036,  ...,  0.0196,  0.0036,  0.0132],
        [ 0.0031,  0.0109, -0.0164,  ..., -0.0142, -0.0184, -0.0015],
        ...,
        [-0.0140, -0.0080, -0.0014,  ...,  0.0057, -0.0006,  0.0194],
        [-0.0077,  0.0155,  0.0174,  ..., -0.0095, -0.0104,  0.0206],
        [-0.0058,  0.0171,  0.0030,  ..., -0.0097,  0.0120, -0.0213]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.6719, -0.4253, -0.6738,  ..., -4.3477,  2.2480, -1.3799]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 23:03:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for preventing is the active form of prevent
becoming is the active form of become
creating is the active form of create
applying is the active form of apply
attending is the active form of attend
protecting is the active form of protect
requiring is the active form of require
developing is the active form of
2024-07-17 23:03:59 root INFO     [order_1_approx] starting weight calculation for creating is the active form of create
protecting is the active form of protect
becoming is the active form of become
preventing is the active form of prevent
requiring is the active form of require
developing is the active form of develop
attending is the active form of attend
applying is the active form of
2024-07-17 23:03:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 23:08:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5127, -1.0117,  0.0641,  ...,  0.0901, -0.4634,  0.0112],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9941,  0.5029, -0.8828,  ...,  0.2021, -2.2773,  3.0039],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0132,  0.0126,  0.0344,  ..., -0.0052, -0.0075,  0.0085],
        [ 0.0013, -0.0126, -0.0100,  ...,  0.0043, -0.0185,  0.0144],
        [ 0.0027,  0.0065, -0.0070,  ...,  0.0135, -0.0002, -0.0081],
        ...,
        [-0.0157,  0.0027, -0.0054,  ..., -0.0084,  0.0036, -0.0116],
        [-0.0280,  0.0157,  0.0399,  ..., -0.0349, -0.0148,  0.0112],
        [-0.0050,  0.0143,  0.0232,  ..., -0.0090,  0.0047, -0.0057]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7773,  1.2549, -0.5195,  ..., -0.6665, -1.5605,  2.8828]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 23:08:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for creating is the active form of create
protecting is the active form of protect
becoming is the active form of become
preventing is the active form of prevent
requiring is the active form of require
developing is the active form of develop
attending is the active form of attend
applying is the active form of
2024-07-17 23:08:06 root INFO     [order_1_approx] starting weight calculation for developing is the active form of develop
requiring is the active form of require
creating is the active form of create
applying is the active form of apply
preventing is the active form of prevent
protecting is the active form of protect
becoming is the active form of become
attending is the active form of
2024-07-17 23:08:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 23:12:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7700, -0.0012,  0.0288,  ..., -0.2463, -0.1785, -0.1609],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([2.7207, 1.3877, 3.4805,  ..., 1.2812, 1.7510, 2.0488], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0072,  0.0175,  0.0043,  ..., -0.0101, -0.0069, -0.0018],
        [ 0.0028,  0.0007, -0.0096,  ...,  0.0203,  0.0043, -0.0049],
        [ 0.0137, -0.0042, -0.0016,  ..., -0.0049,  0.0124, -0.0007],
        ...,
        [-0.0109,  0.0019, -0.0020,  ...,  0.0081, -0.0096, -0.0004],
        [-0.0111,  0.0019,  0.0131,  ..., -0.0098, -0.0277,  0.0113],
        [-0.0155,  0.0109,  0.0101,  ..., -0.0223,  0.0122, -0.0015]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[2.8555, 0.9780, 3.8242,  ..., 0.2715, 1.7031, 2.8867]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 23:12:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for developing is the active form of develop
requiring is the active form of require
creating is the active form of create
applying is the active form of apply
preventing is the active form of prevent
protecting is the active form of protect
becoming is the active form of become
attending is the active form of
2024-07-17 23:12:15 root INFO     total operator prediction time: 1987.4157238006592 seconds
2024-07-17 23:12:15 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_Ving - Ved
2024-07-17 23:12:15 root INFO     building operator verb_Ving - Ved
2024-07-17 23:12:15 root INFO     [order_1_approx] starting weight calculation for After something is receiving, it has received
After something is involving, it has involved
After something is operating, it has operated
After something is describing, it has described
After something is appointing, it has appointed
After something is replacing, it has replaced
After something is failing, it has failed
After something is requiring, it has
2024-07-17 23:12:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 23:16:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7852,  0.3865, -0.1494,  ...,  0.2971,  0.0400,  0.5850],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.5928,  0.3291,  1.0596,  ...,  2.7754, -1.2725,  3.0820],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0175, -0.0278,  0.0039,  ..., -0.0378, -0.0212,  0.0073],
        [-0.0308, -0.0187, -0.0094,  ...,  0.0200, -0.0261, -0.0164],
        [ 0.0285, -0.0063, -0.0313,  ...,  0.0109, -0.0067, -0.0135],
        ...,
        [-0.0403, -0.0269, -0.0107,  ..., -0.0456,  0.0033,  0.0094],
        [-0.0047, -0.0349,  0.0180,  ..., -0.0263, -0.0390,  0.0174],
        [-0.0083, -0.0110,  0.0162,  ...,  0.0092, -0.0062, -0.0271]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5635, -0.3135, -0.4189,  ...,  2.8223, -1.3066,  2.6484]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 23:16:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is receiving, it has received
After something is involving, it has involved
After something is operating, it has operated
After something is describing, it has described
After something is appointing, it has appointed
After something is replacing, it has replaced
After something is failing, it has failed
After something is requiring, it has
2024-07-17 23:16:22 root INFO     [order_1_approx] starting weight calculation for After something is involving, it has involved
After something is operating, it has operated
After something is requiring, it has required
After something is describing, it has described
After something is replacing, it has replaced
After something is failing, it has failed
After something is receiving, it has received
After something is appointing, it has
2024-07-17 23:16:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 23:20:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6636, -0.1810, -0.0636,  ...,  0.0201, -0.1965,  0.8999],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.0156, -0.1421,  2.0312,  ...,  6.8438, -1.2822,  1.3838],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0155, -0.0095,  0.0177,  ...,  0.0180, -0.0454, -0.0294],
        [-0.0016, -0.0114, -0.0217,  ..., -0.0020, -0.0117,  0.0081],
        [-0.0164, -0.0049, -0.0105,  ...,  0.0178, -0.0042, -0.0164],
        ...,
        [-0.0499, -0.0195, -0.0413,  ..., -0.0138,  0.0072,  0.0111],
        [ 0.0077,  0.0008,  0.0232,  ..., -0.0077, -0.0512,  0.0254],
        [-0.0296, -0.0122,  0.0045,  ..., -0.0054,  0.0103, -0.0557]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.4961, -0.3936,  1.4414,  ...,  6.6914, -0.9111,  1.5371]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 23:20:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is involving, it has involved
After something is operating, it has operated
After something is requiring, it has required
After something is describing, it has described
After something is replacing, it has replaced
After something is failing, it has failed
After something is receiving, it has received
After something is appointing, it has
2024-07-17 23:20:31 root INFO     [order_1_approx] starting weight calculation for After something is involving, it has involved
After something is replacing, it has replaced
After something is receiving, it has received
After something is appointing, it has appointed
After something is describing, it has described
After something is failing, it has failed
After something is requiring, it has required
After something is operating, it has
2024-07-17 23:20:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 23:24:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1443, -0.0103,  0.2258,  ...,  0.1980,  0.1787,  0.4690],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2305, -0.5107,  2.7930,  ...,  2.0391, -0.9092,  2.8320],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0113, -0.0081,  0.0056,  ...,  0.0185, -0.0193, -0.0070],
        [-0.0116, -0.0217, -0.0063,  ...,  0.0017, -0.0116,  0.0036],
        [ 0.0065, -0.0103, -0.0186,  ...,  0.0041,  0.0173,  0.0088],
        ...,
        [-0.0101, -0.0060, -0.0260,  ..., -0.0325, -0.0065, -0.0113],
        [ 0.0005, -0.0028, -0.0074,  ..., -0.0074, -0.0301,  0.0030],
        [ 0.0074, -0.0003,  0.0023,  ..., -0.0053, -0.0057, -0.0304]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9805, -1.1631,  2.3164,  ...,  1.3633, -2.2871,  1.5039]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 23:24:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is involving, it has involved
After something is replacing, it has replaced
After something is receiving, it has received
After something is appointing, it has appointed
After something is describing, it has described
After something is failing, it has failed
After something is requiring, it has required
After something is operating, it has
2024-07-17 23:24:38 root INFO     [order_1_approx] starting weight calculation for After something is appointing, it has appointed
After something is receiving, it has received
After something is failing, it has failed
After something is operating, it has operated
After something is describing, it has described
After something is involving, it has involved
After something is requiring, it has required
After something is replacing, it has
2024-07-17 23:24:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 23:28:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9951,  0.3044,  0.0024,  ..., -0.2251,  0.3088, -0.0974],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.6250,  3.1523, -0.7612,  ...,  3.9902, -0.7441,  5.1797],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0172, -0.0071,  0.0164,  ..., -0.0086, -0.0233, -0.0207],
        [-0.0095, -0.0223, -0.0325,  ..., -0.0078,  0.0042,  0.0193],
        [-0.0028, -0.0230, -0.0185,  ...,  0.0060,  0.0146, -0.0079],
        ...,
        [-0.0268, -0.0243, -0.0271,  ..., -0.0436,  0.0198, -0.0045],
        [-0.0071,  0.0020, -0.0217,  ..., -0.0039, -0.0534,  0.0027],
        [ 0.0164,  0.0131, -0.0289,  ..., -0.0578,  0.0026, -0.0423]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.0234,  2.6660, -0.2280,  ...,  3.1230, -0.6465,  3.3281]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 23:28:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is appointing, it has appointed
After something is receiving, it has received
After something is failing, it has failed
After something is operating, it has operated
After something is describing, it has described
After something is involving, it has involved
After something is requiring, it has required
After something is replacing, it has
2024-07-17 23:28:47 root INFO     [order_1_approx] starting weight calculation for After something is appointing, it has appointed
After something is receiving, it has received
After something is failing, it has failed
After something is requiring, it has required
After something is operating, it has operated
After something is replacing, it has replaced
After something is involving, it has involved
After something is describing, it has
2024-07-17 23:28:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 23:32:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6816,  0.2212, -0.0574,  ..., -0.5820, -0.5981, -0.1614],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7617, -0.4194,  2.7812,  ...,  2.2402, -1.7139,  2.2676],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0149, -0.0264,  0.0035,  ...,  0.0027, -0.0262, -0.0098],
        [-0.0002, -0.0301,  0.0046,  ...,  0.0060, -0.0046,  0.0051],
        [ 0.0082, -0.0070, -0.0231,  ..., -0.0123,  0.0108, -0.0087],
        ...,
        [-0.0212, -0.0082,  0.0197,  ..., -0.0142,  0.0242, -0.0276],
        [ 0.0105,  0.0093,  0.0046,  ..., -0.0206, -0.0500,  0.0003],
        [-0.0021, -0.0106,  0.0149,  ..., -0.0020, -0.0111, -0.0264]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2300, -0.8813,  0.9570,  ...,  1.3652, -1.2744,  1.4883]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 23:32:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is appointing, it has appointed
After something is receiving, it has received
After something is failing, it has failed
After something is requiring, it has required
After something is operating, it has operated
After something is replacing, it has replaced
After something is involving, it has involved
After something is describing, it has
2024-07-17 23:32:54 root INFO     [order_1_approx] starting weight calculation for After something is appointing, it has appointed
After something is describing, it has described
After something is failing, it has failed
After something is replacing, it has replaced
After something is requiring, it has required
After something is operating, it has operated
After something is involving, it has involved
After something is receiving, it has
2024-07-17 23:32:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 23:37:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3760, -0.1942, -0.0742,  ..., -0.2656,  0.0229,  1.1104],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([1.4580, 1.0361, 0.0098,  ..., 3.1895, 0.5483, 4.8398], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.5116e-02, -2.5375e-02,  1.1230e-02,  ...,  6.5575e-03,
         -3.5278e-02, -3.0609e-02],
        [ 1.4114e-02, -5.6190e-03,  8.9874e-03,  ..., -5.0354e-03,
         -7.2746e-03,  1.6937e-02],
        [ 4.4365e-03, -7.9575e-03, -1.5244e-02,  ...,  1.2863e-02,
         -1.0128e-03, -7.3318e-03],
        ...,
        [-2.4319e-05, -1.2680e-02, -2.8992e-03,  ..., -4.6875e-02,
         -1.3985e-02,  9.5367e-03],
        [ 2.5223e-02,  2.1545e-02, -7.3738e-03,  ..., -1.1421e-02,
         -2.7878e-02,  6.1096e-02],
        [-2.3712e-02, -1.4076e-02,  1.1635e-02,  ..., -1.1093e-02,
          1.1383e-02, -2.0218e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.8003,  0.2222, -1.6064,  ...,  3.7773, -1.0625,  5.2969]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 23:37:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is appointing, it has appointed
After something is describing, it has described
After something is failing, it has failed
After something is replacing, it has replaced
After something is requiring, it has required
After something is operating, it has operated
After something is involving, it has involved
After something is receiving, it has
2024-07-17 23:37:02 root INFO     [order_1_approx] starting weight calculation for After something is replacing, it has replaced
After something is requiring, it has required
After something is receiving, it has received
After something is involving, it has involved
After something is appointing, it has appointed
After something is operating, it has operated
After something is describing, it has described
After something is failing, it has
2024-07-17 23:37:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 23:41:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0486,  0.6016,  0.1006,  ..., -0.0857,  0.6304,  0.2012],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7686,  2.7871,  2.5215,  ..., -0.0410, -0.1011,  3.7773],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0315, -0.0078, -0.0008,  ...,  0.0170, -0.0055, -0.0067],
        [-0.0239, -0.0036, -0.0174,  ...,  0.0104, -0.0145,  0.0162],
        [ 0.0103, -0.0029, -0.0457,  ...,  0.0142,  0.0025, -0.0040],
        ...,
        [-0.0073, -0.0065, -0.0272,  ..., -0.0427, -0.0138, -0.0171],
        [ 0.0136,  0.0063,  0.0117,  ..., -0.0259, -0.0258,  0.0193],
        [ 0.0059,  0.0061, -0.0063,  ...,  0.0077, -0.0099, -0.0326]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6709,  1.7422,  3.1055,  ...,  0.1954, -1.2910,  3.1367]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 23:41:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is replacing, it has replaced
After something is requiring, it has required
After something is receiving, it has received
After something is involving, it has involved
After something is appointing, it has appointed
After something is operating, it has operated
After something is describing, it has described
After something is failing, it has
2024-07-17 23:41:10 root INFO     [order_1_approx] starting weight calculation for After something is describing, it has described
After something is appointing, it has appointed
After something is failing, it has failed
After something is replacing, it has replaced
After something is receiving, it has received
After something is requiring, it has required
After something is operating, it has operated
After something is involving, it has
2024-07-17 23:41:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 23:45:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7446,  0.0286,  0.1222,  ...,  0.5039, -0.3164,  0.3706],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([0.7041, 0.9556, 1.5068,  ..., 3.5996, 0.8574, 2.9082], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0203, -0.0165,  0.0396,  ...,  0.0172, -0.0206,  0.0018],
        [-0.0094, -0.0112, -0.0096,  ..., -0.0049, -0.0150,  0.0008],
        [ 0.0251, -0.0068, -0.0262,  ...,  0.0140,  0.0026,  0.0045],
        ...,
        [-0.0254, -0.0243, -0.0216,  ..., -0.0125,  0.0176, -0.0007],
        [ 0.0079,  0.0070,  0.0055,  ..., -0.0068, -0.0461, -0.0047],
        [ 0.0059, -0.0070,  0.0222,  ..., -0.0386,  0.0282, -0.0228]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[1.4160, 1.5215, 1.5234,  ..., 3.2441, 0.4702, 2.2539]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 23:45:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is describing, it has described
After something is appointing, it has appointed
After something is failing, it has failed
After something is replacing, it has replaced
After something is receiving, it has received
After something is requiring, it has required
After something is operating, it has operated
After something is involving, it has
2024-07-17 23:45:18 root INFO     total operator prediction time: 1983.154794216156 seconds
2024-07-17 23:45:18 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_inf - Ved
2024-07-17 23:45:18 root INFO     building operator verb_inf - Ved
2024-07-17 23:45:18 root INFO     [order_1_approx] starting weight calculation for If the present form is hear, the past form is heard
If the present form is ask, the past form is asked
If the present form is perform, the past form is performed
If the present form is enjoy, the past form is enjoyed
If the present form is consider, the past form is considered
If the present form is believe, the past form is believed
If the present form is lose, the past form is lost
If the present form is agree, the past form is
2024-07-17 23:45:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 23:49:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8457,  0.1768, -0.0703,  ...,  0.0841, -1.1797, -0.3076],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6523,  0.7173,  0.6768,  ..., -0.1207, -2.8828, -0.2314],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0319, -0.0014,  0.0005,  ...,  0.0193, -0.0069, -0.0079],
        [-0.0051, -0.0202, -0.0011,  ..., -0.0184, -0.0071, -0.0075],
        [ 0.0030, -0.0026, -0.0203,  ...,  0.0042, -0.0075, -0.0015],
        ...,
        [-0.0037,  0.0228, -0.0039,  ..., -0.0228, -0.0008,  0.0020],
        [-0.0032,  0.0013, -0.0077,  ..., -0.0209, -0.0456, -0.0072],
        [-0.0099,  0.0118,  0.0170,  ...,  0.0074, -0.0034, -0.0391]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7578, -0.5991, -0.2949,  ...,  0.1796, -3.3398, -0.3044]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 23:49:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is hear, the past form is heard
If the present form is ask, the past form is asked
If the present form is perform, the past form is performed
If the present form is enjoy, the past form is enjoyed
If the present form is consider, the past form is considered
If the present form is believe, the past form is believed
If the present form is lose, the past form is lost
If the present form is agree, the past form is
2024-07-17 23:49:24 root INFO     [order_1_approx] starting weight calculation for If the present form is ask, the past form is asked
If the present form is consider, the past form is considered
If the present form is agree, the past form is agreed
If the present form is enjoy, the past form is enjoyed
If the present form is hear, the past form is heard
If the present form is believe, the past form is believed
If the present form is perform, the past form is performed
If the present form is lose, the past form is
2024-07-17 23:49:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 23:53:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5850,  0.1042,  0.5361,  ...,  0.7124, -1.4268,  0.6396],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.9062, -1.3066, -3.1289,  ...,  0.4971, -1.7441,  1.7070],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0160, -0.0010,  0.0116,  ...,  0.0176, -0.0042, -0.0146],
        [-0.0067, -0.0240,  0.0020,  ..., -0.0187, -0.0007, -0.0108],
        [-0.0041,  0.0009, -0.0168,  ..., -0.0023,  0.0005,  0.0016],
        ...,
        [-0.0022,  0.0050, -0.0112,  ..., -0.0275, -0.0064,  0.0032],
        [ 0.0163,  0.0090, -0.0065,  ..., -0.0118, -0.0211, -0.0040],
        [-0.0329,  0.0035,  0.0038,  ...,  0.0036,  0.0068, -0.0263]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.2891, -0.9688, -2.8633,  ..., -0.6064, -1.9717,  1.0977]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 23:53:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is ask, the past form is asked
If the present form is consider, the past form is considered
If the present form is agree, the past form is agreed
If the present form is enjoy, the past form is enjoyed
If the present form is hear, the past form is heard
If the present form is believe, the past form is believed
If the present form is perform, the past form is performed
If the present form is lose, the past form is
2024-07-17 23:53:31 root INFO     [order_1_approx] starting weight calculation for If the present form is hear, the past form is heard
If the present form is ask, the past form is asked
If the present form is believe, the past form is believed
If the present form is consider, the past form is considered
If the present form is perform, the past form is performed
If the present form is agree, the past form is agreed
If the present form is lose, the past form is lost
If the present form is enjoy, the past form is
2024-07-17 23:53:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-17 23:57:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1179,  0.8320, -0.1572,  ...,  0.1558, -0.0544, -0.2202],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2417,  1.8428,  2.4570,  ..., -2.1309, -2.6934,  2.8281],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0344, -0.0109,  0.0132,  ..., -0.0004, -0.0060, -0.0099],
        [-0.0277, -0.0191,  0.0020,  ...,  0.0016,  0.0096,  0.0100],
        [ 0.0020, -0.0106, -0.0457,  ...,  0.0050, -0.0038, -0.0231],
        ...,
        [-0.0142, -0.0054,  0.0055,  ..., -0.0508,  0.0087,  0.0008],
        [ 0.0088,  0.0291, -0.0137,  ..., -0.0069, -0.0275,  0.0074],
        [-0.0081, -0.0075, -0.0065,  ..., -0.0013,  0.0187, -0.0535]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6841,  2.0234,  2.5957,  ..., -2.5801, -2.6816,  4.1406]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 23:57:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is hear, the past form is heard
If the present form is ask, the past form is asked
If the present form is believe, the past form is believed
If the present form is consider, the past form is considered
If the present form is perform, the past form is performed
If the present form is agree, the past form is agreed
If the present form is lose, the past form is lost
If the present form is enjoy, the past form is
2024-07-17 23:57:37 root INFO     [order_1_approx] starting weight calculation for If the present form is hear, the past form is heard
If the present form is enjoy, the past form is enjoyed
If the present form is believe, the past form is believed
If the present form is consider, the past form is considered
If the present form is lose, the past form is lost
If the present form is agree, the past form is agreed
If the present form is perform, the past form is performed
If the present form is ask, the past form is
2024-07-17 23:57:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 00:01:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1653,  0.7412,  0.0282,  ...,  0.1821,  0.3157, -0.6406],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([0.7856, 3.6680, 3.2070,  ..., 1.3916, 0.1104, 1.2725], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0237, -0.0173,  0.0078,  ...,  0.0199, -0.0264, -0.0073],
        [-0.0217, -0.0240, -0.0088,  ...,  0.0085, -0.0014, -0.0135],
        [-0.0026, -0.0058, -0.0270,  ..., -0.0078,  0.0089, -0.0112],
        ...,
        [-0.0047,  0.0223,  0.0006,  ..., -0.0208,  0.0059, -0.0004],
        [ 0.0039,  0.0179,  0.0160,  ..., -0.0227, -0.0468, -0.0142],
        [-0.0121,  0.0039,  0.0013,  ..., -0.0105, -0.0117, -0.0343]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[0.5283, 2.5879, 3.9570,  ..., 1.8545, 0.9077, 1.1045]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 00:01:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is hear, the past form is heard
If the present form is enjoy, the past form is enjoyed
If the present form is believe, the past form is believed
If the present form is consider, the past form is considered
If the present form is lose, the past form is lost
If the present form is agree, the past form is agreed
If the present form is perform, the past form is performed
If the present form is ask, the past form is
2024-07-18 00:01:44 root INFO     [order_1_approx] starting weight calculation for If the present form is perform, the past form is performed
If the present form is agree, the past form is agreed
If the present form is lose, the past form is lost
If the present form is believe, the past form is believed
If the present form is ask, the past form is asked
If the present form is consider, the past form is considered
If the present form is enjoy, the past form is enjoyed
If the present form is hear, the past form is
2024-07-18 00:01:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 00:05:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4314,  1.0420, -0.3772,  ...,  1.1396,  0.0206,  0.5381],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.6289,  4.3945, -0.9561,  ..., -2.4512,  2.9023,  6.0117],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0238, -0.0072, -0.0010,  ...,  0.0297, -0.0135, -0.0075],
        [-0.0268, -0.0247,  0.0179,  ...,  0.0196,  0.0055,  0.0207],
        [-0.0037, -0.0008, -0.0154,  ...,  0.0003,  0.0138, -0.0111],
        ...,
        [-0.0011,  0.0117, -0.0118,  ..., -0.0252,  0.0090, -0.0025],
        [ 0.0120,  0.0137, -0.0022,  ..., -0.0248, -0.0404,  0.0052],
        [-0.0272,  0.0021,  0.0242,  ..., -0.0062, -0.0023, -0.0013]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.4688,  4.3125, -0.9600,  ..., -1.8711,  1.9756,  6.1523]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 00:05:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is perform, the past form is performed
If the present form is agree, the past form is agreed
If the present form is lose, the past form is lost
If the present form is believe, the past form is believed
If the present form is ask, the past form is asked
If the present form is consider, the past form is considered
If the present form is enjoy, the past form is enjoyed
If the present form is hear, the past form is
2024-07-18 00:05:49 root INFO     [order_1_approx] starting weight calculation for If the present form is lose, the past form is lost
If the present form is agree, the past form is agreed
If the present form is ask, the past form is asked
If the present form is consider, the past form is considered
If the present form is enjoy, the past form is enjoyed
If the present form is hear, the past form is heard
If the present form is believe, the past form is believed
If the present form is perform, the past form is
2024-07-18 00:05:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 00:09:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0198,  0.6104,  0.2136,  ..., -0.0686,  0.4395, -0.0555],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6641, -0.4856, -0.5156,  ...,  2.5488,  0.5332,  1.2891],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0184, -0.0254,  0.0011,  ...,  0.0110, -0.0213, -0.0032],
        [-0.0146, -0.0209,  0.0080,  ..., -0.0075,  0.0046,  0.0056],
        [ 0.0071, -0.0119, -0.0172,  ..., -0.0023,  0.0088, -0.0108],
        ...,
        [-0.0206, -0.0043, -0.0082,  ..., -0.0271, -0.0110, -0.0027],
        [ 0.0088,  0.0063,  0.0060,  ..., -0.0245, -0.0127,  0.0133],
        [-0.0225,  0.0054,  0.0121,  ..., -0.0062,  0.0034, -0.0186]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6816,  0.0339, -0.5693,  ...,  2.1777,  0.9697,  0.7305]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 00:09:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is lose, the past form is lost
If the present form is agree, the past form is agreed
If the present form is ask, the past form is asked
If the present form is consider, the past form is considered
If the present form is enjoy, the past form is enjoyed
If the present form is hear, the past form is heard
If the present form is believe, the past form is believed
If the present form is perform, the past form is
2024-07-18 00:09:56 root INFO     [order_1_approx] starting weight calculation for If the present form is hear, the past form is heard
If the present form is perform, the past form is performed
If the present form is lose, the past form is lost
If the present form is ask, the past form is asked
If the present form is believe, the past form is believed
If the present form is enjoy, the past form is enjoyed
If the present form is agree, the past form is agreed
If the present form is consider, the past form is
2024-07-18 00:09:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 00:14:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0061,  0.4658, -0.1523,  ...,  0.1206, -0.3586, -0.1587],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.0781,  1.5381, -1.2607,  ...,  0.4155, -2.2695, -2.1270],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0076, -0.0148,  0.0011,  ...,  0.0118, -0.0183, -0.0071],
        [-0.0320, -0.0013,  0.0051,  ...,  0.0037, -0.0088,  0.0130],
        [ 0.0092, -0.0221, -0.0304,  ..., -0.0252,  0.0106, -0.0030],
        ...,
        [-0.0071,  0.0098, -0.0139,  ..., -0.0256, -0.0066, -0.0032],
        [ 0.0030, -0.0031,  0.0030,  ..., -0.0133, -0.0086, -0.0141],
        [-0.0193,  0.0280,  0.0062,  ..., -0.0077,  0.0116, -0.0076]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.0645,  0.3086, -1.4229,  ...,  0.1104, -0.8730, -1.8906]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 00:14:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is hear, the past form is heard
If the present form is perform, the past form is performed
If the present form is lose, the past form is lost
If the present form is ask, the past form is asked
If the present form is believe, the past form is believed
If the present form is enjoy, the past form is enjoyed
If the present form is agree, the past form is agreed
If the present form is consider, the past form is
2024-07-18 00:14:03 root INFO     [order_1_approx] starting weight calculation for If the present form is enjoy, the past form is enjoyed
If the present form is hear, the past form is heard
If the present form is ask, the past form is asked
If the present form is perform, the past form is performed
If the present form is lose, the past form is lost
If the present form is agree, the past form is agreed
If the present form is consider, the past form is considered
If the present form is believe, the past form is
2024-07-18 00:14:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 00:18:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3091,  0.2026, -0.3513,  ...,  1.2012, -0.0205, -0.5537],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.5234,  0.2686, -0.7852,  ...,  1.1484, -3.0293,  0.4707],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.9526e-02, -1.6159e-02,  6.3248e-03,  ...,  6.9733e-03,
          5.7831e-03,  8.7051e-03],
        [-2.9602e-02, -2.7039e-02, -8.1100e-03,  ..., -3.1452e-03,
          9.2545e-03,  6.6566e-03],
        [ 1.7309e-03,  6.6605e-03, -2.2614e-02,  ...,  8.0566e-03,
          3.7766e-03, -8.4915e-03],
        ...,
        [-1.3313e-03, -6.1035e-05, -1.3077e-02,  ..., -1.9882e-02,
          1.1360e-02,  1.0376e-02],
        [ 2.8210e-03, -9.0408e-03, -1.1185e-02,  ..., -4.0680e-02,
         -3.6407e-02, -2.4204e-03],
        [-1.2312e-03,  1.7639e-02,  7.7896e-03,  ...,  1.1314e-02,
         -4.1618e-03, -5.0934e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.9609, -0.4126, -0.1914,  ...,  2.6328, -1.5732,  1.2305]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 00:18:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is enjoy, the past form is enjoyed
If the present form is hear, the past form is heard
If the present form is ask, the past form is asked
If the present form is perform, the past form is performed
If the present form is lose, the past form is lost
If the present form is agree, the past form is agreed
If the present form is consider, the past form is considered
If the present form is believe, the past form is
2024-07-18 00:18:10 root INFO     total operator prediction time: 1972.1826484203339 seconds
2024-07-18 00:18:10 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_inf - 3pSg
2024-07-18 00:18:10 root INFO     building operator verb_inf - 3pSg
2024-07-18 00:18:10 root INFO     [order_1_approx] starting weight calculation for I improve, he improves
I consist, he consists
I understand, he understands
I identify, he identifies
I include, he includes
I continue, he continues
I enjoy, he enjoys
I accept, he
2024-07-18 00:18:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 00:22:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7373, -0.2773,  0.6890,  ..., -0.2476,  0.2832,  0.1561],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.4395, -4.3633,  1.9424,  ...,  0.7383, -0.7529,  2.6934],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0217,  0.0010,  0.0301,  ...,  0.0197, -0.0089, -0.0058],
        [ 0.0103, -0.0177,  0.0008,  ..., -0.0033,  0.0162,  0.0046],
        [-0.0039, -0.0076, -0.0249,  ...,  0.0163,  0.0109, -0.0143],
        ...,
        [ 0.0098,  0.0021, -0.0068,  ..., -0.0269, -0.0100, -0.0030],
        [ 0.0055, -0.0106,  0.0139,  ..., -0.0202, -0.0327,  0.0231],
        [-0.0139,  0.0086,  0.0048,  ..., -0.0118,  0.0053, -0.0186]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.5957, -3.7012,  0.6318,  ...,  0.6934, -0.5112,  2.4668]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 00:22:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I improve, he improves
I consist, he consists
I understand, he understands
I identify, he identifies
I include, he includes
I continue, he continues
I enjoy, he enjoys
I accept, he
2024-07-18 00:22:17 root INFO     [order_1_approx] starting weight calculation for I accept, he accepts
I consist, he consists
I include, he includes
I continue, he continues
I enjoy, he enjoys
I understand, he understands
I identify, he identifies
I improve, he
2024-07-18 00:22:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 00:26:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0400, -0.2686,  0.9253,  ...,  0.0334, -0.0460, -0.2070],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 6.2148, -2.4258,  0.5693,  ...,  4.4375, -1.2930,  3.3828],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0156, -0.0110,  0.0084,  ..., -0.0100, -0.0011, -0.0079],
        [ 0.0074, -0.0098,  0.0052,  ..., -0.0023,  0.0010, -0.0060],
        [-0.0004, -0.0220, -0.0232,  ...,  0.0061,  0.0079, -0.0067],
        ...,
        [ 0.0157,  0.0151, -0.0194,  ..., -0.0308, -0.0027,  0.0087],
        [ 0.0274,  0.0079,  0.0176,  ..., -0.0139, -0.0221,  0.0110],
        [-0.0145,  0.0217,  0.0094,  ..., -0.0034,  0.0005, -0.0101]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.5625, -1.9932, -0.2046,  ...,  4.3945, -1.6035,  2.7324]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 00:26:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I accept, he accepts
I consist, he consists
I include, he includes
I continue, he continues
I enjoy, he enjoys
I understand, he understands
I identify, he identifies
I improve, he
2024-07-18 00:26:24 root INFO     [order_1_approx] starting weight calculation for I improve, he improves
I consist, he consists
I understand, he understands
I identify, he identifies
I accept, he accepts
I enjoy, he enjoys
I continue, he continues
I include, he
2024-07-18 00:26:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 00:30:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8076, -0.0227,  0.9473,  ..., -0.6099, -0.4512,  0.7900],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7520, -2.3984, -1.4043,  ...,  2.1270, -1.8271,  0.1802],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0025,  0.0130,  0.0116,  ...,  0.0139, -0.0138, -0.0156],
        [ 0.0203,  0.0059,  0.0092,  ..., -0.0019,  0.0131, -0.0019],
        [-0.0129, -0.0050, -0.0147,  ...,  0.0241,  0.0147,  0.0043],
        ...,
        [ 0.0146, -0.0147, -0.0251,  ..., -0.0211, -0.0103,  0.0103],
        [ 0.0226,  0.0087,  0.0033,  ..., -0.0230, -0.0410,  0.0159],
        [-0.0245,  0.0047,  0.0094,  ..., -0.0251,  0.0205, -0.0038]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.6289, -2.1074, -1.1523,  ...,  1.3994, -2.0391, -0.4634]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 00:30:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I improve, he improves
I consist, he consists
I understand, he understands
I identify, he identifies
I accept, he accepts
I enjoy, he enjoys
I continue, he continues
I include, he
2024-07-18 00:30:31 root INFO     [order_1_approx] starting weight calculation for I consist, he consists
I enjoy, he enjoys
I accept, he accepts
I improve, he improves
I include, he includes
I identify, he identifies
I continue, he continues
I understand, he
2024-07-18 00:30:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 00:34:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0695, -0.8257,  0.6484,  ..., -0.9619, -0.0393, -0.1865],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8379, -1.7031, -0.2798,  ...,  0.9990, -0.5645,  3.4219],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0226, -0.0109,  0.0213,  ...,  0.0049,  0.0098, -0.0288],
        [-0.0095, -0.0248,  0.0118,  ..., -0.0037,  0.0104,  0.0081],
        [ 0.0006,  0.0019, -0.0570,  ...,  0.0468,  0.0295,  0.0021],
        ...,
        [ 0.0009,  0.0048,  0.0005,  ..., -0.0374, -0.0083, -0.0002],
        [ 0.0219,  0.0047,  0.0081,  ..., -0.0099, -0.0349,  0.0095],
        [-0.0072,  0.0221, -0.0114,  ..., -0.0179,  0.0242, -0.0484]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7383, -1.7881, -0.4128,  ...,  2.1641,  0.1099,  1.3984]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 00:34:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I consist, he consists
I enjoy, he enjoys
I accept, he accepts
I improve, he improves
I include, he includes
I identify, he identifies
I continue, he continues
I understand, he
2024-07-18 00:34:38 root INFO     [order_1_approx] starting weight calculation for I accept, he accepts
I understand, he understands
I improve, he improves
I continue, he continues
I consist, he consists
I include, he includes
I identify, he identifies
I enjoy, he
2024-07-18 00:34:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 00:38:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0471, -0.6504,  0.5059,  ..., -0.4431, -0.2236,  0.1467],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9390, -2.6816,  1.7021,  ...,  2.6934,  1.1436,  6.7891],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0229, -0.0218,  0.0320,  ..., -0.0015,  0.0050, -0.0185],
        [-0.0007, -0.0283, -0.0050,  ..., -0.0100,  0.0256,  0.0280],
        [-0.0247,  0.0270, -0.0554,  ...,  0.0279,  0.0263, -0.0043],
        ...,
        [-0.0235, -0.0279,  0.0022,  ..., -0.0523, -0.0243,  0.0086],
        [ 0.0008,  0.0043, -0.0015,  ..., -0.0058, -0.0477,  0.0201],
        [-0.0158,  0.0020,  0.0101,  ..., -0.0146,  0.0069, -0.0497]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7778, -2.4023,  1.5352,  ...,  1.9570, -0.1328,  5.9922]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 00:38:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I accept, he accepts
I understand, he understands
I improve, he improves
I continue, he continues
I consist, he consists
I include, he includes
I identify, he identifies
I enjoy, he
2024-07-18 00:38:45 root INFO     [order_1_approx] starting weight calculation for I enjoy, he enjoys
I accept, he accepts
I improve, he improves
I identify, he identifies
I continue, he continues
I understand, he understands
I include, he includes
I consist, he
2024-07-18 00:38:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 00:42:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0690, -0.9922,  0.5615,  ..., -0.1210,  0.1577, -0.0537],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.2520, -1.7158, -1.9619,  ...,  3.9297, -5.6719,  0.5488],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0289, -0.0130,  0.0303,  ...,  0.0159, -0.0236, -0.0041],
        [ 0.0036, -0.0164, -0.0129,  ...,  0.0060,  0.0082,  0.0041],
        [ 0.0005,  0.0299, -0.0341,  ...,  0.0463,  0.0130,  0.0010],
        ...,
        [-0.0146,  0.0112, -0.0250,  ..., -0.0288,  0.0132,  0.0165],
        [ 0.0165,  0.0190,  0.0360,  ..., -0.0255, -0.0334,  0.0149],
        [-0.0043,  0.0183,  0.0010,  ..., -0.0171,  0.0026, -0.0233]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.2227, -1.2461, -1.1240,  ...,  3.7012, -5.6211,  0.4985]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 00:42:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I enjoy, he enjoys
I accept, he accepts
I improve, he improves
I identify, he identifies
I continue, he continues
I understand, he understands
I include, he includes
I consist, he
2024-07-18 00:42:52 root INFO     [order_1_approx] starting weight calculation for I identify, he identifies
I include, he includes
I consist, he consists
I understand, he understands
I accept, he accepts
I improve, he improves
I enjoy, he enjoys
I continue, he
2024-07-18 00:42:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 00:46:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4021, -0.7480,  0.6445,  ..., -0.2196, -0.4890, -0.3679],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.1250, -3.4863, -1.3965,  ...,  2.4434, -1.5312,  2.5039],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0222, -0.0171,  0.0208,  ...,  0.0077, -0.0173, -0.0166],
        [ 0.0002, -0.0217, -0.0032,  ..., -0.0046,  0.0103, -0.0034],
        [-0.0071, -0.0106, -0.0450,  ...,  0.0067,  0.0197, -0.0096],
        ...,
        [-0.0009, -0.0108, -0.0217,  ..., -0.0294, -0.0142,  0.0215],
        [ 0.0224,  0.0045,  0.0238,  ..., -0.0040, -0.0635,  0.0010],
        [-0.0355,  0.0162, -0.0015,  ..., -0.0015,  0.0165, -0.0333]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.6152, -4.0234, -1.0029,  ...,  2.9551, -1.5986,  2.5625]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 00:46:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I identify, he identifies
I include, he includes
I consist, he consists
I understand, he understands
I accept, he accepts
I improve, he improves
I enjoy, he enjoys
I continue, he
2024-07-18 00:46:58 root INFO     [order_1_approx] starting weight calculation for I understand, he understands
I continue, he continues
I include, he includes
I enjoy, he enjoys
I consist, he consists
I improve, he improves
I accept, he accepts
I identify, he
2024-07-18 00:46:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 00:51:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7207,  0.0093, -0.2871,  ..., -0.4653, -0.1948, -0.1224],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7021, -3.7773, -0.5381,  ...,  2.9453,  1.1396,  2.6523],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.8610e-05, -1.3123e-02,  2.1072e-02,  ..., -1.8148e-03,
         -7.2136e-03, -2.7191e-02],
        [-8.8577e-03,  6.5422e-03,  1.6464e-02,  ..., -5.4398e-03,
          1.5350e-02,  1.1810e-02],
        [-7.9346e-03, -1.1574e-02, -3.2623e-02,  ...,  1.1398e-02,
          1.1780e-02,  2.3861e-03],
        ...,
        [-2.6131e-03, -1.4656e-02, -4.4937e-03,  ..., -2.7802e-02,
         -4.0970e-03,  1.8387e-02],
        [ 2.1637e-02,  1.5671e-02,  1.7120e-02,  ..., -3.7994e-02,
         -3.6530e-02,  5.7983e-03],
        [ 9.5444e-03,  1.6739e-02,  1.7593e-02,  ..., -3.6316e-02,
          1.2680e-02, -1.4656e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9707, -3.2363, -0.0676,  ...,  1.8252,  1.4707,  2.3320]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 00:51:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I understand, he understands
I continue, he continues
I include, he includes
I enjoy, he enjoys
I consist, he consists
I improve, he improves
I accept, he accepts
I identify, he
2024-07-18 00:51:04 root INFO     total operator prediction time: 1974.3503818511963 seconds
2024-07-18 00:51:04 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_Ving - 3pSg
2024-07-18 00:51:04 root INFO     building operator verb_Ving - 3pSg
2024-07-18 00:51:04 root INFO     [order_1_approx] starting weight calculation for When something is applying, it applies
When something is thanking, it thanks
When something is improving, it improves
When something is reducing, it reduces
When something is continuing, it continues
When something is hearing, it hears
When something is considering, it considers
When something is managing, it
2024-07-18 00:51:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 00:55:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0889,  0.3381, -0.0699,  ...,  0.3169, -0.3442, -0.1940],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4053, -0.8462,  1.0596,  ...,  3.2871,  0.1277,  1.3018],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0073, -0.0064,  0.0158,  ...,  0.0064, -0.0168, -0.0089],
        [ 0.0107, -0.0042,  0.0006,  ...,  0.0004,  0.0004,  0.0019],
        [ 0.0076,  0.0042,  0.0044,  ...,  0.0147, -0.0009,  0.0192],
        ...,
        [-0.0147, -0.0016, -0.0082,  ..., -0.0339,  0.0194, -0.0086],
        [-0.0069, -0.0171, -0.0024,  ..., -0.0233, -0.0156,  0.0036],
        [ 0.0010,  0.0219, -0.0123,  ...,  0.0062,  0.0083, -0.0331]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2539, -0.4827,  1.4297,  ...,  2.3926,  0.4294,  0.3198]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 00:55:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is applying, it applies
When something is thanking, it thanks
When something is improving, it improves
When something is reducing, it reduces
When something is continuing, it continues
When something is hearing, it hears
When something is considering, it considers
When something is managing, it
2024-07-18 00:55:13 root INFO     [order_1_approx] starting weight calculation for When something is hearing, it hears
When something is improving, it improves
When something is thanking, it thanks
When something is considering, it considers
When something is continuing, it continues
When something is reducing, it reduces
When something is managing, it manages
When something is applying, it
2024-07-18 00:55:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 00:59:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0625, -0.3799,  0.4023,  ..., -0.1487,  0.0968,  0.6489],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5879,  0.9844,  2.3125,  ...,  3.1680, -2.2422,  3.0469],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0158,  0.0055,  0.0220,  ...,  0.0143, -0.0038,  0.0099],
        [ 0.0039, -0.0003,  0.0130,  ..., -0.0032, -0.0123,  0.0003],
        [-0.0042,  0.0131,  0.0103,  ...,  0.0185, -0.0058, -0.0041],
        ...,
        [ 0.0047,  0.0124, -0.0117,  ..., -0.0322,  0.0084,  0.0011],
        [ 0.0040, -0.0083,  0.0207,  ..., -0.0030, -0.0160, -0.0025],
        [-0.0033, -0.0054, -0.0014,  ..., -0.0008,  0.0153, -0.0289]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.0898,  1.0947,  2.8945,  ...,  2.1719, -2.0918,  2.3535]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 00:59:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is hearing, it hears
When something is improving, it improves
When something is thanking, it thanks
When something is considering, it considers
When something is continuing, it continues
When something is reducing, it reduces
When something is managing, it manages
When something is applying, it
2024-07-18 00:59:20 root INFO     [order_1_approx] starting weight calculation for When something is considering, it considers
When something is managing, it manages
When something is thanking, it thanks
When something is reducing, it reduces
When something is applying, it applies
When something is hearing, it hears
When something is continuing, it continues
When something is improving, it
2024-07-18 00:59:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 01:03:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3745, -0.0017,  0.8252,  ..., -0.2292,  0.6509,  0.1962],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([7.4922, 0.9375, 2.3477,  ..., 3.0781, 0.6123, 1.0957], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0093, -0.0229,  0.0011,  ..., -0.0168, -0.0240, -0.0086],
        [-0.0037, -0.0005,  0.0044,  ..., -0.0033, -0.0126, -0.0149],
        [-0.0103, -0.0210, -0.0077,  ...,  0.0184,  0.0094,  0.0034],
        ...,
        [ 0.0096,  0.0162, -0.0055,  ..., -0.0325, -0.0096, -0.0142],
        [ 0.0007, -0.0013,  0.0055,  ..., -0.0062, -0.0284,  0.0094],
        [-0.0184,  0.0125, -0.0066,  ...,  0.0154,  0.0008, -0.0236]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[7.0078, 1.1553, 2.8477,  ..., 2.1523, 1.0312, 0.7529]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 01:03:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is considering, it considers
When something is managing, it manages
When something is thanking, it thanks
When something is reducing, it reduces
When something is applying, it applies
When something is hearing, it hears
When something is continuing, it continues
When something is improving, it
2024-07-18 01:03:27 root INFO     [order_1_approx] starting weight calculation for When something is managing, it manages
When something is improving, it improves
When something is reducing, it reduces
When something is thanking, it thanks
When something is continuing, it continues
When something is applying, it applies
When something is hearing, it hears
When something is considering, it
2024-07-18 01:03:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 01:07:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8286, -0.3777, -0.3435,  ..., -0.0293, -0.0598,  0.9902],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8330,  3.0723,  1.9971,  ...,  2.0977, -3.1035, -1.2158],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.2278e-02,  8.5144e-03,  1.9562e-02,  ...,  2.9648e-02,
         -2.1790e-02,  1.7662e-03],
        [-2.0309e-02,  5.5389e-03, -1.4143e-03,  ..., -5.5313e-04,
         -1.9028e-02, -2.3743e-02],
        [ 5.8060e-03,  1.5907e-03,  1.3657e-03,  ...,  1.3397e-02,
          7.7362e-03, -2.2888e-05],
        ...,
        [-1.7471e-02, -3.1891e-02, -2.4429e-02,  ..., -2.0386e-02,
          5.0583e-03, -4.9934e-03],
        [ 1.9745e-02, -9.6741e-03,  1.3939e-02,  ..., -1.6113e-02,
         -3.2288e-02,  7.1831e-03],
        [-1.6312e-02,  2.6138e-02, -1.7700e-03,  ...,  1.1421e-02,
         -2.1687e-03, -4.9316e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6113,  3.5430,  3.2480,  ...,  0.9365, -2.8711, -1.1289]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 01:07:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is managing, it manages
When something is improving, it improves
When something is reducing, it reduces
When something is thanking, it thanks
When something is continuing, it continues
When something is applying, it applies
When something is hearing, it hears
When something is considering, it
2024-07-18 01:07:33 root INFO     [order_1_approx] starting weight calculation for When something is thanking, it thanks
When something is improving, it improves
When something is considering, it considers
When something is applying, it applies
When something is managing, it manages
When something is hearing, it hears
When something is continuing, it continues
When something is reducing, it
2024-07-18 01:07:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 01:11:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0676,  0.0153,  0.0973,  ..., -0.3350, -0.3376,  0.3188],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4004,  0.4578,  3.5312,  ..., -0.6738, -0.5615,  3.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0210,  0.0147,  0.0237,  ...,  0.0006, -0.0284, -0.0269],
        [-0.0152, -0.0161, -0.0076,  ..., -0.0162, -0.0035,  0.0094],
        [-0.0166, -0.0084, -0.0133,  ..., -0.0059,  0.0147,  0.0045],
        ...,
        [ 0.0082,  0.0065,  0.0088,  ..., -0.0356,  0.0058, -0.0213],
        [ 0.0120, -0.0246,  0.0240,  ..., -0.0298, -0.0371, -0.0046],
        [-0.0144,  0.0103, -0.0084,  ..., -0.0103,  0.0211, -0.0245]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0178,  0.8623,  2.9707,  ..., -1.2949, -0.4443,  2.2227]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 01:11:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is thanking, it thanks
When something is improving, it improves
When something is considering, it considers
When something is applying, it applies
When something is managing, it manages
When something is hearing, it hears
When something is continuing, it continues
When something is reducing, it
2024-07-18 01:11:40 root INFO     [order_1_approx] starting weight calculation for When something is thanking, it thanks
When something is considering, it considers
When something is applying, it applies
When something is managing, it manages
When something is reducing, it reduces
When something is improving, it improves
When something is hearing, it hears
When something is continuing, it
2024-07-18 01:11:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 01:15:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.6338, 0.2729, 0.1260,  ..., 0.0980, 0.8306, 0.3462], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8320, -1.3652,  0.9414,  ...,  1.1709, -1.6348,  2.8281],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-3.0655e-02, -1.0315e-02,  4.7760e-03,  ...,  1.9287e-02,
         -1.5579e-02, -1.0178e-02],
        [-6.9122e-03, -1.8509e-02, -8.9569e-03,  ..., -4.5280e-03,
          8.9264e-03,  4.5929e-03],
        [ 6.1607e-03, -2.6207e-03,  1.4267e-03,  ...,  2.3823e-03,
         -2.2888e-05,  2.1992e-03],
        ...,
        [-7.7553e-03, -5.0964e-03, -2.3594e-03,  ..., -1.4832e-02,
          2.1133e-03,  2.8648e-03],
        [ 8.1329e-03,  2.5787e-03,  5.1498e-04,  ..., -1.1887e-02,
         -2.0035e-02,  1.5121e-02],
        [-1.8730e-03,  3.1586e-03,  7.8964e-03,  ...,  5.1003e-03,
          2.0691e-02, -3.7048e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0371, -1.2412,  1.0000,  ...,  1.8223, -0.8774,  2.3906]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 01:15:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is thanking, it thanks
When something is considering, it considers
When something is applying, it applies
When something is managing, it manages
When something is reducing, it reduces
When something is improving, it improves
When something is hearing, it hears
When something is continuing, it
2024-07-18 01:15:46 root INFO     [order_1_approx] starting weight calculation for When something is hearing, it hears
When something is continuing, it continues
When something is reducing, it reduces
When something is improving, it improves
When something is managing, it manages
When something is applying, it applies
When something is considering, it considers
When something is thanking, it
2024-07-18 01:15:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 01:19:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.5527, 0.0826, 0.7275,  ..., 0.2141, 0.0482, 0.7036], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9888,  1.7070,  3.1758,  ...,  3.1230, -3.2070,  4.6172],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0509, -0.0387,  0.0299,  ..., -0.0012,  0.0084, -0.0371],
        [-0.0370, -0.0624, -0.0009,  ...,  0.0168,  0.0078, -0.0075],
        [ 0.0102,  0.0506, -0.0152,  ...,  0.0260,  0.0262, -0.0045],
        ...,
        [-0.0012, -0.0251,  0.0177,  ..., -0.0513, -0.0252, -0.0210],
        [ 0.0024,  0.0291,  0.0040,  ..., -0.0405, -0.0426,  0.0138],
        [-0.0020,  0.0352, -0.0017,  ..., -0.0073,  0.0025, -0.0508]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3320,  1.0840,  3.9551,  ...,  1.4893, -2.6523,  4.2148]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 01:19:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is hearing, it hears
When something is continuing, it continues
When something is reducing, it reduces
When something is improving, it improves
When something is managing, it manages
When something is applying, it applies
When something is considering, it considers
When something is thanking, it
2024-07-18 01:19:53 root INFO     [order_1_approx] starting weight calculation for When something is improving, it improves
When something is considering, it considers
When something is reducing, it reduces
When something is applying, it applies
When something is managing, it manages
When something is thanking, it thanks
When something is continuing, it continues
When something is hearing, it
2024-07-18 01:19:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 01:24:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.4121, 0.2646, 0.6011,  ..., 0.1650, 0.1887, 0.5186], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([0.6191, 3.8887, 0.4160,  ..., 0.7544, 0.8164, 5.5586], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0141, -0.0065,  0.0052,  ...,  0.0135, -0.0238,  0.0011],
        [ 0.0070, -0.0148, -0.0023,  ...,  0.0065, -0.0021,  0.0048],
        [-0.0183,  0.0060,  0.0175,  ...,  0.0095,  0.0074,  0.0235],
        ...,
        [-0.0010,  0.0010, -0.0091,  ..., -0.0231,  0.0021, -0.0149],
        [-0.0038, -0.0182,  0.0073,  ..., -0.0197, -0.0338,  0.0028],
        [-0.0023,  0.0105, -0.0266,  ..., -0.0338,  0.0183, -0.0403]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[0.1272, 3.1211, 0.2024,  ..., 0.8579, 0.6602, 4.6602]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 01:24:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is improving, it improves
When something is considering, it considers
When something is reducing, it reduces
When something is applying, it applies
When something is managing, it manages
When something is thanking, it thanks
When something is continuing, it continues
When something is hearing, it
2024-07-18 01:24:02 root INFO     total operator prediction time: 1977.7705900669098 seconds
2024-07-18 01:24:02 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on noun - plural_reg
2024-07-18 01:24:02 root INFO     building operator noun - plural_reg
2024-07-18 01:24:02 root INFO     [order_1_approx] starting weight calculation for The plural form of government is governments
The plural form of month is months
The plural form of application is applications
The plural form of river is rivers
The plural form of director is directors
The plural form of street is streets
The plural form of idea is ideas
The plural form of town is
2024-07-18 01:24:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 01:28:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4321,  0.0925, -0.6548,  ...,  0.6416, -0.9150, -0.3938],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5293,  1.2285, -0.7334,  ...,  0.2764,  2.5156,  3.3887],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.4658e-02, -1.1948e-02,  7.9803e-03,  ...,  1.3695e-02,
         -4.7302e-03, -2.4368e-02],
        [ 4.4899e-03, -2.0325e-02, -3.1185e-03,  ...,  1.1963e-02,
         -2.8458e-03,  2.9373e-03],
        [ 8.7738e-03,  9.1400e-03, -2.2568e-02,  ..., -8.9874e-03,
          3.0869e-02, -6.3934e-03],
        ...,
        [-9.1934e-04, -2.0523e-02, -4.3144e-03,  ..., -9.3460e-05,
         -3.4847e-03,  9.7580e-03],
        [ 1.4137e-02, -1.6861e-02, -1.9699e-02,  ..., -5.9776e-03,
         -6.0081e-03,  1.3351e-02],
        [-7.9651e-03, -1.3931e-02,  8.2245e-03,  ..., -3.3512e-03,
          2.0996e-02, -3.7842e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7534,  1.2422, -0.5400,  ...,  0.0411,  2.7715,  3.1504]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 01:28:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of government is governments
The plural form of month is months
The plural form of application is applications
The plural form of river is rivers
The plural form of director is directors
The plural form of street is streets
The plural form of idea is ideas
The plural form of town is
2024-07-18 01:28:09 root INFO     [order_1_approx] starting weight calculation for The plural form of director is directors
The plural form of application is applications
The plural form of government is governments
The plural form of street is streets
The plural form of month is months
The plural form of town is towns
The plural form of idea is ideas
The plural form of river is
2024-07-18 01:28:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 01:32:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2487, -0.9990,  0.2074,  ...,  1.1973, -0.8613,  0.3784],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7676,  2.5430,  0.2207,  ...,  3.4805, -3.3652,  2.7441],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.0757e-02, -4.3182e-03,  1.2535e-02,  ..., -1.9264e-04,
         -6.5880e-03, -9.1629e-03],
        [-9.8610e-04, -1.0834e-03,  4.0359e-03,  ...,  8.2626e-03,
         -1.1627e-02,  9.4147e-03],
        [ 1.2760e-03,  1.4465e-02, -1.6861e-02,  ...,  1.0063e-02,
          1.8890e-02, -5.2299e-03],
        ...,
        [-3.2043e-04, -1.2444e-02,  8.3923e-05,  ..., -1.9741e-03,
         -1.7643e-04,  9.6436e-03],
        [ 9.3918e-03, -1.3512e-02, -6.9237e-03,  ...,  1.3916e-02,
         -9.9945e-03,  6.5842e-03],
        [ 1.0818e-02,  2.3632e-03,  8.7280e-03,  ..., -1.2642e-02,
          7.7438e-03, -2.2522e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.9678,  3.1738,  0.6055,  ...,  3.8398, -2.3828,  2.8203]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 01:32:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of director is directors
The plural form of application is applications
The plural form of government is governments
The plural form of street is streets
The plural form of month is months
The plural form of town is towns
The plural form of idea is ideas
The plural form of river is
2024-07-18 01:32:16 root INFO     [order_1_approx] starting weight calculation for The plural form of town is towns
The plural form of application is applications
The plural form of government is governments
The plural form of river is rivers
The plural form of director is directors
The plural form of street is streets
The plural form of idea is ideas
The plural form of month is
2024-07-18 01:32:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 01:36:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4277, -0.4707, -0.6553,  ...,  0.8369, -1.2529,  0.8091],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.8203, -2.0645, -2.9102,  ...,  3.2773,  1.6328,  3.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0173, -0.0200, -0.0004,  ...,  0.0069,  0.0126, -0.0088],
        [ 0.0007, -0.0108, -0.0076,  ...,  0.0031,  0.0092,  0.0044],
        [ 0.0195,  0.0048, -0.0116,  ...,  0.0068,  0.0170, -0.0033],
        ...,
        [ 0.0150, -0.0093, -0.0144,  ..., -0.0019,  0.0002,  0.0027],
        [ 0.0139,  0.0048, -0.0096,  ...,  0.0055, -0.0271,  0.0119],
        [-0.0034, -0.0075,  0.0089,  ..., -0.0101,  0.0042, -0.0222]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7715, -1.4297, -2.1172,  ...,  2.0898,  1.7637,  3.8496]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 01:36:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of town is towns
The plural form of application is applications
The plural form of government is governments
The plural form of river is rivers
The plural form of director is directors
The plural form of street is streets
The plural form of idea is ideas
The plural form of month is
2024-07-18 01:36:23 root INFO     [order_1_approx] starting weight calculation for The plural form of month is months
The plural form of government is governments
The plural form of director is directors
The plural form of idea is ideas
The plural form of river is rivers
The plural form of application is applications
The plural form of town is towns
The plural form of street is
2024-07-18 01:36:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 01:40:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4329, -0.0936, -0.4512,  ...,  2.0391, -0.4346, -1.7471],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7031, -0.2061, -0.5557,  ..., -4.3594,  1.6709,  3.0898],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0145, -0.0174,  0.0238,  ..., -0.0070, -0.0052, -0.0152],
        [ 0.0029,  0.0003,  0.0041,  ...,  0.0091, -0.0119,  0.0052],
        [ 0.0064,  0.0025, -0.0130,  ..., -0.0052,  0.0331,  0.0019],
        ...,
        [-0.0040, -0.0071,  0.0085,  ..., -0.0024,  0.0084, -0.0113],
        [ 0.0081, -0.0002, -0.0109,  ..., -0.0047, -0.0263,  0.0159],
        [-0.0081, -0.0049,  0.0109,  ..., -0.0153,  0.0170, -0.0342]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.8936,  0.3115, -0.3035,  ..., -3.9844,  1.6533,  3.0078]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 01:40:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of month is months
The plural form of government is governments
The plural form of director is directors
The plural form of idea is ideas
The plural form of river is rivers
The plural form of application is applications
The plural form of town is towns
The plural form of street is
2024-07-18 01:40:30 root INFO     [order_1_approx] starting weight calculation for The plural form of idea is ideas
The plural form of month is months
The plural form of government is governments
The plural form of town is towns
The plural form of street is streets
The plural form of application is applications
The plural form of river is rivers
The plural form of director is
2024-07-18 01:40:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 01:44:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5283, -0.1746, -0.5591,  ...,  0.5254, -0.4031,  0.3420],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.4844, -2.5605,  2.0664,  ..., -3.3242, -0.8384,  7.6328],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0165, -0.0203,  0.0224,  ..., -0.0124, -0.0144, -0.0307],
        [ 0.0039,  0.0062, -0.0194,  ...,  0.0072,  0.0067,  0.0120],
        [ 0.0085,  0.0085, -0.0088,  ...,  0.0051,  0.0219, -0.0004],
        ...,
        [-0.0414, -0.0362, -0.0044,  ..., -0.0021,  0.0179, -0.0012],
        [ 0.0164, -0.0147, -0.0012,  ..., -0.0140, -0.0197, -0.0002],
        [-0.0206,  0.0214, -0.0070,  ..., -0.0276,  0.0406,  0.0120]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.0391, -2.8496,  2.1152,  ..., -4.2070, -0.9199,  7.3867]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 01:44:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of idea is ideas
The plural form of month is months
The plural form of government is governments
The plural form of town is towns
The plural form of street is streets
The plural form of application is applications
The plural form of river is rivers
The plural form of director is
2024-07-18 01:44:38 root INFO     [order_1_approx] starting weight calculation for The plural form of street is streets
The plural form of river is rivers
The plural form of director is directors
The plural form of government is governments
The plural form of idea is ideas
The plural form of town is towns
The plural form of month is months
The plural form of application is
2024-07-18 01:44:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 01:48:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8237, -0.2438,  0.4697,  ..., -0.3879, -0.6113,  0.0420],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.3398,  0.4932,  1.0625,  ...,  0.3125, -0.2197,  4.2656],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0082, -0.0047,  0.0285,  ...,  0.0027,  0.0005, -0.0081],
        [-0.0135,  0.0201, -0.0011,  ...,  0.0001, -0.0211, -0.0043],
        [ 0.0255,  0.0251, -0.0081,  ..., -0.0074,  0.0164,  0.0096],
        ...,
        [ 0.0025, -0.0176, -0.0032,  ...,  0.0146,  0.0033, -0.0045],
        [ 0.0213,  0.0072,  0.0118,  ..., -0.0045, -0.0219,  0.0024],
        [-0.0177,  0.0110, -0.0126,  ..., -0.0043,  0.0152, -0.0201]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2686,  1.6211,  0.3193,  ...,  0.7471,  0.0901,  4.7461]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 01:48:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of street is streets
The plural form of river is rivers
The plural form of director is directors
The plural form of government is governments
The plural form of idea is ideas
The plural form of town is towns
The plural form of month is months
The plural form of application is
2024-07-18 01:48:44 root INFO     [order_1_approx] starting weight calculation for The plural form of month is months
The plural form of idea is ideas
The plural form of town is towns
The plural form of street is streets
The plural form of river is rivers
The plural form of application is applications
The plural form of director is directors
The plural form of government is
2024-07-18 01:48:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 01:52:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3669, -0.0901, -0.0064,  ..., -0.2622, -0.6582,  0.0646],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0137, -0.9717, -0.8530,  ...,  2.6992, -0.1865,  0.5156],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0230, -0.0258,  0.0197,  ...,  0.0121, -0.0075, -0.0170],
        [ 0.0039, -0.0114,  0.0007,  ...,  0.0061, -0.0095, -0.0028],
        [ 0.0195,  0.0040, -0.0172,  ..., -0.0305,  0.0217,  0.0003],
        ...,
        [-0.0154, -0.0139, -0.0060,  ..., -0.0004,  0.0069,  0.0125],
        [-0.0040, -0.0035, -0.0064,  ..., -0.0121, -0.0241,  0.0086],
        [ 0.0064,  0.0159,  0.0092,  ..., -0.0098,  0.0253, -0.0134]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.5098,  0.1182, -1.7070,  ...,  2.2109, -0.0535, -0.3677]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 01:52:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of month is months
The plural form of idea is ideas
The plural form of town is towns
The plural form of street is streets
The plural form of river is rivers
The plural form of application is applications
The plural form of director is directors
The plural form of government is
2024-07-18 01:52:52 root INFO     [order_1_approx] starting weight calculation for The plural form of government is governments
The plural form of director is directors
The plural form of town is towns
The plural form of street is streets
The plural form of river is rivers
The plural form of month is months
The plural form of application is applications
The plural form of idea is
2024-07-18 01:52:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 01:56:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0118,  0.2357,  0.5298,  ..., -0.3545, -0.3879, -0.5405],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.8145, -1.2441, -0.2949,  ...,  3.4844, -2.3535, -0.2676],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0171, -0.0169,  0.0029,  ..., -0.0031,  0.0026,  0.0042],
        [-0.0136, -0.0089,  0.0029,  ..., -0.0011, -0.0159, -0.0056],
        [-0.0022, -0.0024, -0.0475,  ...,  0.0065,  0.0096,  0.0252],
        ...,
        [-0.0005,  0.0050,  0.0048,  ..., -0.0279,  0.0079,  0.0117],
        [-0.0022,  0.0041, -0.0152,  ...,  0.0050, -0.0263,  0.0016],
        [-0.0186,  0.0277, -0.0072,  ..., -0.0406,  0.0154, -0.0227]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.1172, -1.5527,  0.7549,  ...,  2.7891, -2.1738, -0.4312]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 01:56:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of government is governments
The plural form of director is directors
The plural form of town is towns
The plural form of street is streets
The plural form of river is rivers
The plural form of month is months
The plural form of application is applications
The plural form of idea is
2024-07-18 01:56:59 root INFO     total operator prediction time: 1977.0575058460236 seconds
2024-07-18 01:56:59 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_3pSg - Ved
2024-07-18 01:56:59 root INFO     building operator verb_3pSg - Ved
2024-07-18 01:56:59 root INFO     [order_1_approx] starting weight calculation for When he occurs something, something has been occurred
When he spends something, something has been spent
When he considers something, something has been considered
When he relates something, something has been related
When he allows something, something has been allowed
When he describes something, something has been described
When he represents something, something has been represented
When he proposes something, something has been
2024-07-18 01:57:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 02:01:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2424,  0.6338,  0.3577,  ..., -0.1815, -0.0698,  0.2385],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8916,  0.3696,  3.9277,  ..., -0.9409, -2.0430,  1.1943],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0018, -0.0220,  0.0198,  ..., -0.0029, -0.0288, -0.0083],
        [-0.0012, -0.0146, -0.0032,  ...,  0.0055, -0.0095,  0.0066],
        [ 0.0068,  0.0134, -0.0004,  ..., -0.0041,  0.0015, -0.0062],
        ...,
        [-0.0090,  0.0022,  0.0107,  ..., -0.0044, -0.0086,  0.0024],
        [ 0.0147,  0.0111,  0.0159,  ..., -0.0209, -0.0189, -0.0076],
        [-0.0044, -0.0075, -0.0014,  ...,  0.0038,  0.0304, -0.0175]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.2070,  0.5986,  4.2891,  ..., -1.7549, -2.1289,  0.6641]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 02:01:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he occurs something, something has been occurred
When he spends something, something has been spent
When he considers something, something has been considered
When he relates something, something has been related
When he allows something, something has been allowed
When he describes something, something has been described
When he represents something, something has been represented
When he proposes something, something has been
2024-07-18 02:01:07 root INFO     [order_1_approx] starting weight calculation for When he considers something, something has been considered
When he describes something, something has been described
When he spends something, something has been spent
When he relates something, something has been related
When he proposes something, something has been proposed
When he represents something, something has been represented
When he occurs something, something has been occurred
When he allows something, something has been
2024-07-18 02:01:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 02:05:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5342, -0.1956,  0.6348,  ...,  0.3044, -0.1768,  0.4119],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0811,  0.1997,  1.6592,  ...,  0.8203, -1.2500, -0.8306],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0149,  0.0077,  0.0137,  ...,  0.0043,  0.0038, -0.0033],
        [-0.0035, -0.0211,  0.0012,  ..., -0.0127,  0.0121, -0.0002],
        [ 0.0052,  0.0197, -0.0286,  ...,  0.0188,  0.0081,  0.0004],
        ...,
        [-0.0118,  0.0152,  0.0028,  ..., -0.0135,  0.0035,  0.0098],
        [-0.0031,  0.0113,  0.0015,  ..., -0.0176, -0.0359,  0.0106],
        [-0.0158, -0.0012,  0.0026,  ..., -0.0054, -0.0069, -0.0134]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5820,  0.8037,  1.2344,  ...,  0.9316, -0.5044, -0.4004]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 02:05:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he considers something, something has been considered
When he describes something, something has been described
When he spends something, something has been spent
When he relates something, something has been related
When he proposes something, something has been proposed
When he represents something, something has been represented
When he occurs something, something has been occurred
When he allows something, something has been
2024-07-18 02:05:14 root INFO     [order_1_approx] starting weight calculation for When he occurs something, something has been occurred
When he proposes something, something has been proposed
When he allows something, something has been allowed
When he describes something, something has been described
When he represents something, something has been represented
When he spends something, something has been spent
When he relates something, something has been related
When he considers something, something has been
2024-07-18 02:05:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 02:09:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7061,  0.4871,  0.4080,  ..., -0.4541,  0.0280,  0.6729],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6123,  4.0078,  0.4570,  ..., -0.5732, -3.7578, -4.6445],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0024,  0.0044,  0.0152,  ...,  0.0172, -0.0089,  0.0091],
        [-0.0123,  0.0005, -0.0002,  ..., -0.0084, -0.0058, -0.0140],
        [ 0.0175,  0.0223, -0.0142,  ...,  0.0195,  0.0253, -0.0038],
        ...,
        [-0.0025,  0.0033, -0.0031,  ..., -0.0180, -0.0106,  0.0153],
        [ 0.0292, -0.0053,  0.0037,  ..., -0.0179, -0.0533,  0.0163],
        [-0.0209,  0.0233,  0.0141,  ...,  0.0248,  0.0279, -0.0155]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9346,  4.5234,  0.7363,  ..., -1.6787, -2.8125, -4.8125]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 02:09:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he occurs something, something has been occurred
When he proposes something, something has been proposed
When he allows something, something has been allowed
When he describes something, something has been described
When he represents something, something has been represented
When he spends something, something has been spent
When he relates something, something has been related
When he considers something, something has been
2024-07-18 02:09:22 root INFO     [order_1_approx] starting weight calculation for When he relates something, something has been related
When he represents something, something has been represented
When he describes something, something has been described
When he considers something, something has been considered
When he spends something, something has been spent
When he proposes something, something has been proposed
When he allows something, something has been allowed
When he occurs something, something has been
2024-07-18 02:09:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 02:13:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0068,  0.3184,  1.1582,  ...,  0.1648,  1.0771, -0.3252],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6768, -0.0995, -1.1289,  ..., -2.1172, -1.0098, -0.2871],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0147, -0.0168, -0.0081,  ...,  0.0124, -0.0185, -0.0217],
        [ 0.0130, -0.0182, -0.0068,  ...,  0.0086, -0.0002, -0.0014],
        [ 0.0224, -0.0213, -0.0332,  ..., -0.0141,  0.0317, -0.0021],
        ...,
        [-0.0173,  0.0177, -0.0149,  ..., -0.0214,  0.0096,  0.0095],
        [ 0.0348, -0.0110,  0.0135,  ...,  0.0076, -0.0022, -0.0069],
        [-0.0054, -0.0073,  0.0144,  ..., -0.0154, -0.0058, -0.0292]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1387,  0.9443, -2.0820,  ..., -2.0527, -2.9355, -0.7314]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 02:13:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he relates something, something has been related
When he represents something, something has been represented
When he describes something, something has been described
When he considers something, something has been considered
When he spends something, something has been spent
When he proposes something, something has been proposed
When he allows something, something has been allowed
When he occurs something, something has been
2024-07-18 02:13:30 root INFO     [order_1_approx] starting weight calculation for When he allows something, something has been allowed
When he spends something, something has been spent
When he considers something, something has been considered
When he proposes something, something has been proposed
When he relates something, something has been related
When he describes something, something has been described
When he occurs something, something has been occurred
When he represents something, something has been
2024-07-18 02:13:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 02:17:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7119,  0.0854,  0.7158,  ..., -0.4736, -0.3267,  0.6777],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.0547, -0.7051,  0.2998,  ..., -0.1968, -0.4126,  2.3105],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0086,  0.0075,  0.0055,  ...,  0.0104,  0.0123,  0.0136],
        [-0.0090, -0.0044, -0.0118,  ..., -0.0047, -0.0052,  0.0105],
        [ 0.0141, -0.0031, -0.0175,  ..., -0.0032,  0.0107, -0.0254],
        ...,
        [-0.0086,  0.0024,  0.0045,  ..., -0.0318,  0.0014,  0.0115],
        [ 0.0155,  0.0042,  0.0104,  ..., -0.0093, -0.0242,  0.0016],
        [-0.0128, -0.0015,  0.0125,  ..., -0.0067,  0.0190, -0.0426]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.6777, -0.5376, -0.5791,  ..., -0.7876,  0.2915,  1.8623]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 02:17:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he allows something, something has been allowed
When he spends something, something has been spent
When he considers something, something has been considered
When he proposes something, something has been proposed
When he relates something, something has been related
When he describes something, something has been described
When he occurs something, something has been occurred
When he represents something, something has been
2024-07-18 02:17:38 root INFO     [order_1_approx] starting weight calculation for When he proposes something, something has been proposed
When he represents something, something has been represented
When he considers something, something has been considered
When he occurs something, something has been occurred
When he spends something, something has been spent
When he allows something, something has been allowed
When he relates something, something has been related
When he describes something, something has been
2024-07-18 02:17:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 02:21:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0622,  0.6206,  0.7134,  ..., -0.6450, -0.6616, -0.4387],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8887, -1.2051,  3.3379,  ..., -1.9180, -1.3555, -0.3193],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0100, -0.0264,  0.0095,  ...,  0.0151, -0.0079,  0.0076],
        [-0.0184, -0.0210,  0.0022,  ..., -0.0016, -0.0183,  0.0020],
        [ 0.0006,  0.0095, -0.0053,  ...,  0.0060,  0.0057, -0.0060],
        ...,
        [-0.0413,  0.0059,  0.0199,  ..., -0.0276,  0.0162,  0.0078],
        [ 0.0055,  0.0147,  0.0025,  ..., -0.0334, -0.0341,  0.0048],
        [-0.0411,  0.0018,  0.0303,  ...,  0.0189,  0.0002, -0.0553]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6880, -1.5547,  3.0977,  ..., -2.3281, -0.5215, -0.3142]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 02:21:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he proposes something, something has been proposed
When he represents something, something has been represented
When he considers something, something has been considered
When he occurs something, something has been occurred
When he spends something, something has been spent
When he allows something, something has been allowed
When he relates something, something has been related
When he describes something, something has been
2024-07-18 02:21:47 root INFO     [order_1_approx] starting weight calculation for When he spends something, something has been spent
When he allows something, something has been allowed
When he describes something, something has been described
When he occurs something, something has been occurred
When he considers something, something has been considered
When he represents something, something has been represented
When he proposes something, something has been proposed
When he relates something, something has been
2024-07-18 02:21:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 02:25:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0947, -0.2083,  0.8481,  ..., -0.4297, -0.4502,  0.1967],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.3320,  0.4102, -0.2021,  ..., -2.9570,  2.7363,  1.2842],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0080,  0.0072,  0.0094,  ..., -0.0010,  0.0099, -0.0202],
        [ 0.0011, -0.0065, -0.0028,  ..., -0.0017, -0.0113,  0.0007],
        [ 0.0130,  0.0029, -0.0167,  ..., -0.0027,  0.0199, -0.0194],
        ...,
        [-0.0070,  0.0092, -0.0161,  ..., -0.0182,  0.0140, -0.0043],
        [ 0.0148,  0.0009,  0.0056,  ..., -0.0284, -0.0383,  0.0090],
        [-0.0275, -0.0033,  0.0076,  ..., -0.0055,  0.0128, -0.0226]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2871,  0.1816, -0.2952,  ..., -3.0176,  2.5215,  1.2910]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 02:25:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he spends something, something has been spent
When he allows something, something has been allowed
When he describes something, something has been described
When he occurs something, something has been occurred
When he considers something, something has been considered
When he represents something, something has been represented
When he proposes something, something has been proposed
When he relates something, something has been
2024-07-18 02:25:56 root INFO     [order_1_approx] starting weight calculation for When he describes something, something has been described
When he considers something, something has been considered
When he relates something, something has been related
When he represents something, something has been represented
When he proposes something, something has been proposed
When he allows something, something has been allowed
When he occurs something, something has been occurred
When he spends something, something has been
2024-07-18 02:25:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 02:30:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1289,  0.6221,  0.5708,  ...,  0.2993,  0.1788, -0.6196],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.2715,  3.2500,  1.8145,  ..., -0.1401, -2.2051,  1.8428],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0225, -0.0010,  0.0148,  ..., -0.0021, -0.0199, -0.0129],
        [-0.0152, -0.0120,  0.0103,  ..., -0.0178, -0.0009,  0.0230],
        [ 0.0061,  0.0124, -0.0251,  ..., -0.0080,  0.0056, -0.0067],
        ...,
        [ 0.0124,  0.0095, -0.0119,  ..., -0.0227, -0.0165, -0.0077],
        [ 0.0044,  0.0111,  0.0119,  ..., -0.0147, -0.0219, -0.0061],
        [-0.0033,  0.0166,  0.0035,  ...,  0.0055,  0.0127, -0.0224]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4531,  3.4141,  1.4785,  ..., -0.7021, -1.9463,  2.5195]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 02:30:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he describes something, something has been described
When he considers something, something has been considered
When he relates something, something has been related
When he represents something, something has been represented
When he proposes something, something has been proposed
When he allows something, something has been allowed
When he occurs something, something has been occurred
When he spends something, something has been
2024-07-18 02:30:03 root INFO     total operator prediction time: 1984.386291027069 seconds
2024-07-18 02:30:03 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on adj - superlative
2024-07-18 02:30:03 root INFO     building operator adj - superlative
2024-07-18 02:30:04 root INFO     [order_1_approx] starting weight calculation for If something is the most mild, it is mildest
If something is the most able, it is ablest
If something is the most rude, it is rudest
If something is the most polite, it is politest
If something is the most noisy, it is noisiest
If something is the most tricky, it is trickiest
If something is the most dumb, it is dumbest
If something is the most hardy, it is
2024-07-18 02:30:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 02:34:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7280,  1.1035,  0.2450,  ...,  0.1511, -0.9888,  1.0410],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.6523, -2.5391, -3.0859,  ..., -3.0391,  3.0898,  8.4531],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0125, -0.0140,  0.0047,  ..., -0.0055, -0.0064, -0.0099],
        [-0.0069, -0.0209,  0.0059,  ...,  0.0040, -0.0006, -0.0073],
        [ 0.0184, -0.0022, -0.0131,  ...,  0.0118,  0.0053, -0.0038],
        ...,
        [ 0.0032, -0.0090, -0.0023,  ..., -0.0329,  0.0035, -0.0032],
        [-0.0100,  0.0001,  0.0009,  ..., -0.0091, -0.0242,  0.0110],
        [-0.0300,  0.0024,  0.0162,  ..., -0.0130,  0.0101, -0.0161]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.0664, -2.9062, -3.3516,  ..., -3.4160,  2.6543,  9.2656]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 02:34:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most mild, it is mildest
If something is the most able, it is ablest
If something is the most rude, it is rudest
If something is the most polite, it is politest
If something is the most noisy, it is noisiest
If something is the most tricky, it is trickiest
If something is the most dumb, it is dumbest
If something is the most hardy, it is
2024-07-18 02:34:10 root INFO     [order_1_approx] starting weight calculation for If something is the most polite, it is politest
If something is the most mild, it is mildest
If something is the most hardy, it is hardiest
If something is the most able, it is ablest
If something is the most dumb, it is dumbest
If something is the most rude, it is rudest
If something is the most tricky, it is trickiest
If something is the most noisy, it is
2024-07-18 02:34:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 02:38:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4321,  0.7939,  0.2373,  ..., -0.3062, -0.5122,  0.6313],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5107, -1.5303, -3.1406,  ...,  3.0430,  2.7520,  4.6367],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0071, -0.0337, -0.0038,  ...,  0.0195, -0.0156, -0.0035],
        [-0.0117, -0.0191,  0.0005,  ...,  0.0106,  0.0009,  0.0147],
        [ 0.0100,  0.0046, -0.0114,  ...,  0.0016,  0.0116, -0.0037],
        ...,
        [ 0.0007, -0.0160,  0.0050,  ..., -0.0386, -0.0060, -0.0143],
        [ 0.0196, -0.0156,  0.0155,  ..., -0.0131, -0.0240,  0.0086],
        [-0.0193, -0.0026, -0.0066,  ..., -0.0233, -0.0011, -0.0305]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1785, -1.3154, -2.4023,  ...,  3.4570,  2.5527,  3.7422]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 02:38:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most polite, it is politest
If something is the most mild, it is mildest
If something is the most hardy, it is hardiest
If something is the most able, it is ablest
If something is the most dumb, it is dumbest
If something is the most rude, it is rudest
If something is the most tricky, it is trickiest
If something is the most noisy, it is
2024-07-18 02:38:18 root INFO     [order_1_approx] starting weight calculation for If something is the most mild, it is mildest
If something is the most noisy, it is noisiest
If something is the most rude, it is rudest
If something is the most dumb, it is dumbest
If something is the most hardy, it is hardiest
If something is the most tricky, it is trickiest
If something is the most polite, it is politest
If something is the most able, it is
2024-07-18 02:38:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 02:42:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1068, -0.6582,  0.0604,  ...,  0.6772, -0.7422, -0.7993],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5029,  2.3242, -2.0371,  ...,  0.7534,  1.5186,  1.7871],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.2436e-02, -3.6774e-03, -5.8479e-03,  ..., -5.5313e-05,
          4.5013e-03, -1.1337e-02],
        [ 2.4948e-03, -1.1017e-02,  5.0049e-03,  ..., -6.3705e-03,
         -1.0681e-04,  2.5558e-04],
        [-2.2945e-03,  2.5940e-02,  5.2528e-03,  ...,  1.1536e-02,
         -3.2387e-03,  7.0534e-03],
        ...,
        [-2.0874e-02, -3.0914e-02, -1.4839e-02,  ..., -3.3813e-02,
         -1.5656e-02, -2.3041e-02],
        [ 1.7410e-02, -6.5384e-03,  6.7062e-03,  ..., -5.4474e-03,
         -2.1484e-02, -1.4221e-02],
        [-1.2550e-02,  2.4475e-02,  1.7654e-02,  ...,  4.7340e-03,
         -6.8665e-05, -7.2060e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3643,  2.8457, -0.9492,  ...,  0.0454,  0.8403,  1.9609]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 02:42:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most mild, it is mildest
If something is the most noisy, it is noisiest
If something is the most rude, it is rudest
If something is the most dumb, it is dumbest
If something is the most hardy, it is hardiest
If something is the most tricky, it is trickiest
If something is the most polite, it is politest
If something is the most able, it is
2024-07-18 02:42:25 root INFO     [order_1_approx] starting weight calculation for If something is the most noisy, it is noisiest
If something is the most polite, it is politest
If something is the most able, it is ablest
If something is the most hardy, it is hardiest
If something is the most tricky, it is trickiest
If something is the most dumb, it is dumbest
If something is the most rude, it is rudest
If something is the most mild, it is
2024-07-18 02:42:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 02:46:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3838, -0.2378,  0.5352,  ...,  0.9229, -0.8037,  0.5269],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5132,  0.4905, -6.1484,  ..., -0.7695, -1.6396,  5.3242],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0006, -0.0215,  0.0011,  ...,  0.0006, -0.0182, -0.0173],
        [-0.0055, -0.0196, -0.0154,  ..., -0.0138,  0.0042, -0.0122],
        [ 0.0022,  0.0070,  0.0002,  ...,  0.0314, -0.0053,  0.0138],
        ...,
        [-0.0020, -0.0361,  0.0049,  ..., -0.0153,  0.0027,  0.0074],
        [-0.0013,  0.0042,  0.0203,  ..., -0.0142, -0.0075,  0.0125],
        [-0.0253, -0.0083, -0.0086,  ..., -0.0181,  0.0149, -0.0238]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.1582, -0.6777, -5.2344,  ..., -0.2441, -1.6055,  4.8281]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 02:46:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most noisy, it is noisiest
If something is the most polite, it is politest
If something is the most able, it is ablest
If something is the most hardy, it is hardiest
If something is the most tricky, it is trickiest
If something is the most dumb, it is dumbest
If something is the most rude, it is rudest
If something is the most mild, it is
2024-07-18 02:46:32 root INFO     [order_1_approx] starting weight calculation for If something is the most noisy, it is noisiest
If something is the most hardy, it is hardiest
If something is the most mild, it is mildest
If something is the most tricky, it is trickiest
If something is the most polite, it is politest
If something is the most rude, it is rudest
If something is the most able, it is ablest
If something is the most dumb, it is
2024-07-18 02:46:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 02:50:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0549, -0.0510, -0.2502,  ..., -0.4695, -0.7061, -0.7935],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5859, -4.4688, -3.4844,  ..., -4.8789,  5.9766,  7.4805],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0237, -0.0152,  0.0021,  ...,  0.0071, -0.0054, -0.0281],
        [-0.0088, -0.0209,  0.0187,  ...,  0.0075, -0.0057,  0.0132],
        [-0.0066, -0.0042, -0.0089,  ...,  0.0306, -0.0016, -0.0019],
        ...,
        [-0.0075, -0.0276, -0.0004,  ..., -0.0146, -0.0132, -0.0043],
        [ 0.0058,  0.0033, -0.0151,  ..., -0.0055, -0.0130,  0.0058],
        [-0.0263,  0.0015, -0.0180,  ..., -0.0325, -0.0008, -0.0161]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2617, -4.0195, -3.1133,  ..., -4.2539,  5.2969,  4.8047]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 02:50:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most noisy, it is noisiest
If something is the most hardy, it is hardiest
If something is the most mild, it is mildest
If something is the most tricky, it is trickiest
If something is the most polite, it is politest
If something is the most rude, it is rudest
If something is the most able, it is ablest
If something is the most dumb, it is
2024-07-18 02:50:38 root INFO     [order_1_approx] starting weight calculation for If something is the most mild, it is mildest
If something is the most tricky, it is trickiest
If something is the most noisy, it is noisiest
If something is the most hardy, it is hardiest
If something is the most dumb, it is dumbest
If something is the most able, it is ablest
If something is the most rude, it is rudest
If something is the most polite, it is
2024-07-18 02:50:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 02:54:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7031, -0.6587,  0.5190,  ...,  0.2046, -0.3135,  1.3965],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6230,  0.8735, -2.9609,  ...,  2.1348, -1.1748,  0.3604],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0090, -0.0165,  0.0107,  ...,  0.0053, -0.0107, -0.0084],
        [-0.0106, -0.0276, -0.0099,  ...,  0.0079,  0.0001, -0.0144],
        [ 0.0015, -0.0159, -0.0140,  ...,  0.0079, -0.0101,  0.0054],
        ...,
        [-0.0215, -0.0330,  0.0069,  ..., -0.0262, -0.0119, -0.0177],
        [-0.0068, -0.0089,  0.0028,  ..., -0.0276, -0.0166,  0.0100],
        [-0.0118,  0.0107,  0.0075,  ..., -0.0165,  0.0106, -0.0288]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3369,  0.1060, -3.8086,  ...,  2.8086, -0.6616, -0.9580]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 02:54:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most mild, it is mildest
If something is the most tricky, it is trickiest
If something is the most noisy, it is noisiest
If something is the most hardy, it is hardiest
If something is the most dumb, it is dumbest
If something is the most able, it is ablest
If something is the most rude, it is rudest
If something is the most polite, it is
2024-07-18 02:54:44 root INFO     [order_1_approx] starting weight calculation for If something is the most noisy, it is noisiest
If something is the most polite, it is politest
If something is the most mild, it is mildest
If something is the most dumb, it is dumbest
If something is the most able, it is ablest
If something is the most hardy, it is hardiest
If something is the most rude, it is rudest
If something is the most tricky, it is
2024-07-18 02:54:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 02:58:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1884,  0.4548, -0.5869,  ..., -0.1505, -0.8496, -0.0264],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-6.8789,  0.3379,  1.9590,  ...,  5.6016,  0.7871,  4.1172],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0058,  0.0071,  0.0022,  ..., -0.0080, -0.0135, -0.0300],
        [-0.0086, -0.0134,  0.0097,  ..., -0.0104, -0.0146,  0.0004],
        [-0.0310, -0.0102,  0.0077,  ...,  0.0239,  0.0090, -0.0021],
        ...,
        [-0.0168, -0.0354, -0.0048,  ..., -0.0388,  0.0186, -0.0280],
        [ 0.0112, -0.0042,  0.0222,  ..., -0.0260, -0.0187,  0.0133],
        [-0.0157,  0.0046,  0.0222,  ..., -0.0288,  0.0104, -0.0358]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-6.3711,  0.8174,  1.3916,  ...,  5.8828,  1.1670,  2.9531]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 02:58:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most noisy, it is noisiest
If something is the most polite, it is politest
If something is the most mild, it is mildest
If something is the most dumb, it is dumbest
If something is the most able, it is ablest
If something is the most hardy, it is hardiest
If something is the most rude, it is rudest
If something is the most tricky, it is
2024-07-18 02:58:51 root INFO     [order_1_approx] starting weight calculation for If something is the most dumb, it is dumbest
If something is the most noisy, it is noisiest
If something is the most mild, it is mildest
If something is the most polite, it is politest
If something is the most able, it is ablest
If something is the most tricky, it is trickiest
If something is the most hardy, it is hardiest
If something is the most rude, it is
2024-07-18 02:58:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 03:02:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5156, -0.6133, -0.1118,  ...,  0.2295, -0.1001,  0.5830],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.4512, -1.9893, -4.2578,  ...,  2.5742,  0.4443,  5.0820],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0162, -0.0258,  0.0052,  ..., -0.0096, -0.0075, -0.0154],
        [ 0.0142, -0.0163, -0.0020,  ..., -0.0068,  0.0038,  0.0054],
        [-0.0017, -0.0231, -0.0065,  ...,  0.0167, -0.0025,  0.0065],
        ...,
        [-0.0118, -0.0183, -0.0027,  ..., -0.0468, -0.0015, -0.0192],
        [ 0.0045,  0.0099,  0.0170,  ..., -0.0186, -0.0250,  0.0105],
        [-0.0211, -0.0011,  0.0044,  ..., -0.0053, -0.0071, -0.0236]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.6719, -1.3145, -4.8516,  ...,  2.6738,  1.0078,  4.9180]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 03:02:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most dumb, it is dumbest
If something is the most noisy, it is noisiest
If something is the most mild, it is mildest
If something is the most polite, it is politest
If something is the most able, it is ablest
If something is the most tricky, it is trickiest
If something is the most hardy, it is hardiest
If something is the most rude, it is
2024-07-18 03:02:58 root INFO     total operator prediction time: 1974.7107281684875 seconds
2024-07-18 03:02:58 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+er_irreg
2024-07-18 03:02:58 root INFO     building operator verb+er_irreg
2024-07-18 03:02:59 root INFO     [order_1_approx] starting weight calculation for If you discover something, you are a discoverer
If you lose something, you are a loser
If you tell something, you are a teller
If you achieve something, you are a achiever
If you interpret something, you are a interpreter
If you believe something, you are a believer
If you choreograph something, you are a choreographer
If you write something, you are a
2024-07-18 03:02:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 03:07:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0496, -0.7505,  0.2458,  ...,  0.0684, -0.8340,  0.3630],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.0684,  1.2646, -1.5605,  ...,  2.3281,  0.0886,  1.6846],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.8168e-02, -1.8372e-02, -3.2425e-03,  ...,  2.7008e-03,
         -1.0895e-02,  2.3365e-03],
        [-2.1606e-02, -2.3651e-03, -3.8147e-06,  ..., -4.0436e-03,
          5.2948e-03, -1.8568e-03],
        [ 7.0343e-03,  1.2566e-02, -4.3526e-03,  ...,  1.2064e-03,
          2.0721e-02, -5.2567e-03],
        ...,
        [-2.3972e-02, -1.4847e-02, -8.8730e-03,  ..., -6.2027e-03,
         -1.0010e-02,  3.2425e-03],
        [ 1.8280e-02,  1.3428e-02,  1.0666e-02,  ..., -2.2842e-02,
         -1.5747e-02,  8.2397e-03],
        [-8.6823e-03, -1.3565e-02,  5.1498e-03,  ..., -1.4000e-02,
          2.7828e-03,  3.7422e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4844,  0.9194, -1.1641,  ...,  1.8047,  0.5791,  0.9214]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 03:07:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you discover something, you are a discoverer
If you lose something, you are a loser
If you tell something, you are a teller
If you achieve something, you are a achiever
If you interpret something, you are a interpreter
If you believe something, you are a believer
If you choreograph something, you are a choreographer
If you write something, you are a
2024-07-18 03:07:05 root INFO     [order_1_approx] starting weight calculation for If you achieve something, you are a achiever
If you interpret something, you are a interpreter
If you discover something, you are a discoverer
If you choreograph something, you are a choreographer
If you write something, you are a writer
If you believe something, you are a believer
If you lose something, you are a loser
If you tell something, you are a
2024-07-18 03:07:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 03:11:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6387,  0.0034,  0.6265,  ...,  0.8574, -0.4302,  0.7275],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0840,  1.3066, -1.5781,  ..., -0.2202,  2.2949,  2.9492],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0130,  0.0074,  0.0098,  ..., -0.0002,  0.0025,  0.0068],
        [-0.0325, -0.0197,  0.0091,  ...,  0.0061,  0.0042, -0.0017],
        [ 0.0068,  0.0100, -0.0016,  ..., -0.0068, -0.0112,  0.0109],
        ...,
        [-0.0063, -0.0201,  0.0046,  ..., -0.0035, -0.0010,  0.0236],
        [ 0.0328,  0.0113,  0.0094,  ...,  0.0005, -0.0141, -0.0015],
        [ 0.0045, -0.0142,  0.0052,  ..., -0.0113,  0.0046,  0.0185]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3672,  1.1758, -2.4844,  ..., -0.9019,  2.3867,  2.5000]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 03:11:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you achieve something, you are a achiever
If you interpret something, you are a interpreter
If you discover something, you are a discoverer
If you choreograph something, you are a choreographer
If you write something, you are a writer
If you believe something, you are a believer
If you lose something, you are a loser
If you tell something, you are a
2024-07-18 03:11:12 root INFO     [order_1_approx] starting weight calculation for If you write something, you are a writer
If you lose something, you are a loser
If you discover something, you are a discoverer
If you tell something, you are a teller
If you believe something, you are a believer
If you choreograph something, you are a choreographer
If you interpret something, you are a interpreter
If you achieve something, you are a
2024-07-18 03:11:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 03:15:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6484,  0.0767,  0.2251,  ...,  0.1509, -0.8955,  0.4089],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.4375,  1.0010, -4.8516,  ..., -3.3027,  1.5879,  0.9565],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0068, -0.0038,  0.0053,  ...,  0.0063,  0.0092,  0.0105],
        [-0.0041, -0.0132,  0.0006,  ..., -0.0117,  0.0073,  0.0052],
        [ 0.0088,  0.0095,  0.0021,  ...,  0.0204,  0.0017,  0.0016],
        ...,
        [-0.0074, -0.0040,  0.0053,  ..., -0.0130, -0.0184, -0.0025],
        [ 0.0128,  0.0096,  0.0076,  ..., -0.0053, -0.0148,  0.0074],
        [ 0.0059, -0.0059,  0.0097,  ..., -0.0063,  0.0171, -0.0123]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.7715,  1.4609, -5.1406,  ..., -3.6953,  1.1914,  0.0493]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 03:15:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you write something, you are a writer
If you lose something, you are a loser
If you discover something, you are a discoverer
If you tell something, you are a teller
If you believe something, you are a believer
If you choreograph something, you are a choreographer
If you interpret something, you are a interpreter
If you achieve something, you are a
2024-07-18 03:15:19 root INFO     [order_1_approx] starting weight calculation for If you discover something, you are a discoverer
If you write something, you are a writer
If you lose something, you are a loser
If you interpret something, you are a interpreter
If you tell something, you are a teller
If you achieve something, you are a achiever
If you believe something, you are a believer
If you choreograph something, you are a
2024-07-18 03:15:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 03:19:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5640, -0.0720,  0.8516,  ...,  0.6372, -0.9341,  0.5371],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.9805, -1.7139, -1.2803,  ..., -2.2578, -1.5635,  2.5996],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0105,  0.0039, -0.0121,  ...,  0.0046,  0.0034, -0.0093],
        [-0.0027,  0.0016,  0.0005,  ...,  0.0072,  0.0035,  0.0016],
        [ 0.0007, -0.0062,  0.0057,  ...,  0.0135,  0.0019,  0.0329],
        ...,
        [-0.0082, -0.0196,  0.0013,  ...,  0.0042,  0.0043,  0.0094],
        [ 0.0206,  0.0147, -0.0099,  ..., -0.0084, -0.0006, -0.0005],
        [-0.0099,  0.0043,  0.0029,  ..., -0.0040,  0.0104,  0.0042]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.3633, -1.5518, -1.9316,  ..., -1.9541, -2.1016,  2.6562]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 03:19:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you discover something, you are a discoverer
If you write something, you are a writer
If you lose something, you are a loser
If you interpret something, you are a interpreter
If you tell something, you are a teller
If you achieve something, you are a achiever
If you believe something, you are a believer
If you choreograph something, you are a
2024-07-18 03:19:26 root INFO     [order_1_approx] starting weight calculation for If you tell something, you are a teller
If you choreograph something, you are a choreographer
If you interpret something, you are a interpreter
If you discover something, you are a discoverer
If you write something, you are a writer
If you achieve something, you are a achiever
If you believe something, you are a believer
If you lose something, you are a
2024-07-18 03:19:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 03:23:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2148, -0.3557,  0.2834,  ...,  0.1710, -1.0498,  1.0381],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 6.0586,  1.8984, -1.7822,  ...,  1.0537, -0.6953,  0.8018],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.1877e-03, -3.8086e-02, -3.9825e-03,  ..., -3.6697e-03,
          2.9907e-03,  1.5495e-02],
        [-7.6065e-03, -2.7985e-02,  6.1035e-05,  ...,  3.7060e-03,
         -7.4768e-04,  8.0872e-03],
        [-4.6158e-03, -1.9379e-03, -2.7618e-03,  ...,  3.1281e-04,
          4.0016e-03,  1.2909e-02],
        ...,
        [-5.8136e-03, -2.8397e-02,  2.2888e-04,  ..., -1.1917e-02,
         -3.7613e-03,  4.4785e-03],
        [-9.5444e-03,  7.6141e-03,  6.8321e-03,  ...,  2.5177e-04,
         -1.6998e-02, -5.5161e-03],
        [-1.3189e-03,  7.0267e-03, -7.0915e-03,  ...,  1.2665e-03,
          7.7744e-03, -9.8267e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.2461,  2.1641, -2.0586,  ...,  0.8281, -0.1777, -0.4170]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 03:23:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you tell something, you are a teller
If you choreograph something, you are a choreographer
If you interpret something, you are a interpreter
If you discover something, you are a discoverer
If you write something, you are a writer
If you achieve something, you are a achiever
If you believe something, you are a believer
If you lose something, you are a
2024-07-18 03:23:34 root INFO     [order_1_approx] starting weight calculation for If you tell something, you are a teller
If you interpret something, you are a interpreter
If you write something, you are a writer
If you achieve something, you are a achiever
If you believe something, you are a believer
If you choreograph something, you are a choreographer
If you lose something, you are a loser
If you discover something, you are a
2024-07-18 03:23:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 03:27:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6055, -0.3623,  0.0526,  ...,  0.3247, -0.2034,  0.4407],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.3242, -1.4492, -3.6602,  ..., -6.5742,  0.0855,  2.6484],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0070,  0.0086,  0.0037,  ...,  0.0066,  0.0028, -0.0088],
        [-0.0077, -0.0078,  0.0079,  ...,  0.0075, -0.0031,  0.0022],
        [ 0.0146,  0.0142, -0.0088,  ..., -0.0060, -0.0111,  0.0011],
        ...,
        [-0.0076,  0.0074, -0.0044,  ..., -0.0179, -0.0101, -0.0018],
        [ 0.0241,  0.0184, -0.0068,  ..., -0.0026,  0.0013,  0.0014],
        [ 0.0101, -0.0116, -0.0113,  ..., -0.0069,  0.0075,  0.0057]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.7539e+00, -1.4570e+00, -4.2539e+00,  ..., -6.8945e+00,
         -6.5918e-03,  2.1641e+00]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SubBackward0>) 

                    
2024-07-18 03:27:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you tell something, you are a teller
If you interpret something, you are a interpreter
If you write something, you are a writer
If you achieve something, you are a achiever
If you believe something, you are a believer
If you choreograph something, you are a choreographer
If you lose something, you are a loser
If you discover something, you are a
2024-07-18 03:27:42 root INFO     [order_1_approx] starting weight calculation for If you tell something, you are a teller
If you discover something, you are a discoverer
If you choreograph something, you are a choreographer
If you believe something, you are a believer
If you achieve something, you are a achiever
If you lose something, you are a loser
If you write something, you are a writer
If you interpret something, you are a
2024-07-18 03:27:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 03:31:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3398, -0.2024, -0.0602,  ..., -0.0598, -0.2974,  0.8286],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.2119,  1.0527, -1.4385,  ..., -3.7852,  0.4778,  5.2969],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0122,  0.0149,  0.0016,  ...,  0.0124, -0.0042, -0.0202],
        [-0.0029, -0.0026,  0.0060,  ..., -0.0029, -0.0014, -0.0025],
        [-0.0097, -0.0012, -0.0035,  ...,  0.0081,  0.0117,  0.0064],
        ...,
        [-0.0037, -0.0096, -0.0086,  ..., -0.0031, -0.0037,  0.0034],
        [-0.0024,  0.0015,  0.0081,  ...,  0.0006, -0.0241, -0.0026],
        [-0.0035, -0.0176, -0.0064,  ..., -0.0062,  0.0091, -0.0018]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0146,  1.2832, -1.1025,  ..., -3.9883,  0.3777,  5.4961]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 03:31:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you tell something, you are a teller
If you discover something, you are a discoverer
If you choreograph something, you are a choreographer
If you believe something, you are a believer
If you achieve something, you are a achiever
If you lose something, you are a loser
If you write something, you are a writer
If you interpret something, you are a
2024-07-18 03:31:47 root INFO     [order_1_approx] starting weight calculation for If you choreograph something, you are a choreographer
If you tell something, you are a teller
If you achieve something, you are a achiever
If you discover something, you are a discoverer
If you lose something, you are a loser
If you write something, you are a writer
If you interpret something, you are a interpreter
If you believe something, you are a
2024-07-18 03:31:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 03:35:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1199, -0.3728, -0.2269,  ...,  0.6172,  0.3174,  0.5913],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.9297, -1.0205, -3.9785,  ..., -1.7285,  0.6646,  2.1758],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0095, -0.0024, -0.0008,  ..., -0.0086, -0.0047, -0.0096],
        [ 0.0002,  0.0018,  0.0113,  ..., -0.0016,  0.0013,  0.0097],
        [ 0.0030,  0.0039, -0.0076,  ...,  0.0131, -0.0047,  0.0093],
        ...,
        [-0.0091,  0.0029,  0.0063,  ..., -0.0210, -0.0102,  0.0021],
        [-0.0043,  0.0012,  0.0074,  ...,  0.0105, -0.0345,  0.0002],
        [-0.0002,  0.0057, -0.0118,  ..., -0.0061, -0.0023, -0.0174]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.4609, -0.4482, -4.2188,  ..., -1.7764,  0.8931,  2.1719]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 03:35:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you choreograph something, you are a choreographer
If you tell something, you are a teller
If you achieve something, you are a achiever
If you discover something, you are a discoverer
If you lose something, you are a loser
If you write something, you are a writer
If you interpret something, you are a interpreter
If you believe something, you are a
2024-07-18 03:35:52 root INFO     total operator prediction time: 1974.2489702701569 seconds
2024-07-18 03:35:52 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on over+adj_reg
2024-07-18 03:35:52 root INFO     building operator over+adj_reg
2024-07-18 03:35:53 root INFO     [order_1_approx] starting weight calculation for If something is too saturated, it is oversaturated
If something is too laid, it is overlaid
If something is too played, it is overplayed
If something is too simplified, it is oversimplified
If something is too heard, it is overheard
If something is too grown, it is overgrown
If something is too charged, it is overcharged
If something is too protective, it is
2024-07-18 03:35:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 03:39:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5269, -0.2422,  0.6211,  ...,  0.6572, -0.4968,  0.4075],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1250,  0.5801,  0.1045,  ...,  0.9277,  1.5439,  1.3145],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0084,  0.0062,  0.0066,  ..., -0.0081, -0.0242, -0.0265],
        [ 0.0106,  0.0050, -0.0084,  ...,  0.0064,  0.0036, -0.0065],
        [-0.0007, -0.0009,  0.0012,  ...,  0.0096, -0.0044, -0.0056],
        ...,
        [-0.0238, -0.0236, -0.0075,  ...,  0.0041, -0.0183, -0.0068],
        [ 0.0005, -0.0077,  0.0266,  ..., -0.0006, -0.0076,  0.0004],
        [ 0.0028,  0.0126,  0.0083,  ..., -0.0085,  0.0095,  0.0216]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7500,  1.2578,  0.4360,  ...,  0.4993,  1.2129,  1.7207]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 03:40:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too saturated, it is oversaturated
If something is too laid, it is overlaid
If something is too played, it is overplayed
If something is too simplified, it is oversimplified
If something is too heard, it is overheard
If something is too grown, it is overgrown
If something is too charged, it is overcharged
If something is too protective, it is
2024-07-18 03:40:00 root INFO     [order_1_approx] starting weight calculation for If something is too saturated, it is oversaturated
If something is too grown, it is overgrown
If something is too charged, it is overcharged
If something is too protective, it is overprotective
If something is too heard, it is overheard
If something is too laid, it is overlaid
If something is too simplified, it is oversimplified
If something is too played, it is
2024-07-18 03:40:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 03:44:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0089,  0.2883,  0.1428,  ..., -0.1727, -0.5322,  0.0527],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1816, -0.4546,  0.9102,  ...,  1.5186,  0.3088,  4.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0094,  0.0019, -0.0064,  ..., -0.0023, -0.0095, -0.0326],
        [-0.0153, -0.0019,  0.0073,  ..., -0.0077, -0.0037, -0.0359],
        [ 0.0171, -0.0126, -0.0034,  ...,  0.0073,  0.0117,  0.0196],
        ...,
        [-0.0015, -0.0016,  0.0176,  ..., -0.0193,  0.0126,  0.0176],
        [ 0.0098,  0.0264, -0.0101,  ..., -0.0205, -0.0204,  0.0025],
        [-0.0221, -0.0069,  0.0095,  ..., -0.0048,  0.0163, -0.0086]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2791,  1.1445,  0.9258,  ...,  0.3564, -0.7900,  3.5957]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 03:44:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too saturated, it is oversaturated
If something is too grown, it is overgrown
If something is too charged, it is overcharged
If something is too protective, it is overprotective
If something is too heard, it is overheard
If something is too laid, it is overlaid
If something is too simplified, it is oversimplified
If something is too played, it is
2024-07-18 03:44:07 root INFO     [order_1_approx] starting weight calculation for If something is too grown, it is overgrown
If something is too heard, it is overheard
If something is too played, it is overplayed
If something is too charged, it is overcharged
If something is too simplified, it is oversimplified
If something is too laid, it is overlaid
If something is too protective, it is overprotective
If something is too saturated, it is
2024-07-18 03:44:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 03:48:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1599,  0.7104,  1.5918,  ..., -0.5991, -0.5640, -0.5317],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.2324,  0.5288,  1.0166,  ...,  1.4287,  2.8555,  2.3359],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0201, -0.0063,  0.0014,  ...,  0.0059, -0.0046, -0.0268],
        [-0.0164,  0.0097,  0.0105,  ..., -0.0128, -0.0106, -0.0054],
        [-0.0193,  0.0157, -0.0156,  ...,  0.0066,  0.0069, -0.0144],
        ...,
        [-0.0268, -0.0424, -0.0021,  ...,  0.0056,  0.0021, -0.0014],
        [-0.0088, -0.0145, -0.0017,  ..., -0.0157, -0.0199, -0.0075],
        [-0.0035, -0.0129, -0.0047,  ..., -0.0121,  0.0205, -0.0166]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.0820,  0.8403,  0.3389,  ...,  1.1016,  2.9023,  1.4434]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 03:48:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too grown, it is overgrown
If something is too heard, it is overheard
If something is too played, it is overplayed
If something is too charged, it is overcharged
If something is too simplified, it is oversimplified
If something is too laid, it is overlaid
If something is too protective, it is overprotective
If something is too saturated, it is
2024-07-18 03:48:16 root INFO     [order_1_approx] starting weight calculation for If something is too saturated, it is oversaturated
If something is too heard, it is overheard
If something is too simplified, it is oversimplified
If something is too grown, it is overgrown
If something is too protective, it is overprotective
If something is too played, it is overplayed
If something is too laid, it is overlaid
If something is too charged, it is
2024-07-18 03:48:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 03:52:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6787,  0.4121,  0.9590,  ...,  0.5933, -0.0515, -0.3501],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.6250, -0.8877, -1.1709,  ...,  0.9844,  2.9219,  2.0566],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0003, -0.0122,  0.0067,  ..., -0.0046,  0.0080, -0.0313],
        [ 0.0006,  0.0218,  0.0110,  ...,  0.0040, -0.0085,  0.0028],
        [ 0.0107,  0.0158,  0.0146,  ..., -0.0127,  0.0024,  0.0204],
        ...,
        [-0.0049, -0.0325, -0.0016,  ..., -0.0038,  0.0031,  0.0057],
        [-0.0072, -0.0357, -0.0128,  ..., -0.0270,  0.0058, -0.0052],
        [-0.0027,  0.0142,  0.0188,  ...,  0.0038, -0.0012, -0.0175]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.9688, -1.1152, -0.0996,  ...,  0.9014,  2.0234,  0.8779]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 03:52:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too saturated, it is oversaturated
If something is too heard, it is overheard
If something is too simplified, it is oversimplified
If something is too grown, it is overgrown
If something is too protective, it is overprotective
If something is too played, it is overplayed
If something is too laid, it is overlaid
If something is too charged, it is
2024-07-18 03:52:23 root INFO     [order_1_approx] starting weight calculation for If something is too charged, it is overcharged
If something is too protective, it is overprotective
If something is too simplified, it is oversimplified
If something is too played, it is overplayed
If something is too saturated, it is oversaturated
If something is too grown, it is overgrown
If something is too laid, it is overlaid
If something is too heard, it is
2024-07-18 03:52:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 03:56:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3276,  0.8232,  0.5317,  ...,  0.4395,  0.1537,  0.7598],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.5664,  0.7705, -0.4570,  ..., -0.1133,  4.1680,  4.8516],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0049, -0.0244,  0.0049,  ...,  0.0152,  0.0045, -0.0197],
        [ 0.0058,  0.0052, -0.0140,  ...,  0.0019, -0.0021,  0.0073],
        [ 0.0048, -0.0017,  0.0274,  ...,  0.0021,  0.0028,  0.0302],
        ...,
        [ 0.0023, -0.0174, -0.0251,  ..., -0.0163,  0.0138, -0.0061],
        [-0.0014, -0.0137, -0.0128,  ..., -0.0040, -0.0216, -0.0182],
        [ 0.0023, -0.0012, -0.0109,  ..., -0.0134, -0.0075, -0.0005]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[3.2656, 0.9102, 0.0093,  ..., 0.3689, 3.3945, 4.2500]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 03:56:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too charged, it is overcharged
If something is too protective, it is overprotective
If something is too simplified, it is oversimplified
If something is too played, it is overplayed
If something is too saturated, it is oversaturated
If something is too grown, it is overgrown
If something is too laid, it is overlaid
If something is too heard, it is
2024-07-18 03:56:30 root INFO     [order_1_approx] starting weight calculation for If something is too laid, it is overlaid
If something is too heard, it is overheard
If something is too played, it is overplayed
If something is too charged, it is overcharged
If something is too protective, it is overprotective
If something is too saturated, it is oversaturated
If something is too grown, it is overgrown
If something is too simplified, it is
2024-07-18 03:56:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 04:00:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4019,  0.2764,  0.6040,  ..., -0.0448, -0.5244,  0.3542],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2754,  0.0212,  1.6025,  ..., -0.5952,  1.7979,  2.0996],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0064, -0.0249,  0.0003,  ...,  0.0067,  0.0001, -0.0089],
        [ 0.0001,  0.0008,  0.0078,  ..., -0.0057, -0.0030, -0.0057],
        [-0.0049, -0.0015, -0.0130,  ...,  0.0058,  0.0077,  0.0069],
        ...,
        [-0.0040, -0.0039,  0.0087,  ..., -0.0071, -0.0161,  0.0055],
        [-0.0089, -0.0149,  0.0087,  ..., -0.0250, -0.0208, -0.0022],
        [-0.0016,  0.0096, -0.0013,  ..., -0.0133,  0.0106, -0.0069]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1113,  0.2998,  1.2500,  ..., -0.4380,  0.5400,  1.3613]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 04:00:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too laid, it is overlaid
If something is too heard, it is overheard
If something is too played, it is overplayed
If something is too charged, it is overcharged
If something is too protective, it is overprotective
If something is too saturated, it is oversaturated
If something is too grown, it is overgrown
If something is too simplified, it is
2024-07-18 04:00:36 root INFO     [order_1_approx] starting weight calculation for If something is too protective, it is overprotective
If something is too played, it is overplayed
If something is too laid, it is overlaid
If something is too saturated, it is oversaturated
If something is too heard, it is overheard
If something is too simplified, it is oversimplified
If something is too charged, it is overcharged
If something is too grown, it is
2024-07-18 04:00:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 04:04:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1760,  0.7632,  1.0771,  ...,  1.4316,  0.1420, -0.2737],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5781, -0.0591, -0.1973,  ..., -1.3438,  3.0254,  0.6309],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0126, -0.0370, -0.0033,  ...,  0.0128, -0.0007, -0.0040],
        [-0.0123,  0.0011,  0.0106,  ...,  0.0038,  0.0071, -0.0076],
        [ 0.0105,  0.0047,  0.0092,  ..., -0.0026,  0.0022,  0.0171],
        ...,
        [ 0.0063, -0.0019, -0.0142,  ..., -0.0114, -0.0131,  0.0008],
        [-0.0102, -0.0099,  0.0019,  ...,  0.0044,  0.0184, -0.0010],
        [-0.0094, -0.0194,  0.0073,  ...,  0.0034, -0.0001, -0.0004]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7324,  0.6929, -1.1396,  ..., -3.0977,  3.9219, -0.2104]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 04:04:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too protective, it is overprotective
If something is too played, it is overplayed
If something is too laid, it is overlaid
If something is too saturated, it is oversaturated
If something is too heard, it is overheard
If something is too simplified, it is oversimplified
If something is too charged, it is overcharged
If something is too grown, it is
2024-07-18 04:04:43 root INFO     [order_1_approx] starting weight calculation for If something is too charged, it is overcharged
If something is too played, it is overplayed
If something is too grown, it is overgrown
If something is too protective, it is overprotective
If something is too simplified, it is oversimplified
If something is too saturated, it is oversaturated
If something is too heard, it is overheard
If something is too laid, it is
2024-07-18 04:04:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 04:08:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1978,  0.3574, -0.0469,  ...,  1.4863, -0.0603, -0.0171],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.1562,  1.0459, -2.3555,  ..., -1.1797, -0.8701,  4.7812],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0007, -0.0289,  0.0021,  ...,  0.0105, -0.0004, -0.0060],
        [-0.0016,  0.0147,  0.0101,  ..., -0.0135, -0.0026,  0.0010],
        [ 0.0103,  0.0397,  0.0015,  ...,  0.0090, -0.0265,  0.0273],
        ...,
        [ 0.0088, -0.0209, -0.0124,  ...,  0.0051,  0.0018,  0.0123],
        [ 0.0029, -0.0108, -0.0056,  ..., -0.0210, -0.0142, -0.0079],
        [-0.0177, -0.0299,  0.0189,  ..., -0.0078,  0.0197, -0.0300]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.9766,  1.7949, -2.4453,  ..., -1.0566, -0.5410,  4.9062]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 04:08:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too charged, it is overcharged
If something is too played, it is overplayed
If something is too grown, it is overgrown
If something is too protective, it is overprotective
If something is too simplified, it is oversimplified
If something is too saturated, it is oversaturated
If something is too heard, it is overheard
If something is too laid, it is
2024-07-18 04:08:51 root INFO     total operator prediction time: 1978.1125531196594 seconds
2024-07-18 04:08:51 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on adj+ly_reg
2024-07-18 04:08:51 root INFO     building operator adj+ly_reg
2024-07-18 04:08:51 root INFO     [order_1_approx] starting weight calculation for The adjective form of apparent is apparently
The adjective form of decided is decidedly
The adjective form of typical is typically
The adjective form of historical is historically
The adjective form of different is differently
The adjective form of nice is nicely
The adjective form of virtual is virtually
The adjective form of international is
2024-07-18 04:08:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 04:12:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6191,  0.5674, -0.0343,  ..., -0.1738, -0.7510, -0.3403],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3257,  0.1213, -3.7031,  ...,  1.7656, -0.6611, -0.3105],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.3544e-02, -1.2054e-02,  1.8112e-02,  ...,  6.8207e-03,
         -1.2383e-02, -1.3832e-02],
        [-4.0054e-04, -2.7008e-03,  3.7079e-03,  ...,  7.4654e-03,
          9.2773e-03, -2.2919e-02],
        [-1.3237e-02, -4.5547e-03,  6.5765e-03,  ...,  9.1553e-05,
         -3.2043e-04,  1.6373e-02],
        ...,
        [ 2.7313e-03, -2.8839e-03,  5.0735e-03,  ..., -1.0956e-02,
         -5.1422e-03,  4.3564e-03],
        [ 5.1956e-03,  1.1864e-03,  9.4795e-04,  ..., -6.2866e-03,
         -1.9112e-03, -4.3068e-03],
        [-1.2550e-02,  6.7139e-03, -7.4387e-03,  ...,  1.1642e-02,
          8.7738e-03, -1.4709e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3950, -0.0297, -3.6680,  ...,  1.3984, -1.1836,  0.9521]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 04:12:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of apparent is apparently
The adjective form of decided is decidedly
The adjective form of typical is typically
The adjective form of historical is historically
The adjective form of different is differently
The adjective form of nice is nicely
The adjective form of virtual is virtually
The adjective form of international is
2024-07-18 04:12:57 root INFO     [order_1_approx] starting weight calculation for The adjective form of virtual is virtually
The adjective form of different is differently
The adjective form of apparent is apparently
The adjective form of decided is decidedly
The adjective form of international is internationally
The adjective form of historical is historically
The adjective form of typical is typically
The adjective form of nice is
2024-07-18 04:12:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 04:17:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7275, -0.6787, -0.3350,  ..., -0.5283, -0.3335,  0.5146],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8828,  0.8232, -2.6016,  ...,  3.6211,  2.2148, -0.5215],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 5.6419e-03, -7.2861e-03,  1.4557e-02,  ..., -2.1915e-03,
          3.5458e-03, -4.0398e-03],
        [ 6.7368e-03, -1.1810e-02, -5.6229e-03,  ...,  9.2316e-04,
          6.4316e-03, -9.3842e-04],
        [ 6.0844e-03,  4.5624e-03,  2.2507e-03,  ...,  1.5373e-03,
          9.1171e-03,  2.3987e-02],
        ...,
        [ 6.1798e-04, -2.1957e-02,  4.1962e-05,  ..., -3.9444e-03,
         -8.1100e-03,  1.3344e-02],
        [-1.0010e-02,  1.3962e-03, -2.3193e-03,  ...,  5.6992e-03,
         -7.6256e-03, -6.3629e-03],
        [ 5.7640e-03,  6.8054e-03,  2.5520e-03,  ...,  8.8043e-03,
          6.4468e-03, -1.3527e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7358,  1.1133, -2.3184,  ...,  3.0430,  2.8203, -0.4187]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 04:17:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of virtual is virtually
The adjective form of different is differently
The adjective form of apparent is apparently
The adjective form of decided is decidedly
The adjective form of international is internationally
The adjective form of historical is historically
The adjective form of typical is typically
The adjective form of nice is
2024-07-18 04:17:05 root INFO     [order_1_approx] starting weight calculation for The adjective form of historical is historically
The adjective form of different is differently
The adjective form of nice is nicely
The adjective form of international is internationally
The adjective form of apparent is apparently
The adjective form of typical is typically
The adjective form of virtual is virtually
The adjective form of decided is
2024-07-18 04:17:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 04:21:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1602,  0.2006, -1.4980,  ...,  0.6016, -0.7480, -0.1420],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.3125,  0.0439,  1.3477,  ..., -2.8496,  1.1992,  0.5098],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0013, -0.0185,  0.0033,  ..., -0.0200, -0.0303, -0.0040],
        [ 0.0033, -0.0034,  0.0015,  ..., -0.0053,  0.0026, -0.0120],
        [-0.0022, -0.0069,  0.0108,  ..., -0.0184, -0.0035,  0.0267],
        ...,
        [-0.0149, -0.0155, -0.0124,  ..., -0.0256, -0.0071,  0.0184],
        [-0.0054,  0.0110,  0.0034,  ...,  0.0041, -0.0017, -0.0254],
        [-0.0262,  0.0146,  0.0003,  ..., -0.0011, -0.0150, -0.0166]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.2227, -1.2725,  0.1514,  ..., -2.5664,  2.2012,  1.3086]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 04:21:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of historical is historically
The adjective form of different is differently
The adjective form of nice is nicely
The adjective form of international is internationally
The adjective form of apparent is apparently
The adjective form of typical is typically
The adjective form of virtual is virtually
The adjective form of decided is
2024-07-18 04:21:12 root INFO     [order_1_approx] starting weight calculation for The adjective form of decided is decidedly
The adjective form of international is internationally
The adjective form of typical is typically
The adjective form of different is differently
The adjective form of historical is historically
The adjective form of nice is nicely
The adjective form of virtual is virtually
The adjective form of apparent is
2024-07-18 04:21:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 04:25:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2585, -0.0637,  0.1088,  ...,  0.3330, -0.8716, -0.3289],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.6602,  2.3594, -0.5205,  ...,  2.3105, -2.9922, -2.4297],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0301, -0.0266, -0.0004,  ..., -0.0118, -0.0054, -0.0083],
        [-0.0063, -0.0116,  0.0020,  ...,  0.0007,  0.0008,  0.0065],
        [-0.0032,  0.0175, -0.0133,  ..., -0.0050, -0.0094,  0.0121],
        ...,
        [-0.0102, -0.0230,  0.0046,  ..., -0.0127, -0.0066,  0.0110],
        [ 0.0066,  0.0365, -0.0005,  ..., -0.0115, -0.0112, -0.0305],
        [-0.0030,  0.0219,  0.0143,  ...,  0.0041,  0.0074, -0.0572]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.9160,  2.5840, -1.2178,  ...,  1.6582, -3.7539, -2.7773]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 04:25:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of decided is decidedly
The adjective form of international is internationally
The adjective form of typical is typically
The adjective form of different is differently
The adjective form of historical is historically
The adjective form of nice is nicely
The adjective form of virtual is virtually
The adjective form of apparent is
2024-07-18 04:25:20 root INFO     [order_1_approx] starting weight calculation for The adjective form of international is internationally
The adjective form of different is differently
The adjective form of typical is typically
The adjective form of apparent is apparently
The adjective form of nice is nicely
The adjective form of virtual is virtually
The adjective form of decided is decidedly
The adjective form of historical is
2024-07-18 04:25:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 04:29:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5303,  0.8345,  0.3738,  ..., -0.6050, -1.0078, -0.7617],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0000,  0.9302, -2.0625,  ..., -0.9546,  2.7930,  4.1836],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0131, -0.0207,  0.0091,  ..., -0.0058, -0.0199, -0.0019],
        [-0.0071, -0.0140, -0.0071,  ...,  0.0008, -0.0089,  0.0040],
        [-0.0139, -0.0243, -0.0051,  ..., -0.0105,  0.0047,  0.0129],
        ...,
        [-0.0151,  0.0015, -0.0013,  ..., -0.0146,  0.0078, -0.0005],
        [-0.0162,  0.0013, -0.0042,  ...,  0.0009, -0.0098,  0.0045],
        [-0.0068, -0.0048,  0.0003,  ..., -0.0013, -0.0038, -0.0332]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.6396,  0.7373, -2.2891,  ..., -1.9766,  2.1738,  3.8008]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 04:29:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of international is internationally
The adjective form of different is differently
The adjective form of typical is typically
The adjective form of apparent is apparently
The adjective form of nice is nicely
The adjective form of virtual is virtually
The adjective form of decided is decidedly
The adjective form of historical is
2024-07-18 04:29:27 root INFO     [order_1_approx] starting weight calculation for The adjective form of decided is decidedly
The adjective form of different is differently
The adjective form of international is internationally
The adjective form of historical is historically
The adjective form of apparent is apparently
The adjective form of nice is nicely
The adjective form of virtual is virtually
The adjective form of typical is
2024-07-18 04:29:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 04:33:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8882,  0.7573, -0.7002,  ...,  0.2422, -0.3418,  0.3972],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0996,  1.8115,  1.9385,  ...,  4.2031, -0.1201,  0.6748],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0330, -0.0046, -0.0215,  ..., -0.0020, -0.0263, -0.0257],
        [-0.0084, -0.0168,  0.0127,  ..., -0.0159,  0.0059,  0.0161],
        [ 0.0031, -0.0030, -0.0040,  ...,  0.0008, -0.0251,  0.0296],
        ...,
        [ 0.0113, -0.0273,  0.0246,  ..., -0.0320,  0.0213,  0.0403],
        [-0.0018, -0.0060,  0.0123,  ..., -0.0187, -0.0042, -0.0061],
        [-0.0156,  0.0091,  0.0127,  ..., -0.0083,  0.0094, -0.0417]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9595,  1.6592,  2.7617,  ...,  4.2070, -0.5664,  1.1738]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 04:33:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of decided is decidedly
The adjective form of different is differently
The adjective form of international is internationally
The adjective form of historical is historically
The adjective form of apparent is apparently
The adjective form of nice is nicely
The adjective form of virtual is virtually
The adjective form of typical is
2024-07-18 04:33:33 root INFO     [order_1_approx] starting weight calculation for The adjective form of historical is historically
The adjective form of typical is typically
The adjective form of international is internationally
The adjective form of different is differently
The adjective form of decided is decidedly
The adjective form of apparent is apparently
The adjective form of nice is nicely
The adjective form of virtual is
2024-07-18 04:33:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 04:37:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2944, -0.2996, -0.1965,  ...,  0.6538, -0.2131, -0.4949],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6445,  2.5957,  0.8027,  ...,  2.9648,  0.1943, -2.6074],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0170, -0.0201,  0.0203,  ..., -0.0085, -0.0264, -0.0023],
        [-0.0087, -0.0025, -0.0049,  ..., -0.0020, -0.0168, -0.0038],
        [-0.0249, -0.0266, -0.0023,  ..., -0.0031, -0.0311, -0.0091],
        ...,
        [-0.0250, -0.0245,  0.0205,  ..., -0.0262,  0.0215,  0.0088],
        [ 0.0116,  0.0088, -0.0118,  ..., -0.0011,  0.0012, -0.0296],
        [-0.0214,  0.0297,  0.0048,  ...,  0.0025,  0.0055, -0.0245]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5645,  1.6660,  0.6299,  ...,  1.7578,  0.5430, -1.7402]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 04:37:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of historical is historically
The adjective form of typical is typically
The adjective form of international is internationally
The adjective form of different is differently
The adjective form of decided is decidedly
The adjective form of apparent is apparently
The adjective form of nice is nicely
The adjective form of virtual is
2024-07-18 04:37:39 root INFO     [order_1_approx] starting weight calculation for The adjective form of typical is typically
The adjective form of nice is nicely
The adjective form of historical is historically
The adjective form of apparent is apparently
The adjective form of decided is decidedly
The adjective form of international is internationally
The adjective form of virtual is virtually
The adjective form of different is
2024-07-18 04:37:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 04:41:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6890,  0.0793, -0.5205,  ...,  0.5645, -0.4509, -0.6631],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0615,  0.1255,  0.2954,  ..., -0.9287,  3.9043, -3.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0378, -0.0091,  0.0148,  ..., -0.0389, -0.0331, -0.0399],
        [-0.0053, -0.0155,  0.0013,  ...,  0.0007,  0.0012, -0.0292],
        [-0.0061,  0.0012, -0.0009,  ..., -0.0177, -0.0080, -0.0157],
        ...,
        [-0.0180, -0.0157, -0.0028,  ..., -0.0123,  0.0077,  0.0270],
        [ 0.0120, -0.0118,  0.0061,  ..., -0.0240, -0.0201,  0.0145],
        [-0.0047,  0.0011, -0.0238,  ..., -0.0070,  0.0019, -0.0591]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3174, -0.5635, -0.2109,  ..., -1.9141,  2.3047, -3.2227]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 04:41:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of typical is typically
The adjective form of nice is nicely
The adjective form of historical is historically
The adjective form of apparent is apparently
The adjective form of decided is decidedly
The adjective form of international is internationally
The adjective form of virtual is virtually
The adjective form of different is
2024-07-18 04:41:46 root INFO     total operator prediction time: 1975.4177920818329 seconds
2024-07-18 04:41:46 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+tion_irreg
2024-07-18 04:41:46 root INFO     building operator verb+tion_irreg
2024-07-18 04:41:46 root INFO     [order_1_approx] starting weight calculation for To compile results in compilation
To stabilize results in stabilization
To characterize results in characterization
To imagine results in imagination
To determine results in determination
To continue results in continuation
To degrade results in degradation
To randomize results in
2024-07-18 04:41:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 04:45:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5244, -0.2576, -0.6479,  ..., -0.3174, -0.9463,  0.5288],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.2227,  3.0566, -1.8213,  ...,  2.2695,  4.6523,  6.9844],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0020, -0.0046,  0.0061,  ...,  0.0040,  0.0026, -0.0004],
        [ 0.0004, -0.0069,  0.0009,  ...,  0.0217, -0.0138,  0.0093],
        [ 0.0137, -0.0036,  0.0004,  ..., -0.0019,  0.0126,  0.0041],
        ...,
        [-0.0122, -0.0145,  0.0057,  ...,  0.0079, -0.0075,  0.0162],
        [-0.0007,  0.0004,  0.0018,  ...,  0.0097,  0.0054,  0.0048],
        [ 0.0033,  0.0103,  0.0024,  ...,  0.0055,  0.0157,  0.0061]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6914,  2.5156, -1.8926,  ...,  2.3496,  4.8125,  6.5469]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 04:45:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To compile results in compilation
To stabilize results in stabilization
To characterize results in characterization
To imagine results in imagination
To determine results in determination
To continue results in continuation
To degrade results in degradation
To randomize results in
2024-07-18 04:45:52 root INFO     [order_1_approx] starting weight calculation for To randomize results in randomization
To degrade results in degradation
To stabilize results in stabilization
To characterize results in characterization
To imagine results in imagination
To continue results in continuation
To compile results in compilation
To determine results in
2024-07-18 04:45:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 04:49:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4783,  0.1721, -1.0938,  ...,  0.4607, -0.4204,  0.2632],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.9648, -1.2822, -3.0820,  ..., -4.0273,  4.5312,  5.6680],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0222, -0.0122, -0.0104,  ...,  0.0047,  0.0143, -0.0070],
        [-0.0087, -0.0031,  0.0010,  ...,  0.0119, -0.0072, -0.0259],
        [ 0.0079,  0.0076, -0.0088,  ...,  0.0073,  0.0006, -0.0013],
        ...,
        [-0.0227, -0.0125,  0.0045,  ..., -0.0049, -0.0024, -0.0049],
        [-0.0002, -0.0046,  0.0176,  ..., -0.0195, -0.0089,  0.0076],
        [-0.0112,  0.0026,  0.0088,  ...,  0.0156,  0.0135, -0.0177]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.5664, -2.6367, -2.5859,  ..., -4.8945,  4.9453,  6.2539]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 04:49:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To randomize results in randomization
To degrade results in degradation
To stabilize results in stabilization
To characterize results in characterization
To imagine results in imagination
To continue results in continuation
To compile results in compilation
To determine results in
2024-07-18 04:49:57 root INFO     [order_1_approx] starting weight calculation for To degrade results in degradation
To compile results in compilation
To randomize results in randomization
To determine results in determination
To characterize results in characterization
To imagine results in imagination
To continue results in continuation
To stabilize results in
2024-07-18 04:49:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 04:54:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4314, -0.1650,  0.3906,  ...,  0.2515, -0.6533, -0.0188],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([2.5625, 1.8105, 0.2188,  ..., 0.6001, 0.3164, 7.1367], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0030, -0.0048, -0.0136,  ...,  0.0153,  0.0162, -0.0220],
        [-0.0054, -0.0089, -0.0104,  ..., -0.0019, -0.0142, -0.0101],
        [ 0.0061,  0.0054, -0.0050,  ...,  0.0082,  0.0026, -0.0104],
        ...,
        [-0.0236, -0.0128, -0.0138,  ..., -0.0036, -0.0079, -0.0142],
        [-0.0035, -0.0136,  0.0115,  ..., -0.0176, -0.0125,  0.0099],
        [-0.0070,  0.0242, -0.0095,  ..., -0.0094,  0.0041, -0.0060]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0703,  1.0020,  0.1613,  ...,  0.2156, -0.2842,  7.0117]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 04:54:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To degrade results in degradation
To compile results in compilation
To randomize results in randomization
To determine results in determination
To characterize results in characterization
To imagine results in imagination
To continue results in continuation
To stabilize results in
2024-07-18 04:54:03 root INFO     [order_1_approx] starting weight calculation for To determine results in determination
To compile results in compilation
To stabilize results in stabilization
To imagine results in imagination
To degrade results in degradation
To characterize results in characterization
To randomize results in randomization
To continue results in
2024-07-18 04:54:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 04:58:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1250,  0.0320, -0.1484,  ...,  0.0898, -0.1809,  0.1240],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5010, -3.0137, -3.2461,  ...,  1.2197, -1.0469,  1.6572],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0081, -0.0065,  0.0059,  ...,  0.0014,  0.0104, -0.0240],
        [-0.0178, -0.0028, -0.0162,  ...,  0.0026, -0.0035,  0.0012],
        [ 0.0084, -0.0207, -0.0050,  ..., -0.0100,  0.0066, -0.0029],
        ...,
        [-0.0172, -0.0123, -0.0062,  ..., -0.0087, -0.0004,  0.0084],
        [-0.0099,  0.0010,  0.0097,  ...,  0.0090, -0.0223,  0.0027],
        [ 0.0013,  0.0147,  0.0125,  ...,  0.0096,  0.0126, -0.0178]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0469, -3.3301, -2.4492,  ...,  0.7920, -0.9072,  2.8379]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 04:58:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To determine results in determination
To compile results in compilation
To stabilize results in stabilization
To imagine results in imagination
To degrade results in degradation
To characterize results in characterization
To randomize results in randomization
To continue results in
2024-07-18 04:58:08 root INFO     [order_1_approx] starting weight calculation for To degrade results in degradation
To randomize results in randomization
To continue results in continuation
To characterize results in characterization
To stabilize results in stabilization
To determine results in determination
To imagine results in imagination
To compile results in
2024-07-18 04:58:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 05:02:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3640,  0.0819, -0.7363,  ...,  0.7300,  0.2725,  0.2004],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9551,  0.2422, -6.3789,  ...,  0.2515, -1.3525,  2.8125],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0015, -0.0183,  0.0003,  ..., -0.0114,  0.0128, -0.0151],
        [-0.0082, -0.0029, -0.0024,  ...,  0.0137, -0.0162, -0.0074],
        [ 0.0098,  0.0058,  0.0076,  ..., -0.0219,  0.0057, -0.0071],
        ...,
        [-0.0113, -0.0067, -0.0048,  ...,  0.0213,  0.0039,  0.0117],
        [ 0.0060,  0.0135,  0.0159,  ...,  0.0003,  0.0099,  0.0065],
        [ 0.0099,  0.0014,  0.0031,  ..., -0.0058,  0.0223, -0.0109]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.1777e-01,  7.7734e-01, -5.9453e+00,  ...,  4.6387e-03,
         -2.4121e+00,  2.9688e+00]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SubBackward0>) 

                    
2024-07-18 05:02:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To degrade results in degradation
To randomize results in randomization
To continue results in continuation
To characterize results in characterization
To stabilize results in stabilization
To determine results in determination
To imagine results in imagination
To compile results in
2024-07-18 05:02:13 root INFO     [order_1_approx] starting weight calculation for To stabilize results in stabilization
To degrade results in degradation
To compile results in compilation
To continue results in continuation
To determine results in determination
To randomize results in randomization
To imagine results in imagination
To characterize results in
2024-07-18 05:02:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 05:06:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2852, -0.0122, -0.3645,  ..., -0.4395, -0.8271,  0.0363],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7202,  0.5908, -3.6934,  ..., -0.0908, -1.1543,  2.5664],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-6.5384e-03, -3.0670e-03, -1.2970e-02,  ..., -3.9482e-04,
          1.3252e-02, -9.6283e-03],
        [-1.2932e-02,  1.7075e-02, -4.7684e-04,  ...,  4.3411e-03,
         -2.5139e-03, -1.0384e-02],
        [ 2.9373e-03, -1.7700e-03,  2.1744e-04,  ...,  1.5497e-03,
         -3.5820e-03, -1.0971e-02],
        ...,
        [-3.6041e-02,  6.7062e-03, -9.8944e-06,  ..., -5.9242e-03,
         -1.0513e-02, -2.5558e-03],
        [-3.4828e-03, -1.0277e-02,  1.1578e-03,  ..., -5.8632e-03,
          8.1024e-03,  1.1612e-02],
        [ 3.9711e-03,  3.2837e-02, -9.9182e-03,  ...,  6.0234e-03,
          3.0441e-03, -1.1940e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6377,  0.6304, -4.1641,  ..., -0.5879, -1.8066,  2.8516]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 05:06:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To stabilize results in stabilization
To degrade results in degradation
To compile results in compilation
To continue results in continuation
To determine results in determination
To randomize results in randomization
To imagine results in imagination
To characterize results in
2024-07-18 05:06:18 root INFO     [order_1_approx] starting weight calculation for To stabilize results in stabilization
To degrade results in degradation
To randomize results in randomization
To compile results in compilation
To continue results in continuation
To determine results in determination
To characterize results in characterization
To imagine results in
2024-07-18 05:06:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 05:10:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3630, -0.6553, -0.6099,  ...,  0.6738, -0.2656, -0.2891],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2656,  1.9053, -4.3594,  ...,  1.8662, -3.5645,  0.3281],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0126, -0.0208, -0.0068,  ...,  0.0188, -0.0020, -0.0051],
        [ 0.0049, -0.0012,  0.0112,  ..., -0.0089, -0.0095, -0.0237],
        [ 0.0064,  0.0137,  0.0089,  ...,  0.0200, -0.0148,  0.0087],
        ...,
        [-0.0135, -0.0115,  0.0037,  ...,  0.0087, -0.0049, -0.0005],
        [ 0.0070, -0.0004, -0.0073,  ..., -0.0171, -0.0050,  0.0095],
        [ 0.0080,  0.0382, -0.0064,  ...,  0.0061, -0.0046,  0.0108]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2432,  1.6660, -4.4648,  ...,  1.3535, -3.6250,  1.4463]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 05:10:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To stabilize results in stabilization
To degrade results in degradation
To randomize results in randomization
To compile results in compilation
To continue results in continuation
To determine results in determination
To characterize results in characterization
To imagine results in
2024-07-18 05:10:25 root INFO     [order_1_approx] starting weight calculation for To imagine results in imagination
To determine results in determination
To compile results in compilation
To randomize results in randomization
To characterize results in characterization
To continue results in continuation
To stabilize results in stabilization
To degrade results in
2024-07-18 05:10:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 05:14:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0626, -0.6797,  0.1462,  ...,  0.1145, -0.5605, -0.1827],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.3535,  1.0762, -0.5605,  ..., -1.6191, -0.8359,  2.7129],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0088,  0.0010, -0.0124,  ...,  0.0033, -0.0112,  0.0153],
        [-0.0032, -0.0069, -0.0052,  ...,  0.0111, -0.0024,  0.0086],
        [-0.0021, -0.0049, -0.0091,  ...,  0.0020, -0.0068,  0.0109],
        ...,
        [-0.0112, -0.0163,  0.0104,  ...,  0.0126, -0.0209,  0.0036],
        [ 0.0040,  0.0202, -0.0104,  ..., -0.0110, -0.0078,  0.0130],
        [-0.0186,  0.0235, -0.0042,  ..., -0.0057,  0.0123, -0.0182]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.8926,  1.1807, -0.6069,  ..., -2.3340, -1.8672,  3.1582]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 05:14:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To imagine results in imagination
To determine results in determination
To compile results in compilation
To randomize results in randomization
To characterize results in characterization
To continue results in continuation
To stabilize results in stabilization
To degrade results in
2024-07-18 05:14:31 root INFO     total operator prediction time: 1965.2934637069702 seconds
2024-07-18 05:14:31 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+able_reg
2024-07-18 05:14:31 root INFO     building operator verb+able_reg
2024-07-18 05:14:32 root INFO     [order_1_approx] starting weight calculation for If you can sustain something, that thing is sustainable
If you can contain something, that thing is containable
If you can vary something, that thing is variable
If you can extend something, that thing is extendable
If you can explain something, that thing is explainable
If you can prefer something, that thing is preferable
If you can advise something, that thing is advisable
If you can protect something, that thing is
2024-07-18 05:14:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 05:18:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3613,  0.6865,  0.0568,  ...,  0.7178, -0.3501, -0.0679],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6367,  3.0391, -2.6445,  ..., -1.5566,  0.7612,  3.0508],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0059, -0.0017,  0.0089,  ...,  0.0127, -0.0043,  0.0066],
        [-0.0029, -0.0174, -0.0103,  ...,  0.0027, -0.0085,  0.0142],
        [ 0.0122,  0.0076, -0.0004,  ...,  0.0093,  0.0050,  0.0030],
        ...,
        [-0.0180, -0.0112, -0.0176,  ..., -0.0125, -0.0002, -0.0047],
        [ 0.0057, -0.0011,  0.0053,  ..., -0.0260, -0.0127,  0.0130],
        [-0.0085, -0.0091,  0.0067,  ..., -0.0130,  0.0145, -0.0045]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6797,  3.6484, -2.9551,  ..., -2.1055,  0.1128,  2.2910]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 05:18:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can sustain something, that thing is sustainable
If you can contain something, that thing is containable
If you can vary something, that thing is variable
If you can extend something, that thing is extendable
If you can explain something, that thing is explainable
If you can prefer something, that thing is preferable
If you can advise something, that thing is advisable
If you can protect something, that thing is
2024-07-18 05:18:39 root INFO     [order_1_approx] starting weight calculation for If you can extend something, that thing is extendable
If you can explain something, that thing is explainable
If you can protect something, that thing is protectable
If you can sustain something, that thing is sustainable
If you can contain something, that thing is containable
If you can vary something, that thing is variable
If you can prefer something, that thing is preferable
If you can advise something, that thing is
2024-07-18 05:18:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 05:22:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3005,  1.1885, -0.6797,  ...,  0.4167, -0.3081,  0.5503],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.9355,  5.0156, -1.7168,  ..., -1.6885,  2.2969,  2.0098],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0205, -0.0107,  0.0091,  ...,  0.0207,  0.0015,  0.0039],
        [-0.0158, -0.0097, -0.0075,  ..., -0.0113,  0.0009, -0.0076],
        [-0.0016,  0.0041, -0.0110,  ...,  0.0117,  0.0105,  0.0082],
        ...,
        [-0.0038, -0.0229, -0.0063,  ..., -0.0155,  0.0015,  0.0066],
        [-0.0005,  0.0064,  0.0012,  ..., -0.0307, -0.0303,  0.0014],
        [-0.0129, -0.0177,  0.0061,  ..., -0.0125,  0.0168, -0.0228]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9888,  4.6562, -1.9531,  ..., -0.9067,  1.7617,  1.9453]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 05:22:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can extend something, that thing is extendable
If you can explain something, that thing is explainable
If you can protect something, that thing is protectable
If you can sustain something, that thing is sustainable
If you can contain something, that thing is containable
If you can vary something, that thing is variable
If you can prefer something, that thing is preferable
If you can advise something, that thing is
2024-07-18 05:22:46 root INFO     [order_1_approx] starting weight calculation for If you can extend something, that thing is extendable
If you can vary something, that thing is variable
If you can contain something, that thing is containable
If you can prefer something, that thing is preferable
If you can protect something, that thing is protectable
If you can advise something, that thing is advisable
If you can sustain something, that thing is sustainable
If you can explain something, that thing is
2024-07-18 05:22:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 05:26:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7715,  0.7305,  0.8623,  ..., -0.2234, -0.6304, -0.3022],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.8633,  0.6519, -3.5977,  ..., -0.5933,  2.8184, -1.6953],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0046, -0.0050,  0.0158,  ...,  0.0190, -0.0183,  0.0046],
        [ 0.0070, -0.0169,  0.0050,  ..., -0.0157,  0.0050, -0.0015],
        [-0.0051, -0.0146,  0.0003,  ..., -0.0008,  0.0135,  0.0021],
        ...,
        [-0.0035,  0.0101,  0.0023,  ..., -0.0120, -0.0025, -0.0035],
        [ 0.0121,  0.0163,  0.0019,  ..., -0.0198, -0.0062,  0.0152],
        [ 0.0004, -0.0003,  0.0065,  ..., -0.0079,  0.0165, -0.0229]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.0703,  1.1348, -3.9316,  ..., -0.7432,  1.0361, -1.8154]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 05:26:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can extend something, that thing is extendable
If you can vary something, that thing is variable
If you can contain something, that thing is containable
If you can prefer something, that thing is preferable
If you can protect something, that thing is protectable
If you can advise something, that thing is advisable
If you can sustain something, that thing is sustainable
If you can explain something, that thing is
2024-07-18 05:26:53 root INFO     [order_1_approx] starting weight calculation for If you can prefer something, that thing is preferable
If you can sustain something, that thing is sustainable
If you can explain something, that thing is explainable
If you can vary something, that thing is variable
If you can advise something, that thing is advisable
If you can extend something, that thing is extendable
If you can protect something, that thing is protectable
If you can contain something, that thing is
2024-07-18 05:26:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 05:31:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0403,  0.3411, -0.3774,  ..., -0.9717, -0.7344,  0.9512],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5605,  3.2539, -3.1094,  ..., -0.2363,  1.4053,  3.2461],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0036,  0.0178,  0.0156,  ...,  0.0108, -0.0082,  0.0016],
        [-0.0162, -0.0148, -0.0006,  ..., -0.0031,  0.0070,  0.0072],
        [ 0.0031, -0.0041, -0.0170,  ...,  0.0056,  0.0227,  0.0046],
        ...,
        [-0.0118,  0.0057, -0.0106,  ..., -0.0049,  0.0056,  0.0019],
        [ 0.0053,  0.0030,  0.0094,  ..., -0.0383, -0.0098,  0.0070],
        [-0.0180, -0.0231, -0.0168,  ..., -0.0079,  0.0238, -0.0320]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0771,  3.6758, -3.6836,  ...,  0.1006,  0.0488,  3.0781]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 05:31:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can prefer something, that thing is preferable
If you can sustain something, that thing is sustainable
If you can explain something, that thing is explainable
If you can vary something, that thing is variable
If you can advise something, that thing is advisable
If you can extend something, that thing is extendable
If you can protect something, that thing is protectable
If you can contain something, that thing is
2024-07-18 05:31:01 root INFO     [order_1_approx] starting weight calculation for If you can advise something, that thing is advisable
If you can prefer something, that thing is preferable
If you can sustain something, that thing is sustainable
If you can protect something, that thing is protectable
If you can explain something, that thing is explainable
If you can contain something, that thing is containable
If you can extend something, that thing is extendable
If you can vary something, that thing is
2024-07-18 05:31:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 05:35:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1483,  0.2395,  0.1030,  ..., -0.6465, -0.4253,  0.4946],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0996,  3.1836, -3.3457,  ...,  0.6895,  6.1641, -0.8354],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.0269e-02,  7.2823e-03, -1.3428e-03,  ...,  2.0046e-03,
         -3.5906e-04,  4.2763e-03],
        [-3.1464e-02, -5.7144e-03, -4.3449e-03,  ..., -8.0109e-05,
         -8.5983e-03, -3.5610e-03],
        [ 1.0483e-02,  1.2146e-02, -1.9608e-02,  ...,  4.4136e-03,
         -1.2207e-02, -2.0695e-03],
        ...,
        [-5.4321e-03,  7.9346e-03,  2.2263e-02,  ..., -1.9363e-02,
         -2.9221e-03, -9.1553e-03],
        [-5.9052e-03,  8.1863e-03,  9.4910e-03,  ..., -3.2043e-02,
         -1.9302e-02,  3.5744e-03],
        [-1.2146e-02, -4.2000e-03, -7.0992e-03,  ..., -1.1726e-02,
         -1.4200e-03, -1.4717e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9590,  4.6367, -2.8887,  ...,  0.4016,  5.1992, -1.5547]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 05:35:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can advise something, that thing is advisable
If you can prefer something, that thing is preferable
If you can sustain something, that thing is sustainable
If you can protect something, that thing is protectable
If you can explain something, that thing is explainable
If you can contain something, that thing is containable
If you can extend something, that thing is extendable
If you can vary something, that thing is
2024-07-18 05:35:08 root INFO     [order_1_approx] starting weight calculation for If you can contain something, that thing is containable
If you can protect something, that thing is protectable
If you can prefer something, that thing is preferable
If you can explain something, that thing is explainable
If you can advise something, that thing is advisable
If you can extend something, that thing is extendable
If you can vary something, that thing is variable
If you can sustain something, that thing is
2024-07-18 05:35:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 05:39:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2322,  0.8672,  0.4631,  ...,  0.1631,  0.0555, -0.3311],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.2949,  1.7900, -2.8164,  ...,  0.4934,  0.2739,  4.4102],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.2610e-03,  9.4604e-03,  3.3703e-03,  ...,  2.0386e-02,
         -8.8501e-04, -8.3618e-03],
        [ 2.3956e-03, -6.9656e-03, -1.0986e-03,  ..., -2.8549e-02,
          1.6388e-02,  2.8152e-03],
        [ 8.0490e-03,  6.9618e-03, -1.0704e-02,  ..., -9.3994e-03,
          1.9165e-02, -1.1642e-02],
        ...,
        [-1.6846e-02, -1.8890e-02, -8.2588e-04,  ..., -4.0283e-03,
         -1.7071e-03, -3.0518e-05],
        [ 1.0223e-02, -1.9714e-02, -6.8817e-03,  ..., -9.2392e-03,
         -9.9487e-03,  7.1907e-03],
        [-1.7731e-02, -1.2955e-02, -3.9978e-03,  ..., -1.7273e-02,
          2.3010e-02, -1.4549e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.0430,  1.4336, -2.5996,  ..., -0.1980,  0.3965,  4.7109]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 05:39:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can contain something, that thing is containable
If you can protect something, that thing is protectable
If you can prefer something, that thing is preferable
If you can explain something, that thing is explainable
If you can advise something, that thing is advisable
If you can extend something, that thing is extendable
If you can vary something, that thing is variable
If you can sustain something, that thing is
2024-07-18 05:39:15 root INFO     [order_1_approx] starting weight calculation for If you can prefer something, that thing is preferable
If you can explain something, that thing is explainable
If you can sustain something, that thing is sustainable
If you can advise something, that thing is advisable
If you can vary something, that thing is variable
If you can protect something, that thing is protectable
If you can contain something, that thing is containable
If you can extend something, that thing is
2024-07-18 05:39:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 05:43:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0971,  0.0441,  0.4946,  ..., -0.1437, -0.4502,  0.3857],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.0898,  0.3955, -3.1387,  ..., -0.4761, -0.3574,  3.1523],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.0738e-03,  2.2888e-05, -1.2184e-02,  ...,  1.5640e-04,
          1.8730e-03,  1.2489e-02],
        [-1.2787e-02,  9.6664e-03,  3.2616e-03,  ..., -1.6068e-02,
          6.0196e-03,  5.6992e-03],
        [ 9.2010e-03,  3.4828e-03, -3.1185e-03,  ...,  3.7613e-03,
         -4.6921e-04, -6.4049e-03],
        ...,
        [-1.9684e-03, -1.6068e-02,  5.8365e-04,  ...,  2.1725e-03,
         -9.4910e-03,  1.3626e-02],
        [ 3.2196e-03, -1.2550e-03,  6.9885e-03,  ..., -1.3824e-02,
         -2.7237e-03,  1.8036e-02],
        [-2.8133e-03, -8.3313e-03, -6.6376e-03,  ...,  3.1662e-04,
          9.3384e-03, -1.2054e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.7891,  0.6353, -3.3672,  ..., -0.4802, -0.8350,  2.6406]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 05:43:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can prefer something, that thing is preferable
If you can explain something, that thing is explainable
If you can sustain something, that thing is sustainable
If you can advise something, that thing is advisable
If you can vary something, that thing is variable
If you can protect something, that thing is protectable
If you can contain something, that thing is containable
If you can extend something, that thing is
2024-07-18 05:43:24 root INFO     [order_1_approx] starting weight calculation for If you can vary something, that thing is variable
If you can sustain something, that thing is sustainable
If you can protect something, that thing is protectable
If you can extend something, that thing is extendable
If you can contain something, that thing is containable
If you can advise something, that thing is advisable
If you can explain something, that thing is explainable
If you can prefer something, that thing is
2024-07-18 05:43:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 05:47:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2688,  0.4360, -0.6992,  ..., -0.1333,  0.6509,  0.3755],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1885,  2.8477,  0.4194,  ...,  1.1943,  0.7280, -0.6680],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0205, -0.0083,  0.0190,  ...,  0.0156,  0.0025, -0.0134],
        [-0.0117, -0.0119,  0.0056,  ...,  0.0060, -0.0175, -0.0150],
        [ 0.0007,  0.0131,  0.0118,  ...,  0.0034, -0.0078, -0.0071],
        ...,
        [-0.0114, -0.0175,  0.0043,  ..., -0.0221, -0.0150, -0.0137],
        [ 0.0071,  0.0015, -0.0022,  ..., -0.0196, -0.0279,  0.0087],
        [-0.0142, -0.0063,  0.0072,  ...,  0.0145,  0.0263, -0.0269]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2178,  2.5137,  0.5059,  ...,  1.0625,  0.4795, -0.5659]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 05:47:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can vary something, that thing is variable
If you can sustain something, that thing is sustainable
If you can protect something, that thing is protectable
If you can extend something, that thing is extendable
If you can contain something, that thing is containable
If you can advise something, that thing is advisable
If you can explain something, that thing is explainable
If you can prefer something, that thing is
2024-07-18 05:47:30 root INFO     total operator prediction time: 1979.0723190307617 seconds
2024-07-18 05:47:30 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on un+adj_reg
2024-07-18 05:47:30 root INFO     building operator un+adj_reg
2024-07-18 05:47:31 root INFO     [order_1_approx] starting weight calculation for The opposite of employed is unemployed
The opposite of biased is unbiased
The opposite of published is unpublished
The opposite of predictable is unpredictable
The opposite of sustainable is unsustainable
The opposite of available is unavailable
The opposite of related is unrelated
The opposite of acceptable is
2024-07-18 05:47:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 05:51:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5386, -0.3130,  0.2632,  ...,  0.1040, -0.3594, -0.2744],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6133, -2.6328,  0.1953,  ...,  3.2832,  4.0742, -0.7886],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.2697e-04,  7.1945e-03, -3.8147e-05,  ..., -1.9760e-02,
         -9.7466e-04,  2.7390e-03],
        [-1.5976e-02,  1.3847e-03, -1.3580e-02,  ..., -7.9956e-03,
          1.0338e-02, -1.3710e-02],
        [ 6.7902e-03,  1.1139e-03, -2.3193e-02,  ..., -1.0315e-02,
         -1.5274e-02,  6.2256e-03],
        ...,
        [-2.0447e-02, -1.6235e-02, -2.1179e-02,  ...,  1.3733e-03,
          3.7060e-03,  2.9259e-03],
        [-1.9043e-02,  5.7526e-03,  6.0547e-02,  ...,  4.7340e-03,
         -3.1555e-02, -5.4245e-03],
        [-4.9133e-03,  1.7532e-02, -1.7166e-03,  ..., -1.9669e-02,
         -5.0125e-03, -1.8631e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3340, -1.6035,  0.0958,  ...,  1.1953,  4.6680, -0.4871]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 05:51:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of employed is unemployed
The opposite of biased is unbiased
The opposite of published is unpublished
The opposite of predictable is unpredictable
The opposite of sustainable is unsustainable
The opposite of available is unavailable
The opposite of related is unrelated
The opposite of acceptable is
2024-07-18 05:51:39 root INFO     [order_1_approx] starting weight calculation for The opposite of acceptable is unacceptable
The opposite of biased is unbiased
The opposite of published is unpublished
The opposite of employed is unemployed
The opposite of related is unrelated
The opposite of predictable is unpredictable
The opposite of available is unavailable
The opposite of sustainable is
2024-07-18 05:51:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 05:55:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8672,  0.1329,  0.2515,  ...,  0.0757,  0.0632, -0.6899],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.4336, -1.3262, -1.8818,  ...,  1.9277,  0.7183,  3.3008],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.3163e-02,  1.5182e-03,  5.7602e-03,  ...,  1.6174e-02,
         -1.4145e-02, -8.4686e-03],
        [-2.3178e-02, -1.2138e-02,  2.8198e-02,  ...,  1.0620e-02,
          9.4986e-04,  4.7302e-04],
        [-1.1993e-02,  1.3428e-02, -1.5045e-02,  ..., -7.6294e-06,
         -2.1652e-02, -8.8882e-03],
        ...,
        [ 1.4160e-02, -1.1749e-02,  5.5466e-03,  ..., -4.3579e-02,
         -5.1689e-03,  3.4943e-03],
        [-3.4637e-03, -1.0353e-02,  2.5940e-02,  ..., -1.9684e-03,
         -1.5671e-02, -5.2757e-03],
        [-1.3084e-03,  4.8599e-03,  6.4316e-03,  ..., -4.9019e-03,
         -4.0321e-03, -4.1565e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.8008, -0.1494, -3.1211,  ...,  2.5527,  0.8379,  4.3594]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 05:55:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of acceptable is unacceptable
The opposite of biased is unbiased
The opposite of published is unpublished
The opposite of employed is unemployed
The opposite of related is unrelated
The opposite of predictable is unpredictable
The opposite of available is unavailable
The opposite of sustainable is
2024-07-18 05:55:46 root INFO     [order_1_approx] starting weight calculation for The opposite of employed is unemployed
The opposite of acceptable is unacceptable
The opposite of biased is unbiased
The opposite of related is unrelated
The opposite of sustainable is unsustainable
The opposite of published is unpublished
The opposite of predictable is unpredictable
The opposite of available is
2024-07-18 05:55:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 05:59:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2744, -0.0488,  0.4360,  ...,  0.5156, -0.4622,  0.0576],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.4980, -1.0205,  3.2734,  ...,  2.3555,  3.7598, -2.0156],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-9.0485e-03, -1.6891e-02,  3.0022e-03,  ...,  2.7603e-02,
         -1.8661e-02,  3.0518e-05],
        [ 1.0010e-02, -6.3438e-03,  7.0038e-03,  ..., -1.4359e-02,
         -2.0645e-02, -1.2161e-02],
        [ 1.2856e-03,  2.0508e-02, -2.6352e-02,  ..., -1.3443e-02,
          5.1804e-03,  1.4133e-03],
        ...,
        [-2.6962e-02,  1.4038e-03, -4.2908e-02,  ..., -3.8269e-02,
          2.7588e-02, -8.8806e-03],
        [ 1.7456e-02, -1.6968e-02, -9.5291e-03,  ..., -7.9651e-03,
          3.0556e-03, -2.8748e-02],
        [ 3.8071e-03,  1.5915e-02, -6.9923e-03,  ..., -2.2919e-02,
          5.5847e-03, -3.0182e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.5547, -0.8350,  1.9727,  ...,  0.8730,  4.6211, -2.0879]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 05:59:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of employed is unemployed
The opposite of acceptable is unacceptable
The opposite of biased is unbiased
The opposite of related is unrelated
The opposite of sustainable is unsustainable
The opposite of published is unpublished
The opposite of predictable is unpredictable
The opposite of available is
2024-07-18 05:59:52 root INFO     [order_1_approx] starting weight calculation for The opposite of acceptable is unacceptable
The opposite of biased is unbiased
The opposite of related is unrelated
The opposite of predictable is unpredictable
The opposite of published is unpublished
The opposite of available is unavailable
The opposite of sustainable is unsustainable
The opposite of employed is
2024-07-18 05:59:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 06:03:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0869,  0.8740,  0.0623,  ...,  0.5967, -0.9854, -0.4053],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6035, -0.1504, -2.8301,  ...,  2.3457,  0.2529, -1.7441],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0151,  0.0090, -0.0079,  ...,  0.0163,  0.0010, -0.0125],
        [-0.0128, -0.0288, -0.0234,  ..., -0.0017, -0.0088,  0.0046],
        [ 0.0144, -0.0259, -0.0021,  ..., -0.0188, -0.0241, -0.0139],
        ...,
        [-0.0046, -0.0191,  0.0025,  ..., -0.0050, -0.0114, -0.0072],
        [ 0.0136, -0.0170,  0.0240,  ...,  0.0030, -0.0259, -0.0033],
        [-0.0042,  0.0363, -0.0085,  ..., -0.0138,  0.0045, -0.0176]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2109, -0.4187, -3.2383,  ...,  3.0879,  1.2422, -0.7998]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 06:03:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of acceptable is unacceptable
The opposite of biased is unbiased
The opposite of related is unrelated
The opposite of predictable is unpredictable
The opposite of published is unpublished
The opposite of available is unavailable
The opposite of sustainable is unsustainable
The opposite of employed is
2024-07-18 06:03:59 root INFO     [order_1_approx] starting weight calculation for The opposite of predictable is unpredictable
The opposite of available is unavailable
The opposite of acceptable is unacceptable
The opposite of sustainable is unsustainable
The opposite of related is unrelated
The opposite of employed is unemployed
The opposite of published is unpublished
The opposite of biased is
2024-07-18 06:03:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 06:08:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4568, -0.1162, -1.1641,  ...,  0.1836, -0.2930, -0.2101],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([2.4238, 3.6211, 1.4453,  ..., 0.0645, 1.1270, 2.6250], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-3.5797e-02, -2.2873e-02, -7.6942e-03,  ...,  3.1395e-03,
         -6.0196e-03, -1.0574e-02],
        [-2.0599e-02, -3.8086e-02, -1.0490e-04,  ...,  6.8665e-04,
         -4.5532e-02,  4.9515e-03],
        [ 2.1019e-03,  2.2064e-02, -1.3245e-02,  ..., -1.8539e-02,
          4.3602e-03,  1.1459e-02],
        ...,
        [ 4.3716e-03,  5.2223e-03,  2.0935e-02,  ..., -4.2114e-03,
         -1.6800e-02, -7.6294e-06],
        [-1.6464e-02, -3.7422e-03,  6.6757e-03,  ..., -1.3855e-02,
         -1.8478e-02, -2.2400e-02],
        [ 1.8860e-02, -1.6556e-03, -2.0691e-02,  ..., -1.4725e-02,
          1.7273e-02, -7.1831e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9033,  2.2383,  1.9248,  ..., -0.1895,  0.0488,  3.6875]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 06:08:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of predictable is unpredictable
The opposite of available is unavailable
The opposite of acceptable is unacceptable
The opposite of sustainable is unsustainable
The opposite of related is unrelated
The opposite of employed is unemployed
The opposite of published is unpublished
The opposite of biased is
2024-07-18 06:08:08 root INFO     [order_1_approx] starting weight calculation for The opposite of biased is unbiased
The opposite of available is unavailable
The opposite of published is unpublished
The opposite of acceptable is unacceptable
The opposite of sustainable is unsustainable
The opposite of predictable is unpredictable
The opposite of employed is unemployed
The opposite of related is
2024-07-18 06:08:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 06:12:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5693,  0.6782, -0.5332,  ...,  0.1943, -0.8770, -0.3911],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.0840, -3.1562,  0.2871,  ..., -0.3262,  4.0898, -0.6509],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0086,  0.0099, -0.0006,  ...,  0.0122, -0.0092, -0.0102],
        [ 0.0077, -0.0165, -0.0278,  ...,  0.0173, -0.0049, -0.0211],
        [ 0.0302, -0.0153, -0.0257,  ..., -0.0194, -0.0144,  0.0086],
        ...,
        [-0.0142, -0.0154, -0.0059,  ..., -0.0144,  0.0097,  0.0197],
        [-0.0281,  0.0020,  0.0297,  ...,  0.0073, -0.0224,  0.0080],
        [ 0.0103, -0.0067, -0.0269,  ..., -0.0139,  0.0404, -0.0121]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.0078, -2.6602,  1.3281,  ...,  1.1309,  3.2227,  1.3750]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 06:12:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of biased is unbiased
The opposite of available is unavailable
The opposite of published is unpublished
The opposite of acceptable is unacceptable
The opposite of sustainable is unsustainable
The opposite of predictable is unpredictable
The opposite of employed is unemployed
The opposite of related is
2024-07-18 06:12:16 root INFO     [order_1_approx] starting weight calculation for The opposite of related is unrelated
The opposite of sustainable is unsustainable
The opposite of predictable is unpredictable
The opposite of acceptable is unacceptable
The opposite of biased is unbiased
The opposite of employed is unemployed
The opposite of available is unavailable
The opposite of published is
2024-07-18 06:12:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 06:16:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2463,  0.0215,  1.0020,  ...,  0.9424, -0.0500,  0.2397],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.1270, -3.6562,  2.5352,  ...,  2.5664,  1.1162, -2.7109],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.9333e-02, -2.0599e-02,  2.2827e-02,  ..., -5.9967e-03,
         -1.1009e-02,  7.8354e-03],
        [ 3.5324e-03, -1.0033e-03, -6.7368e-03,  ...,  2.7046e-03,
         -1.7883e-02, -5.7449e-03],
        [ 9.0485e-03,  1.3153e-02, -4.6631e-02,  ..., -9.4604e-03,
         -1.1024e-03, -1.3687e-02],
        ...,
        [ 8.3008e-03, -2.1973e-03,  8.0109e-03,  ..., -1.0452e-02,
          3.9825e-03,  1.8112e-02],
        [ 6.8207e-03,  1.2054e-03,  2.0172e-02,  ..., -3.4142e-04,
         -1.5083e-02,  7.6294e-06],
        [ 2.1530e-02, -4.1122e-03,  4.2038e-03,  ..., -7.6523e-03,
          1.2352e-02, -1.9989e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.2812, -2.9883,  0.3555,  ...,  2.6699,  1.7246, -2.2578]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 06:16:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of related is unrelated
The opposite of sustainable is unsustainable
The opposite of predictable is unpredictable
The opposite of acceptable is unacceptable
The opposite of biased is unbiased
The opposite of employed is unemployed
The opposite of available is unavailable
The opposite of published is
2024-07-18 06:16:23 root INFO     [order_1_approx] starting weight calculation for The opposite of sustainable is unsustainable
The opposite of biased is unbiased
The opposite of available is unavailable
The opposite of published is unpublished
The opposite of employed is unemployed
The opposite of acceptable is unacceptable
The opposite of related is unrelated
The opposite of predictable is
2024-07-18 06:16:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 06:20:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4309,  0.0918, -0.5122,  ..., -0.3777,  0.5801, -0.2910],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.3643, -3.9336, -2.4531,  ...,  3.6309,  4.3789,  4.9023],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0144, -0.0246,  0.0125,  ...,  0.0123, -0.0031, -0.0144],
        [-0.0149, -0.0273, -0.0155,  ..., -0.0094, -0.0093, -0.0192],
        [-0.0036, -0.0148, -0.0235,  ..., -0.0151, -0.0046,  0.0130],
        ...,
        [-0.0163, -0.0430, -0.0129,  ..., -0.0065,  0.0070, -0.0075],
        [-0.0122,  0.0078,  0.0287,  ..., -0.0143, -0.0128, -0.0059],
        [-0.0076,  0.0270,  0.0027,  ..., -0.0122,  0.0109, -0.0103]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.1172, -3.8496, -2.6445,  ...,  2.7559,  3.5586,  5.1328]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 06:20:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of sustainable is unsustainable
The opposite of biased is unbiased
The opposite of available is unavailable
The opposite of published is unpublished
The opposite of employed is unemployed
The opposite of acceptable is unacceptable
The opposite of related is unrelated
The opposite of predictable is
2024-07-18 06:20:30 root INFO     total operator prediction time: 1979.1403360366821 seconds
2024-07-18 06:20:30 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on re+verb_reg
2024-07-18 06:20:30 root INFO     building operator re+verb_reg
2024-07-18 06:20:30 root INFO     [order_1_approx] starting weight calculation for To locate again is to relocate
To decorate again is to redecorate
To occur again is to reoccur
To discover again is to rediscover
To configure again is to reconfigure
To cognize again is to recognize
To learn again is to relearn
To interpret again is to
2024-07-18 06:20:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 06:24:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1694, -0.3137, -0.5610,  ..., -0.3613,  0.0510,  0.6074],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.9473,  2.0293, -6.9609,  ..., -1.2617, -2.8320,  2.5469],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0069, -0.0075, -0.0036,  ...,  0.0163,  0.0103, -0.0038],
        [-0.0131, -0.0127,  0.0007,  ..., -0.0152,  0.0030,  0.0050],
        [ 0.0264,  0.0014, -0.0096,  ...,  0.0231,  0.0122,  0.0040],
        ...,
        [ 0.0016, -0.0143, -0.0153,  ..., -0.0043,  0.0027, -0.0183],
        [ 0.0297, -0.0085,  0.0165,  ..., -0.0064,  0.0005, -0.0135],
        [ 0.0218, -0.0096, -0.0075,  ..., -0.0144,  0.0004,  0.0027]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.8223,  1.0303, -5.9844,  ..., -1.2812, -0.8887,  3.4160]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 06:24:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To locate again is to relocate
To decorate again is to redecorate
To occur again is to reoccur
To discover again is to rediscover
To configure again is to reconfigure
To cognize again is to recognize
To learn again is to relearn
To interpret again is to
2024-07-18 06:24:37 root INFO     [order_1_approx] starting weight calculation for To locate again is to relocate
To learn again is to relearn
To configure again is to reconfigure
To occur again is to reoccur
To interpret again is to reinterpret
To discover again is to rediscover
To cognize again is to recognize
To decorate again is to
2024-07-18 06:24:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 06:28:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4067, -0.2039,  0.5278,  ..., -0.1331, -0.1484, -0.2324],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.7012,  1.2676, -4.6562,  ..., -3.0234, -1.2451,  0.8721],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0163, -0.0115, -0.0038,  ..., -0.0056, -0.0072, -0.0121],
        [-0.0110, -0.0199,  0.0154,  ...,  0.0044,  0.0174,  0.0120],
        [ 0.0263,  0.0065, -0.0077,  ..., -0.0087, -0.0213, -0.0070],
        ...,
        [ 0.0215,  0.0007, -0.0129,  ..., -0.0226, -0.0393,  0.0050],
        [ 0.0226,  0.0231, -0.0012,  ..., -0.0181, -0.0054, -0.0078],
        [ 0.0023,  0.0028,  0.0091,  ..., -0.0020,  0.0124,  0.0094]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.9922,  1.5332, -4.8125,  ..., -3.1465, -1.8555,  0.4189]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 06:28:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To locate again is to relocate
To learn again is to relearn
To configure again is to reconfigure
To occur again is to reoccur
To interpret again is to reinterpret
To discover again is to rediscover
To cognize again is to recognize
To decorate again is to
2024-07-18 06:28:44 root INFO     [order_1_approx] starting weight calculation for To occur again is to reoccur
To interpret again is to reinterpret
To learn again is to relearn
To cognize again is to recognize
To discover again is to rediscover
To locate again is to relocate
To decorate again is to redecorate
To configure again is to
2024-07-18 06:28:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 06:32:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6255,  1.0039, -0.2300,  ...,  0.4128, -0.8374, -0.2256],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6045, -0.4756, -6.6328,  ..., -2.9688,  0.1450,  0.0449],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0020, -0.0117,  0.0005,  ...,  0.0151,  0.0005, -0.0105],
        [-0.0175, -0.0075, -0.0004,  ...,  0.0097,  0.0020, -0.0005],
        [ 0.0305,  0.0139, -0.0069,  ...,  0.0160, -0.0332,  0.0142],
        ...,
        [ 0.0056, -0.0108,  0.0007,  ..., -0.0113, -0.0194,  0.0093],
        [ 0.0125,  0.0113, -0.0099,  ..., -0.0254, -0.0032, -0.0086],
        [-0.0038,  0.0083,  0.0037,  ..., -0.0037, -0.0008,  0.0156]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5830,  0.2695, -7.0312,  ..., -3.2207,  0.6035, -0.0170]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 06:32:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To occur again is to reoccur
To interpret again is to reinterpret
To learn again is to relearn
To cognize again is to recognize
To discover again is to rediscover
To locate again is to relocate
To decorate again is to redecorate
To configure again is to
2024-07-18 06:32:49 root INFO     [order_1_approx] starting weight calculation for To discover again is to rediscover
To interpret again is to reinterpret
To learn again is to relearn
To cognize again is to recognize
To configure again is to reconfigure
To decorate again is to redecorate
To locate again is to relocate
To occur again is to
2024-07-18 06:32:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 06:36:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6211,  0.2524,  0.4351,  ..., -0.3823,  0.3635, -0.1941],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2949, -1.0488, -6.6094,  ...,  3.0430,  1.2275,  0.7695],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0077,  0.0014,  0.0061,  ...,  0.0066,  0.0093, -0.0190],
        [ 0.0068, -0.0023, -0.0011,  ..., -0.0050,  0.0236, -0.0119],
        [ 0.0246,  0.0107, -0.0145,  ..., -0.0081, -0.0202, -0.0128],
        ...,
        [ 0.0070, -0.0129,  0.0031,  ..., -0.0080, -0.0056, -0.0018],
        [-0.0097, -0.0101,  0.0065,  ...,  0.0080,  0.0026,  0.0055],
        [-0.0125, -0.0086,  0.0225,  ...,  0.0015,  0.0037,  0.0106]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.6865, -0.7842, -5.4688,  ...,  3.2168,  1.5713, -0.1152]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 06:36:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To discover again is to rediscover
To interpret again is to reinterpret
To learn again is to relearn
To cognize again is to recognize
To configure again is to reconfigure
To decorate again is to redecorate
To locate again is to relocate
To occur again is to
2024-07-18 06:36:55 root INFO     [order_1_approx] starting weight calculation for To learn again is to relearn
To locate again is to relocate
To configure again is to reconfigure
To cognize again is to recognize
To occur again is to reoccur
To decorate again is to redecorate
To interpret again is to reinterpret
To discover again is to
2024-07-18 06:36:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 06:40:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1636, -0.2595, -0.7651,  ..., -0.0291,  0.1292,  0.3264],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4395,  0.7700, -7.7422,  ..., -3.5820, -1.0332, -2.6309],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0232, -0.0113,  0.0087,  ...,  0.0173, -0.0096,  0.0044],
        [-0.0232, -0.0067, -0.0071,  ...,  0.0022,  0.0333,  0.0140],
        [ 0.0429, -0.0058, -0.0193,  ..., -0.0002, -0.0142, -0.0338],
        ...,
        [-0.0218, -0.0253,  0.0111,  ..., -0.0126,  0.0004,  0.0111],
        [ 0.0085,  0.0251,  0.0041,  ..., -0.0194, -0.0024, -0.0015],
        [-0.0013,  0.0120,  0.0058,  ..., -0.0113, -0.0022, -0.0061]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.6924,  0.1367, -7.6641,  ..., -4.1875, -2.5918, -1.6982]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 06:41:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To learn again is to relearn
To locate again is to relocate
To configure again is to reconfigure
To cognize again is to recognize
To occur again is to reoccur
To decorate again is to redecorate
To interpret again is to reinterpret
To discover again is to
2024-07-18 06:41:00 root INFO     [order_1_approx] starting weight calculation for To configure again is to reconfigure
To learn again is to relearn
To interpret again is to reinterpret
To discover again is to rediscover
To occur again is to reoccur
To locate again is to relocate
To decorate again is to redecorate
To cognize again is to
2024-07-18 06:41:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 06:45:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2644, -0.2473, -0.2197,  ...,  0.1544, -1.1514,  0.6797],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.7793,  0.1221, -6.7266,  ...,  0.1670, -3.1250, -3.7109],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.1330e-02, -1.4053e-02,  5.2299e-03,  ...,  2.9526e-03,
         -1.7044e-02, -2.4223e-03],
        [-6.2065e-03, -7.6027e-03,  4.3716e-03,  ..., -2.2461e-02,
          2.2430e-03, -2.7428e-03],
        [ 1.8463e-02, -2.3499e-03, -1.1795e-02,  ...,  4.1809e-02,
          1.0712e-02, -6.5155e-03],
        ...,
        [ 7.3471e-03,  3.5954e-03, -2.0981e-03,  ..., -1.2875e-04,
         -3.8147e-05,  5.8403e-03],
        [ 3.7231e-02, -9.0179e-03, -4.0364e-04,  ...,  2.3026e-02,
         -8.8348e-03, -1.1490e-02],
        [ 1.6556e-02,  2.4300e-03, -1.5747e-02,  ...,  1.4336e-02,
          1.5091e-02, -3.1776e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.6172, -0.1394, -6.2969,  ...,  0.5547, -2.4492, -3.1484]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 06:45:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To configure again is to reconfigure
To learn again is to relearn
To interpret again is to reinterpret
To discover again is to rediscover
To occur again is to reoccur
To locate again is to relocate
To decorate again is to redecorate
To cognize again is to
2024-07-18 06:45:05 root INFO     [order_1_approx] starting weight calculation for To interpret again is to reinterpret
To learn again is to relearn
To discover again is to rediscover
To decorate again is to redecorate
To configure again is to reconfigure
To cognize again is to recognize
To occur again is to reoccur
To locate again is to
2024-07-18 06:45:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 06:49:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1765,  0.6187, -0.5107,  ..., -0.3047, -0.4336, -0.3650],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4180,  1.7188, -7.9844,  ..., -3.2617,  2.5039,  1.5762],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0247, -0.0161,  0.0029,  ...,  0.0243, -0.0251, -0.0003],
        [-0.0437, -0.0226,  0.0033,  ...,  0.0091,  0.0431, -0.0070],
        [ 0.0554,  0.0227, -0.0126,  ...,  0.0110, -0.0357,  0.0077],
        ...,
        [ 0.0089, -0.0127,  0.0009,  ..., -0.0380, -0.0135, -0.0093],
        [ 0.0227,  0.0013,  0.0026,  ..., -0.0206, -0.0098,  0.0151],
        [-0.0031, -0.0039,  0.0174,  ..., -0.0110, -0.0058, -0.0141]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5723,  1.8291, -7.7227,  ..., -3.3066,  2.6738,  2.1523]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 06:49:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To interpret again is to reinterpret
To learn again is to relearn
To discover again is to rediscover
To decorate again is to redecorate
To configure again is to reconfigure
To cognize again is to recognize
To occur again is to reoccur
To locate again is to
2024-07-18 06:49:09 root INFO     [order_1_approx] starting weight calculation for To discover again is to rediscover
To decorate again is to redecorate
To configure again is to reconfigure
To locate again is to relocate
To cognize again is to recognize
To interpret again is to reinterpret
To occur again is to reoccur
To learn again is to
2024-07-18 06:49:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 06:53:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2798,  0.3508,  0.3604,  ..., -0.1145,  0.2979,  0.0948],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0527,  0.6855, -7.7656,  ..., -1.0918, -2.7344, -1.7832],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0116, -0.0111,  0.0092,  ..., -0.0014, -0.0037, -0.0128],
        [-0.0250,  0.0097, -0.0147,  ..., -0.0060,  0.0458,  0.0120],
        [ 0.0187, -0.0104,  0.0094,  ...,  0.0164, -0.0199, -0.0157],
        ...,
        [ 0.0006, -0.0232,  0.0096,  ..., -0.0175, -0.0468, -0.0006],
        [ 0.0418, -0.0210,  0.0189,  ..., -0.0176, -0.0268,  0.0067],
        [ 0.0177, -0.0101,  0.0033,  ..., -0.0113, -0.0160, -0.0100]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.8936,  0.2109, -6.5312,  ..., -1.0635, -2.7871, -1.6348]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 06:53:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To discover again is to rediscover
To decorate again is to redecorate
To configure again is to reconfigure
To locate again is to relocate
To cognize again is to recognize
To interpret again is to reinterpret
To occur again is to reoccur
To learn again is to
2024-07-18 06:53:14 root INFO     total operator prediction time: 1964.6788029670715 seconds
2024-07-18 06:53:14 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on adj+ness_reg
2024-07-18 06:53:14 root INFO     building operator adj+ness_reg
2024-07-18 06:53:15 root INFO     [order_1_approx] starting weight calculation for The state of being unique is uniqueness
The state of being weak is weakness
The state of being prepared is preparedness
The state of being marked is markedness
The state of being rare is rareness
The state of being obvious is obviousness
The state of being directed is directedness
The state of being hot is
2024-07-18 06:53:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 06:57:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0386,  0.2050,  0.5176,  ...,  0.5195,  0.5215,  0.6465],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8359,  1.5918, -0.6455,  ..., -3.4531,  3.7852,  7.5703],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0006, -0.0091,  0.0119,  ..., -0.0101, -0.0165, -0.0131],
        [-0.0110, -0.0088, -0.0007,  ..., -0.0022,  0.0103, -0.0003],
        [ 0.0078,  0.0079,  0.0101,  ...,  0.0106,  0.0207,  0.0061],
        ...,
        [ 0.0055, -0.0072,  0.0008,  ..., -0.0128, -0.0104,  0.0088],
        [-0.0040,  0.0066,  0.0061,  ..., -0.0033, -0.0260, -0.0039],
        [-0.0242, -0.0207, -0.0163,  ..., -0.0229,  0.0073, -0.0255]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2607,  1.4619, -0.0830,  ..., -3.3711,  3.4355,  6.4609]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 06:57:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being unique is uniqueness
The state of being weak is weakness
The state of being prepared is preparedness
The state of being marked is markedness
The state of being rare is rareness
The state of being obvious is obviousness
The state of being directed is directedness
The state of being hot is
2024-07-18 06:57:24 root INFO     [order_1_approx] starting weight calculation for The state of being rare is rareness
The state of being obvious is obviousness
The state of being directed is directedness
The state of being hot is hotness
The state of being weak is weakness
The state of being marked is markedness
The state of being prepared is preparedness
The state of being unique is
2024-07-18 06:57:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 07:01:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1238,  0.0190, -0.0236,  ..., -0.1904,  1.1777, -0.0801],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8457,  1.7090,  0.7607,  ...,  0.9883,  1.1562,  6.1055],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0024, -0.0143,  0.0153,  ..., -0.0120, -0.0189, -0.0206],
        [-0.0083,  0.0020, -0.0128,  ...,  0.0065,  0.0182, -0.0098],
        [ 0.0262,  0.0202, -0.0125,  ...,  0.0073, -0.0034, -0.0012],
        ...,
        [ 0.0179,  0.0010, -0.0026,  ..., -0.0210,  0.0109, -0.0151],
        [-0.0011,  0.0067,  0.0052,  ..., -0.0111, -0.0180,  0.0075],
        [ 0.0031,  0.0258,  0.0110,  ..., -0.0283,  0.0212, -0.0270]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5220,  1.9600,  1.6875,  ...,  0.0957,  1.0410,  5.3359]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 07:01:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being rare is rareness
The state of being obvious is obviousness
The state of being directed is directedness
The state of being hot is hotness
The state of being weak is weakness
The state of being marked is markedness
The state of being prepared is preparedness
The state of being unique is
2024-07-18 07:01:33 root INFO     [order_1_approx] starting weight calculation for The state of being prepared is preparedness
The state of being obvious is obviousness
The state of being unique is uniqueness
The state of being rare is rareness
The state of being directed is directedness
The state of being weak is weakness
The state of being hot is hotness
The state of being marked is
2024-07-18 07:01:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 07:05:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0043, -0.2727,  0.2487,  ...,  0.6816, -0.2268,  0.2334],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.4609,  1.1260, -1.2402,  ..., -3.0195,  1.2842,  6.7109],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0169,  0.0166,  0.0137,  ...,  0.0074,  0.0035, -0.0100],
        [-0.0235, -0.0122, -0.0116,  ..., -0.0080,  0.0090, -0.0009],
        [ 0.0296,  0.0052, -0.0123,  ...,  0.0056, -0.0104, -0.0029],
        ...,
        [ 0.0016, -0.0217, -0.0103,  ..., -0.0241,  0.0027,  0.0033],
        [ 0.0028,  0.0068, -0.0179,  ...,  0.0007, -0.0026,  0.0240],
        [-0.0179,  0.0066,  0.0085,  ..., -0.0155,  0.0056, -0.0181]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.1699,  0.8940, -0.9287,  ..., -3.2930,  1.3467,  6.0352]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 07:05:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being prepared is preparedness
The state of being obvious is obviousness
The state of being unique is uniqueness
The state of being rare is rareness
The state of being directed is directedness
The state of being weak is weakness
The state of being hot is hotness
The state of being marked is
2024-07-18 07:05:42 root INFO     [order_1_approx] starting weight calculation for The state of being prepared is preparedness
The state of being unique is uniqueness
The state of being directed is directedness
The state of being hot is hotness
The state of being weak is weakness
The state of being rare is rareness
The state of being marked is markedness
The state of being obvious is
2024-07-18 07:05:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 07:09:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7461, -0.6943,  0.5205,  ..., -0.2805, -0.7041,  0.3677],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9868,  4.4766, -5.2266,  ..., -1.8545, -3.8086,  4.3672],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0245, -0.0052,  0.0109,  ..., -0.0114, -0.0197, -0.0152],
        [-0.0018, -0.0041,  0.0021,  ..., -0.0072, -0.0153, -0.0144],
        [-0.0066, -0.0056, -0.0428,  ..., -0.0007, -0.0047, -0.0092],
        ...,
        [ 0.0257, -0.0009, -0.0173,  ..., -0.0214, -0.0090, -0.0013],
        [ 0.0025, -0.0123, -0.0130,  ..., -0.0006, -0.0285, -0.0001],
        [-0.0013,  0.0200, -0.0053,  ...,  0.0071,  0.0070, -0.0159]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.8853,  3.8965, -4.4453,  ..., -1.4004, -4.2773,  4.0977]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 07:09:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being prepared is preparedness
The state of being unique is uniqueness
The state of being directed is directedness
The state of being hot is hotness
The state of being weak is weakness
The state of being rare is rareness
The state of being marked is markedness
The state of being obvious is
2024-07-18 07:09:51 root INFO     [order_1_approx] starting weight calculation for The state of being rare is rareness
The state of being marked is markedness
The state of being unique is uniqueness
The state of being obvious is obviousness
The state of being directed is directedness
The state of being hot is hotness
The state of being weak is weakness
The state of being prepared is
2024-07-18 07:09:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 07:13:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0532, -0.4741, -0.7686,  ..., -0.1033,  1.6592, -0.2949],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8228,  0.7637, -0.3301,  ..., -3.2500, -2.3828,  3.7734],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0071, -0.0153,  0.0215,  ...,  0.0006, -0.0007,  0.0019],
        [-0.0158,  0.0065, -0.0182,  ...,  0.0087, -0.0055, -0.0107],
        [ 0.0113,  0.0049,  0.0136,  ...,  0.0039, -0.0005,  0.0155],
        ...,
        [-0.0212, -0.0294, -0.0031,  ..., -0.0076, -0.0185,  0.0110],
        [-0.0060, -0.0101,  0.0209,  ..., -0.0201, -0.0129,  0.0164],
        [-0.0101, -0.0169, -0.0016,  ...,  0.0032,  0.0016, -0.0030]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0205,  0.4136, -0.1040,  ..., -2.6289, -2.6406,  2.9473]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 07:14:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being rare is rareness
The state of being marked is markedness
The state of being unique is uniqueness
The state of being obvious is obviousness
The state of being directed is directedness
The state of being hot is hotness
The state of being weak is weakness
The state of being prepared is
2024-07-18 07:14:00 root INFO     [order_1_approx] starting weight calculation for The state of being marked is markedness
The state of being obvious is obviousness
The state of being prepared is preparedness
The state of being hot is hotness
The state of being rare is rareness
The state of being unique is uniqueness
The state of being weak is weakness
The state of being directed is
2024-07-18 07:14:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 07:18:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3311, -0.9434, -0.0635,  ..., -0.6172,  0.6113,  0.2649],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2268, -1.6396,  1.7070,  ..., -3.4180,  4.9492,  1.8691],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-5.8479e-03, -5.7678e-03,  1.4572e-03,  ..., -1.2650e-02,
         -1.0452e-02, -1.3039e-02],
        [-1.3466e-02, -1.0529e-03, -2.4300e-03,  ...,  2.6360e-03,
         -9.7961e-03,  7.4348e-03],
        [ 1.1795e-02, -4.6158e-03, -1.0635e-02,  ..., -1.0780e-02,
         -4.2458e-03, -2.1076e-03],
        ...,
        [-1.6464e-02,  1.5488e-03, -5.7068e-03,  ..., -5.6610e-03,
          1.7426e-02,  1.3206e-02],
        [-2.1286e-03, -1.1543e-02, -8.2016e-05,  ..., -5.9967e-03,
         -9.2468e-03,  5.6572e-03],
        [-3.5820e-03,  4.8752e-03,  1.8883e-03,  ...,  1.0887e-02,
          1.8982e-02, -1.3046e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3792, -1.1094,  1.5625,  ..., -2.2949,  5.1875,  1.0430]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 07:18:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being marked is markedness
The state of being obvious is obviousness
The state of being prepared is preparedness
The state of being hot is hotness
The state of being rare is rareness
The state of being unique is uniqueness
The state of being weak is weakness
The state of being directed is
2024-07-18 07:18:09 root INFO     [order_1_approx] starting weight calculation for The state of being marked is markedness
The state of being rare is rareness
The state of being obvious is obviousness
The state of being unique is uniqueness
The state of being prepared is preparedness
The state of being hot is hotness
The state of being directed is directedness
The state of being weak is
2024-07-18 07:18:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 07:22:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3982, -0.7778,  0.3865,  ...,  0.1179,  0.6064, -0.4580],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5957, -0.2139, -1.0488,  ..., -2.1152, -0.7212,  1.8906],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0127, -0.0200,  0.0218,  ..., -0.0005, -0.0087, -0.0068],
        [-0.0017,  0.0047, -0.0038,  ...,  0.0093,  0.0136,  0.0070],
        [ 0.0178, -0.0016, -0.0025,  ..., -0.0010, -0.0221, -0.0066],
        ...,
        [-0.0016, -0.0132,  0.0089,  ..., -0.0175, -0.0066, -0.0061],
        [-0.0075, -0.0084,  0.0189,  ..., -0.0144, -0.0158,  0.0059],
        [-0.0201,  0.0024,  0.0223,  ..., -0.0155,  0.0185, -0.0133]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0361, -0.4150, -0.9888,  ..., -2.3496, -0.9585,  1.2402]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 07:22:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being marked is markedness
The state of being rare is rareness
The state of being obvious is obviousness
The state of being unique is uniqueness
The state of being prepared is preparedness
The state of being hot is hotness
The state of being directed is directedness
The state of being weak is
2024-07-18 07:22:16 root INFO     [order_1_approx] starting weight calculation for The state of being directed is directedness
The state of being obvious is obviousness
The state of being weak is weakness
The state of being unique is uniqueness
The state of being prepared is preparedness
The state of being hot is hotness
The state of being marked is markedness
The state of being rare is
2024-07-18 07:22:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 07:26:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4434, -0.3374,  0.3257,  ...,  0.1644,  0.6104,  0.0049],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.4062,  2.1504,  1.8564,  ..., -0.7456,  1.7725,  5.7344],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0031, -0.0105,  0.0136,  ..., -0.0135, -0.0186, -0.0212],
        [-0.0114, -0.0081,  0.0102,  ..., -0.0107, -0.0100,  0.0030],
        [ 0.0100, -0.0261, -0.0007,  ...,  0.0106,  0.0055,  0.0138],
        ...,
        [ 0.0202, -0.0188, -0.0052,  ..., -0.0372,  0.0059, -0.0152],
        [-0.0019, -0.0169,  0.0073,  ..., -0.0185, -0.0088,  0.0171],
        [-0.0294, -0.0138,  0.0107,  ..., -0.0306, -0.0106, -0.0337]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.3945,  1.1436,  3.0820,  ..., -2.4531,  1.9414,  4.2266]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 07:26:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being directed is directedness
The state of being obvious is obviousness
The state of being weak is weakness
The state of being unique is uniqueness
The state of being prepared is preparedness
The state of being hot is hotness
The state of being marked is markedness
The state of being rare is
2024-07-18 07:26:23 root INFO     total operator prediction time: 1989.1418137550354 seconds
2024-07-18 07:26:23 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on noun+less_reg
2024-07-18 07:26:23 root INFO     building operator noun+less_reg
2024-07-18 07:26:24 root INFO     [order_1_approx] starting weight calculation for Something without arm is armless
Something without speech is speechless
Something without tooth is toothless
Something without law is lawless
Something without luck is luckless
Something without child is childless
Something without soul is soulless
Something without goal is
2024-07-18 07:26:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 07:30:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5508,  0.9189,  0.2080,  ..., -0.0444,  1.2959,  0.9121],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.0840,  1.6260, -3.2031,  ..., -0.3452,  2.2266,  2.6602],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0090, -0.0116, -0.0017,  ...,  0.0083, -0.0110, -0.0081],
        [-0.0030, -0.0152, -0.0105,  ...,  0.0034,  0.0056, -0.0064],
        [ 0.0118,  0.0007, -0.0056,  ..., -0.0046,  0.0007,  0.0026],
        ...,
        [ 0.0088,  0.0036, -0.0096,  ..., -0.0137,  0.0072,  0.0099],
        [-0.0088,  0.0178, -0.0119,  ...,  0.0062, -0.0204,  0.0056],
        [-0.0095,  0.0104,  0.0164,  ..., -0.0016,  0.0156, -0.0275]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4023,  1.6465, -1.9131,  ..., -0.5713,  1.3926,  2.1328]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 07:30:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without arm is armless
Something without speech is speechless
Something without tooth is toothless
Something without law is lawless
Something without luck is luckless
Something without child is childless
Something without soul is soulless
Something without goal is
2024-07-18 07:30:31 root INFO     [order_1_approx] starting weight calculation for Something without goal is goalless
Something without arm is armless
Something without speech is speechless
Something without law is lawless
Something without luck is luckless
Something without tooth is toothless
Something without child is childless
Something without soul is
2024-07-18 07:30:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 07:34:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6162,  0.1696, -0.7324,  ...,  0.4998, -0.1229,  0.1912],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.2578, -0.2319, -0.3701,  ..., -2.9902, -0.6807,  2.8301],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.5558e-02, -8.4610e-03,  6.9427e-03,  ...,  7.0457e-03,
         -3.4821e-02, -1.3351e-02],
        [ 1.2283e-03, -3.0853e-02,  6.8665e-04,  ..., -2.3079e-03,
          2.1019e-03,  5.8517e-03],
        [ 1.1024e-02, -1.2177e-02, -2.5879e-02,  ...,  2.3117e-02,
         -9.4461e-04,  3.8414e-03],
        ...,
        [-7.7209e-03, -7.6294e-06, -1.8707e-02,  ..., -2.1271e-02,
          2.9266e-02,  1.0574e-02],
        [-1.9409e-02, -1.2749e-02, -1.6174e-02,  ...,  4.3335e-03,
         -6.2065e-03, -6.9695e-03],
        [-1.0147e-03, -1.0967e-03,  1.2741e-02,  ..., -1.2100e-02,
          2.6054e-03, -4.0314e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4570,  0.3389, -0.7139,  ..., -2.3027,  0.0713,  2.8320]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 07:34:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without goal is goalless
Something without arm is armless
Something without speech is speechless
Something without law is lawless
Something without luck is luckless
Something without tooth is toothless
Something without child is childless
Something without soul is
2024-07-18 07:34:40 root INFO     [order_1_approx] starting weight calculation for Something without tooth is toothless
Something without arm is armless
Something without soul is soulless
Something without goal is goalless
Something without child is childless
Something without luck is luckless
Something without speech is speechless
Something without law is
2024-07-18 07:34:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 07:38:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1040, -0.5688,  0.2969,  ..., -0.5977, -0.0725,  0.5498],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5649, -1.0713, -2.2305,  ...,  0.7715,  4.4922,  0.6279],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0049, -0.0013,  0.0170,  ...,  0.0036, -0.0070, -0.0194],
        [ 0.0036, -0.0130,  0.0114,  ..., -0.0045, -0.0050, -0.0157],
        [ 0.0231,  0.0113, -0.0240,  ..., -0.0066, -0.0025, -0.0082],
        ...,
        [-0.0204,  0.0110, -0.0031,  ..., -0.0013,  0.0173,  0.0119],
        [-0.0185, -0.0240,  0.0037,  ..., -0.0163, -0.0136, -0.0153],
        [-0.0055, -0.0007,  0.0224,  ..., -0.0131, -0.0022, -0.0091]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7446, -0.2041, -1.7676,  ..., -0.1089,  3.6328,  0.5420]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 07:38:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without tooth is toothless
Something without arm is armless
Something without soul is soulless
Something without goal is goalless
Something without child is childless
Something without luck is luckless
Something without speech is speechless
Something without law is
2024-07-18 07:38:49 root INFO     [order_1_approx] starting weight calculation for Something without child is childless
Something without speech is speechless
Something without goal is goalless
Something without law is lawless
Something without soul is soulless
Something without luck is luckless
Something without arm is armless
Something without tooth is
2024-07-18 07:38:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 07:42:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0034,  0.2544, -1.0264,  ..., -0.2131, -0.2559,  0.1201],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7344,  3.0137, -0.1777,  ..., -3.4336,  1.6416, -1.7383],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 8.1635e-03, -5.9509e-03, -7.0496e-03,  ...,  1.3153e-02,
         -5.0812e-03,  1.1078e-02],
        [-2.3911e-02, -1.8250e-02,  7.6294e-05,  ..., -1.0956e-02,
         -8.9111e-03,  1.2512e-02],
        [-5.0201e-03, -2.2831e-03, -3.2166e-02,  ...,  1.4008e-02,
          8.6823e-03, -2.0218e-04],
        ...,
        [ 1.6434e-02,  9.9792e-03, -2.2030e-03,  ..., -3.0762e-02,
          7.9117e-03,  1.1856e-02],
        [-1.8829e-02, -1.2474e-02,  8.1329e-03,  ..., -1.2169e-02,
         -3.2349e-02,  1.1475e-02],
        [-2.3941e-02, -3.9940e-03,  3.4580e-03,  ...,  6.6605e-03,
          7.4844e-03, -5.2856e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5820,  2.2500,  0.3926,  ..., -4.0195,  0.3896, -2.0469]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 07:42:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without child is childless
Something without speech is speechless
Something without goal is goalless
Something without law is lawless
Something without soul is soulless
Something without luck is luckless
Something without arm is armless
Something without tooth is
2024-07-18 07:42:56 root INFO     [order_1_approx] starting weight calculation for Something without luck is luckless
Something without law is lawless
Something without child is childless
Something without goal is goalless
Something without speech is speechless
Something without tooth is toothless
Something without soul is soulless
Something without arm is
2024-07-18 07:42:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 07:47:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8345,  0.5361, -0.1915,  ...,  0.8799, -0.2891,  0.2842],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9922,  0.9736,  2.0938,  ..., -1.4736,  0.8599,  4.6445],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0255,  0.0114, -0.0018,  ...,  0.0263, -0.0068,  0.0028],
        [-0.0115, -0.0201,  0.0010,  ..., -0.0038, -0.0040,  0.0048],
        [ 0.0280,  0.0053, -0.0200,  ...,  0.0071, -0.0023,  0.0130],
        ...,
        [ 0.0188, -0.0016,  0.0043,  ..., -0.0190, -0.0075,  0.0039],
        [ 0.0148, -0.0178,  0.0036,  ..., -0.0014, -0.0313,  0.0024],
        [-0.0053, -0.0172, -0.0030,  ...,  0.0040, -0.0003, -0.0180]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9004,  1.0781,  2.7246,  ..., -1.1201, -0.4067,  4.8516]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 07:47:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without luck is luckless
Something without law is lawless
Something without child is childless
Something without goal is goalless
Something without speech is speechless
Something without tooth is toothless
Something without soul is soulless
Something without arm is
2024-07-18 07:47:04 root INFO     [order_1_approx] starting weight calculation for Something without law is lawless
Something without luck is luckless
Something without tooth is toothless
Something without child is childless
Something without soul is soulless
Something without goal is goalless
Something without arm is armless
Something without speech is
2024-07-18 07:47:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 07:51:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9678,  0.2397,  0.5327,  ..., -0.1873, -0.3274,  0.3679],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.1270,  2.3730, -1.8203,  ...,  0.5967,  0.7383,  0.6611],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0255, -0.0155, -0.0089,  ..., -0.0124, -0.0225,  0.0092],
        [-0.0085, -0.0153, -0.0310,  ...,  0.0068, -0.0113,  0.0441],
        [ 0.0131,  0.0061, -0.0402,  ..., -0.0074,  0.0068,  0.0198],
        ...,
        [-0.0052, -0.0050,  0.0149,  ..., -0.0350, -0.0186,  0.0011],
        [-0.0342, -0.0217,  0.0195,  ..., -0.0156, -0.0348,  0.0172],
        [ 0.0098,  0.0281,  0.0126,  ..., -0.0120,  0.0077, -0.0486]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5020,  2.7812, -2.0078,  ...,  0.9424,  0.7856,  0.6812]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 07:51:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without law is lawless
Something without luck is luckless
Something without tooth is toothless
Something without child is childless
Something without soul is soulless
Something without goal is goalless
Something without arm is armless
Something without speech is
2024-07-18 07:51:12 root INFO     [order_1_approx] starting weight calculation for Something without law is lawless
Something without arm is armless
Something without speech is speechless
Something without soul is soulless
Something without tooth is toothless
Something without goal is goalless
Something without luck is luckless
Something without child is
2024-07-18 07:51:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 07:55:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0342,  0.1455,  0.2678,  ...,  0.5952, -0.2020,  0.6035],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3027, -0.9014, -6.0742,  ..., -2.3418,  4.9609,  1.9609],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0213,  0.0125,  0.0017,  ...,  0.0142, -0.0330, -0.0214],
        [-0.0111, -0.0249, -0.0031,  ...,  0.0124,  0.0012,  0.0056],
        [ 0.0039,  0.0061, -0.0323,  ...,  0.0088,  0.0208,  0.0049],
        ...,
        [-0.0044, -0.0104,  0.0073,  ..., -0.0269, -0.0108, -0.0139],
        [-0.0285, -0.0094,  0.0062,  ..., -0.0002, -0.0388,  0.0070],
        [-0.0079,  0.0034, -0.0017,  ...,  0.0006,  0.0140, -0.0202]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0059, -0.7803, -5.5195,  ..., -2.1953,  3.9512,  2.1836]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 07:55:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without law is lawless
Something without arm is armless
Something without speech is speechless
Something without soul is soulless
Something without tooth is toothless
Something without goal is goalless
Something without luck is luckless
Something without child is
2024-07-18 07:55:20 root INFO     [order_1_approx] starting weight calculation for Something without law is lawless
Something without soul is soulless
Something without child is childless
Something without tooth is toothless
Something without goal is goalless
Something without arm is armless
Something without speech is speechless
Something without luck is
2024-07-18 07:55:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 07:59:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2195,  0.6279, -1.1953,  ...,  0.3159, -0.2568, -0.3667],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0063,  2.1211, -3.5840,  ...,  0.1375,  1.7910,  1.5449],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.1853e-03, -8.6975e-04,  1.8707e-02,  ...,  3.7048e-02,
         -5.5466e-03,  1.4099e-02],
        [-9.0027e-04, -1.9287e-02,  6.1264e-03,  ..., -5.7220e-04,
         -1.7334e-02,  5.9509e-04],
        [ 6.0616e-03,  1.8661e-02, -2.1530e-02,  ...,  4.8141e-03,
          5.9586e-03,  2.2202e-02],
        ...,
        [-7.3433e-03, -6.8665e-05, -2.8801e-03,  ..., -2.4994e-02,
          7.5684e-03,  2.0218e-03],
        [-1.3840e-02, -2.0859e-02, -1.4336e-02,  ..., -2.0645e-02,
         -1.5793e-02, -1.0712e-02],
        [-1.6434e-02,  1.4374e-02,  1.0880e-02,  ...,  8.3923e-05,
         -1.3809e-02, -2.8030e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1226,  1.8135, -3.3359,  ..., -0.7793,  1.0469,  0.5127]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 07:59:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without law is lawless
Something without soul is soulless
Something without child is childless
Something without tooth is toothless
Something without goal is goalless
Something without arm is armless
Something without speech is speechless
Something without luck is
2024-07-18 07:59:30 root INFO     total operator prediction time: 1987.0547993183136 seconds
2024-07-18 07:59:30 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+ment_irreg
2024-07-18 07:59:30 root INFO     building operator verb+ment_irreg
2024-07-18 07:59:31 root INFO     [order_1_approx] starting weight calculation for To assess results in a assessment
To entitle results in a entitlement
To arrange results in a arrangement
To adjust results in a adjustment
To amuse results in a amusement
To encourage results in a encouragement
To achieve results in a achievement
To enroll results in a
2024-07-18 07:59:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 08:03:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4114,  0.1090, -0.4155,  ...,  1.4531, -0.6157, -0.1335],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0703,  0.9331, -1.0586,  ...,  1.5479,  1.1650,  6.3438],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0016, -0.0069, -0.0090,  ...,  0.0042,  0.0024, -0.0002],
        [-0.0047, -0.0222,  0.0057,  ..., -0.0093, -0.0003,  0.0076],
        [ 0.0091,  0.0139, -0.0021,  ...,  0.0221, -0.0023, -0.0051],
        ...,
        [-0.0126, -0.0133, -0.0023,  ..., -0.0012,  0.0028, -0.0049],
        [ 0.0033, -0.0149,  0.0066,  ..., -0.0052, -0.0048,  0.0183],
        [ 0.0071,  0.0032,  0.0033,  ...,  0.0084,  0.0138, -0.0093]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5098,  0.0430, -0.2949,  ...,  1.7627,  1.2129,  5.9219]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 08:03:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To assess results in a assessment
To entitle results in a entitlement
To arrange results in a arrangement
To adjust results in a adjustment
To amuse results in a amusement
To encourage results in a encouragement
To achieve results in a achievement
To enroll results in a
2024-07-18 08:03:39 root INFO     [order_1_approx] starting weight calculation for To enroll results in a enrollment
To adjust results in a adjustment
To arrange results in a arrangement
To encourage results in a encouragement
To achieve results in a achievement
To amuse results in a amusement
To assess results in a assessment
To entitle results in a
2024-07-18 08:03:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 08:07:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0173, -0.3677,  0.3853,  ...,  0.2908, -0.4990, -0.6099],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.3867,  1.4473,  1.3945,  ...,  5.4727, -0.5664,  2.3145],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0023,  0.0018,  0.0057,  ...,  0.0092, -0.0012,  0.0117],
        [-0.0157, -0.0308, -0.0023,  ..., -0.0057,  0.0043,  0.0106],
        [ 0.0049,  0.0239, -0.0002,  ...,  0.0153,  0.0070, -0.0136],
        ...,
        [-0.0271, -0.0248, -0.0149,  ..., -0.0030,  0.0010,  0.0149],
        [ 0.0077, -0.0003, -0.0025,  ...,  0.0015, -0.0212,  0.0136],
        [ 0.0257,  0.0425,  0.0003,  ..., -0.0123,  0.0184, -0.0203]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5352,  0.9658,  0.8711,  ...,  3.8203, -0.3025,  3.0078]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 08:07:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To enroll results in a enrollment
To adjust results in a adjustment
To arrange results in a arrangement
To encourage results in a encouragement
To achieve results in a achievement
To amuse results in a amusement
To assess results in a assessment
To entitle results in a
2024-07-18 08:07:47 root INFO     [order_1_approx] starting weight calculation for To enroll results in a enrollment
To assess results in a assessment
To entitle results in a entitlement
To achieve results in a achievement
To encourage results in a encouragement
To arrange results in a arrangement
To amuse results in a amusement
To adjust results in a
2024-07-18 08:07:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 08:11:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6646,  0.4775,  0.4536,  ...,  0.4746, -0.2329, -0.9634],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0404,  2.6719, -4.1836,  ..., -0.5664,  1.8145,  5.9609],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.4832e-02,  1.4172e-03, -8.4782e-04,  ...,  1.8845e-02,
         -1.0178e-02, -7.7591e-03],
        [ 7.4844e-03, -2.3453e-02, -7.0343e-03,  ...,  1.2932e-02,
          3.5019e-03, -2.7466e-03],
        [ 3.5782e-03,  1.2283e-02, -2.0355e-02,  ...,  6.0150e-02,
          8.0566e-03, -1.9741e-04],
        ...,
        [-2.4078e-02, -2.6581e-02,  2.0790e-03,  ...,  3.8147e-05,
          1.3100e-02,  3.5095e-03],
        [-2.3163e-02, -3.6507e-03,  1.3321e-02,  ...,  4.7302e-03,
          6.1798e-04,  4.8561e-03],
        [-2.1286e-02,  1.1505e-02,  5.8975e-03,  ...,  1.5297e-02,
          1.8463e-02, -3.7937e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5933,  4.2031, -2.1406,  ..., -0.2551,  2.1738,  6.0820]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 08:11:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To enroll results in a enrollment
To assess results in a assessment
To entitle results in a entitlement
To achieve results in a achievement
To encourage results in a encouragement
To arrange results in a arrangement
To amuse results in a amusement
To adjust results in a
2024-07-18 08:11:56 root INFO     [order_1_approx] starting weight calculation for To achieve results in a achievement
To entitle results in a entitlement
To adjust results in a adjustment
To arrange results in a arrangement
To amuse results in a amusement
To enroll results in a enrollment
To encourage results in a encouragement
To assess results in a
2024-07-18 08:11:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 08:16:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0295, -0.4751, -0.7197,  ..., -0.1738, -0.4746, -0.2510],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.8750,  2.1250, -2.6309,  ...,  0.3584,  0.0933,  4.2383],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.0040e-02, -8.9188e-03, -4.6539e-03,  ...,  2.4963e-02,
          1.1795e-02, -2.1072e-02],
        [ 4.2343e-03, -1.4526e-02,  1.1505e-02,  ..., -9.2621e-03,
         -7.6294e-05, -2.5558e-04],
        [-1.6708e-03,  1.2543e-02, -3.9429e-02,  ...,  5.0354e-02,
          2.2064e-02, -6.9122e-03],
        ...,
        [-2.4246e-02, -8.9645e-03, -1.8692e-03,  ...,  5.1422e-03,
          1.5488e-03, -5.7755e-03],
        [-1.6571e-02, -6.3171e-03,  1.9897e-02,  ...,  5.8594e-03,
         -3.9825e-03,  2.9770e-02],
        [-4.2267e-03,  1.1337e-02, -1.4000e-03,  ...,  2.8625e-02,
          1.4565e-02, -9.7885e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3008,  1.1836, -0.7783,  ...,  1.0977,  1.1777,  5.3672]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 08:16:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To achieve results in a achievement
To entitle results in a entitlement
To adjust results in a adjustment
To arrange results in a arrangement
To amuse results in a amusement
To enroll results in a enrollment
To encourage results in a encouragement
To assess results in a
2024-07-18 08:16:04 root INFO     [order_1_approx] starting weight calculation for To arrange results in a arrangement
To entitle results in a entitlement
To enroll results in a enrollment
To assess results in a assessment
To encourage results in a encouragement
To amuse results in a amusement
To adjust results in a adjustment
To achieve results in a
2024-07-18 08:16:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 08:20:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6826, -0.2241,  0.4590,  ...,  0.3816, -0.7568, -1.0127],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9629,  2.4258, -3.4453,  ..., -2.3965,  4.3281,  2.0117],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0061, -0.0013,  0.0076,  ...,  0.0115, -0.0156, -0.0093],
        [-0.0057, -0.0259,  0.0074,  ..., -0.0083, -0.0060, -0.0174],
        [-0.0061,  0.0066, -0.0157,  ...,  0.0214,  0.0059,  0.0034],
        ...,
        [-0.0024, -0.0125, -0.0229,  ...,  0.0010,  0.0023,  0.0034],
        [-0.0034, -0.0062,  0.0084,  ..., -0.0069, -0.0087,  0.0036],
        [ 0.0095,  0.0124, -0.0096,  ...,  0.0016,  0.0167, -0.0075]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1602,  1.7461, -2.4961,  ..., -2.2852,  4.3867,  3.5273]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 08:20:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To arrange results in a arrangement
To entitle results in a entitlement
To enroll results in a enrollment
To assess results in a assessment
To encourage results in a encouragement
To amuse results in a amusement
To adjust results in a adjustment
To achieve results in a
2024-07-18 08:20:10 root INFO     [order_1_approx] starting weight calculation for To amuse results in a amusement
To enroll results in a enrollment
To assess results in a assessment
To arrange results in a arrangement
To entitle results in a entitlement
To adjust results in a adjustment
To achieve results in a achievement
To encourage results in a
2024-07-18 08:20:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 08:24:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0959, -0.1914, -0.0265,  ...,  0.4668,  0.3325,  0.0607],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7969, -1.8848, -1.3711,  ...,  1.2070, -0.1582,  1.8906],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0076, -0.0038, -0.0107,  ...,  0.0161, -0.0096, -0.0156],
        [ 0.0012, -0.0150, -0.0034,  ...,  0.0080, -0.0023,  0.0003],
        [ 0.0105,  0.0083, -0.0191,  ...,  0.0089, -0.0062, -0.0061],
        ...,
        [-0.0088, -0.0145,  0.0035,  ..., -0.0002, -0.0030, -0.0037],
        [-0.0024, -0.0049,  0.0091,  ..., -0.0157,  0.0146,  0.0205],
        [-0.0055,  0.0103,  0.0083,  ...,  0.0230,  0.0041, -0.0102]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4180, -1.5537, -0.7173,  ...,  2.1445, -0.5381,  4.3164]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 08:24:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To amuse results in a amusement
To enroll results in a enrollment
To assess results in a assessment
To arrange results in a arrangement
To entitle results in a entitlement
To adjust results in a adjustment
To achieve results in a achievement
To encourage results in a
2024-07-18 08:24:18 root INFO     [order_1_approx] starting weight calculation for To achieve results in a achievement
To assess results in a assessment
To enroll results in a enrollment
To entitle results in a entitlement
To adjust results in a adjustment
To arrange results in a arrangement
To encourage results in a encouragement
To amuse results in a
2024-07-18 08:24:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 08:28:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3613, -0.0787,  0.3564,  ...,  0.4072,  0.0882, -0.0753],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.4336,  1.8984, -0.7119,  ...,  0.6870, -0.7788,  6.6562],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0067, -0.0095, -0.0084,  ...,  0.0177, -0.0075, -0.0158],
        [-0.0163, -0.0309,  0.0021,  ...,  0.0036, -0.0067, -0.0105],
        [-0.0030,  0.0060, -0.0222,  ...,  0.0119, -0.0022, -0.0043],
        ...,
        [-0.0103, -0.0210, -0.0004,  ...,  0.0111, -0.0043, -0.0022],
        [ 0.0129,  0.0165, -0.0029,  ..., -0.0056, -0.0165,  0.0025],
        [-0.0031,  0.0113, -0.0020,  ..., -0.0095,  0.0085, -0.0226]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3457,  0.8281, -0.2693,  ...,  0.3337, -0.6641,  6.9414]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 08:28:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To achieve results in a achievement
To assess results in a assessment
To enroll results in a enrollment
To entitle results in a entitlement
To adjust results in a adjustment
To arrange results in a arrangement
To encourage results in a encouragement
To amuse results in a
2024-07-18 08:28:24 root INFO     [order_1_approx] starting weight calculation for To adjust results in a adjustment
To enroll results in a enrollment
To encourage results in a encouragement
To amuse results in a amusement
To entitle results in a entitlement
To achieve results in a achievement
To assess results in a assessment
To arrange results in a
2024-07-18 08:28:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 08:32:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5591,  0.2515, -0.0791,  ..., -0.0522,  0.1296, -0.1375],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.5508,  0.7344, -1.0059,  ...,  4.3711,  1.3145,  5.2070],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0015, -0.0261,  0.0058,  ...,  0.0113,  0.0132, -0.0220],
        [-0.0074, -0.0083, -0.0081,  ..., -0.0198, -0.0034,  0.0016],
        [-0.0372,  0.0031,  0.0031,  ...,  0.0221,  0.0307, -0.0044],
        ...,
        [-0.0119, -0.0113, -0.0264,  ...,  0.0099,  0.0179, -0.0006],
        [-0.0179, -0.0039, -0.0120,  ..., -0.0233, -0.0049,  0.0081],
        [-0.0087,  0.0133,  0.0023,  ...,  0.0036,  0.0296, -0.0139]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.9531,  0.4121,  0.2520,  ...,  3.8750,  1.2246,  6.2891]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 08:32:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To adjust results in a adjustment
To enroll results in a enrollment
To encourage results in a encouragement
To amuse results in a amusement
To entitle results in a entitlement
To achieve results in a achievement
To assess results in a assessment
To arrange results in a
2024-07-18 08:32:30 root INFO     total operator prediction time: 1979.7781360149384 seconds
2024-07-18 08:32:30 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on name - nationality
2024-07-18 08:32:30 root INFO     building operator name - nationality
2024-07-18 08:32:31 root INFO     [order_1_approx] starting weight calculation for aristotle was greek
marx was german
confucius was chinese
fermi was italian
strauss was austrian
raphael was italian
einstein was jewish
lincoln was
2024-07-18 08:32:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 08:36:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2588, -0.4062, -0.7930,  ...,  0.5039, -0.2651,  0.1626],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.9238,  0.1216, -5.1719,  ..., -0.2590, -1.7500, -2.6309],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0029, -0.0174, -0.0008,  ..., -0.0096,  0.0094,  0.0080],
        [ 0.0248,  0.0105,  0.0194,  ...,  0.0029, -0.0034, -0.0079],
        [ 0.0344,  0.0014,  0.0033,  ..., -0.0079,  0.0285,  0.0046],
        ...,
        [-0.0123,  0.0119,  0.0067,  ...,  0.0182,  0.0078,  0.0120],
        [-0.0105, -0.0211, -0.0139,  ..., -0.0015,  0.0018,  0.0206],
        [ 0.0432,  0.0033, -0.0011,  ..., -0.0037,  0.0215,  0.0107]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.5371,  0.1458, -5.6953,  ..., -0.3479, -1.5723, -2.8223]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 08:36:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for aristotle was greek
marx was german
confucius was chinese
fermi was italian
strauss was austrian
raphael was italian
einstein was jewish
lincoln was
2024-07-18 08:36:32 root INFO     [order_1_approx] starting weight calculation for marx was german
fermi was italian
raphael was italian
lincoln was american
aristotle was greek
strauss was austrian
einstein was jewish
confucius was
2024-07-18 08:36:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 08:40:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7139,  0.1431, -1.0977,  ..., -0.1864, -0.1066,  0.5112],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.2773, -4.1758, -9.1641,  ..., -1.9863, -0.4919, -5.2305],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0135, -0.0041,  0.0017,  ..., -0.0244,  0.0034,  0.0062],
        [ 0.0094, -0.0003,  0.0183,  ...,  0.0083, -0.0269, -0.0063],
        [ 0.0064, -0.0247, -0.0221,  ..., -0.0044,  0.0171, -0.0195],
        ...,
        [-0.0224,  0.0182,  0.0136,  ...,  0.0143, -0.0101,  0.0096],
        [-0.0053, -0.0033,  0.0031,  ..., -0.0059,  0.0037,  0.0174],
        [-0.0185, -0.0034,  0.0132,  ..., -0.0087,  0.0144,  0.0089]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1094, -4.1797, -9.3203,  ..., -1.4609, -1.2490, -5.5156]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 08:40:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for marx was german
fermi was italian
raphael was italian
lincoln was american
aristotle was greek
strauss was austrian
einstein was jewish
confucius was
2024-07-18 08:40:39 root INFO     [order_1_approx] starting weight calculation for lincoln was american
strauss was austrian
confucius was chinese
marx was german
raphael was italian
einstein was jewish
aristotle was greek
fermi was
2024-07-18 08:40:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 08:44:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.3806, 0.8516, 0.9346,  ..., 0.0518, 0.4177, 0.5078], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9385, -0.1084, -3.3555,  ..., -0.6484, -3.7461, -2.9980],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.5076e-02, -1.9951e-03,  1.5121e-02,  ..., -1.8372e-02,
          3.3760e-03, -1.3275e-02],
        [ 9.4147e-03,  1.4206e-02,  1.6083e-02,  ...,  2.3056e-02,
          1.7181e-02, -2.5604e-02],
        [-2.4033e-03,  1.1490e-02,  2.9297e-02,  ..., -3.3302e-03,
          1.9775e-02,  4.9133e-03],
        ...,
        [-2.9282e-02, -4.8065e-04, -2.8519e-02,  ..., -8.6212e-03,
         -6.2103e-03,  1.8402e-02],
        [ 1.4221e-02, -1.1826e-02, -5.4779e-03,  ...,  7.1487e-03,
          3.6469e-03,  6.4850e-05],
        [-1.7700e-02,  3.4008e-03, -6.2637e-03,  ..., -1.1215e-03,
         -1.3046e-02,  2.5665e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3418,  0.1229, -3.1758,  ..., -0.7529, -5.1719, -3.3047]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 08:44:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for lincoln was american
strauss was austrian
confucius was chinese
marx was german
raphael was italian
einstein was jewish
aristotle was greek
fermi was
2024-07-18 08:44:46 root INFO     [order_1_approx] starting weight calculation for aristotle was greek
einstein was jewish
confucius was chinese
fermi was italian
lincoln was american
raphael was italian
marx was german
strauss was
2024-07-18 08:44:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 08:48:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0971, -0.4456, -0.4441,  ...,  0.5654,  0.6997, -0.4456],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0957, -3.4121, -7.8203,  ..., -7.1719, -4.4922, -2.9609],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0070, -0.0173,  0.0105,  ..., -0.0066,  0.0081,  0.0047],
        [ 0.0062, -0.0021,  0.0109,  ...,  0.0184, -0.0062,  0.0016],
        [ 0.0171,  0.0073, -0.0086,  ...,  0.0070,  0.0177,  0.0006],
        ...,
        [-0.0104,  0.0127, -0.0012,  ...,  0.0154,  0.0011, -0.0043],
        [-0.0125, -0.0136, -0.0134,  ..., -0.0169, -0.0113,  0.0049],
        [ 0.0053, -0.0091,  0.0062,  ...,  0.0026, -0.0050,  0.0082]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4233, -3.5625, -7.4883,  ..., -7.4023, -5.0625, -3.7383]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 08:48:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for aristotle was greek
einstein was jewish
confucius was chinese
fermi was italian
lincoln was american
raphael was italian
marx was german
strauss was
2024-07-18 08:48:54 root INFO     [order_1_approx] starting weight calculation for confucius was chinese
lincoln was american
raphael was italian
fermi was italian
aristotle was greek
strauss was austrian
einstein was jewish
marx was
2024-07-18 08:48:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 08:53:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2155,  0.4319, -0.8521,  ..., -0.7412, -0.2183, -0.2241],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.6719, -1.4658, -8.7266,  ..., -6.5117, -4.7422,  0.7061],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0189, -0.0130,  0.0059,  ..., -0.0069,  0.0114, -0.0106],
        [ 0.0225,  0.0096,  0.0015,  ...,  0.0125,  0.0013, -0.0061],
        [ 0.0030, -0.0001,  0.0062,  ..., -0.0050, -0.0092,  0.0090],
        ...,
        [-0.0073, -0.0069,  0.0108,  ...,  0.0314,  0.0169,  0.0072],
        [-0.0016,  0.0013, -0.0030,  ..., -0.0143, -0.0084,  0.0043],
        [ 0.0038,  0.0059, -0.0058,  ...,  0.0197,  0.0215,  0.0235]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.5273, -2.1641, -9.5625,  ..., -5.4883, -5.0703, -0.5889]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 08:53:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for confucius was chinese
lincoln was american
raphael was italian
fermi was italian
aristotle was greek
strauss was austrian
einstein was jewish
marx was
2024-07-18 08:53:02 root INFO     [order_1_approx] starting weight calculation for lincoln was american
fermi was italian
strauss was austrian
raphael was italian
einstein was jewish
confucius was chinese
marx was german
aristotle was
2024-07-18 08:53:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 08:57:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3057, -0.4402, -0.2764,  ...,  0.7329, -0.1799,  1.1260],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3848, -1.5244, -2.6172,  ..., -0.1665, -6.1758, -3.8125],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0344, -0.0092,  0.0152,  ..., -0.0197, -0.0030, -0.0150],
        [ 0.0016,  0.0234,  0.0105,  ...,  0.0235,  0.0030,  0.0210],
        [-0.0026, -0.0017,  0.0228,  ..., -0.0108,  0.0122, -0.0184],
        ...,
        [ 0.0001,  0.0110,  0.0151,  ...,  0.0099, -0.0096,  0.0130],
        [ 0.0039,  0.0037, -0.0099,  ...,  0.0125, -0.0040, -0.0106],
        [-0.0068, -0.0187, -0.0072,  ..., -0.0045, -0.0036, -0.0214]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6133, -1.8096, -3.6621,  ...,  0.5156, -6.2227, -3.8418]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 08:57:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for lincoln was american
fermi was italian
strauss was austrian
raphael was italian
einstein was jewish
confucius was chinese
marx was german
aristotle was
2024-07-18 08:57:11 root INFO     [order_1_approx] starting weight calculation for strauss was austrian
lincoln was american
marx was german
confucius was chinese
fermi was italian
aristotle was greek
einstein was jewish
raphael was
2024-07-18 08:57:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 09:01:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.6377, -0.8252, -0.3928,  ...,  1.2314,  0.9800,  0.5005],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5010, -2.9707, -5.9141,  ..., -0.3965, -8.2109,  0.0928],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.4368e-02, -2.0691e-02, -3.2082e-03,  ..., -1.3519e-02,
         -1.9226e-03, -7.9956e-03],
        [ 7.8125e-03, -3.1242e-03,  1.1055e-02,  ...,  9.7809e-03,
         -1.4061e-02, -1.7166e-02],
        [-6.7444e-03, -1.1444e-03,  2.3365e-03,  ..., -5.4054e-03,
          1.6232e-03, -1.0376e-03],
        ...,
        [-6.1455e-03,  5.2071e-03, -5.0163e-03,  ...,  1.5213e-02,
         -1.3466e-03,  6.2943e-04],
        [-1.3000e-02, -2.4078e-02, -1.1642e-02,  ..., -1.5038e-02,
          2.6627e-03,  2.6199e-02],
        [-1.3702e-02, -5.6686e-03,  1.0941e-02,  ...,  7.3242e-03,
          7.6294e-06, -6.6681e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1140, -3.3281, -6.7188,  ..., -1.0098, -8.6328, -0.3013]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 09:01:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for strauss was austrian
lincoln was american
marx was german
confucius was chinese
fermi was italian
aristotle was greek
einstein was jewish
raphael was
2024-07-18 09:01:19 root INFO     [order_1_approx] starting weight calculation for confucius was chinese
raphael was italian
fermi was italian
marx was german
lincoln was american
strauss was austrian
aristotle was greek
einstein was
2024-07-18 09:01:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 09:05:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1221,  0.6523,  0.6045,  ...,  0.4304, -0.2297,  1.1201],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7344, -2.9043, -4.0898,  ..., -1.2061, -2.7754, -3.4336],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0076, -0.0267,  0.0084,  ...,  0.0041,  0.0171, -0.0010],
        [ 0.0142,  0.0117,  0.0014,  ...,  0.0097, -0.0042, -0.0042],
        [ 0.0309,  0.0066, -0.0110,  ...,  0.0019, -0.0110, -0.0167],
        ...,
        [-0.0215,  0.0142,  0.0065,  ...,  0.0004,  0.0013,  0.0114],
        [ 0.0097, -0.0131, -0.0009,  ...,  0.0079,  0.0158,  0.0079],
        [-0.0053,  0.0049,  0.0200,  ..., -0.0089,  0.0040,  0.0243]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2061, -2.5195, -5.1602,  ..., -0.2544, -2.6230, -5.6016]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 09:05:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for confucius was chinese
raphael was italian
fermi was italian
marx was german
lincoln was american
strauss was austrian
aristotle was greek
einstein was
2024-07-18 09:05:27 root INFO     total operator prediction time: 1976.3587620258331 seconds
2024-07-18 09:05:27 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on country - language
2024-07-18 09:05:27 root INFO     building operator country - language
2024-07-18 09:05:27 root INFO     [order_1_approx] starting weight calculation for The country of argentina primarily speaks the language of spanish
The country of fiji primarily speaks the language of english
The country of kosovo primarily speaks the language of albanian
The country of venezuela primarily speaks the language of spanish
The country of brazil primarily speaks the language of portuguese
The country of netherlands primarily speaks the language of dutch
The country of usa primarily speaks the language of english
The country of mozambique primarily speaks the language of
2024-07-18 09:05:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 09:09:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0938, -0.9502, -0.2291,  ..., -0.0728,  1.4766, -0.1641],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.4668, -5.9023, -7.8672,  ...,  1.1270, -0.9194, -8.6406],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0062, -0.0294, -0.0019,  ..., -0.0051,  0.0014, -0.0047],
        [ 0.0247, -0.0074, -0.0013,  ...,  0.0032,  0.0090, -0.0018],
        [ 0.0035,  0.0068, -0.0113,  ..., -0.0123, -0.0143, -0.0106],
        ...,
        [-0.0085, -0.0055,  0.0112,  ...,  0.0019, -0.0123, -0.0056],
        [ 0.0166, -0.0127,  0.0036,  ...,  0.0162,  0.0103,  0.0004],
        [ 0.0142, -0.0092,  0.0184,  ...,  0.0104,  0.0255,  0.0019]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6523, -5.8477, -7.0625,  ...,  0.1348,  0.1733, -6.8672]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 09:09:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of argentina primarily speaks the language of spanish
The country of fiji primarily speaks the language of english
The country of kosovo primarily speaks the language of albanian
The country of venezuela primarily speaks the language of spanish
The country of brazil primarily speaks the language of portuguese
The country of netherlands primarily speaks the language of dutch
The country of usa primarily speaks the language of english
The country of mozambique primarily speaks the language of
2024-07-18 09:09:36 root INFO     [order_1_approx] starting weight calculation for The country of mozambique primarily speaks the language of portuguese
The country of netherlands primarily speaks the language of dutch
The country of venezuela primarily speaks the language of spanish
The country of brazil primarily speaks the language of portuguese
The country of kosovo primarily speaks the language of albanian
The country of argentina primarily speaks the language of spanish
The country of fiji primarily speaks the language of english
The country of usa primarily speaks the language of
2024-07-18 09:09:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 09:13:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1105, -0.0519,  0.0790,  ...,  0.3855,  0.0698, -0.0111],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4766, -3.1680, -1.2314,  ..., -2.4453,  3.9727, -1.2666],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0011,  0.0026, -0.0093,  ..., -0.0018,  0.0039, -0.0086],
        [ 0.0075,  0.0097, -0.0019,  ...,  0.0135, -0.0003, -0.0063],
        [-0.0066,  0.0004, -0.0054,  ..., -0.0020, -0.0060, -0.0041],
        ...,
        [-0.0066, -0.0063,  0.0087,  ...,  0.0014, -0.0085,  0.0014],
        [ 0.0012,  0.0023, -0.0039,  ...,  0.0094, -0.0009,  0.0022],
        [-0.0001, -0.0084,  0.0001,  ...,  0.0053, -0.0011,  0.0014]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9062, -3.1797, -1.5088,  ..., -2.7891,  3.3887, -1.2109]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 09:13:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of mozambique primarily speaks the language of portuguese
The country of netherlands primarily speaks the language of dutch
The country of venezuela primarily speaks the language of spanish
The country of brazil primarily speaks the language of portuguese
The country of kosovo primarily speaks the language of albanian
The country of argentina primarily speaks the language of spanish
The country of fiji primarily speaks the language of english
The country of usa primarily speaks the language of
2024-07-18 09:13:46 root INFO     [order_1_approx] starting weight calculation for The country of fiji primarily speaks the language of english
The country of kosovo primarily speaks the language of albanian
The country of netherlands primarily speaks the language of dutch
The country of venezuela primarily speaks the language of spanish
The country of usa primarily speaks the language of english
The country of mozambique primarily speaks the language of portuguese
The country of argentina primarily speaks the language of spanish
The country of brazil primarily speaks the language of
2024-07-18 09:13:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 09:17:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7656,  0.2443, -0.4299,  ..., -0.3459,  0.3701, -1.0850],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.6172, -3.3125, -6.4219,  ..., -2.4746,  0.1555, -3.2988],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0035, -0.0160,  0.0073,  ..., -0.0112, -0.0182, -0.0053],
        [ 0.0016,  0.0154, -0.0053,  ...,  0.0025,  0.0043, -0.0106],
        [-0.0272, -0.0096,  0.0137,  ..., -0.0027, -0.0199, -0.0269],
        ...,
        [-0.0164, -0.0269,  0.0216,  ...,  0.0048, -0.0179, -0.0402],
        [-0.0047, -0.0009,  0.0155,  ..., -0.0141, -0.0213, -0.0259],
        [-0.0284, -0.0176,  0.0226,  ..., -0.0033, -0.0122, -0.0298]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.1250, -3.3398, -5.7578,  ..., -0.7422,  0.2676, -2.9375]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 09:17:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of fiji primarily speaks the language of english
The country of kosovo primarily speaks the language of albanian
The country of netherlands primarily speaks the language of dutch
The country of venezuela primarily speaks the language of spanish
The country of usa primarily speaks the language of english
The country of mozambique primarily speaks the language of portuguese
The country of argentina primarily speaks the language of spanish
The country of brazil primarily speaks the language of
2024-07-18 09:17:54 root INFO     [order_1_approx] starting weight calculation for The country of fiji primarily speaks the language of english
The country of mozambique primarily speaks the language of portuguese
The country of venezuela primarily speaks the language of spanish
The country of usa primarily speaks the language of english
The country of brazil primarily speaks the language of portuguese
The country of kosovo primarily speaks the language of albanian
The country of argentina primarily speaks the language of spanish
The country of netherlands primarily speaks the language of
2024-07-18 09:17:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 09:22:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3821, -0.3875, -0.6597,  ..., -0.0081, -0.6914, -0.2131],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6899, -3.7266, -1.1982,  ..., -4.8008, -0.5708,  0.5234],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 5.1804e-03,  2.1935e-05, -4.6883e-03,  ..., -2.1591e-03,
          2.0828e-03, -4.5052e-03],
        [ 7.8735e-03,  3.5858e-03,  9.5978e-03,  ...,  2.1912e-02,
          1.1276e-02,  5.6877e-03],
        [-2.2873e-02, -2.6093e-03, -1.4221e-02,  ...,  7.1955e-04,
         -1.4534e-02,  7.7820e-03],
        ...,
        [ 1.6384e-03, -5.6000e-03,  4.1656e-03,  ...,  7.3929e-03,
         -1.1856e-02,  2.9297e-03],
        [ 1.2024e-02,  1.5640e-03,  2.6398e-03,  ..., -2.2644e-02,
         -3.4771e-03,  9.5291e-03],
        [-3.9368e-03,  7.3471e-03,  5.3101e-03,  ...,  2.4414e-04,
          4.3983e-03,  1.4198e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5469, -3.5938, -1.7207,  ..., -4.8086, -1.2949, -0.1021]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 09:22:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of fiji primarily speaks the language of english
The country of mozambique primarily speaks the language of portuguese
The country of venezuela primarily speaks the language of spanish
The country of usa primarily speaks the language of english
The country of brazil primarily speaks the language of portuguese
The country of kosovo primarily speaks the language of albanian
The country of argentina primarily speaks the language of spanish
The country of netherlands primarily speaks the language of
2024-07-18 09:22:02 root INFO     [order_1_approx] starting weight calculation for The country of netherlands primarily speaks the language of dutch
The country of usa primarily speaks the language of english
The country of mozambique primarily speaks the language of portuguese
The country of venezuela primarily speaks the language of spanish
The country of kosovo primarily speaks the language of albanian
The country of argentina primarily speaks the language of spanish
The country of brazil primarily speaks the language of portuguese
The country of fiji primarily speaks the language of
2024-07-18 09:22:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 09:26:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4380,  0.0953, -1.4043,  ..., -0.0776, -0.6460,  0.7061],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.5918, -5.5664, -5.9922,  ..., -0.8789,  1.6445, -1.0674],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0163, -0.0231,  0.0037,  ..., -0.0016,  0.0077, -0.0063],
        [-0.0128,  0.0162,  0.0314,  ...,  0.0138,  0.0154, -0.0064],
        [ 0.0064,  0.0115, -0.0202,  ..., -0.0018, -0.0073, -0.0041],
        ...,
        [ 0.0015, -0.0142, -0.0054,  ...,  0.0040, -0.0084, -0.0054],
        [ 0.0038, -0.0027, -0.0137,  ..., -0.0059, -0.0073, -0.0094],
        [ 0.0160, -0.0026, -0.0247,  ..., -0.0040, -0.0053,  0.0052]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.3867, -6.0312, -5.2422,  ..., -1.2539,  3.1133,  0.6982]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 09:26:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of netherlands primarily speaks the language of dutch
The country of usa primarily speaks the language of english
The country of mozambique primarily speaks the language of portuguese
The country of venezuela primarily speaks the language of spanish
The country of kosovo primarily speaks the language of albanian
The country of argentina primarily speaks the language of spanish
The country of brazil primarily speaks the language of portuguese
The country of fiji primarily speaks the language of
2024-07-18 09:26:11 root INFO     [order_1_approx] starting weight calculation for The country of usa primarily speaks the language of english
The country of netherlands primarily speaks the language of dutch
The country of kosovo primarily speaks the language of albanian
The country of fiji primarily speaks the language of english
The country of argentina primarily speaks the language of spanish
The country of brazil primarily speaks the language of portuguese
The country of mozambique primarily speaks the language of portuguese
The country of venezuela primarily speaks the language of
2024-07-18 09:26:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 09:30:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2328, -0.1379, -0.8164,  ..., -0.4048,  0.1378, -0.9009],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5986, -4.1133, -3.8906,  ..., -2.8125,  0.2881, -0.8525],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.6880e-03, -1.0597e-02, -1.3573e-02,  ..., -9.7198e-03,
         -7.4387e-05, -5.4741e-04],
        [ 2.8763e-03,  3.2005e-03, -3.2997e-04,  ...,  9.9945e-03,
          9.9411e-03, -5.7487e-03],
        [-5.7106e-03, -3.3188e-04, -1.1749e-02,  ..., -3.4943e-03,
         -3.0804e-03,  2.2316e-03],
        ...,
        [-1.0109e-02, -6.0883e-03,  6.5994e-04,  ..., -1.5678e-03,
         -9.0637e-03,  1.2665e-03],
        [ 8.0414e-03, -7.6332e-03, -1.3027e-03,  ..., -7.7438e-03,
          4.0588e-03, -8.7738e-04],
        [ 2.1019e-03, -1.0269e-02,  1.3290e-02,  ...,  8.9836e-04,
          1.3748e-02, -4.5700e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6284, -4.3906, -3.9531,  ..., -2.3086,  0.6631, -0.7993]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 09:30:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of usa primarily speaks the language of english
The country of netherlands primarily speaks the language of dutch
The country of kosovo primarily speaks the language of albanian
The country of fiji primarily speaks the language of english
The country of argentina primarily speaks the language of spanish
The country of brazil primarily speaks the language of portuguese
The country of mozambique primarily speaks the language of portuguese
The country of venezuela primarily speaks the language of
2024-07-18 09:30:20 root INFO     [order_1_approx] starting weight calculation for The country of netherlands primarily speaks the language of dutch
The country of kosovo primarily speaks the language of albanian
The country of mozambique primarily speaks the language of portuguese
The country of venezuela primarily speaks the language of spanish
The country of brazil primarily speaks the language of portuguese
The country of fiji primarily speaks the language of english
The country of usa primarily speaks the language of english
The country of argentina primarily speaks the language of
2024-07-18 09:30:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 09:34:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1123, -0.2045,  0.4536,  ...,  0.4141,  1.1094, -0.4226],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2520, -2.3633, -5.0117,  ..., -1.9180, -0.3757, -5.9883],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0017, -0.0348,  0.0002,  ..., -0.0204, -0.0078, -0.0149],
        [-0.0093,  0.0233, -0.0097,  ...,  0.0152,  0.0208, -0.0105],
        [ 0.0142, -0.0013, -0.0219,  ..., -0.0159, -0.0123, -0.0171],
        ...,
        [-0.0001, -0.0121,  0.0177,  ..., -0.0101, -0.0169, -0.0092],
        [ 0.0164, -0.0200, -0.0114,  ..., -0.0141, -0.0143, -0.0168],
        [ 0.0030, -0.0213,  0.0004,  ..., -0.0248, -0.0122, -0.0225]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9805, -2.8516, -4.4102,  ..., -1.1973,  0.0530, -5.3125]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 09:34:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of netherlands primarily speaks the language of dutch
The country of kosovo primarily speaks the language of albanian
The country of mozambique primarily speaks the language of portuguese
The country of venezuela primarily speaks the language of spanish
The country of brazil primarily speaks the language of portuguese
The country of fiji primarily speaks the language of english
The country of usa primarily speaks the language of english
The country of argentina primarily speaks the language of
2024-07-18 09:34:29 root INFO     [order_1_approx] starting weight calculation for The country of mozambique primarily speaks the language of portuguese
The country of usa primarily speaks the language of english
The country of argentina primarily speaks the language of spanish
The country of brazil primarily speaks the language of portuguese
The country of fiji primarily speaks the language of english
The country of venezuela primarily speaks the language of spanish
The country of netherlands primarily speaks the language of dutch
The country of kosovo primarily speaks the language of
2024-07-18 09:34:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 09:38:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.2256, 0.3633, 0.3389,  ..., 0.2281, 0.5903, 0.7490], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3213, -6.0391, -2.3086,  ..., -0.0723, -3.0625, -6.2188],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0095, -0.0006,  0.0131,  ..., -0.0086, -0.0254, -0.0021],
        [ 0.0163, -0.0007,  0.0151,  ...,  0.0029, -0.0021, -0.0069],
        [ 0.0035,  0.0140, -0.0026,  ..., -0.0067, -0.0197,  0.0086],
        ...,
        [-0.0060, -0.0073,  0.0053,  ..., -0.0031, -0.0121,  0.0005],
        [ 0.0009,  0.0124,  0.0074,  ..., -0.0005,  0.0027,  0.0090],
        [ 0.0027,  0.0112,  0.0165,  ..., -0.0013,  0.0184,  0.0057]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5029, -5.6992, -2.3359,  ..., -0.1785, -3.1621, -5.1758]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 09:38:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of mozambique primarily speaks the language of portuguese
The country of usa primarily speaks the language of english
The country of argentina primarily speaks the language of spanish
The country of brazil primarily speaks the language of portuguese
The country of fiji primarily speaks the language of english
The country of venezuela primarily speaks the language of spanish
The country of netherlands primarily speaks the language of dutch
The country of kosovo primarily speaks the language of
2024-07-18 09:38:37 root INFO     total operator prediction time: 1990.3438637256622 seconds
2024-07-18 09:38:37 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on animal - shelter
2024-07-18 09:38:37 root INFO     building operator animal - shelter
2024-07-18 09:38:37 root INFO     [order_1_approx] starting weight calculation for The place rabbit lives in is called burrow
The place fly lives in is called nest
The place dog lives in is called doghouse
The place raven lives in is called nest
The place herring lives in is called sea
The place trout lives in is called river
The place mouse lives in is called nest
The place fish lives in is called
2024-07-18 09:38:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 09:42:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3831,  0.1213, -0.2964,  ...,  0.2585, -0.0861, -0.2299],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3369,  0.4072, -4.5312,  ..., -1.1289,  1.1240, -1.6670],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0082, -0.0002, -0.0012,  ..., -0.0006,  0.0072,  0.0024],
        [-0.0136, -0.0101, -0.0063,  ...,  0.0094, -0.0168, -0.0103],
        [ 0.0145,  0.0075,  0.0070,  ..., -0.0148,  0.0151,  0.0126],
        ...,
        [-0.0102, -0.0168,  0.0019,  ...,  0.0094, -0.0264,  0.0135],
        [-0.0085, -0.0149,  0.0071,  ..., -0.0034,  0.0071, -0.0012],
        [ 0.0024, -0.0093, -0.0006,  ..., -0.0071,  0.0029,  0.0082]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3242, -0.1836, -4.9922,  ...,  0.1348,  1.4600, -2.3516]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 09:42:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place rabbit lives in is called burrow
The place fly lives in is called nest
The place dog lives in is called doghouse
The place raven lives in is called nest
The place herring lives in is called sea
The place trout lives in is called river
The place mouse lives in is called nest
The place fish lives in is called
2024-07-18 09:42:43 root INFO     [order_1_approx] starting weight calculation for The place fly lives in is called nest
The place rabbit lives in is called burrow
The place fish lives in is called sea
The place herring lives in is called sea
The place mouse lives in is called nest
The place dog lives in is called doghouse
The place trout lives in is called river
The place raven lives in is called
2024-07-18 09:42:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 09:46:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4629, -1.0254, -0.8765,  ...,  0.1359, -0.4121, -0.4858],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.2422, -0.3140,  0.0171,  ..., -0.9253,  0.8403, -0.3203],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0011, -0.0330, -0.0136,  ...,  0.0142,  0.0144, -0.0102],
        [ 0.0054,  0.0277,  0.0025,  ..., -0.0079,  0.0063,  0.0096],
        [-0.0011,  0.0194,  0.0059,  ...,  0.0301,  0.0046, -0.0143],
        ...,
        [-0.0083, -0.0096, -0.0032,  ...,  0.0145,  0.0000,  0.0223],
        [-0.0089, -0.0118, -0.0026,  ..., -0.0005, -0.0003, -0.0090],
        [-0.0122, -0.0017,  0.0039,  ..., -0.0057, -0.0077,  0.0074]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.9297, -0.9214,  1.8574,  ..., -0.3936,  1.1523,  0.4653]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 09:46:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place fly lives in is called nest
The place rabbit lives in is called burrow
The place fish lives in is called sea
The place herring lives in is called sea
The place mouse lives in is called nest
The place dog lives in is called doghouse
The place trout lives in is called river
The place raven lives in is called
2024-07-18 09:46:53 root INFO     [order_1_approx] starting weight calculation for The place trout lives in is called river
The place mouse lives in is called nest
The place herring lives in is called sea
The place fish lives in is called sea
The place raven lives in is called nest
The place fly lives in is called nest
The place rabbit lives in is called burrow
The place dog lives in is called
2024-07-18 09:46:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 09:51:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3579, -0.2246, -0.7749,  ...,  0.5586, -0.7500,  0.5449],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1602, -5.9961, -0.4458,  ..., -6.9844,  3.7402,  2.8203],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0053, -0.0052, -0.0011,  ...,  0.0130, -0.0035,  0.0166],
        [ 0.0132,  0.0115,  0.0022,  ...,  0.0167, -0.0022, -0.0139],
        [-0.0064,  0.0228,  0.0031,  ...,  0.0089,  0.0178, -0.0133],
        ...,
        [-0.0029, -0.0023,  0.0241,  ...,  0.0084,  0.0035,  0.0045],
        [-0.0093,  0.0104, -0.0003,  ..., -0.0202, -0.0016, -0.0096],
        [-0.0112, -0.0102,  0.0113,  ..., -0.0102,  0.0003, -0.0024]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4722, -5.5195,  0.2563,  ..., -6.5664,  3.7637,  2.8633]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 09:51:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place trout lives in is called river
The place mouse lives in is called nest
The place herring lives in is called sea
The place fish lives in is called sea
The place raven lives in is called nest
The place fly lives in is called nest
The place rabbit lives in is called burrow
The place dog lives in is called
2024-07-18 09:51:03 root INFO     [order_1_approx] starting weight calculation for The place mouse lives in is called nest
The place herring lives in is called sea
The place fish lives in is called sea
The place fly lives in is called nest
The place trout lives in is called river
The place raven lives in is called nest
The place dog lives in is called doghouse
The place rabbit lives in is called
2024-07-18 09:51:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 09:55:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4407, -0.0278, -0.9854,  ...,  0.2849, -1.1055, -0.1926],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2720,  3.2266,  0.5918,  ..., -4.6055,  0.5322, -0.4062],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0013, -0.0257, -0.0170,  ...,  0.0154, -0.0105,  0.0218],
        [-0.0070,  0.0013,  0.0037,  ...,  0.0345,  0.0054,  0.0252],
        [-0.0047,  0.0256,  0.0072,  ...,  0.0062,  0.0056, -0.0144],
        ...,
        [ 0.0114, -0.0111, -0.0009,  ..., -0.0076, -0.0196,  0.0057],
        [-0.0013, -0.0052,  0.0087,  ..., -0.0216,  0.0107,  0.0063],
        [-0.0177, -0.0050, -0.0143,  ..., -0.0065,  0.0193, -0.0094]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2065,  2.2520,  1.3633,  ..., -4.2188,  0.1919, -0.1632]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 09:55:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place mouse lives in is called nest
The place herring lives in is called sea
The place fish lives in is called sea
The place fly lives in is called nest
The place trout lives in is called river
The place raven lives in is called nest
The place dog lives in is called doghouse
The place rabbit lives in is called
2024-07-18 09:55:11 root INFO     [order_1_approx] starting weight calculation for The place raven lives in is called nest
The place rabbit lives in is called burrow
The place mouse lives in is called nest
The place fish lives in is called sea
The place fly lives in is called nest
The place dog lives in is called doghouse
The place trout lives in is called river
The place herring lives in is called
2024-07-18 09:55:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 09:59:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5479,  0.5684,  0.1272,  ...,  0.6055, -0.5078,  0.4141],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.4688,  2.9219, -0.3677,  ..., -1.0215,  0.0117, -2.9551],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0053, -0.0118, -0.0025,  ...,  0.0123,  0.0038, -0.0049],
        [-0.0167, -0.0007,  0.0050,  ...,  0.0062, -0.0176,  0.0108],
        [ 0.0040, -0.0005,  0.0092,  ...,  0.0185,  0.0202, -0.0004],
        ...,
        [ 0.0170, -0.0154, -0.0177,  ...,  0.0064, -0.0204, -0.0009],
        [ 0.0055, -0.0164,  0.0064,  ..., -0.0093,  0.0170, -0.0094],
        [ 0.0148, -0.0037, -0.0152,  ..., -0.0106,  0.0233, -0.0189]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.3672,  2.3555, -0.6123,  ..., -0.2139,  0.1615, -2.4062]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 09:59:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place raven lives in is called nest
The place rabbit lives in is called burrow
The place mouse lives in is called nest
The place fish lives in is called sea
The place fly lives in is called nest
The place dog lives in is called doghouse
The place trout lives in is called river
The place herring lives in is called
2024-07-18 09:59:19 root INFO     [order_1_approx] starting weight calculation for The place raven lives in is called nest
The place dog lives in is called doghouse
The place trout lives in is called river
The place rabbit lives in is called burrow
The place fish lives in is called sea
The place herring lives in is called sea
The place fly lives in is called nest
The place mouse lives in is called
2024-07-18 09:59:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 10:03:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.7246,  0.7173, -0.6836,  ...,  0.3018, -0.6421,  0.6338],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0996, -1.5898, -0.7061,  ..., -6.7969, -3.3066, -0.2288],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-7.1564e-03, -7.8964e-04, -9.5215e-03,  ...,  5.2986e-03,
          1.0185e-03,  1.1589e-02],
        [ 1.1597e-02, -1.0986e-02,  3.3875e-03,  ...,  1.5732e-02,
         -2.4780e-02, -4.2229e-03],
        [-6.6910e-03,  3.1616e-02, -2.9755e-04,  ...,  2.6031e-02,
          1.4259e-02, -2.6978e-02],
        ...,
        [-8.2016e-05, -7.3814e-03,  1.5579e-02,  ..., -1.1292e-02,
         -6.4774e-03, -1.0490e-02],
        [ 1.0399e-02,  1.6815e-02,  1.6235e-02,  ..., -1.8799e-02,
          7.1945e-03, -2.9266e-02],
        [-1.0605e-02, -1.8188e-02, -3.6869e-03,  ..., -9.5901e-03,
          1.2115e-02, -1.9836e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.0498e-01, -8.6377e-01,  1.1279e+00,  ..., -5.8438e+00,
         -2.3398e+00, -5.0049e-03]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SubBackward0>) 

                    
2024-07-18 10:03:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place raven lives in is called nest
The place dog lives in is called doghouse
The place trout lives in is called river
The place rabbit lives in is called burrow
The place fish lives in is called sea
The place herring lives in is called sea
The place fly lives in is called nest
The place mouse lives in is called
2024-07-18 10:03:28 root INFO     [order_1_approx] starting weight calculation for The place trout lives in is called river
The place herring lives in is called sea
The place raven lives in is called nest
The place dog lives in is called doghouse
The place fish lives in is called sea
The place rabbit lives in is called burrow
The place mouse lives in is called nest
The place fly lives in is called
2024-07-18 10:03:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 10:07:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1289,  0.3127, -0.5000,  ...,  0.0441, -0.1082,  0.9814],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1387,  1.0430,  1.7070,  ..., -1.9824, -2.3086, -2.8555],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 7.2250e-03, -1.5726e-03, -3.0632e-03,  ...,  1.2711e-02,
         -1.2138e-02,  1.5594e-02],
        [-9.9640e-03,  7.3395e-03, -6.9466e-03,  ..., -1.9455e-04,
         -7.1259e-03,  1.8997e-02],
        [-1.3800e-03, -7.6675e-04,  1.4549e-02,  ..., -2.7924e-03,
          1.1917e-02,  1.8051e-02],
        ...,
        [ 1.4744e-03, -3.5572e-03, -5.2795e-03,  ...,  9.0408e-03,
         -5.0583e-03,  2.3300e-02],
        [ 3.2349e-03, -1.6998e-02,  1.5259e-05,  ...,  5.4855e-03,
          3.5477e-03, -5.6190e-03],
        [-1.0651e-02, -1.0422e-02, -2.7313e-03,  ...,  4.3678e-03,
          6.6757e-04, -1.3466e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0233,  1.8262,  2.5254,  ..., -1.1641, -1.4512, -3.4531]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 10:07:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place trout lives in is called river
The place herring lives in is called sea
The place raven lives in is called nest
The place dog lives in is called doghouse
The place fish lives in is called sea
The place rabbit lives in is called burrow
The place mouse lives in is called nest
The place fly lives in is called
2024-07-18 10:07:37 root INFO     [order_1_approx] starting weight calculation for The place herring lives in is called sea
The place fish lives in is called sea
The place dog lives in is called doghouse
The place fly lives in is called nest
The place raven lives in is called nest
The place mouse lives in is called nest
The place rabbit lives in is called burrow
The place trout lives in is called
2024-07-18 10:07:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 10:11:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4111,  0.7788, -0.4585,  ...,  0.1583, -0.5005, -0.0254],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3379,  2.8555, -2.6855,  ...,  1.3643, -0.0945,  0.0479],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0055, -0.0059, -0.0073,  ...,  0.0028, -0.0087,  0.0031],
        [-0.0089, -0.0067,  0.0133,  ..., -0.0097, -0.0081,  0.0109],
        [ 0.0058,  0.0272,  0.0066,  ...,  0.0074,  0.0112,  0.0173],
        ...,
        [-0.0044, -0.0216, -0.0011,  ..., -0.0138, -0.0163, -0.0112],
        [ 0.0015,  0.0073,  0.0106,  ..., -0.0049,  0.0039, -0.0086],
        [-0.0027,  0.0020,  0.0085,  ..., -0.0032,  0.0104,  0.0098]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5049,  2.2539, -2.7832,  ...,  0.9199,  0.4250, -0.6162]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 10:11:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place herring lives in is called sea
The place fish lives in is called sea
The place dog lives in is called doghouse
The place fly lives in is called nest
The place raven lives in is called nest
The place mouse lives in is called nest
The place rabbit lives in is called burrow
The place trout lives in is called
2024-07-18 10:11:45 root INFO     total operator prediction time: 1988.0828399658203 seconds
2024-07-18 10:11:45 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on male - female
2024-07-18 10:11:45 root INFO     building operator male - female
2024-07-18 10:11:45 root INFO     [order_1_approx] starting weight calculation for A female manager is known as a manageress
A female buck is known as a doe
A female dad is known as a mom
A female grandson is known as a granddaughter
A female emperor is known as a empress
A female boar is known as a sow
A female murderer is known as a murderess
A female daddy is known as a
2024-07-18 10:11:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 10:15:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.1482, 0.2773, 0.5625,  ..., 0.6147, 0.2180, 0.3066], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.3398, -8.0703, -2.8125,  ..., -2.8320,  6.8047, -1.5801],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0576, -0.0036,  0.0240,  ...,  0.0164, -0.0057, -0.0123],
        [ 0.0322, -0.0187, -0.0305,  ..., -0.0357,  0.0214,  0.0027],
        [ 0.0025, -0.0080, -0.0187,  ..., -0.0290, -0.0083, -0.0076],
        ...,
        [-0.0156, -0.0215,  0.0116,  ..., -0.0141, -0.0090, -0.0097],
        [ 0.0018,  0.0103,  0.0136,  ...,  0.0125, -0.0085, -0.0044],
        [-0.0271, -0.0056,  0.0028,  ...,  0.0008,  0.0090, -0.0171]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.4180, -7.8398, -3.3027,  ..., -3.2461,  7.6914, -1.7178]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 10:15:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female manager is known as a manageress
A female buck is known as a doe
A female dad is known as a mom
A female grandson is known as a granddaughter
A female emperor is known as a empress
A female boar is known as a sow
A female murderer is known as a murderess
A female daddy is known as a
2024-07-18 10:15:53 root INFO     [order_1_approx] starting weight calculation for A female daddy is known as a mommy
A female dad is known as a mom
A female grandson is known as a granddaughter
A female emperor is known as a empress
A female buck is known as a doe
A female murderer is known as a murderess
A female boar is known as a sow
A female manager is known as a
2024-07-18 10:15:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 10:20:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4875,  0.3145,  0.2043,  ...,  0.5020, -0.2969,  0.4114],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6719, -6.0977, -1.6172,  ..., -2.4551,  1.6934,  1.5488],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0019, -0.0119,  0.0113,  ...,  0.0055,  0.0030,  0.0002],
        [-0.0008, -0.0038, -0.0079,  ...,  0.0056,  0.0007, -0.0059],
        [ 0.0024, -0.0120,  0.0178,  ...,  0.0208, -0.0018, -0.0236],
        ...,
        [-0.0107, -0.0029,  0.0045,  ...,  0.0056,  0.0009, -0.0138],
        [ 0.0189,  0.0087,  0.0018,  ..., -0.0146, -0.0104,  0.0116],
        [ 0.0063,  0.0091,  0.0012,  ...,  0.0054,  0.0098, -0.0071]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3438, -4.9141, -1.4316,  ..., -1.9746,  2.0859,  1.4062]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 10:20:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female daddy is known as a mommy
A female dad is known as a mom
A female grandson is known as a granddaughter
A female emperor is known as a empress
A female buck is known as a doe
A female murderer is known as a murderess
A female boar is known as a sow
A female manager is known as a
2024-07-18 10:20:03 root INFO     [order_1_approx] starting weight calculation for A female manager is known as a manageress
A female boar is known as a sow
A female murderer is known as a murderess
A female grandson is known as a granddaughter
A female buck is known as a doe
A female dad is known as a mom
A female daddy is known as a mommy
A female emperor is known as a
2024-07-18 10:20:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 10:24:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1885, -0.1964, -0.0316,  ..., -0.0786,  0.2207,  0.1033],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7949, -2.0547, -3.7500,  ..., -1.9453, -2.2441, -4.3828],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0144, -0.0032, -0.0196,  ...,  0.0221,  0.0059, -0.0022],
        [ 0.0186,  0.0036, -0.0066,  ..., -0.0153,  0.0007,  0.0045],
        [ 0.0240, -0.0067,  0.0182,  ...,  0.0053,  0.0090, -0.0219],
        ...,
        [-0.0242,  0.0139, -0.0110,  ...,  0.0078, -0.0210, -0.0093],
        [ 0.0116,  0.0008,  0.0144,  ...,  0.0039, -0.0175, -0.0095],
        [ 0.0150,  0.0064,  0.0094,  ...,  0.0071,  0.0016, -0.0203]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5859, -3.0195, -2.7578,  ..., -2.5117, -2.7930, -5.2227]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 10:24:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female manager is known as a manageress
A female boar is known as a sow
A female murderer is known as a murderess
A female grandson is known as a granddaughter
A female buck is known as a doe
A female dad is known as a mom
A female daddy is known as a mommy
A female emperor is known as a
2024-07-18 10:24:11 root INFO     [order_1_approx] starting weight calculation for A female buck is known as a doe
A female daddy is known as a mommy
A female grandson is known as a granddaughter
A female murderer is known as a murderess
A female emperor is known as a empress
A female manager is known as a manageress
A female boar is known as a sow
A female dad is known as a
2024-07-18 10:24:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 10:28:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.6450, 0.7241, 0.1733,  ..., 0.4443, 0.2441, 0.2891], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.6719, -8.6719, -3.7441,  ..., -3.6113,  6.0391, -0.6406],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0418, -0.0006,  0.0290,  ...,  0.0176, -0.0234,  0.0147],
        [ 0.0139, -0.0037,  0.0294,  ..., -0.0244,  0.0110, -0.0026],
        [ 0.0490, -0.0018, -0.0546,  ..., -0.0464,  0.0300, -0.0232],
        ...,
        [-0.0055, -0.0035, -0.0114,  ..., -0.0354,  0.0007, -0.0054],
        [ 0.0065,  0.0019, -0.0115,  ...,  0.0208, -0.0089, -0.0142],
        [-0.0075, -0.0252, -0.0050,  ...,  0.0024,  0.0138,  0.0097]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.6094, -8.9375, -3.8027,  ..., -3.2285,  6.0117, -0.5039]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 10:28:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female buck is known as a doe
A female daddy is known as a mommy
A female grandson is known as a granddaughter
A female murderer is known as a murderess
A female emperor is known as a empress
A female manager is known as a manageress
A female boar is known as a sow
A female dad is known as a
2024-07-18 10:28:18 root INFO     [order_1_approx] starting weight calculation for A female dad is known as a mom
A female buck is known as a doe
A female murderer is known as a murderess
A female manager is known as a manageress
A female daddy is known as a mommy
A female grandson is known as a granddaughter
A female emperor is known as a empress
A female boar is known as a
2024-07-18 10:28:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 10:32:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4839, -0.0287, -0.7998,  ...,  0.4983,  0.4326, -0.4834],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.3711, -0.3491, -4.7305,  ..., -5.1445,  1.8467,  0.1128],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0217, -0.0251,  0.0053,  ...,  0.0217, -0.0046, -0.0092],
        [ 0.0027,  0.0090, -0.0040,  ..., -0.0004, -0.0079,  0.0199],
        [-0.0250,  0.0081, -0.0142,  ..., -0.0120, -0.0042, -0.0260],
        ...,
        [ 0.0168, -0.0103,  0.0114,  ..., -0.0155, -0.0249, -0.0293],
        [-0.0118, -0.0005, -0.0053,  ...,  0.0147, -0.0074,  0.0128],
        [-0.0338,  0.0028, -0.0088,  ..., -0.0089,  0.0312, -0.0224]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.7129,  0.7417, -4.5781,  ..., -4.3438,  1.2207, -0.0637]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 10:32:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female dad is known as a mom
A female buck is known as a doe
A female murderer is known as a murderess
A female manager is known as a manageress
A female daddy is known as a mommy
A female grandson is known as a granddaughter
A female emperor is known as a empress
A female boar is known as a
2024-07-18 10:32:25 root INFO     [order_1_approx] starting weight calculation for A female manager is known as a manageress
A female murderer is known as a murderess
A female daddy is known as a mommy
A female dad is known as a mom
A female emperor is known as a empress
A female grandson is known as a granddaughter
A female boar is known as a sow
A female buck is known as a
2024-07-18 10:32:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 10:36:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3479, -0.1963, -0.1364,  ...,  0.5308, -1.1299,  0.6650],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.1484, -0.9033, -7.5508,  ..., -4.7305,  1.6289,  0.6226],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0228,  0.0004,  0.0197,  ...,  0.0159,  0.0025, -0.0313],
        [-0.0077,  0.0156, -0.0066,  ..., -0.0125,  0.0269,  0.0094],
        [-0.0099,  0.0181, -0.0040,  ..., -0.0153,  0.0081, -0.0071],
        ...,
        [-0.0015, -0.0171, -0.0030,  ..., -0.0133, -0.0163, -0.0439],
        [ 0.0047, -0.0040,  0.0053,  ..., -0.0059, -0.0114,  0.0071],
        [-0.0350,  0.0236, -0.0063,  ...,  0.0048,  0.0187,  0.0064]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6406, -0.6733, -7.7852,  ..., -4.2344,  2.2109,  0.4033]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 10:36:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female manager is known as a manageress
A female murderer is known as a murderess
A female daddy is known as a mommy
A female dad is known as a mom
A female emperor is known as a empress
A female grandson is known as a granddaughter
A female boar is known as a sow
A female buck is known as a
2024-07-18 10:36:33 root INFO     [order_1_approx] starting weight calculation for A female manager is known as a manageress
A female emperor is known as a empress
A female dad is known as a mom
A female murderer is known as a murderess
A female daddy is known as a mommy
A female boar is known as a sow
A female buck is known as a doe
A female grandson is known as a
2024-07-18 10:36:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 10:40:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([1.4580, 0.3540, 0.4868,  ..., 0.1052, 0.2065, 0.2673], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.9453, -3.8359, -1.3438,  ...,  1.4180,  2.3359, -3.1484],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.3512e-02, -1.4145e-02, -5.8098e-03,  ...,  2.7664e-02,
          6.3057e-03, -9.1248e-03],
        [-3.8261e-03, -1.6308e-03,  5.7335e-03,  ...,  6.8207e-03,
          9.5367e-05, -3.5248e-03],
        [ 3.3569e-04,  6.1798e-03, -6.4392e-03,  ..., -1.3809e-03,
          1.6022e-03,  2.5826e-03],
        ...,
        [-1.8448e-02, -1.7883e-02, -1.2833e-02,  ..., -1.3077e-02,
         -1.5152e-02, -9.1858e-03],
        [-7.1106e-03,  3.3073e-03,  6.2943e-03,  ..., -5.8060e-03,
         -8.6899e-03, -1.0376e-02],
        [-1.0643e-03, -1.8578e-03,  4.6768e-03,  ..., -5.0201e-03,
         -4.4098e-03,  3.8986e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.1562, -3.2656, -1.5117,  ...,  1.6387,  2.2812, -3.1172]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 10:40:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female manager is known as a manageress
A female emperor is known as a empress
A female dad is known as a mom
A female murderer is known as a murderess
A female daddy is known as a mommy
A female boar is known as a sow
A female buck is known as a doe
A female grandson is known as a
2024-07-18 10:40:42 root INFO     [order_1_approx] starting weight calculation for A female manager is known as a manageress
A female boar is known as a sow
A female buck is known as a doe
A female daddy is known as a mommy
A female emperor is known as a empress
A female grandson is known as a granddaughter
A female dad is known as a mom
A female murderer is known as a
2024-07-18 10:40:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 10:44:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1650,  0.5786,  0.3501,  ...,  0.2437, -1.0518, -0.1753],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.9961, -3.6328, -1.5889,  ...,  0.4111,  3.3906, -0.8506],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.9638e-02, -2.0142e-02, -6.4850e-05,  ...,  1.1230e-02,
         -1.6998e-02, -9.1782e-03],
        [ 2.7580e-03, -1.7090e-02, -3.5858e-03,  ...,  1.2962e-02,
          2.9373e-03,  4.9629e-03],
        [ 7.4100e-04, -2.7218e-03, -1.9054e-03,  ..., -1.3367e-02,
          2.8610e-03, -6.4964e-03],
        ...,
        [-2.0996e-02, -1.0086e-02, -1.7899e-02,  ..., -1.4862e-02,
         -2.8259e-02, -5.3635e-03],
        [ 9.6436e-03,  7.0114e-03,  1.9089e-02,  ...,  1.0529e-03,
         -7.4425e-03,  1.7075e-02],
        [-1.2871e-02,  4.9286e-03, -8.5831e-05,  ..., -8.0261e-03,
         -4.6539e-03, -1.0757e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.3516, -3.1230, -1.6279,  ..., -0.4536,  3.2324, -0.3408]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 10:44:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female manager is known as a manageress
A female boar is known as a sow
A female buck is known as a doe
A female daddy is known as a mommy
A female emperor is known as a empress
A female grandson is known as a granddaughter
A female dad is known as a mom
A female murderer is known as a
2024-07-18 10:44:52 root INFO     total operator prediction time: 1986.4533565044403 seconds
2024-07-18 10:44:52 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on name - occupation
2024-07-18 10:44:52 root INFO     building operator name - occupation
2024-07-18 10:44:52 root INFO     [order_1_approx] starting weight calculation for jolie was known for their work as a  actress
wagner was known for their work as a  composer
picasso was known for their work as a  painter
strauss was known for their work as a  composer
descartes was known for their work as a  mathematician
plato was known for their work as a  philosopher
euler was known for their work as a  mathematician
darwin was known for their work as a 
2024-07-18 10:44:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 10:48:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3005, -0.9492,  0.2439,  ...,  0.4309,  0.0324, -0.5449],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.1719, -4.4922, -1.1895,  ..., -2.0645, -0.4751, -0.9531],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0173, -0.0130, -0.0159,  ..., -0.0059, -0.0065,  0.0040],
        [-0.0078,  0.0144, -0.0025,  ...,  0.0057,  0.0096,  0.0004],
        [ 0.0011, -0.0041, -0.0050,  ..., -0.0137,  0.0045, -0.0105],
        ...,
        [-0.0053,  0.0083, -0.0004,  ...,  0.0045, -0.0144,  0.0140],
        [ 0.0047,  0.0035,  0.0083,  ...,  0.0024, -0.0080, -0.0016],
        [-0.0016, -0.0086, -0.0024,  ...,  0.0006, -0.0041, -0.0045]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.6348, -4.7617, -1.8203,  ..., -2.2891, -0.1765, -0.6875]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 10:48:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for jolie was known for their work as a  actress
wagner was known for their work as a  composer
picasso was known for their work as a  painter
strauss was known for their work as a  composer
descartes was known for their work as a  mathematician
plato was known for their work as a  philosopher
euler was known for their work as a  mathematician
darwin was known for their work as a 
2024-07-18 10:49:00 root INFO     [order_1_approx] starting weight calculation for darwin was known for their work as a  naturalist
picasso was known for their work as a  painter
euler was known for their work as a  mathematician
descartes was known for their work as a  mathematician
plato was known for their work as a  philosopher
wagner was known for their work as a  composer
strauss was known for their work as a  composer
jolie was known for their work as a 
2024-07-18 10:49:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 10:53:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6182, -1.1191,  0.3193,  ...,  0.5728, -0.4966,  0.3137],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9932, -2.4883,  2.0020,  ..., -4.6133,  0.1033,  2.7207],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0040,  0.0045,  0.0016,  ..., -0.0003,  0.0068,  0.0107],
        [-0.0044,  0.0074, -0.0007,  ...,  0.0086, -0.0039, -0.0207],
        [ 0.0061,  0.0034,  0.0032,  ...,  0.0223,  0.0146,  0.0030],
        ...,
        [ 0.0142,  0.0020, -0.0039,  ..., -0.0107, -0.0194, -0.0024],
        [-0.0025, -0.0020, -0.0111,  ...,  0.0232, -0.0030, -0.0176],
        [-0.0047, -0.0194,  0.0009,  ...,  0.0029,  0.0122, -0.0162]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0488, -2.2832,  1.9424,  ..., -5.0898,  0.4346,  1.9834]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 10:53:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for darwin was known for their work as a  naturalist
picasso was known for their work as a  painter
euler was known for their work as a  mathematician
descartes was known for their work as a  mathematician
plato was known for their work as a  philosopher
wagner was known for their work as a  composer
strauss was known for their work as a  composer
jolie was known for their work as a 
2024-07-18 10:53:07 root INFO     [order_1_approx] starting weight calculation for plato was known for their work as a  philosopher
jolie was known for their work as a  actress
descartes was known for their work as a  mathematician
euler was known for their work as a  mathematician
wagner was known for their work as a  composer
darwin was known for their work as a  naturalist
strauss was known for their work as a  composer
picasso was known for their work as a 
2024-07-18 10:53:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 10:57:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5605, -0.6650, -0.9302,  ...,  1.2676,  0.3052, -0.0717],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3755,  0.0088, -2.5957,  ..., -2.1387, -2.0059,  1.6895],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0072,  0.0032, -0.0088,  ..., -0.0299,  0.0314,  0.0223],
        [-0.0164,  0.0010,  0.0167,  ...,  0.0080,  0.0048,  0.0007],
        [ 0.0006,  0.0033, -0.0053,  ...,  0.0071, -0.0124, -0.0086],
        ...,
        [-0.0060,  0.0103,  0.0091,  ..., -0.0008, -0.0167,  0.0274],
        [ 0.0023,  0.0063,  0.0107,  ...,  0.0227, -0.0168, -0.0041],
        [-0.0097, -0.0164, -0.0051,  ..., -0.0036,  0.0150,  0.0014]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3213,  1.0859, -3.4375,  ..., -2.4766, -2.4082,  0.7534]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 10:57:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for plato was known for their work as a  philosopher
jolie was known for their work as a  actress
descartes was known for their work as a  mathematician
euler was known for their work as a  mathematician
wagner was known for their work as a  composer
darwin was known for their work as a  naturalist
strauss was known for their work as a  composer
picasso was known for their work as a 
2024-07-18 10:57:15 root INFO     [order_1_approx] starting weight calculation for jolie was known for their work as a  actress
strauss was known for their work as a  composer
descartes was known for their work as a  mathematician
darwin was known for their work as a  naturalist
wagner was known for their work as a  composer
plato was known for their work as a  philosopher
picasso was known for their work as a  painter
euler was known for their work as a 
2024-07-18 10:57:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 11:01:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4043,  0.2057,  0.4573,  ...,  1.1748, -0.5186, -0.2686],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9097,  0.7998,  0.5654,  ..., -1.2920, -4.5664,  3.3262],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0009, -0.0013, -0.0115,  ..., -0.0125,  0.0231, -0.0285],
        [ 0.0120,  0.0412, -0.0173,  ..., -0.0067,  0.0123, -0.0328],
        [ 0.0341,  0.0135, -0.0289,  ...,  0.0172,  0.0241,  0.0067],
        ...,
        [-0.0124, -0.0115, -0.0069,  ...,  0.0023, -0.0212, -0.0191],
        [ 0.0014, -0.0178,  0.0031,  ...,  0.0215, -0.0109,  0.0032],
        [-0.0175, -0.0057, -0.0308,  ...,  0.0010,  0.0104,  0.0085]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.2285,  2.8086,  0.3643,  ..., -1.6396, -7.1953,  2.0098]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 11:01:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for jolie was known for their work as a  actress
strauss was known for their work as a  composer
descartes was known for their work as a  mathematician
darwin was known for their work as a  naturalist
wagner was known for their work as a  composer
plato was known for their work as a  philosopher
picasso was known for their work as a  painter
euler was known for their work as a 
2024-07-18 11:01:23 root INFO     [order_1_approx] starting weight calculation for plato was known for their work as a  philosopher
jolie was known for their work as a  actress
darwin was known for their work as a  naturalist
strauss was known for their work as a  composer
wagner was known for their work as a  composer
picasso was known for their work as a  painter
euler was known for their work as a  mathematician
descartes was known for their work as a 
2024-07-18 11:01:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 11:05:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3130, -0.0906,  0.0289,  ...,  0.2759, -0.9131,  0.7153],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3066, -4.1602, -1.6855,  ..., -4.6289, -2.1621, -1.3691],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.4061e-02, -9.9640e-03,  1.7242e-02,  ..., -1.0357e-03,
          9.9564e-03,  1.2741e-02],
        [-7.5111e-03,  1.9699e-02,  1.9180e-02,  ...,  1.1467e-02,
         -9.9335e-03,  6.0120e-03],
        [-1.2684e-03,  3.3455e-03,  1.4961e-02,  ...,  3.6011e-03,
         -1.1063e-02, -8.0872e-03],
        ...,
        [-6.5651e-03,  8.3466e-03,  1.1101e-02,  ..., -2.3987e-02,
         -1.1749e-02,  1.4465e-02],
        [ 7.4387e-03,  1.9073e-06,  8.1062e-04,  ...,  1.2245e-03,
         -1.5472e-02, -1.2314e-02],
        [-1.8372e-02, -8.6975e-03,  3.0842e-03,  ...,  5.6362e-04,
         -5.1575e-03, -1.9562e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1328, -4.3945, -1.7607,  ..., -4.4805, -2.4629, -1.9453]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 11:05:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for plato was known for their work as a  philosopher
jolie was known for their work as a  actress
darwin was known for their work as a  naturalist
strauss was known for their work as a  composer
wagner was known for their work as a  composer
picasso was known for their work as a  painter
euler was known for their work as a  mathematician
descartes was known for their work as a 
2024-07-18 11:05:30 root INFO     [order_1_approx] starting weight calculation for strauss was known for their work as a  composer
euler was known for their work as a  mathematician
picasso was known for their work as a  painter
wagner was known for their work as a  composer
jolie was known for their work as a  actress
descartes was known for their work as a  mathematician
darwin was known for their work as a  naturalist
plato was known for their work as a 
2024-07-18 11:05:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 11:09:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0264,  0.6143,  1.3438,  ...,  0.2145,  1.0254,  0.8018],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4912,  0.7383,  0.0466,  ..., -5.0898, -0.4548,  0.0664],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-7.8430e-03, -1.8661e-02,  1.0948e-02,  ...,  4.3793e-03,
          1.3054e-02,  4.4594e-03],
        [-4.0131e-03,  1.0971e-02,  3.3722e-03,  ...,  5.1880e-03,
         -4.5776e-04,  9.6560e-06],
        [ 1.7883e-02,  4.5242e-03, -1.8864e-03,  ..., -8.9874e-03,
          1.4748e-02, -3.2539e-03],
        ...,
        [ 8.3084e-03,  6.9351e-03, -1.4877e-02,  ..., -5.8899e-03,
         -1.7563e-02,  6.5918e-03],
        [ 1.2695e-02,  2.6550e-02,  2.0645e-02,  ...,  6.5613e-04,
         -1.7654e-02,  1.0635e-02],
        [-2.1896e-03,  1.7303e-02,  4.8752e-03,  ...,  6.6643e-03,
         -6.7558e-03,  1.4290e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0059,  0.7461, -0.7046,  ..., -5.3633, -0.3047, -0.1699]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 11:09:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for strauss was known for their work as a  composer
euler was known for their work as a  mathematician
picasso was known for their work as a  painter
wagner was known for their work as a  composer
jolie was known for their work as a  actress
descartes was known for their work as a  mathematician
darwin was known for their work as a  naturalist
plato was known for their work as a 
2024-07-18 11:09:36 root INFO     [order_1_approx] starting weight calculation for euler was known for their work as a  mathematician
wagner was known for their work as a  composer
picasso was known for their work as a  painter
darwin was known for their work as a  naturalist
jolie was known for their work as a  actress
plato was known for their work as a  philosopher
descartes was known for their work as a  mathematician
strauss was known for their work as a 
2024-07-18 11:09:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 11:13:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3596, -0.3457, -0.1847,  ...,  0.7021,  1.0703, -0.1158],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.4453, -3.8965,  2.5723,  ..., -5.8516,  1.1729,  4.2266],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0029, -0.0138,  0.0081,  ..., -0.0085,  0.0131, -0.0024],
        [-0.0105,  0.0273,  0.0173,  ...,  0.0131, -0.0008, -0.0052],
        [-0.0043, -0.0129,  0.0153,  ...,  0.0033,  0.0324, -0.0013],
        ...,
        [-0.0168, -0.0133, -0.0012,  ...,  0.0238, -0.0080,  0.0086],
        [ 0.0070, -0.0117,  0.0093,  ...,  0.0051, -0.0085,  0.0017],
        [ 0.0049, -0.0166,  0.0144,  ..., -0.0041,  0.0011,  0.0089]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.4336, -4.4961,  1.9355,  ..., -6.5039,  1.5166,  4.0508]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 11:13:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for euler was known for their work as a  mathematician
wagner was known for their work as a  composer
picasso was known for their work as a  painter
darwin was known for their work as a  naturalist
jolie was known for their work as a  actress
plato was known for their work as a  philosopher
descartes was known for their work as a  mathematician
strauss was known for their work as a 
2024-07-18 11:13:42 root INFO     [order_1_approx] starting weight calculation for descartes was known for their work as a  mathematician
jolie was known for their work as a  actress
strauss was known for their work as a  composer
darwin was known for their work as a  naturalist
plato was known for their work as a  philosopher
euler was known for their work as a  mathematician
picasso was known for their work as a  painter
wagner was known for their work as a 
2024-07-18 11:13:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 11:17:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4272, -0.2405, -0.2150,  ...,  0.4543, -0.3882,  0.3101],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.1250, -0.0229, -3.5996,  ..., -0.4131,  1.6797, -1.9756],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0269, -0.0018,  0.0012,  ...,  0.0061,  0.0074, -0.0149],
        [-0.0126,  0.0017,  0.0039,  ...,  0.0279, -0.0084, -0.0039],
        [ 0.0114, -0.0066, -0.0038,  ...,  0.0073, -0.0003,  0.0105],
        ...,
        [-0.0076,  0.0031,  0.0011,  ...,  0.0044,  0.0002,  0.0020],
        [ 0.0200, -0.0080, -0.0042,  ...,  0.0037, -0.0008,  0.0136],
        [-0.0053,  0.0026,  0.0202,  ...,  0.0002, -0.0039,  0.0043]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.3906, -0.1511, -4.1328,  ..., -0.6978,  1.3164, -1.8281]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 11:17:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for descartes was known for their work as a  mathematician
jolie was known for their work as a  actress
strauss was known for their work as a  composer
darwin was known for their work as a  naturalist
plato was known for their work as a  philosopher
euler was known for their work as a  mathematician
picasso was known for their work as a  painter
wagner was known for their work as a 
2024-07-18 11:17:48 root INFO     total operator prediction time: 1976.2026906013489 seconds
2024-07-18 11:17:48 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on country - capital
2024-07-18 11:17:48 root INFO     building operator country - capital
2024-07-18 11:17:48 root INFO     [order_1_approx] starting weight calculation for The country with conakry as its capital is known as guinea
The country with canberra as its capital is known as australia
The country with cairo as its capital is known as egypt
The country with kabul as its capital is known as afghanistan
The country with stockholm as its capital is known as sweden
The country with lima as its capital is known as peru
The country with warsaw as its capital is known as poland
The country with beirut as its capital is known as
2024-07-18 11:17:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 11:21:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4275, -0.4497, -0.9102,  ...,  0.0730, -0.1539,  0.7695],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6934,  0.9824, -0.0857,  ..., -1.6016, -3.1328, -2.9941],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0106, -0.0083,  0.0191,  ...,  0.0102,  0.0025, -0.0327],
        [ 0.0309, -0.0038,  0.0002,  ...,  0.0143,  0.0163,  0.0233],
        [ 0.0042, -0.0196, -0.0253,  ..., -0.0079, -0.0107, -0.0081],
        ...,
        [-0.0083,  0.0107,  0.0137,  ...,  0.0023, -0.0013,  0.0019],
        [-0.0365, -0.0046, -0.0154,  ..., -0.0298, -0.0204, -0.0196],
        [-0.0211, -0.0206, -0.0106,  ..., -0.0149, -0.0039, -0.0163]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4199,  2.6250, -1.3193,  ..., -1.6377, -5.4961, -3.9727]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 11:21:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with conakry as its capital is known as guinea
The country with canberra as its capital is known as australia
The country with cairo as its capital is known as egypt
The country with kabul as its capital is known as afghanistan
The country with stockholm as its capital is known as sweden
The country with lima as its capital is known as peru
The country with warsaw as its capital is known as poland
The country with beirut as its capital is known as
2024-07-18 11:21:54 root INFO     [order_1_approx] starting weight calculation for The country with cairo as its capital is known as egypt
The country with kabul as its capital is known as afghanistan
The country with warsaw as its capital is known as poland
The country with beirut as its capital is known as lebanon
The country with conakry as its capital is known as guinea
The country with lima as its capital is known as peru
The country with canberra as its capital is known as australia
The country with stockholm as its capital is known as
2024-07-18 11:21:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 11:26:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5684,  1.5010, -0.1499,  ...,  0.1129,  0.8394, -0.3103],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-6.2812,  0.7754, -3.3848,  ..., -3.4727, -0.3049, -6.0781],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0175,  0.0212, -0.0193,  ...,  0.0056,  0.0377, -0.0277],
        [-0.0121, -0.0293, -0.0016,  ...,  0.0074, -0.0282,  0.0116],
        [ 0.0186,  0.0236, -0.0063,  ..., -0.0152,  0.0239, -0.0235],
        ...,
        [-0.0123,  0.0061,  0.0122,  ...,  0.0068, -0.0165,  0.0019],
        [-0.0030,  0.0162, -0.0004,  ..., -0.0223,  0.0014, -0.0047],
        [ 0.0140,  0.0187, -0.0205,  ..., -0.0046,  0.0381, -0.0270]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.1602,  0.5728, -3.3125,  ..., -3.3066,  0.6055, -5.4883]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 11:26:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with cairo as its capital is known as egypt
The country with kabul as its capital is known as afghanistan
The country with warsaw as its capital is known as poland
The country with beirut as its capital is known as lebanon
The country with conakry as its capital is known as guinea
The country with lima as its capital is known as peru
The country with canberra as its capital is known as australia
The country with stockholm as its capital is known as
2024-07-18 11:26:01 root INFO     [order_1_approx] starting weight calculation for The country with stockholm as its capital is known as sweden
The country with lima as its capital is known as peru
The country with kabul as its capital is known as afghanistan
The country with warsaw as its capital is known as poland
The country with cairo as its capital is known as egypt
The country with beirut as its capital is known as lebanon
The country with conakry as its capital is known as guinea
The country with canberra as its capital is known as
2024-07-18 11:26:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 11:30:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1211, -0.8926, -1.4150,  ...,  0.3159, -0.6113,  0.4578],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4043,  1.2490, -0.7959,  ..., -2.8711,  0.8379, -5.8789],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 8.4076e-03,  1.6708e-02, -1.4200e-03,  ..., -9.7809e-03,
         -1.0712e-02, -9.4910e-03],
        [-1.7273e-02, -3.2349e-02,  1.2138e-02,  ...,  2.6672e-02,
          2.3987e-02,  1.1566e-02],
        [ 9.7351e-03,  2.4643e-02, -1.4488e-02,  ..., -1.1848e-02,
         -4.7836e-03, -1.0086e-02],
        ...,
        [-1.3680e-02,  1.5001e-03, -2.9373e-04,  ...,  4.3678e-04,
         -1.7349e-02,  1.1444e-05],
        [-1.7853e-02, -1.9360e-03, -5.9204e-03,  ..., -7.3395e-03,
         -1.9272e-02,  5.1346e-03],
        [ 3.4576e-02,  2.1790e-02,  3.0518e-03,  ..., -1.8478e-02,
          6.0425e-03, -8.0719e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2832,  2.4922, -1.7354,  ..., -3.1348,  0.6865, -6.8359]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 11:30:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with stockholm as its capital is known as sweden
The country with lima as its capital is known as peru
The country with kabul as its capital is known as afghanistan
The country with warsaw as its capital is known as poland
The country with cairo as its capital is known as egypt
The country with beirut as its capital is known as lebanon
The country with conakry as its capital is known as guinea
The country with canberra as its capital is known as
2024-07-18 11:30:09 root INFO     [order_1_approx] starting weight calculation for The country with lima as its capital is known as peru
The country with beirut as its capital is known as lebanon
The country with stockholm as its capital is known as sweden
The country with canberra as its capital is known as australia
The country with warsaw as its capital is known as poland
The country with conakry as its capital is known as guinea
The country with kabul as its capital is known as afghanistan
The country with cairo as its capital is known as
2024-07-18 11:30:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 11:34:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4941, -0.9209, -0.9180,  ...,  1.6875, -0.7373, -0.4626],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.9609,  2.1172,  5.5820,  ..., -1.6582, -4.5898, -2.0898],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0036,  0.0070, -0.0013,  ..., -0.0042,  0.0214, -0.0363],
        [-0.0001, -0.0263,  0.0136,  ...,  0.0362,  0.0052, -0.0110],
        [-0.0323, -0.0048,  0.0024,  ..., -0.0027, -0.0099, -0.0244],
        ...,
        [-0.0140, -0.0017, -0.0107,  ...,  0.0294, -0.0220, -0.0076],
        [ 0.0078, -0.0118, -0.0095,  ..., -0.0038, -0.0111,  0.0431],
        [-0.0142,  0.0147, -0.0069,  ...,  0.0053,  0.0080, -0.0196]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3047,  1.6260,  4.7383,  ..., -1.3984, -3.9961, -0.4502]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 11:34:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with lima as its capital is known as peru
The country with beirut as its capital is known as lebanon
The country with stockholm as its capital is known as sweden
The country with canberra as its capital is known as australia
The country with warsaw as its capital is known as poland
The country with conakry as its capital is known as guinea
The country with kabul as its capital is known as afghanistan
The country with cairo as its capital is known as
2024-07-18 11:34:17 root INFO     [order_1_approx] starting weight calculation for The country with conakry as its capital is known as guinea
The country with warsaw as its capital is known as poland
The country with beirut as its capital is known as lebanon
The country with canberra as its capital is known as australia
The country with stockholm as its capital is known as sweden
The country with lima as its capital is known as peru
The country with cairo as its capital is known as egypt
The country with kabul as its capital is known as
2024-07-18 11:34:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 11:38:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2078, -0.9536,  0.4290,  ...,  0.6465,  0.7104,  0.3518],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.1602,  0.5537, -0.5312,  ..., -3.6738,  0.9521,  1.3174],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0067,  0.0014,  0.0118,  ...,  0.0170,  0.0034, -0.0111],
        [ 0.0116, -0.0342, -0.0046,  ...,  0.0256,  0.0036, -0.0207],
        [ 0.0073,  0.0055, -0.0205,  ..., -0.0112,  0.0147, -0.0085],
        ...,
        [-0.0127,  0.0088, -0.0047,  ...,  0.0128, -0.0105, -0.0202],
        [-0.0128, -0.0163,  0.0061,  ..., -0.0101, -0.0357,  0.0371],
        [-0.0090, -0.0053,  0.0040,  ...,  0.0075, -0.0079,  0.0087]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.7383, -0.3389, -0.0400,  ..., -3.9883,  1.1943,  1.3154]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 11:38:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with conakry as its capital is known as guinea
The country with warsaw as its capital is known as poland
The country with beirut as its capital is known as lebanon
The country with canberra as its capital is known as australia
The country with stockholm as its capital is known as sweden
The country with lima as its capital is known as peru
The country with cairo as its capital is known as egypt
The country with kabul as its capital is known as
2024-07-18 11:38:24 root INFO     [order_1_approx] starting weight calculation for The country with canberra as its capital is known as australia
The country with cairo as its capital is known as egypt
The country with beirut as its capital is known as lebanon
The country with lima as its capital is known as peru
The country with warsaw as its capital is known as poland
The country with stockholm as its capital is known as sweden
The country with kabul as its capital is known as afghanistan
The country with conakry as its capital is known as
2024-07-18 11:38:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 11:42:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0908, -0.3406,  0.8096,  ...,  0.8872, -0.1637, -0.0510],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6309,  5.2109, -0.4604,  ..., -2.3164, -1.2305,  0.8643],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0048,  0.0144,  0.0151,  ..., -0.0077,  0.0359, -0.0102],
        [-0.0167, -0.0196,  0.0169,  ...,  0.0435, -0.0521, -0.0398],
        [-0.0123,  0.0232, -0.0079,  ...,  0.0005, -0.0084, -0.0044],
        ...,
        [ 0.0044, -0.0284,  0.0105,  ...,  0.0129,  0.0288, -0.0077],
        [-0.0105, -0.0048, -0.0200,  ...,  0.0314,  0.0151,  0.0235],
        [ 0.0033, -0.0223,  0.0172,  ..., -0.0097,  0.0284, -0.0170]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6992,  6.9297, -1.2539,  ..., -2.4492, -2.3086,  1.8252]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 11:42:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with canberra as its capital is known as australia
The country with cairo as its capital is known as egypt
The country with beirut as its capital is known as lebanon
The country with lima as its capital is known as peru
The country with warsaw as its capital is known as poland
The country with stockholm as its capital is known as sweden
The country with kabul as its capital is known as afghanistan
The country with conakry as its capital is known as
2024-07-18 11:42:33 root INFO     [order_1_approx] starting weight calculation for The country with beirut as its capital is known as lebanon
The country with canberra as its capital is known as australia
The country with lima as its capital is known as peru
The country with kabul as its capital is known as afghanistan
The country with conakry as its capital is known as guinea
The country with stockholm as its capital is known as sweden
The country with cairo as its capital is known as egypt
The country with warsaw as its capital is known as
2024-07-18 11:42:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 11:46:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.3145,  0.0643, -0.7021,  ..., -0.0021,  0.8140,  0.5894],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7080,  1.2002, -0.7207,  ...,  1.3896,  4.0547,  5.6719],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0034, -0.0076,  0.0116,  ...,  0.0185, -0.0139, -0.0077],
        [-0.0070, -0.0199, -0.0054,  ..., -0.0072, -0.0024, -0.0084],
        [ 0.0296,  0.0023, -0.0144,  ...,  0.0121,  0.0123, -0.0132],
        ...,
        [-0.0407, -0.0130, -0.0168,  ..., -0.0045, -0.0194, -0.0023],
        [-0.0353,  0.0084,  0.0015,  ..., -0.0019, -0.0361,  0.0048],
        [-0.0279, -0.0358,  0.0040,  ...,  0.0196, -0.0114,  0.0046]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4678,  1.4287, -0.9517,  ...,  0.1084,  4.1992,  5.8594]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 11:46:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with beirut as its capital is known as lebanon
The country with canberra as its capital is known as australia
The country with lima as its capital is known as peru
The country with kabul as its capital is known as afghanistan
The country with conakry as its capital is known as guinea
The country with stockholm as its capital is known as sweden
The country with cairo as its capital is known as egypt
The country with warsaw as its capital is known as
2024-07-18 11:46:40 root INFO     [order_1_approx] starting weight calculation for The country with conakry as its capital is known as guinea
The country with stockholm as its capital is known as sweden
The country with canberra as its capital is known as australia
The country with cairo as its capital is known as egypt
The country with kabul as its capital is known as afghanistan
The country with warsaw as its capital is known as poland
The country with beirut as its capital is known as lebanon
The country with lima as its capital is known as
2024-07-18 11:46:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 11:50:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0864, -0.2988,  0.1877,  ...,  0.4082, -0.0527, -0.3079],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.6543, -0.0908, -0.1934,  ..., -1.3887, -0.4502,  0.3459],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 9.4910e-03, -1.9592e-02, -4.7684e-03,  ...,  3.0121e-02,
          1.2405e-02, -2.6047e-02],
        [-1.1581e-02, -1.3306e-02, -5.7030e-03,  ..., -1.3329e-02,
          6.0501e-03, -8.7891e-03],
        [-1.0353e-02, -4.7531e-03, -3.4668e-02,  ...,  7.7972e-03,
         -1.9257e-02,  8.0109e-05],
        ...,
        [-1.4191e-02, -1.9836e-02,  1.5701e-02,  ...,  4.7363e-02,
         -1.6876e-02, -3.4237e-03],
        [-1.4565e-02, -2.2522e-02, -1.7853e-02,  ...,  3.1082e-02,
         -3.0975e-02,  8.3008e-03],
        [-2.0905e-02, -6.2523e-03, -2.8019e-03,  ...,  1.7975e-02,
          2.1400e-03, -1.2718e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.2852, -0.0198, -1.8516,  ..., -1.6816, -0.9170,  0.0413]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 11:50:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with conakry as its capital is known as guinea
The country with stockholm as its capital is known as sweden
The country with canberra as its capital is known as australia
The country with cairo as its capital is known as egypt
The country with kabul as its capital is known as afghanistan
The country with warsaw as its capital is known as poland
The country with beirut as its capital is known as lebanon
The country with lima as its capital is known as
2024-07-18 11:50:47 root INFO     total operator prediction time: 1979.2191290855408 seconds
2024-07-18 11:50:47 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on things - color
2024-07-18 11:50:47 root INFO     building operator things - color
2024-07-18 11:50:47 root INFO     [order_1_approx] starting weight calculation for The fridge is colored white
The blueberry is colored blue
The salt is colored white
The frog is colored green
The rose is colored red
The cabbage is colored green
The toothpaste is colored white
The cherry is colored
2024-07-18 11:50:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 11:54:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0748,  0.3604,  0.6362,  ..., -0.9761, -1.1953,  0.0115],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0601,  3.7227, -2.8008,  ...,  0.0654,  3.6543, -0.6602],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0002,  0.0084, -0.0062,  ...,  0.0089, -0.0055, -0.0158],
        [ 0.0060,  0.0007,  0.0074,  ...,  0.0032,  0.0063,  0.0066],
        [-0.0062,  0.0093, -0.0041,  ..., -0.0035,  0.0029,  0.0088],
        ...,
        [ 0.0064, -0.0010, -0.0103,  ...,  0.0145, -0.0168,  0.0090],
        [-0.0072,  0.0030,  0.0060,  ...,  0.0015, -0.0103, -0.0015],
        [-0.0054,  0.0106,  0.0011,  ..., -0.0050,  0.0021, -0.0110]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5137,  2.9238, -1.6445,  ...,  0.5610,  3.8691, -0.1201]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 11:54:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The fridge is colored white
The blueberry is colored blue
The salt is colored white
The frog is colored green
The rose is colored red
The cabbage is colored green
The toothpaste is colored white
The cherry is colored
2024-07-18 11:54:55 root INFO     [order_1_approx] starting weight calculation for The rose is colored red
The salt is colored white
The cherry is colored red
The toothpaste is colored white
The fridge is colored white
The blueberry is colored blue
The cabbage is colored green
The frog is colored
2024-07-18 11:54:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 11:59:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3813,  0.3359, -0.7856,  ...,  0.1300, -0.4343,  0.5005],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3145,  5.0039,  2.1562,  ..., -1.5391,  1.8242, -1.5176],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-4.4403e-03, -5.9090e-03, -1.0662e-03,  ...,  4.1771e-03,
         -2.8934e-03,  8.3466e-03],
        [-4.4746e-03, -9.3231e-03,  1.7967e-03,  ..., -6.5880e-03,
          3.7994e-03,  1.2417e-03],
        [ 7.8201e-05,  1.1459e-02,  5.1003e-03,  ..., -1.1997e-03,
         -1.1047e-02,  1.0109e-02],
        ...,
        [-1.1765e-02, -1.1688e-02, -5.2910e-03,  ...,  2.3136e-03,
          1.0624e-03,  4.7989e-03],
        [-5.5771e-03,  1.0620e-02, -5.1270e-03,  ...,  6.7787e-03,
         -8.0490e-03, -1.6594e-03],
        [-1.5198e-02,  1.8463e-03,  6.5041e-03,  ..., -1.0384e-02,
         -2.5215e-03, -8.1635e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3362,  4.6602,  2.3281,  ..., -0.9458,  1.7295, -1.8711]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 11:59:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The rose is colored red
The salt is colored white
The cherry is colored red
The toothpaste is colored white
The fridge is colored white
The blueberry is colored blue
The cabbage is colored green
The frog is colored
2024-07-18 11:59:02 root INFO     [order_1_approx] starting weight calculation for The cabbage is colored green
The fridge is colored white
The cherry is colored red
The toothpaste is colored white
The blueberry is colored blue
The salt is colored white
The frog is colored green
The rose is colored
2024-07-18 11:59:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 12:03:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6475, -0.7861,  0.9634,  ...,  0.0767, -1.0303,  0.0620],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5977,  4.6836,  1.2520,  ..., -0.8887,  4.4453, -2.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0019, -0.0001,  0.0005,  ...,  0.0009, -0.0003,  0.0035],
        [-0.0027, -0.0051, -0.0034,  ..., -0.0032, -0.0076, -0.0089],
        [-0.0006, -0.0027,  0.0039,  ..., -0.0026, -0.0048,  0.0090],
        ...,
        [-0.0113, -0.0004, -0.0050,  ...,  0.0017,  0.0006,  0.0008],
        [-0.0029,  0.0002,  0.0028,  ...,  0.0006, -0.0140,  0.0012],
        [ 0.0021, -0.0011, -0.0037,  ..., -0.0019, -0.0005, -0.0087]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2090,  4.6211,  1.4492,  ..., -0.8467,  4.2109, -2.8477]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 12:03:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The cabbage is colored green
The fridge is colored white
The cherry is colored red
The toothpaste is colored white
The blueberry is colored blue
The salt is colored white
The frog is colored green
The rose is colored
2024-07-18 12:03:09 root INFO     [order_1_approx] starting weight calculation for The cherry is colored red
The fridge is colored white
The salt is colored white
The cabbage is colored green
The frog is colored green
The toothpaste is colored white
The rose is colored red
The blueberry is colored
2024-07-18 12:03:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 12:07:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5654,  0.2549,  0.4253,  ..., -1.1191, -0.3086,  0.6826],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4268,  3.3711, -1.3750,  ..., -0.1846,  5.8281, -1.8418],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0084, -0.0045, -0.0024,  ...,  0.0022, -0.0002, -0.0096],
        [-0.0018,  0.0133,  0.0123,  ...,  0.0062,  0.0054,  0.0038],
        [ 0.0047, -0.0086, -0.0024,  ..., -0.0021, -0.0046,  0.0001],
        ...,
        [ 0.0020, -0.0026, -0.0040,  ...,  0.0038, -0.0066,  0.0097],
        [-0.0037,  0.0111, -0.0018,  ...,  0.0056,  0.0044, -0.0048],
        [-0.0060, -0.0048,  0.0046,  ..., -0.0132, -0.0016,  0.0044]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1436,  2.2656, -0.7568,  ..., -0.4714,  4.7656, -1.3750]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 12:07:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The cherry is colored red
The fridge is colored white
The salt is colored white
The cabbage is colored green
The frog is colored green
The toothpaste is colored white
The rose is colored red
The blueberry is colored
2024-07-18 12:07:14 root INFO     [order_1_approx] starting weight calculation for The rose is colored red
The cherry is colored red
The salt is colored white
The blueberry is colored blue
The cabbage is colored green
The toothpaste is colored white
The frog is colored green
The fridge is colored
2024-07-18 12:07:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 12:11:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3533, -0.1240, -0.7915,  ..., -1.1143,  0.5239,  0.3906],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.5820,  1.6055, -1.8203,  ..., -3.1738,  1.6104, -1.8242],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0079, -0.0006, -0.0003,  ..., -0.0060, -0.0005, -0.0036],
        [-0.0009,  0.0075, -0.0027,  ...,  0.0040,  0.0039,  0.0050],
        [-0.0153,  0.0129, -0.0038,  ...,  0.0082,  0.0030,  0.0141],
        ...,
        [-0.0081, -0.0015,  0.0069,  ...,  0.0061, -0.0031,  0.0093],
        [ 0.0078, -0.0035,  0.0057,  ...,  0.0050,  0.0012,  0.0022],
        [-0.0042,  0.0076,  0.0021,  ..., -0.0039,  0.0063,  0.0039]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.7559,  2.0176, -1.4111,  ..., -2.7949,  1.2275, -1.3877]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 12:11:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The rose is colored red
The cherry is colored red
The salt is colored white
The blueberry is colored blue
The cabbage is colored green
The toothpaste is colored white
The frog is colored green
The fridge is colored
2024-07-18 12:11:20 root INFO     [order_1_approx] starting weight calculation for The fridge is colored white
The frog is colored green
The cherry is colored red
The rose is colored red
The cabbage is colored green
The toothpaste is colored white
The blueberry is colored blue
The salt is colored
2024-07-18 12:11:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 12:15:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3135, -0.2974,  0.7588,  ..., -0.1333,  1.7305,  0.1537],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.2842,  5.6719, -2.3398,  ..., -0.6211,  3.6016, -4.3633],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0036, -0.0042,  0.0013,  ...,  0.0005, -0.0042,  0.0006],
        [-0.0112, -0.0066,  0.0017,  ..., -0.0059, -0.0066, -0.0003],
        [ 0.0048,  0.0049, -0.0054,  ...,  0.0059,  0.0053, -0.0037],
        ...,
        [-0.0145, -0.0092,  0.0100,  ...,  0.0022,  0.0023,  0.0107],
        [ 0.0105,  0.0022,  0.0048,  ...,  0.0034, -0.0042, -0.0014],
        [-0.0017,  0.0037,  0.0049,  ...,  0.0133,  0.0006, -0.0085]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.1260,  5.6875, -2.5137,  ..., -0.2363,  3.6074, -4.1953]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 12:15:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The fridge is colored white
The frog is colored green
The cherry is colored red
The rose is colored red
The cabbage is colored green
The toothpaste is colored white
The blueberry is colored blue
The salt is colored
2024-07-18 12:15:28 root INFO     [order_1_approx] starting weight calculation for The fridge is colored white
The blueberry is colored blue
The frog is colored green
The rose is colored red
The cherry is colored red
The cabbage is colored green
The salt is colored white
The toothpaste is colored
2024-07-18 12:15:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 12:19:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2505, -0.5645,  0.0502,  ..., -0.5166,  0.4919,  0.5596],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.7285,  4.1953, -3.1387,  ..., -0.8354,  3.3457, -0.2371],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 5.4283e-03,  4.3755e-03, -9.7466e-04,  ...,  1.5888e-03,
          3.7289e-03, -1.1520e-02],
        [-1.9264e-03,  4.1275e-03,  5.6992e-03,  ..., -8.6021e-04,
          4.0092e-03,  4.3793e-03],
        [ 7.1430e-04, -6.1531e-03, -1.4595e-02,  ...,  7.2250e-03,
          4.3373e-03, -8.5678e-03],
        ...,
        [-5.1651e-03, -1.0223e-03, -2.8133e-03,  ...,  4.5586e-03,
          8.6060e-03,  8.8425e-03],
        [-4.8332e-03,  7.5073e-03, -1.2517e-06,  ...,  4.3035e-04,
         -2.6608e-03, -4.1847e-03],
        [-4.9362e-03,  9.0027e-03,  3.2578e-03,  ...,  2.1591e-03,
          1.0405e-03,  1.8330e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.8066,  3.9766, -3.2734,  ...,  0.0503,  3.0449,  0.3064]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 12:19:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The fridge is colored white
The blueberry is colored blue
The frog is colored green
The rose is colored red
The cherry is colored red
The cabbage is colored green
The salt is colored white
The toothpaste is colored
2024-07-18 12:19:35 root INFO     [order_1_approx] starting weight calculation for The frog is colored green
The salt is colored white
The blueberry is colored blue
The cherry is colored red
The toothpaste is colored white
The rose is colored red
The fridge is colored white
The cabbage is colored
2024-07-18 12:19:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 12:23:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4316, -1.4502,  0.6104,  ..., -0.2098, -0.1527,  0.0253],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6758,  3.4746, -0.4082,  ..., -1.5488,  5.5508, -3.7305],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.2112e-04,  5.5389e-03,  1.7586e-03,  ...,  2.6073e-03,
          5.9738e-03, -8.2016e-04],
        [-2.3071e-02, -9.7275e-03,  7.0572e-05,  ..., -4.1122e-03,
          1.0094e-02,  6.3171e-03],
        [ 2.1637e-02,  7.7820e-03, -1.7967e-03,  ...,  1.4847e-02,
          2.9221e-03,  4.5013e-03],
        ...,
        [ 1.8829e-02, -1.4248e-03,  7.7782e-03,  ...,  5.8403e-03,
         -1.3855e-02, -1.0052e-03],
        [-7.6790e-03, -3.1242e-03,  5.3329e-03,  ...,  6.6605e-03,
         -1.0155e-02, -8.6823e-03],
        [-4.4861e-03,  5.4626e-03,  8.9569e-03,  ..., -1.2062e-02,
         -3.3073e-03, -1.4496e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3438,  2.4980, -0.0618,  ..., -1.3145,  4.7500, -3.7969]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 12:23:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The frog is colored green
The salt is colored white
The blueberry is colored blue
The cherry is colored red
The toothpaste is colored white
The rose is colored red
The fridge is colored white
The cabbage is colored
2024-07-18 12:23:41 root INFO     total operator prediction time: 1974.0537192821503 seconds
2024-07-18 12:23:41 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on animal - sound
2024-07-18 12:23:41 root INFO     building operator animal - sound
2024-07-18 12:23:41 root INFO     [order_1_approx] starting weight calculation for The sound that a cattle makes is called a moo
The sound that a goat makes is called a bleat
The sound that a dog makes is called a bark
The sound that a pigeon makes is called a coo
The sound that a donkey makes is called a bray
The sound that a chimpanzee makes is called a scream
The sound that a frog makes is called a ribbit
The sound that a whale makes is called a
2024-07-18 12:23:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 12:27:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2715,  1.1836,  0.2788,  ..., -0.2103,  0.0809,  0.7925],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3687,  3.0859, -2.4512,  ...,  1.5762, -0.5488, -2.0215],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0193, -0.0014,  0.0027,  ...,  0.0057,  0.0010,  0.0077],
        [ 0.0039,  0.0184,  0.0013,  ...,  0.0052, -0.0181, -0.0243],
        [ 0.0238,  0.0117, -0.0143,  ..., -0.0005, -0.0086, -0.0050],
        ...,
        [-0.0063, -0.0062, -0.0099,  ...,  0.0031, -0.0006,  0.0141],
        [-0.0034, -0.0206,  0.0147,  ..., -0.0253,  0.0132, -0.0149],
        [-0.0112, -0.0299,  0.0023,  ..., -0.0021,  0.0078, -0.0037]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3384,  3.4883, -2.1836,  ...,  1.8984,  0.6338,  0.1211]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 12:27:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a cattle makes is called a moo
The sound that a goat makes is called a bleat
The sound that a dog makes is called a bark
The sound that a pigeon makes is called a coo
The sound that a donkey makes is called a bray
The sound that a chimpanzee makes is called a scream
The sound that a frog makes is called a ribbit
The sound that a whale makes is called a
2024-07-18 12:27:48 root INFO     [order_1_approx] starting weight calculation for The sound that a dog makes is called a bark
The sound that a pigeon makes is called a coo
The sound that a goat makes is called a bleat
The sound that a chimpanzee makes is called a scream
The sound that a frog makes is called a ribbit
The sound that a whale makes is called a sing
The sound that a cattle makes is called a moo
The sound that a donkey makes is called a
2024-07-18 12:27:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 12:31:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6074,  0.2048, -0.5425,  ...,  0.9775, -1.0391,  0.9102],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0215,  0.1328, -6.7930,  ..., -3.9316,  1.3721,  0.6528],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0052, -0.0174,  0.0197,  ...,  0.0014, -0.0056,  0.0027],
        [ 0.0051,  0.0263, -0.0125,  ...,  0.0070,  0.0039,  0.0055],
        [-0.0199, -0.0093, -0.0033,  ..., -0.0013, -0.0090,  0.0222],
        ...,
        [ 0.0034, -0.0232,  0.0068,  ..., -0.0119, -0.0037, -0.0051],
        [-0.0020, -0.0171, -0.0108,  ..., -0.0256,  0.0207, -0.0046],
        [ 0.0041,  0.0090, -0.0158,  ...,  0.0085,  0.0185, -0.0103]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7168, -0.3271, -5.9961,  ..., -3.6055,  1.3262,  0.6006]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 12:31:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a dog makes is called a bark
The sound that a pigeon makes is called a coo
The sound that a goat makes is called a bleat
The sound that a chimpanzee makes is called a scream
The sound that a frog makes is called a ribbit
The sound that a whale makes is called a sing
The sound that a cattle makes is called a moo
The sound that a donkey makes is called a
2024-07-18 12:31:55 root INFO     [order_1_approx] starting weight calculation for The sound that a pigeon makes is called a coo
The sound that a chimpanzee makes is called a scream
The sound that a whale makes is called a sing
The sound that a frog makes is called a ribbit
The sound that a donkey makes is called a bray
The sound that a goat makes is called a bleat
The sound that a cattle makes is called a moo
The sound that a dog makes is called a
2024-07-18 12:31:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 12:36:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8047,  0.0137, -0.0847,  ...,  0.5713, -0.3899,  0.9531],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.8945, -1.1094, -3.2578,  ..., -0.9829,  4.0898, -3.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 8.9569e-03, -1.0040e-02,  1.0941e-02,  ...,  4.5776e-04,
          3.4332e-04,  1.8341e-02],
        [ 1.9043e-02,  2.7599e-03,  1.8280e-02,  ..., -1.7136e-02,
         -4.1962e-03,  2.5558e-04],
        [-1.3069e-02,  3.4313e-03,  6.0425e-03,  ..., -1.1826e-03,
          1.0052e-03, -5.6915e-03],
        ...,
        [ 1.2215e-02, -1.9073e-02,  1.6068e-02,  ..., -8.1635e-03,
         -1.2009e-02, -9.1553e-05],
        [-1.5564e-02,  2.5177e-04, -1.7471e-02,  ...,  8.0338e-03,
          7.6828e-03, -1.0834e-02],
        [ 5.9700e-03, -9.3079e-03, -4.2191e-03,  ..., -1.2299e-02,
         -1.1368e-02,  5.1003e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5195, -1.2305, -2.7422,  ..., -0.4165,  3.9668, -1.3984]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 12:36:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a pigeon makes is called a coo
The sound that a chimpanzee makes is called a scream
The sound that a whale makes is called a sing
The sound that a frog makes is called a ribbit
The sound that a donkey makes is called a bray
The sound that a goat makes is called a bleat
The sound that a cattle makes is called a moo
The sound that a dog makes is called a
2024-07-18 12:36:02 root INFO     [order_1_approx] starting weight calculation for The sound that a chimpanzee makes is called a scream
The sound that a whale makes is called a sing
The sound that a donkey makes is called a bray
The sound that a cattle makes is called a moo
The sound that a dog makes is called a bark
The sound that a pigeon makes is called a coo
The sound that a frog makes is called a ribbit
The sound that a goat makes is called a
2024-07-18 12:36:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 12:40:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1705, -0.5879, -0.7109,  ...,  1.5244, -1.1270,  1.1416],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2153,  3.4941, -3.2578,  ...,  0.8394, -1.5430, -0.2720],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-6.2828e-03, -3.6652e-02,  2.1477e-03,  ...,  1.1566e-02,
          3.3875e-03,  4.5357e-03],
        [-7.1487e-03,  2.3407e-02, -2.2705e-02,  ..., -4.8256e-03,
          1.6327e-02, -9.2392e-03],
        [-2.7103e-03, -4.7505e-05,  1.7929e-03,  ..., -7.5302e-03,
          6.3057e-03,  2.1805e-02],
        ...,
        [-1.2207e-02, -1.8646e-02,  5.5161e-03,  ..., -9.3307e-03,
          6.7596e-03, -2.3727e-03],
        [-2.4597e-02, -1.4381e-02, -1.7105e-02,  ..., -2.1408e-02,
         -7.3624e-04,  9.7885e-03],
        [-1.3214e-02,  1.0307e-02,  3.5648e-03,  ..., -9.9182e-03,
          1.4717e-02, -2.1088e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3164,  1.6279, -2.7871,  ...,  1.1582, -0.3359, -0.5654]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 12:40:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a chimpanzee makes is called a scream
The sound that a whale makes is called a sing
The sound that a donkey makes is called a bray
The sound that a cattle makes is called a moo
The sound that a dog makes is called a bark
The sound that a pigeon makes is called a coo
The sound that a frog makes is called a ribbit
The sound that a goat makes is called a
2024-07-18 12:40:09 root INFO     [order_1_approx] starting weight calculation for The sound that a dog makes is called a bark
The sound that a cattle makes is called a moo
The sound that a frog makes is called a ribbit
The sound that a pigeon makes is called a coo
The sound that a whale makes is called a sing
The sound that a goat makes is called a bleat
The sound that a donkey makes is called a bray
The sound that a chimpanzee makes is called a
2024-07-18 12:40:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 12:44:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5161,  0.6509, -0.3730,  ...,  0.2080, -0.8359,  1.2783],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1523,  0.6309, -1.6406,  ...,  1.2354, -0.8428, -2.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0111, -0.0037, -0.0047,  ..., -0.0040, -0.0096, -0.0114],
        [ 0.0071,  0.0103,  0.0122,  ..., -0.0021, -0.0027, -0.0051],
        [-0.0013, -0.0068, -0.0039,  ..., -0.0028, -0.0071,  0.0057],
        ...,
        [-0.0020, -0.0145, -0.0129,  ..., -0.0036, -0.0027,  0.0039],
        [-0.0034, -0.0004,  0.0011,  ..., -0.0001, -0.0037,  0.0069],
        [-0.0048, -0.0017,  0.0005,  ...,  0.0078,  0.0076,  0.0079]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0850,  0.2029, -1.5928,  ...,  1.9219, -0.6284, -2.7207]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 12:44:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a dog makes is called a bark
The sound that a cattle makes is called a moo
The sound that a frog makes is called a ribbit
The sound that a pigeon makes is called a coo
The sound that a whale makes is called a sing
The sound that a goat makes is called a bleat
The sound that a donkey makes is called a bray
The sound that a chimpanzee makes is called a
2024-07-18 12:44:08 root INFO     [order_1_approx] starting weight calculation for The sound that a whale makes is called a sing
The sound that a chimpanzee makes is called a scream
The sound that a goat makes is called a bleat
The sound that a frog makes is called a ribbit
The sound that a dog makes is called a bark
The sound that a donkey makes is called a bray
The sound that a cattle makes is called a moo
The sound that a pigeon makes is called a
2024-07-18 12:44:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 12:48:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8389, -0.2297, -0.6865,  ...,  0.4343, -0.5972,  1.8887],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3789,  1.8340,  1.8359,  ...,  1.9141, -3.9688, -7.8398],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0099, -0.0020, -0.0218,  ..., -0.0089,  0.0089,  0.0157],
        [ 0.0193, -0.0028,  0.0203,  ...,  0.0030, -0.0251, -0.0080],
        [ 0.0107,  0.0232, -0.0062,  ..., -0.0267, -0.0105,  0.0306],
        ...,
        [-0.0001, -0.0155,  0.0081,  ..., -0.0070, -0.0003,  0.0028],
        [-0.0115, -0.0278, -0.0002,  ...,  0.0204,  0.0214, -0.0335],
        [-0.0063, -0.0252, -0.0081,  ...,  0.0444, -0.0050, -0.0174]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3594,  2.5801, -2.1875,  ...,  1.4883, -0.7988, -3.0547]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 12:48:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a whale makes is called a sing
The sound that a chimpanzee makes is called a scream
The sound that a goat makes is called a bleat
The sound that a frog makes is called a ribbit
The sound that a dog makes is called a bark
The sound that a donkey makes is called a bray
The sound that a cattle makes is called a moo
The sound that a pigeon makes is called a
2024-07-18 12:48:15 root INFO     [order_1_approx] starting weight calculation for The sound that a goat makes is called a bleat
The sound that a dog makes is called a bark
The sound that a chimpanzee makes is called a scream
The sound that a frog makes is called a ribbit
The sound that a pigeon makes is called a coo
The sound that a whale makes is called a sing
The sound that a donkey makes is called a bray
The sound that a cattle makes is called a
2024-07-18 12:48:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 12:52:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5156,  0.2729,  0.2262,  ...,  1.2441, -0.7275,  1.3115],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.4434,  2.6367, -4.6992,  ..., -1.3252, -0.2832, -2.7422],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0182, -0.0003,  0.0016,  ..., -0.0048,  0.0105, -0.0062],
        [ 0.0020,  0.0263, -0.0015,  ...,  0.0084, -0.0033, -0.0068],
        [-0.0169,  0.0112, -0.0164,  ..., -0.0020, -0.0068,  0.0165],
        ...,
        [-0.0171,  0.0095, -0.0019,  ...,  0.0089, -0.0100,  0.0030],
        [-0.0120,  0.0094, -0.0181,  ..., -0.0211, -0.0040,  0.0019],
        [ 0.0238, -0.0073, -0.0053,  ..., -0.0108,  0.0011,  0.0014]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.5820,  2.0781, -5.0703,  ..., -0.5562,  0.1743, -1.6201]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 12:52:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a goat makes is called a bleat
The sound that a dog makes is called a bark
The sound that a chimpanzee makes is called a scream
The sound that a frog makes is called a ribbit
The sound that a pigeon makes is called a coo
The sound that a whale makes is called a sing
The sound that a donkey makes is called a bray
The sound that a cattle makes is called a
2024-07-18 12:52:21 root INFO     [order_1_approx] starting weight calculation for The sound that a donkey makes is called a bray
The sound that a cattle makes is called a moo
The sound that a whale makes is called a sing
The sound that a pigeon makes is called a coo
The sound that a goat makes is called a bleat
The sound that a chimpanzee makes is called a scream
The sound that a dog makes is called a bark
The sound that a frog makes is called a
2024-07-18 12:52:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 12:56:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3088,  0.2720, -0.0225,  ...,  0.8535, -1.0742,  0.9468],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.1094,  4.2578,  3.5195,  ...,  0.7520, -2.1992, -7.6641],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0056, -0.0086, -0.0144,  ...,  0.0101, -0.0049,  0.0164],
        [ 0.0048, -0.0075,  0.0010,  ..., -0.0274, -0.0111,  0.0179],
        [ 0.0011,  0.0246,  0.0091,  ..., -0.0076, -0.0098,  0.0490],
        ...,
        [ 0.0018, -0.0102,  0.0078,  ..., -0.0059, -0.0186, -0.0003],
        [-0.0181, -0.0157, -0.0053,  ..., -0.0046,  0.0162, -0.0392],
        [-0.0109, -0.0059,  0.0045,  ...,  0.0107,  0.0126, -0.0327]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1758,  1.1914,  1.3027,  ...,  2.1211,  1.2168, -3.6680]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 12:56:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a donkey makes is called a bray
The sound that a cattle makes is called a moo
The sound that a whale makes is called a sing
The sound that a pigeon makes is called a coo
The sound that a goat makes is called a bleat
The sound that a chimpanzee makes is called a scream
The sound that a dog makes is called a bark
The sound that a frog makes is called a
2024-07-18 12:56:27 root INFO     total operator prediction time: 1966.1808996200562 seconds
2024-07-18 12:56:27 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on animal - youth
2024-07-18 12:56:27 root INFO     building operator animal - youth
2024-07-18 12:56:28 root INFO     [order_1_approx] starting weight calculation for The offspring of a goldfish is referred to as a fingerling
The offspring of a wolf is referred to as a cub
The offspring of a cricket is referred to as a larva
The offspring of a cockroach is referred to as a nymph
The offspring of a fish is referred to as a fingerling
The offspring of a muskrat is referred to as a kit
The offspring of a camel is referred to as a calf
The offspring of a deer is referred to as a
2024-07-18 12:56:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 13:00:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2136,  0.2678, -0.2534,  ...,  1.4463, -0.1023,  0.2107],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7256,  0.5508, -8.3438,  ...,  0.2861,  0.6567,  3.4844],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0070, -0.0117, -0.0042,  ...,  0.0147,  0.0049, -0.0120],
        [-0.0076, -0.0117, -0.0056,  ...,  0.0144, -0.0087,  0.0101],
        [-0.0139, -0.0009,  0.0181,  ..., -0.0034,  0.0198,  0.0038],
        ...,
        [-0.0034, -0.0092, -0.0159,  ...,  0.0019, -0.0011,  0.0045],
        [ 0.0002, -0.0070, -0.0117,  ...,  0.0041, -0.0023,  0.0046],
        [-0.0173, -0.0019, -0.0022,  ...,  0.0020, -0.0009, -0.0019]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2441, -0.5820, -8.2969,  ..., -0.4580,  0.7607,  2.5625]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 13:00:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a goldfish is referred to as a fingerling
The offspring of a wolf is referred to as a cub
The offspring of a cricket is referred to as a larva
The offspring of a cockroach is referred to as a nymph
The offspring of a fish is referred to as a fingerling
The offspring of a muskrat is referred to as a kit
The offspring of a camel is referred to as a calf
The offspring of a deer is referred to as a
2024-07-18 13:00:35 root INFO     [order_1_approx] starting weight calculation for The offspring of a fish is referred to as a fingerling
The offspring of a camel is referred to as a calf
The offspring of a deer is referred to as a fawn
The offspring of a goldfish is referred to as a fingerling
The offspring of a muskrat is referred to as a kit
The offspring of a wolf is referred to as a cub
The offspring of a cockroach is referred to as a nymph
The offspring of a cricket is referred to as a
2024-07-18 13:00:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 13:04:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3228,  0.6953, -1.0322,  ...,  0.4775,  0.1375,  1.0293],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6914, -0.6504, -2.4297,  ...,  1.9512, -1.2510,  2.4629],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0093, -0.0418,  0.0160,  ...,  0.0210, -0.0066, -0.0078],
        [ 0.0020, -0.0068,  0.0334,  ...,  0.0133, -0.0175,  0.0046],
        [-0.0183, -0.0158,  0.0018,  ..., -0.0025,  0.0159,  0.0213],
        ...,
        [ 0.0093, -0.0095, -0.0024,  ...,  0.0236, -0.0122,  0.0003],
        [-0.0035, -0.0110,  0.0030,  ...,  0.0161, -0.0007, -0.0034],
        [-0.0115,  0.0088,  0.0084,  ..., -0.0210,  0.0273,  0.0027]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.4102, -0.1777, -3.1074,  ...,  1.9785, -0.2139,  0.8252]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 13:04:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a fish is referred to as a fingerling
The offspring of a camel is referred to as a calf
The offspring of a deer is referred to as a fawn
The offspring of a goldfish is referred to as a fingerling
The offspring of a muskrat is referred to as a kit
The offspring of a wolf is referred to as a cub
The offspring of a cockroach is referred to as a nymph
The offspring of a cricket is referred to as a
2024-07-18 13:04:43 root INFO     [order_1_approx] starting weight calculation for The offspring of a fish is referred to as a fingerling
The offspring of a camel is referred to as a calf
The offspring of a goldfish is referred to as a fingerling
The offspring of a deer is referred to as a fawn
The offspring of a cockroach is referred to as a nymph
The offspring of a muskrat is referred to as a kit
The offspring of a cricket is referred to as a larva
The offspring of a wolf is referred to as a
2024-07-18 13:04:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 13:08:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2256, -0.5166, -1.0020,  ...,  0.9404, -0.5889,  0.1410],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0088, -4.5703, -7.4453,  ...,  2.2734,  2.9180,  2.6211],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0046, -0.0141, -0.0238,  ..., -0.0072,  0.0061, -0.0225],
        [ 0.0097,  0.0004, -0.0032,  ...,  0.0167,  0.0114, -0.0370],
        [-0.0181, -0.0081, -0.0048,  ..., -0.0072, -0.0007, -0.0442],
        ...,
        [ 0.0085,  0.0025,  0.0098,  ...,  0.0022, -0.0259,  0.0242],
        [ 0.0017, -0.0182, -0.0002,  ...,  0.0038, -0.0063,  0.0161],
        [-0.0059, -0.0049, -0.0070,  ...,  0.0013, -0.0070,  0.0160]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4751, -3.9062, -5.6953,  ...,  1.7520,  2.5332,  1.7949]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 13:08:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a fish is referred to as a fingerling
The offspring of a camel is referred to as a calf
The offspring of a goldfish is referred to as a fingerling
The offspring of a deer is referred to as a fawn
The offspring of a cockroach is referred to as a nymph
The offspring of a muskrat is referred to as a kit
The offspring of a cricket is referred to as a larva
The offspring of a wolf is referred to as a
2024-07-18 13:08:49 root INFO     [order_1_approx] starting weight calculation for The offspring of a deer is referred to as a fawn
The offspring of a cricket is referred to as a larva
The offspring of a camel is referred to as a calf
The offspring of a muskrat is referred to as a kit
The offspring of a wolf is referred to as a cub
The offspring of a fish is referred to as a fingerling
The offspring of a goldfish is referred to as a fingerling
The offspring of a cockroach is referred to as a
2024-07-18 13:08:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 13:12:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6069,  0.1458, -0.1913,  ...,  0.7485, -0.2341,  0.0757],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6758, -1.3193, -3.9785,  ...,  1.2246, -2.0820,  1.4248],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0002, -0.0036,  0.0216,  ...,  0.0106, -0.0075, -0.0183],
        [-0.0003, -0.0071,  0.0195,  ...,  0.0058,  0.0004,  0.0067],
        [-0.0190,  0.0016, -0.0048,  ..., -0.0025,  0.0067, -0.0057],
        ...,
        [ 0.0064, -0.0055,  0.0085,  ..., -0.0024, -0.0033, -0.0131],
        [ 0.0030, -0.0094,  0.0082,  ...,  0.0063, -0.0013,  0.0151],
        [-0.0181,  0.0081, -0.0051,  ..., -0.0031,  0.0016,  0.0112]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.1719, -2.1250, -3.3555,  ...,  0.5425, -1.4277,  1.2607]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 13:12:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a deer is referred to as a fawn
The offspring of a cricket is referred to as a larva
The offspring of a camel is referred to as a calf
The offspring of a muskrat is referred to as a kit
The offspring of a wolf is referred to as a cub
The offspring of a fish is referred to as a fingerling
The offspring of a goldfish is referred to as a fingerling
The offspring of a cockroach is referred to as a
2024-07-18 13:12:57 root INFO     [order_1_approx] starting weight calculation for The offspring of a cricket is referred to as a larva
The offspring of a camel is referred to as a calf
The offspring of a cockroach is referred to as a nymph
The offspring of a muskrat is referred to as a kit
The offspring of a goldfish is referred to as a fingerling
The offspring of a wolf is referred to as a cub
The offspring of a deer is referred to as a fawn
The offspring of a fish is referred to as a
2024-07-18 13:12:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 13:17:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0564,  0.0507, -0.1472,  ...,  0.1216, -0.0375,  0.2603],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1768,  0.1487, -1.0059,  ..., -0.2380,  0.2676,  1.0361],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0153,  0.0159, -0.0036,  ...,  0.0053,  0.0242, -0.0035],
        [-0.0115,  0.0108,  0.0145,  ...,  0.0056, -0.0152, -0.0142],
        [-0.0060, -0.0054, -0.0091,  ..., -0.0177, -0.0031,  0.0035],
        ...,
        [ 0.0088, -0.0094, -0.0197,  ..., -0.0086, -0.0076, -0.0022],
        [-0.0087, -0.0036,  0.0129,  ..., -0.0049,  0.0041,  0.0059],
        [-0.0352, -0.0129,  0.0129,  ..., -0.0045, -0.0039, -0.0107]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7061,  0.2512, -1.1250,  ..., -0.0491, -0.7144,  1.2178]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 13:17:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a cricket is referred to as a larva
The offspring of a camel is referred to as a calf
The offspring of a cockroach is referred to as a nymph
The offspring of a muskrat is referred to as a kit
The offspring of a goldfish is referred to as a fingerling
The offspring of a wolf is referred to as a cub
The offspring of a deer is referred to as a fawn
The offspring of a fish is referred to as a
2024-07-18 13:17:04 root INFO     [order_1_approx] starting weight calculation for The offspring of a cockroach is referred to as a nymph
The offspring of a fish is referred to as a fingerling
The offspring of a deer is referred to as a fawn
The offspring of a cricket is referred to as a larva
The offspring of a wolf is referred to as a cub
The offspring of a goldfish is referred to as a fingerling
The offspring of a muskrat is referred to as a kit
The offspring of a camel is referred to as a
2024-07-18 13:17:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 13:21:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1846, -1.0107, -0.5859,  ...,  0.4922, -0.7974,  0.7959],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4233, -2.6270, -5.5859,  ..., -1.8320,  3.2617,  3.4648],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0188, -0.0384,  0.0204,  ...,  0.0121,  0.0082, -0.0107],
        [-0.0109,  0.0021,  0.0010,  ...,  0.0215,  0.0021, -0.0084],
        [-0.0146,  0.0175, -0.0134,  ..., -0.0132,  0.0129, -0.0169],
        ...,
        [-0.0051,  0.0037, -0.0217,  ..., -0.0094, -0.0110, -0.0138],
        [ 0.0181,  0.0073,  0.0026,  ..., -0.0072, -0.0269, -0.0059],
        [-0.0184,  0.0050,  0.0102,  ..., -0.0074, -0.0130,  0.0127]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8628, -2.7129, -4.6875,  ..., -2.1543,  3.6309,  2.6426]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 13:21:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a cockroach is referred to as a nymph
The offspring of a fish is referred to as a fingerling
The offspring of a deer is referred to as a fawn
The offspring of a cricket is referred to as a larva
The offspring of a wolf is referred to as a cub
The offspring of a goldfish is referred to as a fingerling
The offspring of a muskrat is referred to as a kit
The offspring of a camel is referred to as a
2024-07-18 13:21:10 root INFO     [order_1_approx] starting weight calculation for The offspring of a fish is referred to as a fingerling
The offspring of a deer is referred to as a fawn
The offspring of a wolf is referred to as a cub
The offspring of a camel is referred to as a calf
The offspring of a goldfish is referred to as a fingerling
The offspring of a cockroach is referred to as a nymph
The offspring of a cricket is referred to as a larva
The offspring of a muskrat is referred to as a
2024-07-18 13:21:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 13:25:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0884, -0.5366,  0.6396,  ...,  0.7017,  0.3269,  0.9194],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9800,  0.9297, -3.5664,  ...,  1.5518,  0.4922,  1.0576],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0024, -0.0069,  0.0042,  ...,  0.0176,  0.0057, -0.0052],
        [-0.0153,  0.0107,  0.0160,  ...,  0.0113,  0.0038,  0.0049],
        [-0.0035,  0.0153,  0.0049,  ...,  0.0010,  0.0054, -0.0139],
        ...,
        [ 0.0056, -0.0097,  0.0051,  ..., -0.0078, -0.0004,  0.0017],
        [ 0.0018, -0.0189,  0.0031,  ..., -0.0071,  0.0091,  0.0168],
        [-0.0017, -0.0054, -0.0098,  ..., -0.0063,  0.0038,  0.0201]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5229,  0.0742, -3.0742,  ...,  1.7939,  0.5410,  0.6123]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 13:25:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a fish is referred to as a fingerling
The offspring of a deer is referred to as a fawn
The offspring of a wolf is referred to as a cub
The offspring of a camel is referred to as a calf
The offspring of a goldfish is referred to as a fingerling
The offspring of a cockroach is referred to as a nymph
The offspring of a cricket is referred to as a larva
The offspring of a muskrat is referred to as a
2024-07-18 13:25:19 root INFO     [order_1_approx] starting weight calculation for The offspring of a cricket is referred to as a larva
The offspring of a deer is referred to as a fawn
The offspring of a cockroach is referred to as a nymph
The offspring of a muskrat is referred to as a kit
The offspring of a camel is referred to as a calf
The offspring of a fish is referred to as a fingerling
The offspring of a wolf is referred to as a cub
The offspring of a goldfish is referred to as a
2024-07-18 13:25:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 13:29:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3179,  0.3801, -0.2246,  ..., -0.0938,  0.5825,  0.1814],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0176,  0.8721,  0.3457,  ..., -3.4961, -1.7559,  0.4790],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0056, -0.0186,  0.0067,  ...,  0.0114,  0.0009, -0.0116],
        [-0.0002,  0.0041,  0.0094,  ..., -0.0040, -0.0067, -0.0097],
        [-0.0098,  0.0126, -0.0055,  ..., -0.0122,  0.0079,  0.0049],
        ...,
        [ 0.0042,  0.0040, -0.0017,  ..., -0.0037, -0.0213, -0.0038],
        [-0.0178,  0.0045, -0.0166,  ...,  0.0123, -0.0253,  0.0197],
        [-0.0167, -0.0108,  0.0309,  ..., -0.0146,  0.0085,  0.0017]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0140,  1.4863,  0.1013,  ..., -1.5928, -2.7129,  0.3655]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 13:29:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a cricket is referred to as a larva
The offspring of a deer is referred to as a fawn
The offspring of a cockroach is referred to as a nymph
The offspring of a muskrat is referred to as a kit
The offspring of a camel is referred to as a calf
The offspring of a fish is referred to as a fingerling
The offspring of a wolf is referred to as a cub
The offspring of a goldfish is referred to as a
2024-07-18 13:29:25 root INFO     total operator prediction time: 1977.535762310028 seconds
2024-07-18 13:29:25 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on UK_city - county
2024-07-18 13:29:25 root INFO     building operator UK_city - county
2024-07-18 13:29:25 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of preston is in the county of lancashire
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of wakefield is in the county of yorkshire
In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of coventry is in the county of midlands
In the United Kingdom, the city of winchester is in the county of hampshire
In the United Kingdom, the city of brighton is in the county of sussex
In the United Kingdom, the city of bath is in the county of
2024-07-18 13:29:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 13:33:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3223, -0.1782, -0.0027,  ..., -0.3003, -1.3740, -0.8125],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9702,  1.0195, -4.1445,  ..., -2.0137,  2.7559,  2.7266],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0186,  0.0056, -0.0122,  ...,  0.0214,  0.0089, -0.0086],
        [-0.0157, -0.0268, -0.0303,  ...,  0.0111, -0.0025, -0.0169],
        [ 0.0346,  0.0301, -0.0026,  ...,  0.0255,  0.0226, -0.0136],
        ...,
        [ 0.0262,  0.0149,  0.0101,  ..., -0.0154,  0.0053, -0.0034],
        [ 0.0041,  0.0082, -0.0119,  ..., -0.0115,  0.0132, -0.0158],
        [-0.0074, -0.0238, -0.0165,  ..., -0.0031,  0.0147, -0.0080]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5723, -0.4072, -1.6328,  ..., -1.1094,  3.1016,  1.2217]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 13:33:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of preston is in the county of lancashire
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of wakefield is in the county of yorkshire
In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of coventry is in the county of midlands
In the United Kingdom, the city of winchester is in the county of hampshire
In the United Kingdom, the city of brighton is in the county of sussex
In the United Kingdom, the city of bath is in the county of
2024-07-18 13:33:34 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of wakefield is in the county of yorkshire
In the United Kingdom, the city of brighton is in the county of sussex
In the United Kingdom, the city of coventry is in the county of midlands
In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of winchester is in the county of hampshire
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of bath is in the county of somerset
In the United Kingdom, the city of preston is in the county of
2024-07-18 13:33:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 13:37:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0952, -0.5068, -0.1714,  ...,  0.3772, -0.5776,  0.3462],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3098, -3.8613, -7.4609,  ...,  0.1140,  3.0840, -2.4141],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0159,  0.0136, -0.0006,  ...,  0.0074,  0.0034, -0.0115],
        [-0.0271,  0.0014,  0.0018,  ...,  0.0212, -0.0258,  0.0100],
        [ 0.0479,  0.0282, -0.0334,  ..., -0.0077,  0.0202,  0.0045],
        ...,
        [ 0.0141, -0.0086, -0.0004,  ...,  0.0019, -0.0008,  0.0085],
        [ 0.0131, -0.0047, -0.0018,  ..., -0.0076,  0.0160,  0.0048],
        [-0.0184,  0.0105,  0.0007,  ...,  0.0079,  0.0116, -0.0025]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1897, -3.1719, -8.0625,  ..., -0.6299,  3.5117, -2.3730]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 13:37:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of wakefield is in the county of yorkshire
In the United Kingdom, the city of brighton is in the county of sussex
In the United Kingdom, the city of coventry is in the county of midlands
In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of winchester is in the county of hampshire
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of bath is in the county of somerset
In the United Kingdom, the city of preston is in the county of
2024-07-18 13:37:41 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of preston is in the county of lancashire
In the United Kingdom, the city of coventry is in the county of midlands
In the United Kingdom, the city of wakefield is in the county of yorkshire
In the United Kingdom, the city of bath is in the county of somerset
In the United Kingdom, the city of brighton is in the county of sussex
In the United Kingdom, the city of winchester is in the county of hampshire
In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of sheffield is in the county of
2024-07-18 13:37:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 13:41:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5410,  0.6050,  0.6953,  ...,  0.9048,  0.6558,  0.3923],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3018, -4.1445, -4.9219,  ...,  2.9590,  2.2324,  1.5811],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0159,  0.0118, -0.0283,  ..., -0.0049, -0.0058, -0.0063],
        [-0.0090,  0.0143,  0.0083,  ...,  0.0142,  0.0124, -0.0044],
        [ 0.0484,  0.0204, -0.0316,  ..., -0.0071, -0.0131, -0.0227],
        ...,
        [ 0.0073, -0.0066, -0.0021,  ..., -0.0164, -0.0103, -0.0069],
        [ 0.0027, -0.0093,  0.0122,  ..., -0.0182,  0.0215, -0.0007],
        [ 0.0046, -0.0148, -0.0083,  ..., -0.0033, -0.0092, -0.0156]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3882, -4.7031, -4.1367,  ...,  3.1523,  2.6680,  1.5137]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 13:41:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of preston is in the county of lancashire
In the United Kingdom, the city of coventry is in the county of midlands
In the United Kingdom, the city of wakefield is in the county of yorkshire
In the United Kingdom, the city of bath is in the county of somerset
In the United Kingdom, the city of brighton is in the county of sussex
In the United Kingdom, the city of winchester is in the county of hampshire
In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of sheffield is in the county of
2024-07-18 13:41:51 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of preston is in the county of lancashire
In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of wakefield is in the county of yorkshire
In the United Kingdom, the city of coventry is in the county of midlands
In the United Kingdom, the city of bath is in the county of somerset
In the United Kingdom, the city of brighton is in the county of sussex
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of winchester is in the county of
2024-07-18 13:41:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 13:45:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1190, -0.2068, -0.5820,  ...,  1.5654,  1.1953,  0.4360],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2852, -5.7148, -5.9883,  ..., -2.5859,  6.7891, -0.7563],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.1475e-02, -1.1177e-03, -7.4043e-03,  ...,  2.2217e-02,
          1.5421e-03,  4.1161e-03],
        [-1.3313e-02,  1.8509e-02,  1.9283e-03,  ...,  2.8458e-02,
         -1.1215e-02, -2.5673e-03],
        [ 3.9276e-02,  3.5095e-02,  2.6703e-05,  ...,  1.9318e-02,
         -7.6141e-03,  9.0179e-03],
        ...,
        [ 1.1360e-02, -1.1383e-02,  4.4708e-03,  ..., -1.8692e-03,
          5.7678e-03,  8.9493e-03],
        [-1.5221e-03,  1.1581e-02,  5.4092e-03,  ..., -1.3313e-02,
         -7.8430e-03, -5.6229e-03],
        [-6.1646e-03, -8.9645e-03, -3.3741e-03,  ...,  3.4065e-03,
         -7.8201e-04, -1.2245e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5928, -5.8281, -5.6328,  ..., -2.4551,  6.3711, -0.6934]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 13:45:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of preston is in the county of lancashire
In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of wakefield is in the county of yorkshire
In the United Kingdom, the city of coventry is in the county of midlands
In the United Kingdom, the city of bath is in the county of somerset
In the United Kingdom, the city of brighton is in the county of sussex
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of winchester is in the county of
2024-07-18 13:45:58 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of bath is in the county of somerset
In the United Kingdom, the city of brighton is in the county of sussex
In the United Kingdom, the city of preston is in the county of lancashire
In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of winchester is in the county of hampshire
In the United Kingdom, the city of coventry is in the county of midlands
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of wakefield is in the county of
2024-07-18 13:45:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 13:50:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1001,  0.5488, -0.2168,  ...,  1.0361, -0.5303, -0.0303],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7837, -4.7227, -4.2656,  ..., -1.9268,  2.5020, -1.1455],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0221,  0.0093,  0.0077,  ...,  0.0079, -0.0002, -0.0045],
        [ 0.0205, -0.0095, -0.0218,  ...,  0.0386, -0.0053, -0.0123],
        [ 0.0019, -0.0055, -0.0267,  ...,  0.0009, -0.0092,  0.0189],
        ...,
        [-0.0181, -0.0061,  0.0065,  ..., -0.0308,  0.0112, -0.0082],
        [ 0.0093, -0.0094, -0.0013,  ..., -0.0253, -0.0035,  0.0228],
        [ 0.0017, -0.0083, -0.0100,  ..., -0.0024, -0.0083, -0.0011]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.8394, -4.3672, -3.3672,  ..., -2.3320,  2.4023, -1.1660]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 13:50:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of bath is in the county of somerset
In the United Kingdom, the city of brighton is in the county of sussex
In the United Kingdom, the city of preston is in the county of lancashire
In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of winchester is in the county of hampshire
In the United Kingdom, the city of coventry is in the county of midlands
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of wakefield is in the county of
2024-07-18 13:50:09 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of winchester is in the county of hampshire
In the United Kingdom, the city of wakefield is in the county of yorkshire
In the United Kingdom, the city of preston is in the county of lancashire
In the United Kingdom, the city of coventry is in the county of midlands
In the United Kingdom, the city of bath is in the county of somerset
In the United Kingdom, the city of brighton is in the county of
2024-07-18 13:50:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 13:54:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0579, -0.3516, -0.9116,  ...,  0.7256, -0.1196, -0.1080],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.2051, -3.1680, -5.2734,  ..., -3.2012,  6.8750,  2.4102],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0218, -0.0372, -0.0119,  ...,  0.0251,  0.0065, -0.0280],
        [ 0.0116, -0.0103, -0.0284,  ...,  0.0359, -0.0232, -0.0067],
        [ 0.0023,  0.0117, -0.0095,  ...,  0.0319,  0.0063, -0.0097],
        ...,
        [-0.0249, -0.0068,  0.0284,  ..., -0.0098,  0.0068, -0.0098],
        [ 0.0145,  0.0237,  0.0174,  ..., -0.0443,  0.0126,  0.0153],
        [-0.0220,  0.0241, -0.0112,  ...,  0.0143, -0.0137, -0.0260]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9668, -2.6777, -5.1758,  ..., -4.5547,  7.5664,  2.1562]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 13:54:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of winchester is in the county of hampshire
In the United Kingdom, the city of wakefield is in the county of yorkshire
In the United Kingdom, the city of preston is in the county of lancashire
In the United Kingdom, the city of coventry is in the county of midlands
In the United Kingdom, the city of bath is in the county of somerset
In the United Kingdom, the city of brighton is in the county of
2024-07-18 13:54:19 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of brighton is in the county of sussex
In the United Kingdom, the city of preston is in the county of lancashire
In the United Kingdom, the city of winchester is in the county of hampshire
In the United Kingdom, the city of bath is in the county of somerset
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of wakefield is in the county of yorkshire
In the United Kingdom, the city of coventry is in the county of
2024-07-18 13:54:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 13:58:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3108,  0.1002, -0.1946,  ...,  0.6260, -0.5923, -1.2812],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.3936, -3.9180, -7.3047,  ..., -1.3398,  2.7520, -0.7729],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.4473e-02,  1.3191e-02,  1.7319e-03,  ...,  2.2003e-02,
          1.1375e-02,  4.3182e-03],
        [ 1.3016e-02,  9.4528e-03, -2.2797e-02,  ...,  3.9825e-02,
         -6.2103e-03,  1.1917e-02],
        [ 3.9551e-02,  3.5950e-02, -4.2725e-02,  ...,  1.0300e-02,
         -1.9058e-02,  3.3325e-02],
        ...,
        [ 1.4038e-02,  2.1698e-02,  1.6647e-02,  ...,  5.4855e-03,
         -9.1553e-05,  9.2392e-03],
        [ 4.0321e-03, -9.8228e-04, -1.0223e-02,  ..., -1.0132e-02,
         -7.9117e-03,  1.1345e-02],
        [-4.8828e-04,  1.2733e-02, -1.3485e-03,  ...,  4.0894e-03,
          1.7303e-02, -4.2992e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9805, -3.5723, -6.0312,  ..., -1.2432,  3.2578, -1.4141]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 13:58:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of brighton is in the county of sussex
In the United Kingdom, the city of preston is in the county of lancashire
In the United Kingdom, the city of winchester is in the county of hampshire
In the United Kingdom, the city of bath is in the county of somerset
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of crawley is in the county of sussex
In the United Kingdom, the city of wakefield is in the county of yorkshire
In the United Kingdom, the city of coventry is in the county of
2024-07-18 13:58:27 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of preston is in the county of lancashire
In the United Kingdom, the city of bath is in the county of somerset
In the United Kingdom, the city of wakefield is in the county of yorkshire
In the United Kingdom, the city of brighton is in the county of sussex
In the United Kingdom, the city of winchester is in the county of hampshire
In the United Kingdom, the city of coventry is in the county of midlands
In the United Kingdom, the city of crawley is in the county of
2024-07-18 13:58:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.4
2024-07-18 14:02:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0231,  0.6133, -0.4092,  ...,  1.0537, -0.0924,  0.7476],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9385, -3.6953, -5.0469,  ..., -2.9023,  4.0977, -3.7793],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0285,  0.0100,  0.0299,  ...,  0.0370,  0.0044, -0.0177],
        [-0.0065,  0.0025,  0.0121,  ...,  0.0399, -0.0181, -0.0129],
        [ 0.0123,  0.0363,  0.0132,  ...,  0.0032, -0.0194, -0.0154],
        ...,
        [-0.0139, -0.0088,  0.0067,  ...,  0.0180, -0.0154,  0.0018],
        [-0.0077,  0.0066,  0.0106,  ...,  0.0156, -0.0288,  0.0090],
        [ 0.0070, -0.0166,  0.0056,  ...,  0.0166,  0.0118,  0.0301]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0566, -2.9414, -4.3398,  ..., -1.8828,  3.9316, -2.7500]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 14:02:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of preston is in the county of lancashire
In the United Kingdom, the city of bath is in the county of somerset
In the United Kingdom, the city of wakefield is in the county of yorkshire
In the United Kingdom, the city of brighton is in the county of sussex
In the United Kingdom, the city of winchester is in the county of hampshire
In the United Kingdom, the city of coventry is in the county of midlands
In the United Kingdom, the city of crawley is in the county of
2024-07-18 14:02:36 root INFO     total operator prediction time: 1991.2315907478333 seconds
