2024-07-16 10:52:47 root INFO     loading model + tokenizer
2024-07-16 10:53:03 root INFO     model + tokenizer loaded
2024-07-16 10:53:03 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on old country capital
2024-07-16 10:53:03 root INFO     building operator old country capital
2024-07-16 10:53:04 root INFO     [order_1_approx] starting weight calculation for Colombia B
Germany B
Argentina B
Japan T
Canada O
South Korea S
Saudi Arabia R
United States
2024-07-16 10:53:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-16 10:59:43 root INFO     loading model + tokenizer
2024-07-16 11:00:00 root INFO     model + tokenizer loaded
2024-07-16 11:00:00 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on old country capital
2024-07-16 11:00:00 root INFO     building operator old country capital
2024-07-16 11:00:01 root INFO     [order_1_approx] starting weight calculation for The capital city of Mexico is ['M', 'e', 'x', 'i', 'c', 'o', ' ', 'C', 'i', 't', 'y']
The capital city of Pakistan is ['I', 's', 'l', 'a', 'm', 'a', 'b', 'a', 'd']
The capital city of Russia is ['M', 'o', 's', 'c', 'o', 'w']
The capital city of Canada is ['O', 't', 't', 'a', 'w', 'a']
The capital city of Peru is ['L', 'i', 'm', 'a']
The capital city of Germany is ['B', 'e', 'r', 'l', 'i', 'n']
The capital city of Saudi Arabia is ['R', 'i', 'y', 'a', 'd', 'h']
The capital city of Colombia is
2024-07-16 11:00:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-16 11:01:00 root INFO     loading model + tokenizer
2024-07-16 11:01:16 root INFO     model + tokenizer loaded
2024-07-16 11:01:16 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on old country capital
2024-07-16 11:01:16 root INFO     building operator old country capital
2024-07-16 11:02:22 root INFO     loading model + tokenizer
2024-07-16 11:02:38 root INFO     model + tokenizer loaded
2024-07-16 11:02:38 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on old country capital
2024-07-16 11:02:38 root INFO     building operator old country capital
2024-07-16 11:02:39 root INFO     [order_1_approx] starting weight calculation for The capital city of Turkey is Ankara
The capital city of Pakistan is Islamabad
The capital city of Colombia is Bogot\u00e1
The capital city of Argentina is Buenos Aires
The capital city of Peru is Lima
The capital city of Spain is Madrid
The capital city of Saudi Arabia is Riyadh
The capital city of Japan is
2024-07-16 11:02:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-16 11:06:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8101,  0.1172,  0.4688,  ..., -0.0338, -0.4028,  0.4600],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6489,  0.5039, -2.5020,  ..., -1.9102, -0.3154, -1.3359],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0101,  0.0038,  0.0136,  ...,  0.0073,  0.0196, -0.0224],
        [ 0.0055,  0.0052,  0.0235,  ...,  0.0181,  0.0044,  0.0093],
        [ 0.0055, -0.0065, -0.0088,  ..., -0.0032, -0.0049, -0.0036],
        ...,
        [-0.0036,  0.0019,  0.0259,  ...,  0.0157, -0.0006,  0.0121],
        [ 0.0042, -0.0036, -0.0300,  ..., -0.0239, -0.0196, -0.0018],
        [-0.0186, -0.0029,  0.0120,  ..., -0.0036,  0.0131, -0.0154]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7583, -0.0098, -2.0449,  ..., -2.4082,  0.3657, -1.9180]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 11:06:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The capital city of Turkey is Ankara
The capital city of Pakistan is Islamabad
The capital city of Colombia is Bogot\u00e1
The capital city of Argentina is Buenos Aires
The capital city of Peru is Lima
The capital city of Spain is Madrid
The capital city of Saudi Arabia is Riyadh
The capital city of Japan is
2024-07-16 11:06:52 root INFO     [order_1_approx] starting weight calculation for The capital city of Turkey is Ankara
The capital city of Peru is Lima
The capital city of Pakistan is Islamabad
The capital city of Argentina is Buenos Aires
The capital city of Colombia is Bogot\u00e1
The capital city of Japan is Tokyo
The capital city of Spain is Madrid
The capital city of Saudi Arabia is
2024-07-16 11:06:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-16 11:11:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7651, -0.9536,  0.2993,  ..., -0.4087,  0.2363,  0.3379],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.1797, -1.4707, -0.9688,  ..., -1.3984,  1.1846,  0.1863],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0010, -0.0005,  0.0034,  ..., -0.0085,  0.0009, -0.0182],
        [-0.0147,  0.0097,  0.0125,  ...,  0.0409,  0.0109,  0.0165],
        [ 0.0083,  0.0026, -0.0276,  ..., -0.0125, -0.0201, -0.0091],
        ...,
        [-0.0105, -0.0010, -0.0131,  ...,  0.0126, -0.0104, -0.0015],
        [ 0.0377, -0.0322, -0.0095,  ..., -0.0559,  0.0088, -0.0126],
        [ 0.0018, -0.0139, -0.0043,  ...,  0.0006,  0.0069, -0.0191]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.3242, -1.5576, -0.7490,  ..., -0.3359,  2.0020,  0.7246]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 11:11:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The capital city of Turkey is Ankara
The capital city of Peru is Lima
The capital city of Pakistan is Islamabad
The capital city of Argentina is Buenos Aires
The capital city of Colombia is Bogot\u00e1
The capital city of Japan is Tokyo
The capital city of Spain is Madrid
The capital city of Saudi Arabia is
2024-07-16 11:11:06 root INFO     [order_1_approx] starting weight calculation for The capital city of Peru is Lima
The capital city of Colombia is Bogot\u00e1
The capital city of Saudi Arabia is Riyadh
The capital city of Turkey is Ankara
The capital city of Japan is Tokyo
The capital city of Pakistan is Islamabad
The capital city of Argentina is Buenos Aires
The capital city of Spain is
2024-07-16 11:11:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-16 11:15:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6030,  0.1306, -0.2029,  ...,  0.4050, -0.2465,  0.0194],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2949,  0.0625,  0.2261,  ...,  1.0381, -0.0884, -3.6836],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0083,  0.0068,  0.0018,  ..., -0.0021,  0.0153, -0.0088],
        [ 0.0006,  0.0106,  0.0329,  ...,  0.0186, -0.0041, -0.0128],
        [-0.0144,  0.0084, -0.0276,  ...,  0.0047,  0.0043,  0.0084],
        ...,
        [-0.0049, -0.0016,  0.0330,  ...,  0.0345, -0.0133, -0.0204],
        [-0.0022, -0.0018, -0.0239,  ...,  0.0054,  0.0061, -0.0020],
        [-0.0060, -0.0151, -0.0036,  ..., -0.0196,  0.0083,  0.0006]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9331, -0.3977, -0.0074,  ...,  0.4863,  0.4175, -4.0273]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 11:15:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The capital city of Peru is Lima
The capital city of Colombia is Bogot\u00e1
The capital city of Saudi Arabia is Riyadh
The capital city of Turkey is Ankara
The capital city of Japan is Tokyo
The capital city of Pakistan is Islamabad
The capital city of Argentina is Buenos Aires
The capital city of Spain is
2024-07-16 11:15:19 root INFO     [order_1_approx] starting weight calculation for The capital city of Argentina is Buenos Aires
The capital city of Colombia is Bogot\u00e1
The capital city of Japan is Tokyo
The capital city of Turkey is Ankara
The capital city of Spain is Madrid
The capital city of Pakistan is Islamabad
The capital city of Saudi Arabia is Riyadh
The capital city of Peru is
2024-07-16 11:15:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-16 11:19:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3582, -0.2013, -1.3281,  ...,  0.3496,  1.0400,  0.1194],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.7422,  1.0059, -0.7026,  ...,  0.6348,  2.7422,  1.4443],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0058,  0.0073, -0.0044,  ...,  0.0007,  0.0017, -0.0016],
        [-0.0058,  0.0071,  0.0275,  ...,  0.0096, -0.0093, -0.0068],
        [ 0.0004, -0.0056, -0.0221,  ...,  0.0041,  0.0102,  0.0096],
        ...,
        [ 0.0056,  0.0144,  0.0253,  ...,  0.0009, -0.0116, -0.0006],
        [-0.0117, -0.0161,  0.0024,  ..., -0.0008, -0.0012, -0.0018],
        [ 0.0154, -0.0167, -0.0047,  ...,  0.0048,  0.0036, -0.0014]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7949,  0.8198, -1.8418,  ...,  0.9150,  2.2070,  0.3760]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 11:19:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The capital city of Argentina is Buenos Aires
The capital city of Colombia is Bogot\u00e1
The capital city of Japan is Tokyo
The capital city of Turkey is Ankara
The capital city of Spain is Madrid
The capital city of Pakistan is Islamabad
The capital city of Saudi Arabia is Riyadh
The capital city of Peru is
2024-07-16 11:19:34 root INFO     [order_1_approx] starting weight calculation for The capital city of Peru is Lima
The capital city of Pakistan is Islamabad
The capital city of Turkey is Ankara
The capital city of Saudi Arabia is Riyadh
The capital city of Spain is Madrid
The capital city of Argentina is Buenos Aires
The capital city of Japan is Tokyo
The capital city of Colombia is
2024-07-16 11:19:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-16 11:23:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0669,  0.5898, -0.6519,  ...,  1.1602,  0.8936,  0.3462],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.7695,  0.3101,  3.8906,  ..., -1.8887, -1.3408,  3.4727],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0130, -0.0036, -0.0043,  ...,  0.0230, -0.0030, -0.0064],
        [ 0.0114, -0.0002,  0.0125,  ...,  0.0231, -0.0048,  0.0094],
        [-0.0279, -0.0140, -0.0456,  ...,  0.0265,  0.0058, -0.0089],
        ...,
        [ 0.0096,  0.0051,  0.0180,  ...,  0.0184,  0.0125,  0.0160],
        [ 0.0140, -0.0193,  0.0131,  ..., -0.0274,  0.0030,  0.0103],
        [ 0.0044, -0.0088, -0.0138,  ...,  0.0255,  0.0148, -0.0310]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.0938, -0.2007,  1.6035,  ..., -3.0234,  0.0566,  2.5703]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 11:23:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The capital city of Peru is Lima
The capital city of Pakistan is Islamabad
The capital city of Turkey is Ankara
The capital city of Saudi Arabia is Riyadh
The capital city of Spain is Madrid
The capital city of Argentina is Buenos Aires
The capital city of Japan is Tokyo
The capital city of Colombia is
2024-07-16 11:23:44 root INFO     [order_1_approx] starting weight calculation for The capital city of Peru is Lima
The capital city of Saudi Arabia is Riyadh
The capital city of Japan is Tokyo
The capital city of Pakistan is Islamabad
The capital city of Spain is Madrid
The capital city of Colombia is Bogot\u00e1
The capital city of Turkey is Ankara
The capital city of Argentina is
2024-07-16 11:23:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-16 11:27:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.8149e-01,  6.2744e-01, -5.4980e-01,  ..., -2.8687e-01,
         7.4609e-01, -2.7466e-04], device='cuda:0', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1328,  2.8594, -3.4355,  ...,  0.3994, -0.4976, -1.9932],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0081,  0.0070, -0.0041,  ...,  0.0137,  0.0054, -0.0058],
        [ 0.0019, -0.0075,  0.0147,  ...,  0.0163,  0.0088, -0.0065],
        [ 0.0007,  0.0153, -0.0128,  ...,  0.0050,  0.0005,  0.0122],
        ...,
        [-0.0010, -0.0045,  0.0230,  ...,  0.0109, -0.0161,  0.0048],
        [-0.0096, -0.0146,  0.0029,  ..., -0.0187,  0.0277,  0.0042],
        [-0.0044, -0.0075, -0.0161,  ..., -0.0165, -0.0089,  0.0047]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2393,  2.7793, -3.7012,  ...,  0.9258, -0.2783, -1.4111]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 11:27:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The capital city of Peru is Lima
The capital city of Saudi Arabia is Riyadh
The capital city of Japan is Tokyo
The capital city of Pakistan is Islamabad
The capital city of Spain is Madrid
The capital city of Colombia is Bogot\u00e1
The capital city of Turkey is Ankara
The capital city of Argentina is
2024-07-16 11:27:56 root INFO     [order_1_approx] starting weight calculation for The capital city of Japan is Tokyo
The capital city of Argentina is Buenos Aires
The capital city of Pakistan is Islamabad
The capital city of Spain is Madrid
The capital city of Peru is Lima
The capital city of Colombia is Bogot\u00e1
The capital city of Saudi Arabia is Riyadh
The capital city of Turkey is
2024-07-16 11:27:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-16 11:32:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3149,  0.4121, -0.4614,  ..., -0.6235, -0.1089,  0.2556],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1783,  2.2812,  0.5664,  ...,  2.5605,  2.0879, -6.2734],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0006, -0.0071,  0.0026,  ...,  0.0102,  0.0315, -0.0293],
        [-0.0145, -0.0307,  0.0159,  ...,  0.0097, -0.0134,  0.0084],
        [ 0.0100,  0.0090, -0.0145,  ...,  0.0004, -0.0150, -0.0269],
        ...,
        [-0.0186, -0.0077,  0.0006,  ...,  0.0247,  0.0052,  0.0048],
        [ 0.0066, -0.0167, -0.0133,  ..., -0.0136,  0.0119,  0.0285],
        [ 0.0003,  0.0005,  0.0011,  ...,  0.0074,  0.0248, -0.0285]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4893,  2.0020, -0.3149,  ...,  2.6738,  2.8965, -6.0820]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 11:32:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The capital city of Japan is Tokyo
The capital city of Argentina is Buenos Aires
The capital city of Pakistan is Islamabad
The capital city of Spain is Madrid
The capital city of Peru is Lima
The capital city of Colombia is Bogot\u00e1
The capital city of Saudi Arabia is Riyadh
The capital city of Turkey is
2024-07-16 11:32:11 root INFO     [order_1_approx] starting weight calculation for The capital city of Japan is Tokyo
The capital city of Argentina is Buenos Aires
The capital city of Peru is Lima
The capital city of Turkey is Ankara
The capital city of Colombia is Bogot\u00e1
The capital city of Spain is Madrid
The capital city of Saudi Arabia is Riyadh
The capital city of Pakistan is
2024-07-16 11:32:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-16 11:36:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1035,  0.0009, -0.3552,  ..., -0.3584,  0.5044,  0.0632],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6860, -2.4551,  0.2993,  ...,  0.9951,  2.7246,  0.8696],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0042, -0.0082, -0.0078,  ...,  0.0407,  0.0064, -0.0064],
        [ 0.0161, -0.0163,  0.0402,  ...,  0.0013, -0.0032,  0.0074],
        [-0.0037,  0.0113, -0.0255,  ...,  0.0056, -0.0016, -0.0112],
        ...,
        [ 0.0004, -0.0053,  0.0368,  ..., -0.0114, -0.0073,  0.0171],
        [ 0.0003,  0.0083, -0.0049,  ..., -0.0026, -0.0077,  0.0082],
        [-0.0104, -0.0051, -0.0039,  ...,  0.0160, -0.0054, -0.0100]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1895, -1.3223,  0.1814,  ...,  0.9717,  2.3184, -0.5522]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 11:36:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The capital city of Japan is Tokyo
The capital city of Argentina is Buenos Aires
The capital city of Peru is Lima
The capital city of Turkey is Ankara
The capital city of Colombia is Bogot\u00e1
The capital city of Spain is Madrid
The capital city of Saudi Arabia is Riyadh
The capital city of Pakistan is
2024-07-16 11:36:26 root INFO     total operator prediction time: 2028.0289063453674 seconds
2024-07-16 14:40:55 root INFO     loading model + tokenizer
2024-07-16 14:41:12 root INFO     model + tokenizer loaded
2024-07-16 14:41:12 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - part
2024-07-16 14:41:12 root INFO     building operator meronyms - part
2024-07-16 14:41:13 root INFO     [order_1_approx] starting weight calculation for A part of a pub is a barbarroomtaproomsaloonginmill
A part of a dress is a sleeveslideplastronzipzipperfastenerhemlinebodicenecklinewaistlinebeltslide_fastenerzip_fastener
A part of a gun is a triggerholdersafetyextractorclipmagazinebarrelhammerviewfindergunsightstockcartridgeremoverchamberejectorcockcatchfindermechanismgunstocklockactionmuzzlekeygunlockcartridge_holdergun_muzzlegun-sightaction_mechanismgun_chambergun_triggerfiring_mechanismsafety_lockcartridge_extractorfiring_chambercartridge_ejectorview_findercartridge_clipgun_barrelsafety_catchcartridge_remover
A part of a litre is a millilitreccmlmillilitercldldecilitermilcubic_centimetrecentiliterdecilitrecubic_centimetercentilitrecubic_millimetercubic_millimetre
A part of a movie is a scenesubtitlesfootagecreditepisodeshotsequencecreditscaption
A part of a byte is a bit
A part of a deer is a antlerantlerswithersflagscut
A part of a radio is a
2024-07-16 14:41:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 14:41:49 root INFO     loading model + tokenizer
2024-07-16 14:42:05 root INFO     model + tokenizer loaded
2024-07-16 14:42:05 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - part
2024-07-16 14:42:05 root INFO     building operator meronyms - part
2024-07-16 14:42:06 root INFO     [order_1_approx] starting weight calculation for A part of a academia is a college
A part of a jail is a cell
A part of a piano is a keyboard
A part of a brush is a bristle
A part of a gramm is a milligram
A part of a door is a hinge
A part of a day is a hour
A part of a tripod is a
2024-07-16 14:42:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 14:45:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1823,  0.0708, -0.2332,  ...,  0.6836, -1.1465, -0.8110],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.2031,  1.8926,  2.0039,  ...,  1.7891, -2.3379,  2.7754],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0392, -0.0132, -0.0064,  ..., -0.0053, -0.0126, -0.0278],
        [-0.0049, -0.0028,  0.0089,  ..., -0.0077, -0.0146,  0.0038],
        [-0.0047, -0.0021,  0.0379,  ..., -0.0075,  0.0029,  0.0165],
        ...,
        [-0.0026,  0.0158,  0.0008,  ...,  0.0118,  0.0140,  0.0062],
        [-0.0012,  0.0271, -0.0043,  ...,  0.0046,  0.0056,  0.0141],
        [ 0.0037,  0.0056,  0.0047,  ..., -0.0050,  0.0212,  0.0559]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5947,  1.8662,  1.6934,  ...,  1.4941, -2.2305,  3.6289]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 14:45:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a academia is a college
A part of a jail is a cell
A part of a piano is a keyboard
A part of a brush is a bristle
A part of a gramm is a milligram
A part of a door is a hinge
A part of a day is a hour
A part of a tripod is a
2024-07-16 14:45:51 root INFO     [order_1_approx] starting weight calculation for A part of a brush is a bristle
A part of a tripod is a leg
A part of a piano is a keyboard
A part of a day is a hour
A part of a academia is a college
A part of a door is a hinge
A part of a jail is a cell
A part of a gramm is a
2024-07-16 14:45:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 14:49:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7598,  0.5947,  0.4312,  ...,  0.6064, -0.4075,  0.2271],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4668,  0.2136, -4.6602,  ..., -3.2969, -2.2676, -1.2871],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.9363e-02, -1.3062e-02, -7.0915e-03,  ...,  2.1038e-03,
          7.1411e-03, -1.0643e-02],
        [ 1.2131e-02,  1.0323e-02,  1.6846e-02,  ...,  3.7613e-03,
         -1.2299e-02,  7.0648e-03],
        [ 6.3019e-03, -9.0485e-03,  1.1749e-03,  ...,  1.5297e-03,
         -1.9341e-03,  9.7961e-03],
        ...,
        [ 7.2556e-03,  9.0122e-05,  1.4229e-03,  ...,  1.5274e-02,
         -1.8072e-03, -8.2207e-04],
        [ 8.1635e-04, -7.1945e-03,  1.3409e-03,  ...,  7.7362e-03,
          6.7444e-03,  1.4435e-02],
        [ 5.6305e-03, -7.7477e-03, -1.3000e-02,  ..., -8.4991e-03,
          1.0956e-02,  9.0103e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6797,  0.6216, -4.3164,  ..., -3.1191, -2.0840, -1.2422]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 14:49:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a brush is a bristle
A part of a tripod is a leg
A part of a piano is a keyboard
A part of a day is a hour
A part of a academia is a college
A part of a door is a hinge
A part of a jail is a cell
A part of a gramm is a
2024-07-16 14:49:37 root INFO     [order_1_approx] starting weight calculation for A part of a academia is a college
A part of a piano is a keyboard
A part of a gramm is a milligram
A part of a day is a hour
A part of a tripod is a leg
A part of a door is a hinge
A part of a jail is a cell
A part of a brush is a
2024-07-16 14:49:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 14:53:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1804, -0.2106,  0.2759,  ...,  1.4668, -1.7979,  0.8037],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.9297, -0.9219, -5.6719,  ..., -2.5703, -0.3628,  3.1816],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0245, -0.0157,  0.0053,  ...,  0.0163,  0.0104, -0.0034],
        [-0.0113,  0.0156,  0.0272,  ...,  0.0250, -0.0055,  0.0076],
        [ 0.0084, -0.0061,  0.0232,  ...,  0.0036,  0.0009,  0.0108],
        ...,
        [-0.0069, -0.0134, -0.0083,  ...,  0.0162, -0.0046,  0.0029],
        [ 0.0195, -0.0082,  0.0002,  ...,  0.0058, -0.0033,  0.0004],
        [ 0.0160, -0.0027,  0.0039,  ..., -0.0054,  0.0290,  0.0196]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4219,  0.0811, -5.0547,  ..., -2.3281,  0.0713,  3.9414]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 14:53:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a academia is a college
A part of a piano is a keyboard
A part of a gramm is a milligram
A part of a day is a hour
A part of a tripod is a leg
A part of a door is a hinge
A part of a jail is a cell
A part of a brush is a
2024-07-16 14:53:24 root INFO     [order_1_approx] starting weight calculation for A part of a gramm is a milligram
A part of a jail is a cell
A part of a piano is a keyboard
A part of a door is a hinge
A part of a tripod is a leg
A part of a brush is a bristle
A part of a academia is a college
A part of a day is a
2024-07-16 14:53:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 14:57:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1448, -1.1377,  0.4619,  ...,  0.4702,  0.1307,  0.6655],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 6.4180,  0.1836, -1.1748,  ..., -3.3555,  1.5664,  0.0225],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0026, -0.0135,  0.0107,  ..., -0.0125,  0.0116, -0.0062],
        [-0.0040, -0.0140, -0.0003,  ..., -0.0058, -0.0069,  0.0058],
        [-0.0156, -0.0078, -0.0097,  ..., -0.0180, -0.0029,  0.0111],
        ...,
        [-0.0339,  0.0112, -0.0004,  ..., -0.0118, -0.0049,  0.0128],
        [ 0.0126,  0.0182, -0.0059,  ...,  0.0037, -0.0183,  0.0230],
        [ 0.0320, -0.0153, -0.0020,  ..., -0.0125,  0.0343, -0.0219]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.3750,  0.3472, -0.6030,  ..., -3.2402,  2.0898,  0.2416]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 14:57:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a gramm is a milligram
A part of a jail is a cell
A part of a piano is a keyboard
A part of a door is a hinge
A part of a tripod is a leg
A part of a brush is a bristle
A part of a academia is a college
A part of a day is a
2024-07-16 14:57:10 root INFO     [order_1_approx] starting weight calculation for A part of a gramm is a milligram
A part of a door is a hinge
A part of a academia is a college
A part of a day is a hour
A part of a jail is a cell
A part of a tripod is a leg
A part of a brush is a bristle
A part of a piano is a
2024-07-16 14:57:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 15:00:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2891, -0.4482,  0.3062,  ..., -0.0159, -0.0961, -0.2476],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3594, -1.7656, -3.1055,  ..., -4.4258, -0.7827, -2.1680],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0005, -0.0287, -0.0014,  ...,  0.0058, -0.0049, -0.0142],
        [-0.0109,  0.0213,  0.0047,  ...,  0.0002, -0.0207, -0.0011],
        [-0.0119, -0.0045, -0.0123,  ...,  0.0064,  0.0169, -0.0044],
        ...,
        [-0.0146, -0.0131,  0.0002,  ...,  0.0024, -0.0204,  0.0063],
        [-0.0102,  0.0200, -0.0111,  ...,  0.0001, -0.0231,  0.0069],
        [ 0.0048,  0.0151,  0.0099,  ...,  0.0049,  0.0131,  0.0074]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4741,  0.2559, -1.4990,  ..., -3.8750, -1.0312, -2.4453]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 15:00:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a gramm is a milligram
A part of a door is a hinge
A part of a academia is a college
A part of a day is a hour
A part of a jail is a cell
A part of a tripod is a leg
A part of a brush is a bristle
A part of a piano is a
2024-07-16 15:00:57 root INFO     [order_1_approx] starting weight calculation for A part of a piano is a keyboard
A part of a tripod is a leg
A part of a brush is a bristle
A part of a door is a hinge
A part of a day is a hour
A part of a gramm is a milligram
A part of a academia is a college
A part of a jail is a
2024-07-16 15:00:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 15:04:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3269, -0.8359, -0.8560,  ...,  0.3479, -0.1111, -0.8433],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5234, -1.0391, -1.1768,  ..., -2.3047, -1.4307,  1.4385],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0044, -0.0181, -0.0009,  ..., -0.0034, -0.0100,  0.0027],
        [ 0.0158, -0.0066,  0.0072,  ...,  0.0043, -0.0033, -0.0219],
        [-0.0163,  0.0111,  0.0013,  ..., -0.0016,  0.0070, -0.0089],
        ...,
        [-0.0052, -0.0102, -0.0003,  ..., -0.0006, -0.0063,  0.0144],
        [ 0.0008,  0.0041,  0.0024,  ...,  0.0074, -0.0071, -0.0012],
        [-0.0027, -0.0092, -0.0013,  ..., -0.0081,  0.0014, -0.0004]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3438, -0.1343, -0.9629,  ..., -2.2832, -1.7949,  2.8164]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 15:04:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a piano is a keyboard
A part of a tripod is a leg
A part of a brush is a bristle
A part of a door is a hinge
A part of a day is a hour
A part of a gramm is a milligram
A part of a academia is a college
A part of a jail is a
2024-07-16 15:04:43 root INFO     [order_1_approx] starting weight calculation for A part of a jail is a cell
A part of a brush is a bristle
A part of a tripod is a leg
A part of a gramm is a milligram
A part of a day is a hour
A part of a academia is a college
A part of a piano is a keyboard
A part of a door is a
2024-07-16 15:04:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 15:08:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3096, -0.4353, -0.2175,  ...,  0.6699, -0.8262,  0.9751],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.7578,  1.5918, -1.8223,  ..., -4.3203, -1.6406,  3.0703],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.2192e-02, -4.3182e-03, -3.1509e-03,  ..., -6.5918e-03,
         -1.8768e-03,  5.7831e-03],
        [ 6.9656e-03,  1.6907e-02, -5.3406e-05,  ..., -7.5836e-03,
          4.6692e-03, -5.4550e-03],
        [-1.2306e-02,  1.3672e-02,  5.7449e-03,  ...,  6.3095e-03,
         -1.9989e-02, -3.0251e-03],
        ...,
        [-8.4534e-03, -8.7891e-03,  3.1528e-03,  ...,  2.7130e-02,
         -5.6000e-03, -1.5221e-02],
        [-3.8605e-03,  1.6678e-02,  1.7776e-02,  ...,  2.0615e-02,
          7.4482e-04,  5.9052e-03],
        [ 6.6948e-03,  1.0178e-02, -2.1072e-02,  ..., -9.0561e-03,
          1.2924e-02,  1.3428e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.1641,  1.6016, -1.0723,  ..., -4.5195, -1.4209,  3.3535]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 15:08:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a jail is a cell
A part of a brush is a bristle
A part of a tripod is a leg
A part of a gramm is a milligram
A part of a day is a hour
A part of a academia is a college
A part of a piano is a keyboard
A part of a door is a
2024-07-16 15:08:30 root INFO     [order_1_approx] starting weight calculation for A part of a day is a hour
A part of a jail is a cell
A part of a piano is a keyboard
A part of a gramm is a milligram
A part of a tripod is a leg
A part of a brush is a bristle
A part of a door is a hinge
A part of a academia is a
2024-07-16 15:08:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 15:12:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9624,  1.5566,  0.7271,  ..., -0.6450, -0.0090,  0.9043],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9487,  3.6387, -0.0879,  ..., -1.0586, -0.4546,  1.6475],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0056, -0.0331, -0.0103,  ..., -0.0076, -0.0019, -0.0037],
        [ 0.0103,  0.0046, -0.0031,  ..., -0.0016, -0.0253, -0.0163],
        [ 0.0175, -0.0121, -0.0044,  ..., -0.0093, -0.0231,  0.0047],
        ...,
        [-0.0053, -0.0082, -0.0134,  ..., -0.0132,  0.0063,  0.0149],
        [ 0.0154,  0.0083,  0.0105,  ..., -0.0023, -0.0109,  0.0098],
        [-0.0129, -0.0059, -0.0061,  ..., -0.0043,  0.0158,  0.0166]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5156,  4.9609, -0.5229,  ..., -2.2090, -1.1855,  2.1641]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 15:12:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a day is a hour
A part of a jail is a cell
A part of a piano is a keyboard
A part of a gramm is a milligram
A part of a tripod is a leg
A part of a brush is a bristle
A part of a door is a hinge
A part of a academia is a
2024-07-16 15:12:18 root INFO     total operator prediction time: 1812.821659564972 seconds
2024-07-16 15:12:18 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on synonyms - exact
2024-07-16 15:12:18 root INFO     building operator synonyms - exact
2024-07-16 15:12:19 root INFO     [order_1_approx] starting weight calculation for Another word for clothes is clothing
Another word for style is manner
Another word for snake is serpent
Another word for baby is infant
Another word for murder is slaying
Another word for help is aid
Another word for mend is repair
Another word for auto is
2024-07-16 15:12:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 15:16:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5117,  0.3953, -0.5010,  ...,  0.3721, -0.6396,  1.1035],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.0000, -0.7485, -2.3750,  ..., -1.8926,  0.0205, -1.3789],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0095, -0.0179,  0.0088,  ...,  0.0049,  0.0015,  0.0003],
        [-0.0025,  0.0079, -0.0013,  ...,  0.0118,  0.0039,  0.0176],
        [ 0.0003,  0.0053, -0.0169,  ..., -0.0251,  0.0264,  0.0344],
        ...,
        [-0.0103,  0.0194,  0.0070,  ..., -0.0061,  0.0035,  0.0229],
        [ 0.0170,  0.0053, -0.0179,  ...,  0.0004,  0.0049,  0.0050],
        [ 0.0009, -0.0027,  0.0004,  ..., -0.0124,  0.0068, -0.0182]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.3164, -0.0205, -2.4121,  ..., -2.1602,  1.0781, -0.5522]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 15:16:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for clothes is clothing
Another word for style is manner
Another word for snake is serpent
Another word for baby is infant
Another word for murder is slaying
Another word for help is aid
Another word for mend is repair
Another word for auto is
2024-07-16 15:16:06 root INFO     [order_1_approx] starting weight calculation for Another word for mend is repair
Another word for style is manner
Another word for snake is serpent
Another word for auto is car
Another word for help is aid
Another word for clothes is clothing
Another word for baby is infant
Another word for murder is
2024-07-16 15:16:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 15:19:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9531, -0.6133,  0.4067,  ...,  0.5977, -0.9082,  0.4692],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.2070, -0.7271, -2.2988,  ..., -5.1016,  4.9531, -0.8940],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0005, -0.0074, -0.0061,  ...,  0.0072, -0.0129, -0.0083],
        [ 0.0195, -0.0084, -0.0030,  ...,  0.0155, -0.0085, -0.0046],
        [-0.0014,  0.0229, -0.0086,  ..., -0.0105, -0.0008,  0.0195],
        ...,
        [-0.0254,  0.0189,  0.0114,  ..., -0.0088, -0.0109, -0.0053],
        [ 0.0091, -0.0041, -0.0018,  ...,  0.0082,  0.0046,  0.0071],
        [ 0.0142, -0.0014,  0.0039,  ...,  0.0040,  0.0048,  0.0163]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.8818, -0.8657, -3.3047,  ..., -4.7109,  5.0312, -0.3491]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 15:19:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for mend is repair
Another word for style is manner
Another word for snake is serpent
Another word for auto is car
Another word for help is aid
Another word for clothes is clothing
Another word for baby is infant
Another word for murder is
2024-07-16 15:19:53 root INFO     [order_1_approx] starting weight calculation for Another word for murder is slaying
Another word for help is aid
Another word for style is manner
Another word for snake is serpent
Another word for mend is repair
Another word for clothes is clothing
Another word for auto is car
Another word for baby is
2024-07-16 15:19:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 15:23:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3643,  0.2949, -0.6440,  ..., -0.0150, -0.8169, -0.1400],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2012,  0.5586, -4.7148,  ...,  0.2169,  2.0859,  0.0752],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0200, -0.0086, -0.0019,  ...,  0.0043, -0.0039,  0.0068],
        [ 0.0054, -0.0153, -0.0055,  ..., -0.0018, -0.0041, -0.0048],
        [ 0.0089,  0.0106, -0.0131,  ..., -0.0037,  0.0062,  0.0023],
        ...,
        [-0.0158,  0.0012, -0.0010,  ..., -0.0096, -0.0062, -0.0170],
        [ 0.0069,  0.0124, -0.0248,  ..., -0.0025, -0.0134, -0.0009],
        [-0.0119, -0.0096,  0.0038,  ..., -0.0036,  0.0020,  0.0102]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7422,  1.3428, -4.3828,  ...,  1.0469,  2.6758,  0.4329]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 15:23:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for murder is slaying
Another word for help is aid
Another word for style is manner
Another word for snake is serpent
Another word for mend is repair
Another word for clothes is clothing
Another word for auto is car
Another word for baby is
2024-07-16 15:23:38 root INFO     [order_1_approx] starting weight calculation for Another word for baby is infant
Another word for style is manner
Another word for snake is serpent
Another word for murder is slaying
Another word for help is aid
Another word for mend is repair
Another word for auto is car
Another word for clothes is
2024-07-16 15:23:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 15:27:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2812,  0.3684,  0.1337,  ...,  0.9321, -0.2827,  0.1187],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.2969,  1.2979, -3.4512,  ..., -5.3984,  1.6543, -0.2075],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0087, -0.0235,  0.0066,  ..., -0.0007, -0.0124, -0.0084],
        [ 0.0164, -0.0051,  0.0003,  ...,  0.0054,  0.0072, -0.0101],
        [ 0.0095,  0.0157, -0.0117,  ..., -0.0011, -0.0070, -0.0003],
        ...,
        [-0.0147,  0.0110,  0.0017,  ...,  0.0083, -0.0081,  0.0035],
        [-0.0024, -0.0083, -0.0193,  ...,  0.0108, -0.0053,  0.0102],
        [-0.0100, -0.0050,  0.0109,  ..., -0.0095,  0.0010, -0.0006]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.3828,  1.4814, -3.6211,  ..., -4.8516,  1.4844,  0.7041]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 15:27:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for baby is infant
Another word for style is manner
Another word for snake is serpent
Another word for murder is slaying
Another word for help is aid
Another word for mend is repair
Another word for auto is car
Another word for clothes is
2024-07-16 15:27:25 root INFO     [order_1_approx] starting weight calculation for Another word for help is aid
Another word for auto is car
Another word for baby is infant
Another word for murder is slaying
Another word for style is manner
Another word for snake is serpent
Another word for clothes is clothing
Another word for mend is
2024-07-16 15:27:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 15:31:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8491, -1.3604, -1.5879,  ...,  1.2490,  0.8584, -0.4180],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1514,  0.0635, -5.6211,  ..., -2.3867,  2.1543,  6.7461],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0042, -0.0073, -0.0115,  ...,  0.0061, -0.0046,  0.0070],
        [-0.0081,  0.0027,  0.0022,  ...,  0.0040, -0.0079,  0.0031],
        [-0.0103,  0.0077, -0.0262,  ..., -0.0054,  0.0034, -0.0068],
        ...,
        [ 0.0126, -0.0053,  0.0011,  ..., -0.0238,  0.0089, -0.0086],
        [-0.0165, -0.0291, -0.0062,  ..., -0.0031, -0.0253, -0.0053],
        [-0.0156, -0.0152,  0.0170,  ..., -0.0094, -0.0039,  0.0200]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5254,  0.7461, -5.4336,  ..., -3.3828,  2.3691,  7.8086]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 15:31:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for help is aid
Another word for auto is car
Another word for baby is infant
Another word for murder is slaying
Another word for style is manner
Another word for snake is serpent
Another word for clothes is clothing
Another word for mend is
2024-07-16 15:31:13 root INFO     [order_1_approx] starting weight calculation for Another word for clothes is clothing
Another word for snake is serpent
Another word for help is aid
Another word for mend is repair
Another word for murder is slaying
Another word for baby is infant
Another word for auto is car
Another word for style is
2024-07-16 15:31:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 15:34:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.8682, -0.6357, -0.0601,  ...,  1.0020, -0.1989, -0.0597],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.5859, -2.3809, -3.3301,  ..., -1.8135, -1.3135,  2.2656],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0101, -0.0040,  0.0173,  ...,  0.0004, -0.0075, -0.0162],
        [-0.0004, -0.0081, -0.0085,  ..., -0.0031, -0.0236, -0.0154],
        [-0.0139, -0.0062, -0.0233,  ..., -0.0086,  0.0008, -0.0162],
        ...,
        [-0.0077, -0.0041,  0.0024,  ...,  0.0033, -0.0007, -0.0034],
        [-0.0077,  0.0146,  0.0019,  ..., -0.0140,  0.0049, -0.0052],
        [ 0.0062, -0.0136,  0.0045,  ...,  0.0042,  0.0001,  0.0149]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.8516, -1.0010, -3.2852,  ..., -1.9668, -0.5122,  1.7803]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 15:34:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for clothes is clothing
Another word for snake is serpent
Another word for help is aid
Another word for mend is repair
Another word for murder is slaying
Another word for baby is infant
Another word for auto is car
Another word for style is
2024-07-16 15:35:00 root INFO     [order_1_approx] starting weight calculation for Another word for auto is car
Another word for baby is infant
Another word for clothes is clothing
Another word for mend is repair
Another word for style is manner
Another word for help is aid
Another word for murder is slaying
Another word for snake is
2024-07-16 15:35:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 15:38:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5161, -0.4790, -1.6338,  ...,  0.7124,  0.2817,  0.9463],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.8848, -1.6504, -0.3076,  ...,  1.0947,  0.6680, -3.1367],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0074, -0.0030,  0.0119,  ...,  0.0161, -0.0026, -0.0229],
        [-0.0071, -0.0045, -0.0078,  ..., -0.0019, -0.0204,  0.0003],
        [-0.0253,  0.0057, -0.0059,  ..., -0.0006,  0.0064,  0.0125],
        ...,
        [ 0.0004,  0.0119,  0.0065,  ...,  0.0050, -0.0058,  0.0096],
        [ 0.0033, -0.0389,  0.0085,  ...,  0.0032,  0.0106,  0.0155],
        [-0.0124, -0.0135, -0.0029,  ...,  0.0021,  0.0002,  0.0100]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.9805, -1.3672, -0.2791,  ...,  1.4316,  1.2324, -3.1934]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 15:38:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for auto is car
Another word for baby is infant
Another word for clothes is clothing
Another word for mend is repair
Another word for style is manner
Another word for help is aid
Another word for murder is slaying
Another word for snake is
2024-07-16 15:38:47 root INFO     [order_1_approx] starting weight calculation for Another word for mend is repair
Another word for clothes is clothing
Another word for baby is infant
Another word for auto is car
Another word for murder is slaying
Another word for style is manner
Another word for snake is serpent
Another word for help is
2024-07-16 15:38:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 15:42:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2595,  0.8447, -1.0723,  ...,  0.8066,  0.9360,  0.8872],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5703,  0.5161, -2.9844,  ..., -6.2109,  0.6172,  3.4727],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0100,  0.0023, -0.0087,  ..., -0.0034, -0.0041, -0.0049],
        [ 0.0043, -0.0199,  0.0083,  ..., -0.0173,  0.0049, -0.0037],
        [-0.0002,  0.0314, -0.0016,  ...,  0.0079,  0.0158, -0.0009],
        ...,
        [-0.0236,  0.0108,  0.0018,  ..., -0.0041,  0.0008,  0.0005],
        [ 0.0064, -0.0072, -0.0082,  ..., -0.0174,  0.0135, -0.0215],
        [-0.0019, -0.0099, -0.0048,  ...,  0.0023,  0.0124,  0.0264]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0464, -0.3301, -1.2080,  ..., -4.9219,  0.0952,  2.6543]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 15:42:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for mend is repair
Another word for clothes is clothing
Another word for baby is infant
Another word for auto is car
Another word for murder is slaying
Another word for style is manner
Another word for snake is serpent
Another word for help is
2024-07-16 15:42:35 root INFO     total operator prediction time: 1816.507375717163 seconds
2024-07-16 15:42:35 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on hypernyms - misc
2024-07-16 15:42:35 root INFO     building operator hypernyms - misc
2024-07-16 15:42:35 root INFO     [order_1_approx] starting weight calculation for The perfume falls into the category of toiletry
The hamburger falls into the category of sandwich
The tub falls into the category of container
The tv falls into the category of device
The postcard falls into the category of card
The peach falls into the category of fruit
The cake falls into the category of dessert
The deodorant falls into the category of
2024-07-16 15:42:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 15:46:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0654, -0.4565, -0.0056,  ..., -1.4004,  1.2500,  0.4385],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.7266, -2.8750,  0.7148,  ..., -1.9209, -1.0664,  1.2285],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.5976e-02,  6.6490e-03, -6.6795e-03,  ..., -7.5989e-03,
         -9.1324e-03,  1.0719e-03],
        [-5.8098e-03,  1.0262e-02, -6.7711e-05,  ...,  7.0953e-03,
          5.6763e-03,  7.5798e-03],
        [-4.8752e-03, -7.0381e-03,  1.0658e-02,  ...,  1.5545e-04,
          2.1629e-03, -7.6904e-03],
        ...,
        [-1.3123e-02, -4.4327e-03, -5.5695e-03,  ...,  9.6741e-03,
         -4.5853e-03,  1.2146e-02],
        [ 9.5062e-03, -1.7338e-03, -6.3515e-03,  ..., -4.8141e-03,
          5.8327e-03,  2.2545e-03],
        [ 4.9133e-03, -6.7406e-03, -6.5804e-04,  ..., -4.3488e-03,
          1.8372e-02,  2.0493e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.8242, -2.8125,  0.3027,  ..., -2.0488, -1.4795,  1.0742]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 15:46:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The perfume falls into the category of toiletry
The hamburger falls into the category of sandwich
The tub falls into the category of container
The tv falls into the category of device
The postcard falls into the category of card
The peach falls into the category of fruit
The cake falls into the category of dessert
The deodorant falls into the category of
2024-07-16 15:46:20 root INFO     [order_1_approx] starting weight calculation for The perfume falls into the category of toiletry
The tv falls into the category of device
The peach falls into the category of fruit
The postcard falls into the category of card
The hamburger falls into the category of sandwich
The deodorant falls into the category of toiletry
The tub falls into the category of container
The cake falls into the category of
2024-07-16 15:46:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 15:50:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9302,  0.9482, -0.8652,  ..., -0.5879, -0.5283,  1.0723],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.3359,  0.6729, -1.0596,  ..., -1.4336,  1.6553, -0.0986],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0100,  0.0012, -0.0007,  ..., -0.0057,  0.0046,  0.0064],
        [-0.0096,  0.0069, -0.0025,  ...,  0.0085, -0.0029,  0.0034],
        [-0.0017,  0.0002, -0.0235,  ...,  0.0097,  0.0017,  0.0081],
        ...,
        [-0.0039, -0.0033,  0.0087,  ...,  0.0094, -0.0092,  0.0006],
        [-0.0012, -0.0104,  0.0041,  ...,  0.0015,  0.0034,  0.0067],
        [-0.0125, -0.0089,  0.0088,  ...,  0.0023,  0.0038,  0.0133]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.5820,  1.8916, -1.0830,  ..., -1.3906,  0.5342,  0.2324]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 15:50:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The perfume falls into the category of toiletry
The tv falls into the category of device
The peach falls into the category of fruit
The postcard falls into the category of card
The hamburger falls into the category of sandwich
The deodorant falls into the category of toiletry
The tub falls into the category of container
The cake falls into the category of
2024-07-16 15:50:08 root INFO     [order_1_approx] starting weight calculation for The tv falls into the category of device
The peach falls into the category of fruit
The cake falls into the category of dessert
The postcard falls into the category of card
The deodorant falls into the category of toiletry
The tub falls into the category of container
The perfume falls into the category of toiletry
The hamburger falls into the category of
2024-07-16 15:50:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 15:53:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2086,  1.1211, -0.4897,  ..., -0.2126,  0.3157,  0.5122],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2910,  1.9629,  1.0703,  ..., -2.0977, -1.3037, -0.5474],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0134, -0.0116, -0.0046,  ...,  0.0036, -0.0085,  0.0053],
        [-0.0089, -0.0031,  0.0050,  ...,  0.0094,  0.0039, -0.0028],
        [-0.0025, -0.0015, -0.0110,  ...,  0.0055,  0.0029,  0.0027],
        ...,
        [-0.0108, -0.0037, -0.0008,  ...,  0.0031, -0.0077,  0.0026],
        [ 0.0084,  0.0077,  0.0010,  ...,  0.0009,  0.0018,  0.0041],
        [-0.0166,  0.0004,  0.0005,  ..., -0.0019,  0.0045,  0.0030]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7734,  2.3125,  0.5498,  ..., -2.3438, -1.7881,  0.0527]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 15:53:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The tv falls into the category of device
The peach falls into the category of fruit
The cake falls into the category of dessert
The postcard falls into the category of card
The deodorant falls into the category of toiletry
The tub falls into the category of container
The perfume falls into the category of toiletry
The hamburger falls into the category of
2024-07-16 15:53:56 root INFO     [order_1_approx] starting weight calculation for The deodorant falls into the category of toiletry
The cake falls into the category of dessert
The hamburger falls into the category of sandwich
The perfume falls into the category of toiletry
The postcard falls into the category of card
The tub falls into the category of container
The tv falls into the category of device
The peach falls into the category of
2024-07-16 15:53:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 15:57:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1040, -0.0195, -0.0096,  ..., -1.3389, -0.4346,  0.1299],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.9629, -0.6338, -4.3008,  ..., -3.2520, -0.6504,  0.3579],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0220,  0.0081, -0.0069,  ..., -0.0108,  0.0008,  0.0086],
        [-0.0019,  0.0071,  0.0044,  ...,  0.0084, -0.0014,  0.0019],
        [-0.0040,  0.0139, -0.0090,  ...,  0.0085,  0.0005,  0.0091],
        ...,
        [-0.0047, -0.0137,  0.0041,  ...,  0.0073, -0.0041,  0.0079],
        [ 0.0208,  0.0050, -0.0103,  ..., -0.0010,  0.0047,  0.0048],
        [-0.0112, -0.0162,  0.0120,  ..., -0.0055,  0.0040,  0.0027]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.6035, -0.8247, -4.2891,  ..., -3.3223, -1.2246,  0.0352]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 15:57:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The deodorant falls into the category of toiletry
The cake falls into the category of dessert
The hamburger falls into the category of sandwich
The perfume falls into the category of toiletry
The postcard falls into the category of card
The tub falls into the category of container
The tv falls into the category of device
The peach falls into the category of
2024-07-16 15:57:42 root INFO     [order_1_approx] starting weight calculation for The peach falls into the category of fruit
The postcard falls into the category of card
The deodorant falls into the category of toiletry
The hamburger falls into the category of sandwich
The perfume falls into the category of toiletry
The tub falls into the category of container
The cake falls into the category of dessert
The tv falls into the category of
2024-07-16 15:57:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 16:01:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3716,  0.7085, -0.6821,  ...,  1.1924,  0.2100,  0.0981],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.2207,  0.1699, -1.0566,  ..., -2.7812, -0.5869,  0.5781],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.8336e-02,  4.6463e-03, -1.7319e-02,  ...,  1.9417e-03,
          3.5439e-03,  3.8528e-04],
        [-1.5808e-02,  3.5686e-03,  1.7365e-02,  ..., -5.6000e-03,
          1.8864e-03, -3.0689e-03],
        [ 5.7411e-03,  8.2016e-03, -3.5381e-03,  ...,  1.4633e-02,
          1.3561e-03,  8.9111e-03],
        ...,
        [-6.7024e-03, -4.7951e-03,  1.5564e-03,  ...,  8.5602e-03,
         -1.1101e-02, -7.0667e-04],
        [ 3.7537e-03, -2.1400e-03,  7.9727e-03,  ...,  4.2877e-03,
         -1.3885e-02,  8.1062e-05],
        [ 4.8218e-03, -6.4888e-03,  4.7455e-03,  ...,  4.4937e-03,
          7.7171e-03,  1.7166e-04]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.0547,  1.0078, -1.1982,  ..., -2.9531, -0.6235,  0.8799]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 16:01:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The peach falls into the category of fruit
The postcard falls into the category of card
The deodorant falls into the category of toiletry
The hamburger falls into the category of sandwich
The perfume falls into the category of toiletry
The tub falls into the category of container
The cake falls into the category of dessert
The tv falls into the category of
2024-07-16 16:01:30 root INFO     [order_1_approx] starting weight calculation for The hamburger falls into the category of sandwich
The postcard falls into the category of card
The deodorant falls into the category of toiletry
The perfume falls into the category of toiletry
The cake falls into the category of dessert
The tv falls into the category of device
The peach falls into the category of fruit
The tub falls into the category of
2024-07-16 16:01:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 16:05:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5435,  0.7759, -0.0765,  ..., -0.2900, -0.7852,  0.0399],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.4316,  1.8418, -1.1904,  ..., -0.0269, -0.5146,  1.6826],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0234,  0.0042, -0.0087,  ...,  0.0042,  0.0176, -0.0048],
        [ 0.0003,  0.0166, -0.0039,  ...,  0.0026,  0.0018,  0.0005],
        [-0.0009,  0.0115,  0.0064,  ..., -0.0025,  0.0093,  0.0072],
        ...,
        [-0.0094, -0.0097,  0.0083,  ...,  0.0181, -0.0144, -0.0013],
        [-0.0032,  0.0026,  0.0045,  ..., -0.0015,  0.0155, -0.0005],
        [ 0.0047, -0.0168,  0.0051,  ...,  0.0194,  0.0076,  0.0156]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.2344,  1.8428, -1.7568,  ..., -0.2234, -1.2900,  1.5537]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 16:05:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The hamburger falls into the category of sandwich
The postcard falls into the category of card
The deodorant falls into the category of toiletry
The perfume falls into the category of toiletry
The cake falls into the category of dessert
The tv falls into the category of device
The peach falls into the category of fruit
The tub falls into the category of
2024-07-16 16:05:17 root INFO     [order_1_approx] starting weight calculation for The hamburger falls into the category of sandwich
The tub falls into the category of container
The cake falls into the category of dessert
The peach falls into the category of fruit
The postcard falls into the category of card
The tv falls into the category of device
The deodorant falls into the category of toiletry
The perfume falls into the category of
2024-07-16 16:05:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 16:09:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4355,  0.3408, -0.5923,  ..., -2.0137,  0.2415, -1.0781],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.7812, -2.2246, -1.5010,  ..., -4.1211, -2.1602, -3.6680],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0089, -0.0001, -0.0176,  ..., -0.0045,  0.0072, -0.0053],
        [ 0.0048, -0.0050, -0.0005,  ...,  0.0085, -0.0032, -0.0019],
        [ 0.0112,  0.0088,  0.0007,  ...,  0.0024,  0.0020,  0.0107],
        ...,
        [-0.0052,  0.0043, -0.0003,  ...,  0.0071, -0.0082,  0.0110],
        [-0.0081, -0.0020, -0.0054,  ...,  0.0073, -0.0142,  0.0017],
        [-0.0001,  0.0102,  0.0010,  ..., -0.0068,  0.0104,  0.0190]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.4453, -2.1953, -1.9609,  ..., -4.1836, -2.3535, -3.5586]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 16:09:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The hamburger falls into the category of sandwich
The tub falls into the category of container
The cake falls into the category of dessert
The peach falls into the category of fruit
The postcard falls into the category of card
The tv falls into the category of device
The deodorant falls into the category of toiletry
The perfume falls into the category of
2024-07-16 16:09:04 root INFO     [order_1_approx] starting weight calculation for The tub falls into the category of container
The peach falls into the category of fruit
The cake falls into the category of dessert
The perfume falls into the category of toiletry
The hamburger falls into the category of sandwich
The deodorant falls into the category of toiletry
The tv falls into the category of device
The postcard falls into the category of
2024-07-16 16:09:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 16:12:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2314,  0.5142, -1.0264,  ..., -0.5244,  0.9023,  1.3760],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.2500, -0.7778, -5.6484,  ..., -2.2695,  1.3828,  2.1777],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0172,  0.0092,  0.0003,  ..., -0.0114, -0.0004, -0.0046],
        [ 0.0017,  0.0006,  0.0047,  ...,  0.0077, -0.0030,  0.0010],
        [ 0.0062,  0.0034, -0.0008,  ..., -0.0037,  0.0005,  0.0031],
        ...,
        [-0.0037, -0.0067, -0.0037,  ...,  0.0016, -0.0124, -0.0011],
        [-0.0019, -0.0005,  0.0001,  ..., -0.0050, -0.0068,  0.0018],
        [-0.0043, -0.0142,  0.0010,  ...,  0.0006,  0.0114,  0.0068]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.2500, -0.5117, -5.6484,  ..., -2.5566,  1.4043,  2.0098]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 16:12:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The tub falls into the category of container
The peach falls into the category of fruit
The cake falls into the category of dessert
The perfume falls into the category of toiletry
The hamburger falls into the category of sandwich
The deodorant falls into the category of toiletry
The tv falls into the category of device
The postcard falls into the category of
2024-07-16 16:12:51 root INFO     total operator prediction time: 1816.4908123016357 seconds
2024-07-16 16:12:51 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - substance
2024-07-16 16:12:51 root INFO     building operator meronyms - substance
2024-07-16 16:12:51 root INFO     [order_1_approx] starting weight calculation for A cocktail is made up of alcohol
A bag is made up of leather
A chocolate is made up of cocoa
A bowl is made up of glass
A wig is made up of hair
A mirror is made up of glass
A candy is made up of sugar
A sea is made up of
2024-07-16 16:12:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 16:16:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4167,  0.2244,  0.8335,  ..., -0.5811, -0.7510, -0.1616],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5889,  3.0078,  0.6699,  ...,  1.5791, -1.2822, -0.7104],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0055, -0.0132, -0.0065,  ...,  0.0026,  0.0071, -0.0052],
        [-0.0008, -0.0036,  0.0134,  ...,  0.0126,  0.0003, -0.0060],
        [ 0.0012,  0.0010, -0.0084,  ..., -0.0029,  0.0097, -0.0041],
        ...,
        [ 0.0098,  0.0005,  0.0058,  ...,  0.0006, -0.0009,  0.0098],
        [ 0.0104,  0.0066,  0.0095,  ..., -0.0020,  0.0091,  0.0039],
        [-0.0040, -0.0049,  0.0035,  ..., -0.0113,  0.0105,  0.0117]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4116,  2.8477,  0.9580,  ...,  1.8418, -1.5605, -0.7251]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 16:16:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A cocktail is made up of alcohol
A bag is made up of leather
A chocolate is made up of cocoa
A bowl is made up of glass
A wig is made up of hair
A mirror is made up of glass
A candy is made up of sugar
A sea is made up of
2024-07-16 16:16:37 root INFO     [order_1_approx] starting weight calculation for A wig is made up of hair
A sea is made up of water
A bowl is made up of glass
A cocktail is made up of alcohol
A mirror is made up of glass
A bag is made up of leather
A candy is made up of sugar
A chocolate is made up of
2024-07-16 16:16:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 16:20:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9849,  0.7900,  1.6895,  ...,  0.6724,  0.1738,  0.7173],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0303,  2.2617,  3.0352,  ...,  2.7930, -0.0898,  1.3428],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0011, -0.0188, -0.0075,  ...,  0.0101, -0.0046, -0.0005],
        [-0.0056, -0.0066, -0.0073,  ..., -0.0002,  0.0043, -0.0102],
        [ 0.0001, -0.0026, -0.0186,  ...,  0.0032, -0.0022, -0.0014],
        ...,
        [-0.0035, -0.0123, -0.0086,  ..., -0.0053, -0.0038,  0.0094],
        [ 0.0100, -0.0092,  0.0040,  ..., -0.0004, -0.0131,  0.0020],
        [ 0.0030, -0.0047, -0.0038,  ..., -0.0125,  0.0157, -0.0010]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0693,  2.2246,  2.8262,  ...,  2.8438, -0.8042,  1.6602]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 16:20:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A wig is made up of hair
A sea is made up of water
A bowl is made up of glass
A cocktail is made up of alcohol
A mirror is made up of glass
A bag is made up of leather
A candy is made up of sugar
A chocolate is made up of
2024-07-16 16:20:21 root INFO     [order_1_approx] starting weight calculation for A candy is made up of sugar
A mirror is made up of glass
A wig is made up of hair
A chocolate is made up of cocoa
A bag is made up of leather
A sea is made up of water
A cocktail is made up of alcohol
A bowl is made up of
2024-07-16 16:20:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 16:24:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2778, -0.0105,  0.4497,  ...,  0.7852,  0.7539, -0.1818],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.4492,  3.5273, -0.0391,  ...,  4.7500,  0.6357, -0.5273],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0208,  0.0054, -0.0082,  ..., -0.0181, -0.0001, -0.0060],
        [-0.0109,  0.0130,  0.0170,  ...,  0.0150, -0.0023, -0.0032],
        [ 0.0078, -0.0032,  0.0064,  ...,  0.0003,  0.0049,  0.0026],
        ...,
        [ 0.0086, -0.0167, -0.0067,  ...,  0.0052, -0.0113,  0.0120],
        [ 0.0033, -0.0180,  0.0122,  ...,  0.0114,  0.0093,  0.0035],
        [-0.0085, -0.0066,  0.0003,  ..., -0.0092,  0.0162,  0.0224]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.7480,  3.3125, -0.3145,  ...,  5.3281,  0.0806, -0.7046]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 16:24:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A candy is made up of sugar
A mirror is made up of glass
A wig is made up of hair
A chocolate is made up of cocoa
A bag is made up of leather
A sea is made up of water
A cocktail is made up of alcohol
A bowl is made up of
2024-07-16 16:24:07 root INFO     [order_1_approx] starting weight calculation for A mirror is made up of glass
A wig is made up of hair
A bag is made up of leather
A chocolate is made up of cocoa
A cocktail is made up of alcohol
A bowl is made up of glass
A sea is made up of water
A candy is made up of
2024-07-16 16:24:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 16:27:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.7979,  1.3789,  0.0981,  ...,  0.3271,  0.6587, -0.6113],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.8145, -0.5215,  1.6475,  ..., -1.2207,  0.2886,  1.6982],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0087, -0.0142, -0.0091,  ..., -0.0073,  0.0026, -0.0034],
        [-0.0029, -0.0029, -0.0064,  ...,  0.0069,  0.0018,  0.0066],
        [ 0.0025,  0.0031, -0.0072,  ...,  0.0017,  0.0133,  0.0048],
        ...,
        [ 0.0082,  0.0058, -0.0028,  ..., -0.0112, -0.0103, -0.0066],
        [ 0.0037, -0.0007,  0.0073,  ...,  0.0141,  0.0030,  0.0028],
        [-0.0059, -0.0020, -0.0031,  ..., -0.0044,  0.0057, -0.0021]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.4062, -1.0898,  1.1230,  ..., -0.9326, -0.3188,  1.7451]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 16:27:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A mirror is made up of glass
A wig is made up of hair
A bag is made up of leather
A chocolate is made up of cocoa
A cocktail is made up of alcohol
A bowl is made up of glass
A sea is made up of water
A candy is made up of
2024-07-16 16:27:51 root INFO     [order_1_approx] starting weight calculation for A candy is made up of sugar
A chocolate is made up of cocoa
A bowl is made up of glass
A bag is made up of leather
A wig is made up of hair
A mirror is made up of glass
A sea is made up of water
A cocktail is made up of
2024-07-16 16:27:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 16:31:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6616,  0.4492, -0.7334,  ..., -0.6035, -0.5068,  0.0880],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6621,  0.2310,  1.1426,  ...,  4.5117, -2.4473, -0.1338],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0041, -0.0090, -0.0030,  ..., -0.0120,  0.0029, -0.0010],
        [-0.0025, -0.0042, -0.0026,  ...,  0.0096, -0.0042, -0.0009],
        [-0.0037, -0.0016, -0.0044,  ..., -0.0008,  0.0084, -0.0038],
        ...,
        [-0.0032, -0.0032, -0.0079,  ...,  0.0105, -0.0133, -0.0020],
        [-0.0014,  0.0065, -0.0058,  ...,  0.0031, -0.0049,  0.0046],
        [ 0.0003,  0.0060, -0.0011,  ..., -0.0140,  0.0116,  0.0035]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.9404,  0.1754,  0.7471,  ...,  4.2266, -2.6172, -0.0641]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 16:31:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A candy is made up of sugar
A chocolate is made up of cocoa
A bowl is made up of glass
A bag is made up of leather
A wig is made up of hair
A mirror is made up of glass
A sea is made up of water
A cocktail is made up of
2024-07-16 16:31:38 root INFO     [order_1_approx] starting weight calculation for A bowl is made up of glass
A sea is made up of water
A mirror is made up of glass
A candy is made up of sugar
A bag is made up of leather
A cocktail is made up of alcohol
A chocolate is made up of cocoa
A wig is made up of
2024-07-16 16:31:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 16:35:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1533,  0.2666, -1.5186,  ...,  0.8672, -1.3135,  1.1602],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.5469, -1.3291, -0.8770,  ...,  3.2461, -0.0430, -0.0786],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 9.0790e-03, -3.3798e-03, -9.3765e-03,  ..., -8.2493e-04,
          2.2888e-04, -5.3749e-03],
        [-5.3101e-03,  6.6452e-03,  6.3972e-03,  ...,  5.5656e-03,
          4.1809e-03, -7.5340e-04],
        [ 9.2163e-03,  1.7929e-04, -2.9278e-03,  ...,  4.1008e-03,
          5.9586e-03,  8.0795e-03],
        ...,
        [ 5.3825e-03, -2.2545e-03, -1.2939e-02,  ..., -5.8365e-04,
         -1.1551e-02, -5.3406e-05],
        [-3.1815e-03, -1.1192e-02, -1.2398e-03,  ...,  1.0223e-02,
         -3.9177e-03,  8.9722e-03],
        [-4.7646e-03, -2.8858e-03, -2.0256e-03,  ..., -7.4577e-04,
          1.4755e-02, -9.9564e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.8066, -1.3701, -1.4102,  ...,  3.4492, -0.6528, -0.1035]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 16:35:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A bowl is made up of glass
A sea is made up of water
A mirror is made up of glass
A candy is made up of sugar
A bag is made up of leather
A cocktail is made up of alcohol
A chocolate is made up of cocoa
A wig is made up of
2024-07-16 16:35:24 root INFO     [order_1_approx] starting weight calculation for A sea is made up of water
A chocolate is made up of cocoa
A bowl is made up of glass
A cocktail is made up of alcohol
A mirror is made up of glass
A wig is made up of hair
A candy is made up of sugar
A bag is made up of
2024-07-16 16:35:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 16:39:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1594, -0.4868,  0.2512,  ...,  0.3823, -0.9536,  0.1526],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6133,  1.7637,  0.6465,  ...,  2.4121, -1.2881, -1.1953],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.7365e-02, -1.1917e-02,  1.1187e-03,  ...,  3.4943e-03,
         -4.9362e-03,  5.6572e-03],
        [-1.8585e-02,  1.2634e-02,  2.2240e-03,  ...,  1.0727e-02,
         -9.0790e-04, -1.0628e-02],
        [-8.6823e-03, -2.7618e-03,  8.0032e-03,  ..., -5.4741e-03,
          6.5193e-03, -1.9760e-03],
        ...,
        [-3.1662e-04, -6.2218e-03, -4.1199e-03,  ...,  2.5749e-03,
         -1.6754e-02,  9.4452e-03],
        [ 9.9792e-03, -1.0662e-03,  3.1872e-03,  ..., -6.7139e-03,
          4.8637e-05,  1.0834e-02],
        [-7.6981e-03, -1.5083e-02,  1.6899e-03,  ..., -4.8561e-03,
          1.9836e-02,  1.0483e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6738,  2.1836, -0.0664,  ...,  2.2500, -1.1318, -1.2246]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 16:39:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A sea is made up of water
A chocolate is made up of cocoa
A bowl is made up of glass
A cocktail is made up of alcohol
A mirror is made up of glass
A wig is made up of hair
A candy is made up of sugar
A bag is made up of
2024-07-16 16:39:10 root INFO     [order_1_approx] starting weight calculation for A cocktail is made up of alcohol
A wig is made up of hair
A chocolate is made up of cocoa
A bag is made up of leather
A bowl is made up of glass
A candy is made up of sugar
A sea is made up of water
A mirror is made up of
2024-07-16 16:39:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 16:42:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2197, -1.1445, -0.2065,  ...,  1.4043, -1.9443,  0.1326],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2649,  1.6650,  0.9341,  ..., -0.7188, -5.1367,  0.5459],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0095, -0.0066,  0.0090,  ..., -0.0004,  0.0019, -0.0120],
        [-0.0099, -0.0038,  0.0114,  ...,  0.0108,  0.0100, -0.0115],
        [ 0.0105, -0.0102,  0.0016,  ...,  0.0030,  0.0100,  0.0016],
        ...,
        [ 0.0134,  0.0038, -0.0036,  ...,  0.0094, -0.0060,  0.0033],
        [ 0.0083,  0.0006, -0.0017,  ...,  0.0022, -0.0037,  0.0011],
        [-0.0035, -0.0030, -0.0054,  ..., -0.0126,  0.0144,  0.0205]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1646,  1.5088,  0.5977,  ..., -0.2246, -5.1094,  0.4834]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 16:42:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A cocktail is made up of alcohol
A wig is made up of hair
A chocolate is made up of cocoa
A bag is made up of leather
A bowl is made up of glass
A candy is made up of sugar
A sea is made up of water
A mirror is made up of
2024-07-16 16:42:55 root INFO     total operator prediction time: 1803.7036609649658 seconds
2024-07-16 16:42:55 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on synonyms - intensity
2024-07-16 16:42:55 root INFO     building operator synonyms - intensity
2024-07-16 16:42:55 root INFO     [order_1_approx] starting weight calculation for A more intense word for poorly is afflicted
A more intense word for nap is sleep
A more intense word for lake is sea
A more intense word for confused is lost
A more intense word for love is adore
A more intense word for sniffles is pneumonia
A more intense word for irritate is enrage
A more intense word for necessary is
2024-07-16 16:42:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 16:46:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.7080, -0.2017,  0.5654,  ..., -0.5312,  0.2893,  0.8052],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.8438, -0.3066,  1.1602,  ..., -4.6445,  0.4590,  2.7285],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0022, -0.0197,  0.0089,  ..., -0.0046, -0.0026,  0.0084],
        [-0.0001,  0.0084, -0.0070,  ...,  0.0078, -0.0041,  0.0039],
        [ 0.0117,  0.0031, -0.0053,  ..., -0.0046,  0.0048,  0.0207],
        ...,
        [-0.0053, -0.0030,  0.0195,  ...,  0.0013, -0.0003, -0.0157],
        [ 0.0130,  0.0024, -0.0064,  ..., -0.0123,  0.0005, -0.0095],
        [-0.0053, -0.0231,  0.0106,  ..., -0.0014,  0.0256,  0.0069]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.4648, -0.7202,  0.7905,  ..., -3.3730,  0.3457,  2.9883]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 16:46:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for poorly is afflicted
A more intense word for nap is sleep
A more intense word for lake is sea
A more intense word for confused is lost
A more intense word for love is adore
A more intense word for sniffles is pneumonia
A more intense word for irritate is enrage
A more intense word for necessary is
2024-07-16 16:46:42 root INFO     [order_1_approx] starting weight calculation for A more intense word for poorly is afflicted
A more intense word for irritate is enrage
A more intense word for love is adore
A more intense word for necessary is essential
A more intense word for nap is sleep
A more intense word for lake is sea
A more intense word for sniffles is pneumonia
A more intense word for confused is
2024-07-16 16:46:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 16:50:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6816,  1.0068, -0.7949,  ...,  1.4863,  0.0557,  0.1777],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.5771, -2.3691, -0.1641,  ...,  2.1758,  3.4453,  0.4219],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0057, -0.0045,  0.0140,  ..., -0.0017, -0.0046,  0.0042],
        [ 0.0206, -0.0052,  0.0021,  ...,  0.0129,  0.0024, -0.0166],
        [-0.0141, -0.0017,  0.0173,  ...,  0.0062,  0.0139,  0.0057],
        ...,
        [-0.0107,  0.0074,  0.0221,  ...,  0.0045, -0.0242,  0.0134],
        [ 0.0076, -0.0131, -0.0031,  ...,  0.0006,  0.0117,  0.0026],
        [-0.0088, -0.0069,  0.0070,  ...,  0.0117, -0.0031,  0.0077]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7412, -2.8184, -0.0305,  ...,  2.1426,  2.8008, -0.1777]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 16:50:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for poorly is afflicted
A more intense word for irritate is enrage
A more intense word for love is adore
A more intense word for necessary is essential
A more intense word for nap is sleep
A more intense word for lake is sea
A more intense word for sniffles is pneumonia
A more intense word for confused is
2024-07-16 16:50:31 root INFO     [order_1_approx] starting weight calculation for A more intense word for necessary is essential
A more intense word for sniffles is pneumonia
A more intense word for confused is lost
A more intense word for love is adore
A more intense word for lake is sea
A more intense word for nap is sleep
A more intense word for poorly is afflicted
A more intense word for irritate is
2024-07-16 16:50:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 16:54:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.6758, -0.2280,  0.3389,  ...,  1.9160, -0.3408,  1.4307],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2402, -4.7188, -2.7852,  ...,  4.9609,  4.6133,  0.4600],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0104, -0.0065,  0.0114,  ..., -0.0036, -0.0065, -0.0097],
        [ 0.0090,  0.0117, -0.0013,  ..., -0.0052,  0.0041, -0.0200],
        [-0.0101,  0.0086,  0.0010,  ..., -0.0060, -0.0006, -0.0022],
        ...,
        [-0.0067, -0.0195,  0.0180,  ...,  0.0084, -0.0119,  0.0247],
        [-0.0173, -0.0112, -0.0011,  ...,  0.0109,  0.0106,  0.0168],
        [ 0.0101,  0.0107, -0.0013,  ...,  0.0056,  0.0008, -0.0149]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0215, -4.3867, -2.5625,  ...,  5.1055,  5.0195,  0.9678]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 16:54:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for necessary is essential
A more intense word for sniffles is pneumonia
A more intense word for confused is lost
A more intense word for love is adore
A more intense word for lake is sea
A more intense word for nap is sleep
A more intense word for poorly is afflicted
A more intense word for irritate is
2024-07-16 16:54:18 root INFO     [order_1_approx] starting weight calculation for A more intense word for irritate is enrage
A more intense word for lake is sea
A more intense word for confused is lost
A more intense word for necessary is essential
A more intense word for love is adore
A more intense word for poorly is afflicted
A more intense word for sniffles is pneumonia
A more intense word for nap is
2024-07-16 16:54:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 16:58:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.7412,  0.1655, -0.1953,  ...,  0.3579,  0.1460,  0.4678],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1094,  3.0820, -2.1133,  ..., -3.1445, -0.5474,  1.2246],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0017, -0.0053,  0.0002,  ...,  0.0051,  0.0055,  0.0038],
        [ 0.0065,  0.0008, -0.0045,  ...,  0.0020, -0.0151,  0.0056],
        [ 0.0012,  0.0043,  0.0037,  ..., -0.0230, -0.0077,  0.0042],
        ...,
        [ 0.0088, -0.0127,  0.0068,  ...,  0.0103, -0.0186,  0.0235],
        [-0.0079,  0.0010,  0.0108,  ..., -0.0088,  0.0053, -0.0022],
        [-0.0032,  0.0003, -0.0038,  ..., -0.0097,  0.0019,  0.0152]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5449,  2.1270, -2.0781,  ..., -3.0371, -0.3267,  1.3037]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 16:58:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for irritate is enrage
A more intense word for lake is sea
A more intense word for confused is lost
A more intense word for necessary is essential
A more intense word for love is adore
A more intense word for poorly is afflicted
A more intense word for sniffles is pneumonia
A more intense word for nap is
2024-07-16 16:58:06 root INFO     [order_1_approx] starting weight calculation for A more intense word for nap is sleep
A more intense word for necessary is essential
A more intense word for love is adore
A more intense word for poorly is afflicted
A more intense word for lake is sea
A more intense word for irritate is enrage
A more intense word for confused is lost
A more intense word for sniffles is
2024-07-16 16:58:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 17:01:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4004, -0.9756, -0.9355,  ...,  1.9814,  0.7334,  0.6084],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.5156, -3.3516,  3.4766,  ..., -2.0586,  2.6641, -2.6641],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0037, -0.0054,  0.0125,  ...,  0.0019, -0.0062, -0.0037],
        [ 0.0061, -0.0005, -0.0040,  ...,  0.0074,  0.0131,  0.0022],
        [-0.0011, -0.0050, -0.0084,  ..., -0.0043, -0.0071, -0.0005],
        ...,
        [-0.0214, -0.0110, -0.0111,  ..., -0.0056, -0.0090, -0.0020],
        [ 0.0007, -0.0058, -0.0024,  ..., -0.0107,  0.0033, -0.0067],
        [ 0.0016,  0.0095, -0.0040,  ..., -0.0035,  0.0095, -0.0079]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.5234, -3.5156,  3.1602,  ..., -1.1787,  2.5078, -2.5938]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 17:01:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for nap is sleep
A more intense word for necessary is essential
A more intense word for love is adore
A more intense word for poorly is afflicted
A more intense word for lake is sea
A more intense word for irritate is enrage
A more intense word for confused is lost
A more intense word for sniffles is
2024-07-16 17:01:54 root INFO     [order_1_approx] starting weight calculation for A more intense word for sniffles is pneumonia
A more intense word for irritate is enrage
A more intense word for nap is sleep
A more intense word for love is adore
A more intense word for confused is lost
A more intense word for necessary is essential
A more intense word for lake is sea
A more intense word for poorly is
2024-07-16 17:01:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 17:05:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6035, -1.1875,  0.4143,  ...,  0.2905, -0.4219,  1.1826],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4785, -2.2344,  0.3867,  ...,  0.3179, -0.8125, -2.6719],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0069, -0.0014, -0.0091,  ...,  0.0063,  0.0008,  0.0217],
        [-0.0031, -0.0034, -0.0001,  ..., -0.0062,  0.0119, -0.0239],
        [-0.0076,  0.0027,  0.0112,  ...,  0.0007,  0.0029,  0.0086],
        ...,
        [-0.0152, -0.0310,  0.0191,  ..., -0.0133, -0.0092, -0.0101],
        [ 0.0008,  0.0112,  0.0121,  ..., -0.0137,  0.0134, -0.0126],
        [-0.0148,  0.0299,  0.0037,  ...,  0.0099,  0.0110,  0.0113]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.4355, -1.7266,  0.5479,  ...,  0.8115, -1.7275, -2.0273]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 17:05:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for sniffles is pneumonia
A more intense word for irritate is enrage
A more intense word for nap is sleep
A more intense word for love is adore
A more intense word for confused is lost
A more intense word for necessary is essential
A more intense word for lake is sea
A more intense word for poorly is
2024-07-16 17:05:42 root INFO     [order_1_approx] starting weight calculation for A more intense word for nap is sleep
A more intense word for necessary is essential
A more intense word for poorly is afflicted
A more intense word for confused is lost
A more intense word for irritate is enrage
A more intense word for sniffles is pneumonia
A more intense word for love is adore
A more intense word for lake is
2024-07-16 17:05:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 17:09:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.8867,  0.3547,  0.9712,  ...,  0.0952, -0.1152,  1.1455],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2649,  3.6152, -4.1758,  ...,  3.0332, -2.0664, -0.9229],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0015, -0.0034,  0.0150,  ...,  0.0037, -0.0040,  0.0037],
        [ 0.0072, -0.0151, -0.0120,  ...,  0.0030,  0.0038,  0.0045],
        [-0.0038,  0.0127,  0.0086,  ...,  0.0064,  0.0014, -0.0029],
        ...,
        [ 0.0004, -0.0100,  0.0019,  ..., -0.0005, -0.0068,  0.0238],
        [-0.0013, -0.0006, -0.0006,  ..., -0.0065,  0.0129, -0.0063],
        [-0.0080,  0.0079,  0.0178,  ...,  0.0062,  0.0069, -0.0125]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5000,  4.6719, -2.6035,  ...,  2.5957, -1.8594,  0.2646]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 17:09:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for nap is sleep
A more intense word for necessary is essential
A more intense word for poorly is afflicted
A more intense word for confused is lost
A more intense word for irritate is enrage
A more intense word for sniffles is pneumonia
A more intense word for love is adore
A more intense word for lake is
2024-07-16 17:09:29 root INFO     [order_1_approx] starting weight calculation for A more intense word for poorly is afflicted
A more intense word for irritate is enrage
A more intense word for sniffles is pneumonia
A more intense word for nap is sleep
A more intense word for confused is lost
A more intense word for necessary is essential
A more intense word for lake is sea
A more intense word for love is
2024-07-16 17:09:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 17:13:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.4590,  0.0369,  0.4990,  ...,  0.7007, -0.8740,  1.0127],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5098,  0.0371, -1.7676,  ..., -4.0859,  2.2109, -1.9531],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0040,  0.0052, -0.0011,  ..., -0.0060, -0.0028,  0.0074],
        [ 0.0075, -0.0121, -0.0058,  ...,  0.0014, -0.0017, -0.0020],
        [-0.0093, -0.0030, -0.0036,  ...,  0.0026, -0.0021, -0.0053],
        ...,
        [-0.0117,  0.0103,  0.0218,  ..., -0.0233, -0.0053, -0.0150],
        [ 0.0051, -0.0053,  0.0077,  ..., -0.0054, -0.0028, -0.0109],
        [ 0.0002, -0.0049,  0.0112,  ...,  0.0125,  0.0040, -0.0064]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0508, -0.3103, -0.5176,  ..., -3.6992,  2.2461, -1.2461]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 17:13:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for poorly is afflicted
A more intense word for irritate is enrage
A more intense word for sniffles is pneumonia
A more intense word for nap is sleep
A more intense word for confused is lost
A more intense word for necessary is essential
A more intense word for lake is sea
A more intense word for love is
2024-07-16 17:13:15 root INFO     total operator prediction time: 1820.2335736751556 seconds
2024-07-16 17:13:15 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on hypernyms - animals
2024-07-16 17:13:15 root INFO     building operator hypernyms - animals
2024-07-16 17:13:15 root INFO     [order_1_approx] starting weight calculation for The cat falls into the category of feline
The falcon falls into the category of raptor
The cow falls into the category of bovid
The owl falls into the category of raptor
The tiger falls into the category of feline
The orangutan falls into the category of primate
The beetle falls into the category of insect
The duck falls into the category of
2024-07-16 17:13:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 17:17:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3328, -1.5137, -0.5869,  ...,  0.8057, -1.4297,  0.4341],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6855,  1.4590, -4.3320,  ...,  3.9141,  2.2207,  0.0713],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0023, -0.0060, -0.0002,  ...,  0.0025,  0.0095, -0.0060],
        [-0.0035, -0.0013, -0.0023,  ...,  0.0068, -0.0087,  0.0002],
        [ 0.0108, -0.0090, -0.0088,  ..., -0.0160,  0.0079,  0.0035],
        ...,
        [-0.0260, -0.0179,  0.0118,  ...,  0.0204, -0.0120,  0.0075],
        [-0.0051, -0.0085, -0.0011,  ..., -0.0021,  0.0020, -0.0004],
        [-0.0090, -0.0137,  0.0127,  ...,  0.0034,  0.0077,  0.0136]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2783,  3.3008, -5.8750,  ...,  5.1797,  2.0957,  0.3879]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 17:17:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The cat falls into the category of feline
The falcon falls into the category of raptor
The cow falls into the category of bovid
The owl falls into the category of raptor
The tiger falls into the category of feline
The orangutan falls into the category of primate
The beetle falls into the category of insect
The duck falls into the category of
2024-07-16 17:17:02 root INFO     [order_1_approx] starting weight calculation for The cat falls into the category of feline
The duck falls into the category of fowl
The falcon falls into the category of raptor
The cow falls into the category of bovid
The beetle falls into the category of insect
The owl falls into the category of raptor
The orangutan falls into the category of primate
The tiger falls into the category of
2024-07-16 17:17:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 17:20:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3945, -0.5610, -0.2330,  ...,  1.5518,  0.5127, -0.3403],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.5703,  0.1991,  1.1572,  ..., -2.1875, -6.5469,  2.0508],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0014, -0.0135, -0.0052,  ..., -0.0047,  0.0122, -0.0009],
        [ 0.0001, -0.0106,  0.0062,  ...,  0.0024, -0.0056,  0.0016],
        [-0.0032, -0.0096, -0.0036,  ...,  0.0026, -0.0068,  0.0109],
        ...,
        [-0.0012, -0.0046,  0.0008,  ...,  0.0142, -0.0126,  0.0137],
        [ 0.0079,  0.0066, -0.0037,  ...,  0.0007,  0.0053, -0.0129],
        [-0.0029, -0.0123,  0.0094,  ..., -0.0020,  0.0020,  0.0095]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.6807,  0.5591,  1.4238,  ..., -2.2891, -6.8281,  2.2227]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 17:20:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The cat falls into the category of feline
The duck falls into the category of fowl
The falcon falls into the category of raptor
The cow falls into the category of bovid
The beetle falls into the category of insect
The owl falls into the category of raptor
The orangutan falls into the category of primate
The tiger falls into the category of
2024-07-16 17:20:48 root INFO     [order_1_approx] starting weight calculation for The orangutan falls into the category of primate
The duck falls into the category of fowl
The owl falls into the category of raptor
The beetle falls into the category of insect
The tiger falls into the category of feline
The cat falls into the category of feline
The cow falls into the category of bovid
The falcon falls into the category of
2024-07-16 17:20:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 17:24:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2988, -1.2881, -0.9863,  ..., -0.8740, -0.2520,  0.4575],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7969,  0.3804, -5.4102,  ...,  1.0342, -0.6758,  2.2188],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0003, -0.0023, -0.0018,  ..., -0.0042, -0.0021, -0.0016],
        [ 0.0063,  0.0013,  0.0107,  ...,  0.0063,  0.0058, -0.0001],
        [ 0.0021, -0.0074,  0.0033,  ..., -0.0056,  0.0004, -0.0015],
        ...,
        [-0.0036, -0.0115, -0.0032,  ...,  0.0111, -0.0019,  0.0107],
        [-0.0064, -0.0031,  0.0005,  ..., -0.0006,  0.0072,  0.0046],
        [-0.0062, -0.0023,  0.0038,  ..., -0.0035,  0.0094,  0.0146]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9365,  0.6221, -5.7930,  ...,  1.1309, -1.0586,  2.2734]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 17:24:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The orangutan falls into the category of primate
The duck falls into the category of fowl
The owl falls into the category of raptor
The beetle falls into the category of insect
The tiger falls into the category of feline
The cat falls into the category of feline
The cow falls into the category of bovid
The falcon falls into the category of
2024-07-16 17:24:34 root INFO     [order_1_approx] starting weight calculation for The orangutan falls into the category of primate
The cat falls into the category of feline
The falcon falls into the category of raptor
The duck falls into the category of fowl
The beetle falls into the category of insect
The cow falls into the category of bovid
The tiger falls into the category of feline
The owl falls into the category of
2024-07-16 17:24:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 17:28:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8125, -0.0867, -0.4380,  ..., -0.6152, -0.5513,  0.3809],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1807,  4.3164, -0.8188,  ...,  1.5078, -2.2344, -0.0664],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0107, -0.0123, -0.0031,  ...,  0.0031,  0.0026,  0.0052],
        [ 0.0153, -0.0004,  0.0269,  ...,  0.0069, -0.0052,  0.0119],
        [ 0.0032,  0.0005, -0.0097,  ..., -0.0006,  0.0143,  0.0097],
        ...,
        [-0.0093, -0.0066,  0.0107,  ...,  0.0134,  0.0031,  0.0135],
        [-0.0072, -0.0133,  0.0070,  ..., -0.0010,  0.0090, -0.0119],
        [-0.0030, -0.0103,  0.0048,  ..., -0.0110,  0.0057,  0.0175]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0273,  5.2109, -1.6924,  ...,  2.4004, -2.2598, -0.1392]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 17:28:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The orangutan falls into the category of primate
The cat falls into the category of feline
The falcon falls into the category of raptor
The duck falls into the category of fowl
The beetle falls into the category of insect
The cow falls into the category of bovid
The tiger falls into the category of feline
The owl falls into the category of
2024-07-16 17:28:20 root INFO     [order_1_approx] starting weight calculation for The falcon falls into the category of raptor
The tiger falls into the category of feline
The owl falls into the category of raptor
The orangutan falls into the category of primate
The cat falls into the category of feline
The duck falls into the category of fowl
The beetle falls into the category of insect
The cow falls into the category of
2024-07-16 17:28:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 17:32:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2971,  0.0369,  0.0084,  ...,  0.3809, -0.2456,  1.4004],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.2461, -0.5850, -0.2600,  ..., -1.7852, -1.7744,  0.5762],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-7.1526e-04, -1.1284e-02, -5.0240e-03,  ...,  3.8948e-03,
          1.9035e-03, -5.2719e-03],
        [ 2.3708e-03,  2.6188e-03,  1.7586e-03,  ...,  4.3335e-03,
          8.6164e-04,  2.7828e-03],
        [-3.1643e-03, -4.7646e-03, -8.8577e-03,  ..., -5.6992e-03,
          9.1171e-04,  1.4668e-03],
        ...,
        [-7.4921e-03, -4.9171e-03, -1.2226e-03,  ..., -6.6147e-03,
          3.1090e-04,  1.5167e-02],
        [-2.0981e-05, -1.1581e-02, -7.3090e-03,  ..., -4.6577e-03,
          9.1553e-03, -2.4796e-03],
        [-1.4626e-02, -3.4618e-03,  1.1063e-02,  ...,  2.1744e-04,
         -2.0676e-03,  8.3313e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.4805,  0.6357, -0.3193,  ..., -1.7705, -1.7441,  0.3826]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 17:32:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The falcon falls into the category of raptor
The tiger falls into the category of feline
The owl falls into the category of raptor
The orangutan falls into the category of primate
The cat falls into the category of feline
The duck falls into the category of fowl
The beetle falls into the category of insect
The cow falls into the category of
2024-07-16 17:32:07 root INFO     [order_1_approx] starting weight calculation for The falcon falls into the category of raptor
The owl falls into the category of raptor
The tiger falls into the category of feline
The cow falls into the category of bovid
The orangutan falls into the category of primate
The beetle falls into the category of insect
The duck falls into the category of fowl
The cat falls into the category of
2024-07-16 17:32:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 17:35:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3999,  1.1836,  0.9209,  ..., -1.2256, -0.4768, -1.3516],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8809,  0.9834,  2.0000,  ..., -1.4619, -3.3223,  0.3403],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0015, -0.0043, -0.0074,  ..., -0.0022,  0.0008, -0.0010],
        [ 0.0057, -0.0022,  0.0052,  ...,  0.0027, -0.0004,  0.0034],
        [ 0.0016, -0.0028, -0.0030,  ..., -0.0001, -0.0041,  0.0020],
        ...,
        [ 0.0008, -0.0010, -0.0024,  ..., -0.0003, -0.0021,  0.0039],
        [ 0.0039, -0.0042, -0.0008,  ...,  0.0008, -0.0028, -0.0037],
        [-0.0015,  0.0005, -0.0022,  ..., -0.0014,  0.0010,  0.0017]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9097,  0.8330,  1.9170,  ..., -1.3770, -3.1973,  0.3987]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 17:35:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The falcon falls into the category of raptor
The owl falls into the category of raptor
The tiger falls into the category of feline
The cow falls into the category of bovid
The orangutan falls into the category of primate
The beetle falls into the category of insect
The duck falls into the category of fowl
The cat falls into the category of
2024-07-16 17:35:53 root INFO     [order_1_approx] starting weight calculation for The cow falls into the category of bovid
The tiger falls into the category of feline
The duck falls into the category of fowl
The beetle falls into the category of insect
The owl falls into the category of raptor
The cat falls into the category of feline
The falcon falls into the category of raptor
The orangutan falls into the category of
2024-07-16 17:35:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 17:39:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8291, -1.6934, -0.8467,  ...,  0.3975,  0.4424,  0.5137],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.7383, -1.6328,  2.8984,  ..., -0.4165, -2.4785, -1.6816],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0076, -0.0023,  0.0023,  ...,  0.0045, -0.0007, -0.0010],
        [ 0.0063,  0.0043,  0.0061,  ...,  0.0115, -0.0049, -0.0011],
        [ 0.0058,  0.0058,  0.0055,  ...,  0.0044, -0.0062, -0.0004],
        ...,
        [-0.0115, -0.0017, -0.0039,  ...,  0.0046, -0.0102,  0.0098],
        [ 0.0028, -0.0041, -0.0044,  ...,  0.0054,  0.0070, -0.0058],
        [ 0.0040,  0.0014,  0.0040,  ..., -0.0004,  0.0016,  0.0090]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.0742, -1.3223,  2.8086,  ..., -0.1665, -3.3789, -1.5938]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 17:39:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The cow falls into the category of bovid
The tiger falls into the category of feline
The duck falls into the category of fowl
The beetle falls into the category of insect
The owl falls into the category of raptor
The cat falls into the category of feline
The falcon falls into the category of raptor
The orangutan falls into the category of
2024-07-16 17:39:38 root INFO     [order_1_approx] starting weight calculation for The tiger falls into the category of feline
The duck falls into the category of fowl
The cat falls into the category of feline
The orangutan falls into the category of primate
The falcon falls into the category of raptor
The owl falls into the category of raptor
The cow falls into the category of bovid
The beetle falls into the category of
2024-07-16 17:39:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 17:43:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1821, -0.2915, -0.3782,  ..., -0.1158,  0.5464,  0.0613],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.2988,  0.8672, -1.1494,  ...,  0.6533, -3.5957,  0.6494],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0056, -0.0050, -0.0033,  ...,  0.0058, -0.0045, -0.0028],
        [ 0.0010, -0.0042,  0.0100,  ...,  0.0140, -0.0115,  0.0072],
        [-0.0007, -0.0157, -0.0082,  ..., -0.0010, -0.0072,  0.0108],
        ...,
        [-0.0115, -0.0166, -0.0006,  ...,  0.0253, -0.0059,  0.0063],
        [ 0.0033, -0.0148,  0.0024,  ..., -0.0099, -0.0048,  0.0051],
        [-0.0081, -0.0055,  0.0051,  ..., -0.0131,  0.0052,  0.0011]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.4648,  0.6240, -2.5000,  ...,  0.7168, -3.4805,  0.6753]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 17:43:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The tiger falls into the category of feline
The duck falls into the category of fowl
The cat falls into the category of feline
The orangutan falls into the category of primate
The falcon falls into the category of raptor
The owl falls into the category of raptor
The cow falls into the category of bovid
The beetle falls into the category of
2024-07-16 17:43:27 root INFO     total operator prediction time: 1811.6871473789215 seconds
2024-07-16 17:43:27 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on hyponyms - misc
2024-07-16 17:43:27 root INFO     building operator hyponyms - misc
2024-07-16 17:43:27 root INFO     [order_1_approx] starting weight calculation for A more specific term for a cloud is thundercloud
A more specific term for a weapon is gun
A more specific term for a citrus is lemon
A more specific term for a guitar is ukulele
A more specific term for a candy is lollipop
A more specific term for a sweater is turtleneck
A more specific term for a seat is chair
A more specific term for a dress is
2024-07-16 17:43:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 17:47:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3530, -0.5552,  0.4275,  ...,  1.1562, -1.2656,  0.0791],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3281, -0.8682, -2.6953,  ...,  0.7466, -1.0137, -0.9658],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.1856e-02, -7.5493e-03, -1.9318e-02,  ...,  3.7670e-05,
         -4.5242e-03,  1.3466e-02],
        [-3.6278e-03, -3.1052e-03,  5.9814e-03,  ...,  9.1629e-03,
          2.9984e-03, -9.3155e-03],
        [-1.1749e-02,  3.9291e-03,  7.0114e-03,  ..., -8.1406e-03,
         -5.7602e-03,  2.1088e-02],
        ...,
        [-1.3412e-02, -1.0773e-02,  2.0447e-03,  ...,  2.9175e-02,
         -9.5367e-03,  5.0354e-04],
        [ 9.1171e-04,  9.8495e-03,  7.2861e-04,  ..., -7.4577e-04,
          7.2975e-03, -2.0142e-03],
        [-9.3651e-04,  4.4823e-04,  1.5053e-02,  ..., -4.8332e-03,
          1.0460e-02,  1.1955e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4229, -0.7588, -2.0059,  ...,  1.5693, -1.2949, -0.9170]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 17:47:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a cloud is thundercloud
A more specific term for a weapon is gun
A more specific term for a citrus is lemon
A more specific term for a guitar is ukulele
A more specific term for a candy is lollipop
A more specific term for a sweater is turtleneck
A more specific term for a seat is chair
A more specific term for a dress is
2024-07-16 17:47:14 root INFO     [order_1_approx] starting weight calculation for A more specific term for a sweater is turtleneck
A more specific term for a cloud is thundercloud
A more specific term for a dress is gown
A more specific term for a citrus is lemon
A more specific term for a candy is lollipop
A more specific term for a weapon is gun
A more specific term for a guitar is ukulele
A more specific term for a seat is
2024-07-16 17:47:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 17:51:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0586, -0.1807,  0.2119,  ...,  1.4785, -0.6362,  0.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0723, -2.7891, -2.9883,  ..., -0.2881,  0.3606, -0.1714],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.5421e-02,  1.3485e-03, -7.8011e-03,  ...,  5.8784e-03,
         -7.2002e-05, -1.2756e-02],
        [ 1.8631e-02,  5.4550e-03,  1.1230e-02,  ..., -5.3253e-03,
          7.2556e-03, -1.1566e-02],
        [-2.4536e-02,  7.2746e-03,  3.1372e-02,  ..., -1.2131e-02,
          3.6354e-03,  3.7079e-03],
        ...,
        [-1.3145e-02, -6.4354e-03, -1.8311e-03,  ...,  1.1475e-02,
         -1.6968e-02,  9.9869e-03],
        [-3.2425e-05, -1.3474e-02,  5.5275e-03,  ...,  6.1073e-03,
          9.9182e-04, -5.2261e-03],
        [ 1.0757e-03,  6.8207e-03,  1.7380e-02,  ..., -1.5961e-02,
         -5.3329e-03,  3.1219e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5278, -3.1699, -1.9980,  ..., -0.1062,  0.3516,  0.1514]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 17:51:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a sweater is turtleneck
A more specific term for a cloud is thundercloud
A more specific term for a dress is gown
A more specific term for a citrus is lemon
A more specific term for a candy is lollipop
A more specific term for a weapon is gun
A more specific term for a guitar is ukulele
A more specific term for a seat is
2024-07-16 17:51:01 root INFO     [order_1_approx] starting weight calculation for A more specific term for a candy is lollipop
A more specific term for a cloud is thundercloud
A more specific term for a seat is chair
A more specific term for a dress is gown
A more specific term for a citrus is lemon
A more specific term for a sweater is turtleneck
A more specific term for a weapon is gun
A more specific term for a guitar is
2024-07-16 17:51:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 17:54:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.7441, -0.0063,  0.3335,  ...,  0.6929, -0.4956,  0.0322],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2041, -5.4102, -0.3037,  ..., -3.0781,  4.1836, -3.5020],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 8.2092e-03, -2.7954e-02, -1.2199e-02,  ...,  1.6342e-02,
         -1.3153e-02, -6.1951e-03],
        [-1.0315e-02,  1.3809e-03,  1.2421e-02,  ...,  4.3335e-03,
         -5.3902e-03, -9.2926e-03],
        [-1.6670e-03,  2.3224e-02, -4.9553e-03,  ..., -6.6528e-03,
          1.2238e-02,  2.1713e-02],
        ...,
        [-2.7115e-02, -1.9394e-02, -3.8338e-03,  ...,  1.2802e-02,
          2.6150e-03,  1.0147e-03],
        [-2.3613e-03, -9.8419e-03,  2.5024e-03,  ...,  8.9645e-04,
         -2.4216e-02,  1.7838e-02],
        [-4.5509e-03, -1.1168e-03,  7.8201e-05,  ..., -4.1351e-03,
          2.6443e-02,  2.3621e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1272, -5.8477, -0.4277,  ..., -2.1836,  4.7109, -3.4180]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 17:54:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a candy is lollipop
A more specific term for a cloud is thundercloud
A more specific term for a seat is chair
A more specific term for a dress is gown
A more specific term for a citrus is lemon
A more specific term for a sweater is turtleneck
A more specific term for a weapon is gun
A more specific term for a guitar is
2024-07-16 17:54:47 root INFO     [order_1_approx] starting weight calculation for A more specific term for a candy is lollipop
A more specific term for a cloud is thundercloud
A more specific term for a weapon is gun
A more specific term for a seat is chair
A more specific term for a dress is gown
A more specific term for a guitar is ukulele
A more specific term for a sweater is turtleneck
A more specific term for a citrus is
2024-07-16 17:54:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 17:58:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9355, -0.8359,  1.5781,  ...,  0.7231, -0.9346,  1.1660],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0312, -1.5469, -2.9414,  ..., -2.3965,  2.6895,  0.7734],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0110, -0.0109,  0.0007,  ..., -0.0051, -0.0085,  0.0105],
        [-0.0038, -0.0104,  0.0139,  ..., -0.0065, -0.0007,  0.0213],
        [-0.0117,  0.0091,  0.0077,  ...,  0.0187, -0.0092,  0.0210],
        ...,
        [-0.0012, -0.0141,  0.0150,  ...,  0.0248,  0.0055, -0.0044],
        [ 0.0182,  0.0009,  0.0072,  ...,  0.0065,  0.0024,  0.0091],
        [ 0.0043, -0.0019,  0.0035,  ..., -0.0029,  0.0167,  0.0252]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0742, -0.3291, -2.3789,  ..., -1.7305,  2.9883, -0.5195]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 17:58:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a candy is lollipop
A more specific term for a cloud is thundercloud
A more specific term for a weapon is gun
A more specific term for a seat is chair
A more specific term for a dress is gown
A more specific term for a guitar is ukulele
A more specific term for a sweater is turtleneck
A more specific term for a citrus is
2024-07-16 17:58:34 root INFO     [order_1_approx] starting weight calculation for A more specific term for a seat is chair
A more specific term for a dress is gown
A more specific term for a weapon is gun
A more specific term for a cloud is thundercloud
A more specific term for a candy is lollipop
A more specific term for a guitar is ukulele
A more specific term for a citrus is lemon
A more specific term for a sweater is
2024-07-16 17:58:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 18:02:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4795,  0.2805, -1.3496,  ...,  1.6953,  0.4617, -0.3601],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0439,  1.0469,  4.1562,  ..., -0.6592, -1.0771, -4.0078],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0124, -0.0162,  0.0015,  ...,  0.0069,  0.0039,  0.0134],
        [ 0.0005,  0.0038,  0.0098,  ..., -0.0010,  0.0143, -0.0044],
        [-0.0196, -0.0073,  0.0076,  ..., -0.0141, -0.0053,  0.0162],
        ...,
        [-0.0077,  0.0077, -0.0099,  ...,  0.0179, -0.0346,  0.0100],
        [ 0.0058,  0.0008,  0.0031,  ..., -0.0080,  0.0155,  0.0193],
        [-0.0049, -0.0178,  0.0019,  ..., -0.0023,  0.0138,  0.0233]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4011,  1.5859,  4.9023,  ...,  0.8926, -1.4180, -3.4219]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 18:02:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a seat is chair
A more specific term for a dress is gown
A more specific term for a weapon is gun
A more specific term for a cloud is thundercloud
A more specific term for a candy is lollipop
A more specific term for a guitar is ukulele
A more specific term for a citrus is lemon
A more specific term for a sweater is
2024-07-16 18:02:17 root INFO     [order_1_approx] starting weight calculation for A more specific term for a seat is chair
A more specific term for a sweater is turtleneck
A more specific term for a dress is gown
A more specific term for a citrus is lemon
A more specific term for a cloud is thundercloud
A more specific term for a guitar is ukulele
A more specific term for a weapon is gun
A more specific term for a candy is
2024-07-16 18:02:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 18:06:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2449,  0.5742, -0.3762,  ..., -0.0592,  0.1552,  0.2717],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.2793,  3.3945, -2.5371,  ..., -2.2578,  2.0195,  2.2559],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0163, -0.0207, -0.0182,  ..., -0.0042, -0.0050, -0.0088],
        [-0.0001, -0.0111, -0.0164,  ...,  0.0053, -0.0052,  0.0023],
        [-0.0160,  0.0086,  0.0059,  ..., -0.0013,  0.0013,  0.0103],
        ...,
        [ 0.0032, -0.0070, -0.0046,  ...,  0.0109,  0.0027,  0.0030],
        [-0.0085,  0.0057,  0.0050,  ..., -0.0032,  0.0152,  0.0104],
        [-0.0145, -0.0066,  0.0014,  ..., -0.0041,  0.0067,  0.0081]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0234,  3.3535, -2.6172,  ..., -1.4482,  1.6934,  2.3945]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 18:06:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a seat is chair
A more specific term for a sweater is turtleneck
A more specific term for a dress is gown
A more specific term for a citrus is lemon
A more specific term for a cloud is thundercloud
A more specific term for a guitar is ukulele
A more specific term for a weapon is gun
A more specific term for a candy is
2024-07-16 18:06:02 root INFO     [order_1_approx] starting weight calculation for A more specific term for a sweater is turtleneck
A more specific term for a weapon is gun
A more specific term for a citrus is lemon
A more specific term for a dress is gown
A more specific term for a guitar is ukulele
A more specific term for a seat is chair
A more specific term for a candy is lollipop
A more specific term for a cloud is
2024-07-16 18:06:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 18:09:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2476, -0.2396,  0.1594,  ...,  0.2563, -0.0454,  1.7031],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6226,  3.4375, -0.8081,  ...,  3.0840, -0.2344,  1.0381],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0054,  0.0088,  0.0056,  ...,  0.0019,  0.0128, -0.0137],
        [-0.0024, -0.0023,  0.0155,  ...,  0.0103,  0.0014, -0.0008],
        [-0.0046, -0.0114,  0.0130,  ..., -0.0006, -0.0172,  0.0133],
        ...,
        [-0.0129,  0.0207, -0.0017,  ...,  0.0104, -0.0184, -0.0133],
        [ 0.0067, -0.0062, -0.0064,  ..., -0.0143,  0.0022, -0.0031],
        [ 0.0093,  0.0161,  0.0201,  ..., -0.0146, -0.0061,  0.0059]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0244,  2.8516, -0.8101,  ...,  3.5430, -0.2111,  0.9688]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 18:09:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a sweater is turtleneck
A more specific term for a weapon is gun
A more specific term for a citrus is lemon
A more specific term for a dress is gown
A more specific term for a guitar is ukulele
A more specific term for a seat is chair
A more specific term for a candy is lollipop
A more specific term for a cloud is
2024-07-16 18:09:45 root INFO     [order_1_approx] starting weight calculation for A more specific term for a sweater is turtleneck
A more specific term for a seat is chair
A more specific term for a cloud is thundercloud
A more specific term for a candy is lollipop
A more specific term for a citrus is lemon
A more specific term for a guitar is ukulele
A more specific term for a dress is gown
A more specific term for a weapon is
2024-07-16 18:09:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 18:13:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4355,  0.6572,  0.6265,  ...,  0.3159, -0.4814,  0.5952],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6074, -0.0574, -2.5625,  ..., -0.8589,  3.1289, -0.0830],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0007, -0.0044, -0.0002,  ..., -0.0104, -0.0138, -0.0042],
        [ 0.0064,  0.0008, -0.0046,  ...,  0.0024, -0.0051, -0.0140],
        [-0.0010, -0.0033,  0.0168,  ..., -0.0041, -0.0029,  0.0114],
        ...,
        [-0.0089, -0.0107,  0.0073,  ...,  0.0146, -0.0108,  0.0009],
        [-0.0275, -0.0021,  0.0077,  ..., -0.0160, -0.0098,  0.0081],
        [-0.0017, -0.0002,  0.0019,  ...,  0.0091,  0.0190,  0.0116]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.9941, -0.3755, -1.9502,  ..., -0.8408,  2.9082, -0.7280]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 18:13:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a sweater is turtleneck
A more specific term for a seat is chair
A more specific term for a cloud is thundercloud
A more specific term for a candy is lollipop
A more specific term for a citrus is lemon
A more specific term for a guitar is ukulele
A more specific term for a dress is gown
A more specific term for a weapon is
2024-07-16 18:13:32 root INFO     total operator prediction time: 1805.238317489624 seconds
2024-07-16 18:13:32 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on antonyms - binary
2024-07-16 18:13:32 root INFO     building operator antonyms - binary
2024-07-16 18:13:32 root INFO     [order_1_approx] starting weight calculation for The opposite of out is in
The opposite of inhale is exhale
The opposite of inbound is outbound
The opposite of first is last
The opposite of fall is rise
The opposite of below is above
The opposite of submerge is emerge
The opposite of anterior is
2024-07-16 18:13:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 18:17:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0059, -0.9648, -0.3159,  ..., -0.2134,  1.6670,  0.8506],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.3203, -4.5078, -4.6562,  ...,  1.6758,  0.8779,  0.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.5787e-02, -1.8330e-03,  1.1673e-03,  ...,  1.5213e-02,
         -8.5754e-03, -2.2278e-03],
        [ 9.2621e-03,  4.4022e-03, -2.4323e-02,  ..., -2.8839e-03,
         -2.3666e-02,  1.2863e-02],
        [-1.2207e-02, -1.6144e-02, -2.8015e-02,  ..., -1.1406e-02,
          1.0902e-02,  5.1856e-05],
        ...,
        [ 2.2202e-03,  3.4294e-03,  1.3428e-02,  ...,  2.2003e-02,
          4.4441e-04,  8.1635e-03],
        [ 9.3994e-03,  3.2196e-03,  1.7700e-02,  ..., -1.3031e-02,
         -9.9182e-03,  1.8854e-03],
        [-5.5313e-03, -2.3880e-02, -9.4681e-03,  ..., -2.2736e-02,
         -1.1169e-02, -3.1185e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.5176, -2.5059, -4.6250,  ...,  2.0254,  0.8706,  0.2152]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 18:17:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of out is in
The opposite of inhale is exhale
The opposite of inbound is outbound
The opposite of first is last
The opposite of fall is rise
The opposite of below is above
The opposite of submerge is emerge
The opposite of anterior is
2024-07-16 18:17:19 root INFO     [order_1_approx] starting weight calculation for The opposite of fall is rise
The opposite of inbound is outbound
The opposite of first is last
The opposite of below is above
The opposite of inhale is exhale
The opposite of anterior is posterior
The opposite of submerge is emerge
The opposite of out is
2024-07-16 18:17:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 18:21:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1338, -0.0962, -0.7671,  ..., -0.7632,  1.4824,  1.2441],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.8086, -3.0723, -0.8867,  ..., -2.6426,  4.0625, -0.0591],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-4.3526e-03, -1.2787e-02,  9.3079e-03,  ...,  1.3718e-02,
         -6.9580e-03, -7.1716e-04],
        [-2.1759e-02,  1.5854e-02, -5.5847e-03,  ...,  1.4748e-02,
         -6.9733e-03, -3.9062e-03],
        [-3.7384e-03,  9.6359e-03, -1.2398e-05,  ...,  1.1642e-02,
          2.4891e-03,  1.7288e-02],
        ...,
        [ 5.8746e-03,  1.4053e-02,  2.9526e-03,  ...,  1.3275e-02,
         -6.9656e-03,  7.0801e-03],
        [-9.4795e-04, -1.2733e-02,  5.4054e-03,  ..., -1.2337e-02,
         -1.0818e-02, -1.2062e-02],
        [ 1.0788e-02, -1.2199e-02, -1.2650e-02,  ..., -9.3536e-03,
         -1.6113e-02,  2.0844e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6992, -3.8652, -0.8306,  ..., -2.8750,  4.0742, -0.2371]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 18:21:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of fall is rise
The opposite of inbound is outbound
The opposite of first is last
The opposite of below is above
The opposite of inhale is exhale
The opposite of anterior is posterior
The opposite of submerge is emerge
The opposite of out is
2024-07-16 18:21:05 root INFO     [order_1_approx] starting weight calculation for The opposite of anterior is posterior
The opposite of below is above
The opposite of first is last
The opposite of submerge is emerge
The opposite of fall is rise
The opposite of out is in
The opposite of inbound is outbound
The opposite of inhale is
2024-07-16 18:21:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 18:24:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0938, -0.2266, -0.6084,  ..., -0.3452, -0.3660,  1.4053],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2891,  0.4863,  0.2793,  ..., -0.2290,  2.0312, -2.2324],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0342, -0.0101, -0.0133,  ..., -0.0051, -0.0143, -0.0049],
        [-0.0156, -0.0060, -0.0018,  ..., -0.0070,  0.0098,  0.0075],
        [-0.0038, -0.0004, -0.0350,  ..., -0.0062,  0.0049, -0.0018],
        ...,
        [-0.0130, -0.0183, -0.0196,  ..., -0.0258, -0.0069, -0.0040],
        [-0.0010, -0.0037,  0.0194,  ..., -0.0072, -0.0443,  0.0099],
        [ 0.0111, -0.0031, -0.0114,  ..., -0.0154,  0.0045, -0.0171]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3623,  1.4346,  0.0496,  ..., -0.0831,  2.6777, -2.1621]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 18:24:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of anterior is posterior
The opposite of below is above
The opposite of first is last
The opposite of submerge is emerge
The opposite of fall is rise
The opposite of out is in
The opposite of inbound is outbound
The opposite of inhale is
2024-07-16 18:24:52 root INFO     [order_1_approx] starting weight calculation for The opposite of submerge is emerge
The opposite of inhale is exhale
The opposite of fall is rise
The opposite of out is in
The opposite of anterior is posterior
The opposite of below is above
The opposite of first is last
The opposite of inbound is
2024-07-16 18:24:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 18:28:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4639,  0.1345, -0.5449,  ..., -1.2422, -0.3264, -1.0723],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1881, -4.0703, -1.0449,  ...,  1.9707,  5.2344,  0.9512],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-4.2343e-03, -1.6617e-02,  1.1559e-02,  ..., -2.2980e-02,
         -2.1591e-02, -1.7975e-02],
        [-1.5076e-02, -2.1042e-02, -9.9182e-04,  ...,  2.6627e-03,
         -5.1498e-03, -2.9488e-03],
        [-2.7283e-02, -1.0910e-02, -4.4678e-02,  ..., -6.1531e-03,
          4.0588e-03, -2.3117e-02],
        ...,
        [ 1.7883e-02, -1.3962e-03,  4.9019e-03,  ...,  1.4725e-03,
          1.2383e-02,  2.2858e-02],
        [-1.9913e-02,  1.7441e-02,  9.0561e-03,  ..., -4.5319e-03,
         -3.8986e-03, -5.6801e-03],
        [-1.6113e-02,  6.1874e-03,  8.7738e-05,  ..., -2.6569e-03,
          3.7270e-03, -1.4191e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4580, -4.0195, -1.4668,  ...,  3.0000,  5.3555,  1.4521]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 18:28:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of submerge is emerge
The opposite of inhale is exhale
The opposite of fall is rise
The opposite of out is in
The opposite of anterior is posterior
The opposite of below is above
The opposite of first is last
The opposite of inbound is
2024-07-16 18:28:38 root INFO     [order_1_approx] starting weight calculation for The opposite of inhale is exhale
The opposite of first is last
The opposite of anterior is posterior
The opposite of out is in
The opposite of below is above
The opposite of inbound is outbound
The opposite of fall is rise
The opposite of submerge is
2024-07-16 18:28:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 18:32:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9609, -0.3862,  0.6387,  ..., -0.6621,  0.1957,  0.0647],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6680, -6.3516, -3.0312,  ...,  4.1367, -2.1953,  0.2598],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0271, -0.0104, -0.0071,  ...,  0.0162, -0.0077, -0.0020],
        [-0.0037,  0.0097,  0.0030,  ..., -0.0136, -0.0100, -0.0088],
        [ 0.0089, -0.0096, -0.0147,  ..., -0.0001,  0.0005,  0.0081],
        ...,
        [-0.0116, -0.0067, -0.0130,  ..., -0.0059,  0.0107, -0.0027],
        [-0.0156, -0.0024,  0.0044,  ..., -0.0081, -0.0153, -0.0082],
        [ 0.0039, -0.0061,  0.0084,  ...,  0.0078, -0.0025, -0.0235]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7324, -6.2109, -3.4355,  ...,  4.8516, -1.9062,  0.3708]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 18:32:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of inhale is exhale
The opposite of first is last
The opposite of anterior is posterior
The opposite of out is in
The opposite of below is above
The opposite of inbound is outbound
The opposite of fall is rise
The opposite of submerge is
2024-07-16 18:32:25 root INFO     [order_1_approx] starting weight calculation for The opposite of below is above
The opposite of anterior is posterior
The opposite of inbound is outbound
The opposite of inhale is exhale
The opposite of submerge is emerge
The opposite of out is in
The opposite of fall is rise
The opposite of first is
2024-07-16 18:32:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 18:36:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1272,  0.6284,  0.3271,  ..., -0.8115,  1.3711,  0.5737],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4336, -2.6816,  0.0996,  ...,  3.6289,  5.9883,  0.9766],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0366, -0.0237, -0.0097,  ..., -0.0129,  0.0027, -0.0103],
        [ 0.0020, -0.0130,  0.0069,  ..., -0.0189,  0.0041,  0.0106],
        [-0.0066,  0.0167, -0.0278,  ..., -0.0038,  0.0137,  0.0027],
        ...,
        [ 0.0084,  0.0086, -0.0052,  ..., -0.0183,  0.0078,  0.0169],
        [-0.0162, -0.0105,  0.0325,  ..., -0.0034, -0.0171, -0.0153],
        [ 0.0073,  0.0026, -0.0039,  ..., -0.0072,  0.0154, -0.0036]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.8418, -2.5352,  0.7588,  ...,  2.2695,  6.3320,  1.4434]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 18:36:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of below is above
The opposite of anterior is posterior
The opposite of inbound is outbound
The opposite of inhale is exhale
The opposite of submerge is emerge
The opposite of out is in
The opposite of fall is rise
The opposite of first is
2024-07-16 18:36:12 root INFO     [order_1_approx] starting weight calculation for The opposite of anterior is posterior
The opposite of inhale is exhale
The opposite of below is above
The opposite of inbound is outbound
The opposite of first is last
The opposite of out is in
The opposite of submerge is emerge
The opposite of fall is
2024-07-16 18:36:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 18:39:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.5791,  0.3398, -0.4404,  ...,  0.0879, -0.6118, -0.0790],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.1543, -4.2109,  4.7852,  ..., -2.5918,  0.1448,  4.1367],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0185, -0.0150, -0.0222,  ...,  0.0133, -0.0068, -0.0062],
        [-0.0044, -0.0108, -0.0019,  ..., -0.0214, -0.0030, -0.0087],
        [ 0.0161, -0.0118, -0.0073,  ..., -0.0029, -0.0055,  0.0132],
        ...,
        [-0.0094, -0.0049,  0.0055,  ..., -0.0065,  0.0024,  0.0079],
        [ 0.0091, -0.0073, -0.0141,  ..., -0.0122, -0.0045, -0.0414],
        [ 0.0075, -0.0087,  0.0102,  ...,  0.0109, -0.0068, -0.0176]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.3477, -3.6426,  5.9062,  ..., -2.0156, -0.2883,  4.1953]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 18:39:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of anterior is posterior
The opposite of inhale is exhale
The opposite of below is above
The opposite of inbound is outbound
The opposite of first is last
The opposite of out is in
The opposite of submerge is emerge
The opposite of fall is
2024-07-16 18:39:53 root INFO     [order_1_approx] starting weight calculation for The opposite of anterior is posterior
The opposite of submerge is emerge
The opposite of inhale is exhale
The opposite of first is last
The opposite of out is in
The opposite of fall is rise
The opposite of inbound is outbound
The opposite of below is
2024-07-16 18:39:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 18:43:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7593, -1.3799, -0.8018,  ...,  0.4021,  1.4219,  0.0388],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.1562, -5.7812,  2.4609,  ..., -0.2954,  5.6094,  2.7227],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0341, -0.0105, -0.0018,  ...,  0.0095, -0.0143, -0.0136],
        [-0.0105, -0.0276,  0.0141,  ...,  0.0061,  0.0062,  0.0087],
        [ 0.0135,  0.0014, -0.0546,  ...,  0.0049,  0.0013,  0.0171],
        ...,
        [-0.0073, -0.0028,  0.0115,  ..., -0.0356,  0.0215,  0.0136],
        [-0.0052, -0.0058,  0.0425,  ..., -0.0417, -0.0294,  0.0040],
        [-0.0153, -0.0263, -0.0240,  ...,  0.0164, -0.0107, -0.0223]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.1055, -4.8242,  1.8242,  ..., -0.2686,  5.9023,  3.8516]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 18:43:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of anterior is posterior
The opposite of submerge is emerge
The opposite of inhale is exhale
The opposite of first is last
The opposite of out is in
The opposite of fall is rise
The opposite of inbound is outbound
The opposite of below is
2024-07-16 18:43:40 root INFO     total operator prediction time: 1807.6911919116974 seconds
2024-07-16 18:43:40 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - member
2024-07-16 18:43:40 root INFO     building operator meronyms - member
2024-07-16 18:43:40 root INFO     [order_1_approx] starting weight calculation for A listener is a member of a audience
A college is a member of a university
A photo is a member of a album
A letter is a member of a alphabet
A policeman is a member of a police
A crow is a member of a murder
A nomad is a member of a horde
A wolf is a member of a
2024-07-16 18:43:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 18:47:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8535, -1.3555, -0.4531,  ...,  0.8848, -0.6489,  0.1357],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1162, -1.2334, -3.2812,  ..., -0.0781,  0.1882,  1.9434],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.0880e-02, -1.4175e-02, -6.5384e-03,  ..., -5.3596e-03,
         -7.1220e-03,  1.0498e-02],
        [ 6.2561e-04,  1.4534e-03,  6.5727e-03,  ...,  8.7738e-03,
         -6.0692e-03, -9.8648e-03],
        [ 5.3558e-03,  6.0883e-03,  6.2828e-03,  ..., -4.5052e-03,
         -5.8937e-04,  3.5267e-03],
        ...,
        [-2.1381e-03, -7.9575e-03,  9.0599e-05,  ...,  2.5520e-03,
         -1.3781e-03,  1.1017e-02],
        [ 3.2711e-03, -1.1093e-02, -2.2202e-03,  ..., -3.5381e-04,
         -7.7820e-03, -3.1395e-03],
        [-1.4954e-03, -3.7842e-03,  6.4316e-03,  ..., -1.2527e-02,
          1.6317e-03,  4.3068e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2397, -1.3281, -3.0273,  ...,  0.1488,  0.1240,  1.6895]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 18:47:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A listener is a member of a audience
A college is a member of a university
A photo is a member of a album
A letter is a member of a alphabet
A policeman is a member of a police
A crow is a member of a murder
A nomad is a member of a horde
A wolf is a member of a
2024-07-16 18:47:27 root INFO     [order_1_approx] starting weight calculation for A letter is a member of a alphabet
A nomad is a member of a horde
A wolf is a member of a pack
A policeman is a member of a police
A photo is a member of a album
A crow is a member of a murder
A college is a member of a university
A listener is a member of a
2024-07-16 18:47:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 18:51:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3948, -0.2839,  0.8159,  ...,  1.1660,  0.8491,  0.3926],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.1699,  2.7051, -0.5996,  ..., -1.8711,  1.7227,  1.7266],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 8.1024e-03,  8.7738e-03, -2.6016e-03,  ...,  2.9526e-03,
         -6.2103e-03, -2.3689e-03],
        [ 1.1223e-02,  6.4278e-03,  1.9653e-02,  ..., -3.8929e-03,
         -8.8577e-03,  3.8605e-03],
        [-4.1809e-03,  5.7526e-03,  1.6346e-03,  ...,  1.0391e-02,
          7.3528e-04,  1.7643e-05],
        ...,
        [-7.1182e-03,  1.8845e-03, -2.5024e-03,  ..., -3.6049e-03,
         -1.5656e-02,  2.5940e-03],
        [ 8.3351e-04, -1.5022e-02, -7.7286e-03,  ..., -1.5182e-03,
         -1.4687e-03, -1.3611e-02],
        [-2.6550e-03,  1.1803e-02, -3.6926e-03,  ..., -9.2163e-03,
          1.8204e-02,  1.1353e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3340,  3.5410, -1.0234,  ..., -1.3057,  1.1074,  1.6406]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 18:51:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A letter is a member of a alphabet
A nomad is a member of a horde
A wolf is a member of a pack
A policeman is a member of a police
A photo is a member of a album
A crow is a member of a murder
A college is a member of a university
A listener is a member of a
2024-07-16 18:51:14 root INFO     [order_1_approx] starting weight calculation for A college is a member of a university
A crow is a member of a murder
A letter is a member of a alphabet
A listener is a member of a audience
A nomad is a member of a horde
A photo is a member of a album
A wolf is a member of a pack
A policeman is a member of a
2024-07-16 18:51:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 18:55:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0627, -0.6738, -0.8462,  ..., -0.4622,  0.2759,  0.4473],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0625, -2.2461, -0.7305,  ...,  1.4785, -0.3281,  4.7070],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.0014e-03, -1.2886e-02, -3.2101e-03,  ..., -2.8133e-03,
         -4.9896e-03, -5.2452e-06],
        [-1.0452e-02, -1.0628e-02,  1.4252e-02,  ...,  9.9182e-03,
          4.5547e-03, -9.5367e-03],
        [-1.1314e-02,  1.9135e-02,  4.1914e-04,  ...,  2.9240e-03,
          4.2191e-03,  8.6288e-03],
        ...,
        [-1.5961e-02, -5.0011e-03,  1.7109e-03,  ...,  1.1955e-02,
         -3.9172e-04,  5.7526e-03],
        [-3.5095e-04, -8.5983e-03,  1.1986e-02,  ...,  7.1001e-04,
         -1.1063e-02, -7.1678e-03],
        [ 3.6736e-03, -4.3945e-03,  2.0695e-03,  ..., -4.9667e-03,
          5.0430e-03,  5.1689e-04]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7012, -2.0918, -0.9844,  ...,  1.1191, -0.2493,  4.5273]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 18:55:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A college is a member of a university
A crow is a member of a murder
A letter is a member of a alphabet
A listener is a member of a audience
A nomad is a member of a horde
A photo is a member of a album
A wolf is a member of a pack
A policeman is a member of a
2024-07-16 18:55:01 root INFO     [order_1_approx] starting weight calculation for A listener is a member of a audience
A college is a member of a university
A letter is a member of a alphabet
A crow is a member of a murder
A wolf is a member of a pack
A photo is a member of a album
A policeman is a member of a police
A nomad is a member of a
2024-07-16 18:55:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 18:58:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0918, -1.6836,  0.0117,  ..., -0.3647,  0.4358, -0.3691],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.0195,  0.0359, -0.7817,  ..., -0.3516,  1.0225,  1.3857],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0172, -0.0112,  0.0070,  ..., -0.0039, -0.0118, -0.0095],
        [ 0.0052, -0.0040,  0.0047,  ...,  0.0109,  0.0054,  0.0020],
        [ 0.0042, -0.0050,  0.0070,  ..., -0.0008,  0.0027,  0.0014],
        ...,
        [-0.0105,  0.0014, -0.0028,  ...,  0.0029, -0.0125,  0.0102],
        [ 0.0044, -0.0094,  0.0028,  ...,  0.0005, -0.0038, -0.0108],
        [ 0.0045, -0.0019, -0.0015,  ..., -0.0085,  0.0059,  0.0147]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.1211e+00,  4.7656e-01, -1.0977e+00,  ...,  2.9297e-03,
          8.7500e-01,  1.3516e+00]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SubBackward0>) 

                    
2024-07-16 18:58:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A listener is a member of a audience
A college is a member of a university
A letter is a member of a alphabet
A crow is a member of a murder
A wolf is a member of a pack
A photo is a member of a album
A policeman is a member of a police
A nomad is a member of a
2024-07-16 18:58:46 root INFO     [order_1_approx] starting weight calculation for A wolf is a member of a pack
A nomad is a member of a horde
A letter is a member of a alphabet
A crow is a member of a murder
A listener is a member of a audience
A policeman is a member of a police
A college is a member of a university
A photo is a member of a
2024-07-16 18:58:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 19:02:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5308,  1.0312,  0.7773,  ..., -0.5962,  0.3589, -0.2686],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6641, -1.7988, -0.7754,  ..., -0.4287, -3.9902,  0.6211],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0241, -0.0010,  0.0052,  ..., -0.0116,  0.0041,  0.0006],
        [-0.0014,  0.0048,  0.0095,  ..., -0.0026, -0.0104,  0.0098],
        [ 0.0110,  0.0068, -0.0037,  ..., -0.0088, -0.0003,  0.0071],
        ...,
        [-0.0176, -0.0088, -0.0050,  ...,  0.0035, -0.0105,  0.0059],
        [ 0.0118, -0.0011, -0.0051,  ..., -0.0024,  0.0057, -0.0106],
        [-0.0029,  0.0021, -0.0072,  ..., -0.0114,  0.0225,  0.0146]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6230, -1.2002, -1.0586,  ...,  0.1382, -4.4258,  0.7402]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 19:02:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A wolf is a member of a pack
A nomad is a member of a horde
A letter is a member of a alphabet
A crow is a member of a murder
A listener is a member of a audience
A policeman is a member of a police
A college is a member of a university
A photo is a member of a
2024-07-16 19:02:32 root INFO     [order_1_approx] starting weight calculation for A policeman is a member of a police
A photo is a member of a album
A nomad is a member of a horde
A listener is a member of a audience
A college is a member of a university
A wolf is a member of a pack
A crow is a member of a murder
A letter is a member of a
2024-07-16 19:02:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 19:06:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8501,  0.3257,  1.2002,  ...,  0.0833, -0.2820,  0.3276],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4346,  1.0508,  0.6523,  ..., -2.0625,  4.5977,  3.1406],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0146, -0.0115, -0.0065,  ...,  0.0100,  0.0009,  0.0073],
        [-0.0050, -0.0007, -0.0072,  ..., -0.0013, -0.0051,  0.0047],
        [ 0.0061,  0.0018,  0.0039,  ...,  0.0005, -0.0072,  0.0039],
        ...,
        [ 0.0021, -0.0038, -0.0027,  ...,  0.0030, -0.0044, -0.0010],
        [-0.0026, -0.0155,  0.0062,  ...,  0.0022, -0.0049, -0.0151],
        [-0.0034, -0.0127, -0.0008,  ..., -0.0085,  0.0155,  0.0029]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2554,  1.4688,  0.2380,  ..., -2.1387,  4.7500,  2.3965]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 19:06:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A policeman is a member of a police
A photo is a member of a album
A nomad is a member of a horde
A listener is a member of a audience
A college is a member of a university
A wolf is a member of a pack
A crow is a member of a murder
A letter is a member of a
2024-07-16 19:06:18 root INFO     [order_1_approx] starting weight calculation for A letter is a member of a alphabet
A wolf is a member of a pack
A listener is a member of a audience
A policeman is a member of a police
A crow is a member of a murder
A nomad is a member of a horde
A photo is a member of a album
A college is a member of a
2024-07-16 19:06:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 19:10:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.1331, 0.1853, 0.3340,  ..., 0.7222, 1.6182, 0.1990], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5884, -0.1626, -0.4199,  ..., -0.0137,  3.0312,  1.6934],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0097, -0.0086,  0.0042,  ...,  0.0002,  0.0074, -0.0028],
        [-0.0078, -0.0005,  0.0075,  ...,  0.0031, -0.0008, -0.0061],
        [ 0.0058,  0.0066,  0.0067,  ...,  0.0023, -0.0056,  0.0004],
        ...,
        [-0.0087, -0.0025, -0.0038,  ...,  0.0117, -0.0116,  0.0087],
        [ 0.0042, -0.0112,  0.0116,  ...,  0.0054,  0.0011, -0.0206],
        [-0.0170, -0.0037, -0.0010,  ..., -0.0109,  0.0135,  0.0125]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2805,  0.1748, -0.7109,  ..., -0.2598,  2.9688,  2.3242]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 19:10:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A letter is a member of a alphabet
A wolf is a member of a pack
A listener is a member of a audience
A policeman is a member of a police
A crow is a member of a murder
A nomad is a member of a horde
A photo is a member of a album
A college is a member of a
2024-07-16 19:10:04 root INFO     [order_1_approx] starting weight calculation for A photo is a member of a album
A policeman is a member of a police
A listener is a member of a audience
A wolf is a member of a pack
A nomad is a member of a horde
A letter is a member of a alphabet
A college is a member of a university
A crow is a member of a
2024-07-16 19:10:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 19:13:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.7998, -1.5850,  0.3171,  ...,  0.7886, -0.5200,  0.1284],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6123,  1.1348, -0.7134,  ...,  3.5059,  0.4229,  1.6445],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0099, -0.0133,  0.0001,  ..., -0.0012, -0.0059, -0.0016],
        [-0.0064,  0.0165,  0.0148,  ...,  0.0077, -0.0068,  0.0176],
        [ 0.0036, -0.0026,  0.0023,  ...,  0.0029,  0.0059, -0.0014],
        ...,
        [-0.0152,  0.0059, -0.0016,  ..., -0.0020, -0.0013,  0.0034],
        [-0.0044, -0.0102, -0.0104,  ...,  0.0047,  0.0079, -0.0025],
        [-0.0074, -0.0199, -0.0005,  ..., -0.0105, -0.0017,  0.0085]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7095,  0.9346, -1.3574,  ...,  3.8438,  0.0405,  2.0566]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 19:13:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A photo is a member of a album
A policeman is a member of a police
A listener is a member of a audience
A wolf is a member of a pack
A nomad is a member of a horde
A letter is a member of a alphabet
A college is a member of a university
A crow is a member of a
2024-07-16 19:13:50 root INFO     total operator prediction time: 1809.8452122211456 seconds
2024-07-16 19:13:50 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on noun - plural_irreg
2024-07-16 19:13:50 root INFO     building operator noun - plural_irreg
2024-07-16 19:13:50 root INFO     [order_1_approx] starting weight calculation for The plural form of success is successes
The plural form of business is businesses
The plural form of theory is theories
The plural form of authority is authorities
The plural form of activity is activities
The plural form of safety is safeties
The plural form of energy is energies
The plural form of basis is
2024-07-16 19:13:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 19:17:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4390, -0.0786,  0.4517,  ..., -0.5283,  0.6963,  0.3245],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3865,  2.3262, -6.3203,  ...,  4.4727,  3.2871,  7.2852],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0044, -0.0153,  0.0303,  ..., -0.0089,  0.0056, -0.0215],
        [-0.0104, -0.0339, -0.0045,  ...,  0.0161, -0.0112,  0.0089],
        [-0.0079,  0.0377, -0.0246,  ..., -0.0056,  0.0067,  0.0016],
        ...,
        [-0.0111, -0.0230, -0.0043,  ..., -0.0188, -0.0084, -0.0112],
        [ 0.0068, -0.0130,  0.0013,  ..., -0.0168, -0.0297, -0.0032],
        [ 0.0007,  0.0239,  0.0016,  ..., -0.0200,  0.0175, -0.0199]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0828,  2.4863, -6.1328,  ...,  3.4316,  3.5312,  7.6992]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 19:17:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of success is successes
The plural form of business is businesses
The plural form of theory is theories
The plural form of authority is authorities
The plural form of activity is activities
The plural form of safety is safeties
The plural form of energy is energies
The plural form of basis is
2024-07-16 19:17:36 root INFO     [order_1_approx] starting weight calculation for The plural form of energy is energies
The plural form of activity is activities
The plural form of safety is safeties
The plural form of theory is theories
The plural form of success is successes
The plural form of basis is bases
The plural form of business is businesses
The plural form of authority is
2024-07-16 19:17:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 19:21:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3018, -0.7495,  0.6587,  ..., -0.2539,  0.1628,  0.9136],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.4023, -0.0059,  0.1748,  ..., -2.2344, -0.5303, -0.7070],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-3.9291e-03, -3.1471e-03,  1.8387e-02,  ..., -6.3782e-03,
          8.4076e-03, -5.5542e-03],
        [ 8.6212e-04,  1.2550e-03, -7.8392e-04,  ...,  9.4757e-03,
         -8.1329e-03, -1.0750e-02],
        [ 1.6312e-02,  5.2338e-03, -1.4366e-02,  ...,  2.5425e-03,
         -2.4643e-03, -2.4796e-05],
        ...,
        [-1.2894e-02, -2.3621e-02,  2.5539e-03,  ..., -1.2177e-02,
          1.5182e-02,  9.1553e-05],
        [ 7.1945e-03, -2.6703e-03,  7.8964e-03,  ..., -2.1057e-02,
         -1.0483e-02,  5.4703e-03],
        [-7.0648e-03,  3.2120e-03, -2.4872e-03,  ..., -1.3634e-02,
          1.4008e-02, -3.5210e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.2637,  0.6313, -0.1604,  ..., -1.9492, -0.1018, -1.5010]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 19:21:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of energy is energies
The plural form of activity is activities
The plural form of safety is safeties
The plural form of theory is theories
The plural form of success is successes
The plural form of basis is bases
The plural form of business is businesses
The plural form of authority is
2024-07-16 19:21:21 root INFO     [order_1_approx] starting weight calculation for The plural form of business is businesses
The plural form of activity is activities
The plural form of basis is bases
The plural form of energy is energies
The plural form of authority is authorities
The plural form of success is successes
The plural form of theory is theories
The plural form of safety is
2024-07-16 19:21:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 19:25:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0297,  1.2012,  0.0254,  ...,  0.4314,  0.1871, -0.0903],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.4648,  6.2852,  4.9492,  ..., -1.9355, -1.8066,  2.6523],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0296, -0.0093,  0.0153,  ..., -0.0179, -0.0024, -0.0135],
        [ 0.0210, -0.0060, -0.0183,  ..., -0.0020, -0.0254,  0.0127],
        [ 0.0076, -0.0071, -0.0075,  ..., -0.0081,  0.0125,  0.0256],
        ...,
        [-0.0150, -0.0130, -0.0108,  ..., -0.0102,  0.0052, -0.0241],
        [-0.0020, -0.0207,  0.0069,  ..., -0.0170, -0.0203, -0.0027],
        [-0.0132,  0.0256,  0.0077,  ..., -0.0057,  0.0104, -0.0208]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.7070,  6.0938,  5.7266,  ..., -2.2012, -2.4160,  2.6738]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 19:25:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of business is businesses
The plural form of activity is activities
The plural form of basis is bases
The plural form of energy is energies
The plural form of authority is authorities
The plural form of success is successes
The plural form of theory is theories
The plural form of safety is
2024-07-16 19:25:07 root INFO     [order_1_approx] starting weight calculation for The plural form of theory is theories
The plural form of safety is safeties
The plural form of basis is bases
The plural form of activity is activities
The plural form of energy is energies
The plural form of authority is authorities
The plural form of business is businesses
The plural form of success is
2024-07-16 19:25:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 19:28:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3018,  0.8179,  0.1946,  ...,  0.7725,  0.2042, -0.0281],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-8.0156,  0.8838,  4.6641,  ..., -1.2822, -1.4629,  3.4688],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0076, -0.0094,  0.0199,  ...,  0.0024,  0.0046, -0.0286],
        [-0.0081, -0.0162, -0.0150,  ...,  0.0075, -0.0012,  0.0081],
        [ 0.0015, -0.0161, -0.0134,  ..., -0.0189,  0.0225,  0.0085],
        ...,
        [-0.0061, -0.0092, -0.0009,  ..., -0.0213, -0.0172, -0.0083],
        [-0.0126, -0.0098, -0.0013,  ..., -0.0015, -0.0117,  0.0181],
        [ 0.0052,  0.0185,  0.0122,  ...,  0.0063,  0.0066, -0.0145]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-8.1172,  1.3564,  5.6484,  ..., -1.7949, -1.4053,  3.3887]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 19:28:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of theory is theories
The plural form of safety is safeties
The plural form of basis is bases
The plural form of activity is activities
The plural form of energy is energies
The plural form of authority is authorities
The plural form of business is businesses
The plural form of success is
2024-07-16 19:28:53 root INFO     [order_1_approx] starting weight calculation for The plural form of authority is authorities
The plural form of success is successes
The plural form of energy is energies
The plural form of theory is theories
The plural form of safety is safeties
The plural form of activity is activities
The plural form of basis is bases
The plural form of business is
2024-07-16 19:28:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 19:32:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([1.2197, 0.4167, 0.7632,  ..., 0.4302, 0.2781, 0.3193], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.4375, -0.3123, -1.9561,  ..., -0.0068,  1.9834,  1.4404],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0064, -0.0049,  0.0184,  ..., -0.0154,  0.0220, -0.0202],
        [-0.0095, -0.0134,  0.0039,  ...,  0.0031, -0.0051,  0.0061],
        [ 0.0065,  0.0066, -0.0179,  ..., -0.0133,  0.0326,  0.0110],
        ...,
        [-0.0147, -0.0180, -0.0055,  ...,  0.0010,  0.0021, -0.0236],
        [-0.0056, -0.0163,  0.0092,  ..., -0.0062, -0.0185,  0.0073],
        [-0.0026,  0.0034, -0.0161,  ...,  0.0060,  0.0213, -0.0116]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.0781, -0.4895, -1.4092,  ..., -0.6709,  2.0000,  1.0215]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 19:32:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of authority is authorities
The plural form of success is successes
The plural form of energy is energies
The plural form of theory is theories
The plural form of safety is safeties
The plural form of activity is activities
The plural form of basis is bases
The plural form of business is
2024-07-16 19:32:39 root INFO     [order_1_approx] starting weight calculation for The plural form of theory is theories
The plural form of success is successes
The plural form of safety is safeties
The plural form of basis is bases
The plural form of energy is energies
The plural form of authority is authorities
The plural form of business is businesses
The plural form of activity is
2024-07-16 19:32:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 19:36:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2490,  0.4993,  0.0950,  ...,  0.8916, -0.0684,  0.0289],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.3672,  2.2031,  1.8867,  ..., -1.6670,  4.6719,  2.4219],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0010, -0.0032,  0.0166,  ..., -0.0074,  0.0011, -0.0159],
        [-0.0142, -0.0078, -0.0061,  ...,  0.0024, -0.0046,  0.0104],
        [ 0.0125,  0.0091, -0.0224,  ..., -0.0005,  0.0223, -0.0053],
        ...,
        [-0.0005, -0.0053, -0.0014,  ..., -0.0047, -0.0058, -0.0112],
        [-0.0062, -0.0072,  0.0090,  ..., -0.0191, -0.0248,  0.0178],
        [-0.0119,  0.0219, -0.0121,  ..., -0.0061,  0.0241, -0.0173]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.2461,  2.6543,  1.3760,  ..., -2.2656,  5.0938,  2.6895]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 19:36:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of theory is theories
The plural form of success is successes
The plural form of safety is safeties
The plural form of basis is bases
The plural form of energy is energies
The plural form of authority is authorities
The plural form of business is businesses
The plural form of activity is
2024-07-16 19:36:26 root INFO     [order_1_approx] starting weight calculation for The plural form of safety is safeties
The plural form of business is businesses
The plural form of basis is bases
The plural form of authority is authorities
The plural form of activity is activities
The plural form of success is successes
The plural form of theory is theories
The plural form of energy is
2024-07-16 19:36:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 19:40:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.1074,  1.1172, -0.2563,  ...,  1.1387,  0.8799,  0.3792],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-5.1836e+00,  2.4414e-03,  1.1182e+00,  ..., -4.4531e-01,
        -3.1094e+00,  5.2930e+00], device='cuda:0', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0013, -0.0396,  0.0146,  ..., -0.0099,  0.0048,  0.0044],
        [-0.0089, -0.0246, -0.0056,  ...,  0.0198, -0.0190,  0.0053],
        [-0.0013,  0.0162, -0.0115,  ..., -0.0086,  0.0269,  0.0073],
        ...,
        [-0.0036, -0.0053, -0.0157,  ..., -0.0170, -0.0003, -0.0097],
        [ 0.0023, -0.0232,  0.0014,  ..., -0.0114, -0.0221,  0.0124],
        [-0.0221,  0.0113, -0.0057,  ..., -0.0114,  0.0085, -0.0148]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.8242,  0.0944,  1.7051,  ..., -0.7197, -3.5195,  6.0352]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 19:40:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of safety is safeties
The plural form of business is businesses
The plural form of basis is bases
The plural form of authority is authorities
The plural form of activity is activities
The plural form of success is successes
The plural form of theory is theories
The plural form of energy is
2024-07-16 19:40:12 root INFO     [order_1_approx] starting weight calculation for The plural form of activity is activities
The plural form of energy is energies
The plural form of basis is bases
The plural form of business is businesses
The plural form of success is successes
The plural form of safety is safeties
The plural form of authority is authorities
The plural form of theory is
2024-07-16 19:40:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 19:43:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.6914, -0.1997,  0.0444,  ...,  0.7949,  0.4663,  0.5083],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.8809, -3.1328, -1.3057,  ...,  2.9492, -3.7812, -0.9873],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0074, -0.0098,  0.0081,  ...,  0.0029,  0.0085, -0.0253],
        [-0.0106, -0.0021, -0.0071,  ...,  0.0084, -0.0142,  0.0029],
        [ 0.0216,  0.0117, -0.0266,  ..., -0.0095,  0.0125,  0.0098],
        ...,
        [-0.0159, -0.0168, -0.0067,  ..., -0.0185,  0.0024, -0.0068],
        [-0.0051, -0.0083,  0.0099,  ..., -0.0125, -0.0411,  0.0068],
        [ 0.0037,  0.0358,  0.0037,  ..., -0.0063,  0.0189, -0.0153]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.3164, -2.4219, -1.7793,  ...,  2.1641, -3.3145, -0.8584]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 19:43:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of activity is activities
The plural form of energy is energies
The plural form of basis is bases
The plural form of business is businesses
The plural form of success is successes
The plural form of safety is safeties
The plural form of authority is authorities
The plural form of theory is
2024-07-16 19:43:59 root INFO     total operator prediction time: 1808.8783411979675 seconds
2024-07-16 19:43:59 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on Ving - verb_inf
2024-07-16 19:43:59 root INFO     building operator Ving - verb_inf
2024-07-16 19:43:59 root INFO     [order_1_approx] starting weight calculation for representing is the active form of represent
protecting is the active form of protect
establishing is the active form of establish
maintaining is the active form of maintain
reducing is the active form of reduce
achieving is the active form of achieve
considering is the active form of consider
providing is the active form of
2024-07-16 19:43:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 19:47:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0615, -0.9326,  1.6104,  ...,  0.4316,  1.6748,  0.3916],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.1914,  0.3462, -0.3594,  ..., -2.3906, -2.9043, -2.2305],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0086, -0.0088,  0.0088,  ..., -0.0157, -0.0019, -0.0105],
        [ 0.0097,  0.0004, -0.0044,  ..., -0.0077, -0.0011, -0.0055],
        [ 0.0157,  0.0098, -0.0107,  ..., -0.0021,  0.0078, -0.0106],
        ...,
        [-0.0122, -0.0123,  0.0047,  ..., -0.0081, -0.0193,  0.0023],
        [-0.0019,  0.0054,  0.0189,  ..., -0.0154, -0.0184,  0.0142],
        [-0.0083,  0.0228,  0.0074,  ..., -0.0055,  0.0211, -0.0179]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.8633, -0.0208, -1.1719,  ..., -1.5801, -3.3477, -2.6172]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 19:47:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for representing is the active form of represent
protecting is the active form of protect
establishing is the active form of establish
maintaining is the active form of maintain
reducing is the active form of reduce
achieving is the active form of achieve
considering is the active form of consider
providing is the active form of
2024-07-16 19:47:45 root INFO     [order_1_approx] starting weight calculation for establishing is the active form of establish
maintaining is the active form of maintain
achieving is the active form of achieve
providing is the active form of provide
considering is the active form of consider
representing is the active form of represent
protecting is the active form of protect
reducing is the active form of
2024-07-16 19:47:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 19:51:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2625, -0.6562,  1.2422,  ..., -0.8623,  1.2441, -0.3330],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.5273, -1.2695,  0.4287,  ..., -2.4531,  1.1045, -0.3667],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.9945e-03, -1.6541e-02,  1.2184e-02,  ..., -6.9847e-03,
          3.2043e-03, -4.4975e-03],
        [-9.6054e-03, -7.6637e-03, -2.5864e-03,  ...,  1.1322e-02,
         -1.0376e-02, -1.5396e-02],
        [ 1.5221e-03,  1.3290e-02, -1.3451e-02,  ...,  3.5629e-03,
         -2.0084e-03, -2.5421e-02],
        ...,
        [-2.0355e-02, -1.4580e-02,  1.4297e-02,  ..., -1.9897e-02,
          8.3923e-05,  1.3695e-03],
        [-9.3460e-03, -5.8823e-03, -8.0795e-03,  ..., -9.4299e-03,
         -2.1317e-02,  1.7944e-02],
        [ 1.7548e-03,  4.1809e-03,  6.1569e-03,  ..., -1.1269e-02,
         -4.7607e-03, -2.6627e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.6016, -0.6587,  0.6665,  ..., -2.3496,  1.1816, -0.7759]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 19:51:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for establishing is the active form of establish
maintaining is the active form of maintain
achieving is the active form of achieve
providing is the active form of provide
considering is the active form of consider
representing is the active form of represent
protecting is the active form of protect
reducing is the active form of
2024-07-16 19:51:32 root INFO     [order_1_approx] starting weight calculation for protecting is the active form of protect
representing is the active form of represent
reducing is the active form of reduce
providing is the active form of provide
establishing is the active form of establish
achieving is the active form of achieve
considering is the active form of consider
maintaining is the active form of
2024-07-16 19:51:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 19:55:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2773, -0.6748,  1.5137,  ...,  0.2407,  1.0664,  0.8672],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.0117, -3.4062, -6.2148,  ...,  0.3489,  0.6245,  3.5430],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0137,  0.0034,  0.0049,  ..., -0.0081,  0.0144, -0.0188],
        [ 0.0006,  0.0120,  0.0081,  ..., -0.0099,  0.0049,  0.0116],
        [ 0.0010,  0.0237,  0.0220,  ..., -0.0065,  0.0038, -0.0040],
        ...,
        [-0.0230, -0.0201, -0.0113,  ..., -0.0162, -0.0029, -0.0046],
        [-0.0013,  0.0207,  0.0040,  ..., -0.0121, -0.0280,  0.0091],
        [ 0.0031,  0.0160,  0.0097,  ..., -0.0168,  0.0179, -0.0331]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.4414, -3.9238, -8.5156,  ...,  0.9355,  0.9141,  4.4258]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 19:55:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for protecting is the active form of protect
representing is the active form of represent
reducing is the active form of reduce
providing is the active form of provide
establishing is the active form of establish
achieving is the active form of achieve
considering is the active form of consider
maintaining is the active form of
2024-07-16 19:55:17 root INFO     [order_1_approx] starting weight calculation for achieving is the active form of achieve
reducing is the active form of reduce
representing is the active form of represent
protecting is the active form of protect
maintaining is the active form of maintain
considering is the active form of consider
providing is the active form of provide
establishing is the active form of
2024-07-16 19:55:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 19:59:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1759, -0.9390,  1.5000,  ...,  0.8540,  1.3584, -0.0806],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.5664,  0.6777,  1.6816,  ..., -1.3105, -1.6035,  1.3369],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0173, -0.0060,  0.0118,  ..., -0.0229, -0.0087, -0.0090],
        [-0.0012, -0.0081,  0.0032,  ...,  0.0066,  0.0038, -0.0100],
        [-0.0037,  0.0076, -0.0097,  ...,  0.0025,  0.0124, -0.0092],
        ...,
        [-0.0041, -0.0147, -0.0067,  ..., -0.0307, -0.0007,  0.0130],
        [-0.0144, -0.0008,  0.0030,  ..., -0.0140, -0.0292,  0.0114],
        [-0.0040,  0.0196, -0.0028,  ..., -0.0121, -0.0090, -0.0072]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.3652,  0.9561,  1.1699,  ..., -1.3203, -1.7051,  1.1846]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 19:59:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for achieving is the active form of achieve
reducing is the active form of reduce
representing is the active form of represent
protecting is the active form of protect
maintaining is the active form of maintain
considering is the active form of consider
providing is the active form of provide
establishing is the active form of
2024-07-16 19:59:03 root INFO     [order_1_approx] starting weight calculation for representing is the active form of represent
protecting is the active form of protect
establishing is the active form of establish
maintaining is the active form of maintain
achieving is the active form of achieve
reducing is the active form of reduce
providing is the active form of provide
considering is the active form of
2024-07-16 19:59:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 20:02:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5479, -0.7480,  1.4893,  ...,  0.3909,  2.3340,  0.3669],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.9531,  1.5723, -0.0645,  ..., -2.1270, -0.8901, -3.5391],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0090, -0.0034,  0.0031,  ..., -0.0037,  0.0039, -0.0085],
        [-0.0062,  0.0008,  0.0058,  ..., -0.0138, -0.0022, -0.0019],
        [ 0.0088,  0.0122, -0.0058,  ...,  0.0026,  0.0053, -0.0125],
        ...,
        [-0.0270, -0.0308,  0.0074,  ..., -0.0217, -0.0032,  0.0009],
        [-0.0008,  0.0147,  0.0037,  ..., -0.0005, -0.0300,  0.0119],
        [-0.0017,  0.0356,  0.0153,  ..., -0.0072,  0.0004, -0.0218]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.2188,  1.7939,  0.2805,  ..., -1.6387, -0.4663, -3.4648]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 20:02:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for representing is the active form of represent
protecting is the active form of protect
establishing is the active form of establish
maintaining is the active form of maintain
achieving is the active form of achieve
reducing is the active form of reduce
providing is the active form of provide
considering is the active form of
2024-07-16 20:02:49 root INFO     [order_1_approx] starting weight calculation for considering is the active form of consider
establishing is the active form of establish
protecting is the active form of protect
achieving is the active form of achieve
maintaining is the active form of maintain
reducing is the active form of reduce
providing is the active form of provide
representing is the active form of
2024-07-16 20:02:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 20:06:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5557, -1.4014,  1.3184,  ..., -0.1169,  2.1523, -0.1843],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8574,  0.8252, -1.1328,  ...,  0.2920,  1.0352,  1.4365],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0050, -0.0052,  0.0023,  ...,  0.0018, -0.0049, -0.0037],
        [-0.0012, -0.0133,  0.0005,  ..., -0.0105,  0.0004, -0.0039],
        [ 0.0137,  0.0098, -0.0134,  ...,  0.0077,  0.0011, -0.0089],
        ...,
        [-0.0121, -0.0033,  0.0086,  ..., -0.0226,  0.0034,  0.0158],
        [ 0.0041,  0.0061,  0.0063,  ..., -0.0163, -0.0265,  0.0060],
        [ 0.0035,  0.0145,  0.0096,  ..., -0.0125, -0.0030, -0.0205]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4473,  1.4434, -1.4346,  ...,  0.9844,  1.0713,  1.4854]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 20:06:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for considering is the active form of consider
establishing is the active form of establish
protecting is the active form of protect
achieving is the active form of achieve
maintaining is the active form of maintain
reducing is the active form of reduce
providing is the active form of provide
representing is the active form of
2024-07-16 20:06:35 root INFO     [order_1_approx] starting weight calculation for representing is the active form of represent
maintaining is the active form of maintain
achieving is the active form of achieve
establishing is the active form of establish
reducing is the active form of reduce
providing is the active form of provide
considering is the active form of consider
protecting is the active form of
2024-07-16 20:06:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 20:10:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8975, -0.3826,  1.7070,  ...,  1.0068,  0.6133,  0.4292],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3223,  1.1699,  0.3164,  ..., -2.2793, -1.6221,  3.4570],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0082, -0.0217,  0.0096,  ..., -0.0195, -0.0148, -0.0096],
        [ 0.0140, -0.0101, -0.0043,  ...,  0.0020, -0.0038, -0.0034],
        [ 0.0061,  0.0088, -0.0157,  ..., -0.0142,  0.0006, -0.0064],
        ...,
        [-0.0227, -0.0222, -0.0023,  ..., -0.0183,  0.0032, -0.0080],
        [-0.0076,  0.0124, -0.0002,  ..., -0.0093, -0.0282,  0.0209],
        [-0.0101,  0.0212,  0.0079,  ..., -0.0262,  0.0227, -0.0289]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9229,  0.9678,  0.4141,  ..., -1.9424, -2.3281,  4.5586]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 20:10:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for representing is the active form of represent
maintaining is the active form of maintain
achieving is the active form of achieve
establishing is the active form of establish
reducing is the active form of reduce
providing is the active form of provide
considering is the active form of consider
protecting is the active form of
2024-07-16 20:10:21 root INFO     [order_1_approx] starting weight calculation for maintaining is the active form of maintain
reducing is the active form of reduce
considering is the active form of consider
providing is the active form of provide
protecting is the active form of protect
representing is the active form of represent
establishing is the active form of establish
achieving is the active form of
2024-07-16 20:10:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 20:14:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6172, -0.1598,  2.9531,  ..., -0.7383,  1.7305, -0.4775],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.3848,  0.7690,  0.9951,  ..., -4.0625, -0.2393, -3.1680],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0156, -0.0087,  0.0202,  ...,  0.0035, -0.0045, -0.0042],
        [-0.0058, -0.0138, -0.0103,  ..., -0.0017,  0.0053, -0.0106],
        [-0.0155, -0.0044, -0.0139,  ...,  0.0074,  0.0161, -0.0228],
        ...,
        [-0.0151, -0.0038, -0.0147,  ..., -0.0279, -0.0058, -0.0164],
        [-0.0033,  0.0219,  0.0253,  ..., -0.0079, -0.0204,  0.0287],
        [ 0.0040,  0.0213,  0.0078,  ..., -0.0234,  0.0044, -0.0164]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1973,  0.9453,  1.1250,  ..., -4.0859,  0.5059, -2.7871]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 20:14:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for maintaining is the active form of maintain
reducing is the active form of reduce
considering is the active form of consider
providing is the active form of provide
protecting is the active form of protect
representing is the active form of represent
establishing is the active form of establish
achieving is the active form of
2024-07-16 20:14:07 root INFO     total operator prediction time: 1808.1641488075256 seconds
2024-07-16 20:14:07 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_Ving - Ved
2024-07-16 20:14:07 root INFO     building operator verb_Ving - Ved
2024-07-16 20:14:07 root INFO     [order_1_approx] starting weight calculation for After something is representing, it has represented
After something is marrying, it has married
After something is understanding, it has understood
After something is existing, it has existed
After something is providing, it has provided
After something is improving, it has improved
After something is introducing, it has introduced
After something is adding, it has
2024-07-16 20:14:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 20:17:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0483, -0.0476,  1.5303,  ...,  0.3848,  0.4399, -0.0930],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7998,  1.9785,  5.2734,  ..., -0.6987, -1.0811,  3.0898],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0213,  0.0029,  0.0141,  ..., -0.0100,  0.0163,  0.0015],
        [ 0.0082, -0.0131,  0.0076,  ..., -0.0079, -0.0123,  0.0040],
        [-0.0008, -0.0103, -0.0278,  ...,  0.0044, -0.0072, -0.0024],
        ...,
        [-0.0328, -0.0054,  0.0063,  ..., -0.0148,  0.0292,  0.0142],
        [-0.0003, -0.0087,  0.0150,  ...,  0.0007, -0.0416,  0.0012],
        [-0.0020,  0.0282, -0.0161,  ..., -0.0206, -0.0070, -0.0391]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5449,  2.2012,  4.6758,  ..., -0.2935, -1.6572,  3.1602]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 20:17:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is representing, it has represented
After something is marrying, it has married
After something is understanding, it has understood
After something is existing, it has existed
After something is providing, it has provided
After something is improving, it has improved
After something is introducing, it has introduced
After something is adding, it has
2024-07-16 20:17:53 root INFO     [order_1_approx] starting weight calculation for After something is understanding, it has understood
After something is providing, it has provided
After something is adding, it has added
After something is representing, it has represented
After something is existing, it has existed
After something is improving, it has improved
After something is introducing, it has introduced
After something is marrying, it has
2024-07-16 20:17:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 20:21:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2424, -0.4595,  1.2578,  ...,  0.3430,  0.2759, -0.0317],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2115, -0.2401, -0.4912,  ..., -2.1523,  2.2617,  1.4590],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0145, -0.0024,  0.0018,  ...,  0.0002, -0.0122, -0.0138],
        [ 0.0024, -0.0130, -0.0004,  ..., -0.0075,  0.0136, -0.0034],
        [-0.0044, -0.0024, -0.0056,  ...,  0.0007,  0.0045, -0.0054],
        ...,
        [-0.0150, -0.0154, -0.0163,  ..., -0.0327, -0.0025,  0.0008],
        [ 0.0044, -0.0053, -0.0004,  ...,  0.0083, -0.0286,  0.0194],
        [-0.0095,  0.0065, -0.0075,  ..., -0.0206,  0.0222, -0.0254]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4487,  0.7446, -0.7974,  ..., -2.4648,  1.9365,  1.3936]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 20:21:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is understanding, it has understood
After something is providing, it has provided
After something is adding, it has added
After something is representing, it has represented
After something is existing, it has existed
After something is improving, it has improved
After something is introducing, it has introduced
After something is marrying, it has
2024-07-16 20:21:40 root INFO     [order_1_approx] starting weight calculation for After something is providing, it has provided
After something is adding, it has added
After something is understanding, it has understood
After something is marrying, it has married
After something is improving, it has improved
After something is introducing, it has introduced
After something is existing, it has existed
After something is representing, it has
2024-07-16 20:21:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 20:25:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1377,  0.0032,  0.8169,  ..., -0.2063,  1.2812,  0.3975],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.6914,  0.3945,  1.2080,  ...,  2.2227, -0.5977,  3.9395],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 9.5367e-05, -3.5309e-02,  1.2913e-03,  ...,  1.1223e-02,
         -2.8038e-03, -1.1459e-02],
        [ 9.6130e-04, -2.2156e-02, -2.4414e-04,  ..., -8.4381e-03,
         -4.1466e-03,  8.8654e-03],
        [ 7.5722e-03,  1.2230e-02, -1.1406e-02,  ..., -1.6918e-03,
          1.3763e-02,  8.8215e-04],
        ...,
        [-6.2943e-03,  7.4158e-03, -7.6370e-03,  ..., -2.3376e-02,
         -5.4779e-03,  2.8915e-03],
        [ 9.9373e-04, -8.5754e-03,  5.6152e-03,  ..., -1.7654e-02,
         -3.4271e-02,  1.8463e-03],
        [-1.8539e-03,  9.2163e-03, -6.8512e-03,  ..., -3.0327e-03,
          7.4997e-03, -1.9318e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.7500,  0.2028,  1.9238,  ...,  1.6777, -0.9717,  3.2188]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 20:25:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is providing, it has provided
After something is adding, it has added
After something is understanding, it has understood
After something is marrying, it has married
After something is improving, it has improved
After something is introducing, it has introduced
After something is existing, it has existed
After something is representing, it has
2024-07-16 20:25:24 root INFO     [order_1_approx] starting weight calculation for After something is providing, it has provided
After something is existing, it has existed
After something is marrying, it has married
After something is improving, it has improved
After something is adding, it has added
After something is representing, it has represented
After something is introducing, it has introduced
After something is understanding, it has
2024-07-16 20:25:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 20:29:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0615, -1.0957,  1.1973,  ..., -0.2104,  0.7876, -0.3477],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0410,  4.2695,  1.8672,  ...,  0.0566, -1.5391,  1.1758],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0216, -0.0265,  0.0071,  ..., -0.0164, -0.0101, -0.0017],
        [-0.0086, -0.0117, -0.0024,  ..., -0.0002, -0.0123, -0.0121],
        [-0.0023,  0.0072, -0.0071,  ...,  0.0051,  0.0112,  0.0043],
        ...,
        [-0.0079, -0.0123, -0.0011,  ..., -0.0025,  0.0067,  0.0083],
        [ 0.0091,  0.0001,  0.0098,  ..., -0.0156, -0.0471, -0.0015],
        [-0.0118,  0.0135, -0.0044,  ..., -0.0146,  0.0184, -0.0403]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3726,  4.7188,  2.2852,  ..., -0.3201, -1.5889,  0.8330]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 20:29:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is providing, it has provided
After something is existing, it has existed
After something is marrying, it has married
After something is improving, it has improved
After something is adding, it has added
After something is representing, it has represented
After something is introducing, it has introduced
After something is understanding, it has
2024-07-16 20:29:11 root INFO     [order_1_approx] starting weight calculation for After something is providing, it has provided
After something is adding, it has added
After something is representing, it has represented
After something is improving, it has improved
After something is understanding, it has understood
After something is marrying, it has married
After something is introducing, it has introduced
After something is existing, it has
2024-07-16 20:29:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 20:32:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0381,  0.7070,  1.5820,  ...,  0.7100,  1.2539, -0.4153],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7495,  0.7417,  2.6172,  ...,  0.8125, -1.4033,  2.8281],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0014, -0.0270,  0.0084,  ..., -0.0148, -0.0037, -0.0008],
        [ 0.0090, -0.0069, -0.0037,  ...,  0.0029, -0.0064,  0.0241],
        [-0.0070, -0.0026,  0.0008,  ..., -0.0003,  0.0035,  0.0034],
        ...,
        [-0.0063, -0.0310, -0.0165,  ..., -0.0369, -0.0085,  0.0022],
        [-0.0078, -0.0094,  0.0194,  ..., -0.0129, -0.0390, -0.0058],
        [ 0.0027,  0.0046, -0.0091,  ..., -0.0150, -0.0166, -0.0208]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9814,  0.4167,  3.2773,  ...,  0.6182, -1.5078,  3.6797]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 20:32:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is providing, it has provided
After something is adding, it has added
After something is representing, it has represented
After something is improving, it has improved
After something is understanding, it has understood
After something is marrying, it has married
After something is introducing, it has introduced
After something is existing, it has
2024-07-16 20:32:58 root INFO     [order_1_approx] starting weight calculation for After something is existing, it has existed
After something is adding, it has added
After something is marrying, it has married
After something is representing, it has represented
After something is providing, it has provided
After something is understanding, it has understood
After something is improving, it has improved
After something is introducing, it has
2024-07-16 20:32:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 20:36:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0215,  0.8789,  0.6060,  ..., -0.1102, -0.0591, -0.0317],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.3857, -1.5332,  1.3604,  ...,  4.9531, -0.5742,  5.1758],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0017, -0.0218,  0.0060,  ...,  0.0028,  0.0130, -0.0145],
        [-0.0021, -0.0051, -0.0065,  ..., -0.0036, -0.0260,  0.0099],
        [-0.0122, -0.0069, -0.0148,  ..., -0.0010,  0.0088, -0.0011],
        ...,
        [ 0.0107, -0.0155, -0.0219,  ..., -0.0033, -0.0074,  0.0172],
        [-0.0082,  0.0257,  0.0065,  ..., -0.0181, -0.0494,  0.0004],
        [-0.0091,  0.0113,  0.0013,  ..., -0.0184,  0.0002, -0.0159]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.4199, -1.1074,  1.3984,  ...,  4.3125, -0.0181,  4.8750]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 20:36:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is existing, it has existed
After something is adding, it has added
After something is marrying, it has married
After something is representing, it has represented
After something is providing, it has provided
After something is understanding, it has understood
After something is improving, it has improved
After something is introducing, it has
2024-07-16 20:36:43 root INFO     [order_1_approx] starting weight calculation for After something is adding, it has added
After something is providing, it has provided
After something is introducing, it has introduced
After something is understanding, it has understood
After something is existing, it has existed
After something is representing, it has represented
After something is marrying, it has married
After something is improving, it has
2024-07-16 20:36:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 20:40:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2170, -0.4854,  1.1104,  ..., -0.2612,  1.2715, -0.2368],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.6250,  1.6279,  1.1426,  ...,  3.9102, -1.3008, -0.2598],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-4.7455e-03, -1.0773e-02, -6.8665e-05,  ..., -1.2497e-02,
         -4.6310e-03, -1.6052e-02],
        [ 6.3286e-03,  8.7166e-04, -7.3586e-03,  ...,  1.4000e-03,
         -5.3978e-03, -1.2169e-03],
        [-4.5776e-04, -1.5228e-02,  1.5898e-03,  ..., -3.9406e-03,
          6.4735e-03,  1.5427e-02],
        ...,
        [-8.6212e-03, -1.4557e-02,  1.3752e-03,  ..., -1.1520e-02,
          1.1696e-02,  1.4973e-03],
        [-1.5076e-02, -1.9516e-02,  4.4861e-03,  ..., -3.1342e-02,
         -4.4159e-02, -1.4969e-02],
        [-1.7204e-03,  1.7059e-02, -3.1376e-04,  ..., -2.0485e-03,
         -2.5711e-03, -3.4668e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.7617,  2.0156,  0.5381,  ...,  4.0664, -1.8584, -1.1934]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 20:40:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is adding, it has added
After something is providing, it has provided
After something is introducing, it has introduced
After something is understanding, it has understood
After something is existing, it has existed
After something is representing, it has represented
After something is marrying, it has married
After something is improving, it has
2024-07-16 20:40:30 root INFO     [order_1_approx] starting weight calculation for After something is representing, it has represented
After something is improving, it has improved
After something is introducing, it has introduced
After something is existing, it has existed
After something is marrying, it has married
After something is adding, it has added
After something is understanding, it has understood
After something is providing, it has
2024-07-16 20:40:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 20:44:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([1.0000, 0.3335, 0.1658,  ..., 0.7617, 0.9561, 0.2139], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6396,  1.5342, -1.0498,  ...,  0.6357, -3.3066,  1.5439],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0084, -0.0100,  0.0176,  ..., -0.0015,  0.0079, -0.0099],
        [ 0.0066, -0.0025,  0.0051,  ..., -0.0029, -0.0030,  0.0002],
        [-0.0051, -0.0008,  0.0146,  ...,  0.0003,  0.0238, -0.0044],
        ...,
        [-0.0148, -0.0029, -0.0157,  ..., -0.0156, -0.0129,  0.0126],
        [ 0.0011, -0.0188,  0.0131,  ..., -0.0238, -0.0404,  0.0188],
        [-0.0034,  0.0136,  0.0011,  ..., -0.0045,  0.0167, -0.0203]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.8906,  2.0156, -0.8740,  ...,  1.2812, -3.7012,  1.5098]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 20:44:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is representing, it has represented
After something is improving, it has improved
After something is introducing, it has introduced
After something is existing, it has existed
After something is marrying, it has married
After something is adding, it has added
After something is understanding, it has understood
After something is providing, it has
2024-07-16 20:44:17 root INFO     total operator prediction time: 1809.844618320465 seconds
2024-07-16 20:44:17 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_inf - Ved
2024-07-16 20:44:17 root INFO     building operator verb_inf - Ved
2024-07-16 20:44:17 root INFO     [order_1_approx] starting weight calculation for If the present form is expect, the past form is expected
If the present form is hear, the past form is heard
If the present form is send, the past form is sent
If the present form is unite, the past form is united
If the present form is spend, the past form is spent
If the present form is reduce, the past form is reduced
If the present form is believe, the past form is believed
If the present form is receive, the past form is
2024-07-16 20:44:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 20:48:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1992, -0.2905,  1.3350,  ...,  1.8145,  0.1316,  0.9233],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4102, -0.0796, -0.5166,  ...,  2.4727,  1.8789,  1.9189],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0133, -0.0158, -0.0120,  ..., -0.0029,  0.0005, -0.0254],
        [-0.0077, -0.0117,  0.0041,  ..., -0.0069, -0.0005, -0.0062],
        [-0.0032, -0.0031, -0.0196,  ..., -0.0037,  0.0069,  0.0075],
        ...,
        [-0.0085,  0.0017, -0.0131,  ..., -0.0027,  0.0002,  0.0102],
        [-0.0110,  0.0048,  0.0099,  ..., -0.0152, -0.0135,  0.0075],
        [-0.0163,  0.0008,  0.0004,  ..., -0.0135,  0.0062, -0.0154]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6875,  0.3799, -0.4429,  ...,  2.2832,  1.5352,  2.4590]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 20:48:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is expect, the past form is expected
If the present form is hear, the past form is heard
If the present form is send, the past form is sent
If the present form is unite, the past form is united
If the present form is spend, the past form is spent
If the present form is reduce, the past form is reduced
If the present form is believe, the past form is believed
If the present form is receive, the past form is
2024-07-16 20:48:02 root INFO     [order_1_approx] starting weight calculation for If the present form is receive, the past form is received
If the present form is unite, the past form is united
If the present form is hear, the past form is heard
If the present form is send, the past form is sent
If the present form is spend, the past form is spent
If the present form is believe, the past form is believed
If the present form is expect, the past form is expected
If the present form is reduce, the past form is
2024-07-16 20:48:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 20:51:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0134,  0.0654,  0.4258,  ...,  0.5439, -0.3389,  0.1794],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8730, -0.3394,  0.5059,  ..., -1.4102, -2.0273, -0.8594],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-9.1476e-03, -5.5389e-03,  5.2261e-03,  ...,  4.7836e-03,
          1.8311e-04, -2.1576e-02],
        [-4.5166e-03, -2.3560e-02, -1.0834e-02,  ..., -7.3814e-04,
         -7.0724e-03, -1.0185e-02],
        [-5.7487e-03, -2.6627e-03, -2.1637e-02,  ..., -8.5678e-03,
          1.2787e-02, -2.7428e-03],
        ...,
        [-7.2746e-03,  1.2426e-03,  9.3689e-03,  ..., -1.7288e-02,
          4.1122e-03,  8.1024e-03],
        [-1.6693e-02, -4.5090e-03,  1.2756e-02,  ..., -1.9272e-02,
         -2.5070e-02, -3.2997e-04],
        [-1.8883e-04,  7.6942e-03, -1.9073e-05,  ..., -9.4833e-03,
          9.8114e-03, -2.3468e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.2383, -0.3462,  0.8115,  ..., -2.0117, -2.5645, -1.0283]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 20:51:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is receive, the past form is received
If the present form is unite, the past form is united
If the present form is hear, the past form is heard
If the present form is send, the past form is sent
If the present form is spend, the past form is spent
If the present form is believe, the past form is believed
If the present form is expect, the past form is expected
If the present form is reduce, the past form is
2024-07-16 20:51:47 root INFO     [order_1_approx] starting weight calculation for If the present form is hear, the past form is heard
If the present form is receive, the past form is received
If the present form is unite, the past form is united
If the present form is send, the past form is sent
If the present form is reduce, the past form is reduced
If the present form is expect, the past form is expected
If the present form is spend, the past form is spent
If the present form is believe, the past form is
2024-07-16 20:51:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 20:55:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3306,  0.3789,  1.1465,  ...,  1.5381,  0.9609, -0.7451],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.8848,  1.3867,  0.0000,  ...,  1.4443, -4.6094,  0.3945],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.6510e-02, -3.1464e-02,  2.0218e-03,  ..., -1.2543e-02,
          1.7700e-03, -1.1322e-02],
        [-1.8478e-02, -2.1118e-02, -1.0347e-03,  ...,  5.1308e-04,
          5.2910e-03, -1.2978e-02],
        [-6.7520e-03,  4.3106e-03, -1.6327e-02,  ..., -2.7676e-03,
          7.2021e-03,  1.2760e-03],
        ...,
        [-6.5689e-03, -5.7602e-03, -2.0752e-03,  ..., -1.9180e-02,
          2.5520e-03,  6.1684e-03],
        [-7.8354e-03,  1.1551e-02,  4.1199e-04,  ..., -2.1301e-02,
         -3.4637e-02,  3.4332e-05],
        [-8.8654e-03,  3.0090e-02,  1.4095e-03,  ...,  1.1238e-02,
          7.6599e-03, -3.7109e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.0078,  1.3955,  0.3176,  ...,  2.1953, -4.2461,  0.7798]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 20:55:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is hear, the past form is heard
If the present form is receive, the past form is received
If the present form is unite, the past form is united
If the present form is send, the past form is sent
If the present form is reduce, the past form is reduced
If the present form is expect, the past form is expected
If the present form is spend, the past form is spent
If the present form is believe, the past form is
2024-07-16 20:55:33 root INFO     [order_1_approx] starting weight calculation for If the present form is reduce, the past form is reduced
If the present form is unite, the past form is united
If the present form is spend, the past form is spent
If the present form is expect, the past form is expected
If the present form is believe, the past form is believed
If the present form is send, the past form is sent
If the present form is receive, the past form is received
If the present form is hear, the past form is
2024-07-16 20:55:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 20:59:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.8877, 0.5190, 0.7935,  ..., 1.6758, 0.7871, 1.3174], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.0938,  3.6641, -0.8477,  ..., -2.3770,  3.5723,  3.9766],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.0126e-02, -1.8143e-02, -8.7738e-03,  ...,  1.5717e-02,
         -3.0136e-04, -2.1393e-02],
        [-7.5493e-03, -1.8768e-02,  6.2675e-03,  ...,  1.1284e-02,
         -8.3923e-05, -1.6205e-02],
        [ 1.6823e-03, -7.4310e-03, -9.9106e-03,  ..., -2.8381e-03,
          1.2032e-02,  3.6125e-03],
        ...,
        [-1.3229e-02,  8.0414e-03, -1.0315e-02,  ..., -1.9653e-02,
          9.1553e-04,  1.1948e-02],
        [-8.0872e-03,  4.9133e-03,  7.4768e-04,  ..., -1.7151e-02,
         -2.1622e-02, -4.9362e-03],
        [-4.5929e-03,  6.9046e-03,  1.3657e-02,  ..., -1.9928e-02,
         -1.9608e-03, -1.7532e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.2734,  4.4766, -1.0830,  ..., -2.4434,  3.3594,  4.1992]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 20:59:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is reduce, the past form is reduced
If the present form is unite, the past form is united
If the present form is spend, the past form is spent
If the present form is expect, the past form is expected
If the present form is believe, the past form is believed
If the present form is send, the past form is sent
If the present form is receive, the past form is received
If the present form is hear, the past form is
2024-07-16 20:59:18 root INFO     [order_1_approx] starting weight calculation for If the present form is hear, the past form is heard
If the present form is send, the past form is sent
If the present form is reduce, the past form is reduced
If the present form is believe, the past form is believed
If the present form is spend, the past form is spent
If the present form is unite, the past form is united
If the present form is receive, the past form is received
If the present form is expect, the past form is
2024-07-16 20:59:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 21:03:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.4736, 0.9097, 0.8472,  ..., 1.2754, 1.9395, 0.5439], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6758,  0.2092, -0.0322,  ..., -0.7666, -1.0879,  1.0273],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.1795e-02, -1.9608e-02, -7.3090e-03,  ..., -1.6289e-03,
         -1.0162e-02, -2.2125e-02],
        [-2.3590e-02, -2.9419e-02, -3.5896e-03,  ...,  2.9125e-03,
          7.9536e-04, -1.7212e-02],
        [-1.5686e-02,  4.5547e-03, -1.4153e-02,  ...,  1.2810e-02,
         -6.3744e-03, -4.9820e-03],
        ...,
        [-1.2863e-02,  1.0666e-02, -3.2406e-03,  ..., -1.5419e-02,
         -2.9316e-03,  4.8676e-03],
        [-1.2264e-03,  1.2115e-02,  2.1019e-03,  ..., -2.0782e-02,
         -2.6154e-02,  1.1253e-03],
        [-1.8578e-03,  1.4519e-02,  9.5367e-07,  ..., -1.1101e-02,
         -3.7537e-03, -2.7420e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.6230,  0.5288,  0.6626,  ..., -1.0586, -1.0420,  1.5000]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 21:03:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is hear, the past form is heard
If the present form is send, the past form is sent
If the present form is reduce, the past form is reduced
If the present form is believe, the past form is believed
If the present form is spend, the past form is spent
If the present form is unite, the past form is united
If the present form is receive, the past form is received
If the present form is expect, the past form is
2024-07-16 21:03:03 root INFO     [order_1_approx] starting weight calculation for If the present form is believe, the past form is believed
If the present form is unite, the past form is united
If the present form is hear, the past form is heard
If the present form is expect, the past form is expected
If the present form is send, the past form is sent
If the present form is receive, the past form is received
If the present form is reduce, the past form is reduced
If the present form is spend, the past form is
2024-07-16 21:03:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 21:06:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2686, -0.2023,  1.5273,  ...,  1.4873,  0.2898, -0.6924],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6680,  3.3301,  2.4766,  ...,  2.1250, -1.7041,  0.9277],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.3451e-02, -6.1646e-03,  8.3923e-03,  ..., -1.7147e-03,
         -4.1809e-03, -8.7433e-03],
        [-5.8746e-03, -1.1536e-02,  1.9073e-04,  ..., -5.2414e-03,
         -6.1188e-03, -1.2451e-02],
        [-6.3629e-03, -6.5002e-03, -1.6525e-02,  ..., -9.0866e-03,
         -7.2145e-04, -5.7983e-03],
        ...,
        [-6.8359e-03, -1.4296e-03, -4.3907e-03,  ..., -2.0889e-02,
         -3.4676e-03, -6.2599e-03],
        [-1.5640e-02,  3.5744e-03, -5.2948e-03,  ..., -1.0094e-02,
         -1.4305e-02,  1.5579e-02],
        [-1.1642e-02,  1.8875e-02,  6.3782e-03,  ..., -1.1147e-02,
          9.5367e-05, -2.5986e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.8301,  3.4375,  2.5430,  ...,  1.3301, -1.5303,  0.8721]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 21:06:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is believe, the past form is believed
If the present form is unite, the past form is united
If the present form is hear, the past form is heard
If the present form is expect, the past form is expected
If the present form is send, the past form is sent
If the present form is receive, the past form is received
If the present form is reduce, the past form is reduced
If the present form is spend, the past form is
2024-07-16 21:06:48 root INFO     [order_1_approx] starting weight calculation for If the present form is spend, the past form is spent
If the present form is expect, the past form is expected
If the present form is hear, the past form is heard
If the present form is receive, the past form is received
If the present form is believe, the past form is believed
If the present form is reduce, the past form is reduced
If the present form is unite, the past form is united
If the present form is send, the past form is
2024-07-16 21:06:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 21:10:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([1.1045, 0.3105, 1.5039,  ..., 1.2344, 0.1576, 0.0637], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.1562,  0.7305,  2.1914,  ...,  3.3711, -2.0566,  1.1855],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0118, -0.0060, -0.0008,  ..., -0.0004,  0.0064, -0.0045],
        [-0.0135, -0.0257,  0.0078,  ...,  0.0095, -0.0021, -0.0087],
        [-0.0022, -0.0081, -0.0200,  ..., -0.0051,  0.0089, -0.0048],
        ...,
        [-0.0328,  0.0030, -0.0094,  ..., -0.0123, -0.0055, -0.0080],
        [-0.0021, -0.0085,  0.0074,  ..., -0.0167, -0.0191,  0.0120],
        [-0.0098,  0.0068,  0.0063,  ..., -0.0114,  0.0081, -0.0171]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.5254,  0.4905,  2.2363,  ...,  3.2949, -2.2969,  0.7500]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 21:10:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is spend, the past form is spent
If the present form is expect, the past form is expected
If the present form is hear, the past form is heard
If the present form is receive, the past form is received
If the present form is believe, the past form is believed
If the present form is reduce, the past form is reduced
If the present form is unite, the past form is united
If the present form is send, the past form is
2024-07-16 21:10:34 root INFO     [order_1_approx] starting weight calculation for If the present form is send, the past form is sent
If the present form is receive, the past form is received
If the present form is hear, the past form is heard
If the present form is reduce, the past form is reduced
If the present form is spend, the past form is spent
If the present form is expect, the past form is expected
If the present form is believe, the past form is believed
If the present form is unite, the past form is
2024-07-16 21:10:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 21:14:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0375,  0.4473,  0.5337,  ...,  0.6680,  0.0659,  0.1071],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6284,  0.8940, -1.6133,  ...,  1.8389, -0.5352, -4.0469],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0166, -0.0125, -0.0028,  ...,  0.0021,  0.0075, -0.0087],
        [-0.0055, -0.0157, -0.0124,  ...,  0.0084,  0.0042, -0.0191],
        [ 0.0091, -0.0061, -0.0136,  ...,  0.0107,  0.0073, -0.0099],
        ...,
        [-0.0358,  0.0013,  0.0038,  ..., -0.0226, -0.0074,  0.0033],
        [-0.0174,  0.0085, -0.0029,  ..., -0.0407, -0.0291,  0.0040],
        [-0.0153,  0.0204,  0.0074,  ...,  0.0076,  0.0096, -0.0389]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3379,  0.6572, -0.8906,  ...,  1.9346, -0.5117, -4.4492]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 21:14:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is send, the past form is sent
If the present form is receive, the past form is received
If the present form is hear, the past form is heard
If the present form is reduce, the past form is reduced
If the present form is spend, the past form is spent
If the present form is expect, the past form is expected
If the present form is believe, the past form is believed
If the present form is unite, the past form is
2024-07-16 21:14:18 root INFO     total operator prediction time: 1801.6339061260223 seconds
2024-07-16 21:14:18 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_inf - 3pSg
2024-07-16 21:14:18 root INFO     building operator verb_inf - 3pSg
2024-07-16 21:14:19 root INFO     [order_1_approx] starting weight calculation for I remember, he remembers
I protect, he protects
I refer, he refers
I enjoy, he enjoys
I ensure, he ensures
I accept, he accepts
I represent, he represents
I send, he
2024-07-16 21:14:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 21:18:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5469, -0.5859,  1.3594,  ...,  0.6826,  0.0544,  0.8257],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.4688, -0.5918,  4.0508,  ...,  2.5352, -2.1836,  4.6953],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0157, -0.0070,  0.0045,  ...,  0.0026, -0.0045, -0.0147],
        [-0.0253, -0.0259,  0.0038,  ..., -0.0017, -0.0088,  0.0205],
        [-0.0166,  0.0032, -0.0251,  ...,  0.0085, -0.0062,  0.0003],
        ...,
        [-0.0073, -0.0145, -0.0093,  ..., -0.0083, -0.0035, -0.0095],
        [-0.0016, -0.0104,  0.0155,  ..., -0.0088, -0.0293,  0.0141],
        [ 0.0021,  0.0207,  0.0184,  ..., -0.0077,  0.0141, -0.0278]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.2949,  0.3154,  4.0898,  ...,  2.0820, -1.6211,  4.0625]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 21:18:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I remember, he remembers
I protect, he protects
I refer, he refers
I enjoy, he enjoys
I ensure, he ensures
I accept, he accepts
I represent, he represents
I send, he
2024-07-16 21:18:06 root INFO     [order_1_approx] starting weight calculation for I send, he sends
I accept, he accepts
I represent, he represents
I remember, he remembers
I ensure, he ensures
I enjoy, he enjoys
I protect, he protects
I refer, he
2024-07-16 21:18:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 21:21:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.6846, 0.0935, 0.1050,  ..., 0.2136, 1.3906, 0.5684], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.2227,  1.4805,  1.5215,  ...,  6.5312,  1.3828,  4.2852],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0163, -0.0369,  0.0072,  ...,  0.0153, -0.0030, -0.0125],
        [-0.0020, -0.0203,  0.0007,  ...,  0.0116,  0.0101, -0.0095],
        [-0.0143, -0.0116, -0.0281,  ...,  0.0096, -0.0084, -0.0110],
        ...,
        [-0.0074, -0.0073, -0.0129,  ..., -0.0237, -0.0053, -0.0145],
        [-0.0028, -0.0029,  0.0311,  ..., -0.0197, -0.0432,  0.0024],
        [ 0.0058,  0.0276,  0.0149,  ..., -0.0040,  0.0063, -0.0336]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5645,  1.4795,  1.2207,  ...,  5.9531,  2.3047,  4.7539]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 21:21:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I send, he sends
I accept, he accepts
I represent, he represents
I remember, he remembers
I ensure, he ensures
I enjoy, he enjoys
I protect, he protects
I refer, he
2024-07-16 21:21:50 root INFO     [order_1_approx] starting weight calculation for I enjoy, he enjoys
I accept, he accepts
I refer, he refers
I protect, he protects
I represent, he represents
I ensure, he ensures
I send, he sends
I remember, he
2024-07-16 21:21:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 21:25:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.6597, 0.3657, 0.1802,  ..., 0.0515, 0.5034, 0.7422], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.3398, -0.9048, -2.9492,  ...,  0.9995, -3.2734,  5.7734],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0137, -0.0101,  0.0167,  ...,  0.0026, -0.0040, -0.0182],
        [-0.0160, -0.0114,  0.0148,  ..., -0.0075,  0.0112,  0.0096],
        [-0.0229,  0.0131, -0.0314,  ...,  0.0083, -0.0025, -0.0103],
        ...,
        [ 0.0030, -0.0199, -0.0066,  ..., -0.0291, -0.0217,  0.0019],
        [ 0.0068,  0.0158,  0.0203,  ..., -0.0106, -0.0456,  0.0057],
        [ 0.0010,  0.0207,  0.0073,  ..., -0.0060,  0.0154, -0.0300]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.0039, -1.0146, -2.2988,  ...,  0.4395, -2.7656,  5.5938]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 21:25:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I enjoy, he enjoys
I accept, he accepts
I refer, he refers
I protect, he protects
I represent, he represents
I ensure, he ensures
I send, he sends
I remember, he
2024-07-16 21:25:35 root INFO     [order_1_approx] starting weight calculation for I remember, he remembers
I protect, he protects
I refer, he refers
I accept, he accepts
I ensure, he ensures
I send, he sends
I enjoy, he enjoys
I represent, he
2024-07-16 21:25:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 21:29:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1230, -0.8281,  1.1602,  ..., -0.6655,  1.0537,  0.8389],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.3906, -2.7461,  0.4102,  ...,  2.3789, -1.3154,  5.8711],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0146, -0.0081, -0.0021,  ..., -0.0058, -0.0034, -0.0025],
        [-0.0019, -0.0139,  0.0069,  ..., -0.0085, -0.0055,  0.0156],
        [-0.0054, -0.0080, -0.0161,  ...,  0.0065,  0.0002,  0.0041],
        ...,
        [-0.0125, -0.0008, -0.0063,  ..., -0.0109, -0.0036,  0.0060],
        [-0.0028,  0.0029,  0.0119,  ..., -0.0141, -0.0408,  0.0034],
        [-0.0060,  0.0147,  0.0033,  ..., -0.0125,  0.0061, -0.0047]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.6719, -1.7988,  0.7070,  ...,  2.2246, -1.2939,  6.5508]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 21:29:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I remember, he remembers
I protect, he protects
I refer, he refers
I accept, he accepts
I ensure, he ensures
I send, he sends
I enjoy, he enjoys
I represent, he
2024-07-16 21:29:19 root INFO     [order_1_approx] starting weight calculation for I enjoy, he enjoys
I remember, he remembers
I send, he sends
I protect, he protects
I ensure, he ensures
I refer, he refers
I represent, he represents
I accept, he
2024-07-16 21:29:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 21:33:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2607, -0.6475,  1.3232,  ...,  0.6006,  0.7739,  0.5981],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.6172, -1.9912,  3.5527,  ...,  0.7354, -2.1445,  3.7676],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0106, -0.0116, -0.0010,  ...,  0.0064, -0.0090, -0.0215],
        [-0.0139, -0.0031, -0.0038,  ..., -0.0071, -0.0037,  0.0090],
        [-0.0246, -0.0031, -0.0202,  ...,  0.0152,  0.0040,  0.0027],
        ...,
        [-0.0037, -0.0010, -0.0062,  ...,  0.0047,  0.0067, -0.0213],
        [ 0.0011,  0.0026,  0.0145,  ..., -0.0231, -0.0158,  0.0077],
        [ 0.0015,  0.0058, -0.0050,  ...,  0.0025,  0.0076, -0.0098]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.5078, -1.9727,  4.0742,  ..., -0.0825, -0.8027,  3.4375]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 21:33:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I enjoy, he enjoys
I remember, he remembers
I send, he sends
I protect, he protects
I ensure, he ensures
I refer, he refers
I represent, he represents
I accept, he
2024-07-16 21:33:05 root INFO     [order_1_approx] starting weight calculation for I send, he sends
I refer, he refers
I represent, he represents
I ensure, he ensures
I remember, he remembers
I enjoy, he enjoys
I accept, he accepts
I protect, he
2024-07-16 21:33:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 21:36:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9619, -0.0574,  1.4990,  ...,  1.1914,  1.2129,  0.8779],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3254, -1.1055,  3.1914,  ...,  2.6309, -1.2422,  4.4844],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.4282e-02,  4.5776e-05,  1.4229e-02,  ...,  5.1804e-03,
         -6.5231e-03, -2.5299e-02],
        [-1.1650e-02,  5.7144e-03,  1.7456e-02,  ...,  3.2120e-03,
         -1.2337e-02,  1.4595e-02],
        [-9.8419e-03, -8.7738e-05, -2.3621e-02,  ...,  5.1079e-03,
          6.2180e-03,  4.6616e-03],
        ...,
        [-1.6907e-02, -1.8204e-02, -1.8097e-02,  ..., -6.5689e-03,
         -1.7883e-02, -9.6512e-03],
        [-1.4732e-02,  4.9286e-03,  1.1978e-03,  ..., -8.8501e-03,
         -3.7231e-02, -1.1826e-04],
        [-1.3229e-02,  9.5444e-03, -8.4915e-03,  ..., -1.5167e-02,
          2.4124e-02, -2.6962e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3438, -0.9277,  2.9414,  ...,  2.3926, -0.4556,  4.5312]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 21:36:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I send, he sends
I refer, he refers
I represent, he represents
I ensure, he ensures
I remember, he remembers
I enjoy, he enjoys
I accept, he accepts
I protect, he
2024-07-16 21:36:52 root INFO     [order_1_approx] starting weight calculation for I protect, he protects
I refer, he refers
I send, he sends
I remember, he remembers
I ensure, he ensures
I accept, he accepts
I represent, he represents
I enjoy, he
2024-07-16 21:36:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 21:40:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1664,  0.1113,  0.8018,  ..., -0.7915,  0.5190,  0.3989],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.6348, -2.4219,  3.3594,  ...,  1.5391,  0.1353,  7.1211],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0142, -0.0096,  0.0019,  ...,  0.0033, -0.0127, -0.0216],
        [-0.0215, -0.0199,  0.0045,  ..., -0.0014,  0.0046,  0.0161],
        [-0.0096,  0.0047, -0.0436,  ...,  0.0155,  0.0065, -0.0035],
        ...,
        [-0.0087, -0.0113, -0.0242,  ..., -0.0158, -0.0033, -0.0161],
        [ 0.0014,  0.0039, -0.0001,  ..., -0.0135, -0.0390,  0.0048],
        [ 0.0032,  0.0239,  0.0046,  ..., -0.0180,  0.0199, -0.0332]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.2305, -2.8066,  3.8418,  ...,  0.3242,  0.7930,  7.7148]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 21:40:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I protect, he protects
I refer, he refers
I send, he sends
I remember, he remembers
I ensure, he ensures
I accept, he accepts
I represent, he represents
I enjoy, he
2024-07-16 21:40:38 root INFO     [order_1_approx] starting weight calculation for I send, he sends
I refer, he refers
I accept, he accepts
I remember, he remembers
I represent, he represents
I protect, he protects
I enjoy, he enjoys
I ensure, he
2024-07-16 21:40:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 21:44:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8105, -0.6514,  0.4551,  ..., -0.0453,  0.4089,  0.6440],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5635,  2.0391,  2.8301,  ...,  1.8447,  0.7207,  3.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0163, -0.0075,  0.0182,  ..., -0.0052,  0.0012, -0.0316],
        [-0.0116, -0.0137, -0.0073,  ...,  0.0135,  0.0132,  0.0164],
        [-0.0152,  0.0034, -0.0155,  ...,  0.0269,  0.0247,  0.0115],
        ...,
        [-0.0126, -0.0150, -0.0288,  ..., -0.0044, -0.0043, -0.0108],
        [-0.0083,  0.0161,  0.0123,  ..., -0.0239, -0.0394,  0.0272],
        [ 0.0021,  0.0253, -0.0027,  ..., -0.0098, -0.0058, -0.0173]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2695,  2.0137,  3.3262,  ...,  0.5469,  1.6768,  3.3340]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 21:44:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I send, he sends
I refer, he refers
I accept, he accepts
I remember, he remembers
I represent, he represents
I protect, he protects
I enjoy, he enjoys
I ensure, he
2024-07-16 21:44:23 root INFO     total operator prediction time: 1804.6225821971893 seconds
2024-07-16 21:44:23 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_Ving - 3pSg
2024-07-16 21:44:23 root INFO     building operator verb_Ving - 3pSg
2024-07-16 21:44:23 root INFO     [order_1_approx] starting weight calculation for When something is creating, it creates
When something is consisting, it consists
When something is managing, it manages
When something is discovering, it discovers
When something is publishing, it publishes
When something is remaining, it remains
When something is involving, it involves
When something is understanding, it
2024-07-16 21:44:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 21:48:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2471, -0.7500,  0.5352,  ...,  0.6064,  1.2422, -0.3896],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1543,  0.5054,  2.1836,  ...,  1.5938, -2.0977,  2.7891],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0179, -0.0237, -0.0068,  ...,  0.0030,  0.0011, -0.0076],
        [-0.0007,  0.0046, -0.0088,  ..., -0.0126, -0.0092, -0.0116],
        [-0.0078,  0.0088, -0.0140,  ...,  0.0034, -0.0005, -0.0036],
        ...,
        [ 0.0059, -0.0090,  0.0046,  ...,  0.0009,  0.0073, -0.0074],
        [ 0.0149, -0.0086,  0.0006,  ...,  0.0024, -0.0222, -0.0117],
        [-0.0061,  0.0204, -0.0012,  ..., -0.0130,  0.0289, -0.0193]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5215,  0.8950,  1.9863,  ...,  1.7852, -2.6836,  2.2949]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 21:48:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is creating, it creates
When something is consisting, it consists
When something is managing, it manages
When something is discovering, it discovers
When something is publishing, it publishes
When something is remaining, it remains
When something is involving, it involves
When something is understanding, it
2024-07-16 21:48:08 root INFO     [order_1_approx] starting weight calculation for When something is remaining, it remains
When something is discovering, it discovers
When something is consisting, it consists
When something is understanding, it understands
When something is involving, it involves
When something is creating, it creates
When something is publishing, it publishes
When something is managing, it
2024-07-16 21:48:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 21:51:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0420, -0.3967,  0.2488,  ...,  0.7773,  0.5713, -0.9692],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.2227, -2.4121,  1.2637,  ...,  2.4219, -1.8594,  0.8633],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0094, -0.0090,  0.0068,  ...,  0.0012, -0.0007, -0.0058],
        [ 0.0076,  0.0050, -0.0066,  ..., -0.0093,  0.0029,  0.0071],
        [ 0.0057,  0.0041, -0.0184,  ...,  0.0107,  0.0034, -0.0036],
        ...,
        [ 0.0003, -0.0059,  0.0033,  ..., -0.0146,  0.0053,  0.0029],
        [ 0.0040, -0.0102,  0.0023,  ..., -0.0165, -0.0111,  0.0020],
        [-0.0071,  0.0129,  0.0091,  ..., -0.0002,  0.0117, -0.0075]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6162, -2.3926,  0.6709,  ...,  2.0508, -1.9697,  0.6846]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 21:51:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is remaining, it remains
When something is discovering, it discovers
When something is consisting, it consists
When something is understanding, it understands
When something is involving, it involves
When something is creating, it creates
When something is publishing, it publishes
When something is managing, it
2024-07-16 21:51:52 root INFO     [order_1_approx] starting weight calculation for When something is consisting, it consists
When something is discovering, it discovers
When something is involving, it involves
When something is managing, it manages
When something is publishing, it publishes
When something is understanding, it understands
When something is creating, it creates
When something is remaining, it
2024-07-16 21:51:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 21:55:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0425,  0.7896, -0.0479,  ...,  1.1309,  1.4590, -0.5200],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.3809,  1.8691,  0.4141,  ...,  3.0332, -2.1953,  4.4688],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0167, -0.0084,  0.0039,  ...,  0.0137, -0.0089, -0.0120],
        [-0.0043, -0.0093, -0.0069,  ..., -0.0157, -0.0047, -0.0023],
        [-0.0032,  0.0128, -0.0190,  ...,  0.0026, -0.0006,  0.0104],
        ...,
        [-0.0045, -0.0047,  0.0032,  ..., -0.0129, -0.0094, -0.0048],
        [ 0.0169,  0.0131, -0.0001,  ..., -0.0087, -0.0143,  0.0019],
        [-0.0121,  0.0056, -0.0019,  ...,  0.0061,  0.0253, -0.0114]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1035,  1.9043,  0.6855,  ...,  2.8711, -2.5859,  4.4883]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 21:55:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is consisting, it consists
When something is discovering, it discovers
When something is involving, it involves
When something is managing, it manages
When something is publishing, it publishes
When something is understanding, it understands
When something is creating, it creates
When something is remaining, it
2024-07-16 21:55:38 root INFO     [order_1_approx] starting weight calculation for When something is publishing, it publishes
When something is consisting, it consists
When something is remaining, it remains
When something is discovering, it discovers
When something is managing, it manages
When something is creating, it creates
When something is understanding, it understands
When something is involving, it
2024-07-16 21:55:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 21:59:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.7686, 0.6611, 0.0598,  ..., 1.5176, 0.7285, 0.7871], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.2500, -0.5146,  1.0234,  ...,  3.3770, -0.4810,  1.7363],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-9.7198e-03, -7.5836e-03, -1.1711e-03,  ...,  1.3626e-02,
         -5.3406e-05, -1.7822e-02],
        [-1.5404e-02,  3.4332e-04, -1.9798e-03,  ..., -2.4738e-03,
         -2.9221e-03, -1.1772e-02],
        [ 1.0529e-02,  1.0757e-03, -1.3824e-02,  ...,  7.1182e-03,
         -1.8005e-03,  1.7151e-02],
        ...,
        [-1.0910e-02, -4.0131e-03, -2.3926e-02,  ...,  6.5727e-03,
         -1.8585e-02, -1.6312e-02],
        [-1.3680e-02,  6.9199e-03, -3.9978e-03,  ...,  8.3466e-03,
         -2.2934e-02, -1.6693e-02],
        [-8.2016e-03,  1.9379e-02, -6.4697e-03,  ...,  3.7689e-03,
          1.2604e-02, -2.0309e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3809, -0.0776,  1.3799,  ...,  2.8438, -0.8037,  1.7979]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 21:59:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is publishing, it publishes
When something is consisting, it consists
When something is remaining, it remains
When something is discovering, it discovers
When something is managing, it manages
When something is creating, it creates
When something is understanding, it understands
When something is involving, it
2024-07-16 21:59:24 root INFO     [order_1_approx] starting weight calculation for When something is understanding, it understands
When something is managing, it manages
When something is remaining, it remains
When something is involving, it involves
When something is consisting, it consists
When something is discovering, it discovers
When something is publishing, it publishes
When something is creating, it
2024-07-16 21:59:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 22:03:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4434, -0.2642, -0.2266,  ...,  1.9492,  0.8594, -1.1133],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4580, -0.1799,  1.4395,  ...,  0.5708,  0.0430,  1.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0120, -0.0162, -0.0109,  ...,  0.0093,  0.0033, -0.0234],
        [-0.0087, -0.0058, -0.0120,  ..., -0.0165,  0.0002, -0.0080],
        [-0.0188,  0.0052, -0.0100,  ...,  0.0088,  0.0070,  0.0026],
        ...,
        [-0.0018, -0.0072, -0.0211,  ..., -0.0137, -0.0051, -0.0099],
        [-0.0069,  0.0115,  0.0084,  ...,  0.0011, -0.0332,  0.0098],
        [-0.0169,  0.0057,  0.0027,  ..., -0.0054,  0.0150, -0.0089]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0000,  0.3503,  1.7549,  ...,  0.4883, -0.2083,  0.5889]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 22:03:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is understanding, it understands
When something is managing, it manages
When something is remaining, it remains
When something is involving, it involves
When something is consisting, it consists
When something is discovering, it discovers
When something is publishing, it publishes
When something is creating, it
2024-07-16 22:03:12 root INFO     [order_1_approx] starting weight calculation for When something is managing, it manages
When something is understanding, it understands
When something is discovering, it discovers
When something is remaining, it remains
When something is involving, it involves
When something is consisting, it consists
When something is creating, it creates
When something is publishing, it
2024-07-16 22:03:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 22:06:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0273, -0.4050,  1.4736,  ...,  2.1680,  0.7544, -0.2415],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3975,  0.0127, -0.8159,  ...,  4.1250, -2.6387,  1.5098],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-7.4615e-03, -6.7520e-03,  6.7291e-03,  ...,  1.0567e-02,
         -1.5640e-03, -1.9638e-02],
        [ 3.4714e-04, -5.9662e-03, -8.3618e-03,  ..., -7.5607e-03,
         -9.4986e-03,  7.6294e-06],
        [-2.1439e-03,  5.3101e-03, -6.4240e-03,  ...,  1.3031e-02,
         -2.4796e-04, -1.8415e-03],
        ...,
        [-1.7319e-03, -1.1513e-02, -7.6180e-03,  ..., -7.1983e-03,
         -1.9623e-02, -1.1993e-02],
        [-9.5367e-03,  7.6294e-06,  1.4626e-02,  ...,  5.3329e-03,
         -2.5848e-02,  3.5381e-03],
        [-1.8326e-02,  9.2545e-03, -2.1629e-03,  ..., -1.4008e-02,
          1.3184e-02, -1.5991e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5928,  0.6123, -0.8994,  ...,  3.9824, -3.2148,  1.7461]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 22:06:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is managing, it manages
When something is understanding, it understands
When something is discovering, it discovers
When something is remaining, it remains
When something is involving, it involves
When something is consisting, it consists
When something is creating, it creates
When something is publishing, it
2024-07-16 22:06:59 root INFO     [order_1_approx] starting weight calculation for When something is understanding, it understands
When something is managing, it manages
When something is creating, it creates
When something is discovering, it discovers
When something is involving, it involves
When something is remaining, it remains
When something is publishing, it publishes
When something is consisting, it
2024-07-16 22:06:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 22:10:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2222,  0.0818,  0.3799,  ...,  1.3076,  1.0684, -0.2913],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3081,  1.2988,  0.0605,  ...,  4.3516, -5.4883,  0.3604],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0241, -0.0041,  0.0032,  ..., -0.0012, -0.0049, -0.0270],
        [-0.0034, -0.0082,  0.0055,  ..., -0.0236, -0.0102, -0.0098],
        [-0.0181, -0.0124, -0.0161,  ...,  0.0105, -0.0090,  0.0029],
        ...,
        [-0.0245, -0.0107, -0.0239,  ..., -0.0201,  0.0101,  0.0008],
        [ 0.0132,  0.0175,  0.0122,  ..., -0.0152, -0.0196,  0.0087],
        [-0.0041,  0.0200,  0.0010,  ..., -0.0060,  0.0187, -0.0161]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1388,  2.2148,  1.7812,  ...,  4.1211, -5.3750, -0.1265]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 22:10:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is understanding, it understands
When something is managing, it manages
When something is creating, it creates
When something is discovering, it discovers
When something is involving, it involves
When something is remaining, it remains
When something is publishing, it publishes
When something is consisting, it
2024-07-16 22:10:45 root INFO     [order_1_approx] starting weight calculation for When something is managing, it manages
When something is consisting, it consists
When something is involving, it involves
When something is understanding, it understands
When something is remaining, it remains
When something is creating, it creates
When something is publishing, it publishes
When something is discovering, it
2024-07-16 22:10:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 22:14:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1182, -0.4517,  0.0098,  ...,  0.4644,  0.3552,  0.0441],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0166,  0.2422,  1.4668,  ..., -1.5312, -4.5859,  2.6562],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0126, -0.0139, -0.0011,  ...,  0.0173,  0.0118, -0.0314],
        [ 0.0047,  0.0006, -0.0055,  ...,  0.0073,  0.0034,  0.0052],
        [ 0.0051, -0.0017, -0.0209,  ..., -0.0106, -0.0016, -0.0086],
        ...,
        [-0.0021,  0.0080, -0.0220,  ..., -0.0083, -0.0042, -0.0053],
        [ 0.0143,  0.0106,  0.0112,  ...,  0.0076, -0.0197, -0.0013],
        [-0.0024,  0.0190, -0.0026,  ..., -0.0128,  0.0242, -0.0188]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7529,  1.1963,  1.3359,  ..., -1.0352, -4.9102,  2.7637]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 22:14:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is managing, it manages
When something is consisting, it consists
When something is involving, it involves
When something is understanding, it understands
When something is remaining, it remains
When something is creating, it creates
When something is publishing, it publishes
When something is discovering, it
2024-07-16 22:14:31 root INFO     total operator prediction time: 1807.9359419345856 seconds
2024-07-16 22:14:31 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on noun - plural_reg
2024-07-16 22:14:31 root INFO     building operator noun - plural_reg
2024-07-16 22:14:31 root INFO     [order_1_approx] starting weight calculation for The plural form of user is users
The plural form of period is periods
The plural form of day is days
The plural form of version is versions
The plural form of resource is resources
The plural form of law is laws
The plural form of language is languages
The plural form of event is
2024-07-16 22:14:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 22:18:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2832,  0.4595, -0.2476,  ...,  0.3782,  0.0275,  1.2021],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4985, -1.4150,  3.6855,  ...,  0.2996,  3.6328,  4.1914],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0172, -0.0041,  0.0062,  ..., -0.0170,  0.0103, -0.0034],
        [-0.0003, -0.0171,  0.0010,  ...,  0.0057, -0.0038,  0.0093],
        [ 0.0066, -0.0026, -0.0189,  ..., -0.0069,  0.0246,  0.0138],
        ...,
        [-0.0090, -0.0102, -0.0161,  ..., -0.0053,  0.0038,  0.0044],
        [ 0.0146,  0.0011,  0.0079,  ..., -0.0078, -0.0177,  0.0172],
        [-0.0048,  0.0129, -0.0120,  ..., -0.0033,  0.0121, -0.0047]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2029, -1.5557,  4.1445,  ..., -0.1567,  3.6934,  4.7891]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 22:18:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of user is users
The plural form of period is periods
The plural form of day is days
The plural form of version is versions
The plural form of resource is resources
The plural form of law is laws
The plural form of language is languages
The plural form of event is
2024-07-16 22:18:17 root INFO     [order_1_approx] starting weight calculation for The plural form of user is users
The plural form of language is languages
The plural form of event is events
The plural form of law is laws
The plural form of period is periods
The plural form of version is versions
The plural form of day is days
The plural form of resource is
2024-07-16 22:18:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 22:22:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5752,  0.0811, -0.9507,  ...,  0.9111,  1.0947,  0.1178],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5254, -0.4646,  3.1230,  ..., -0.5713,  1.6406,  2.1641],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.5205e-02,  5.6000e-03,  1.0567e-02,  ..., -2.9907e-03,
          8.7585e-03, -2.2598e-02],
        [-2.3060e-03, -8.1177e-03, -1.3962e-02,  ...,  1.2863e-02,
         -2.8305e-02,  5.6648e-03],
        [ 6.8016e-03, -3.9215e-03, -1.6083e-02,  ..., -2.1324e-03,
          2.0752e-03,  7.8201e-05],
        ...,
        [ 4.7531e-03, -1.4030e-02, -1.0124e-02,  ..., -1.1749e-02,
         -2.5940e-03, -1.6556e-02],
        [-5.9204e-03,  6.9962e-03,  4.3411e-03,  ...,  1.1444e-03,
         -1.8646e-02, -1.1795e-02],
        [-7.4387e-03,  1.1902e-02, -1.2337e-02,  ..., -3.5492e-02,
          1.4709e-02, -1.9348e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5850, -0.6685,  3.1250,  ..., -0.6035,  0.9023,  2.3398]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 22:22:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of user is users
The plural form of language is languages
The plural form of event is events
The plural form of law is laws
The plural form of period is periods
The plural form of version is versions
The plural form of day is days
The plural form of resource is
2024-07-16 22:22:03 root INFO     [order_1_approx] starting weight calculation for The plural form of resource is resources
The plural form of event is events
The plural form of version is versions
The plural form of day is days
The plural form of period is periods
The plural form of user is users
The plural form of law is laws
The plural form of language is
2024-07-16 22:22:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 22:25:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2822, -1.6074, -0.0983,  ...,  0.4834, -0.2695, -0.3406],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0577, -2.6094, -0.5420,  ...,  2.3848,  2.8125,  3.0020],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.6830e-02, -2.1790e-02,  8.0261e-03,  ..., -7.7057e-03,
          6.3705e-03, -1.3153e-02],
        [-3.0060e-02, -3.8071e-03,  1.2695e-02,  ...,  1.2222e-02,
         -2.4475e-02,  9.9182e-05],
        [ 7.8583e-03,  1.5549e-02, -1.9958e-02,  ...,  1.2100e-02,
          7.6942e-03, -2.4796e-03],
        ...,
        [-1.5427e-02, -2.8439e-03, -6.9389e-03,  ..., -9.1705e-03,
          1.2299e-02, -1.5266e-02],
        [-7.4806e-03, -1.5396e-02, -1.1093e-02,  ..., -2.0416e-02,
         -1.5343e-02, -2.1744e-04],
        [ 1.2321e-02,  7.0076e-03, -1.2207e-03,  ..., -1.1337e-02,
          1.3542e-02,  2.9373e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3726, -1.9336, -0.6748,  ...,  1.7598,  3.1055,  2.2520]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 22:25:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of resource is resources
The plural form of event is events
The plural form of version is versions
The plural form of day is days
The plural form of period is periods
The plural form of user is users
The plural form of law is laws
The plural form of language is
2024-07-16 22:25:49 root INFO     [order_1_approx] starting weight calculation for The plural form of version is versions
The plural form of event is events
The plural form of day is days
The plural form of law is laws
The plural form of resource is resources
The plural form of language is languages
The plural form of user is users
The plural form of period is
2024-07-16 22:25:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 22:29:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4009,  0.5884, -1.0801,  ..., -1.0938,  0.6812,  0.5410],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6782,  0.2070, -0.8994,  ...,  2.1094,  1.9551,  0.3967],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0160,  0.0054,  0.0193,  ..., -0.0006, -0.0074,  0.0004],
        [-0.0114, -0.0067, -0.0073,  ...,  0.0163, -0.0008,  0.0015],
        [-0.0029, -0.0018, -0.0189,  ..., -0.0180,  0.0136,  0.0045],
        ...,
        [-0.0091, -0.0047,  0.0055,  ..., -0.0070,  0.0094, -0.0075],
        [ 0.0013,  0.0058, -0.0010,  ..., -0.0154, -0.0319,  0.0125],
        [-0.0026,  0.0129, -0.0009,  ..., -0.0021,  0.0182, -0.0149]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5020, -0.4043, -0.0635,  ...,  1.8320,  1.5918,  0.8721]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 22:29:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of version is versions
The plural form of event is events
The plural form of day is days
The plural form of law is laws
The plural form of resource is resources
The plural form of language is languages
The plural form of user is users
The plural form of period is
2024-07-16 22:29:35 root INFO     [order_1_approx] starting weight calculation for The plural form of event is events
The plural form of period is periods
The plural form of day is days
The plural form of language is languages
The plural form of law is laws
The plural form of user is users
The plural form of resource is resources
The plural form of version is
2024-07-16 22:29:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 22:33:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1328, -0.2065,  0.2666,  ...,  0.2986,  0.1616,  0.1704],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9834, -1.3291,  1.7217,  ...,  0.9863, -0.9927, -0.0615],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0105,  0.0003,  0.0166,  ...,  0.0073, -0.0006, -0.0105],
        [ 0.0020, -0.0159,  0.0077,  ...,  0.0120, -0.0106,  0.0098],
        [ 0.0169, -0.0081, -0.0156,  ..., -0.0035,  0.0305, -0.0035],
        ...,
        [-0.0128,  0.0009, -0.0063,  ..., -0.0160,  0.0037, -0.0138],
        [ 0.0064,  0.0143, -0.0016,  ..., -0.0109, -0.0128,  0.0033],
        [ 0.0046,  0.0257, -0.0051,  ..., -0.0015,  0.0126, -0.0086]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7334, -1.5527,  1.4727,  ...,  0.4048, -1.2773, -0.3872]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 22:33:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of event is events
The plural form of period is periods
The plural form of day is days
The plural form of language is languages
The plural form of law is laws
The plural form of user is users
The plural form of resource is resources
The plural form of version is
2024-07-16 22:33:20 root INFO     [order_1_approx] starting weight calculation for The plural form of version is versions
The plural form of language is languages
The plural form of period is periods
The plural form of event is events
The plural form of law is laws
The plural form of day is days
The plural form of resource is resources
The plural form of user is
2024-07-16 22:33:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 22:37:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4219, -0.6631, -0.5586,  ...,  1.2461,  0.2433,  0.7090],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.1172,  0.9854,  2.5938,  ..., -0.3237, -3.4473,  0.0156],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0168, -0.0035,  0.0352,  ...,  0.0118,  0.0050, -0.0033],
        [-0.0045, -0.0340, -0.0007,  ..., -0.0015,  0.0074, -0.0045],
        [ 0.0119,  0.0028, -0.0153,  ...,  0.0031,  0.0008, -0.0017],
        ...,
        [-0.0186,  0.0027, -0.0093,  ..., -0.0170, -0.0015, -0.0134],
        [-0.0003, -0.0010,  0.0081,  ..., -0.0036, -0.0268,  0.0017],
        [-0.0078,  0.0086, -0.0084,  ..., -0.0073,  0.0166, -0.0166]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.9893,  1.1533,  2.5156,  ..., -0.4731, -3.5195, -0.1101]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 22:37:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of version is versions
The plural form of language is languages
The plural form of period is periods
The plural form of event is events
The plural form of law is laws
The plural form of day is days
The plural form of resource is resources
The plural form of user is
2024-07-16 22:37:06 root INFO     [order_1_approx] starting weight calculation for The plural form of user is users
The plural form of period is periods
The plural form of language is languages
The plural form of law is laws
The plural form of resource is resources
The plural form of version is versions
The plural form of event is events
The plural form of day is
2024-07-16 22:37:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 22:40:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3350, -0.9507, -0.6455,  ..., -0.3755, -0.7197,  0.1418],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8574, -3.0488,  0.2539,  ..., -1.9297,  2.7871, -1.4922],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0069, -0.0127,  0.0109,  ...,  0.0051,  0.0008, -0.0110],
        [-0.0106, -0.0032,  0.0120,  ..., -0.0009,  0.0044, -0.0068],
        [ 0.0094,  0.0022, -0.0160,  ..., -0.0037,  0.0061, -0.0013],
        ...,
        [-0.0151,  0.0102, -0.0117,  ..., -0.0082, -0.0011,  0.0054],
        [-0.0030,  0.0048, -0.0061,  ..., -0.0076, -0.0254,  0.0181],
        [ 0.0017, -0.0048, -0.0068,  ..., -0.0155,  0.0195, -0.0291]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9429, -3.0625,  0.2593,  ..., -1.9170,  3.3633, -1.0459]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 22:40:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of user is users
The plural form of period is periods
The plural form of language is languages
The plural form of law is laws
The plural form of resource is resources
The plural form of version is versions
The plural form of event is events
The plural form of day is
2024-07-16 22:40:51 root INFO     [order_1_approx] starting weight calculation for The plural form of period is periods
The plural form of version is versions
The plural form of resource is resources
The plural form of event is events
The plural form of day is days
The plural form of language is languages
The plural form of user is users
The plural form of law is
2024-07-16 22:40:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 22:44:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2686, -0.2795,  0.7554,  ...,  0.3726, -0.0781,  0.3420],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1426,  0.5005, -2.6719,  ..., -1.6836,  1.4375,  0.9170],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0052,  0.0051,  0.0209,  ...,  0.0043,  0.0057, -0.0199],
        [-0.0008, -0.0063, -0.0070,  ...,  0.0007, -0.0013, -0.0033],
        [ 0.0100,  0.0034, -0.0023,  ...,  0.0054,  0.0112,  0.0054],
        ...,
        [-0.0141, -0.0060, -0.0035,  ...,  0.0072, -0.0017, -0.0126],
        [ 0.0148, -0.0058, -0.0018,  ..., -0.0099, -0.0085, -0.0025],
        [-0.0011,  0.0124,  0.0039,  ..., -0.0061,  0.0169,  0.0028]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5791,  0.8984, -2.5918,  ..., -2.5859,  1.2324,  1.4561]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 22:44:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of period is periods
The plural form of version is versions
The plural form of resource is resources
The plural form of event is events
The plural form of day is days
The plural form of language is languages
The plural form of user is users
The plural form of law is
2024-07-16 22:44:36 root INFO     total operator prediction time: 1804.6172654628754 seconds
2024-07-16 22:44:36 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_3pSg - Ved
2024-07-16 22:44:36 root INFO     building operator verb_3pSg - Ved
2024-07-16 22:44:36 root INFO     [order_1_approx] starting weight calculation for When he seems something, something has been seemed
When he tells something, something has been told
When he introduces something, something has been introduced
When he replaces something, something has been replaced
When he describes something, something has been described
When he relates something, something has been related
When he asks something, something has been asked
When he considers something, something has been
2024-07-16 22:44:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 22:48:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7363,  1.4258, -0.0470,  ..., -0.3181,  0.3538,  0.5796],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4922,  4.6797, -0.6050,  ..., -0.1875, -3.6289, -3.6641],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-6.9695e-03, -1.0147e-03,  3.0914e-02,  ...,  1.1749e-02,
         -5.1575e-03,  6.8130e-03],
        [-9.4604e-03,  6.2294e-03,  1.9073e-05,  ...,  5.8136e-03,
         -4.0245e-03, -7.9727e-04],
        [ 1.5202e-03,  2.1576e-02, -6.4964e-03,  ...,  1.1154e-02,
          7.8506e-03, -8.2588e-04],
        ...,
        [-1.3618e-02, -1.3000e-02, -1.3218e-03,  ..., -4.7073e-03,
         -8.9798e-03,  2.3460e-03],
        [ 1.8829e-02, -2.2888e-04,  4.3259e-03,  ..., -2.9251e-02,
         -2.0264e-02,  1.2283e-03],
        [-3.1567e-03,  2.1149e-02,  2.5826e-03,  ...,  2.8362e-03,
          2.0218e-02, -1.4374e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6934,  5.7031, -0.2717,  ..., -0.0588, -3.8828, -3.7812]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 22:48:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he seems something, something has been seemed
When he tells something, something has been told
When he introduces something, something has been introduced
When he replaces something, something has been replaced
When he describes something, something has been described
When he relates something, something has been related
When he asks something, something has been asked
When he considers something, something has been
2024-07-16 22:48:21 root INFO     [order_1_approx] starting weight calculation for When he considers something, something has been considered
When he seems something, something has been seemed
When he relates something, something has been related
When he replaces something, something has been replaced
When he introduces something, something has been introduced
When he asks something, something has been asked
When he tells something, something has been told
When he describes something, something has been
2024-07-16 22:48:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 22:52:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9941,  1.5039,  1.3887,  ..., -0.6396, -0.4319, -1.6348],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6221,  0.2510,  2.1152,  ..., -1.8428, -1.4189, -0.2666],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0030, -0.0129,  0.0023,  ..., -0.0032,  0.0018,  0.0014],
        [ 0.0001, -0.0087,  0.0136,  ..., -0.0043, -0.0042, -0.0016],
        [ 0.0049,  0.0054, -0.0083,  ...,  0.0009, -0.0020,  0.0084],
        ...,
        [-0.0273, -0.0088,  0.0131,  ...,  0.0018, -0.0056, -0.0098],
        [ 0.0014, -0.0008,  0.0025,  ..., -0.0182, -0.0266,  0.0056],
        [-0.0005,  0.0051,  0.0102,  ...,  0.0044,  0.0026, -0.0325]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4189,  0.3274,  1.4697,  ..., -2.4961, -1.6270, -1.4131]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 22:52:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he considers something, something has been considered
When he seems something, something has been seemed
When he relates something, something has been related
When he replaces something, something has been replaced
When he introduces something, something has been introduced
When he asks something, something has been asked
When he tells something, something has been told
When he describes something, something has been
2024-07-16 22:52:09 root INFO     [order_1_approx] starting weight calculation for When he relates something, something has been related
When he asks something, something has been asked
When he describes something, something has been described
When he seems something, something has been seemed
When he replaces something, something has been replaced
When he introduces something, something has been introduced
When he considers something, something has been considered
When he tells something, something has been
2024-07-16 22:52:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 22:55:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.6807,  0.7373,  0.6592,  ...,  0.9717, -0.0208, -0.4575],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([2.6406, 0.9062, 1.5996,  ..., 0.6357, 2.7148, 2.4180], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0002, -0.0122,  0.0039,  ...,  0.0017, -0.0043, -0.0131],
        [-0.0043,  0.0064,  0.0090,  ...,  0.0068,  0.0033,  0.0243],
        [ 0.0078,  0.0173, -0.0106,  ..., -0.0166,  0.0134,  0.0025],
        ...,
        [-0.0234, -0.0158, -0.0154,  ..., -0.0073, -0.0023, -0.0048],
        [ 0.0099,  0.0023, -0.0044,  ..., -0.0069, -0.0384, -0.0092],
        [-0.0173,  0.0287,  0.0173,  ...,  0.0008,  0.0103, -0.0309]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[2.6172, 1.1250, 1.8467,  ..., 1.0527, 1.9404, 2.4141]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 22:55:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he relates something, something has been related
When he asks something, something has been asked
When he describes something, something has been described
When he seems something, something has been seemed
When he replaces something, something has been replaced
When he introduces something, something has been introduced
When he considers something, something has been considered
When he tells something, something has been
2024-07-16 22:55:56 root INFO     [order_1_approx] starting weight calculation for When he describes something, something has been described
When he considers something, something has been considered
When he asks something, something has been asked
When he seems something, something has been seemed
When he tells something, something has been told
When he replaces something, something has been replaced
When he relates something, something has been related
When he introduces something, something has been
2024-07-16 22:55:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 22:59:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0098,  0.8330,  0.7871,  ...,  0.0822, -0.9316, -0.3699],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3984, -1.1846,  0.6348,  ...,  3.2754, -0.1600,  2.5703],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-9.8038e-03,  4.7302e-04,  9.0714e-03,  ...,  1.8402e-02,
         -1.1612e-02, -1.5526e-02],
        [ 2.0351e-03, -6.7520e-04, -4.9591e-05,  ...,  3.2463e-03,
          8.8882e-03,  1.3046e-02],
        [-7.3853e-03,  1.1772e-02,  2.7695e-03,  ..., -5.8136e-03,
         -6.5155e-03,  6.8283e-04],
        ...,
        [-6.7520e-03, -2.6646e-03, -8.9035e-03,  ...,  2.2621e-03,
         -1.4412e-02,  1.0742e-02],
        [-3.0060e-03,  1.5472e-02, -2.2221e-03,  ..., -2.1255e-02,
         -2.5146e-02, -5.2414e-03],
        [ 4.6387e-03,  9.5215e-03, -2.4338e-03,  ..., -4.1275e-03,
          1.2276e-02, -1.6785e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2446, -1.4482,  1.4219,  ...,  2.7109, -0.1913,  2.0547]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 22:59:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he describes something, something has been described
When he considers something, something has been considered
When he asks something, something has been asked
When he seems something, something has been seemed
When he tells something, something has been told
When he replaces something, something has been replaced
When he relates something, something has been related
When he introduces something, something has been
2024-07-16 22:59:44 root INFO     [order_1_approx] starting weight calculation for When he describes something, something has been described
When he tells something, something has been told
When he replaces something, something has been replaced
When he seems something, something has been seemed
When he relates something, something has been related
When he introduces something, something has been introduced
When he considers something, something has been considered
When he asks something, something has been
2024-07-16 22:59:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 23:03:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0869,  0.6221,  0.5342,  ...,  0.2383,  1.0684, -0.0703],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([2.5352, 3.9180, 0.3535,  ..., 1.7539, 1.4561, 2.3242], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0028, -0.0087,  0.0096,  ...,  0.0068, -0.0116, -0.0020],
        [-0.0101, -0.0069,  0.0045,  ..., -0.0006,  0.0091, -0.0053],
        [ 0.0071,  0.0035, -0.0228,  ..., -0.0014, -0.0056,  0.0071],
        ...,
        [-0.0182,  0.0025, -0.0041,  ..., -0.0063,  0.0065, -0.0039],
        [ 0.0151,  0.0002, -0.0004,  ..., -0.0062, -0.0291, -0.0011],
        [-0.0012,  0.0076,  0.0056,  ..., -0.0061,  0.0130, -0.0277]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[2.5664, 3.9863, 0.2072,  ..., 1.3486, 0.8535, 1.5830]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 23:03:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he describes something, something has been described
When he tells something, something has been told
When he replaces something, something has been replaced
When he seems something, something has been seemed
When he relates something, something has been related
When he introduces something, something has been introduced
When he considers something, something has been considered
When he asks something, something has been
2024-07-16 23:03:31 root INFO     [order_1_approx] starting weight calculation for When he considers something, something has been considered
When he asks something, something has been asked
When he relates something, something has been related
When he seems something, something has been seemed
When he describes something, something has been described
When he introduces something, something has been introduced
When he tells something, something has been told
When he replaces something, something has been
2024-07-16 23:03:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 23:07:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3984,  0.0807,  0.3511,  ...,  0.7090, -0.0597, -0.8022],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4414,  4.1875, -0.8325,  ...,  1.4980,  2.1426,  3.3457],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0169,  0.0016,  0.0021,  ..., -0.0102,  0.0035, -0.0086],
        [-0.0002, -0.0004, -0.0049,  ...,  0.0014,  0.0080,  0.0073],
        [-0.0039,  0.0055, -0.0103,  ..., -0.0033,  0.0023,  0.0105],
        ...,
        [-0.0277, -0.0036, -0.0075,  ..., -0.0121,  0.0042, -0.0029],
        [-0.0106, -0.0006, -0.0012,  ..., -0.0088, -0.0037, -0.0115],
        [ 0.0065,  0.0120, -0.0003,  ..., -0.0066,  0.0032, -0.0237]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.2754,  4.2578, -0.6084,  ...,  1.1992,  2.3945,  3.5234]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 23:07:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he considers something, something has been considered
When he asks something, something has been asked
When he relates something, something has been related
When he seems something, something has been seemed
When he describes something, something has been described
When he introduces something, something has been introduced
When he tells something, something has been told
When he replaces something, something has been
2024-07-16 23:07:18 root INFO     [order_1_approx] starting weight calculation for When he replaces something, something has been replaced
When he considers something, something has been considered
When he seems something, something has been seemed
When he describes something, something has been described
When he introduces something, something has been introduced
When he tells something, something has been told
When he asks something, something has been asked
When he relates something, something has been
2024-07-16 23:07:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 23:11:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9111,  1.1572,  1.7012,  ..., -0.8628, -0.0587, -0.6265],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.4688,  1.6699, -0.5088,  ..., -2.6270,  3.1641,  0.6953],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0115, -0.0013,  0.0115,  ..., -0.0063,  0.0024, -0.0128],
        [-0.0004, -0.0043,  0.0014,  ...,  0.0048,  0.0024,  0.0058],
        [ 0.0050, -0.0002, -0.0101,  ..., -0.0046, -0.0022,  0.0164],
        ...,
        [-0.0178, -0.0072, -0.0081,  ..., -0.0056,  0.0023, -0.0089],
        [ 0.0023,  0.0035,  0.0014,  ..., -0.0136, -0.0122,  0.0019],
        [ 0.0097,  0.0005, -0.0005,  ..., -0.0011,  0.0048, -0.0163]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.1611,  1.9023, -0.5938,  ..., -3.1016,  2.8125,  0.9268]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 23:11:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he replaces something, something has been replaced
When he considers something, something has been considered
When he seems something, something has been seemed
When he describes something, something has been described
When he introduces something, something has been introduced
When he tells something, something has been told
When he asks something, something has been asked
When he relates something, something has been
2024-07-16 23:11:02 root INFO     [order_1_approx] starting weight calculation for When he tells something, something has been told
When he replaces something, something has been replaced
When he introduces something, something has been introduced
When he describes something, something has been described
When he relates something, something has been related
When he asks something, something has been asked
When he considers something, something has been considered
When he seems something, something has been
2024-07-16 23:11:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 23:14:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.3613,  1.4209,  0.3237,  ...,  0.5308,  0.8276, -1.4014],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1387,  5.0156,  1.8779,  ..., -0.0933, -6.6367,  1.1582],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0026, -0.0084, -0.0098,  ...,  0.0125,  0.0030, -0.0057],
        [-0.0135, -0.0120,  0.0049,  ..., -0.0223,  0.0003, -0.0007],
        [ 0.0166,  0.0031, -0.0064,  ...,  0.0014,  0.0066,  0.0025],
        ...,
        [-0.0180, -0.0095,  0.0103,  ...,  0.0007, -0.0021, -0.0169],
        [-0.0063,  0.0098, -0.0134,  ..., -0.0091, -0.0078,  0.0145],
        [-0.0001,  0.0093,  0.0107,  ..., -0.0055, -0.0049, -0.0053]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.2266,  5.3945,  1.4902,  ..., -0.4714, -6.8281,  0.7520]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 23:14:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he tells something, something has been told
When he replaces something, something has been replaced
When he introduces something, something has been introduced
When he describes something, something has been described
When he relates something, something has been related
When he asks something, something has been asked
When he considers something, something has been considered
When he seems something, something has been
2024-07-16 23:14:47 root INFO     total operator prediction time: 1811.7520933151245 seconds
2024-07-16 23:14:47 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on adj - superlative
2024-07-16 23:14:47 root INFO     building operator adj - superlative
2024-07-16 23:14:47 root INFO     [order_1_approx] starting weight calculation for If something is the most strong, it is strongest
If something is the most hungry, it is hungriest
If something is the most sexy, it is sexiest
If something is the most happy, it is happiest
If something is the most lengthy, it is lengthiest
If something is the most costly, it is costliest
If something is the most pure, it is purest
If something is the most harsh, it is
2024-07-16 23:14:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 23:18:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0420, -2.0684, -0.4497,  ..., -0.2732,  1.3535, -0.6890],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.9219, -1.3965, -3.6348,  ...,  1.5273,  2.3652,  7.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0030, -0.0165,  0.0012,  ...,  0.0002, -0.0143, -0.0120],
        [-0.0007, -0.0153, -0.0034,  ..., -0.0044, -0.0165, -0.0131],
        [-0.0042,  0.0029, -0.0204,  ...,  0.0151,  0.0161,  0.0132],
        ...,
        [-0.0096, -0.0052,  0.0033,  ..., -0.0242, -0.0011,  0.0030],
        [-0.0017, -0.0049,  0.0170,  ..., -0.0054, -0.0244, -0.0012],
        [-0.0254,  0.0195,  0.0025,  ..., -0.0106, -0.0063, -0.0308]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.8789, -1.2285, -3.5430,  ...,  1.2480,  2.2812,  7.7188]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 23:18:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most strong, it is strongest
If something is the most hungry, it is hungriest
If something is the most sexy, it is sexiest
If something is the most happy, it is happiest
If something is the most lengthy, it is lengthiest
If something is the most costly, it is costliest
If something is the most pure, it is purest
If something is the most harsh, it is
2024-07-16 23:18:31 root INFO     [order_1_approx] starting weight calculation for If something is the most strong, it is strongest
If something is the most pure, it is purest
If something is the most happy, it is happiest
If something is the most sexy, it is sexiest
If something is the most hungry, it is hungriest
If something is the most costly, it is costliest
If something is the most harsh, it is harshest
If something is the most lengthy, it is
2024-07-16 23:18:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 23:22:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7930,  0.6060,  0.4397,  ..., -0.0542,  0.4346, -0.2292],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0391, -0.6406, -2.3477,  ...,  2.1152, -3.0273,  5.4531],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0068, -0.0088, -0.0099,  ..., -0.0039, -0.0047, -0.0073],
        [ 0.0029, -0.0016, -0.0090,  ..., -0.0021,  0.0023,  0.0100],
        [-0.0081,  0.0073, -0.0001,  ...,  0.0060,  0.0009,  0.0213],
        ...,
        [ 0.0098, -0.0046, -0.0129,  ..., -0.0266, -0.0134, -0.0002],
        [ 0.0077, -0.0163,  0.0045,  ..., -0.0004, -0.0233, -0.0039],
        [ 0.0047,  0.0036, -0.0142,  ...,  0.0033,  0.0058, -0.0062]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.8198, -0.2397, -1.2578,  ...,  2.2344, -3.0234,  4.9375]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 23:22:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most strong, it is strongest
If something is the most pure, it is purest
If something is the most happy, it is happiest
If something is the most sexy, it is sexiest
If something is the most hungry, it is hungriest
If something is the most costly, it is costliest
If something is the most harsh, it is harshest
If something is the most lengthy, it is
2024-07-16 23:22:17 root INFO     [order_1_approx] starting weight calculation for If something is the most hungry, it is hungriest
If something is the most sexy, it is sexiest
If something is the most happy, it is happiest
If something is the most harsh, it is harshest
If something is the most lengthy, it is lengthiest
If something is the most costly, it is costliest
If something is the most pure, it is purest
If something is the most strong, it is
2024-07-16 23:22:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 23:26:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1699, -0.7900,  1.1064,  ...,  0.5010,  1.9219, -1.0342],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4609, -1.1084, -1.6328,  ...,  0.8174, -0.5552,  4.4883],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0010, -0.0191, -0.0042,  ..., -0.0070, -0.0022, -0.0185],
        [-0.0109, -0.0114,  0.0123,  ..., -0.0074, -0.0134, -0.0025],
        [-0.0041,  0.0027, -0.0125,  ...,  0.0188,  0.0127, -0.0038],
        ...,
        [-0.0024, -0.0072,  0.0160,  ..., -0.0145, -0.0092, -0.0047],
        [-0.0028,  0.0106,  0.0051,  ..., -0.0047, -0.0062, -0.0007],
        [-0.0098,  0.0117, -0.0021,  ..., -0.0096,  0.0135, -0.0118]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4595, -0.5938, -1.5117,  ...,  0.7183, -1.3984,  4.1094]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 23:26:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most hungry, it is hungriest
If something is the most sexy, it is sexiest
If something is the most happy, it is happiest
If something is the most harsh, it is harshest
If something is the most lengthy, it is lengthiest
If something is the most costly, it is costliest
If something is the most pure, it is purest
If something is the most strong, it is
2024-07-16 23:26:01 root INFO     [order_1_approx] starting weight calculation for If something is the most lengthy, it is lengthiest
If something is the most sexy, it is sexiest
If something is the most hungry, it is hungriest
If something is the most happy, it is happiest
If something is the most strong, it is strongest
If something is the most pure, it is purest
If something is the most harsh, it is harshest
If something is the most costly, it is
2024-07-16 23:26:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 23:29:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1338, -0.9370, -0.6411,  ...,  1.1377,  1.9785,  0.2207],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.0898, -3.3398, -0.1953,  ..., -0.4844, -0.5195,  3.0371],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.6632e-03, -1.8768e-02,  1.7075e-02,  ...,  3.7460e-03,
         -7.1411e-03, -1.2772e-02],
        [-6.6605e-03, -8.7128e-03,  7.1602e-03,  ..., -6.1989e-03,
         -2.4891e-03, -1.6647e-02],
        [-1.0223e-02, -5.4932e-04, -1.0834e-02,  ..., -2.5368e-04,
          1.1719e-02,  1.2085e-02],
        ...,
        [ 3.2425e-05, -2.9030e-03, -1.9646e-03,  ..., -8.4152e-03,
          1.1238e-02, -1.0643e-02],
        [-3.4542e-03, -2.1915e-03,  1.7197e-02,  ...,  9.3079e-03,
         -3.6072e-02, -4.1199e-04],
        [-1.9943e-02,  9.6588e-03, -1.5182e-03,  ...,  8.4229e-03,
         -2.1744e-03, -4.1809e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.7285, -3.0957,  0.3213,  ..., -0.5439, -0.0195,  2.7793]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 23:29:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most lengthy, it is lengthiest
If something is the most sexy, it is sexiest
If something is the most hungry, it is hungriest
If something is the most happy, it is happiest
If something is the most strong, it is strongest
If something is the most pure, it is purest
If something is the most harsh, it is harshest
If something is the most costly, it is
2024-07-16 23:29:46 root INFO     [order_1_approx] starting weight calculation for If something is the most sexy, it is sexiest
If something is the most happy, it is happiest
If something is the most strong, it is strongest
If something is the most harsh, it is harshest
If something is the most hungry, it is hungriest
If something is the most costly, it is costliest
If something is the most lengthy, it is lengthiest
If something is the most pure, it is
2024-07-16 23:29:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 23:33:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2266, -1.2441, -0.0649,  ...,  0.0183,  1.5283, -1.0830],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4951,  0.8613, -4.4883,  ...,  1.8037,  0.1777,  2.6855],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 9.3460e-03, -2.0218e-02,  1.2100e-02,  ...,  2.5864e-03,
         -6.2561e-03, -9.3155e-03],
        [-2.1400e-03, -5.6839e-03, -2.0237e-03,  ..., -5.2452e-03,
         -3.6564e-03, -7.3395e-03],
        [ 5.4321e-03,  1.5936e-03, -2.8076e-03,  ..., -7.3776e-03,
         -7.2327e-03,  8.0490e-03],
        ...,
        [ 9.3365e-04,  4.7073e-03,  3.9215e-03,  ..., -1.2680e-02,
         -1.5850e-03, -1.7357e-03],
        [-2.5063e-03, -9.7275e-05,  1.2207e-02,  ..., -1.2417e-03,
         -9.4299e-03,  3.1166e-03],
        [-2.9617e-02, -6.6338e-03,  3.5248e-03,  ..., -6.2599e-03,
          1.0643e-02, -6.5689e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4395,  0.5591, -3.9551,  ...,  1.3545,  0.0189,  3.1953]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 23:33:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most sexy, it is sexiest
If something is the most happy, it is happiest
If something is the most strong, it is strongest
If something is the most harsh, it is harshest
If something is the most hungry, it is hungriest
If something is the most costly, it is costliest
If something is the most lengthy, it is lengthiest
If something is the most pure, it is
2024-07-16 23:33:31 root INFO     [order_1_approx] starting weight calculation for If something is the most strong, it is strongest
If something is the most lengthy, it is lengthiest
If something is the most hungry, it is hungriest
If something is the most costly, it is costliest
If something is the most happy, it is happiest
If something is the most harsh, it is harshest
If something is the most pure, it is purest
If something is the most sexy, it is
2024-07-16 23:33:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 23:37:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3159,  0.0601,  0.0225,  ...,  0.6787,  1.5273, -0.6743],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5605,  1.1904,  2.2500,  ..., -0.8999, -1.5049,  4.7109],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0037, -0.0282, -0.0001,  ...,  0.0017,  0.0065, -0.0028],
        [-0.0064, -0.0082,  0.0027,  ...,  0.0027, -0.0151, -0.0039],
        [-0.0061,  0.0001, -0.0169,  ...,  0.0055,  0.0057,  0.0058],
        ...,
        [-0.0091, -0.0096, -0.0026,  ..., -0.0138,  0.0036, -0.0098],
        [-0.0069, -0.0136, -0.0008,  ..., -0.0003, -0.0179,  0.0039],
        [-0.0288, -0.0039,  0.0040,  ...,  0.0015, -0.0099, -0.0060]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.1650,  1.6318,  1.8672,  ..., -0.9478, -1.5742,  5.6172]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 23:37:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most strong, it is strongest
If something is the most lengthy, it is lengthiest
If something is the most hungry, it is hungriest
If something is the most costly, it is costliest
If something is the most happy, it is happiest
If something is the most harsh, it is harshest
If something is the most pure, it is purest
If something is the most sexy, it is
2024-07-16 23:37:16 root INFO     [order_1_approx] starting weight calculation for If something is the most costly, it is costliest
If something is the most hungry, it is hungriest
If something is the most harsh, it is harshest
If something is the most lengthy, it is lengthiest
If something is the most strong, it is strongest
If something is the most sexy, it is sexiest
If something is the most pure, it is purest
If something is the most happy, it is
2024-07-16 23:37:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 23:41:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0206,  0.5474,  0.5122,  ...,  0.9980,  1.9023, -0.3320],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7129, -0.2437, -2.5117,  ..., -0.2905,  4.1953,  4.0352],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0053, -0.0218,  0.0128,  ...,  0.0064, -0.0112, -0.0042],
        [ 0.0097, -0.0086,  0.0024,  ...,  0.0042, -0.0036, -0.0073],
        [-0.0026,  0.0117, -0.0007,  ...,  0.0051,  0.0133, -0.0041],
        ...,
        [-0.0078, -0.0056, -0.0039,  ..., -0.0062, -0.0002, -0.0012],
        [-0.0060, -0.0025, -0.0027,  ..., -0.0077, -0.0266, -0.0013],
        [-0.0041,  0.0010,  0.0115,  ...,  0.0110,  0.0038, -0.0087]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5957, -0.5405, -2.1328,  ...,  0.2168,  4.2227,  4.0430]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 23:41:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most costly, it is costliest
If something is the most hungry, it is hungriest
If something is the most harsh, it is harshest
If something is the most lengthy, it is lengthiest
If something is the most strong, it is strongest
If something is the most sexy, it is sexiest
If something is the most pure, it is purest
If something is the most happy, it is
2024-07-16 23:41:02 root INFO     [order_1_approx] starting weight calculation for If something is the most costly, it is costliest
If something is the most strong, it is strongest
If something is the most sexy, it is sexiest
If something is the most happy, it is happiest
If something is the most lengthy, it is lengthiest
If something is the most pure, it is purest
If something is the most harsh, it is harshest
If something is the most hungry, it is
2024-07-16 23:41:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 23:44:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4131, -1.3262, -0.1025,  ...,  1.3418,  2.1484,  0.0546],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.7891,  0.1208, -3.8203,  ...,  0.2954,  2.9238,  3.0996],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0097, -0.0253,  0.0048,  ..., -0.0044, -0.0101, -0.0049],
        [-0.0072, -0.0017, -0.0074,  ...,  0.0041, -0.0088, -0.0035],
        [-0.0033,  0.0065, -0.0083,  ...,  0.0096,  0.0081,  0.0105],
        ...,
        [-0.0095, -0.0090,  0.0061,  ..., -0.0034,  0.0091, -0.0046],
        [-0.0095, -0.0124, -0.0009,  ..., -0.0087, -0.0176, -0.0045],
        [-0.0273,  0.0064,  0.0098,  ..., -0.0085, -0.0036, -0.0162]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.6602,  1.1035, -3.9609,  ..., -0.0400,  2.6172,  4.4844]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 23:44:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most costly, it is costliest
If something is the most strong, it is strongest
If something is the most sexy, it is sexiest
If something is the most happy, it is happiest
If something is the most lengthy, it is lengthiest
If something is the most pure, it is purest
If something is the most harsh, it is harshest
If something is the most hungry, it is
2024-07-16 23:44:46 root INFO     total operator prediction time: 1798.9302146434784 seconds
2024-07-16 23:44:46 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+er_irreg
2024-07-16 23:44:46 root INFO     building operator verb+er_irreg
2024-07-16 23:44:46 root INFO     [order_1_approx] starting weight calculation for If you subscribe something, you are a subscriber
If you explore something, you are a explorer
If you manage something, you are a manager
If you compose something, you are a composer
If you eat something, you are a eater
If you offend something, you are a offender
If you lose something, you are a loser
If you interpret something, you are a
2024-07-16 23:44:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 23:48:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3711,  0.3567, -0.5107,  ..., -0.7349,  1.0879,  0.3379],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4912,  1.0713, -1.5488,  ..., -3.5645,  1.0449,  5.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 6.7444e-03,  4.4632e-03,  6.3934e-03,  ...,  8.0109e-05,
          9.0637e-03,  2.1667e-03],
        [ 5.9280e-03,  1.0925e-02,  5.5313e-03,  ..., -3.7041e-03,
         -4.6692e-03, -7.4310e-03],
        [ 3.1853e-03,  1.5755e-03,  5.8784e-03,  ...,  1.2207e-03,
          3.5686e-03,  6.2637e-03],
        ...,
        [-4.0131e-03, -1.9913e-02, -7.9422e-03,  ...,  1.0086e-02,
          7.0095e-04, -4.2763e-03],
        [-3.1700e-03,  2.5063e-03,  1.3237e-02,  ...,  2.8992e-04,
         -6.3133e-03, -1.1795e-02],
        [-1.1047e-02,  5.0735e-04,  6.4468e-03,  ..., -1.6693e-02,
          1.5823e-02,  5.1308e-04]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0645,  0.7793, -1.8281,  ..., -3.7578,  1.1875,  5.7031]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 23:48:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you subscribe something, you are a subscriber
If you explore something, you are a explorer
If you manage something, you are a manager
If you compose something, you are a composer
If you eat something, you are a eater
If you offend something, you are a offender
If you lose something, you are a loser
If you interpret something, you are a
2024-07-16 23:48:33 root INFO     [order_1_approx] starting weight calculation for If you explore something, you are a explorer
If you lose something, you are a loser
If you manage something, you are a manager
If you compose something, you are a composer
If you eat something, you are a eater
If you interpret something, you are a interpreter
If you subscribe something, you are a subscriber
If you offend something, you are a
2024-07-16 23:48:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 23:52:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.7266, -1.1504,  0.4966,  ...,  0.2859,  0.9502,  0.2231],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1445,  1.9863, -2.2871,  ..., -1.1816,  4.1953,  0.5107],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0109, -0.0040,  0.0085,  ...,  0.0009,  0.0106,  0.0034],
        [-0.0010, -0.0017, -0.0036,  ...,  0.0005, -0.0003, -0.0060],
        [-0.0013,  0.0148, -0.0022,  ...,  0.0109,  0.0038, -0.0072],
        ...,
        [ 0.0052, -0.0150, -0.0061,  ..., -0.0047,  0.0036,  0.0121],
        [ 0.0064, -0.0044,  0.0084,  ...,  0.0056, -0.0113,  0.0108],
        [-0.0025,  0.0094,  0.0035,  ..., -0.0154,  0.0118, -0.0114]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7295,  1.8291, -2.7246,  ..., -0.9277,  3.9844,  0.9263]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 23:52:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you explore something, you are a explorer
If you lose something, you are a loser
If you manage something, you are a manager
If you compose something, you are a composer
If you eat something, you are a eater
If you interpret something, you are a interpreter
If you subscribe something, you are a subscriber
If you offend something, you are a
2024-07-16 23:52:20 root INFO     [order_1_approx] starting weight calculation for If you interpret something, you are a interpreter
If you offend something, you are a offender
If you manage something, you are a manager
If you explore something, you are a explorer
If you lose something, you are a loser
If you compose something, you are a composer
If you eat something, you are a eater
If you subscribe something, you are a
2024-07-16 23:52:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 23:56:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.8213, -0.0074, -0.4167,  ...,  1.5371, -0.5142,  0.8115],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6309,  0.5244, -3.0859,  ..., -2.8906, -0.3848,  2.0547],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-9.3460e-05, -1.1711e-02,  2.2030e-03,  ...,  9.4299e-03,
         -1.4877e-04,  5.9700e-04],
        [-9.2316e-04, -2.5997e-03,  7.7534e-04,  ..., -3.8624e-04,
          2.0428e-03, -1.1131e-02],
        [-8.8959e-03,  7.3624e-03, -1.0223e-02,  ...,  2.2488e-03,
         -9.8114e-03,  7.3280e-03],
        ...,
        [-1.5312e-02, -1.3275e-02, -3.3054e-03,  ..., -1.2512e-02,
         -6.4468e-03, -2.2011e-03],
        [ 8.8959e-03, -5.0964e-03, -5.3520e-03,  ..., -4.8370e-03,
         -1.4061e-02,  1.2848e-02],
        [-1.5850e-03,  1.9699e-02,  1.3123e-03,  ..., -3.3703e-03,
          7.2250e-03, -1.4877e-04]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4355,  0.4429, -2.9648,  ..., -3.1367, -0.5234,  1.8105]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 23:56:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you interpret something, you are a interpreter
If you offend something, you are a offender
If you manage something, you are a manager
If you explore something, you are a explorer
If you lose something, you are a loser
If you compose something, you are a composer
If you eat something, you are a eater
If you subscribe something, you are a
2024-07-16 23:56:09 root INFO     [order_1_approx] starting weight calculation for If you lose something, you are a loser
If you offend something, you are a offender
If you explore something, you are a explorer
If you subscribe something, you are a subscriber
If you compose something, you are a composer
If you interpret something, you are a interpreter
If you eat something, you are a eater
If you manage something, you are a
2024-07-16 23:56:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-16 23:59:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4514,  0.7217, -0.0484,  ...,  0.1075, -0.3779,  0.3579],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1770, -1.8252, -3.0898,  ...,  0.5967, -0.2822,  4.9688],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0026,  0.0042, -0.0005,  ..., -0.0033,  0.0009, -0.0110],
        [-0.0004,  0.0007,  0.0044,  ..., -0.0062, -0.0090,  0.0022],
        [-0.0037,  0.0105, -0.0063,  ...,  0.0151, -0.0021, -0.0019],
        ...,
        [ 0.0020, -0.0191,  0.0006,  ..., -0.0017, -0.0062,  0.0048],
        [ 0.0211, -0.0019,  0.0042,  ..., -0.0097, -0.0140,  0.0020],
        [-0.0061,  0.0161,  0.0008,  ..., -0.0101,  0.0128, -0.0040]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5068, -1.2949, -2.7070,  ...,  0.6792, -0.0330,  5.1211]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 23:59:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you lose something, you are a loser
If you offend something, you are a offender
If you explore something, you are a explorer
If you subscribe something, you are a subscriber
If you compose something, you are a composer
If you interpret something, you are a interpreter
If you eat something, you are a eater
If you manage something, you are a
2024-07-16 23:59:58 root INFO     [order_1_approx] starting weight calculation for If you offend something, you are a offender
If you explore something, you are a explorer
If you subscribe something, you are a subscriber
If you interpret something, you are a interpreter
If you manage something, you are a manager
If you lose something, you are a loser
If you eat something, you are a eater
If you compose something, you are a
2024-07-16 23:59:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 00:03:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0111,  0.3579, -0.9277,  ..., -0.5674,  0.5840, -0.1248],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.8066,  0.9814, -5.0000,  ..., -0.5420, -0.6387,  1.9453],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0004, -0.0052,  0.0057,  ...,  0.0038, -0.0025,  0.0005],
        [-0.0057,  0.0024,  0.0146,  ...,  0.0011,  0.0041, -0.0089],
        [ 0.0166,  0.0117, -0.0083,  ..., -0.0014, -0.0051,  0.0097],
        ...,
        [-0.0146, -0.0121, -0.0013,  ..., -0.0099,  0.0138,  0.0015],
        [ 0.0163,  0.0045,  0.0021,  ..., -0.0084, -0.0017,  0.0060],
        [-0.0079,  0.0080,  0.0005,  ..., -0.0125,  0.0119,  0.0019]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.3438,  0.8110, -4.5508,  ..., -0.8896, -0.1917,  2.3750]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 00:03:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you offend something, you are a offender
If you explore something, you are a explorer
If you subscribe something, you are a subscriber
If you interpret something, you are a interpreter
If you manage something, you are a manager
If you lose something, you are a loser
If you eat something, you are a eater
If you compose something, you are a
2024-07-17 00:03:45 root INFO     [order_1_approx] starting weight calculation for If you offend something, you are a offender
If you compose something, you are a composer
If you manage something, you are a manager
If you interpret something, you are a interpreter
If you subscribe something, you are a subscriber
If you eat something, you are a eater
If you explore something, you are a explorer
If you lose something, you are a
2024-07-17 00:03:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 00:07:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2715, -0.5337,  0.3960,  ...,  0.1458, -0.7759,  0.7666],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.4922,  1.4355, -3.1113,  ...,  0.1104, -0.2354,  2.2871],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0137, -0.0181, -0.0006,  ..., -0.0004,  0.0016, -0.0101],
        [-0.0019, -0.0101,  0.0060,  ..., -0.0141,  0.0086, -0.0088],
        [-0.0044,  0.0143,  0.0008,  ...,  0.0055, -0.0064,  0.0039],
        ...,
        [ 0.0003, -0.0232,  0.0092,  ...,  0.0027, -0.0027,  0.0059],
        [ 0.0076,  0.0033, -0.0020,  ..., -0.0019, -0.0058,  0.0014],
        [-0.0055,  0.0011,  0.0037,  ..., -0.0136,  0.0122, -0.0042]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.8125,  1.9824, -3.0840,  ...,  0.1790, -0.9131,  2.2090]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 00:07:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you offend something, you are a offender
If you compose something, you are a composer
If you manage something, you are a manager
If you interpret something, you are a interpreter
If you subscribe something, you are a subscriber
If you eat something, you are a eater
If you explore something, you are a explorer
If you lose something, you are a
2024-07-17 00:07:33 root INFO     [order_1_approx] starting weight calculation for If you lose something, you are a loser
If you offend something, you are a offender
If you manage something, you are a manager
If you eat something, you are a eater
If you interpret something, you are a interpreter
If you compose something, you are a composer
If you subscribe something, you are a subscriber
If you explore something, you are a
2024-07-17 00:07:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 00:11:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8882, -0.2329,  0.0157,  ..., -0.5078,  1.1104,  0.0679],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.9883,  2.5664, -2.4688,  ..., -6.1445, -1.9053,  4.6602],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0075, -0.0059, -0.0053,  ..., -0.0038,  0.0018, -0.0027],
        [-0.0143,  0.0024, -0.0004,  ...,  0.0034, -0.0006,  0.0048],
        [ 0.0008,  0.0169, -0.0069,  ...,  0.0084, -0.0007,  0.0050],
        ...,
        [ 0.0056, -0.0178, -0.0089,  ...,  0.0056, -0.0088, -0.0064],
        [ 0.0184, -0.0025, -0.0019,  ..., -0.0080, -0.0216, -0.0032],
        [-0.0021,  0.0215,  0.0100,  ..., -0.0136,  0.0039, -0.0046]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.0312,  2.8691, -2.3281,  ..., -6.0547, -1.4277,  4.3867]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 00:11:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you lose something, you are a loser
If you offend something, you are a offender
If you manage something, you are a manager
If you eat something, you are a eater
If you interpret something, you are a interpreter
If you compose something, you are a composer
If you subscribe something, you are a subscriber
If you explore something, you are a
2024-07-17 00:11:20 root INFO     [order_1_approx] starting weight calculation for If you subscribe something, you are a subscriber
If you explore something, you are a explorer
If you compose something, you are a composer
If you offend something, you are a offender
If you lose something, you are a loser
If you manage something, you are a manager
If you interpret something, you are a interpreter
If you eat something, you are a
2024-07-17 00:11:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 00:15:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3320,  0.0668, -0.4268,  ..., -0.2129,  0.0988,  0.7578],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7285, -0.3865, -1.2871,  ..., -0.7188,  2.0703, -1.9180],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0031,  0.0022,  0.0004,  ..., -0.0046, -0.0020, -0.0002],
        [ 0.0033, -0.0035,  0.0088,  ..., -0.0047, -0.0025, -0.0075],
        [-0.0023,  0.0138, -0.0018,  ...,  0.0076, -0.0028,  0.0057],
        ...,
        [-0.0050,  0.0027,  0.0036,  ..., -0.0017, -0.0109,  0.0042],
        [ 0.0017, -0.0067, -0.0040,  ...,  0.0080, -0.0004, -0.0027],
        [-0.0074,  0.0042,  0.0082,  ..., -0.0079,  0.0173,  0.0003]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6294, -0.1455, -1.4521,  ..., -0.4988,  2.0449, -1.8262]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 00:15:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you subscribe something, you are a subscriber
If you explore something, you are a explorer
If you compose something, you are a composer
If you offend something, you are a offender
If you lose something, you are a loser
If you manage something, you are a manager
If you interpret something, you are a interpreter
If you eat something, you are a
2024-07-17 00:15:07 root INFO     total operator prediction time: 1821.0675749778748 seconds
2024-07-17 00:15:07 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on over+adj_reg
2024-07-17 00:15:07 root INFO     building operator over+adj_reg
2024-07-17 00:15:08 root INFO     [order_1_approx] starting weight calculation for If something is too taken, it is overtaken
If something is too saturated, it is oversaturated
If something is too ambitious, it is overambitious
If something is too used, it is overused
If something is too laid, it is overlaid
If something is too written, it is overwritten
If something is too stimulated, it is overstimulated
If something is too filled, it is
2024-07-17 00:15:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 00:18:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0559,  0.8706,  0.9253,  ...,  0.1406,  1.4160, -0.7710],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5229,  0.0120, -0.8496,  ...,  2.0938,  2.3184,  1.0273],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0227, -0.0160,  0.0045,  ...,  0.0043, -0.0117, -0.0072],
        [-0.0182, -0.0064, -0.0039,  ..., -0.0049, -0.0130, -0.0055],
        [ 0.0116, -0.0024, -0.0087,  ..., -0.0155,  0.0073,  0.0117],
        ...,
        [-0.0063, -0.0060,  0.0058,  ..., -0.0033, -0.0195, -0.0053],
        [-0.0065, -0.0067,  0.0097,  ...,  0.0054, -0.0153, -0.0328],
        [-0.0019,  0.0049,  0.0009,  ..., -0.0106,  0.0009,  0.0125]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2725,  0.5928, -1.0820,  ...,  2.1641,  2.1230,  1.5469]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 00:18:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too taken, it is overtaken
If something is too saturated, it is oversaturated
If something is too ambitious, it is overambitious
If something is too used, it is overused
If something is too laid, it is overlaid
If something is too written, it is overwritten
If something is too stimulated, it is overstimulated
If something is too filled, it is
2024-07-17 00:18:55 root INFO     [order_1_approx] starting weight calculation for If something is too saturated, it is oversaturated
If something is too laid, it is overlaid
If something is too filled, it is overfilled
If something is too taken, it is overtaken
If something is too used, it is overused
If something is too written, it is overwritten
If something is too ambitious, it is overambitious
If something is too stimulated, it is
2024-07-17 00:18:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 00:22:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.3398, 0.4746, 0.6768,  ..., 0.9707, 1.2490, 0.0752], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3228, -1.2266,  1.3525,  ...,  0.5820,  1.2051,  4.6055],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0114, -0.0238,  0.0153,  ...,  0.0062, -0.0036,  0.0045],
        [-0.0128,  0.0080, -0.0002,  ...,  0.0044, -0.0066, -0.0131],
        [-0.0071, -0.0008, -0.0060,  ..., -0.0018, -0.0059,  0.0150],
        ...,
        [-0.0185,  0.0011, -0.0009,  ..., -0.0044, -0.0108,  0.0069],
        [-0.0214, -0.0164,  0.0118,  ...,  0.0020, -0.0050, -0.0038],
        [ 0.0052,  0.0012, -0.0068,  ..., -0.0273, -0.0024, -0.0061]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0303, -0.6636,  1.6436,  ...,  0.6245,  0.3384,  4.8555]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 00:22:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too saturated, it is oversaturated
If something is too laid, it is overlaid
If something is too filled, it is overfilled
If something is too taken, it is overtaken
If something is too used, it is overused
If something is too written, it is overwritten
If something is too ambitious, it is overambitious
If something is too stimulated, it is
2024-07-17 00:22:42 root INFO     [order_1_approx] starting weight calculation for If something is too filled, it is overfilled
If something is too saturated, it is oversaturated
If something is too laid, it is overlaid
If something is too written, it is overwritten
If something is too used, it is overused
If something is too stimulated, it is overstimulated
If something is too taken, it is overtaken
If something is too ambitious, it is
2024-07-17 00:22:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 00:26:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2610, -0.7627,  0.2517,  ..., -0.8701, -0.3252, -0.2271],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.9297,  2.1406, -1.3623,  ...,  2.6211,  2.9727,  3.2480],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0222, -0.0251,  0.0146,  ..., -0.0125, -0.0223, -0.0069],
        [-0.0106, -0.0048, -0.0082,  ..., -0.0021, -0.0110,  0.0016],
        [ 0.0046,  0.0242, -0.0043,  ..., -0.0037,  0.0122,  0.0113],
        ...,
        [-0.0089, -0.0238, -0.0118,  ..., -0.0031, -0.0208, -0.0088],
        [-0.0134,  0.0069,  0.0066,  ..., -0.0130, -0.0166, -0.0045],
        [-0.0027,  0.0089,  0.0153,  ...,  0.0044, -0.0079, -0.0201]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.8379,  2.6758, -1.8574,  ...,  3.3301,  2.5234,  3.8047]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 00:26:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too filled, it is overfilled
If something is too saturated, it is oversaturated
If something is too laid, it is overlaid
If something is too written, it is overwritten
If something is too used, it is overused
If something is too stimulated, it is overstimulated
If something is too taken, it is overtaken
If something is too ambitious, it is
2024-07-17 00:26:30 root INFO     [order_1_approx] starting weight calculation for If something is too written, it is overwritten
If something is too taken, it is overtaken
If something is too used, it is overused
If something is too filled, it is overfilled
If something is too saturated, it is oversaturated
If something is too ambitious, it is overambitious
If something is too stimulated, it is overstimulated
If something is too laid, it is
2024-07-17 00:26:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 00:30:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7720,  0.3647,  0.3220,  ...,  2.3086,  0.6816, -1.0898],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.3008,  0.6396, -2.5508,  ..., -1.1133,  0.3352,  3.6562],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0043, -0.0199,  0.0162,  ...,  0.0026, -0.0220,  0.0052],
        [-0.0132, -0.0087, -0.0128,  ..., -0.0209, -0.0040,  0.0026],
        [-0.0011,  0.0005, -0.0056,  ..., -0.0041,  0.0043, -0.0037],
        ...,
        [-0.0083, -0.0257,  0.0018,  ...,  0.0111, -0.0036,  0.0070],
        [-0.0107,  0.0016, -0.0012,  ..., -0.0142, -0.0200, -0.0019],
        [-0.0039,  0.0088,  0.0017,  ..., -0.0132,  0.0016, -0.0103]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.1406,  1.0752, -2.6582,  ..., -1.5547,  0.8770,  3.5938]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 00:30:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too written, it is overwritten
If something is too taken, it is overtaken
If something is too used, it is overused
If something is too filled, it is overfilled
If something is too saturated, it is oversaturated
If something is too ambitious, it is overambitious
If something is too stimulated, it is overstimulated
If something is too laid, it is
2024-07-17 00:30:17 root INFO     [order_1_approx] starting weight calculation for If something is too ambitious, it is overambitious
If something is too saturated, it is oversaturated
If something is too written, it is overwritten
If something is too laid, it is overlaid
If something is too filled, it is overfilled
If something is too used, it is overused
If something is too stimulated, it is overstimulated
If something is too taken, it is
2024-07-17 00:30:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 00:34:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2089,  0.0909,  0.5527,  ...,  0.0533,  1.4863, -0.1074],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3711, -2.4746,  1.1367,  ...,  1.3193,  2.0918,  3.4355],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.0468e-02, -4.3793e-03,  1.9928e-02,  ...,  8.0719e-03,
         -9.4528e-03, -1.4137e-02],
        [-1.1108e-02, -7.2098e-03,  4.8027e-03,  ..., -4.2801e-03,
          8.4839e-03,  1.1444e-05],
        [-7.0343e-03,  4.8485e-03, -1.6418e-02,  ..., -4.1122e-03,
          1.9257e-02,  1.0178e-02],
        ...,
        [-9.0408e-03, -1.0727e-02, -6.5613e-03,  ...,  1.0529e-02,
         -2.6093e-02, -2.6581e-02],
        [-1.7380e-02,  2.0943e-03,  1.6663e-02,  ..., -1.4427e-02,
         -3.4790e-02,  1.2932e-02],
        [-4.6730e-04,  1.5083e-02, -7.7133e-03,  ..., -1.1543e-02,
         -7.3776e-03, -1.8311e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4561, -1.9707,  1.7422,  ...,  1.0547,  3.1211,  4.4219]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 00:34:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too ambitious, it is overambitious
If something is too saturated, it is oversaturated
If something is too written, it is overwritten
If something is too laid, it is overlaid
If something is too filled, it is overfilled
If something is too used, it is overused
If something is too stimulated, it is overstimulated
If something is too taken, it is
2024-07-17 00:34:03 root INFO     [order_1_approx] starting weight calculation for If something is too written, it is overwritten
If something is too ambitious, it is overambitious
If something is too laid, it is overlaid
If something is too filled, it is overfilled
If something is too used, it is overused
If something is too stimulated, it is overstimulated
If something is too taken, it is overtaken
If something is too saturated, it is
2024-07-17 00:34:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 00:37:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2629,  0.3042,  2.2266,  ..., -0.1216,  0.4861, -0.3223],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2363,  0.6699, -0.5254,  ...,  0.5137,  1.5947,  0.6934],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0100, -0.0089,  0.0078,  ...,  0.0002, -0.0146, -0.0206],
        [-0.0046, -0.0022,  0.0071,  ...,  0.0014, -0.0152, -0.0004],
        [ 0.0120,  0.0041, -0.0066,  ..., -0.0026,  0.0147,  0.0152],
        ...,
        [ 0.0023,  0.0008, -0.0028,  ...,  0.0002, -0.0090,  0.0006],
        [-0.0077, -0.0112,  0.0077,  ..., -0.0018, -0.0060, -0.0185],
        [ 0.0052, -0.0095,  0.0039,  ..., -0.0081,  0.0073,  0.0097]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4238,  1.0459, -1.0400,  ...,  0.6680,  1.4346,  1.1475]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 00:37:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too written, it is overwritten
If something is too ambitious, it is overambitious
If something is too laid, it is overlaid
If something is too filled, it is overfilled
If something is too used, it is overused
If something is too stimulated, it is overstimulated
If something is too taken, it is overtaken
If something is too saturated, it is
2024-07-17 00:37:49 root INFO     [order_1_approx] starting weight calculation for If something is too filled, it is overfilled
If something is too stimulated, it is overstimulated
If something is too saturated, it is oversaturated
If something is too taken, it is overtaken
If something is too laid, it is overlaid
If something is too ambitious, it is overambitious
If something is too used, it is overused
If something is too written, it is
2024-07-17 00:37:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 00:41:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1912,  0.2849,  1.1719,  ...,  0.4321,  1.3799, -0.5728],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7344, -1.3965, -1.8545,  ...,  2.4844,  2.8027,  0.7290],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0036, -0.0208, -0.0026,  ...,  0.0038, -0.0117, -0.0031],
        [-0.0095,  0.0007, -0.0028,  ..., -0.0003, -0.0035, -0.0074],
        [ 0.0133,  0.0013, -0.0044,  ...,  0.0035,  0.0129,  0.0044],
        ...,
        [ 0.0018,  0.0048,  0.0012,  ..., -0.0105, -0.0042, -0.0006],
        [-0.0121, -0.0015, -0.0003,  ...,  0.0003, -0.0255,  0.0007],
        [ 0.0044,  0.0054,  0.0075,  ..., -0.0047,  0.0098,  0.0073]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.0586, -1.6846, -1.8105,  ...,  2.9785,  2.9531,  0.4275]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 00:41:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too filled, it is overfilled
If something is too stimulated, it is overstimulated
If something is too saturated, it is oversaturated
If something is too taken, it is overtaken
If something is too laid, it is overlaid
If something is too ambitious, it is overambitious
If something is too used, it is overused
If something is too written, it is
2024-07-17 00:41:35 root INFO     [order_1_approx] starting weight calculation for If something is too ambitious, it is overambitious
If something is too stimulated, it is overstimulated
If something is too filled, it is overfilled
If something is too laid, it is overlaid
If something is too taken, it is overtaken
If something is too written, it is overwritten
If something is too saturated, it is oversaturated
If something is too used, it is
2024-07-17 00:41:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 00:45:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3403,  0.7998,  1.8721,  ...,  0.4954,  0.8857, -0.1116],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1973,  0.2617, -0.0820,  ...,  1.2119,  0.7383,  2.9355],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0084, -0.0221,  0.0177,  ..., -0.0068, -0.0195, -0.0096],
        [-0.0143, -0.0081,  0.0060,  ..., -0.0145, -0.0082, -0.0034],
        [ 0.0048,  0.0085, -0.0129,  ..., -0.0100,  0.0124,  0.0200],
        ...,
        [ 0.0056,  0.0008, -0.0067,  ..., -0.0043,  0.0010,  0.0081],
        [-0.0141, -0.0006,  0.0049,  ...,  0.0017, -0.0309,  0.0003],
        [-0.0162,  0.0139,  0.0053,  ..., -0.0203, -0.0083, -0.0015]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[2.1406, 0.7397, 0.6655,  ..., 1.2891, 1.1582, 3.5664]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 00:45:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too ambitious, it is overambitious
If something is too stimulated, it is overstimulated
If something is too filled, it is overfilled
If something is too laid, it is overlaid
If something is too taken, it is overtaken
If something is too written, it is overwritten
If something is too saturated, it is oversaturated
If something is too used, it is
2024-07-17 00:45:20 root INFO     total operator prediction time: 1812.939250946045 seconds
2024-07-17 00:45:20 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on adj+ly_reg
2024-07-17 00:45:20 root INFO     building operator adj+ly_reg
2024-07-17 00:45:20 root INFO     [order_1_approx] starting weight calculation for The adjective form of mental is mentally
The adjective form of huge is hugely
The adjective form of global is globally
The adjective form of effective is effectively
The adjective form of physical is physically
The adjective form of regional is regionally
The adjective form of subsequent is subsequently
The adjective form of nice is
2024-07-17 00:45:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 00:49:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1973, -0.8340, -0.8467,  ..., -0.1533,  0.2749,  0.7373],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5166, -0.4443, -1.0352,  ...,  3.1250,  2.1289,  0.7148],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0077, -0.0161,  0.0232,  ...,  0.0038,  0.0045, -0.0084],
        [-0.0127, -0.0346,  0.0101,  ...,  0.0112, -0.0180,  0.0114],
        [-0.0038, -0.0171,  0.0168,  ...,  0.0035,  0.0279,  0.0175],
        ...,
        [-0.0135, -0.0153,  0.0054,  ..., -0.0107, -0.0109,  0.0057],
        [ 0.0005,  0.0188,  0.0039,  ..., -0.0077, -0.0002, -0.0302],
        [-0.0077,  0.0084,  0.0101,  ..., -0.0037, -0.0002, -0.0221]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0107, -0.4856, -1.3008,  ...,  2.9492,  1.9307,  0.8740]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 00:49:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of mental is mentally
The adjective form of huge is hugely
The adjective form of global is globally
The adjective form of effective is effectively
The adjective form of physical is physically
The adjective form of regional is regionally
The adjective form of subsequent is subsequently
The adjective form of nice is
2024-07-17 00:49:06 root INFO     [order_1_approx] starting weight calculation for The adjective form of effective is effectively
The adjective form of huge is hugely
The adjective form of physical is physically
The adjective form of nice is nicely
The adjective form of mental is mentally
The adjective form of regional is regionally
The adjective form of global is globally
The adjective form of subsequent is
2024-07-17 00:49:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 00:52:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.2861, 0.5830, 1.0518,  ..., 0.2588, 0.3435, 0.3408], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0645,  0.9277, -0.4443,  ...,  4.3242, -0.5068,  0.2656],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0170, -0.0169,  0.0181,  ..., -0.0385, -0.0013, -0.0188],
        [-0.0306, -0.0346,  0.0015,  ...,  0.0108, -0.0122,  0.0114],
        [-0.0088, -0.0167, -0.0202,  ..., -0.0062, -0.0056,  0.0114],
        ...,
        [-0.0074, -0.0316, -0.0215,  ..., -0.0125, -0.0193, -0.0018],
        [-0.0086,  0.0221, -0.0001,  ..., -0.0357, -0.0185,  0.0124],
        [-0.0076,  0.0179,  0.0007,  ...,  0.0046,  0.0217, -0.0372]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5273,  1.2656,  0.8262,  ...,  4.3633,  0.2114,  0.7837]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 00:52:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of effective is effectively
The adjective form of huge is hugely
The adjective form of physical is physically
The adjective form of nice is nicely
The adjective form of mental is mentally
The adjective form of regional is regionally
The adjective form of global is globally
The adjective form of subsequent is
2024-07-17 00:52:53 root INFO     [order_1_approx] starting weight calculation for The adjective form of physical is physically
The adjective form of regional is regionally
The adjective form of nice is nicely
The adjective form of subsequent is subsequently
The adjective form of effective is effectively
The adjective form of huge is hugely
The adjective form of mental is mentally
The adjective form of global is
2024-07-17 00:52:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 00:56:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([1.5449, 0.9512, 0.5078,  ..., 0.1660, 0.9043, 0.4529], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.8691,  1.0762,  0.0098,  ...,  0.4353, -1.0449,  0.1592],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0173, -0.0022,  0.0132,  ..., -0.0126,  0.0112, -0.0109],
        [-0.0129, -0.0113,  0.0062,  ...,  0.0074, -0.0172,  0.0037],
        [ 0.0072,  0.0083, -0.0100,  ...,  0.0169,  0.0044,  0.0045],
        ...,
        [-0.0166, -0.0120, -0.0043,  ...,  0.0008, -0.0032, -0.0079],
        [-0.0068, -0.0102,  0.0043,  ..., -0.0263, -0.0154, -0.0041],
        [-0.0144,  0.0050, -0.0054,  ..., -0.0024,  0.0113, -0.0251]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5439,  1.4238,  0.1505,  ..., -0.2268, -0.0610,  0.4407]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 00:56:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of physical is physically
The adjective form of regional is regionally
The adjective form of nice is nicely
The adjective form of subsequent is subsequently
The adjective form of effective is effectively
The adjective form of huge is hugely
The adjective form of mental is mentally
The adjective form of global is
2024-07-17 00:56:38 root INFO     [order_1_approx] starting weight calculation for The adjective form of huge is hugely
The adjective form of effective is effectively
The adjective form of mental is mentally
The adjective form of global is globally
The adjective form of subsequent is subsequently
The adjective form of physical is physically
The adjective form of nice is nicely
The adjective form of regional is
2024-07-17 00:56:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 01:00:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0046,  0.1760, -0.7344,  ...,  0.3191,  1.3506,  0.4160],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6050,  2.4512, -2.9961,  ...,  1.1396,  1.2969,  3.4805],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.8280e-02, -7.2327e-03,  1.1002e-02,  ..., -1.3443e-02,
          1.3557e-02, -3.0899e-03],
        [-2.5925e-02,  3.9520e-03,  6.8665e-05,  ...,  1.0765e-02,
         -1.6968e-02,  1.1398e-02],
        [ 3.2578e-03, -9.9030e-03,  8.7891e-03,  ...,  1.5656e-02,
          1.0902e-02, -6.3324e-04],
        ...,
        [-1.8005e-02, -1.1261e-02, -4.1351e-03,  ..., -3.2959e-03,
         -6.2790e-03, -1.3428e-02],
        [-4.9973e-03,  8.3008e-03,  7.2479e-03,  ..., -2.3941e-02,
         -2.2842e-02, -6.2981e-03],
        [ 4.4937e-03, -2.0599e-04,  9.1553e-03,  ..., -1.3855e-02,
          3.0632e-03, -2.4979e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6372,  2.4551, -2.6172,  ...,  1.3096,  0.8193,  4.7422]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 01:00:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of huge is hugely
The adjective form of effective is effectively
The adjective form of mental is mentally
The adjective form of global is globally
The adjective form of subsequent is subsequently
The adjective form of physical is physically
The adjective form of nice is nicely
The adjective form of regional is
2024-07-17 01:00:23 root INFO     [order_1_approx] starting weight calculation for The adjective form of global is globally
The adjective form of regional is regionally
The adjective form of nice is nicely
The adjective form of physical is physically
The adjective form of effective is effectively
The adjective form of mental is mentally
The adjective form of subsequent is subsequently
The adjective form of huge is
2024-07-17 01:00:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 01:04:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1797,  0.3093, -0.3184,  ...,  0.3428,  1.0469,  0.6035],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6660,  0.7212,  0.4561,  ..., -0.6533,  3.2305,  3.3203],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0314, -0.0093,  0.0101,  ..., -0.0017, -0.0097, -0.0102],
        [-0.0112, -0.0206,  0.0127,  ...,  0.0191, -0.0108, -0.0003],
        [ 0.0105, -0.0042, -0.0066,  ...,  0.0074,  0.0115, -0.0013],
        ...,
        [-0.0002, -0.0338,  0.0179,  ..., -0.0219, -0.0177,  0.0008],
        [-0.0175,  0.0021,  0.0089,  ..., -0.0294, -0.0073, -0.0030],
        [-0.0326,  0.0139, -0.0018,  ..., -0.0069, -0.0030, -0.0421]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2871,  0.5083, -0.4155,  ..., -0.5796,  4.2070,  4.1758]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 01:04:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of global is globally
The adjective form of regional is regionally
The adjective form of nice is nicely
The adjective form of physical is physically
The adjective form of effective is effectively
The adjective form of mental is mentally
The adjective form of subsequent is subsequently
The adjective form of huge is
2024-07-17 01:04:08 root INFO     [order_1_approx] starting weight calculation for The adjective form of nice is nicely
The adjective form of mental is mentally
The adjective form of regional is regionally
The adjective form of effective is effectively
The adjective form of global is globally
The adjective form of subsequent is subsequently
The adjective form of huge is hugely
The adjective form of physical is
2024-07-17 01:04:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 01:07:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8008,  1.6172,  0.7002,  ...,  0.8271,  1.0137, -1.6846],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5879, -1.2441,  2.6602,  ..., -2.4160, -0.5615, -0.2080],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0308, -0.0149,  0.0117,  ..., -0.0159,  0.0030, -0.0247],
        [-0.0206, -0.0136,  0.0082,  ...,  0.0125, -0.0276, -0.0034],
        [-0.0132, -0.0214, -0.0036,  ...,  0.0132, -0.0002,  0.0059],
        ...,
        [-0.0151, -0.0187, -0.0241,  ..., -0.0158,  0.0201, -0.0197],
        [-0.0051,  0.0163,  0.0086,  ..., -0.0345, -0.0145, -0.0080],
        [-0.0129,  0.0090, -0.0117,  ..., -0.0114,  0.0316, -0.0347]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2173, -1.3613,  1.8037,  ..., -2.1016, -0.0464,  0.5820]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 01:07:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of nice is nicely
The adjective form of mental is mentally
The adjective form of regional is regionally
The adjective form of effective is effectively
The adjective form of global is globally
The adjective form of subsequent is subsequently
The adjective form of huge is hugely
The adjective form of physical is
2024-07-17 01:07:56 root INFO     [order_1_approx] starting weight calculation for The adjective form of global is globally
The adjective form of nice is nicely
The adjective form of huge is hugely
The adjective form of subsequent is subsequently
The adjective form of regional is regionally
The adjective form of effective is effectively
The adjective form of physical is physically
The adjective form of mental is
2024-07-17 01:07:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 01:11:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1074,  0.6729, -0.2720,  ...,  0.9688,  0.6270,  0.1835],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6602,  0.4119, -1.4824,  ...,  0.8589,  3.0605,  1.4150],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0070, -0.0114,  0.0107,  ..., -0.0164,  0.0145, -0.0073],
        [-0.0153,  0.0017,  0.0040,  ...,  0.0070, -0.0134,  0.0086],
        [-0.0056, -0.0024, -0.0037,  ...,  0.0062, -0.0013,  0.0208],
        ...,
        [-0.0049, -0.0139,  0.0012,  ...,  0.0018, -0.0014, -0.0115],
        [-0.0005, -0.0032,  0.0078,  ..., -0.0102, -0.0177, -0.0061],
        [ 0.0007, -0.0022,  0.0030,  ..., -0.0126,  0.0142, -0.0104]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6367, -0.1072, -1.2529,  ...,  0.5918,  2.9160,  1.8750]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 01:11:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of global is globally
The adjective form of nice is nicely
The adjective form of huge is hugely
The adjective form of subsequent is subsequently
The adjective form of regional is regionally
The adjective form of effective is effectively
The adjective form of physical is physically
The adjective form of mental is
2024-07-17 01:11:42 root INFO     [order_1_approx] starting weight calculation for The adjective form of nice is nicely
The adjective form of huge is hugely
The adjective form of global is globally
The adjective form of subsequent is subsequently
The adjective form of regional is regionally
The adjective form of mental is mentally
The adjective form of physical is physically
The adjective form of effective is
2024-07-17 01:11:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 01:15:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2900,  0.1987,  0.2400,  ...,  0.7749,  0.4912, -0.1766],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9941,  0.0774,  5.0977,  ..., -0.2839,  0.0879, -1.2812],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.7700e-02, -1.6327e-02,  5.8975e-03,  ..., -1.2375e-02,
         -2.5864e-03, -1.6022e-03],
        [-1.2062e-02, -1.5625e-02,  8.2397e-03,  ...,  6.1722e-03,
         -1.4893e-02,  2.1057e-03],
        [-1.0483e-02, -1.9455e-02, -1.2657e-02,  ..., -1.3771e-03,
          1.4076e-02,  5.1575e-03],
        ...,
        [-1.2634e-02, -2.1683e-02, -3.3760e-03,  ..., -5.3558e-03,
         -1.7395e-03, -1.7700e-02],
        [-7.8201e-05, -7.1182e-03,  7.8430e-03,  ..., -1.9318e-02,
         -1.7441e-02, -1.4999e-02],
        [-5.3024e-04,  8.8043e-03, -8.1635e-03,  ...,  2.9430e-03,
          3.3691e-02, -6.2141e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3125,  0.5137,  5.3359,  ..., -0.8408,  0.0356, -0.9751]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 01:15:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of nice is nicely
The adjective form of huge is hugely
The adjective form of global is globally
The adjective form of subsequent is subsequently
The adjective form of regional is regionally
The adjective form of mental is mentally
The adjective form of physical is physically
The adjective form of effective is
2024-07-17 01:15:28 root INFO     total operator prediction time: 1807.9861075878143 seconds
2024-07-17 01:15:28 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+tion_irreg
2024-07-17 01:15:28 root INFO     building operator verb+tion_irreg
2024-07-17 01:15:28 root INFO     [order_1_approx] starting weight calculation for To configure results in configuration
To standardize results in standardization
To degrade results in degradation
To stabilize results in stabilization
To maximize results in maximization
To allege results in allegation
To modernize results in modernization
To observe results in
2024-07-17 01:15:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 01:19:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0393,  0.1293,  0.6738,  ..., -0.2217, -0.2791,  0.6528],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.8633,  0.8896, -4.8281,  ..., -1.0010, -1.5703,  4.6641],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-9.6321e-05, -5.9509e-04, -2.0924e-03,  ...,  7.3318e-03,
          6.9809e-04, -2.3422e-02],
        [ 3.9253e-03, -6.4583e-03,  2.5978e-03,  ...,  2.3003e-03,
         -6.9199e-03,  5.4665e-03],
        [-1.0803e-02,  5.6114e-03, -1.2131e-03,  ...,  7.6675e-04,
          7.4768e-03, -3.2139e-03],
        ...,
        [-1.8494e-02, -1.8339e-03,  2.1935e-03,  ..., -8.8882e-04,
         -2.1744e-03, -1.4664e-02],
        [-1.2474e-02,  3.3836e-03,  1.2264e-03,  ...,  1.5297e-03,
         -2.3010e-02,  8.6136e-03],
        [-8.6060e-03,  2.4429e-02,  4.9782e-03,  ..., -7.3509e-03,
          3.4866e-03, -3.3226e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.7383,  1.0928, -4.6094,  ...,  0.0381, -1.5176,  5.4336]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 01:19:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To configure results in configuration
To standardize results in standardization
To degrade results in degradation
To stabilize results in stabilization
To maximize results in maximization
To allege results in allegation
To modernize results in modernization
To observe results in
2024-07-17 01:19:15 root INFO     [order_1_approx] starting weight calculation for To stabilize results in stabilization
To standardize results in standardization
To configure results in configuration
To allege results in allegation
To maximize results in maximization
To modernize results in modernization
To observe results in observation
To degrade results in
2024-07-17 01:19:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 01:23:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4673, -0.5732, -0.2612,  ...,  0.2959, -0.4121, -0.1356],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4885,  3.0859,  0.3369,  ..., -1.6748, -0.3164,  4.9219],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.5259e-05,  1.1787e-02,  8.9645e-04,  ..., -1.2367e-02,
          1.0071e-03, -9.1171e-03],
        [ 1.5312e-02,  2.0676e-03,  5.4512e-03,  ...,  1.1353e-02,
         -1.1063e-02,  9.6130e-03],
        [ 6.3400e-03,  1.0605e-03, -9.6817e-03,  ..., -6.2180e-04,
          5.8746e-03,  2.7695e-03],
        ...,
        [-1.6769e-02, -1.1780e-02, -7.7171e-03,  ...,  7.6904e-03,
         -1.1475e-02, -3.1414e-03],
        [ 5.5008e-03, -2.2125e-03,  4.4975e-03,  ..., -1.2009e-02,
         -2.1713e-02,  3.4294e-03],
        [-5.0850e-03,  1.0857e-02,  2.5558e-04,  ..., -1.0155e-02,
          8.9188e-03, -1.0689e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0413,  3.8652, -0.2183,  ..., -2.2891, -0.8257,  4.8008]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 01:23:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To stabilize results in stabilization
To standardize results in standardization
To configure results in configuration
To allege results in allegation
To maximize results in maximization
To modernize results in modernization
To observe results in observation
To degrade results in
2024-07-17 01:23:03 root INFO     [order_1_approx] starting weight calculation for To stabilize results in stabilization
To observe results in observation
To allege results in allegation
To degrade results in degradation
To maximize results in maximization
To standardize results in standardization
To modernize results in modernization
To configure results in
2024-07-17 01:23:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 01:26:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1006,  1.4590, -0.5391,  ...,  0.4351, -0.2131, -0.9775],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3765,  0.8257, -3.6953,  ..., -2.0996,  0.4521,  2.3438],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-7.4387e-03, -1.1925e-02,  5.1498e-05,  ..., -7.2632e-03,
         -5.0697e-03, -9.5062e-03],
        [ 1.5087e-03, -2.6703e-05,  6.0654e-03,  ..., -1.4210e-03,
         -1.1963e-02,  1.4526e-02],
        [ 9.0179e-03, -5.0774e-03, -6.7253e-03,  ..., -1.1292e-02,
          2.4033e-04, -6.1264e-03],
        ...,
        [-8.2245e-03, -1.6479e-02, -9.7656e-03,  ...,  6.2027e-03,
         -9.4757e-03,  1.0574e-02],
        [-5.5084e-03,  1.2856e-02,  4.9496e-04,  ..., -1.0696e-02,
         -1.6205e-02, -1.3260e-02],
        [ 6.3705e-03,  1.2161e-02, -1.0262e-02,  ..., -2.0340e-02,
          3.8719e-03,  7.7782e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3445,  0.9478, -3.7852,  ..., -1.7354,  0.4941,  2.5156]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 01:26:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To stabilize results in stabilization
To observe results in observation
To allege results in allegation
To degrade results in degradation
To maximize results in maximization
To standardize results in standardization
To modernize results in modernization
To configure results in
2024-07-17 01:26:52 root INFO     [order_1_approx] starting weight calculation for To stabilize results in stabilization
To allege results in allegation
To standardize results in standardization
To configure results in configuration
To maximize results in maximization
To degrade results in degradation
To observe results in observation
To modernize results in
2024-07-17 01:26:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 01:30:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1688, -0.6045,  1.3086,  ..., -0.4238,  0.4429,  0.0970],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.4766,  0.4785, -3.9473,  ...,  1.1230,  1.7949,  4.6406],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.8338e-03, -3.7270e-03,  7.5951e-03,  ..., -4.9286e-03,
          1.0689e-02, -1.0376e-02],
        [ 1.1658e-02,  7.7972e-03,  2.4090e-03,  ..., -4.3335e-03,
         -2.1553e-03, -5.6419e-03],
        [-1.3332e-03, -7.0534e-03, -6.8436e-03,  ..., -7.2784e-03,
          7.4997e-03,  5.6534e-03],
        ...,
        [-9.7656e-03, -1.4114e-02,  3.6430e-04,  ...,  7.5912e-04,
         -1.3695e-03, -1.3323e-03],
        [-4.2534e-03,  5.8594e-03,  1.2146e-02,  ..., -5.5275e-03,
         -7.1106e-03, -7.3662e-03],
        [-3.8948e-03,  4.5319e-03,  9.3460e-03,  ...,  4.8065e-03,
          1.0223e-02, -2.7657e-05]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.3223,  0.6172, -4.6445,  ...,  1.2324,  1.6387,  4.8281]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 01:30:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To stabilize results in stabilization
To allege results in allegation
To standardize results in standardization
To configure results in configuration
To maximize results in maximization
To degrade results in degradation
To observe results in observation
To modernize results in
2024-07-17 01:30:41 root INFO     [order_1_approx] starting weight calculation for To degrade results in degradation
To stabilize results in stabilization
To standardize results in standardization
To maximize results in maximization
To modernize results in modernization
To configure results in configuration
To observe results in observation
To allege results in
2024-07-17 01:30:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 01:34:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3887,  0.7422, -0.0287,  ..., -0.3674, -0.1375,  1.0557],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7324,  2.2441, -2.2969,  ...,  0.6562,  1.5605,  4.4023],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0223, -0.0035,  0.0123,  ..., -0.0084, -0.0159, -0.0075],
        [ 0.0077, -0.0150, -0.0099,  ...,  0.0049,  0.0014, -0.0033],
        [-0.0066,  0.0127, -0.0079,  ...,  0.0044, -0.0049, -0.0110],
        ...,
        [-0.0008, -0.0094, -0.0060,  ..., -0.0100, -0.0173, -0.0156],
        [-0.0199, -0.0014,  0.0076,  ..., -0.0072, -0.0206, -0.0144],
        [ 0.0009,  0.0198,  0.0067,  ..., -0.0122,  0.0024, -0.0096]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.8535,  1.6475, -2.6484,  ...,  1.2090,  1.9336,  4.3477]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 01:34:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To degrade results in degradation
To stabilize results in stabilization
To standardize results in standardization
To maximize results in maximization
To modernize results in modernization
To configure results in configuration
To observe results in observation
To allege results in
2024-07-17 01:34:30 root INFO     [order_1_approx] starting weight calculation for To standardize results in standardization
To allege results in allegation
To modernize results in modernization
To observe results in observation
To degrade results in degradation
To configure results in configuration
To stabilize results in stabilization
To maximize results in
2024-07-17 01:34:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 01:38:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8975, -0.3110,  0.1477,  ..., -0.9658, -0.0012, -1.0791],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.1836,  2.3730, -3.6406,  ...,  0.4263,  1.0459,  6.3594],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0005, -0.0059, -0.0005,  ..., -0.0025,  0.0032, -0.0071],
        [-0.0020, -0.0060,  0.0021,  ...,  0.0224,  0.0020,  0.0119],
        [-0.0089,  0.0002,  0.0037,  ...,  0.0076,  0.0063,  0.0011],
        ...,
        [-0.0171, -0.0237, -0.0013,  ...,  0.0085, -0.0093, -0.0053],
        [ 0.0075,  0.0107,  0.0071,  ..., -0.0122,  0.0022,  0.0086],
        [ 0.0124,  0.0136,  0.0018,  ..., -0.0003,  0.0226,  0.0131]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1035,  2.3496, -3.6953,  ...,  0.5889,  0.5747,  5.7578]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 01:38:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To standardize results in standardization
To allege results in allegation
To modernize results in modernization
To observe results in observation
To degrade results in degradation
To configure results in configuration
To stabilize results in stabilization
To maximize results in
2024-07-17 01:38:17 root INFO     [order_1_approx] starting weight calculation for To observe results in observation
To modernize results in modernization
To degrade results in degradation
To configure results in configuration
To allege results in allegation
To maximize results in maximization
To standardize results in standardization
To stabilize results in
2024-07-17 01:38:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 01:42:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2305, -0.0256,  0.5498,  ..., -0.2095, -0.9717, -0.6689],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7148,  1.3994, -0.8916,  ..., -0.0973, -0.2854,  5.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-6.8665e-05, -4.0627e-03, -5.5542e-03,  ...,  7.2289e-03,
          8.0566e-03, -2.2400e-02],
        [ 1.6159e-02, -1.2283e-03, -5.9967e-03,  ..., -5.3711e-03,
         -9.1324e-03,  1.1749e-03],
        [-1.6861e-03, -6.0043e-03, -1.5900e-02,  ...,  8.6670e-03,
          2.2373e-03, -2.7847e-03],
        ...,
        [-2.3285e-02,  5.4932e-04, -1.9379e-02,  ..., -2.7237e-03,
         -4.5090e-03, -1.5930e-02],
        [-1.6907e-02, -3.7842e-03,  2.4223e-03,  ..., -3.8872e-03,
         -2.3941e-02,  1.7557e-03],
        [-4.4479e-03,  2.0569e-02,  1.0090e-03,  ..., -5.4703e-03,
         -4.2915e-03, -8.6975e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4141,  1.2197, -1.1250,  ..., -0.4463, -0.2490,  5.5508]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 01:42:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To observe results in observation
To modernize results in modernization
To degrade results in degradation
To configure results in configuration
To allege results in allegation
To maximize results in maximization
To standardize results in standardization
To stabilize results in
2024-07-17 01:42:06 root INFO     [order_1_approx] starting weight calculation for To stabilize results in stabilization
To maximize results in maximization
To allege results in allegation
To modernize results in modernization
To configure results in configuration
To observe results in observation
To degrade results in degradation
To standardize results in
2024-07-17 01:42:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 01:45:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1875,  0.1157,  0.9355,  ...,  0.0651,  0.2004, -0.0156],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9414,  1.5625, -0.9077,  ...,  2.1855,  4.5898,  5.1172],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 8.9264e-04, -1.1253e-02,  4.4098e-03,  ..., -1.5507e-03,
          8.5754e-03, -1.3573e-02],
        [-1.1463e-03, -9.8495e-03,  2.9945e-04,  ...,  6.6261e-03,
         -9.2316e-03,  1.4629e-03],
        [ 4.0436e-04, -4.9782e-03,  8.1558e-03,  ..., -4.8409e-03,
          4.9210e-03,  1.9951e-03],
        ...,
        [-2.2469e-03, -1.9608e-02, -3.8185e-03,  ...,  6.8665e-05,
          4.9591e-05, -7.2899e-03],
        [-9.2850e-03,  5.2948e-03,  5.8060e-03,  ...,  8.0967e-04,
         -1.1337e-02,  7.0953e-04],
        [-6.8512e-03,  1.5808e-02,  8.7967e-03,  ...,  1.3351e-05,
         -4.9706e-03, -8.1730e-04]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3516,  1.9355, -0.5928,  ...,  2.0449,  4.1406,  5.0508]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 01:45:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To stabilize results in stabilization
To maximize results in maximization
To allege results in allegation
To modernize results in modernization
To configure results in configuration
To observe results in observation
To degrade results in degradation
To standardize results in
2024-07-17 01:45:53 root INFO     total operator prediction time: 1825.092882156372 seconds
2024-07-17 01:45:53 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+able_reg
2024-07-17 01:45:53 root INFO     building operator verb+able_reg
2024-07-17 01:45:54 root INFO     [order_1_approx] starting weight calculation for If you can adjust something, that thing is adjustable
If you can dispose something, that thing is disposable
If you can manage something, that thing is manageable
If you can afford something, that thing is affordable
If you can predict something, that thing is predictable
If you can imagine something, that thing is imaginable
If you can learn something, that thing is learnable
If you can contain something, that thing is
2024-07-17 01:45:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 01:49:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1122,  0.5840, -0.2588,  ..., -0.6821,  0.7822,  1.2480],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.2012,  4.3945, -4.0000,  ..., -0.2417,  1.6289,  3.4375],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0065, -0.0040, -0.0028,  ...,  0.0046,  0.0026, -0.0073],
        [ 0.0108,  0.0060,  0.0105,  ..., -0.0168, -0.0032, -0.0044],
        [-0.0051,  0.0052,  0.0096,  ...,  0.0024,  0.0076,  0.0020],
        ...,
        [-0.0138, -0.0027, -0.0048,  ...,  0.0101,  0.0100,  0.0006],
        [-0.0001, -0.0016,  0.0048,  ..., -0.0115, -0.0078,  0.0011],
        [-0.0093,  0.0037, -0.0045,  ..., -0.0174,  0.0155, -0.0162]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3628,  3.9863, -4.5312,  ...,  0.1985,  1.4453,  2.3125]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 01:49:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can adjust something, that thing is adjustable
If you can dispose something, that thing is disposable
If you can manage something, that thing is manageable
If you can afford something, that thing is affordable
If you can predict something, that thing is predictable
If you can imagine something, that thing is imaginable
If you can learn something, that thing is learnable
If you can contain something, that thing is
2024-07-17 01:49:40 root INFO     [order_1_approx] starting weight calculation for If you can predict something, that thing is predictable
If you can imagine something, that thing is imaginable
If you can afford something, that thing is affordable
If you can learn something, that thing is learnable
If you can adjust something, that thing is adjustable
If you can dispose something, that thing is disposable
If you can contain something, that thing is containable
If you can manage something, that thing is
2024-07-17 01:49:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 01:53:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6938,  0.3169,  0.2327,  ...,  0.2905,  0.7617, -0.0303],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6289,  3.2656, -6.1016,  ...,  0.8418,  3.5957,  3.4004],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.0605e-03,  8.4782e-04, -1.6174e-02,  ...,  4.0817e-03,
          8.8501e-03,  3.6602e-03],
        [-6.6338e-03, -9.2163e-03,  7.8201e-05,  ..., -2.4223e-03,
         -5.8060e-03, -5.5542e-03],
        [ 6.5994e-03, -8.0109e-03, -8.1329e-03,  ...,  2.2125e-04,
          9.0561e-03, -8.2245e-03],
        ...,
        [-7.7171e-03, -1.2024e-02, -9.2163e-03,  ..., -9.1858e-03,
          3.0193e-03, -5.2757e-03],
        [-5.5847e-03, -1.0681e-02,  1.0445e-02,  ..., -1.9226e-02,
         -2.6108e-02,  1.1345e-02],
        [ 7.2136e-03,  2.3453e-02,  4.8065e-04,  ..., -3.7384e-03,
          2.2919e-02, -2.7069e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3213,  3.7578, -5.9766,  ...,  1.2480,  3.9219,  2.9590]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 01:53:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can predict something, that thing is predictable
If you can imagine something, that thing is imaginable
If you can afford something, that thing is affordable
If you can learn something, that thing is learnable
If you can adjust something, that thing is adjustable
If you can dispose something, that thing is disposable
If you can contain something, that thing is containable
If you can manage something, that thing is
2024-07-17 01:53:27 root INFO     [order_1_approx] starting weight calculation for If you can dispose something, that thing is disposable
If you can manage something, that thing is manageable
If you can adjust something, that thing is adjustable
If you can learn something, that thing is learnable
If you can predict something, that thing is predictable
If you can contain something, that thing is containable
If you can imagine something, that thing is imaginable
If you can afford something, that thing is
2024-07-17 01:53:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 01:57:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8267, -0.7979, -0.1639,  ...,  0.0999,  1.5820, -0.1270],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.2812,  3.5352, -2.8750,  ..., -2.7852,  1.0020,  2.9883],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0044, -0.0035,  0.0002,  ..., -0.0011, -0.0055, -0.0017],
        [ 0.0068, -0.0028,  0.0017,  ...,  0.0056,  0.0002,  0.0002],
        [ 0.0018, -0.0040, -0.0049,  ..., -0.0070,  0.0077, -0.0059],
        ...,
        [ 0.0002, -0.0086, -0.0112,  ..., -0.0140,  0.0053, -0.0065],
        [-0.0003, -0.0109, -0.0091,  ..., -0.0072, -0.0129,  0.0039],
        [ 0.0044,  0.0116,  0.0002,  ..., -0.0010,  0.0073, -0.0124]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.9688,  4.0430, -2.9688,  ..., -3.6562,  0.7329,  3.4648]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 01:57:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can dispose something, that thing is disposable
If you can manage something, that thing is manageable
If you can adjust something, that thing is adjustable
If you can learn something, that thing is learnable
If you can predict something, that thing is predictable
If you can contain something, that thing is containable
If you can imagine something, that thing is imaginable
If you can afford something, that thing is
2024-07-17 01:57:12 root INFO     [order_1_approx] starting weight calculation for If you can afford something, that thing is affordable
If you can contain something, that thing is containable
If you can imagine something, that thing is imaginable
If you can dispose something, that thing is disposable
If you can learn something, that thing is learnable
If you can predict something, that thing is predictable
If you can manage something, that thing is manageable
If you can adjust something, that thing is
2024-07-17 01:57:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 02:00:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4487,  1.2031, -0.0422,  ...,  0.1497,  0.9180, -0.3386],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0752,  5.8945, -3.5449,  ..., -2.7305,  2.8008,  2.5156],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0040, -0.0075, -0.0034,  ...,  0.0183, -0.0015,  0.0119],
        [ 0.0028,  0.0072,  0.0023,  ...,  0.0001, -0.0078, -0.0057],
        [ 0.0039,  0.0127, -0.0086,  ...,  0.0087,  0.0025, -0.0100],
        ...,
        [-0.0097, -0.0141, -0.0020,  ..., -0.0063,  0.0073,  0.0017],
        [ 0.0029, -0.0008,  0.0020,  ..., -0.0187, -0.0099, -0.0040],
        [ 0.0011,  0.0016,  0.0024,  ...,  0.0008,  0.0114, -0.0185]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9785,  5.8516, -3.7617,  ..., -2.4141,  2.4160,  2.7734]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 02:00:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can afford something, that thing is affordable
If you can contain something, that thing is containable
If you can imagine something, that thing is imaginable
If you can dispose something, that thing is disposable
If you can learn something, that thing is learnable
If you can predict something, that thing is predictable
If you can manage something, that thing is manageable
If you can adjust something, that thing is
2024-07-17 02:00:57 root INFO     [order_1_approx] starting weight calculation for If you can adjust something, that thing is adjustable
If you can imagine something, that thing is imaginable
If you can contain something, that thing is containable
If you can afford something, that thing is affordable
If you can manage something, that thing is manageable
If you can predict something, that thing is predictable
If you can learn something, that thing is learnable
If you can dispose something, that thing is
2024-07-17 02:00:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 02:04:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4951,  1.1572,  0.2336,  ...,  0.8687,  0.2264, -0.2585],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7617,  4.9844, -3.3555,  ..., -3.3398,  1.1992,  2.0469],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0031, -0.0069,  0.0042,  ...,  0.0045, -0.0025,  0.0007],
        [ 0.0006,  0.0066,  0.0009,  ..., -0.0071,  0.0058, -0.0105],
        [-0.0055,  0.0080, -0.0121,  ..., -0.0069,  0.0051, -0.0063],
        ...,
        [-0.0074, -0.0202,  0.0026,  ...,  0.0030, -0.0028,  0.0008],
        [ 0.0058,  0.0005,  0.0023,  ..., -0.0102, -0.0133, -0.0036],
        [-0.0114,  0.0079, -0.0134,  ..., -0.0130,  0.0096, -0.0268]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2007,  4.6641, -3.3691,  ..., -3.1230,  1.1318,  1.8223]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 02:04:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can adjust something, that thing is adjustable
If you can imagine something, that thing is imaginable
If you can contain something, that thing is containable
If you can afford something, that thing is affordable
If you can manage something, that thing is manageable
If you can predict something, that thing is predictable
If you can learn something, that thing is learnable
If you can dispose something, that thing is
2024-07-17 02:04:42 root INFO     [order_1_approx] starting weight calculation for If you can afford something, that thing is affordable
If you can predict something, that thing is predictable
If you can learn something, that thing is learnable
If you can manage something, that thing is manageable
If you can adjust something, that thing is adjustable
If you can dispose something, that thing is disposable
If you can contain something, that thing is containable
If you can imagine something, that thing is
2024-07-17 02:04:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 02:08:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6631, -0.0438, -0.1384,  ..., -0.3022,  1.6182, -0.6514],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.2930,  4.0742, -5.1797,  ..., -2.8066,  2.3555,  1.1963],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0125, -0.0100, -0.0106,  ..., -0.0045,  0.0031, -0.0028],
        [-0.0112, -0.0026,  0.0119,  ..., -0.0167,  0.0034, -0.0122],
        [ 0.0042,  0.0167, -0.0088,  ...,  0.0042, -0.0028,  0.0082],
        ...,
        [-0.0024, -0.0056, -0.0087,  ...,  0.0097,  0.0004,  0.0048],
        [-0.0038, -0.0070, -0.0059,  ..., -0.0051, -0.0051,  0.0008],
        [-0.0035, -0.0061, -0.0018,  ..., -0.0077,  0.0198, -0.0141]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.4863,  3.9062, -5.1680,  ..., -3.0234,  2.1426,  1.3799]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 02:08:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can afford something, that thing is affordable
If you can predict something, that thing is predictable
If you can learn something, that thing is learnable
If you can manage something, that thing is manageable
If you can adjust something, that thing is adjustable
If you can dispose something, that thing is disposable
If you can contain something, that thing is containable
If you can imagine something, that thing is
2024-07-17 02:08:30 root INFO     [order_1_approx] starting weight calculation for If you can learn something, that thing is learnable
If you can imagine something, that thing is imaginable
If you can contain something, that thing is containable
If you can manage something, that thing is manageable
If you can adjust something, that thing is adjustable
If you can dispose something, that thing is disposable
If you can afford something, that thing is affordable
If you can predict something, that thing is
2024-07-17 02:08:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 02:12:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3389,  0.8047, -0.1147,  ..., -0.2437,  1.6758,  0.1257],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1543,  1.6709, -3.3789,  ..., -3.3164,  4.5508,  2.2695],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.3384e-03, -8.5220e-03,  8.3313e-03,  ..., -3.0098e-03,
          1.4000e-03, -5.4855e-03],
        [-5.3062e-03, -7.5912e-03,  2.0538e-02,  ..., -1.9562e-02,
         -9.4910e-03,  1.7815e-03],
        [ 1.3924e-04,  2.0466e-03, -4.6444e-04,  ...,  1.3161e-03,
          6.8436e-03, -4.1656e-03],
        ...,
        [-4.1733e-03, -1.2611e-02, -1.1772e-02,  ...,  5.0545e-03,
         -1.9684e-03, -8.0032e-03],
        [-1.0681e-02, -5.2490e-03, -8.9645e-05,  ..., -2.1286e-02,
         -1.8738e-02,  1.2264e-03],
        [-1.1292e-02,  1.1086e-02,  4.8256e-03,  ..., -1.5900e-02,
          1.1826e-02, -2.6382e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2314,  1.6816, -3.7227,  ..., -3.7207,  5.1250,  1.6309]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 02:12:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can learn something, that thing is learnable
If you can imagine something, that thing is imaginable
If you can contain something, that thing is containable
If you can manage something, that thing is manageable
If you can adjust something, that thing is adjustable
If you can dispose something, that thing is disposable
If you can afford something, that thing is affordable
If you can predict something, that thing is
2024-07-17 02:12:16 root INFO     [order_1_approx] starting weight calculation for If you can dispose something, that thing is disposable
If you can adjust something, that thing is adjustable
If you can manage something, that thing is manageable
If you can afford something, that thing is affordable
If you can imagine something, that thing is imaginable
If you can predict something, that thing is predictable
If you can contain something, that thing is containable
If you can learn something, that thing is
2024-07-17 02:12:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 02:15:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3484,  0.6172,  0.3162,  ..., -0.6343,  1.6260, -0.2457],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.2383,  6.1367, -3.6738,  ..., -1.0664,  1.8115,  0.6543],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0032, -0.0140, -0.0080,  ...,  0.0148, -0.0022, -0.0033],
        [-0.0010, -0.0076,  0.0072,  ..., -0.0070,  0.0020,  0.0002],
        [-0.0005,  0.0064,  0.0031,  ...,  0.0040, -0.0016,  0.0046],
        ...,
        [ 0.0022, -0.0125, -0.0116,  ...,  0.0135, -0.0049,  0.0003],
        [-0.0042, -0.0055,  0.0009,  ..., -0.0114, -0.0146,  0.0006],
        [ 0.0050, -0.0008,  0.0014,  ..., -0.0059,  0.0077, -0.0134]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.9395,  5.5586, -3.7090,  ..., -0.3369,  1.8057,  0.7910]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 02:16:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can dispose something, that thing is disposable
If you can adjust something, that thing is adjustable
If you can manage something, that thing is manageable
If you can afford something, that thing is affordable
If you can imagine something, that thing is imaginable
If you can predict something, that thing is predictable
If you can contain something, that thing is containable
If you can learn something, that thing is
2024-07-17 02:16:00 root INFO     total operator prediction time: 1806.9284822940826 seconds
2024-07-17 02:16:00 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on un+adj_reg
2024-07-17 02:16:00 root INFO     building operator un+adj_reg
2024-07-17 02:16:01 root INFO     [order_1_approx] starting weight calculation for The opposite of biased is unbiased
The opposite of veiled is unveiled
The opposite of forgettable is unforgettable
The opposite of realistic is unrealistic
The opposite of predictable is unpredictable
The opposite of interrupted is uninterrupted
The opposite of changed is unchanged
The opposite of controlled is
2024-07-17 02:16:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 02:19:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9546,  0.9092, -0.0613,  ..., -0.8892,  1.8145, -0.4272],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6035, -4.4727, -1.3223,  ..., -1.5664, -0.6172, -1.2383],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0031, -0.0044,  0.0007,  ..., -0.0094, -0.0087, -0.0130],
        [-0.0108, -0.0239, -0.0014,  ..., -0.0183, -0.0101, -0.0109],
        [-0.0200, -0.0161, -0.0116,  ...,  0.0014,  0.0039, -0.0187],
        ...,
        [-0.0152,  0.0104,  0.0043,  ...,  0.0003,  0.0196,  0.0186],
        [-0.0071, -0.0093,  0.0113,  ..., -0.0100,  0.0005,  0.0151],
        [ 0.0120, -0.0088, -0.0025,  ...,  0.0045,  0.0111, -0.0159]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9263, -3.5781, -1.2070,  ..., -1.7793,  0.2212, -1.8223]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 02:19:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of biased is unbiased
The opposite of veiled is unveiled
The opposite of forgettable is unforgettable
The opposite of realistic is unrealistic
The opposite of predictable is unpredictable
The opposite of interrupted is uninterrupted
The opposite of changed is unchanged
The opposite of controlled is
2024-07-17 02:19:46 root INFO     [order_1_approx] starting weight calculation for The opposite of controlled is uncontrolled
The opposite of changed is unchanged
The opposite of veiled is unveiled
The opposite of forgettable is unforgettable
The opposite of predictable is unpredictable
The opposite of realistic is unrealistic
The opposite of interrupted is uninterrupted
The opposite of biased is
2024-07-17 02:19:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 02:23:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2539, -1.2295, -0.9248,  ...,  0.0768,  1.3867, -0.1957],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7397,  3.9531,  0.4043,  ..., -1.0430,  3.2188,  3.2168],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0238, -0.0247, -0.0155,  ..., -0.0092, -0.0196, -0.0287],
        [ 0.0118, -0.0275, -0.0256,  ..., -0.0078, -0.0087, -0.0003],
        [ 0.0070,  0.0030, -0.0134,  ..., -0.0107,  0.0231,  0.0082],
        ...,
        [ 0.0026,  0.0053, -0.0086,  ...,  0.0004,  0.0029, -0.0022],
        [-0.0388,  0.0062, -0.0068,  ..., -0.0125, -0.0176, -0.0138],
        [ 0.0204, -0.0120, -0.0112,  ..., -0.0125,  0.0173, -0.0065]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.8213,  4.4883,  0.2418,  ..., -0.8223,  3.0117,  4.5508]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 02:23:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of controlled is uncontrolled
The opposite of changed is unchanged
The opposite of veiled is unveiled
The opposite of forgettable is unforgettable
The opposite of predictable is unpredictable
The opposite of realistic is unrealistic
The opposite of interrupted is uninterrupted
The opposite of biased is
2024-07-17 02:23:33 root INFO     [order_1_approx] starting weight calculation for The opposite of forgettable is unforgettable
The opposite of interrupted is uninterrupted
The opposite of changed is unchanged
The opposite of controlled is uncontrolled
The opposite of predictable is unpredictable
The opposite of veiled is unveiled
The opposite of biased is unbiased
The opposite of realistic is
2024-07-17 02:23:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 02:27:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0212, -1.3135,  0.7622,  ...,  0.2117,  2.7891,  0.2920],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2456, -2.6836, -0.2729,  ..., -1.8408,  2.8105,  3.0332],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0055, -0.0146,  0.0016,  ..., -0.0029, -0.0087,  0.0062],
        [ 0.0019, -0.0131,  0.0007,  ..., -0.0226,  0.0078,  0.0050],
        [-0.0169, -0.0104, -0.0269,  ..., -0.0115,  0.0147,  0.0035],
        ...,
        [ 0.0100, -0.0033, -0.0190,  ..., -0.0234,  0.0154, -0.0009],
        [-0.0091, -0.0083,  0.0258,  ..., -0.0068,  0.0072, -0.0064],
        [ 0.0186,  0.0099, -0.0084,  ...,  0.0024,  0.0202, -0.0116]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2900, -2.4219, -1.0254,  ..., -1.9795,  3.3203,  2.8965]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 02:27:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of forgettable is unforgettable
The opposite of interrupted is uninterrupted
The opposite of changed is unchanged
The opposite of controlled is uncontrolled
The opposite of predictable is unpredictable
The opposite of veiled is unveiled
The opposite of biased is unbiased
The opposite of realistic is
2024-07-17 02:27:22 root INFO     [order_1_approx] starting weight calculation for The opposite of realistic is unrealistic
The opposite of predictable is unpredictable
The opposite of changed is unchanged
The opposite of controlled is uncontrolled
The opposite of forgettable is unforgettable
The opposite of biased is unbiased
The opposite of veiled is unveiled
The opposite of interrupted is
2024-07-17 02:27:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 02:31:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3186, -0.5420,  0.2167,  ...,  1.3965,  0.3066,  0.3096],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.4160, -2.9883, -7.4453,  ...,  4.3320,  0.3174, -0.0127],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0105,  0.0054, -0.0027,  ...,  0.0010, -0.0266, -0.0358],
        [ 0.0111, -0.0009, -0.0118,  ..., -0.0181, -0.0097,  0.0167],
        [ 0.0090, -0.0201, -0.0210,  ..., -0.0177,  0.0043,  0.0026],
        ...,
        [-0.0039, -0.0068,  0.0165,  ...,  0.0022,  0.0134,  0.0158],
        [-0.0061, -0.0022,  0.0194,  ..., -0.0145,  0.0031, -0.0019],
        [ 0.0171, -0.0056,  0.0015,  ...,  0.0128, -0.0066, -0.0098]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.3496, -1.2383, -7.0078,  ...,  4.3594,  0.5825, -0.9932]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 02:31:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of realistic is unrealistic
The opposite of predictable is unpredictable
The opposite of changed is unchanged
The opposite of controlled is uncontrolled
The opposite of forgettable is unforgettable
The opposite of biased is unbiased
The opposite of veiled is unveiled
The opposite of interrupted is
2024-07-17 02:31:10 root INFO     [order_1_approx] starting weight calculation for The opposite of realistic is unrealistic
The opposite of biased is unbiased
The opposite of forgettable is unforgettable
The opposite of veiled is unveiled
The opposite of predictable is unpredictable
The opposite of controlled is uncontrolled
The opposite of interrupted is uninterrupted
The opposite of changed is
2024-07-17 02:31:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 02:34:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9907, -0.8867,  0.5791,  ...,  0.3228,  1.1855, -0.2278],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.0664, -3.8574, -2.6719,  ..., -2.3340,  0.6162, -0.3486],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.9073e-02, -6.1340e-03, -7.4463e-03,  ..., -7.4348e-03,
         -1.9485e-02, -5.4817e-03],
        [ 3.8662e-03, -1.9226e-03, -9.5978e-03,  ..., -1.8280e-02,
         -1.3962e-02, -2.7771e-02],
        [ 6.9656e-03, -3.9520e-03, -2.4200e-02,  ..., -1.2589e-04,
         -1.6083e-02,  2.9869e-03],
        ...,
        [-2.5291e-03, -1.5594e-02, -7.3814e-03,  ..., -1.8295e-02,
          2.0752e-03,  4.9820e-03],
        [-8.1024e-03,  5.2261e-04, -4.0092e-03,  ...,  4.5776e-05,
         -2.8442e-02, -2.8191e-03],
        [-6.8779e-03,  7.6294e-03, -1.3247e-03,  ..., -1.5106e-03,
          1.2474e-02, -2.7313e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.6914, -3.5703, -2.1719,  ..., -2.2832,  1.0176,  0.1382]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 02:34:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of realistic is unrealistic
The opposite of biased is unbiased
The opposite of forgettable is unforgettable
The opposite of veiled is unveiled
The opposite of predictable is unpredictable
The opposite of controlled is uncontrolled
The opposite of interrupted is uninterrupted
The opposite of changed is
2024-07-17 02:34:58 root INFO     [order_1_approx] starting weight calculation for The opposite of biased is unbiased
The opposite of predictable is unpredictable
The opposite of veiled is unveiled
The opposite of realistic is unrealistic
The opposite of changed is unchanged
The opposite of controlled is uncontrolled
The opposite of interrupted is uninterrupted
The opposite of forgettable is
2024-07-17 02:34:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 02:38:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6968,  0.4604,  0.4731,  ...,  0.1293,  1.0020, -0.7715],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.2803, -0.9502, -0.3506,  ...,  1.2744,  0.2937,  7.6719],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0227,  0.0127,  0.0009,  ..., -0.0054, -0.0042, -0.0381],
        [ 0.0112,  0.0029, -0.0069,  ...,  0.0001, -0.0139,  0.0058],
        [-0.0045, -0.0152, -0.0344,  ..., -0.0032,  0.0188,  0.0003],
        ...,
        [ 0.0009, -0.0183, -0.0052,  ..., -0.0339, -0.0094,  0.0164],
        [-0.0018,  0.0134,  0.0064,  ..., -0.0285, -0.0572, -0.0144],
        [ 0.0105, -0.0022, -0.0061,  ..., -0.0097,  0.0212, -0.0166]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.8594, -0.5977,  0.7861,  ...,  1.5449,  0.6504,  7.0234]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 02:38:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of biased is unbiased
The opposite of predictable is unpredictable
The opposite of veiled is unveiled
The opposite of realistic is unrealistic
The opposite of changed is unchanged
The opposite of controlled is uncontrolled
The opposite of interrupted is uninterrupted
The opposite of forgettable is
2024-07-17 02:38:44 root INFO     [order_1_approx] starting weight calculation for The opposite of controlled is uncontrolled
The opposite of biased is unbiased
The opposite of changed is unchanged
The opposite of interrupted is uninterrupted
The opposite of predictable is unpredictable
The opposite of forgettable is unforgettable
The opposite of realistic is unrealistic
The opposite of veiled is
2024-07-17 02:38:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 02:42:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6553, -1.5938, -0.2585,  ...,  1.0381,  0.6836,  0.2366],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0369,  1.7441, -3.5586,  ...,  0.4639, -3.5059, -1.2480],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0108, -0.0184,  0.0021,  ..., -0.0006, -0.0015, -0.0207],
        [-0.0197, -0.0045, -0.0113,  ..., -0.0201, -0.0059, -0.0064],
        [-0.0012, -0.0077, -0.0302,  ...,  0.0016, -0.0126,  0.0086],
        ...,
        [-0.0049, -0.0086,  0.0096,  ...,  0.0046, -0.0090,  0.0228],
        [-0.0026, -0.0079,  0.0029,  ...,  0.0073,  0.0007, -0.0244],
        [ 0.0237, -0.0040, -0.0111,  ..., -0.0111,  0.0111,  0.0066]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3777,  2.4512, -3.0977,  ...,  0.9355, -2.8555, -1.2891]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 02:42:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of controlled is uncontrolled
The opposite of biased is unbiased
The opposite of changed is unchanged
The opposite of interrupted is uninterrupted
The opposite of predictable is unpredictable
The opposite of forgettable is unforgettable
The opposite of realistic is unrealistic
The opposite of veiled is
2024-07-17 02:42:30 root INFO     [order_1_approx] starting weight calculation for The opposite of changed is unchanged
The opposite of forgettable is unforgettable
The opposite of controlled is uncontrolled
The opposite of biased is unbiased
The opposite of veiled is unveiled
The opposite of interrupted is uninterrupted
The opposite of realistic is unrealistic
The opposite of predictable is
2024-07-17 02:42:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 02:46:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5962, -0.1877,  0.3481,  ..., -0.3833,  2.0586, -0.0079],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5361, -3.7246, -2.5508,  ...,  1.7598,  3.8711,  5.9688],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0023, -0.0188,  0.0058,  ...,  0.0168,  0.0026, -0.0015],
        [ 0.0041, -0.0136, -0.0136,  ..., -0.0185,  0.0011,  0.0053],
        [ 0.0170, -0.0137, -0.0200,  ..., -0.0207,  0.0120, -0.0061],
        ...,
        [-0.0075, -0.0180, -0.0100,  ...,  0.0047, -0.0052, -0.0211],
        [-0.0040, -0.0028,  0.0169,  ..., -0.0009, -0.0232, -0.0199],
        [ 0.0164, -0.0034, -0.0178,  ..., -0.0163,  0.0131, -0.0010]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0735, -3.2715, -1.9707,  ...,  1.4697,  3.3047,  5.2578]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 02:46:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of changed is unchanged
The opposite of forgettable is unforgettable
The opposite of controlled is uncontrolled
The opposite of biased is unbiased
The opposite of veiled is unveiled
The opposite of interrupted is uninterrupted
The opposite of realistic is unrealistic
The opposite of predictable is
2024-07-17 02:46:16 root INFO     total operator prediction time: 1815.3287546634674 seconds
2024-07-17 02:46:16 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on re+verb_reg
2024-07-17 02:46:16 root INFO     building operator re+verb_reg
2024-07-17 02:46:16 root INFO     [order_1_approx] starting weight calculation for To locate again is to relocate
To marry again is to remarry
To investigate again is to reinvestigate
To establish again is to reestablish
To arrange again is to rearrange
To write again is to rewrite
To unite again is to reunite
To integrate again is to
2024-07-17 02:46:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 02:50:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1372,  1.3545,  1.4268,  ...,  1.8477, -1.1934,  0.8809],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4053, -0.9043, -5.5469,  ...,  0.3074, -1.5635,  1.7715],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0227, -0.0087,  0.0030,  ..., -0.0013, -0.0040, -0.0100],
        [ 0.0003, -0.0046,  0.0039,  ...,  0.0010, -0.0012,  0.0038],
        [ 0.0092,  0.0011, -0.0064,  ..., -0.0144, -0.0063, -0.0137],
        ...,
        [-0.0137, -0.0026, -0.0138,  ...,  0.0002, -0.0207,  0.0039],
        [-0.0076, -0.0122,  0.0084,  ..., -0.0032, -0.0058, -0.0092],
        [ 0.0043,  0.0104,  0.0124,  ..., -0.0028,  0.0150, -0.0013]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2903, -0.6094, -6.0664,  ...,  0.4753, -1.6475,  0.7031]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 02:50:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To locate again is to relocate
To marry again is to remarry
To investigate again is to reinvestigate
To establish again is to reestablish
To arrange again is to rearrange
To write again is to rewrite
To unite again is to reunite
To integrate again is to
2024-07-17 02:50:02 root INFO     [order_1_approx] starting weight calculation for To unite again is to reunite
To arrange again is to rearrange
To locate again is to relocate
To establish again is to reestablish
To integrate again is to reintegrate
To investigate again is to reinvestigate
To marry again is to remarry
To write again is to
2024-07-17 02:50:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 02:53:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.9365, -0.0605,  0.4114,  ...,  0.3665, -0.5664,  0.3833],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8271, -0.2344, -4.1445,  ...,  3.7109,  0.1772, -1.1309],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0080, -0.0067,  0.0045,  ..., -0.0063,  0.0050, -0.0075],
        [-0.0122, -0.0073, -0.0002,  ..., -0.0013, -0.0024,  0.0011],
        [ 0.0036,  0.0083, -0.0022,  ..., -0.0011, -0.0013, -0.0100],
        ...,
        [-0.0172, -0.0027,  0.0122,  ..., -0.0094, -0.0092,  0.0049],
        [ 0.0038, -0.0023,  0.0010,  ...,  0.0019, -0.0284,  0.0058],
        [-0.0009,  0.0122,  0.0173,  ..., -0.0078,  0.0012, -0.0049]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2676, -0.0765, -3.4922,  ...,  2.9727,  0.7817, -1.3223]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 02:53:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To unite again is to reunite
To arrange again is to rearrange
To locate again is to relocate
To establish again is to reestablish
To integrate again is to reintegrate
To investigate again is to reinvestigate
To marry again is to remarry
To write again is to
2024-07-17 02:53:48 root INFO     [order_1_approx] starting weight calculation for To marry again is to remarry
To integrate again is to reintegrate
To write again is to rewrite
To arrange again is to rearrange
To unite again is to reunite
To locate again is to relocate
To investigate again is to reinvestigate
To establish again is to
2024-07-17 02:53:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 02:57:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0762,  0.1763,  0.7197,  ...,  0.9355, -0.2197,  0.5176],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1729,  0.3809, -1.8623,  ...,  2.5820, -0.4412,  5.1484],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0036, -0.0063,  0.0091,  ..., -0.0081, -0.0009, -0.0183],
        [ 0.0068,  0.0082,  0.0111,  ..., -0.0015, -0.0104, -0.0030],
        [ 0.0046,  0.0135, -0.0015,  ...,  0.0007,  0.0088,  0.0056],
        ...,
        [-0.0210, -0.0094,  0.0015,  ..., -0.0136, -0.0086,  0.0030],
        [ 0.0036, -0.0026, -0.0005,  ...,  0.0023, -0.0008, -0.0022],
        [ 0.0026,  0.0023,  0.0033,  ..., -0.0037, -0.0069,  0.0110]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1519,  0.1270, -2.8438,  ...,  3.2500, -0.1519,  5.2227]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 02:57:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To marry again is to remarry
To integrate again is to reintegrate
To write again is to rewrite
To arrange again is to rearrange
To unite again is to reunite
To locate again is to relocate
To investigate again is to reinvestigate
To establish again is to
2024-07-17 02:57:36 root INFO     [order_1_approx] starting weight calculation for To integrate again is to reintegrate
To locate again is to relocate
To arrange again is to rearrange
To marry again is to remarry
To investigate again is to reinvestigate
To write again is to rewrite
To establish again is to reestablish
To unite again is to
2024-07-17 02:57:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 03:01:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6294,  0.6104,  0.9668,  ...,  0.7539, -0.0596,  0.6748],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7822, -1.1123, -7.6211,  ..., -0.6738, -0.8564, -2.2578],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0016, -0.0154,  0.0086,  ..., -0.0154, -0.0193, -0.0119],
        [ 0.0089, -0.0067, -0.0025,  ...,  0.0015,  0.0039, -0.0070],
        [ 0.0187, -0.0064, -0.0239,  ..., -0.0084, -0.0028, -0.0339],
        ...,
        [-0.0265, -0.0118,  0.0176,  ..., -0.0070,  0.0056,  0.0146],
        [ 0.0068, -0.0004,  0.0032,  ...,  0.0107, -0.0004, -0.0112],
        [ 0.0088, -0.0036,  0.0089,  ..., -0.0074,  0.0120,  0.0108]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5371, -1.3975, -7.0273,  ..., -0.3374, -0.5967, -2.4766]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 03:01:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To integrate again is to reintegrate
To locate again is to relocate
To arrange again is to rearrange
To marry again is to remarry
To investigate again is to reinvestigate
To write again is to rewrite
To establish again is to reestablish
To unite again is to
2024-07-17 03:01:21 root INFO     [order_1_approx] starting weight calculation for To locate again is to relocate
To arrange again is to rearrange
To integrate again is to reintegrate
To write again is to rewrite
To establish again is to reestablish
To unite again is to reunite
To investigate again is to reinvestigate
To marry again is to
2024-07-17 03:01:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 03:05:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1155, -0.1395,  1.2305,  ...,  0.4810, -0.1174,  0.2681],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7988, -2.5625, -4.4375,  ..., -4.0938,  0.1678, -0.4688],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0031, -0.0081,  0.0167,  ..., -0.0202,  0.0040, -0.0025],
        [-0.0010, -0.0032, -0.0022,  ...,  0.0044,  0.0136,  0.0039],
        [-0.0073,  0.0019, -0.0019,  ...,  0.0093,  0.0014, -0.0041],
        ...,
        [-0.0079, -0.0102, -0.0036,  ..., -0.0204,  0.0060,  0.0092],
        [ 0.0066, -0.0008,  0.0008,  ...,  0.0131, -0.0079, -0.0093],
        [ 0.0075,  0.0046,  0.0106,  ..., -0.0019,  0.0062,  0.0010]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6729, -2.1367, -4.6211,  ..., -5.1367,  0.1699, -1.0723]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 03:05:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To locate again is to relocate
To arrange again is to rearrange
To integrate again is to reintegrate
To write again is to rewrite
To establish again is to reestablish
To unite again is to reunite
To investigate again is to reinvestigate
To marry again is to
2024-07-17 03:05:08 root INFO     [order_1_approx] starting weight calculation for To integrate again is to reintegrate
To establish again is to reestablish
To investigate again is to reinvestigate
To unite again is to reunite
To write again is to rewrite
To marry again is to remarry
To arrange again is to rearrange
To locate again is to
2024-07-17 03:05:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 03:08:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3164,  0.5234, -0.3545,  ...,  0.3569,  0.6260,  0.1895],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3867,  0.5991, -7.6367,  ..., -2.2734,  2.1641,  2.5527],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.2970e-03, -1.5244e-02, -4.5471e-03,  ..., -7.2098e-04,
          1.3351e-05, -9.3079e-03],
        [-8.4381e-03,  9.4833e-03,  1.4618e-02,  ..., -1.0704e-02,
          7.1869e-03,  7.8812e-03],
        [ 2.0294e-02, -1.3943e-03, -2.0203e-02,  ...,  7.8735e-03,
         -1.4687e-03, -8.6212e-03],
        ...,
        [-1.5854e-02, -6.7215e-03, -2.0771e-03,  ..., -7.3242e-03,
         -6.9084e-03, -5.3482e-03],
        [ 5.8746e-04, -4.6082e-03, -1.1292e-02,  ..., -2.9793e-03,
         -1.8372e-02,  1.4067e-04],
        [ 5.1689e-03,  4.6692e-03,  1.5533e-02,  ...,  2.1439e-03,
          6.6223e-03, -1.3420e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1504,  0.2163, -8.2656,  ..., -1.9854,  2.3984,  2.5176]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 03:08:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To integrate again is to reintegrate
To establish again is to reestablish
To investigate again is to reinvestigate
To unite again is to reunite
To write again is to rewrite
To marry again is to remarry
To arrange again is to rearrange
To locate again is to
2024-07-17 03:08:55 root INFO     [order_1_approx] starting weight calculation for To write again is to rewrite
To investigate again is to reinvestigate
To marry again is to remarry
To locate again is to relocate
To integrate again is to reintegrate
To unite again is to reunite
To establish again is to reestablish
To arrange again is to
2024-07-17 03:08:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 03:12:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2910,  0.5410,  1.1025,  ..., -0.0632,  0.3384,  0.8960],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0361,  0.7158, -5.1641,  ...,  1.6494,  0.5933,  2.1270],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0016, -0.0112,  0.0023,  ..., -0.0072,  0.0060, -0.0146],
        [ 0.0095, -0.0081,  0.0236,  ..., -0.0126,  0.0126,  0.0170],
        [ 0.0075, -0.0044, -0.0137,  ...,  0.0008,  0.0106, -0.0172],
        ...,
        [-0.0115, -0.0246,  0.0071,  ..., -0.0106,  0.0048,  0.0232],
        [-0.0007, -0.0120,  0.0040,  ..., -0.0111, -0.0073,  0.0106],
        [ 0.0065,  0.0222,  0.0004,  ..., -0.0019,  0.0125, -0.0002]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3379,  1.1211, -6.0156,  ...,  2.3574,  0.9546,  2.1641]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 03:12:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To write again is to rewrite
To investigate again is to reinvestigate
To marry again is to remarry
To locate again is to relocate
To integrate again is to reintegrate
To unite again is to reunite
To establish again is to reestablish
To arrange again is to
2024-07-17 03:12:42 root INFO     [order_1_approx] starting weight calculation for To marry again is to remarry
To unite again is to reunite
To establish again is to reestablish
To arrange again is to rearrange
To locate again is to relocate
To write again is to rewrite
To integrate again is to reintegrate
To investigate again is to
2024-07-17 03:12:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 03:16:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9678,  0.1104,  0.6196,  ..., -0.1741,  0.0825,  0.9756],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7812,  1.8018, -7.1719,  ...,  1.2441, -0.5215, -1.3037],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0129, -0.0082, -0.0028,  ..., -0.0175,  0.0083, -0.0128],
        [-0.0054, -0.0027,  0.0046,  ...,  0.0053, -0.0046, -0.0021],
        [ 0.0313,  0.0158, -0.0091,  ..., -0.0007,  0.0186,  0.0034],
        ...,
        [-0.0288, -0.0201,  0.0016,  ..., -0.0220,  0.0107,  0.0093],
        [ 0.0204,  0.0098,  0.0193,  ..., -0.0135, -0.0245, -0.0008],
        [ 0.0145,  0.0209,  0.0142,  ..., -0.0053,  0.0224, -0.0079]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.2031,  2.2051, -7.6875,  ...,  1.1953, -0.5342, -1.6816]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 03:16:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To marry again is to remarry
To unite again is to reunite
To establish again is to reestablish
To arrange again is to rearrange
To locate again is to relocate
To write again is to rewrite
To integrate again is to reintegrate
To investigate again is to
2024-07-17 03:16:28 root INFO     total operator prediction time: 1812.1342391967773 seconds
2024-07-17 03:16:28 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on adj+ness_reg
2024-07-17 03:16:28 root INFO     building operator adj+ness_reg
2024-07-17 03:16:28 root INFO     [order_1_approx] starting weight calculation for The state of being happy is happiness
The state of being situated is situatedness
The state of being righteous is righteousness
The state of being interesting is interestingness
The state of being rare is rareness
The state of being competitive is competitiveness
The state of being extensive is extensiveness
The state of being distinctive is
2024-07-17 03:16:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 03:20:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9775, -0.0308,  1.0693,  ...,  1.7422,  2.2285, -0.9160],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.6953,  0.5830,  1.2754,  ..., -3.6406, -2.8027,  1.1289],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0055, -0.0068,  0.0113,  ...,  0.0009,  0.0076, -0.0296],
        [ 0.0037, -0.0017,  0.0090,  ..., -0.0078,  0.0119,  0.0067],
        [ 0.0002, -0.0062, -0.0051,  ...,  0.0045,  0.0063,  0.0028],
        ...,
        [-0.0196, -0.0104,  0.0029,  ..., -0.0222,  0.0078, -0.0012],
        [ 0.0139,  0.0118,  0.0113,  ..., -0.0087, -0.0308, -0.0122],
        [-0.0235,  0.0094,  0.0104,  ..., -0.0054,  0.0072, -0.0121]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.2109,  0.8135,  1.7158,  ..., -3.8027, -2.9766,  0.9316]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 03:20:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being happy is happiness
The state of being situated is situatedness
The state of being righteous is righteousness
The state of being interesting is interestingness
The state of being rare is rareness
The state of being competitive is competitiveness
The state of being extensive is extensiveness
The state of being distinctive is
2024-07-17 03:20:15 root INFO     [order_1_approx] starting weight calculation for The state of being interesting is interestingness
The state of being righteous is righteousness
The state of being happy is happiness
The state of being extensive is extensiveness
The state of being rare is rareness
The state of being situated is situatedness
The state of being distinctive is distinctiveness
The state of being competitive is
2024-07-17 03:20:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 03:24:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5576,  0.4604,  1.2227,  ...,  0.1018,  1.9473, -0.0923],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-6.1406, -0.4548, -3.7559,  ...,  1.2363,  1.9590,  2.3105],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0023,  0.0008,  0.0149,  ..., -0.0187,  0.0019, -0.0197],
        [-0.0121, -0.0103,  0.0050,  ...,  0.0053, -0.0058, -0.0161],
        [ 0.0083,  0.0037,  0.0023,  ..., -0.0051,  0.0143,  0.0117],
        ...,
        [-0.0138, -0.0056,  0.0087,  ..., -0.0077,  0.0021, -0.0175],
        [-0.0077,  0.0072,  0.0023,  ..., -0.0084, -0.0035,  0.0101],
        [-0.0101,  0.0156,  0.0057,  ..., -0.0055,  0.0188, -0.0069]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-6.2773,  0.7656, -3.9707,  ...,  1.0273,  2.0430,  3.2285]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 03:24:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being interesting is interestingness
The state of being righteous is righteousness
The state of being happy is happiness
The state of being extensive is extensiveness
The state of being rare is rareness
The state of being situated is situatedness
The state of being distinctive is distinctiveness
The state of being competitive is
2024-07-17 03:24:01 root INFO     [order_1_approx] starting weight calculation for The state of being competitive is competitiveness
The state of being righteous is righteousness
The state of being extensive is extensiveness
The state of being situated is situatedness
The state of being happy is happiness
The state of being rare is rareness
The state of being distinctive is distinctiveness
The state of being interesting is
2024-07-17 03:24:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 03:27:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4695,  0.4028,  0.4453,  ...,  1.6074,  1.2969, -0.3018],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6240,  3.8672, -0.3867,  ...,  1.0049,  0.3538,  7.5234],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0122, -0.0074,  0.0055,  ..., -0.0044, -0.0054, -0.0434],
        [ 0.0030, -0.0032,  0.0141,  ..., -0.0014, -0.0089, -0.0074],
        [ 0.0058, -0.0108, -0.0167,  ...,  0.0010,  0.0058,  0.0003],
        ...,
        [-0.0123,  0.0112,  0.0050,  ..., -0.0085, -0.0082, -0.0141],
        [-0.0067,  0.0014,  0.0062,  ..., -0.0140, -0.0195, -0.0141],
        [-0.0069,  0.0192, -0.0119,  ..., -0.0308,  0.0017, -0.0267]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1128,  4.2656, -0.4319,  ...,  1.2266,  0.9502,  8.2969]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 03:27:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being competitive is competitiveness
The state of being righteous is righteousness
The state of being extensive is extensiveness
The state of being situated is situatedness
The state of being happy is happiness
The state of being rare is rareness
The state of being distinctive is distinctiveness
The state of being interesting is
2024-07-17 03:27:48 root INFO     [order_1_approx] starting weight calculation for The state of being happy is happiness
The state of being rare is rareness
The state of being competitive is competitiveness
The state of being interesting is interestingness
The state of being righteous is righteousness
The state of being distinctive is distinctiveness
The state of being extensive is extensiveness
The state of being situated is
2024-07-17 03:27:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 03:31:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6812, -1.2305,  0.3745,  ...,  0.1887,  1.3994, -0.6382],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.2852,  4.2266,  1.7363,  ..., -2.5566, -2.7695,  3.5898],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.3618e-02, -1.6724e-02,  8.6212e-04,  ...,  4.2191e-03,
         -2.6226e-05, -3.2593e-02],
        [-3.2692e-03, -1.0460e-02, -1.5144e-03,  ..., -7.9269e-03,
         -5.8365e-03, -1.3428e-03],
        [-1.6907e-02, -1.3954e-02, -2.4734e-02,  ..., -9.7122e-03,
          5.0545e-03,  8.7814e-03],
        ...,
        [-3.3875e-02, -6.7062e-03, -1.5392e-03,  ..., -1.9531e-02,
         -4.2839e-03,  1.9217e-03],
        [ 1.7212e-02,  1.6266e-02,  1.3046e-02,  ..., -4.3297e-03,
         -2.8137e-02, -1.3123e-02],
        [-2.7039e-02,  1.7319e-02,  4.6005e-03,  ..., -9.9945e-03,
          3.1548e-03, -3.0579e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.1094,  4.7188,  1.5264,  ..., -2.8281, -2.6445,  4.7891]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 03:31:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being happy is happiness
The state of being rare is rareness
The state of being competitive is competitiveness
The state of being interesting is interestingness
The state of being righteous is righteousness
The state of being distinctive is distinctiveness
The state of being extensive is extensiveness
The state of being situated is
2024-07-17 03:31:35 root INFO     [order_1_approx] starting weight calculation for The state of being competitive is competitiveness
The state of being extensive is extensiveness
The state of being situated is situatedness
The state of being interesting is interestingness
The state of being happy is happiness
The state of being righteous is righteousness
The state of being distinctive is distinctiveness
The state of being rare is
2024-07-17 03:31:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 03:35:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6592,  0.1384,  0.1552,  ...,  0.6318,  2.1426, -0.3789],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-5.0664,  2.8730,  2.8242,  ...,  0.9604,  1.8496,  5.1445],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0132, -0.0004,  0.0137,  ..., -0.0103, -0.0015, -0.0304],
        [-0.0140, -0.0222,  0.0190,  ..., -0.0073, -0.0065, -0.0112],
        [-0.0014, -0.0142,  0.0079,  ...,  0.0029,  0.0042,  0.0173],
        ...,
        [ 0.0021, -0.0070,  0.0011,  ..., -0.0196, -0.0028, -0.0140],
        [-0.0060, -0.0214,  0.0205,  ..., -0.0130, -0.0193,  0.0019],
        [-0.0188, -0.0041,  0.0031,  ..., -0.0238,  0.0053, -0.0371]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.2891,  3.9414,  3.4023,  ...,  0.6748,  1.5508,  6.0156]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 03:35:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being competitive is competitiveness
The state of being extensive is extensiveness
The state of being situated is situatedness
The state of being interesting is interestingness
The state of being happy is happiness
The state of being righteous is righteousness
The state of being distinctive is distinctiveness
The state of being rare is
2024-07-17 03:35:24 root INFO     [order_1_approx] starting weight calculation for The state of being distinctive is distinctiveness
The state of being competitive is competitiveness
The state of being righteous is righteousness
The state of being situated is situatedness
The state of being interesting is interestingness
The state of being rare is rareness
The state of being extensive is extensiveness
The state of being happy is
2024-07-17 03:35:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 03:39:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9893,  0.8032,  0.6655,  ...,  0.8237,  2.0781, -0.2759],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0620,  2.5117, -0.7534,  ..., -2.1816,  2.9648,  4.7656],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.0208e-02, -1.3199e-02,  1.2177e-02,  ...,  7.9117e-03,
         -3.5400e-03, -2.2171e-02],
        [ 1.5190e-02, -1.7380e-02,  1.8494e-02,  ..., -8.5144e-03,
         -8.7128e-03, -1.3741e-02],
        [-1.0574e-02, -4.1428e-03, -6.6452e-03,  ..., -7.9880e-03,
          1.8341e-02,  3.4580e-03],
        ...,
        [-1.3916e-02, -4.5776e-05, -5.7335e-03,  ..., -5.9547e-03,
          1.8234e-03, -1.7929e-04],
        [-1.2230e-02,  6.8970e-03,  1.5802e-03,  ..., -8.4534e-03,
         -2.4490e-02, -6.1531e-03],
        [-1.8616e-02,  7.9727e-03,  1.8509e-02,  ...,  7.8506e-03,
          1.6479e-02, -1.8875e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6089,  2.1309, -0.6387,  ..., -2.2305,  2.6738,  5.3750]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 03:39:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being distinctive is distinctiveness
The state of being competitive is competitiveness
The state of being righteous is righteousness
The state of being situated is situatedness
The state of being interesting is interestingness
The state of being rare is rareness
The state of being extensive is extensiveness
The state of being happy is
2024-07-17 03:39:10 root INFO     [order_1_approx] starting weight calculation for The state of being competitive is competitiveness
The state of being situated is situatedness
The state of being rare is rareness
The state of being happy is happiness
The state of being distinctive is distinctiveness
The state of being interesting is interestingness
The state of being extensive is extensiveness
The state of being righteous is
2024-07-17 03:39:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 03:42:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.7217,  0.0958,  0.3359,  ...,  1.1973,  1.6172, -0.7480],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.2139,  4.8438, -4.0898,  ..., -0.0283, -0.2681,  6.8828],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0105, -0.0185,  0.0120,  ..., -0.0124, -0.0181, -0.0219],
        [-0.0005, -0.0061, -0.0054,  ...,  0.0001,  0.0021, -0.0054],
        [-0.0083, -0.0103, -0.0188,  ...,  0.0177,  0.0103,  0.0147],
        ...,
        [-0.0203, -0.0124, -0.0079,  ..., -0.0202,  0.0142, -0.0101],
        [-0.0118, -0.0018,  0.0110,  ..., -0.0041, -0.0203, -0.0125],
        [-0.0280,  0.0061,  0.0145,  ..., -0.0114, -0.0023, -0.0246]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3535,  4.8398, -3.4316,  ...,  0.6064, -0.5435,  7.5703]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 03:42:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being competitive is competitiveness
The state of being situated is situatedness
The state of being rare is rareness
The state of being happy is happiness
The state of being distinctive is distinctiveness
The state of being interesting is interestingness
The state of being extensive is extensiveness
The state of being righteous is
2024-07-17 03:42:58 root INFO     [order_1_approx] starting weight calculation for The state of being rare is rareness
The state of being situated is situatedness
The state of being interesting is interestingness
The state of being distinctive is distinctiveness
The state of being competitive is competitiveness
The state of being righteous is righteousness
The state of being happy is happiness
The state of being extensive is
2024-07-17 03:42:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 03:46:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3704,  1.3154,  0.4780,  ..., -0.4238,  1.5557,  0.2429],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-5.2852,  2.2598,  4.2227,  ...,  1.4229, -4.1133,  2.3789],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0154, -0.0151, -0.0051,  ..., -0.0084, -0.0029, -0.0245],
        [ 0.0036, -0.0060, -0.0023,  ...,  0.0172, -0.0327, -0.0086],
        [-0.0034,  0.0013, -0.0160,  ..., -0.0037,  0.0118, -0.0239],
        ...,
        [-0.0075, -0.0083,  0.0032,  ...,  0.0080, -0.0105,  0.0119],
        [ 0.0023, -0.0042,  0.0070,  ..., -0.0252,  0.0051, -0.0031],
        [-0.0144,  0.0204, -0.0106,  ...,  0.0031,  0.0104, -0.0091]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.7227,  2.0391,  4.3164,  ...,  2.5938, -4.2734,  2.7051]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 03:46:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being rare is rareness
The state of being situated is situatedness
The state of being interesting is interestingness
The state of being distinctive is distinctiveness
The state of being competitive is competitiveness
The state of being righteous is righteousness
The state of being happy is happiness
The state of being extensive is
2024-07-17 03:46:44 root INFO     total operator prediction time: 1816.439646244049 seconds
2024-07-17 03:46:44 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on noun+less_reg
2024-07-17 03:46:44 root INFO     building operator noun+less_reg
2024-07-17 03:46:44 root INFO     [order_1_approx] starting weight calculation for Something without tooth is toothless
Something without heir is heirless
Something without law is lawless
Something without ego is egoless
Something without goal is goalless
Something without faith is faithless
Something without carbon is carbonless
Something without heart is
2024-07-17 03:46:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 03:50:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6514,  0.3755, -0.1118,  ..., -0.2391,  0.0830,  1.0293],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.4287, -1.2627, -0.9424,  ..., -2.9668,  4.9688,  3.5723],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0255, -0.0010,  0.0101,  ..., -0.0052, -0.0096, -0.0171],
        [-0.0169, -0.0219, -0.0039,  ..., -0.0013, -0.0072,  0.0059],
        [-0.0047,  0.0082, -0.0197,  ...,  0.0153,  0.0166,  0.0035],
        ...,
        [ 0.0155, -0.0076,  0.0034,  ...,  0.0013, -0.0107, -0.0107],
        [-0.0055, -0.0216,  0.0168,  ...,  0.0142, -0.0296,  0.0084],
        [-0.0087,  0.0003,  0.0027,  ...,  0.0071,  0.0001, -0.0329]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3809, -1.0645, -0.5029,  ..., -3.1973,  4.5781,  2.6211]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 03:50:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without tooth is toothless
Something without heir is heirless
Something without law is lawless
Something without ego is egoless
Something without goal is goalless
Something without faith is faithless
Something without carbon is carbonless
Something without heart is
2024-07-17 03:50:33 root INFO     [order_1_approx] starting weight calculation for Something without ego is egoless
Something without heart is heartless
Something without carbon is carbonless
Something without tooth is toothless
Something without faith is faithless
Something without law is lawless
Something without heir is heirless
Something without goal is
2024-07-17 03:50:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 03:54:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5791,  1.4570, -0.1553,  ..., -0.3496,  2.6289,  0.7871],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0645,  0.8398, -1.4541,  ...,  0.2593,  2.9531,  1.7217],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.4078e-02, -1.0101e-02,  1.3794e-02,  ..., -8.7738e-05,
         -1.6266e-02, -9.6817e-03],
        [-3.0060e-03, -2.0737e-02, -4.6635e-04,  ...,  1.1581e-02,
         -1.1162e-02, -6.3553e-03],
        [ 1.7605e-03,  6.4850e-04, -1.7288e-02,  ...,  7.1335e-04,
          2.4300e-03, -3.3379e-03],
        ...,
        [ 6.1493e-03, -1.3527e-02,  2.8706e-04,  ..., -3.0884e-02,
          1.9970e-03, -7.2517e-03],
        [ 2.4338e-03,  3.2196e-03,  3.4275e-03,  ..., -7.9727e-03,
         -9.4147e-03,  3.7918e-03],
        [-4.0131e-03,  1.4854e-02,  7.9346e-03,  ...,  1.5274e-02,
          2.7275e-03, -2.2491e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1680,  0.5796, -1.4814,  ..., -0.3584,  2.6328,  1.2920]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 03:54:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without ego is egoless
Something without heart is heartless
Something without carbon is carbonless
Something without tooth is toothless
Something without faith is faithless
Something without law is lawless
Something without heir is heirless
Something without goal is
2024-07-17 03:54:20 root INFO     [order_1_approx] starting weight calculation for Something without heart is heartless
Something without goal is goalless
Something without faith is faithless
Something without ego is egoless
Something without tooth is toothless
Something without law is lawless
Something without heir is heirless
Something without carbon is
2024-07-17 03:54:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 03:58:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.6104,  0.8203, -1.7021,  ...,  0.6777,  0.5078,  0.6084],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6113, -3.1191,  1.5664,  ..., -0.8154, -0.2979,  1.7754],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0074, -0.0028, -0.0033,  ...,  0.0051, -0.0059, -0.0364],
        [-0.0068, -0.0156,  0.0049,  ..., -0.0072, -0.0019,  0.0004],
        [-0.0112, -0.0019, -0.0095,  ..., -0.0044, -0.0017,  0.0163],
        ...,
        [-0.0086,  0.0083,  0.0040,  ..., -0.0126,  0.0036, -0.0025],
        [ 0.0081, -0.0090,  0.0101,  ..., -0.0111, -0.0039,  0.0065],
        [-0.0113,  0.0075,  0.0075,  ...,  0.0028,  0.0145, -0.0107]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2881, -2.5156,  1.0781,  ..., -1.2510,  0.7354,  1.7393]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 03:58:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without heart is heartless
Something without goal is goalless
Something without faith is faithless
Something without ego is egoless
Something without tooth is toothless
Something without law is lawless
Something without heir is heirless
Something without carbon is
2024-07-17 03:58:06 root INFO     [order_1_approx] starting weight calculation for Something without goal is goalless
Something without heart is heartless
Something without ego is egoless
Something without carbon is carbonless
Something without tooth is toothless
Something without law is lawless
Something without faith is faithless
Something without heir is
2024-07-17 03:58:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 04:01:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5029,  1.2764, -0.3596,  ...,  1.7168,  1.3418,  1.3867],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4316, -0.2686, -4.3555,  ..., -2.6562, -1.6777,  3.6934],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0195,  0.0007,  0.0041,  ...,  0.0084, -0.0096, -0.0139],
        [-0.0123, -0.0179, -0.0275,  ...,  0.0051, -0.0077, -0.0201],
        [-0.0020, -0.0020, -0.0278,  ..., -0.0096,  0.0040, -0.0012],
        ...,
        [-0.0027, -0.0173, -0.0235,  ..., -0.0207, -0.0149, -0.0077],
        [ 0.0021,  0.0037,  0.0141,  ...,  0.0069, -0.0430, -0.0065],
        [-0.0115,  0.0424, -0.0013,  ..., -0.0097, -0.0021, -0.0233]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.8242,  0.7061, -4.0703,  ..., -1.8789, -1.3633,  3.8105]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 04:01:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without goal is goalless
Something without heart is heartless
Something without ego is egoless
Something without carbon is carbonless
Something without tooth is toothless
Something without law is lawless
Something without faith is faithless
Something without heir is
2024-07-17 04:01:54 root INFO     [order_1_approx] starting weight calculation for Something without heir is heirless
Something without law is lawless
Something without ego is egoless
Something without goal is goalless
Something without carbon is carbonless
Something without heart is heartless
Something without tooth is toothless
Something without faith is
2024-07-17 04:01:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 04:05:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5947, -1.0498,  0.9004,  ...,  0.5439,  0.5889, -0.3481],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7510,  3.8477, -0.0186,  ...,  2.0312,  2.9062,  0.1973],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0291, -0.0130,  0.0174,  ...,  0.0020, -0.0163, -0.0357],
        [-0.0115, -0.0380,  0.0060,  ...,  0.0080, -0.0100,  0.0054],
        [ 0.0150, -0.0020, -0.0289,  ...,  0.0186,  0.0155, -0.0040],
        ...,
        [ 0.0142, -0.0020, -0.0010,  ..., -0.0307, -0.0004, -0.0020],
        [ 0.0062, -0.0185,  0.0133,  ...,  0.0026, -0.0299,  0.0009],
        [-0.0071,  0.0224,  0.0077,  ...,  0.0130, -0.0087, -0.0386]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3516,  4.7539, -0.8335,  ...,  1.4727,  2.3066, -0.5518]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 04:05:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without heir is heirless
Something without law is lawless
Something without ego is egoless
Something without goal is goalless
Something without carbon is carbonless
Something without heart is heartless
Something without tooth is toothless
Something without faith is
2024-07-17 04:05:40 root INFO     [order_1_approx] starting weight calculation for Something without heir is heirless
Something without faith is faithless
Something without goal is goalless
Something without tooth is toothless
Something without carbon is carbonless
Something without heart is heartless
Something without ego is egoless
Something without law is
2024-07-17 04:05:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 04:09:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0156, -0.1077,  0.4106,  ..., -0.1709,  0.4087,  0.6621],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.4023, -2.5117, -3.5625,  ...,  1.0586,  5.2656,  1.5791],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0095, -0.0023,  0.0055,  ...,  0.0091, -0.0240, -0.0151],
        [-0.0093, -0.0258, -0.0051,  ..., -0.0059, -0.0135, -0.0003],
        [ 0.0037,  0.0131, -0.0234,  ..., -0.0137,  0.0051, -0.0149],
        ...,
        [ 0.0273,  0.0060,  0.0052,  ...,  0.0038, -0.0017,  0.0055],
        [-0.0065, -0.0200,  0.0070,  ..., -0.0184, -0.0102, -0.0168],
        [-0.0044,  0.0059, -0.0082,  ..., -0.0114,  0.0059, -0.0142]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.1641, -2.4922, -3.6465,  ...,  0.4209,  4.8125,  0.5957]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 04:09:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without heir is heirless
Something without faith is faithless
Something without goal is goalless
Something without tooth is toothless
Something without carbon is carbonless
Something without heart is heartless
Something without ego is egoless
Something without law is
2024-07-17 04:09:26 root INFO     [order_1_approx] starting weight calculation for Something without goal is goalless
Something without heir is heirless
Something without law is lawless
Something without heart is heartless
Something without faith is faithless
Something without carbon is carbonless
Something without ego is egoless
Something without tooth is
2024-07-17 04:09:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 04:13:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9644, -0.2463, -0.9570,  ...,  0.3467, -0.1812, -0.1464],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2300,  1.4824,  0.3911,  ..., -1.9639,  1.1191, -0.1530],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0004, -0.0112,  0.0117,  ...,  0.0072, -0.0142, -0.0056],
        [-0.0104, -0.0180, -0.0046,  ..., -0.0235,  0.0009,  0.0075],
        [ 0.0088,  0.0047, -0.0180,  ..., -0.0036,  0.0036, -0.0065],
        ...,
        [ 0.0209, -0.0026,  0.0095,  ..., -0.0164, -0.0014,  0.0002],
        [ 0.0040, -0.0163,  0.0164,  ...,  0.0111, -0.0245, -0.0044],
        [-0.0162,  0.0090,  0.0074,  ..., -0.0055,  0.0021, -0.0263]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3721,  1.6016,  0.5684,  ..., -2.1289,  1.0010, -0.7090]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 04:13:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without goal is goalless
Something without heir is heirless
Something without law is lawless
Something without heart is heartless
Something without faith is faithless
Something without carbon is carbonless
Something without ego is egoless
Something without tooth is
2024-07-17 04:13:12 root INFO     [order_1_approx] starting weight calculation for Something without law is lawless
Something without carbon is carbonless
Something without tooth is toothless
Something without heart is heartless
Something without goal is goalless
Something without faith is faithless
Something without heir is heirless
Something without ego is
2024-07-17 04:13:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 04:16:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6143, -0.1416, -0.3823,  ..., -0.7412,  1.6191,  1.3838],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4395, -1.0508,  0.1650,  ...,  1.1904, -0.9351, -0.5957],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0102, -0.0192,  0.0154,  ..., -0.0131, -0.0235, -0.0284],
        [-0.0158, -0.0311,  0.0057,  ...,  0.0002, -0.0097, -0.0132],
        [-0.0072,  0.0244, -0.0386,  ...,  0.0134,  0.0096,  0.0017],
        ...,
        [-0.0013, -0.0054,  0.0012,  ..., -0.0104,  0.0089,  0.0016],
        [ 0.0072, -0.0159,  0.0185,  ...,  0.0054, -0.0168,  0.0040],
        [-0.0281,  0.0210, -0.0007,  ..., -0.0156,  0.0017, -0.0360]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2500, -0.5913, -0.3662,  ...,  1.2539, -1.2334, -0.5229]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 04:16:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without law is lawless
Something without carbon is carbonless
Something without tooth is toothless
Something without heart is heartless
Something without goal is goalless
Something without faith is faithless
Something without heir is heirless
Something without ego is
2024-07-17 04:16:57 root INFO     total operator prediction time: 1812.4801366329193 seconds
2024-07-17 04:16:57 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+ment_irreg
2024-07-17 04:16:57 root INFO     building operator verb+ment_irreg
2024-07-17 04:16:57 root INFO     [order_1_approx] starting weight calculation for To displace results in a displacement
To appoint results in a appointment
To detach results in a detachment
To establish results in a establishment
To announce results in a announcement
To resent results in a resentment
To infringe results in a infringement
To embarrass results in a
2024-07-17 04:16:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 04:20:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4409, -0.3970, -0.0368,  ...,  0.9580, -0.0930,  0.2856],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.1641,  2.6641, -2.8281,  ...,  5.9844, -1.2832,  5.3164],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0243, -0.0143,  0.0030,  ...,  0.0073, -0.0046, -0.0058],
        [-0.0159, -0.0261,  0.0112,  ..., -0.0008, -0.0045, -0.0056],
        [-0.0159,  0.0206,  0.0018,  ...,  0.0030,  0.0247,  0.0160],
        ...,
        [-0.0164, -0.0214, -0.0105,  ..., -0.0110, -0.0076, -0.0197],
        [ 0.0091,  0.0191, -0.0128,  ..., -0.0056, -0.0065,  0.0168],
        [-0.0073,  0.0339, -0.0016,  ...,  0.0073,  0.0048, -0.0063]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5352,  3.0938, -3.4297,  ...,  5.6875, -1.2568,  6.4062]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 04:20:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To displace results in a displacement
To appoint results in a appointment
To detach results in a detachment
To establish results in a establishment
To announce results in a announcement
To resent results in a resentment
To infringe results in a infringement
To embarrass results in a
2024-07-17 04:20:43 root INFO     [order_1_approx] starting weight calculation for To detach results in a detachment
To announce results in a announcement
To resent results in a resentment
To embarrass results in a embarrassment
To displace results in a displacement
To infringe results in a infringement
To appoint results in a appointment
To establish results in a
2024-07-17 04:20:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 04:24:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.4009, 0.6069, 0.1620,  ..., 1.4482, 0.2505, 0.3711], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([1.7910, 0.6089, 0.1011,  ..., 2.9141, 0.0405, 7.1562], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0037, -0.0006,  0.0094,  ..., -0.0153,  0.0096, -0.0215],
        [ 0.0031, -0.0080, -0.0038,  ..., -0.0002, -0.0033, -0.0003],
        [-0.0016,  0.0053, -0.0038,  ...,  0.0041, -0.0007, -0.0103],
        ...,
        [-0.0124,  0.0028, -0.0140,  ..., -0.0157, -0.0053, -0.0010],
        [-0.0058,  0.0231, -0.0024,  ..., -0.0006, -0.0026,  0.0117],
        [-0.0102,  0.0294, -0.0018,  ..., -0.0060, -0.0040, -0.0148]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[1.5107, 1.7090, 0.4446,  ..., 2.7305, 0.4429, 7.8359]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 04:24:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To detach results in a detachment
To announce results in a announcement
To resent results in a resentment
To embarrass results in a embarrassment
To displace results in a displacement
To infringe results in a infringement
To appoint results in a appointment
To establish results in a
2024-07-17 04:24:30 root INFO     [order_1_approx] starting weight calculation for To displace results in a displacement
To detach results in a detachment
To infringe results in a infringement
To establish results in a establishment
To embarrass results in a embarrassment
To resent results in a resentment
To announce results in a announcement
To appoint results in a
2024-07-17 04:24:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 04:28:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7080, -0.5781,  0.6436,  ...,  1.1426,  0.2104,  0.4390],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1992,  2.1406, -1.3965,  ...,  2.9883, -0.9312,  2.5996],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0068, -0.0012,  0.0050,  ..., -0.0076,  0.0272, -0.0071],
        [ 0.0011, -0.0112,  0.0139,  ...,  0.0036, -0.0094, -0.0026],
        [-0.0080,  0.0066, -0.0048,  ...,  0.0121, -0.0021, -0.0046],
        ...,
        [-0.0172, -0.0058, -0.0099,  ..., -0.0035, -0.0011,  0.0094],
        [ 0.0022,  0.0010,  0.0027,  ..., -0.0002, -0.0194,  0.0149],
        [ 0.0020,  0.0191,  0.0047,  ...,  0.0006, -0.0037, -0.0074]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7266,  2.5762, -1.4131,  ...,  3.7500, -1.0732,  2.7793]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 04:28:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To displace results in a displacement
To detach results in a detachment
To infringe results in a infringement
To establish results in a establishment
To embarrass results in a embarrassment
To resent results in a resentment
To announce results in a announcement
To appoint results in a
2024-07-17 04:28:17 root INFO     [order_1_approx] starting weight calculation for To embarrass results in a embarrassment
To detach results in a detachment
To establish results in a establishment
To displace results in a displacement
To infringe results in a infringement
To announce results in a announcement
To appoint results in a appointment
To resent results in a
2024-07-17 04:28:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 04:32:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.2031, -0.7607,  0.0773,  ...,  0.3545,  0.5410, -0.5332],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.8164,  5.7656,  0.9370,  ...,  5.6562, -1.7100,  4.1289],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0028,  0.0129, -0.0051,  ...,  0.0086,  0.0008, -0.0125],
        [-0.0027, -0.0275,  0.0116,  ..., -0.0167,  0.0007,  0.0029],
        [ 0.0012,  0.0025, -0.0043,  ...,  0.0057,  0.0092, -0.0077],
        ...,
        [-0.0180, -0.0297, -0.0147,  ..., -0.0093,  0.0028, -0.0115],
        [-0.0149,  0.0056,  0.0009,  ..., -0.0015, -0.0210,  0.0153],
        [-0.0176,  0.0278,  0.0150,  ..., -0.0284, -0.0141, -0.0055]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.8535,  6.4062,  0.4978,  ...,  5.0352, -1.0664,  4.4805]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 04:32:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To embarrass results in a embarrassment
To detach results in a detachment
To establish results in a establishment
To displace results in a displacement
To infringe results in a infringement
To announce results in a announcement
To appoint results in a appointment
To resent results in a
2024-07-17 04:32:06 root INFO     [order_1_approx] starting weight calculation for To establish results in a establishment
To announce results in a announcement
To infringe results in a infringement
To resent results in a resentment
To appoint results in a appointment
To detach results in a detachment
To embarrass results in a embarrassment
To displace results in a
2024-07-17 04:32:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 04:35:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9277, -0.8423,  0.5630,  ...,  0.9316, -0.1168, -0.8877],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2808,  0.5127, -2.2812,  ...,  0.8047, -2.5234,  5.4844],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0156,  0.0003,  0.0001,  ..., -0.0095, -0.0120, -0.0083],
        [-0.0149,  0.0049,  0.0121,  ..., -0.0066, -0.0047, -0.0056],
        [ 0.0245,  0.0075, -0.0103,  ..., -0.0126,  0.0152, -0.0111],
        ...,
        [-0.0276, -0.0320, -0.0299,  ..., -0.0152,  0.0101,  0.0141],
        [-0.0137,  0.0190,  0.0209,  ..., -0.0066, -0.0223,  0.0064],
        [-0.0023,  0.0333, -0.0142,  ...,  0.0040,  0.0191, -0.0395]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1499,  1.2998, -2.9277,  ...,  0.6304, -2.2832,  4.8359]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 04:35:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To establish results in a establishment
To announce results in a announcement
To infringe results in a infringement
To resent results in a resentment
To appoint results in a appointment
To detach results in a detachment
To embarrass results in a embarrassment
To displace results in a
2024-07-17 04:35:54 root INFO     [order_1_approx] starting weight calculation for To resent results in a resentment
To establish results in a establishment
To displace results in a displacement
To infringe results in a infringement
To appoint results in a appointment
To embarrass results in a embarrassment
To announce results in a announcement
To detach results in a
2024-07-17 04:35:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 04:39:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0427,  0.0405,  0.4622,  ...,  0.1570, -0.5850,  0.9131],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0396, -1.0010, -0.2886,  ..., -0.8643,  0.3047,  3.2793],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0075, -0.0112,  0.0085,  ..., -0.0090,  0.0029, -0.0228],
        [ 0.0024, -0.0173,  0.0114,  ..., -0.0188, -0.0063, -0.0056],
        [-0.0063,  0.0171,  0.0001,  ...,  0.0025, -0.0066, -0.0085],
        ...,
        [-0.0297, -0.0084, -0.0019,  ..., -0.0148, -0.0016, -0.0013],
        [-0.0137,  0.0003,  0.0063,  ...,  0.0005, -0.0095,  0.0156],
        [-0.0059,  0.0268,  0.0075,  ..., -0.0150, -0.0071,  0.0032]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1802, -1.4512, -0.7827,  ..., -1.1992,  0.7554,  3.5977]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 04:39:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To resent results in a resentment
To establish results in a establishment
To displace results in a displacement
To infringe results in a infringement
To appoint results in a appointment
To embarrass results in a embarrassment
To announce results in a announcement
To detach results in a
2024-07-17 04:39:41 root INFO     [order_1_approx] starting weight calculation for To appoint results in a appointment
To resent results in a resentment
To establish results in a establishment
To announce results in a announcement
To displace results in a displacement
To embarrass results in a embarrassment
To detach results in a detachment
To infringe results in a
2024-07-17 04:39:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 04:43:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3953, -0.7139,  0.6743,  ...,  0.5161,  0.2495,  0.8945],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6953, -0.3882, -2.9219,  ...,  2.5449, -1.3281,  0.9326],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.0700e-03, -9.7122e-03,  1.0696e-02,  ..., -3.0384e-03,
          8.6365e-03, -7.5607e-03],
        [-2.5291e-03,  8.8120e-04,  8.5831e-05,  ..., -7.3242e-03,
          3.5667e-03, -2.1000e-03],
        [ 1.7624e-03, -6.9046e-04,  3.8853e-03,  ..., -1.1391e-02,
          3.9368e-03, -1.4221e-02],
        ...,
        [-4.7607e-03, -1.3268e-02,  3.8338e-04,  ..., -1.0429e-02,
          1.2665e-03, -4.3106e-03],
        [-9.2697e-03,  1.5182e-02,  2.5749e-03,  ...,  1.5001e-03,
         -2.8503e-02,  8.2970e-04],
        [-4.8447e-03,  8.6136e-03,  6.9580e-03,  ...,  1.8096e-04,
          1.1909e-02, -7.8049e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1655,  0.2256, -3.4492,  ...,  3.0430, -1.2666,  0.6313]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 04:43:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To appoint results in a appointment
To resent results in a resentment
To establish results in a establishment
To announce results in a announcement
To displace results in a displacement
To embarrass results in a embarrassment
To detach results in a detachment
To infringe results in a
2024-07-17 04:43:27 root INFO     [order_1_approx] starting weight calculation for To appoint results in a appointment
To embarrass results in a embarrassment
To detach results in a detachment
To displace results in a displacement
To infringe results in a infringement
To establish results in a establishment
To resent results in a resentment
To announce results in a
2024-07-17 04:43:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 04:47:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3809,  0.1018,  0.6826,  ...,  0.7393,  0.0739,  0.9443],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7461,  0.0215, -2.4922,  ...,  2.9746,  2.4160,  1.9043],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-8.3923e-03, -6.4926e-03, -2.5692e-03,  ...,  1.9302e-03,
          8.4534e-03, -7.3776e-03],
        [-1.5320e-02, -2.2842e-02,  7.2861e-03,  ...,  6.1073e-03,
         -1.5747e-02, -4.1809e-03],
        [-5.4474e-03,  7.0801e-03, -1.1940e-03,  ...,  9.4032e-04,
          8.0338e-03,  3.8223e-03],
        ...,
        [-4.7684e-03, -1.5564e-02, -1.9989e-03,  ..., -2.8519e-02,
          3.1338e-03,  3.1261e-03],
        [-4.0817e-03,  2.4796e-05, -8.1024e-03,  ...,  3.8185e-03,
         -1.9318e-02,  7.6103e-03],
        [ 3.4599e-03,  2.2110e-02,  2.1420e-03,  ..., -9.5062e-03,
          5.8670e-03, -9.5520e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6338,  0.0784, -3.0820,  ...,  2.3633,  2.7363,  2.9961]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 04:47:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To appoint results in a appointment
To embarrass results in a embarrassment
To detach results in a detachment
To displace results in a displacement
To infringe results in a infringement
To establish results in a establishment
To resent results in a resentment
To announce results in a
2024-07-17 04:47:14 root INFO     total operator prediction time: 1817.728034734726 seconds
2024-07-17 04:47:14 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on name - nationality
2024-07-17 04:47:14 root INFO     building operator name - nationality
2024-07-17 04:47:15 root INFO     [order_1_approx] starting weight calculation for darwin was english
hume was scottish
hitler was german
confucius was chinese
napoleon was french
tchaikovsky was russian
machiavelli was italian
homer was
2024-07-17 04:47:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 04:51:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1294,  0.6074, -1.1318,  ...,  1.4131,  0.5771, -0.5947],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0996,  0.5767, -4.9922,  ..., -1.8838, -4.2891, -2.0039],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0073, -0.0045, -0.0045,  ..., -0.0026,  0.0104, -0.0130],
        [-0.0021,  0.0086,  0.0040,  ...,  0.0157,  0.0024,  0.0027],
        [ 0.0017,  0.0098, -0.0142,  ..., -0.0192,  0.0087,  0.0004],
        ...,
        [-0.0038, -0.0068, -0.0047,  ...,  0.0179, -0.0004, -0.0054],
        [-0.0057,  0.0035, -0.0039,  ..., -0.0081,  0.0211,  0.0024],
        [-0.0108,  0.0055, -0.0070,  ..., -0.0094,  0.0317,  0.0109]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7349,  0.3752, -5.3086,  ..., -2.6367, -4.2422, -1.5410]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 04:51:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for darwin was english
hume was scottish
hitler was german
confucius was chinese
napoleon was french
tchaikovsky was russian
machiavelli was italian
homer was
2024-07-17 04:51:01 root INFO     [order_1_approx] starting weight calculation for hume was scottish
napoleon was french
homer was greek
hitler was german
tchaikovsky was russian
darwin was english
confucius was chinese
machiavelli was
2024-07-17 04:51:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 04:54:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0546, -0.7896, -0.1782,  ...,  0.2366, -0.4150,  0.7422],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.1367, -2.6562, -6.8359,  ..., -2.0840, -4.5820, -1.5400],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.9210e-03, -5.9586e-03,  4.4861e-03,  ..., -1.5366e-02,
         -3.5057e-03,  7.2594e-03],
        [-9.3651e-04,  1.7792e-02,  4.4098e-03,  ...,  3.3112e-03,
         -8.0490e-04,  8.8310e-04],
        [-6.6147e-03, -5.0354e-03,  1.6724e-02,  ..., -2.5513e-02,
         -3.5248e-03, -8.2016e-03],
        ...,
        [-1.0269e-02, -4.9362e-03,  1.3695e-02,  ...,  2.1683e-02,
         -2.0828e-02,  1.6518e-03],
        [-7.9155e-05, -1.9760e-03,  5.1079e-03,  ..., -1.3718e-02,
          6.9275e-03,  3.3569e-03],
        [-8.0261e-03,  3.0479e-03, -1.6222e-03,  ..., -3.1555e-02,
          4.9591e-04,  1.5732e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7344, -2.4805, -7.1953,  ..., -2.5117, -3.8418, -1.4238]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 04:54:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for hume was scottish
napoleon was french
homer was greek
hitler was german
tchaikovsky was russian
darwin was english
confucius was chinese
machiavelli was
2024-07-17 04:54:48 root INFO     [order_1_approx] starting weight calculation for napoleon was french
tchaikovsky was russian
machiavelli was italian
hume was scottish
darwin was english
homer was greek
hitler was german
confucius was
2024-07-17 04:54:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 04:58:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9419, -0.4316, -2.0059,  ..., -0.4771,  0.4573,  1.6338],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6650, -4.5508, -7.9961,  ..., -2.6367, -1.2549, -4.0391],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0080, -0.0112,  0.0090,  ..., -0.0130,  0.0062,  0.0081],
        [ 0.0018,  0.0141,  0.0038,  ..., -0.0005, -0.0150,  0.0046],
        [ 0.0038, -0.0043, -0.0304,  ...,  0.0068, -0.0016, -0.0260],
        ...,
        [-0.0158,  0.0213, -0.0119,  ...,  0.0242, -0.0083, -0.0193],
        [-0.0022,  0.0030,  0.0029,  ..., -0.0117,  0.0005, -0.0070],
        [-0.0145,  0.0163, -0.0182,  ..., -0.0139,  0.0029, -0.0105]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0566, -5.0312, -5.2773,  ..., -1.5908, -1.4219, -3.3750]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 04:58:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for napoleon was french
tchaikovsky was russian
machiavelli was italian
hume was scottish
darwin was english
homer was greek
hitler was german
confucius was
2024-07-17 04:58:34 root INFO     [order_1_approx] starting weight calculation for napoleon was french
confucius was chinese
machiavelli was italian
hume was scottish
homer was greek
hitler was german
tchaikovsky was russian
darwin was
2024-07-17 04:58:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 05:02:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3459, -0.2314, -0.3772,  ...,  0.3108, -0.2246,  0.7305],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6953, -2.8848, -3.6562,  ..., -1.3525, -0.4766, -3.7578],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0182, -0.0163, -0.0008,  ..., -0.0086, -0.0131,  0.0006],
        [ 0.0028,  0.0102,  0.0021,  ...,  0.0081, -0.0066, -0.0008],
        [ 0.0101, -0.0070,  0.0017,  ..., -0.0175, -0.0003,  0.0077],
        ...,
        [-0.0158,  0.0048, -0.0111,  ...,  0.0114, -0.0026,  0.0055],
        [-0.0051, -0.0085,  0.0064,  ..., -0.0076,  0.0091, -0.0001],
        [-0.0130,  0.0016, -0.0002,  ..., -0.0130, -0.0132,  0.0093]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0430, -3.0684, -4.3320,  ..., -0.7998, -0.4810, -3.6250]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 05:02:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for napoleon was french
confucius was chinese
machiavelli was italian
hume was scottish
homer was greek
hitler was german
tchaikovsky was russian
darwin was
2024-07-17 05:02:20 root INFO     [order_1_approx] starting weight calculation for tchaikovsky was russian
machiavelli was italian
darwin was english
confucius was chinese
homer was greek
hitler was german
napoleon was french
hume was
2024-07-17 05:02:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 05:06:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1873, -1.0488,  0.1865,  ...,  0.3875, -0.3057,  1.7715],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0508, -2.4941, -6.7852,  ..., -1.1133, -1.1133, -1.4092],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0166, -0.0325, -0.0063,  ..., -0.0082, -0.0051,  0.0042],
        [ 0.0041, -0.0022, -0.0017,  ...,  0.0099,  0.0117,  0.0031],
        [ 0.0140, -0.0157,  0.0044,  ..., -0.0078, -0.0027,  0.0034],
        ...,
        [-0.0014,  0.0045,  0.0050,  ...,  0.0227, -0.0249,  0.0125],
        [ 0.0008, -0.0072,  0.0218,  ..., -0.0017, -0.0022,  0.0137],
        [-0.0151, -0.0068,  0.0056,  ..., -0.0118,  0.0048,  0.0127]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3125, -2.7598, -6.5664,  ..., -1.2061, -1.4199, -2.0723]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 05:06:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for tchaikovsky was russian
machiavelli was italian
darwin was english
confucius was chinese
homer was greek
hitler was german
napoleon was french
hume was
2024-07-17 05:06:04 root INFO     [order_1_approx] starting weight calculation for tchaikovsky was russian
darwin was english
homer was greek
hume was scottish
confucius was chinese
hitler was german
machiavelli was italian
napoleon was
2024-07-17 05:06:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 05:09:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.3027, -0.6455,  0.1497,  ...,  0.0591, -0.2725,  0.9458],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.2969, -2.0684, -5.5352,  ..., -4.8164, -4.8984, -3.7480],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.9373e-03, -9.1095e-03,  4.4441e-04,  ..., -1.0414e-03,
         -3.2692e-03,  7.8201e-05],
        [ 1.2772e-02,  1.6922e-02,  2.2659e-03,  ...,  2.4643e-03,
         -1.5823e-02,  1.0559e-02],
        [-4.2648e-03,  4.9667e-03,  5.6610e-03,  ..., -2.5955e-02,
         -6.5918e-03, -7.3357e-03],
        ...,
        [-1.0963e-02,  1.2875e-03,  5.5809e-03,  ...,  1.7426e-02,
          1.6693e-02, -1.5900e-02],
        [ 2.9182e-04, -4.2267e-03,  1.6785e-02,  ..., -4.0817e-04,
          4.2839e-03,  3.7193e-04],
        [-2.6779e-03,  2.2018e-02, -7.1869e-03,  ..., -1.4595e-02,
          1.8021e-02,  1.9417e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.1016, -2.3145, -5.9141,  ..., -4.9727, -5.1289, -4.0156]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 05:09:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for tchaikovsky was russian
darwin was english
homer was greek
hume was scottish
confucius was chinese
hitler was german
machiavelli was italian
napoleon was
2024-07-17 05:09:52 root INFO     [order_1_approx] starting weight calculation for darwin was english
hitler was german
confucius was chinese
hume was scottish
homer was greek
machiavelli was italian
napoleon was french
tchaikovsky was
2024-07-17 05:09:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 05:13:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1301, -0.3955,  1.8750,  ...,  0.4199,  0.7124,  1.1172],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6387, -3.0078, -4.6445,  ..., -4.4922, -1.5146, -3.4531],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0010, -0.0203,  0.0085,  ..., -0.0178,  0.0087, -0.0064],
        [ 0.0179,  0.0286, -0.0098,  ...,  0.0002, -0.0089,  0.0034],
        [ 0.0083,  0.0104, -0.0040,  ..., -0.0099, -0.0096, -0.0057],
        ...,
        [-0.0165,  0.0117, -0.0088,  ...,  0.0029, -0.0020,  0.0055],
        [-0.0088,  0.0025,  0.0051,  ..., -0.0086,  0.0060,  0.0164],
        [-0.0057, -0.0067, -0.0005,  ..., -0.0222,  0.0185,  0.0020]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0420, -3.0977, -4.2227,  ..., -5.4375, -1.3877, -2.3203]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 05:13:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for darwin was english
hitler was german
confucius was chinese
hume was scottish
homer was greek
machiavelli was italian
napoleon was french
tchaikovsky was
2024-07-17 05:13:39 root INFO     [order_1_approx] starting weight calculation for hume was scottish
napoleon was french
tchaikovsky was russian
machiavelli was italian
darwin was english
homer was greek
confucius was chinese
hitler was
2024-07-17 05:13:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 05:17:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7412,  0.5415, -1.0820,  ..., -0.2472, -0.4597, -0.9961],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2559, -1.4277, -8.3281,  ..., -2.4180, -6.1406, -4.5234],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-6.4468e-04, -8.3160e-03, -9.6283e-03,  ..., -8.1329e-03,
          9.2316e-03, -7.4158e-03],
        [ 3.0231e-03,  1.6098e-02, -7.2975e-03,  ...,  2.1744e-04,
         -8.3466e-03,  9.9792e-03],
        [ 1.6880e-04, -1.4374e-02, -1.2054e-03,  ..., -1.4709e-02,
         -5.2338e-03, -1.0033e-02],
        ...,
        [-6.7101e-03,  7.6294e-05, -8.6975e-03,  ...,  1.5182e-02,
         -4.5624e-03, -9.0637e-03],
        [-5.5313e-03, -1.2955e-02,  3.7308e-03,  ...,  2.7180e-05,
         -3.4714e-03, -1.4496e-02],
        [-7.0143e-04,  2.6855e-03,  3.9673e-04,  ..., -1.0521e-02,
          1.6937e-02, -2.3056e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4597, -0.6592, -9.4688,  ..., -3.2109, -7.3867, -5.3008]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 05:17:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for hume was scottish
napoleon was french
tchaikovsky was russian
machiavelli was italian
darwin was english
homer was greek
confucius was chinese
hitler was
2024-07-17 05:17:27 root INFO     total operator prediction time: 1812.2992358207703 seconds
2024-07-17 05:17:27 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on country - language
2024-07-17 05:17:27 root INFO     building operator country - language
2024-07-17 05:17:27 root INFO     [order_1_approx] starting weight calculation for The country of australia primarily speaks the language of english
The country of austria primarily speaks the language of german
The country of ireland primarily speaks the language of english
The country of chile primarily speaks the language of spanish
The country of belize primarily speaks the language of english
The country of guyana primarily speaks the language of english
The country of kosovo primarily speaks the language of albanian
The country of mexico primarily speaks the language of
2024-07-17 05:17:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 05:21:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9121,  0.0132, -0.0200,  ..., -1.3516,  1.1133,  0.5020],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1309, -1.8438, -1.8281,  ..., -4.0391, -1.2471, -0.7686],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0003, -0.0042, -0.0043,  ..., -0.0023, -0.0052,  0.0056],
        [ 0.0009,  0.0042, -0.0010,  ...,  0.0126,  0.0024,  0.0047],
        [-0.0092,  0.0066, -0.0101,  ..., -0.0037,  0.0068,  0.0013],
        ...,
        [ 0.0008,  0.0025,  0.0004,  ...,  0.0130, -0.0143, -0.0016],
        [-0.0015, -0.0028, -0.0043,  ..., -0.0045, -0.0133, -0.0035],
        [ 0.0038, -0.0094, -0.0050,  ..., -0.0135,  0.0102, -0.0026]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3281, -1.3320, -1.7178,  ..., -4.5391, -1.7637, -0.6582]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 05:21:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of australia primarily speaks the language of english
The country of austria primarily speaks the language of german
The country of ireland primarily speaks the language of english
The country of chile primarily speaks the language of spanish
The country of belize primarily speaks the language of english
The country of guyana primarily speaks the language of english
The country of kosovo primarily speaks the language of albanian
The country of mexico primarily speaks the language of
2024-07-17 05:21:13 root INFO     [order_1_approx] starting weight calculation for The country of chile primarily speaks the language of spanish
The country of mexico primarily speaks the language of spanish
The country of kosovo primarily speaks the language of albanian
The country of guyana primarily speaks the language of english
The country of belize primarily speaks the language of english
The country of australia primarily speaks the language of english
The country of ireland primarily speaks the language of english
The country of austria primarily speaks the language of
2024-07-17 05:21:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 05:24:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.3320,  0.2656,  0.0095,  ..., -0.9482,  0.6567,  0.8384],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8335, -3.4434, -3.0391,  ..., -6.5977,  0.5176, -4.1719],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0081, -0.0090, -0.0017,  ...,  0.0058,  0.0066, -0.0031],
        [ 0.0039,  0.0060,  0.0081,  ...,  0.0158,  0.0293, -0.0197],
        [ 0.0149,  0.0167, -0.0228,  ...,  0.0071, -0.0159, -0.0011],
        ...,
        [-0.0150, -0.0011,  0.0049,  ..., -0.0071, -0.0185,  0.0117],
        [ 0.0088, -0.0154, -0.0029,  ...,  0.0010, -0.0096,  0.0117],
        [-0.0041, -0.0010, -0.0024,  ..., -0.0111,  0.0052,  0.0091]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9463, -4.3672, -2.5996,  ..., -5.8711,  0.3977, -4.0117]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 05:24:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of chile primarily speaks the language of spanish
The country of mexico primarily speaks the language of spanish
The country of kosovo primarily speaks the language of albanian
The country of guyana primarily speaks the language of english
The country of belize primarily speaks the language of english
The country of australia primarily speaks the language of english
The country of ireland primarily speaks the language of english
The country of austria primarily speaks the language of
2024-07-17 05:24:58 root INFO     [order_1_approx] starting weight calculation for The country of belize primarily speaks the language of english
The country of austria primarily speaks the language of german
The country of kosovo primarily speaks the language of albanian
The country of australia primarily speaks the language of english
The country of guyana primarily speaks the language of english
The country of chile primarily speaks the language of spanish
The country of mexico primarily speaks the language of spanish
The country of ireland primarily speaks the language of
2024-07-17 05:24:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 05:28:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.3408, -0.5420, -1.5762,  ...,  0.8555, -1.2285,  0.6475],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7290, -1.6855, -1.5049,  ..., -0.9927, -1.6973, -1.0752],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.8606e-03, -6.6071e-03, -1.0506e-02,  ..., -1.3351e-05,
         -1.0204e-04,  7.6389e-04],
        [-1.5526e-03, -6.5613e-04,  1.8806e-03,  ...,  6.7806e-04,
          9.9182e-03, -1.2445e-03],
        [-2.5826e-03,  1.3496e-02, -1.2115e-02,  ...,  1.3514e-03,
         -7.5798e-03, -9.1362e-04],
        ...,
        [ 3.6259e-03, -4.4250e-04, -2.9812e-03,  ..., -2.7847e-03,
         -2.4033e-02,  9.7580e-03],
        [-7.7667e-03, -3.9597e-03, -3.9043e-03,  ...,  1.1963e-02,
         -2.1805e-02, -4.3068e-03],
        [-5.3062e-03, -9.2621e-03,  2.4471e-03,  ...,  5.8327e-03,
          3.4866e-03, -7.1182e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4412, -2.0879, -1.2783,  ..., -1.0869, -2.0234, -1.4229]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 05:28:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of belize primarily speaks the language of english
The country of austria primarily speaks the language of german
The country of kosovo primarily speaks the language of albanian
The country of australia primarily speaks the language of english
The country of guyana primarily speaks the language of english
The country of chile primarily speaks the language of spanish
The country of mexico primarily speaks the language of spanish
The country of ireland primarily speaks the language of
2024-07-17 05:28:42 root INFO     [order_1_approx] starting weight calculation for The country of kosovo primarily speaks the language of albanian
The country of guyana primarily speaks the language of english
The country of ireland primarily speaks the language of english
The country of austria primarily speaks the language of german
The country of mexico primarily speaks the language of spanish
The country of belize primarily speaks the language of english
The country of australia primarily speaks the language of english
The country of chile primarily speaks the language of
2024-07-17 05:28:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 05:32:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3179, -1.0029, -0.4792,  ..., -0.2489,  2.0684, -0.2759],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6553, -2.0508, -4.4609,  ..., -2.9102,  1.1514, -2.8398],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-9.4223e-04, -4.7073e-03, -1.9741e-03,  ..., -5.0049e-03,
          6.1188e-03,  1.3809e-03],
        [ 2.9411e-03, -2.6836e-03,  9.3307e-03,  ...,  1.2276e-02,
          9.9182e-03, -2.8400e-03],
        [-1.0399e-02,  1.3336e-02, -2.6779e-02,  ...,  3.1433e-03,
          3.6507e-03,  1.1292e-02],
        ...,
        [-4.7836e-03, -6.9275e-03, -7.3242e-03,  ...,  8.4534e-03,
          1.2302e-03,  7.4425e-03],
        [-8.5878e-04,  9.0361e-05, -4.2038e-03,  ...,  2.1191e-03,
         -6.7139e-03, -4.1237e-03],
        [-1.1711e-03, -2.8191e-03, -3.6201e-03,  ..., -1.7090e-03,
          1.1169e-02, -5.8365e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5332, -1.4102, -4.6523,  ..., -2.7930,  1.3584, -3.1387]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 05:32:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of kosovo primarily speaks the language of albanian
The country of guyana primarily speaks the language of english
The country of ireland primarily speaks the language of english
The country of austria primarily speaks the language of german
The country of mexico primarily speaks the language of spanish
The country of belize primarily speaks the language of english
The country of australia primarily speaks the language of english
The country of chile primarily speaks the language of
2024-07-17 05:32:28 root INFO     [order_1_approx] starting weight calculation for The country of chile primarily speaks the language of spanish
The country of mexico primarily speaks the language of spanish
The country of belize primarily speaks the language of english
The country of austria primarily speaks the language of german
The country of kosovo primarily speaks the language of albanian
The country of ireland primarily speaks the language of english
The country of australia primarily speaks the language of english
The country of guyana primarily speaks the language of
2024-07-17 05:32:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 05:36:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1724, -1.2148,  0.3521,  ..., -0.6479, -0.4058,  1.4785],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.2148, -6.8164, -7.2695,  ..., -3.8945,  0.2261, -0.6709],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0002, -0.0113,  0.0070,  ...,  0.0099,  0.0004, -0.0088],
        [-0.0089,  0.0081,  0.0029,  ...,  0.0091,  0.0060, -0.0053],
        [ 0.0077,  0.0174, -0.0237,  ..., -0.0106, -0.0061, -0.0005],
        ...,
        [-0.0029, -0.0079, -0.0053,  ...,  0.0034, -0.0096,  0.0024],
        [-0.0094, -0.0006, -0.0033,  ...,  0.0203,  0.0081,  0.0120],
        [-0.0119, -0.0126,  0.0003,  ...,  0.0021,  0.0193, -0.0007]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.0391, -6.6562, -6.4141,  ..., -3.4961,  0.5146, -0.4111]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 05:36:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of chile primarily speaks the language of spanish
The country of mexico primarily speaks the language of spanish
The country of belize primarily speaks the language of english
The country of austria primarily speaks the language of german
The country of kosovo primarily speaks the language of albanian
The country of ireland primarily speaks the language of english
The country of australia primarily speaks the language of english
The country of guyana primarily speaks the language of
2024-07-17 05:36:13 root INFO     [order_1_approx] starting weight calculation for The country of guyana primarily speaks the language of english
The country of belize primarily speaks the language of english
The country of kosovo primarily speaks the language of albanian
The country of chile primarily speaks the language of spanish
The country of austria primarily speaks the language of german
The country of mexico primarily speaks the language of spanish
The country of ireland primarily speaks the language of english
The country of australia primarily speaks the language of
2024-07-17 05:36:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 05:39:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.7246, -0.7139,  0.0640,  ..., -1.5479, -0.1211, -0.1499],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7285, -3.2637, -5.1562,  ..., -3.3320,  3.7891, -6.7461],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.0518e-05,  2.9640e-03,  1.5316e-03,  ..., -1.8845e-03,
          1.3458e-02, -3.7651e-03],
        [ 1.3008e-03, -4.4327e-03,  6.5079e-03,  ...,  2.5131e-02,
          1.2314e-02, -4.2953e-03],
        [-2.4223e-03,  2.5368e-03,  2.9030e-03,  ...,  7.9422e-03,
         -7.2174e-03, -1.2016e-02],
        ...,
        [-1.0437e-02,  1.0361e-02,  7.7820e-04,  ..., -9.0637e-03,
         -1.0376e-02,  1.1612e-02],
        [-6.8903e-04, -9.0256e-03, -3.4332e-03,  ...,  5.0163e-04,
         -3.2330e-03, -1.4329e-04],
        [-5.0316e-03, -1.5335e-03, -3.6049e-03,  ...,  8.1158e-04,
         -1.5144e-03,  1.1520e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0254, -3.8516, -5.4102,  ..., -3.2793,  3.3145, -6.5781]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 05:39:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of guyana primarily speaks the language of english
The country of belize primarily speaks the language of english
The country of kosovo primarily speaks the language of albanian
The country of chile primarily speaks the language of spanish
The country of austria primarily speaks the language of german
The country of mexico primarily speaks the language of spanish
The country of ireland primarily speaks the language of english
The country of australia primarily speaks the language of
2024-07-17 05:39:58 root INFO     [order_1_approx] starting weight calculation for The country of austria primarily speaks the language of german
The country of chile primarily speaks the language of spanish
The country of kosovo primarily speaks the language of albanian
The country of australia primarily speaks the language of english
The country of ireland primarily speaks the language of english
The country of mexico primarily speaks the language of spanish
The country of guyana primarily speaks the language of english
The country of belize primarily speaks the language of
2024-07-17 05:39:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 05:43:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8022, -0.8501, -1.6191,  ...,  0.4888, -0.0761,  0.4470],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0254, -4.4375, -7.5586,  ..., -1.2021,  3.3848, -0.6270],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0011, -0.0094,  0.0077,  ..., -0.0054, -0.0005, -0.0057],
        [-0.0002,  0.0093,  0.0042,  ...,  0.0043,  0.0157, -0.0008],
        [-0.0012,  0.0121, -0.0063,  ..., -0.0047, -0.0078, -0.0031],
        ...,
        [-0.0065, -0.0066, -0.0061,  ...,  0.0138, -0.0069, -0.0032],
        [-0.0083, -0.0164, -0.0059,  ...,  0.0043, -0.0030, -0.0050],
        [ 0.0010, -0.0043,  0.0034,  ..., -0.0068,  0.0025,  0.0055]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7944, -3.8477, -7.4062,  ..., -1.3691,  2.7676, -0.6899]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 05:43:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of austria primarily speaks the language of german
The country of chile primarily speaks the language of spanish
The country of kosovo primarily speaks the language of albanian
The country of australia primarily speaks the language of english
The country of ireland primarily speaks the language of english
The country of mexico primarily speaks the language of spanish
The country of guyana primarily speaks the language of english
The country of belize primarily speaks the language of
2024-07-17 05:43:42 root INFO     [order_1_approx] starting weight calculation for The country of belize primarily speaks the language of english
The country of ireland primarily speaks the language of english
The country of mexico primarily speaks the language of spanish
The country of guyana primarily speaks the language of english
The country of austria primarily speaks the language of german
The country of australia primarily speaks the language of english
The country of chile primarily speaks the language of spanish
The country of kosovo primarily speaks the language of
2024-07-17 05:43:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 05:47:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.3809, 0.8350, 0.2710,  ..., 0.7573, 0.5435, 0.9468], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5596, -6.1328, -2.3477,  ..., -0.3086, -2.3340, -5.0508],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0017, -0.0069, -0.0021,  ...,  0.0022, -0.0112, -0.0037],
        [ 0.0105,  0.0014,  0.0037,  ..., -0.0031,  0.0049, -0.0047],
        [ 0.0012,  0.0074, -0.0112,  ..., -0.0086, -0.0154,  0.0010],
        ...,
        [-0.0081, -0.0069, -0.0045,  ..., -0.0002, -0.0175,  0.0011],
        [-0.0058,  0.0008, -0.0038,  ...,  0.0131, -0.0017,  0.0027],
        [ 0.0038, -0.0039,  0.0065,  ..., -0.0045,  0.0065,  0.0038]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3398, -5.5977, -2.3984,  ..., -0.7549, -2.7305, -5.3164]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 05:47:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of belize primarily speaks the language of english
The country of ireland primarily speaks the language of english
The country of mexico primarily speaks the language of spanish
The country of guyana primarily speaks the language of english
The country of austria primarily speaks the language of german
The country of australia primarily speaks the language of english
The country of chile primarily speaks the language of spanish
The country of kosovo primarily speaks the language of
2024-07-17 05:47:30 root INFO     total operator prediction time: 1802.8494355678558 seconds
2024-07-17 05:47:30 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on animal - shelter
2024-07-17 05:47:30 root INFO     building operator animal - shelter
2024-07-17 05:47:30 root INFO     [order_1_approx] starting weight calculation for The place hippopotamus lives in is called river
The place whale lives in is called sea
The place cattle lives in is called barn
The place locust lives in is called nest
The place cricket lives in is called nest
The place mole lives in is called hole
The place mallard lives in is called nest
The place spider lives in is called
2024-07-17 05:47:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 05:51:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0220,  1.0010, -0.5044,  ...,  1.1035,  0.5430,  0.9116],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6436, -1.6934,  0.8193,  ..., -6.2578, -0.0356,  3.7129],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0063, -0.0209, -0.0109,  ...,  0.0123,  0.0112,  0.0039],
        [ 0.0018,  0.0099,  0.0067,  ...,  0.0048,  0.0067, -0.0041],
        [-0.0108,  0.0138, -0.0057,  ...,  0.0012,  0.0140,  0.0127],
        ...,
        [-0.0033, -0.0093, -0.0022,  ...,  0.0018, -0.0044,  0.0066],
        [ 0.0017, -0.0015,  0.0118,  ...,  0.0014,  0.0053,  0.0147],
        [-0.0009, -0.0065, -0.0053,  ..., -0.0093,  0.0064,  0.0036]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7568, -2.1836,  0.5234,  ..., -6.6133,  0.3691,  3.9648]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 05:51:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place hippopotamus lives in is called river
The place whale lives in is called sea
The place cattle lives in is called barn
The place locust lives in is called nest
The place cricket lives in is called nest
The place mole lives in is called hole
The place mallard lives in is called nest
The place spider lives in is called
2024-07-17 05:51:16 root INFO     [order_1_approx] starting weight calculation for The place cattle lives in is called barn
The place whale lives in is called sea
The place cricket lives in is called nest
The place spider lives in is called web
The place mallard lives in is called nest
The place mole lives in is called hole
The place hippopotamus lives in is called river
The place locust lives in is called
2024-07-17 05:51:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 05:55:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5522, -0.1938,  0.7295,  ...,  1.9092, -0.9351,  0.6831],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0098, -1.2949, -0.6528,  ..., -3.3555,  2.5156, -2.4473],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0044, -0.0074,  0.0018,  ..., -0.0002,  0.0057,  0.0007],
        [ 0.0044,  0.0132,  0.0092,  ...,  0.0067,  0.0103, -0.0010],
        [-0.0045,  0.0036, -0.0005,  ..., -0.0042,  0.0026,  0.0082],
        ...,
        [-0.0030, -0.0110, -0.0027,  ...,  0.0037, -0.0017,  0.0048],
        [ 0.0033, -0.0078, -0.0060,  ..., -0.0028,  0.0069,  0.0094],
        [-0.0063, -0.0106,  0.0041,  ..., -0.0120,  0.0078,  0.0162]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7627, -1.6221, -0.9238,  ..., -2.9082,  2.5215, -2.5723]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 05:55:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place cattle lives in is called barn
The place whale lives in is called sea
The place cricket lives in is called nest
The place spider lives in is called web
The place mallard lives in is called nest
The place mole lives in is called hole
The place hippopotamus lives in is called river
The place locust lives in is called
2024-07-17 05:55:03 root INFO     [order_1_approx] starting weight calculation for The place cricket lives in is called nest
The place mallard lives in is called nest
The place cattle lives in is called barn
The place spider lives in is called web
The place locust lives in is called nest
The place whale lives in is called sea
The place mole lives in is called hole
The place hippopotamus lives in is called
2024-07-17 05:55:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 05:58:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4316, -1.0137,  0.4526,  ...,  0.4146, -0.5713,  2.2695],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0205, -3.5254, -2.8379,  ..., -2.9355,  0.5293,  0.2651],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0034, -0.0025, -0.0075,  ...,  0.0010,  0.0068, -0.0048],
        [ 0.0020,  0.0007,  0.0027,  ...,  0.0036,  0.0009, -0.0049],
        [ 0.0021,  0.0042, -0.0090,  ..., -0.0063,  0.0073,  0.0069],
        ...,
        [-0.0136, -0.0003, -0.0086,  ..., -0.0018, -0.0016,  0.0088],
        [-0.0074,  0.0019,  0.0092,  ...,  0.0022,  0.0072,  0.0080],
        [-0.0002,  0.0039,  0.0057,  ..., -0.0010,  0.0101,  0.0058]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0204, -3.1328, -3.2852,  ..., -3.1113,  0.6382, -0.3540]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 05:58:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place cricket lives in is called nest
The place mallard lives in is called nest
The place cattle lives in is called barn
The place spider lives in is called web
The place locust lives in is called nest
The place whale lives in is called sea
The place mole lives in is called hole
The place hippopotamus lives in is called
2024-07-17 05:58:50 root INFO     [order_1_approx] starting weight calculation for The place cattle lives in is called barn
The place hippopotamus lives in is called river
The place spider lives in is called web
The place locust lives in is called nest
The place cricket lives in is called nest
The place whale lives in is called sea
The place mole lives in is called hole
The place mallard lives in is called
2024-07-17 05:58:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 06:02:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5737, -1.0059, -0.0073,  ...,  1.0010, -1.4502, -0.3608],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.9883, -2.1660, -3.5840,  ..., -1.4453,  0.0112, -2.7051],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0158, -0.0125,  0.0014,  ...,  0.0033,  0.0029,  0.0056],
        [ 0.0018,  0.0143, -0.0009,  ...,  0.0103,  0.0085,  0.0003],
        [ 0.0052, -0.0003, -0.0005,  ..., -0.0090,  0.0202,  0.0211],
        ...,
        [-0.0051,  0.0040, -0.0025,  ...,  0.0047,  0.0002,  0.0086],
        [-0.0152,  0.0028, -0.0050,  ..., -0.0029,  0.0109,  0.0057],
        [-0.0184, -0.0207,  0.0040,  ..., -0.0077,  0.0168,  0.0141]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.2090, -1.0020, -4.4258,  ..., -1.3057, -0.5430, -2.7070]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 06:02:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place cattle lives in is called barn
The place hippopotamus lives in is called river
The place spider lives in is called web
The place locust lives in is called nest
The place cricket lives in is called nest
The place whale lives in is called sea
The place mole lives in is called hole
The place mallard lives in is called
2024-07-17 06:02:37 root INFO     [order_1_approx] starting weight calculation for The place spider lives in is called web
The place mole lives in is called hole
The place cricket lives in is called nest
The place whale lives in is called sea
The place hippopotamus lives in is called river
The place mallard lives in is called nest
The place locust lives in is called nest
The place cattle lives in is called
2024-07-17 06:02:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 06:06:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5273,  0.8208,  0.3481,  ...,  0.6460, -0.2144,  1.0684],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5527, -3.1152,  1.1777,  ..., -4.1562,  2.4375,  0.4824],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0031, -0.0211,  0.0024,  ...,  0.0117, -0.0127, -0.0013],
        [-0.0019,  0.0167,  0.0075,  ...,  0.0017,  0.0108, -0.0188],
        [-0.0035,  0.0102, -0.0005,  ...,  0.0031,  0.0076,  0.0094],
        ...,
        [-0.0171, -0.0048, -0.0009,  ...,  0.0060, -0.0049,  0.0159],
        [-0.0103,  0.0026,  0.0060,  ..., -0.0014,  0.0026, -0.0003],
        [-0.0002, -0.0071, -0.0011,  ..., -0.0073,  0.0059,  0.0103]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3379, -1.8994,  1.4834,  ..., -4.0820,  1.6357,  0.4863]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 06:06:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place spider lives in is called web
The place mole lives in is called hole
The place cricket lives in is called nest
The place whale lives in is called sea
The place hippopotamus lives in is called river
The place mallard lives in is called nest
The place locust lives in is called nest
The place cattle lives in is called
2024-07-17 06:06:24 root INFO     [order_1_approx] starting weight calculation for The place whale lives in is called sea
The place spider lives in is called web
The place cattle lives in is called barn
The place hippopotamus lives in is called river
The place locust lives in is called nest
The place mallard lives in is called nest
The place cricket lives in is called nest
The place mole lives in is called
2024-07-17 06:06:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 06:10:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.2148, -0.3279, -0.5928,  ...,  1.1738, -0.8721,  0.3857],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.2393, -2.0293, -4.1016,  ..., -6.4141, -0.6631,  1.1748],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0011, -0.0204, -0.0118,  ...,  0.0316, -0.0076,  0.0056],
        [-0.0105,  0.0063,  0.0007,  ...,  0.0173,  0.0096,  0.0055],
        [ 0.0021, -0.0133, -0.0155,  ...,  0.0129,  0.0328, -0.0053],
        ...,
        [-0.0085, -0.0076,  0.0040,  ...,  0.0007, -0.0027,  0.0082],
        [-0.0035, -0.0105, -0.0070,  ..., -0.0068,  0.0139, -0.0017],
        [-0.0080,  0.0010, -0.0131,  ...,  0.0016,  0.0070,  0.0115]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.9102, -1.3223, -4.4375,  ..., -6.7070, -0.4653,  1.6250]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 06:10:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place whale lives in is called sea
The place spider lives in is called web
The place cattle lives in is called barn
The place hippopotamus lives in is called river
The place locust lives in is called nest
The place mallard lives in is called nest
The place cricket lives in is called nest
The place mole lives in is called
2024-07-17 06:10:11 root INFO     [order_1_approx] starting weight calculation for The place mole lives in is called hole
The place mallard lives in is called nest
The place hippopotamus lives in is called river
The place cattle lives in is called barn
The place spider lives in is called web
The place locust lives in is called nest
The place whale lives in is called sea
The place cricket lives in is called
2024-07-17 06:10:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 06:13:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3928,  0.3345, -0.4702,  ...,  1.0898, -0.1748,  1.4062],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8633, -3.2930,  0.2588,  ..., -3.9883,  1.0527, -0.9707],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0044, -0.0211, -0.0058,  ...,  0.0140,  0.0108,  0.0032],
        [ 0.0007, -0.0064,  0.0027,  ...,  0.0192, -0.0131,  0.0030],
        [-0.0068,  0.0113, -0.0140,  ..., -0.0150,  0.0160,  0.0078],
        ...,
        [-0.0122,  0.0130, -0.0068,  ...,  0.0053,  0.0051,  0.0031],
        [-0.0045, -0.0062, -0.0072,  ..., -0.0079, -0.0022, -0.0002],
        [ 0.0068, -0.0105, -0.0009,  ..., -0.0145,  0.0135,  0.0153]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3232, -3.0410,  0.4761,  ..., -3.8418,  1.4297, -1.0225]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 06:13:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place mole lives in is called hole
The place mallard lives in is called nest
The place hippopotamus lives in is called river
The place cattle lives in is called barn
The place spider lives in is called web
The place locust lives in is called nest
The place whale lives in is called sea
The place cricket lives in is called
2024-07-17 06:13:56 root INFO     [order_1_approx] starting weight calculation for The place hippopotamus lives in is called river
The place locust lives in is called nest
The place mole lives in is called hole
The place spider lives in is called web
The place cricket lives in is called nest
The place cattle lives in is called barn
The place mallard lives in is called nest
The place whale lives in is called
2024-07-17 06:13:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 06:17:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1699,  0.8027, -0.4927,  ...,  0.2300, -0.4395,  0.7505],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6523, -0.5693, -3.2227,  ..., -2.5508,  1.8203,  1.7920],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0120, -0.0131, -0.0015,  ...,  0.0129,  0.0027,  0.0026],
        [ 0.0111,  0.0029, -0.0032,  ..., -0.0061, -0.0032, -0.0076],
        [-0.0006, -0.0024, -0.0022,  ...,  0.0022,  0.0039,  0.0117],
        ...,
        [-0.0202, -0.0106, -0.0057,  ...,  0.0023,  0.0013,  0.0060],
        [-0.0094,  0.0116,  0.0063,  ...,  0.0065,  0.0037, -0.0013],
        [-0.0013, -0.0266, -0.0017,  ...,  0.0093,  0.0085,  0.0146]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0801,  0.3540, -3.7949,  ..., -2.0938,  2.0898,  1.5547]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 06:17:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place hippopotamus lives in is called river
The place locust lives in is called nest
The place mole lives in is called hole
The place spider lives in is called web
The place cricket lives in is called nest
The place cattle lives in is called barn
The place mallard lives in is called nest
The place whale lives in is called
2024-07-17 06:17:42 root INFO     total operator prediction time: 1812.8412590026855 seconds
2024-07-17 06:17:42 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on male - female
2024-07-17 06:17:42 root INFO     building operator male - female
2024-07-17 06:17:43 root INFO     [order_1_approx] starting weight calculation for A female god is known as a goddess
A female superman is known as a superwoman
A female chairman is known as a chairwoman
A female mister is known as a miss
A female nephew is known as a niece
A female grandfather is known as a grandmother
A female dad is known as a mom
A female husband is known as a
2024-07-17 06:17:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 06:21:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([1.4648, 0.0480, 0.9395,  ..., 0.7593, 0.0674, 0.4824], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.0586, -2.4648, -1.8506,  ..., -4.4062,  1.4590, -2.5156],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-3.1719e-03,  1.2192e-02,  9.3002e-03,  ..., -1.5259e-05,
          1.4786e-02,  1.9302e-03],
        [ 4.5662e-03, -5.8746e-04, -1.6689e-04,  ...,  2.9564e-04,
          1.9470e-02, -1.1856e-02],
        [-2.4643e-03,  1.1292e-02,  2.9755e-04,  ...,  2.9984e-03,
          1.4435e-02, -1.7834e-03],
        ...,
        [-1.1841e-02, -8.0338e-03,  8.5297e-03,  ...,  8.2397e-03,
         -2.0081e-02,  7.3242e-03],
        [ 1.9531e-03,  7.9575e-03,  9.0485e-03,  ..., -2.7390e-03,
         -2.2659e-03,  1.7914e-02],
        [ 7.7820e-03, -1.7776e-03,  1.1475e-02,  ..., -8.4610e-03,
          2.4567e-02,  8.9951e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.7188, -2.0977, -1.6709,  ..., -3.5859,  0.9658, -2.9023]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 06:21:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female god is known as a goddess
A female superman is known as a superwoman
A female chairman is known as a chairwoman
A female mister is known as a miss
A female nephew is known as a niece
A female grandfather is known as a grandmother
A female dad is known as a mom
A female husband is known as a
2024-07-17 06:21:29 root INFO     [order_1_approx] starting weight calculation for A female chairman is known as a chairwoman
A female grandfather is known as a grandmother
A female mister is known as a miss
A female dad is known as a mom
A female husband is known as a wife
A female superman is known as a superwoman
A female god is known as a goddess
A female nephew is known as a
2024-07-17 06:21:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 06:25:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.3750, -1.4248,  0.1373,  ...,  0.3721,  1.1641, -1.0098],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.2695, -3.4883, -0.3350,  ..., -1.8203, -0.0737, -0.5469],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0140, -0.0073, -0.0068,  ...,  0.0107,  0.0203, -0.0046],
        [ 0.0019,  0.0050,  0.0182,  ...,  0.0056, -0.0156,  0.0078],
        [ 0.0141,  0.0046, -0.0164,  ..., -0.0008,  0.0096, -0.0003],
        ...,
        [ 0.0032, -0.0126, -0.0051,  ..., -0.0180, -0.0096, -0.0032],
        [ 0.0127, -0.0091, -0.0024,  ..., -0.0006, -0.0198, -0.0130],
        [ 0.0013,  0.0142, -0.0063,  ..., -0.0061,  0.0185, -0.0005]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4902, -3.0781, -0.2825,  ..., -1.8682,  0.3621, -1.1914]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 06:25:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female chairman is known as a chairwoman
A female grandfather is known as a grandmother
A female mister is known as a miss
A female dad is known as a mom
A female husband is known as a wife
A female superman is known as a superwoman
A female god is known as a goddess
A female nephew is known as a
2024-07-17 06:25:14 root INFO     [order_1_approx] starting weight calculation for A female grandfather is known as a grandmother
A female nephew is known as a niece
A female husband is known as a wife
A female superman is known as a superwoman
A female god is known as a goddess
A female dad is known as a mom
A female mister is known as a miss
A female chairman is known as a
2024-07-17 06:25:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 06:28:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1167, -0.3967,  0.7119,  ..., -0.1544, -0.2800, -0.8389],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4805, -2.6641,  1.4219,  ..., -2.5430,  2.0254, -2.2734],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0173, -0.0113, -0.0109,  ...,  0.0210,  0.0139, -0.0133],
        [-0.0071, -0.0106,  0.0005,  ..., -0.0154,  0.0039,  0.0172],
        [ 0.0169,  0.0095, -0.0173,  ...,  0.0033, -0.0034,  0.0120],
        ...,
        [-0.0092, -0.0012, -0.0103,  ..., -0.0020, -0.0114,  0.0004],
        [ 0.0066, -0.0050, -0.0080,  ...,  0.0082, -0.0228, -0.0021],
        [-0.0089,  0.0136,  0.0071,  ...,  0.0006,  0.0126,  0.0062]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.0684, -2.8594,  0.5693,  ..., -1.8613,  2.3633, -2.3438]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 06:29:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female grandfather is known as a grandmother
A female nephew is known as a niece
A female husband is known as a wife
A female superman is known as a superwoman
A female god is known as a goddess
A female dad is known as a mom
A female mister is known as a miss
A female chairman is known as a
2024-07-17 06:29:00 root INFO     [order_1_approx] starting weight calculation for A female grandfather is known as a grandmother
A female dad is known as a mom
A female god is known as a goddess
A female chairman is known as a chairwoman
A female husband is known as a wife
A female nephew is known as a niece
A female superman is known as a superwoman
A female mister is known as a
2024-07-17 06:29:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 06:32:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.5039, -0.8286, -0.3875,  ...,  0.9844,  0.0469, -0.1282],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3145, -3.6855, -2.2363,  ..., -2.5605, -0.2288, -4.3242],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0292, -0.0072,  0.0100,  ...,  0.0055,  0.0081, -0.0504],
        [ 0.0159, -0.0197,  0.0062,  ..., -0.0067,  0.0100,  0.0085],
        [-0.0086, -0.0002, -0.0111,  ...,  0.0191, -0.0049, -0.0164],
        ...,
        [-0.0049, -0.0351, -0.0092,  ..., -0.0176,  0.0021,  0.0050],
        [ 0.0118,  0.0061, -0.0021,  ...,  0.0099, -0.0167,  0.0244],
        [-0.0005,  0.0163, -0.0054,  ..., -0.0130,  0.0176, -0.0132]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2109, -3.3691, -1.4395,  ..., -1.4561, -1.3037, -4.1367]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 06:32:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female grandfather is known as a grandmother
A female dad is known as a mom
A female god is known as a goddess
A female chairman is known as a chairwoman
A female husband is known as a wife
A female nephew is known as a niece
A female superman is known as a superwoman
A female mister is known as a
2024-07-17 06:32:48 root INFO     [order_1_approx] starting weight calculation for A female dad is known as a mom
A female husband is known as a wife
A female nephew is known as a niece
A female mister is known as a miss
A female grandfather is known as a grandmother
A female superman is known as a superwoman
A female chairman is known as a chairwoman
A female god is known as a
2024-07-17 06:32:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 06:36:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8057,  0.0770,  0.2686,  ..., -0.1682, -0.5977, -0.9893],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.2402, -0.3535,  0.2412,  ...,  0.6572, -0.2166, -2.8047],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0220, -0.0087,  0.0134,  ..., -0.0112, -0.0089, -0.0115],
        [ 0.0005, -0.0168, -0.0116,  ...,  0.0014,  0.0141, -0.0083],
        [ 0.0087,  0.0161, -0.0088,  ..., -0.0062,  0.0166, -0.0056],
        ...,
        [-0.0109, -0.0091, -0.0034,  ..., -0.0028, -0.0097, -0.0096],
        [ 0.0030,  0.0006,  0.0236,  ...,  0.0044, -0.0102, -0.0016],
        [ 0.0111,  0.0277,  0.0092,  ...,  0.0027,  0.0201, -0.0234]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.2812, -2.1250, -0.5977,  ...,  0.1270, -0.6821, -4.3203]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 06:36:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female dad is known as a mom
A female husband is known as a wife
A female nephew is known as a niece
A female mister is known as a miss
A female grandfather is known as a grandmother
A female superman is known as a superwoman
A female chairman is known as a chairwoman
A female god is known as a
2024-07-17 06:36:34 root INFO     [order_1_approx] starting weight calculation for A female mister is known as a miss
A female nephew is known as a niece
A female chairman is known as a chairwoman
A female husband is known as a wife
A female dad is known as a mom
A female god is known as a goddess
A female grandfather is known as a grandmother
A female superman is known as a
2024-07-17 06:36:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 06:40:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0721, -0.1169,  0.0989,  ..., -0.0753, -0.3433, -1.4980],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3799, -1.9902, -3.2461,  ..., -3.1230, -2.8164, -1.1133],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 5.5313e-05, -1.6251e-02,  2.1248e-03,  ..., -1.0651e-02,
          2.7008e-03, -2.9419e-02],
        [ 5.8098e-03, -6.7635e-03, -5.7945e-03,  ...,  1.2993e-02,
         -7.4053e-04,  1.9394e-02],
        [-2.7390e-03,  2.2697e-03, -1.4679e-02,  ..., -4.9934e-03,
          1.2329e-02, -6.0005e-03],
        ...,
        [ 1.5106e-03,  1.5221e-02, -8.1406e-03,  ..., -1.5621e-03,
         -1.6632e-02,  1.6983e-02],
        [-8.3313e-03, -2.9411e-03,  2.2297e-03,  ...,  1.0513e-02,
         -1.2115e-02, -9.9792e-03],
        [ 1.9407e-03,  1.1795e-02, -4.2419e-03,  ..., -1.1721e-03,
          1.9501e-02, -3.5267e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2244, -1.2207, -3.2832,  ..., -2.5879, -2.0586, -1.4590]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 06:40:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female mister is known as a miss
A female nephew is known as a niece
A female chairman is known as a chairwoman
A female husband is known as a wife
A female dad is known as a mom
A female god is known as a goddess
A female grandfather is known as a grandmother
A female superman is known as a
2024-07-17 06:40:20 root INFO     [order_1_approx] starting weight calculation for A female nephew is known as a niece
A female superman is known as a superwoman
A female husband is known as a wife
A female mister is known as a miss
A female chairman is known as a chairwoman
A female god is known as a goddess
A female grandfather is known as a grandmother
A female dad is known as a
2024-07-17 06:40:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 06:44:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5713,  0.2046, -0.0503,  ...,  0.1421,  0.1785, -0.3022],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.0078, -6.0977, -0.3022,  ..., -1.6807,  5.3867, -2.4355],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0204, -0.0004,  0.0062,  ...,  0.0027,  0.0087, -0.0170],
        [-0.0001, -0.0186, -0.0122,  ..., -0.0076,  0.0077,  0.0060],
        [-0.0007,  0.0008, -0.0283,  ..., -0.0013,  0.0024, -0.0010],
        ...,
        [-0.0186, -0.0150, -0.0025,  ..., -0.0052, -0.0194, -0.0067],
        [ 0.0113,  0.0042,  0.0084,  ...,  0.0027, -0.0313, -0.0147],
        [ 0.0119,  0.0061,  0.0015,  ..., -0.0220,  0.0237,  0.0069]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.9141, -5.9297, -0.1188,  ..., -2.7266,  5.0977, -2.8496]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 06:44:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female nephew is known as a niece
A female superman is known as a superwoman
A female husband is known as a wife
A female mister is known as a miss
A female chairman is known as a chairwoman
A female god is known as a goddess
A female grandfather is known as a grandmother
A female dad is known as a
2024-07-17 06:44:06 root INFO     [order_1_approx] starting weight calculation for A female chairman is known as a chairwoman
A female mister is known as a miss
A female nephew is known as a niece
A female god is known as a goddess
A female husband is known as a wife
A female superman is known as a superwoman
A female dad is known as a mom
A female grandfather is known as a
2024-07-17 06:44:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 06:47:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6196, -1.7988,  0.1138,  ..., -0.5703,  0.6602, -0.9668],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 8.3516, -5.3047, -0.6787,  ..., -0.8784,  2.8848, -3.5176],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0100, -0.0047,  0.0010,  ...,  0.0112, -0.0051, -0.0172],
        [ 0.0094,  0.0033,  0.0164,  ...,  0.0031, -0.0071,  0.0020],
        [ 0.0127,  0.0145,  0.0078,  ..., -0.0016,  0.0120,  0.0144],
        ...,
        [-0.0003, -0.0065,  0.0119,  ..., -0.0094, -0.0201,  0.0064],
        [ 0.0100,  0.0177,  0.0063,  ..., -0.0002, -0.0223, -0.0068],
        [ 0.0211,  0.0090,  0.0082,  ...,  0.0020,  0.0223,  0.0040]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 8.0000, -5.5430, -0.9473,  ..., -1.0713,  2.8574, -4.3438]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 06:47:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female chairman is known as a chairwoman
A female mister is known as a miss
A female nephew is known as a niece
A female god is known as a goddess
A female husband is known as a wife
A female superman is known as a superwoman
A female dad is known as a mom
A female grandfather is known as a
2024-07-17 06:47:52 root INFO     total operator prediction time: 1809.608950138092 seconds
2024-07-17 06:47:52 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on name - occupation
2024-07-17 06:47:52 root INFO     building operator name - occupation
2024-07-17 06:47:52 root INFO     [order_1_approx] starting weight calculation for pascal was known for their work as a  mathematician
descartes was known for their work as a  mathematician
andersen was known for their work as a  writer
wagner was known for their work as a  composer
napoleon was known for their work as a  emperor
einstein was known for their work as a  physicist
balzac was known for their work as a  novelist
edison was known for their work as a 
2024-07-17 06:47:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 06:51:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.1394, 0.7529, 0.8613,  ..., 1.5557, 0.0209, 1.0469], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6328, -0.3623, -3.2539,  ..., -3.5293, -4.4609, -2.9531],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.8358e-03, -1.1002e-02, -7.4921e-03,  ...,  2.1420e-03,
          1.4603e-02,  2.7637e-03],
        [-3.1891e-03, -1.2398e-03,  1.1177e-03,  ...,  5.6992e-03,
         -3.8528e-03, -1.6441e-03],
        [ 2.3232e-03,  6.1035e-03, -6.7902e-03,  ..., -2.1229e-03,
          9.1248e-03, -2.5215e-03],
        ...,
        [-8.0261e-03,  2.2736e-02, -1.3390e-03,  ...,  8.6308e-05,
         -2.0294e-02,  7.7972e-03],
        [-1.9073e-05,  2.6207e-03,  3.0956e-03,  ...,  5.0507e-03,
         -3.0823e-03,  1.9073e-03],
        [-2.2385e-02,  3.0727e-03, -1.0269e-02,  ...,  7.5989e-03,
          9.9487e-03,  1.0704e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0273, -0.0742, -4.7500,  ..., -4.4141, -5.3359, -2.5703]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 06:51:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for pascal was known for their work as a  mathematician
descartes was known for their work as a  mathematician
andersen was known for their work as a  writer
wagner was known for their work as a  composer
napoleon was known for their work as a  emperor
einstein was known for their work as a  physicist
balzac was known for their work as a  novelist
edison was known for their work as a 
2024-07-17 06:51:37 root INFO     [order_1_approx] starting weight calculation for einstein was known for their work as a  physicist
edison was known for their work as a  inventor
napoleon was known for their work as a  emperor
balzac was known for their work as a  novelist
andersen was known for their work as a  writer
pascal was known for their work as a  mathematician
wagner was known for their work as a  composer
descartes was known for their work as a 
2024-07-17 06:51:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 06:55:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0771,  0.0844, -0.5903,  ...,  1.1709, -0.6719,  0.2710],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3379, -4.4219, -2.8984,  ..., -4.8008, -3.3652, -2.3770],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0117, -0.0060,  0.0020,  ...,  0.0086,  0.0024, -0.0043],
        [-0.0001,  0.0162,  0.0056,  ...,  0.0122,  0.0065,  0.0065],
        [ 0.0015,  0.0127, -0.0030,  ..., -0.0048, -0.0013,  0.0040],
        ...,
        [-0.0057, -0.0031, -0.0010,  ..., -0.0051, -0.0125,  0.0082],
        [ 0.0028, -0.0211,  0.0078,  ...,  0.0067, -0.0037, -0.0006],
        [-0.0171, -0.0097, -0.0001,  ..., -0.0018, -0.0014,  0.0002]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9121, -4.5742, -2.9336,  ..., -4.9961, -3.1816, -2.6445]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 06:55:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for einstein was known for their work as a  physicist
edison was known for their work as a  inventor
napoleon was known for their work as a  emperor
balzac was known for their work as a  novelist
andersen was known for their work as a  writer
pascal was known for their work as a  mathematician
wagner was known for their work as a  composer
descartes was known for their work as a 
2024-07-17 06:55:21 root INFO     [order_1_approx] starting weight calculation for andersen was known for their work as a  writer
wagner was known for their work as a  composer
descartes was known for their work as a  mathematician
pascal was known for their work as a  mathematician
napoleon was known for their work as a  emperor
edison was known for their work as a  inventor
balzac was known for their work as a  novelist
einstein was known for their work as a 
2024-07-17 06:55:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 06:59:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4368,  0.8989,  1.2617,  ...,  0.3628, -0.0056,  0.2854],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.6328, -0.6411, -0.9150,  ..., -1.1396, -3.7676, -4.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0013, -0.0097, -0.0117,  ...,  0.0021,  0.0033,  0.0033],
        [ 0.0041, -0.0048,  0.0002,  ...,  0.0154, -0.0038, -0.0021],
        [-0.0072,  0.0145, -0.0014,  ..., -0.0063, -0.0107, -0.0092],
        ...,
        [-0.0074,  0.0076,  0.0031,  ..., -0.0004, -0.0220, -0.0009],
        [ 0.0052, -0.0053,  0.0067,  ...,  0.0078,  0.0010, -0.0030],
        [-0.0084, -0.0053, -0.0026,  ..., -0.0046,  0.0109,  0.0044]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.0859, -0.3547, -2.2363,  ..., -1.6387, -3.2949, -4.6016]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 06:59:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for andersen was known for their work as a  writer
wagner was known for their work as a  composer
descartes was known for their work as a  mathematician
pascal was known for their work as a  mathematician
napoleon was known for their work as a  emperor
edison was known for their work as a  inventor
balzac was known for their work as a  novelist
einstein was known for their work as a 
2024-07-17 06:59:08 root INFO     [order_1_approx] starting weight calculation for descartes was known for their work as a  mathematician
wagner was known for their work as a  composer
napoleon was known for their work as a  emperor
andersen was known for their work as a  writer
pascal was known for their work as a  mathematician
edison was known for their work as a  inventor
einstein was known for their work as a  physicist
balzac was known for their work as a 
2024-07-17 06:59:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 07:02:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2654,  0.8872, -1.3770,  ..., -0.2952, -0.4263,  1.6602],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8350, -2.9102, -2.7793,  ..., -0.5371, -3.9121, -1.9883],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0018,  0.0041, -0.0078,  ..., -0.0104,  0.0062, -0.0104],
        [ 0.0025,  0.0281,  0.0115,  ...,  0.0045, -0.0061, -0.0139],
        [ 0.0034,  0.0083, -0.0111,  ..., -0.0122, -0.0064, -0.0109],
        ...,
        [-0.0121, -0.0036, -0.0062,  ...,  0.0101, -0.0026, -0.0017],
        [ 0.0049, -0.0170,  0.0053,  ..., -0.0006, -0.0061,  0.0010],
        [-0.0123, -0.0194,  0.0092,  ..., -0.0084,  0.0117,  0.0206]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5664, -3.3535, -3.1855,  ..., -1.0254, -3.6602, -1.3926]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 07:02:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for descartes was known for their work as a  mathematician
wagner was known for their work as a  composer
napoleon was known for their work as a  emperor
andersen was known for their work as a  writer
pascal was known for their work as a  mathematician
edison was known for their work as a  inventor
einstein was known for their work as a  physicist
balzac was known for their work as a 
2024-07-17 07:02:53 root INFO     [order_1_approx] starting weight calculation for edison was known for their work as a  inventor
andersen was known for their work as a  writer
pascal was known for their work as a  mathematician
descartes was known for their work as a  mathematician
balzac was known for their work as a  novelist
einstein was known for their work as a  physicist
wagner was known for their work as a  composer
napoleon was known for their work as a 
2024-07-17 07:02:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 07:06:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.4102, -0.7173,  0.4626,  ...,  0.4282, -0.0999,  0.8647],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3926, -0.5854, -2.6055,  ..., -3.2227, -0.3628, -2.2188],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0031, -0.0098, -0.0067,  ...,  0.0013,  0.0108, -0.0152],
        [-0.0042, -0.0011,  0.0008,  ...,  0.0006,  0.0044, -0.0049],
        [-0.0028,  0.0200, -0.0023,  ..., -0.0114,  0.0099, -0.0087],
        ...,
        [-0.0082, -0.0023, -0.0006,  ...,  0.0002,  0.0079, -0.0110],
        [ 0.0023,  0.0036,  0.0031,  ...,  0.0012, -0.0025, -0.0012],
        [-0.0065, -0.0044, -0.0015,  ...,  0.0013,  0.0056,  0.0096]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1797, -0.8726, -3.1445,  ..., -3.2969, -0.0396, -2.0703]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 07:06:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for edison was known for their work as a  inventor
andersen was known for their work as a  writer
pascal was known for their work as a  mathematician
descartes was known for their work as a  mathematician
balzac was known for their work as a  novelist
einstein was known for their work as a  physicist
wagner was known for their work as a  composer
napoleon was known for their work as a 
2024-07-17 07:06:38 root INFO     [order_1_approx] starting weight calculation for descartes was known for their work as a  mathematician
wagner was known for their work as a  composer
balzac was known for their work as a  novelist
napoleon was known for their work as a  emperor
einstein was known for their work as a  physicist
edison was known for their work as a  inventor
andersen was known for their work as a  writer
pascal was known for their work as a 
2024-07-17 07:06:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 07:10:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8555, -0.0892, -1.3516,  ...,  0.4395, -0.1548,  1.3145],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5098, -3.1211, -6.2461,  ..., -3.5605, -0.9463, -1.4688],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0025, -0.0065, -0.0059,  ..., -0.0178,  0.0082, -0.0011],
        [-0.0006,  0.0228,  0.0155,  ...,  0.0244, -0.0002, -0.0074],
        [ 0.0031,  0.0058, -0.0154,  ..., -0.0178, -0.0059, -0.0183],
        ...,
        [-0.0242, -0.0080, -0.0053,  ..., -0.0190, -0.0007, -0.0005],
        [ 0.0013, -0.0213,  0.0136,  ...,  0.0147, -0.0088, -0.0164],
        [-0.0258,  0.0052,  0.0131,  ...,  0.0083,  0.0217,  0.0077]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3486, -3.3320, -6.6250,  ..., -3.7480, -1.3467, -0.8672]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 07:10:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for descartes was known for their work as a  mathematician
wagner was known for their work as a  composer
balzac was known for their work as a  novelist
napoleon was known for their work as a  emperor
einstein was known for their work as a  physicist
edison was known for their work as a  inventor
andersen was known for their work as a  writer
pascal was known for their work as a 
2024-07-17 07:10:23 root INFO     [order_1_approx] starting weight calculation for napoleon was known for their work as a  emperor
pascal was known for their work as a  mathematician
andersen was known for their work as a  writer
edison was known for their work as a  inventor
balzac was known for their work as a  novelist
descartes was known for their work as a  mathematician
einstein was known for their work as a  physicist
wagner was known for their work as a 
2024-07-17 07:10:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 07:14:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2251,  0.4609,  0.2693,  ...,  0.8174, -0.3550,  0.3662],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.1328,  0.1538, -3.4102,  ..., -0.7642,  2.1484, -2.3047],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0203,  0.0073, -0.0059,  ...,  0.0036,  0.0085, -0.0032],
        [-0.0021, -0.0060, -0.0044,  ...,  0.0061, -0.0097, -0.0143],
        [ 0.0002,  0.0012, -0.0089,  ..., -0.0018, -0.0011, -0.0051],
        ...,
        [ 0.0012, -0.0017, -0.0028,  ..., -0.0035,  0.0096, -0.0057],
        [ 0.0041, -0.0195, -0.0112,  ...,  0.0093,  0.0012,  0.0043],
        [-0.0025, -0.0062, -0.0001,  ..., -0.0060,  0.0029,  0.0078]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.0352, -0.1902, -3.7500,  ..., -1.5801,  2.3945, -1.7900]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 07:14:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for napoleon was known for their work as a  emperor
pascal was known for their work as a  mathematician
andersen was known for their work as a  writer
edison was known for their work as a  inventor
balzac was known for their work as a  novelist
descartes was known for their work as a  mathematician
einstein was known for their work as a  physicist
wagner was known for their work as a 
2024-07-17 07:14:07 root INFO     [order_1_approx] starting weight calculation for descartes was known for their work as a  mathematician
edison was known for their work as a  inventor
pascal was known for their work as a  mathematician
balzac was known for their work as a  novelist
wagner was known for their work as a  composer
napoleon was known for their work as a  emperor
einstein was known for their work as a  physicist
andersen was known for their work as a 
2024-07-17 07:14:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 07:17:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1611,  0.4661, -0.3018,  ...,  0.6387, -0.1704,  0.5195],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.0156, -2.5625, -0.2244,  ..., -6.1797, -2.1445,  3.5527],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.6621e-04, -1.2802e-02, -1.5396e-02,  ..., -1.1734e-02,
         -8.0872e-04, -7.8659e-03],
        [-1.0353e-02,  1.3802e-02, -7.9651e-03,  ...,  1.1490e-02,
         -9.8267e-03, -9.9030e-03],
        [-1.0345e-02, -1.6556e-03,  4.8161e-05,  ...,  2.8229e-04,
         -3.9139e-03, -1.8311e-02],
        ...,
        [-1.3901e-02, -5.2834e-04, -1.7195e-03,  ..., -4.5471e-03,
         -1.2484e-03,  1.0071e-02],
        [-9.7656e-04, -7.5302e-03,  9.8724e-03,  ...,  6.7673e-03,
         -5.4855e-03, -5.1651e-03],
        [ 1.3680e-02,  1.7281e-03, -4.5395e-04,  ..., -2.5940e-03,
         -9.0179e-03, -4.3907e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.5117, -2.2734,  0.0430,  ..., -6.2422, -1.9248,  3.7305]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 07:17:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for descartes was known for their work as a  mathematician
edison was known for their work as a  inventor
pascal was known for their work as a  mathematician
balzac was known for their work as a  novelist
wagner was known for their work as a  composer
napoleon was known for their work as a  emperor
einstein was known for their work as a  physicist
andersen was known for their work as a 
2024-07-17 07:17:52 root INFO     total operator prediction time: 1799.634976387024 seconds
2024-07-17 07:17:52 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on country - capital
2024-07-17 07:17:52 root INFO     building operator country - capital
2024-07-17 07:17:52 root INFO     [order_1_approx] starting weight calculation for The country with dhaka as its capital is known as bangladesh
The country with lima as its capital is known as peru
The country with damascus as its capital is known as syria
The country with berlin as its capital is known as germany
The country with brussels as its capital is known as belgium
The country with tbilisi as its capital is known as georgia
The country with warsaw as its capital is known as poland
The country with jakarta as its capital is known as
2024-07-17 07:17:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 07:21:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0242,  0.0647,  0.2949,  ..., -0.1943, -0.0278,  0.3174],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1240, -2.8262, -2.0254,  ..., -5.5312, -0.7969, -0.1045],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0031,  0.0024, -0.0114,  ..., -0.0024, -0.0007, -0.0094],
        [ 0.0037, -0.0083,  0.0135,  ...,  0.0073,  0.0189, -0.0029],
        [ 0.0033,  0.0065, -0.0128,  ..., -0.0030, -0.0040, -0.0129],
        ...,
        [-0.0027,  0.0050, -0.0208,  ..., -0.0023, -0.0107, -0.0019],
        [ 0.0021, -0.0027, -0.0268,  ..., -0.0013, -0.0083,  0.0023],
        [-0.0049,  0.0006, -0.0169,  ..., -0.0087, -0.0032, -0.0069]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2131, -2.1973, -2.6406,  ..., -5.8828, -1.1074, -0.1267]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 07:21:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with dhaka as its capital is known as bangladesh
The country with lima as its capital is known as peru
The country with damascus as its capital is known as syria
The country with berlin as its capital is known as germany
The country with brussels as its capital is known as belgium
The country with tbilisi as its capital is known as georgia
The country with warsaw as its capital is known as poland
The country with jakarta as its capital is known as
2024-07-17 07:21:39 root INFO     [order_1_approx] starting weight calculation for The country with dhaka as its capital is known as bangladesh
The country with tbilisi as its capital is known as georgia
The country with brussels as its capital is known as belgium
The country with damascus as its capital is known as syria
The country with jakarta as its capital is known as indonesia
The country with warsaw as its capital is known as poland
The country with berlin as its capital is known as germany
The country with lima as its capital is known as
2024-07-17 07:21:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 07:25:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5615, -1.1123,  0.7402,  ...,  0.5557, -0.4331, -0.5352],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6445, -0.6279, -0.9189,  ..., -0.5249, -2.7500, -1.4238],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0021, -0.0138, -0.0082,  ...,  0.0017,  0.0103, -0.0124],
        [-0.0070, -0.0057,  0.0062,  ...,  0.0126, -0.0021, -0.0055],
        [ 0.0016,  0.0101, -0.0146,  ..., -0.0030,  0.0033, -0.0105],
        ...,
        [-0.0089, -0.0024, -0.0029,  ...,  0.0122, -0.0096, -0.0111],
        [-0.0193, -0.0040, -0.0115,  ..., -0.0013, -0.0230,  0.0299],
        [-0.0001,  0.0064,  0.0032,  ..., -0.0053,  0.0001, -0.0062]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7891,  0.9072, -1.9590,  ..., -1.5293, -3.8789, -1.6572]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 07:25:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with dhaka as its capital is known as bangladesh
The country with tbilisi as its capital is known as georgia
The country with brussels as its capital is known as belgium
The country with damascus as its capital is known as syria
The country with jakarta as its capital is known as indonesia
The country with warsaw as its capital is known as poland
The country with berlin as its capital is known as germany
The country with lima as its capital is known as
2024-07-17 07:25:26 root INFO     [order_1_approx] starting weight calculation for The country with berlin as its capital is known as germany
The country with dhaka as its capital is known as bangladesh
The country with lima as its capital is known as peru
The country with brussels as its capital is known as belgium
The country with jakarta as its capital is known as indonesia
The country with damascus as its capital is known as syria
The country with warsaw as its capital is known as poland
The country with tbilisi as its capital is known as
2024-07-17 07:25:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 07:29:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1562, -0.3491,  1.1094,  ...,  0.3550, -0.2026,  0.2888],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1924,  1.2969,  7.3555,  ...,  1.5732, -1.2021, -2.1602],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0289, -0.0221,  0.0138,  ...,  0.0054,  0.0115, -0.0130],
        [-0.0159, -0.0251,  0.0183,  ...,  0.0155,  0.0161,  0.0006],
        [-0.0109, -0.0074, -0.0044,  ..., -0.0015, -0.0018, -0.0088],
        ...,
        [-0.0134,  0.0099, -0.0026,  ..., -0.0003,  0.0043,  0.0113],
        [-0.0044,  0.0021, -0.0169,  ..., -0.0130, -0.0297,  0.0094],
        [ 0.0325,  0.0019, -0.0158,  ..., -0.0310,  0.0123, -0.0207]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7871,  3.3711,  6.9609,  ...,  2.0234, -1.9092, -3.6094]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 07:29:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with berlin as its capital is known as germany
The country with dhaka as its capital is known as bangladesh
The country with lima as its capital is known as peru
The country with brussels as its capital is known as belgium
The country with jakarta as its capital is known as indonesia
The country with damascus as its capital is known as syria
The country with warsaw as its capital is known as poland
The country with tbilisi as its capital is known as
2024-07-17 07:29:12 root INFO     [order_1_approx] starting weight calculation for The country with tbilisi as its capital is known as georgia
The country with jakarta as its capital is known as indonesia
The country with dhaka as its capital is known as bangladesh
The country with brussels as its capital is known as belgium
The country with lima as its capital is known as peru
The country with warsaw as its capital is known as poland
The country with berlin as its capital is known as germany
The country with damascus as its capital is known as
2024-07-17 07:29:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 07:32:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.6016, -0.6865, -0.7524,  ..., -0.3574, -1.4033,  0.2471],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6406,  0.6328,  1.0352,  ..., -6.0625,  0.2681, -2.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.6346e-03, -5.3787e-03, -3.9368e-03,  ..., -8.5602e-03,
          6.6566e-03, -1.2711e-02],
        [-1.9417e-03, -9.7656e-03,  3.8986e-03,  ...,  4.4403e-03,
          6.2218e-03, -3.0518e-03],
        [-4.1809e-03, -8.9645e-05, -6.8741e-03,  ...,  1.0628e-02,
         -3.3188e-03,  1.8005e-03],
        ...,
        [-5.8594e-03,  5.5027e-04,  2.0390e-03,  ...,  1.1078e-02,
         -1.2978e-02, -4.8637e-04],
        [-1.1810e-02, -3.9825e-03, -1.1261e-02,  ..., -9.9564e-03,
         -1.6403e-02,  9.7961e-03],
        [-4.5280e-03, -3.0346e-03, -2.8267e-03,  ...,  1.9913e-03,
          5.2719e-03,  1.4076e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.8721,  0.9478,  1.2070,  ..., -5.8320,  0.0626, -2.5898]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 07:32:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with tbilisi as its capital is known as georgia
The country with jakarta as its capital is known as indonesia
The country with dhaka as its capital is known as bangladesh
The country with brussels as its capital is known as belgium
The country with lima as its capital is known as peru
The country with warsaw as its capital is known as poland
The country with berlin as its capital is known as germany
The country with damascus as its capital is known as
2024-07-17 07:32:58 root INFO     [order_1_approx] starting weight calculation for The country with warsaw as its capital is known as poland
The country with jakarta as its capital is known as indonesia
The country with dhaka as its capital is known as bangladesh
The country with damascus as its capital is known as syria
The country with lima as its capital is known as peru
The country with brussels as its capital is known as belgium
The country with tbilisi as its capital is known as georgia
The country with berlin as its capital is known as
2024-07-17 07:32:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 07:36:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.5117,  0.8008, -0.5757,  ..., -0.4170,  0.3574, -0.4548],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9517, -1.7598,  0.3589,  ..., -2.1602,  1.5146, -1.9287],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0107,  0.0060, -0.0005,  ...,  0.0090,  0.0162, -0.0162],
        [ 0.0027,  0.0006,  0.0011,  ...,  0.0083, -0.0013,  0.0129],
        [ 0.0021,  0.0081, -0.0083,  ..., -0.0008, -0.0062, -0.0064],
        ...,
        [-0.0116, -0.0054, -0.0080,  ...,  0.0074, -0.0107, -0.0022],
        [-0.0210, -0.0213,  0.0022,  ...,  0.0051, -0.0090,  0.0078],
        [-0.0053,  0.0011, -0.0073,  ...,  0.0006,  0.0228, -0.0044]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7632, -0.7100, -0.2437,  ..., -2.3984,  1.3916, -2.2500]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 07:36:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with warsaw as its capital is known as poland
The country with jakarta as its capital is known as indonesia
The country with dhaka as its capital is known as bangladesh
The country with damascus as its capital is known as syria
The country with lima as its capital is known as peru
The country with brussels as its capital is known as belgium
The country with tbilisi as its capital is known as georgia
The country with berlin as its capital is known as
2024-07-17 07:36:44 root INFO     [order_1_approx] starting weight calculation for The country with lima as its capital is known as peru
The country with warsaw as its capital is known as poland
The country with tbilisi as its capital is known as georgia
The country with damascus as its capital is known as syria
The country with brussels as its capital is known as belgium
The country with jakarta as its capital is known as indonesia
The country with berlin as its capital is known as germany
The country with dhaka as its capital is known as
2024-07-17 07:36:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 07:40:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0664,  0.3706,  1.2471,  ..., -0.5327, -0.2021,  1.9551],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.1133, -1.1719, -2.2051,  ..., -0.6885,  2.3203,  3.4238],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0050, -0.0033, -0.0172,  ...,  0.0015,  0.0058,  0.0003],
        [-0.0039, -0.0089,  0.0091,  ...,  0.0077,  0.0103,  0.0018],
        [ 0.0125,  0.0089, -0.0164,  ...,  0.0037,  0.0049, -0.0116],
        ...,
        [-0.0154, -0.0075, -0.0085,  ...,  0.0080, -0.0038, -0.0036],
        [-0.0038, -0.0056, -0.0028,  ...,  0.0011, -0.0181,  0.0070],
        [-0.0137, -0.0142, -0.0063,  ..., -0.0104,  0.0028, -0.0046]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.6582,  0.0693, -2.9570,  ..., -0.6016,  1.9512,  3.1836]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 07:40:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with lima as its capital is known as peru
The country with warsaw as its capital is known as poland
The country with tbilisi as its capital is known as georgia
The country with damascus as its capital is known as syria
The country with brussels as its capital is known as belgium
The country with jakarta as its capital is known as indonesia
The country with berlin as its capital is known as germany
The country with dhaka as its capital is known as
2024-07-17 07:40:32 root INFO     [order_1_approx] starting weight calculation for The country with lima as its capital is known as peru
The country with tbilisi as its capital is known as georgia
The country with dhaka as its capital is known as bangladesh
The country with warsaw as its capital is known as poland
The country with berlin as its capital is known as germany
The country with damascus as its capital is known as syria
The country with jakarta as its capital is known as indonesia
The country with brussels as its capital is known as
2024-07-17 07:40:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 07:44:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9478, -0.2517,  0.2710,  ...,  1.0488, -0.1495, -0.3945],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0234,  0.5923,  0.0420,  ..., -4.3164, -2.9922, -5.7188],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0040, -0.0065,  0.0052,  ...,  0.0048,  0.0141,  0.0051],
        [-0.0019, -0.0168,  0.0173,  ...,  0.0220,  0.0179,  0.0046],
        [ 0.0015,  0.0063, -0.0109,  ...,  0.0047, -0.0075, -0.0030],
        ...,
        [-0.0073, -0.0021,  0.0004,  ...,  0.0027, -0.0084,  0.0008],
        [ 0.0020, -0.0017, -0.0147,  ..., -0.0070, -0.0185,  0.0029],
        [ 0.0078,  0.0114, -0.0269,  ..., -0.0232,  0.0091, -0.0100]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.6797,  0.1438, -0.3689,  ..., -4.6992, -2.5371, -5.2852]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 07:44:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with lima as its capital is known as peru
The country with tbilisi as its capital is known as georgia
The country with dhaka as its capital is known as bangladesh
The country with warsaw as its capital is known as poland
The country with berlin as its capital is known as germany
The country with damascus as its capital is known as syria
The country with jakarta as its capital is known as indonesia
The country with brussels as its capital is known as
2024-07-17 07:44:21 root INFO     [order_1_approx] starting weight calculation for The country with dhaka as its capital is known as bangladesh
The country with berlin as its capital is known as germany
The country with brussels as its capital is known as belgium
The country with tbilisi as its capital is known as georgia
The country with damascus as its capital is known as syria
The country with jakarta as its capital is known as indonesia
The country with lima as its capital is known as peru
The country with warsaw as its capital is known as
2024-07-17 07:44:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-17 07:48:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.6055, -0.1173, -0.2668,  ...,  0.0299,  1.4180,  1.1084],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3477,  0.7773, -0.2393,  ...,  2.1660,  2.3320,  5.4766],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0170, -0.0126,  0.0027,  ...,  0.0066,  0.0057, -0.0170],
        [ 0.0098, -0.0094,  0.0127,  ...,  0.0035,  0.0058,  0.0112],
        [ 0.0094,  0.0003, -0.0313,  ...,  0.0030, -0.0033, -0.0238],
        ...,
        [-0.0071, -0.0010, -0.0012,  ...,  0.0094, -0.0208,  0.0132],
        [-0.0326, -0.0122, -0.0179,  ..., -0.0079, -0.0282,  0.0180],
        [-0.0163, -0.0154,  0.0121,  ...,  0.0119,  0.0054,  0.0005]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2198,  2.0742, -0.7061,  ...,  2.1328,  2.3828,  6.1523]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 07:48:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with dhaka as its capital is known as bangladesh
The country with berlin as its capital is known as germany
The country with brussels as its capital is known as belgium
The country with tbilisi as its capital is known as georgia
The country with damascus as its capital is known as syria
The country with jakarta as its capital is known as indonesia
The country with lima as its capital is known as peru
The country with warsaw as its capital is known as
2024-07-17 07:48:08 root INFO     total operator prediction time: 1816.7333369255066 seconds
