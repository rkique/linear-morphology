2024-07-25 10:17:15 root INFO     loading model + tokenizer
2024-07-25 10:17:18 accelerate.big_modeling WARNING  Some parameters are on the meta device device because they were offloaded to the cpu.
2024-07-25 10:17:18 root INFO     model + tokenizer loaded
2024-07-25 10:17:18 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on meronyms - part
2024-07-25 10:17:18 root INFO     building operator meronyms - part
2024-07-25 10:17:43 root INFO     loading model + tokenizer
2024-07-25 10:17:51 root INFO     model + tokenizer loaded
2024-07-25 10:17:51 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on meronyms - part
2024-07-25 10:17:51 root INFO     building operator meronyms - part
2024-07-25 10:19:39 root INFO     loading model + tokenizer
2024-07-25 10:19:45 root INFO     model + tokenizer loaded
2024-07-25 10:19:45 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on meronyms - part
2024-07-25 10:19:45 root INFO     building operator meronyms - part
2024-07-25 10:19:46 root INFO     [order_1_approx] starting weight calculation for A part of a brush is a bristle
A part of a window is a pane
A part of a sword is a blade
A part of a apartment is a bedroom
A part of a gun is a trigger
A part of a shilling is a pence
A part of a gigabit is a megabit
A part of a gramm is a
2024-07-25 10:19:46 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 10:22:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0586,  0.5820,  2.9219,  ...,  0.0771, -1.9297, -1.7578],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([17.8750, -3.9375, -6.5625,  ...,  8.0625,  2.8438, -1.5938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.6855e-02,  4.7913e-03,  2.5024e-03,  ..., -9.5825e-03,
          2.7710e-02,  1.1230e-02],
        [ 4.6692e-03,  3.8605e-03, -2.8839e-03,  ..., -1.2207e-03,
         -9.2773e-03,  1.7548e-04],
        [ 3.1738e-03,  8.3008e-03,  1.5625e-01,  ..., -1.1719e-02,
         -2.3193e-03, -1.9897e-02],
        ...,
        [-7.6599e-03,  5.3711e-03, -2.1973e-02,  ...,  2.4658e-02,
          1.2695e-02,  2.5177e-04],
        [-7.0801e-03, -6.0730e-03, -1.1230e-02,  ..., -3.0823e-03,
          7.8613e-02,  9.6436e-03],
        [ 1.4343e-03, -1.2207e-04, -1.6174e-03,  ..., -2.6550e-03,
         -1.5015e-02,  3.2227e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[16.6719, -4.3359, -9.0469,  ...,  8.5547,  2.5098, -2.3574]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 10:22:34 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A part of a brush is a bristle
A part of a window is a pane
A part of a sword is a blade
A part of a apartment is a bedroom
A part of a gun is a trigger
A part of a shilling is a pence
A part of a gigabit is a megabit
A part of a gramm is a
2024-07-25 10:22:35 root INFO     [order_1_approx] starting weight calculation for A part of a gramm is a milligram
A part of a sword is a blade
A part of a apartment is a bedroom
A part of a window is a pane
A part of a brush is a bristle
A part of a shilling is a pence
A part of a gigabit is a megabit
A part of a gun is a
2024-07-25 10:22:35 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 10:25:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2500,  2.9062, -1.3516,  ..., -0.5039, -2.0938, -3.4688],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 7.6250,  4.1250,  6.8125,  ..., -6.0625,  6.5312, -3.2188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.8931e-02,  1.0681e-03,  2.2461e-02,  ..., -3.8147e-03,
          9.1553e-03,  8.4305e-04],
        [-9.1553e-05,  1.6235e-02, -9.4604e-03,  ..., -2.5177e-04,
         -4.6387e-03, -5.1117e-04],
        [ 4.5776e-04, -4.9438e-03,  1.3477e-01,  ..., -6.8970e-03,
          7.9346e-03,  4.1809e-03],
        ...,
        [-1.5869e-03,  2.2583e-03, -1.9226e-03,  ...,  2.0752e-02,
          3.5400e-03,  4.5776e-03],
        [-9.9182e-04,  1.1139e-03, -5.2185e-03,  ..., -4.4556e-03,
          4.0039e-02,  1.4954e-03],
        [ 1.3550e-02,  6.3324e-04,  9.7046e-03,  ...,  3.9673e-04,
          6.1798e-04,  3.9062e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.0391,  3.5977,  6.2969,  ..., -3.5801,  7.0859, -4.0938]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 10:25:21 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A part of a gramm is a milligram
A part of a sword is a blade
A part of a apartment is a bedroom
A part of a window is a pane
A part of a brush is a bristle
A part of a shilling is a pence
A part of a gigabit is a megabit
A part of a gun is a
2024-07-25 10:25:22 root INFO     [order_1_approx] starting weight calculation for A part of a window is a pane
A part of a brush is a bristle
A part of a gun is a trigger
A part of a apartment is a bedroom
A part of a shilling is a pence
A part of a gigabit is a megabit
A part of a gramm is a milligram
A part of a sword is a
2024-07-25 10:25:22 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 10:28:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4199,  2.0312, -4.2188,  ..., -0.5898, -2.5938, -3.1562],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 1.6719,  9.3750,  3.9375,  ..., 13.2500, -0.6875, -4.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0270,  0.0015,  0.0315,  ..., -0.0014,  0.0099, -0.0055],
        [ 0.0066,  0.0148, -0.0065,  ..., -0.0002, -0.0032,  0.0039],
        [-0.0084, -0.0009,  0.1309,  ..., -0.0135,  0.0004, -0.0027],
        ...,
        [-0.0014,  0.0029, -0.0168,  ...,  0.0281, -0.0005,  0.0099],
        [-0.0082,  0.0071, -0.0009,  ..., -0.0003,  0.0364,  0.0022],
        [ 0.0046, -0.0034,  0.0066,  ...,  0.0024, -0.0063,  0.0527]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7739,  8.9688,  4.9609,  ..., 13.1328,  0.8945, -4.5156]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 10:28:08 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A part of a window is a pane
A part of a brush is a bristle
A part of a gun is a trigger
A part of a apartment is a bedroom
A part of a shilling is a pence
A part of a gigabit is a megabit
A part of a gramm is a milligram
A part of a sword is a
2024-07-25 10:28:09 root INFO     [order_1_approx] starting weight calculation for A part of a gramm is a milligram
A part of a apartment is a bedroom
A part of a gun is a trigger
A part of a window is a pane
A part of a gigabit is a megabit
A part of a brush is a bristle
A part of a sword is a blade
A part of a shilling is a
2024-07-25 10:28:09 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 10:30:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3867,  0.8125, -3.1250,  ...,  0.9883,  0.3945, -0.8438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([15.0625,  2.4844,  4.9688,  ..., -7.6875, -6.0625, -7.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.8320e-02, -1.1597e-03,  3.7354e-02,  ..., -1.4038e-02,
          9.7656e-03, -4.8218e-03],
        [ 7.3242e-03, -2.5635e-03, -1.2939e-02,  ...,  6.9580e-03,
         -1.9775e-02, -5.0049e-03],
        [ 1.2024e-02, -3.5400e-03,  1.0791e-01,  ..., -1.5259e-03,
         -1.5640e-03,  5.6763e-03],
        ...,
        [-4.8523e-03, -5.7220e-06,  5.6152e-03,  ...,  1.5259e-02,
         -2.3346e-03,  7.5073e-03],
        [ 5.6763e-03,  3.8605e-03, -2.7832e-02,  ..., -9.0332e-03,
          3.8574e-02,  9.6436e-03],
        [-3.8757e-03,  7.8201e-04, -1.7090e-02,  ...,  1.0010e-02,
          2.6245e-03,  3.6133e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[14.3828,  2.6992,  3.0957,  ..., -7.8945, -5.9922, -8.3359]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 10:30:56 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A part of a gramm is a milligram
A part of a apartment is a bedroom
A part of a gun is a trigger
A part of a window is a pane
A part of a gigabit is a megabit
A part of a brush is a bristle
A part of a sword is a blade
A part of a shilling is a
2024-07-25 10:30:56 root INFO     [order_1_approx] starting weight calculation for A part of a sword is a blade
A part of a gun is a trigger
A part of a shilling is a pence
A part of a gramm is a milligram
A part of a gigabit is a megabit
A part of a brush is a bristle
A part of a apartment is a bedroom
A part of a window is a
2024-07-25 10:30:56 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 10:33:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3789,  0.6211,  0.9453,  ..., -2.5312, -2.9844, -2.8281],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 3.0469, -7.3750, -7.0625,  ...,  8.5000,  2.7969,  3.9688],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.4180e-02,  3.8147e-05,  1.1353e-02,  ...,  1.2970e-03,
         -1.6785e-03, -4.4861e-03],
        [-3.4027e-03,  2.0874e-02, -1.1902e-02,  ..., -6.0730e-03,
          6.0425e-03, -2.3193e-03],
        [-5.9204e-03, -1.7166e-03,  1.2598e-01,  ..., -1.9379e-03,
          5.7068e-03,  4.2725e-03],
        ...,
        [ 8.0566e-03,  6.7749e-03, -5.9814e-03,  ...,  2.2705e-02,
         -3.4027e-03, -2.1820e-03],
        [ 7.3242e-03,  7.5684e-03, -2.1057e-03,  ..., -2.9755e-03,
          4.4922e-02,  2.9144e-03],
        [ 2.7771e-03, -7.2021e-03, -2.5391e-02,  ...,  5.7983e-03,
          2.2316e-04,  4.2969e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7891, -7.1484, -8.5469,  ...,  9.0391,  2.9785,  4.1719]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 10:33:43 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A part of a sword is a blade
A part of a gun is a trigger
A part of a shilling is a pence
A part of a gramm is a milligram
A part of a gigabit is a megabit
A part of a brush is a bristle
A part of a apartment is a bedroom
A part of a window is a
2024-07-25 10:33:43 root INFO     [order_1_approx] starting weight calculation for A part of a gramm is a milligram
A part of a brush is a bristle
A part of a shilling is a pence
A part of a gun is a trigger
A part of a sword is a blade
A part of a window is a pane
A part of a gigabit is a megabit
A part of a apartment is a
2024-07-25 10:33:43 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 10:36:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8203,  3.3750, -3.2812,  ..., -0.2148, -0.7305, -3.8906],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([18.1250,  7.2500,  0.3125,  ..., -8.6250, -4.0000, -7.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0466, -0.0070,  0.0239,  ..., -0.0026, -0.0072, -0.0072],
        [-0.0051,  0.0208, -0.0244,  ...,  0.0022,  0.0028,  0.0057],
        [ 0.0013,  0.0030,  0.1279,  ..., -0.0070, -0.0076, -0.0033],
        ...,
        [ 0.0023,  0.0035, -0.0256,  ...,  0.0283,  0.0006, -0.0021],
        [-0.0024,  0.0036,  0.0160,  ..., -0.0047,  0.0393, -0.0002],
        [ 0.0042, -0.0013, -0.0293,  ...,  0.0020,  0.0002,  0.0386]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[18.6406,  6.3516,  1.9434,  ..., -6.6406, -3.5918, -5.8711]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 10:36:29 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A part of a gramm is a milligram
A part of a brush is a bristle
A part of a shilling is a pence
A part of a gun is a trigger
A part of a sword is a blade
A part of a window is a pane
A part of a gigabit is a megabit
A part of a apartment is a
2024-07-25 10:36:29 root INFO     [order_1_approx] starting weight calculation for A part of a window is a pane
A part of a sword is a blade
A part of a gun is a trigger
A part of a shilling is a pence
A part of a gramm is a milligram
A part of a apartment is a bedroom
A part of a brush is a bristle
A part of a gigabit is a
2024-07-25 10:36:29 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 10:39:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5039, -1.5156, -4.8438,  ...,  1.7422,  0.0195, -1.3359],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([25.6250,  1.6562, -6.1875,  ..., 21.1250, 14.5000, -5.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0231,  0.0066, -0.0085,  ...,  0.0002,  0.0007,  0.0160],
        [-0.0013,  0.0161,  0.0011,  ..., -0.0015, -0.0115, -0.0006],
        [ 0.0026,  0.0063,  0.1040,  ...,  0.0028,  0.0009,  0.0070],
        ...,
        [-0.0027,  0.0045, -0.0072,  ...,  0.0310,  0.0022,  0.0181],
        [-0.0026,  0.0056,  0.0187,  ...,  0.0068,  0.0562,  0.0098],
        [ 0.0087,  0.0012, -0.0162,  ...,  0.0002,  0.0026,  0.0449]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[25.8281,  1.0645, -3.8535,  ..., 20.6875, 15.7656, -5.5117]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 10:39:17 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A part of a window is a pane
A part of a sword is a blade
A part of a gun is a trigger
A part of a shilling is a pence
A part of a gramm is a milligram
A part of a apartment is a bedroom
A part of a brush is a bristle
A part of a gigabit is a
2024-07-25 10:39:17 root INFO     [order_1_approx] starting weight calculation for A part of a window is a pane
A part of a gramm is a milligram
A part of a sword is a blade
A part of a gigabit is a megabit
A part of a shilling is a pence
A part of a gun is a trigger
A part of a apartment is a bedroom
A part of a brush is a
2024-07-25 10:39:18 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 10:42:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2109,  0.6602,  1.5000,  ..., -1.5625, -3.1875, -3.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([25.0000, -3.5469, 15.8125,  ...,  5.5938,  0.3906, -6.0312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0454,  0.0036,  0.0430,  ...,  0.0017,  0.0011, -0.0021],
        [ 0.0082,  0.0287,  0.0131,  ..., -0.0024, -0.0016,  0.0016],
        [ 0.0048, -0.0055,  0.1621,  ...,  0.0011, -0.0083,  0.0007],
        ...,
        [ 0.0006,  0.0076, -0.0040,  ...,  0.0332,  0.0057, -0.0011],
        [-0.0054,  0.0042, -0.0093,  ..., -0.0012,  0.0420, -0.0021],
        [ 0.0028, -0.0018,  0.0049,  ..., -0.0019,  0.0019,  0.0557]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[25.2188, -2.0156, 14.2969,  ...,  5.5117,  1.2266, -5.9922]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 10:42:04 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A part of a window is a pane
A part of a gramm is a milligram
A part of a sword is a blade
A part of a gigabit is a megabit
A part of a shilling is a pence
A part of a gun is a trigger
A part of a apartment is a bedroom
A part of a brush is a
2024-07-25 10:42:04 root INFO     total operator prediction time: 1338.8528497219086 seconds
2024-07-25 10:42:04 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on synonyms - exact
2024-07-25 10:42:04 root INFO     building operator synonyms - exact
2024-07-25 10:42:04 root INFO     [order_1_approx] starting weight calculation for Another word for baby is infant
Another word for sofa is couch
Another word for mother is mom
Another word for portion is part
Another word for list is listing
Another word for hieroglyph is hieroglyphic
Another word for rock is stone
Another word for auto is
2024-07-25 10:42:05 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 10:44:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2188, -0.3359,  1.1406,  ..., -1.3438, -0.4766, -0.1348],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 0.0000, -4.0938, 10.0000,  ..., -2.7969,  2.0000, -3.9219],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0417,  0.0153,  0.0181,  ..., -0.0059,  0.0040,  0.0003],
        [ 0.0116,  0.0200,  0.0022,  ...,  0.0037, -0.0067,  0.0014],
        [-0.0036, -0.0038,  0.1572,  ..., -0.0074, -0.0015,  0.0021],
        ...,
        [-0.0016,  0.0018, -0.0082,  ...,  0.0303, -0.0098,  0.0067],
        [ 0.0058, -0.0037, -0.0037,  ...,  0.0022,  0.0571, -0.0045],
        [ 0.0012, -0.0063, -0.0261,  ...,  0.0010,  0.0050,  0.0542]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9458, -3.2891,  9.6797,  ..., -2.5488,  0.8271, -4.7578]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 10:44:53 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Another word for baby is infant
Another word for sofa is couch
Another word for mother is mom
Another word for portion is part
Another word for list is listing
Another word for hieroglyph is hieroglyphic
Another word for rock is stone
Another word for auto is
2024-07-25 10:44:53 root INFO     [order_1_approx] starting weight calculation for Another word for portion is part
Another word for auto is car
Another word for baby is infant
Another word for list is listing
Another word for rock is stone
Another word for mother is mom
Another word for hieroglyph is hieroglyphic
Another word for sofa is
2024-07-25 10:44:53 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 10:47:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1094, -2.4688, -6.1562,  ...,  0.1709, -1.1406, -2.9062],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 0.8984, -0.1270, -2.4375,  ...,  9.5625, -5.0000,  2.3281],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0254,  0.0044, -0.0036,  ...,  0.0014,  0.0033,  0.0033],
        [ 0.0035,  0.0227, -0.0035,  ..., -0.0053,  0.0108,  0.0020],
        [-0.0048, -0.0062,  0.1084,  ..., -0.0014, -0.0049, -0.0035],
        ...,
        [-0.0074, -0.0073,  0.0010,  ...,  0.0332, -0.0019,  0.0026],
        [ 0.0013,  0.0070, -0.0044,  ...,  0.0049,  0.0508, -0.0030],
        [ 0.0074,  0.0003, -0.0261,  ...,  0.0070,  0.0052,  0.0537]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0547, -0.8774, -2.2324,  ..., 10.4141, -3.7051,  1.2529]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 10:47:42 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Another word for portion is part
Another word for auto is car
Another word for baby is infant
Another word for list is listing
Another word for rock is stone
Another word for mother is mom
Another word for hieroglyph is hieroglyphic
Another word for sofa is
2024-07-25 10:47:42 root INFO     [order_1_approx] starting weight calculation for Another word for auto is car
Another word for baby is infant
Another word for sofa is couch
Another word for portion is part
Another word for hieroglyph is hieroglyphic
Another word for mother is mom
Another word for list is listing
Another word for rock is
2024-07-25 10:47:42 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 10:50:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.6016, -2.0938, -3.9062,  ..., -3.6094, -1.6484, -1.9609],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([11.5000, -6.3125, 14.0625,  ...,  9.8750,  2.5156, -6.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0376,  0.0051,  0.0143,  ...,  0.0020, -0.0055,  0.0068],
        [ 0.0002,  0.0267,  0.0066,  ..., -0.0027, -0.0088,  0.0258],
        [ 0.0049, -0.0096,  0.1445,  ..., -0.0050, -0.0089,  0.0056],
        ...,
        [ 0.0091,  0.0003, -0.0121,  ...,  0.0225, -0.0100, -0.0045],
        [ 0.0009,  0.0067,  0.0067,  ..., -0.0088,  0.0547, -0.0156],
        [ 0.0087, -0.0034,  0.0080,  ..., -0.0023, -0.0026,  0.0608]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[11.8359, -3.0996, 12.2109,  ...,  9.5469,  0.7598, -7.1484]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 10:50:30 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Another word for auto is car
Another word for baby is infant
Another word for sofa is couch
Another word for portion is part
Another word for hieroglyph is hieroglyphic
Another word for mother is mom
Another word for list is listing
Another word for rock is
2024-07-25 10:50:31 root INFO     [order_1_approx] starting weight calculation for Another word for mother is mom
Another word for list is listing
Another word for portion is part
Another word for rock is stone
Another word for sofa is couch
Another word for auto is car
Another word for baby is infant
Another word for hieroglyph is
2024-07-25 10:50:31 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 10:53:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4258, -1.5781, -2.5156,  ...,  1.1250,  1.2656, -0.7109],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  5.6250, -13.2500,   6.6250,  ...,  -3.2500,   5.6250,  -3.9844],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0103, -0.0005, -0.0021,  ...,  0.0005,  0.0024,  0.0038],
        [-0.0005,  0.0049, -0.0022,  ..., -0.0038,  0.0014,  0.0004],
        [-0.0004, -0.0018,  0.0508,  ..., -0.0028, -0.0002, -0.0029],
        ...,
        [ 0.0035, -0.0025, -0.0017,  ...,  0.0078,  0.0014, -0.0007],
        [-0.0023,  0.0019, -0.0073,  ..., -0.0017,  0.0172,  0.0020],
        [ 0.0008, -0.0030, -0.0011,  ...,  0.0018,  0.0038,  0.0131]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  5.4727, -13.4844,   6.8984,  ...,  -3.1562,   5.6289,  -3.3477]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 10:53:18 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Another word for mother is mom
Another word for list is listing
Another word for portion is part
Another word for rock is stone
Another word for sofa is couch
Another word for auto is car
Another word for baby is infant
Another word for hieroglyph is
2024-07-25 10:53:18 root INFO     [order_1_approx] starting weight calculation for Another word for mother is mom
Another word for sofa is couch
Another word for auto is car
Another word for hieroglyph is hieroglyphic
Another word for rock is stone
Another word for baby is infant
Another word for portion is part
Another word for list is
2024-07-25 10:53:18 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 10:56:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1094, -3.7188,  2.3281,  ..., -2.0469, -0.7148,  0.1094],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-3.3125, -0.8281,  5.8438,  ...,  6.2500,  6.0312, 11.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0449,  0.0057,  0.0009,  ..., -0.0021,  0.0003, -0.0046],
        [ 0.0002,  0.0194,  0.0067,  ...,  0.0017, -0.0097,  0.0084],
        [-0.0090, -0.0031,  0.1553,  ..., -0.0027, -0.0004, -0.0035],
        ...,
        [ 0.0101,  0.0002, -0.0058,  ...,  0.0245,  0.0092,  0.0070],
        [-0.0019,  0.0045,  0.0145,  ...,  0.0031,  0.0566, -0.0021],
        [-0.0020, -0.0005, -0.0025,  ..., -0.0011,  0.0002,  0.0562]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0078,  0.5176,  5.9883,  ...,  4.3125,  4.5547,  9.8828]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 10:56:06 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Another word for mother is mom
Another word for sofa is couch
Another word for auto is car
Another word for hieroglyph is hieroglyphic
Another word for rock is stone
Another word for baby is infant
Another word for portion is part
Another word for list is
2024-07-25 10:56:07 root INFO     [order_1_approx] starting weight calculation for Another word for sofa is couch
Another word for portion is part
Another word for auto is car
Another word for list is listing
Another word for baby is infant
Another word for hieroglyph is hieroglyphic
Another word for rock is stone
Another word for mother is
2024-07-25 10:56:07 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 10:58:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.3594,  0.8438, -0.3047,  ..., -2.5625, -2.3125, -1.8672],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 6.5000,  3.4531, 13.5625,  ..., -8.1875,  9.4375, -2.3281],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.9541e-02,  6.5308e-03,  1.4038e-02,  ...,  3.8147e-03,
          1.0834e-03, -2.4796e-04],
        [ 5.5237e-03,  1.4099e-02, -2.0264e-02,  ..., -5.3406e-05,
         -2.6245e-03, -2.1973e-03],
        [ 2.2736e-03,  4.4250e-03,  1.0400e-01,  ..., -2.1820e-03,
         -2.8076e-03,  1.5717e-03],
        ...,
        [-9.3842e-04,  4.8218e-03,  1.2512e-03,  ...,  2.5146e-02,
         -8.5449e-03,  7.9956e-03],
        [ 4.7302e-03,  2.0294e-03, -9.6436e-03,  ...,  2.4567e-03,
          3.7842e-02,  7.5912e-04],
        [-5.6458e-03, -4.8828e-03, -1.5259e-05,  ..., -1.6785e-04,
         -6.1035e-04,  4.1504e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 7.7344,  3.3457, 12.4062,  ..., -8.8906,  8.3828, -2.4648]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 10:58:56 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Another word for sofa is couch
Another word for portion is part
Another word for auto is car
Another word for list is listing
Another word for baby is infant
Another word for hieroglyph is hieroglyphic
Another word for rock is stone
Another word for mother is
2024-07-25 10:58:56 root INFO     [order_1_approx] starting weight calculation for Another word for list is listing
Another word for mother is mom
Another word for baby is infant
Another word for rock is stone
Another word for hieroglyph is hieroglyphic
Another word for sofa is couch
Another word for auto is car
Another word for portion is
2024-07-25 10:58:56 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 11:01:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0312, -3.3438, -1.6406,  ..., -0.7266, -0.7891,  0.2617],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  1.8750,   9.3750,  -4.2188,  ...,   6.4375,   4.8750, -13.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0493,  0.0018,  0.0228,  ..., -0.0013,  0.0080,  0.0097],
        [ 0.0009,  0.0337, -0.0087,  ..., -0.0017, -0.0009,  0.0046],
        [ 0.0016, -0.0034,  0.1875,  ..., -0.0036, -0.0036,  0.0126],
        ...,
        [-0.0063, -0.0066,  0.0008,  ...,  0.0420,  0.0053,  0.0057],
        [-0.0082,  0.0040,  0.0089,  ...,  0.0060,  0.0728, -0.0012],
        [-0.0009, -0.0079, -0.0277,  ..., -0.0031,  0.0108,  0.0718]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  3.5820,   7.4414,  -3.2012,  ...,   9.2969,   2.1875, -13.4453]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 11:01:52 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Another word for list is listing
Another word for mother is mom
Another word for baby is infant
Another word for rock is stone
Another word for hieroglyph is hieroglyphic
Another word for sofa is couch
Another word for auto is car
Another word for portion is
2024-07-25 11:01:52 root INFO     [order_1_approx] starting weight calculation for Another word for portion is part
Another word for sofa is couch
Another word for list is listing
Another word for rock is stone
Another word for hieroglyph is hieroglyphic
Another word for mother is mom
Another word for auto is car
Another word for baby is
2024-07-25 11:01:53 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 11:04:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2344, -2.5625, -1.4219,  ..., -1.5781, -0.7891,  0.2070],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  6.6562,  12.1875,   6.6875,  ...,  12.6875,   2.8438, -10.8125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0332,  0.0059,  0.0015,  ..., -0.0062,  0.0046, -0.0046],
        [ 0.0017,  0.0156, -0.0118,  ...,  0.0016, -0.0048,  0.0126],
        [-0.0004,  0.0019,  0.1040,  ..., -0.0060, -0.0019, -0.0010],
        ...,
        [ 0.0045, -0.0089, -0.0029,  ...,  0.0276, -0.0084,  0.0046],
        [-0.0089,  0.0059,  0.0019,  ...,  0.0017,  0.0405, -0.0055],
        [-0.0018, -0.0057, -0.0054,  ..., -0.0010, -0.0063,  0.0422]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  7.6914,  12.0000,   5.4375,  ...,  11.7500,   3.4492, -10.6797]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 11:04:41 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Another word for portion is part
Another word for sofa is couch
Another word for list is listing
Another word for rock is stone
Another word for hieroglyph is hieroglyphic
Another word for mother is mom
Another word for auto is car
Another word for baby is
2024-07-25 11:04:41 root INFO     total operator prediction time: 1356.628571987152 seconds
2024-07-25 11:04:41 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on hypernyms - misc
2024-07-25 11:04:41 root INFO     building operator hypernyms - misc
2024-07-25 11:04:41 root INFO     [order_1_approx] starting weight calculation for The photo falls into the category of picture
The diary falls into the category of journal
The vase falls into the category of jar
The deodorant falls into the category of toiletry
The brooch falls into the category of jewelry
The gasoline falls into the category of fuel
The stapler falls into the category of device
The sweater falls into the category of
2024-07-25 11:04:41 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 11:07:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.1094, -1.6641, -0.5508,  ...,  0.6250, -3.0156,  1.0703],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 2.8438, 11.5000, -4.1875,  ...,  3.7500,  3.2500, -6.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.7100e-02,  5.2490e-03,  4.4250e-03,  ...,  1.0498e-02,
          2.6398e-03, -2.0752e-03],
        [-3.5400e-03,  2.1729e-02,  9.0027e-04,  ..., -3.7537e-03,
          1.3885e-03,  5.6152e-03],
        [ 5.0659e-03, -4.4861e-03,  1.1084e-01,  ..., -4.6997e-03,
         -7.3853e-03,  7.6294e-06],
        ...,
        [ 1.9073e-03,  3.2616e-04,  7.8735e-03,  ...,  2.4170e-02,
         -4.9744e-03,  1.8463e-03],
        [ 5.5237e-03,  8.3618e-03,  6.3782e-03,  ...,  8.8501e-04,
          4.2725e-02,  4.1809e-03],
        [ 6.0120e-03, -3.9673e-03, -1.3611e-02,  ..., -4.2114e-03,
          7.1716e-03,  4.3457e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.3574, 10.5703, -1.9766,  ...,  3.1309,  5.3750, -6.0508]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 11:07:32 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The photo falls into the category of picture
The diary falls into the category of journal
The vase falls into the category of jar
The deodorant falls into the category of toiletry
The brooch falls into the category of jewelry
The gasoline falls into the category of fuel
The stapler falls into the category of device
The sweater falls into the category of
2024-07-25 11:07:32 root INFO     [order_1_approx] starting weight calculation for The stapler falls into the category of device
The diary falls into the category of journal
The deodorant falls into the category of toiletry
The gasoline falls into the category of fuel
The sweater falls into the category of clothes
The photo falls into the category of picture
The vase falls into the category of jar
The brooch falls into the category of
2024-07-25 11:07:32 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 11:10:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.7812, -2.3438, -0.1250,  ..., -1.0547, -2.9844,  0.0215],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-7.4375, -1.7188, -0.3750,  ...,  3.7812,  2.6875, -4.3125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0371,  0.0013,  0.0173,  ...,  0.0034, -0.0040, -0.0024],
        [-0.0001,  0.0208, -0.0007,  ..., -0.0034, -0.0070,  0.0046],
        [-0.0038, -0.0075,  0.1240,  ..., -0.0059,  0.0040, -0.0041],
        ...,
        [-0.0040,  0.0002, -0.0012,  ...,  0.0344, -0.0078,  0.0004],
        [ 0.0075,  0.0052,  0.0034,  ...,  0.0008,  0.0503,  0.0023],
        [-0.0010, -0.0027, -0.0032,  ..., -0.0040,  0.0080,  0.0498]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-7.8203, -1.4697, -0.1350,  ...,  4.2188,  2.0859, -5.4297]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 11:10:24 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The stapler falls into the category of device
The diary falls into the category of journal
The deodorant falls into the category of toiletry
The gasoline falls into the category of fuel
The sweater falls into the category of clothes
The photo falls into the category of picture
The vase falls into the category of jar
The brooch falls into the category of
2024-07-25 11:10:24 root INFO     [order_1_approx] starting weight calculation for The deodorant falls into the category of toiletry
The vase falls into the category of jar
The gasoline falls into the category of fuel
The photo falls into the category of picture
The sweater falls into the category of clothes
The diary falls into the category of journal
The brooch falls into the category of jewelry
The stapler falls into the category of
2024-07-25 11:10:24 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 11:13:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1875, -1.5938,  1.6797,  ...,  0.3047, -6.6875, -1.0703],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -4.3125,   0.6562,  11.0000,  ...,   8.6250, -16.2500,   4.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0464,  0.0069, -0.0009,  ...,  0.0006, -0.0044,  0.0030],
        [ 0.0030,  0.0383, -0.0052,  ...,  0.0058,  0.0042,  0.0082],
        [-0.0017, -0.0056,  0.1641,  ..., -0.0095, -0.0087, -0.0055],
        ...,
        [ 0.0052,  0.0155, -0.0063,  ...,  0.0371, -0.0084,  0.0156],
        [ 0.0096, -0.0023,  0.0078,  ...,  0.0028,  0.0684, -0.0109],
        [-0.0050,  0.0044, -0.0085,  ...,  0.0109, -0.0111,  0.0645]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -2.6133,   1.6074,  10.5391,  ...,   7.7188, -17.7188,   6.0234]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 11:13:15 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The deodorant falls into the category of toiletry
The vase falls into the category of jar
The gasoline falls into the category of fuel
The photo falls into the category of picture
The sweater falls into the category of clothes
The diary falls into the category of journal
The brooch falls into the category of jewelry
The stapler falls into the category of
2024-07-25 11:13:15 root INFO     [order_1_approx] starting weight calculation for The photo falls into the category of picture
The deodorant falls into the category of toiletry
The vase falls into the category of jar
The sweater falls into the category of clothes
The brooch falls into the category of jewelry
The stapler falls into the category of device
The diary falls into the category of journal
The gasoline falls into the category of
2024-07-25 11:13:15 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 11:16:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2266, -1.1719,  4.4375,  ..., -1.7422, -4.9375, -1.7578],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 0.2188,  0.4688, -5.4375,  ..., 17.6250, -3.6875, -2.5938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0212,  0.0055, -0.0013,  ...,  0.0001, -0.0038, -0.0030],
        [ 0.0045,  0.0200, -0.0023,  ..., -0.0077, -0.0092,  0.0010],
        [-0.0014, -0.0009,  0.0742,  ..., -0.0013, -0.0065,  0.0002],
        ...,
        [ 0.0028,  0.0019, -0.0128,  ...,  0.0125,  0.0029, -0.0018],
        [ 0.0042,  0.0029,  0.0065,  ..., -0.0003,  0.0420,  0.0024],
        [ 0.0053,  0.0056, -0.0055,  ..., -0.0034,  0.0109,  0.0291]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.4453,  1.0566, -4.9062,  ..., 17.9688, -4.7227, -1.2256]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 11:16:06 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The photo falls into the category of picture
The deodorant falls into the category of toiletry
The vase falls into the category of jar
The sweater falls into the category of clothes
The brooch falls into the category of jewelry
The stapler falls into the category of device
The diary falls into the category of journal
The gasoline falls into the category of
2024-07-25 11:16:07 root INFO     [order_1_approx] starting weight calculation for The sweater falls into the category of clothes
The vase falls into the category of jar
The stapler falls into the category of device
The brooch falls into the category of jewelry
The diary falls into the category of journal
The gasoline falls into the category of fuel
The photo falls into the category of picture
The deodorant falls into the category of
2024-07-25 11:16:07 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 11:18:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.2500, -1.0156,  1.4688,  ..., -0.4473, -6.0625, -1.3594],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 6.9062, -6.7188,  4.8750,  ..., 15.3750, -2.3906, -1.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.3691e-02,  1.1108e-02, -1.5106e-03,  ...,  3.1281e-03,
          3.6774e-03,  5.2795e-03],
        [-2.6093e-03,  2.5879e-02,  7.0190e-04,  ..., -2.5940e-04,
         -1.0742e-02,  1.0681e-04],
        [ 3.9673e-03, -3.6163e-03,  1.1914e-01,  ..., -5.5847e-03,
         -1.2360e-03, -1.4343e-03],
        ...,
        [ 4.6997e-03,  5.4932e-03, -8.6670e-03,  ...,  3.1250e-02,
          1.3428e-02,  2.7924e-03],
        [ 6.5002e-03,  1.0559e-02, -1.5259e-02,  ...,  5.0049e-03,
          5.7617e-02, -1.4954e-03],
        [-5.3711e-03,  9.9182e-04, -1.2573e-02,  ...,  6.2561e-04,
          7.5073e-03,  4.7119e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.7891, -7.1523,  6.2305,  ..., 13.8984, -3.9297, -3.0430]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 11:18:58 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The sweater falls into the category of clothes
The vase falls into the category of jar
The stapler falls into the category of device
The brooch falls into the category of jewelry
The diary falls into the category of journal
The gasoline falls into the category of fuel
The photo falls into the category of picture
The deodorant falls into the category of
2024-07-25 11:18:58 root INFO     [order_1_approx] starting weight calculation for The photo falls into the category of picture
The sweater falls into the category of clothes
The gasoline falls into the category of fuel
The diary falls into the category of journal
The deodorant falls into the category of toiletry
The brooch falls into the category of jewelry
The stapler falls into the category of device
The vase falls into the category of
2024-07-25 11:18:58 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 11:21:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0742, -3.0312, -4.3125,  ..., -0.3984, -0.6992, -1.6016],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-10.1250,   3.5781, -16.3750,  ...,   5.5000,  -1.3125,  -7.5938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0312,  0.0015,  0.0106,  ...,  0.0053, -0.0020,  0.0006],
        [ 0.0026,  0.0250,  0.0039,  ...,  0.0016, -0.0043,  0.0033],
        [-0.0016,  0.0021,  0.0913,  ...,  0.0026, -0.0023, -0.0006],
        ...,
        [ 0.0061,  0.0013, -0.0057,  ...,  0.0201, -0.0031,  0.0002],
        [ 0.0052,  0.0029,  0.0004,  ..., -0.0017,  0.0469,  0.0016],
        [ 0.0042,  0.0021, -0.0025,  ..., -0.0024,  0.0049,  0.0459]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -7.4102,   2.1895, -14.9453,  ...,   6.0703,  -1.8379,  -8.9219]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 11:21:47 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The photo falls into the category of picture
The sweater falls into the category of clothes
The gasoline falls into the category of fuel
The diary falls into the category of journal
The deodorant falls into the category of toiletry
The brooch falls into the category of jewelry
The stapler falls into the category of device
The vase falls into the category of
2024-07-25 11:21:47 root INFO     [order_1_approx] starting weight calculation for The diary falls into the category of journal
The stapler falls into the category of device
The deodorant falls into the category of toiletry
The brooch falls into the category of jewelry
The sweater falls into the category of clothes
The vase falls into the category of jar
The gasoline falls into the category of fuel
The photo falls into the category of
2024-07-25 11:21:48 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 11:24:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7891, -0.7891, -0.2520,  ...,  0.8789, -0.4531,  0.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 2.8438,  7.3750, -6.9375,  ..., -0.1934,  4.7188,  1.5391],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.4668e-02, -2.5787e-03,  4.5166e-03,  ...,  2.8229e-03,
          5.2490e-03, -6.7444e-03],
        [ 8.3160e-04,  1.8799e-02,  9.1553e-03,  ..., -4.0894e-03,
         -9.0027e-04, -1.5869e-03],
        [-1.8005e-03, -1.2817e-03,  9.0820e-02,  ..., -5.1270e-03,
          1.1063e-04, -3.3264e-03],
        ...,
        [ 2.2888e-03, -4.6387e-03, -8.1787e-03,  ...,  1.8799e-02,
         -8.4686e-04, -5.9128e-05],
        [ 6.9275e-03,  3.7994e-03,  5.0354e-03,  ..., -3.0518e-03,
          3.9062e-02,  1.8158e-03],
        [-3.4180e-03, -9.5367e-04, -1.5259e-03,  ..., -7.9346e-03,
          2.3956e-03,  3.1738e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4023,  8.1484, -7.2773,  ..., -0.2042,  4.7227,  0.3701]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 11:24:35 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The diary falls into the category of journal
The stapler falls into the category of device
The deodorant falls into the category of toiletry
The brooch falls into the category of jewelry
The sweater falls into the category of clothes
The vase falls into the category of jar
The gasoline falls into the category of fuel
The photo falls into the category of
2024-07-25 11:24:35 root INFO     [order_1_approx] starting weight calculation for The gasoline falls into the category of fuel
The sweater falls into the category of clothes
The stapler falls into the category of device
The deodorant falls into the category of toiletry
The brooch falls into the category of jewelry
The vase falls into the category of jar
The photo falls into the category of picture
The diary falls into the category of
2024-07-25 11:24:35 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 11:27:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6406, -2.9062,  4.8125,  ...,  0.3281, -4.2500,  2.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 5.9688,  3.2656, 14.3750,  ..., -1.5469, -4.9375, 10.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0330,  0.0017,  0.0070,  ...,  0.0023,  0.0078, -0.0085],
        [ 0.0044,  0.0208,  0.0022,  ..., -0.0021,  0.0017,  0.0029],
        [-0.0065, -0.0017,  0.0762,  ..., -0.0011, -0.0018, -0.0063],
        ...,
        [-0.0023,  0.0024, -0.0071,  ...,  0.0212, -0.0013, -0.0008],
        [ 0.0029,  0.0005, -0.0007,  ..., -0.0022,  0.0378,  0.0001],
        [-0.0022,  0.0001, -0.0079,  ..., -0.0068,  0.0099,  0.0437]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 7.0234,  4.9023, 15.2812,  ..., -3.4375, -5.1875, 10.0078]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 11:27:24 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The gasoline falls into the category of fuel
The sweater falls into the category of clothes
The stapler falls into the category of device
The deodorant falls into the category of toiletry
The brooch falls into the category of jewelry
The vase falls into the category of jar
The photo falls into the category of picture
The diary falls into the category of
2024-07-25 11:27:24 root INFO     total operator prediction time: 1363.284728527069 seconds
2024-07-25 11:27:24 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on meronyms - substance
2024-07-25 11:27:24 root INFO     building operator meronyms - substance
2024-07-25 11:27:24 root INFO     [order_1_approx] starting weight calculation for A bronze is made up of copper
A roof is made up of shingles
A cloud is made up of vapor
A concrete is made up of silicon
A flag is made up of fabric
A box is made up of cardboard
A bottle is made up of glass
A wall is made up of
2024-07-25 11:27:25 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 11:30:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1328,  0.5859,  5.4062,  ..., -3.8125, -3.8750, -2.0938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 8.3125,  7.8750,  6.6250,  ..., 10.1250, -0.1250,  3.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0305, -0.0078,  0.0361,  ...,  0.0056, -0.0100,  0.0095],
        [-0.0040,  0.0160, -0.0046,  ..., -0.0036,  0.0077,  0.0093],
        [ 0.0037, -0.0021,  0.1348,  ..., -0.0043, -0.0140, -0.0064],
        ...,
        [ 0.0019,  0.0091, -0.0057,  ...,  0.0243, -0.0018,  0.0008],
        [ 0.0075, -0.0011,  0.0209,  ..., -0.0043,  0.0439, -0.0043],
        [-0.0024, -0.0018, -0.0074,  ...,  0.0065,  0.0037,  0.0483]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[8.2656, 8.2031, 6.2969,  ..., 9.2031, 0.3931, 1.7734]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 11:30:15 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A bronze is made up of copper
A roof is made up of shingles
A cloud is made up of vapor
A concrete is made up of silicon
A flag is made up of fabric
A box is made up of cardboard
A bottle is made up of glass
A wall is made up of
2024-07-25 11:30:15 root INFO     [order_1_approx] starting weight calculation for A box is made up of cardboard
A flag is made up of fabric
A concrete is made up of silicon
A roof is made up of shingles
A bottle is made up of glass
A wall is made up of cement
A cloud is made up of vapor
A bronze is made up of
2024-07-25 11:30:15 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 11:33:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.8438, -0.1914,  1.3438,  ..., -1.6250, -5.3438, -2.7188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -3.1562,  22.7500,   1.5625,  ...,  -8.8125,   9.5000, -15.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0356, -0.0009,  0.0004,  ...,  0.0037,  0.0031,  0.0100],
        [ 0.0014,  0.0220,  0.0205,  ..., -0.0029,  0.0058, -0.0120],
        [ 0.0022, -0.0153,  0.1543,  ...,  0.0016,  0.0037, -0.0095],
        ...,
        [ 0.0073,  0.0017, -0.0059,  ...,  0.0332,  0.0066, -0.0047],
        [ 0.0019,  0.0058,  0.0244,  ...,  0.0012,  0.0571, -0.0103],
        [ 0.0045,  0.0019, -0.0024,  ...,  0.0005,  0.0089,  0.0605]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -1.6777,  22.0312,   0.3848,  ...,  -6.0547,   8.8594, -18.8125]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 11:33:06 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A box is made up of cardboard
A flag is made up of fabric
A concrete is made up of silicon
A roof is made up of shingles
A bottle is made up of glass
A wall is made up of cement
A cloud is made up of vapor
A bronze is made up of
2024-07-25 11:33:06 root INFO     [order_1_approx] starting weight calculation for A box is made up of cardboard
A wall is made up of cement
A cloud is made up of vapor
A flag is made up of fabric
A roof is made up of shingles
A bronze is made up of copper
A concrete is made up of silicon
A bottle is made up of
2024-07-25 11:33:06 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 11:35:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0781, -0.2617,  3.9688,  ..., -2.0312, -5.5312, -1.8906],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 7.2500,  3.2656, -6.9375,  ...,  9.5000, -1.3281, -7.2812],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.3203e-02,  6.6223e-03,  1.9897e-02,  ..., -1.8616e-03,
          1.9989e-03,  9.3079e-04],
        [-8.7891e-03,  2.3926e-02, -5.7983e-03,  ..., -7.2479e-04,
          6.6833e-03,  3.7994e-03],
        [ 4.6387e-03,  6.8665e-04,  1.4258e-01,  ...,  6.1035e-05,
         -9.0332e-03, -5.3101e-03],
        ...,
        [ 2.7771e-03,  2.1210e-03,  1.3794e-02,  ...,  2.9541e-02,
          1.3184e-02,  0.0000e+00],
        [-8.6670e-03,  1.1292e-03,  2.5940e-03,  ..., -8.5449e-03,
          6.4453e-02, -2.2888e-03],
        [-3.4485e-03, -6.2866e-03, -2.1118e-02,  ...,  3.7231e-03,
          2.8687e-03,  5.6885e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 7.8164,  4.0234, -7.3672,  ...,  9.4453, -0.5767, -8.4922]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 11:35:57 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A box is made up of cardboard
A wall is made up of cement
A cloud is made up of vapor
A flag is made up of fabric
A roof is made up of shingles
A bronze is made up of copper
A concrete is made up of silicon
A bottle is made up of
2024-07-25 11:35:58 root INFO     [order_1_approx] starting weight calculation for A bottle is made up of glass
A roof is made up of shingles
A wall is made up of cement
A flag is made up of fabric
A concrete is made up of silicon
A bronze is made up of copper
A cloud is made up of vapor
A box is made up of
2024-07-25 11:35:58 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 11:38:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1094,  0.6992,  3.7656,  ..., -2.2969, -3.8438, -4.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -0.7188,   6.0625,  16.0000,  ...,   4.4375,  -9.1250, -10.9375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0430,  0.0021,  0.0244,  ...,  0.0003,  0.0027,  0.0157],
        [ 0.0003,  0.0215, -0.0176,  ...,  0.0015,  0.0015,  0.0010],
        [ 0.0016,  0.0009,  0.1475,  ..., -0.0034, -0.0109,  0.0004],
        ...,
        [-0.0027,  0.0065,  0.0095,  ...,  0.0270,  0.0018, -0.0022],
        [ 0.0014,  0.0034,  0.0060,  ..., -0.0090,  0.0610, -0.0005],
        [-0.0003,  0.0021, -0.0173,  ...,  0.0056,  0.0039,  0.0684]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -0.3049,   8.9375,  13.7969,  ...,   3.8203,  -8.9141, -10.7812]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 11:38:48 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A bottle is made up of glass
A roof is made up of shingles
A wall is made up of cement
A flag is made up of fabric
A concrete is made up of silicon
A bronze is made up of copper
A cloud is made up of vapor
A box is made up of
2024-07-25 11:38:48 root INFO     [order_1_approx] starting weight calculation for A bottle is made up of glass
A cloud is made up of vapor
A wall is made up of cement
A box is made up of cardboard
A bronze is made up of copper
A concrete is made up of silicon
A roof is made up of shingles
A flag is made up of
2024-07-25 11:38:48 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 11:41:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5625,  0.1016,  0.7148,  ..., -1.2812, -5.7812, -2.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 0.5312,  8.1875, -1.3125,  ...,  8.9375, -2.8750, -7.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.6865e-02, -7.8583e-04,  2.4658e-02,  ..., -3.2043e-03,
          6.1035e-05,  0.0000e+00],
        [-8.9722e-03,  2.0752e-02,  2.0905e-03,  ..., -2.2888e-03,
          9.4604e-04,  7.2937e-03],
        [-4.0588e-03, -2.4414e-03,  1.2988e-01,  ..., -4.9438e-03,
         -4.7302e-03, -1.2512e-03],
        ...,
        [ 1.8921e-03,  4.8218e-03, -1.1108e-02,  ...,  3.4424e-02,
         -2.8534e-03, -8.6670e-03],
        [ 2.3956e-03, -2.0599e-04,  1.2024e-02,  ...,  8.7738e-04,
          3.8086e-02, -2.2278e-03],
        [ 3.1738e-03, -3.4332e-03, -8.3008e-03,  ...,  8.1635e-04,
          1.1139e-03,  4.8340e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1621,  7.3164, -1.7500,  ...,  7.2266, -2.1816, -7.2852]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 11:41:41 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A bottle is made up of glass
A cloud is made up of vapor
A wall is made up of cement
A box is made up of cardboard
A bronze is made up of copper
A concrete is made up of silicon
A roof is made up of shingles
A flag is made up of
2024-07-25 11:41:42 root INFO     [order_1_approx] starting weight calculation for A flag is made up of fabric
A bottle is made up of glass
A concrete is made up of silicon
A cloud is made up of vapor
A box is made up of cardboard
A bronze is made up of copper
A wall is made up of cement
A roof is made up of
2024-07-25 11:41:42 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 11:44:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9336, -0.4004,  6.0938,  ..., -2.6406, -1.7188, -3.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 0.0469,  6.8125, -0.7500,  ..., -0.9922,  2.8438, -5.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0233,  0.0031, -0.0151,  ...,  0.0029, -0.0014,  0.0117],
        [ 0.0072,  0.0188,  0.0107,  ..., -0.0042,  0.0024,  0.0014],
        [ 0.0003, -0.0011,  0.1055,  ..., -0.0062, -0.0013, -0.0087],
        ...,
        [ 0.0023,  0.0056,  0.0092,  ...,  0.0183,  0.0031, -0.0021],
        [ 0.0017, -0.0003,  0.0264,  ..., -0.0041,  0.0337, -0.0014],
        [-0.0023, -0.0014, -0.0012,  ...,  0.0081, -0.0027,  0.0415]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9385,  6.5898, -1.2422,  ...,  0.3086,  4.6914, -5.9141]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 11:44:32 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A flag is made up of fabric
A bottle is made up of glass
A concrete is made up of silicon
A cloud is made up of vapor
A box is made up of cardboard
A bronze is made up of copper
A wall is made up of cement
A roof is made up of
2024-07-25 11:44:33 root INFO     [order_1_approx] starting weight calculation for A flag is made up of fabric
A concrete is made up of silicon
A bottle is made up of glass
A box is made up of cardboard
A bronze is made up of copper
A roof is made up of shingles
A wall is made up of cement
A cloud is made up of
2024-07-25 11:44:33 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 11:47:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5273, -2.1719,  2.6875,  ..., -1.7188, -5.9375, -0.8672],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 5.0625, 17.5000, -3.7812,  ...,  9.7500, -2.4688, -2.8125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.3691e-02, -8.8120e-04,  1.4572e-03,  ...,  9.1553e-05,
          4.2725e-04,  1.2939e-02],
        [-2.9907e-03,  2.3560e-02,  1.2817e-02,  ...,  2.4719e-03,
         -1.1749e-03,  6.6528e-03],
        [-2.6855e-03, -3.2806e-03,  9.8633e-02,  ..., -6.8970e-03,
         -3.8452e-03,  8.7280e-03],
        ...,
        [-8.4686e-04,  4.8218e-03,  1.1780e-02,  ...,  1.8188e-02,
          3.0212e-03, -9.3994e-03],
        [ 1.1658e-02,  3.2806e-04, -1.4160e-02,  ..., -1.2894e-03,
          4.4922e-02,  1.8311e-02],
        [ 7.2632e-03, -5.4932e-03, -1.7212e-02,  ..., -1.4801e-03,
         -3.4180e-03,  4.3213e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.2148, 17.9219, -4.6172,  ...,  8.9844, -2.1465, -2.1641]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 11:47:24 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A flag is made up of fabric
A concrete is made up of silicon
A bottle is made up of glass
A box is made up of cardboard
A bronze is made up of copper
A roof is made up of shingles
A wall is made up of cement
A cloud is made up of
2024-07-25 11:47:25 root INFO     [order_1_approx] starting weight calculation for A wall is made up of cement
A bronze is made up of copper
A box is made up of cardboard
A flag is made up of fabric
A bottle is made up of glass
A roof is made up of shingles
A cloud is made up of vapor
A concrete is made up of
2024-07-25 11:47:25 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 11:50:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 3.1875, -0.2734,  5.5625,  ..., -1.7422, -4.2812, -3.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([10.6250, 12.0000,  6.3438,  ..., -6.5625, -1.9922,  2.4688],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.6611e-02,  1.7853e-03, -5.7983e-03,  ...,  1.3580e-03,
          2.4109e-03,  1.2329e-02],
        [-5.0049e-03,  2.0630e-02,  3.2043e-04,  ..., -4.2114e-03,
          1.8311e-04,  7.7515e-03],
        [-7.5378e-03, -4.0894e-03,  1.2207e-01,  ..., -2.8992e-03,
         -6.5918e-03, -5.1880e-03],
        ...,
        [ 1.1353e-02, -2.8687e-03, -8.1787e-03,  ...,  1.6602e-02,
          6.1035e-03, -1.1902e-03],
        [ 1.0204e-04,  6.7139e-03,  4.2419e-03,  ..., -1.0986e-03,
          3.8086e-02,  4.3335e-03],
        [ 1.1520e-03,  3.2196e-03,  3.5706e-03,  ..., -1.5869e-03,
         -9.8877e-03,  4.5654e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 8.6562, 12.6250,  6.1133,  ..., -6.9141, -3.3789,  0.8760]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 11:50:16 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A wall is made up of cement
A bronze is made up of copper
A box is made up of cardboard
A flag is made up of fabric
A bottle is made up of glass
A roof is made up of shingles
A cloud is made up of vapor
A concrete is made up of
2024-07-25 11:50:16 root INFO     total operator prediction time: 1372.2842564582825 seconds
2024-07-25 11:50:16 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on synonyms - intensity
2024-07-25 11:50:16 root INFO     building operator synonyms - intensity
2024-07-25 11:50:17 root INFO     [order_1_approx] starting weight calculation for A more intense word for like is love
A more intense word for dinner is feast
A more intense word for creative is ingenious
A more intense word for boring is tedious
A more intense word for doze is sleep
A more intense word for well is flourishing
A more intense word for faith is fanatism
A more intense word for sad is
2024-07-25 11:50:17 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 11:53:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.5625, -0.2031,  2.9531,  ...,  0.8125, -0.0039,  0.4844],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -7.9375,  -4.0625,   7.8438,  ...,   2.5625,   3.2656, -21.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.7109e-02,  3.0518e-03,  4.9438e-03,  ..., -5.1498e-05,
          5.8899e-03, -4.2419e-03],
        [ 1.0986e-03,  1.8677e-02,  1.6174e-03,  ..., -3.6163e-03,
         -1.2131e-03,  3.2043e-03],
        [ 1.9989e-03, -4.0894e-03,  1.0938e-01,  ..., -6.0425e-03,
         -1.1444e-03, -1.1749e-03],
        ...,
        [ 3.9368e-03, -2.2888e-03, -8.0566e-03,  ...,  2.7222e-02,
         -4.0283e-03, -7.3242e-03],
        [ 3.6163e-03,  8.9111e-03, -7.3853e-03,  ..., -6.3324e-04,
          4.7852e-02,  3.5095e-03],
        [-2.6550e-03, -5.1880e-03, -1.9775e-02,  ..., -5.6076e-04,
         -9.6436e-03,  3.8818e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -8.6484,  -3.7500,   6.8711,  ...,   2.8516,   3.0098, -21.6094]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 11:53:07 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for like is love
A more intense word for dinner is feast
A more intense word for creative is ingenious
A more intense word for boring is tedious
A more intense word for doze is sleep
A more intense word for well is flourishing
A more intense word for faith is fanatism
A more intense word for sad is
2024-07-25 11:53:07 root INFO     [order_1_approx] starting weight calculation for A more intense word for boring is tedious
A more intense word for sad is desparate
A more intense word for well is flourishing
A more intense word for creative is ingenious
A more intense word for faith is fanatism
A more intense word for dinner is feast
A more intense word for doze is sleep
A more intense word for like is
2024-07-25 11:53:07 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 11:55:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9609, -0.5000, -1.1094,  ...,  1.7031,  1.2344, -0.9688],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  2.9531,  -2.3438,  14.6250,  ...,  -3.8906,   2.1875, -15.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 5.2246e-02,  3.1433e-03,  2.2949e-02,  ..., -3.5553e-03,
         -1.0681e-04,  2.6703e-03],
        [ 1.1475e-02,  2.7710e-02,  2.6245e-02,  ..., -3.2043e-03,
          5.5695e-04,  1.1597e-02],
        [ 1.5991e-02, -1.3672e-02,  1.3867e-01,  ..., -1.8768e-03,
         -3.7384e-03,  4.8828e-04],
        ...,
        [-3.8452e-03, -8.6670e-03, -2.1851e-02,  ...,  3.1982e-02,
         -1.1475e-02, -1.4877e-03],
        [-7.9346e-03,  2.5482e-03,  7.2632e-03,  ...,  2.9907e-03,
          6.2012e-02,  4.9438e-03],
        [ 2.9144e-03, -6.4087e-03, -1.7212e-02,  ..., -5.1575e-03,
          4.5204e-04,  4.7607e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  2.0547,  -1.9512,  15.2422,  ...,  -4.8086,   2.9844, -16.4219]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 11:56:00 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for boring is tedious
A more intense word for sad is desparate
A more intense word for well is flourishing
A more intense word for creative is ingenious
A more intense word for faith is fanatism
A more intense word for dinner is feast
A more intense word for doze is sleep
A more intense word for like is
2024-07-25 11:56:00 root INFO     [order_1_approx] starting weight calculation for A more intense word for doze is sleep
A more intense word for dinner is feast
A more intense word for sad is desparate
A more intense word for boring is tedious
A more intense word for creative is ingenious
A more intense word for like is love
A more intense word for well is flourishing
A more intense word for faith is
2024-07-25 11:56:01 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 11:58:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.8516,  0.2188,  1.3672,  ..., -1.2031, -0.0332, -1.1406],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  0.8125,  -1.0625,  -8.1875,  ...,   3.4375,   4.8125, -11.3125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.5645e-02,  1.9836e-04,  1.9043e-02,  ..., -1.2283e-03,
          1.3428e-03, -7.2632e-03],
        [ 2.3956e-03,  2.0508e-02,  1.3672e-02,  ..., -6.3782e-03,
         -1.0010e-02, -3.5553e-03],
        [ 1.7242e-03,  1.3275e-03,  1.2598e-01,  ..., -6.6223e-03,
          1.4801e-03, -6.5231e-04],
        ...,
        [-9.2163e-03, -6.7139e-03, -2.8320e-02,  ...,  3.1738e-02,
         -2.4414e-04,  2.1458e-05],
        [ 5.1880e-03, -9.6893e-04,  9.2773e-03,  ...,  5.4321e-03,
          4.6387e-02,  7.8735e-03],
        [ 8.7280e-03, -4.7913e-03,  5.2185e-03,  ..., -6.4850e-04,
          1.2741e-03,  4.1992e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  2.9980,  -2.2188,  -7.5273,  ...,   2.8125,   3.8125, -10.2031]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 11:58:52 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for doze is sleep
A more intense word for dinner is feast
A more intense word for sad is desparate
A more intense word for boring is tedious
A more intense word for creative is ingenious
A more intense word for like is love
A more intense word for well is flourishing
A more intense word for faith is
2024-07-25 11:58:52 root INFO     [order_1_approx] starting weight calculation for A more intense word for boring is tedious
A more intense word for sad is desparate
A more intense word for creative is ingenious
A more intense word for like is love
A more intense word for doze is sleep
A more intense word for well is flourishing
A more intense word for faith is fanatism
A more intense word for dinner is
2024-07-25 11:58:52 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 12:01:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1562, -1.6406,  2.6719,  ...,  2.0312,  0.7812, -0.1523],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 2.9219,  2.8125, -4.0938,  ...,  3.9375, 12.1250, -8.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.8311e-02, -1.1749e-03,  6.9885e-03,  ...,  4.3030e-03,
          3.8910e-03, -3.7384e-03],
        [ 1.4801e-03,  2.1973e-02,  2.7161e-03,  ..., -4.2419e-03,
         -5.2185e-03,  9.1553e-03],
        [ 1.5945e-03,  5.1270e-03,  8.3984e-02,  ..., -3.6316e-03,
         -1.5320e-02,  4.5776e-03],
        ...,
        [-6.7139e-03, -2.8992e-03,  3.5858e-03,  ...,  2.0996e-02,
         -4.6997e-03, -4.5776e-03],
        [ 9.2773e-03,  3.5095e-04,  1.5381e-02,  ...,  2.2888e-05,
          4.1748e-02,  9.6436e-03],
        [ 1.1139e-03,  3.5706e-03, -1.6357e-02,  ..., -2.6703e-03,
          7.3242e-03,  2.3071e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4551,  3.8438, -4.2383,  ...,  3.1836, 13.3750, -7.7969]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 12:01:44 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for boring is tedious
A more intense word for sad is desparate
A more intense word for creative is ingenious
A more intense word for like is love
A more intense word for doze is sleep
A more intense word for well is flourishing
A more intense word for faith is fanatism
A more intense word for dinner is
2024-07-25 12:01:44 root INFO     [order_1_approx] starting weight calculation for A more intense word for creative is ingenious
A more intense word for like is love
A more intense word for sad is desparate
A more intense word for faith is fanatism
A more intense word for doze is sleep
A more intense word for dinner is feast
A more intense word for boring is tedious
A more intense word for well is
2024-07-25 12:01:45 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 12:04:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9375,  2.4062, -3.3750,  ..., -0.8594,  0.2539, -0.4062],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 5.9062,  2.3125,  5.0938,  ...,  2.0469, -0.8672,  1.4297],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 6.3965e-02, -3.0518e-05,  8.6060e-03,  ..., -2.1362e-03,
          1.1841e-02, -8.7891e-03],
        [-8.9111e-03,  2.6611e-02,  1.2207e-04,  ..., -3.2959e-03,
          1.0803e-02,  2.1606e-02],
        [ 1.2024e-02, -4.2419e-03,  1.8945e-01,  ..., -6.0120e-03,
          2.1667e-03, -9.8267e-03],
        ...,
        [-3.1891e-03, -3.7537e-03, -2.1729e-02,  ...,  3.6865e-02,
          2.3499e-03, -1.5106e-03],
        [ 1.5564e-02,  2.7100e-02,  6.1340e-03,  ..., -5.3406e-05,
          8.8867e-02, -1.1841e-02],
        [ 1.6968e-02,  4.0283e-03,  1.9836e-03,  ...,  1.6632e-03,
          1.2085e-02,  5.0049e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.3047,  0.9365,  4.3320,  ...,  0.5566, -1.2148, -1.1191]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 12:04:35 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for creative is ingenious
A more intense word for like is love
A more intense word for sad is desparate
A more intense word for faith is fanatism
A more intense word for doze is sleep
A more intense word for dinner is feast
A more intense word for boring is tedious
A more intense word for well is
2024-07-25 12:04:35 root INFO     [order_1_approx] starting weight calculation for A more intense word for faith is fanatism
A more intense word for doze is sleep
A more intense word for sad is desparate
A more intense word for creative is ingenious
A more intense word for like is love
A more intense word for well is flourishing
A more intense word for dinner is feast
A more intense word for boring is
2024-07-25 12:04:35 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 12:07:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5156, -3.2500,  0.1133,  ..., -1.8516,  1.3594,  2.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 2.2188, -1.2891,  6.8750,  ..., -0.5625,  1.5938, -1.5938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0337,  0.0012, -0.0043,  ..., -0.0018,  0.0090,  0.0081],
        [-0.0040,  0.0157, -0.0040,  ..., -0.0024,  0.0015,  0.0040],
        [ 0.0007, -0.0039,  0.1025,  ..., -0.0042, -0.0068, -0.0025],
        ...,
        [-0.0004, -0.0067, -0.0198,  ...,  0.0249, -0.0052, -0.0084],
        [ 0.0028,  0.0063, -0.0045,  ...,  0.0059,  0.0518,  0.0068],
        [ 0.0040, -0.0067,  0.0017,  ..., -0.0072,  0.0080,  0.0508]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.2754, -1.9590,  6.2656,  ..., -1.2656,  0.6548, -0.3154]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 12:07:25 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for faith is fanatism
A more intense word for doze is sleep
A more intense word for sad is desparate
A more intense word for creative is ingenious
A more intense word for like is love
A more intense word for well is flourishing
A more intense word for dinner is feast
A more intense word for boring is
2024-07-25 12:07:25 root INFO     [order_1_approx] starting weight calculation for A more intense word for faith is fanatism
A more intense word for boring is tedious
A more intense word for sad is desparate
A more intense word for like is love
A more intense word for dinner is feast
A more intense word for doze is sleep
A more intense word for well is flourishing
A more intense word for creative is
2024-07-25 12:07:25 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 12:10:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.5000, -1.7578,  1.1406,  ...,  1.0703, -1.8672, -0.8477],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  3.3594,  -5.6562,   2.9375,  ...,  14.8125,  -2.2500, -13.9375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 4.1260e-02,  3.0365e-03, -4.3945e-03,  ...,  1.3046e-03,
         -3.7842e-03, -3.7079e-03],
        [ 2.3193e-03,  1.7456e-02,  1.1414e-02,  ..., -3.8605e-03,
         -5.7373e-03,  1.6846e-02],
        [ 1.1597e-03, -4.5776e-03,  1.2012e-01,  ..., -8.4839e-03,
          2.2888e-05, -3.0518e-03],
        ...,
        [ 2.0294e-03,  2.4109e-03, -1.3550e-02,  ...,  3.0762e-02,
         -5.5237e-03, -9.5215e-03],
        [ 9.3079e-04,  1.2360e-03, -3.7842e-03,  ..., -1.8921e-03,
          5.7617e-02, -1.0010e-02],
        [ 1.5869e-03, -7.9956e-03, -4.0894e-03,  ..., -5.8899e-03,
          1.0681e-04,  4.9805e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  2.4336,  -5.9805,   1.9102,  ...,  15.3828,  -1.7012, -13.9453]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 12:10:15 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for faith is fanatism
A more intense word for boring is tedious
A more intense word for sad is desparate
A more intense word for like is love
A more intense word for dinner is feast
A more intense word for doze is sleep
A more intense word for well is flourishing
A more intense word for creative is
2024-07-25 12:10:16 root INFO     [order_1_approx] starting weight calculation for A more intense word for creative is ingenious
A more intense word for dinner is feast
A more intense word for like is love
A more intense word for well is flourishing
A more intense word for boring is tedious
A more intense word for sad is desparate
A more intense word for faith is fanatism
A more intense word for doze is
2024-07-25 12:10:16 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 12:13:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1289, -0.6055,  1.0938,  ..., -0.9102,  0.2109,  1.9062],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-5.4375,  1.9141, 17.1250,  ..., -8.3750,  1.2266, -5.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0305,  0.0077,  0.0091,  ..., -0.0120,  0.0112,  0.0049],
        [ 0.0063,  0.0234, -0.0007,  ..., -0.0107, -0.0006,  0.0003],
        [ 0.0048,  0.0024,  0.1113,  ..., -0.0166, -0.0028, -0.0003],
        ...,
        [-0.0137, -0.0068, -0.0037,  ...,  0.0388, -0.0069,  0.0060],
        [-0.0073,  0.0020, -0.0118,  ...,  0.0069,  0.0508, -0.0073],
        [ 0.0009, -0.0125, -0.0162,  ..., -0.0012,  0.0036,  0.0566]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-6.3047,  2.3633, 14.4766,  ..., -8.1641,  1.8086, -5.9609]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 12:13:07 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for creative is ingenious
A more intense word for dinner is feast
A more intense word for like is love
A more intense word for well is flourishing
A more intense word for boring is tedious
A more intense word for sad is desparate
A more intense word for faith is fanatism
A more intense word for doze is
2024-07-25 12:13:07 root INFO     total operator prediction time: 1370.6835808753967 seconds
2024-07-25 12:13:07 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on hypernyms - animals
2024-07-25 12:13:07 root INFO     building operator hypernyms - animals
2024-07-25 12:13:07 root INFO     [order_1_approx] starting weight calculation for The triceratops falls into the category of dinosaur
The turkey falls into the category of fowl
The chinchilla falls into the category of rodent
The owl falls into the category of raptor
The human falls into the category of primate
The tyrannosaurus falls into the category of dinosaur
The rattlesnake falls into the category of snake
The ant falls into the category of
2024-07-25 12:13:08 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 12:15:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.2969, -0.8750,  2.9688,  ...,  0.9688, -2.8750, -5.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-10.3750,   3.0625,  -6.0000,  ..., -11.3750,   5.3125,  -6.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0186,  0.0005, -0.0033,  ...,  0.0032,  0.0002,  0.0150],
        [ 0.0064,  0.0205, -0.0131,  ..., -0.0042,  0.0098, -0.0042],
        [-0.0035, -0.0016,  0.1152,  ..., -0.0062,  0.0019,  0.0012],
        ...,
        [ 0.0117, -0.0028,  0.0079,  ...,  0.0151,  0.0121,  0.0018],
        [-0.0070, -0.0003,  0.0108,  ..., -0.0100,  0.0557, -0.0116],
        [ 0.0025,  0.0013,  0.0006,  ...,  0.0021, -0.0109,  0.0388]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-10.8047,   4.6641,  -7.7305,  ..., -11.3359,   4.4102,  -5.9531]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 12:15:55 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The triceratops falls into the category of dinosaur
The turkey falls into the category of fowl
The chinchilla falls into the category of rodent
The owl falls into the category of raptor
The human falls into the category of primate
The tyrannosaurus falls into the category of dinosaur
The rattlesnake falls into the category of snake
The ant falls into the category of
2024-07-25 12:15:56 root INFO     [order_1_approx] starting weight calculation for The turkey falls into the category of fowl
The ant falls into the category of insect
The chinchilla falls into the category of rodent
The owl falls into the category of raptor
The rattlesnake falls into the category of snake
The human falls into the category of primate
The tyrannosaurus falls into the category of dinosaur
The triceratops falls into the category of
2024-07-25 12:15:56 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 12:18:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4688, -1.2969, -0.9688,  ..., -1.7578, -2.7188, -0.2910],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-24.7500,  11.6250,   5.4688,  ...,  -6.8125,   4.8438,   9.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0405,  0.0081, -0.0095,  ...,  0.0157,  0.0048,  0.0173],
        [ 0.0026,  0.0381,  0.0024,  ..., -0.0043, -0.0005, -0.0071],
        [-0.0034, -0.0079,  0.1719,  ..., -0.0096,  0.0070, -0.0015],
        ...,
        [ 0.0099, -0.0071,  0.0058,  ...,  0.0311,  0.0034,  0.0148],
        [-0.0090,  0.0072, -0.0033,  ..., -0.0022,  0.0708,  0.0048],
        [ 0.0042, -0.0085, -0.0161,  ...,  0.0029, -0.0089,  0.0654]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-25.2344,  12.7891,   5.7852,  ...,  -5.7188,   7.3008,   9.7812]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 12:18:45 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The turkey falls into the category of fowl
The ant falls into the category of insect
The chinchilla falls into the category of rodent
The owl falls into the category of raptor
The rattlesnake falls into the category of snake
The human falls into the category of primate
The tyrannosaurus falls into the category of dinosaur
The triceratops falls into the category of
2024-07-25 12:18:45 root INFO     [order_1_approx] starting weight calculation for The human falls into the category of primate
The triceratops falls into the category of dinosaur
The rattlesnake falls into the category of snake
The chinchilla falls into the category of rodent
The tyrannosaurus falls into the category of dinosaur
The turkey falls into the category of fowl
The ant falls into the category of insect
The owl falls into the category of
2024-07-25 12:18:45 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 12:21:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.0781,  0.2812,  1.5547,  ...,  0.0078, -3.2656, -2.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-8.6250, -0.4844, -9.6250,  ..., -7.8125,  4.7812, -7.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0277, -0.0061,  0.0143,  ...,  0.0127, -0.0060,  0.0125],
        [ 0.0056,  0.0242, -0.0048,  ..., -0.0033,  0.0038, -0.0079],
        [ 0.0002, -0.0019,  0.1025,  ..., -0.0032, -0.0012,  0.0030],
        ...,
        [ 0.0000, -0.0101,  0.0095,  ...,  0.0255, -0.0114,  0.0140],
        [-0.0032,  0.0017,  0.0116,  ..., -0.0067,  0.0474, -0.0107],
        [ 0.0041, -0.0074,  0.0096,  ...,  0.0069, -0.0018,  0.0347]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-7.4297e+00, -8.5449e-03, -1.0023e+01,  ..., -8.1875e+00,
          5.2109e+00, -8.6562e+00]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SubBackward0>) 

                    
2024-07-25 12:21:35 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The human falls into the category of primate
The triceratops falls into the category of dinosaur
The rattlesnake falls into the category of snake
The chinchilla falls into the category of rodent
The tyrannosaurus falls into the category of dinosaur
The turkey falls into the category of fowl
The ant falls into the category of insect
The owl falls into the category of
2024-07-25 12:21:35 root INFO     [order_1_approx] starting weight calculation for The chinchilla falls into the category of rodent
The tyrannosaurus falls into the category of dinosaur
The owl falls into the category of raptor
The turkey falls into the category of fowl
The triceratops falls into the category of dinosaur
The ant falls into the category of insect
The human falls into the category of primate
The rattlesnake falls into the category of
2024-07-25 12:21:35 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 12:24:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 3.2500, -0.3906,  2.0938,  ..., -2.7812, -0.8398, -0.5195],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -0.0625,   1.7969,   3.2812,  ..., -12.6250,  18.8750,   6.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.9775e-02, -4.4250e-03,  2.5024e-02,  ...,  5.4321e-03,
          1.7166e-03,  7.4768e-03],
        [ 3.3569e-03,  2.0508e-02,  3.9062e-03,  ..., -8.0872e-04,
          5.3406e-05, -2.1057e-03],
        [-2.5024e-03, -8.9264e-04,  7.8125e-02,  ..., -5.9814e-03,
          6.3477e-03, -6.3324e-04],
        ...,
        [-2.4986e-04,  6.1035e-05,  2.2705e-02,  ...,  1.9043e-02,
          2.6550e-03,  8.6670e-03],
        [-7.9956e-03,  1.8005e-03,  4.0894e-03,  ...,  1.6785e-04,
          4.2969e-02, -2.3346e-03],
        [ 2.4719e-03, -2.8687e-03, -3.3722e-03,  ...,  2.2888e-03,
         -4.4556e-03,  3.1494e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  0.4434,   2.0293,   3.2227,  ..., -12.1094,  19.0938,   5.1484]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 12:24:26 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The chinchilla falls into the category of rodent
The tyrannosaurus falls into the category of dinosaur
The owl falls into the category of raptor
The turkey falls into the category of fowl
The triceratops falls into the category of dinosaur
The ant falls into the category of insect
The human falls into the category of primate
The rattlesnake falls into the category of
2024-07-25 12:24:26 root INFO     [order_1_approx] starting weight calculation for The triceratops falls into the category of dinosaur
The human falls into the category of primate
The turkey falls into the category of fowl
The chinchilla falls into the category of rodent
The owl falls into the category of raptor
The rattlesnake falls into the category of snake
The ant falls into the category of insect
The tyrannosaurus falls into the category of
2024-07-25 12:24:26 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 12:27:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5781,  0.2578, -1.1250,  ...,  0.1250, -1.1953,  1.3594],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-9.9375, 12.7500, -4.5625,  ..., -2.4219, 15.1250,  2.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0227,  0.0056,  0.0161,  ..., -0.0013, -0.0027,  0.0020],
        [ 0.0005,  0.0212, -0.0072,  ..., -0.0011, -0.0022, -0.0056],
        [-0.0013, -0.0090,  0.1113,  ..., -0.0068,  0.0070,  0.0023],
        ...,
        [ 0.0009, -0.0064,  0.0146,  ...,  0.0215, -0.0067,  0.0120],
        [-0.0104,  0.0090, -0.0039,  ...,  0.0009,  0.0459, -0.0057],
        [ 0.0048, -0.0052, -0.0135,  ...,  0.0053, -0.0069,  0.0430]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-10.6719,  13.1953,  -4.1445,  ...,  -2.1074,  15.9688,   2.8223]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 12:27:15 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The triceratops falls into the category of dinosaur
The human falls into the category of primate
The turkey falls into the category of fowl
The chinchilla falls into the category of rodent
The owl falls into the category of raptor
The rattlesnake falls into the category of snake
The ant falls into the category of insect
The tyrannosaurus falls into the category of
2024-07-25 12:27:15 root INFO     [order_1_approx] starting weight calculation for The owl falls into the category of raptor
The triceratops falls into the category of dinosaur
The chinchilla falls into the category of rodent
The ant falls into the category of insect
The human falls into the category of primate
The rattlesnake falls into the category of snake
The tyrannosaurus falls into the category of dinosaur
The turkey falls into the category of
2024-07-25 12:27:15 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 12:30:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7852,  0.8906,  3.4844,  ...,  0.6172, -3.7812, -1.9375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -9.6875,  -1.2031,  -4.4375,  ..., -11.1250,  13.7500,   6.5312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0312, -0.0065,  0.0058,  ...,  0.0044, -0.0056,  0.0085],
        [ 0.0099,  0.0221,  0.0031,  ..., -0.0013,  0.0062, -0.0004],
        [ 0.0018, -0.0014,  0.1089,  ..., -0.0071,  0.0172,  0.0021],
        ...,
        [ 0.0055, -0.0030, -0.0030,  ...,  0.0190,  0.0037,  0.0070],
        [-0.0146,  0.0077,  0.0187,  ..., -0.0017,  0.0417, -0.0037],
        [ 0.0033, -0.0054,  0.0143,  ...,  0.0079,  0.0039,  0.0386]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -9.0078,  -1.1094,  -4.9102,  ..., -10.6250,  14.1016,   6.2383]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 12:30:05 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The owl falls into the category of raptor
The triceratops falls into the category of dinosaur
The chinchilla falls into the category of rodent
The ant falls into the category of insect
The human falls into the category of primate
The rattlesnake falls into the category of snake
The tyrannosaurus falls into the category of dinosaur
The turkey falls into the category of
2024-07-25 12:30:05 root INFO     [order_1_approx] starting weight calculation for The triceratops falls into the category of dinosaur
The ant falls into the category of insect
The tyrannosaurus falls into the category of dinosaur
The turkey falls into the category of fowl
The human falls into the category of primate
The rattlesnake falls into the category of snake
The owl falls into the category of raptor
The chinchilla falls into the category of
2024-07-25 12:30:06 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 12:32:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8711, -0.7891,  0.9258,  ..., -1.0625, -3.4531, -0.4316],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  3.0156,   5.5625,   8.7500,  ...,  -7.3125,  10.8750, -13.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.7842e-02, -3.9062e-03,  3.9673e-03,  ...,  1.2207e-02,
         -1.6479e-03,  3.2227e-02],
        [ 1.1414e-02,  4.2969e-02, -3.2227e-02,  ..., -9.2163e-03,
          2.1240e-02,  2.8229e-03],
        [-7.3853e-03, -1.4648e-02,  1.9238e-01,  ..., -6.5918e-03,
          2.4719e-03,  6.5002e-03],
        ...,
        [ 8.0566e-03,  7.6294e-05, -5.8594e-03,  ...,  4.4189e-02,
         -2.5269e-02,  2.2217e-02],
        [-8.3008e-03, -2.6703e-04,  1.2756e-02,  ..., -1.3489e-02,
          1.0645e-01, -6.3171e-03],
        [-1.4420e-03, -9.8267e-03,  2.5635e-03,  ...,  5.9814e-03,
         -1.7822e-02,  7.6172e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  3.8965,   5.6250,   9.8516,  ...,  -7.6914,  11.0000, -15.2500]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 12:32:55 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The triceratops falls into the category of dinosaur
The ant falls into the category of insect
The tyrannosaurus falls into the category of dinosaur
The turkey falls into the category of fowl
The human falls into the category of primate
The rattlesnake falls into the category of snake
The owl falls into the category of raptor
The chinchilla falls into the category of
2024-07-25 12:32:55 root INFO     [order_1_approx] starting weight calculation for The ant falls into the category of insect
The rattlesnake falls into the category of snake
The triceratops falls into the category of dinosaur
The turkey falls into the category of fowl
The owl falls into the category of raptor
The tyrannosaurus falls into the category of dinosaur
The chinchilla falls into the category of rodent
The human falls into the category of
2024-07-25 12:32:55 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 12:35:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.9219, -3.0625,  6.2812,  ..., -1.1562, -0.1709,  1.3516],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-0.8398,  1.0000,  8.2500,  ...,  0.4844,  9.6250, -3.7031],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0228, -0.0032,  0.0067,  ...,  0.0043, -0.0090,  0.0049],
        [ 0.0042,  0.0233, -0.0051,  ...,  0.0009,  0.0013, -0.0048],
        [-0.0054, -0.0043,  0.0952,  ..., -0.0015, -0.0023, -0.0018],
        ...,
        [ 0.0038, -0.0010,  0.0047,  ...,  0.0237, -0.0089,  0.0059],
        [-0.0038,  0.0038,  0.0058,  ..., -0.0002,  0.0461, -0.0026],
        [ 0.0021, -0.0023,  0.0095,  ..., -0.0011, -0.0072,  0.0339]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8486,  1.5059,  7.6328,  ...,  0.5288,  9.3594, -3.9531]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 12:35:43 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The ant falls into the category of insect
The rattlesnake falls into the category of snake
The triceratops falls into the category of dinosaur
The turkey falls into the category of fowl
The owl falls into the category of raptor
The tyrannosaurus falls into the category of dinosaur
The chinchilla falls into the category of rodent
The human falls into the category of
2024-07-25 12:35:43 root INFO     total operator prediction time: 1355.8820595741272 seconds
2024-07-25 12:35:43 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on hyponyms - misc
2024-07-25 12:35:43 root INFO     building operator hyponyms - misc
2024-07-25 12:35:43 root INFO     [order_1_approx] starting weight calculation for A more specific term for a jewelry is bracelet
A more specific term for a cup is teacup
A more specific term for a boat is ferry
A more specific term for a poem is haiku
A more specific term for a tool is rake
A more specific term for a weekday is monday
A more specific term for a guitar is ukulele
A more specific term for a computer is
2024-07-25 12:35:44 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 12:38:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0625, -0.5898,  3.9375,  ..., -0.9297, -3.9844, -3.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-15.6875,  14.6250,  19.3750,  ...,   4.1875,  -6.1562,   1.7656],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0272, -0.0018, -0.0168,  ..., -0.0036,  0.0010, -0.0070],
        [-0.0034,  0.0162,  0.0035,  ..., -0.0021,  0.0050, -0.0012],
        [-0.0034,  0.0033,  0.0957,  ..., -0.0062, -0.0008, -0.0011],
        ...,
        [-0.0059,  0.0009, -0.0102,  ...,  0.0159,  0.0067,  0.0037],
        [-0.0021, -0.0054,  0.0046,  ..., -0.0054,  0.0493, -0.0154],
        [-0.0059, -0.0077, -0.0022,  ...,  0.0022,  0.0039,  0.0371]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-15.3125,  13.1875,  18.8906,  ...,   4.3086,  -4.8711,   1.8320]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 12:38:30 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a jewelry is bracelet
A more specific term for a cup is teacup
A more specific term for a boat is ferry
A more specific term for a poem is haiku
A more specific term for a tool is rake
A more specific term for a weekday is monday
A more specific term for a guitar is ukulele
A more specific term for a computer is
2024-07-25 12:38:31 root INFO     [order_1_approx] starting weight calculation for A more specific term for a boat is ferry
A more specific term for a weekday is monday
A more specific term for a computer is laptop
A more specific term for a jewelry is bracelet
A more specific term for a tool is rake
A more specific term for a guitar is ukulele
A more specific term for a cup is teacup
A more specific term for a poem is
2024-07-25 12:38:31 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 12:41:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1172, -0.9766, -0.6641,  ...,  0.1934, -4.3750, -1.4141],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-11.5000,   2.1719,  23.2500,  ...,   0.6406, -19.7500,  -7.6562],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0242,  0.0004, -0.0067,  ..., -0.0018,  0.0172, -0.0030],
        [ 0.0090,  0.0089, -0.0110,  ...,  0.0024, -0.0062,  0.0016],
        [ 0.0019, -0.0023,  0.0957,  ..., -0.0053, -0.0007, -0.0018],
        ...,
        [-0.0003,  0.0079, -0.0215,  ...,  0.0179,  0.0201,  0.0041],
        [-0.0024,  0.0021,  0.0173,  ..., -0.0076,  0.0396, -0.0006],
        [ 0.0025, -0.0014,  0.0045,  ...,  0.0056, -0.0103,  0.0366]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -9.7500,   1.3691,  22.2812,  ...,   1.2734, -16.8125,  -5.7578]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 12:41:19 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a boat is ferry
A more specific term for a weekday is monday
A more specific term for a computer is laptop
A more specific term for a jewelry is bracelet
A more specific term for a tool is rake
A more specific term for a guitar is ukulele
A more specific term for a cup is teacup
A more specific term for a poem is
2024-07-25 12:41:19 root INFO     [order_1_approx] starting weight calculation for A more specific term for a jewelry is bracelet
A more specific term for a tool is rake
A more specific term for a computer is laptop
A more specific term for a boat is ferry
A more specific term for a poem is haiku
A more specific term for a cup is teacup
A more specific term for a weekday is monday
A more specific term for a guitar is
2024-07-25 12:41:20 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 12:44:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.5000, -2.6094, -3.3906,  ...,  0.6055, -4.5625,  1.5078],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([11.6250, 12.1250, 14.0000,  ..., 12.5625,  0.7500,  7.2812],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0193,  0.0065, -0.0096,  ..., -0.0045,  0.0095,  0.0082],
        [ 0.0039,  0.0181, -0.0186,  ..., -0.0067, -0.0027,  0.0004],
        [ 0.0048, -0.0045,  0.0879,  ..., -0.0016,  0.0020,  0.0023],
        ...,
        [-0.0024,  0.0075, -0.0132,  ...,  0.0250,  0.0069,  0.0020],
        [-0.0036,  0.0088, -0.0022,  ..., -0.0083,  0.0464,  0.0076],
        [ 0.0101, -0.0045,  0.0013,  ..., -0.0051,  0.0007,  0.0264]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[11.7031, 13.0312, 14.8438,  ..., 13.7969,  1.5840,  7.6797]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 12:44:08 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a jewelry is bracelet
A more specific term for a tool is rake
A more specific term for a computer is laptop
A more specific term for a boat is ferry
A more specific term for a poem is haiku
A more specific term for a cup is teacup
A more specific term for a weekday is monday
A more specific term for a guitar is
2024-07-25 12:44:09 root INFO     [order_1_approx] starting weight calculation for A more specific term for a poem is haiku
A more specific term for a cup is teacup
A more specific term for a guitar is ukulele
A more specific term for a tool is rake
A more specific term for a weekday is monday
A more specific term for a boat is ferry
A more specific term for a computer is laptop
A more specific term for a jewelry is
2024-07-25 12:44:09 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 12:46:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5547,  2.3125, -2.1250,  ..., -1.1797, -2.8594, -6.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-13.8125,   8.1875,   1.4375,  ...,  14.3750,   8.5625,   8.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.3691e-02,  8.3923e-05,  1.3794e-02,  ...,  5.6152e-03,
         -6.5613e-04, -1.7548e-03],
        [-5.3101e-03,  1.2939e-02, -1.5869e-02,  ..., -2.7466e-03,
          1.7776e-03,  6.8054e-03],
        [ 5.0354e-03,  0.0000e+00,  9.4727e-02,  ..., -1.6937e-03,
         -1.6327e-03, -4.0588e-03],
        ...,
        [ 3.7842e-03,  9.3079e-04,  1.5106e-03,  ...,  2.2095e-02,
         -3.8757e-03, -2.7618e-03],
        [ 7.6294e-05,  8.2397e-03,  7.7820e-03,  ..., -2.8534e-03,
          3.9551e-02,  3.5095e-03],
        [ 8.4839e-03, -3.7231e-03,  3.9673e-04,  ...,  2.7008e-03,
         -7.4158e-03,  3.5645e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-13.9688,   7.8398,   1.5977,  ...,  15.5703,   7.7656,   8.5859]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 12:46:56 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a poem is haiku
A more specific term for a cup is teacup
A more specific term for a guitar is ukulele
A more specific term for a tool is rake
A more specific term for a weekday is monday
A more specific term for a boat is ferry
A more specific term for a computer is laptop
A more specific term for a jewelry is
2024-07-25 12:46:56 root INFO     [order_1_approx] starting weight calculation for A more specific term for a cup is teacup
A more specific term for a weekday is monday
A more specific term for a poem is haiku
A more specific term for a computer is laptop
A more specific term for a guitar is ukulele
A more specific term for a jewelry is bracelet
A more specific term for a boat is ferry
A more specific term for a tool is
2024-07-25 12:46:56 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 12:49:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9141, -0.1641,  1.7031,  ...,  0.2188, -5.3125, -1.1953],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-14.1875,   4.6250,   3.6562,  ...,   4.4375,  -6.3750,  12.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.3438e-02,  4.6692e-03,  4.8218e-03,  ..., -6.0120e-03,
         -3.2043e-03, -1.1353e-02],
        [-1.1139e-03,  1.2817e-02, -6.3477e-03,  ...,  3.0518e-05,
          1.1108e-02, -5.9891e-04],
        [-7.6294e-03, -5.9204e-03,  1.0791e-01,  ..., -7.0190e-03,
          4.1809e-03,  6.4087e-03],
        ...,
        [ 5.3406e-04,  7.2021e-03, -1.3977e-02,  ...,  1.4099e-02,
          1.1536e-02, -1.1215e-03],
        [ 1.0986e-02,  5.9814e-03, -1.4526e-02,  ...,  6.7139e-04,
          4.0283e-02, -1.3123e-02],
        [ 7.6294e-05,  2.0294e-03,  9.7046e-03,  ..., -4.1504e-03,
         -8.5449e-03,  3.1982e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-13.8516,   5.1875,   4.0273,  ...,   4.8516,  -5.8047,  12.2578]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 12:49:44 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a cup is teacup
A more specific term for a weekday is monday
A more specific term for a poem is haiku
A more specific term for a computer is laptop
A more specific term for a guitar is ukulele
A more specific term for a jewelry is bracelet
A more specific term for a boat is ferry
A more specific term for a tool is
2024-07-25 12:49:44 root INFO     [order_1_approx] starting weight calculation for A more specific term for a jewelry is bracelet
A more specific term for a tool is rake
A more specific term for a poem is haiku
A more specific term for a boat is ferry
A more specific term for a guitar is ukulele
A more specific term for a weekday is monday
A more specific term for a computer is laptop
A more specific term for a cup is
2024-07-25 12:49:44 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 12:52:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4297,  0.2324,  0.9805,  ...,  0.9023, -3.6250, -2.4844],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-4.0000,  7.7812, -1.0469,  ..., 11.5625, -2.5000,  4.6562],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0281,  0.0042,  0.0115,  ..., -0.0068,  0.0026, -0.0007],
        [ 0.0078,  0.0171,  0.0044,  ..., -0.0004, -0.0095,  0.0027],
        [ 0.0074, -0.0014,  0.1064,  ..., -0.0011, -0.0076, -0.0046],
        ...,
        [ 0.0027,  0.0063,  0.0025,  ...,  0.0199,  0.0013,  0.0035],
        [ 0.0013,  0.0003,  0.0009,  ..., -0.0016,  0.0403, -0.0075],
        [-0.0031,  0.0079, -0.0079,  ...,  0.0059, -0.0076,  0.0386]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.8281,  9.2266, -2.4141,  ..., 13.1484, -1.7891,  4.2266]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 12:52:33 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a jewelry is bracelet
A more specific term for a tool is rake
A more specific term for a poem is haiku
A more specific term for a boat is ferry
A more specific term for a guitar is ukulele
A more specific term for a weekday is monday
A more specific term for a computer is laptop
A more specific term for a cup is
2024-07-25 12:52:34 root INFO     [order_1_approx] starting weight calculation for A more specific term for a poem is haiku
A more specific term for a cup is teacup
A more specific term for a jewelry is bracelet
A more specific term for a tool is rake
A more specific term for a computer is laptop
A more specific term for a guitar is ukulele
A more specific term for a boat is ferry
A more specific term for a weekday is
2024-07-25 12:52:34 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 12:55:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 3.1875,  0.2266,  7.8125,  ..., -0.2500, -4.4688, -0.8984],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-15.5000,  -5.2500,   6.5625,  ...,  -7.3438,   0.7812,   1.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0342, -0.0029,  0.0028,  ..., -0.0059,  0.0062, -0.0038],
        [ 0.0010,  0.0115, -0.0043,  ..., -0.0009,  0.0060, -0.0023],
        [-0.0031, -0.0011,  0.1016,  ..., -0.0031, -0.0007, -0.0016],
        ...,
        [-0.0019,  0.0005, -0.0189,  ...,  0.0228, -0.0002,  0.0018],
        [-0.0041, -0.0013,  0.0012,  ..., -0.0042,  0.0552, -0.0040],
        [-0.0030, -0.0017, -0.0073,  ..., -0.0021, -0.0036,  0.0432]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-15.4375,  -3.3711,   6.2109,  ...,  -7.1328,   0.7148,   0.9048]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 12:55:22 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a poem is haiku
A more specific term for a cup is teacup
A more specific term for a jewelry is bracelet
A more specific term for a tool is rake
A more specific term for a computer is laptop
A more specific term for a guitar is ukulele
A more specific term for a boat is ferry
A more specific term for a weekday is
2024-07-25 12:55:22 root INFO     [order_1_approx] starting weight calculation for A more specific term for a computer is laptop
A more specific term for a guitar is ukulele
A more specific term for a tool is rake
A more specific term for a weekday is monday
A more specific term for a cup is teacup
A more specific term for a jewelry is bracelet
A more specific term for a poem is haiku
A more specific term for a boat is
2024-07-25 12:55:22 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 12:58:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8750, -2.2969,  1.1719,  ...,  0.3262, -2.4219, -1.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-8.3750, -4.0000,  9.9375,  ..., 16.0000,  2.6875,  3.7344],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0159, -0.0001,  0.0116,  ..., -0.0046, -0.0006,  0.0019],
        [-0.0022,  0.0053, -0.0156,  ..., -0.0006, -0.0003, -0.0046],
        [ 0.0044, -0.0009,  0.0811,  ..., -0.0033,  0.0016, -0.0020],
        ...,
        [ 0.0059,  0.0013, -0.0123,  ...,  0.0156, -0.0009,  0.0071],
        [ 0.0005,  0.0015, -0.0203,  ..., -0.0028,  0.0361,  0.0018],
        [ 0.0067,  0.0012,  0.0135,  ..., -0.0008,  0.0029,  0.0295]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-7.5352, -2.7305,  8.9531,  ..., 15.7188,  3.4297,  4.5742]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 12:58:10 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a computer is laptop
A more specific term for a guitar is ukulele
A more specific term for a tool is rake
A more specific term for a weekday is monday
A more specific term for a cup is teacup
A more specific term for a jewelry is bracelet
A more specific term for a poem is haiku
A more specific term for a boat is
2024-07-25 12:58:10 root INFO     total operator prediction time: 1346.970927476883 seconds
2024-07-25 12:58:10 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on antonyms - binary
2024-07-25 12:58:10 root INFO     building operator antonyms - binary
2024-07-25 12:58:10 root INFO     [order_1_approx] starting weight calculation for The opposite of climb is descend
The opposite of after is before
The opposite of mortal is immortal
The opposite of dead is alive
The opposite of employ is dismiss
The opposite of below is above
The opposite of front is back
The opposite of inbound is
2024-07-25 12:58:11 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 13:00:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1514, -2.0938,  2.5781,  ...,  0.7812, -1.6094,  1.9531],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -3.1406, -14.9375,   6.5000,  ..., -10.7500,  -8.4375,  -0.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 5.4443e-02, -1.6174e-03,  1.9043e-02,  ..., -3.0518e-05,
         -1.1719e-02, -1.1841e-02],
        [ 9.6436e-03,  1.6846e-02, -3.2501e-03,  ..., -7.3242e-03,
          8.3618e-03,  1.0986e-02],
        [ 5.0964e-03,  6.7749e-03,  1.8945e-01,  ..., -7.7515e-03,
          3.3569e-03, -7.2021e-03],
        ...,
        [ 3.1128e-03,  3.0823e-03, -5.6458e-03,  ...,  4.1748e-02,
         -3.8147e-04, -9.7046e-03],
        [ 4.5776e-04,  7.0801e-03,  5.7068e-03,  ..., -4.7302e-04,
          6.8848e-02,  4.5471e-03],
        [-1.9150e-03,  1.5717e-03, -1.3855e-02,  ..., -4.3335e-03,
          2.1362e-03,  6.0059e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -4.2617, -14.1250,   5.9961,  ...,  -9.9609,  -8.5703,  -1.1523]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 13:00:59 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of climb is descend
The opposite of after is before
The opposite of mortal is immortal
The opposite of dead is alive
The opposite of employ is dismiss
The opposite of below is above
The opposite of front is back
The opposite of inbound is
2024-07-25 13:00:59 root INFO     [order_1_approx] starting weight calculation for The opposite of front is back
The opposite of employ is dismiss
The opposite of after is before
The opposite of below is above
The opposite of dead is alive
The opposite of climb is descend
The opposite of inbound is outbound
The opposite of mortal is
2024-07-25 13:00:59 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 13:03:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4141, -0.6562, -1.0547,  ..., -1.0000, -2.9375,  0.0859],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-1.9531, -3.0781,  1.6250,  ...,  2.5312, -8.2500, -8.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0276, -0.0079,  0.0058,  ...,  0.0023,  0.0090, -0.0076],
        [-0.0045,  0.0164,  0.0170,  ...,  0.0017,  0.0070,  0.0071],
        [-0.0064,  0.0102,  0.1445,  ..., -0.0079, -0.0090, -0.0079],
        ...,
        [ 0.0161, -0.0021, -0.0082,  ...,  0.0220,  0.0031, -0.0027],
        [ 0.0032,  0.0023, -0.0102,  ...,  0.0002,  0.0571,  0.0070],
        [-0.0025,  0.0007, -0.0164,  ..., -0.0081,  0.0081,  0.0437]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2773, -3.4375,  1.8320,  ...,  3.6445, -7.7109, -8.4453]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 13:03:48 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of front is back
The opposite of employ is dismiss
The opposite of after is before
The opposite of below is above
The opposite of dead is alive
The opposite of climb is descend
The opposite of inbound is outbound
The opposite of mortal is
2024-07-25 13:03:49 root INFO     [order_1_approx] starting weight calculation for The opposite of front is back
The opposite of employ is dismiss
The opposite of dead is alive
The opposite of mortal is immortal
The opposite of below is above
The opposite of inbound is outbound
The opposite of after is before
The opposite of climb is
2024-07-25 13:03:49 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 13:06:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0078, -1.2656,  2.1875,  ..., -3.3281, -1.3281, -0.1328],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 4.8750, -5.6250,  5.1875,  ..., -6.9688, -9.0000,  4.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0352,  0.0112,  0.0024,  ...,  0.0022,  0.0061,  0.0031],
        [ 0.0034,  0.0194, -0.0070,  ..., -0.0012, -0.0029, -0.0026],
        [ 0.0024,  0.0033,  0.1123,  ..., -0.0106,  0.0046, -0.0036],
        ...,
        [ 0.0013, -0.0009, -0.0156,  ...,  0.0261,  0.0039, -0.0090],
        [-0.0020,  0.0018,  0.0042,  ..., -0.0030,  0.0420,  0.0118],
        [ 0.0005,  0.0041,  0.0109,  ..., -0.0047,  0.0004,  0.0391]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.9023, -6.5234,  3.4785,  ..., -6.7188, -9.2266,  5.8633]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 13:06:38 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of front is back
The opposite of employ is dismiss
The opposite of dead is alive
The opposite of mortal is immortal
The opposite of below is above
The opposite of inbound is outbound
The opposite of after is before
The opposite of climb is
2024-07-25 13:06:38 root INFO     [order_1_approx] starting weight calculation for The opposite of inbound is outbound
The opposite of climb is descend
The opposite of front is back
The opposite of below is above
The opposite of dead is alive
The opposite of after is before
The opposite of mortal is immortal
The opposite of employ is
2024-07-25 13:06:38 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 13:09:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9492,  0.4805, -0.7812,  ..., -0.8281, -4.7500,  2.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 15.6875, -10.1875,  -0.8438,  ...,  -8.5000,  -0.3281,   2.3906],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.4414e-02, -8.9264e-04,  2.2888e-03,  ..., -8.6975e-04,
         -7.2479e-04,  1.4496e-03],
        [-6.0120e-03,  1.7578e-02,  9.5215e-03,  ..., -5.7678e-03,
         -1.1963e-02, -2.8992e-03],
        [ 5.1270e-03, -4.4556e-03,  1.1621e-01,  ..., -1.0376e-02,
          8.9111e-03, -7.0801e-03],
        ...,
        [ 7.4463e-03, -5.1880e-03, -7.0190e-03,  ...,  2.1484e-02,
          7.7820e-04, -1.0925e-02],
        [ 5.5237e-03, -6.4392e-03, -9.1553e-05,  ...,  2.7771e-03,
          4.1260e-02,  9.6436e-03],
        [ 6.5994e-04, -1.3275e-03, -1.5564e-03,  ...,  2.2583e-03,
          6.1951e-03,  3.8574e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 14.0938, -11.1641,  -0.3865,  ...,  -7.1367,  -0.3618,   2.9727]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 13:09:27 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of inbound is outbound
The opposite of climb is descend
The opposite of front is back
The opposite of below is above
The opposite of dead is alive
The opposite of after is before
The opposite of mortal is immortal
The opposite of employ is
2024-07-25 13:09:27 root INFO     [order_1_approx] starting weight calculation for The opposite of below is above
The opposite of climb is descend
The opposite of mortal is immortal
The opposite of front is back
The opposite of employ is dismiss
The opposite of inbound is outbound
The opposite of dead is alive
The opposite of after is
2024-07-25 13:09:27 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 13:12:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7266,  1.0625,  1.6250,  ...,  2.0312,  0.3867, -1.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 13.1250, -12.7500,   6.8750,  ...,   1.9609,   0.4922, -13.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0479,  0.0052,  0.0093,  ..., -0.0054,  0.0110, -0.0013],
        [-0.0048,  0.0167,  0.0027,  ..., -0.0069, -0.0087, -0.0024],
        [ 0.0078,  0.0021,  0.1602,  ...,  0.0019, -0.0156, -0.0009],
        ...,
        [ 0.0036,  0.0011, -0.0135,  ...,  0.0349, -0.0071, -0.0041],
        [-0.0072,  0.0033, -0.0232,  ..., -0.0118,  0.0659,  0.0032],
        [-0.0105,  0.0086, -0.0309,  ...,  0.0074,  0.0005,  0.0513]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 13.2422, -12.5781,   7.0039,  ...,   2.0605,   1.9150, -12.5078]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 13:12:16 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of below is above
The opposite of climb is descend
The opposite of mortal is immortal
The opposite of front is back
The opposite of employ is dismiss
The opposite of inbound is outbound
The opposite of dead is alive
The opposite of after is
2024-07-25 13:12:16 root INFO     [order_1_approx] starting weight calculation for The opposite of inbound is outbound
The opposite of below is above
The opposite of mortal is immortal
The opposite of front is back
The opposite of employ is dismiss
The opposite of climb is descend
The opposite of after is before
The opposite of dead is
2024-07-25 13:12:17 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 13:15:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.1094,  1.0469, -2.9375,  ...,  1.1406,  1.0625, -0.2227],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-0.7422,  3.5312, -3.5312,  ...,  8.7500, -4.5625,  4.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 4.0039e-02, -1.4343e-03,  1.2573e-02,  ..., -6.6528e-03,
          5.3101e-03,  1.9455e-03],
        [-6.9580e-03,  2.3926e-02,  1.0559e-02,  ..., -7.9956e-03,
          6.1035e-05,  8.4839e-03],
        [-5.1880e-04,  7.3853e-03,  1.0791e-01,  ..., -3.6316e-03,
          9.3384e-03, -3.9368e-03],
        ...,
        [-4.6692e-03,  1.9684e-03, -7.7515e-03,  ...,  2.6489e-02,
         -1.4343e-02, -6.6833e-03],
        [ 3.9673e-03,  4.8523e-03, -1.2451e-02,  ...,  4.2114e-03,
          5.0537e-02,  1.3489e-02],
        [-3.3569e-04,  1.1597e-03, -1.4465e-02,  ...,  8.6060e-03,
          3.4790e-03,  4.4678e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1660,  2.7832, -3.0605,  ...,  6.9922, -4.3594,  4.2734]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 13:15:06 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of inbound is outbound
The opposite of below is above
The opposite of mortal is immortal
The opposite of front is back
The opposite of employ is dismiss
The opposite of climb is descend
The opposite of after is before
The opposite of dead is
2024-07-25 13:15:06 root INFO     [order_1_approx] starting weight calculation for The opposite of dead is alive
The opposite of mortal is immortal
The opposite of climb is descend
The opposite of after is before
The opposite of employ is dismiss
The opposite of inbound is outbound
The opposite of below is above
The opposite of front is
2024-07-25 13:15:06 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 13:17:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.0938, -0.2090, -2.7344,  ..., -1.7812, -1.0234, -0.3711],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-1.9219, -7.1875, 13.6875,  ..., -2.1094, -9.4375, -4.7188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 4.5898e-02,  2.3499e-03,  1.0071e-02,  ..., -1.0376e-03,
          4.4861e-03, -3.3875e-03],
        [ 2.1973e-03,  2.3315e-02, -2.0020e-02,  ...,  3.9673e-04,
         -2.9755e-04,  5.8594e-03],
        [ 2.7771e-03,  7.6294e-05,  1.4062e-01,  ...,  2.1362e-04,
         -2.2888e-04,  1.9836e-03],
        ...,
        [ 9.8877e-03, -6.0425e-03,  8.5449e-03,  ...,  2.8442e-02,
          9.9487e-03, -4.6692e-03],
        [-2.9602e-03,  7.9956e-03,  1.6632e-03,  ..., -4.5471e-03,
          5.0537e-02,  3.2806e-03],
        [-4.2419e-03, -1.3275e-03, -1.7212e-02,  ..., -2.5940e-03,
         -6.1951e-03,  5.4932e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2021, -6.9023, 13.6875,  ..., -0.7334, -8.8125, -4.8008]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 13:17:54 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of dead is alive
The opposite of mortal is immortal
The opposite of climb is descend
The opposite of after is before
The opposite of employ is dismiss
The opposite of inbound is outbound
The opposite of below is above
The opposite of front is
2024-07-25 13:17:54 root INFO     [order_1_approx] starting weight calculation for The opposite of employ is dismiss
The opposite of after is before
The opposite of inbound is outbound
The opposite of front is back
The opposite of mortal is immortal
The opposite of climb is descend
The opposite of dead is alive
The opposite of below is
2024-07-25 13:17:55 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 13:20:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4414, -1.3672,  5.3750,  ...,  1.4219,  1.1250, -1.4922],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 7.6875, -0.3750, 13.4375,  ..., -7.8750, -4.2812, -8.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0513, -0.0031,  0.0228,  ...,  0.0186, -0.0159,  0.0114],
        [-0.0043,  0.0212,  0.0022,  ..., -0.0123,  0.0093, -0.0044],
        [-0.0189,  0.0087,  0.1230,  ..., -0.0294,  0.0156, -0.0225],
        ...,
        [-0.0063, -0.0003, -0.0101,  ...,  0.0064,  0.0090, -0.0153],
        [-0.0010,  0.0074,  0.0051,  ..., -0.0010,  0.0393,  0.0086],
        [-0.0110,  0.0038, -0.0060,  ..., -0.0147,  0.0204,  0.0383]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.8555,  3.5664, 16.4531,  ..., -6.4492, -5.9531, -6.0273]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 13:20:44 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of employ is dismiss
The opposite of after is before
The opposite of inbound is outbound
The opposite of front is back
The opposite of mortal is immortal
The opposite of climb is descend
The opposite of dead is alive
The opposite of below is
2024-07-25 13:20:44 root INFO     total operator prediction time: 1353.7487306594849 seconds
2024-07-25 13:20:44 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on meronyms - member
2024-07-25 13:20:44 root INFO     building operator meronyms - member
2024-07-25 13:20:44 root INFO     [order_1_approx] starting weight calculation for A elephant is a member of a herd
A flower is a member of a bouquet
A juror is a member of a jury
A wolf is a member of a pack
A spouse is a member of a couple
A song is a member of a album
A photo is a member of a album
A musician is a member of a
2024-07-25 13:20:44 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 13:23:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5312, -2.8594,  2.5312,  ..., -1.0312, -2.6250,  1.8281],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  6.7500,  -8.8125, -10.5625,  ...,   3.3438,   2.5938,   0.3906],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0300, -0.0044, -0.0145,  ...,  0.0024,  0.0085,  0.0129],
        [ 0.0001,  0.0193,  0.0114,  ...,  0.0035, -0.0083, -0.0067],
        [ 0.0017, -0.0029,  0.0825,  ..., -0.0043, -0.0008, -0.0020],
        ...,
        [-0.0054,  0.0055,  0.0052,  ...,  0.0212,  0.0001, -0.0049],
        [ 0.0011,  0.0006,  0.0051,  ..., -0.0036,  0.0466, -0.0028],
        [ 0.0041, -0.0027, -0.0047,  ...,  0.0004,  0.0036,  0.0488]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  7.0859,  -8.4062, -11.2266,  ...,   3.6367,   1.7480,  -0.3105]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 13:23:36 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A elephant is a member of a herd
A flower is a member of a bouquet
A juror is a member of a jury
A wolf is a member of a pack
A spouse is a member of a couple
A song is a member of a album
A photo is a member of a album
A musician is a member of a
2024-07-25 13:23:37 root INFO     [order_1_approx] starting weight calculation for A wolf is a member of a pack
A flower is a member of a bouquet
A musician is a member of a orchestra
A spouse is a member of a couple
A song is a member of a album
A juror is a member of a jury
A photo is a member of a album
A elephant is a member of a
2024-07-25 13:23:37 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 13:26:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5703, -3.4062,  3.1875,  ..., -3.4062, -6.5312,  0.0938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-11.6250,  -3.1562,   8.8125,  ...,  -9.5000,  -1.3125,  -7.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.6611e-02,  1.3580e-03,  7.9956e-03,  ..., -3.0518e-03,
          3.0212e-03,  4.8828e-03],
        [ 4.6387e-03,  2.0020e-02, -1.7334e-02,  ..., -3.4790e-03,
         -3.0365e-03,  7.9956e-03],
        [-3.8452e-03, -8.7280e-03,  1.3477e-01,  ..., -2.3804e-03,
         -4.5776e-03,  3.1128e-03],
        ...,
        [ 1.3161e-04,  2.9297e-03,  2.3193e-02,  ...,  3.0273e-02,
         -1.8616e-03,  1.2054e-03],
        [-1.0132e-02,  6.8970e-03, -6.8054e-03,  ..., -1.7834e-04,
          5.5664e-02, -4.7913e-03],
        [ 7.5378e-03, -7.1411e-03, -1.0559e-02,  ...,  1.2589e-03,
         -6.8359e-03,  4.9561e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-12.2266,  -2.2949,   7.8242,  ...,  -9.1875,  -2.0527,  -7.1055]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 13:26:29 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A wolf is a member of a pack
A flower is a member of a bouquet
A musician is a member of a orchestra
A spouse is a member of a couple
A song is a member of a album
A juror is a member of a jury
A photo is a member of a album
A elephant is a member of a
2024-07-25 13:26:29 root INFO     [order_1_approx] starting weight calculation for A flower is a member of a bouquet
A wolf is a member of a pack
A musician is a member of a orchestra
A juror is a member of a jury
A elephant is a member of a herd
A photo is a member of a album
A spouse is a member of a couple
A song is a member of a
2024-07-25 13:26:30 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 13:29:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4805, -1.3906,  4.0312,  ..., -0.3008, -4.3750,  0.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 3.7188, -2.4062, -2.6562,  ...,  4.3750, -0.6875, 12.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0267,  0.0037,  0.0085,  ...,  0.0034,  0.0059, -0.0033],
        [ 0.0037,  0.0155, -0.0027,  ...,  0.0027,  0.0014, -0.0078],
        [-0.0002, -0.0044,  0.1064,  ..., -0.0033, -0.0018, -0.0018],
        ...,
        [ 0.0019,  0.0012, -0.0120,  ...,  0.0264, -0.0083, -0.0080],
        [-0.0044,  0.0017,  0.0159,  ..., -0.0080,  0.0500,  0.0009],
        [ 0.0010, -0.0036,  0.0140,  ..., -0.0002,  0.0008,  0.0352]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.1875, -1.3525, -2.4863,  ...,  5.7812, -1.8516, 12.0547]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 13:29:18 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A flower is a member of a bouquet
A wolf is a member of a pack
A musician is a member of a orchestra
A juror is a member of a jury
A elephant is a member of a herd
A photo is a member of a album
A spouse is a member of a couple
A song is a member of a
2024-07-25 13:29:19 root INFO     [order_1_approx] starting weight calculation for A wolf is a member of a pack
A spouse is a member of a couple
A musician is a member of a orchestra
A photo is a member of a album
A juror is a member of a jury
A elephant is a member of a herd
A song is a member of a album
A flower is a member of a
2024-07-25 13:29:19 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 13:32:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5117, -2.2031,  0.2773,  ..., -2.0938, -3.5156, -0.4316],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 16.6250,  -5.8438, -12.5000,  ...,  14.8125,   0.5000,   5.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0281,  0.0010,  0.0065,  ..., -0.0010,  0.0038, -0.0093],
        [ 0.0037,  0.0154,  0.0005,  ...,  0.0028,  0.0005,  0.0018],
        [-0.0010, -0.0045,  0.1108,  ..., -0.0039,  0.0022,  0.0051],
        ...,
        [-0.0001,  0.0040, -0.0047,  ...,  0.0151,  0.0011, -0.0022],
        [-0.0063,  0.0027,  0.0075,  ..., -0.0028,  0.0439, -0.0049],
        [-0.0022, -0.0026, -0.0078,  ..., -0.0015,  0.0029,  0.0364]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 16.2812,  -4.6172, -13.6406,  ...,  13.8281,  -0.3394,   5.3711]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 13:32:08 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A wolf is a member of a pack
A spouse is a member of a couple
A musician is a member of a orchestra
A photo is a member of a album
A juror is a member of a jury
A elephant is a member of a herd
A song is a member of a album
A flower is a member of a
2024-07-25 13:32:08 root INFO     [order_1_approx] starting weight calculation for A musician is a member of a orchestra
A song is a member of a album
A juror is a member of a jury
A flower is a member of a bouquet
A spouse is a member of a couple
A elephant is a member of a herd
A photo is a member of a album
A wolf is a member of a
2024-07-25 13:32:08 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 13:34:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4883, -3.1719, -3.6875,  ..., -2.2500, -4.5938, -1.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 7.5938, -5.0938, -0.9688,  ..., -9.1250,  3.0312, -3.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0181,  0.0015,  0.0085,  ..., -0.0025,  0.0081,  0.0009],
        [ 0.0065,  0.0154, -0.0090,  ...,  0.0012, -0.0027, -0.0067],
        [ 0.0006, -0.0105,  0.1191,  ..., -0.0047,  0.0002,  0.0068],
        ...,
        [-0.0022,  0.0031,  0.0108,  ...,  0.0258, -0.0027,  0.0030],
        [-0.0056,  0.0038,  0.0103,  ..., -0.0045,  0.0464, -0.0036],
        [ 0.0088, -0.0088, -0.0052,  ..., -0.0053, -0.0010,  0.0447]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.8516, -4.3516, -0.4885,  ..., -8.5234,  2.4160, -3.9336]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 13:34:58 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A musician is a member of a orchestra
A song is a member of a album
A juror is a member of a jury
A flower is a member of a bouquet
A spouse is a member of a couple
A elephant is a member of a herd
A photo is a member of a album
A wolf is a member of a
2024-07-25 13:34:58 root INFO     [order_1_approx] starting weight calculation for A musician is a member of a orchestra
A song is a member of a album
A juror is a member of a jury
A elephant is a member of a herd
A flower is a member of a bouquet
A photo is a member of a album
A wolf is a member of a pack
A spouse is a member of a
2024-07-25 13:34:58 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 13:37:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6094, -4.2500, -0.6641,  ..., -3.1406, -1.9375, -3.0781],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 0.3438, -1.7812, -7.4688,  ..., -7.0312,  2.8906, -3.2969],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0234, -0.0012,  0.0041,  ...,  0.0027, -0.0009, -0.0038],
        [ 0.0032,  0.0176,  0.0103,  ..., -0.0012,  0.0039, -0.0006],
        [ 0.0003, -0.0046,  0.1045,  ..., -0.0079, -0.0145, -0.0017],
        ...,
        [ 0.0034,  0.0021,  0.0199,  ...,  0.0166,  0.0032, -0.0074],
        [-0.0032,  0.0045,  0.0063,  ..., -0.0008,  0.0469,  0.0090],
        [ 0.0029, -0.0032, -0.0031,  ..., -0.0019,  0.0079,  0.0427]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5176, -2.2148, -7.1680,  ..., -3.3906,  3.7695, -3.5039]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 13:37:48 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A musician is a member of a orchestra
A song is a member of a album
A juror is a member of a jury
A elephant is a member of a herd
A flower is a member of a bouquet
A photo is a member of a album
A wolf is a member of a pack
A spouse is a member of a
2024-07-25 13:37:48 root INFO     [order_1_approx] starting weight calculation for A song is a member of a album
A photo is a member of a album
A elephant is a member of a herd
A spouse is a member of a couple
A musician is a member of a orchestra
A wolf is a member of a pack
A flower is a member of a bouquet
A juror is a member of a
2024-07-25 13:37:48 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 13:40:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.7969, -4.3125,  7.5625,  ..., -0.0786, -0.1094,  2.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 1.8281, -8.3750,  0.1875,  ..., -6.0938, -2.3281,  8.4375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0361,  0.0024,  0.0081,  ..., -0.0009,  0.0146, -0.0013],
        [-0.0014,  0.0182,  0.0132,  ...,  0.0021,  0.0067, -0.0038],
        [-0.0002, -0.0069,  0.0913,  ..., -0.0019,  0.0036, -0.0084],
        ...,
        [ 0.0011,  0.0011,  0.0081,  ...,  0.0287,  0.0016, -0.0004],
        [-0.0018, -0.0063,  0.0137,  ...,  0.0013,  0.0457, -0.0030],
        [-0.0020,  0.0004, -0.0056,  ..., -0.0015,  0.0059,  0.0483]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5449, -8.7188, -0.2722,  ..., -4.7344, -2.5059,  9.1719]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 13:40:37 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A song is a member of a album
A photo is a member of a album
A elephant is a member of a herd
A spouse is a member of a couple
A musician is a member of a orchestra
A wolf is a member of a pack
A flower is a member of a bouquet
A juror is a member of a
2024-07-25 13:40:38 root INFO     [order_1_approx] starting weight calculation for A juror is a member of a jury
A musician is a member of a orchestra
A wolf is a member of a pack
A flower is a member of a bouquet
A elephant is a member of a herd
A spouse is a member of a couple
A song is a member of a album
A photo is a member of a
2024-07-25 13:40:38 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 13:43:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0156, -0.3535,  0.3457,  ..., -0.8633, -3.8438,  2.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  8.5625,   0.5156, -12.3750,  ...,  -2.8125,  -8.3750,  15.9375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0255,  0.0069,  0.0129,  ...,  0.0013,  0.0035,  0.0022],
        [ 0.0049,  0.0165,  0.0032,  ...,  0.0016,  0.0011,  0.0010],
        [-0.0030, -0.0056,  0.1030,  ..., -0.0025, -0.0049, -0.0038],
        ...,
        [ 0.0060, -0.0039,  0.0018,  ...,  0.0208, -0.0050, -0.0034],
        [-0.0064,  0.0032,  0.0010,  ..., -0.0087,  0.0464, -0.0047],
        [ 0.0061, -0.0041,  0.0023,  ...,  0.0012,  0.0012,  0.0295]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  7.7734,   0.5908, -12.7734,  ...,  -2.6035,  -8.6562,  16.2812]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 13:43:27 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A juror is a member of a jury
A musician is a member of a orchestra
A wolf is a member of a pack
A flower is a member of a bouquet
A elephant is a member of a herd
A spouse is a member of a couple
A song is a member of a album
A photo is a member of a
2024-07-25 13:43:27 root INFO     total operator prediction time: 1363.2293183803558 seconds
2024-07-25 13:43:27 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on noun - plural_irreg
2024-07-25 13:43:27 root INFO     building operator noun - plural_irreg
2024-07-25 13:43:27 root INFO     [order_1_approx] starting weight calculation for The plural form of variety is varieties
The plural form of opportunity is opportunities
The plural form of city is cities
The plural form of memory is memories
The plural form of energy is energies
The plural form of category is categories
The plural form of responsibility is responsibilities
The plural form of species is
2024-07-25 13:43:27 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 13:46:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9531, -4.9375, -5.3750,  ..., -0.0117,  0.4277, -0.8438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 16.8750, -10.2500,  -8.5000,  ...,   0.2500,   3.2969,  -5.0938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0276,  0.0003,  0.0210,  ...,  0.0073, -0.0034, -0.0052],
        [-0.0026,  0.0212,  0.0043,  ..., -0.0045, -0.0011,  0.0045],
        [ 0.0044,  0.0066,  0.0981,  ...,  0.0030,  0.0048, -0.0005],
        ...,
        [-0.0002,  0.0032,  0.0020,  ...,  0.0190, -0.0047,  0.0005],
        [ 0.0011, -0.0028, -0.0004,  ...,  0.0080,  0.0297,  0.0005],
        [ 0.0002, -0.0025, -0.0095,  ..., -0.0017,  0.0017,  0.0332]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 14.9766, -10.5625,  -7.4688,  ...,   0.1166,   2.2305,  -5.1992]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 13:46:14 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of variety is varieties
The plural form of opportunity is opportunities
The plural form of city is cities
The plural form of memory is memories
The plural form of energy is energies
The plural form of category is categories
The plural form of responsibility is responsibilities
The plural form of species is
2024-07-25 13:46:14 root INFO     [order_1_approx] starting weight calculation for The plural form of opportunity is opportunities
The plural form of energy is energies
The plural form of responsibility is responsibilities
The plural form of species is species
The plural form of city is cities
The plural form of memory is memories
The plural form of category is categories
The plural form of variety is
2024-07-25 13:46:14 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 13:49:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1953e+00, -3.9219e+00, -2.9688e+00,  ..., -8.8281e-01,
         2.4219e+00,  3.9062e-03], device='cuda:0', dtype=torch.float16,
       grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 7.1250, -1.7500, -7.4375,  ..., -5.5938,  2.6562, -8.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0280,  0.0014,  0.0155,  ...,  0.0046,  0.0012, -0.0024],
        [ 0.0012,  0.0181,  0.0081,  ..., -0.0036, -0.0017,  0.0005],
        [ 0.0045,  0.0041,  0.1128,  ..., -0.0005,  0.0048, -0.0014],
        ...,
        [ 0.0013, -0.0023,  0.0009,  ...,  0.0292, -0.0092, -0.0004],
        [ 0.0033,  0.0021,  0.0042,  ...,  0.0003,  0.0381,  0.0052],
        [-0.0016, -0.0007, -0.0054,  ..., -0.0066,  0.0088,  0.0403]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.7617, -3.5254, -6.9844,  ..., -6.2695,  3.3672, -9.0078]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 13:49:01 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of opportunity is opportunities
The plural form of energy is energies
The plural form of responsibility is responsibilities
The plural form of species is species
The plural form of city is cities
The plural form of memory is memories
The plural form of category is categories
The plural form of variety is
2024-07-25 13:49:01 root INFO     [order_1_approx] starting weight calculation for The plural form of responsibility is responsibilities
The plural form of variety is varieties
The plural form of energy is energies
The plural form of memory is memories
The plural form of species is species
The plural form of opportunity is opportunities
The plural form of category is categories
The plural form of city is
2024-07-25 13:49:01 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 13:51:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2656, -2.0312,  2.3906,  ..., -2.0156, -1.8906, -2.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  3.4219, -10.0000,  -1.3359,  ..., -13.6250,   3.1562,  -8.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0396,  0.0151, -0.0542,  ...,  0.0228,  0.0237, -0.0125],
        [-0.0130,  0.0011,  0.0564,  ..., -0.0295, -0.0198,  0.0105],
        [ 0.0021, -0.0019,  0.1182,  ..., -0.0089, -0.0025,  0.0019],
        ...,
        [ 0.0069,  0.0061, -0.0256,  ...,  0.0267, -0.0045, -0.0046],
        [ 0.0028,  0.0058, -0.0074,  ...,  0.0067,  0.0327, -0.0053],
        [ 0.0026,  0.0053, -0.0300,  ...,  0.0064,  0.0099,  0.0270]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  2.8613,  -9.9688,  -1.4717,  ..., -13.9453,   4.1836,  -8.3125]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 13:51:48 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of responsibility is responsibilities
The plural form of variety is varieties
The plural form of energy is energies
The plural form of memory is memories
The plural form of species is species
The plural form of opportunity is opportunities
The plural form of category is categories
The plural form of city is
2024-07-25 13:51:48 root INFO     [order_1_approx] starting weight calculation for The plural form of city is cities
The plural form of responsibility is responsibilities
The plural form of species is species
The plural form of memory is memories
The plural form of opportunity is opportunities
The plural form of variety is varieties
The plural form of category is categories
The plural form of energy is
2024-07-25 13:51:49 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 13:54:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.6328, -1.3359, -0.4316,  ...,  0.3203, -1.2031, -1.6797],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  7.0000,   0.4219,   1.6875,  ...,  -2.8125,  -9.0625, -11.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0282, -0.0011,  0.0100,  ...,  0.0026,  0.0067,  0.0044],
        [ 0.0028,  0.0186,  0.0107,  ..., -0.0026, -0.0046, -0.0029],
        [ 0.0023,  0.0091,  0.0874,  ..., -0.0061,  0.0118,  0.0023],
        ...,
        [ 0.0039, -0.0028, -0.0071,  ...,  0.0278, -0.0098, -0.0052],
        [ 0.0006,  0.0006, -0.0038,  ..., -0.0021,  0.0396,  0.0033],
        [-0.0046,  0.0020, -0.0162,  ...,  0.0007,  0.0045,  0.0391]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  6.8320,   0.4998,   0.9365,  ...,  -3.1953,  -7.4844, -11.4922]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 13:54:36 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of city is cities
The plural form of responsibility is responsibilities
The plural form of species is species
The plural form of memory is memories
The plural form of opportunity is opportunities
The plural form of variety is varieties
The plural form of category is categories
The plural form of energy is
2024-07-25 13:54:36 root INFO     [order_1_approx] starting weight calculation for The plural form of energy is energies
The plural form of opportunity is opportunities
The plural form of city is cities
The plural form of responsibility is responsibilities
The plural form of variety is varieties
The plural form of species is species
The plural form of memory is memories
The plural form of category is
2024-07-25 13:54:36 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 13:57:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.7656, -2.1875, -0.0645,  ..., -2.0312,  0.9453,  1.1172],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 1.5781, -6.4688,  0.4062,  ..., -5.3438,  0.2480, -4.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0284,  0.0026,  0.0089,  ...,  0.0054,  0.0037, -0.0045],
        [ 0.0031,  0.0204,  0.0064,  ..., -0.0012, -0.0014,  0.0040],
        [ 0.0015,  0.0038,  0.1094,  ...,  0.0003,  0.0020, -0.0024],
        ...,
        [ 0.0006, -0.0081,  0.0051,  ...,  0.0259, -0.0121, -0.0022],
        [-0.0002,  0.0076, -0.0023,  ...,  0.0007,  0.0410,  0.0016],
        [-0.0040, -0.0015, -0.0084,  ..., -0.0034,  0.0023,  0.0425]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2666, -4.7500, -0.0371,  ..., -6.0859,  0.0480, -4.2383]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 13:57:23 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of energy is energies
The plural form of opportunity is opportunities
The plural form of city is cities
The plural form of responsibility is responsibilities
The plural form of variety is varieties
The plural form of species is species
The plural form of memory is memories
The plural form of category is
2024-07-25 13:57:23 root INFO     [order_1_approx] starting weight calculation for The plural form of city is cities
The plural form of energy is energies
The plural form of memory is memories
The plural form of opportunity is opportunities
The plural form of category is categories
The plural form of species is species
The plural form of variety is varieties
The plural form of responsibility is
2024-07-25 13:57:24 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 14:00:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6484, -0.7461, -4.0938,  ...,  0.4570, -1.7500,  1.8359],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  4.8125,   8.0625, -14.0625,  ...,  -7.8438,   4.8438,  -2.8125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.7344e-02, -1.2283e-03,  6.1951e-03,  ...,  5.0659e-03,
          1.5106e-03, -1.5717e-03],
        [ 3.2043e-03,  1.2817e-02,  1.4648e-02,  ..., -3.5095e-03,
          6.1035e-05, -1.7242e-03],
        [ 1.6937e-03,  2.6093e-03,  8.6914e-02,  ...,  2.9297e-03,
          2.0447e-03, -6.7444e-03],
        ...,
        [ 7.5989e-03, -1.4420e-03,  2.3499e-03,  ...,  2.2461e-02,
         -3.4180e-03, -1.2054e-03],
        [ 3.4943e-03,  9.3079e-04,  5.6458e-03,  ..., -9.8419e-04,
          3.5645e-02,  2.1973e-03],
        [-2.8687e-03,  2.9602e-03, -1.7242e-03,  ..., -4.2725e-03,
          6.2561e-03,  3.0884e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  5.8203,   8.3359, -12.6562,  ...,  -7.1680,   6.7031,  -3.4609]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 14:00:13 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of city is cities
The plural form of energy is energies
The plural form of memory is memories
The plural form of opportunity is opportunities
The plural form of category is categories
The plural form of species is species
The plural form of variety is varieties
The plural form of responsibility is
2024-07-25 14:00:13 root INFO     [order_1_approx] starting weight calculation for The plural form of category is categories
The plural form of species is species
The plural form of memory is memories
The plural form of energy is energies
The plural form of responsibility is responsibilities
The plural form of city is cities
The plural form of variety is varieties
The plural form of opportunity is
2024-07-25 14:00:13 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 14:02:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4766, -1.2656, -0.2461,  ...,  0.1182,  0.3418,  1.6641],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 9.5000,  4.2500, -2.4062,  ...,  3.0469,  4.5625, -9.4375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.8076e-02, -1.0986e-03,  6.8359e-03,  ...,  7.6294e-06,
          5.1270e-03, -2.4719e-03],
        [-3.4790e-03,  1.7456e-02,  8.0566e-03,  ..., -7.0190e-03,
         -1.6327e-03,  8.3923e-05],
        [-3.0823e-03,  1.6785e-03,  9.3750e-02,  ..., -3.2654e-03,
          4.5166e-03,  2.1667e-03],
        ...,
        [ 8.0109e-04, -5.1117e-04,  4.4250e-03,  ...,  2.3804e-02,
         -4.5166e-03,  1.6937e-03],
        [ 1.7548e-03,  3.8452e-03,  5.1575e-03,  ...,  1.1597e-03,
          3.4424e-02,  1.1902e-03],
        [-6.3782e-03,  3.9368e-03, -7.8735e-03,  ...,  2.1973e-03,
          7.1106e-03,  3.4668e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  9.3594,   4.5859,  -3.9297,  ...,   2.5645,   4.6914, -10.0625]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 14:03:00 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of category is categories
The plural form of species is species
The plural form of memory is memories
The plural form of energy is energies
The plural form of responsibility is responsibilities
The plural form of city is cities
The plural form of variety is varieties
The plural form of opportunity is
2024-07-25 14:03:00 root INFO     [order_1_approx] starting weight calculation for The plural form of responsibility is responsibilities
The plural form of category is categories
The plural form of city is cities
The plural form of energy is energies
The plural form of opportunity is opportunities
The plural form of variety is varieties
The plural form of species is species
The plural form of memory is
2024-07-25 14:03:00 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 14:05:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.0312, -0.8359,  0.2383,  ..., -0.3477, -1.2109, -1.3438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  1.6406,   4.3438,  -2.3281,  ..., -11.7500,   6.5000,  -4.6562],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0356,  0.0043, -0.0020,  ...,  0.0048,  0.0065, -0.0018],
        [ 0.0049,  0.0221,  0.0084,  ..., -0.0097, -0.0047,  0.0022],
        [-0.0004,  0.0009,  0.1147,  ..., -0.0065,  0.0021, -0.0022],
        ...,
        [ 0.0052,  0.0018, -0.0060,  ...,  0.0294, -0.0027, -0.0058],
        [ 0.0055,  0.0043, -0.0017,  ...,  0.0015,  0.0464,  0.0041],
        [-0.0058,  0.0018, -0.0034,  ..., -0.0059,  0.0049,  0.0540]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  1.2930,   5.0312,  -3.7695,  ..., -12.3281,   7.6562,  -3.0664]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 14:05:48 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of responsibility is responsibilities
The plural form of category is categories
The plural form of city is cities
The plural form of energy is energies
The plural form of opportunity is opportunities
The plural form of variety is varieties
The plural form of species is species
The plural form of memory is
2024-07-25 14:05:48 root INFO     total operator prediction time: 1340.5122525691986 seconds
2024-07-25 14:05:48 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on Ving - verb_inf
2024-07-25 14:05:48 root INFO     building operator Ving - verb_inf
2024-07-25 14:05:48 root INFO     [order_1_approx] starting weight calculation for understanding is the active form of understand
teaching is the active form of teach
identifying is the active form of identify
providing is the active form of provide
encouraging is the active form of encourage
developing is the active form of develop
happening is the active form of happen
preventing is the active form of
2024-07-25 14:05:48 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 14:08:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1914, -0.5508, -0.0508,  ...,  1.9609, -0.8555, -1.5859],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([16.7500, -4.8125,  0.6328,  ..., -9.1250, -7.0938, -6.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0152, -0.0003, -0.0021,  ...,  0.0014,  0.0015, -0.0006],
        [ 0.0019,  0.0088,  0.0004,  ...,  0.0002, -0.0011,  0.0045],
        [ 0.0018,  0.0017,  0.0562,  ..., -0.0002, -0.0010, -0.0029],
        ...,
        [ 0.0006, -0.0005, -0.0043,  ...,  0.0118, -0.0053, -0.0022],
        [ 0.0029,  0.0007, -0.0025,  ...,  0.0035,  0.0154,  0.0006],
        [-0.0035,  0.0033,  0.0008,  ..., -0.0040,  0.0006,  0.0220]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[17.4062, -4.3164,  0.1389,  ..., -7.8008, -6.8320, -6.7539]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 14:08:37 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for understanding is the active form of understand
teaching is the active form of teach
identifying is the active form of identify
providing is the active form of provide
encouraging is the active form of encourage
developing is the active form of develop
happening is the active form of happen
preventing is the active form of
2024-07-25 14:08:37 root INFO     [order_1_approx] starting weight calculation for preventing is the active form of prevent
happening is the active form of happen
identifying is the active form of identify
developing is the active form of develop
encouraging is the active form of encourage
understanding is the active form of understand
teaching is the active form of teach
providing is the active form of
2024-07-25 14:08:37 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 14:11:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.2656,  1.5938,  3.8281,  ...,  2.0469,  1.8359, -1.6094],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([13.2500, -7.5625,  6.8125,  ..., -7.2500,  2.1094, -6.6562],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0208,  0.0018, -0.0052,  ...,  0.0062,  0.0040,  0.0005],
        [ 0.0042,  0.0162, -0.0016,  ...,  0.0018, -0.0043,  0.0023],
        [ 0.0026, -0.0048,  0.0742,  ..., -0.0065, -0.0006,  0.0021],
        ...,
        [-0.0011,  0.0039,  0.0005,  ...,  0.0160, -0.0021, -0.0014],
        [ 0.0071,  0.0005, -0.0067,  ..., -0.0028,  0.0259,  0.0011],
        [-0.0077,  0.0033,  0.0011,  ...,  0.0015, -0.0008,  0.0304]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[13.0469, -6.8984,  7.5430,  ..., -8.0547,  1.5840, -6.0391]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 14:11:26 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for preventing is the active form of prevent
happening is the active form of happen
identifying is the active form of identify
developing is the active form of develop
encouraging is the active form of encourage
understanding is the active form of understand
teaching is the active form of teach
providing is the active form of
2024-07-25 14:11:26 root INFO     [order_1_approx] starting weight calculation for encouraging is the active form of encourage
providing is the active form of provide
teaching is the active form of teach
understanding is the active form of understand
preventing is the active form of prevent
identifying is the active form of identify
developing is the active form of develop
happening is the active form of
2024-07-25 14:11:26 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 14:14:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9688,  2.2656, -1.6641,  ...,  4.0625,  2.2812,  2.7812],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 11.5625,   1.3125,  10.5000,  ...,  -0.5000, -14.8750,  -9.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.3315e-02, -1.2589e-03, -2.3499e-03,  ..., -4.1504e-03,
          1.6174e-03,  8.4877e-05],
        [ 4.6997e-03,  1.7334e-02,  2.9297e-03,  ...,  5.3711e-03,
         -3.6926e-03,  8.2397e-03],
        [ 2.8534e-03, -2.9907e-03,  1.0254e-01,  ...,  3.7766e-04,
         -1.9379e-03, -2.4719e-03],
        ...,
        [ 2.6855e-03, -2.2583e-03,  2.9907e-03,  ...,  1.8433e-02,
         -5.3711e-03, -6.9275e-03],
        [ 7.7209e-03, -1.2131e-03, -7.8735e-03,  ...,  5.4932e-03,
          3.1494e-02,  3.0518e-05],
        [-1.8158e-03, -9.0790e-04,  5.7983e-03,  ..., -3.3875e-03,
          4.5776e-05,  3.4180e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 10.7266,   2.3359,  10.3359,  ...,  -0.4778, -13.9766, -10.3203]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 14:14:14 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for encouraging is the active form of encourage
providing is the active form of provide
teaching is the active form of teach
understanding is the active form of understand
preventing is the active form of prevent
identifying is the active form of identify
developing is the active form of develop
happening is the active form of
2024-07-25 14:14:15 root INFO     [order_1_approx] starting weight calculation for preventing is the active form of prevent
identifying is the active form of identify
understanding is the active form of understand
providing is the active form of provide
teaching is the active form of teach
encouraging is the active form of encourage
happening is the active form of happen
developing is the active form of
2024-07-25 14:14:15 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 14:17:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-3.3594,  0.7344, -4.7500,  ...,  2.5312,  1.3047, -2.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 15.2500,  -6.6562,   2.6406,  ..., -14.3750,  -2.0625,  -2.5938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.3672e-02, -3.6316e-03, -1.8158e-03,  ...,  2.5482e-03,
          2.0447e-03,  7.6294e-04],
        [ 8.0490e-04,  1.5137e-02,  3.4637e-03,  ..., -1.3046e-03,
          1.8768e-03,  4.2725e-03],
        [ 6.1035e-04,  1.8616e-03,  6.7383e-02,  ..., -5.3101e-03,
         -2.2278e-03,  4.6997e-03],
        ...,
        [ 3.0212e-03,  4.8218e-03, -7.6904e-03,  ...,  1.3000e-02,
         -5.4932e-04, -7.4768e-03],
        [ 5.9204e-03,  3.3722e-03, -1.9073e-03,  ...,  9.3842e-04,
          2.3804e-02, -2.8534e-03],
        [-3.0518e-03,  1.1292e-03,  7.6294e-04,  ...,  1.7166e-05,
         -1.2970e-03,  3.0151e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 15.2969,  -7.4180,   4.0391,  ..., -14.7344,  -2.1328,  -2.8223]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 14:17:04 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for preventing is the active form of prevent
identifying is the active form of identify
understanding is the active form of understand
providing is the active form of provide
teaching is the active form of teach
encouraging is the active form of encourage
happening is the active form of happen
developing is the active form of
2024-07-25 14:17:04 root INFO     [order_1_approx] starting weight calculation for preventing is the active form of prevent
happening is the active form of happen
teaching is the active form of teach
providing is the active form of provide
encouraging is the active form of encourage
understanding is the active form of understand
developing is the active form of develop
identifying is the active form of
2024-07-25 14:17:04 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 14:19:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2734,  1.2188,  1.2734,  ...,  0.1484,  0.4062, -1.8438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 3.8125, -5.0312,  2.9531,  ...,  1.8438,  2.8906, -1.8594],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 9.5825e-03,  1.2398e-04, -1.7471e-03,  ..., -9.9182e-04,
          2.7313e-03, -5.1498e-05],
        [ 2.4719e-03,  1.0742e-02,  2.3651e-03,  ...,  1.5640e-04,
          1.4114e-03,  1.7395e-03],
        [ 6.3171e-03, -2.5024e-03,  6.9824e-02,  ..., -3.2654e-03,
          5.3406e-05, -3.5095e-04],
        ...,
        [ 1.8463e-03,  9.0790e-04,  1.0986e-03,  ...,  1.3245e-02,
          1.5335e-03, -3.6011e-03],
        [ 6.5002e-03,  1.0376e-03, -5.4626e-03,  ..., -5.3024e-04,
          2.2827e-02,  2.7466e-03],
        [-5.4932e-03,  1.2054e-03, -4.3335e-03,  ...,  1.3580e-03,
         -5.9204e-03,  2.6367e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.6699, -4.9023,  3.2012,  ...,  1.8262,  2.5527, -2.7227]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 14:19:53 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for preventing is the active form of prevent
happening is the active form of happen
teaching is the active form of teach
providing is the active form of provide
encouraging is the active form of encourage
understanding is the active form of understand
developing is the active form of develop
identifying is the active form of
2024-07-25 14:19:53 root INFO     [order_1_approx] starting weight calculation for understanding is the active form of understand
providing is the active form of provide
developing is the active form of develop
identifying is the active form of identify
happening is the active form of happen
preventing is the active form of prevent
encouraging is the active form of encourage
teaching is the active form of
2024-07-25 14:19:53 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 14:22:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2539, -0.1719,  4.5938,  ...,  2.6406,  1.1562, -0.4922],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([20.8750, -5.5625,  6.4688,  ..., -6.0312, -9.0000, -4.5938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0099,  0.0010,  0.0017,  ...,  0.0024,  0.0056, -0.0014],
        [ 0.0046,  0.0095,  0.0014,  ...,  0.0018,  0.0002, -0.0012],
        [-0.0014, -0.0022,  0.0481,  ..., -0.0013, -0.0020,  0.0008],
        ...,
        [ 0.0023,  0.0040, -0.0019,  ...,  0.0092,  0.0007, -0.0019],
        [ 0.0051,  0.0012, -0.0084,  ...,  0.0015,  0.0137,  0.0028],
        [-0.0042, -0.0002, -0.0013,  ..., -0.0015,  0.0034,  0.0184]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[20.9844, -5.7031,  7.1758,  ..., -5.2188, -8.7656, -3.8867]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 14:22:42 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for understanding is the active form of understand
providing is the active form of provide
developing is the active form of develop
identifying is the active form of identify
happening is the active form of happen
preventing is the active form of prevent
encouraging is the active form of encourage
teaching is the active form of
2024-07-25 14:22:42 root INFO     [order_1_approx] starting weight calculation for identifying is the active form of identify
happening is the active form of happen
preventing is the active form of prevent
encouraging is the active form of encourage
developing is the active form of develop
providing is the active form of provide
teaching is the active form of teach
understanding is the active form of
2024-07-25 14:22:42 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 14:25:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.4844,  0.8867,  3.8750,  ...,  0.5312,  0.6758,  0.8047],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 14.2500, -10.5625,   1.8359,  ...,  -8.6875,  -4.9375,  -1.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.6357e-02,  2.5330e-03, -5.4932e-03,  ...,  4.1504e-03,
          9.2163e-03, -2.2736e-03],
        [ 2.0905e-03,  1.3977e-02, -8.3008e-03,  ...,  6.4087e-04,
          1.4496e-03, -8.3923e-05],
        [ 3.4943e-03, -3.7079e-03,  7.4707e-02,  ..., -1.1826e-03,
          6.4850e-05,  2.8229e-03],
        ...,
        [ 3.9673e-04,  4.0588e-03,  1.2970e-03,  ...,  1.2695e-02,
         -1.4343e-03, -6.1951e-03],
        [ 5.4321e-03,  2.6703e-05, -9.6436e-03,  ..., -1.6098e-03,
          2.1606e-02,  4.2114e-03],
        [-1.2665e-03,  6.7520e-04,  4.5166e-03,  ..., -1.8463e-03,
         -5.1270e-03,  2.9297e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 14.4062, -10.8281,   2.2109,  ...,  -9.5547,  -4.9609,  -1.3643]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 14:25:30 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for identifying is the active form of identify
happening is the active form of happen
preventing is the active form of prevent
encouraging is the active form of encourage
developing is the active form of develop
providing is the active form of provide
teaching is the active form of teach
understanding is the active form of
2024-07-25 14:25:30 root INFO     [order_1_approx] starting weight calculation for preventing is the active form of prevent
providing is the active form of provide
identifying is the active form of identify
teaching is the active form of teach
developing is the active form of develop
understanding is the active form of understand
happening is the active form of happen
encouraging is the active form of
2024-07-25 14:25:30 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 14:28:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5625, -0.1426,  2.8125,  ...,  1.5312,  0.2227, -0.1348],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 11.2500,  -6.5625,   2.5312,  ...,  -3.9688, -13.3750,  -1.9844],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0197,  0.0024, -0.0071,  ..., -0.0044,  0.0057,  0.0042],
        [-0.0014,  0.0110, -0.0072,  ...,  0.0025,  0.0012,  0.0065],
        [ 0.0039,  0.0013,  0.0737,  ..., -0.0045,  0.0019, -0.0046],
        ...,
        [ 0.0014,  0.0025,  0.0011,  ...,  0.0175,  0.0026, -0.0033],
        [ 0.0068,  0.0013, -0.0032,  ...,  0.0019,  0.0188,  0.0007],
        [-0.0056,  0.0017, -0.0007,  ..., -0.0014,  0.0030,  0.0210]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 11.4375,  -6.4883,   2.3418,  ...,  -2.3047, -13.2109,  -2.8633]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 14:28:17 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for preventing is the active form of prevent
providing is the active form of provide
identifying is the active form of identify
teaching is the active form of teach
developing is the active form of develop
understanding is the active form of understand
happening is the active form of happen
encouraging is the active form of
2024-07-25 14:28:17 root INFO     total operator prediction time: 1349.847799539566 seconds
2024-07-25 14:28:17 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on verb_Ving - Ved
2024-07-25 14:28:17 root INFO     building operator verb_Ving - Ved
2024-07-25 14:28:18 root INFO     [order_1_approx] starting weight calculation for After something is losing, it has lost
After something is introducing, it has introduced
After something is providing, it has provided
After something is adding, it has added
After something is marrying, it has married
After something is suffering, it has suffered
After something is appearing, it has appeared
After something is requiring, it has
2024-07-25 14:28:18 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 14:31:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1250, -0.8359,  2.9375,  ...,  0.9219,  3.7812,  1.7188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 18.7500, -10.1250,  -2.7812,  ..., -18.5000,   6.5625, -12.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.4424e-02, -3.9673e-03,  5.9204e-03,  ...,  3.9673e-03,
          1.0437e-02, -7.8125e-03],
        [-7.9346e-03,  2.0264e-02, -1.9409e-02,  ..., -1.1841e-02,
         -6.8359e-03,  1.9165e-02],
        [-7.6904e-03,  3.7231e-03,  5.8105e-02,  ..., -6.6528e-03,
         -3.0975e-03,  2.5177e-03],
        ...,
        [ 1.3733e-03,  2.6703e-04, -6.0120e-03,  ...,  1.8311e-02,
         -3.6316e-03, -3.3417e-03],
        [ 9.6436e-03, -9.2983e-05,  4.2114e-03,  ..., -6.0272e-04,
          3.3936e-02,  3.8147e-03],
        [-1.6785e-03,  1.2665e-03, -1.5747e-02,  ..., -1.6785e-04,
          2.3499e-03,  4.0283e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 18.9375, -10.3516,  -2.2969,  ..., -18.5000,   7.6289, -12.6250]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 14:31:06 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for After something is losing, it has lost
After something is introducing, it has introduced
After something is providing, it has provided
After something is adding, it has added
After something is marrying, it has married
After something is suffering, it has suffered
After something is appearing, it has appeared
After something is requiring, it has
2024-07-25 14:31:06 root INFO     [order_1_approx] starting weight calculation for After something is adding, it has added
After something is marrying, it has married
After something is losing, it has lost
After something is introducing, it has introduced
After something is appearing, it has appeared
After something is requiring, it has required
After something is providing, it has provided
After something is suffering, it has
2024-07-25 14:31:07 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 14:33:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0781, -0.9609,  2.7188,  ..., -0.4668,  0.4414, -1.6406],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 11.6250,  -3.2031,  14.9375,  ...,  -9.3125,  -6.2812, -13.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0190,  0.0020,  0.0020,  ..., -0.0017,  0.0015, -0.0009],
        [ 0.0039,  0.0142,  0.0103,  ..., -0.0057,  0.0043,  0.0028],
        [ 0.0009,  0.0026,  0.0898,  ..., -0.0074,  0.0094, -0.0030],
        ...,
        [-0.0032,  0.0011, -0.0145,  ...,  0.0200, -0.0041, -0.0062],
        [ 0.0048,  0.0035, -0.0078,  ..., -0.0034,  0.0325,  0.0010],
        [-0.0022, -0.0023, -0.0104,  ..., -0.0020,  0.0020,  0.0297]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 10.4375,  -2.0625,  16.1875,  ...,  -9.7500,  -6.6250, -13.2578]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 14:33:55 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for After something is adding, it has added
After something is marrying, it has married
After something is losing, it has lost
After something is introducing, it has introduced
After something is appearing, it has appeared
After something is requiring, it has required
After something is providing, it has provided
After something is suffering, it has
2024-07-25 14:33:55 root INFO     [order_1_approx] starting weight calculation for After something is appearing, it has appeared
After something is introducing, it has introduced
After something is marrying, it has married
After something is suffering, it has suffered
After something is providing, it has provided
After something is adding, it has added
After something is requiring, it has required
After something is losing, it has
2024-07-25 14:33:55 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 14:36:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7344,  0.1055,  0.7656,  ...,  1.2656,  0.1953, -2.2031],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 17.0000, -16.6250,   5.5625,  ...,  -2.9688,   3.2188,  -7.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0221,  0.0035,  0.0067,  ...,  0.0033, -0.0019,  0.0011],
        [ 0.0048,  0.0148, -0.0046,  ..., -0.0087,  0.0035,  0.0028],
        [ 0.0018,  0.0022,  0.0801,  ..., -0.0009, -0.0024, -0.0068],
        ...,
        [-0.0036, -0.0011, -0.0014,  ...,  0.0192,  0.0030, -0.0033],
        [-0.0032,  0.0002,  0.0003,  ..., -0.0012,  0.0266,  0.0019],
        [-0.0060,  0.0048, -0.0070,  ...,  0.0044, -0.0003,  0.0270]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 15.7344, -14.5703,   4.6172,  ...,  -3.7207,   3.6367,  -8.5547]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 14:36:44 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for After something is appearing, it has appeared
After something is introducing, it has introduced
After something is marrying, it has married
After something is suffering, it has suffered
After something is providing, it has provided
After something is adding, it has added
After something is requiring, it has required
After something is losing, it has
2024-07-25 14:36:44 root INFO     [order_1_approx] starting weight calculation for After something is requiring, it has required
After something is marrying, it has married
After something is appearing, it has appeared
After something is losing, it has lost
After something is suffering, it has suffered
After something is introducing, it has introduced
After something is adding, it has added
After something is providing, it has
2024-07-25 14:36:44 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 14:39:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3750, -1.4219,  4.6250,  ...,  1.6016,  1.3203, -0.0215],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 14.5625, -18.5000,  11.0000,  ..., -13.8125,   4.8125,  -7.9375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0310, -0.0036,  0.0165,  ...,  0.0098,  0.0005, -0.0121],
        [-0.0055,  0.0236, -0.0293,  ..., -0.0127, -0.0037,  0.0162],
        [ 0.0060, -0.0008,  0.0723,  ..., -0.0020,  0.0022, -0.0069],
        ...,
        [ 0.0021, -0.0023, -0.0028,  ...,  0.0212, -0.0003, -0.0025],
        [ 0.0052, -0.0010,  0.0051,  ..., -0.0013,  0.0265,  0.0012],
        [ 0.0007, -0.0018, -0.0007,  ...,  0.0091, -0.0015,  0.0264]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 15.5312, -19.6250,  12.0469,  ..., -15.1172,   5.5352,  -9.1094]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 14:39:33 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for After something is requiring, it has required
After something is marrying, it has married
After something is appearing, it has appeared
After something is losing, it has lost
After something is suffering, it has suffered
After something is introducing, it has introduced
After something is adding, it has added
After something is providing, it has
2024-07-25 14:39:33 root INFO     [order_1_approx] starting weight calculation for After something is losing, it has lost
After something is providing, it has provided
After something is requiring, it has required
After something is marrying, it has married
After something is suffering, it has suffered
After something is adding, it has added
After something is introducing, it has introduced
After something is appearing, it has
2024-07-25 14:39:33 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 14:42:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6953, -1.0859, -1.6797,  ...,  3.3438,  1.1484,  2.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 18.0000, -17.7500,   1.0938,  ..., -10.9375,   4.3438,   2.3594],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0300, -0.0033, -0.0048,  ...,  0.0053,  0.0054, -0.0058],
        [ 0.0041,  0.0189,  0.0001,  ..., -0.0076,  0.0011,  0.0062],
        [ 0.0057,  0.0006,  0.1016,  ...,  0.0006,  0.0016, -0.0024],
        ...,
        [ 0.0064,  0.0010, -0.0058,  ...,  0.0234, -0.0039, -0.0016],
        [-0.0003,  0.0054,  0.0039,  ...,  0.0019,  0.0354, -0.0013],
        [-0.0012,  0.0049, -0.0014,  ...,  0.0061, -0.0036,  0.0356]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 16.5312, -15.3281,   1.6816,  ..., -10.9141,   3.1758,  -0.4355]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 14:42:22 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for After something is losing, it has lost
After something is providing, it has provided
After something is requiring, it has required
After something is marrying, it has married
After something is suffering, it has suffered
After something is adding, it has added
After something is introducing, it has introduced
After something is appearing, it has
2024-07-25 14:42:22 root INFO     [order_1_approx] starting weight calculation for After something is suffering, it has suffered
After something is providing, it has provided
After something is introducing, it has introduced
After something is appearing, it has appeared
After something is marrying, it has married
After something is requiring, it has required
After something is losing, it has lost
After something is adding, it has
2024-07-25 14:42:22 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 14:45:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.4688, -1.6562,  5.8438,  ...,  1.2891,  2.0625,  0.0742],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 11.8125, -11.1875,   5.6250,  ..., -13.5000,  -2.0938,  -6.9062],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0228,  0.0053, -0.0103,  ..., -0.0010,  0.0034,  0.0090],
        [ 0.0050,  0.0190, -0.0026,  ...,  0.0005, -0.0017,  0.0017],
        [ 0.0004, -0.0057,  0.0952,  ..., -0.0013, -0.0023, -0.0034],
        ...,
        [ 0.0042, -0.0032, -0.0093,  ...,  0.0227,  0.0052, -0.0061],
        [ 0.0008,  0.0011, -0.0005,  ...,  0.0017,  0.0361,  0.0001],
        [-0.0021,  0.0013, -0.0123,  ...,  0.0043,  0.0032,  0.0437]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 12.4531, -10.7422,   5.9961,  ..., -14.9375,  -2.3086,  -8.4219]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 14:45:11 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for After something is suffering, it has suffered
After something is providing, it has provided
After something is introducing, it has introduced
After something is appearing, it has appeared
After something is marrying, it has married
After something is requiring, it has required
After something is losing, it has lost
After something is adding, it has
2024-07-25 14:45:11 root INFO     [order_1_approx] starting weight calculation for After something is providing, it has provided
After something is marrying, it has married
After something is adding, it has added
After something is appearing, it has appeared
After something is losing, it has lost
After something is suffering, it has suffered
After something is requiring, it has required
After something is introducing, it has
2024-07-25 14:45:11 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 14:48:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0625, -2.6562,  2.3750,  ...,  1.9531,  0.1621,  2.6719],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 10.3750, -11.1250,   8.5000,  ...,  -6.4062,   3.6094,   0.3438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.2959e-02,  3.6926e-03, -9.7046e-03,  ...,  3.2043e-04,
          2.3193e-03,  9.8419e-04],
        [ 4.3945e-03,  2.2949e-02, -6.6833e-03,  ...,  2.1362e-03,
         -1.0681e-03,  2.9297e-03],
        [ 1.6327e-03, -4.5776e-04,  1.0547e-01,  ..., -3.2501e-03,
          2.7771e-03, -7.4463e-03],
        ...,
        [-3.2654e-03,  5.5695e-04, -6.6223e-03,  ...,  2.2949e-02,
          1.3428e-03,  9.0790e-04],
        [ 3.9978e-03,  2.6855e-03,  1.8005e-03,  ..., -1.9989e-03,
          4.5898e-02,  2.5635e-03],
        [-2.7924e-03,  1.8311e-03, -1.9531e-03,  ...,  8.9645e-05,
          5.9814e-03,  3.8086e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 11.1250, -11.1719,   7.8711,  ...,  -7.7148,   4.0977,  -2.3320]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 14:48:01 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for After something is providing, it has provided
After something is marrying, it has married
After something is adding, it has added
After something is appearing, it has appeared
After something is losing, it has lost
After something is suffering, it has suffered
After something is requiring, it has required
After something is introducing, it has
2024-07-25 14:48:01 root INFO     [order_1_approx] starting weight calculation for After something is introducing, it has introduced
After something is providing, it has provided
After something is losing, it has lost
After something is appearing, it has appeared
After something is requiring, it has required
After something is suffering, it has suffered
After something is adding, it has added
After something is marrying, it has
2024-07-25 14:48:01 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 14:50:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.5000, -1.7031, -4.1250,  ..., -0.6953,  1.3125,  2.3125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  9.8750,  -8.8750,  -2.1562,  ..., -13.1250,   4.7812,  -1.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0267, -0.0024,  0.0052,  ..., -0.0005,  0.0008, -0.0006],
        [ 0.0041,  0.0144, -0.0056,  ..., -0.0017, -0.0044,  0.0057],
        [ 0.0062,  0.0052,  0.0762,  ..., -0.0056,  0.0036, -0.0022],
        ...,
        [ 0.0006, -0.0018, -0.0093,  ...,  0.0225,  0.0015,  0.0005],
        [ 0.0023,  0.0037,  0.0003,  ..., -0.0008,  0.0300, -0.0010],
        [-0.0018,  0.0006, -0.0085,  ..., -0.0006,  0.0056,  0.0310]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 10.7422,  -8.8125,  -1.6602,  ..., -13.0547,   4.4805,  -2.8652]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 14:50:50 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for After something is introducing, it has introduced
After something is providing, it has provided
After something is losing, it has lost
After something is appearing, it has appeared
After something is requiring, it has required
After something is suffering, it has suffered
After something is adding, it has added
After something is marrying, it has
2024-07-25 14:50:50 root INFO     total operator prediction time: 1352.3759410381317 seconds
2024-07-25 14:50:50 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on verb_inf - Ved
2024-07-25 14:50:50 root INFO     building operator verb_inf - Ved
2024-07-25 14:50:50 root INFO     [order_1_approx] starting weight calculation for If the present form is follow, the past form is followed
If the present form is tell, the past form is told
If the present form is appear, the past form is appeared
If the present form is receive, the past form is received
If the present form is spend, the past form is spent
If the present form is seem, the past form is seemed
If the present form is involve, the past form is involved
If the present form is create, the past form is
2024-07-25 14:50:50 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 14:53:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4727, -1.3750, -1.6641,  ...,  2.4531,  0.7500, -0.3320],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-2.9688, -1.9062,  4.4688,  ..., -9.4375, -7.8750,  0.9688],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0303, -0.0023,  0.0013,  ...,  0.0027, -0.0025, -0.0038],
        [ 0.0028,  0.0193,  0.0125,  ..., -0.0082,  0.0023,  0.0024],
        [ 0.0004,  0.0070,  0.0840,  ..., -0.0038,  0.0014, -0.0023],
        ...,
        [ 0.0016,  0.0007, -0.0065,  ...,  0.0128,  0.0019, -0.0053],
        [ 0.0038,  0.0048, -0.0019,  ...,  0.0014,  0.0344,  0.0011],
        [-0.0016,  0.0052, -0.0059,  ..., -0.0040,  0.0051,  0.0354]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -4.4023,  -2.1426,   5.1055,  ..., -10.8750,  -7.3750,   2.2051]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 14:53:37 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is follow, the past form is followed
If the present form is tell, the past form is told
If the present form is appear, the past form is appeared
If the present form is receive, the past form is received
If the present form is spend, the past form is spent
If the present form is seem, the past form is seemed
If the present form is involve, the past form is involved
If the present form is create, the past form is
2024-07-25 14:53:38 root INFO     [order_1_approx] starting weight calculation for If the present form is appear, the past form is appeared
If the present form is involve, the past form is involved
If the present form is seem, the past form is seemed
If the present form is receive, the past form is received
If the present form is spend, the past form is spent
If the present form is tell, the past form is told
If the present form is create, the past form is created
If the present form is follow, the past form is
2024-07-25 14:53:38 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 14:56:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.6953, -1.9453,  1.1719,  ..., -0.9414,  0.0312,  1.8906],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 5.0312, -1.5000,  7.4375,  ...,  0.5938, -1.2734, -7.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.0020e-02, -1.5259e-03,  1.5564e-03,  ...,  5.2185e-03,
         -1.5564e-03, -1.1902e-03],
        [ 2.5635e-03,  1.4404e-02,  2.9449e-03,  ..., -1.1230e-02,
         -1.3351e-05,  4.3640e-03],
        [-8.0872e-04, -7.0190e-04,  7.0312e-02,  ..., -5.1880e-03,
          8.3618e-03,  2.5482e-03],
        ...,
        [-1.3351e-04,  1.9455e-04,  8.2779e-04,  ...,  1.2085e-02,
         -8.7357e-04, -3.8147e-04],
        [ 2.7466e-03,  8.5449e-03,  8.2397e-04,  ...,  4.5776e-04,
          2.7588e-02, -5.0049e-03],
        [ 1.2741e-03,  7.3242e-03, -6.0425e-03,  ...,  1.5640e-03,
          1.3885e-03,  3.0518e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.4453, -1.2129,  6.7109,  ..., -0.3232, -1.3418, -6.7891]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 14:56:25 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is appear, the past form is appeared
If the present form is involve, the past form is involved
If the present form is seem, the past form is seemed
If the present form is receive, the past form is received
If the present form is spend, the past form is spent
If the present form is tell, the past form is told
If the present form is create, the past form is created
If the present form is follow, the past form is
2024-07-25 14:56:25 root INFO     [order_1_approx] starting weight calculation for If the present form is seem, the past form is seemed
If the present form is tell, the past form is told
If the present form is follow, the past form is followed
If the present form is appear, the past form is appeared
If the present form is receive, the past form is received
If the present form is spend, the past form is spent
If the present form is create, the past form is created
If the present form is involve, the past form is
2024-07-25 14:56:26 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 14:59:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1914, -1.7500, -0.8086,  ...,  0.0234,  0.7031,  1.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 4.0312, -3.5938, 12.2500,  ..., -8.6250,  7.3438,  1.3125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0288,  0.0036, -0.0067,  ...,  0.0061, -0.0071, -0.0078],
        [-0.0006,  0.0077,  0.0271,  ..., -0.0139,  0.0127,  0.0088],
        [-0.0014,  0.0013,  0.0962,  ..., -0.0096,  0.0146,  0.0017],
        ...,
        [ 0.0041, -0.0030, -0.0074,  ...,  0.0228, -0.0013,  0.0016],
        [ 0.0027,  0.0049,  0.0075,  ...,  0.0026,  0.0366, -0.0005],
        [-0.0038,  0.0029,  0.0047,  ..., -0.0010,  0.0018,  0.0432]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.5957, -5.3906, 10.7812,  ..., -8.8047,  7.8125,  0.5430]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 14:59:13 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is seem, the past form is seemed
If the present form is tell, the past form is told
If the present form is follow, the past form is followed
If the present form is appear, the past form is appeared
If the present form is receive, the past form is received
If the present form is spend, the past form is spent
If the present form is create, the past form is created
If the present form is involve, the past form is
2024-07-25 14:59:14 root INFO     [order_1_approx] starting weight calculation for If the present form is receive, the past form is received
If the present form is create, the past form is created
If the present form is follow, the past form is followed
If the present form is tell, the past form is told
If the present form is seem, the past form is seemed
If the present form is involve, the past form is involved
If the present form is appear, the past form is appeared
If the present form is spend, the past form is
2024-07-25 14:59:14 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 15:02:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0391, -0.1992,  0.3379,  ..., -0.0840, -0.9219,  0.1738],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([10.7500, -2.3125,  9.0625,  ...,  0.0625,  1.6719,  0.4844],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0184, -0.0005,  0.0084,  ..., -0.0023, -0.0060, -0.0032],
        [ 0.0033,  0.0171,  0.0083,  ..., -0.0089,  0.0043,  0.0007],
        [ 0.0003,  0.0040,  0.0781,  ..., -0.0023,  0.0062, -0.0042],
        ...,
        [ 0.0070, -0.0012,  0.0005,  ...,  0.0098,  0.0007, -0.0020],
        [ 0.0021,  0.0018, -0.0038,  ..., -0.0011,  0.0277,  0.0027],
        [-0.0023,  0.0013, -0.0035,  ...,  0.0021, -0.0021,  0.0369]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[11.0391, -2.9746,  9.4531,  ..., -0.9883,  1.8408,  0.4031]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 15:02:02 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is receive, the past form is received
If the present form is create, the past form is created
If the present form is follow, the past form is followed
If the present form is tell, the past form is told
If the present form is seem, the past form is seemed
If the present form is involve, the past form is involved
If the present form is appear, the past form is appeared
If the present form is spend, the past form is
2024-07-25 15:02:02 root INFO     [order_1_approx] starting weight calculation for If the present form is create, the past form is created
If the present form is tell, the past form is told
If the present form is receive, the past form is received
If the present form is spend, the past form is spent
If the present form is involve, the past form is involved
If the present form is seem, the past form is seemed
If the present form is follow, the past form is followed
If the present form is appear, the past form is
2024-07-25 15:02:02 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 15:04:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9375, -0.9375, -2.2500,  ...,  1.5000,  3.9219,  1.8125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 6.1562, -5.1875,  6.0000,  ..., -1.7031,  3.0938, -3.4062],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0216, -0.0033, -0.0042,  ...,  0.0034, -0.0026, -0.0008],
        [ 0.0045,  0.0124,  0.0154,  ..., -0.0019,  0.0011,  0.0022],
        [ 0.0036, -0.0009,  0.0737,  ..., -0.0042,  0.0105, -0.0036],
        ...,
        [ 0.0047, -0.0058, -0.0048,  ...,  0.0116, -0.0035, -0.0019],
        [ 0.0032,  0.0027,  0.0080,  ...,  0.0023,  0.0251,  0.0027],
        [-0.0040,  0.0012, -0.0067,  ..., -0.0035, -0.0011,  0.0271]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.0312, -5.0781,  6.4922,  ..., -2.6816,  2.5781, -4.3711]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 15:04:49 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is create, the past form is created
If the present form is tell, the past form is told
If the present form is receive, the past form is received
If the present form is spend, the past form is spent
If the present form is involve, the past form is involved
If the present form is seem, the past form is seemed
If the present form is follow, the past form is followed
If the present form is appear, the past form is
2024-07-25 15:04:50 root INFO     [order_1_approx] starting weight calculation for If the present form is receive, the past form is received
If the present form is appear, the past form is appeared
If the present form is follow, the past form is followed
If the present form is tell, the past form is told
If the present form is spend, the past form is spent
If the present form is involve, the past form is involved
If the present form is create, the past form is created
If the present form is seem, the past form is
2024-07-25 15:04:50 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 15:07:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3594, -0.1875, -5.2500,  ...,  0.3086,  3.3125,  1.7422],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 3.5938, -8.6250, 11.9375,  ...,  0.1953,  1.5078,  3.3438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 4.0039e-02,  1.7090e-02, -1.8799e-02,  ...,  1.3611e-02,
         -6.5918e-03, -4.9438e-03],
        [-9.1553e-05,  1.4709e-02,  1.9409e-02,  ..., -9.8877e-03,
          2.6245e-03,  3.0823e-03],
        [ 2.7771e-03, -1.3428e-02,  1.1523e-01,  ..., -1.5747e-02,
          1.4771e-02, -6.0730e-03],
        ...,
        [ 1.8311e-03, -1.0864e-02,  2.2278e-03,  ...,  1.0681e-02,
          1.5259e-04, -2.6703e-03],
        [ 5.6763e-03,  5.1880e-03,  2.2583e-03,  ...,  4.6997e-03,
          3.9551e-02,  4.3640e-03],
        [-3.2349e-03, -4.5166e-03,  8.6670e-03,  ..., -9.2773e-03,
          7.5073e-03,  4.7363e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1797, -8.2891, 13.7578,  ...,  0.2664,  1.7051,  5.1875]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 15:07:38 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is receive, the past form is received
If the present form is appear, the past form is appeared
If the present form is follow, the past form is followed
If the present form is tell, the past form is told
If the present form is spend, the past form is spent
If the present form is involve, the past form is involved
If the present form is create, the past form is created
If the present form is seem, the past form is
2024-07-25 15:07:38 root INFO     [order_1_approx] starting weight calculation for If the present form is follow, the past form is followed
If the present form is spend, the past form is spent
If the present form is create, the past form is created
If the present form is seem, the past form is seemed
If the present form is involve, the past form is involved
If the present form is receive, the past form is received
If the present form is appear, the past form is appeared
If the present form is tell, the past form is
2024-07-25 15:07:38 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 15:10:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4336,  0.9023,  0.6406,  ...,  0.4531,  0.1562, -0.7344],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([17.1250, -2.6094, 11.0000,  ..., -3.7188, -0.5000, -6.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.4170e-02,  8.9645e-04,  6.0425e-03,  ...,  4.1809e-03,
         -1.8005e-03, -7.2479e-05],
        [-8.8882e-04,  1.2695e-02,  3.4180e-03,  ..., -9.8877e-03,
         -1.8692e-03,  5.2490e-03],
        [ 6.1035e-05, -1.2207e-03,  6.5918e-02,  ..., -1.0742e-02,
         -6.1035e-05, -7.8583e-04],
        ...,
        [-1.7929e-03, -2.0294e-03, -1.2695e-02,  ...,  1.0254e-02,
         -7.2327e-03,  2.1973e-03],
        [ 2.9297e-03,  2.8076e-03, -9.7656e-03,  ...,  1.6327e-03,
          2.6489e-02,  4.3335e-03],
        [ 9.4223e-04, -1.1749e-03, -9.0942e-03,  ..., -7.1335e-04,
          6.0425e-03,  2.5024e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[16.5625, -3.1875, 12.1094,  ..., -5.1641,  0.0732, -6.0625]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 15:10:25 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is follow, the past form is followed
If the present form is spend, the past form is spent
If the present form is create, the past form is created
If the present form is seem, the past form is seemed
If the present form is involve, the past form is involved
If the present form is receive, the past form is received
If the present form is appear, the past form is appeared
If the present form is tell, the past form is
2024-07-25 15:10:26 root INFO     [order_1_approx] starting weight calculation for If the present form is spend, the past form is spent
If the present form is seem, the past form is seemed
If the present form is appear, the past form is appeared
If the present form is tell, the past form is told
If the present form is follow, the past form is followed
If the present form is involve, the past form is involved
If the present form is create, the past form is created
If the present form is receive, the past form is
2024-07-25 15:10:26 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 15:13:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0703, -0.8086,  0.3457,  ...,  1.3516,  1.6719,  2.7031],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 15.4375,  -6.2812,   1.7734,  ..., -12.0000,  -2.5000, -13.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0242, -0.0037,  0.0095,  ..., -0.0039,  0.0012,  0.0040],
        [ 0.0007,  0.0170,  0.0117,  ..., -0.0073,  0.0030,  0.0039],
        [-0.0020,  0.0048,  0.0869,  ..., -0.0051,  0.0066, -0.0008],
        ...,
        [-0.0002, -0.0007, -0.0025,  ...,  0.0154, -0.0015,  0.0012],
        [ 0.0026,  0.0041, -0.0011,  ...,  0.0029,  0.0332,  0.0031],
        [ 0.0068, -0.0002, -0.0131,  ..., -0.0022,  0.0025,  0.0410]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 14.3125,  -6.4961,   2.0449,  ..., -12.8125,  -2.4062, -13.3594]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 15:13:12 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is spend, the past form is spent
If the present form is seem, the past form is seemed
If the present form is appear, the past form is appeared
If the present form is tell, the past form is told
If the present form is follow, the past form is followed
If the present form is involve, the past form is involved
If the present form is create, the past form is created
If the present form is receive, the past form is
2024-07-25 15:13:12 root INFO     total operator prediction time: 1342.4282450675964 seconds
2024-07-25 15:13:12 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on verb_inf - 3pSg
2024-07-25 15:13:12 root INFO     building operator verb_inf - 3pSg
2024-07-25 15:13:13 root INFO     [order_1_approx] starting weight calculation for I describe, he describes
I provide, he provides
I enjoy, he enjoys
I appear, he appears
I allow, he allows
I become, he becomes
I identify, he identifies
I ask, he
2024-07-25 15:13:13 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 15:15:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3730, -1.1250,  6.4688,  ..., -0.5078,  0.7812, -2.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 21.8750, -17.8750,  10.5625,  ...,  -1.8828,  12.1875,  -0.8047],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0240,  0.0026,  0.0036,  ...,  0.0002,  0.0006,  0.0006],
        [ 0.0037,  0.0137, -0.0004,  ..., -0.0013,  0.0048,  0.0068],
        [ 0.0021,  0.0071,  0.0874,  ..., -0.0006, -0.0032, -0.0103],
        ...,
        [ 0.0004,  0.0021, -0.0116,  ...,  0.0182,  0.0016, -0.0034],
        [ 0.0061,  0.0070, -0.0083,  ..., -0.0017,  0.0295,  0.0013],
        [ 0.0012,  0.0004,  0.0014,  ..., -0.0025, -0.0023,  0.0349]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 20.2969, -15.0781,  11.1406,  ...,  -1.1543,  12.2969,  -1.1240]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 15:15:58 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for I describe, he describes
I provide, he provides
I enjoy, he enjoys
I appear, he appears
I allow, he allows
I become, he becomes
I identify, he identifies
I ask, he
2024-07-25 15:15:59 root INFO     [order_1_approx] starting weight calculation for I identify, he identifies
I provide, he provides
I allow, he allows
I ask, he asks
I appear, he appears
I become, he becomes
I enjoy, he enjoys
I describe, he
2024-07-25 15:15:59 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 15:18:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0234,  2.3281,  5.5938,  ..., -1.9531, -1.3828,  0.2812],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 23.0000, -13.5625,   8.0625,  ..., -14.5000,   2.6719,  -5.3438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.6245e-02,  2.0447e-03, -3.2043e-03,  ...,  4.8523e-03,
         -6.1951e-03,  2.4796e-05],
        [ 6.8970e-03,  1.6602e-02,  4.5471e-03,  ..., -6.8359e-03,
          8.9722e-03,  3.4790e-03],
        [ 2.2888e-03, -2.4719e-03,  9.6191e-02,  ...,  3.0060e-03,
         -6.0425e-03, -9.3384e-03],
        ...,
        [ 2.0447e-03, -1.5259e-03, -7.1716e-03,  ...,  2.2705e-02,
         -3.2349e-03, -3.9673e-03],
        [-3.7384e-04,  2.9907e-03,  5.3101e-03,  ...,  1.5945e-03,
          3.0762e-02,  3.2959e-03],
        [-2.0142e-03,  1.2436e-03, -4.2725e-03,  ...,  4.5166e-03,
         -7.8735e-03,  3.8086e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 22.6875, -12.9375,   8.3828,  ..., -15.1328,   3.1387,  -6.8203]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 15:18:45 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for I identify, he identifies
I provide, he provides
I allow, he allows
I ask, he asks
I appear, he appears
I become, he becomes
I enjoy, he enjoys
I describe, he
2024-07-25 15:18:45 root INFO     [order_1_approx] starting weight calculation for I identify, he identifies
I ask, he asks
I describe, he describes
I become, he becomes
I provide, he provides
I appear, he appears
I enjoy, he enjoys
I allow, he
2024-07-25 15:18:45 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 15:21:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6484,  0.5469,  6.7500,  ..., -1.8047,  3.7188,  1.0312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([13.6250, -9.6875,  4.5312,  ..., -7.0000,  5.3125, -7.0312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0261,  0.0012, -0.0040,  ...,  0.0096, -0.0107, -0.0051],
        [ 0.0093,  0.0229, -0.0023,  ..., -0.0168,  0.0186,  0.0100],
        [ 0.0038,  0.0017,  0.0962,  ..., -0.0114,  0.0058,  0.0023],
        ...,
        [ 0.0068,  0.0026, -0.0092,  ...,  0.0144,  0.0094,  0.0016],
        [-0.0002,  0.0017, -0.0021,  ...,  0.0020,  0.0325,  0.0025],
        [-0.0008,  0.0064,  0.0004,  ...,  0.0037, -0.0055,  0.0374]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[13.5859, -7.5625,  5.5273,  ..., -4.9414,  6.0898, -8.6250]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 15:21:33 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for I identify, he identifies
I ask, he asks
I describe, he describes
I become, he becomes
I provide, he provides
I appear, he appears
I enjoy, he enjoys
I allow, he
2024-07-25 15:21:33 root INFO     [order_1_approx] starting weight calculation for I describe, he describes
I become, he becomes
I appear, he appears
I ask, he asks
I enjoy, he enjoys
I allow, he allows
I identify, he identifies
I provide, he
2024-07-25 15:21:33 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 15:24:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.4531,  0.1289,  7.4688,  ..., -1.4844,  0.5586, -0.9297],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 22.1250, -16.3750,  11.1250,  ..., -10.1875,  11.0000,  -7.5312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.4902e-02,  7.6294e-05,  3.5706e-03,  ...,  5.6458e-03,
         -2.8992e-03, -3.6469e-03],
        [ 8.1177e-03,  1.7334e-02, -3.6926e-03,  ..., -1.0834e-03,
          4.3030e-03,  8.8501e-03],
        [ 7.1411e-03, -2.6245e-03,  7.9590e-02,  ..., -2.5940e-03,
          1.1368e-03, -8.7891e-03],
        ...,
        [ 1.1978e-03,  1.4114e-03, -1.0132e-02,  ...,  1.9165e-02,
         -2.2888e-03, -5.1880e-04],
        [ 3.6621e-03,  3.4485e-03,  6.1035e-05,  ...,  1.4801e-03,
          2.4902e-02,  2.0599e-03],
        [ 1.1826e-03,  3.8910e-03, -1.4648e-03,  ...,  3.8147e-04,
         -4.9438e-03,  3.4424e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 20.6094, -14.8750,  11.8281,  ...,  -9.8047,  11.9844,  -8.6875]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 15:24:17 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for I describe, he describes
I become, he becomes
I appear, he appears
I ask, he asks
I enjoy, he enjoys
I allow, he allows
I identify, he identifies
I provide, he
2024-07-25 15:24:17 root INFO     [order_1_approx] starting weight calculation for I provide, he provides
I allow, he allows
I identify, he identifies
I become, he becomes
I ask, he asks
I enjoy, he enjoys
I describe, he describes
I appear, he
2024-07-25 15:24:17 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 15:27:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9141,  1.7656, -0.3457,  ..., -0.3398,  0.9297,  0.9453],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 21.7500, -11.8750,   2.3594,  ..., -10.0000,  11.0000,  -5.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0225, -0.0009, -0.0159,  ...,  0.0178, -0.0125,  0.0044],
        [ 0.0144,  0.0115,  0.0131,  ..., -0.0229,  0.0145, -0.0085],
        [ 0.0083,  0.0003,  0.0889,  ..., -0.0078,  0.0096, -0.0029],
        ...,
        [ 0.0059,  0.0010, -0.0028,  ...,  0.0063,  0.0061, -0.0067],
        [ 0.0015,  0.0039,  0.0057,  ...,  0.0023,  0.0249,  0.0020],
        [-0.0018,  0.0033, -0.0022,  ..., -0.0016, -0.0045,  0.0303]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[19.2188, -7.7539,  5.2500,  ..., -9.2656, 10.4688, -6.7070]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 15:27:04 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for I provide, he provides
I allow, he allows
I identify, he identifies
I become, he becomes
I ask, he asks
I enjoy, he enjoys
I describe, he describes
I appear, he
2024-07-25 15:27:04 root INFO     [order_1_approx] starting weight calculation for I provide, he provides
I describe, he describes
I allow, he allows
I appear, he appears
I ask, he asks
I identify, he identifies
I become, he becomes
I enjoy, he
2024-07-25 15:27:04 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 15:29:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3086, -0.9922,  9.2500,  ..., -1.6875, -0.4551,  0.6094],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 18.0000, -12.3750,  13.8750,  ...,  -5.5000,   8.5000,  -2.7031],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0206, -0.0009,  0.0063,  ...,  0.0176, -0.0083,  0.0060],
        [ 0.0125,  0.0253, -0.0123,  ..., -0.0255,  0.0171, -0.0092],
        [ 0.0050,  0.0027,  0.0840,  ..., -0.0058, -0.0004, -0.0005],
        ...,
        [ 0.0015, -0.0005, -0.0132,  ...,  0.0249, -0.0070, -0.0022],
        [ 0.0057,  0.0036, -0.0031,  ..., -0.0115,  0.0315,  0.0006],
        [-0.0032,  0.0029, -0.0048,  ...,  0.0078, -0.0022,  0.0317]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[15.0156, -8.1953, 15.1562,  ..., -4.1289, 10.0938, -4.7656]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 15:29:51 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for I provide, he provides
I describe, he describes
I allow, he allows
I appear, he appears
I ask, he asks
I identify, he identifies
I become, he becomes
I enjoy, he
2024-07-25 15:29:52 root INFO     [order_1_approx] starting weight calculation for I ask, he asks
I allow, he allows
I appear, he appears
I describe, he describes
I become, he becomes
I enjoy, he enjoys
I provide, he provides
I identify, he
2024-07-25 15:29:52 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 15:32:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9922,  1.9531,  4.7500,  ..., -1.9297, -1.8438,  0.4844],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([12.7500, -9.4375, 16.8750,  ..., -5.1875, 10.7500, -2.4531],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0244,  0.0052, -0.0026,  ..., -0.0018,  0.0030, -0.0037],
        [ 0.0054,  0.0225, -0.0020,  ..., -0.0003,  0.0089, -0.0023],
        [ 0.0047, -0.0016,  0.0942,  ..., -0.0005,  0.0013, -0.0023],
        ...,
        [ 0.0041,  0.0002, -0.0049,  ...,  0.0205, -0.0006, -0.0001],
        [ 0.0037,  0.0062,  0.0057,  ..., -0.0009,  0.0282,  0.0044],
        [-0.0009,  0.0028, -0.0114,  ...,  0.0036, -0.0013,  0.0320]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[11.6406, -9.0234, 18.1250,  ..., -4.6875, 11.4844, -3.8438]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 15:32:39 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for I ask, he asks
I allow, he allows
I appear, he appears
I describe, he describes
I become, he becomes
I enjoy, he enjoys
I provide, he provides
I identify, he
2024-07-25 15:32:39 root INFO     [order_1_approx] starting weight calculation for I enjoy, he enjoys
I ask, he asks
I allow, he allows
I appear, he appears
I provide, he provides
I describe, he describes
I identify, he identifies
I become, he
2024-07-25 15:32:39 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 15:35:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.0000,  1.6875, -0.5547,  ..., -0.6289,  1.2656,  0.1055],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([21.8750, -6.5312, -0.2656,  ..., -6.1875,  8.5000,  2.3125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.6733e-02,  9.1553e-05,  7.9346e-03,  ...,  9.3079e-04,
         -4.9438e-03, -4.7112e-04],
        [ 7.4463e-03,  1.3794e-02,  3.9673e-03,  ..., -2.7924e-03,
          9.8877e-03,  1.1978e-03],
        [ 1.1215e-03, -1.0910e-03,  8.6914e-02,  ..., -7.4387e-04,
         -2.6855e-03, -3.2043e-03],
        ...,
        [ 2.1973e-03,  2.9602e-03, -1.5442e-02,  ...,  2.1240e-02,
          3.4027e-03, -6.3477e-03],
        [ 1.9073e-03,  3.7537e-03,  9.6130e-04,  ..., -2.2736e-03,
          2.8564e-02,  3.4180e-03],
        [-4.6539e-04,  7.4463e-03, -6.9275e-03,  ...,  5.2490e-03,
         -3.1128e-03,  3.5889e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[22.1094, -5.6289,  1.1611,  ..., -5.5156,  9.5156,  1.9873]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 15:35:26 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for I enjoy, he enjoys
I ask, he asks
I allow, he allows
I appear, he appears
I provide, he provides
I describe, he describes
I identify, he identifies
I become, he
2024-07-25 15:35:26 root INFO     total operator prediction time: 1333.3514137268066 seconds
2024-07-25 15:35:26 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on verb_Ving - 3pSg
2024-07-25 15:35:26 root INFO     building operator verb_Ving - 3pSg
2024-07-25 15:35:26 root INFO     [order_1_approx] starting weight calculation for When something is depending, it depends
When something is publishing, it publishes
When something is receiving, it receives
When something is telling, it tells
When something is learning, it learns
When something is applying, it applies
When something is becoming, it becomes
When something is understanding, it
2024-07-25 15:35:26 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 15:38:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0547, -1.2656,  8.3750,  ..., -0.1641, -0.7500,  0.5938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 19.0000, -10.8125,  16.1250,  ..., -13.4375,   0.9219,  -2.3438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.8809e-02, -4.4250e-04, -6.5308e-03,  ...,  2.0752e-03,
          9.2773e-03, -5.2490e-03],
        [ 5.7373e-03,  1.9409e-02,  2.2278e-03,  ...,  1.5411e-03,
          1.2665e-03, -4.1504e-03],
        [-1.5259e-05, -2.8687e-03,  9.5703e-02,  ...,  6.7139e-04,
         -1.9226e-03, -1.2970e-04],
        ...,
        [ 8.3923e-04,  5.4321e-03, -1.5030e-03,  ...,  2.3926e-02,
          5.2490e-03, -2.0447e-03],
        [ 2.7924e-03,  6.1035e-03, -7.5684e-03,  ..., -4.5166e-03,
          3.8574e-02, -1.0681e-03],
        [-4.1199e-03, -2.5940e-03, -7.3547e-03,  ...,  2.6550e-03,
         -5.9204e-03,  3.3447e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 19.6875,  -9.8281,  16.0156,  ..., -12.8984,  -0.1660,  -2.7188]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 15:38:14 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When something is depending, it depends
When something is publishing, it publishes
When something is receiving, it receives
When something is telling, it tells
When something is learning, it learns
When something is applying, it applies
When something is becoming, it becomes
When something is understanding, it
2024-07-25 15:38:15 root INFO     [order_1_approx] starting weight calculation for When something is applying, it applies
When something is telling, it tells
When something is learning, it learns
When something is becoming, it becomes
When something is understanding, it understands
When something is receiving, it receives
When something is depending, it depends
When something is publishing, it
2024-07-25 15:38:15 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 15:41:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2656,  0.5430,  1.3281,  ...,  2.2031, -2.2969,  2.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 23.5000,   4.0625,   9.2500,  ..., -10.0000,   6.4062,  -8.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.4180e-02, -8.2397e-04,  4.5776e-05,  ...,  1.1520e-03,
          2.5177e-03, -2.5177e-03],
        [ 3.2654e-03,  2.1118e-02, -3.2959e-03,  ..., -4.7913e-03,
          5.9509e-04,  1.7090e-03],
        [ 4.2114e-03, -1.1673e-03,  1.0107e-01,  ...,  5.0659e-03,
         -6.4087e-04, -5.9509e-03],
        ...,
        [ 4.3640e-03,  1.0300e-04, -1.4282e-02,  ...,  2.1729e-02,
          2.5330e-03, -1.5335e-03],
        [ 2.1210e-03,  3.8300e-03, -3.8300e-03,  ..., -2.7771e-03,
          3.7598e-02, -3.8147e-03],
        [-3.4790e-03,  1.1520e-03, -9.3994e-03,  ...,  1.3885e-03,
         -6.5918e-03,  3.9795e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[23.3750,  4.0898,  9.1641,  ..., -9.9766,  7.8281, -8.4609]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 15:41:04 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When something is applying, it applies
When something is telling, it tells
When something is learning, it learns
When something is becoming, it becomes
When something is understanding, it understands
When something is receiving, it receives
When something is depending, it depends
When something is publishing, it
2024-07-25 15:41:04 root INFO     [order_1_approx] starting weight calculation for When something is receiving, it receives
When something is depending, it depends
When something is learning, it learns
When something is understanding, it understands
When something is becoming, it becomes
When something is applying, it applies
When something is publishing, it publishes
When something is telling, it
2024-07-25 15:41:04 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 15:43:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.8438,  0.7578,  2.6875,  ...,  1.5469, -0.2695, -1.5703],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 24.0000,  -1.0000,  12.5000,  ..., -12.1250,   3.6875,  -7.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0280,  0.0008, -0.0014,  ...,  0.0019,  0.0039,  0.0003],
        [ 0.0014,  0.0183, -0.0074,  ..., -0.0004, -0.0038,  0.0038],
        [ 0.0047, -0.0035,  0.1021,  ...,  0.0032, -0.0003,  0.0014],
        ...,
        [ 0.0045, -0.0015, -0.0099,  ...,  0.0216,  0.0057,  0.0003],
        [ 0.0052,  0.0041, -0.0080,  ...,  0.0018,  0.0393,  0.0005],
        [ 0.0005,  0.0030, -0.0025,  ...,  0.0059, -0.0057,  0.0342]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 26.0938,  -1.0205,  13.0391,  ..., -12.1250,   3.9551,  -8.4297]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 15:43:53 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When something is receiving, it receives
When something is depending, it depends
When something is learning, it learns
When something is understanding, it understands
When something is becoming, it becomes
When something is applying, it applies
When something is publishing, it publishes
When something is telling, it
2024-07-25 15:43:53 root INFO     [order_1_approx] starting weight calculation for When something is learning, it learns
When something is becoming, it becomes
When something is publishing, it publishes
When something is receiving, it receives
When something is telling, it tells
When something is depending, it depends
When something is understanding, it understands
When something is applying, it
2024-07-25 15:43:54 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 15:46:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.2812, -3.1094,  7.0625,  ..., -1.6719, -0.4805,  1.5156],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([23.6250, -6.7188, 20.6250,  ..., -6.9688,  3.2188,  1.0234],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.9907e-02, -3.1128e-03, -5.6458e-03,  ..., -1.8311e-03,
          3.2501e-03, -5.1498e-04],
        [-5.5695e-04,  2.0630e-02, -3.0670e-03,  ..., -3.8910e-03,
         -5.7983e-04,  1.7242e-03],
        [ 1.6098e-03, -1.2054e-03,  8.7402e-02,  ..., -1.0071e-03,
         -4.8218e-03, -1.6098e-03],
        ...,
        [ 3.4485e-03,  2.0447e-03, -7.9346e-03,  ...,  2.1729e-02,
          7.2327e-03,  1.7242e-03],
        [ 2.0142e-03, -2.8992e-04, -6.8970e-03,  ..., -5.2643e-04,
          3.3691e-02, -2.3193e-03],
        [-8.7738e-04, -9.9182e-05,  1.6785e-03,  ...,  6.4392e-03,
         -4.0588e-03,  3.6621e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[24.0938, -6.1172, 21.6094,  ..., -6.7344,  2.7949, -0.4463]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 15:46:43 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When something is learning, it learns
When something is becoming, it becomes
When something is publishing, it publishes
When something is receiving, it receives
When something is telling, it tells
When something is depending, it depends
When something is understanding, it understands
When something is applying, it
2024-07-25 15:46:43 root INFO     [order_1_approx] starting weight calculation for When something is applying, it applies
When something is becoming, it becomes
When something is depending, it depends
When something is understanding, it understands
When something is publishing, it publishes
When something is receiving, it receives
When something is telling, it tells
When something is learning, it
2024-07-25 15:46:44 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 15:49:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-3.6094, -1.2500,  8.5000,  ..., -0.0234, -1.8281,  1.5156],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 22.5000, -13.2500,  14.0625,  ..., -16.6250,  -3.8438,  -0.9844],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0250, -0.0019,  0.0028,  ...,  0.0070,  0.0087, -0.0034],
        [ 0.0027,  0.0131, -0.0105,  ..., -0.0121, -0.0031,  0.0017],
        [-0.0014, -0.0021,  0.0859,  ...,  0.0055, -0.0050,  0.0021],
        ...,
        [ 0.0011,  0.0031,  0.0013,  ...,  0.0208,  0.0062, -0.0020],
        [ 0.0023,  0.0043, -0.0078,  ..., -0.0025,  0.0311, -0.0015],
        [-0.0035, -0.0017,  0.0092,  ...,  0.0068, -0.0021,  0.0293]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 24.4375, -14.0938,  14.3125,  ..., -14.9609,  -3.3379,  -0.4937]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 15:49:33 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When something is applying, it applies
When something is becoming, it becomes
When something is depending, it depends
When something is understanding, it understands
When something is publishing, it publishes
When something is receiving, it receives
When something is telling, it tells
When something is learning, it
2024-07-25 15:49:34 root INFO     [order_1_approx] starting weight calculation for When something is learning, it learns
When something is publishing, it publishes
When something is understanding, it understands
When something is becoming, it becomes
When something is depending, it depends
When something is applying, it applies
When something is telling, it tells
When something is receiving, it
2024-07-25 15:49:34 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 15:52:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.9375, -1.6406,  3.3281,  ...,  1.1250,  1.5781,  0.4590],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 24.6250,  -2.0938,   6.5625,  ..., -17.2500,   7.1250, -11.8125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.2339e-02, -2.1362e-03, -2.6855e-03,  ..., -3.5706e-03,
          8.5449e-03,  5.1880e-04],
        [ 1.1749e-03,  1.4648e-02, -8.3160e-04,  ..., -3.8605e-03,
          8.1635e-04,  9.7656e-04],
        [ 2.0142e-03,  1.1292e-03,  7.9102e-02,  ...,  2.6245e-03,
         -9.7275e-04, -3.2043e-04],
        ...,
        [-8.0109e-05,  3.3722e-03, -9.3842e-04,  ...,  1.7700e-02,
          8.6975e-04,  2.0752e-03],
        [ 4.6387e-03,  1.9531e-03, -5.3101e-03,  ..., -1.3580e-03,
          2.9785e-02,  3.1128e-03],
        [-3.4332e-03, -2.3193e-03, -2.6398e-03,  ...,  2.4261e-03,
         -5.4932e-04,  3.3691e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 25.8906,  -1.7109,   6.5977,  ..., -16.2969,   7.6016, -12.7344]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 15:52:24 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When something is learning, it learns
When something is publishing, it publishes
When something is understanding, it understands
When something is becoming, it becomes
When something is depending, it depends
When something is applying, it applies
When something is telling, it tells
When something is receiving, it
2024-07-25 15:52:24 root INFO     [order_1_approx] starting weight calculation for When something is applying, it applies
When something is telling, it tells
When something is understanding, it understands
When something is publishing, it publishes
When something is depending, it depends
When something is receiving, it receives
When something is learning, it learns
When something is becoming, it
2024-07-25 15:52:24 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 15:55:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.6875,  1.9844, -1.6016,  ...,  0.2969,  1.8516, -0.3203],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 1.6125e+01,  1.5625e-02,  6.9062e+00,  ..., -8.1875e+00,
         1.1406e+00,  6.8125e+00], device='cuda:0', dtype=torch.float16,
       grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.4912e-02,  2.6855e-03,  7.9956e-03,  ...,  3.2959e-03,
         -2.5940e-03, -1.2665e-03],
        [-3.7384e-03,  1.7700e-02,  3.0823e-03,  ..., -7.8583e-04,
          6.4850e-05, -4.2114e-03],
        [ 3.3264e-03,  2.0599e-03,  9.8633e-02,  ...,  2.1973e-03,
         -5.9204e-03,  4.6387e-03],
        ...,
        [-1.0376e-02, -7.4463e-03, -1.2329e-02,  ...,  2.7100e-02,
          9.0027e-04, -3.4180e-03],
        [ 7.4463e-03,  6.1951e-03,  7.9346e-03,  ..., -2.7466e-04,
          4.3701e-02,  2.5940e-03],
        [-1.0864e-02, -1.2283e-03, -1.0071e-03,  ...,  3.2043e-03,
         -5.0049e-03,  4.1260e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[17.6094, -0.2297,  7.2812,  ..., -7.2812,  1.4609,  6.5000]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 15:55:14 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When something is applying, it applies
When something is telling, it tells
When something is understanding, it understands
When something is publishing, it publishes
When something is depending, it depends
When something is receiving, it receives
When something is learning, it learns
When something is becoming, it
2024-07-25 15:55:14 root INFO     [order_1_approx] starting weight calculation for When something is learning, it learns
When something is telling, it tells
When something is understanding, it understands
When something is publishing, it publishes
When something is receiving, it receives
When something is applying, it applies
When something is becoming, it becomes
When something is depending, it
2024-07-25 15:55:14 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 15:58:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.8438,  2.2812,  3.2188,  ..., -1.5625, -0.4727, -0.7539],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 18.5000,  -1.8438,  16.1250,  ..., -22.8750,   8.6250,   0.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.4180e-02,  3.8910e-03, -6.1951e-03,  ..., -2.4414e-03,
          1.9836e-03, -6.5918e-03],
        [ 1.0254e-02,  2.2339e-02, -7.0190e-04,  ...,  8.7738e-04,
          1.5259e-05, -1.0300e-03],
        [ 1.6403e-03, -3.0975e-03,  1.2109e-01,  ..., -4.5166e-03,
         -6.3324e-04,  6.8359e-03],
        ...,
        [ 7.3853e-03,  2.6398e-03, -1.0315e-02,  ...,  2.9053e-02,
          2.4872e-03, -3.1128e-03],
        [ 6.4850e-04,  1.6556e-03, -7.9956e-03,  ...,  9.1553e-05,
          4.2725e-02, -1.4191e-03],
        [-2.1057e-03,  2.8687e-03, -8.6060e-03,  ...,  9.5215e-03,
         -9.7046e-03,  4.0283e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 18.3125,  -1.1152,  18.3906,  ..., -21.7656,   9.9922,  -1.1768]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 15:58:04 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When something is learning, it learns
When something is telling, it tells
When something is understanding, it understands
When something is publishing, it publishes
When something is receiving, it receives
When something is applying, it applies
When something is becoming, it becomes
When something is depending, it
2024-07-25 15:58:04 root INFO     total operator prediction time: 1357.9356729984283 seconds
2024-07-25 15:58:04 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on noun - plural_reg
2024-07-25 15:58:04 root INFO     building operator noun - plural_reg
2024-07-25 15:58:04 root INFO     [order_1_approx] starting weight calculation for The plural form of user is users
The plural form of website is websites
The plural form of council is councils
The plural form of college is colleges
The plural form of friend is friends
The plural form of night is nights
The plural form of month is months
The plural form of song is
2024-07-25 15:58:04 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 16:00:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2188, -1.1250, -0.0488,  ...,  0.0703, -1.7891,  0.2090],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([15.2500, -6.3125,  6.1562,  ..., -0.1172,  6.1250, -5.1562],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0093, -0.0175,  0.0854,  ..., -0.0206, -0.0073,  0.0007],
        [ 0.0147,  0.0276, -0.0464,  ...,  0.0095,  0.0028,  0.0017],
        [ 0.0155,  0.0145,  0.0284,  ...,  0.0111,  0.0076, -0.0007],
        ...,
        [-0.0025, -0.0075,  0.0212,  ...,  0.0188, -0.0063, -0.0043],
        [-0.0051,  0.0030,  0.0181,  ..., -0.0005,  0.0315, -0.0056],
        [ 0.0036, -0.0001, -0.0102,  ..., -0.0023,  0.0016,  0.0312]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 9.9219, -3.4883,  9.3906,  ..., -1.9561,  7.0703, -4.6914]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 16:00:53 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of user is users
The plural form of website is websites
The plural form of council is councils
The plural form of college is colleges
The plural form of friend is friends
The plural form of night is nights
The plural form of month is months
The plural form of song is
2024-07-25 16:00:54 root INFO     [order_1_approx] starting weight calculation for The plural form of night is nights
The plural form of council is councils
The plural form of friend is friends
The plural form of user is users
The plural form of song is songs
The plural form of month is months
The plural form of website is websites
The plural form of college is
2024-07-25 16:00:54 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 16:03:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7109, -0.9609, -0.4922,  ..., -1.0078, -2.1719, -1.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 4.4375, -4.2188, -8.9375,  ..., -8.1250, -2.7031, -4.7188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0291,  0.0022,  0.0064,  ...,  0.0044,  0.0055,  0.0036],
        [ 0.0053,  0.0153,  0.0056,  ..., -0.0039, -0.0010, -0.0012],
        [ 0.0051,  0.0005,  0.0908,  ..., -0.0019, -0.0015,  0.0020],
        ...,
        [ 0.0040, -0.0005, -0.0063,  ...,  0.0262, -0.0042,  0.0033],
        [ 0.0005,  0.0033,  0.0020,  ...,  0.0019,  0.0305, -0.0005],
        [-0.0027,  0.0008, -0.0121,  ..., -0.0023,  0.0018,  0.0366]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  4.6680,  -3.3516, -10.8828,  ...,  -8.4219,  -0.3789,  -5.2812]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 16:03:43 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of night is nights
The plural form of council is councils
The plural form of friend is friends
The plural form of user is users
The plural form of song is songs
The plural form of month is months
The plural form of website is websites
The plural form of college is
2024-07-25 16:03:43 root INFO     [order_1_approx] starting weight calculation for The plural form of website is websites
The plural form of council is councils
The plural form of song is songs
The plural form of friend is friends
The plural form of user is users
The plural form of college is colleges
The plural form of month is months
The plural form of night is
2024-07-25 16:03:43 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 16:06:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1328, -0.6016,  2.3750,  ..., -0.4219,  1.4609, -2.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  9.4375,  -2.8750,  -1.8281,  ...,   3.5156,   0.3906, -11.3125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0303, -0.0007, -0.0138,  ...,  0.0103,  0.0049, -0.0056],
        [ 0.0008,  0.0143,  0.0160,  ..., -0.0052, -0.0086,  0.0051],
        [ 0.0022, -0.0002,  0.0957,  ..., -0.0072,  0.0065,  0.0058],
        ...,
        [ 0.0031, -0.0022, -0.0115,  ...,  0.0242,  0.0003, -0.0052],
        [ 0.0029,  0.0053, -0.0102,  ...,  0.0019,  0.0322, -0.0016],
        [-0.0027, -0.0015, -0.0043,  ..., -0.0015,  0.0071,  0.0322]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 10.0859,  -3.8184,  -3.9199,  ...,   4.1328,   2.2188, -10.6484]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 16:06:33 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of website is websites
The plural form of council is councils
The plural form of song is songs
The plural form of friend is friends
The plural form of user is users
The plural form of college is colleges
The plural form of month is months
The plural form of night is
2024-07-25 16:06:33 root INFO     [order_1_approx] starting weight calculation for The plural form of user is users
The plural form of song is songs
The plural form of college is colleges
The plural form of night is nights
The plural form of council is councils
The plural form of website is websites
The plural form of friend is friends
The plural form of month is
2024-07-25 16:06:33 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 16:09:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2812, -2.5625,  1.3125,  ..., -2.2188, -0.2520, -0.6914],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([10.0625, -7.5312, -6.0625,  ..., -2.6406, 11.1250,  5.9062],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0293, -0.0010, -0.0019,  ...,  0.0099,  0.0103, -0.0043],
        [ 0.0099,  0.0206, -0.0099,  ...,  0.0045, -0.0044,  0.0007],
        [ 0.0044,  0.0028,  0.0923,  ...,  0.0023,  0.0052,  0.0000],
        ...,
        [ 0.0004, -0.0068, -0.0020,  ...,  0.0153, -0.0050,  0.0036],
        [-0.0031,  0.0052,  0.0056,  ...,  0.0031,  0.0391, -0.0003],
        [-0.0008, -0.0006, -0.0067,  ..., -0.0008,  0.0051,  0.0391]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 8.6797, -6.1641, -6.8086,  ..., -1.4141, 12.5938,  5.2422]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 16:09:22 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of user is users
The plural form of song is songs
The plural form of college is colleges
The plural form of night is nights
The plural form of council is councils
The plural form of website is websites
The plural form of friend is friends
The plural form of month is
2024-07-25 16:09:23 root INFO     [order_1_approx] starting weight calculation for The plural form of month is months
The plural form of night is nights
The plural form of council is councils
The plural form of website is websites
The plural form of friend is friends
The plural form of song is songs
The plural form of college is colleges
The plural form of user is
2024-07-25 16:09:23 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 16:12:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.2812, -1.8438,  3.0156,  ..., -0.4141,  1.9844, -1.7188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 5.4375, -2.8125, -7.2500,  ..., -2.8438,  8.1250, -7.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0427,  0.0118, -0.0361,  ...,  0.0131,  0.0032,  0.0046],
        [-0.0002,  0.0114,  0.0289,  ..., -0.0038, -0.0015,  0.0127],
        [-0.0039, -0.0032,  0.1113,  ..., -0.0052,  0.0057, -0.0034],
        ...,
        [ 0.0020, -0.0042,  0.0012,  ...,  0.0237, -0.0107, -0.0066],
        [ 0.0016,  0.0037, -0.0011,  ...,  0.0013,  0.0281, -0.0009],
        [-0.0021, -0.0028, -0.0129,  ..., -0.0021,  0.0024,  0.0299]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 7.2227, -3.7402, -8.9375,  ..., -2.6680,  8.0312, -5.8164]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 16:12:12 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of month is months
The plural form of night is nights
The plural form of council is councils
The plural form of website is websites
The plural form of friend is friends
The plural form of song is songs
The plural form of college is colleges
The plural form of user is
2024-07-25 16:12:12 root INFO     [order_1_approx] starting weight calculation for The plural form of friend is friends
The plural form of college is colleges
The plural form of month is months
The plural form of user is users
The plural form of council is councils
The plural form of night is nights
The plural form of song is songs
The plural form of website is
2024-07-25 16:12:12 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 16:15:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.8594, -1.0781, -3.9844,  ..., -0.3242, -1.2500, -1.9531],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  0.6406,   3.9688, -14.8750,  ...,  -4.1250,  11.0000,  -7.6562],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0374,  0.0033, -0.0116,  ...,  0.0075,  0.0038,  0.0024],
        [ 0.0054,  0.0200,  0.0071,  ..., -0.0049, -0.0015,  0.0086],
        [ 0.0037,  0.0003,  0.1021,  ...,  0.0036,  0.0077, -0.0091],
        ...,
        [ 0.0065, -0.0022,  0.0002,  ...,  0.0237, -0.0012, -0.0067],
        [-0.0079,  0.0033,  0.0049,  ...,  0.0006,  0.0410, -0.0038],
        [-0.0026, -0.0078, -0.0024,  ..., -0.0040,  0.0016,  0.0444]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  0.4736,   2.5078, -14.2266,  ...,  -4.2891,  12.3047,  -7.5547]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 16:15:02 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of friend is friends
The plural form of college is colleges
The plural form of month is months
The plural form of user is users
The plural form of council is councils
The plural form of night is nights
The plural form of song is songs
The plural form of website is
2024-07-25 16:15:02 root INFO     [order_1_approx] starting weight calculation for The plural form of night is nights
The plural form of college is colleges
The plural form of month is months
The plural form of user is users
The plural form of song is songs
The plural form of website is websites
The plural form of friend is friends
The plural form of council is
2024-07-25 16:15:02 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 16:17:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2812, -1.1406, -3.5938,  ..., -0.0977, -1.5781, -0.9062],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -4.0625,  -7.0938, -17.6250,  ...,  -3.8125,  -1.2422,  -7.4062],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.3203e-02,  4.7607e-03, -2.4414e-03,  ...,  5.1880e-03,
         -3.1586e-03, -2.8381e-03],
        [ 1.0742e-02,  2.0386e-02,  5.4016e-03,  ..., -4.1199e-04,
         -1.9531e-03,  2.0447e-03],
        [ 6.4850e-04,  3.5858e-04,  1.0352e-01,  ..., -1.2054e-03,
          1.0010e-02, -4.1199e-04],
        ...,
        [ 1.0910e-03, -6.2866e-03,  1.1536e-02,  ...,  2.0874e-02,
         -7.2327e-03, -5.6458e-03],
        [-5.6839e-04,  5.0354e-04,  7.6294e-06,  ...,  4.8828e-04,
          4.2969e-02, -4.2114e-03],
        [-3.8300e-03, -2.1210e-03, -1.1658e-02,  ..., -1.2589e-03,
          3.9673e-03,  4.4434e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -5.5859,  -8.2656, -17.5625,  ...,  -4.6719,  -0.0986,  -8.5156]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 16:17:51 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of night is nights
The plural form of college is colleges
The plural form of month is months
The plural form of user is users
The plural form of song is songs
The plural form of website is websites
The plural form of friend is friends
The plural form of council is
2024-07-25 16:17:52 root INFO     [order_1_approx] starting weight calculation for The plural form of council is councils
The plural form of song is songs
The plural form of night is nights
The plural form of website is websites
The plural form of month is months
The plural form of user is users
The plural form of college is colleges
The plural form of friend is
2024-07-25 16:17:52 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 16:20:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.3438, -1.5625, -0.6328,  ..., -1.0156, -0.7969,  0.8477],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 13.5625, -11.3750,   0.1211,  ...,  -1.2656,   4.8125,  -4.9375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0276,  0.0010,  0.0058,  ...,  0.0040, -0.0045,  0.0018],
        [ 0.0020,  0.0171, -0.0034,  ...,  0.0011, -0.0036,  0.0019],
        [ 0.0050,  0.0023,  0.0811,  ...,  0.0037,  0.0029, -0.0014],
        ...,
        [ 0.0011, -0.0052, -0.0060,  ...,  0.0188, -0.0082, -0.0009],
        [ 0.0006,  0.0032, -0.0004,  ...,  0.0002,  0.0330,  0.0001],
        [-0.0006, -0.0023, -0.0085,  ..., -0.0029,  0.0062,  0.0337]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 12.3281, -10.6797,  -0.5078,  ...,  -0.9043,   5.0430,  -4.3867]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 16:20:41 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of council is councils
The plural form of song is songs
The plural form of night is nights
The plural form of website is websites
The plural form of month is months
The plural form of user is users
The plural form of college is colleges
The plural form of friend is
2024-07-25 16:20:41 root INFO     total operator prediction time: 1357.167504787445 seconds
2024-07-25 16:20:41 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on verb_3pSg - Ved
2024-07-25 16:20:41 root INFO     building operator verb_3pSg - Ved
2024-07-25 16:20:41 root INFO     [order_1_approx] starting weight calculation for When he provides something, something has been provided
When he receives something, something has been received
When he operates something, something has been operated
When he adds something, something has been added
When he considers something, something has been considered
When he suggests something, something has been suggested
When he relates something, something has been related
When he manages something, something has been
2024-07-25 16:20:41 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 16:23:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3125,  1.0078,  3.5625,  ..., -0.6719, -0.6875,  0.4727],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 15.3750, -15.0625,  -2.3594,  ..., -21.2500,  -5.6875,  -4.4688],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.8564e-02, -2.3193e-03,  5.0049e-03,  ...,  1.2512e-03,
         -7.8125e-03, -5.7373e-03],
        [-2.1362e-04,  2.3315e-02, -5.0964e-03,  ..., -1.0254e-02,
          1.9287e-02,  1.7822e-02],
        [ 9.9182e-04,  4.3640e-03,  7.5195e-02,  ..., -2.7466e-04,
          2.1210e-03,  7.6294e-06],
        ...,
        [ 2.5330e-03, -1.1520e-03, -9.1171e-04,  ...,  1.7822e-02,
         -1.8692e-03, -3.4180e-03],
        [ 2.9144e-03,  4.4556e-03, -6.0730e-03,  ..., -3.2654e-03,
          3.8330e-02,  3.0518e-03],
        [ 3.3722e-03,  6.3324e-04, -2.5940e-03,  ...,  4.3945e-03,
         -1.1414e-02,  2.4902e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 14.6328, -11.7031,  -2.0273,  ..., -22.2656,  -4.1875,  -5.9688]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 16:23:28 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When he provides something, something has been provided
When he receives something, something has been received
When he operates something, something has been operated
When he adds something, something has been added
When he considers something, something has been considered
When he suggests something, something has been suggested
When he relates something, something has been related
When he manages something, something has been
2024-07-25 16:23:28 root INFO     [order_1_approx] starting weight calculation for When he considers something, something has been considered
When he provides something, something has been provided
When he manages something, something has been managed
When he operates something, something has been operated
When he receives something, something has been received
When he suggests something, something has been suggested
When he adds something, something has been added
When he relates something, something has been
2024-07-25 16:23:28 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 16:26:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0391, -1.6406,  2.5312,  ..., -1.3750, -0.8828,  0.8633],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 16.6250, -16.7500,  -7.4062,  ..., -16.3750,   5.6250,  -1.7188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0249,  0.0066, -0.0095,  ..., -0.0005,  0.0004, -0.0094],
        [ 0.0029,  0.0097, -0.0091,  ..., -0.0008, -0.0008,  0.0007],
        [ 0.0011, -0.0079,  0.0718,  ...,  0.0013, -0.0096, -0.0007],
        ...,
        [ 0.0038, -0.0033, -0.0139,  ...,  0.0199,  0.0034,  0.0035],
        [ 0.0042,  0.0057, -0.0063,  ..., -0.0013,  0.0317,  0.0020],
        [ 0.0040,  0.0061, -0.0085,  ...,  0.0032, -0.0041,  0.0366]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 14.6562, -16.6406,  -6.1328,  ..., -15.9141,   6.9258,  -2.8184]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 16:26:14 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When he considers something, something has been considered
When he provides something, something has been provided
When he manages something, something has been managed
When he operates something, something has been operated
When he receives something, something has been received
When he suggests something, something has been suggested
When he adds something, something has been added
When he relates something, something has been
2024-07-25 16:26:15 root INFO     [order_1_approx] starting weight calculation for When he provides something, something has been provided
When he manages something, something has been managed
When he relates something, something has been related
When he suggests something, something has been suggested
When he receives something, something has been received
When he operates something, something has been operated
When he adds something, something has been added
When he considers something, something has been
2024-07-25 16:26:15 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 16:29:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7148,  0.7344,  6.0000,  ..., -1.7500, -0.2441,  0.9453],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  6.6250, -16.5000,  -0.7734,  ..., -12.4375,   5.3750,  -5.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.2217e-02,  5.8594e-03, -4.1199e-03,  ..., -5.2795e-03,
         -1.0376e-03, -1.0254e-02],
        [ 7.9346e-03,  6.8359e-03, -6.4392e-03,  ...,  5.7983e-03,
          2.9602e-03,  4.0894e-03],
        [ 3.0670e-03,  1.1444e-03,  6.5430e-02,  ...,  9.1553e-05,
         -2.2583e-03, -4.4861e-03],
        ...,
        [ 7.4768e-04,  2.1973e-03, -1.9150e-03,  ...,  1.3062e-02,
          1.4114e-03, -1.8997e-03],
        [ 2.4872e-03,  8.7357e-04, -6.7749e-03,  ..., -2.6093e-03,
          2.9663e-02,  3.6621e-04],
        [-1.0986e-03,  3.0212e-03, -4.3335e-03,  ...,  1.3733e-04,
         -2.8534e-03,  3.2227e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  7.0625, -17.0781,  -1.2197,  ..., -14.2734,   4.8203,  -6.8906]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 16:29:02 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When he provides something, something has been provided
When he manages something, something has been managed
When he relates something, something has been related
When he suggests something, something has been suggested
When he receives something, something has been received
When he operates something, something has been operated
When he adds something, something has been added
When he considers something, something has been
2024-07-25 16:29:02 root INFO     [order_1_approx] starting weight calculation for When he considers something, something has been considered
When he relates something, something has been related
When he provides something, something has been provided
When he adds something, something has been added
When he operates something, something has been operated
When he manages something, something has been managed
When he receives something, something has been received
When he suggests something, something has been
2024-07-25 16:29:02 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 16:31:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.4062, -0.1797,  2.3594,  ...,  1.0625,  0.2500, -1.2344],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 20.1250, -14.7500,   2.2500,  ...,  -8.5625,   2.9375,  -5.8438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.1606e-02,  1.0010e-02, -1.1719e-02,  ..., -1.1536e-02,
          5.4932e-04, -3.8605e-03],
        [ 4.6387e-03,  2.8992e-03, -2.0294e-03,  ...,  1.1719e-02,
         -4.3335e-03,  4.3335e-03],
        [ 3.8147e-05, -3.8757e-03,  7.2266e-02,  ...,  2.7313e-03,
         -5.0964e-03, -6.1646e-03],
        ...,
        [ 1.5068e-04, -9.9945e-04, -5.9814e-03,  ...,  1.5442e-02,
          2.9602e-03, -3.3569e-03],
        [ 4.2114e-03,  5.0049e-03, -6.9885e-03,  ..., -3.9368e-03,
          2.8687e-02, -4.0283e-03],
        [-3.8147e-06, -2.2888e-05, -6.0425e-03,  ...,  5.9204e-03,
         -6.4697e-03,  3.2227e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 19.7969, -14.9766,   1.4258,  ...,  -9.3281,   2.5059,  -5.9414]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 16:31:48 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When he considers something, something has been considered
When he relates something, something has been related
When he provides something, something has been provided
When he adds something, something has been added
When he operates something, something has been operated
When he manages something, something has been managed
When he receives something, something has been received
When he suggests something, something has been
2024-07-25 16:31:48 root INFO     [order_1_approx] starting weight calculation for When he operates something, something has been operated
When he adds something, something has been added
When he manages something, something has been managed
When he provides something, something has been provided
When he relates something, something has been related
When he considers something, something has been considered
When he suggests something, something has been suggested
When he receives something, something has been
2024-07-25 16:31:48 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 16:34:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5000, -0.7812,  4.5000,  ...,  0.2031,  0.3281,  1.7188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 22.7500, -23.3750,  -5.7500,  ..., -19.2500,   3.3281,  -6.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.4170e-02,  1.2512e-03, -9.7656e-04,  ..., -5.7983e-03,
         -7.4768e-04, -1.3123e-03],
        [ 3.6163e-03,  1.3489e-02, -5.1117e-04,  ..., -6.8054e-03,
          2.4719e-03,  2.1667e-03],
        [-4.5776e-05,  2.6093e-03,  8.5938e-02,  ...,  1.2131e-03,
         -1.3504e-03, -3.5706e-03],
        ...,
        [ 2.7618e-03,  4.4556e-03, -2.9755e-03,  ...,  1.6968e-02,
          4.0894e-03,  1.9531e-03],
        [ 9.0942e-03,  1.6022e-03, -1.0437e-02,  ..., -1.2970e-04,
          3.1494e-02, -3.8300e-03],
        [-7.6675e-04,  4.0283e-03, -4.4250e-03,  ...,  1.7929e-03,
          8.3160e-04,  3.9062e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 23.2031, -23.5312,  -5.4961,  ..., -16.8281,   3.9102,  -6.7578]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 16:34:35 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When he operates something, something has been operated
When he adds something, something has been added
When he manages something, something has been managed
When he provides something, something has been provided
When he relates something, something has been related
When he considers something, something has been considered
When he suggests something, something has been suggested
When he receives something, something has been
2024-07-25 16:34:35 root INFO     [order_1_approx] starting weight calculation for When he receives something, something has been received
When he manages something, something has been managed
When he suggests something, something has been suggested
When he operates something, something has been operated
When he considers something, something has been considered
When he relates something, something has been related
When he adds something, something has been added
When he provides something, something has been
2024-07-25 16:34:35 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 16:37:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1016,  0.0215,  7.0625,  ...,  0.0781, -0.4648,  0.5469],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 12.7500, -17.7500,   7.7812,  ..., -16.7500,   8.1250,  -7.4375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0144,  0.0225, -0.0085,  ..., -0.0098,  0.0017,  0.0109],
        [ 0.0148, -0.0081, -0.0045,  ...,  0.0103, -0.0060, -0.0145],
        [ 0.0135, -0.0171,  0.0742,  ...,  0.0037, -0.0032, -0.0160],
        ...,
        [ 0.0048, -0.0046, -0.0065,  ...,  0.0182,  0.0002, -0.0008],
        [ 0.0072, -0.0044, -0.0026,  ..., -0.0003,  0.0259, -0.0014],
        [ 0.0015,  0.0014, -0.0035,  ...,  0.0069, -0.0052,  0.0304]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 13.0469, -18.7656,   7.4219,  ..., -17.2188,   9.3359,  -8.4844]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 16:37:21 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When he receives something, something has been received
When he manages something, something has been managed
When he suggests something, something has been suggested
When he operates something, something has been operated
When he considers something, something has been considered
When he relates something, something has been related
When he adds something, something has been added
When he provides something, something has been
2024-07-25 16:37:21 root INFO     [order_1_approx] starting weight calculation for When he provides something, something has been provided
When he manages something, something has been managed
When he receives something, something has been received
When he relates something, something has been related
When he considers something, something has been considered
When he suggests something, something has been suggested
When he operates something, something has been operated
When he adds something, something has been
2024-07-25 16:37:21 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 16:40:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3672, -0.1641,  6.7500,  ...,  1.6719,  1.1094, -0.9844],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  8.1250,  -8.8750,   7.0625,  ..., -12.6250,  -0.9219,  -3.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0277,  0.0084, -0.0010,  ..., -0.0078, -0.0011,  0.0056],
        [-0.0036,  0.0018, -0.0132,  ...,  0.0062,  0.0014, -0.0056],
        [-0.0011, -0.0087,  0.0728,  ...,  0.0021, -0.0030, -0.0057],
        ...,
        [-0.0042, -0.0182, -0.0237,  ...,  0.0283,  0.0069, -0.0096],
        [ 0.0039,  0.0064, -0.0002,  ..., -0.0050,  0.0352, -0.0011],
        [ 0.0023,  0.0010, -0.0044,  ...,  0.0034, -0.0008,  0.0391]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 10.4844, -12.8672,   5.7734,  ..., -17.5938,   0.3818,  -4.6289]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 16:40:05 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When he provides something, something has been provided
When he manages something, something has been managed
When he receives something, something has been received
When he relates something, something has been related
When he considers something, something has been considered
When he suggests something, something has been suggested
When he operates something, something has been operated
When he adds something, something has been
2024-07-25 16:40:05 root INFO     [order_1_approx] starting weight calculation for When he provides something, something has been provided
When he receives something, something has been received
When he relates something, something has been related
When he considers something, something has been considered
When he suggests something, something has been suggested
When he manages something, something has been managed
When he adds something, something has been added
When he operates something, something has been
2024-07-25 16:40:05 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 16:42:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8594,  0.6367,  5.0625,  ..., -2.4844, -1.2734,  0.6914],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 18.2500, -10.3125,   5.9062,  ..., -12.0625,   0.6406,  -3.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0262, -0.0018, -0.0003,  ...,  0.0015, -0.0014, -0.0007],
        [ 0.0031,  0.0181, -0.0065,  ..., -0.0065,  0.0090,  0.0047],
        [ 0.0042,  0.0043,  0.0703,  ...,  0.0021,  0.0036, -0.0009],
        ...,
        [ 0.0025,  0.0014, -0.0090,  ...,  0.0128,  0.0079,  0.0035],
        [ 0.0046,  0.0033,  0.0047,  ..., -0.0020,  0.0298, -0.0039],
        [-0.0005,  0.0029,  0.0018,  ...,  0.0014, -0.0022,  0.0320]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 17.0469,  -7.4766,   7.1250,  ..., -10.9609,   0.4807,  -3.9297]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 16:42:51 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When he provides something, something has been provided
When he receives something, something has been received
When he relates something, something has been related
When he considers something, something has been considered
When he suggests something, something has been suggested
When he manages something, something has been managed
When he adds something, something has been added
When he operates something, something has been
2024-07-25 16:42:51 root INFO     total operator prediction time: 1330.1732385158539 seconds
2024-07-25 16:42:51 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on adj - superlative
2024-07-25 16:42:51 root INFO     building operator adj - superlative
2024-07-25 16:42:51 root INFO     [order_1_approx] starting weight calculation for If something is the most tiny, it is tiniest
If something is the most pure, it is purest
If something is the most proud, it is proudest
If something is the most polite, it is politest
If something is the most merry, it is merriest
If something is the most risky, it is riskiest
If something is the most hungry, it is hungriest
If something is the most subtle, it is
2024-07-25 16:42:51 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 16:45:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.0508, 0.5859, 3.8906,  ..., 0.5781, 1.0547, 1.1562], device='cuda:0',
       dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([16.2500, 20.1250,  4.2188,  ..., -8.9375, -3.1875,  8.9375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.1362e-02,  7.0190e-04, -4.6387e-03,  ...,  7.7057e-04,
          5.2795e-03, -3.8147e-03],
        [ 2.8381e-03,  2.5146e-02, -7.6599e-03,  ..., -3.6316e-03,
          4.8828e-03, -3.8757e-03],
        [-3.2043e-04,  4.1809e-03,  7.9590e-02,  ..., -8.1787e-03,
          1.0376e-03,  5.7068e-03],
        ...,
        [-1.7624e-03, -1.9455e-03,  2.2888e-04,  ...,  1.4893e-02,
         -3.8147e-06, -8.1787e-03],
        [ 1.1520e-03,  1.1230e-02, -2.7275e-04,  ..., -4.7607e-03,
          2.0874e-02, -1.0834e-03],
        [-5.8289e-03,  2.7771e-03,  4.6539e-04,  ..., -2.6550e-03,
          3.6926e-03,  3.1738e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[14.4922, 24.2188,  3.4551,  ..., -8.2969, -1.6562,  9.4688]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 16:45:36 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most tiny, it is tiniest
If something is the most pure, it is purest
If something is the most proud, it is proudest
If something is the most polite, it is politest
If something is the most merry, it is merriest
If something is the most risky, it is riskiest
If something is the most hungry, it is hungriest
If something is the most subtle, it is
2024-07-25 16:45:37 root INFO     [order_1_approx] starting weight calculation for If something is the most hungry, it is hungriest
If something is the most tiny, it is tiniest
If something is the most subtle, it is subtlest
If something is the most pure, it is purest
If something is the most proud, it is proudest
If something is the most polite, it is politest
If something is the most merry, it is merriest
If something is the most risky, it is
2024-07-25 16:45:37 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 16:48:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5547, -0.6172,  2.9219,  ..., -1.7891, -0.8984,  1.9531],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 20.5000,  22.2500, -10.3750,  ..., -11.8125,   7.9688,  -2.9062],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.7090e-02,  3.6774e-03,  6.8665e-04,  ...,  2.7924e-03,
          1.2589e-03, -5.0659e-03],
        [ 2.4109e-03,  1.4526e-02,  2.1667e-03,  ..., -1.3199e-03,
          4.7302e-03,  8.6670e-03],
        [ 2.9907e-03, -3.6621e-04,  5.3955e-02,  ..., -2.8992e-03,
          1.3733e-03,  2.1057e-03],
        ...,
        [ 4.6082e-03,  7.6294e-06,  5.3406e-03,  ...,  1.0376e-02,
         -3.8147e-03, -7.0190e-03],
        [ 4.1809e-03,  3.8452e-03, -2.5482e-03,  ...,  3.0518e-04,
          1.9287e-02,  5.1575e-03],
        [-6.5308e-03,  4.8523e-03, -3.7537e-03,  ..., -1.1047e-02,
         -9.4604e-03,  2.6367e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 19.5312,  23.0000,  -8.9375,  ..., -12.5234,   7.1328,  -2.3809]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 16:48:23 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most hungry, it is hungriest
If something is the most tiny, it is tiniest
If something is the most subtle, it is subtlest
If something is the most pure, it is purest
If something is the most proud, it is proudest
If something is the most polite, it is politest
If something is the most merry, it is merriest
If something is the most risky, it is
2024-07-25 16:48:23 root INFO     [order_1_approx] starting weight calculation for If something is the most merry, it is merriest
If something is the most subtle, it is subtlest
If something is the most polite, it is politest
If something is the most pure, it is purest
If something is the most tiny, it is tiniest
If something is the most proud, it is proudest
If something is the most risky, it is riskiest
If something is the most hungry, it is
2024-07-25 16:48:23 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 16:51:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.9219, -1.3594,  0.4863,  ...,  2.3906, -0.5547,  2.9375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 17.6250,  24.8750,   1.4531,  ...,  -4.4375,  -0.8906, -16.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0166, -0.0022,  0.0050,  ...,  0.0007,  0.0063,  0.0013],
        [-0.0012,  0.0164,  0.0054,  ...,  0.0035,  0.0088,  0.0005],
        [ 0.0046,  0.0050,  0.0586,  ..., -0.0045,  0.0069, -0.0087],
        ...,
        [ 0.0034, -0.0006,  0.0005,  ...,  0.0090, -0.0050, -0.0047],
        [-0.0009, -0.0023, -0.0153,  ..., -0.0037,  0.0240,  0.0084],
        [-0.0079,  0.0029, -0.0068,  ..., -0.0036,  0.0024,  0.0243]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 18.4531,  24.0000,   1.2148,  ...,  -4.5156,  -0.8013, -16.9219]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 16:51:11 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most merry, it is merriest
If something is the most subtle, it is subtlest
If something is the most polite, it is politest
If something is the most pure, it is purest
If something is the most tiny, it is tiniest
If something is the most proud, it is proudest
If something is the most risky, it is riskiest
If something is the most hungry, it is
2024-07-25 16:51:11 root INFO     [order_1_approx] starting weight calculation for If something is the most subtle, it is subtlest
If something is the most hungry, it is hungriest
If something is the most pure, it is purest
If something is the most proud, it is proudest
If something is the most polite, it is politest
If something is the most risky, it is riskiest
If something is the most merry, it is merriest
If something is the most tiny, it is
2024-07-25 16:51:11 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 16:53:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3906,  0.2383,  3.4844,  ...,  1.2734, -0.7148,  1.5312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  9.9375,  21.6250,  14.0000,  ...,  -4.2188, -10.6250,   3.4062],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0106, -0.0013, -0.0123,  ...,  0.0031,  0.0036, -0.0020],
        [ 0.0009,  0.0156,  0.0014,  ...,  0.0021,  0.0042,  0.0034],
        [ 0.0038,  0.0012,  0.0571,  ...,  0.0031, -0.0003,  0.0073],
        ...,
        [ 0.0087, -0.0026,  0.0006,  ...,  0.0127, -0.0052, -0.0109],
        [ 0.0013,  0.0059, -0.0049,  ..., -0.0070,  0.0172,  0.0035],
        [-0.0014,  0.0068,  0.0003,  ..., -0.0026, -0.0034,  0.0222]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  9.3203,  20.8594,  11.4766,  ...,  -3.7480, -12.6328,   2.1641]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 16:53:57 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most subtle, it is subtlest
If something is the most hungry, it is hungriest
If something is the most pure, it is purest
If something is the most proud, it is proudest
If something is the most polite, it is politest
If something is the most risky, it is riskiest
If something is the most merry, it is merriest
If something is the most tiny, it is
2024-07-25 16:53:57 root INFO     [order_1_approx] starting weight calculation for If something is the most hungry, it is hungriest
If something is the most pure, it is purest
If something is the most subtle, it is subtlest
If something is the most tiny, it is tiniest
If something is the most proud, it is proudest
If something is the most merry, it is merriest
If something is the most risky, it is riskiest
If something is the most polite, it is
2024-07-25 16:53:57 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 16:56:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0430,  1.2734,  3.1875,  ...,  1.5469, -1.0703,  1.3281],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([19.0000, 17.7500, -7.0625,  ...,  2.7656,  4.5938, 21.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0229, -0.0039,  0.0038,  ..., -0.0022, -0.0052, -0.0005],
        [ 0.0024,  0.0159, -0.0065,  ..., -0.0025,  0.0015,  0.0029],
        [-0.0029, -0.0013,  0.0723,  ...,  0.0001,  0.0024, -0.0014],
        ...,
        [ 0.0024, -0.0039,  0.0004,  ...,  0.0135, -0.0062, -0.0106],
        [ 0.0024,  0.0027, -0.0023,  ..., -0.0084,  0.0166,  0.0048],
        [-0.0040,  0.0068,  0.0004,  ..., -0.0074,  0.0045,  0.0284]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[20.0625, 18.8125, -8.7266,  ...,  1.5391,  4.5391, 18.9531]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 16:56:44 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most hungry, it is hungriest
If something is the most pure, it is purest
If something is the most subtle, it is subtlest
If something is the most tiny, it is tiniest
If something is the most proud, it is proudest
If something is the most merry, it is merriest
If something is the most risky, it is riskiest
If something is the most polite, it is
2024-07-25 16:56:45 root INFO     [order_1_approx] starting weight calculation for If something is the most proud, it is proudest
If something is the most merry, it is merriest
If something is the most subtle, it is subtlest
If something is the most polite, it is politest
If something is the most hungry, it is hungriest
If something is the most tiny, it is tiniest
If something is the most risky, it is riskiest
If something is the most pure, it is
2024-07-25 16:56:45 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 16:59:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0000,  0.3594, -0.0479,  ...,  1.7344,  2.4375, -0.4922],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 17.2500,  20.0000,   0.0645,  ..., -10.0000,  -4.6250,   3.6406],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.9165e-02, -7.8583e-04,  5.0964e-03,  ..., -3.2501e-03,
          6.6376e-04, -6.1417e-04],
        [ 2.3193e-03,  1.8677e-02, -2.1362e-04,  ..., -9.1553e-04,
         -9.9182e-05,  4.5166e-03],
        [ 5.2795e-03,  4.6387e-03,  5.9814e-02,  ..., -6.6376e-04,
         -1.7548e-03,  1.6251e-03],
        ...,
        [ 5.8365e-04, -4.0894e-03, -4.4556e-03,  ...,  1.1475e-02,
         -3.5477e-04, -2.8687e-03],
        [-4.9591e-04,  1.0071e-03, -8.1787e-03,  ..., -4.2419e-03,
          2.2705e-02,  6.0730e-03],
        [ 1.0986e-03,  7.3242e-03,  9.6130e-04,  ..., -2.6398e-03,
         -1.7853e-03,  2.0752e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[16.9688, 21.1562,  0.2905,  ..., -9.4141, -3.9570,  3.9824]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 16:59:32 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most proud, it is proudest
If something is the most merry, it is merriest
If something is the most subtle, it is subtlest
If something is the most polite, it is politest
If something is the most hungry, it is hungriest
If something is the most tiny, it is tiniest
If something is the most risky, it is riskiest
If something is the most pure, it is
2024-07-25 16:59:32 root INFO     [order_1_approx] starting weight calculation for If something is the most pure, it is purest
If something is the most tiny, it is tiniest
If something is the most subtle, it is subtlest
If something is the most risky, it is riskiest
If something is the most merry, it is merriest
If something is the most hungry, it is hungriest
If something is the most polite, it is politest
If something is the most proud, it is
2024-07-25 16:59:33 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 17:02:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2080, -0.1699, -1.2578,  ..., -0.3633, -1.6172,  1.8359],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 5.3750, 16.7500, -0.8516,  ...,  1.0625, -0.6875,  5.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0242,  0.0006, -0.0047,  ..., -0.0039, -0.0050,  0.0055],
        [-0.0014,  0.0205, -0.0049,  ...,  0.0009,  0.0005,  0.0074],
        [ 0.0014,  0.0041,  0.0693,  ..., -0.0014,  0.0024, -0.0053],
        ...,
        [ 0.0067, -0.0004,  0.0068,  ...,  0.0164, -0.0121, -0.0049],
        [-0.0051,  0.0004, -0.0062,  ..., -0.0082,  0.0249,  0.0009],
        [-0.0020,  0.0045, -0.0117,  ..., -0.0059, -0.0045,  0.0327]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.2070, 15.8750, -1.9092,  ...,  3.7676, -1.0156,  7.1328]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 17:02:20 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most pure, it is purest
If something is the most tiny, it is tiniest
If something is the most subtle, it is subtlest
If something is the most risky, it is riskiest
If something is the most merry, it is merriest
If something is the most hungry, it is hungriest
If something is the most polite, it is politest
If something is the most proud, it is
2024-07-25 17:02:21 root INFO     [order_1_approx] starting weight calculation for If something is the most proud, it is proudest
If something is the most tiny, it is tiniest
If something is the most hungry, it is hungriest
If something is the most polite, it is politest
If something is the most risky, it is riskiest
If something is the most subtle, it is subtlest
If something is the most pure, it is purest
If something is the most merry, it is
2024-07-25 17:02:21 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 17:05:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0664,  1.2031,  4.4062,  ...,  0.4609, -1.6172,  3.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([16.5000, 15.8125,  9.5625,  ..., -8.3750, 12.1250,  6.9062],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.1484e-02,  1.5259e-03,  6.8665e-05,  ..., -2.4414e-03,
          8.8692e-05, -1.0803e-02],
        [ 5.8594e-03,  1.8066e-02,  4.1199e-03,  ...,  2.1057e-03,
          6.8359e-03, -2.2888e-04],
        [ 4.0894e-03,  7.3853e-03,  7.0801e-02,  ..., -3.7537e-03,
          3.5095e-03,  2.7466e-03],
        ...,
        [ 9.3460e-04,  8.6212e-04,  2.9907e-03,  ...,  1.3062e-02,
         -6.7139e-03, -3.8452e-03],
        [ 6.2256e-03, -3.4714e-04,  3.9368e-03,  ...,  9.7656e-04,
          2.1729e-02,  5.9814e-03],
        [-5.8594e-03,  1.2207e-04, -9.5825e-03,  ..., -6.4697e-03,
         -1.0925e-02,  2.7466e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[16.2812, 16.2656,  8.6875,  ..., -7.3359, 12.1797,  6.9414]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 17:05:08 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most proud, it is proudest
If something is the most tiny, it is tiniest
If something is the most hungry, it is hungriest
If something is the most polite, it is politest
If something is the most risky, it is riskiest
If something is the most subtle, it is subtlest
If something is the most pure, it is purest
If something is the most merry, it is
2024-07-25 17:05:08 root INFO     total operator prediction time: 1336.9689433574677 seconds
2024-07-25 17:05:08 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on verb+er_irreg
2024-07-25 17:05:08 root INFO     building operator verb+er_irreg
2024-07-25 17:05:08 root INFO     [order_1_approx] starting weight calculation for If you send something, you are a sender
If you follow something, you are a follower
If you teach something, you are a teacher
If you determine something, you are a determiner
If you manage something, you are a manager
If you suffer something, you are a sufferer
If you defend something, you are a defender
If you compose something, you are a
2024-07-25 17:05:08 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 17:07:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9531, -0.9805, -2.4688,  ...,  0.8750, -3.1719,  3.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -0.7344,   0.7188,  -4.1250,  ..., -10.0000,  -1.4844,  13.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0295,  0.0025, -0.0014,  ...,  0.0044,  0.0060, -0.0051],
        [-0.0010,  0.0143,  0.0028,  ...,  0.0012, -0.0057,  0.0042],
        [-0.0042, -0.0020,  0.0869,  ...,  0.0021, -0.0098,  0.0063],
        ...,
        [ 0.0034,  0.0042, -0.0052,  ...,  0.0151, -0.0029,  0.0019],
        [-0.0029,  0.0067,  0.0043,  ..., -0.0046,  0.0425, -0.0036],
        [-0.0037, -0.0009, -0.0072,  ...,  0.0003, -0.0117,  0.0576]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -1.8564,   1.2227,  -3.1582,  ..., -10.5078,  -1.3008,  13.1562]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 17:07:54 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you send something, you are a sender
If you follow something, you are a follower
If you teach something, you are a teacher
If you determine something, you are a determiner
If you manage something, you are a manager
If you suffer something, you are a sufferer
If you defend something, you are a defender
If you compose something, you are a
2024-07-25 17:07:55 root INFO     [order_1_approx] starting weight calculation for If you teach something, you are a teacher
If you manage something, you are a manager
If you follow something, you are a follower
If you compose something, you are a composer
If you send something, you are a sender
If you defend something, you are a defender
If you determine something, you are a determiner
If you suffer something, you are a
2024-07-25 17:07:55 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 17:10:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.8438,  0.9375,  5.0312,  ..., -0.6797, -1.3828,  0.9062],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 12.6875,   1.9688,  13.6250,  ..., -10.6250, -11.7500,  -4.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0186,  0.0039,  0.0027,  ...,  0.0009,  0.0045, -0.0024],
        [-0.0054,  0.0148,  0.0036,  ..., -0.0006,  0.0010,  0.0057],
        [-0.0068, -0.0005,  0.0762,  ...,  0.0004,  0.0011,  0.0053],
        ...,
        [ 0.0029,  0.0021, -0.0026,  ...,  0.0151, -0.0014, -0.0041],
        [ 0.0023,  0.0026, -0.0029,  ..., -0.0016,  0.0250, -0.0031],
        [-0.0010,  0.0008, -0.0044,  ..., -0.0065, -0.0019,  0.0322]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 11.4297,   3.5723,  15.9531,  ..., -11.8281, -12.1172,  -2.4805]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 17:10:41 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you teach something, you are a teacher
If you manage something, you are a manager
If you follow something, you are a follower
If you compose something, you are a composer
If you send something, you are a sender
If you defend something, you are a defender
If you determine something, you are a determiner
If you suffer something, you are a
2024-07-25 17:10:41 root INFO     [order_1_approx] starting weight calculation for If you defend something, you are a defender
If you manage something, you are a manager
If you send something, you are a sender
If you determine something, you are a determiner
If you teach something, you are a teacher
If you suffer something, you are a sufferer
If you compose something, you are a composer
If you follow something, you are a
2024-07-25 17:10:41 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 17:13:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.8672, -0.6328,  5.7188,  ..., -0.9844, -1.5781,  2.8906],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 5.5000,  0.0938, 10.1250,  ..., -0.7812,  0.5156, -1.7031],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0295, -0.0035,  0.0009,  ...,  0.0011,  0.0087, -0.0025],
        [ 0.0042,  0.0178, -0.0023,  ...,  0.0005,  0.0005,  0.0089],
        [-0.0054,  0.0033,  0.0830,  ...,  0.0055,  0.0053,  0.0056],
        ...,
        [-0.0010,  0.0029, -0.0001,  ...,  0.0150, -0.0030,  0.0042],
        [ 0.0017,  0.0049,  0.0018,  ..., -0.0028,  0.0273, -0.0049],
        [ 0.0029,  0.0024,  0.0045,  ..., -0.0066,  0.0012,  0.0391]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.7891,  0.6597, 10.0469,  ..., -1.0410,  0.9639,  1.0664]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 17:13:27 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you defend something, you are a defender
If you manage something, you are a manager
If you send something, you are a sender
If you determine something, you are a determiner
If you teach something, you are a teacher
If you suffer something, you are a sufferer
If you compose something, you are a composer
If you follow something, you are a
2024-07-25 17:13:28 root INFO     [order_1_approx] starting weight calculation for If you manage something, you are a manager
If you follow something, you are a follower
If you determine something, you are a determiner
If you compose something, you are a composer
If you send something, you are a sender
If you suffer something, you are a sufferer
If you teach something, you are a teacher
If you defend something, you are a
2024-07-25 17:13:28 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 17:16:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2500, -0.7852,  2.5625,  ..., -1.9688, -0.8047,  2.9531],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -3.6094,   0.2188,   7.8750,  ..., -11.6875,   2.2656,   0.8906],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.3926e-02,  5.5695e-04,  4.5166e-03,  ...,  3.0365e-03,
          7.7209e-03, -8.7280e-03],
        [ 1.7166e-03,  1.2878e-02,  4.3335e-03,  ...,  5.4550e-04,
         -1.0071e-03,  1.0132e-02],
        [ 8.2397e-04, -6.5613e-04,  8.5938e-02,  ...,  2.5330e-03,
          9.9182e-04, -9.5367e-05],
        ...,
        [ 1.8692e-04,  1.8005e-03, -2.6855e-03,  ...,  1.6968e-02,
         -5.2490e-03,  1.8616e-03],
        [-5.0964e-03,  7.0801e-03,  1.1597e-02,  ..., -5.4169e-04,
          3.0273e-02,  3.2654e-03],
        [-1.7700e-03,  2.1820e-03, -6.5613e-04,  ..., -2.6093e-03,
         -6.1646e-03,  3.6133e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.8184e+00,  1.0864e-02,  8.8750e+00,  ..., -1.1320e+01,
          2.3984e+00, -5.0000e-01]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SubBackward0>) 

                    
2024-07-25 17:16:14 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you manage something, you are a manager
If you follow something, you are a follower
If you determine something, you are a determiner
If you compose something, you are a composer
If you send something, you are a sender
If you suffer something, you are a sufferer
If you teach something, you are a teacher
If you defend something, you are a
2024-07-25 17:16:14 root INFO     [order_1_approx] starting weight calculation for If you determine something, you are a determiner
If you manage something, you are a manager
If you send something, you are a sender
If you follow something, you are a follower
If you defend something, you are a defender
If you compose something, you are a composer
If you suffer something, you are a sufferer
If you teach something, you are a
2024-07-25 17:16:14 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 17:18:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4531,  0.3789,  5.9688,  ...,  0.5273, -1.3984,  1.7891],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([18.2500, -2.7188,  0.3125,  ..., -6.3750, -4.6875,  1.8438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.6602e-02,  2.9907e-03,  9.6130e-04,  ...,  3.7994e-03,
          8.2397e-03, -2.2278e-03],
        [ 9.2983e-06,  1.1230e-02,  3.0060e-03,  ..., -2.4796e-04,
          1.0986e-03,  1.9455e-04],
        [-2.4414e-03, -1.5259e-04,  6.6895e-02,  ..., -8.3923e-04,
         -2.1362e-03,  3.0670e-03],
        ...,
        [ 1.8768e-03,  5.6152e-03, -5.1575e-03,  ...,  1.3306e-02,
         -2.7466e-03, -1.2665e-03],
        [-1.3962e-03,  2.9449e-03, -7.0190e-04,  ...,  9.1553e-05,
          2.1118e-02,  2.1362e-03],
        [ 2.6093e-03, -2.0790e-04, -3.6926e-03,  ..., -3.5477e-04,
         -1.3504e-03,  2.7100e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[18.5000, -3.1797,  0.8550,  ..., -6.1211, -3.7695,  2.0898]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 17:19:00 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you determine something, you are a determiner
If you manage something, you are a manager
If you send something, you are a sender
If you follow something, you are a follower
If you defend something, you are a defender
If you compose something, you are a composer
If you suffer something, you are a sufferer
If you teach something, you are a
2024-07-25 17:19:00 root INFO     [order_1_approx] starting weight calculation for If you compose something, you are a composer
If you defend something, you are a defender
If you follow something, you are a follower
If you suffer something, you are a sufferer
If you teach something, you are a teacher
If you send something, you are a sender
If you manage something, you are a manager
If you determine something, you are a
2024-07-25 17:19:01 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 17:21:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2031,  0.8008,  7.2812,  ...,  0.8984, -1.4453,  4.9688],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-1.6250,  0.3750,  2.7656,  ..., -9.8750, -0.3594, 26.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0237,  0.0034, -0.0017,  ..., -0.0013,  0.0057, -0.0137],
        [ 0.0001,  0.0134,  0.0002,  ..., -0.0046, -0.0010,  0.0040],
        [-0.0045, -0.0019,  0.0835,  ...,  0.0031, -0.0054,  0.0012],
        ...,
        [ 0.0003,  0.0004, -0.0075,  ...,  0.0205, -0.0034,  0.0057],
        [-0.0036, -0.0002,  0.0106,  ..., -0.0029,  0.0293,  0.0026],
        [-0.0049, -0.0044, -0.0129,  ..., -0.0008, -0.0071,  0.0439]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.1250, -0.3843,  1.8174,  ..., -9.0000, -0.8418, 26.3750]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 17:21:47 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you compose something, you are a composer
If you defend something, you are a defender
If you follow something, you are a follower
If you suffer something, you are a sufferer
If you teach something, you are a teacher
If you send something, you are a sender
If you manage something, you are a manager
If you determine something, you are a
2024-07-25 17:21:47 root INFO     [order_1_approx] starting weight calculation for If you suffer something, you are a sufferer
If you defend something, you are a defender
If you follow something, you are a follower
If you send something, you are a sender
If you compose something, you are a composer
If you teach something, you are a teacher
If you determine something, you are a determiner
If you manage something, you are a
2024-07-25 17:21:47 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 17:24:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5000, -0.7422,  3.5625,  ..., -1.1875, -1.1641,  2.2188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  7.5312,   4.2812,   0.6719,  ..., -18.3750,  -9.1875,   3.2188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0277,  0.0004,  0.0015,  ..., -0.0002,  0.0089, -0.0009],
        [ 0.0033,  0.0145,  0.0050,  ..., -0.0009, -0.0006,  0.0027],
        [-0.0030,  0.0020,  0.0898,  ...,  0.0038,  0.0007, -0.0004],
        ...,
        [ 0.0016,  0.0007, -0.0040,  ...,  0.0165, -0.0031,  0.0026],
        [ 0.0012,  0.0033,  0.0049,  ..., -0.0019,  0.0332, -0.0038],
        [ 0.0012,  0.0008, -0.0067,  ..., -0.0039, -0.0060,  0.0396]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  8.2188,   4.9883,   1.2480,  ..., -19.5781,  -8.7500,   3.7617]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 17:24:34 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you suffer something, you are a sufferer
If you defend something, you are a defender
If you follow something, you are a follower
If you send something, you are a sender
If you compose something, you are a composer
If you teach something, you are a teacher
If you determine something, you are a determiner
If you manage something, you are a
2024-07-25 17:24:34 root INFO     [order_1_approx] starting weight calculation for If you manage something, you are a manager
If you determine something, you are a determiner
If you suffer something, you are a sufferer
If you follow something, you are a follower
If you defend something, you are a defender
If you compose something, you are a composer
If you teach something, you are a teacher
If you send something, you are a
2024-07-25 17:24:34 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 17:27:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1172,  0.7188,  2.1875,  ..., -0.4023, -2.3906,  4.0938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 9.4375, -7.0938,  8.8125,  ..., -3.4062,  1.9531, -5.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.2705e-02,  5.7678e-03, -1.6785e-04,  ..., -2.6093e-03,
          6.8359e-03, -4.5166e-03],
        [-4.9591e-04,  7.7515e-03,  1.0834e-03,  ...,  4.3869e-04,
         -1.8692e-04,  5.7068e-03],
        [-8.0566e-03, -5.5542e-03,  6.4941e-02,  ...,  9.8877e-03,
         -2.7313e-03,  7.5073e-03],
        ...,
        [-1.1978e-03,  6.2466e-05, -2.3041e-03,  ...,  1.2939e-02,
         -1.9264e-04,  3.3264e-03],
        [-3.0060e-03,  1.4267e-03, -3.6926e-03,  ..., -3.3188e-04,
          2.1484e-02,  6.0425e-03],
        [-1.9379e-03, -1.1444e-03, -9.2163e-03,  ..., -6.2180e-04,
         -8.7280e-03,  3.0518e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 8.9531, -6.8789, 10.1562,  ..., -2.4941,  3.0078, -6.1719]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 17:27:20 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you manage something, you are a manager
If you determine something, you are a determiner
If you suffer something, you are a sufferer
If you follow something, you are a follower
If you defend something, you are a defender
If you compose something, you are a composer
If you teach something, you are a teacher
If you send something, you are a
2024-07-25 17:27:20 root INFO     total operator prediction time: 1332.2699041366577 seconds
2024-07-25 17:27:20 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on over+adj_reg
2024-07-25 17:27:20 root INFO     building operator over+adj_reg
2024-07-25 17:27:20 root INFO     [order_1_approx] starting weight calculation for If something is too stimulated, it is overstimulated
If something is too zealous, it is overzealous
If something is too sold, it is oversold
If something is too dressed, it is overdressed
If something is too powered, it is overpowered
If something is too strained, it is overstrained
If something is too used, it is overused
If something is too painted, it is
2024-07-25 17:27:20 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 17:30:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8203,  1.6172, -1.1406,  ...,  1.2734,  2.0625, -0.3906],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 0.6562,  0.5938, -5.6250,  ..., -1.7812, 12.3125,  5.2812],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.8564e-02, -1.3885e-03,  4.1504e-03,  ...,  1.4496e-03,
         -2.5940e-03,  1.9073e-05],
        [ 7.0190e-03,  1.3611e-02, -6.8665e-04,  ..., -2.3499e-03,
          3.4943e-03, -9.0942e-03],
        [ 8.3008e-03, -1.5137e-02,  1.0156e-01,  ..., -4.8218e-03,
          7.6904e-03, -7.8735e-03],
        ...,
        [-1.5717e-03,  3.5706e-03, -1.8539e-03,  ...,  1.5625e-02,
         -3.5400e-03, -1.0498e-02],
        [ 5.9128e-04,  1.6022e-03, -1.7242e-03,  ..., -3.4180e-03,
          4.8096e-02,  2.5024e-03],
        [ 3.0670e-03,  1.6632e-03, -1.4404e-02,  ..., -5.9204e-03,
         -1.1719e-02,  4.6631e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5732e+00, -4.8828e-04, -6.8750e+00,  ..., -2.7910e+00,
          1.2203e+01,  2.8496e+00]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SubBackward0>) 

                    
2024-07-25 17:30:06 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is too stimulated, it is overstimulated
If something is too zealous, it is overzealous
If something is too sold, it is oversold
If something is too dressed, it is overdressed
If something is too powered, it is overpowered
If something is too strained, it is overstrained
If something is too used, it is overused
If something is too painted, it is
2024-07-25 17:30:06 root INFO     [order_1_approx] starting weight calculation for If something is too used, it is overused
If something is too zealous, it is overzealous
If something is too sold, it is oversold
If something is too dressed, it is overdressed
If something is too powered, it is overpowered
If something is too strained, it is overstrained
If something is too painted, it is overpainted
If something is too stimulated, it is
2024-07-25 17:30:06 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 17:32:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.8203, -1.8047,  5.0000,  ...,  0.7031, -1.8984,  2.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 3.2812,  2.0156, -5.1562,  ..., -8.9375, 10.0625,  3.7969],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0325, -0.0044,  0.0064,  ..., -0.0023,  0.0087, -0.0029],
        [ 0.0105,  0.0208,  0.0120,  ..., -0.0031,  0.0042,  0.0126],
        [ 0.0031,  0.0008,  0.0981,  ..., -0.0101, -0.0035, -0.0024],
        ...,
        [-0.0056,  0.0042, -0.0250,  ...,  0.0137,  0.0087, -0.0074],
        [-0.0024,  0.0082, -0.0320,  ..., -0.0011,  0.0361, -0.0036],
        [ 0.0049, -0.0002, -0.0061,  ..., -0.0080, -0.0166,  0.0520]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  3.1602,   1.7227,  -9.0078,  ..., -10.7500,  10.5234,   2.2148]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 17:32:52 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is too used, it is overused
If something is too zealous, it is overzealous
If something is too sold, it is oversold
If something is too dressed, it is overdressed
If something is too powered, it is overpowered
If something is too strained, it is overstrained
If something is too painted, it is overpainted
If something is too stimulated, it is
2024-07-25 17:32:53 root INFO     [order_1_approx] starting weight calculation for If something is too stimulated, it is overstimulated
If something is too dressed, it is overdressed
If something is too powered, it is overpowered
If something is too strained, it is overstrained
If something is too painted, it is overpainted
If something is too sold, it is oversold
If something is too zealous, it is overzealous
If something is too used, it is
2024-07-25 17:32:53 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 17:35:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.0352, 1.7031, 3.0000,  ..., 1.6719, 0.4414, 0.5000], device='cuda:0',
       dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-10.0000,  -3.3125,  -7.4375,  ...,  -2.4062,   6.7188,  -7.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.7842e-02, -4.1504e-03, -3.1128e-03,  ...,  2.0599e-03,
          1.0498e-02, -7.8125e-03],
        [-7.4768e-04,  2.0508e-02,  8.4229e-03,  ...,  4.0588e-03,
          1.2207e-03, -8.2397e-04],
        [ 6.5918e-03, -3.5248e-03,  1.0742e-01,  ..., -1.9684e-03,
          6.9580e-03, -3.0060e-03],
        ...,
        [ 1.0193e-02,  2.8534e-03,  8.8501e-04,  ...,  1.4343e-02,
         -3.1433e-03,  4.5776e-05],
        [ 2.8839e-03,  7.8125e-03, -9.7656e-03,  ..., -3.8147e-03,
          4.1016e-02, -1.8463e-03],
        [ 5.4626e-03,  8.6670e-03, -1.6724e-02,  ...,  3.8147e-03,
          4.7607e-03,  4.6387e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-9.8203, -3.3867, -5.1875,  ..., -2.4629,  7.4414, -7.0859]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 17:35:39 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is too stimulated, it is overstimulated
If something is too dressed, it is overdressed
If something is too powered, it is overpowered
If something is too strained, it is overstrained
If something is too painted, it is overpainted
If something is too sold, it is oversold
If something is too zealous, it is overzealous
If something is too used, it is
2024-07-25 17:35:39 root INFO     [order_1_approx] starting weight calculation for If something is too used, it is overused
If something is too dressed, it is overdressed
If something is too strained, it is overstrained
If something is too painted, it is overpainted
If something is too zealous, it is overzealous
If something is too stimulated, it is overstimulated
If something is too sold, it is oversold
If something is too powered, it is
2024-07-25 17:35:39 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 17:38:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3906,  0.6367,  0.2891,  ...,  3.0938,  0.1016,  0.3340],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-4.7500, -3.5625,  6.3750,  ...,  0.7188,  3.5312, -2.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0405, -0.0036, -0.0008,  ..., -0.0037,  0.0023, -0.0045],
        [ 0.0063,  0.0210,  0.0135,  ...,  0.0034,  0.0070, -0.0045],
        [-0.0056, -0.0023,  0.1221,  ..., -0.0027,  0.0040, -0.0024],
        ...,
        [-0.0026,  0.0005, -0.0058,  ...,  0.0198, -0.0055, -0.0062],
        [-0.0006,  0.0068,  0.0004,  ..., -0.0042,  0.0459, -0.0006],
        [ 0.0142,  0.0086, -0.0104,  ...,  0.0032,  0.0008,  0.0449]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.8750, -2.2734,  6.5195,  ...,  1.0576,  4.2695, -2.7695]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 17:38:24 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is too used, it is overused
If something is too dressed, it is overdressed
If something is too strained, it is overstrained
If something is too painted, it is overpainted
If something is too zealous, it is overzealous
If something is too stimulated, it is overstimulated
If something is too sold, it is oversold
If something is too powered, it is
2024-07-25 17:38:24 root INFO     [order_1_approx] starting weight calculation for If something is too used, it is overused
If something is too dressed, it is overdressed
If something is too stimulated, it is overstimulated
If something is too strained, it is overstrained
If something is too painted, it is overpainted
If something is too zealous, it is overzealous
If something is too powered, it is overpowered
If something is too sold, it is
2024-07-25 17:38:24 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 17:41:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 3.1250e-02,  1.2969e+00,  1.9531e-03,  ...,  3.8125e+00,
        -1.7188e+00, -4.0625e-01], device='cuda:0', dtype=torch.float16,
       grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 1.8906,  3.1562, -5.2188,  ...,  4.4375, 11.1250, -2.1719],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.1494e-02, -6.0425e-03,  2.3682e-02,  ...,  3.3875e-03,
          6.5613e-03, -1.3428e-02],
        [-4.5776e-05,  1.8311e-02,  8.4229e-03,  ..., -1.2436e-03,
         -8.3923e-05,  2.7618e-03],
        [ 3.0823e-03, -2.1515e-03,  1.0254e-01,  ..., -6.8359e-03,
          1.4221e-02, -5.2490e-03],
        ...,
        [ 2.8687e-03,  4.5776e-03,  2.7924e-03,  ...,  1.0742e-02,
         -1.4343e-03, -2.5635e-03],
        [ 4.2725e-04,  2.6245e-03, -6.8970e-03,  ..., -1.1902e-03,
          3.9795e-02, -3.1281e-04],
        [ 1.3794e-02,  4.6387e-03, -2.2827e-02,  ..., -5.6152e-03,
         -5.5313e-04,  3.8574e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3652,  3.6465, -4.4023,  ...,  3.6758, 11.9688, -1.5674]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 17:41:09 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is too used, it is overused
If something is too dressed, it is overdressed
If something is too stimulated, it is overstimulated
If something is too strained, it is overstrained
If something is too painted, it is overpainted
If something is too zealous, it is overzealous
If something is too powered, it is overpowered
If something is too sold, it is
2024-07-25 17:41:09 root INFO     [order_1_approx] starting weight calculation for If something is too stimulated, it is overstimulated
If something is too used, it is overused
If something is too painted, it is overpainted
If something is too powered, it is overpowered
If something is too dressed, it is overdressed
If something is too sold, it is oversold
If something is too zealous, it is overzealous
If something is too strained, it is
2024-07-25 17:41:10 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 17:43:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2109, -0.7656,  3.1562,  ..., -0.6016,  0.4141,  0.5234],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 0.9688, -7.5000, -1.1094,  ..., -8.3750,  7.6875,  3.0781],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 4.9316e-02, -7.5989e-03,  1.2085e-02,  ...,  1.6785e-04,
          1.3733e-02, -2.2278e-03],
        [ 5.7983e-04,  2.1362e-02, -1.3611e-02,  ..., -7.2098e-04,
          3.3722e-03,  4.1504e-03],
        [ 1.6861e-03,  4.5776e-04,  1.2891e-01,  ..., -9.1553e-03,
          4.8523e-03,  1.0681e-03],
        ...,
        [ 5.0049e-03, -3.2806e-04, -1.0254e-02,  ...,  3.1006e-02,
          1.1047e-02,  6.1035e-05],
        [-2.0752e-03,  1.2634e-02, -1.7456e-02,  ..., -9.6130e-04,
          5.4688e-02,  2.8839e-03],
        [ 2.8687e-03,  3.2043e-04, -1.6846e-02,  ..., -4.5776e-04,
          2.4109e-03,  6.8359e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7363, -7.8633, -1.4980,  ..., -9.9688,  5.5391,  4.5742]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 17:43:55 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is too stimulated, it is overstimulated
If something is too used, it is overused
If something is too painted, it is overpainted
If something is too powered, it is overpowered
If something is too dressed, it is overdressed
If something is too sold, it is oversold
If something is too zealous, it is overzealous
If something is too strained, it is
2024-07-25 17:43:55 root INFO     [order_1_approx] starting weight calculation for If something is too strained, it is overstrained
If something is too dressed, it is overdressed
If something is too painted, it is overpainted
If something is too sold, it is oversold
If something is too powered, it is overpowered
If something is too stimulated, it is overstimulated
If something is too used, it is overused
If something is too zealous, it is
2024-07-25 17:43:55 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 17:46:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9219,  1.2344,  5.7812,  ..., -0.1445, -1.6172,  1.9531],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-0.2158, -9.6875, -0.3438,  ..., -1.7031,  4.9375, -2.5938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0322, -0.0088,  0.0200,  ..., -0.0043,  0.0011,  0.0044],
        [-0.0024,  0.0261, -0.0194,  ...,  0.0038,  0.0013,  0.0090],
        [ 0.0043, -0.0059,  0.1377,  ..., -0.0090,  0.0142, -0.0057],
        ...,
        [-0.0013,  0.0065, -0.0082,  ...,  0.0237, -0.0080, -0.0024],
        [ 0.0042,  0.0002, -0.0009,  ..., -0.0085,  0.0366,  0.0009],
        [ 0.0045,  0.0028,  0.0014,  ..., -0.0067, -0.0043,  0.0500]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -0.1289, -10.7734,  -2.1914,  ...,  -0.4463,   6.6562,  -4.5312]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 17:46:40 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is too strained, it is overstrained
If something is too dressed, it is overdressed
If something is too painted, it is overpainted
If something is too sold, it is oversold
If something is too powered, it is overpowered
If something is too stimulated, it is overstimulated
If something is too used, it is overused
If something is too zealous, it is
2024-07-25 17:46:40 root INFO     [order_1_approx] starting weight calculation for If something is too painted, it is overpainted
If something is too strained, it is overstrained
If something is too stimulated, it is overstimulated
If something is too sold, it is oversold
If something is too used, it is overused
If something is too powered, it is overpowered
If something is too zealous, it is overzealous
If something is too dressed, it is
2024-07-25 17:46:40 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 17:49:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7656,  2.5625, -0.6875,  ...,  1.2031, -0.1445,  0.1089],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -1.1875,  -4.5000,  -5.5312,  ..., -10.0000,  20.8750,   1.2656],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.5391e-02, -6.1340e-03,  4.3335e-03,  ..., -3.6316e-03,
         -1.5259e-05,  1.8692e-03],
        [-9.9945e-04,  1.5869e-02, -7.4463e-03,  ...,  2.0599e-04,
         -1.2436e-03,  5.1575e-03],
        [ 4.8218e-03, -6.4087e-03,  8.9844e-02,  ..., -6.8359e-03,
          3.0365e-03, -2.8534e-03],
        ...,
        [ 4.1809e-03,  5.1270e-03, -7.8125e-03,  ...,  1.2695e-02,
         -4.9438e-03, -4.8218e-03],
        [ 1.8921e-03, -7.7820e-03,  8.5449e-03,  ..., -5.1575e-03,
          3.0884e-02, -6.8665e-04],
        [ 3.5706e-03,  6.7139e-04, -4.3030e-03,  ...,  2.4414e-03,
         -2.5940e-04,  3.4424e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.7461, -5.8359, -5.1445,  ..., -7.4219, 20.4531,  0.8496]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 17:49:25 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is too painted, it is overpainted
If something is too strained, it is overstrained
If something is too stimulated, it is overstimulated
If something is too sold, it is oversold
If something is too used, it is overused
If something is too powered, it is overpowered
If something is too zealous, it is overzealous
If something is too dressed, it is
2024-07-25 17:49:25 root INFO     total operator prediction time: 1325.0036664009094 seconds
2024-07-25 17:49:25 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on adj+ly_reg
2024-07-25 17:49:25 root INFO     building operator adj+ly_reg
2024-07-25 17:49:25 root INFO     [order_1_approx] starting weight calculation for The adjective form of historical is historically
The adjective form of internal is internally
The adjective form of visual is visually
The adjective form of global is globally
The adjective form of actual is actually
The adjective form of subsequent is subsequently
The adjective form of similar is similarly
The adjective form of traditional is
2024-07-25 17:49:26 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 17:52:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.1562, -0.4609,  0.9883,  ...,  0.6875,  0.8789,  1.1094],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-2.4688,  1.9375,  2.4531,  ...,  5.5000, -2.2812, -6.4688],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0281,  0.0014,  0.0029,  ...,  0.0043, -0.0029, -0.0035],
        [ 0.0042,  0.0152,  0.0160,  ..., -0.0069,  0.0098,  0.0099],
        [ 0.0011, -0.0017,  0.0874,  ..., -0.0047, -0.0043,  0.0074],
        ...,
        [-0.0004,  0.0031, -0.0034,  ...,  0.0231, -0.0178, -0.0048],
        [ 0.0019, -0.0046, -0.0074,  ...,  0.0025,  0.0349, -0.0046],
        [-0.0058,  0.0071,  0.0003,  ..., -0.0107,  0.0065,  0.0325]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8135,  3.2188,  2.3789,  ...,  4.6016, -3.3398, -5.5000]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 17:52:14 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of historical is historically
The adjective form of internal is internally
The adjective form of visual is visually
The adjective form of global is globally
The adjective form of actual is actually
The adjective form of subsequent is subsequently
The adjective form of similar is similarly
The adjective form of traditional is
2024-07-25 17:52:14 root INFO     [order_1_approx] starting weight calculation for The adjective form of global is globally
The adjective form of visual is visually
The adjective form of traditional is traditionally
The adjective form of historical is historically
The adjective form of internal is internally
The adjective form of actual is actually
The adjective form of similar is similarly
The adjective form of subsequent is
2024-07-25 17:52:14 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 17:55:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2734, -1.1016,  4.6562,  ...,  1.5781,  1.4375,  1.0234],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 6.3750,  2.5312, 11.4375,  ...,  7.7500,  5.0312,  2.5469],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0356,  0.0043,  0.0048,  ...,  0.0024, -0.0046, -0.0067],
        [-0.0008,  0.0227,  0.0049,  ..., -0.0051,  0.0069,  0.0054],
        [ 0.0075, -0.0002,  0.0947,  ..., -0.0098, -0.0019, -0.0007],
        ...,
        [-0.0024,  0.0013,  0.0079,  ...,  0.0188, -0.0126, -0.0014],
        [-0.0004, -0.0011, -0.0129,  ..., -0.0020,  0.0332, -0.0070],
        [-0.0058,  0.0017, -0.0005,  ..., -0.0049,  0.0039,  0.0298]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 7.3906,  3.7461, 12.2109,  ...,  8.8516,  5.6797,  3.0664]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 17:55:04 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of global is globally
The adjective form of visual is visually
The adjective form of traditional is traditionally
The adjective form of historical is historically
The adjective form of internal is internally
The adjective form of actual is actually
The adjective form of similar is similarly
The adjective form of subsequent is
2024-07-25 17:55:04 root INFO     [order_1_approx] starting weight calculation for The adjective form of global is globally
The adjective form of subsequent is subsequently
The adjective form of historical is historically
The adjective form of internal is internally
The adjective form of traditional is traditionally
The adjective form of similar is similarly
The adjective form of actual is actually
The adjective form of visual is
2024-07-25 17:55:04 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 17:57:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.9531, -0.5781,  3.1406,  ...,  0.7578,  1.7422, -0.5508],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([11.5000, 11.0000,  1.4062,  ..., -2.1250,  5.4375, -2.7812],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0254,  0.0020,  0.0027,  ...,  0.0048,  0.0012, -0.0040],
        [ 0.0058,  0.0219,  0.0123,  ..., -0.0114,  0.0076,  0.0072],
        [ 0.0096, -0.0019,  0.0713,  ...,  0.0018, -0.0038,  0.0012],
        ...,
        [ 0.0054,  0.0061, -0.0004,  ...,  0.0226, -0.0110, -0.0070],
        [-0.0054, -0.0017, -0.0067,  ...,  0.0063,  0.0231, -0.0002],
        [-0.0078,  0.0014, -0.0094,  ..., -0.0048,  0.0022,  0.0262]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[11.4453, 12.1172,  1.1982,  ..., -2.2695,  3.7461, -2.6738]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 17:57:54 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of global is globally
The adjective form of subsequent is subsequently
The adjective form of historical is historically
The adjective form of internal is internally
The adjective form of traditional is traditionally
The adjective form of similar is similarly
The adjective form of actual is actually
The adjective form of visual is
2024-07-25 17:57:54 root INFO     [order_1_approx] starting weight calculation for The adjective form of actual is actually
The adjective form of historical is historically
The adjective form of visual is visually
The adjective form of traditional is traditionally
The adjective form of global is globally
The adjective form of similar is similarly
The adjective form of subsequent is subsequently
The adjective form of internal is
2024-07-25 17:57:54 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 18:00:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.6562, -2.0469,  2.0156,  ...,  2.2344,  1.5156,  0.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 2.5625,  2.3750,  8.7500,  ...,  0.1758,  5.8750, 14.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.3804e-02,  7.4158e-03, -5.3406e-04,  ...,  2.4109e-03,
          8.3923e-04, -7.3242e-03],
        [ 8.0566e-03,  1.4160e-02, -1.1444e-03,  ..., -6.5308e-03,
          1.4160e-02,  1.4404e-02],
        [ 6.4087e-03, -4.3335e-03,  7.9102e-02,  ..., -4.4556e-03,
         -7.6904e-03,  6.7139e-03],
        ...,
        [ 1.5869e-03,  4.0283e-03, -6.7749e-03,  ...,  2.0996e-02,
         -1.1719e-02, -6.5613e-03],
        [ 6.8665e-05,  3.3875e-03, -1.6937e-03,  ..., -2.8076e-03,
          2.7100e-02, -2.5635e-03],
        [-7.3853e-03,  2.4109e-03,  1.2207e-02,  ..., -1.4282e-02,
          9.8877e-03,  3.6133e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.8311,  2.7246,  7.6602,  ...,  0.7979,  6.2539, 12.1875]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 18:00:44 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of actual is actually
The adjective form of historical is historically
The adjective form of visual is visually
The adjective form of traditional is traditionally
The adjective form of global is globally
The adjective form of similar is similarly
The adjective form of subsequent is subsequently
The adjective form of internal is
2024-07-25 18:00:44 root INFO     [order_1_approx] starting weight calculation for The adjective form of internal is internally
The adjective form of actual is actually
The adjective form of global is globally
The adjective form of traditional is traditionally
The adjective form of visual is visually
The adjective form of historical is historically
The adjective form of subsequent is subsequently
The adjective form of similar is
2024-07-25 18:00:44 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 18:03:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.9062, -3.6250, -0.2324,  ..., -1.0625,  1.8203, -1.0312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([15.8750, -3.3750, -5.1562,  ..., -4.4375,  0.5703,  6.9688],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0250, -0.0009,  0.0089,  ...,  0.0027, -0.0025, -0.0022],
        [ 0.0084,  0.0149,  0.0078,  ..., -0.0115,  0.0077,  0.0057],
        [ 0.0092,  0.0053,  0.0630,  ...,  0.0019, -0.0123,  0.0023],
        ...,
        [ 0.0012,  0.0028,  0.0017,  ...,  0.0171, -0.0098, -0.0014],
        [-0.0018,  0.0043, -0.0128,  ...,  0.0039,  0.0179, -0.0028],
        [-0.0129, -0.0005,  0.0062,  ..., -0.0079,  0.0046,  0.0195]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[17.8750, -1.2285, -4.8047,  ..., -5.0586, -0.6533,  7.3320]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 18:03:34 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of internal is internally
The adjective form of actual is actually
The adjective form of global is globally
The adjective form of traditional is traditionally
The adjective form of visual is visually
The adjective form of historical is historically
The adjective form of subsequent is subsequently
The adjective form of similar is
2024-07-25 18:03:34 root INFO     [order_1_approx] starting weight calculation for The adjective form of actual is actually
The adjective form of subsequent is subsequently
The adjective form of traditional is traditionally
The adjective form of visual is visually
The adjective form of global is globally
The adjective form of similar is similarly
The adjective form of internal is internally
The adjective form of historical is
2024-07-25 18:03:34 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 18:06:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.9453, -1.3828,  4.7812,  ...,  1.9766,  0.8477,  0.8633],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 9.3750,  8.6250, 12.4375,  ...,  6.2188,  4.1250, -0.5156],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0253, -0.0003, -0.0012,  ...,  0.0055,  0.0033, -0.0023],
        [ 0.0032,  0.0192,  0.0139,  ..., -0.0092,  0.0048,  0.0105],
        [ 0.0028,  0.0033,  0.0928,  ...,  0.0025, -0.0053,  0.0030],
        ...,
        [ 0.0030,  0.0058, -0.0023,  ...,  0.0259, -0.0178, -0.0020],
        [-0.0035, -0.0016, -0.0035,  ...,  0.0056,  0.0262, -0.0021],
        [-0.0070,  0.0029,  0.0049,  ..., -0.0123,  0.0013,  0.0359]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 8.4531, 11.2578, 12.7422,  ...,  5.1797,  4.3789, -0.8896]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 18:06:23 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of actual is actually
The adjective form of subsequent is subsequently
The adjective form of traditional is traditionally
The adjective form of visual is visually
The adjective form of global is globally
The adjective form of similar is similarly
The adjective form of internal is internally
The adjective form of historical is
2024-07-25 18:06:24 root INFO     [order_1_approx] starting weight calculation for The adjective form of subsequent is subsequently
The adjective form of similar is similarly
The adjective form of traditional is traditionally
The adjective form of visual is visually
The adjective form of internal is internally
The adjective form of global is globally
The adjective form of historical is historically
The adjective form of actual is
2024-07-25 18:06:24 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 18:09:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.2188,  0.3555,  0.5156,  ...,  1.1641,  0.9141,  0.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 8.6875,  3.8125,  5.5625,  ..., -3.1406, -1.6562,  4.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.0151e-02, -4.4250e-04, -1.1444e-03,  ...,  7.9956e-03,
         -2.6245e-03, -5.2490e-03],
        [ 1.4648e-03,  1.9287e-02,  8.9111e-03,  ..., -1.0193e-02,
          8.3008e-03,  9.1553e-03],
        [ 8.6670e-03, -3.0823e-03,  8.6426e-02,  ..., -1.4305e-05,
         -8.2397e-03,  7.3853e-03],
        ...,
        [-9.1553e-05,  5.1880e-03,  5.9204e-03,  ...,  2.8809e-02,
         -1.3367e-02, -4.1199e-03],
        [-1.9684e-03,  3.9368e-03, -1.1353e-02,  ...,  1.8921e-03,
          3.1250e-02,  2.7466e-03],
        [-7.2021e-03,  2.5635e-03, -5.2490e-03,  ..., -1.0315e-02,
          1.0498e-02,  3.5645e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 8.7500,  5.0195,  6.7266,  ..., -3.9746, -2.6602,  4.2031]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 18:09:11 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of subsequent is subsequently
The adjective form of similar is similarly
The adjective form of traditional is traditionally
The adjective form of visual is visually
The adjective form of internal is internally
The adjective form of global is globally
The adjective form of historical is historically
The adjective form of actual is
2024-07-25 18:09:11 root INFO     [order_1_approx] starting weight calculation for The adjective form of subsequent is subsequently
The adjective form of actual is actually
The adjective form of historical is historically
The adjective form of internal is internally
The adjective form of traditional is traditionally
The adjective form of visual is visually
The adjective form of similar is similarly
The adjective form of global is
2024-07-25 18:09:11 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 18:12:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2031, -1.5781,  4.0000,  ...,  2.3750, -0.1079,  2.1562],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([4.4688, 6.9375, 5.7500,  ..., 4.1875, 2.9844, 4.3750], device='cuda:0',
       dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0210,  0.0012,  0.0020,  ...,  0.0054, -0.0013, -0.0033],
        [ 0.0041,  0.0131,  0.0023,  ..., -0.0092,  0.0113,  0.0089],
        [ 0.0033,  0.0030,  0.0723,  ...,  0.0024, -0.0041, -0.0002],
        ...,
        [ 0.0035,  0.0031,  0.0075,  ...,  0.0193, -0.0118, -0.0013],
        [ 0.0017,  0.0018,  0.0004,  ...,  0.0033,  0.0271,  0.0002],
        [-0.0047,  0.0039, -0.0006,  ..., -0.0085,  0.0076,  0.0255]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[4.2422, 8.5625, 4.5078,  ..., 4.5859, 3.2715, 6.1094]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 18:12:01 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of subsequent is subsequently
The adjective form of actual is actually
The adjective form of historical is historically
The adjective form of internal is internally
The adjective form of traditional is traditionally
The adjective form of visual is visually
The adjective form of similar is similarly
The adjective form of global is
2024-07-25 18:12:01 root INFO     total operator prediction time: 1356.1057777404785 seconds
2024-07-25 18:12:01 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on verb+tion_irreg
2024-07-25 18:12:01 root INFO     building operator verb+tion_irreg
2024-07-25 18:12:02 root INFO     [order_1_approx] starting weight calculation for To improvize results in improvization
To specialize results in specialization
To aspire results in aspiration
To install results in installation
To reorganize results in reorganization
To imagine results in imagination
To organize results in organization
To determine results in
2024-07-25 18:12:02 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 18:14:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0781,  0.0469,  5.2812,  ...,  1.3594, -0.3867,  3.4219],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([13.5000,  4.1250, -9.8125,  ...,  2.3438, -7.7500, 13.4375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0320,  0.0024,  0.0008,  ...,  0.0011,  0.0012, -0.0012],
        [ 0.0020,  0.0214,  0.0022,  ..., -0.0023, -0.0024,  0.0052],
        [ 0.0026,  0.0039,  0.1089,  ..., -0.0016, -0.0031, -0.0027],
        ...,
        [ 0.0038,  0.0014, -0.0087,  ...,  0.0216, -0.0004,  0.0040],
        [-0.0023, -0.0013,  0.0022,  ..., -0.0027,  0.0376,  0.0010],
        [-0.0041, -0.0081, -0.0053,  ..., -0.0070, -0.0008,  0.0479]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 12.8750,   4.3672, -11.0469,  ...,   2.8516,  -7.1484,  11.8516]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 18:14:49 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To improvize results in improvization
To specialize results in specialization
To aspire results in aspiration
To install results in installation
To reorganize results in reorganization
To imagine results in imagination
To organize results in organization
To determine results in
2024-07-25 18:14:49 root INFO     [order_1_approx] starting weight calculation for To reorganize results in reorganization
To improvize results in improvization
To determine results in determination
To aspire results in aspiration
To organize results in organization
To install results in installation
To imagine results in imagination
To specialize results in
2024-07-25 18:14:49 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 18:17:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.1562,  0.0781,  1.9062,  ..., -0.2578, -0.7812,  1.6328],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 15.5000,  13.1875,  -1.7188,  ...,  17.0000, -13.3750,   4.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0320, -0.0016,  0.0009,  ...,  0.0092,  0.0045,  0.0092],
        [ 0.0012,  0.0248,  0.0093,  ..., -0.0055,  0.0027,  0.0057],
        [ 0.0008,  0.0017,  0.1172,  ..., -0.0007, -0.0008, -0.0029],
        ...,
        [ 0.0035,  0.0056, -0.0067,  ...,  0.0272,  0.0049,  0.0038],
        [-0.0007,  0.0014, -0.0061,  ...,  0.0033,  0.0400,  0.0016],
        [ 0.0029,  0.0041, -0.0038,  ..., -0.0051,  0.0022,  0.0503]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 15.9844,  12.4766,  -0.6787,  ...,  17.9375, -14.1953,   4.0586]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 18:17:34 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To reorganize results in reorganization
To improvize results in improvization
To determine results in determination
To aspire results in aspiration
To organize results in organization
To install results in installation
To imagine results in imagination
To specialize results in
2024-07-25 18:17:35 root INFO     [order_1_approx] starting weight calculation for To reorganize results in reorganization
To improvize results in improvization
To organize results in organization
To specialize results in specialization
To aspire results in aspiration
To determine results in determination
To install results in installation
To imagine results in
2024-07-25 18:17:35 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 18:20:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0156,  1.7578,  1.4375,  ...,  1.1562, -0.0156,  0.3711],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 15.2500,  15.8750, -12.8750,  ...,  12.0000, -17.1250,  -8.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0364, -0.0018,  0.0009,  ...,  0.0046, -0.0036,  0.0031],
        [ 0.0026,  0.0212,  0.0089,  ..., -0.0036, -0.0006,  0.0101],
        [ 0.0020,  0.0037,  0.1064,  ..., -0.0025,  0.0022, -0.0019],
        ...,
        [ 0.0109,  0.0008, -0.0051,  ...,  0.0208, -0.0053,  0.0037],
        [-0.0022,  0.0008, -0.0068,  ...,  0.0010,  0.0476, -0.0025],
        [-0.0015,  0.0018, -0.0135,  ..., -0.0039,  0.0021,  0.0393]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 15.8828,  16.5000, -13.6250,  ...,  12.7578, -18.2969,  -8.2734]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 18:20:22 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To reorganize results in reorganization
To improvize results in improvization
To organize results in organization
To specialize results in specialization
To aspire results in aspiration
To determine results in determination
To install results in installation
To imagine results in
2024-07-25 18:20:22 root INFO     [order_1_approx] starting weight calculation for To specialize results in specialization
To reorganize results in reorganization
To determine results in determination
To install results in installation
To improvize results in improvization
To organize results in organization
To imagine results in imagination
To aspire results in
2024-07-25 18:20:22 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 18:23:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0703,  1.4844,  0.8359,  ..., -0.7656,  0.8516,  0.2891],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 18.5000,   9.3125, -16.2500,  ...,   7.3438, -14.1250,   3.9531],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.8809e-02, -1.7090e-03, -7.7209e-03,  ...,  9.0790e-04,
         -5.3406e-05,  4.5471e-03],
        [ 6.0272e-04,  1.7334e-02,  6.4087e-03,  ..., -4.0894e-03,
          4.2114e-03,  7.4463e-03],
        [ 2.3346e-03,  6.7444e-03,  1.0547e-01,  ..., -5.8594e-03,
          1.0147e-03, -4.8523e-03],
        ...,
        [ 9.2773e-03,  9.8877e-03, -2.0996e-02,  ...,  2.2217e-02,
         -4.7913e-03,  1.3924e-04],
        [ 9.3842e-04,  2.4567e-03, -1.3672e-02,  ...,  1.2970e-03,
          3.2715e-02, -3.7842e-03],
        [ 4.8523e-03,  3.4180e-03, -3.0518e-04,  ..., -8.3618e-03,
         -1.3123e-03,  3.5889e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 19.2188,   9.0234, -16.7344,  ...,   8.5625, -12.9844,   4.5742]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 18:23:10 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To specialize results in specialization
To reorganize results in reorganization
To determine results in determination
To install results in installation
To improvize results in improvization
To organize results in organization
To imagine results in imagination
To aspire results in
2024-07-25 18:23:10 root INFO     [order_1_approx] starting weight calculation for To reorganize results in reorganization
To install results in installation
To imagine results in imagination
To specialize results in specialization
To organize results in organization
To determine results in determination
To aspire results in aspiration
To improvize results in
2024-07-25 18:23:11 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 18:25:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.8828,  0.2734, -3.9688,  ...,  1.7188, -0.8320,  2.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([18.2500, 11.6875, -5.1875,  ..., 13.0625, -3.2188,  2.4062],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0425, -0.0044,  0.0148,  ...,  0.0005,  0.0032,  0.0067],
        [ 0.0002,  0.0181,  0.0254,  ..., -0.0047,  0.0049,  0.0157],
        [-0.0017, -0.0068,  0.1396,  ..., -0.0024,  0.0019,  0.0093],
        ...,
        [ 0.0042,  0.0082, -0.0087,  ...,  0.0247, -0.0018,  0.0003],
        [-0.0052,  0.0018, -0.0147,  ...,  0.0006,  0.0510,  0.0050],
        [ 0.0005,  0.0020, -0.0034,  ...,  0.0030,  0.0054,  0.0630]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[17.7812, 13.7500, -9.2969,  ..., 12.2422, -5.2227,  0.7861]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 18:25:58 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To reorganize results in reorganization
To install results in installation
To imagine results in imagination
To specialize results in specialization
To organize results in organization
To determine results in determination
To aspire results in aspiration
To improvize results in
2024-07-25 18:25:58 root INFO     [order_1_approx] starting weight calculation for To organize results in organization
To reorganize results in reorganization
To imagine results in imagination
To aspire results in aspiration
To improvize results in improvization
To specialize results in specialization
To determine results in determination
To install results in
2024-07-25 18:25:58 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 18:28:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6094,  0.1172,  1.3516,  ...,  1.6250,  1.4141,  2.9219],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 7.5000,  5.1250, -5.3125,  ..., 15.6250, -0.6562,  5.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0271, -0.0023, -0.0105,  ..., -0.0020,  0.0007,  0.0026],
        [ 0.0040,  0.0214,  0.0020,  ...,  0.0034,  0.0049,  0.0048],
        [-0.0012, -0.0021,  0.0879,  ...,  0.0030, -0.0032,  0.0001],
        ...,
        [ 0.0054,  0.0009, -0.0181,  ...,  0.0221, -0.0056,  0.0092],
        [-0.0030, -0.0036, -0.0045,  ..., -0.0018,  0.0393, -0.0001],
        [-0.0015,  0.0095, -0.0039,  ..., -0.0034,  0.0039,  0.0383]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 9.3828,  5.9609, -5.7070,  ..., 15.1250, -1.7344,  5.6211]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 18:28:46 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To organize results in organization
To reorganize results in reorganization
To imagine results in imagination
To aspire results in aspiration
To improvize results in improvization
To specialize results in specialization
To determine results in determination
To install results in
2024-07-25 18:28:46 root INFO     [order_1_approx] starting weight calculation for To specialize results in specialization
To install results in installation
To determine results in determination
To aspire results in aspiration
To improvize results in improvization
To imagine results in imagination
To organize results in organization
To reorganize results in
2024-07-25 18:28:46 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 18:31:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7422,  0.5312, -2.8125,  ..., -0.4883,  1.1094,  2.2969],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([16.6250, -6.2188, -2.4062,  ...,  8.3750,  1.2500, -7.5938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.3000e-02, -8.3160e-04,  2.9907e-03,  ...,  1.0872e-04,
          1.0014e-04, -2.7924e-03],
        [-8.7357e-04,  8.6670e-03,  3.6926e-03,  ..., -3.5095e-04,
          2.2430e-03,  3.6316e-03],
        [ 1.8311e-03,  2.1667e-03,  4.1504e-02,  ..., -2.8687e-03,
         -2.5787e-03,  9.1553e-05],
        ...,
        [ 2.1515e-03,  1.0223e-03, -1.2970e-03,  ...,  9.0332e-03,
          1.7090e-03,  9.4986e-04],
        [ 1.8692e-03,  2.5940e-04, -4.0283e-03,  ...,  1.2283e-03,
          1.5259e-02,  2.1057e-03],
        [ 2.4414e-04,  1.8082e-03, -5.3101e-03,  ..., -1.2016e-04,
          4.5776e-04,  1.1658e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[15.8516, -6.1562, -2.5312,  ...,  8.7344,  1.3623, -8.6406]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 18:31:34 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To specialize results in specialization
To install results in installation
To determine results in determination
To aspire results in aspiration
To improvize results in improvization
To imagine results in imagination
To organize results in organization
To reorganize results in
2024-07-25 18:31:34 root INFO     [order_1_approx] starting weight calculation for To specialize results in specialization
To install results in installation
To imagine results in imagination
To aspire results in aspiration
To reorganize results in reorganization
To improvize results in improvization
To determine results in determination
To organize results in
2024-07-25 18:31:34 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 18:34:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3906, -0.8828, -3.1094,  ...,  1.7656,  1.3203,  1.7109],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([15.8750,  7.8750, -2.0781,  ..., 13.8750, -3.4688, -3.3438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.5146e-02, -2.4719e-03, -3.3264e-03,  ...,  2.0905e-03,
          4.3945e-03,  8.1787e-03],
        [ 3.9368e-03,  1.4648e-02,  3.2043e-04,  ..., -2.7771e-03,
         -3.3569e-03,  6.1035e-03],
        [ 2.1515e-03,  1.4648e-03,  1.0107e-01,  ..., -4.8523e-03,
         -3.3264e-03, -2.6855e-03],
        ...,
        [ 5.7068e-03, -1.2131e-03, -5.0049e-03,  ...,  1.7090e-02,
         -4.0283e-03,  1.3580e-03],
        [-1.7166e-03,  7.6294e-06, -4.5776e-03,  ..., -5.8174e-05,
          3.2959e-02,  1.7242e-03],
        [-2.2888e-03,  4.8828e-03, -1.0620e-02,  ..., -5.5237e-03,
         -4.2725e-04,  3.4424e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[16.0156,  7.9531, -2.9395,  ..., 13.1719, -3.0430, -3.0234]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 18:34:21 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To specialize results in specialization
To install results in installation
To imagine results in imagination
To aspire results in aspiration
To reorganize results in reorganization
To improvize results in improvization
To determine results in determination
To organize results in
2024-07-25 18:34:21 root INFO     total operator prediction time: 1339.9067466259003 seconds
2024-07-25 18:34:21 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on verb+able_reg
2024-07-25 18:34:21 root INFO     building operator verb+able_reg
2024-07-25 18:34:21 root INFO     [order_1_approx] starting weight calculation for If you can admire something, that thing is admirable
If you can understand something, that thing is understandable
If you can dispose something, that thing is disposable
If you can expand something, that thing is expandable
If you can contain something, that thing is containable
If you can recognize something, that thing is recognizable
If you can download something, that thing is downloadable
If you can manage something, that thing is
2024-07-25 18:34:22 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 18:37:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0938,  2.0938,  2.7031,  ...,  1.0078, -0.5547,  0.7773],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 14.6250,   7.9688, -12.0000,  ...,  -9.5625,  -7.5625,  -7.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0332, -0.0028, -0.0038,  ...,  0.0027, -0.0022,  0.0012],
        [ 0.0022,  0.0170,  0.0048,  ...,  0.0015,  0.0043,  0.0003],
        [-0.0036,  0.0005,  0.0889,  ..., -0.0014, -0.0010, -0.0023],
        ...,
        [-0.0019,  0.0024, -0.0120,  ...,  0.0177, -0.0055,  0.0001],
        [ 0.0015,  0.0008,  0.0015,  ..., -0.0009,  0.0403, -0.0039],
        [-0.0027,  0.0012, -0.0017,  ..., -0.0012, -0.0003,  0.0376]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 14.4219,   8.3750, -11.3672,  ...,  -9.7656,  -7.1875,  -7.4766]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 18:37:07 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you can admire something, that thing is admirable
If you can understand something, that thing is understandable
If you can dispose something, that thing is disposable
If you can expand something, that thing is expandable
If you can contain something, that thing is containable
If you can recognize something, that thing is recognizable
If you can download something, that thing is downloadable
If you can manage something, that thing is
2024-07-25 18:37:07 root INFO     [order_1_approx] starting weight calculation for If you can admire something, that thing is admirable
If you can dispose something, that thing is disposable
If you can expand something, that thing is expandable
If you can manage something, that thing is manageable
If you can recognize something, that thing is recognizable
If you can contain something, that thing is containable
If you can understand something, that thing is understandable
If you can download something, that thing is
2024-07-25 18:37:07 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 18:39:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.6328, -2.1094,  5.5312,  ...,  1.9844, -1.2344,  1.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  5.3125,   6.7500,  -4.5938,  ..., -16.7500,   1.0625,   4.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0310, -0.0015, -0.0103,  ...,  0.0002,  0.0010,  0.0076],
        [ 0.0051,  0.0178,  0.0016,  ..., -0.0019,  0.0045,  0.0033],
        [-0.0015, -0.0027,  0.0923,  ..., -0.0009, -0.0051,  0.0025],
        ...,
        [ 0.0043,  0.0034, -0.0093,  ...,  0.0176, -0.0008,  0.0041],
        [-0.0034,  0.0014,  0.0009,  ..., -0.0032,  0.0410,  0.0003],
        [-0.0049,  0.0005, -0.0002,  ..., -0.0034,  0.0019,  0.0388]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  4.4062,   6.7852,  -4.6602,  ..., -16.6875,   1.3516,   4.9922]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 18:39:53 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you can admire something, that thing is admirable
If you can dispose something, that thing is disposable
If you can expand something, that thing is expandable
If you can manage something, that thing is manageable
If you can recognize something, that thing is recognizable
If you can contain something, that thing is containable
If you can understand something, that thing is understandable
If you can download something, that thing is
2024-07-25 18:39:53 root INFO     [order_1_approx] starting weight calculation for If you can dispose something, that thing is disposable
If you can manage something, that thing is manageable
If you can contain something, that thing is containable
If you can admire something, that thing is admirable
If you can recognize something, that thing is recognizable
If you can expand something, that thing is expandable
If you can download something, that thing is downloadable
If you can understand something, that thing is
2024-07-25 18:39:53 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 18:42:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.4219,  0.4766,  5.1875,  ..., -0.2891, -0.5273,  2.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([15.1250, -4.4375, -9.0000,  ..., -3.1562, -4.6875,  4.2188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.7588e-02, -5.1880e-04, -6.5308e-03,  ...,  3.9978e-03,
          2.4872e-03,  5.0659e-03],
        [ 5.6152e-03,  1.8311e-02, -8.5449e-03,  ..., -4.0436e-04,
          1.9531e-03, -1.9989e-03],
        [-3.4180e-03, -4.7302e-03,  9.4727e-02,  ..., -1.1292e-03,
          2.1057e-03, -1.5411e-03],
        ...,
        [-1.9836e-03,  7.1716e-03, -1.4465e-02,  ...,  1.7822e-02,
         -5.0354e-04, -2.3041e-03],
        [ 4.1504e-03,  6.2561e-03, -5.3406e-03,  ..., -6.9046e-04,
          3.7109e-02, -3.4485e-03],
        [-2.3651e-04, -1.6556e-03,  2.4567e-03,  ...,  2.2125e-04,
          8.0109e-05,  3.6621e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[15.7812, -4.4688, -8.5234,  ..., -4.1953, -4.8320,  5.1172]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 18:42:39 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you can dispose something, that thing is disposable
If you can manage something, that thing is manageable
If you can contain something, that thing is containable
If you can admire something, that thing is admirable
If you can recognize something, that thing is recognizable
If you can expand something, that thing is expandable
If you can download something, that thing is downloadable
If you can understand something, that thing is
2024-07-25 18:42:39 root INFO     [order_1_approx] starting weight calculation for If you can understand something, that thing is understandable
If you can expand something, that thing is expandable
If you can dispose something, that thing is disposable
If you can recognize something, that thing is recognizable
If you can contain something, that thing is containable
If you can manage something, that thing is manageable
If you can download something, that thing is downloadable
If you can admire something, that thing is
2024-07-25 18:42:39 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 18:45:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.7109,  3.3125,  0.5977,  ..., -1.3359, -2.1562,  2.1562],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 16.3750,  13.6250, -21.1250,  ...,  -3.3750,  -8.6875,   1.0469],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0330, -0.0005, -0.0015,  ..., -0.0039,  0.0106,  0.0033],
        [-0.0023,  0.0165, -0.0038,  ...,  0.0030,  0.0017,  0.0054],
        [ 0.0042, -0.0043,  0.1187,  ..., -0.0005,  0.0006,  0.0099],
        ...,
        [ 0.0107,  0.0058, -0.0058,  ...,  0.0208, -0.0092, -0.0039],
        [-0.0084,  0.0052, -0.0053,  ..., -0.0003,  0.0410, -0.0011],
        [-0.0034, -0.0003, -0.0063,  ..., -0.0037, -0.0012,  0.0391]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 17.6406,  11.3594, -20.0625,  ...,  -0.6016,  -9.4922,  -0.1436]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 18:45:25 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you can understand something, that thing is understandable
If you can expand something, that thing is expandable
If you can dispose something, that thing is disposable
If you can recognize something, that thing is recognizable
If you can contain something, that thing is containable
If you can manage something, that thing is manageable
If you can download something, that thing is downloadable
If you can admire something, that thing is
2024-07-25 18:45:25 root INFO     [order_1_approx] starting weight calculation for If you can contain something, that thing is containable
If you can expand something, that thing is expandable
If you can recognize something, that thing is recognizable
If you can understand something, that thing is understandable
If you can admire something, that thing is admirable
If you can download something, that thing is downloadable
If you can manage something, that thing is manageable
If you can dispose something, that thing is
2024-07-25 18:45:25 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 18:48:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.6328, -0.2891,  2.9688,  ...,  0.2207, -2.0625, -0.2656],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  7.9375,   4.1250, -12.2500,  ...,   4.3438,   2.9531,  -0.2031],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0300,  0.0045,  0.0073,  ...,  0.0025, -0.0007, -0.0011],
        [ 0.0004,  0.0184, -0.0006,  ...,  0.0005,  0.0074,  0.0015],
        [ 0.0057, -0.0096,  0.0850,  ...,  0.0012, -0.0016,  0.0042],
        ...,
        [ 0.0040,  0.0035, -0.0069,  ...,  0.0162,  0.0020,  0.0027],
        [-0.0042,  0.0019,  0.0008,  ..., -0.0034,  0.0339, -0.0016],
        [-0.0029, -0.0010, -0.0152,  ..., -0.0019,  0.0010,  0.0344]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  5.6719,   4.8828, -11.3359,  ...,   4.2227,   3.8438,  -0.6348]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 18:48:08 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you can contain something, that thing is containable
If you can expand something, that thing is expandable
If you can recognize something, that thing is recognizable
If you can understand something, that thing is understandable
If you can admire something, that thing is admirable
If you can download something, that thing is downloadable
If you can manage something, that thing is manageable
If you can dispose something, that thing is
2024-07-25 18:48:08 root INFO     [order_1_approx] starting weight calculation for If you can manage something, that thing is manageable
If you can understand something, that thing is understandable
If you can contain something, that thing is containable
If you can dispose something, that thing is disposable
If you can admire something, that thing is admirable
If you can expand something, that thing is expandable
If you can download something, that thing is downloadable
If you can recognize something, that thing is
2024-07-25 18:48:08 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 18:50:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.7500,  2.6875,  0.2773,  ..., -0.7422,  0.1328,  2.8438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  4.4062,  11.8125, -20.6250,  ...,   7.7500,   0.9531,  -6.2812],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.2217e-02, -1.2360e-03, -3.6621e-04,  ...,  3.8910e-04,
          1.0132e-02,  4.8523e-03],
        [-1.8158e-03,  1.5259e-02, -1.6846e-02,  ..., -1.7471e-03,
          4.0894e-03, -2.6855e-03],
        [ 4.6997e-03, -4.4861e-03,  1.0645e-01,  ..., -8.3923e-04,
         -1.5717e-03,  8.5068e-04],
        ...,
        [-2.0599e-03,  6.8665e-03, -2.7466e-03,  ...,  1.5198e-02,
         -3.4943e-03, -2.1210e-03],
        [ 3.8147e-06,  6.6833e-03,  2.2278e-03,  ..., -4.8828e-04,
          3.8574e-02,  1.6632e-03],
        [-4.9973e-04,  2.3193e-03, -7.1106e-03,  ..., -2.5024e-03,
         -5.0964e-03,  3.8330e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  4.5625,  12.1719, -18.4844,  ...,   7.9219,   0.5645,  -5.9414]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 18:50:53 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you can manage something, that thing is manageable
If you can understand something, that thing is understandable
If you can contain something, that thing is containable
If you can dispose something, that thing is disposable
If you can admire something, that thing is admirable
If you can expand something, that thing is expandable
If you can download something, that thing is downloadable
If you can recognize something, that thing is
2024-07-25 18:50:54 root INFO     [order_1_approx] starting weight calculation for If you can understand something, that thing is understandable
If you can manage something, that thing is manageable
If you can recognize something, that thing is recognizable
If you can download something, that thing is downloadable
If you can dispose something, that thing is disposable
If you can contain something, that thing is containable
If you can admire something, that thing is admirable
If you can expand something, that thing is
2024-07-25 18:50:54 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 18:53:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1875,  1.1797, -1.6797,  ...,  0.7227, -2.5312,  1.3125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 20.0000,   7.1875,  -7.3750,  ...,   2.3594, -18.7500,  -3.0469],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.0874e-02,  1.5106e-03, -1.4648e-03,  ...,  6.5308e-03,
         -8.3008e-03, -1.9836e-03],
        [ 4.1504e-03,  1.5503e-02, -3.8300e-03,  ..., -8.8501e-04,
          3.1433e-03,  4.1199e-03],
        [ 3.8452e-03, -3.2806e-03,  9.6680e-02,  ..., -5.1880e-03,
         -6.9046e-04,  7.5073e-03],
        ...,
        [ 8.6670e-03,  2.4719e-03, -1.1597e-02,  ...,  1.8066e-02,
          2.0447e-03, -1.2665e-03],
        [ 1.6937e-03,  9.7656e-04, -6.1951e-03,  ..., -7.5073e-03,
          3.6133e-02, -1.6708e-03],
        [-2.3651e-03,  1.7471e-03, -7.6294e-05,  ..., -3.5553e-03,
         -4.9591e-04,  3.8330e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 19.2500,   8.5625,  -6.3047,  ...,   2.5742, -20.0781,  -3.4844]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 18:53:39 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you can understand something, that thing is understandable
If you can manage something, that thing is manageable
If you can recognize something, that thing is recognizable
If you can download something, that thing is downloadable
If you can dispose something, that thing is disposable
If you can contain something, that thing is containable
If you can admire something, that thing is admirable
If you can expand something, that thing is
2024-07-25 18:53:39 root INFO     [order_1_approx] starting weight calculation for If you can admire something, that thing is admirable
If you can dispose something, that thing is disposable
If you can recognize something, that thing is recognizable
If you can expand something, that thing is expandable
If you can download something, that thing is downloadable
If you can understand something, that thing is understandable
If you can manage something, that thing is manageable
If you can contain something, that thing is
2024-07-25 18:53:39 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 18:56:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6250,  0.4219,  2.1719,  ...,  0.9375, -1.2500,  2.0938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 6.5000, 12.2500, -7.2812,  ...,  4.8750, -9.8125, -0.1562],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0396, -0.0025, -0.0064,  ...,  0.0041, -0.0056,  0.0019],
        [ 0.0048,  0.0208,  0.0025,  ..., -0.0012,  0.0045,  0.0039],
        [-0.0010, -0.0007,  0.1250,  ..., -0.0025,  0.0059,  0.0034],
        ...,
        [-0.0018,  0.0044, -0.0065,  ...,  0.0205, -0.0042, -0.0020],
        [-0.0055, -0.0015,  0.0006,  ..., -0.0089,  0.0566,  0.0051],
        [-0.0012, -0.0009, -0.0092,  ..., -0.0006,  0.0078,  0.0479]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  6.0000,  12.7344,  -7.0039,  ...,   4.7500, -11.1484,   0.8301]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 18:56:26 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you can admire something, that thing is admirable
If you can dispose something, that thing is disposable
If you can recognize something, that thing is recognizable
If you can expand something, that thing is expandable
If you can download something, that thing is downloadable
If you can understand something, that thing is understandable
If you can manage something, that thing is manageable
If you can contain something, that thing is
2024-07-25 18:56:26 root INFO     total operator prediction time: 1324.3859241008759 seconds
2024-07-25 18:56:26 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on un+adj_reg
2024-07-25 18:56:26 root INFO     building operator un+adj_reg
2024-07-25 18:56:26 root INFO     [order_1_approx] starting weight calculation for The opposite of changed is unchanged
The opposite of pleasant is unpleasant
The opposite of acceptable is unacceptable
The opposite of wanted is unwanted
The opposite of restricted is unrestricted
The opposite of fortunate is unfortunate
The opposite of successful is unsuccessful
The opposite of authorized is
2024-07-25 18:56:26 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 18:59:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8828,  1.4375,  4.4062,  ...,  1.8438,  2.2812,  3.7188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 3.4531, -6.6250,  7.6250,  ...,  6.9375,  9.2500,  9.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.2949e-02, -6.1035e-05,  2.9907e-03,  ...,  2.6398e-03,
         -4.4861e-03, -4.6387e-03],
        [-1.5259e-05,  1.2695e-02,  9.3384e-03,  ..., -5.5237e-03,
         -7.1106e-03,  5.5542e-03],
        [-7.7057e-04, -5.3406e-03,  1.0205e-01,  ..., -1.5259e-05,
          1.0132e-02, -3.6774e-03],
        ...,
        [ 5.3406e-03, -1.5717e-03, -7.6294e-04,  ...,  1.0498e-02,
          1.2329e-02, -1.5106e-03],
        [ 2.6245e-03,  2.9602e-03,  4.7607e-03,  ..., -2.3804e-03,
          3.7842e-02, -4.2725e-03],
        [-9.2773e-03, -1.8921e-03, -1.4099e-02,  ..., -1.4160e-02,
         -1.8921e-03,  3.7842e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.8730, -5.7734,  9.4922,  ...,  8.1641, 11.2422,  8.1094]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 18:59:13 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of changed is unchanged
The opposite of pleasant is unpleasant
The opposite of acceptable is unacceptable
The opposite of wanted is unwanted
The opposite of restricted is unrestricted
The opposite of fortunate is unfortunate
The opposite of successful is unsuccessful
The opposite of authorized is
2024-07-25 18:59:13 root INFO     [order_1_approx] starting weight calculation for The opposite of authorized is unauthorized
The opposite of fortunate is unfortunate
The opposite of pleasant is unpleasant
The opposite of wanted is unwanted
The opposite of restricted is unrestricted
The opposite of acceptable is unacceptable
The opposite of changed is unchanged
The opposite of successful is
2024-07-25 18:59:13 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 19:01:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8789, -2.3125,  1.5234,  ..., -1.3281,  0.4414, -0.3789],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 2.7188, -0.5312,  3.9375,  ...,  0.0469,  3.6875, -3.1094],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0271,  0.0014, -0.0035,  ...,  0.0039,  0.0059, -0.0005],
        [-0.0044,  0.0143, -0.0136,  ...,  0.0057,  0.0047,  0.0099],
        [ 0.0016, -0.0019,  0.1260,  ..., -0.0026,  0.0025, -0.0070],
        ...,
        [-0.0128, -0.0043,  0.0014,  ...,  0.0205, -0.0092, -0.0090],
        [-0.0007,  0.0003, -0.0137,  ...,  0.0040,  0.0327,  0.0014],
        [-0.0053,  0.0028, -0.0242,  ..., -0.0092, -0.0020,  0.0376]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.0195, -1.1152,  5.0039,  ...,  0.1159,  4.3633, -5.0781]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 19:01:59 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of authorized is unauthorized
The opposite of fortunate is unfortunate
The opposite of pleasant is unpleasant
The opposite of wanted is unwanted
The opposite of restricted is unrestricted
The opposite of acceptable is unacceptable
The opposite of changed is unchanged
The opposite of successful is
2024-07-25 19:02:00 root INFO     [order_1_approx] starting weight calculation for The opposite of successful is unsuccessful
The opposite of restricted is unrestricted
The opposite of wanted is unwanted
The opposite of authorized is unauthorized
The opposite of changed is unchanged
The opposite of acceptable is unacceptable
The opposite of fortunate is unfortunate
The opposite of pleasant is
2024-07-25 19:02:00 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 19:04:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3750, -2.1406,  1.4375,  ..., -0.9766,  2.1094,  0.5898],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  5.7188, -18.6250,  -6.2812,  ...,   2.7969,  -0.3945,   2.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0317,  0.0035, -0.0062,  ...,  0.0030, -0.0096, -0.0131],
        [ 0.0027,  0.0189, -0.0247,  ...,  0.0040,  0.0183,  0.0085],
        [ 0.0033, -0.0017,  0.1196,  ...,  0.0008,  0.0129, -0.0007],
        ...,
        [ 0.0035,  0.0008, -0.0172,  ...,  0.0208, -0.0128, -0.0066],
        [ 0.0043,  0.0101, -0.0208,  ..., -0.0025,  0.0295,  0.0071],
        [ 0.0018,  0.0078, -0.0145,  ..., -0.0075,  0.0014,  0.0425]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  6.6797, -19.7188,  -3.7051,  ...,   4.2773,  -2.2930,   0.5273]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 19:04:50 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of successful is unsuccessful
The opposite of restricted is unrestricted
The opposite of wanted is unwanted
The opposite of authorized is unauthorized
The opposite of changed is unchanged
The opposite of acceptable is unacceptable
The opposite of fortunate is unfortunate
The opposite of pleasant is
2024-07-25 19:05:05 root INFO     [order_1_approx] starting weight calculation for The opposite of acceptable is unacceptable
The opposite of wanted is unwanted
The opposite of successful is unsuccessful
The opposite of authorized is unauthorized
The opposite of fortunate is unfortunate
The opposite of restricted is unrestricted
The opposite of pleasant is unpleasant
The opposite of changed is
2024-07-25 19:05:06 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 19:07:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.6250, -1.6094, -5.8438,  ...,  0.9688,  2.4062, -1.0781],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  9.1250, -11.5000,   0.5625,  ...,   0.4062,  -3.6875,   4.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 4.1504e-02, -1.0681e-03,  1.4099e-02,  ...,  1.1963e-02,
          2.8229e-03,  1.8311e-02],
        [-8.3923e-04,  2.6245e-02, -7.9956e-03,  ..., -3.7842e-03,
          1.6022e-04,  4.5776e-03],
        [ 2.1667e-03, -5.9204e-03,  1.4062e-01,  ..., -4.6692e-03,
          1.4648e-03, -2.1362e-03],
        ...,
        [ 6.6528e-03, -1.0498e-02, -1.7456e-02,  ...,  1.7944e-02,
          3.9673e-04, -1.2695e-02],
        [ 3.8147e-04, -1.9455e-03, -9.7656e-03,  ..., -3.3875e-03,
          4.8828e-02, -4.0588e-03],
        [ 9.1553e-04,  9.1553e-05, -2.4872e-03,  ..., -7.9346e-03,
         -5.2795e-03,  4.9316e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  9.0859, -12.0312,   1.1543,  ...,  -1.0674,  -4.5664,   3.2539]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 19:07:52 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of acceptable is unacceptable
The opposite of wanted is unwanted
The opposite of successful is unsuccessful
The opposite of authorized is unauthorized
The opposite of fortunate is unfortunate
The opposite of restricted is unrestricted
The opposite of pleasant is unpleasant
The opposite of changed is
2024-07-25 19:07:52 root INFO     [order_1_approx] starting weight calculation for The opposite of pleasant is unpleasant
The opposite of changed is unchanged
The opposite of authorized is unauthorized
The opposite of fortunate is unfortunate
The opposite of successful is unsuccessful
The opposite of wanted is unwanted
The opposite of restricted is unrestricted
The opposite of acceptable is
2024-07-25 19:07:52 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 19:10:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0938, -2.3438,  1.2422,  ...,  0.0000,  3.4844,  0.3125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 0.4531, -9.6250,  1.1875,  ..., -7.2500, -0.3672,  4.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0253,  0.0002, -0.0067,  ...,  0.0019, -0.0086, -0.0052],
        [ 0.0036,  0.0131, -0.0140,  ..., -0.0056,  0.0133,  0.0039],
        [-0.0033, -0.0030,  0.1157,  ...,  0.0012,  0.0181, -0.0005],
        ...,
        [ 0.0005, -0.0015, -0.0025,  ...,  0.0130,  0.0055, -0.0032],
        [-0.0022, -0.0027, -0.0047,  ..., -0.0001,  0.0303,  0.0029],
        [-0.0032,  0.0016, -0.0081,  ...,  0.0006, -0.0034,  0.0339]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  1.1895, -10.6094,   1.5234,  ...,  -8.9297,  -0.7441,   4.2344]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 19:10:39 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of pleasant is unpleasant
The opposite of changed is unchanged
The opposite of authorized is unauthorized
The opposite of fortunate is unfortunate
The opposite of successful is unsuccessful
The opposite of wanted is unwanted
The opposite of restricted is unrestricted
The opposite of acceptable is
2024-07-25 19:10:39 root INFO     [order_1_approx] starting weight calculation for The opposite of wanted is unwanted
The opposite of changed is unchanged
The opposite of acceptable is unacceptable
The opposite of fortunate is unfortunate
The opposite of successful is unsuccessful
The opposite of pleasant is unpleasant
The opposite of authorized is unauthorized
The opposite of restricted is
2024-07-25 19:10:39 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 19:13:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2109, -1.0625,  0.5820,  ..., -0.5625,  1.3984,  1.6719],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([12.6250,  2.8125, -0.6367,  ..., -2.0000,  3.2188,  6.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0215,  0.0017,  0.0126,  ...,  0.0057, -0.0020, -0.0022],
        [ 0.0055,  0.0134, -0.0164,  ..., -0.0008,  0.0020,  0.0102],
        [-0.0015,  0.0014,  0.1182,  ...,  0.0041,  0.0067,  0.0002],
        ...,
        [ 0.0042, -0.0044, -0.0084,  ...,  0.0198, -0.0005, -0.0013],
        [ 0.0013,  0.0013,  0.0096,  ..., -0.0033,  0.0255,  0.0015],
        [ 0.0024, -0.0007, -0.0178,  ..., -0.0135, -0.0070,  0.0435]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[11.5156,  4.6172, -0.4595,  ..., -3.9961,  4.2461,  5.1172]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 19:13:26 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of wanted is unwanted
The opposite of changed is unchanged
The opposite of acceptable is unacceptable
The opposite of fortunate is unfortunate
The opposite of successful is unsuccessful
The opposite of pleasant is unpleasant
The opposite of authorized is unauthorized
The opposite of restricted is
2024-07-25 19:13:26 root INFO     [order_1_approx] starting weight calculation for The opposite of authorized is unauthorized
The opposite of changed is unchanged
The opposite of fortunate is unfortunate
The opposite of acceptable is unacceptable
The opposite of successful is unsuccessful
The opposite of restricted is unrestricted
The opposite of pleasant is unpleasant
The opposite of wanted is
2024-07-25 19:13:27 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 19:16:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 3.1250, -1.4609, -1.5469,  ..., -0.1289,  0.7891,  1.4922],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-2.1562,  1.4844,  7.4375,  ..., -3.6875, -1.3750, -5.9688],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 4.1016e-02, -2.2888e-05, -1.0254e-02,  ...,  3.1128e-03,
         -4.6997e-03, -2.1362e-03],
        [-2.4414e-03,  1.7334e-02,  7.9346e-03,  ...,  2.8076e-03,
         -7.3853e-03,  2.3651e-03],
        [ 2.9144e-03, -1.2085e-02,  1.2158e-01,  ..., -5.3711e-03,
          8.7891e-03, -1.0071e-03],
        ...,
        [ 1.6632e-03,  2.2430e-03, -7.6294e-03,  ...,  2.5024e-02,
          1.5564e-03, -1.2589e-03],
        [ 5.6152e-03,  6.9275e-03, -4.5166e-03,  ..., -5.0354e-04,
          5.1758e-02, -2.6093e-03],
        [-1.0986e-03, -1.8005e-03, -3.3691e-02,  ..., -1.0803e-02,
         -4.8828e-04,  4.7119e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3750,  1.2539,  9.8281,  ..., -3.1836, -1.9590, -4.1562]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 19:16:14 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of authorized is unauthorized
The opposite of changed is unchanged
The opposite of fortunate is unfortunate
The opposite of acceptable is unacceptable
The opposite of successful is unsuccessful
The opposite of restricted is unrestricted
The opposite of pleasant is unpleasant
The opposite of wanted is
2024-07-25 19:16:14 root INFO     [order_1_approx] starting weight calculation for The opposite of restricted is unrestricted
The opposite of authorized is unauthorized
The opposite of wanted is unwanted
The opposite of successful is unsuccessful
The opposite of changed is unchanged
The opposite of acceptable is unacceptable
The opposite of pleasant is unpleasant
The opposite of fortunate is
2024-07-25 19:16:14 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 19:19:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-4.6875e-01, -3.6621e-03,  3.7031e+00,  ..., -1.5859e+00,
        -1.0625e+00,  2.0938e+00], device='cuda:0', dtype=torch.float16,
       grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 2.0312, -8.9375, -3.4688,  ...,  8.5625,  4.7500, -9.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0354,  0.0015,  0.0068,  ...,  0.0018, -0.0004, -0.0077],
        [ 0.0006,  0.0127, -0.0249,  ..., -0.0046,  0.0108,  0.0071],
        [ 0.0071, -0.0010,  0.1074,  ..., -0.0136,  0.0069, -0.0006],
        ...,
        [-0.0023,  0.0051,  0.0098,  ...,  0.0103, -0.0018, -0.0024],
        [ 0.0095, -0.0050, -0.0031,  ...,  0.0011,  0.0295,  0.0008],
        [ 0.0034,  0.0036, -0.0232,  ..., -0.0101, -0.0045,  0.0391]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  2.2363, -10.8281,  -3.8906,  ...,   9.8906,   1.2031,  -9.8203]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 19:19:01 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of restricted is unrestricted
The opposite of authorized is unauthorized
The opposite of wanted is unwanted
The opposite of successful is unsuccessful
The opposite of changed is unchanged
The opposite of acceptable is unacceptable
The opposite of pleasant is unpleasant
The opposite of fortunate is
2024-07-25 19:19:01 root INFO     total operator prediction time: 1355.2307164669037 seconds
2024-07-25 19:19:01 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on re+verb_reg
2024-07-25 19:19:01 root INFO     building operator re+verb_reg
2024-07-25 19:19:01 root INFO     [order_1_approx] starting weight calculation for To generate again is to regenerate
To acquire again is to reacquire
To deem again is to redeem
To commend again is to recommend
To tell again is to retell
To upload again is to reupload
To cognize again is to recognize
To occur again is to
2024-07-25 19:19:01 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 19:21:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.4375,  1.8750,  3.1250,  ...,  0.2227, -0.2061,  2.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 6.5625, -7.0625,  1.5938,  ...,  8.0625, -3.3438, -9.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0215, -0.0058, -0.0100,  ...,  0.0006, -0.0018, -0.0068],
        [ 0.0039,  0.0187,  0.0143,  ..., -0.0004,  0.0082, -0.0022],
        [ 0.0018, -0.0026,  0.1035,  ...,  0.0033,  0.0018, -0.0042],
        ...,
        [ 0.0018,  0.0053, -0.0073,  ...,  0.0103, -0.0109, -0.0025],
        [ 0.0027, -0.0025,  0.0121,  ...,  0.0074,  0.0253,  0.0025],
        [ 0.0004, -0.0018, -0.0168,  ...,  0.0015, -0.0065,  0.0410]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 8.9375, -7.2188,  0.4873,  ..., 10.4688, -1.1367, -8.0859]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 19:21:49 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To generate again is to regenerate
To acquire again is to reacquire
To deem again is to redeem
To commend again is to recommend
To tell again is to retell
To upload again is to reupload
To cognize again is to recognize
To occur again is to
2024-07-25 19:21:49 root INFO     [order_1_approx] starting weight calculation for To upload again is to reupload
To occur again is to reoccur
To acquire again is to reacquire
To deem again is to redeem
To commend again is to recommend
To cognize again is to recognize
To tell again is to retell
To generate again is to
2024-07-25 19:21:49 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 19:24:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.1875, -1.0156,  3.1250,  ..., -1.3516,  1.0156,  2.3594],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 16.6250,   5.8750,   0.0625,  ...,  -7.7500,  -6.9062, -10.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.2705e-02, -1.1215e-03, -2.0294e-03,  ...,  3.0518e-03,
         -6.6833e-03,  3.3569e-03],
        [ 2.8992e-04,  1.3733e-02, -6.1951e-03,  ..., -1.7395e-03,
         -7.0801e-03,  7.7209e-03],
        [ 3.1738e-03, -2.0981e-04,  9.3262e-02,  ...,  3.2043e-04,
         -1.0147e-03, -3.8147e-05],
        ...,
        [ 6.9275e-03, -1.4114e-04, -1.0437e-02,  ...,  1.2878e-02,
         -1.3672e-02,  2.8687e-03],
        [-1.4496e-03, -7.2479e-05,  7.6294e-05,  ...,  2.0447e-03,
          3.0029e-02,  6.4392e-03],
        [ 3.6774e-03,  1.9226e-03, -5.8594e-03,  ..., -1.5564e-03,
         -3.3379e-04,  3.6865e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[18.3594,  6.1992, -1.1738,  ..., -6.7344, -7.6367, -9.4922]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 19:24:39 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To upload again is to reupload
To occur again is to reoccur
To acquire again is to reacquire
To deem again is to redeem
To commend again is to recommend
To cognize again is to recognize
To tell again is to retell
To generate again is to
2024-07-25 19:24:39 root INFO     [order_1_approx] starting weight calculation for To occur again is to reoccur
To upload again is to reupload
To cognize again is to recognize
To acquire again is to reacquire
To tell again is to retell
To generate again is to regenerate
To commend again is to recommend
To deem again is to
2024-07-25 19:24:40 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 19:27:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4062,  1.6328,  2.0781,  ..., -1.3438,  0.9336,  2.5312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 14.9375,  -5.5625,   1.4531,  ..., -10.8750,  -4.5312,   4.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0308,  0.0024,  0.0079,  ..., -0.0031, -0.0018, -0.0118],
        [ 0.0043,  0.0172, -0.0006,  ...,  0.0049,  0.0029,  0.0066],
        [ 0.0072, -0.0073,  0.0894,  ..., -0.0029,  0.0026, -0.0100],
        ...,
        [ 0.0008, -0.0010, -0.0200,  ...,  0.0190, -0.0154,  0.0049],
        [ 0.0033,  0.0002, -0.0008,  ..., -0.0026,  0.0388,  0.0029],
        [ 0.0023,  0.0014,  0.0034,  ...,  0.0020,  0.0020,  0.0479]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[13.9219, -6.3281,  0.9082,  ..., -9.2969, -6.3906,  2.9316]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 19:27:27 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To occur again is to reoccur
To upload again is to reupload
To cognize again is to recognize
To acquire again is to reacquire
To tell again is to retell
To generate again is to regenerate
To commend again is to recommend
To deem again is to
2024-07-25 19:27:27 root INFO     [order_1_approx] starting weight calculation for To acquire again is to reacquire
To tell again is to retell
To upload again is to reupload
To cognize again is to recognize
To occur again is to reoccur
To deem again is to redeem
To generate again is to regenerate
To commend again is to
2024-07-25 19:27:27 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 19:30:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1719,  2.6250,  3.5312,  ..., -0.9688, -1.8203,  3.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([14.8125, -1.4062, -2.3438,  ..., -6.3750, -8.1250,  3.9531],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0217, -0.0042,  0.0030,  ..., -0.0007,  0.0090, -0.0063],
        [-0.0002,  0.0186,  0.0048,  ...,  0.0032,  0.0056,  0.0078],
        [ 0.0024, -0.0002,  0.0830,  ...,  0.0005, -0.0013, -0.0032],
        ...,
        [ 0.0014, -0.0052, -0.0171,  ...,  0.0234, -0.0182,  0.0098],
        [ 0.0037,  0.0029, -0.0006,  ...,  0.0063,  0.0405,  0.0023],
        [-0.0031, -0.0035, -0.0002,  ..., -0.0030,  0.0005,  0.0471]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 16.7344,  -0.0195,  -3.8203,  ...,  -4.1406, -10.7031,   5.5391]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 19:30:16 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To acquire again is to reacquire
To tell again is to retell
To upload again is to reupload
To cognize again is to recognize
To occur again is to reoccur
To deem again is to redeem
To generate again is to regenerate
To commend again is to
2024-07-25 19:30:17 root INFO     [order_1_approx] starting weight calculation for To deem again is to redeem
To acquire again is to reacquire
To generate again is to regenerate
To tell again is to retell
To commend again is to recommend
To occur again is to reoccur
To upload again is to reupload
To cognize again is to
2024-07-25 19:30:17 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 19:33:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0000,  4.0938,  3.5312,  ..., -0.2852, -1.8906,  3.5938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 14.1875,  -7.7188,   0.2500,  ...,  19.1250,  -1.1719, -16.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0256, -0.0026,  0.0208,  ..., -0.0038,  0.0034, -0.0089],
        [ 0.0048,  0.0087,  0.0058,  ...,  0.0006,  0.0068,  0.0015],
        [-0.0012, -0.0046,  0.0986,  ...,  0.0055, -0.0037, -0.0063],
        ...,
        [-0.0016, -0.0074, -0.0016,  ...,  0.0114, -0.0003, -0.0017],
        [-0.0018, -0.0018,  0.0032,  ..., -0.0033,  0.0332,  0.0095],
        [-0.0003, -0.0017,  0.0049,  ..., -0.0021, -0.0071,  0.0339]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 13.3672,  -6.9766,   0.7422,  ...,  18.4375,  -1.9092, -17.6094]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 19:33:06 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To deem again is to redeem
To acquire again is to reacquire
To generate again is to regenerate
To tell again is to retell
To commend again is to recommend
To occur again is to reoccur
To upload again is to reupload
To cognize again is to
2024-07-25 19:33:07 root INFO     [order_1_approx] starting weight calculation for To occur again is to reoccur
To generate again is to regenerate
To acquire again is to reacquire
To commend again is to recommend
To deem again is to redeem
To cognize again is to recognize
To upload again is to reupload
To tell again is to
2024-07-25 19:33:07 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 19:35:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.5469,  3.4375,  2.9844,  ..., -1.5781, -0.3164, -0.2812],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([15.8125,  1.1875,  7.0000,  ..., -3.8438,  1.7031, -2.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0188, -0.0087, -0.0048,  ..., -0.0051,  0.0048, -0.0034],
        [ 0.0026,  0.0023, -0.0079,  ..., -0.0048,  0.0003,  0.0038],
        [ 0.0023, -0.0078,  0.0952,  ...,  0.0065,  0.0041, -0.0022],
        ...,
        [-0.0037, -0.0003, -0.0059,  ...,  0.0164, -0.0023, -0.0012],
        [ 0.0025,  0.0030,  0.0013,  ...,  0.0051,  0.0256,  0.0054],
        [ 0.0058,  0.0057,  0.0003,  ..., -0.0013, -0.0116,  0.0280]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[14.7891,  0.5537,  5.7383,  ..., -4.1250,  1.1592, -1.2461]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 19:35:56 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To occur again is to reoccur
To generate again is to regenerate
To acquire again is to reacquire
To commend again is to recommend
To deem again is to redeem
To cognize again is to recognize
To upload again is to reupload
To tell again is to
2024-07-25 19:35:57 root INFO     [order_1_approx] starting weight calculation for To tell again is to retell
To acquire again is to reacquire
To cognize again is to recognize
To generate again is to regenerate
To commend again is to recommend
To occur again is to reoccur
To deem again is to redeem
To upload again is to
2024-07-25 19:35:57 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 19:38:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4941, -1.3047,  1.4531,  ..., -0.8945, -1.1875,  2.9688],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([13.9375,  0.1484, 11.9375,  ..., -3.1094, -0.5586, -6.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0281,  0.0014, -0.0034,  ...,  0.0065, -0.0082,  0.0005],
        [ 0.0029,  0.0183,  0.0004,  ..., -0.0007,  0.0004,  0.0063],
        [-0.0049, -0.0020,  0.0752,  ...,  0.0010, -0.0011, -0.0052],
        ...,
        [ 0.0042, -0.0046, -0.0043,  ...,  0.0071, -0.0039,  0.0055],
        [ 0.0012,  0.0046,  0.0027,  ..., -0.0011,  0.0410, -0.0031],
        [ 0.0005,  0.0048, -0.0187,  ..., -0.0054, -0.0038,  0.0291]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[14.3281,  0.1481,  9.0859,  ..., -5.8008, -1.9648, -5.1602]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 19:38:46 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To tell again is to retell
To acquire again is to reacquire
To cognize again is to recognize
To generate again is to regenerate
To commend again is to recommend
To occur again is to reoccur
To deem again is to redeem
To upload again is to
2024-07-25 19:38:46 root INFO     [order_1_approx] starting weight calculation for To occur again is to reoccur
To generate again is to regenerate
To upload again is to reupload
To tell again is to retell
To deem again is to redeem
To cognize again is to recognize
To commend again is to recommend
To acquire again is to
2024-07-25 19:38:46 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 19:41:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-3.7188, -0.5586,  7.5938,  ..., -0.9023,  0.5586,  2.9219],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 9.8750,  2.5312, -3.7188,  ..., -3.6094, -2.6406, -7.1562],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0161, -0.0032,  0.0063,  ..., -0.0061,  0.0123, -0.0034],
        [ 0.0022,  0.0181, -0.0094,  ..., -0.0026,  0.0019,  0.0073],
        [-0.0035,  0.0016,  0.0698,  ...,  0.0042, -0.0053, -0.0018],
        ...,
        [-0.0014,  0.0035, -0.0210,  ...,  0.0111, -0.0079, -0.0042],
        [ 0.0016, -0.0012,  0.0074,  ...,  0.0047,  0.0305,  0.0095],
        [ 0.0060,  0.0058, -0.0020,  ..., -0.0013,  0.0021,  0.0244]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[10.5625,  0.4766, -6.6016,  ..., -4.0391, -2.2285, -5.8320]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 19:41:36 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To occur again is to reoccur
To generate again is to regenerate
To upload again is to reupload
To tell again is to retell
To deem again is to redeem
To cognize again is to recognize
To commend again is to recommend
To acquire again is to
2024-07-25 19:41:36 root INFO     total operator prediction time: 1354.8890500068665 seconds
2024-07-25 19:41:36 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on adj+ness_reg
2024-07-25 19:41:36 root INFO     building operator adj+ness_reg
2024-07-25 19:41:36 root INFO     [order_1_approx] starting weight calculation for The state of being hot is hotness
The state of being strange is strangeness
The state of being devoted is devotedness
The state of being odd is oddness
The state of being sad is sadness
The state of being related is relatedness
The state of being mad is madness
The state of being random is
2024-07-25 19:41:36 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 19:44:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3438,  1.0625, -0.1973,  ...,  1.9688,  0.4785,  3.9688],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -9.3750,  21.2500, -10.5625,  ..., -12.5625,  -7.5000,  -8.4375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0311, -0.0013, -0.0071,  ...,  0.0056,  0.0002,  0.0010],
        [ 0.0024,  0.0166,  0.0085,  ..., -0.0067,  0.0114,  0.0045],
        [ 0.0024,  0.0024,  0.1050,  ..., -0.0007, -0.0075,  0.0044],
        ...,
        [-0.0007, -0.0013, -0.0146,  ...,  0.0260, -0.0081, -0.0095],
        [ 0.0045,  0.0031,  0.0022,  ..., -0.0016,  0.0361, -0.0022],
        [-0.0021,  0.0017,  0.0003,  ..., -0.0047, -0.0027,  0.0378]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -9.6172,  19.9375, -10.1953,  ..., -14.2031,  -6.5195,  -7.6484]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 19:44:23 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The state of being hot is hotness
The state of being strange is strangeness
The state of being devoted is devotedness
The state of being odd is oddness
The state of being sad is sadness
The state of being related is relatedness
The state of being mad is madness
The state of being random is
2024-07-25 19:44:24 root INFO     [order_1_approx] starting weight calculation for The state of being hot is hotness
The state of being sad is sadness
The state of being odd is oddness
The state of being strange is strangeness
The state of being random is randomness
The state of being related is relatedness
The state of being mad is madness
The state of being devoted is
2024-07-25 19:44:24 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 19:47:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.2188, -1.2266, -0.1797,  ..., -2.0156,  0.3477,  2.2969],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -4.6562,  15.8750,  -3.7344,  ..., -14.6250,   4.6562,   1.0781],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0334, -0.0038,  0.0019,  ...,  0.0048,  0.0031, -0.0071],
        [ 0.0042,  0.0189,  0.0100,  ..., -0.0096,  0.0090,  0.0056],
        [ 0.0041,  0.0049,  0.1270,  ..., -0.0067, -0.0118,  0.0002],
        ...,
        [-0.0004,  0.0070, -0.0170,  ...,  0.0437, -0.0108, -0.0022],
        [ 0.0057,  0.0012,  0.0107,  ..., -0.0049,  0.0442,  0.0065],
        [-0.0014,  0.0063, -0.0005,  ..., -0.0008,  0.0103,  0.0515]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -6.3398,  15.5234,  -3.0703,  ..., -14.8438,   6.0859,   0.3794]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 19:47:12 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The state of being hot is hotness
The state of being sad is sadness
The state of being odd is oddness
The state of being strange is strangeness
The state of being random is randomness
The state of being related is relatedness
The state of being mad is madness
The state of being devoted is
2024-07-25 19:47:12 root INFO     [order_1_approx] starting weight calculation for The state of being strange is strangeness
The state of being sad is sadness
The state of being mad is madness
The state of being odd is oddness
The state of being random is randomness
The state of being devoted is devotedness
The state of being related is relatedness
The state of being hot is
2024-07-25 19:47:12 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 19:50:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3672,  1.6562,  0.3770,  ..., -0.7930, -1.1875, -0.0742],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -6.9062,  25.8750,  12.3750,  ...,   0.7500,  -6.3438, -11.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0234, -0.0018, -0.0049,  ...,  0.0050,  0.0013,  0.0004],
        [ 0.0024,  0.0166,  0.0012,  ..., -0.0012,  0.0060,  0.0081],
        [ 0.0076,  0.0002,  0.0747,  ..., -0.0027, -0.0060, -0.0030],
        ...,
        [ 0.0043,  0.0020, -0.0081,  ...,  0.0214, -0.0005, -0.0079],
        [ 0.0030, -0.0005,  0.0039,  ..., -0.0051,  0.0272, -0.0020],
        [-0.0008,  0.0066,  0.0009,  ..., -0.0035,  0.0035,  0.0295]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.8828, 25.2031, 11.4062,  ...,  0.6187, -6.5938, -8.6875]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 19:50:00 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The state of being strange is strangeness
The state of being sad is sadness
The state of being mad is madness
The state of being odd is oddness
The state of being random is randomness
The state of being devoted is devotedness
The state of being related is relatedness
The state of being hot is
2024-07-25 19:50:01 root INFO     [order_1_approx] starting weight calculation for The state of being odd is oddness
The state of being devoted is devotedness
The state of being random is randomness
The state of being strange is strangeness
The state of being sad is sadness
The state of being related is relatedness
The state of being hot is hotness
The state of being mad is
2024-07-25 19:50:01 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 19:52:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4219,  1.0000, -2.9844,  ..., -1.1406,  0.8555,  0.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-17.2500,  20.8750, -11.6250,  ...,  -9.2500,  -6.1250,  -4.3438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0327,  0.0028, -0.0095,  ...,  0.0086,  0.0090,  0.0014],
        [ 0.0018,  0.0197,  0.0069,  ..., -0.0043, -0.0076,  0.0113],
        [ 0.0037,  0.0016,  0.1094,  ...,  0.0007, -0.0033,  0.0028],
        ...,
        [ 0.0021,  0.0026, -0.0088,  ...,  0.0201, -0.0002, -0.0132],
        [ 0.0035,  0.0012, -0.0081,  ...,  0.0020,  0.0420,  0.0040],
        [ 0.0016, -0.0002, -0.0146,  ..., -0.0006, -0.0009,  0.0469]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-17.2188,  19.2344, -11.2031,  ...,  -8.4531,  -5.3281,  -2.4258]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 19:52:50 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The state of being odd is oddness
The state of being devoted is devotedness
The state of being random is randomness
The state of being strange is strangeness
The state of being sad is sadness
The state of being related is relatedness
The state of being hot is hotness
The state of being mad is
2024-07-25 19:52:50 root INFO     [order_1_approx] starting weight calculation for The state of being devoted is devotedness
The state of being mad is madness
The state of being related is relatedness
The state of being random is randomness
The state of being hot is hotness
The state of being sad is sadness
The state of being odd is oddness
The state of being strange is
2024-07-25 19:52:50 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 19:55:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4141, -1.3281, -1.3594,  ..., -2.2031,  2.3438,  0.7070],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  5.3125,  18.0000,  -4.2812,  ..., -21.5000,  -8.4375,  -2.6562],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0266,  0.0062, -0.0057,  ...,  0.0069,  0.0045, -0.0049],
        [ 0.0049,  0.0183,  0.0088,  ..., -0.0068,  0.0084,  0.0031],
        [ 0.0006,  0.0022,  0.1182,  ..., -0.0040, -0.0070, -0.0009],
        ...,
        [-0.0037, -0.0011, -0.0143,  ...,  0.0198,  0.0008, -0.0082],
        [ 0.0016,  0.0015, -0.0107,  ..., -0.0053,  0.0366,  0.0025],
        [-0.0064,  0.0058, -0.0027,  ..., -0.0055, -0.0038,  0.0410]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  5.1328,  16.5312,  -3.0176,  ..., -25.0781,  -8.3828,   0.7598]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 19:55:39 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The state of being devoted is devotedness
The state of being mad is madness
The state of being related is relatedness
The state of being random is randomness
The state of being hot is hotness
The state of being sad is sadness
The state of being odd is oddness
The state of being strange is
2024-07-25 19:55:39 root INFO     [order_1_approx] starting weight calculation for The state of being hot is hotness
The state of being devoted is devotedness
The state of being random is randomness
The state of being mad is madness
The state of being related is relatedness
The state of being strange is strangeness
The state of being odd is oddness
The state of being sad is
2024-07-25 19:55:39 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 19:58:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.6250,  2.0938,  4.1875,  ..., -0.8828, -0.0391, -0.5117],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-0.1562, 10.3750, -5.9375,  ..., -6.3750, -9.8750, -9.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.9775e-02,  6.3324e-04, -5.7983e-03,  ...,  2.5482e-03,
          4.0283e-03,  2.2583e-03],
        [-2.9602e-03,  1.5381e-02,  6.1951e-03,  ..., -3.7231e-03,
          1.8387e-03,  7.9346e-03],
        [ 1.0529e-03,  3.0670e-03,  6.2256e-02,  ...,  1.2665e-03,
         -2.8534e-03, -5.3406e-04],
        ...,
        [ 5.3711e-03, -7.6294e-05, -7.3853e-03,  ...,  1.7456e-02,
          1.2589e-03, -9.5825e-03],
        [ 2.0752e-03,  4.5013e-04, -7.3242e-03,  ..., -2.4414e-04,
          2.4902e-02,  2.9907e-03],
        [ 8.3923e-05,  7.8125e-03, -7.5989e-03,  ...,  8.0109e-05,
         -1.6708e-03,  2.5757e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6782,  7.6562, -5.1289,  ..., -5.5742, -8.9219, -5.3711]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 19:58:28 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The state of being hot is hotness
The state of being devoted is devotedness
The state of being random is randomness
The state of being mad is madness
The state of being related is relatedness
The state of being strange is strangeness
The state of being odd is oddness
The state of being sad is
2024-07-25 19:58:28 root INFO     [order_1_approx] starting weight calculation for The state of being hot is hotness
The state of being devoted is devotedness
The state of being strange is strangeness
The state of being mad is madness
The state of being related is relatedness
The state of being random is randomness
The state of being sad is sadness
The state of being odd is
2024-07-25 19:58:28 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 20:01:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0820, -1.0547,  0.8555,  ..., -1.4922,  1.0938, -0.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 2.0469, 14.8750, -5.0000,  ..., -6.7188, -6.8125, -0.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0258, -0.0010, -0.0101,  ...,  0.0049,  0.0081, -0.0031],
        [-0.0012,  0.0109,  0.0033,  ..., -0.0055,  0.0009,  0.0076],
        [-0.0004,  0.0018,  0.0977,  ..., -0.0006, -0.0047,  0.0031],
        ...,
        [ 0.0007, -0.0007, -0.0078,  ...,  0.0183,  0.0020, -0.0038],
        [-0.0044,  0.0066, -0.0052,  ...,  0.0010,  0.0317,  0.0034],
        [ 0.0023,  0.0031, -0.0040,  ..., -0.0008, -0.0030,  0.0356]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2266, 13.6406, -5.4609,  ..., -7.1836, -5.9961,  1.7500]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 20:01:17 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The state of being hot is hotness
The state of being devoted is devotedness
The state of being strange is strangeness
The state of being mad is madness
The state of being related is relatedness
The state of being random is randomness
The state of being sad is sadness
The state of being odd is
2024-07-25 20:01:18 root INFO     [order_1_approx] starting weight calculation for The state of being random is randomness
The state of being mad is madness
The state of being hot is hotness
The state of being strange is strangeness
The state of being odd is oddness
The state of being sad is sadness
The state of being devoted is devotedness
The state of being related is
2024-07-25 20:01:18 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 20:04:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.2031, -1.4922, -5.1250,  ..., -0.5781,  0.6562,  1.0859],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -9.5000,  18.3750, -17.5000,  ..., -19.3750,   4.6562,   0.1094],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0330, -0.0025, -0.0059,  ...,  0.0035,  0.0057, -0.0090],
        [ 0.0069,  0.0217,  0.0059,  ..., -0.0033,  0.0083,  0.0021],
        [ 0.0032,  0.0008,  0.1035,  ...,  0.0033, -0.0049,  0.0004],
        ...,
        [ 0.0055, -0.0016, -0.0145,  ...,  0.0342, -0.0077, -0.0034],
        [ 0.0073,  0.0004,  0.0013,  ..., -0.0031,  0.0349,  0.0064],
        [ 0.0045,  0.0048,  0.0042,  ..., -0.0044,  0.0008,  0.0486]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -9.2109,  17.7812, -16.0312,  ..., -19.4844,   4.4492,   1.8867]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 20:04:07 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The state of being random is randomness
The state of being mad is madness
The state of being hot is hotness
The state of being strange is strangeness
The state of being odd is oddness
The state of being sad is sadness
The state of being devoted is devotedness
The state of being related is
2024-07-25 20:04:07 root INFO     total operator prediction time: 1351.056359052658 seconds
2024-07-25 20:04:07 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on noun+less_reg
2024-07-25 20:04:07 root INFO     building operator noun+less_reg
2024-07-25 20:04:07 root INFO     [order_1_approx] starting weight calculation for Something without penny is penniless
Something without luck is luckless
Something without sleeve is sleeveless
Something without spine is spineless
Something without heart is heartless
Something without gender is genderless
Something without breath is breathless
Something without soul is
2024-07-25 20:04:07 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 20:06:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.8359, -1.9297, -3.2656,  ...,  0.8047,  0.6914,  1.3984],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 8.1250,  7.6875,  5.4062,  ..., -6.8438, -3.5000, -7.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0287, -0.0043,  0.0095,  ...,  0.0059, -0.0013, -0.0034],
        [-0.0024,  0.0228, -0.0006,  ...,  0.0017,  0.0048,  0.0125],
        [-0.0016,  0.0006,  0.0898,  ...,  0.0031,  0.0003, -0.0008],
        ...,
        [ 0.0008,  0.0002, -0.0003,  ...,  0.0212,  0.0013, -0.0062],
        [ 0.0047,  0.0037, -0.0132,  ...,  0.0003,  0.0405, -0.0004],
        [-0.0005,  0.0007, -0.0124,  ..., -0.0008,  0.0016,  0.0393]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 9.6328,  8.7500,  7.0000,  ..., -6.5938, -2.8262, -5.4844]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 20:06:55 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Something without penny is penniless
Something without luck is luckless
Something without sleeve is sleeveless
Something without spine is spineless
Something without heart is heartless
Something without gender is genderless
Something without breath is breathless
Something without soul is
2024-07-25 20:06:56 root INFO     [order_1_approx] starting weight calculation for Something without spine is spineless
Something without gender is genderless
Something without breath is breathless
Something without heart is heartless
Something without luck is luckless
Something without penny is penniless
Something without soul is soulless
Something without sleeve is
2024-07-25 20:06:56 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 20:09:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0703, -1.1562, -2.2500,  ...,  0.4219, -1.1172,  0.7070],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 7.0938,  1.5625,  5.7500,  ..., -6.2812,  1.7656,  5.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.1982e-02, -1.1597e-03,  9.1553e-03,  ...,  1.0742e-02,
         -3.2654e-03, -1.1841e-02],
        [-6.7139e-03,  1.4771e-02,  5.2490e-03,  ..., -1.7395e-03,
          3.2196e-03,  1.6846e-02],
        [ 1.2329e-02,  1.8997e-03,  1.0742e-01,  ..., -2.5940e-03,
         -6.6223e-03, -6.1646e-03],
        ...,
        [ 4.6539e-04, -1.9302e-03,  6.9580e-03,  ...,  2.9541e-02,
         -4.2114e-03,  1.2207e-04],
        [-1.4496e-04, -5.3406e-05, -2.0752e-02,  ..., -1.1215e-03,
          4.3701e-02,  7.6904e-03],
        [-5.6152e-03,  9.3384e-03, -3.7231e-03,  ...,  1.0986e-03,
          5.9128e-04,  3.8574e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.1641,  0.8804,  5.3750,  ..., -5.4609,  2.2070,  6.3086]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 20:09:43 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Something without spine is spineless
Something without gender is genderless
Something without breath is breathless
Something without heart is heartless
Something without luck is luckless
Something without penny is penniless
Something without soul is soulless
Something without sleeve is
2024-07-25 20:09:43 root INFO     [order_1_approx] starting weight calculation for Something without sleeve is sleeveless
Something without luck is luckless
Something without penny is penniless
Something without gender is genderless
Something without spine is spineless
Something without soul is soulless
Something without breath is breathless
Something without heart is
2024-07-25 20:09:43 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 20:12:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3750, -0.8906, -1.2266,  ..., -0.3340,  0.1338,  1.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  0.1777,  10.7500,   8.5000,  ...,  -9.7500, -12.3750,  -5.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0228, -0.0062,  0.0041,  ...,  0.0031,  0.0009, -0.0017],
        [-0.0013,  0.0123,  0.0000,  ..., -0.0012,  0.0128,  0.0013],
        [ 0.0056,  0.0023,  0.0635,  ..., -0.0012,  0.0055, -0.0034],
        ...,
        [ 0.0036, -0.0019, -0.0051,  ...,  0.0178, -0.0015,  0.0002],
        [ 0.0072,  0.0017, -0.0032,  ...,  0.0020,  0.0308, -0.0072],
        [ 0.0027,  0.0005, -0.0112,  ...,  0.0023, -0.0010,  0.0388]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  0.4155,  10.5234,   8.6172,  ..., -10.0234, -11.4375,  -4.3867]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 20:12:30 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Something without sleeve is sleeveless
Something without luck is luckless
Something without penny is penniless
Something without gender is genderless
Something without spine is spineless
Something without soul is soulless
Something without breath is breathless
Something without heart is
2024-07-25 20:12:31 root INFO     [order_1_approx] starting weight calculation for Something without spine is spineless
Something without luck is luckless
Something without soul is soulless
Something without sleeve is sleeveless
Something without gender is genderless
Something without penny is penniless
Something without heart is heartless
Something without breath is
2024-07-25 20:12:31 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 20:15:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0469, -1.6094,  3.0938,  ..., -0.6250,  1.0938,  3.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  7.8438,   8.0000,   2.3594,  ..., -19.0000,  -8.8750,  -3.7812],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.8687e-02, -6.8054e-03,  6.7139e-03,  ...,  7.7515e-03,
         -2.5940e-04, -3.2806e-03],
        [-2.9602e-03,  1.5503e-02, -3.3875e-03,  ...,  3.2654e-03,
          4.0436e-04,  1.1902e-02],
        [ 3.0823e-03, -2.5940e-03,  8.3008e-02,  ...,  3.2959e-03,
          3.6240e-05, -7.6904e-03],
        ...,
        [ 6.5002e-03,  3.1662e-04, -1.9531e-02,  ...,  2.3560e-02,
         -1.7853e-03,  1.9684e-03],
        [-2.5558e-04,  5.4016e-03, -1.7334e-02,  ...,  2.2278e-03,
          3.1738e-02, -3.1738e-03],
        [ 7.7515e-03,  6.0730e-03, -3.2806e-03,  ...,  2.0294e-03,
          4.5166e-03,  3.8818e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  8.5312,   7.4453,   3.0078,  ..., -19.3125,  -8.1406,  -3.6445]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 20:15:18 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Something without spine is spineless
Something without luck is luckless
Something without soul is soulless
Something without sleeve is sleeveless
Something without gender is genderless
Something without penny is penniless
Something without heart is heartless
Something without breath is
2024-07-25 20:15:18 root INFO     [order_1_approx] starting weight calculation for Something without spine is spineless
Something without sleeve is sleeveless
Something without soul is soulless
Something without breath is breathless
Something without gender is genderless
Something without heart is heartless
Something without luck is luckless
Something without penny is
2024-07-25 20:15:18 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 20:18:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2969, -0.4922,  0.8750,  ...,  1.9688,  0.7969,  3.5781],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  6.0000,  10.1250,   5.5000,  ..., -17.1250,  -1.5938,   0.5312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0309, -0.0033,  0.0143,  ...,  0.0047, -0.0039, -0.0012],
        [-0.0049,  0.0142, -0.0040,  ...,  0.0008, -0.0002,  0.0085],
        [ 0.0009, -0.0038,  0.1064,  ..., -0.0046,  0.0038, -0.0072],
        ...,
        [ 0.0022, -0.0063, -0.0107,  ...,  0.0312,  0.0035, -0.0041],
        [ 0.0030,  0.0021, -0.0244,  ...,  0.0012,  0.0452,  0.0078],
        [-0.0042, -0.0003,  0.0124,  ..., -0.0002,  0.0058,  0.0320]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  5.2500,   9.4453,   4.1484,  ..., -14.9219,  -1.4580,  -1.3750]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 20:18:07 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Something without spine is spineless
Something without sleeve is sleeveless
Something without soul is soulless
Something without breath is breathless
Something without gender is genderless
Something without heart is heartless
Something without luck is luckless
Something without penny is
2024-07-25 20:18:07 root INFO     [order_1_approx] starting weight calculation for Something without soul is soulless
Something without gender is genderless
Something without sleeve is sleeveless
Something without spine is spineless
Something without breath is breathless
Something without penny is penniless
Something without heart is heartless
Something without luck is
2024-07-25 20:18:07 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 20:20:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9609, -1.3594,  1.9062,  ..., -0.9297,  0.1895,  2.7812],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  1.3125,   6.0625,  -8.2500,  ...,  -5.5625,  -8.7500, -15.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0303, -0.0146, -0.0063,  ...,  0.0106,  0.0040, -0.0088],
        [-0.0045,  0.0214, -0.0160,  ...,  0.0030,  0.0109,  0.0150],
        [ 0.0074,  0.0082,  0.0996,  ...,  0.0052, -0.0023,  0.0001],
        ...,
        [-0.0020,  0.0011,  0.0007,  ...,  0.0193, -0.0042, -0.0120],
        [-0.0013,  0.0045, -0.0080,  ..., -0.0020,  0.0400, -0.0036],
        [ 0.0063,  0.0077, -0.0153,  ..., -0.0038, -0.0030,  0.0413]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  1.2178,   6.2578,  -6.4609,  ...,  -5.1992,  -8.1484, -14.0547]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 20:20:56 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Something without soul is soulless
Something without gender is genderless
Something without sleeve is sleeveless
Something without spine is spineless
Something without breath is breathless
Something without penny is penniless
Something without heart is heartless
Something without luck is
2024-07-25 20:20:56 root INFO     [order_1_approx] starting weight calculation for Something without breath is breathless
Something without spine is spineless
Something without luck is luckless
Something without sleeve is sleeveless
Something without heart is heartless
Something without penny is penniless
Something without soul is soulless
Something without gender is
2024-07-25 20:20:56 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 20:23:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3594, -2.2969, -1.4922,  ...,  0.7969, -0.7227,  3.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 7.9375,  3.3125, -5.6875,  ..., -8.2500, -4.5625, -2.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0315, -0.0037,  0.0058,  ...,  0.0039,  0.0043, -0.0079],
        [ 0.0014,  0.0125, -0.0065,  ...,  0.0026,  0.0073,  0.0068],
        [-0.0083, -0.0022,  0.0869,  ..., -0.0076, -0.0054, -0.0018],
        ...,
        [-0.0012, -0.0032, -0.0118,  ...,  0.0254, -0.0065, -0.0006],
        [ 0.0041,  0.0036,  0.0034,  ..., -0.0049,  0.0403, -0.0038],
        [-0.0024, -0.0032, -0.0036,  ..., -0.0013,  0.0072,  0.0447]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 7.7656,  3.0820, -5.8633,  ..., -7.6250, -5.5859, -3.4023]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 20:23:45 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Something without breath is breathless
Something without spine is spineless
Something without luck is luckless
Something without sleeve is sleeveless
Something without heart is heartless
Something without penny is penniless
Something without soul is soulless
Something without gender is
2024-07-25 20:23:45 root INFO     [order_1_approx] starting weight calculation for Something without soul is soulless
Something without penny is penniless
Something without breath is breathless
Something without sleeve is sleeveless
Something without heart is heartless
Something without luck is luckless
Something without gender is genderless
Something without spine is
2024-07-25 20:23:45 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 20:26:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1895, -2.0469, -2.5156,  ..., -1.4766, -1.2734,  2.5938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -6.5312,   3.4688,   7.6250,  ..., -15.9375, -11.9375,  10.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0383, -0.0010,  0.0078,  ...,  0.0081,  0.0052, -0.0038],
        [-0.0051,  0.0233,  0.0009,  ...,  0.0012,  0.0152,  0.0109],
        [ 0.0069, -0.0012,  0.1089,  ...,  0.0002, -0.0009, -0.0048],
        ...,
        [-0.0006, -0.0026,  0.0014,  ...,  0.0312, -0.0007, -0.0054],
        [ 0.0005,  0.0017, -0.0176,  ..., -0.0019,  0.0464,  0.0022],
        [ 0.0118,  0.0067,  0.0039,  ...,  0.0022, -0.0040,  0.0574]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -7.9727,   3.1035,   7.1719,  ..., -16.2188, -11.6641,  11.3047]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 20:26:34 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Something without soul is soulless
Something without penny is penniless
Something without breath is breathless
Something without sleeve is sleeveless
Something without heart is heartless
Something without luck is luckless
Something without gender is genderless
Something without spine is
2024-07-25 20:26:34 root INFO     total operator prediction time: 1346.9126102924347 seconds
2024-07-25 20:26:34 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on verb+ment_irreg
2024-07-25 20:26:34 root INFO     building operator verb+ment_irreg
2024-07-25 20:26:34 root INFO     [order_1_approx] starting weight calculation for To entertain results in a entertainment
To endorse results in a endorsement
To fulfill results in a fulfillment
To amuse results in a amusement
To reinforce results in a reinforcement
To engage results in a engagement
To displace results in a displacement
To embarrass results in a
2024-07-25 20:26:34 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 20:29:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5508,  0.8281,  0.4219,  ..., -0.7344, -1.8281,  0.6836],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -0.9609,  -1.2656, -19.7500,  ...,  -8.7500,  -9.1875,   2.3438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.8574e-02, -4.1809e-03, -1.8692e-03,  ...,  7.9727e-04,
          1.2573e-02, -2.9602e-03],
        [ 2.3651e-04,  2.2705e-02,  1.0864e-02,  ..., -7.2632e-03,
          4.3945e-03,  8.7891e-03],
        [ 1.6022e-04,  1.5259e-05,  1.2695e-01,  ..., -9.9487e-03,
          8.6670e-03, -6.3171e-03],
        ...,
        [ 6.2561e-04,  2.2430e-03, -2.1057e-03,  ...,  3.1738e-02,
         -3.3569e-03, -4.8218e-03],
        [ 5.0964e-03,  6.3477e-03, -1.1230e-02,  ...,  2.3041e-03,
          4.4922e-02,  6.1798e-04],
        [-6.2561e-03,  3.3569e-03, -1.2817e-02,  ..., -4.0283e-03,
          1.3123e-03,  5.7617e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -1.1445,  -1.3301, -17.9062,  ..., -10.3594,  -8.6797,   1.8945]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 20:29:21 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To entertain results in a entertainment
To endorse results in a endorsement
To fulfill results in a fulfillment
To amuse results in a amusement
To reinforce results in a reinforcement
To engage results in a engagement
To displace results in a displacement
To embarrass results in a
2024-07-25 20:29:22 root INFO     [order_1_approx] starting weight calculation for To embarrass results in a embarrassment
To entertain results in a entertainment
To endorse results in a endorsement
To amuse results in a amusement
To reinforce results in a reinforcement
To fulfill results in a fulfillment
To displace results in a displacement
To engage results in a
2024-07-25 20:29:22 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 20:32:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7109, -1.8672, -1.4844,  ...,  0.7031,  0.4668, -0.1982],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 10.3750,  -1.5000, -13.1250,  ...,  -0.6797, -13.0625,  -7.2188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0298, -0.0038,  0.0033,  ...,  0.0009,  0.0079,  0.0046],
        [ 0.0060,  0.0203,  0.0101,  ..., -0.0093,  0.0002,  0.0071],
        [-0.0004,  0.0037,  0.1167,  ..., -0.0074, -0.0002, -0.0036],
        ...,
        [ 0.0034,  0.0014, -0.0064,  ...,  0.0275, -0.0046, -0.0027],
        [ 0.0036,  0.0005,  0.0024,  ...,  0.0014,  0.0413, -0.0070],
        [ 0.0020,  0.0009, -0.0021,  ..., -0.0085,  0.0036,  0.0474]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 10.4453,  -0.7510, -13.6719,  ...,  -0.3171, -12.6875,  -8.2500]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 20:32:08 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To embarrass results in a embarrassment
To entertain results in a entertainment
To endorse results in a endorsement
To amuse results in a amusement
To reinforce results in a reinforcement
To fulfill results in a fulfillment
To displace results in a displacement
To engage results in a
2024-07-25 20:32:09 root INFO     [order_1_approx] starting weight calculation for To entertain results in a entertainment
To engage results in a engagement
To reinforce results in a reinforcement
To amuse results in a amusement
To embarrass results in a embarrassment
To endorse results in a endorsement
To displace results in a displacement
To fulfill results in a
2024-07-25 20:32:09 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 20:34:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0156, -0.5039,  0.3008,  ...,  0.7344,  0.9961,  1.7266],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 10.2500,   5.5000,   0.4688,  ...,   3.9375, -13.2500,  10.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0310,  0.0074, -0.0079,  ...,  0.0031,  0.0027, -0.0063],
        [ 0.0068,  0.0172,  0.0047,  ..., -0.0089,  0.0068,  0.0121],
        [ 0.0056,  0.0060,  0.1406,  ..., -0.0003, -0.0034,  0.0033],
        ...,
        [ 0.0057,  0.0060, -0.0074,  ...,  0.0342,  0.0034,  0.0050],
        [ 0.0085,  0.0010, -0.0078,  ...,  0.0078,  0.0449, -0.0041],
        [-0.0034,  0.0050,  0.0022,  ..., -0.0090,  0.0034,  0.0500]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 10.2500,   5.4922,   0.1741,  ...,   4.5430, -15.1797,   8.8281]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 20:34:54 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To entertain results in a entertainment
To engage results in a engagement
To reinforce results in a reinforcement
To amuse results in a amusement
To embarrass results in a embarrassment
To endorse results in a endorsement
To displace results in a displacement
To fulfill results in a
2024-07-25 20:34:54 root INFO     [order_1_approx] starting weight calculation for To fulfill results in a fulfillment
To amuse results in a amusement
To engage results in a engagement
To embarrass results in a embarrassment
To reinforce results in a reinforcement
To endorse results in a endorsement
To entertain results in a entertainment
To displace results in a
2024-07-25 20:34:54 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 20:37:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.4219, -0.8633,  0.5625,  ..., -1.1094,  1.4766,  0.7773],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  4.2188,  -6.1250, -13.0625,  ...,  -3.5625,  -7.0000,   5.8438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0317, -0.0049, -0.0044,  ...,  0.0059,  0.0056, -0.0063],
        [ 0.0006,  0.0231,  0.0079,  ..., -0.0067,  0.0109,  0.0070],
        [-0.0016,  0.0011,  0.1279,  ..., -0.0137, -0.0059, -0.0031],
        ...,
        [ 0.0065,  0.0098, -0.0105,  ...,  0.0179,  0.0005,  0.0069],
        [ 0.0009,  0.0017,  0.0007,  ..., -0.0031,  0.0383,  0.0027],
        [ 0.0036,  0.0089, -0.0052,  ..., -0.0039,  0.0017,  0.0479]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  2.4766,  -5.2656, -11.0859,  ...,  -2.8711,  -9.0312,   8.0469]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 20:37:38 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To fulfill results in a fulfillment
To amuse results in a amusement
To engage results in a engagement
To embarrass results in a embarrassment
To reinforce results in a reinforcement
To endorse results in a endorsement
To entertain results in a entertainment
To displace results in a
2024-07-25 20:37:39 root INFO     [order_1_approx] starting weight calculation for To embarrass results in a embarrassment
To engage results in a engagement
To reinforce results in a reinforcement
To endorse results in a endorsement
To displace results in a displacement
To fulfill results in a fulfillment
To entertain results in a entertainment
To amuse results in a
2024-07-25 20:37:39 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 20:40:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6992, -2.2344,  1.5312,  ..., -0.8203,  0.6484,  0.4414],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  3.4844,  10.4375, -24.7500,  ...,  -0.3906, -16.5000,  -0.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.3560e-02, -2.5482e-03,  1.5869e-03,  ...,  3.5553e-03,
          6.7444e-03, -2.5024e-03],
        [ 6.2866e-03,  1.6724e-02,  2.3315e-02,  ..., -2.5024e-03,
          1.5106e-03,  1.1230e-02],
        [ 5.8899e-03, -5.1880e-04,  1.2109e-01,  ..., -1.0986e-02,
         -3.7689e-03, -4.8523e-03],
        ...,
        [ 1.2756e-02,  4.2725e-03, -5.4932e-03,  ...,  2.1484e-02,
          7.4768e-04, -2.0447e-03],
        [ 2.1667e-03, -3.8452e-03, -5.9814e-03,  ..., -2.4414e-03,
          4.5166e-02, -7.9346e-03],
        [ 3.1281e-04,  8.2016e-05, -3.7537e-03,  ..., -7.1411e-03,
          3.5553e-03,  4.4189e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  2.8711,  11.1953, -21.0781,  ...,  -0.5410, -17.3438,   0.4248]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 20:40:25 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To embarrass results in a embarrassment
To engage results in a engagement
To reinforce results in a reinforcement
To endorse results in a endorsement
To displace results in a displacement
To fulfill results in a fulfillment
To entertain results in a entertainment
To amuse results in a
2024-07-25 20:40:25 root INFO     [order_1_approx] starting weight calculation for To amuse results in a amusement
To endorse results in a endorsement
To displace results in a displacement
To engage results in a engagement
To reinforce results in a reinforcement
To fulfill results in a fulfillment
To embarrass results in a embarrassment
To entertain results in a
2024-07-25 20:40:25 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 20:43:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3516, -4.2188,  2.7031,  ...,  0.1602,  2.5156, -0.1357],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  8.4375,   7.4375, -12.2500,  ...,   4.1875,  -8.8750,  -1.0312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0273, -0.0013,  0.0074,  ..., -0.0003,  0.0118,  0.0036],
        [ 0.0115,  0.0172,  0.0197,  ..., -0.0125,  0.0013,  0.0124],
        [ 0.0015,  0.0057,  0.1069,  ..., -0.0097, -0.0014, -0.0062],
        ...,
        [ 0.0048,  0.0086,  0.0013,  ...,  0.0082, -0.0052, -0.0005],
        [-0.0008, -0.0024, -0.0043,  ...,  0.0010,  0.0325, -0.0039],
        [-0.0055,  0.0038,  0.0116,  ..., -0.0070, -0.0011,  0.0471]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  9.1016,   8.3281, -12.1797,  ...,   3.5293,  -8.3672,  -1.8867]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 20:43:11 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To amuse results in a amusement
To endorse results in a endorsement
To displace results in a displacement
To engage results in a engagement
To reinforce results in a reinforcement
To fulfill results in a fulfillment
To embarrass results in a embarrassment
To entertain results in a
2024-07-25 20:43:11 root INFO     [order_1_approx] starting weight calculation for To embarrass results in a embarrassment
To endorse results in a endorsement
To engage results in a engagement
To amuse results in a amusement
To entertain results in a entertainment
To fulfill results in a fulfillment
To displace results in a displacement
To reinforce results in a
2024-07-25 20:43:12 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 20:45:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2109, -0.9297,  1.9062,  ...,  1.3906,  1.0000,  2.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  4.1875,   6.7500,  -2.8125,  ...,   3.3281, -11.0625,   2.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0332, -0.0018, -0.0109,  ...,  0.0062,  0.0193,  0.0082],
        [ 0.0079,  0.0227,  0.0128,  ..., -0.0120, -0.0006,  0.0075],
        [ 0.0076,  0.0098,  0.1426,  ..., -0.0087, -0.0084, -0.0036],
        ...,
        [-0.0037,  0.0101, -0.0129,  ...,  0.0254, -0.0036,  0.0027],
        [ 0.0104,  0.0044,  0.0047,  ...,  0.0040,  0.0447, -0.0005],
        [ 0.0028,  0.0067,  0.0067,  ..., -0.0084,  0.0056,  0.0493]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  2.9785,   9.4609,  -2.6523,  ...,   2.7324, -12.2500,   2.1836]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 20:45:58 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To embarrass results in a embarrassment
To endorse results in a endorsement
To engage results in a engagement
To amuse results in a amusement
To entertain results in a entertainment
To fulfill results in a fulfillment
To displace results in a displacement
To reinforce results in a
2024-07-25 20:45:58 root INFO     [order_1_approx] starting weight calculation for To reinforce results in a reinforcement
To entertain results in a entertainment
To displace results in a displacement
To fulfill results in a fulfillment
To embarrass results in a embarrassment
To amuse results in a amusement
To engage results in a engagement
To endorse results in a
2024-07-25 20:45:59 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 20:48:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3594, -0.9531,  2.2188,  ...,  1.9062,  0.2871,  2.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 4.1250,  5.8438, -1.9844,  ...,  6.1250, -0.2188,  1.4375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0378,  0.0045, -0.0057,  ...,  0.0016,  0.0146,  0.0019],
        [ 0.0067,  0.0248,  0.0146,  ..., -0.0026,  0.0039,  0.0099],
        [ 0.0035, -0.0014,  0.1309,  ..., -0.0024,  0.0073, -0.0024],
        ...,
        [ 0.0013,  0.0032, -0.0176,  ...,  0.0282, -0.0098, -0.0052],
        [ 0.0025,  0.0060, -0.0064,  ..., -0.0025,  0.0505, -0.0025],
        [-0.0024,  0.0098,  0.0028,  ..., -0.0064,  0.0016,  0.0588]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.1055,  6.8047, -2.8965,  ...,  5.4531, -2.3281, -0.6367]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 20:48:41 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To reinforce results in a reinforcement
To entertain results in a entertainment
To displace results in a displacement
To fulfill results in a fulfillment
To embarrass results in a embarrassment
To amuse results in a amusement
To engage results in a engagement
To endorse results in a
2024-07-25 20:48:41 root INFO     total operator prediction time: 1326.8264737129211 seconds
2024-07-25 20:48:41 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on UK_city - county
2024-07-25 20:48:41 root INFO     building operator UK_city - county
2024-07-25 20:48:41 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of cambridge is in the county of cambridgeshire
In the United Kingdom, the city of ely is in the county of cambridgeshire
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of stirling is in the county of stirlingshire
In the United Kingdom, the city of coventry is in the county of midlands
In the United Kingdom, the city of glasgow is in the county of lowlands
In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of winchester is in the county of
2024-07-25 20:48:41 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 20:51:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4258,  1.4844, 10.8125,  ..., -0.4609, -1.0781, -0.8594],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-6.8438, -1.1172, 16.2500,  ..., -3.3438,  1.6016, 14.3125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0126,  0.0032,  0.0037,  ..., -0.0034, -0.0089,  0.0070],
        [-0.0082,  0.0049, -0.0175,  ...,  0.0037,  0.0046,  0.0009],
        [ 0.0035, -0.0043,  0.0732,  ..., -0.0009, -0.0081,  0.0024],
        ...,
        [ 0.0093, -0.0005, -0.0145,  ...,  0.0171,  0.0021,  0.0028],
        [-0.0122, -0.0027, -0.0093,  ...,  0.0021,  0.0237,  0.0004],
        [ 0.0092,  0.0040, -0.0040,  ..., -0.0042, -0.0070,  0.0378]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-6.1914, -1.4990, 16.6406,  ..., -3.8320,  2.0273, 14.3359]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 20:51:27 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of cambridge is in the county of cambridgeshire
In the United Kingdom, the city of ely is in the county of cambridgeshire
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of stirling is in the county of stirlingshire
In the United Kingdom, the city of coventry is in the county of midlands
In the United Kingdom, the city of glasgow is in the county of lowlands
In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of winchester is in the county of
2024-07-25 20:51:27 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of ely is in the county of cambridgeshire
In the United Kingdom, the city of cambridge is in the county of cambridgeshire
In the United Kingdom, the city of winchester is in the county of hampshire
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of glasgow is in the county of lowlands
In the United Kingdom, the city of coventry is in the county of midlands
In the United Kingdom, the city of stirling is in the county of
2024-07-25 20:51:27 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 20:54:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3457, -0.6250,  2.7188,  ..., -0.1094, -1.3359, -0.8672],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-18.0000, -18.1250,  12.5000,  ...,  14.0000,  -7.3750,   6.7188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.0264e-02,  4.5776e-05,  5.3711e-03,  ..., -4.5013e-04,
         -5.1880e-04,  1.1475e-02],
        [ 2.0142e-03,  1.5503e-02, -1.2329e-02,  ..., -2.0905e-03,
          5.2795e-03, -5.4932e-03],
        [ 3.7842e-03, -5.5847e-03,  9.1309e-02,  ..., -6.6528e-03,
         -5.3406e-04, -1.2756e-02],
        ...,
        [ 5.9891e-04,  8.1787e-03, -1.5869e-02,  ...,  1.3062e-02,
         -6.4697e-03,  4.6692e-03],
        [-4.5776e-04,  4.0894e-03,  1.0376e-02,  ...,  1.0605e-03,
          2.3315e-02, -1.0071e-03],
        [-4.1962e-04, -3.7842e-03, -2.1057e-03,  ...,  3.8452e-03,
         -3.0518e-03,  2.3682e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-18.1562, -19.0938,  12.2578,  ...,  14.2500,  -7.2500,   5.5312]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 20:54:13 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of ely is in the county of cambridgeshire
In the United Kingdom, the city of cambridge is in the county of cambridgeshire
In the United Kingdom, the city of winchester is in the county of hampshire
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of glasgow is in the county of lowlands
In the United Kingdom, the city of coventry is in the county of midlands
In the United Kingdom, the city of stirling is in the county of
2024-07-25 20:54:14 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of ely is in the county of cambridgeshire
In the United Kingdom, the city of stirling is in the county of stirlingshire
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of winchester is in the county of hampshire
In the United Kingdom, the city of coventry is in the county of midlands
In the United Kingdom, the city of cambridge is in the county of cambridgeshire
In the United Kingdom, the city of glasgow is in the county of lowlands
In the United Kingdom, the city of salford is in the county of
2024-07-25 20:54:14 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 20:56:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.6875, -1.4922,  1.5078,  ..., -0.4727,  0.5781, -5.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 1.3594,  7.5000,  0.2578,  ...,  0.4375, -4.4062,  4.3125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.3062e-02, -9.2316e-04,  1.4343e-02,  ..., -2.0294e-03,
          2.3346e-03,  1.1597e-03],
        [ 5.4932e-03,  9.1553e-03, -1.2512e-02,  ..., -4.8828e-04,
          1.4191e-03, -2.7618e-03],
        [-4.5776e-03, -5.4016e-03,  5.7129e-02,  ..., -3.2806e-04,
          7.5531e-04,  8.8501e-04],
        ...,
        [ 6.2561e-03,  2.8229e-03,  4.3945e-03,  ...,  1.1169e-02,
          6.8054e-03,  9.2773e-03],
        [-1.4801e-03, -6.5613e-04, -4.4861e-03,  ...,  4.9438e-03,
          1.6235e-02,  4.4556e-03],
        [-6.1035e-05, -3.6011e-03, -1.4282e-02,  ...,  1.3885e-03,
          9.4604e-04,  2.1851e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0527,  7.8359,  0.6045,  ...,  0.9468, -3.2695,  2.0742]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 20:56:59 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of ely is in the county of cambridgeshire
In the United Kingdom, the city of stirling is in the county of stirlingshire
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of winchester is in the county of hampshire
In the United Kingdom, the city of coventry is in the county of midlands
In the United Kingdom, the city of cambridge is in the county of cambridgeshire
In the United Kingdom, the city of glasgow is in the county of lowlands
In the United Kingdom, the city of salford is in the county of
2024-07-25 20:56:59 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of cambridge is in the county of cambridgeshire
In the United Kingdom, the city of stirling is in the county of stirlingshire
In the United Kingdom, the city of winchester is in the county of hampshire
In the United Kingdom, the city of glasgow is in the county of lowlands
In the United Kingdom, the city of ely is in the county of cambridgeshire
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of coventry is in the county of
2024-07-25 20:56:59 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 20:59:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.6406, -1.3594,  3.0000,  ...,  1.5234, -3.0938, -0.9688],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-10.4375,   5.0938,   5.6875,  ...,  -0.2461,  -3.9375,   2.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0133, -0.0008,  0.0161,  ...,  0.0057, -0.0004,  0.0105],
        [ 0.0015,  0.0073, -0.0004,  ...,  0.0004,  0.0026, -0.0036],
        [-0.0016, -0.0071,  0.0488,  ..., -0.0020, -0.0030,  0.0038],
        ...,
        [ 0.0057,  0.0008,  0.0040,  ...,  0.0139, -0.0062,  0.0072],
        [ 0.0002,  0.0076, -0.0068,  ..., -0.0042,  0.0103, -0.0003],
        [ 0.0002, -0.0018, -0.0034,  ...,  0.0030,  0.0081,  0.0161]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-9.9531e+00,  4.4023e+00,  6.6484e+00,  ..., -3.1738e-03,
         -3.2422e+00,  2.6094e+00]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SubBackward0>) 

                    
2024-07-25 20:59:45 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of cambridge is in the county of cambridgeshire
In the United Kingdom, the city of stirling is in the county of stirlingshire
In the United Kingdom, the city of winchester is in the county of hampshire
In the United Kingdom, the city of glasgow is in the county of lowlands
In the United Kingdom, the city of ely is in the county of cambridgeshire
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of coventry is in the county of
2024-07-25 20:59:45 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of stirling is in the county of stirlingshire
In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of coventry is in the county of midlands
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of winchester is in the county of hampshire
In the United Kingdom, the city of glasgow is in the county of lowlands
In the United Kingdom, the city of cambridge is in the county of cambridgeshire
In the United Kingdom, the city of ely is in the county of
2024-07-25 20:59:46 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 21:02:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1641, -0.8828, -1.4844,  ..., -0.2266,  1.1250, -1.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-8.3750,  1.4062,  0.7812,  ...,  6.8125, -4.8750, 10.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.5146e-02, -1.9073e-03, -6.1035e-03,  ..., -1.0254e-02,
          1.0254e-02,  1.8555e-02],
        [ 4.7913e-03,  1.4832e-02,  6.1035e-05,  ..., -3.7384e-03,
         -2.0752e-03,  5.9814e-03],
        [-4.2419e-03, -7.0801e-03,  1.2988e-01,  ..., -5.3101e-03,
          8.3008e-03, -1.2512e-02],
        ...,
        [ 1.9226e-03,  1.3733e-03,  4.1504e-03,  ...,  1.3794e-02,
         -9.7656e-03,  1.1292e-02],
        [ 1.2085e-02,  3.7079e-03, -1.3184e-02,  ...,  7.5378e-03,
          2.1851e-02, -1.3123e-03],
        [-2.7618e-03, -7.8125e-03, -2.1606e-02,  ...,  1.2512e-03,
         -1.2207e-02,  3.8330e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-7.4766, -0.0166, -1.4102,  ...,  7.6172, -3.5352,  9.6875]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 21:02:32 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of stirling is in the county of stirlingshire
In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of coventry is in the county of midlands
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of winchester is in the county of hampshire
In the United Kingdom, the city of glasgow is in the county of lowlands
In the United Kingdom, the city of cambridge is in the county of cambridgeshire
In the United Kingdom, the city of ely is in the county of
2024-07-25 21:02:33 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of ely is in the county of cambridgeshire
In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of glasgow is in the county of lowlands
In the United Kingdom, the city of winchester is in the county of hampshire
In the United Kingdom, the city of coventry is in the county of midlands
In the United Kingdom, the city of stirling is in the county of stirlingshire
In the United Kingdom, the city of cambridge is in the county of cambridgeshire
In the United Kingdom, the city of sheffield is in the county of
2024-07-25 21:02:33 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 21:05:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2188, -1.6953,  2.1094,  ..., -1.6406, -0.7930,  0.4141],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-3.7500,  2.0312,  3.4375,  ..., -6.6875,  2.0469,  6.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.7456e-02, -4.0283e-03,  1.4160e-02,  ..., -2.0142e-03,
          7.2632e-03, -3.8147e-05],
        [ 5.7068e-03,  5.6152e-03,  5.9204e-03,  ..., -1.0071e-03,
          9.8877e-03, -6.8970e-03],
        [ 5.2185e-03, -7.0801e-03,  6.3965e-02,  ..., -1.1780e-02,
          2.6093e-03, -1.1902e-02],
        ...,
        [ 2.6245e-03, -3.5400e-03, -1.1475e-02,  ...,  1.3306e-02,
          2.2888e-04,  5.8289e-03],
        [ 1.1139e-03,  3.0518e-03, -2.1851e-02,  ...,  8.3008e-03,
          1.7822e-02,  1.0010e-02],
        [-4.9133e-03, -5.8594e-03, -1.6846e-02,  ...,  3.9673e-03,
         -3.9673e-03,  2.0020e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.2852,  1.5420,  3.7773,  ..., -6.5586,  2.4082,  5.1758]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 21:05:19 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of ely is in the county of cambridgeshire
In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of glasgow is in the county of lowlands
In the United Kingdom, the city of winchester is in the county of hampshire
In the United Kingdom, the city of coventry is in the county of midlands
In the United Kingdom, the city of stirling is in the county of stirlingshire
In the United Kingdom, the city of cambridge is in the county of cambridgeshire
In the United Kingdom, the city of sheffield is in the county of
2024-07-25 21:05:19 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of coventry is in the county of midlands
In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of ely is in the county of cambridgeshire
In the United Kingdom, the city of stirling is in the county of stirlingshire
In the United Kingdom, the city of glasgow is in the county of lowlands
In the United Kingdom, the city of winchester is in the county of hampshire
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of cambridge is in the county of
2024-07-25 21:05:20 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 21:08:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2109, -1.4688,  0.4023,  ..., -0.2656,  0.5742, -3.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-10.6875,  -0.3633,   5.9062,  ...,   6.9062,  -1.2812,   3.9531],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0197,  0.0074,  0.0029,  ..., -0.0047,  0.0027,  0.0019],
        [-0.0035,  0.0071, -0.0024,  ...,  0.0024, -0.0061, -0.0028],
        [ 0.0022, -0.0036,  0.0664,  ..., -0.0031,  0.0003, -0.0068],
        ...,
        [ 0.0079,  0.0002, -0.0052,  ...,  0.0112,  0.0040,  0.0059],
        [ 0.0043,  0.0095, -0.0027,  ..., -0.0021,  0.0114,  0.0004],
        [-0.0002, -0.0011, -0.0203,  ...,  0.0038, -0.0031,  0.0249]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-10.7812,  -0.1409,   4.3984,  ...,   7.5156,  -2.2656,   3.8027]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 21:08:06 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of coventry is in the county of midlands
In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of ely is in the county of cambridgeshire
In the United Kingdom, the city of stirling is in the county of stirlingshire
In the United Kingdom, the city of glasgow is in the county of lowlands
In the United Kingdom, the city of winchester is in the county of hampshire
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of cambridge is in the county of
2024-07-25 21:08:07 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of cambridge is in the county of cambridgeshire
In the United Kingdom, the city of coventry is in the county of midlands
In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of stirling is in the county of stirlingshire
In the United Kingdom, the city of ely is in the county of cambridgeshire
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of winchester is in the county of hampshire
In the United Kingdom, the city of glasgow is in the county of
2024-07-25 21:08:07 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 21:10:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.6562, -2.2031,  0.9062,  ..., -0.1016, -1.2266, -0.7891],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-13.9375, -16.5000,   8.2500,  ...,  -0.7656,   8.1875,  21.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0197, -0.0012,  0.0072,  ...,  0.0018, -0.0018,  0.0022],
        [ 0.0038,  0.0043, -0.0013,  ..., -0.0004, -0.0014, -0.0094],
        [ 0.0125, -0.0029,  0.0479,  ..., -0.0054,  0.0068,  0.0007],
        ...,
        [ 0.0019, -0.0017,  0.0000,  ...,  0.0100, -0.0065,  0.0002],
        [-0.0018,  0.0047,  0.0069,  ...,  0.0034,  0.0107,  0.0092],
        [ 0.0068, -0.0027, -0.0020,  ..., -0.0004,  0.0098,  0.0101]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-12.7109, -15.3359,   8.6797,  ...,  -0.7002,   7.6406,  21.0625]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 21:10:53 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of cambridge is in the county of cambridgeshire
In the United Kingdom, the city of coventry is in the county of midlands
In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of stirling is in the county of stirlingshire
In the United Kingdom, the city of ely is in the county of cambridgeshire
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of winchester is in the county of hampshire
In the United Kingdom, the city of glasgow is in the county of
2024-07-25 21:10:53 root INFO     total operator prediction time: 1332.1380620002747 seconds
2024-07-25 21:10:53 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on name - nationality
2024-07-25 21:10:53 root INFO     building operator name - nationality
2024-07-25 21:10:53 root INFO     [order_1_approx] starting weight calculation for tolstoi was russian
hawking was english
raphael was italian
pascal was french
michelangelo was italian
dostoyevsky was russian
depp was american
descartes was
2024-07-25 21:10:53 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 21:13:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-3.2656, -0.9219,  0.1133,  ..., -1.8906, -5.5938, -4.7188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-5.9062, -6.0000,  5.1250,  ..., -2.4375,  2.2500,  4.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.2227e-02,  4.0283e-03, -1.8066e-02,  ..., -2.5635e-03,
          2.9297e-03, -9.4604e-03],
        [-3.7689e-03,  2.2461e-02,  2.5940e-03,  ..., -1.4191e-03,
          1.1292e-03,  4.8218e-03],
        [-7.7209e-03, -1.1169e-02,  1.3086e-01,  ..., -7.3242e-03,
          1.0254e-02,  1.2817e-02],
        ...,
        [-4.1962e-05, -7.2479e-04, -1.6113e-02,  ...,  1.9165e-02,
         -8.5449e-03,  3.5858e-03],
        [ 7.2479e-04, -5.2185e-03,  2.2461e-02,  ...,  1.8311e-04,
          3.5156e-02, -2.7466e-03],
        [-6.5308e-03, -5.2795e-03, -4.4556e-03,  ...,  3.8452e-03,
          1.1673e-03,  3.2471e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.1719, -7.4688,  4.8633,  ..., -2.2656,  2.4648,  3.8926]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 21:13:40 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for tolstoi was russian
hawking was english
raphael was italian
pascal was french
michelangelo was italian
dostoyevsky was russian
depp was american
descartes was
2024-07-25 21:13:40 root INFO     [order_1_approx] starting weight calculation for depp was american
tolstoi was russian
raphael was italian
pascal was french
dostoyevsky was russian
michelangelo was italian
descartes was french
hawking was
2024-07-25 21:13:40 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 21:16:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-4.4062, -0.9219, -1.2109,  ..., -2.0625, -5.1562, -1.5156],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -5.0938, -15.7500,  13.9375,  ...,   7.9375,   2.2344,   6.2188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.3447e-02,  2.1057e-03, -1.2207e-02,  ..., -3.0212e-03,
          1.2817e-03,  4.9744e-03],
        [ 7.1716e-04,  2.2339e-02,  1.0803e-02,  ...,  1.9226e-03,
          6.9580e-03, -6.0425e-03],
        [ 1.3657e-03, -2.4261e-03,  1.1523e-01,  ...,  8.3923e-05,
          4.5013e-04,  4.6387e-03],
        ...,
        [-1.0071e-03,  1.8845e-03,  3.5400e-03,  ...,  2.2827e-02,
         -4.5776e-03,  2.9907e-03],
        [ 1.2283e-03, -2.3804e-03,  9.0332e-03,  ...,  1.4496e-03,
          3.8818e-02, -6.5613e-04],
        [-9.8419e-04, -1.2970e-03,  9.1553e-04,  ...,  6.2256e-03,
          4.1199e-03,  2.9785e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -4.3789, -14.8438,  15.3516,  ...,   7.7031,   1.9736,   6.3594]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 21:16:27 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for depp was american
tolstoi was russian
raphael was italian
pascal was french
dostoyevsky was russian
michelangelo was italian
descartes was french
hawking was
2024-07-25 21:16:27 root INFO     [order_1_approx] starting weight calculation for depp was american
descartes was french
pascal was french
hawking was english
michelangelo was italian
raphael was italian
tolstoi was russian
dostoyevsky was
2024-07-25 21:16:27 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 21:19:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-3.0938, -4.3750,  2.1094,  ..., -2.4062, -2.3125, -1.1953],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 9.3750, -7.3750, -5.4062,  ..., -4.4375,  6.0312,  0.4375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0206,  0.0014, -0.0069,  ..., -0.0010,  0.0018,  0.0029],
        [-0.0056,  0.0140, -0.0058,  ..., -0.0026,  0.0035,  0.0045],
        [-0.0055,  0.0010,  0.0693,  ..., -0.0022,  0.0031,  0.0040],
        ...,
        [ 0.0034, -0.0014, -0.0045,  ...,  0.0143, -0.0058, -0.0001],
        [ 0.0014,  0.0026,  0.0164,  ...,  0.0007,  0.0312, -0.0067],
        [-0.0058, -0.0049,  0.0060,  ...,  0.0038, -0.0005,  0.0250]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[10.0391, -8.2031, -5.0352,  ..., -4.1758,  5.9336,  0.7031]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 21:19:14 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for depp was american
descartes was french
pascal was french
hawking was english
michelangelo was italian
raphael was italian
tolstoi was russian
dostoyevsky was
2024-07-25 21:19:14 root INFO     [order_1_approx] starting weight calculation for dostoyevsky was russian
hawking was english
depp was american
descartes was french
raphael was italian
pascal was french
michelangelo was italian
tolstoi was
2024-07-25 21:19:14 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 21:22:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5703, -5.5625,  3.0938,  ..., -1.7656, -2.0625, -1.4531],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 6.6562, -7.5625,  0.5781,  ..., -5.7188,  7.9688,  0.2812],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0254, -0.0003, -0.0176,  ...,  0.0020,  0.0009,  0.0032],
        [-0.0041,  0.0190,  0.0009,  ..., -0.0054,  0.0057,  0.0024],
        [-0.0045, -0.0056,  0.0957,  ..., -0.0121,  0.0122,  0.0106],
        ...,
        [ 0.0023, -0.0034, -0.0065,  ...,  0.0188, -0.0078,  0.0015],
        [-0.0010,  0.0007,  0.0178,  ..., -0.0011,  0.0325, -0.0044],
        [-0.0062, -0.0059,  0.0041,  ...,  0.0047,  0.0014,  0.0281]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.7227, -7.6211,  1.8379,  ..., -5.0938,  7.5859,  1.1299]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 21:22:00 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for dostoyevsky was russian
hawking was english
depp was american
descartes was french
raphael was italian
pascal was french
michelangelo was italian
tolstoi was
2024-07-25 21:22:01 root INFO     [order_1_approx] starting weight calculation for raphael was italian
pascal was french
descartes was french
michelangelo was italian
dostoyevsky was russian
tolstoi was russian
hawking was english
depp was
2024-07-25 21:22:01 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 21:24:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.4688,  0.6562,  0.4531,  ..., -0.5078, -5.9062,  1.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -4.1250, -13.3125,  14.3125,  ...,  10.6250,  -3.3281,  12.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0286, -0.0029, -0.0117,  ..., -0.0082,  0.0041,  0.0037],
        [ 0.0058,  0.0294,  0.0073,  ..., -0.0056,  0.0087,  0.0217],
        [-0.0039, -0.0052,  0.1357,  ..., -0.0046,  0.0079,  0.0065],
        ...,
        [ 0.0003,  0.0030, -0.0029,  ...,  0.0269, -0.0074, -0.0009],
        [ 0.0061, -0.0016,  0.0298,  ..., -0.0043,  0.0596,  0.0023],
        [ 0.0070, -0.0023,  0.0038,  ...,  0.0035,  0.0087,  0.0540]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -4.0039, -13.9844,  14.7109,  ...,   9.6562,  -2.9316,  11.1719]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 21:24:47 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for raphael was italian
pascal was french
descartes was french
michelangelo was italian
dostoyevsky was russian
tolstoi was russian
hawking was english
depp was
2024-07-25 21:24:47 root INFO     [order_1_approx] starting weight calculation for descartes was french
pascal was french
depp was american
tolstoi was russian
hawking was english
dostoyevsky was russian
raphael was italian
michelangelo was
2024-07-25 21:24:47 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 21:27:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-3.2500,  0.0908, -1.7344,  ..., -2.3906, -5.5938, -1.9531],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-9.0625, -9.6875,  4.1250,  ..., -4.6250,  5.0000,  8.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0264, -0.0045, -0.0139,  ..., -0.0029,  0.0007,  0.0020],
        [-0.0021,  0.0219, -0.0055,  ...,  0.0023,  0.0099,  0.0020],
        [-0.0073, -0.0005,  0.1069,  ..., -0.0044,  0.0079,  0.0085],
        ...,
        [-0.0003,  0.0012,  0.0050,  ...,  0.0231, -0.0139, -0.0026],
        [ 0.0031,  0.0009,  0.0104,  ..., -0.0015,  0.0415, -0.0044],
        [-0.0055, -0.0027,  0.0062,  ...,  0.0009,  0.0025,  0.0317]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -8.9766, -10.3984,   4.8555,  ...,  -4.3125,   4.9883,   7.2344]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 21:27:33 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for descartes was french
pascal was french
depp was american
tolstoi was russian
hawking was english
dostoyevsky was russian
raphael was italian
michelangelo was
2024-07-25 21:27:34 root INFO     [order_1_approx] starting weight calculation for hawking was english
pascal was french
descartes was french
dostoyevsky was russian
michelangelo was italian
depp was american
tolstoi was russian
raphael was
2024-07-25 21:27:34 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 21:30:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-3.2500, -0.7891, -2.7500,  ..., -1.6719, -3.5156, -3.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-9.8750, -7.3750,  9.6250,  ..., -1.5000,  2.0156,  6.4688],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.6367e-02,  8.3923e-05, -2.0264e-02,  ..., -8.1635e-04,
          3.4485e-03,  1.7090e-03],
        [-2.5330e-03,  1.6113e-02,  3.5706e-03,  ..., -3.0670e-03,
          7.0496e-03,  7.4463e-03],
        [-3.6316e-03, -1.0864e-02,  9.5215e-02,  ..., -1.1475e-02,
          5.8594e-03,  1.7944e-02],
        ...,
        [-2.2507e-04, -3.7842e-03, -4.1199e-04,  ...,  1.5625e-02,
         -5.6763e-03,  2.1210e-03],
        [ 2.1057e-03,  3.8452e-03,  6.5918e-03,  ...,  2.5024e-03,
          3.0396e-02, -7.1411e-03],
        [-2.2125e-03, -3.7231e-03,  9.1553e-03,  ..., -2.0294e-03,
          2.3041e-03,  3.5156e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-9.1250, -7.4844, 11.1172,  ..., -1.3281,  2.0801,  6.4336]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 21:30:20 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for hawking was english
pascal was french
descartes was french
dostoyevsky was russian
michelangelo was italian
depp was american
tolstoi was russian
raphael was
2024-07-25 21:30:21 root INFO     [order_1_approx] starting weight calculation for descartes was french
michelangelo was italian
dostoyevsky was russian
depp was american
hawking was english
tolstoi was russian
raphael was italian
pascal was
2024-07-25 21:30:21 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 21:33:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-3.0625, -3.6094,  5.6875,  ..., -0.0234, -6.6562, -6.4062],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-6.6250, -7.2188, 10.5000,  ..., -0.5625,  7.3750,  6.4375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0220,  0.0014, -0.0242,  ..., -0.0018,  0.0038, -0.0061],
        [-0.0032,  0.0162,  0.0047,  ..., -0.0070,  0.0054,  0.0002],
        [-0.0099, -0.0059,  0.1221,  ..., -0.0108,  0.0075,  0.0145],
        ...,
        [ 0.0027, -0.0010, -0.0156,  ...,  0.0125, -0.0095,  0.0018],
        [ 0.0038, -0.0006,  0.0171,  ..., -0.0039,  0.0320, -0.0023],
        [-0.0046, -0.0037, -0.0001,  ..., -0.0011, -0.0025,  0.0226]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-6.6328, -8.1875,  9.8828,  ..., -1.5820,  7.5039,  6.6250]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 21:33:07 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for descartes was french
michelangelo was italian
dostoyevsky was russian
depp was american
hawking was english
tolstoi was russian
raphael was italian
pascal was
2024-07-25 21:33:07 root INFO     total operator prediction time: 1334.0061922073364 seconds
2024-07-25 21:33:07 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on country - language
2024-07-25 21:33:07 root INFO     building operator country - language
2024-07-25 21:33:07 root INFO     [order_1_approx] starting weight calculation for The country of switzerland primarily speaks the language of german
The country of netherlands primarily speaks the language of dutch
The country of ecuador primarily speaks the language of spanish
The country of canada primarily speaks the language of english
The country of bangladesh primarily speaks the language of bengali
The country of austria primarily speaks the language of german
The country of colombia primarily speaks the language of spanish
The country of jamaica primarily speaks the language of
2024-07-25 21:33:07 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 21:35:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7812, -1.4297,  5.0000,  ..., -1.0703, -1.7031, -2.0469],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 6.5625, -2.2500, 24.0000,  ...,  9.6250,  7.7188, 17.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0089,  0.0030, -0.0064,  ...,  0.0023, -0.0044,  0.0045],
        [ 0.0038,  0.0029, -0.0036,  ..., -0.0035,  0.0098,  0.0014],
        [ 0.0021,  0.0005,  0.0586,  ..., -0.0034,  0.0028, -0.0048],
        ...,
        [ 0.0007,  0.0028,  0.0016,  ...,  0.0107, -0.0103,  0.0042],
        [ 0.0013,  0.0004, -0.0011,  ..., -0.0010,  0.0143,  0.0023],
        [ 0.0019,  0.0009,  0.0104,  ..., -0.0007,  0.0032,  0.0181]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 7.0273, -1.9424, 24.3750,  ..., 10.0703,  8.0234, 17.2344]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 21:35:54 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The country of switzerland primarily speaks the language of german
The country of netherlands primarily speaks the language of dutch
The country of ecuador primarily speaks the language of spanish
The country of canada primarily speaks the language of english
The country of bangladesh primarily speaks the language of bengali
The country of austria primarily speaks the language of german
The country of colombia primarily speaks the language of spanish
The country of jamaica primarily speaks the language of
2024-07-25 21:35:54 root INFO     [order_1_approx] starting weight calculation for The country of bangladesh primarily speaks the language of bengali
The country of colombia primarily speaks the language of spanish
The country of switzerland primarily speaks the language of german
The country of jamaica primarily speaks the language of english
The country of canada primarily speaks the language of english
The country of austria primarily speaks the language of german
The country of netherlands primarily speaks the language of dutch
The country of ecuador primarily speaks the language of
2024-07-25 21:35:55 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 21:38:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.7188,  2.1562,  3.5625,  ..., -1.7031, -0.0283,  0.3359],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-4.2500, -1.7969,  9.3750,  ..., -1.5234,  3.4375, 14.3125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.9653e-02, -3.8757e-03,  3.6621e-04,  ...,  2.6245e-03,
         -3.1738e-03,  1.1719e-02],
        [-8.6670e-03,  4.9438e-03, -1.5442e-02,  ..., -4.5776e-05,
          1.2817e-03, -7.0190e-04],
        [-8.3618e-03,  3.2425e-04,  5.5176e-02,  ..., -6.5918e-03,
         -3.7994e-03,  1.5259e-04],
        ...,
        [-4.5776e-03,  1.9531e-03, -2.6093e-03,  ...,  9.6436e-03,
         -9.2773e-03,  2.7771e-03],
        [-8.9264e-04, -1.8234e-03,  1.3184e-02,  ..., -3.5400e-03,
          2.1240e-02, -7.9346e-03],
        [-1.1719e-02, -5.9814e-03,  2.6855e-03,  ...,  2.9755e-03,
         -8.2397e-04,  1.3855e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.7969, -2.1680,  9.1328,  ..., -2.5547,  3.4102, 13.4062]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 21:38:42 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The country of bangladesh primarily speaks the language of bengali
The country of colombia primarily speaks the language of spanish
The country of switzerland primarily speaks the language of german
The country of jamaica primarily speaks the language of english
The country of canada primarily speaks the language of english
The country of austria primarily speaks the language of german
The country of netherlands primarily speaks the language of dutch
The country of ecuador primarily speaks the language of
2024-07-25 21:38:42 root INFO     [order_1_approx] starting weight calculation for The country of canada primarily speaks the language of english
The country of jamaica primarily speaks the language of english
The country of austria primarily speaks the language of german
The country of bangladesh primarily speaks the language of bengali
The country of ecuador primarily speaks the language of spanish
The country of colombia primarily speaks the language of spanish
The country of netherlands primarily speaks the language of dutch
The country of switzerland primarily speaks the language of
2024-07-25 21:38:42 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 21:41:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0469,  0.7930,  1.7969,  ..., -1.2969, -2.8594, -2.5312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-9.0625, -3.5469, 19.3750,  ..., -4.5312, -0.1348, 27.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0124,  0.0001, -0.0075,  ...,  0.0016, -0.0023,  0.0023],
        [ 0.0002,  0.0067,  0.0035,  ..., -0.0027,  0.0069, -0.0021],
        [ 0.0006, -0.0013,  0.0527,  ..., -0.0034,  0.0005, -0.0027],
        ...,
        [-0.0001, -0.0002, -0.0045,  ...,  0.0102, -0.0089,  0.0048],
        [-0.0052, -0.0033,  0.0073,  ...,  0.0009,  0.0093,  0.0006],
        [-0.0014, -0.0022,  0.0028,  ...,  0.0050, -0.0074,  0.0145]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-8.6719, -3.7949, 18.8594,  ..., -3.7969, -0.6011, 27.3438]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 21:41:31 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The country of canada primarily speaks the language of english
The country of jamaica primarily speaks the language of english
The country of austria primarily speaks the language of german
The country of bangladesh primarily speaks the language of bengali
The country of ecuador primarily speaks the language of spanish
The country of colombia primarily speaks the language of spanish
The country of netherlands primarily speaks the language of dutch
The country of switzerland primarily speaks the language of
2024-07-25 21:41:31 root INFO     [order_1_approx] starting weight calculation for The country of colombia primarily speaks the language of spanish
The country of austria primarily speaks the language of german
The country of netherlands primarily speaks the language of dutch
The country of switzerland primarily speaks the language of german
The country of ecuador primarily speaks the language of spanish
The country of jamaica primarily speaks the language of english
The country of canada primarily speaks the language of english
The country of bangladesh primarily speaks the language of
2024-07-25 21:41:31 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 21:44:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5000,  2.0000,  2.5312,  ...,  0.8711, -1.9062, -2.5312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -1.4922,   4.3750,  23.8750,  ..., -11.4375,  -0.6016,  16.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0083, -0.0032, -0.0083,  ...,  0.0027, -0.0020,  0.0009],
        [-0.0027,  0.0085, -0.0038,  ..., -0.0038,  0.0081,  0.0045],
        [-0.0082, -0.0021,  0.0698,  ..., -0.0028, -0.0011,  0.0013],
        ...,
        [ 0.0068, -0.0015, -0.0033,  ...,  0.0122, -0.0039,  0.0009],
        [-0.0020, -0.0002,  0.0134,  ..., -0.0012,  0.0085,  0.0018],
        [-0.0058, -0.0031,  0.0082,  ...,  0.0047, -0.0033,  0.0149]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -1.2295,   3.6797,  23.7656,  ..., -10.9375,  -0.7388,  16.2188]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 21:44:18 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The country of colombia primarily speaks the language of spanish
The country of austria primarily speaks the language of german
The country of netherlands primarily speaks the language of dutch
The country of switzerland primarily speaks the language of german
The country of ecuador primarily speaks the language of spanish
The country of jamaica primarily speaks the language of english
The country of canada primarily speaks the language of english
The country of bangladesh primarily speaks the language of
2024-07-25 21:44:18 root INFO     [order_1_approx] starting weight calculation for The country of switzerland primarily speaks the language of german
The country of bangladesh primarily speaks the language of bengali
The country of ecuador primarily speaks the language of spanish
The country of jamaica primarily speaks the language of english
The country of canada primarily speaks the language of english
The country of netherlands primarily speaks the language of dutch
The country of colombia primarily speaks the language of spanish
The country of austria primarily speaks the language of
2024-07-25 21:44:19 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 21:47:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6172,  0.1328,  5.9375,  ..., -0.5938,  0.4492, -4.5938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-0.1406, -5.9375,  2.0312,  ..., -1.3438,  0.7031, 20.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0183, -0.0029, -0.0146,  ...,  0.0019, -0.0014,  0.0035],
        [-0.0038,  0.0060, -0.0031,  ..., -0.0022,  0.0086,  0.0022],
        [-0.0050,  0.0022,  0.0625,  ..., -0.0106,  0.0073,  0.0005],
        ...,
        [ 0.0028, -0.0006, -0.0062,  ...,  0.0129, -0.0058,  0.0018],
        [-0.0001, -0.0032,  0.0216,  ..., -0.0025,  0.0188,  0.0055],
        [-0.0034, -0.0096,  0.0063,  ...,  0.0040, -0.0036,  0.0149]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8218, -6.0469,  1.8857,  ..., -1.2266,  0.6499, 19.5000]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 21:47:06 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The country of switzerland primarily speaks the language of german
The country of bangladesh primarily speaks the language of bengali
The country of ecuador primarily speaks the language of spanish
The country of jamaica primarily speaks the language of english
The country of canada primarily speaks the language of english
The country of netherlands primarily speaks the language of dutch
The country of colombia primarily speaks the language of spanish
The country of austria primarily speaks the language of
2024-07-25 21:47:06 root INFO     [order_1_approx] starting weight calculation for The country of ecuador primarily speaks the language of spanish
The country of jamaica primarily speaks the language of english
The country of bangladesh primarily speaks the language of bengali
The country of colombia primarily speaks the language of spanish
The country of netherlands primarily speaks the language of dutch
The country of switzerland primarily speaks the language of german
The country of austria primarily speaks the language of german
The country of canada primarily speaks the language of
2024-07-25 21:47:06 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 21:49:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1406, -0.6172,  2.9062,  ..., -1.8750, -2.3125, -3.0156],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-7.9375, -0.9414, 19.0000,  ..., -4.3125,  2.8750, 12.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0139, -0.0045, -0.0028,  ...,  0.0024, -0.0054,  0.0018],
        [-0.0084,  0.0082,  0.0010,  ..., -0.0035,  0.0029,  0.0093],
        [-0.0122,  0.0014,  0.0635,  ..., -0.0114, -0.0021,  0.0081],
        ...,
        [-0.0068,  0.0042, -0.0060,  ...,  0.0026, -0.0046,  0.0077],
        [ 0.0031, -0.0028,  0.0010,  ...,  0.0024,  0.0178, -0.0053],
        [-0.0059, -0.0040,  0.0044,  ..., -0.0019, -0.0001,  0.0188]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-7.3750, -1.9541, 19.5000,  ..., -3.5586,  2.8418, 12.7188]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 21:49:54 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The country of ecuador primarily speaks the language of spanish
The country of jamaica primarily speaks the language of english
The country of bangladesh primarily speaks the language of bengali
The country of colombia primarily speaks the language of spanish
The country of netherlands primarily speaks the language of dutch
The country of switzerland primarily speaks the language of german
The country of austria primarily speaks the language of german
The country of canada primarily speaks the language of
2024-07-25 21:49:54 root INFO     [order_1_approx] starting weight calculation for The country of ecuador primarily speaks the language of spanish
The country of canada primarily speaks the language of english
The country of austria primarily speaks the language of german
The country of jamaica primarily speaks the language of english
The country of bangladesh primarily speaks the language of bengali
The country of switzerland primarily speaks the language of german
The country of netherlands primarily speaks the language of dutch
The country of colombia primarily speaks the language of
2024-07-25 21:49:54 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 21:52:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.2344,  2.2500,  4.5000,  ..., -0.8125, -0.5195, -0.9688],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-5.1875, -1.3594,  6.1562,  ..., -1.2031,  5.4062, 13.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.4648e-02, -3.2959e-03, -6.5918e-03,  ...,  2.2278e-03,
         -3.3417e-03,  8.6670e-03],
        [-1.7014e-03,  2.9602e-03, -6.5308e-03,  ..., -1.5259e-05,
          4.2114e-03, -1.7395e-03],
        [ 2.3193e-03, -6.8665e-04,  5.2246e-02,  ..., -5.4932e-03,
         -1.3275e-03,  2.6245e-03],
        ...,
        [ 1.5488e-03, -1.3733e-04,  1.8005e-03,  ...,  1.3062e-02,
         -8.5449e-03,  4.9744e-03],
        [-1.9989e-03, -1.8692e-03,  1.1719e-02,  ..., -2.8381e-03,
          1.5137e-02, -3.3417e-03],
        [-6.2866e-03, -8.3008e-03,  4.8828e-03,  ...,  2.9144e-03,
         -1.9836e-03,  1.4526e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.9766, -1.8965,  6.1406,  ..., -1.4746,  4.9766, 12.8359]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 21:52:41 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The country of ecuador primarily speaks the language of spanish
The country of canada primarily speaks the language of english
The country of austria primarily speaks the language of german
The country of jamaica primarily speaks the language of english
The country of bangladesh primarily speaks the language of bengali
The country of switzerland primarily speaks the language of german
The country of netherlands primarily speaks the language of dutch
The country of colombia primarily speaks the language of
2024-07-25 21:52:42 root INFO     [order_1_approx] starting weight calculation for The country of canada primarily speaks the language of english
The country of switzerland primarily speaks the language of german
The country of colombia primarily speaks the language of spanish
The country of bangladesh primarily speaks the language of bengali
The country of jamaica primarily speaks the language of english
The country of austria primarily speaks the language of german
The country of ecuador primarily speaks the language of spanish
The country of netherlands primarily speaks the language of
2024-07-25 21:52:42 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 21:55:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4766, -0.1562,  1.6953,  ..., -2.0625, -2.3125, -3.8281],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 3.1719,  0.6875, 20.6250,  ...,  0.5703, -9.8125, 15.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0135, -0.0038, -0.0157,  ...,  0.0020, -0.0026,  0.0013],
        [ 0.0017,  0.0034, -0.0021,  ...,  0.0004,  0.0009, -0.0025],
        [-0.0049,  0.0003,  0.0420,  ..., -0.0035, -0.0004, -0.0017],
        ...,
        [ 0.0032, -0.0028,  0.0009,  ...,  0.0118, -0.0077,  0.0032],
        [ 0.0009, -0.0025,  0.0104,  ..., -0.0057,  0.0159,  0.0028],
        [-0.0090, -0.0017, -0.0059,  ...,  0.0003, -0.0022,  0.0099]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.0156,  0.6616, 21.1719,  ...,  0.3645, -9.2109, 15.2031]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 21:55:30 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The country of canada primarily speaks the language of english
The country of switzerland primarily speaks the language of german
The country of colombia primarily speaks the language of spanish
The country of bangladesh primarily speaks the language of bengali
The country of jamaica primarily speaks the language of english
The country of austria primarily speaks the language of german
The country of ecuador primarily speaks the language of spanish
The country of netherlands primarily speaks the language of
2024-07-25 21:55:30 root INFO     total operator prediction time: 1342.8850662708282 seconds
2024-07-25 21:55:30 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on animal - shelter
2024-07-25 21:55:30 root INFO     building operator animal - shelter
2024-07-25 21:55:30 root INFO     [order_1_approx] starting weight calculation for The place fox lives in is called den
The place bear lives in is called den
The place hornet lives in is called nest
The place mole lives in is called hole
The place woodchuck lives in is called hole
The place raven lives in is called nest
The place trout lives in is called river
The place fly lives in is called
2024-07-25 21:55:30 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 21:58:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.6797, -0.4570, -0.9102,  ...,  0.3594,  1.2344, -5.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 12.6875,   1.9219,  -7.5625,  ...,  -0.2188,   1.2344, -15.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.9531e-02, -5.1880e-03,  1.9684e-03,  ...,  1.0254e-02,
         -9.1553e-03,  1.1597e-02],
        [-1.2131e-03,  2.2949e-02, -2.5330e-03,  ...,  4.8828e-04,
          1.8005e-03,  5.6458e-03],
        [-1.1368e-03,  1.0681e-04,  1.3086e-01,  ...,  1.6098e-03,
          1.1826e-04, -2.7771e-03],
        ...,
        [ 5.9204e-03,  4.4861e-03,  1.1536e-02,  ...,  1.4709e-02,
          3.4637e-03,  7.9346e-03],
        [-6.4697e-03,  3.0899e-04, -1.4282e-02,  ..., -3.1738e-03,
          5.1758e-02,  4.9133e-03],
        [-6.5002e-03, -5.6763e-03, -4.7852e-02,  ..., -5.5237e-03,
          3.2043e-04,  4.8340e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 11.7578,   2.3789,  -7.9961,  ...,  -0.6064,   0.0928, -15.9453]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 21:58:24 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The place fox lives in is called den
The place bear lives in is called den
The place hornet lives in is called nest
The place mole lives in is called hole
The place woodchuck lives in is called hole
The place raven lives in is called nest
The place trout lives in is called river
The place fly lives in is called
2024-07-25 21:58:25 root INFO     [order_1_approx] starting weight calculation for The place fly lives in is called nest
The place bear lives in is called den
The place trout lives in is called river
The place woodchuck lives in is called hole
The place mole lives in is called hole
The place fox lives in is called den
The place raven lives in is called nest
The place hornet lives in is called
2024-07-25 21:58:25 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 22:01:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3125, -3.9531, -0.2988,  ...,  1.3594, -0.4961, -2.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  7.5938,  -3.5156,  -4.8438,  ...,  -8.3750,   3.6875, -19.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0349,  0.0033,  0.0035,  ..., -0.0028, -0.0109,  0.0035],
        [-0.0071,  0.0216, -0.0120,  ..., -0.0002,  0.0004,  0.0031],
        [ 0.0055,  0.0013,  0.1309,  ..., -0.0013,  0.0014,  0.0014],
        ...,
        [ 0.0068, -0.0002, -0.0058,  ...,  0.0289, -0.0044,  0.0209],
        [ 0.0017,  0.0045, -0.0067,  ...,  0.0030,  0.0498, -0.0026],
        [-0.0074, -0.0071, -0.0065,  ..., -0.0030, -0.0034,  0.0479]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  7.5469,  -3.6855,  -6.5195,  ...,  -9.9219,   2.3594, -22.0625]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 22:01:14 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The place fly lives in is called nest
The place bear lives in is called den
The place trout lives in is called river
The place woodchuck lives in is called hole
The place mole lives in is called hole
The place fox lives in is called den
The place raven lives in is called nest
The place hornet lives in is called
2024-07-25 22:01:14 root INFO     [order_1_approx] starting weight calculation for The place mole lives in is called hole
The place woodchuck lives in is called hole
The place hornet lives in is called nest
The place bear lives in is called den
The place trout lives in is called river
The place fly lives in is called nest
The place fox lives in is called den
The place raven lives in is called
2024-07-25 22:01:14 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 22:04:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.9219, -2.9531,  3.0156,  ...,  0.4551, -1.0000, -2.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 11.8125,   0.9141,  -0.8750,  ...,  -2.8281,  -0.5625, -17.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.6367e-02,  2.9755e-03, -7.2632e-03,  ...,  4.8218e-03,
         -3.1586e-03,  5.3101e-03],
        [-3.4180e-03,  1.5076e-02,  1.8311e-02,  ..., -7.8125e-03,
         -5.5542e-03,  3.6621e-03],
        [ 2.6093e-03,  3.5095e-04,  1.2402e-01,  ..., -2.1667e-03,
          4.4556e-03, -1.2512e-03],
        ...,
        [-1.4114e-04, -2.2278e-03,  3.4027e-03,  ...,  2.0630e-02,
         -5.1880e-03,  1.1841e-02],
        [ 3.3569e-04,  4.7302e-03,  1.4709e-02,  ..., -1.4038e-03,
          4.2969e-02, -1.8311e-03],
        [ 2.0599e-03, -6.1035e-04, -3.0518e-05,  ..., -4.8828e-03,
         -6.3477e-03,  5.3711e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 13.2891,   0.6030,  -2.0898,  ...,  -4.2852,  -2.6758, -20.0625]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 22:04:03 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The place mole lives in is called hole
The place woodchuck lives in is called hole
The place hornet lives in is called nest
The place bear lives in is called den
The place trout lives in is called river
The place fly lives in is called nest
The place fox lives in is called den
The place raven lives in is called
2024-07-25 22:04:04 root INFO     [order_1_approx] starting weight calculation for The place fly lives in is called nest
The place woodchuck lives in is called hole
The place mole lives in is called hole
The place bear lives in is called den
The place raven lives in is called nest
The place hornet lives in is called nest
The place fox lives in is called den
The place trout lives in is called
2024-07-25 22:04:04 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 22:06:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9648, -1.7812,  0.5312,  ..., -1.8438,  1.1875, -0.3477],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 15.5000,   1.0625,  -4.0938,  ..., -10.0625,   9.4375,  -5.0938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0253,  0.0013,  0.0005,  ..., -0.0052,  0.0088,  0.0012],
        [-0.0025,  0.0179, -0.0096,  ...,  0.0007, -0.0115, -0.0070],
        [ 0.0025,  0.0017,  0.0830,  ...,  0.0002, -0.0076, -0.0015],
        ...,
        [ 0.0034,  0.0020,  0.0089,  ...,  0.0152, -0.0030,  0.0167],
        [-0.0035,  0.0044, -0.0075,  ..., -0.0026,  0.0308,  0.0004],
        [-0.0048, -0.0001, -0.0138,  ..., -0.0035,  0.0059,  0.0361]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[14.6484,  1.3076, -4.4062,  ..., -9.2812,  8.6328, -6.0234]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 22:06:53 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The place fly lives in is called nest
The place woodchuck lives in is called hole
The place mole lives in is called hole
The place bear lives in is called den
The place raven lives in is called nest
The place hornet lives in is called nest
The place fox lives in is called den
The place trout lives in is called
2024-07-25 22:06:53 root INFO     [order_1_approx] starting weight calculation for The place trout lives in is called river
The place hornet lives in is called nest
The place bear lives in is called den
The place raven lives in is called nest
The place woodchuck lives in is called hole
The place fly lives in is called nest
The place fox lives in is called den
The place mole lives in is called
2024-07-25 22:06:53 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 22:09:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7500, -4.9062,  4.6562,  ..., -2.0781, -0.8047, -2.2188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 10.7500,   3.5469,   3.0312,  ..., -12.0625,   5.1250, -16.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0254, -0.0074, -0.0111,  ..., -0.0020, -0.0132,  0.0025],
        [ 0.0071,  0.0236,  0.0022,  ..., -0.0077,  0.0173, -0.0075],
        [-0.0084,  0.0005,  0.1250,  ..., -0.0018, -0.0019,  0.0023],
        ...,
        [ 0.0021,  0.0029, -0.0056,  ...,  0.0261, -0.0036,  0.0156],
        [-0.0035, -0.0047,  0.0099,  ..., -0.0016,  0.0513, -0.0060],
        [-0.0007, -0.0024, -0.0085,  ...,  0.0025,  0.0002,  0.0588]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 10.7500,   1.3145,  -0.0352,  ..., -12.3125,   5.3594, -20.1250]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 22:09:42 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The place trout lives in is called river
The place hornet lives in is called nest
The place bear lives in is called den
The place raven lives in is called nest
The place woodchuck lives in is called hole
The place fly lives in is called nest
The place fox lives in is called den
The place mole lives in is called
2024-07-25 22:09:43 root INFO     [order_1_approx] starting weight calculation for The place fly lives in is called nest
The place woodchuck lives in is called hole
The place hornet lives in is called nest
The place fox lives in is called den
The place mole lives in is called hole
The place trout lives in is called river
The place raven lives in is called nest
The place bear lives in is called
2024-07-25 22:09:43 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 22:12:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4043, -2.7812,  2.9844,  ..., -0.5469,  0.4766, -2.2812],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 10.3125,  -6.2188,   1.5078,  ...,  -4.0938,   7.5625, -17.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0208, -0.0051,  0.0194,  ..., -0.0066,  0.0053, -0.0059],
        [ 0.0019,  0.0155,  0.0039,  ..., -0.0042,  0.0042, -0.0074],
        [-0.0004,  0.0011,  0.1118,  ..., -0.0049,  0.0023,  0.0005],
        ...,
        [ 0.0025,  0.0025, -0.0222,  ...,  0.0192, -0.0031,  0.0069],
        [-0.0069,  0.0052,  0.0109,  ..., -0.0003,  0.0337, -0.0042],
        [ 0.0023, -0.0052, -0.0002,  ...,  0.0004, -0.0023,  0.0483]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 11.7734,  -6.0508,   0.4688,  ...,  -4.5938,   7.6953, -19.1250]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 22:12:31 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The place fly lives in is called nest
The place woodchuck lives in is called hole
The place hornet lives in is called nest
The place fox lives in is called den
The place mole lives in is called hole
The place trout lives in is called river
The place raven lives in is called nest
The place bear lives in is called
2024-07-25 22:12:32 root INFO     [order_1_approx] starting weight calculation for The place woodchuck lives in is called hole
The place bear lives in is called den
The place fly lives in is called nest
The place hornet lives in is called nest
The place mole lives in is called hole
The place trout lives in is called river
The place raven lives in is called nest
The place fox lives in is called
2024-07-25 22:12:32 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 22:15:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0938, -0.9961,  2.8906,  ..., -2.0625, -0.0078, -3.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 26.8750, -10.6875,   6.5625,  ...,  -6.8750,   0.0938, -26.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0198, -0.0052,  0.0145,  ..., -0.0042, -0.0007,  0.0046],
        [-0.0022,  0.0176, -0.0010,  ..., -0.0008, -0.0017,  0.0036],
        [-0.0016,  0.0025,  0.1133,  ..., -0.0028, -0.0057,  0.0048],
        ...,
        [ 0.0017, -0.0005, -0.0176,  ...,  0.0206, -0.0098,  0.0068],
        [-0.0016,  0.0031,  0.0010,  ..., -0.0027,  0.0403, -0.0011],
        [ 0.0056, -0.0030, -0.0085,  ..., -0.0029,  0.0051,  0.0459]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 25.6719,  -9.1797,   4.4688,  ...,  -6.7773,   0.7661, -26.1719]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 22:15:21 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The place woodchuck lives in is called hole
The place bear lives in is called den
The place fly lives in is called nest
The place hornet lives in is called nest
The place mole lives in is called hole
The place trout lives in is called river
The place raven lives in is called nest
The place fox lives in is called
2024-07-25 22:15:21 root INFO     [order_1_approx] starting weight calculation for The place raven lives in is called nest
The place trout lives in is called river
The place hornet lives in is called nest
The place mole lives in is called hole
The place fly lives in is called nest
The place bear lives in is called den
The place fox lives in is called den
The place woodchuck lives in is called
2024-07-25 22:15:21 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 22:18:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.9531, -2.3125,  4.3438,  ...,  1.3594, -1.4922, -1.1953],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 17.3750,  -1.5469,   2.5938,  ...,  -9.6250,   6.7500, -22.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.7832e-02,  5.1575e-03,  3.8528e-04,  ..., -1.8616e-03,
         -1.9165e-02, -5.4626e-03],
        [ 4.6692e-03,  2.2705e-02,  4.1504e-03,  ..., -1.8463e-03,
          1.3489e-02, -1.2589e-03],
        [ 5.3406e-05,  2.2583e-03,  1.5430e-01,  ...,  8.8501e-03,
         -1.2573e-02,  1.2207e-04],
        ...,
        [ 1.1658e-02,  7.7515e-03, -2.0508e-02,  ...,  3.2227e-02,
         -1.9287e-02,  1.5198e-02],
        [-2.4414e-03, -4.5471e-03,  2.0874e-02,  ...,  7.0953e-04,
          5.0293e-02,  2.0142e-03],
        [ 1.4267e-03, -6.8970e-03, -2.3804e-02,  ..., -6.7139e-03,
          1.2146e-02,  6.9336e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 17.6562,  -0.7275,   2.0312,  ...,  -8.5781,   5.6992, -22.1094]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 22:18:10 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The place raven lives in is called nest
The place trout lives in is called river
The place hornet lives in is called nest
The place mole lives in is called hole
The place fly lives in is called nest
The place bear lives in is called den
The place fox lives in is called den
The place woodchuck lives in is called
2024-07-25 22:18:10 root INFO     total operator prediction time: 1360.1133995056152 seconds
2024-07-25 22:18:10 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on male - female
2024-07-25 22:18:10 root INFO     building operator male - female
2024-07-25 22:18:10 root INFO     [order_1_approx] starting weight calculation for A female gentleman is known as a lady
A female father is known as a mother
A female grandson is known as a granddaughter
A female headmaster is known as a headmistress
A female boy is known as a girl
A female batman is known as a batwoman
A female god is known as a goddess
A female tiger is known as a
2024-07-25 22:18:10 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 22:20:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4297, -0.6797,  0.1641,  ..., -1.8594, -0.9336,  0.4141],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -1.3906,  -7.9375,   5.3125,  ..., -21.7500,   7.8125,   7.3125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.0620e-02,  4.1199e-03,  1.6968e-02,  ..., -7.4463e-03,
          1.1353e-02,  1.5259e-05],
        [-2.8992e-03,  1.8555e-02, -8.3008e-03,  ..., -3.7231e-03,
          6.1035e-03,  2.0752e-03],
        [-2.4567e-03, -2.6245e-03,  9.2773e-02,  ...,  4.6387e-03,
         -3.5095e-04, -1.6937e-03],
        ...,
        [ 5.7983e-03,  7.9956e-03, -1.1536e-02,  ...,  1.1414e-02,
         -4.5776e-03,  1.0376e-03],
        [-7.5073e-03, -1.1292e-03, -1.9653e-02,  ..., -1.0315e-02,
          2.8442e-02,  3.5095e-04],
        [ 1.1902e-03, -3.3112e-03, -1.3794e-02,  ...,  1.8158e-03,
         -3.6316e-03,  2.6855e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -5.7188, -10.1641,   5.7148,  ..., -20.7188,  10.3828,   8.2734]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 22:20:57 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A female gentleman is known as a lady
A female father is known as a mother
A female grandson is known as a granddaughter
A female headmaster is known as a headmistress
A female boy is known as a girl
A female batman is known as a batwoman
A female god is known as a goddess
A female tiger is known as a
2024-07-25 22:20:57 root INFO     [order_1_approx] starting weight calculation for A female god is known as a goddess
A female father is known as a mother
A female batman is known as a batwoman
A female headmaster is known as a headmistress
A female boy is known as a girl
A female tiger is known as a tigress
A female grandson is known as a granddaughter
A female gentleman is known as a
2024-07-25 22:20:57 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 22:23:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.4375, -0.1162,  1.3125,  ...,  0.1289, -0.4180, -0.1172],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 14.6875,  -1.1016,   1.0547,  ..., -12.7500,   5.9375,  12.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0222,  0.0008, -0.0148,  ..., -0.0009,  0.0125, -0.0019],
        [ 0.0021,  0.0111, -0.0112,  ..., -0.0001,  0.0093,  0.0120],
        [ 0.0036, -0.0043,  0.0991,  ..., -0.0016,  0.0051, -0.0048],
        ...,
        [ 0.0054,  0.0046,  0.0082,  ...,  0.0140, -0.0153, -0.0054],
        [ 0.0040,  0.0077,  0.0107,  ...,  0.0008,  0.0317, -0.0074],
        [ 0.0033, -0.0023, -0.0092,  ..., -0.0063, -0.0030,  0.0349]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 15.8281,  -0.2739,   1.2490,  ..., -11.1328,   5.7383,  11.2109]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 22:23:44 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A female god is known as a goddess
A female father is known as a mother
A female batman is known as a batwoman
A female headmaster is known as a headmistress
A female boy is known as a girl
A female tiger is known as a tigress
A female grandson is known as a granddaughter
A female gentleman is known as a
2024-07-25 22:23:44 root INFO     [order_1_approx] starting weight calculation for A female tiger is known as a tigress
A female gentleman is known as a lady
A female god is known as a goddess
A female father is known as a mother
A female boy is known as a girl
A female headmaster is known as a headmistress
A female batman is known as a batwoman
A female grandson is known as a
2024-07-25 22:23:44 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 22:26:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.9766, -2.5938, -1.8906,  ..., -1.0391, -0.5898,  3.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -3.8281, -13.9375,   9.0000,  ...,  -5.7188,  -0.2344,  -0.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.8799e-02,  7.6294e-05,  3.3569e-04,  ...,  5.1880e-03,
          2.0996e-02,  8.2397e-04],
        [ 2.1515e-03,  1.5381e-02,  1.5503e-02,  ..., -2.1667e-03,
          1.5625e-02,  1.2634e-02],
        [-3.2043e-04, -9.8267e-03,  1.1621e-01,  ..., -3.4332e-04,
          1.3245e-02, -1.7395e-03],
        ...,
        [ 6.0730e-03, -8.0490e-04, -6.7139e-04,  ...,  1.8555e-02,
         -3.8757e-03,  3.5248e-03],
        [ 1.0376e-02,  5.8594e-03, -7.5073e-03,  ..., -3.5553e-03,
          3.6621e-02,  3.5095e-03],
        [ 3.0518e-04, -9.6130e-04, -1.3306e-02,  ..., -4.1504e-03,
         -2.4872e-03,  3.6377e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -2.4414, -14.6797,   8.6016,  ...,  -3.0742,   0.2505,  -0.6631]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 22:26:29 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A female tiger is known as a tigress
A female gentleman is known as a lady
A female god is known as a goddess
A female father is known as a mother
A female boy is known as a girl
A female headmaster is known as a headmistress
A female batman is known as a batwoman
A female grandson is known as a
2024-07-25 22:26:29 root INFO     [order_1_approx] starting weight calculation for A female batman is known as a batwoman
A female god is known as a goddess
A female grandson is known as a granddaughter
A female gentleman is known as a lady
A female boy is known as a girl
A female headmaster is known as a headmistress
A female tiger is known as a tigress
A female father is known as a
2024-07-25 22:26:29 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 22:29:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8281,  2.3125, -1.7578,  ..., -1.4766, -0.9688, -0.9766],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  1.5625,  -3.3594,  14.5000,  ..., -15.6250,   9.5625,   8.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0330,  0.0028,  0.0223,  ..., -0.0041,  0.0066, -0.0029],
        [-0.0040,  0.0110, -0.0098,  ...,  0.0012,  0.0047, -0.0042],
        [ 0.0036,  0.0022,  0.1074,  ..., -0.0107,  0.0009, -0.0036],
        ...,
        [ 0.0033,  0.0053,  0.0284,  ...,  0.0190, -0.0083,  0.0081],
        [ 0.0019, -0.0005,  0.0011,  ..., -0.0009,  0.0430,  0.0010],
        [-0.0025, -0.0053,  0.0036,  ...,  0.0013, -0.0015,  0.0386]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  1.3799,  -3.6211,  13.2734,  ..., -15.7188,  11.3984,   9.1953]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 22:29:15 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A female batman is known as a batwoman
A female god is known as a goddess
A female grandson is known as a granddaughter
A female gentleman is known as a lady
A female boy is known as a girl
A female headmaster is known as a headmistress
A female tiger is known as a tigress
A female father is known as a
2024-07-25 22:29:15 root INFO     [order_1_approx] starting weight calculation for A female grandson is known as a granddaughter
A female headmaster is known as a headmistress
A female tiger is known as a tigress
A female batman is known as a batwoman
A female gentleman is known as a lady
A female father is known as a mother
A female boy is known as a girl
A female god is known as a
2024-07-25 22:29:15 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 22:32:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2520,  2.0469, -3.6406,  ..., -2.5781, -2.1562, -3.0312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 3.5312, -3.1562,  9.9375,  ..., -5.3125, 22.0000, -5.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.1484e-02,  1.7548e-03,  1.5411e-03,  ...,  1.5945e-03,
          7.9346e-03, -3.1891e-03],
        [ 6.5613e-04,  9.8267e-03, -2.5177e-04,  ...,  1.1139e-03,
          2.3193e-03,  1.1108e-02],
        [ 3.7384e-03, -1.4191e-03,  9.4727e-02,  ...,  1.0757e-03,
         -9.1553e-05, -1.4420e-03],
        ...,
        [ 2.7924e-03,  6.2866e-03, -2.0447e-03,  ...,  1.0986e-02,
         -5.8289e-03,  3.5095e-04],
        [ 1.9073e-03,  1.9226e-03,  3.6469e-03,  ..., -6.1035e-03,
          2.6245e-02,  1.8768e-03],
        [-2.8687e-03, -7.3242e-03, -5.1880e-04,  ...,  3.4790e-03,
         -8.9111e-03,  2.6611e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.5430, -2.3633,  9.3984,  ..., -4.9727, 22.8906, -3.9180]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 22:32:02 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A female grandson is known as a granddaughter
A female headmaster is known as a headmistress
A female tiger is known as a tigress
A female batman is known as a batwoman
A female gentleman is known as a lady
A female father is known as a mother
A female boy is known as a girl
A female god is known as a
2024-07-25 22:32:02 root INFO     [order_1_approx] starting weight calculation for A female gentleman is known as a lady
A female tiger is known as a tigress
A female god is known as a goddess
A female batman is known as a batwoman
A female grandson is known as a granddaughter
A female father is known as a mother
A female boy is known as a girl
A female headmaster is known as a
2024-07-25 22:32:02 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 22:34:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0469, -0.1660, -0.9688,  ..., -3.4062, -1.2578,  1.5234],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 13.3750,   3.3438,   5.7500,  ..., -14.9375,  -3.8750,   8.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0322,  0.0049, -0.0056,  ...,  0.0023,  0.0016,  0.0031],
        [ 0.0126,  0.0190,  0.0083,  ..., -0.0012,  0.0055,  0.0044],
        [ 0.0086, -0.0059,  0.1191,  ..., -0.0050,  0.0090,  0.0031],
        ...,
        [ 0.0006, -0.0006, -0.0092,  ...,  0.0164, -0.0010, -0.0045],
        [-0.0017,  0.0073, -0.0181,  ..., -0.0065,  0.0454, -0.0008],
        [ 0.0073,  0.0002, -0.0079,  ..., -0.0036,  0.0003,  0.0282]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 13.7500,   2.9316,   8.8984,  ..., -16.1562,  -2.6367,   6.6328]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 22:34:49 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A female gentleman is known as a lady
A female tiger is known as a tigress
A female god is known as a goddess
A female batman is known as a batwoman
A female grandson is known as a granddaughter
A female father is known as a mother
A female boy is known as a girl
A female headmaster is known as a
2024-07-25 22:34:49 root INFO     [order_1_approx] starting weight calculation for A female gentleman is known as a lady
A female grandson is known as a granddaughter
A female boy is known as a girl
A female god is known as a goddess
A female tiger is known as a tigress
A female headmaster is known as a headmistress
A female father is known as a mother
A female batman is known as a
2024-07-25 22:34:49 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 22:37:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5000, -0.3008, -0.0427,  ..., -0.7500, -2.4375, -0.4141],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-1.5938, -8.8125, -1.5781,  ...,  0.9453, 18.2500, 13.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0508, -0.0007, -0.0199,  ...,  0.0055,  0.0055, -0.0067],
        [ 0.0109,  0.0337,  0.0084,  ...,  0.0055,  0.0093,  0.0166],
        [ 0.0109, -0.0096,  0.1992,  ..., -0.0083,  0.0061, -0.0026],
        ...,
        [ 0.0278,  0.0003, -0.0006,  ...,  0.0300, -0.0186, -0.0171],
        [-0.0083,  0.0027, -0.0004,  ..., -0.0126,  0.0623,  0.0095],
        [-0.0013, -0.0026,  0.0178,  ...,  0.0083,  0.0043,  0.0532]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6211, -8.8828, -0.5811,  ...,  2.0332, 19.2031, 10.8359]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 22:37:36 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A female gentleman is known as a lady
A female grandson is known as a granddaughter
A female boy is known as a girl
A female god is known as a goddess
A female tiger is known as a tigress
A female headmaster is known as a headmistress
A female father is known as a mother
A female batman is known as a
2024-07-25 22:37:37 root INFO     [order_1_approx] starting weight calculation for A female god is known as a goddess
A female tiger is known as a tigress
A female headmaster is known as a headmistress
A female batman is known as a batwoman
A female father is known as a mother
A female grandson is known as a granddaughter
A female gentleman is known as a lady
A female boy is known as a
2024-07-25 22:37:37 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 22:40:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.8906,  0.0586,  0.7461,  ..., -0.7500, -2.2500,  0.7344],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 11.7500, -11.3750,  13.8750,  ...,  -4.1562,  -2.0000,  -0.1680],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0265,  0.0023, -0.0023,  ..., -0.0062,  0.0142, -0.0019],
        [ 0.0024,  0.0119,  0.0079,  ...,  0.0012,  0.0049,  0.0055],
        [ 0.0006, -0.0065,  0.1025,  ..., -0.0015, -0.0003,  0.0013],
        ...,
        [ 0.0093,  0.0038, -0.0003,  ...,  0.0137,  0.0018,  0.0034],
        [-0.0042,  0.0048, -0.0021,  ..., -0.0037,  0.0415,  0.0009],
        [-0.0004, -0.0052, -0.0143,  ..., -0.0007, -0.0031,  0.0291]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[12.5000, -9.9766, 13.6953,  ..., -4.1484, -1.5420,  1.2197]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 22:40:24 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A female god is known as a goddess
A female tiger is known as a tigress
A female headmaster is known as a headmistress
A female batman is known as a batwoman
A female father is known as a mother
A female grandson is known as a granddaughter
A female gentleman is known as a lady
A female boy is known as a
2024-07-25 22:40:24 root INFO     total operator prediction time: 1333.718432188034 seconds
2024-07-25 22:40:24 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on name - occupation
2024-07-25 22:40:24 root INFO     building operator name - occupation
2024-07-25 22:40:24 root INFO     [order_1_approx] starting weight calculation for mencius was known for their work as a  philosopher
picasso was known for their work as a  painter
shakespeare was known for their work as a  playwright
tolstoi was known for their work as a  novelist
raphael was known for their work as a  painter
marx was known for their work as a  philosopher
hawking was known for their work as a  physicist
maxwell was known for their work as a 
2024-07-25 22:40:24 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 22:43:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1641, -1.2344, -0.9805,  ..., -0.7734, -1.7188, -3.4688],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-11.6875,  35.2500,   1.5000,  ...,  -2.3438,   5.7188,  15.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.2949e-02, -8.8120e-04, -1.4160e-02,  ...,  1.2817e-03,
         -2.2888e-03,  3.9062e-03],
        [ 6.4087e-03,  1.1719e-02, -2.1362e-03,  ..., -7.4005e-04,
         -2.0905e-03,  5.3711e-03],
        [ 2.9907e-03, -4.0283e-03,  9.5703e-02,  ..., -1.4343e-03,
         -1.2054e-03,  6.1951e-03],
        ...,
        [ 9.7656e-04, -3.9368e-03,  1.3672e-02,  ...,  1.0803e-02,
          1.6785e-03,  9.2163e-03],
        [-1.5411e-03,  3.0518e-05, -8.3618e-03,  ..., -2.9449e-03,
          3.0396e-02,  5.0354e-03],
        [-1.3504e-03, -3.9062e-03,  1.9836e-03,  ..., -2.2278e-03,
          6.2943e-05,  3.7109e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-12.2188,  34.7188,   1.2930,  ...,  -1.8574,   6.0430,  15.6094]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 22:43:11 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for mencius was known for their work as a  philosopher
picasso was known for their work as a  painter
shakespeare was known for their work as a  playwright
tolstoi was known for their work as a  novelist
raphael was known for their work as a  painter
marx was known for their work as a  philosopher
hawking was known for their work as a  physicist
maxwell was known for their work as a 
2024-07-25 22:43:11 root INFO     [order_1_approx] starting weight calculation for marx was known for their work as a  philosopher
maxwell was known for their work as a  physicist
hawking was known for their work as a  physicist
tolstoi was known for their work as a  novelist
mencius was known for their work as a  philosopher
shakespeare was known for their work as a  playwright
raphael was known for their work as a  painter
picasso was known for their work as a 
2024-07-25 22:43:11 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 22:45:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.2500, -2.3438, -3.7188,  ..., -1.6875, -4.3125,  0.9297],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-16.0000,  30.8750,   0.0938,  ...,   3.5000,   6.7812,   7.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.0315e-02,  6.1798e-04, -2.6321e-04,  ...,  2.4109e-03,
          1.3885e-03,  3.5706e-03],
        [ 1.8387e-03,  6.0425e-03,  9.9182e-05,  ..., -1.2741e-03,
          2.4414e-03,  1.2970e-04],
        [ 7.0190e-04, -1.5259e-03,  3.3691e-02,  ..., -2.0599e-03,
          7.6294e-05, -1.2054e-03],
        ...,
        [-1.2817e-03,  3.5400e-03,  3.0518e-03,  ...,  7.9346e-03,
          1.3428e-03,  1.9836e-03],
        [-7.8201e-04,  1.6327e-03, -2.5330e-03,  ..., -2.0752e-03,
          1.4893e-02, -4.3869e-04],
        [ 6.7902e-04, -2.3041e-03, -2.1973e-03,  ...,  1.2436e-03,
         -9.1171e-04,  1.3184e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-15.7344,  30.9375,   0.0317,  ...,   3.6484,   6.7422,   6.8516]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 22:45:57 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for marx was known for their work as a  philosopher
maxwell was known for their work as a  physicist
hawking was known for their work as a  physicist
tolstoi was known for their work as a  novelist
mencius was known for their work as a  philosopher
shakespeare was known for their work as a  playwright
raphael was known for their work as a  painter
picasso was known for their work as a 
2024-07-25 22:45:58 root INFO     [order_1_approx] starting weight calculation for maxwell was known for their work as a  physicist
mencius was known for their work as a  philosopher
hawking was known for their work as a  physicist
shakespeare was known for their work as a  playwright
raphael was known for their work as a  painter
picasso was known for their work as a  painter
marx was known for their work as a  philosopher
tolstoi was known for their work as a 
2024-07-25 22:45:58 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 22:48:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.7188, -4.3125,  2.8906,  ..., -2.3594, -4.1250,  0.7812],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-16.6250,  32.5000,   5.8438,  ...,   4.0938,   6.0625,   5.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.3184e-02, -8.9264e-04, -3.9673e-03,  ...,  1.0910e-03,
          1.9379e-03,  1.6785e-03],
        [ 1.1902e-03,  7.7515e-03, -2.7161e-03,  ..., -1.1749e-03,
         -4.0245e-04, -1.5259e-05],
        [ 3.2806e-04, -3.6316e-03,  4.1016e-02,  ...,  6.5994e-04,
         -3.5095e-04,  3.6430e-04],
        ...,
        [ 1.2054e-03, -3.6430e-04,  1.5182e-03,  ...,  9.0332e-03,
          9.4223e-04,  1.7853e-03],
        [-4.3869e-04,  1.2589e-03, -1.7548e-03,  ..., -2.1362e-03,
          1.5259e-02, -9.8419e-04],
        [ 2.9907e-03, -3.5095e-03, -3.5248e-03,  ..., -8.0490e-04,
         -5.9509e-04,  1.9409e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-17.2188,  32.4375,   5.9180,  ...,   4.7227,   6.2891,   5.9414]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 22:48:47 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for maxwell was known for their work as a  physicist
mencius was known for their work as a  philosopher
hawking was known for their work as a  physicist
shakespeare was known for their work as a  playwright
raphael was known for their work as a  painter
picasso was known for their work as a  painter
marx was known for their work as a  philosopher
tolstoi was known for their work as a 
2024-07-25 22:49:01 root INFO     [order_1_approx] starting weight calculation for marx was known for their work as a  philosopher
tolstoi was known for their work as a  novelist
raphael was known for their work as a  painter
picasso was known for their work as a  painter
mencius was known for their work as a  philosopher
shakespeare was known for their work as a  playwright
maxwell was known for their work as a  physicist
hawking was known for their work as a 
2024-07-25 22:49:01 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 22:51:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.2344, -0.0234, -1.9062,  ..., -2.1406, -5.0625,  0.3828],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-10.0000,  28.5000,   0.0625,  ...,  -0.1875,   2.5938,  11.9375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.0630e-02,  1.6174e-03, -7.5989e-03,  ...,  3.8910e-04,
          2.1362e-03,  4.5166e-03],
        [ 6.0425e-03,  9.6436e-03,  3.0212e-03,  ...,  2.6855e-03,
         -2.7161e-03,  2.4719e-03],
        [ 2.1057e-03, -5.1880e-03,  7.5195e-02,  ...,  3.1891e-03,
         -3.5706e-03,  4.5471e-03],
        ...,
        [-3.3722e-03,  5.7220e-05,  5.0049e-03,  ...,  1.2024e-02,
         -2.0294e-03,  2.9602e-03],
        [ 2.6550e-03,  1.7624e-03, -3.8757e-03,  ..., -3.3264e-03,
          2.1118e-02, -4.9210e-04],
        [ 2.4109e-03, -4.9438e-03, -2.8381e-03,  ...,  1.0376e-03,
         -8.3923e-04,  2.8320e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-10.1484,  29.2188,   0.3599,  ...,  -0.2759,   3.0391,  12.7188]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 22:51:49 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for marx was known for their work as a  philosopher
tolstoi was known for their work as a  novelist
raphael was known for their work as a  painter
picasso was known for their work as a  painter
mencius was known for their work as a  philosopher
shakespeare was known for their work as a  playwright
maxwell was known for their work as a  physicist
hawking was known for their work as a 
2024-07-25 22:51:49 root INFO     [order_1_approx] starting weight calculation for maxwell was known for their work as a  physicist
picasso was known for their work as a  painter
marx was known for their work as a  philosopher
tolstoi was known for their work as a  novelist
shakespeare was known for their work as a  playwright
hawking was known for their work as a  physicist
mencius was known for their work as a  philosopher
raphael was known for their work as a 
2024-07-25 22:51:49 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 22:54:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7812, -0.1641, -1.0078,  ..., -0.0781, -5.0938, -0.5312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-18.3750,  35.2500,  -0.8438,  ...,  -0.2188,   3.3125,  10.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.1719e-02, -1.2970e-03,  1.8234e-03,  ...,  3.4142e-04,
          2.8076e-03,  1.8921e-03],
        [ 1.7014e-03,  9.2163e-03,  3.2425e-04,  ...,  3.4637e-03,
          1.2360e-03,  5.6458e-04],
        [ 2.6550e-03, -1.2054e-03,  3.1738e-02,  ..., -1.2894e-03,
         -1.1749e-03,  4.0436e-04],
        ...,
        [ 1.0071e-03,  8.8882e-04,  7.4768e-03,  ...,  1.0376e-02,
          7.1716e-04,  1.4191e-03],
        [-2.7161e-03,  3.8147e-04, -3.5400e-03,  ..., -1.7166e-03,
          1.7212e-02,  2.6703e-04],
        [-2.6703e-05, -2.1973e-03, -2.2278e-03,  ...,  2.2278e-03,
          8.5449e-04,  1.5869e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-17.9062,  35.4062,  -0.4543,  ...,   0.2317,   3.4395,  10.6172]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 22:54:37 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for maxwell was known for their work as a  physicist
picasso was known for their work as a  painter
marx was known for their work as a  philosopher
tolstoi was known for their work as a  novelist
shakespeare was known for their work as a  playwright
hawking was known for their work as a  physicist
mencius was known for their work as a  philosopher
raphael was known for their work as a 
2024-07-25 22:54:37 root INFO     [order_1_approx] starting weight calculation for marx was known for their work as a  philosopher
raphael was known for their work as a  painter
hawking was known for their work as a  physicist
mencius was known for their work as a  philosopher
tolstoi was known for their work as a  novelist
maxwell was known for their work as a  physicist
picasso was known for their work as a  painter
shakespeare was known for their work as a 
2024-07-25 22:54:37 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 22:57:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.3438, -2.8125, -2.7500,  ..., -1.1875, -4.6875, -1.0078],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-21.2500,  31.5000,   2.8438,  ...,   6.5000,   4.4375,   1.3125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.1292e-02,  1.1673e-03, -1.0071e-03,  ...,  3.7670e-05,
          3.4943e-03,  4.2152e-04],
        [ 1.0834e-03,  5.6763e-03,  3.9062e-03,  ...,  2.0905e-03,
          3.2349e-03, -1.0986e-03],
        [ 2.1667e-03, -3.9978e-03,  4.3945e-02,  ...,  1.3733e-04,
         -1.6098e-03, -3.5477e-04],
        ...,
        [ 6.8665e-04,  3.2425e-04,  3.9062e-03,  ...,  8.4229e-03,
         -3.4904e-04,  1.0681e-03],
        [-3.1090e-04, -5.4169e-04, -4.2725e-03,  ..., -9.6512e-04,
          1.4221e-02, -3.4332e-04],
        [ 3.3264e-03, -3.5706e-03, -1.6937e-03,  ...,  2.3499e-03,
          7.6294e-04,  1.4038e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-21.4688,  31.4531,   3.2324,  ...,   6.3438,   4.4688,   1.5078]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 22:57:24 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for marx was known for their work as a  philosopher
raphael was known for their work as a  painter
hawking was known for their work as a  physicist
mencius was known for their work as a  philosopher
tolstoi was known for their work as a  novelist
maxwell was known for their work as a  physicist
picasso was known for their work as a  painter
shakespeare was known for their work as a 
2024-07-25 22:57:24 root INFO     [order_1_approx] starting weight calculation for picasso was known for their work as a  painter
raphael was known for their work as a  painter
maxwell was known for their work as a  physicist
tolstoi was known for their work as a  novelist
hawking was known for their work as a  physicist
shakespeare was known for their work as a  playwright
mencius was known for their work as a  philosopher
marx was known for their work as a 
2024-07-25 22:57:25 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 23:00:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2188, -3.2656,  2.5781,  ..., -2.0781, -4.2188, -1.5703],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-14.3125,  35.0000,   0.3750,  ...,  -1.7656,   1.8828,  17.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0164, -0.0012, -0.0046,  ...,  0.0018,  0.0009, -0.0033],
        [ 0.0039,  0.0095,  0.0040,  ...,  0.0016, -0.0004,  0.0005],
        [-0.0011, -0.0026,  0.0557,  ...,  0.0006,  0.0008,  0.0023],
        ...,
        [-0.0025,  0.0008,  0.0083,  ...,  0.0077, -0.0025,  0.0054],
        [-0.0026,  0.0034, -0.0027,  ..., -0.0028,  0.0240, -0.0009],
        [ 0.0007, -0.0025, -0.0018,  ..., -0.0005, -0.0013,  0.0195]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-14.5078,  34.5938,   0.2340,  ...,  -1.8887,   2.3496,  16.8281]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 23:00:12 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for picasso was known for their work as a  painter
raphael was known for their work as a  painter
maxwell was known for their work as a  physicist
tolstoi was known for their work as a  novelist
hawking was known for their work as a  physicist
shakespeare was known for their work as a  playwright
mencius was known for their work as a  philosopher
marx was known for their work as a 
2024-07-25 23:00:12 root INFO     [order_1_approx] starting weight calculation for hawking was known for their work as a  physicist
picasso was known for their work as a  painter
tolstoi was known for their work as a  novelist
marx was known for their work as a  philosopher
raphael was known for their work as a  painter
maxwell was known for their work as a  physicist
shakespeare was known for their work as a  playwright
mencius was known for their work as a 
2024-07-25 23:00:12 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 23:02:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-3.5625,  0.8398,  1.7500,  ...,  0.5234, -5.8125, -4.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-14.0000,  35.0000,   5.0000,  ...,  -3.2188,  -1.0000,   9.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0332, -0.0038, -0.0123,  ...,  0.0046, -0.0011,  0.0079],
        [-0.0018,  0.0127,  0.0041,  ..., -0.0026,  0.0098,  0.0007],
        [ 0.0010, -0.0089,  0.0811,  ...,  0.0017, -0.0008,  0.0092],
        ...,
        [-0.0014,  0.0035,  0.0079,  ...,  0.0148, -0.0070,  0.0048],
        [-0.0002,  0.0029, -0.0047,  ..., -0.0017,  0.0369, -0.0006],
        [ 0.0008, -0.0022,  0.0057,  ..., -0.0023,  0.0049,  0.0310]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-13.7656,  35.8438,   4.8633,  ...,  -2.2891,  -0.7749,   9.7656]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 23:02:59 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for hawking was known for their work as a  physicist
picasso was known for their work as a  painter
tolstoi was known for their work as a  novelist
marx was known for their work as a  philosopher
raphael was known for their work as a  painter
maxwell was known for their work as a  physicist
shakespeare was known for their work as a  playwright
mencius was known for their work as a 
2024-07-25 23:02:59 root INFO     total operator prediction time: 1355.8305141925812 seconds
2024-07-25 23:02:59 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on country - capital
2024-07-25 23:02:59 root INFO     building operator country - capital
2024-07-25 23:03:00 root INFO     [order_1_approx] starting weight calculation for The country with damascus as its capital is known as syria
The country with nairobi as its capital is known as kenya
The country with tokyo as its capital is known as japan
The country with athens as its capital is known as greece
The country with hanoi as its capital is known as vietnam
The country with paris as its capital is known as france
The country with belgrade as its capital is known as serbia
The country with warsaw as its capital is known as
2024-07-25 23:03:00 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 23:05:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0625,  0.4141,  0.8828,  ...,  0.8320, -0.5508, -0.4297],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 3.3906,  3.4531,  5.5625,  ..., -0.5938, -0.9297,  8.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.4832e-02, -4.2725e-04,  1.0254e-02,  ...,  2.9297e-03,
          2.7008e-03,  2.1667e-03],
        [-2.7466e-04,  8.2397e-03, -1.9836e-04,  ...,  1.3428e-03,
          2.6703e-03, -2.2125e-03],
        [ 6.1035e-05,  2.7008e-03,  6.3477e-02,  ..., -3.5706e-03,
         -1.5640e-03,  6.9580e-03],
        ...,
        [ 2.9297e-03, -9.4986e-04, -3.8605e-03,  ...,  9.5825e-03,
         -7.9346e-03,  2.3193e-03],
        [-3.7079e-03,  2.9144e-03,  6.4697e-03,  ..., -2.0905e-03,
          3.2227e-02,  3.0518e-03],
        [-3.0060e-03, -4.6387e-03,  5.4626e-03,  ...,  9.0599e-06,
         -3.5095e-03,  1.9287e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.9121,  3.2422,  6.2148,  ..., -0.5537, -1.0078,  8.7969]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 23:05:46 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The country with damascus as its capital is known as syria
The country with nairobi as its capital is known as kenya
The country with tokyo as its capital is known as japan
The country with athens as its capital is known as greece
The country with hanoi as its capital is known as vietnam
The country with paris as its capital is known as france
The country with belgrade as its capital is known as serbia
The country with warsaw as its capital is known as
2024-07-25 23:05:46 root INFO     [order_1_approx] starting weight calculation for The country with nairobi as its capital is known as kenya
The country with damascus as its capital is known as syria
The country with paris as its capital is known as france
The country with warsaw as its capital is known as poland
The country with tokyo as its capital is known as japan
The country with belgrade as its capital is known as serbia
The country with athens as its capital is known as greece
The country with hanoi as its capital is known as
2024-07-25 23:05:46 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 23:08:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0156, -0.4883,  4.5312,  ..., -1.4219,  0.5156, -2.2656],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -9.1250,  12.4375,  19.8750,  ...,  -3.0938, -11.0625,  21.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.2817e-02, -8.2779e-04,  2.4261e-03,  ..., -8.7738e-04,
          5.4169e-04,  6.3324e-04],
        [-1.5259e-05,  9.2773e-03, -1.7395e-03,  ..., -2.5024e-03,
          4.5471e-03, -4.6692e-03],
        [-3.4790e-03, -4.5776e-03,  6.1523e-02,  ...,  1.5259e-03,
         -3.5858e-03,  2.8381e-03],
        ...,
        [-6.8665e-05, -1.9379e-03,  5.4321e-03,  ...,  1.0498e-02,
         -3.1128e-03,  1.3657e-03],
        [-8.0872e-04,  3.1433e-03, -5.3101e-03,  ..., -5.1270e-03,
          2.4902e-02,  2.0752e-03],
        [-5.7373e-03, -4.6997e-03,  4.6997e-03,  ...,  7.1335e-04,
          2.3499e-03,  2.5024e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -8.2188,  12.2812,  19.7969,  ...,  -2.7227, -11.4609,  20.7188]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 23:08:33 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The country with nairobi as its capital is known as kenya
The country with damascus as its capital is known as syria
The country with paris as its capital is known as france
The country with warsaw as its capital is known as poland
The country with tokyo as its capital is known as japan
The country with belgrade as its capital is known as serbia
The country with athens as its capital is known as greece
The country with hanoi as its capital is known as
2024-07-25 23:08:33 root INFO     [order_1_approx] starting weight calculation for The country with tokyo as its capital is known as japan
The country with athens as its capital is known as greece
The country with hanoi as its capital is known as vietnam
The country with warsaw as its capital is known as poland
The country with paris as its capital is known as france
The country with damascus as its capital is known as syria
The country with belgrade as its capital is known as serbia
The country with nairobi as its capital is known as
2024-07-25 23:08:34 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 23:11:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.6250, -1.7109,  7.1250,  ..., -1.6641, -1.0156, -2.4688],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-12.5000,   6.2500,  23.3750,  ...,  -7.4375,  -6.5625,  -2.9375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0193, -0.0016,  0.0014,  ...,  0.0032, -0.0005,  0.0062],
        [ 0.0023,  0.0077, -0.0058,  ..., -0.0024,  0.0041, -0.0033],
        [-0.0098, -0.0034,  0.0610,  ..., -0.0017,  0.0060,  0.0049],
        ...,
        [-0.0015,  0.0006,  0.0000,  ...,  0.0093, -0.0036, -0.0034],
        [-0.0023, -0.0028,  0.0066,  ..., -0.0028,  0.0239, -0.0031],
        [-0.0037, -0.0068,  0.0027,  ..., -0.0014,  0.0017,  0.0247]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-12.7734,   6.0195,  23.5469,  ...,  -7.2188,  -6.1484,  -2.8945]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 23:11:21 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The country with tokyo as its capital is known as japan
The country with athens as its capital is known as greece
The country with hanoi as its capital is known as vietnam
The country with warsaw as its capital is known as poland
The country with paris as its capital is known as france
The country with damascus as its capital is known as syria
The country with belgrade as its capital is known as serbia
The country with nairobi as its capital is known as
2024-07-25 23:11:21 root INFO     [order_1_approx] starting weight calculation for The country with warsaw as its capital is known as poland
The country with athens as its capital is known as greece
The country with nairobi as its capital is known as kenya
The country with belgrade as its capital is known as serbia
The country with paris as its capital is known as france
The country with damascus as its capital is known as syria
The country with hanoi as its capital is known as vietnam
The country with tokyo as its capital is known as
2024-07-25 23:11:21 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 23:14:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.2812, -0.0352,  1.4922,  ..., -1.3281, -1.2969, -2.4844],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 0.0156,  5.9688, 10.0625,  ..., -0.6719, -9.0000,  3.5312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0094,  0.0004,  0.0012,  ...,  0.0032,  0.0022,  0.0043],
        [ 0.0049,  0.0047,  0.0035,  ...,  0.0006,  0.0015, -0.0037],
        [-0.0027, -0.0029,  0.0596,  ..., -0.0032, -0.0018,  0.0021],
        ...,
        [ 0.0005, -0.0012,  0.0024,  ...,  0.0108, -0.0019,  0.0013],
        [-0.0026,  0.0022,  0.0006,  ..., -0.0054,  0.0244, -0.0044],
        [-0.0001, -0.0022, -0.0004,  ..., -0.0020,  0.0017,  0.0168]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5952,  6.4570,  9.8203,  ..., -0.8647, -9.4219,  3.5430]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 23:14:08 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The country with warsaw as its capital is known as poland
The country with athens as its capital is known as greece
The country with nairobi as its capital is known as kenya
The country with belgrade as its capital is known as serbia
The country with paris as its capital is known as france
The country with damascus as its capital is known as syria
The country with hanoi as its capital is known as vietnam
The country with tokyo as its capital is known as
2024-07-25 23:14:08 root INFO     [order_1_approx] starting weight calculation for The country with nairobi as its capital is known as kenya
The country with belgrade as its capital is known as serbia
The country with hanoi as its capital is known as vietnam
The country with warsaw as its capital is known as poland
The country with paris as its capital is known as france
The country with athens as its capital is known as greece
The country with tokyo as its capital is known as japan
The country with damascus as its capital is known as
2024-07-25 23:14:08 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 23:16:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2031, -1.8516,  2.9219,  ...,  1.1719, -3.2031, -2.1094],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -7.0938,  11.1875,  17.3750,  ..., -12.3750,  -1.2656,  10.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.4282e-02, -3.7766e-04,  1.9836e-03,  ..., -4.1962e-05,
          3.9673e-03,  3.3264e-03],
        [ 1.8082e-03,  8.3618e-03,  8.9722e-03,  ..., -2.0599e-03,
          5.4321e-03, -6.4697e-03],
        [-4.5776e-04,  4.8218e-03,  7.2266e-02,  ..., -1.5259e-03,
          2.2125e-03, -8.7357e-04],
        ...,
        [ 4.0894e-03,  1.2817e-03, -1.3123e-02,  ...,  8.3008e-03,
         -6.7139e-04,  2.0599e-03],
        [-1.4343e-03,  2.0905e-03,  7.8583e-04,  ..., -4.4556e-03,
          2.3682e-02, -4.8218e-03],
        [-2.2583e-03, -2.5024e-03, -1.2360e-03,  ...,  1.9379e-03,
          9.9182e-04,  1.6113e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -7.0078,  11.5625,  17.8594,  ..., -12.6406,  -1.3418,  10.4453]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 23:16:55 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The country with nairobi as its capital is known as kenya
The country with belgrade as its capital is known as serbia
The country with hanoi as its capital is known as vietnam
The country with warsaw as its capital is known as poland
The country with paris as its capital is known as france
The country with athens as its capital is known as greece
The country with tokyo as its capital is known as japan
The country with damascus as its capital is known as
2024-07-25 23:16:55 root INFO     [order_1_approx] starting weight calculation for The country with nairobi as its capital is known as kenya
The country with tokyo as its capital is known as japan
The country with belgrade as its capital is known as serbia
The country with hanoi as its capital is known as vietnam
The country with paris as its capital is known as france
The country with damascus as its capital is known as syria
The country with warsaw as its capital is known as poland
The country with athens as its capital is known as
2024-07-25 23:16:55 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 23:19:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8125, -0.8750,  4.0000,  ..., -2.1250, -1.0078, -2.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -9.6250,   2.3125,  12.3750,  ..., -13.4375,  -6.4375,   4.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.8677e-02, -7.6294e-05,  9.6436e-03,  ...,  9.2316e-04,
          2.9907e-03, -3.2806e-04],
        [ 7.4768e-04,  8.5449e-03, -7.1106e-03,  ...,  9.4223e-04,
          3.3264e-03, -7.9346e-04],
        [-1.6602e-02,  6.6528e-03,  4.4678e-02,  ..., -4.3945e-03,
         -1.2512e-03,  6.1035e-04],
        ...,
        [-1.2817e-03,  4.9973e-04, -6.1035e-03,  ...,  8.1787e-03,
         -7.5989e-03, -1.2741e-03],
        [-1.9226e-03,  6.7139e-03,  1.0742e-02,  ..., -1.6403e-03,
          2.7710e-02, -3.2654e-03],
        [-6.7444e-03, -5.0354e-03, -3.7689e-03,  ...,  1.0147e-03,
         -3.3722e-03,  1.7822e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -9.2500,   2.1094,  12.4453,  ..., -12.5234,  -5.9688,   5.3906]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 23:19:39 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The country with nairobi as its capital is known as kenya
The country with tokyo as its capital is known as japan
The country with belgrade as its capital is known as serbia
The country with hanoi as its capital is known as vietnam
The country with paris as its capital is known as france
The country with damascus as its capital is known as syria
The country with warsaw as its capital is known as poland
The country with athens as its capital is known as
2024-07-25 23:19:39 root INFO     [order_1_approx] starting weight calculation for The country with nairobi as its capital is known as kenya
The country with tokyo as its capital is known as japan
The country with damascus as its capital is known as syria
The country with belgrade as its capital is known as serbia
The country with hanoi as its capital is known as vietnam
The country with athens as its capital is known as greece
The country with warsaw as its capital is known as poland
The country with paris as its capital is known as
2024-07-25 23:19:39 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 23:22:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7891, -0.4609,  5.0000,  ..., -2.2812, -3.0625, -3.0938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-5.2500, -4.8750, 14.1250,  ..., -8.1250, -6.0000,  1.5312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.2573e-02,  2.5940e-04, -5.0049e-03,  ...,  9.5367e-04,
          3.0518e-04,  2.3842e-04],
        [ 6.1035e-05,  1.0681e-02,  9.4604e-04,  ..., -9.5367e-04,
          3.6926e-03, -4.6692e-03],
        [-5.8899e-03,  4.6387e-03,  6.3477e-02,  ..., -1.4420e-03,
         -1.4572e-03, -5.7983e-04],
        ...,
        [ 4.9744e-03,  1.0986e-03, -1.9836e-04,  ...,  5.0659e-03,
         -5.7678e-03,  7.8201e-05],
        [-1.1444e-04,  2.4414e-03,  6.0425e-03,  ..., -3.7079e-03,
          2.3438e-02, -9.7656e-04],
        [-2.5635e-03, -6.5308e-03,  3.3569e-03,  ...,  2.0294e-03,
         -2.2125e-03,  1.4099e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.3164, -5.0820, 14.5391,  ..., -8.8047, -6.2305,  1.6357]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 23:22:26 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The country with nairobi as its capital is known as kenya
The country with tokyo as its capital is known as japan
The country with damascus as its capital is known as syria
The country with belgrade as its capital is known as serbia
The country with hanoi as its capital is known as vietnam
The country with athens as its capital is known as greece
The country with warsaw as its capital is known as poland
The country with paris as its capital is known as
2024-07-25 23:22:26 root INFO     [order_1_approx] starting weight calculation for The country with tokyo as its capital is known as japan
The country with damascus as its capital is known as syria
The country with nairobi as its capital is known as kenya
The country with hanoi as its capital is known as vietnam
The country with athens as its capital is known as greece
The country with paris as its capital is known as france
The country with warsaw as its capital is known as poland
The country with belgrade as its capital is known as
2024-07-25 23:22:27 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 23:25:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3926, -1.3594,  8.9375,  ..., -0.7930, -0.6094,  0.0547],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-5.7500, 12.5625, 18.6250,  ..., -6.5938, -2.3438, 16.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0140, -0.0036,  0.0032,  ...,  0.0042,  0.0059,  0.0045],
        [ 0.0020,  0.0094,  0.0010,  ..., -0.0049,  0.0072, -0.0109],
        [-0.0015,  0.0020,  0.0674,  ..., -0.0014, -0.0018,  0.0026],
        ...,
        [ 0.0003,  0.0051, -0.0234,  ...,  0.0101, -0.0101,  0.0002],
        [-0.0017,  0.0019,  0.0050,  ..., -0.0005,  0.0244,  0.0042],
        [-0.0107, -0.0034,  0.0083,  ...,  0.0012, -0.0004,  0.0232]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-6.2656, 12.7188, 18.5938,  ..., -7.7148, -1.4238, 15.8516]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 23:25:13 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The country with tokyo as its capital is known as japan
The country with damascus as its capital is known as syria
The country with nairobi as its capital is known as kenya
The country with hanoi as its capital is known as vietnam
The country with athens as its capital is known as greece
The country with paris as its capital is known as france
The country with warsaw as its capital is known as poland
The country with belgrade as its capital is known as
2024-07-25 23:25:13 root INFO     total operator prediction time: 1334.1113340854645 seconds
2024-07-25 23:25:13 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on things - color
2024-07-25 23:25:13 root INFO     building operator things - color
2024-07-25 23:25:14 root INFO     [order_1_approx] starting weight calculation for The sapphire is colored blue
The emerald is colored green
The swan is colored white
The ant is colored black
The fridge is colored white
The potato is colored brown
The cherry is colored red
The sea is colored
2024-07-25 23:25:14 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 23:28:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.2188,  0.2402,  4.5625,  ..., -3.0625, -2.3750, -2.3594],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 6.4375, -3.0625, -6.8125,  ...,  4.1875,  3.4062, -8.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0217, -0.0022, -0.0049,  ..., -0.0044,  0.0070, -0.0028],
        [-0.0027,  0.0155,  0.0134,  ...,  0.0004,  0.0021, -0.0030],
        [-0.0012,  0.0042,  0.0874,  ...,  0.0004,  0.0048, -0.0025],
        ...,
        [ 0.0019,  0.0022, -0.0014,  ...,  0.0153, -0.0002, -0.0025],
        [-0.0020,  0.0019, -0.0035,  ..., -0.0006,  0.0354, -0.0009],
        [-0.0005, -0.0035, -0.0173,  ...,  0.0018, -0.0022,  0.0261]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.4219, -3.1406, -6.8594,  ...,  5.3906,  3.8652, -8.4609]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 23:28:01 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The sapphire is colored blue
The emerald is colored green
The swan is colored white
The ant is colored black
The fridge is colored white
The potato is colored brown
The cherry is colored red
The sea is colored
2024-07-25 23:28:01 root INFO     [order_1_approx] starting weight calculation for The cherry is colored red
The ant is colored black
The fridge is colored white
The potato is colored brown
The swan is colored white
The sapphire is colored blue
The sea is colored blue
The emerald is colored
2024-07-25 23:28:02 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 23:30:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0234, -0.3008,  0.0356,  ..., -1.8281, -0.8906, -0.0957],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 10.3750, -12.8125,  -5.3750,  ...,   0.7969,   1.1719, -11.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0352, -0.0050,  0.0012,  ...,  0.0080, -0.0066,  0.0079],
        [-0.0099,  0.0276, -0.0033,  ..., -0.0050,  0.0017, -0.0115],
        [-0.0088,  0.0029,  0.1475,  ..., -0.0042,  0.0142, -0.0057],
        ...,
        [ 0.0010, -0.0005, -0.0262,  ...,  0.0249,  0.0073, -0.0068],
        [-0.0041,  0.0010, -0.0102,  ...,  0.0012,  0.0522,  0.0050],
        [-0.0048, -0.0065,  0.0093,  ...,  0.0016,  0.0017,  0.0352]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  9.4453, -12.9453,  -4.9883,  ...,   0.9062,   2.3066, -12.9766]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 23:30:48 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The cherry is colored red
The ant is colored black
The fridge is colored white
The potato is colored brown
The swan is colored white
The sapphire is colored blue
The sea is colored blue
The emerald is colored
2024-07-25 23:30:49 root INFO     [order_1_approx] starting weight calculation for The sapphire is colored blue
The cherry is colored red
The fridge is colored white
The sea is colored blue
The emerald is colored green
The swan is colored white
The ant is colored black
The potato is colored
2024-07-25 23:30:49 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 23:33:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.6406,  0.5547,  2.1875,  ..., -1.4375, -2.3125,  1.3359],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 5.5938, -3.4062, -5.2812,  ..., -5.8125,  2.1250, -5.9375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0250,  0.0028,  0.0161,  ..., -0.0063,  0.0010, -0.0070],
        [-0.0057,  0.0280, -0.0011,  ..., -0.0010,  0.0087,  0.0009],
        [-0.0013, -0.0007,  0.1187,  ...,  0.0077,  0.0076,  0.0040],
        ...,
        [-0.0002, -0.0007, -0.0200,  ...,  0.0215, -0.0028,  0.0013],
        [-0.0013, -0.0010,  0.0143,  ..., -0.0094,  0.0557, -0.0019],
        [-0.0027, -0.0018, -0.0063,  ...,  0.0058, -0.0079,  0.0459]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.0312, -4.2656, -5.8516,  ..., -4.0352,  1.4033, -7.0508]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 23:33:36 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The sapphire is colored blue
The cherry is colored red
The fridge is colored white
The sea is colored blue
The emerald is colored green
The swan is colored white
The ant is colored black
The potato is colored
2024-07-25 23:33:36 root INFO     [order_1_approx] starting weight calculation for The sea is colored blue
The swan is colored white
The cherry is colored red
The fridge is colored white
The potato is colored brown
The ant is colored black
The emerald is colored green
The sapphire is colored
2024-07-25 23:33:36 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 23:36:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1074,  0.5234,  0.4805,  ..., -0.3555, -0.9414, -2.9219],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  0.5586, -10.0000,  -1.1562,  ...,   6.0000,   4.0000, -13.9375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0327,  0.0042, -0.0075,  ...,  0.0034, -0.0028,  0.0056],
        [-0.0031,  0.0243,  0.0085,  ...,  0.0071, -0.0002, -0.0045],
        [ 0.0002,  0.0018,  0.1494,  ...,  0.0019,  0.0072, -0.0036],
        ...,
        [ 0.0054, -0.0044, -0.0248,  ...,  0.0302, -0.0060, -0.0116],
        [ 0.0010,  0.0054,  0.0051,  ..., -0.0013,  0.0493,  0.0062],
        [ 0.0063, -0.0103, -0.0171,  ..., -0.0002,  0.0042,  0.0300]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -0.5645, -10.8359,  -0.8994,  ...,   6.2656,   4.6992, -13.3516]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 23:36:23 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The sea is colored blue
The swan is colored white
The cherry is colored red
The fridge is colored white
The potato is colored brown
The ant is colored black
The emerald is colored green
The sapphire is colored
2024-07-25 23:36:23 root INFO     [order_1_approx] starting weight calculation for The cherry is colored red
The sea is colored blue
The emerald is colored green
The sapphire is colored blue
The potato is colored brown
The ant is colored black
The fridge is colored white
The swan is colored
2024-07-25 23:36:23 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 23:39:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.4922, -1.2188,  0.8398,  ..., -2.2188, -3.0312, -0.9375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 4.4688, -0.2539, -7.3125,  ..., -2.3438, -2.0312, -6.4688],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.6377e-02, -6.5002e-03,  3.7231e-03,  ...,  3.9673e-03,
         -8.7738e-04,  1.1520e-03],
        [-4.1504e-03,  3.1494e-02,  1.2085e-02,  ..., -2.0905e-03,
         -3.1433e-03,  1.2512e-03],
        [-3.7689e-03,  8.0566e-03,  1.4551e-01,  ..., -3.7384e-03,
          3.2654e-03,  7.1106e-03],
        ...,
        [ 2.5635e-03,  5.3711e-03, -1.4404e-02,  ...,  2.9419e-02,
         -4.6692e-03, -5.6763e-03],
        [-1.0864e-02,  6.1951e-03, -2.6489e-02,  ..., -7.6294e-05,
          5.2979e-02,  3.2654e-03],
        [-4.5166e-03, -4.8218e-03,  4.2725e-03,  ..., -4.7684e-05,
          2.4109e-03,  5.3711e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5508, -1.7949, -7.2344,  ..., -2.0039, -2.0762, -9.6953]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 23:39:10 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The cherry is colored red
The sea is colored blue
The emerald is colored green
The sapphire is colored blue
The potato is colored brown
The ant is colored black
The fridge is colored white
The swan is colored
2024-07-25 23:39:10 root INFO     [order_1_approx] starting weight calculation for The swan is colored white
The emerald is colored green
The sapphire is colored blue
The potato is colored brown
The ant is colored black
The sea is colored blue
The cherry is colored red
The fridge is colored
2024-07-25 23:39:10 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 23:41:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5117,  2.5625,  1.0156,  ..., -3.4688, -3.5469, -0.9219],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -7.2188,  -4.3125,  -0.7500,  ...,   4.8438,  -0.7461, -11.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.2959e-02,  1.1292e-03,  8.3618e-03,  ..., -2.9297e-03,
         -2.7618e-03,  2.0409e-04],
        [ 5.4550e-04,  2.9785e-02, -1.2207e-04,  ...,  2.8381e-03,
         -5.3406e-05,  3.2654e-03],
        [ 5.6152e-03,  6.3477e-03,  1.1523e-01,  ..., -6.6528e-03,
         -2.5940e-03, -1.7578e-02],
        ...,
        [-1.7166e-04,  5.4321e-03, -6.1646e-03,  ...,  2.5391e-02,
         -1.3428e-03,  3.4027e-03],
        [-4.4556e-03, -7.9956e-03, -4.0283e-03,  ..., -2.3041e-03,
          5.3467e-02,  4.0588e-03],
        [-1.8463e-03, -3.8452e-03, -1.2512e-02,  ..., -3.1128e-03,
          5.0659e-03,  4.5410e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -6.7812,  -3.9434,  -0.9834,  ...,   5.0781,  -1.3594, -11.3359]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 23:41:57 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The swan is colored white
The emerald is colored green
The sapphire is colored blue
The potato is colored brown
The ant is colored black
The sea is colored blue
The cherry is colored red
The fridge is colored
2024-07-25 23:41:57 root INFO     [order_1_approx] starting weight calculation for The cherry is colored red
The swan is colored white
The emerald is colored green
The sea is colored blue
The fridge is colored white
The sapphire is colored blue
The potato is colored brown
The ant is colored
2024-07-25 23:41:57 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 23:44:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.2656,  1.5312,  5.5312,  ..., -1.1719, -4.2812, -4.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -8.8750,  -4.6250,  -1.2812,  ..., -12.5000,   3.3125,  -0.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0223, -0.0032, -0.0069,  ..., -0.0029, -0.0074,  0.0079],
        [-0.0025,  0.0204, -0.0070,  ..., -0.0027, -0.0008, -0.0026],
        [ 0.0015, -0.0040,  0.1226,  ...,  0.0045, -0.0052,  0.0027],
        ...,
        [ 0.0009,  0.0063, -0.0229,  ...,  0.0205, -0.0042, -0.0020],
        [-0.0019, -0.0050,  0.0047,  ...,  0.0013,  0.0503, -0.0044],
        [-0.0079,  0.0054, -0.0178,  ..., -0.0007, -0.0085,  0.0308]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -9.1875,  -5.0195,  -2.9102,  ..., -10.9219,   2.9727,  -1.2354]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 23:44:44 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The cherry is colored red
The swan is colored white
The emerald is colored green
The sea is colored blue
The fridge is colored white
The sapphire is colored blue
The potato is colored brown
The ant is colored
2024-07-25 23:44:44 root INFO     [order_1_approx] starting weight calculation for The emerald is colored green
The swan is colored white
The sapphire is colored blue
The sea is colored blue
The fridge is colored white
The potato is colored brown
The ant is colored black
The cherry is colored
2024-07-25 23:44:44 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 23:47:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0312, -0.2236,  9.5000,  ...,  0.3711, -1.3281, -0.4590],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 2.1094e-01, -4.0625e+00,  1.2875e+01,  ..., -7.8125e-03,
         5.6562e+00, -6.8125e+00], device='cuda:0', dtype=torch.float16,
       grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0254, -0.0029,  0.0126,  ..., -0.0010, -0.0002,  0.0004],
        [ 0.0029,  0.0203,  0.0032,  ..., -0.0012,  0.0062,  0.0042],
        [ 0.0039,  0.0050,  0.1118,  ..., -0.0025, -0.0051,  0.0044],
        ...,
        [-0.0007,  0.0042, -0.0325,  ...,  0.0193, -0.0017, -0.0007],
        [-0.0059, -0.0006,  0.0060,  ..., -0.0001,  0.0410, -0.0047],
        [ 0.0023,  0.0005,  0.0016,  ...,  0.0002, -0.0052,  0.0349]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0592, -5.5898, 11.4922,  ...,  0.3699,  5.6641, -8.2344]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 23:47:30 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The emerald is colored green
The swan is colored white
The sapphire is colored blue
The sea is colored blue
The fridge is colored white
The potato is colored brown
The ant is colored black
The cherry is colored
2024-07-25 23:47:30 root INFO     total operator prediction time: 1336.9606938362122 seconds
2024-07-25 23:47:30 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on animal - sound
2024-07-25 23:47:30 root INFO     building operator animal - sound
2024-07-25 23:47:31 root INFO     [order_1_approx] starting weight calculation for The sound that a seal makes is called a bark
The sound that a chimpanzee makes is called a scream
The sound that a toad makes is called a ribbit
The sound that a fly makes is called a buzz
The sound that a pig makes is called a oink
The sound that a beetle makes is called a drone
The sound that a monkey makes is called a chatter
The sound that a donkey makes is called a
2024-07-25 23:47:31 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 23:50:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0859, -1.7344,  0.5781,  ...,  2.3906, -3.8125, -2.5312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 2.0938, 12.8125, 23.8750,  ...,  1.4844, -4.6250, -1.3438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0298, -0.0002, -0.0084,  ..., -0.0090,  0.0210, -0.0034],
        [ 0.0047,  0.0123,  0.0171,  ..., -0.0018,  0.0129, -0.0107],
        [-0.0078, -0.0049,  0.1182,  ..., -0.0004,  0.0071,  0.0018],
        ...,
        [ 0.0053,  0.0089, -0.0189,  ...,  0.0236,  0.0075, -0.0045],
        [-0.0071,  0.0023,  0.0001,  ..., -0.0074,  0.0298,  0.0264],
        [-0.0056,  0.0078, -0.0190,  ..., -0.0071,  0.0026,  0.0386]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0508, 13.7266, 23.0781,  ...,  1.1895, -4.3516, -1.7256]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 23:50:17 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a seal makes is called a bark
The sound that a chimpanzee makes is called a scream
The sound that a toad makes is called a ribbit
The sound that a fly makes is called a buzz
The sound that a pig makes is called a oink
The sound that a beetle makes is called a drone
The sound that a monkey makes is called a chatter
The sound that a donkey makes is called a
2024-07-25 23:50:17 root INFO     [order_1_approx] starting weight calculation for The sound that a chimpanzee makes is called a scream
The sound that a beetle makes is called a drone
The sound that a monkey makes is called a chatter
The sound that a fly makes is called a buzz
The sound that a pig makes is called a oink
The sound that a toad makes is called a ribbit
The sound that a donkey makes is called a bray
The sound that a seal makes is called a
2024-07-25 23:50:17 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 23:53:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7500,  0.0547,  1.3906,  ..., -0.6641, -4.2188, -1.7812],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-4.5312,  2.3750, -0.9375,  ..., -0.0811, -5.1875, -3.4062],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.2461e-02, -6.1035e-04, -8.0566e-03,  ..., -4.2419e-03,
          1.3245e-02,  1.2878e-02],
        [ 4.1809e-03,  1.3916e-02,  5.5542e-03,  ...,  4.6692e-03,
          5.1880e-03,  5.0659e-03],
        [-6.5918e-03,  3.6926e-03,  1.1279e-01,  ..., -3.6774e-03,
         -3.3264e-03, -6.0730e-03],
        ...,
        [-1.6022e-03,  6.8359e-03,  4.8828e-04,  ...,  1.5991e-02,
         -3.6316e-03, -5.0049e-03],
        [-5.9509e-03, -5.4321e-03,  1.0681e-03,  ..., -7.0801e-03,
          6.3477e-02,  1.4771e-02],
        [ 6.7139e-03,  5.9204e-03, -2.1515e-03,  ...,  1.6022e-03,
         -4.5776e-05,  4.0039e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.8887,  3.2070, -1.4121,  ...,  0.4692, -6.3984, -5.0508]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 23:53:03 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a chimpanzee makes is called a scream
The sound that a beetle makes is called a drone
The sound that a monkey makes is called a chatter
The sound that a fly makes is called a buzz
The sound that a pig makes is called a oink
The sound that a toad makes is called a ribbit
The sound that a donkey makes is called a bray
The sound that a seal makes is called a
2024-07-25 23:53:03 root INFO     [order_1_approx] starting weight calculation for The sound that a monkey makes is called a chatter
The sound that a toad makes is called a ribbit
The sound that a seal makes is called a bark
The sound that a fly makes is called a buzz
The sound that a chimpanzee makes is called a scream
The sound that a donkey makes is called a bray
The sound that a beetle makes is called a drone
The sound that a pig makes is called a
2024-07-25 23:53:03 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 23:55:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7266, -0.3398, -0.4414,  ...,  1.5469, -2.6562, -0.8984],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  2.6406,  10.8125,  15.8750,  ...,   8.4375, -10.5000,  11.3125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0183, -0.0031,  0.0147,  ..., -0.0077,  0.0083,  0.0065],
        [ 0.0032,  0.0107,  0.0060,  ...,  0.0062,  0.0047, -0.0048],
        [-0.0078, -0.0010,  0.0869,  ..., -0.0061,  0.0093, -0.0094],
        ...,
        [ 0.0022,  0.0034, -0.0042,  ...,  0.0106,  0.0054,  0.0017],
        [-0.0029, -0.0011, -0.0010,  ..., -0.0052,  0.0294,  0.0067],
        [ 0.0006,  0.0033, -0.0026,  ...,  0.0004,  0.0062,  0.0287]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  2.6699,  10.9375,  15.8672,  ...,   7.4648, -10.5078,  10.3203]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 23:55:49 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a monkey makes is called a chatter
The sound that a toad makes is called a ribbit
The sound that a seal makes is called a bark
The sound that a fly makes is called a buzz
The sound that a chimpanzee makes is called a scream
The sound that a donkey makes is called a bray
The sound that a beetle makes is called a drone
The sound that a pig makes is called a
2024-07-25 23:55:50 root INFO     [order_1_approx] starting weight calculation for The sound that a chimpanzee makes is called a scream
The sound that a donkey makes is called a bray
The sound that a seal makes is called a bark
The sound that a monkey makes is called a chatter
The sound that a pig makes is called a oink
The sound that a beetle makes is called a drone
The sound that a fly makes is called a buzz
The sound that a toad makes is called a
2024-07-25 23:55:50 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-25 23:58:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2695,  1.6562,  0.1328,  ...,  0.1133, -0.2988, -1.9766],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-8.8125, 10.3125,  0.4805,  ..., -8.3125, 10.5000,  9.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0261,  0.0002, -0.0051,  ..., -0.0057,  0.0078,  0.0106],
        [ 0.0067,  0.0143,  0.0237,  ...,  0.0037,  0.0107, -0.0029],
        [ 0.0048, -0.0006,  0.1348,  ..., -0.0058,  0.0010, -0.0216],
        ...,
        [-0.0013,  0.0079,  0.0059,  ...,  0.0112,  0.0062, -0.0058],
        [-0.0009, -0.0029,  0.0115,  ..., -0.0079,  0.0571,  0.0032],
        [ 0.0058,  0.0019,  0.0002,  ..., -0.0027, -0.0012,  0.0449]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-7.3906, 11.4766,  1.9863,  ..., -8.3594, 11.0469,  9.3438]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 23:58:36 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a chimpanzee makes is called a scream
The sound that a donkey makes is called a bray
The sound that a seal makes is called a bark
The sound that a monkey makes is called a chatter
The sound that a pig makes is called a oink
The sound that a beetle makes is called a drone
The sound that a fly makes is called a buzz
The sound that a toad makes is called a
2024-07-25 23:58:36 root INFO     [order_1_approx] starting weight calculation for The sound that a beetle makes is called a drone
The sound that a donkey makes is called a bray
The sound that a toad makes is called a ribbit
The sound that a chimpanzee makes is called a scream
The sound that a monkey makes is called a chatter
The sound that a pig makes is called a oink
The sound that a seal makes is called a bark
The sound that a fly makes is called a
2024-07-25 23:58:36 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 00:01:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.4219,  2.4688, -0.0625,  ...,  1.1094, -0.4258, -5.8125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([10.1875,  7.3125, 19.7500,  ..., -1.2734, 21.7500,  0.7812],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0200, -0.0055, -0.0007,  ...,  0.0033,  0.0034,  0.0142],
        [ 0.0020,  0.0229,  0.0114,  ...,  0.0022, -0.0083,  0.0036],
        [ 0.0023,  0.0022,  0.0962,  ..., -0.0023,  0.0014, -0.0020],
        ...,
        [-0.0007,  0.0054,  0.0003,  ...,  0.0164,  0.0029, -0.0060],
        [-0.0024, -0.0043, -0.0052,  ..., -0.0066,  0.0439,  0.0045],
        [-0.0002,  0.0002, -0.0079,  ...,  0.0008,  0.0022,  0.0304]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 8.6328,  7.4102, 18.7188,  ..., -2.1504, 21.8281,  1.4336]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 00:01:23 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a beetle makes is called a drone
The sound that a donkey makes is called a bray
The sound that a toad makes is called a ribbit
The sound that a chimpanzee makes is called a scream
The sound that a monkey makes is called a chatter
The sound that a pig makes is called a oink
The sound that a seal makes is called a bark
The sound that a fly makes is called a
2024-07-26 00:01:23 root INFO     [order_1_approx] starting weight calculation for The sound that a seal makes is called a bark
The sound that a toad makes is called a ribbit
The sound that a donkey makes is called a bray
The sound that a beetle makes is called a drone
The sound that a fly makes is called a buzz
The sound that a pig makes is called a oink
The sound that a monkey makes is called a chatter
The sound that a chimpanzee makes is called a
2024-07-26 00:01:23 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 00:04:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6641,  1.2656, -3.1250,  ...,  0.4375, -1.0156, -1.7109],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([21.1250,  7.5625,  9.1875,  ...,  1.9922, -2.3750,  0.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 5.0659e-03, -1.8234e-03,  1.0681e-03,  ...,  1.0834e-03,
          2.1935e-04,  9.0027e-04],
        [ 4.3297e-04,  4.9744e-03,  2.2888e-03,  ..., -7.6294e-04,
          7.3242e-04,  3.4332e-04],
        [-2.4414e-03,  4.6539e-04,  2.4536e-02,  ..., -1.4496e-04,
          1.6022e-03, -1.2817e-03],
        ...,
        [ 1.4496e-03,  1.9455e-03, -2.2888e-03,  ...,  4.3640e-03,
         -3.8147e-04, -6.4087e-04],
        [ 1.9455e-04,  6.0654e-04, -3.7689e-03,  ..., -5.1498e-04,
          8.9111e-03,  7.1716e-04],
        [-1.2779e-04,  3.7193e-05,  2.1973e-03,  ..., -2.4223e-04,
         -5.3787e-04,  9.2163e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[21.5469,  7.2617,  9.3984,  ...,  1.9023, -2.4004,  0.7769]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 00:04:09 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a seal makes is called a bark
The sound that a toad makes is called a ribbit
The sound that a donkey makes is called a bray
The sound that a beetle makes is called a drone
The sound that a fly makes is called a buzz
The sound that a pig makes is called a oink
The sound that a monkey makes is called a chatter
The sound that a chimpanzee makes is called a
2024-07-26 00:04:09 root INFO     [order_1_approx] starting weight calculation for The sound that a toad makes is called a ribbit
The sound that a monkey makes is called a chatter
The sound that a donkey makes is called a bray
The sound that a fly makes is called a buzz
The sound that a chimpanzee makes is called a scream
The sound that a seal makes is called a bark
The sound that a pig makes is called a oink
The sound that a beetle makes is called a
2024-07-26 00:04:10 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 00:06:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0322,  1.4688, -0.9570,  ..., -0.7188, -0.5586, -3.7188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 4.0312,  6.4375, 20.1250,  ..., -9.8750, 21.8750,  3.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.4048e-02, -7.2479e-05, -9.9182e-04,  ...,  2.8229e-03,
          5.2490e-03,  1.3916e-02],
        [ 5.4016e-03,  2.1484e-02,  7.5073e-03,  ...,  6.2561e-04,
         -1.2665e-03, -1.7853e-03],
        [ 4.3030e-03, -3.9978e-03,  1.2012e-01,  ..., -4.8218e-03,
          4.0588e-03, -1.4496e-03],
        ...,
        [ 3.4790e-03,  6.1035e-03, -1.1963e-02,  ...,  1.6113e-02,
          4.3335e-03,  4.7302e-03],
        [-1.1215e-03, -9.3079e-04,  2.7161e-03,  ..., -5.3711e-03,
          6.3477e-02,  1.7700e-03],
        [-1.8311e-03,  2.5024e-03,  1.9989e-03,  ...,  1.1902e-03,
         -9.2316e-04,  4.0527e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.4062,  7.2266, 19.7344,  ..., -8.9297, 20.4688,  4.0898]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 00:06:54 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a toad makes is called a ribbit
The sound that a monkey makes is called a chatter
The sound that a donkey makes is called a bray
The sound that a fly makes is called a buzz
The sound that a chimpanzee makes is called a scream
The sound that a seal makes is called a bark
The sound that a pig makes is called a oink
The sound that a beetle makes is called a
2024-07-26 00:06:54 root INFO     [order_1_approx] starting weight calculation for The sound that a donkey makes is called a bray
The sound that a seal makes is called a bark
The sound that a chimpanzee makes is called a scream
The sound that a pig makes is called a oink
The sound that a fly makes is called a buzz
The sound that a toad makes is called a ribbit
The sound that a beetle makes is called a drone
The sound that a monkey makes is called a
2024-07-26 00:06:54 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 00:09:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2344, -1.8516,  1.1875,  ..., -0.7422, -1.5469, -4.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([16.8750, 10.3750, 12.0625,  ...,  5.0000,  1.5547, -0.1562],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0206, -0.0003,  0.0005,  ...,  0.0023,  0.0123,  0.0069],
        [ 0.0088,  0.0164,  0.0126,  ...,  0.0016,  0.0047,  0.0025],
        [-0.0075,  0.0043,  0.1250,  ..., -0.0075,  0.0068,  0.0054],
        ...,
        [ 0.0052,  0.0044,  0.0009,  ...,  0.0188, -0.0002,  0.0053],
        [-0.0034,  0.0008,  0.0023,  ..., -0.0083,  0.0498,  0.0021],
        [ 0.0043,  0.0008,  0.0046,  ..., -0.0020, -0.0007,  0.0442]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[17.6875, 10.4375, 12.1328,  ...,  5.1055,  2.4688, -0.8716]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 00:09:40 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a donkey makes is called a bray
The sound that a seal makes is called a bark
The sound that a chimpanzee makes is called a scream
The sound that a pig makes is called a oink
The sound that a fly makes is called a buzz
The sound that a toad makes is called a ribbit
The sound that a beetle makes is called a drone
The sound that a monkey makes is called a
2024-07-26 00:09:40 root INFO     total operator prediction time: 1329.5971472263336 seconds
2024-07-26 00:09:40 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on animal - youth
2024-07-26 00:09:40 root INFO     building operator animal - youth
2024-07-26 00:09:40 root INFO     [order_1_approx] starting weight calculation for The offspring of a chimpanzee is referred to as a baby
The offspring of a ape is referred to as a baby
The offspring of a horse is referred to as a foal
The offspring of a lion is referred to as a cub
The offspring of a salmon is referred to as a smolt
The offspring of a cat is referred to as a kitten
The offspring of a seal is referred to as a pup
The offspring of a duck is referred to as a
2024-07-26 00:09:40 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 00:12:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7344,  1.5781, -2.0781,  ...,  1.2969, -4.7188, -2.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 13.0625, -15.9375,  -3.6875,  ..., -13.6875, -11.8125, -26.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0188, -0.0008, -0.0058,  ...,  0.0046,  0.0059, -0.0019],
        [-0.0027,  0.0182, -0.0203,  ...,  0.0024, -0.0066,  0.0073],
        [ 0.0021,  0.0007,  0.0947,  ..., -0.0017, -0.0020, -0.0019],
        ...,
        [ 0.0050, -0.0020,  0.0075,  ...,  0.0155, -0.0039,  0.0012],
        [-0.0106, -0.0026, -0.0032,  ..., -0.0048,  0.0325,  0.0043],
        [-0.0037, -0.0021,  0.0009,  ..., -0.0060, -0.0133,  0.0286]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 14.0703, -16.6875,  -3.5469,  ..., -12.1406, -11.0000, -27.7656]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 00:12:26 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a chimpanzee is referred to as a baby
The offspring of a ape is referred to as a baby
The offspring of a horse is referred to as a foal
The offspring of a lion is referred to as a cub
The offspring of a salmon is referred to as a smolt
The offspring of a cat is referred to as a kitten
The offspring of a seal is referred to as a pup
The offspring of a duck is referred to as a
2024-07-26 00:12:27 root INFO     [order_1_approx] starting weight calculation for The offspring of a chimpanzee is referred to as a baby
The offspring of a seal is referred to as a pup
The offspring of a cat is referred to as a kitten
The offspring of a horse is referred to as a foal
The offspring of a lion is referred to as a cub
The offspring of a salmon is referred to as a smolt
The offspring of a duck is referred to as a duckling
The offspring of a ape is referred to as a
2024-07-26 00:12:27 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 00:15:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.1562, -1.7344, -1.2344,  ..., -0.5312, -0.8203, -2.9062],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 11.1875, -11.7500,  -1.7656,  ...,   3.7188,   8.9375,  -5.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0182, -0.0060,  0.0014,  ...,  0.0063,  0.0049,  0.0102],
        [ 0.0029,  0.0181, -0.0029,  ...,  0.0026,  0.0030,  0.0112],
        [ 0.0010,  0.0032,  0.1074,  ..., -0.0018,  0.0047, -0.0007],
        ...,
        [ 0.0115,  0.0045, -0.0086,  ...,  0.0161, -0.0107,  0.0034],
        [-0.0109, -0.0011,  0.0142,  ..., -0.0052,  0.0322,  0.0029],
        [ 0.0068, -0.0061, -0.0031,  ..., -0.0011, -0.0143,  0.0425]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 11.3828, -10.5703,  -2.2695,  ...,   2.9336,   8.6250,  -7.3086]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 00:15:14 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a chimpanzee is referred to as a baby
The offspring of a seal is referred to as a pup
The offspring of a cat is referred to as a kitten
The offspring of a horse is referred to as a foal
The offspring of a lion is referred to as a cub
The offspring of a salmon is referred to as a smolt
The offspring of a duck is referred to as a duckling
The offspring of a ape is referred to as a
2024-07-26 00:15:15 root INFO     [order_1_approx] starting weight calculation for The offspring of a lion is referred to as a cub
The offspring of a duck is referred to as a duckling
The offspring of a horse is referred to as a foal
The offspring of a salmon is referred to as a smolt
The offspring of a cat is referred to as a kitten
The offspring of a seal is referred to as a pup
The offspring of a ape is referred to as a baby
The offspring of a chimpanzee is referred to as a
2024-07-26 00:15:15 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 00:18:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.0312,  3.0000, -6.1562,  ...,  0.4688,  0.2969,  0.3125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 19.6250, -13.4375,  -6.9375,  ...,   1.5625,  -1.1094,  -4.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 4.0283e-03, -1.1520e-03, -1.1902e-03,  ...,  3.9864e-04,
          4.1199e-04,  1.4572e-03],
        [-5.3787e-04,  4.2725e-03,  1.0147e-03,  ..., -1.8387e-03,
         -2.7084e-04,  1.0071e-03],
        [-9.0790e-04,  1.6556e-03,  1.4465e-02,  ..., -4.2725e-04,
         -5.1498e-05, -2.0905e-03],
        ...,
        [ 1.0910e-03,  2.1667e-03, -1.6632e-03,  ...,  4.4250e-03,
         -8.0872e-04,  8.6594e-04],
        [-5.7220e-04,  9.1171e-04,  2.6550e-03,  ..., -9.7656e-04,
          5.3711e-03, -1.7090e-03],
        [-7.8201e-04,  2.8229e-04, -3.8910e-04,  ..., -2.1057e-03,
         -9.7275e-04,  6.4697e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 19.3750, -13.4922,  -7.1602,  ...,   1.4521,  -1.5625,  -3.8945]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 00:18:02 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a lion is referred to as a cub
The offspring of a duck is referred to as a duckling
The offspring of a horse is referred to as a foal
The offspring of a salmon is referred to as a smolt
The offspring of a cat is referred to as a kitten
The offspring of a seal is referred to as a pup
The offspring of a ape is referred to as a baby
The offspring of a chimpanzee is referred to as a
2024-07-26 00:18:02 root INFO     [order_1_approx] starting weight calculation for The offspring of a salmon is referred to as a smolt
The offspring of a chimpanzee is referred to as a baby
The offspring of a ape is referred to as a baby
The offspring of a seal is referred to as a pup
The offspring of a cat is referred to as a kitten
The offspring of a lion is referred to as a cub
The offspring of a duck is referred to as a duckling
The offspring of a horse is referred to as a
2024-07-26 00:18:03 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 00:20:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.7812, -0.5195, -2.4688,  ...,  0.6484, -3.0156, -2.5156],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 6.6562,  4.1875, -5.8750,  ...,  2.6719, -8.0625,  6.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.1475e-02, -2.5330e-03,  4.3945e-03,  ...,  1.5259e-05,
         -1.0681e-04, -6.0272e-04],
        [ 2.2430e-03,  1.0559e-02, -1.7395e-03,  ...,  2.0142e-03,
         -2.9907e-03,  3.6469e-03],
        [ 3.7994e-03, -3.0975e-03,  6.0547e-02,  ..., -1.1749e-03,
         -2.1667e-03, -5.7983e-04],
        ...,
        [ 2.4719e-03,  1.7700e-03, -3.4332e-03,  ...,  1.3123e-02,
         -5.6839e-04,  1.5869e-03],
        [ 6.1035e-05,  4.6730e-04,  1.3550e-02,  ..., -2.9144e-03,
          1.5259e-02,  2.6550e-03],
        [-1.5182e-03,  1.1292e-03, -3.9368e-03,  ..., -1.6022e-03,
         -5.9204e-03,  2.6367e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 7.0078,  3.2461, -6.2148,  ...,  2.8066, -8.5781,  5.3086]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 00:20:50 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a salmon is referred to as a smolt
The offspring of a chimpanzee is referred to as a baby
The offspring of a ape is referred to as a baby
The offspring of a seal is referred to as a pup
The offspring of a cat is referred to as a kitten
The offspring of a lion is referred to as a cub
The offspring of a duck is referred to as a duckling
The offspring of a horse is referred to as a
2024-07-26 00:20:51 root INFO     [order_1_approx] starting weight calculation for The offspring of a duck is referred to as a duckling
The offspring of a seal is referred to as a pup
The offspring of a chimpanzee is referred to as a baby
The offspring of a lion is referred to as a cub
The offspring of a salmon is referred to as a smolt
The offspring of a horse is referred to as a foal
The offspring of a ape is referred to as a baby
The offspring of a cat is referred to as a
2024-07-26 00:20:51 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 00:23:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.4531,  2.2500, -0.7578,  ..., -0.2500,  0.2695, -4.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 11.0625,   2.6250,   2.5312,  ...,  -1.7031,  -1.7188, -12.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.5076e-02, -1.1444e-03, -6.1646e-03,  ...,  5.2795e-03,
         -1.8768e-03, -3.8757e-03],
        [-9.1553e-05,  9.0332e-03,  1.3428e-03,  ...,  6.7520e-04,
         -2.2602e-04,  1.7166e-03],
        [-7.9727e-04,  2.0447e-03,  5.9570e-02,  ..., -8.7738e-04,
          1.9455e-03, -6.1646e-03],
        ...,
        [ 4.6387e-03,  2.7161e-03, -2.6550e-03,  ...,  1.2085e-02,
         -2.3499e-03, -3.6926e-03],
        [-3.3875e-03, -1.9684e-03,  9.2773e-03,  ..., -4.2419e-03,
          2.4658e-02, -5.0964e-03],
        [ 0.0000e+00, -1.5411e-03,  2.2125e-03,  ...,  1.2054e-03,
          8.0872e-04,  2.1118e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 11.5078,   2.9746,   1.9014,  ...,  -2.3184,  -0.5518, -13.3672]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 00:23:39 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a duck is referred to as a duckling
The offspring of a seal is referred to as a pup
The offspring of a chimpanzee is referred to as a baby
The offspring of a lion is referred to as a cub
The offspring of a salmon is referred to as a smolt
The offspring of a horse is referred to as a foal
The offspring of a ape is referred to as a baby
The offspring of a cat is referred to as a
2024-07-26 00:23:39 root INFO     [order_1_approx] starting weight calculation for The offspring of a cat is referred to as a kitten
The offspring of a seal is referred to as a pup
The offspring of a ape is referred to as a baby
The offspring of a lion is referred to as a cub
The offspring of a horse is referred to as a foal
The offspring of a chimpanzee is referred to as a baby
The offspring of a duck is referred to as a duckling
The offspring of a salmon is referred to as a
2024-07-26 00:23:39 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 00:26:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 3.4531,  2.3594, -0.2969,  ...,  0.1562, -3.2969,  1.4531],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 14.0000, -18.5000, -10.5000,  ...,  -2.5000,  -3.7188, -15.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.7100e-02,  1.1230e-02,  1.6357e-02,  ..., -4.4556e-03,
          9.3079e-04, -1.0300e-04],
        [ 2.0027e-04,  9.1553e-03, -2.8442e-02,  ...,  1.0757e-03,
         -4.0894e-03,  2.9297e-03],
        [-4.4632e-04, -2.1362e-03,  8.3496e-02,  ...,  2.0447e-03,
         -4.5776e-03, -4.6387e-03],
        ...,
        [ 3.9673e-03,  9.9182e-05,  6.2256e-03,  ...,  1.9165e-02,
         -8.6060e-03,  9.0332e-03],
        [-4.3945e-03,  3.3569e-04,  9.4604e-03,  ..., -1.0925e-02,
          4.5410e-02, -6.2866e-03],
        [-3.2501e-03, -3.2654e-03, -9.2163e-03,  ...,  3.9673e-03,
          4.2419e-03,  3.4180e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 14.5312, -17.7969, -10.8281,  ...,  -2.1523,  -4.4453, -18.1250]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 00:26:25 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a cat is referred to as a kitten
The offspring of a seal is referred to as a pup
The offspring of a ape is referred to as a baby
The offspring of a lion is referred to as a cub
The offspring of a horse is referred to as a foal
The offspring of a chimpanzee is referred to as a baby
The offspring of a duck is referred to as a duckling
The offspring of a salmon is referred to as a
2024-07-26 00:26:26 root INFO     [order_1_approx] starting weight calculation for The offspring of a chimpanzee is referred to as a baby
The offspring of a lion is referred to as a cub
The offspring of a duck is referred to as a duckling
The offspring of a salmon is referred to as a smolt
The offspring of a cat is referred to as a kitten
The offspring of a horse is referred to as a foal
The offspring of a ape is referred to as a baby
The offspring of a seal is referred to as a
2024-07-26 00:26:26 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 00:29:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.4219,  1.2188, -0.2109,  ..., -1.0625, -3.5625, -1.8594],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 8.6250, -6.0000, -3.7656,  ..., -7.2500, -8.7500, -7.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0253,  0.0088, -0.0264,  ...,  0.0004, -0.0016,  0.0007],
        [ 0.0118,  0.0150, -0.0386,  ...,  0.0050, -0.0065,  0.0067],
        [-0.0020, -0.0038,  0.0938,  ...,  0.0016,  0.0052, -0.0045],
        ...,
        [-0.0056,  0.0085,  0.0153,  ...,  0.0187, -0.0077,  0.0066],
        [-0.0125,  0.0016,  0.0024,  ..., -0.0040,  0.0420, -0.0082],
        [-0.0048, -0.0019, -0.0186,  ..., -0.0005, -0.0036,  0.0425]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[10.0625, -6.6094, -3.4707,  ..., -6.8516, -7.6523, -9.3125]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 00:29:13 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a chimpanzee is referred to as a baby
The offspring of a lion is referred to as a cub
The offspring of a duck is referred to as a duckling
The offspring of a salmon is referred to as a smolt
The offspring of a cat is referred to as a kitten
The offspring of a horse is referred to as a foal
The offspring of a ape is referred to as a baby
The offspring of a seal is referred to as a
2024-07-26 00:29:14 root INFO     [order_1_approx] starting weight calculation for The offspring of a ape is referred to as a baby
The offspring of a seal is referred to as a pup
The offspring of a horse is referred to as a foal
The offspring of a duck is referred to as a duckling
The offspring of a chimpanzee is referred to as a baby
The offspring of a cat is referred to as a kitten
The offspring of a salmon is referred to as a smolt
The offspring of a lion is referred to as a
2024-07-26 00:29:14 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 00:32:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.4219,  2.0312, -4.1875,  ..., -1.0156, -0.0059, -1.7344],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  4.5000,  -9.7500,   5.3125,  ..., -12.6250,  -2.4375,  -4.1562],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0173,  0.0007,  0.0027,  ...,  0.0026, -0.0003,  0.0007],
        [ 0.0007,  0.0091, -0.0085,  ..., -0.0002, -0.0106,  0.0080],
        [-0.0011,  0.0027,  0.0767,  ...,  0.0002,  0.0030, -0.0007],
        ...,
        [ 0.0062, -0.0007,  0.0042,  ...,  0.0142, -0.0071, -0.0006],
        [-0.0024, -0.0004,  0.0234,  ..., -0.0092,  0.0317, -0.0068],
        [ 0.0001, -0.0001, -0.0128,  ...,  0.0035, -0.0015,  0.0325]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  5.9844, -10.0781,   5.9258,  ..., -11.4844,  -1.6953,  -5.5938]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 00:32:01 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a ape is referred to as a baby
The offspring of a seal is referred to as a pup
The offspring of a horse is referred to as a foal
The offspring of a duck is referred to as a duckling
The offspring of a chimpanzee is referred to as a baby
The offspring of a cat is referred to as a kitten
The offspring of a salmon is referred to as a smolt
The offspring of a lion is referred to as a
2024-07-26 00:32:01 root INFO     total operator prediction time: 1341.1415762901306 seconds
2024-07-26 00:32:01 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on meronyms - part
2024-07-26 00:32:01 root INFO     building operator meronyms - part
2024-07-26 00:32:01 root INFO     [order_1_approx] starting weight calculation for A part of a radio is a receiver
A part of a torso is a chest
A part of a shilling is a pence
A part of a gun is a trigger
A part of a sword is a blade
A part of a seafront is a harbor
A part of a academia is a college
A part of a tonne is a
2024-07-26 00:32:02 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 00:34:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.6719, -2.4062, -4.0938,  ..., -1.8438, -1.1797, -0.5195],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([24.1250, -1.9844, -4.4062,  ..., -3.6094,  0.4297, -7.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0308,  0.0034,  0.0222,  ..., -0.0050,  0.0096, -0.0023],
        [ 0.0020,  0.0101,  0.0033,  ...,  0.0034, -0.0095,  0.0030],
        [ 0.0062,  0.0011,  0.1230,  ..., -0.0095,  0.0084,  0.0065],
        ...,
        [ 0.0026, -0.0003, -0.0114,  ...,  0.0261,  0.0076, -0.0011],
        [-0.0043,  0.0012,  0.0288,  ..., -0.0065,  0.0449, -0.0034],
        [-0.0023, -0.0061, -0.0114,  ...,  0.0002, -0.0095,  0.0410]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[21.3438, -2.5801, -5.0234,  ..., -1.7969,  1.0127, -9.2031]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 00:34:50 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A part of a radio is a receiver
A part of a torso is a chest
A part of a shilling is a pence
A part of a gun is a trigger
A part of a sword is a blade
A part of a seafront is a harbor
A part of a academia is a college
A part of a tonne is a
2024-07-26 00:34:50 root INFO     [order_1_approx] starting weight calculation for A part of a torso is a chest
A part of a seafront is a harbor
A part of a tonne is a kilogram
A part of a gun is a trigger
A part of a sword is a blade
A part of a academia is a college
A part of a shilling is a pence
A part of a radio is a
2024-07-26 00:34:50 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 00:37:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6875,  1.7969, -0.4629,  ..., -1.4297, -0.3359, -3.9531],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-6.2188, -3.4688,  8.4375,  ..., -8.6250,  4.9375, -4.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0259,  0.0030,  0.0227,  ...,  0.0017,  0.0133,  0.0069],
        [ 0.0018,  0.0149, -0.0178,  ..., -0.0005,  0.0014,  0.0010],
        [ 0.0039, -0.0051,  0.1094,  ..., -0.0003, -0.0066,  0.0033],
        ...,
        [ 0.0003, -0.0019, -0.0124,  ...,  0.0222,  0.0078,  0.0087],
        [ 0.0033,  0.0005,  0.0078,  ..., -0.0081,  0.0459, -0.0161],
        [-0.0002, -0.0009,  0.0119,  ...,  0.0006,  0.0076,  0.0596]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-6.1406, -3.5098,  8.2656,  ..., -7.7031,  4.6367, -4.4844]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 00:37:38 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A part of a torso is a chest
A part of a seafront is a harbor
A part of a tonne is a kilogram
A part of a gun is a trigger
A part of a sword is a blade
A part of a academia is a college
A part of a shilling is a pence
A part of a radio is a
2024-07-26 00:37:39 root INFO     [order_1_approx] starting weight calculation for A part of a gun is a trigger
A part of a seafront is a harbor
A part of a sword is a blade
A part of a radio is a receiver
A part of a shilling is a pence
A part of a tonne is a kilogram
A part of a academia is a college
A part of a torso is a
2024-07-26 00:37:39 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 00:40:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4492,  0.6016, -1.2969,  ..., -3.2344, -1.1406, -0.8438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -6.8750,   2.8125,  21.7500,  ..., -13.0000, -10.1250,   7.9062],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.6367e-02,  1.7853e-03,  3.3569e-03,  ...,  4.8828e-04,
          6.3782e-03, -3.9368e-03],
        [ 1.1353e-02,  2.0264e-02,  9.1553e-04,  ..., -1.8005e-03,
          7.8735e-03,  1.5450e-04],
        [ 4.2725e-03, -2.5635e-03,  1.2598e-01,  ..., -4.5166e-03,
          1.0437e-02, -2.6245e-03],
        ...,
        [-5.2795e-03,  6.7139e-03, -1.1841e-02,  ...,  2.3438e-02,
          1.2894e-03,  1.3550e-02],
        [-9.2773e-03,  4.5776e-03, -8.6060e-03,  ...,  5.7220e-05,
          5.9326e-02,  1.4038e-02],
        [ 2.2736e-03,  1.0986e-03,  1.7334e-02,  ...,  3.0670e-03,
         -3.7689e-03,  4.9072e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -5.8750,   3.8008,  20.1719,  ..., -11.2734, -10.3047,   7.1562]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 00:40:27 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A part of a gun is a trigger
A part of a seafront is a harbor
A part of a sword is a blade
A part of a radio is a receiver
A part of a shilling is a pence
A part of a tonne is a kilogram
A part of a academia is a college
A part of a torso is a
2024-07-26 00:40:28 root INFO     [order_1_approx] starting weight calculation for A part of a sword is a blade
A part of a seafront is a harbor
A part of a gun is a trigger
A part of a radio is a receiver
A part of a torso is a chest
A part of a academia is a college
A part of a tonne is a kilogram
A part of a shilling is a
2024-07-26 00:40:28 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 00:43:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4688,  0.7500, -3.9531,  ..., -0.2500,  0.4688, -1.5312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([15.8125,  1.5703,  3.9688,  ..., -6.6875, -6.0938, -5.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0325,  0.0003,  0.0376,  ..., -0.0125,  0.0085, -0.0002],
        [ 0.0033,  0.0007, -0.0048,  ...,  0.0065, -0.0156, -0.0047],
        [ 0.0122, -0.0039,  0.1040,  ..., -0.0035, -0.0041,  0.0053],
        ...,
        [-0.0034,  0.0035,  0.0052,  ...,  0.0154,  0.0023,  0.0058],
        [ 0.0063,  0.0064, -0.0181,  ..., -0.0100,  0.0361,  0.0095],
        [-0.0055,  0.0015, -0.0116,  ...,  0.0037,  0.0035,  0.0376]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[14.8438,  1.8291,  2.4219,  ..., -6.1836, -5.4180, -6.4102]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 00:43:15 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A part of a sword is a blade
A part of a seafront is a harbor
A part of a gun is a trigger
A part of a radio is a receiver
A part of a torso is a chest
A part of a academia is a college
A part of a tonne is a kilogram
A part of a shilling is a
2024-07-26 00:43:16 root INFO     [order_1_approx] starting weight calculation for A part of a sword is a blade
A part of a seafront is a harbor
A part of a gun is a trigger
A part of a shilling is a pence
A part of a torso is a chest
A part of a radio is a receiver
A part of a tonne is a kilogram
A part of a academia is a
2024-07-26 00:43:16 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 00:46:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9570,  1.9375,  0.1172,  ..., -0.3535, -1.7422, -2.5312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 0.3281, -3.6250, 12.1250,  ...,  1.0156,  1.4844, -3.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 4.5410e-02,  4.4250e-03,  1.1902e-02,  ..., -4.0817e-04,
         -2.6398e-03, -3.4790e-03],
        [ 3.6774e-03,  3.4180e-02,  2.0996e-02,  ..., -6.8970e-03,
         -7.3242e-03,  2.1210e-03],
        [-1.7822e-02, -5.9204e-03,  1.8457e-01,  ...,  1.2207e-04,
         -4.0588e-03,  8.1787e-03],
        ...,
        [-2.7771e-03, -3.8147e-04, -2.3315e-02,  ...,  4.1260e-02,
          1.1841e-02,  4.1809e-03],
        [ 1.7548e-03,  9.1553e-04, -6.7444e-03,  ...,  3.1281e-03,
          7.3242e-02, -1.1902e-02],
        [ 5.4321e-03, -1.1444e-03,  1.4771e-02,  ..., -4.6082e-03,
         -3.1738e-03,  5.3467e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7910, -4.1953, 10.6719,  ...,  1.6338,  1.4521, -4.4766]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 00:46:04 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A part of a sword is a blade
A part of a seafront is a harbor
A part of a gun is a trigger
A part of a shilling is a pence
A part of a torso is a chest
A part of a radio is a receiver
A part of a tonne is a kilogram
A part of a academia is a
2024-07-26 00:46:04 root INFO     [order_1_approx] starting weight calculation for A part of a shilling is a pence
A part of a torso is a chest
A part of a radio is a receiver
A part of a seafront is a harbor
A part of a tonne is a kilogram
A part of a academia is a college
A part of a gun is a trigger
A part of a sword is a
2024-07-26 00:46:04 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 00:48:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2109,  0.5820, -5.3438,  ..., -2.0625, -2.8750, -2.5781],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 1.2500,  8.6875,  3.3750,  ..., 11.2500, -1.9297, -5.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0293,  0.0016,  0.0330,  ..., -0.0026,  0.0098, -0.0076],
        [ 0.0048,  0.0156,  0.0009,  ..., -0.0003, -0.0034,  0.0066],
        [-0.0095, -0.0013,  0.1250,  ..., -0.0144,  0.0007, -0.0052],
        ...,
        [-0.0033,  0.0044, -0.0115,  ...,  0.0293, -0.0018,  0.0092],
        [-0.0075,  0.0071, -0.0010,  ..., -0.0006,  0.0366,  0.0031],
        [ 0.0056, -0.0039,  0.0055,  ...,  0.0005, -0.0059,  0.0522]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0537,  8.4219,  3.9941,  ..., 10.9062, -0.0547, -4.8555]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 00:48:52 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A part of a shilling is a pence
A part of a torso is a chest
A part of a radio is a receiver
A part of a seafront is a harbor
A part of a tonne is a kilogram
A part of a academia is a college
A part of a gun is a trigger
A part of a sword is a
2024-07-26 00:48:52 root INFO     [order_1_approx] starting weight calculation for A part of a radio is a receiver
A part of a academia is a college
A part of a sword is a blade
A part of a shilling is a pence
A part of a seafront is a harbor
A part of a tonne is a kilogram
A part of a torso is a chest
A part of a gun is a
2024-07-26 00:48:53 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 00:51:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0625,  1.9531, -2.0156,  ..., -0.9297, -2.3438, -2.7031],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 3.7812,  4.4688,  5.3125,  ..., -5.3750,  4.8750, -3.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0273,  0.0021,  0.0259,  ..., -0.0026,  0.0121, -0.0014],
        [ 0.0005,  0.0165, -0.0070,  ...,  0.0004, -0.0073,  0.0035],
        [ 0.0005, -0.0040,  0.1289,  ..., -0.0081,  0.0081,  0.0010],
        ...,
        [-0.0014,  0.0016, -0.0068,  ...,  0.0227,  0.0010,  0.0042],
        [-0.0027,  0.0022,  0.0035,  ..., -0.0068,  0.0400,  0.0011],
        [ 0.0149,  0.0008,  0.0065,  ...,  0.0002, -0.0024,  0.0388]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4277,  3.9707,  5.3047,  ..., -3.4629,  4.5938, -4.8906]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 00:51:41 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A part of a radio is a receiver
A part of a academia is a college
A part of a sword is a blade
A part of a shilling is a pence
A part of a seafront is a harbor
A part of a tonne is a kilogram
A part of a torso is a chest
A part of a gun is a
2024-07-26 00:51:41 root INFO     [order_1_approx] starting weight calculation for A part of a gun is a trigger
A part of a sword is a blade
A part of a radio is a receiver
A part of a academia is a college
A part of a tonne is a kilogram
A part of a torso is a chest
A part of a shilling is a pence
A part of a seafront is a
2024-07-26 00:51:41 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 00:54:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.9219,  0.7266,  4.4062,  ..., -4.6875, -2.7812, -0.8516],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 15.1250,  -6.6875,  10.8750,  ...,  -2.6562, -11.6875,   0.3965],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0332, -0.0040,  0.0083,  ..., -0.0054,  0.0022,  0.0004],
        [ 0.0038,  0.0166,  0.0067,  ...,  0.0018, -0.0031, -0.0082],
        [ 0.0016, -0.0036,  0.1582,  ..., -0.0026, -0.0020,  0.0040],
        ...,
        [ 0.0014,  0.0042,  0.0014,  ...,  0.0317,  0.0006,  0.0051],
        [-0.0112,  0.0012,  0.0092,  ..., -0.0066,  0.0693, -0.0067],
        [-0.0020, -0.0107, -0.0327,  ..., -0.0133, -0.0052,  0.0635]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 13.6797,  -6.4297,  10.7578,  ...,  -2.4922, -10.6016,   0.3965]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 00:54:29 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A part of a gun is a trigger
A part of a sword is a blade
A part of a radio is a receiver
A part of a academia is a college
A part of a tonne is a kilogram
A part of a torso is a chest
A part of a shilling is a pence
A part of a seafront is a
2024-07-26 00:54:29 root INFO     total operator prediction time: 1348.158685207367 seconds
2024-07-26 00:54:29 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on synonyms - exact
2024-07-26 00:54:29 root INFO     building operator synonyms - exact
2024-07-26 00:54:30 root INFO     [order_1_approx] starting weight calculation for Another word for reasonable is sensible
Another word for sweets is confectionery
Another word for rock is stone
Another word for identical is same
Another word for snake is serpent
Another word for lad is chap
Another word for murder is slaying
Another word for homogeneous is
2024-07-26 00:54:30 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 00:57:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2969, -2.0312, -2.9062,  ..., -4.4688, -1.3047,  1.9062],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 15.3750,   6.6250, -10.8750,  ...,   4.0938,  -4.7188,  -0.5469],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0493,  0.0034,  0.0042,  ...,  0.0126,  0.0081,  0.0015],
        [ 0.0009,  0.0259, -0.0010,  ..., -0.0068,  0.0026,  0.0182],
        [-0.0011, -0.0019,  0.1523,  ..., -0.0107,  0.0089,  0.0004],
        ...,
        [-0.0043, -0.0030, -0.0267,  ...,  0.0464,  0.0117, -0.0037],
        [ 0.0027,  0.0052,  0.0019,  ...,  0.0142,  0.0854,  0.0093],
        [ 0.0043, -0.0068, -0.0059,  ...,  0.0020,  0.0050,  0.0674]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 12.1484,  10.0156, -10.9844,  ...,   4.1758,  -2.7266,  -2.7031]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 00:57:16 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Another word for reasonable is sensible
Another word for sweets is confectionery
Another word for rock is stone
Another word for identical is same
Another word for snake is serpent
Another word for lad is chap
Another word for murder is slaying
Another word for homogeneous is
2024-07-26 00:57:16 root INFO     [order_1_approx] starting weight calculation for Another word for lad is chap
Another word for identical is same
Another word for sweets is confectionery
Another word for homogeneous is uniform
Another word for reasonable is sensible
Another word for snake is serpent
Another word for murder is slaying
Another word for rock is
2024-07-26 00:57:16 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 01:00:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.8750, -1.6094, -3.1875,  ..., -4.4688, -1.0547, -1.4531],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 7.4375, -1.9297, 14.7500,  ...,  8.0000,  6.2500, -3.9375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0308,  0.0073,  0.0100,  ...,  0.0058, -0.0057,  0.0046],
        [-0.0002,  0.0280,  0.0048,  ..., -0.0031, -0.0069,  0.0260],
        [ 0.0030, -0.0094,  0.1465,  ..., -0.0026, -0.0052,  0.0086],
        ...,
        [ 0.0022,  0.0044, -0.0154,  ...,  0.0212, -0.0096, -0.0029],
        [-0.0034,  0.0021, -0.0009,  ..., -0.0046,  0.0459, -0.0186],
        [ 0.0133, -0.0089,  0.0192,  ..., -0.0040, -0.0015,  0.0498]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 8.1953e+00, -3.9062e-03,  1.2250e+01,  ...,  8.4609e+00,
          4.5977e+00, -5.5781e+00]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SubBackward0>) 

                    
2024-07-26 01:00:02 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Another word for lad is chap
Another word for identical is same
Another word for sweets is confectionery
Another word for homogeneous is uniform
Another word for reasonable is sensible
Another word for snake is serpent
Another word for murder is slaying
Another word for rock is
2024-07-26 01:00:02 root INFO     [order_1_approx] starting weight calculation for Another word for identical is same
Another word for murder is slaying
Another word for homogeneous is uniform
Another word for rock is stone
Another word for reasonable is sensible
Another word for sweets is confectionery
Another word for snake is serpent
Another word for lad is
2024-07-26 01:00:03 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 01:02:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4727,  0.8164, -2.0938,  ..., -1.8047, -1.9844, -1.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([10.4375, -0.0547, -4.9688,  ..., 10.0625,  4.3750, -7.5312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 4.9805e-02,  1.3672e-02,  1.1108e-02,  ..., -7.8125e-03,
          9.7656e-03, -8.8501e-03],
        [ 2.2583e-03,  3.8574e-02,  1.4282e-02,  ...,  8.0566e-03,
          8.5449e-03, -1.0742e-02],
        [-6.2256e-03, -6.1035e-05,  1.8555e-01,  ..., -1.3367e-02,
          2.6855e-02, -1.1841e-02],
        ...,
        [ 1.0559e-02, -5.8594e-03,  1.3062e-02,  ...,  3.0518e-02,
          2.7618e-03,  1.5259e-02],
        [-1.6602e-02,  8.3618e-03,  1.8311e-02,  ..., -6.5613e-03,
          7.5195e-02, -6.7139e-03],
        [-2.5330e-03, -1.5747e-02, -1.0559e-02,  ...,  1.6632e-03,
          0.0000e+00,  6.9824e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[10.8828,  2.3574, -5.3086,  ..., 11.0469,  5.0195, -7.2852]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 01:02:49 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Another word for identical is same
Another word for murder is slaying
Another word for homogeneous is uniform
Another word for rock is stone
Another word for reasonable is sensible
Another word for sweets is confectionery
Another word for snake is serpent
Another word for lad is
2024-07-26 01:02:50 root INFO     [order_1_approx] starting weight calculation for Another word for snake is serpent
Another word for murder is slaying
Another word for rock is stone
Another word for homogeneous is uniform
Another word for identical is same
Another word for lad is chap
Another word for sweets is confectionery
Another word for reasonable is
2024-07-26 01:02:50 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 01:05:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7031, -1.2656, -0.6562,  ..., -2.5312, -1.3672,  0.3906],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 6.4375,  4.5625, -6.3750,  ...,  9.8750, 10.6875, 15.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0601, -0.0076, -0.0012,  ...,  0.0041, -0.0021,  0.0053],
        [-0.0061,  0.0366,  0.0070,  ..., -0.0104,  0.0074,  0.0128],
        [-0.0064, -0.0050,  0.1455,  ...,  0.0002,  0.0140, -0.0005],
        ...,
        [-0.0053,  0.0010, -0.0056,  ...,  0.0339,  0.0192,  0.0129],
        [-0.0048,  0.0052,  0.0023,  ...,  0.0060,  0.0854,  0.0098],
        [ 0.0121,  0.0034, -0.0003,  ..., -0.0117,  0.0059,  0.0698]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.2812,  5.7539, -5.7773,  ..., 11.2344, 14.7812, 17.1250]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 01:05:33 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Another word for snake is serpent
Another word for murder is slaying
Another word for rock is stone
Another word for homogeneous is uniform
Another word for identical is same
Another word for lad is chap
Another word for sweets is confectionery
Another word for reasonable is
2024-07-26 01:05:34 root INFO     [order_1_approx] starting weight calculation for Another word for homogeneous is uniform
Another word for snake is serpent
Another word for sweets is confectionery
Another word for murder is slaying
Another word for reasonable is sensible
Another word for lad is chap
Another word for rock is stone
Another word for identical is
2024-07-26 01:05:34 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 01:08:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.5938, -3.0625,  1.2266,  ..., -3.1562,  1.3203,  1.4062],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  1.0156,  -5.6875,   0.8125,  ...,  -0.8984,  -5.3125, -10.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0206,  0.0016,  0.0016,  ..., -0.0048,  0.0114,  0.0084],
        [-0.0092,  0.0200, -0.0322,  ...,  0.0008,  0.0023,  0.0006],
        [-0.0079, -0.0063,  0.0898,  ..., -0.0059,  0.0034, -0.0072],
        ...,
        [-0.0025, -0.0078, -0.0156,  ...,  0.0271,  0.0170,  0.0105],
        [-0.0025,  0.0059, -0.0014,  ...,  0.0023,  0.0527,  0.0131],
        [ 0.0024, -0.0078,  0.0015,  ..., -0.0045,  0.0033,  0.0425]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -0.4482,  -5.1484,   1.1055,  ...,   2.3418,  -4.7578, -10.9609]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 01:08:21 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Another word for homogeneous is uniform
Another word for snake is serpent
Another word for sweets is confectionery
Another word for murder is slaying
Another word for reasonable is sensible
Another word for lad is chap
Another word for rock is stone
Another word for identical is
2024-07-26 01:08:21 root INFO     [order_1_approx] starting weight calculation for Another word for lad is chap
Another word for murder is slaying
Another word for reasonable is sensible
Another word for identical is same
Another word for rock is stone
Another word for homogeneous is uniform
Another word for snake is serpent
Another word for sweets is
2024-07-26 01:08:21 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 01:11:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6875, -3.1562, -3.2500,  ..., -3.8906, -2.5000, -2.0469],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  2.1094,  -3.3438,   1.1406,  ...,   2.6094,   7.7812, -20.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0422,  0.0040,  0.0212,  ...,  0.0020,  0.0038,  0.0092],
        [-0.0040,  0.0332,  0.0182,  ..., -0.0072,  0.0067,  0.0101],
        [-0.0027,  0.0020,  0.1201,  ..., -0.0036, -0.0042,  0.0070],
        ...,
        [ 0.0039,  0.0036,  0.0091,  ...,  0.0283, -0.0090, -0.0028],
        [-0.0037,  0.0068,  0.0054,  ...,  0.0023,  0.0508, -0.0010],
        [-0.0004, -0.0092, -0.0135,  ..., -0.0063,  0.0117,  0.0449]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  1.8721,  -3.1875,   0.2021,  ...,   4.1289,   7.1641, -19.2812]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 01:11:09 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Another word for lad is chap
Another word for murder is slaying
Another word for reasonable is sensible
Another word for identical is same
Another word for rock is stone
Another word for homogeneous is uniform
Another word for snake is serpent
Another word for sweets is
2024-07-26 01:11:09 root INFO     [order_1_approx] starting weight calculation for Another word for sweets is confectionery
Another word for rock is stone
Another word for identical is same
Another word for murder is slaying
Another word for lad is chap
Another word for reasonable is sensible
Another word for homogeneous is uniform
Another word for snake is
2024-07-26 01:11:09 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 01:13:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8359, -3.4375, -2.1250,  ..., -2.5312,  0.0234, -3.9844],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 5.0000,  3.6250, -3.4688,  ..., -4.5312, 14.1875, -2.0781],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.0029e-02, -4.9744e-03,  4.1260e-02,  ...,  5.6152e-03,
         -1.2207e-03,  9.0332e-03],
        [ 1.5503e-02,  2.1729e-02,  5.6763e-03,  ..., -3.7384e-03,
         -1.3733e-04, -3.0518e-05],
        [-8.8501e-04,  6.7139e-04,  1.2402e-01,  ..., -2.6093e-03,
          5.4626e-03,  6.5002e-03],
        ...,
        [ 5.3711e-03, -4.1504e-03,  3.9673e-03,  ...,  2.2949e-02,
          1.1719e-02, -7.6294e-03],
        [-4.1504e-03,  5.7068e-03,  8.5449e-04,  ...,  2.0752e-03,
          3.4668e-02, -1.4160e-02],
        [ 1.2894e-03, -4.3335e-03, -1.2085e-02,  ..., -4.2114e-03,
         -2.3346e-03,  3.9551e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.9688,  6.8633, -3.4121,  ..., -3.3086, 14.5625, -2.3652]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 01:13:57 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Another word for sweets is confectionery
Another word for rock is stone
Another word for identical is same
Another word for murder is slaying
Another word for lad is chap
Another word for reasonable is sensible
Another word for homogeneous is uniform
Another word for snake is
2024-07-26 01:13:57 root INFO     [order_1_approx] starting weight calculation for Another word for sweets is confectionery
Another word for snake is serpent
Another word for identical is same
Another word for rock is stone
Another word for reasonable is sensible
Another word for homogeneous is uniform
Another word for lad is chap
Another word for murder is
2024-07-26 01:13:57 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 01:16:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.2500, -1.8203, -2.7969,  ..., -2.7812,  0.0156, -1.4922],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([15.7500,  8.7500,  4.5000,  ...,  3.7500, -4.7812, -4.6562],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.6855e-02, -2.3651e-03,  1.8799e-02,  ..., -5.9814e-03,
          1.4282e-02,  8.6670e-03],
        [-2.0447e-03,  1.8311e-02, -2.1729e-02,  ...,  5.6152e-03,
         -8.1787e-03,  5.7983e-03],
        [-5.6458e-03, -2.3041e-03,  9.3750e-02,  ..., -1.1841e-02,
          2.2583e-03, -1.0300e-03],
        ...,
        [ 1.5259e-03,  2.1820e-03, -8.3618e-03,  ...,  2.6123e-02,
         -1.5564e-03, -4.2725e-03],
        [ 1.1719e-02, -5.5847e-03,  9.0332e-03,  ...,  1.0071e-02,
          5.5176e-02,  4.3945e-03],
        [-4.2114e-03, -7.2021e-03,  4.5776e-05,  ...,  6.9427e-04,
          4.4861e-03,  4.6387e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[17.2812,  9.3359,  2.4414,  ...,  2.9375, -4.6602, -3.0039]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 01:16:44 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Another word for sweets is confectionery
Another word for snake is serpent
Another word for identical is same
Another word for rock is stone
Another word for reasonable is sensible
Another word for homogeneous is uniform
Another word for lad is chap
Another word for murder is
2024-07-26 01:16:44 root INFO     total operator prediction time: 1334.9823260307312 seconds
2024-07-26 01:16:44 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on hypernyms - misc
2024-07-26 01:16:44 root INFO     building operator hypernyms - misc
2024-07-26 01:16:45 root INFO     [order_1_approx] starting weight calculation for The lemon falls into the category of citrus
The jacket falls into the category of clothes
The croissant falls into the category of pastry
The dishwasher falls into the category of appliance
The shelf falls into the category of furniture
The deodorant falls into the category of toiletry
The wristband falls into the category of band
The toaster falls into the category of
2024-07-26 01:16:45 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 01:19:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2969, -3.3281,  1.3594,  ...,  0.4023, -5.3750, -2.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -7.4062,  -2.5938,   0.1875,  ...,   7.7500, -15.3750, -17.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0371,  0.0015,  0.0128,  ...,  0.0052, -0.0021,  0.0067],
        [ 0.0020,  0.0278,  0.0079,  ..., -0.0018,  0.0110,  0.0048],
        [ 0.0026, -0.0013,  0.1309,  ..., -0.0008, -0.0057,  0.0020],
        ...,
        [ 0.0033,  0.0004,  0.0134,  ...,  0.0273,  0.0033,  0.0057],
        [-0.0048,  0.0036, -0.0053,  ..., -0.0053,  0.0581, -0.0024],
        [ 0.0058,  0.0009, -0.0038,  ...,  0.0048,  0.0020,  0.0439]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -6.9062,  -2.2305,   0.6548,  ...,   7.9727, -15.2031, -16.2812]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 01:19:34 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The lemon falls into the category of citrus
The jacket falls into the category of clothes
The croissant falls into the category of pastry
The dishwasher falls into the category of appliance
The shelf falls into the category of furniture
The deodorant falls into the category of toiletry
The wristband falls into the category of band
The toaster falls into the category of
2024-07-26 01:19:35 root INFO     [order_1_approx] starting weight calculation for The dishwasher falls into the category of appliance
The deodorant falls into the category of toiletry
The wristband falls into the category of band
The toaster falls into the category of appliance
The shelf falls into the category of furniture
The jacket falls into the category of clothes
The lemon falls into the category of citrus
The croissant falls into the category of
2024-07-26 01:19:35 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 01:22:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2559, -4.0312, -2.6562,  ...,  1.7656, -2.9062,  2.4688],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-1.6797, -1.7656,  2.5156,  ...,  0.7383,  3.5938,  3.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 4.8828e-02,  6.6528e-03,  2.0996e-02,  ...,  7.8125e-03,
          1.1292e-03,  1.9287e-02],
        [-2.1935e-04,  3.6621e-02,  1.1597e-02,  ..., -8.5449e-04,
          6.0730e-03,  1.2573e-02],
        [ 5.4016e-03, -7.3242e-04,  1.4062e-01,  ...,  8.2397e-03,
         -1.2451e-02,  3.0823e-03],
        ...,
        [ 8.6670e-03, -5.6458e-03,  1.0498e-02,  ...,  4.0771e-02,
         -1.3062e-02, -6.4850e-05],
        [ 1.8539e-03,  2.2888e-03,  2.6855e-03,  ..., -1.0376e-03,
          6.3477e-02, -7.6904e-03],
        [ 8.6212e-04, -2.3651e-03, -1.7090e-02,  ..., -5.0049e-03,
         -5.3101e-03,  5.5664e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.8164, -0.8711,  3.5684,  ...,  2.0664,  3.5723,  0.7969]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 01:22:24 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The dishwasher falls into the category of appliance
The deodorant falls into the category of toiletry
The wristband falls into the category of band
The toaster falls into the category of appliance
The shelf falls into the category of furniture
The jacket falls into the category of clothes
The lemon falls into the category of citrus
The croissant falls into the category of
2024-07-26 01:22:24 root INFO     [order_1_approx] starting weight calculation for The lemon falls into the category of citrus
The croissant falls into the category of pastry
The dishwasher falls into the category of appliance
The deodorant falls into the category of toiletry
The toaster falls into the category of appliance
The wristband falls into the category of band
The jacket falls into the category of clothes
The shelf falls into the category of
2024-07-26 01:22:24 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 01:25:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8281, -1.1484,  3.5469,  ..., -0.8594, -0.4961, -1.6172],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-6.0625, 10.6250, -1.7500,  ...,  6.7500, -6.9375, -8.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.1250e-02,  1.0757e-03,  2.2339e-02,  ...,  7.2632e-03,
          1.8539e-03, -2.2888e-03],
        [ 5.9814e-03,  3.2959e-02, -9.3994e-03,  ...,  1.0376e-03,
          1.0223e-03,  9.5825e-03],
        [-1.2283e-03, -2.0752e-03,  1.0547e-01,  ..., -7.3853e-03,
         -4.7607e-03, -3.2349e-03],
        ...,
        [ 2.1973e-03, -1.7166e-04,  8.5449e-04,  ...,  2.8320e-02,
         -3.2501e-03,  2.1935e-05],
        [ 1.0132e-02,  3.3569e-03,  4.6997e-03,  ...,  1.1063e-03,
          4.8340e-02, -7.0801e-03],
        [ 3.5095e-04, -3.0518e-04, -1.2451e-02,  ...,  6.4850e-04,
          7.4463e-03,  5.5176e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-6.6172, 10.2969, -2.0645,  ...,  6.4883, -6.8555, -8.5156]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 01:25:13 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The lemon falls into the category of citrus
The croissant falls into the category of pastry
The dishwasher falls into the category of appliance
The deodorant falls into the category of toiletry
The toaster falls into the category of appliance
The wristband falls into the category of band
The jacket falls into the category of clothes
The shelf falls into the category of
2024-07-26 01:25:13 root INFO     [order_1_approx] starting weight calculation for The croissant falls into the category of pastry
The lemon falls into the category of citrus
The wristband falls into the category of band
The jacket falls into the category of clothes
The toaster falls into the category of appliance
The shelf falls into the category of furniture
The dishwasher falls into the category of appliance
The deodorant falls into the category of
2024-07-26 01:25:13 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 01:28:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 3.3125, -2.0156,  1.1250,  ..., -0.9219, -5.8125, -2.1562],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  3.3125,  -3.3281,   4.2812,  ...,  13.5000, -14.9375,   1.9297],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.3691e-02,  9.7046e-03,  7.9956e-03,  ...,  5.7983e-03,
          1.9150e-03,  5.4932e-03],
        [-9.9182e-04,  2.6855e-02,  1.5945e-03,  ..., -1.3733e-04,
         -4.8828e-03,  2.0142e-03],
        [ 6.3324e-04, -1.8005e-03,  1.0254e-01,  ..., -7.3242e-03,
         -9.9182e-04, -4.0894e-03],
        ...,
        [ 7.6904e-03,  5.2490e-03, -4.2725e-03,  ...,  2.9419e-02,
          8.2397e-03, -3.2043e-04],
        [ 9.3384e-03,  7.3242e-03, -4.2114e-03,  ...,  2.3804e-03,
          4.7119e-02, -2.3804e-03],
        [-4.5776e-03,  2.9449e-03, -7.9346e-03,  ...,  9.1553e-05,
          6.0120e-03,  3.9062e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  1.2207,  -3.2480,   5.2617,  ...,  12.6719, -15.4297,   0.9209]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 01:28:01 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The croissant falls into the category of pastry
The lemon falls into the category of citrus
The wristband falls into the category of band
The jacket falls into the category of clothes
The toaster falls into the category of appliance
The shelf falls into the category of furniture
The dishwasher falls into the category of appliance
The deodorant falls into the category of
2024-07-26 01:28:01 root INFO     [order_1_approx] starting weight calculation for The dishwasher falls into the category of appliance
The deodorant falls into the category of toiletry
The lemon falls into the category of citrus
The shelf falls into the category of furniture
The croissant falls into the category of pastry
The toaster falls into the category of appliance
The wristband falls into the category of band
The jacket falls into the category of
2024-07-26 01:28:01 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 01:30:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.5625, -0.9141, -2.8906,  ..., -0.0234, -2.9531,  1.0469],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 1.8750, 13.0000, -8.7500,  ...,  2.2344, -0.6875, -5.3438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0312,  0.0066,  0.0024,  ...,  0.0097,  0.0003, -0.0059],
        [-0.0006,  0.0222, -0.0072,  ..., -0.0002,  0.0043,  0.0074],
        [ 0.0039, -0.0036,  0.1279,  ..., -0.0074, -0.0085, -0.0041],
        ...,
        [ 0.0026,  0.0018,  0.0070,  ...,  0.0273, -0.0020,  0.0011],
        [ 0.0068,  0.0051,  0.0068,  ..., -0.0012,  0.0498,  0.0015],
        [-0.0003, -0.0033, -0.0164,  ..., -0.0030,  0.0068,  0.0449]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.5352, 12.5391, -7.8828,  ...,  1.5293,  0.9834, -5.6289]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 01:30:50 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The dishwasher falls into the category of appliance
The deodorant falls into the category of toiletry
The lemon falls into the category of citrus
The shelf falls into the category of furniture
The croissant falls into the category of pastry
The toaster falls into the category of appliance
The wristband falls into the category of band
The jacket falls into the category of
2024-07-26 01:30:51 root INFO     [order_1_approx] starting weight calculation for The croissant falls into the category of pastry
The wristband falls into the category of band
The shelf falls into the category of furniture
The toaster falls into the category of appliance
The dishwasher falls into the category of appliance
The deodorant falls into the category of toiletry
The jacket falls into the category of clothes
The lemon falls into the category of
2024-07-26 01:30:51 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 01:33:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0801, -2.9062,  3.1875,  ..., -0.4531, -3.8438, -0.4629],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 2.5781,  7.5312, -9.5000,  ..., 13.3750, -6.5625,  1.3125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0347,  0.0047,  0.0105,  ...,  0.0001,  0.0081,  0.0089],
        [ 0.0006,  0.0273, -0.0039,  ...,  0.0035,  0.0013,  0.0016],
        [ 0.0009, -0.0021,  0.1074,  ..., -0.0085, -0.0049, -0.0007],
        ...,
        [ 0.0052, -0.0043, -0.0070,  ...,  0.0245, -0.0068, -0.0036],
        [ 0.0006,  0.0007,  0.0060,  ..., -0.0054,  0.0535, -0.0075],
        [ 0.0028,  0.0031, -0.0071,  ..., -0.0013,  0.0066,  0.0393]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7148,  7.5234, -9.0078,  ..., 14.0547, -7.8789,  0.4385]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 01:33:40 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The croissant falls into the category of pastry
The wristband falls into the category of band
The shelf falls into the category of furniture
The toaster falls into the category of appliance
The dishwasher falls into the category of appliance
The deodorant falls into the category of toiletry
The jacket falls into the category of clothes
The lemon falls into the category of
2024-07-26 01:33:40 root INFO     [order_1_approx] starting weight calculation for The wristband falls into the category of band
The toaster falls into the category of appliance
The jacket falls into the category of clothes
The lemon falls into the category of citrus
The deodorant falls into the category of toiletry
The shelf falls into the category of furniture
The croissant falls into the category of pastry
The dishwasher falls into the category of
2024-07-26 01:33:41 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 01:36:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7617, -4.4688,  2.5312,  ..., -1.2344, -5.1562, -0.3691],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-16.2500,   6.2188,   1.0781,  ...,  14.7500, -11.0000, -10.9375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0332, -0.0007,  0.0122,  ...,  0.0013,  0.0065,  0.0032],
        [ 0.0037,  0.0308,  0.0095,  ..., -0.0057,  0.0057,  0.0094],
        [ 0.0009, -0.0028,  0.1074,  ..., -0.0083, -0.0023, -0.0045],
        ...,
        [ 0.0005,  0.0025, -0.0007,  ...,  0.0320,  0.0106,  0.0013],
        [ 0.0046,  0.0055, -0.0070,  ...,  0.0027,  0.0500,  0.0010],
        [ 0.0007,  0.0014, -0.0074,  ...,  0.0003,  0.0037,  0.0425]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-16.4844,   6.3711,   2.0215,  ...,  12.3594, -11.9141, -10.1953]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 01:36:29 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The wristband falls into the category of band
The toaster falls into the category of appliance
The jacket falls into the category of clothes
The lemon falls into the category of citrus
The deodorant falls into the category of toiletry
The shelf falls into the category of furniture
The croissant falls into the category of pastry
The dishwasher falls into the category of
2024-07-26 01:36:30 root INFO     [order_1_approx] starting weight calculation for The deodorant falls into the category of toiletry
The dishwasher falls into the category of appliance
The croissant falls into the category of pastry
The lemon falls into the category of citrus
The shelf falls into the category of furniture
The jacket falls into the category of clothes
The toaster falls into the category of appliance
The wristband falls into the category of
2024-07-26 01:36:30 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 01:39:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 3.6406, -2.6562, -1.7422,  ...,  2.5625, -3.0781, -0.9375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -4.7188,   5.3125,  -3.3281,  ...,   9.8125, -13.8125,   0.5938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0542,  0.0124,  0.0078,  ...,  0.0063, -0.0121, -0.0081],
        [-0.0006,  0.0291, -0.0002,  ..., -0.0074, -0.0023,  0.0159],
        [-0.0047, -0.0063,  0.1260,  ..., -0.0099, -0.0011,  0.0011],
        ...,
        [-0.0005,  0.0024,  0.0137,  ...,  0.0420, -0.0181,  0.0110],
        [ 0.0134,  0.0029, -0.0088,  ...,  0.0079,  0.0640, -0.0059],
        [-0.0007, -0.0012, -0.0070,  ..., -0.0041,  0.0148,  0.0564]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -5.7656,   5.3555,  -3.6074,  ...,   8.7109, -13.5234,   1.2461]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 01:39:19 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The deodorant falls into the category of toiletry
The dishwasher falls into the category of appliance
The croissant falls into the category of pastry
The lemon falls into the category of citrus
The shelf falls into the category of furniture
The jacket falls into the category of clothes
The toaster falls into the category of appliance
The wristband falls into the category of
2024-07-26 01:39:19 root INFO     total operator prediction time: 1354.196435213089 seconds
2024-07-26 01:39:19 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on meronyms - substance
2024-07-26 01:39:19 root INFO     building operator meronyms - substance
2024-07-26 01:39:19 root INFO     [order_1_approx] starting weight calculation for A candy is made up of sugar
A money is made up of paper
A pastry is made up of flour
A bag is made up of leather
A wine is made up of grapes
A glacier is made up of ice
A clothing is made up of fabric
A ocean is made up of
2024-07-26 01:39:19 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 01:42:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2168, -0.1875,  0.1387,  ..., -4.5625, -6.5312, -0.3398],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([13.6250, 15.3125, -5.4688,  ...,  4.6250,  6.2188, -8.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0266, -0.0001,  0.0013,  ...,  0.0009,  0.0009,  0.0037],
        [-0.0013,  0.0184,  0.0143,  ...,  0.0005,  0.0054,  0.0023],
        [-0.0035,  0.0009,  0.1045,  ...,  0.0009, -0.0090,  0.0033],
        ...,
        [ 0.0085,  0.0034, -0.0011,  ...,  0.0155,  0.0021, -0.0038],
        [-0.0031,  0.0017, -0.0087,  ...,  0.0035,  0.0381,  0.0043],
        [ 0.0081, -0.0067, -0.0135,  ...,  0.0011, -0.0008,  0.0413]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[13.3750, 16.0469, -6.1328,  ...,  5.5312,  5.2070, -8.4531]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 01:42:07 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A candy is made up of sugar
A money is made up of paper
A pastry is made up of flour
A bag is made up of leather
A wine is made up of grapes
A glacier is made up of ice
A clothing is made up of fabric
A ocean is made up of
2024-07-26 01:42:07 root INFO     [order_1_approx] starting weight calculation for A glacier is made up of ice
A candy is made up of sugar
A ocean is made up of water
A bag is made up of leather
A clothing is made up of fabric
A pastry is made up of flour
A money is made up of paper
A wine is made up of
2024-07-26 01:42:07 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 01:44:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9258, -1.9766,  3.1250,  ..., -2.1250, -4.0625, -1.5156],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 4.6250, -4.2812,  5.6875,  ...,  2.2812, 17.5000,  1.5781],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0198,  0.0010,  0.0283,  ..., -0.0003,  0.0019,  0.0058],
        [-0.0024,  0.0184,  0.0059,  ...,  0.0022,  0.0031,  0.0029],
        [-0.0019, -0.0017,  0.1108,  ..., -0.0009,  0.0005,  0.0040],
        ...,
        [ 0.0138,  0.0044, -0.0078,  ...,  0.0133,  0.0074, -0.0032],
        [-0.0068,  0.0013,  0.0027,  ...,  0.0005,  0.0311, -0.0052],
        [ 0.0035,  0.0037, -0.0084,  ..., -0.0005, -0.0051,  0.0364]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.9180, -4.1836,  5.4141,  ...,  1.6631, 17.1406,  0.3535]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 01:44:56 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A glacier is made up of ice
A candy is made up of sugar
A ocean is made up of water
A bag is made up of leather
A clothing is made up of fabric
A pastry is made up of flour
A money is made up of paper
A wine is made up of
2024-07-26 01:44:57 root INFO     [order_1_approx] starting weight calculation for A clothing is made up of fabric
A pastry is made up of flour
A money is made up of paper
A wine is made up of grapes
A ocean is made up of water
A bag is made up of leather
A glacier is made up of ice
A candy is made up of
2024-07-26 01:44:57 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 01:47:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3867,  1.1875,  1.9844,  ..., -1.7266, -2.8906, -1.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-2.5938, -2.1406,  6.0625,  ..., -1.1328, 11.4375, -0.3125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.4668e-02,  4.3640e-03,  1.8433e-02,  ..., -3.5248e-03,
         -4.5776e-03,  1.6113e-02],
        [ 9.9182e-05,  3.4180e-02,  1.1414e-02,  ..., -5.0354e-04,
         -7.6294e-04,  3.7842e-03],
        [-4.9438e-03,  2.0599e-03,  1.1084e-01,  ..., -2.8076e-03,
         -1.0742e-02, -5.4932e-03],
        ...,
        [ 7.0801e-03,  4.2725e-04,  7.8125e-03,  ...,  2.2949e-02,
          7.0190e-03,  4.2114e-03],
        [ 6.7902e-04, -3.0823e-03, -1.0132e-02,  ...,  2.2888e-05,
          4.1504e-02, -5.0049e-03],
        [ 5.9814e-03,  3.8757e-03,  3.6621e-04,  ...,  1.4572e-03,
          5.3406e-05,  4.1260e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6074, -1.6934,  5.2773,  ..., -1.8652, 11.1719, -1.1562]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 01:47:45 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A clothing is made up of fabric
A pastry is made up of flour
A money is made up of paper
A wine is made up of grapes
A ocean is made up of water
A bag is made up of leather
A glacier is made up of ice
A candy is made up of
2024-07-26 01:47:46 root INFO     [order_1_approx] starting weight calculation for A clothing is made up of fabric
A candy is made up of sugar
A wine is made up of grapes
A ocean is made up of water
A money is made up of paper
A bag is made up of leather
A glacier is made up of ice
A pastry is made up of
2024-07-26 01:47:46 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 01:50:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.6562, -0.5664,  4.7500,  ..., -1.5312, -3.3438,  0.0957],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-1.2969,  8.2500,  4.4375,  ..., -4.0625,  9.6875, -0.2969],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0374, -0.0010,  0.0095,  ...,  0.0009,  0.0029,  0.0205],
        [ 0.0022,  0.0283,  0.0029,  ..., -0.0048,  0.0066,  0.0100],
        [ 0.0029,  0.0012,  0.1177,  ...,  0.0061, -0.0153, -0.0033],
        ...,
        [ 0.0138, -0.0020,  0.0064,  ...,  0.0273,  0.0040, -0.0016],
        [ 0.0032, -0.0009,  0.0014,  ..., -0.0042,  0.0547, -0.0035],
        [-0.0012,  0.0023, -0.0190,  ...,  0.0006, -0.0009,  0.0540]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2959,  8.7656,  4.4375,  ..., -4.7578, 10.5469,  0.3291]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 01:50:33 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A clothing is made up of fabric
A candy is made up of sugar
A wine is made up of grapes
A ocean is made up of water
A money is made up of paper
A bag is made up of leather
A glacier is made up of ice
A pastry is made up of
2024-07-26 01:50:34 root INFO     [order_1_approx] starting weight calculation for A pastry is made up of flour
A candy is made up of sugar
A clothing is made up of fabric
A glacier is made up of ice
A wine is made up of grapes
A bag is made up of leather
A ocean is made up of water
A money is made up of
2024-07-26 01:50:34 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 01:53:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1094,  2.8594,  6.5938,  ..., -3.5938, -5.4062, -0.7930],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -5.5625,  12.8750,   7.5000,  ..., -17.2500,  10.5000,  -2.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.0874e-02, -4.3030e-03,  1.0620e-02,  ...,  3.5553e-03,
          2.8687e-03, -4.9744e-03],
        [-3.1738e-03,  1.7944e-02, -6.8970e-03,  ..., -2.7466e-04,
          9.6130e-04, -1.3428e-03],
        [-4.6730e-04,  2.1973e-03,  1.0400e-01,  ..., -9.3079e-04,
          8.0109e-05,  4.4861e-03],
        ...,
        [ 8.5449e-04,  2.6855e-03,  5.7373e-03,  ...,  1.9287e-02,
          5.7678e-03, -7.2479e-05],
        [-1.5259e-05, -2.8229e-03, -6.5002e-03,  ..., -1.7319e-03,
          3.2471e-02, -6.7139e-03],
        [-2.3193e-03,  5.1880e-04, -1.6724e-02,  ..., -1.3504e-03,
          3.2959e-03,  4.5166e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -5.3711,  14.0156,   6.3242,  ..., -14.2422,   9.7891,  -2.5391]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 01:53:24 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A pastry is made up of flour
A candy is made up of sugar
A clothing is made up of fabric
A glacier is made up of ice
A wine is made up of grapes
A bag is made up of leather
A ocean is made up of water
A money is made up of
2024-07-26 01:53:24 root INFO     [order_1_approx] starting weight calculation for A money is made up of paper
A pastry is made up of flour
A clothing is made up of fabric
A ocean is made up of water
A bag is made up of leather
A wine is made up of grapes
A candy is made up of sugar
A glacier is made up of
2024-07-26 01:53:24 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 01:56:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3281, -0.3789,  2.3125,  ..., -4.6250, -5.6875,  2.2656],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([11.5000,  9.1250,  7.1875,  ..., 10.5000, -4.4688,  3.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0293, -0.0022, -0.0151,  ...,  0.0003,  0.0013,  0.0058],
        [-0.0017,  0.0217,  0.0021,  ...,  0.0002,  0.0074,  0.0022],
        [-0.0088, -0.0010,  0.1211,  ..., -0.0063, -0.0093, -0.0009],
        ...,
        [ 0.0040,  0.0041, -0.0063,  ...,  0.0220,  0.0019, -0.0031],
        [ 0.0015,  0.0008, -0.0093,  ...,  0.0023,  0.0459,  0.0008],
        [ 0.0030, -0.0050,  0.0161,  ...,  0.0013, -0.0062,  0.0488]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[12.1172,  9.5312,  7.0156,  ...,  9.8125, -4.7461,  3.3086]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 01:56:13 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A money is made up of paper
A pastry is made up of flour
A clothing is made up of fabric
A ocean is made up of water
A bag is made up of leather
A wine is made up of grapes
A candy is made up of sugar
A glacier is made up of
2024-07-26 01:56:14 root INFO     [order_1_approx] starting weight calculation for A bag is made up of leather
A money is made up of paper
A pastry is made up of flour
A wine is made up of grapes
A glacier is made up of ice
A ocean is made up of water
A candy is made up of sugar
A clothing is made up of
2024-07-26 01:56:14 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 01:59:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.1562,  1.3750,  2.7344,  ..., -3.2500, -5.6250, -2.7656],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 2.0469, 12.5000, -9.3125,  ...,  3.7656, -0.4492, -6.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.9785e-02, -2.3193e-03,  4.8828e-03,  ...,  4.5166e-03,
         -4.1199e-03, -3.8147e-06],
        [-4.2114e-03,  1.5381e-02,  6.1340e-03,  ..., -7.5912e-04,
          3.0975e-03,  1.0605e-03],
        [-1.4648e-03, -7.1716e-04,  8.4961e-02,  ..., -2.9907e-03,
         -6.1798e-04, -3.2043e-03],
        ...,
        [-8.0872e-04,  4.8523e-03,  9.2773e-03,  ...,  2.1240e-02,
          1.8234e-03,  4.8828e-04],
        [ 2.1973e-03, -3.6621e-04, -1.4648e-03,  ...,  3.5706e-03,
          2.9785e-02, -3.3264e-03],
        [ 5.8594e-03, -4.1504e-03, -1.6357e-02,  ..., -2.0599e-04,
          4.6387e-03,  3.9307e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6465, 11.8984, -9.9844,  ...,  4.1758, -0.2115, -6.8906]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 01:59:01 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A bag is made up of leather
A money is made up of paper
A pastry is made up of flour
A wine is made up of grapes
A glacier is made up of ice
A ocean is made up of water
A candy is made up of sugar
A clothing is made up of
2024-07-26 01:59:01 root INFO     [order_1_approx] starting weight calculation for A ocean is made up of water
A pastry is made up of flour
A wine is made up of grapes
A candy is made up of sugar
A glacier is made up of ice
A clothing is made up of fabric
A money is made up of paper
A bag is made up of
2024-07-26 01:59:01 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 02:01:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5547,  0.7109,  2.6562,  ..., -3.5469, -3.8125, -2.5938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 1.1172,  6.7188,  2.9219,  ...,  2.8125, -4.2500, -5.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.2227e-02,  8.7738e-04,  3.3447e-02,  ..., -7.7248e-05,
          9.7656e-04,  1.0193e-02],
        [-5.0049e-03,  1.4282e-02, -3.1738e-03,  ..., -9.5367e-04,
          1.6785e-03,  3.3264e-03],
        [-2.0294e-03,  7.1106e-03,  1.3379e-01,  ..., -2.8381e-03,
         -4.8828e-03,  2.0294e-03],
        ...,
        [-1.1749e-03,  5.6763e-03,  6.6528e-03,  ...,  2.4902e-02,
         -2.4261e-03, -4.3945e-03],
        [ 3.3569e-03, -3.5706e-03, -8.4229e-03,  ..., -6.9580e-03,
          4.9316e-02, -5.1880e-03],
        [ 3.4485e-03, -7.8125e-03, -9.9487e-03,  ...,  1.4572e-03,
          5.5847e-03,  5.2979e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9395,  7.8906,  2.4180,  ...,  1.5410, -3.5391, -5.3203]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 02:01:51 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A ocean is made up of water
A pastry is made up of flour
A wine is made up of grapes
A candy is made up of sugar
A glacier is made up of ice
A clothing is made up of fabric
A money is made up of paper
A bag is made up of
2024-07-26 02:01:51 root INFO     total operator prediction time: 1352.4907050132751 seconds
2024-07-26 02:01:51 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on synonyms - intensity
2024-07-26 02:01:51 root INFO     building operator synonyms - intensity
2024-07-26 02:01:51 root INFO     [order_1_approx] starting weight calculation for A more intense word for rain is deluge
A more intense word for chuckle is laugh
A more intense word for angry is furious
A more intense word for ask is beg
A more intense word for pain is torment
A more intense word for happy is ecstatic
A more intense word for sad is desparate
A more intense word for like is
2024-07-26 02:01:52 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 02:04:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.9141, -0.9844, -1.3125,  ...,  2.1875,  1.2422, -1.5781],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  2.9062,  -3.6250,  15.8750,  ...,  -5.3125,   2.5312, -14.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0435,  0.0030,  0.0192,  ..., -0.0007,  0.0025, -0.0014],
        [ 0.0043,  0.0247,  0.0229,  ..., -0.0005, -0.0007,  0.0065],
        [ 0.0139, -0.0101,  0.1104,  ..., -0.0023, -0.0047, -0.0009],
        ...,
        [-0.0032, -0.0071, -0.0188,  ...,  0.0280, -0.0078, -0.0025],
        [-0.0020,  0.0049,  0.0113,  ...,  0.0020,  0.0537,  0.0041],
        [ 0.0002, -0.0054, -0.0094,  ..., -0.0070,  0.0026,  0.0371]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  2.1836,  -3.4473,  16.3438,  ...,  -6.7031,   3.3477, -15.7734]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 02:04:41 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for rain is deluge
A more intense word for chuckle is laugh
A more intense word for angry is furious
A more intense word for ask is beg
A more intense word for pain is torment
A more intense word for happy is ecstatic
A more intense word for sad is desparate
A more intense word for like is
2024-07-26 02:04:41 root INFO     [order_1_approx] starting weight calculation for A more intense word for angry is furious
A more intense word for ask is beg
A more intense word for chuckle is laugh
A more intense word for rain is deluge
A more intense word for like is love
A more intense word for pain is torment
A more intense word for happy is ecstatic
A more intense word for sad is
2024-07-26 02:04:41 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 02:07:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3359, -0.4219,  0.5898,  ..., -0.1914, -0.9062,  1.0703],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -9.1250,  -4.9375,   7.8750,  ...,   0.6562,  -0.4062, -21.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0330,  0.0015,  0.0008,  ..., -0.0012,  0.0045, -0.0045],
        [ 0.0019,  0.0148, -0.0007,  ...,  0.0020, -0.0022,  0.0048],
        [ 0.0022, -0.0031,  0.0947,  ..., -0.0036, -0.0017,  0.0002],
        ...,
        [ 0.0044, -0.0064, -0.0145,  ...,  0.0260, -0.0045, -0.0074],
        [ 0.0030,  0.0083, -0.0018,  ..., -0.0040,  0.0471,  0.0015],
        [-0.0022, -0.0047, -0.0129,  ..., -0.0003, -0.0061,  0.0381]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-10.5547,  -5.0469,   7.8633,  ...,   0.4854,   0.3257, -22.0781]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 02:07:30 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for angry is furious
A more intense word for ask is beg
A more intense word for chuckle is laugh
A more intense word for rain is deluge
A more intense word for like is love
A more intense word for pain is torment
A more intense word for happy is ecstatic
A more intense word for sad is
2024-07-26 02:07:31 root INFO     [order_1_approx] starting weight calculation for A more intense word for sad is desparate
A more intense word for chuckle is laugh
A more intense word for angry is furious
A more intense word for rain is deluge
A more intense word for pain is torment
A more intense word for like is love
A more intense word for happy is ecstatic
A more intense word for ask is
2024-07-26 02:07:31 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 02:10:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.5000, -1.1562,  0.9062,  ...,  1.3750,  1.0391, -1.7031],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  6.8750,   1.2969,   0.6719,  ..., -12.0625,  -3.9688,  -0.0469],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0393, -0.0054, -0.0066,  ...,  0.0048,  0.0107, -0.0065],
        [ 0.0011,  0.0156,  0.0022,  ...,  0.0048, -0.0070,  0.0062],
        [-0.0010,  0.0054,  0.0894,  ..., -0.0053,  0.0006, -0.0011],
        ...,
        [ 0.0034, -0.0020, -0.0165,  ...,  0.0229, -0.0087,  0.0025],
        [ 0.0068,  0.0036,  0.0066,  ...,  0.0024,  0.0439,  0.0008],
        [-0.0003, -0.0063,  0.0030,  ..., -0.0089, -0.0035,  0.0422]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  7.1016,   1.5498,   1.2168,  ..., -12.1250,  -4.1523,  -1.2607]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 02:10:20 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for sad is desparate
A more intense word for chuckle is laugh
A more intense word for angry is furious
A more intense word for rain is deluge
A more intense word for pain is torment
A more intense word for like is love
A more intense word for happy is ecstatic
A more intense word for ask is
2024-07-26 02:10:20 root INFO     [order_1_approx] starting weight calculation for A more intense word for rain is deluge
A more intense word for like is love
A more intense word for sad is desparate
A more intense word for angry is furious
A more intense word for pain is torment
A more intense word for happy is ecstatic
A more intense word for ask is beg
A more intense word for chuckle is
2024-07-26 02:10:21 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 02:13:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2812,  0.3594,  2.5469,  ...,  2.3438, -1.4062,  1.3594],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-1.5625,  6.5938,  8.5625,  ..., 16.2500, -7.0938, -7.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0383,  0.0041,  0.0181,  ...,  0.0006,  0.0051,  0.0010],
        [ 0.0045,  0.0195, -0.0016,  ..., -0.0012,  0.0025,  0.0026],
        [ 0.0054,  0.0010,  0.1270,  ..., -0.0076, -0.0045,  0.0015],
        ...,
        [ 0.0011,  0.0004, -0.0112,  ...,  0.0283, -0.0067, -0.0046],
        [ 0.0023,  0.0115,  0.0172,  ..., -0.0045,  0.0491,  0.0063],
        [ 0.0056, -0.0036, -0.0112,  ..., -0.0007,  0.0094,  0.0430]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9375,  7.2188,  7.1250,  ..., 13.7969, -8.2734, -7.3320]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 02:13:09 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for rain is deluge
A more intense word for like is love
A more intense word for sad is desparate
A more intense word for angry is furious
A more intense word for pain is torment
A more intense word for happy is ecstatic
A more intense word for ask is beg
A more intense word for chuckle is
2024-07-26 02:13:10 root INFO     [order_1_approx] starting weight calculation for A more intense word for pain is torment
A more intense word for chuckle is laugh
A more intense word for like is love
A more intense word for happy is ecstatic
A more intense word for rain is deluge
A more intense word for ask is beg
A more intense word for sad is desparate
A more intense word for angry is
2024-07-26 02:13:10 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 02:15:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2324, -1.5156, -2.5469,  ...,  2.2031, -2.1562,  1.4609],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-17.2500,   0.7344,  -7.8750,  ...,  11.0000,   1.3906,  -6.6562],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0264,  0.0016,  0.0128,  ...,  0.0026,  0.0042, -0.0045],
        [-0.0025,  0.0183,  0.0072,  ...,  0.0020, -0.0057,  0.0031],
        [ 0.0017,  0.0002,  0.0723,  ..., -0.0053,  0.0087, -0.0015],
        ...,
        [ 0.0003,  0.0012, -0.0159,  ...,  0.0225, -0.0076, -0.0007],
        [ 0.0027,  0.0059,  0.0077,  ..., -0.0008,  0.0352,  0.0048],
        [ 0.0018, -0.0076, -0.0027,  ..., -0.0005, -0.0060,  0.0359]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-16.9062,  -0.7070,  -9.0000,  ...,   9.1016,   1.1982,  -6.4727]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 02:15:56 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for pain is torment
A more intense word for chuckle is laugh
A more intense word for like is love
A more intense word for happy is ecstatic
A more intense word for rain is deluge
A more intense word for ask is beg
A more intense word for sad is desparate
A more intense word for angry is
2024-07-26 02:15:56 root INFO     [order_1_approx] starting weight calculation for A more intense word for chuckle is laugh
A more intense word for pain is torment
A more intense word for angry is furious
A more intense word for sad is desparate
A more intense word for ask is beg
A more intense word for happy is ecstatic
A more intense word for like is love
A more intense word for rain is
2024-07-26 02:15:57 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 02:18:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3984, -1.8594,  4.9688,  ..., -0.4258, -0.1172, -2.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-1.0781,  6.2500, -7.8750,  ..., -0.6562,  5.1562,  3.1094],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.6846e-02,  7.3242e-03,  1.2207e-02,  ..., -4.3335e-03,
          1.1597e-03,  5.9204e-03],
        [-1.4038e-03,  2.3804e-02,  6.6223e-03,  ..., -9.7656e-03,
         -6.4697e-03,  1.1841e-02],
        [ 7.1716e-04,  1.9073e-03,  8.6426e-02,  ..., -4.3945e-03,
         -3.6621e-03,  3.2959e-03],
        ...,
        [ 2.6398e-03, -1.1978e-03, -1.5991e-02,  ...,  2.0996e-02,
         -3.4332e-05, -1.9379e-03],
        [ 7.2327e-03,  9.9182e-05,  1.0864e-02,  ...,  2.9907e-03,
          4.0283e-02,  2.5940e-04],
        [-9.0790e-04, -3.9062e-03, -8.3008e-03,  ...,  2.0752e-03,
          4.3945e-03,  2.3438e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.3359,  6.6484, -7.7188,  ..., -0.7734,  5.4180,  3.4355]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 02:18:45 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for chuckle is laugh
A more intense word for pain is torment
A more intense word for angry is furious
A more intense word for sad is desparate
A more intense word for ask is beg
A more intense word for happy is ecstatic
A more intense word for like is love
A more intense word for rain is
2024-07-26 02:18:45 root INFO     [order_1_approx] starting weight calculation for A more intense word for ask is beg
A more intense word for like is love
A more intense word for rain is deluge
A more intense word for sad is desparate
A more intense word for angry is furious
A more intense word for chuckle is laugh
A more intense word for pain is torment
A more intense word for happy is
2024-07-26 02:18:45 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 02:21:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2422,  0.0352, -0.2109,  ...,  3.2344, -2.9062,  2.5938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-5.3750, -7.9688,  1.0391,  ...,  0.8320,  0.9062, -9.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.2471e-02,  2.4796e-04,  3.2959e-03,  ...,  1.7548e-03,
          6.0730e-03, -2.0905e-03],
        [ 9.0027e-04,  2.0630e-02,  8.3618e-03,  ...,  3.2654e-03,
         -4.7302e-03,  7.3853e-03],
        [ 3.7689e-03, -5.0659e-03,  9.9609e-02,  ..., -4.2114e-03,
         -4.2725e-04,  1.2512e-03],
        ...,
        [-4.5776e-05,  2.2125e-04, -8.1177e-03,  ...,  2.2217e-02,
         -5.9509e-03,  2.2430e-03],
        [ 2.8381e-03,  6.2256e-03,  2.2125e-04,  ..., -3.0756e-05,
          4.5166e-02, -3.2234e-04],
        [ 1.0605e-03, -7.5684e-03, -1.1719e-02,  ..., -2.0294e-03,
         -1.3123e-03,  3.4180e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -6.5312,  -7.1914,   1.7793,  ...,   0.4976,   0.9243, -10.3359]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 02:21:34 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for ask is beg
A more intense word for like is love
A more intense word for rain is deluge
A more intense word for sad is desparate
A more intense word for angry is furious
A more intense word for chuckle is laugh
A more intense word for pain is torment
A more intense word for happy is
2024-07-26 02:21:34 root INFO     [order_1_approx] starting weight calculation for A more intense word for rain is deluge
A more intense word for ask is beg
A more intense word for angry is furious
A more intense word for chuckle is laugh
A more intense word for sad is desparate
A more intense word for like is love
A more intense word for happy is ecstatic
A more intense word for pain is
2024-07-26 02:21:34 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 02:24:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.5000, -1.7188, -0.1436,  ..., -1.2031, -0.8555, -2.3281],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-13.2500,   1.7422,   2.7969,  ...,  -1.7500, -12.5625, -16.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.1738e-02, -6.8665e-04,  1.3123e-02,  ..., -4.4861e-03,
          1.3794e-02, -2.0752e-03],
        [ 1.0300e-03,  1.8188e-02,  4.4861e-03,  ...,  3.0518e-05,
         -7.6904e-03, -2.8687e-03],
        [ 2.2888e-04, -7.0190e-04,  9.7168e-02,  ..., -5.6763e-03,
          9.0942e-03,  7.3547e-03],
        ...,
        [ 3.8452e-03, -3.8757e-03, -1.6357e-02,  ...,  3.7109e-02,
         -1.3123e-03, -2.8076e-03],
        [-2.2278e-03,  4.8523e-03, -5.8594e-03,  ...,  5.0659e-03,
          3.7109e-02,  1.4771e-02],
        [ 2.5940e-04, -7.9956e-03, -7.3242e-03,  ..., -5.2185e-03,
          6.2866e-03,  3.1250e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-13.7344,   1.5156,   1.9004,  ...,  -1.3223, -10.6797, -14.2031]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 02:24:23 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for rain is deluge
A more intense word for ask is beg
A more intense word for angry is furious
A more intense word for chuckle is laugh
A more intense word for sad is desparate
A more intense word for like is love
A more intense word for happy is ecstatic
A more intense word for pain is
2024-07-26 02:24:23 root INFO     total operator prediction time: 1352.3991603851318 seconds
2024-07-26 02:24:23 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on hypernyms - animals
2024-07-26 02:24:23 root INFO     building operator hypernyms - animals
2024-07-26 02:24:24 root INFO     [order_1_approx] starting weight calculation for The cat falls into the category of feline
The porcupine falls into the category of rodent
The human falls into the category of primate
The buffalo falls into the category of bovid
The jackal falls into the category of canine
The dog falls into the category of canine
The jaguar falls into the category of feline
The duck falls into the category of
2024-07-26 02:24:24 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 02:27:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0938, -1.4062,  0.0000,  ..., -0.1875, -4.5938, -1.2109],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -5.0625,  -3.3750, -13.0625,  ...,  -4.9688,   5.5625,   0.4062],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0131,  0.0029,  0.0148,  ...,  0.0007,  0.0067,  0.0053],
        [ 0.0046,  0.0195,  0.0049,  ...,  0.0027,  0.0017,  0.0014],
        [ 0.0025,  0.0010,  0.0981,  ..., -0.0029,  0.0056, -0.0010],
        ...,
        [ 0.0029, -0.0008, -0.0030,  ...,  0.0190,  0.0033,  0.0070],
        [-0.0063, -0.0022,  0.0088,  ..., -0.0095,  0.0352, -0.0024],
        [ 0.0009, -0.0070,  0.0143,  ...,  0.0056, -0.0022,  0.0327]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -4.0195,  -3.7695, -13.0234,  ...,  -5.6797,   6.8281,  -0.4468]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 02:27:11 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The cat falls into the category of feline
The porcupine falls into the category of rodent
The human falls into the category of primate
The buffalo falls into the category of bovid
The jackal falls into the category of canine
The dog falls into the category of canine
The jaguar falls into the category of feline
The duck falls into the category of
2024-07-26 02:27:12 root INFO     [order_1_approx] starting weight calculation for The porcupine falls into the category of rodent
The duck falls into the category of fowl
The dog falls into the category of canine
The cat falls into the category of feline
The jackal falls into the category of canine
The buffalo falls into the category of bovid
The human falls into the category of primate
The jaguar falls into the category of
2024-07-26 02:27:12 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 02:29:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 3.2500,  0.9648,  0.4062,  ..., -2.5312, -0.9648,  2.0312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 3.3750, 18.0000, -7.6250,  ..., -7.2188, 10.6250, -2.3125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.3062e-02,  9.1934e-04,  2.0447e-03,  ...,  4.2114e-03,
         -6.8665e-04,  1.8555e-02],
        [ 4.3945e-03,  1.8433e-02, -8.4839e-03,  ..., -9.2316e-04,
         -6.2256e-03,  6.2256e-03],
        [ 1.2665e-03,  4.0894e-03,  8.9844e-02,  ..., -4.1962e-05,
          1.6357e-02, -2.9602e-03],
        ...,
        [-6.1646e-03, -6.2256e-03,  9.3994e-03,  ...,  1.1658e-02,
          7.1335e-04,  4.8218e-03],
        [ 2.8992e-04, -3.9673e-03,  4.9438e-03,  ..., -2.0752e-03,
          2.8320e-02,  5.6458e-04],
        [-3.2196e-03, -7.8125e-03,  1.4893e-02,  ..., -3.3569e-03,
          9.0942e-03,  2.1484e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.9297, 18.0781, -7.2773,  ..., -8.1172, 10.6328, -2.1230]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 02:29:59 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The porcupine falls into the category of rodent
The duck falls into the category of fowl
The dog falls into the category of canine
The cat falls into the category of feline
The jackal falls into the category of canine
The buffalo falls into the category of bovid
The human falls into the category of primate
The jaguar falls into the category of
2024-07-26 02:29:59 root INFO     [order_1_approx] starting weight calculation for The buffalo falls into the category of bovid
The porcupine falls into the category of rodent
The jackal falls into the category of canine
The duck falls into the category of fowl
The human falls into the category of primate
The jaguar falls into the category of feline
The cat falls into the category of feline
The dog falls into the category of
2024-07-26 02:29:59 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 02:32:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8164, -4.7500,  0.8242,  ..., -2.9688, -1.5469, -1.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -3.0000,   1.2812, -16.5000,  ...,  -2.5000, -10.5000, -10.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.2329e-02, -4.0436e-04, -2.0142e-03,  ...,  1.2817e-03,
         -3.2654e-03,  4.3335e-03],
        [-2.3956e-03,  1.2329e-02,  6.2561e-03,  ..., -3.6011e-03,
         -1.2589e-04,  3.4332e-05],
        [ 1.2360e-03, -7.1716e-04,  5.8594e-02,  ...,  4.0054e-05,
          1.9226e-03, -6.5613e-04],
        ...,
        [-3.6926e-03, -1.7166e-03, -1.9989e-03,  ...,  1.1475e-02,
         -2.4719e-03,  5.7068e-03],
        [-7.4768e-04, -1.4038e-03, -1.2665e-03,  ..., -3.5095e-04,
          2.6123e-02, -7.5150e-04],
        [ 2.8381e-03, -1.8311e-03,  8.6212e-04,  ...,  3.0518e-04,
          2.0142e-03,  2.5391e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -2.5293,   1.3506, -16.6562,  ...,  -3.0039,  -9.8047, -10.6719]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 02:32:46 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The buffalo falls into the category of bovid
The porcupine falls into the category of rodent
The jackal falls into the category of canine
The duck falls into the category of fowl
The human falls into the category of primate
The jaguar falls into the category of feline
The cat falls into the category of feline
The dog falls into the category of
2024-07-26 02:32:46 root INFO     [order_1_approx] starting weight calculation for The dog falls into the category of canine
The duck falls into the category of fowl
The porcupine falls into the category of rodent
The cat falls into the category of feline
The jackal falls into the category of canine
The jaguar falls into the category of feline
The buffalo falls into the category of bovid
The human falls into the category of
2024-07-26 02:32:47 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 02:35:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.7031, -2.9062,  6.3750,  ..., -1.8125,  0.6172,  2.2969],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 3.3125,  2.7188,  5.9375,  ...,  7.0625, 11.1875, -2.4375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.7700e-02, -5.0659e-03,  1.7395e-03,  ...,  4.0894e-03,
         -3.2349e-03,  5.9814e-03],
        [ 4.3030e-03,  1.6113e-02,  1.5106e-03,  ...,  2.4719e-03,
         -3.0670e-03,  6.1798e-04],
        [-2.9907e-03, -2.7618e-03,  6.8848e-02,  ...,  5.3406e-05,
         -9.9182e-05, -2.4719e-03],
        ...,
        [ 5.7373e-03, -3.6316e-03,  1.0498e-02,  ...,  1.7090e-02,
         -3.9978e-03,  3.2959e-03],
        [-5.0659e-03,  1.4267e-03,  3.0136e-04,  ..., -1.8082e-03,
          3.4424e-02, -5.5542e-03],
        [ 6.2180e-04, -2.8076e-03,  5.3406e-03,  ..., -6.8665e-04,
         -4.7913e-03,  2.2705e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.6602,  3.3398,  5.9141,  ...,  7.1484, 11.0312, -2.7109]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 02:35:34 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The dog falls into the category of canine
The duck falls into the category of fowl
The porcupine falls into the category of rodent
The cat falls into the category of feline
The jackal falls into the category of canine
The jaguar falls into the category of feline
The buffalo falls into the category of bovid
The human falls into the category of
2024-07-26 02:35:34 root INFO     [order_1_approx] starting weight calculation for The duck falls into the category of fowl
The porcupine falls into the category of rodent
The dog falls into the category of canine
The jackal falls into the category of canine
The jaguar falls into the category of feline
The human falls into the category of primate
The buffalo falls into the category of bovid
The cat falls into the category of
2024-07-26 02:35:34 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 02:38:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-4.1250,  0.5859, -2.6406,  ..., -1.9219,  1.3516, -2.1719],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-1.8281,  6.6562, -9.5625,  ..., -9.6250, -1.6406, -8.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.1902e-03,  1.5640e-04, -9.9182e-04,  ...,  4.7684e-05,
         -2.8610e-05,  4.5013e-04],
        [ 4.6539e-04,  1.9455e-04,  1.1749e-03,  ..., -7.4768e-04,
         -3.4142e-04, -6.2943e-05],
        [-1.8768e-03,  4.9210e-04,  8.0566e-03,  ...,  6.4850e-04,
         -1.3809e-03,  5.3787e-04],
        ...,
        [ 1.5106e-03,  8.8692e-05, -8.0109e-04,  ...,  1.0834e-03,
          1.1635e-04, -2.2030e-04],
        [-3.5858e-04,  1.5068e-04, -6.7902e-04,  ..., -1.2207e-04,
          2.8992e-03,  3.9577e-05],
        [-9.4604e-04,  1.1063e-04,  2.3079e-04,  ...,  7.1716e-04,
         -6.5613e-04,  7.7438e-04]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.9639,  6.4062, -9.7266,  ..., -9.5625, -1.6963, -9.0000]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 02:38:21 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The duck falls into the category of fowl
The porcupine falls into the category of rodent
The dog falls into the category of canine
The jackal falls into the category of canine
The jaguar falls into the category of feline
The human falls into the category of primate
The buffalo falls into the category of bovid
The cat falls into the category of
2024-07-26 02:38:21 root INFO     [order_1_approx] starting weight calculation for The jaguar falls into the category of feline
The jackal falls into the category of canine
The dog falls into the category of canine
The duck falls into the category of fowl
The human falls into the category of primate
The porcupine falls into the category of rodent
The cat falls into the category of feline
The buffalo falls into the category of
2024-07-26 02:38:21 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 02:41:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.7500, -1.2578,  1.3906,  ..., -0.8984, -2.5469,  0.3945],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 0.9805,  6.8125, -0.6719,  ..., -9.9375, -1.0469, -5.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.0874e-02, -9.1553e-04, -3.9062e-03,  ...,  9.5825e-03,
         -8.5449e-04,  8.4229e-03],
        [ 6.7749e-03,  2.0020e-02, -1.7578e-02,  ..., -2.8687e-03,
         -2.1973e-03,  2.7924e-03],
        [-2.6703e-03, -4.9438e-03,  1.0938e-01,  ..., -8.8501e-03,
          1.0559e-02, -6.4087e-03],
        ...,
        [ 5.6763e-03, -8.0109e-05,  1.9897e-02,  ...,  2.4170e-02,
         -3.8910e-04,  7.0801e-03],
        [-5.7983e-03,  6.8970e-03,  3.1433e-03,  ..., -7.0190e-03,
          5.0781e-02, -4.0283e-03],
        [ 4.5471e-03, -2.3346e-03, -9.2697e-04,  ...,  3.9368e-03,
         -9.3460e-04,  3.5645e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0713,  7.2969, -0.7788,  ..., -9.6016,  0.3662, -7.0312]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 02:41:10 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The jaguar falls into the category of feline
The jackal falls into the category of canine
The dog falls into the category of canine
The duck falls into the category of fowl
The human falls into the category of primate
The porcupine falls into the category of rodent
The cat falls into the category of feline
The buffalo falls into the category of
2024-07-26 02:41:11 root INFO     [order_1_approx] starting weight calculation for The jaguar falls into the category of feline
The jackal falls into the category of canine
The human falls into the category of primate
The buffalo falls into the category of bovid
The cat falls into the category of feline
The duck falls into the category of fowl
The dog falls into the category of canine
The porcupine falls into the category of
2024-07-26 02:41:11 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 02:43:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3203, -0.1094,  3.1875,  ..., -1.3984, -0.8984, -1.2656],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  1.6250,  -0.2031,  14.2500,  ..., -14.8125,   1.3125, -10.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0141,  0.0022, -0.0203,  ...,  0.0092, -0.0200,  0.0131],
        [ 0.0083,  0.0342, -0.0427,  ..., -0.0104,  0.0141,  0.0121],
        [-0.0020, -0.0131,  0.1465,  ..., -0.0064,  0.0225,  0.0019],
        ...,
        [ 0.0051,  0.0025, -0.0161,  ...,  0.0291, -0.0255,  0.0093],
        [-0.0099, -0.0025,  0.0151,  ..., -0.0109,  0.0703,  0.0020],
        [-0.0105, -0.0189,  0.0078,  ...,  0.0140, -0.0222,  0.0552]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  0.9033,   0.9717,  12.3438,  ..., -15.7422,  -0.1494, -12.1641]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 02:43:58 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The jaguar falls into the category of feline
The jackal falls into the category of canine
The human falls into the category of primate
The buffalo falls into the category of bovid
The cat falls into the category of feline
The duck falls into the category of fowl
The dog falls into the category of canine
The porcupine falls into the category of
2024-07-26 02:43:58 root INFO     [order_1_approx] starting weight calculation for The jaguar falls into the category of feline
The cat falls into the category of feline
The human falls into the category of primate
The porcupine falls into the category of rodent
The dog falls into the category of canine
The duck falls into the category of fowl
The buffalo falls into the category of bovid
The jackal falls into the category of
2024-07-26 02:43:59 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 02:46:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0000, -3.9062,  3.0625,  ..., -3.2031, -0.7734, -1.0859],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -9.0000,  -0.8984,  -8.8750,  ...,  -4.2500,  -8.5000, -18.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0193,  0.0014,  0.0074,  ...,  0.0040, -0.0087,  0.0117],
        [ 0.0107,  0.0239,  0.0069,  ..., -0.0059,  0.0132,  0.0129],
        [-0.0039,  0.0022,  0.1367,  ..., -0.0059,  0.0068,  0.0029],
        ...,
        [-0.0023,  0.0024, -0.0046,  ...,  0.0232, -0.0068,  0.0050],
        [-0.0079,  0.0013,  0.0060,  ..., -0.0067,  0.0601, -0.0014],
        [ 0.0053, -0.0052, -0.0288,  ...,  0.0067, -0.0005,  0.0430]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -8.1250,  -0.2329,  -8.9922,  ...,  -4.7656,  -7.8203, -18.7031]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 02:46:47 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The jaguar falls into the category of feline
The cat falls into the category of feline
The human falls into the category of primate
The porcupine falls into the category of rodent
The dog falls into the category of canine
The duck falls into the category of fowl
The buffalo falls into the category of bovid
The jackal falls into the category of
2024-07-26 02:46:47 root INFO     total operator prediction time: 1343.1249194145203 seconds
2024-07-26 02:46:47 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on hyponyms - misc
2024-07-26 02:46:47 root INFO     building operator hyponyms - misc
2024-07-26 02:46:47 root INFO     [order_1_approx] starting weight calculation for A more specific term for a color is white
A more specific term for a citrus is lemon
A more specific term for a book is paperback
A more specific term for a cutlery is knife
A more specific term for a car is limousine
A more specific term for a trousers is jeans
A more specific term for a candy is lollipop
A more specific term for a month is
2024-07-26 02:46:47 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 02:49:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.0938, -0.3770,  2.6094,  ...,  0.0078, -2.2969, -0.3984],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-2.6875, -5.0938, 11.3750,  ..., -1.2500,  4.3438, 11.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0284, -0.0067,  0.0014,  ..., -0.0045,  0.0082, -0.0036],
        [ 0.0080,  0.0171, -0.0048,  ..., -0.0009, -0.0035,  0.0017],
        [-0.0022, -0.0011,  0.0889,  ..., -0.0023, -0.0043,  0.0043],
        ...,
        [ 0.0052, -0.0038, -0.0091,  ...,  0.0157, -0.0011,  0.0029],
        [ 0.0034,  0.0018,  0.0119,  ..., -0.0049,  0.0381, -0.0013],
        [-0.0027, -0.0015, -0.0087,  ..., -0.0030,  0.0005,  0.0381]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.0391, -4.5703, 10.6250,  ..., -1.4775,  4.0078, 10.8516]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 02:49:34 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a color is white
A more specific term for a citrus is lemon
A more specific term for a book is paperback
A more specific term for a cutlery is knife
A more specific term for a car is limousine
A more specific term for a trousers is jeans
A more specific term for a candy is lollipop
A more specific term for a month is
2024-07-26 02:49:34 root INFO     [order_1_approx] starting weight calculation for A more specific term for a cutlery is knife
A more specific term for a car is limousine
A more specific term for a color is white
A more specific term for a candy is lollipop
A more specific term for a book is paperback
A more specific term for a citrus is lemon
A more specific term for a month is january
A more specific term for a trousers is
2024-07-26 02:49:35 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 02:52:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.2344, -0.8750, -7.4375,  ..., -0.6641, -4.6562, -0.0117],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-5.5312,  1.4922, -6.6250,  ..., 12.4375,  8.7500,  4.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0276,  0.0033,  0.0017,  ...,  0.0014,  0.0110, -0.0067],
        [ 0.0026,  0.0188,  0.0038,  ..., -0.0035,  0.0117,  0.0062],
        [ 0.0027,  0.0013,  0.1187,  ..., -0.0049, -0.0045, -0.0077],
        ...,
        [ 0.0046,  0.0019, -0.0138,  ...,  0.0255, -0.0008, -0.0078],
        [ 0.0011,  0.0070, -0.0004,  ..., -0.0010,  0.0432, -0.0055],
        [ 0.0074, -0.0112, -0.0344,  ..., -0.0023,  0.0052,  0.0537]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.7578,  2.5527, -7.1406,  ..., 12.7812, 10.0156,  4.3906]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 02:52:22 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a cutlery is knife
A more specific term for a car is limousine
A more specific term for a color is white
A more specific term for a candy is lollipop
A more specific term for a book is paperback
A more specific term for a citrus is lemon
A more specific term for a month is january
A more specific term for a trousers is
2024-07-26 02:52:22 root INFO     [order_1_approx] starting weight calculation for A more specific term for a candy is lollipop
A more specific term for a citrus is lemon
A more specific term for a month is january
A more specific term for a cutlery is knife
A more specific term for a book is paperback
A more specific term for a car is limousine
A more specific term for a trousers is jeans
A more specific term for a color is
2024-07-26 02:52:22 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 02:55:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.4375,  1.1406, -4.2188,  ...,  2.3125, -2.3125,  2.1094],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-8.0000, -5.0938, -0.5000,  ...,  8.8750, -6.2188, -6.1562],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0254, -0.0065, -0.0085,  ..., -0.0074,  0.0087, -0.0008],
        [ 0.0023,  0.0155,  0.0107,  ...,  0.0012,  0.0008, -0.0011],
        [ 0.0001, -0.0025,  0.0977,  ..., -0.0004, -0.0042,  0.0002],
        ...,
        [ 0.0055,  0.0022, -0.0127,  ...,  0.0208,  0.0004, -0.0046],
        [-0.0026,  0.0013,  0.0023,  ..., -0.0054,  0.0430, -0.0012],
        [-0.0019, -0.0048, -0.0062,  ...,  0.0005, -0.0051,  0.0330]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-8.4141, -4.8008, -0.7676,  ...,  7.9062, -6.0430, -5.8281]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 02:55:09 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a candy is lollipop
A more specific term for a citrus is lemon
A more specific term for a month is january
A more specific term for a cutlery is knife
A more specific term for a book is paperback
A more specific term for a car is limousine
A more specific term for a trousers is jeans
A more specific term for a color is
2024-07-26 02:55:09 root INFO     [order_1_approx] starting weight calculation for A more specific term for a color is white
A more specific term for a book is paperback
A more specific term for a citrus is lemon
A more specific term for a trousers is jeans
A more specific term for a car is limousine
A more specific term for a month is january
A more specific term for a cutlery is knife
A more specific term for a candy is
2024-07-26 02:55:10 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 02:57:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7109, -0.0957,  0.0898,  ...,  0.6914, -2.5781, -0.5977],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 2.7969, -3.3438,  2.5938,  ...,  0.0156,  9.0625,  4.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0254, -0.0063,  0.0037,  ..., -0.0042,  0.0068,  0.0112],
        [ 0.0041,  0.0208,  0.0115,  ..., -0.0019, -0.0098,  0.0018],
        [-0.0048, -0.0054,  0.1055,  ..., -0.0048,  0.0031,  0.0007],
        ...,
        [ 0.0093, -0.0066,  0.0175,  ...,  0.0221,  0.0058,  0.0006],
        [ 0.0016,  0.0033, -0.0112,  ...,  0.0018,  0.0479,  0.0039],
        [ 0.0136, -0.0031, -0.0054,  ...,  0.0010, -0.0188,  0.0303]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1250, -2.8691,  2.3281,  ...,  0.3103,  8.6172,  4.2500]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 02:57:57 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a color is white
A more specific term for a book is paperback
A more specific term for a citrus is lemon
A more specific term for a trousers is jeans
A more specific term for a car is limousine
A more specific term for a month is january
A more specific term for a cutlery is knife
A more specific term for a candy is
2024-07-26 02:57:57 root INFO     [order_1_approx] starting weight calculation for A more specific term for a candy is lollipop
A more specific term for a trousers is jeans
A more specific term for a car is limousine
A more specific term for a color is white
A more specific term for a cutlery is knife
A more specific term for a citrus is lemon
A more specific term for a month is january
A more specific term for a book is
2024-07-26 02:57:57 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 03:00:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 3.7188,  0.7344,  3.8750,  ..., -0.4141, -2.4688, -2.5781],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 0.2227, -6.5625, 30.0000,  ..., -5.6875, -9.8125,  4.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0172, -0.0035, -0.0103,  ..., -0.0059,  0.0068,  0.0001],
        [-0.0027,  0.0150,  0.0005,  ..., -0.0022,  0.0022,  0.0012],
        [-0.0095, -0.0019,  0.0947,  ..., -0.0085, -0.0190, -0.0018],
        ...,
        [-0.0019,  0.0032, -0.0107,  ...,  0.0210,  0.0315,  0.0007],
        [-0.0037,  0.0018, -0.0021,  ..., -0.0032,  0.0532, -0.0044],
        [ 0.0081, -0.0032,  0.0018,  ...,  0.0054, -0.0037,  0.0300]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  0.6279,  -5.2891,  29.1719,  ...,  -5.9688, -11.0156,   4.8203]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 03:00:43 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a candy is lollipop
A more specific term for a trousers is jeans
A more specific term for a car is limousine
A more specific term for a color is white
A more specific term for a cutlery is knife
A more specific term for a citrus is lemon
A more specific term for a month is january
A more specific term for a book is
2024-07-26 03:00:44 root INFO     [order_1_approx] starting weight calculation for A more specific term for a color is white
A more specific term for a car is limousine
A more specific term for a candy is lollipop
A more specific term for a month is january
A more specific term for a trousers is jeans
A more specific term for a cutlery is knife
A more specific term for a book is paperback
A more specific term for a citrus is
2024-07-26 03:00:44 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 03:03:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.1406, -1.9297,  5.9062,  ..., -1.2109, -5.3750, -0.4219],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-6.9375,  7.2812, 14.9375,  ..., -2.6562, -1.9688, -2.9688],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.2715e-02, -2.3804e-03,  9.1553e-03,  ..., -3.8147e-03,
          3.8605e-03,  8.3008e-03],
        [-2.5787e-03,  2.2583e-02, -1.0986e-02,  ...,  8.3923e-05,
          8.6670e-03, -1.2665e-03],
        [ 5.7373e-03,  1.9150e-03,  1.1719e-01,  ..., -1.4648e-03,
          0.0000e+00, -4.6387e-03],
        ...,
        [ 1.4496e-03,  5.2185e-03, -2.2888e-04,  ...,  2.0874e-02,
         -3.5706e-03,  4.0894e-03],
        [ 7.0190e-04,  4.3640e-03, -1.4496e-03,  ..., -9.5215e-03,
          4.5410e-02, -5.8899e-03],
        [ 2.5940e-03, -5.5237e-03,  2.7161e-03,  ...,  1.1826e-03,
         -7.3853e-03,  3.5156e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-6.4336,  8.2891, 14.0938,  ..., -2.4355, -1.4336, -4.2109]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 03:03:31 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a color is white
A more specific term for a car is limousine
A more specific term for a candy is lollipop
A more specific term for a month is january
A more specific term for a trousers is jeans
A more specific term for a cutlery is knife
A more specific term for a book is paperback
A more specific term for a citrus is
2024-07-26 03:03:31 root INFO     [order_1_approx] starting weight calculation for A more specific term for a trousers is jeans
A more specific term for a color is white
A more specific term for a candy is lollipop
A more specific term for a book is paperback
A more specific term for a citrus is lemon
A more specific term for a car is limousine
A more specific term for a month is january
A more specific term for a cutlery is
2024-07-26 03:03:31 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 03:06:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8750,  0.6797,  0.3574,  ..., -0.3828, -3.6719, -2.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 0.6094, 13.2500,  5.0938,  ..., 12.8750, 11.2500,  7.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0371,  0.0030, -0.0125,  ...,  0.0003,  0.0001, -0.0014],
        [-0.0037,  0.0203,  0.0010,  ...,  0.0013,  0.0062,  0.0039],
        [ 0.0090, -0.0010,  0.1021,  ..., -0.0027, -0.0063,  0.0026],
        ...,
        [ 0.0017,  0.0039,  0.0036,  ...,  0.0214, -0.0019,  0.0101],
        [ 0.0064, -0.0006, -0.0161,  ...,  0.0021,  0.0430,  0.0011],
        [ 0.0033,  0.0027,  0.0002,  ...,  0.0009, -0.0128,  0.0427]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.8076, 13.8516,  4.7617,  ..., 12.5391, 10.8125,  6.7344]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 03:06:18 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a trousers is jeans
A more specific term for a color is white
A more specific term for a candy is lollipop
A more specific term for a book is paperback
A more specific term for a citrus is lemon
A more specific term for a car is limousine
A more specific term for a month is january
A more specific term for a cutlery is
2024-07-26 03:06:19 root INFO     [order_1_approx] starting weight calculation for A more specific term for a trousers is jeans
A more specific term for a month is january
A more specific term for a candy is lollipop
A more specific term for a cutlery is knife
A more specific term for a citrus is lemon
A more specific term for a color is white
A more specific term for a book is paperback
A more specific term for a car is
2024-07-26 03:06:19 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 03:09:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5859,  0.4023,  1.1094,  ..., -1.7109, -2.7031,  0.5273],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-10.6250,  -0.7344,  11.1250,  ...,  -5.2500,  -4.7500,  10.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0255,  0.0020, -0.0041,  ..., -0.0005,  0.0059, -0.0011],
        [ 0.0032,  0.0116,  0.0008,  ...,  0.0010, -0.0059,  0.0024],
        [-0.0015,  0.0007,  0.0762,  ..., -0.0005, -0.0017,  0.0001],
        ...,
        [ 0.0032,  0.0067, -0.0099,  ...,  0.0145, -0.0002, -0.0085],
        [ 0.0032,  0.0016,  0.0009,  ...,  0.0022,  0.0312,  0.0026],
        [-0.0014, -0.0046,  0.0028,  ...,  0.0031,  0.0023,  0.0267]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-11.7734,  -0.1831,  10.3281,  ...,  -4.7500,  -4.0391,  10.9062]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 03:09:04 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a trousers is jeans
A more specific term for a month is january
A more specific term for a candy is lollipop
A more specific term for a cutlery is knife
A more specific term for a citrus is lemon
A more specific term for a color is white
A more specific term for a book is paperback
A more specific term for a car is
2024-07-26 03:09:04 root INFO     total operator prediction time: 1337.710153579712 seconds
2024-07-26 03:09:04 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on antonyms - binary
2024-07-26 03:09:04 root INFO     building operator antonyms - binary
2024-07-26 03:09:05 root INFO     [order_1_approx] starting weight calculation for The opposite of rise is sink
The opposite of in is out
The opposite of input is output
The opposite of down is up
The opposite of descend is ascend
The opposite of previously is subsequently
The opposite of decrement is increment
The opposite of inside is
2024-07-26 03:09:05 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 03:11:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3223, -3.4062,  1.3906,  ...,  2.2500,  0.5234, -0.9375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -1.1094, -11.3125,  15.3750,  ...,  -4.1250,  -6.3750,  -7.4375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0386, -0.0253, -0.0160,  ...,  0.0142, -0.0103,  0.0254],
        [ 0.0152,  0.0334,  0.0076,  ..., -0.0081,  0.0029, -0.0148],
        [-0.0009,  0.0320,  0.1562,  ..., -0.0106,  0.0106, -0.0312],
        ...,
        [ 0.0107,  0.0303,  0.0332,  ...,  0.0108,  0.0212, -0.0376],
        [-0.0122, -0.0119, -0.0111,  ...,  0.0117,  0.0374,  0.0155],
        [ 0.0065,  0.0264,  0.0203,  ..., -0.0125,  0.0120,  0.0171]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.8477, -9.2188, 18.6250,  ...,  2.0039, -8.4844, -4.4453]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 03:11:52 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of rise is sink
The opposite of in is out
The opposite of input is output
The opposite of down is up
The opposite of descend is ascend
The opposite of previously is subsequently
The opposite of decrement is increment
The opposite of inside is
2024-07-26 03:11:52 root INFO     [order_1_approx] starting weight calculation for The opposite of decrement is increment
The opposite of inside is outside
The opposite of previously is subsequently
The opposite of down is up
The opposite of rise is sink
The opposite of descend is ascend
The opposite of in is out
The opposite of input is
2024-07-26 03:11:52 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 03:14:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8477, -2.2188,  4.2812,  ...,  1.6250,  0.4219,  0.2656],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([14.1250, -6.2812, 15.3125,  ...,  1.1094,  3.6250, -7.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 4.6875e-02, -8.6975e-04,  7.6294e-05,  ...,  7.0801e-03,
          7.8735e-03,  2.7466e-03],
        [ 6.2256e-03,  2.2217e-02, -1.5640e-03,  ..., -2.6093e-03,
          2.1744e-04,  5.3711e-03],
        [ 3.2501e-03,  8.1635e-04,  1.4551e-01,  ..., -1.1658e-02,
         -9.8877e-03, -5.0659e-03],
        ...,
        [ 1.7242e-03, -7.6904e-03, -1.9287e-02,  ...,  3.1494e-02,
          9.1553e-03, -5.0049e-03],
        [ 1.9379e-03,  6.2866e-03,  1.5381e-02,  ..., -8.2397e-03,
          6.2500e-02,  9.5215e-03],
        [-3.0518e-05,  9.7656e-04, -7.3242e-03,  ..., -3.7003e-04,
          2.8229e-03,  5.0781e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[14.8438, -5.0156, 13.3125,  ...,  1.5107,  3.4043, -5.5664]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 03:14:39 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of decrement is increment
The opposite of inside is outside
The opposite of previously is subsequently
The opposite of down is up
The opposite of rise is sink
The opposite of descend is ascend
The opposite of in is out
The opposite of input is
2024-07-26 03:14:40 root INFO     [order_1_approx] starting weight calculation for The opposite of inside is outside
The opposite of input is output
The opposite of previously is subsequently
The opposite of decrement is increment
The opposite of in is out
The opposite of rise is sink
The opposite of down is up
The opposite of descend is
2024-07-26 03:14:40 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 03:17:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.8125, -0.6680,  3.2031,  ..., -1.9297,  0.9688, -0.5703],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 11.8125,   3.0625,   9.4375,  ...,   0.1406, -24.1250,  11.3125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0361,  0.0017, -0.0106,  ...,  0.0006, -0.0061,  0.0074],
        [-0.0018,  0.0216, -0.0041,  ...,  0.0073, -0.0032,  0.0041],
        [-0.0017,  0.0044,  0.1270,  ..., -0.0036, -0.0145, -0.0018],
        ...,
        [-0.0021, -0.0023, -0.0311,  ...,  0.0181,  0.0099, -0.0093],
        [-0.0009,  0.0039,  0.0079,  ..., -0.0045,  0.0439,  0.0029],
        [-0.0063, -0.0012, -0.0215,  ..., -0.0023, -0.0046,  0.0320]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 11.0156,   2.1094,   7.6875,  ...,   2.0078, -23.6719,  11.2969]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 03:17:26 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of inside is outside
The opposite of input is output
The opposite of previously is subsequently
The opposite of decrement is increment
The opposite of in is out
The opposite of rise is sink
The opposite of down is up
The opposite of descend is
2024-07-26 03:17:27 root INFO     [order_1_approx] starting weight calculation for The opposite of previously is subsequently
The opposite of down is up
The opposite of descend is ascend
The opposite of decrement is increment
The opposite of input is output
The opposite of inside is outside
The opposite of rise is sink
The opposite of in is
2024-07-26 03:17:27 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 03:20:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5352,  1.3281, -0.1094,  ...,  1.0625, -0.0781, -2.1562],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-5.8125, -2.3594, 14.2500,  ...,  5.5000, -4.7812, -0.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0461,  0.0050,  0.0249,  ..., -0.0074,  0.0006,  0.0012],
        [ 0.0142,  0.0206,  0.0266,  ..., -0.0182,  0.0035, -0.0156],
        [-0.0052, -0.0054,  0.1309,  ...,  0.0156, -0.0052,  0.0298],
        ...,
        [ 0.0112,  0.0129,  0.0420,  ..., -0.0010,  0.0026, -0.0496],
        [-0.0049,  0.0065, -0.0233,  ...,  0.0105,  0.0522,  0.0217],
        [-0.0082, -0.0019, -0.0181,  ...,  0.0092,  0.0009,  0.0645]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.7188, -1.5537, 15.1406,  ...,  5.9531, -4.4766, -0.1335]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 03:20:14 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of previously is subsequently
The opposite of down is up
The opposite of descend is ascend
The opposite of decrement is increment
The opposite of input is output
The opposite of inside is outside
The opposite of rise is sink
The opposite of in is
2024-07-26 03:20:14 root INFO     [order_1_approx] starting weight calculation for The opposite of descend is ascend
The opposite of input is output
The opposite of inside is outside
The opposite of in is out
The opposite of down is up
The opposite of decrement is increment
The opposite of rise is sink
The opposite of previously is
2024-07-26 03:20:14 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 03:22:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.4062, -0.2852,  1.5625,  ...,  1.7188, -0.7812, -2.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  3.6875, -15.3125,  11.8125,  ...,  16.7500,   5.9062,   8.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0371,  0.0037,  0.0125,  ..., -0.0054,  0.0033,  0.0025],
        [-0.0013,  0.0161, -0.0120,  ...,  0.0035,  0.0060, -0.0002],
        [-0.0007, -0.0045,  0.1602,  ..., -0.0073, -0.0118, -0.0025],
        ...,
        [-0.0025, -0.0013, -0.0031,  ...,  0.0253,  0.0007, -0.0096],
        [ 0.0027,  0.0035,  0.0129,  ...,  0.0027,  0.0503,  0.0053],
        [-0.0048,  0.0022, -0.0107,  ..., -0.0003, -0.0089,  0.0522]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  4.2812, -13.5000,  11.8672,  ...,  17.8438,   4.1992,   9.1328]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 03:23:00 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of descend is ascend
The opposite of input is output
The opposite of inside is outside
The opposite of in is out
The opposite of down is up
The opposite of decrement is increment
The opposite of rise is sink
The opposite of previously is
2024-07-26 03:23:00 root INFO     [order_1_approx] starting weight calculation for The opposite of rise is sink
The opposite of previously is subsequently
The opposite of descend is ascend
The opposite of in is out
The opposite of decrement is increment
The opposite of input is output
The opposite of inside is outside
The opposite of down is
2024-07-26 03:23:01 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 03:25:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.7031, -2.1562,  1.5938,  ..., -1.2031,  0.4336, -2.4688],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 11.7500,  -0.8438,   9.3750,  ...,   0.4688, -13.8750,  -7.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0347,  0.0010,  0.0156,  ..., -0.0057,  0.0078,  0.0024],
        [ 0.0033,  0.0134, -0.0020,  ...,  0.0052, -0.0028,  0.0083],
        [ 0.0029,  0.0056,  0.1348,  ..., -0.0096,  0.0056, -0.0029],
        ...,
        [ 0.0048,  0.0043, -0.0128,  ...,  0.0271, -0.0029, -0.0053],
        [-0.0045,  0.0076,  0.0054,  ...,  0.0007,  0.0388,  0.0081],
        [-0.0027, -0.0046, -0.0056,  ...,  0.0062,  0.0019,  0.0415]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 12.6484,  -0.9897,   8.6641,  ...,   0.1292, -15.1328,  -6.5625]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 03:25:46 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of rise is sink
The opposite of previously is subsequently
The opposite of descend is ascend
The opposite of in is out
The opposite of decrement is increment
The opposite of input is output
The opposite of inside is outside
The opposite of down is
2024-07-26 03:25:47 root INFO     [order_1_approx] starting weight calculation for The opposite of in is out
The opposite of inside is outside
The opposite of input is output
The opposite of previously is subsequently
The opposite of down is up
The opposite of rise is sink
The opposite of descend is ascend
The opposite of decrement is
2024-07-26 03:25:47 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 03:28:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.1406, -0.9961,  4.7812,  ..., -1.3984, -0.0078,  0.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  5.7500,   2.5000,  -2.6875,  ...,   3.8125, -17.5000,  11.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.9307e-02,  1.5259e-05,  9.5825e-03,  ..., -3.4790e-03,
         -5.3711e-03,  6.1646e-03],
        [ 6.5002e-03,  2.5879e-02,  1.6724e-02,  ...,  4.1504e-03,
         -6.9580e-03,  2.2125e-03],
        [ 5.2185e-03, -5.8746e-04,  1.4746e-01,  ..., -1.9043e-02,
          4.8828e-03,  1.1230e-02],
        ...,
        [ 1.4038e-03, -6.8970e-03, -1.4404e-02,  ...,  3.4180e-02,
         -9.7046e-03, -1.3611e-02],
        [-2.7466e-03,  2.9297e-03,  8.4839e-03,  ..., -5.3101e-03,
          6.5430e-02,  9.7656e-03],
        [-6.7444e-03,  2.5330e-03, -2.0874e-02,  ..., -4.0894e-03,
          5.7983e-04,  6.0547e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  5.3438,   6.5195,  -3.3613,  ...,   4.1797, -18.7031,  10.8438]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 03:28:33 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of in is out
The opposite of inside is outside
The opposite of input is output
The opposite of previously is subsequently
The opposite of down is up
The opposite of rise is sink
The opposite of descend is ascend
The opposite of decrement is
2024-07-26 03:28:33 root INFO     [order_1_approx] starting weight calculation for The opposite of input is output
The opposite of inside is outside
The opposite of in is out
The opposite of previously is subsequently
The opposite of down is up
The opposite of decrement is increment
The opposite of descend is ascend
The opposite of rise is
2024-07-26 03:28:33 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 03:31:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3320,  0.2949,  0.6719,  ...,  0.8438, -0.7266,  1.3672],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([12.2500,  1.3125, -6.1250,  ...,  0.6641, -7.0625, 12.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.9795e-02,  5.8594e-03, -5.0964e-03,  ...,  2.7924e-03,
          4.5166e-03,  3.7537e-03],
        [ 7.2021e-03,  2.4414e-02, -1.2024e-02,  ...,  2.7008e-03,
         -1.3672e-02,  7.8125e-03],
        [-4.8828e-04,  1.8005e-03,  1.6406e-01,  ..., -6.5002e-03,
         -4.5776e-05, -9.9182e-04],
        ...,
        [ 2.9449e-03, -2.5177e-04,  1.3885e-03,  ...,  3.4668e-02,
          4.3945e-03, -1.3428e-02],
        [-7.1106e-03, -3.1128e-03,  1.6785e-03,  ..., -3.7994e-03,
          6.1035e-02,  2.4658e-02],
        [ 5.5542e-03, -4.5013e-04, -1.5747e-02,  ..., -1.8463e-03,
          3.7231e-03,  5.1270e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[11.5859,  0.1934, -7.7227,  ...,  0.8706, -5.9883, 11.4766]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 03:31:19 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of input is output
The opposite of inside is outside
The opposite of in is out
The opposite of previously is subsequently
The opposite of down is up
The opposite of decrement is increment
The opposite of descend is ascend
The opposite of rise is
2024-07-26 03:31:19 root INFO     total operator prediction time: 1335.1050772666931 seconds
2024-07-26 03:31:19 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on meronyms - member
2024-07-26 03:31:19 root INFO     building operator meronyms - member
2024-07-26 03:31:20 root INFO     [order_1_approx] starting weight calculation for A letter is a member of a alphabet
A christian is a member of a congregation
A player is a member of a team
A lion is a member of a pride
A galaxy is a member of a universe
A word is a member of a paragraph
A cattle is a member of a herd
A parishioner is a member of a
2024-07-26 03:31:20 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 03:34:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4688, -1.7734,  3.8281,  ..., -2.5938, -1.3828,  1.7109],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  7.3125,   3.1094,   0.1875,  ..., -13.3125,   7.7812,  -4.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0153, -0.0005,  0.0021,  ...,  0.0046,  0.0025,  0.0011],
        [ 0.0027,  0.0109, -0.0045,  ...,  0.0016,  0.0023,  0.0019],
        [ 0.0002, -0.0032,  0.0520,  ..., -0.0009, -0.0035,  0.0016],
        ...,
        [ 0.0039, -0.0019, -0.0075,  ...,  0.0114,  0.0009,  0.0018],
        [-0.0010,  0.0027,  0.0071,  ..., -0.0031,  0.0205, -0.0006],
        [-0.0019, -0.0042,  0.0035,  ..., -0.0021,  0.0025,  0.0222]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  8.0547,   2.6309,   0.0479,  ..., -12.9766,   8.0938,  -4.1289]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 03:34:08 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A letter is a member of a alphabet
A christian is a member of a congregation
A player is a member of a team
A lion is a member of a pride
A galaxy is a member of a universe
A word is a member of a paragraph
A cattle is a member of a herd
A parishioner is a member of a
2024-07-26 03:34:08 root INFO     [order_1_approx] starting weight calculation for A christian is a member of a congregation
A player is a member of a team
A parishioner is a member of a parish
A cattle is a member of a herd
A letter is a member of a alphabet
A lion is a member of a pride
A galaxy is a member of a universe
A word is a member of a
2024-07-26 03:34:09 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 03:36:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.4688, -0.5273,  2.0625,  ..., -0.7734, -3.5469, -0.1406],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  9.7500, -18.7500,   6.2812,  ...,  -8.5625,   2.1875,  -3.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0248,  0.0016,  0.0043,  ...,  0.0038, -0.0031, -0.0052],
        [ 0.0066,  0.0114, -0.0024,  ...,  0.0047, -0.0065, -0.0068],
        [-0.0006, -0.0075,  0.1133,  ...,  0.0005,  0.0085,  0.0068],
        ...,
        [ 0.0024,  0.0013, -0.0189,  ...,  0.0171,  0.0049, -0.0015],
        [ 0.0051,  0.0011,  0.0110,  ..., -0.0044,  0.0396,  0.0033],
        [ 0.0007,  0.0002,  0.0159,  ...,  0.0015,  0.0029,  0.0386]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  7.4570, -17.7500,   4.6719,  ...,  -9.8594,   3.3613,  -2.9531]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 03:36:57 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A christian is a member of a congregation
A player is a member of a team
A parishioner is a member of a parish
A cattle is a member of a herd
A letter is a member of a alphabet
A lion is a member of a pride
A galaxy is a member of a universe
A word is a member of a
2024-07-26 03:36:58 root INFO     [order_1_approx] starting weight calculation for A lion is a member of a pride
A letter is a member of a alphabet
A galaxy is a member of a universe
A word is a member of a paragraph
A parishioner is a member of a parish
A christian is a member of a congregation
A player is a member of a team
A cattle is a member of a
2024-07-26 03:36:58 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 03:39:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 3.4375, -0.3438,  0.6211,  ..., -2.0938, -6.6250, -0.6719],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([18.7500,  5.1875,  4.3750,  ..., -7.1875, -2.3125, -6.4062],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0242,  0.0042, -0.0076,  ...,  0.0026, -0.0006,  0.0015],
        [-0.0028,  0.0219, -0.0222,  ...,  0.0005, -0.0016, -0.0046],
        [ 0.0045, -0.0038,  0.0933,  ..., -0.0042,  0.0010, -0.0032],
        ...,
        [ 0.0041, -0.0020, -0.0048,  ...,  0.0259, -0.0035,  0.0065],
        [-0.0070,  0.0088,  0.0109,  ..., -0.0045,  0.0410,  0.0050],
        [ 0.0020, -0.0015, -0.0123,  ...,  0.0004,  0.0007,  0.0449]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[16.5156,  3.9180,  3.3164,  ..., -7.4023, -1.5830, -6.5586]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 03:39:46 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A lion is a member of a pride
A letter is a member of a alphabet
A galaxy is a member of a universe
A word is a member of a paragraph
A parishioner is a member of a parish
A christian is a member of a congregation
A player is a member of a team
A cattle is a member of a
2024-07-26 03:39:46 root INFO     [order_1_approx] starting weight calculation for A parishioner is a member of a parish
A letter is a member of a alphabet
A cattle is a member of a herd
A galaxy is a member of a universe
A word is a member of a paragraph
A lion is a member of a pride
A player is a member of a team
A christian is a member of a
2024-07-26 03:39:46 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 03:42:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8711, -0.3984,  5.8438,  ..., -1.7656, -3.4531, -1.3438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-6.0625, -1.6094,  6.1250,  ..., -7.4062,  4.6875, -1.1484],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.5879e-02,  4.4556e-03,  1.8845e-03,  ...,  2.0142e-03,
          6.2561e-03, -1.5259e-03],
        [ 2.1973e-03,  2.2217e-02,  8.6594e-04,  ...,  3.0212e-03,
          2.9449e-03,  2.9297e-03],
        [-2.1973e-03, -5.6763e-03,  8.0566e-02,  ..., -3.5286e-04,
         -6.2256e-03,  1.7762e-05],
        ...,
        [ 5.8594e-03, -4.5776e-03, -4.5166e-03,  ...,  1.6357e-02,
         -1.9455e-03, -8.1635e-04],
        [ 2.4414e-04,  5.9204e-03,  1.7944e-02,  ..., -9.6436e-03,
          3.9062e-02,  2.9449e-03],
        [-3.7231e-03, -2.0599e-03,  6.2256e-03,  ..., -2.6245e-03,
          1.2207e-04,  3.6133e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.8672, -1.6855,  4.7148,  ..., -7.8984,  3.7266, -1.5498]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 03:42:32 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A parishioner is a member of a parish
A letter is a member of a alphabet
A cattle is a member of a herd
A galaxy is a member of a universe
A word is a member of a paragraph
A lion is a member of a pride
A player is a member of a team
A christian is a member of a
2024-07-26 03:42:32 root INFO     [order_1_approx] starting weight calculation for A christian is a member of a congregation
A parishioner is a member of a parish
A cattle is a member of a herd
A lion is a member of a pride
A player is a member of a team
A galaxy is a member of a universe
A word is a member of a paragraph
A letter is a member of a
2024-07-26 03:42:32 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 03:45:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.7500, -2.0469,  6.2812,  ..., -3.0469, -4.1250,  1.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 11.1250,  -9.5625,   7.7500,  ..., -14.1250,   4.8438,  -2.0938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0283, -0.0036,  0.0061,  ...,  0.0023,  0.0058, -0.0066],
        [-0.0005,  0.0132, -0.0083,  ...,  0.0012, -0.0045,  0.0029],
        [-0.0050, -0.0058,  0.1348,  ...,  0.0028,  0.0131,  0.0026],
        ...,
        [ 0.0005,  0.0010, -0.0118,  ...,  0.0239,  0.0085, -0.0042],
        [ 0.0040, -0.0003,  0.0040,  ..., -0.0056,  0.0400, -0.0018],
        [ 0.0042, -0.0054,  0.0038,  ...,  0.0010, -0.0025,  0.0439]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 11.1797, -11.0547,   6.1406,  ..., -14.3203,   3.9062,  -1.9834]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 03:45:22 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A christian is a member of a congregation
A parishioner is a member of a parish
A cattle is a member of a herd
A lion is a member of a pride
A player is a member of a team
A galaxy is a member of a universe
A word is a member of a paragraph
A letter is a member of a
2024-07-26 03:45:22 root INFO     [order_1_approx] starting weight calculation for A parishioner is a member of a parish
A word is a member of a paragraph
A cattle is a member of a herd
A galaxy is a member of a universe
A player is a member of a team
A christian is a member of a congregation
A letter is a member of a alphabet
A lion is a member of a
2024-07-26 03:45:22 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 03:48:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.0625, -1.6875, -0.5898,  ..., -5.4375, -4.4062,  1.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -4.8438,  -3.3906,   2.8750,  ..., -22.0000,   9.8750,  -2.9219],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0161,  0.0067,  0.0087,  ..., -0.0003, -0.0025, -0.0006],
        [-0.0015,  0.0159, -0.0026,  ..., -0.0053,  0.0073, -0.0089],
        [-0.0057, -0.0073,  0.1006,  ..., -0.0027, -0.0060,  0.0081],
        ...,
        [ 0.0079,  0.0068, -0.0109,  ...,  0.0239,  0.0070,  0.0069],
        [-0.0027, -0.0010,  0.0157,  ..., -0.0101,  0.0471, -0.0088],
        [ 0.0043, -0.0065,  0.0028,  ..., -0.0002,  0.0071,  0.0359]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -5.6289,  -3.5430,   3.7930,  ..., -22.3750,   7.2891,  -2.9297]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 03:48:11 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A parishioner is a member of a parish
A word is a member of a paragraph
A cattle is a member of a herd
A galaxy is a member of a universe
A player is a member of a team
A christian is a member of a congregation
A letter is a member of a alphabet
A lion is a member of a
2024-07-26 03:48:11 root INFO     [order_1_approx] starting weight calculation for A galaxy is a member of a universe
A christian is a member of a congregation
A parishioner is a member of a parish
A letter is a member of a alphabet
A word is a member of a paragraph
A lion is a member of a pride
A cattle is a member of a herd
A player is a member of a
2024-07-26 03:48:11 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 03:51:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5469, -0.5586,  4.8750,  ..., -1.8594, -3.1562,  1.8594],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 2.5625, 15.6875,  5.7188,  ..., -8.2500,  1.4688,  5.7188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.6855e-02,  2.3041e-03,  9.0332e-03,  ...,  3.3875e-03,
          2.2430e-03,  1.4801e-03],
        [-4.7302e-04,  1.3428e-02,  1.1780e-02,  ...,  2.1057e-03,
         -1.5259e-05,  2.0447e-03],
        [-2.2507e-04, -5.6152e-03,  1.1670e-01,  ...,  1.0376e-03,
          3.1281e-03, -1.0071e-03],
        ...,
        [ 9.1553e-03,  3.2806e-03, -5.4932e-03,  ...,  2.0142e-02,
         -1.6022e-03, -2.2125e-03],
        [-2.0447e-03, -3.3875e-03,  4.1504e-03,  ..., -2.1667e-03,
          4.4678e-02,  3.2043e-04],
        [-8.3160e-04, -2.5482e-03,  3.1738e-03,  ..., -8.4686e-04,
          5.0049e-03,  4.6875e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.9062, 14.7734,  4.8320,  ..., -8.3047,  1.2666,  5.5625]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 03:51:00 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A galaxy is a member of a universe
A christian is a member of a congregation
A parishioner is a member of a parish
A letter is a member of a alphabet
A word is a member of a paragraph
A lion is a member of a pride
A cattle is a member of a herd
A player is a member of a
2024-07-26 03:51:00 root INFO     [order_1_approx] starting weight calculation for A word is a member of a paragraph
A parishioner is a member of a parish
A player is a member of a team
A letter is a member of a alphabet
A lion is a member of a pride
A christian is a member of a congregation
A cattle is a member of a herd
A galaxy is a member of a
2024-07-26 03:51:01 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 03:53:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5781, -2.3438, -3.2812,  ..., -4.0625, -7.1562, -0.9805],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -4.4375,  -7.1562, -11.7500,  ..., -14.2500,   7.0312,   1.9922],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.3438e-02, -1.0681e-04, -3.5400e-03,  ...,  4.9133e-03,
          2.8076e-03,  1.2573e-02],
        [ 3.6469e-03,  1.4160e-02,  1.9836e-03,  ..., -2.5482e-03,
          7.6294e-05, -8.2397e-03],
        [-6.5613e-03, -1.8005e-03,  7.9102e-02,  ..., -3.1433e-03,
          6.2256e-03, -3.5858e-03],
        ...,
        [-2.8687e-03,  5.2795e-03, -6.5918e-03,  ...,  2.0752e-02,
         -3.4180e-03,  4.3335e-03],
        [ 2.6703e-04,  5.1880e-03, -9.0942e-03,  ...,  4.0283e-03,
          1.8677e-02,  3.2654e-03],
        [-3.8147e-04, -1.7014e-03,  5.7983e-04,  ...,  5.0354e-04,
          2.7924e-03,  3.3936e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -4.1953,  -6.7617, -10.8594,  ..., -13.2266,   6.6094,   1.0615]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 03:53:50 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A word is a member of a paragraph
A parishioner is a member of a parish
A player is a member of a team
A letter is a member of a alphabet
A lion is a member of a pride
A christian is a member of a congregation
A cattle is a member of a herd
A galaxy is a member of a
2024-07-26 03:53:50 root INFO     total operator prediction time: 1350.5139648914337 seconds
2024-07-26 03:53:50 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on noun - plural_irreg
2024-07-26 03:53:50 root INFO     building operator noun - plural_irreg
2024-07-26 03:53:50 root INFO     [order_1_approx] starting weight calculation for The plural form of industry is industries
The plural form of society is societies
The plural form of facility is facilities
The plural form of economy is economies
The plural form of datum is data
The plural form of majority is majorities
The plural form of story is stories
The plural form of species is
2024-07-26 03:53:50 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 03:56:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0391, -5.1250, -6.7812,  ..., -1.2812,  0.3828, -0.5898],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 1.6375e+01, -1.1750e+01, -8.5000e+00,  ...,  1.5625e-02,
         2.8750e+00, -6.1875e+00], device='cuda:0', dtype=torch.float16,
       grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0352, -0.0016,  0.0176,  ...,  0.0085, -0.0002, -0.0097],
        [-0.0070,  0.0272,  0.0095,  ..., -0.0081, -0.0042,  0.0049],
        [ 0.0063,  0.0058,  0.1089,  ...,  0.0037,  0.0068, -0.0018],
        ...,
        [ 0.0030,  0.0024,  0.0070,  ...,  0.0244, -0.0029, -0.0007],
        [ 0.0017, -0.0039, -0.0014,  ...,  0.0085,  0.0354,  0.0004],
        [ 0.0015, -0.0049, -0.0070,  ..., -0.0032,  0.0011,  0.0374]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 13.1094, -11.1406,  -7.0781,  ...,  -0.4155,   1.6182,  -7.6250]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 03:56:38 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of industry is industries
The plural form of society is societies
The plural form of facility is facilities
The plural form of economy is economies
The plural form of datum is data
The plural form of majority is majorities
The plural form of story is stories
The plural form of species is
2024-07-26 03:56:38 root INFO     [order_1_approx] starting weight calculation for The plural form of species is species
The plural form of society is societies
The plural form of economy is economies
The plural form of facility is facilities
The plural form of majority is majorities
The plural form of story is stories
The plural form of datum is data
The plural form of industry is
2024-07-26 03:56:38 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 03:59:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1562, -2.4219, -2.0312,  ..., -1.0938, -1.3281,  0.1416],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 7.4375,  0.7656, -1.6562,  ..., -3.7656, -0.9180, -5.7812],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.9062e-02, -2.0752e-03,  5.2490e-03,  ...,  4.0894e-03,
          5.6458e-04,  4.6997e-03],
        [ 2.7466e-03,  2.2827e-02,  8.4229e-03,  ..., -8.6060e-03,
         -6.1035e-03,  5.6458e-03],
        [ 2.8992e-03,  3.4943e-03,  1.1230e-01,  ..., -1.9989e-03,
          8.1787e-03, -5.4932e-03],
        ...,
        [ 5.0049e-03, -8.3618e-03, -9.9182e-05,  ...,  3.0151e-02,
         -1.0315e-02, -5.8899e-03],
        [ 1.4191e-03,  3.6469e-03,  3.0518e-04,  ..., -3.9291e-04,
          4.5654e-02, -3.1738e-03],
        [-2.4109e-03, -7.7820e-04, -1.4038e-02,  ..., -7.8125e-03,
          3.7842e-03,  4.6631e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 7.5898,  1.5586, -0.6270,  ..., -3.9746,  0.1416, -6.0000]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 03:59:25 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of species is species
The plural form of society is societies
The plural form of economy is economies
The plural form of facility is facilities
The plural form of majority is majorities
The plural form of story is stories
The plural form of datum is data
The plural form of industry is
2024-07-26 03:59:25 root INFO     [order_1_approx] starting weight calculation for The plural form of society is societies
The plural form of species is species
The plural form of economy is economies
The plural form of story is stories
The plural form of majority is majorities
The plural form of datum is data
The plural form of industry is industries
The plural form of facility is
2024-07-26 03:59:25 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 04:02:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.8828, -2.3438, -0.0137,  ...,  0.2227, -1.1094,  0.7969],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 1.6484, -7.1875, -5.1562,  ...,  2.0469,  1.8125, -2.5312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 5.2734e-02,  1.0925e-02, -3.0518e-03,  ...,  2.0630e-02,
         -4.6997e-03, -1.8311e-04],
        [-1.2207e-04,  1.5137e-02,  2.5635e-03,  ..., -2.0020e-02,
         -5.5542e-03,  1.2390e-02],
        [-9.1553e-03, -1.3000e-02,  1.3379e-01,  ..., -1.3916e-02,
          9.9182e-04, -2.5330e-03],
        ...,
        [ 1.4404e-02,  1.1292e-03,  7.3547e-03,  ...,  3.9795e-02,
         -5.0659e-03, -5.9814e-03],
        [-3.7384e-04,  3.8300e-03,  3.8452e-03,  ...,  1.3809e-03,
          4.9316e-02,  1.0300e-04],
        [-5.7373e-03, -3.9673e-03, -1.1292e-02,  ..., -9.0332e-03,
          7.5073e-03,  6.5430e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.8662, -8.8984, -8.5312,  ...,  4.7031,  2.3242, -3.9375]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 04:02:12 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of society is societies
The plural form of species is species
The plural form of economy is economies
The plural form of story is stories
The plural form of majority is majorities
The plural form of datum is data
The plural form of industry is industries
The plural form of facility is
2024-07-26 04:02:12 root INFO     [order_1_approx] starting weight calculation for The plural form of majority is majorities
The plural form of datum is data
The plural form of story is stories
The plural form of industry is industries
The plural form of economy is economies
The plural form of facility is facilities
The plural form of species is species
The plural form of society is
2024-07-26 04:02:12 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 04:04:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0156, -2.5938, -0.6250,  ..., -0.2119, -0.8711, -1.6562],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 8.1250, -4.2188, -6.8125,  ..., -7.0312, -1.8906, -9.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0356, -0.0002,  0.0057,  ...,  0.0071,  0.0088, -0.0045],
        [ 0.0070,  0.0177,  0.0114,  ..., -0.0045,  0.0008, -0.0002],
        [-0.0009,  0.0020,  0.1094,  ..., -0.0022,  0.0005, -0.0052],
        ...,
        [ 0.0064, -0.0012,  0.0059,  ...,  0.0189, -0.0122, -0.0015],
        [-0.0012,  0.0046,  0.0046,  ..., -0.0013,  0.0483,  0.0018],
        [-0.0011, -0.0037, -0.0130,  ..., -0.0042,  0.0012,  0.0486]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.1562, -4.7031, -6.0078,  ..., -7.3906, -0.5381, -8.2656]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 04:04:59 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of majority is majorities
The plural form of datum is data
The plural form of story is stories
The plural form of industry is industries
The plural form of economy is economies
The plural form of facility is facilities
The plural form of species is species
The plural form of society is
2024-07-26 04:04:59 root INFO     [order_1_approx] starting weight calculation for The plural form of datum is data
The plural form of facility is facilities
The plural form of species is species
The plural form of majority is majorities
The plural form of industry is industries
The plural form of story is stories
The plural form of society is societies
The plural form of economy is
2024-07-26 04:04:59 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 04:07:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9062, -3.4219,  1.9922,  ..., -0.4844, -0.7578, -0.2773],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([11.3125, -0.2500,  1.9688,  ...,  0.6172,  1.7812, -1.8125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.9541e-02, -2.6398e-03,  1.4160e-02,  ...,  5.5237e-03,
          8.3008e-03, -4.1504e-03],
        [ 4.6997e-03,  2.0630e-02, -5.6152e-03,  ...,  3.1853e-04,
         -5.9814e-03, -3.0060e-03],
        [-5.7373e-03,  8.7891e-03,  9.8633e-02,  ..., -2.1057e-03,
          1.4648e-03, -8.9722e-03],
        ...,
        [ 2.8687e-03, -5.4321e-03,  7.1411e-03,  ...,  2.5391e-02,
         -1.1749e-03, -2.8687e-03],
        [-2.0599e-03,  7.9727e-04,  7.3242e-03,  ...,  2.3041e-03,
          3.5645e-02,  9.1553e-04],
        [ 7.6294e-06,  3.0060e-03, -2.8381e-03,  ..., -5.8594e-03,
          9.3384e-03,  3.7842e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[12.0078,  1.2949,  0.2803,  ...,  0.0229,  2.3164, -2.8398]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 04:07:46 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of datum is data
The plural form of facility is facilities
The plural form of species is species
The plural form of majority is majorities
The plural form of industry is industries
The plural form of story is stories
The plural form of society is societies
The plural form of economy is
2024-07-26 04:07:46 root INFO     [order_1_approx] starting weight calculation for The plural form of datum is data
The plural form of species is species
The plural form of economy is economies
The plural form of society is societies
The plural form of facility is facilities
The plural form of story is stories
The plural form of industry is industries
The plural form of majority is
2024-07-26 04:07:46 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 04:10:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9258, -2.8438, -1.2969,  ...,  0.0781, -1.0547, -0.7930],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([11.0000,  5.9062, -6.1875,  ..., -3.6562,  3.9688, -2.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0330,  0.0074,  0.0121,  ...,  0.0065, -0.0007, -0.0036],
        [ 0.0103,  0.0145,  0.0216,  ..., -0.0043,  0.0042,  0.0052],
        [ 0.0004,  0.0038,  0.1089,  ..., -0.0058,  0.0152, -0.0015],
        ...,
        [-0.0011, -0.0044,  0.0043,  ...,  0.0242, -0.0075, -0.0022],
        [-0.0034,  0.0020,  0.0092,  ..., -0.0034,  0.0393,  0.0003],
        [ 0.0045,  0.0054, -0.0131,  ..., -0.0063,  0.0014,  0.0469]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[13.4531,  5.4531, -6.2422,  ..., -4.0234,  5.2148, -2.9727]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 04:10:35 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of datum is data
The plural form of species is species
The plural form of economy is economies
The plural form of society is societies
The plural form of facility is facilities
The plural form of story is stories
The plural form of industry is industries
The plural form of majority is
2024-07-26 04:10:36 root INFO     [order_1_approx] starting weight calculation for The plural form of facility is facilities
The plural form of majority is majorities
The plural form of datum is data
The plural form of species is species
The plural form of economy is economies
The plural form of industry is industries
The plural form of society is societies
The plural form of story is
2024-07-26 04:10:36 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 04:13:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4062, -1.3125,  0.6094,  ...,  0.1016, -0.3438, -1.0312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 11.0000,   0.0859,   3.4844,  ...,  -4.6250,   4.2500, -16.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0466,  0.0058,  0.0046,  ...,  0.0057,  0.0026, -0.0005],
        [ 0.0068,  0.0247,  0.0150,  ..., -0.0121, -0.0099,  0.0041],
        [ 0.0082,  0.0022,  0.1455,  ..., -0.0037,  0.0018,  0.0012],
        ...,
        [ 0.0088, -0.0122, -0.0019,  ...,  0.0393, -0.0099, -0.0098],
        [-0.0044,  0.0059,  0.0112,  ..., -0.0004,  0.0569, -0.0040],
        [-0.0060, -0.0062, -0.0061,  ..., -0.0082,  0.0173,  0.0493]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  9.4062,   1.3105,   4.5977,  ...,  -6.0703,   5.4219, -17.4062]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 04:13:23 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of facility is facilities
The plural form of majority is majorities
The plural form of datum is data
The plural form of species is species
The plural form of economy is economies
The plural form of industry is industries
The plural form of society is societies
The plural form of story is
2024-07-26 04:13:23 root INFO     [order_1_approx] starting weight calculation for The plural form of facility is facilities
The plural form of majority is majorities
The plural form of story is stories
The plural form of economy is economies
The plural form of industry is industries
The plural form of species is species
The plural form of society is societies
The plural form of datum is
2024-07-26 04:13:23 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 04:16:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.2500, -2.0312,  0.0840,  ..., -1.8672,  0.2490,  2.0469],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-4.5000,  1.3750, 10.4375,  ..., -2.7031, 11.6250,  4.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0447, -0.0076, -0.0291,  ...,  0.0092, -0.0015, -0.0067],
        [ 0.0100,  0.0342,  0.0391,  ..., -0.0148, -0.0148,  0.0155],
        [ 0.0046, -0.0056,  0.1787,  ...,  0.0021,  0.0074,  0.0109],
        ...,
        [ 0.0112, -0.0123,  0.0009,  ...,  0.0435,  0.0094,  0.0017],
        [-0.0120,  0.0111, -0.0277,  ..., -0.0052,  0.0481,  0.0112],
        [ 0.0049,  0.0062, -0.0040,  ..., -0.0093, -0.0005,  0.0500]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-6.0156,  1.9609, 12.1172,  ..., -2.2090, 12.1875,  2.4746]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 04:16:10 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of facility is facilities
The plural form of majority is majorities
The plural form of story is stories
The plural form of economy is economies
The plural form of industry is industries
The plural form of species is species
The plural form of society is societies
The plural form of datum is
2024-07-26 04:16:10 root INFO     total operator prediction time: 1340.399436712265 seconds
2024-07-26 04:16:10 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on Ving - verb_inf
2024-07-26 04:16:10 root INFO     building operator Ving - verb_inf
2024-07-26 04:16:11 root INFO     [order_1_approx] starting weight calculation for enjoying is the active form of enjoy
developing is the active form of develop
improving is the active form of improve
allowing is the active form of allow
understanding is the active form of understand
believing is the active form of believe
seeming is the active form of seem
establishing is the active form of
2024-07-26 04:16:11 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 04:18:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.9609, -0.3359,  2.1875,  ...,  0.8750,  0.9219,  0.1758],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 13.4375,  -8.7500,  -1.5391,  ..., -14.5000,  -1.1172,   7.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.5259e-02,  4.5776e-03, -3.4637e-03,  ...,  1.8768e-03,
          5.5542e-03,  1.1063e-04],
        [ 1.5488e-03,  1.6357e-02,  7.5684e-03,  ...,  3.5095e-04,
          6.4087e-04,  3.0518e-05],
        [ 3.3417e-03,  5.6458e-04,  7.4707e-02,  ..., -2.5940e-03,
         -5.2185e-03,  4.4556e-03],
        ...,
        [ 4.5013e-04,  4.8523e-03, -1.5182e-03,  ...,  1.4282e-02,
         -2.5940e-03, -9.1553e-04],
        [ 7.1411e-03, -3.9368e-03, -4.1809e-03,  ..., -4.4250e-03,
          2.6367e-02,  7.7820e-04],
        [-6.1035e-03,  5.4016e-03, -9.1553e-04,  ..., -1.2054e-03,
         -6.8665e-05,  2.9541e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 11.9922,  -9.0156,  -1.7969,  ..., -13.5234,  -0.7925,   7.9570]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 04:18:57 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for enjoying is the active form of enjoy
developing is the active form of develop
improving is the active form of improve
allowing is the active form of allow
understanding is the active form of understand
believing is the active form of believe
seeming is the active form of seem
establishing is the active form of
2024-07-26 04:18:57 root INFO     [order_1_approx] starting weight calculation for developing is the active form of develop
allowing is the active form of allow
improving is the active form of improve
seeming is the active form of seem
establishing is the active form of establish
understanding is the active form of understand
enjoying is the active form of enjoy
believing is the active form of
2024-07-26 04:18:57 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 04:21:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2168,  1.6562,  0.7461,  ..., -1.3516,  5.1250,  0.7266],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 23.0000,  -2.4688,   6.9375,  ..., -18.0000,  -3.9844,  -1.3906],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0085, -0.0010,  0.0022,  ...,  0.0006, -0.0027,  0.0032],
        [-0.0012,  0.0079, -0.0011,  ..., -0.0025, -0.0048,  0.0009],
        [-0.0004,  0.0012,  0.0393,  ..., -0.0011, -0.0021, -0.0024],
        ...,
        [ 0.0030,  0.0016, -0.0047,  ...,  0.0095,  0.0031, -0.0035],
        [ 0.0020,  0.0018, -0.0020,  ...,  0.0011,  0.0187,  0.0001],
        [-0.0034, -0.0009,  0.0002,  ..., -0.0046, -0.0074,  0.0162]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 22.7656,  -3.2480,   6.8242,  ..., -17.7188,  -4.8555,  -0.8364]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 04:21:46 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for developing is the active form of develop
allowing is the active form of allow
improving is the active form of improve
seeming is the active form of seem
establishing is the active form of establish
understanding is the active form of understand
enjoying is the active form of enjoy
believing is the active form of
2024-07-26 04:21:46 root INFO     [order_1_approx] starting weight calculation for developing is the active form of develop
improving is the active form of improve
enjoying is the active form of enjoy
allowing is the active form of allow
believing is the active form of believe
understanding is the active form of understand
establishing is the active form of establish
seeming is the active form of
2024-07-26 04:21:46 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 04:24:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8906,  1.4688, -4.5625,  ...,  0.6875,  4.3125,  1.5938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 12.5000,  -4.7188,  -0.7656,  ..., -13.8125,  -4.4062,   7.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0232,  0.0060, -0.0027,  ..., -0.0020,  0.0037, -0.0008],
        [ 0.0016,  0.0173, -0.0015,  ...,  0.0027, -0.0002, -0.0034],
        [ 0.0078, -0.0051,  0.0850,  ...,  0.0050,  0.0030, -0.0043],
        ...,
        [-0.0029,  0.0011,  0.0040,  ...,  0.0151,  0.0025, -0.0077],
        [-0.0006,  0.0022, -0.0057,  ...,  0.0016,  0.0281,  0.0016],
        [ 0.0018,  0.0068, -0.0002,  ...,  0.0011, -0.0013,  0.0366]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 11.1328,  -4.3164,   0.5488,  ..., -12.7422,  -4.9961,   7.2070]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 04:24:35 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for developing is the active form of develop
improving is the active form of improve
enjoying is the active form of enjoy
allowing is the active form of allow
believing is the active form of believe
understanding is the active form of understand
establishing is the active form of establish
seeming is the active form of
2024-07-26 04:24:35 root INFO     [order_1_approx] starting weight calculation for allowing is the active form of allow
understanding is the active form of understand
establishing is the active form of establish
enjoying is the active form of enjoy
developing is the active form of develop
believing is the active form of believe
seeming is the active form of seem
improving is the active form of
2024-07-26 04:24:35 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 04:27:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.9375, -2.2656, -1.3438,  ..., -0.3125, -0.4844,  0.2109],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 15.1250,  -9.0000,   2.5469,  ..., -16.3750,   1.0469,  -3.4375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.6479e-02,  2.6321e-04, -4.1504e-03,  ...,  1.5945e-03,
          2.4109e-03,  2.7466e-03],
        [ 9.0027e-04,  1.0803e-02,  2.2430e-03,  ..., -2.1210e-03,
          4.5471e-03,  3.0823e-03],
        [ 2.4109e-03,  1.3275e-03,  5.8105e-02,  ..., -4.0283e-03,
         -3.6774e-03,  4.3030e-03],
        ...,
        [ 5.3406e-04,  4.3106e-04, -6.9275e-03,  ...,  1.5625e-02,
         -2.6703e-04, -1.2512e-03],
        [ 4.6692e-03,  2.1057e-03, -2.8992e-03,  ...,  4.9591e-05,
          1.8311e-02, -1.1215e-03],
        [-5.4321e-03,  2.5024e-03,  9.2316e-04,  ..., -5.7373e-03,
          2.0752e-03,  2.7588e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 15.5703,  -9.3516,   2.0801,  ..., -15.3750,   1.6426,  -3.7559]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 04:27:24 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for allowing is the active form of allow
understanding is the active form of understand
establishing is the active form of establish
enjoying is the active form of enjoy
developing is the active form of develop
believing is the active form of believe
seeming is the active form of seem
improving is the active form of
2024-07-26 04:27:25 root INFO     [order_1_approx] starting weight calculation for seeming is the active form of seem
believing is the active form of believe
understanding is the active form of understand
developing is the active form of develop
improving is the active form of improve
enjoying is the active form of enjoy
establishing is the active form of establish
allowing is the active form of
2024-07-26 04:27:25 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 04:30:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.1250, -1.2578,  3.7031,  ...,  1.2188,  5.0625,  1.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  5.1562,   0.1094,  -2.0469,  ...,  -9.3750, -10.5625,  -3.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.7090e-02,  1.1978e-03, -6.2866e-03,  ...,  3.6926e-03,
          1.9379e-03,  2.1973e-03],
        [-1.8997e-03,  1.1230e-02, -5.8746e-04,  ...,  7.6294e-06,
         -1.8692e-04,  7.2632e-03],
        [-5.0354e-04, -4.0894e-03,  6.3965e-02,  ...,  1.3351e-04,
         -4.3030e-03,  5.1575e-03],
        ...,
        [ 9.3994e-03,  1.9989e-03, -9.2163e-03,  ...,  1.3000e-02,
         -3.8452e-03, -3.8147e-03],
        [ 1.0376e-02,  2.9449e-03, -1.0681e-02,  ...,  2.9144e-03,
          1.8311e-02, -7.1716e-04],
        [ 2.1210e-03,  2.8687e-03,  2.7161e-03,  ...,  7.6294e-05,
         -5.2185e-03,  2.3926e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  6.8750,   0.9326,  -3.0898,  ..., -10.1094,  -9.8906,  -3.4023]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 04:30:13 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for seeming is the active form of seem
believing is the active form of believe
understanding is the active form of understand
developing is the active form of develop
improving is the active form of improve
enjoying is the active form of enjoy
establishing is the active form of establish
allowing is the active form of
2024-07-26 04:30:13 root INFO     [order_1_approx] starting weight calculation for establishing is the active form of establish
improving is the active form of improve
believing is the active form of believe
understanding is the active form of understand
enjoying is the active form of enjoy
seeming is the active form of seem
allowing is the active form of allow
developing is the active form of
2024-07-26 04:30:13 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 04:33:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-3.4375, -1.6484, -0.7734,  ...,  1.1641,  1.9141, -0.5703],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 14.1875,  -6.0000,   4.5312,  ..., -16.2500,  -1.0000,  -3.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0156, -0.0049, -0.0006,  ...,  0.0035,  0.0015,  0.0004],
        [ 0.0014,  0.0171,  0.0070,  ..., -0.0012,  0.0029,  0.0046],
        [ 0.0007, -0.0012,  0.0752,  ..., -0.0058, -0.0027,  0.0054],
        ...,
        [ 0.0030,  0.0047, -0.0080,  ...,  0.0159,  0.0004, -0.0062],
        [ 0.0056,  0.0037, -0.0071,  ..., -0.0006,  0.0267, -0.0008],
        [-0.0063,  0.0002,  0.0016,  ..., -0.0020, -0.0031,  0.0352]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 13.9688,  -7.2266,   4.5742,  ..., -17.0469,  -0.0483,  -3.4609]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 04:33:02 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for establishing is the active form of establish
improving is the active form of improve
believing is the active form of believe
understanding is the active form of understand
enjoying is the active form of enjoy
seeming is the active form of seem
allowing is the active form of allow
developing is the active form of
2024-07-26 04:33:03 root INFO     [order_1_approx] starting weight calculation for believing is the active form of believe
developing is the active form of develop
understanding is the active form of understand
improving is the active form of improve
establishing is the active form of establish
allowing is the active form of allow
seeming is the active form of seem
enjoying is the active form of
2024-07-26 04:33:03 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 04:35:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0327, -2.3281,  0.5547,  ...,  0.4688,  1.0469,  0.4824],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([15.5625, -8.5625, 13.2500,  ..., -5.8125, -5.0938, -6.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 8.4229e-03,  1.8005e-03, -3.6621e-04,  ...,  1.1902e-03,
         -2.6703e-03,  4.2419e-03],
        [-3.2043e-03,  3.5858e-03, -1.8921e-03,  ..., -1.9989e-03,
          2.8687e-03, -3.6316e-03],
        [-7.6294e-05,  1.8768e-03,  3.5400e-02,  ...,  2.1667e-03,
          9.4604e-04, -1.7395e-03],
        ...,
        [ 1.4114e-03, -2.8076e-03, -2.2583e-03,  ...,  8.7891e-03,
         -6.7139e-04, -3.6774e-03],
        [ 3.9062e-03,  3.1662e-04, -3.4332e-04,  ...,  2.3499e-03,
          1.0376e-02,  2.9907e-03],
        [-2.0905e-03,  1.3351e-03, -3.1281e-04,  ..., -1.4343e-03,
         -2.6703e-05,  1.2573e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[15.8906, -9.5625, 13.3125,  ..., -4.8555, -4.2852, -6.9609]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 04:35:52 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for believing is the active form of believe
developing is the active form of develop
understanding is the active form of understand
improving is the active form of improve
establishing is the active form of establish
allowing is the active form of allow
seeming is the active form of seem
enjoying is the active form of
2024-07-26 04:35:52 root INFO     [order_1_approx] starting weight calculation for seeming is the active form of seem
allowing is the active form of allow
developing is the active form of develop
believing is the active form of believe
improving is the active form of improve
enjoying is the active form of enjoy
establishing is the active form of establish
understanding is the active form of
2024-07-26 04:35:52 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 04:38:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.8750, -1.9062,  4.2188,  ...,  0.2969,  1.7266,  1.2969],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 16.6250, -10.5000,   2.1094,  ..., -10.5625,  -4.1875,  -0.4375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.5747e-02,  1.3428e-03, -4.7607e-03,  ...,  2.1820e-03,
          6.1035e-03, -2.5787e-03],
        [ 1.6174e-03,  1.4893e-02, -5.3406e-03,  ..., -9.9182e-04,
          3.9673e-03, -2.8534e-03],
        [ 3.8757e-03, -1.7014e-03,  6.0547e-02,  ..., -6.1798e-04,
         -1.8616e-03,  2.3804e-03],
        ...,
        [ 8.5449e-04,  4.2114e-03,  1.0681e-04,  ...,  1.1658e-02,
         -4.6349e-04, -4.4861e-03],
        [ 3.0518e-03,  2.1973e-03, -9.8267e-03,  ..., -1.4191e-03,
          2.2583e-02,  1.9836e-03],
        [-3.7842e-03,  7.2479e-05,  4.0894e-03,  ..., -2.1667e-03,
         -6.7749e-03,  2.8687e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 17.4688, -11.0312,   1.7129,  ..., -11.7812,  -4.7070,   0.4038]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 04:38:41 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for seeming is the active form of seem
allowing is the active form of allow
developing is the active form of develop
believing is the active form of believe
improving is the active form of improve
enjoying is the active form of enjoy
establishing is the active form of establish
understanding is the active form of
2024-07-26 04:38:41 root INFO     total operator prediction time: 1350.959939956665 seconds
2024-07-26 04:38:41 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on verb_Ving - Ved
2024-07-26 04:38:41 root INFO     building operator verb_Ving - Ved
2024-07-26 04:38:42 root INFO     [order_1_approx] starting weight calculation for After something is suffering, it has suffered
After something is publishing, it has published
After something is expecting, it has expected
After something is operating, it has operated
After something is relating, it has related
After something is representing, it has represented
After something is existing, it has existed
After something is receiving, it has
2024-07-26 04:38:42 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 04:41:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.5312, -1.1406,  3.7344,  ...,  1.2812,  1.4766,  0.7656],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 22.7500, -16.6250,   0.5469,  ..., -16.6250,   0.1992, -11.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0220, -0.0012,  0.0011,  ..., -0.0017,  0.0048,  0.0013],
        [ 0.0041,  0.0181, -0.0023,  ..., -0.0070, -0.0009,  0.0073],
        [-0.0015,  0.0010,  0.0801,  ...,  0.0023, -0.0008, -0.0013],
        ...,
        [ 0.0027,  0.0063, -0.0008,  ...,  0.0168, -0.0019, -0.0032],
        [ 0.0050, -0.0003,  0.0002,  ...,  0.0014,  0.0291,  0.0010],
        [-0.0002,  0.0020, -0.0067,  ...,  0.0028,  0.0033,  0.0364]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 22.2500, -15.6328,   0.0259,  ..., -15.3672,   0.4841, -10.6797]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 04:41:31 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for After something is suffering, it has suffered
After something is publishing, it has published
After something is expecting, it has expected
After something is operating, it has operated
After something is relating, it has related
After something is representing, it has represented
After something is existing, it has existed
After something is receiving, it has
2024-07-26 04:41:32 root INFO     [order_1_approx] starting weight calculation for After something is expecting, it has expected
After something is existing, it has existed
After something is suffering, it has suffered
After something is representing, it has represented
After something is receiving, it has received
After something is publishing, it has published
After something is operating, it has operated
After something is relating, it has
2024-07-26 04:41:32 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 04:44:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-3.0938,  0.0781, -2.7188,  ..., -0.4961,  0.2949, -0.1562],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 19.2500, -10.0000,  -3.8906,  ..., -13.4375,   4.1562,  -2.9531],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.8320e-02,  1.0925e-02, -6.0730e-03,  ...,  3.8147e-03,
          3.7079e-03, -7.2021e-03],
        [ 4.3335e-03,  1.5991e-02, -5.9509e-03,  ..., -3.5095e-03,
          3.5248e-03,  5.8594e-03],
        [-2.8076e-03, -3.3722e-03,  7.2266e-02,  ...,  2.9297e-03,
          1.5793e-03, -5.4016e-03],
        ...,
        [ 3.7575e-04,  1.1368e-03, -7.6599e-03,  ...,  2.2339e-02,
          4.7684e-04, -1.9226e-03],
        [ 2.5635e-03, -9.9182e-05, -1.6327e-03,  ..., -6.0272e-04,
          3.4424e-02, -1.3962e-03],
        [ 3.0365e-03,  1.7090e-03, -1.0193e-02,  ...,  6.4468e-04,
          4.5776e-03,  3.2715e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 18.1875,  -9.0312,  -3.6445,  ..., -13.5312,   4.6719,  -4.4844]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 04:44:20 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for After something is expecting, it has expected
After something is existing, it has existed
After something is suffering, it has suffered
After something is representing, it has represented
After something is receiving, it has received
After something is publishing, it has published
After something is operating, it has operated
After something is relating, it has
2024-07-26 04:44:21 root INFO     [order_1_approx] starting weight calculation for After something is operating, it has operated
After something is relating, it has related
After something is receiving, it has received
After something is publishing, it has published
After something is expecting, it has expected
After something is existing, it has existed
After something is representing, it has represented
After something is suffering, it has
2024-07-26 04:44:21 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 04:47:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.4375, -1.1406,  2.3281,  ..., -0.8633, -0.6484, -1.2344],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 12.1250,  -3.9844,  11.6875,  ...,  -9.0625,  -3.7812, -12.4375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.0508e-02,  5.5237e-03, -1.0986e-03,  ...,  6.4087e-04,
         -4.7302e-04, -3.8147e-05],
        [ 3.7537e-03,  1.3428e-02,  3.7689e-03,  ..., -4.7607e-03,
          5.8899e-03,  2.9602e-03],
        [ 3.2806e-03, -9.4604e-04,  9.2285e-02,  ..., -4.0588e-03,
          8.0566e-03, -3.6774e-03],
        ...,
        [-4.3945e-03,  2.2278e-03, -1.6968e-02,  ...,  2.1240e-02,
         -3.0518e-03, -4.9744e-03],
        [ 4.3030e-03,  2.8381e-03, -6.3477e-03,  ..., -3.4485e-03,
          3.2227e-02,  9.3079e-04],
        [ 4.4823e-04, -2.5177e-03, -1.1719e-02,  ..., -7.0801e-03,
          4.2725e-03,  3.1738e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 10.0781,  -1.4336,  12.7031,  ..., -10.3906,  -4.2617, -10.7188]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 04:47:08 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for After something is operating, it has operated
After something is relating, it has related
After something is receiving, it has received
After something is publishing, it has published
After something is expecting, it has expected
After something is existing, it has existed
After something is representing, it has represented
After something is suffering, it has
2024-07-26 04:47:09 root INFO     [order_1_approx] starting weight calculation for After something is operating, it has operated
After something is expecting, it has expected
After something is suffering, it has suffered
After something is relating, it has related
After something is existing, it has existed
After something is representing, it has represented
After something is receiving, it has received
After something is publishing, it has
2024-07-26 04:47:09 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 04:49:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0156, -2.2188,  1.7812,  ...,  0.4219,  0.0547, -0.1836],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([19.6250, -6.8125, -1.0859,  ..., -6.1562, -1.0156, -2.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.6611e-02, -1.1902e-03,  1.1536e-02,  ...,  1.0254e-02,
         -1.3657e-03, -3.8605e-03],
        [-5.6458e-04,  1.8799e-02, -8.1177e-03,  ..., -6.8665e-03,
         -1.6174e-03,  4.9438e-03],
        [ 5.0049e-03,  1.2054e-03,  7.3242e-02,  ..., -2.5024e-03,
          4.4556e-03, -4.9133e-03],
        ...,
        [ 2.8992e-03, -2.9449e-03, -2.9449e-03,  ...,  1.5869e-02,
          2.0599e-03, -6.3171e-03],
        [ 1.0376e-03,  8.1635e-04, -2.7161e-03,  ..., -2.0447e-03,
          3.1250e-02, -5.9814e-03],
        [-2.6855e-03,  9.9182e-05, -2.1851e-02,  ..., -7.8125e-03,
          3.3875e-03,  3.4912e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[18.1875, -6.1953, -1.4531,  ..., -5.3086, -0.5605, -2.9766]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 04:49:58 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for After something is operating, it has operated
After something is expecting, it has expected
After something is suffering, it has suffered
After something is relating, it has related
After something is existing, it has existed
After something is representing, it has represented
After something is receiving, it has received
After something is publishing, it has
2024-07-26 04:49:58 root INFO     [order_1_approx] starting weight calculation for After something is operating, it has operated
After something is suffering, it has suffered
After something is receiving, it has received
After something is representing, it has represented
After something is relating, it has related
After something is existing, it has existed
After something is publishing, it has published
After something is expecting, it has
2024-07-26 04:49:58 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 04:52:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4141,  0.0547,  3.0469,  ...,  0.0781,  0.4141,  1.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([17.6250, -3.0156,  6.6875,  ..., -3.1875, -0.7578, -3.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0219,  0.0050, -0.0017,  ..., -0.0055,  0.0061, -0.0012],
        [ 0.0097,  0.0142, -0.0014,  ...,  0.0061, -0.0043, -0.0022],
        [ 0.0005,  0.0057,  0.1050,  ..., -0.0003, -0.0021, -0.0054],
        ...,
        [ 0.0061,  0.0029, -0.0008,  ...,  0.0280,  0.0020, -0.0052],
        [ 0.0099,  0.0029, -0.0089,  ...,  0.0009,  0.0398,  0.0007],
        [-0.0010, -0.0019, -0.0106,  ...,  0.0074,  0.0048,  0.0403]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[18.6406, -2.7676,  5.9648,  ..., -2.5098, -2.0273, -1.6533]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 04:52:48 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for After something is operating, it has operated
After something is suffering, it has suffered
After something is receiving, it has received
After something is representing, it has represented
After something is relating, it has related
After something is existing, it has existed
After something is publishing, it has published
After something is expecting, it has
2024-07-26 04:52:48 root INFO     [order_1_approx] starting weight calculation for After something is publishing, it has published
After something is relating, it has related
After something is existing, it has existed
After something is operating, it has operated
After something is suffering, it has suffered
After something is expecting, it has expected
After something is receiving, it has received
After something is representing, it has
2024-07-26 04:52:48 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 04:55:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4531,  1.2344, -2.7812,  ...,  0.8867, -2.0469,  0.6914],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 15.7500,  -2.7500,  -5.0938,  ..., -18.6250,   7.0625,  -5.3438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0369, -0.0035, -0.0041,  ...,  0.0014,  0.0041, -0.0036],
        [ 0.0035,  0.0179, -0.0019,  ..., -0.0093,  0.0046,  0.0027],
        [ 0.0021, -0.0042,  0.1123,  ...,  0.0006,  0.0054, -0.0047],
        ...,
        [ 0.0006,  0.0018,  0.0023,  ...,  0.0215, -0.0009, -0.0038],
        [ 0.0022,  0.0021,  0.0001,  ..., -0.0016,  0.0498,  0.0086],
        [ 0.0025,  0.0023,  0.0030,  ...,  0.0026, -0.0020,  0.0444]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 15.2188,  -1.3379,  -3.8672,  ..., -19.0156,   7.3555,  -7.1133]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 04:55:38 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for After something is publishing, it has published
After something is relating, it has related
After something is existing, it has existed
After something is operating, it has operated
After something is suffering, it has suffered
After something is expecting, it has expected
After something is receiving, it has received
After something is representing, it has
2024-07-26 04:55:38 root INFO     [order_1_approx] starting weight calculation for After something is relating, it has related
After something is publishing, it has published
After something is existing, it has existed
After something is receiving, it has received
After something is representing, it has represented
After something is suffering, it has suffered
After something is expecting, it has expected
After something is operating, it has
2024-07-26 04:55:38 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 04:58:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9688, -0.6289,  3.5938,  ..., -0.6172,  0.0840,  0.8828],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([16.8750, -3.4219,  8.1875,  ..., -2.9688,  0.7852, -0.3594],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0272,  0.0011, -0.0074,  ...,  0.0020,  0.0043, -0.0024],
        [ 0.0041,  0.0181,  0.0131,  ..., -0.0079,  0.0046,  0.0103],
        [ 0.0025,  0.0002,  0.0781,  ...,  0.0011,  0.0055,  0.0011],
        ...,
        [-0.0001, -0.0027, -0.0002,  ...,  0.0162,  0.0060, -0.0049],
        [ 0.0018,  0.0046,  0.0081,  ..., -0.0011,  0.0310, -0.0027],
        [-0.0016, -0.0011, -0.0009,  ..., -0.0018,  0.0058,  0.0322]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[16.7188, -0.8887,  8.4062,  ..., -1.8369,  0.7139, -1.2441]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 04:58:28 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for After something is relating, it has related
After something is publishing, it has published
After something is existing, it has existed
After something is receiving, it has received
After something is representing, it has represented
After something is suffering, it has suffered
After something is expecting, it has expected
After something is operating, it has
2024-07-26 04:58:28 root INFO     [order_1_approx] starting weight calculation for After something is operating, it has operated
After something is receiving, it has received
After something is representing, it has represented
After something is publishing, it has published
After something is expecting, it has expected
After something is suffering, it has suffered
After something is relating, it has related
After something is existing, it has
2024-07-26 04:58:28 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 05:01:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.3984, -0.8125,  0.4160,  ...,  2.6562,  2.3125,  1.0312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 11.0625, -10.3125,   8.9375,  ...,  -7.9062,   3.2500,   1.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0243, -0.0040,  0.0018,  ...,  0.0006,  0.0038,  0.0049],
        [ 0.0019,  0.0090,  0.0005,  ..., -0.0035,  0.0018,  0.0050],
        [ 0.0021, -0.0016,  0.0938,  ...,  0.0020,  0.0014,  0.0034],
        ...,
        [ 0.0016,  0.0049, -0.0081,  ...,  0.0168, -0.0020, -0.0058],
        [-0.0038, -0.0007,  0.0048,  ..., -0.0073,  0.0291,  0.0023],
        [-0.0034, -0.0020,  0.0020,  ..., -0.0006,  0.0034,  0.0325]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[11.8594, -8.3672,  9.3438,  ..., -7.6211,  3.4590, -0.1660]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 05:01:18 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for After something is operating, it has operated
After something is receiving, it has received
After something is representing, it has represented
After something is publishing, it has published
After something is expecting, it has expected
After something is suffering, it has suffered
After something is relating, it has related
After something is existing, it has
2024-07-26 05:01:18 root INFO     total operator prediction time: 1356.2535634040833 seconds
2024-07-26 05:01:18 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on verb_inf - Ved
2024-07-26 05:01:18 root INFO     building operator verb_inf - Ved
2024-07-26 05:01:18 root INFO     [order_1_approx] starting weight calculation for If the present form is ask, the past form is asked
If the present form is create, the past form is created
If the present form is announce, the past form is announced
If the present form is ensure, the past form is ensured
If the present form is consider, the past form is considered
If the present form is enjoy, the past form is enjoyed
If the present form is decide, the past form is decided
If the present form is describe, the past form is
2024-07-26 05:01:18 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 05:04:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2402,  0.7891,  2.2812,  ..., -1.5156,  0.3867,  2.4531],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  4.5625,  -3.7500,  -1.0547,  ..., -12.5000,  -0.7969,  -0.3906],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0244,  0.0009, -0.0066,  ...,  0.0062, -0.0012, -0.0057],
        [-0.0005,  0.0095,  0.0074,  ..., -0.0144,  0.0007,  0.0092],
        [ 0.0003,  0.0006,  0.0728,  ..., -0.0061,  0.0018,  0.0010],
        ...,
        [ 0.0031, -0.0035, -0.0040,  ...,  0.0099,  0.0025, -0.0004],
        [-0.0010, -0.0019,  0.0080,  ..., -0.0004,  0.0253,  0.0023],
        [-0.0030,  0.0053, -0.0064,  ..., -0.0075, -0.0015,  0.0287]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  3.7090,  -5.8945,  -2.1465,  ..., -12.9766,  -2.5938,  -1.2090]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 05:04:06 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is ask, the past form is asked
If the present form is create, the past form is created
If the present form is announce, the past form is announced
If the present form is ensure, the past form is ensured
If the present form is consider, the past form is considered
If the present form is enjoy, the past form is enjoyed
If the present form is decide, the past form is decided
If the present form is describe, the past form is
2024-07-26 05:04:06 root INFO     [order_1_approx] starting weight calculation for If the present form is announce, the past form is announced
If the present form is enjoy, the past form is enjoyed
If the present form is ask, the past form is asked
If the present form is describe, the past form is described
If the present form is decide, the past form is decided
If the present form is consider, the past form is considered
If the present form is ensure, the past form is ensured
If the present form is create, the past form is
2024-07-26 05:04:06 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 05:06:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3750, -1.7031, -1.9375,  ...,  1.6250,  0.5234, -0.1055],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -5.1875,  -1.3594,   1.7422,  ..., -10.8125, -10.0625,   1.2812],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.6245e-02, -2.1057e-03,  2.0447e-03,  ...,  2.4872e-03,
          3.0518e-05, -2.5330e-03],
        [ 1.1215e-03,  1.8066e-02,  1.2756e-02,  ..., -7.7515e-03,
          1.4801e-03,  2.8687e-03],
        [ 1.2283e-03,  6.8970e-03,  8.3008e-02,  ..., -1.8005e-03,
          1.4648e-03, -2.8381e-03],
        ...,
        [-1.6937e-03,  5.3406e-04, -7.5073e-03,  ...,  9.7656e-03,
         -1.0300e-03, -5.1270e-03],
        [ 4.2114e-03,  1.8234e-03,  2.2888e-03,  ...,  1.4191e-03,
          3.1982e-02, -1.0681e-04],
        [-1.3657e-03,  4.8828e-03, -2.9144e-03,  ..., -3.3569e-03,
          3.4485e-03,  3.2471e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -6.2383,  -1.6084,   1.6465,  ..., -12.8281, -10.3281,   2.3750]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 05:06:55 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is announce, the past form is announced
If the present form is enjoy, the past form is enjoyed
If the present form is ask, the past form is asked
If the present form is describe, the past form is described
If the present form is decide, the past form is decided
If the present form is consider, the past form is considered
If the present form is ensure, the past form is ensured
If the present form is create, the past form is
2024-07-26 05:06:55 root INFO     [order_1_approx] starting weight calculation for If the present form is ensure, the past form is ensured
If the present form is announce, the past form is announced
If the present form is enjoy, the past form is enjoyed
If the present form is create, the past form is created
If the present form is decide, the past form is decided
If the present form is ask, the past form is asked
If the present form is describe, the past form is described
If the present form is consider, the past form is
2024-07-26 05:06:55 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 05:09:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7969, -0.3008, -1.2344,  ..., -1.2422,  0.0781,  2.8594],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-1.6562,  0.7305, -1.8984,  ..., -7.1875, -1.8125, -3.8438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.9165e-02,  1.2207e-03,  3.1891e-03,  ...,  2.4261e-03,
         -2.2583e-03, -4.2725e-03],
        [ 3.4485e-03,  9.3994e-03,  1.0925e-02,  ..., -2.9602e-03,
         -2.2583e-03,  1.6403e-03],
        [ 1.4648e-03,  5.0049e-03,  7.2266e-02,  ..., -2.1667e-03,
          3.0212e-03, -9.2773e-03],
        ...,
        [ 3.8757e-03,  1.6785e-04, -1.7548e-03,  ...,  1.3184e-02,
         -1.2283e-03, -3.1433e-03],
        [ 1.1292e-03,  5.8289e-03, -2.2888e-03,  ...,  2.1057e-03,
          2.4170e-02, -4.6387e-03],
        [-6.6757e-06,  1.7776e-03, -7.1411e-03,  ...,  4.6539e-04,
         -4.5776e-03,  3.7354e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.0742,  1.9893, -2.6094,  ..., -9.1953, -2.7812, -3.7520]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 05:09:43 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is ensure, the past form is ensured
If the present form is announce, the past form is announced
If the present form is enjoy, the past form is enjoyed
If the present form is create, the past form is created
If the present form is decide, the past form is decided
If the present form is ask, the past form is asked
If the present form is describe, the past form is described
If the present form is consider, the past form is
2024-07-26 05:09:43 root INFO     [order_1_approx] starting weight calculation for If the present form is announce, the past form is announced
If the present form is create, the past form is created
If the present form is consider, the past form is considered
If the present form is ensure, the past form is ensured
If the present form is describe, the past form is described
If the present form is enjoy, the past form is enjoyed
If the present form is decide, the past form is decided
If the present form is ask, the past form is
2024-07-26 05:09:43 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 05:12:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3984, -1.4219,  0.8945,  ...,  0.3984,  0.8164,  0.6680],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 4.1875, -5.0000,  1.2188,  ..., -2.2500,  4.9688, -6.3125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.1851e-02, -2.3193e-03,  2.1057e-03,  ..., -6.4850e-05,
          4.0588e-03, -2.8076e-03],
        [ 3.0212e-03,  1.0986e-02,  1.4343e-03,  ..., -5.7068e-03,
          2.4109e-03,  3.1281e-03],
        [ 2.8534e-03,  5.0964e-03,  6.7383e-02,  ..., -6.0120e-03,
          3.2959e-03, -3.3264e-03],
        ...,
        [ 2.8076e-03,  4.8065e-04,  4.1199e-04,  ...,  1.1108e-02,
          1.3161e-04, -2.7161e-03],
        [ 4.9744e-03,  3.2196e-03, -2.4414e-03,  ...,  3.0212e-03,
          2.3438e-02, -3.6011e-03],
        [-3.1281e-04, -2.1515e-03, -6.4697e-03,  ..., -1.1902e-03,
          9.1553e-05,  3.1982e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.0859, -5.0586,  1.9727,  ..., -2.6895,  4.1367, -6.0156]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 05:12:31 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is announce, the past form is announced
If the present form is create, the past form is created
If the present form is consider, the past form is considered
If the present form is ensure, the past form is ensured
If the present form is describe, the past form is described
If the present form is enjoy, the past form is enjoyed
If the present form is decide, the past form is decided
If the present form is ask, the past form is
2024-07-26 05:12:31 root INFO     [order_1_approx] starting weight calculation for If the present form is enjoy, the past form is enjoyed
If the present form is describe, the past form is described
If the present form is ask, the past form is asked
If the present form is create, the past form is created
If the present form is ensure, the past form is ensured
If the present form is announce, the past form is announced
If the present form is consider, the past form is considered
If the present form is decide, the past form is
2024-07-26 05:12:31 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 05:15:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1250, -1.6250, -1.8750,  ...,  1.4453,  2.5312,  3.2031],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 3.9531, -8.6250, -1.6875,  ..., -9.2500, -3.5938, -1.3438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0195,  0.0022,  0.0058,  ...,  0.0002, -0.0059, -0.0087],
        [ 0.0005,  0.0139,  0.0106,  ..., -0.0060,  0.0009,  0.0028],
        [ 0.0017,  0.0049,  0.0649,  ...,  0.0004,  0.0049, -0.0024],
        ...,
        [ 0.0015, -0.0012, -0.0125,  ...,  0.0110,  0.0017, -0.0005],
        [-0.0027,  0.0010,  0.0045,  ...,  0.0035,  0.0229, -0.0023],
        [-0.0010, -0.0005,  0.0030,  ..., -0.0088, -0.0010,  0.0273]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  1.8945,  -7.8047,  -1.6260,  ..., -10.7031,  -3.7773,  -2.1719]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 05:15:19 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is enjoy, the past form is enjoyed
If the present form is describe, the past form is described
If the present form is ask, the past form is asked
If the present form is create, the past form is created
If the present form is ensure, the past form is ensured
If the present form is announce, the past form is announced
If the present form is consider, the past form is considered
If the present form is decide, the past form is
2024-07-26 05:15:19 root INFO     [order_1_approx] starting weight calculation for If the present form is enjoy, the past form is enjoyed
If the present form is announce, the past form is announced
If the present form is consider, the past form is considered
If the present form is create, the past form is created
If the present form is ask, the past form is asked
If the present form is describe, the past form is described
If the present form is decide, the past form is decided
If the present form is ensure, the past form is
2024-07-26 05:15:19 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 05:18:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4219, -0.5156,  1.1562,  ...,  1.0234, -0.3633,  1.6172],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 1.7188, -6.8438,  7.0625,  ..., -4.0000, -2.4531,  2.9375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0278,  0.0079,  0.0046,  ...,  0.0075, -0.0047, -0.0036],
        [ 0.0060,  0.0210,  0.0135,  ..., -0.0063,  0.0012, -0.0001],
        [ 0.0023,  0.0025,  0.0732,  ..., -0.0045,  0.0055,  0.0036],
        ...,
        [-0.0024,  0.0004,  0.0017,  ...,  0.0093, -0.0013, -0.0038],
        [ 0.0011, -0.0106,  0.0065,  ..., -0.0037,  0.0273, -0.0005],
        [-0.0015,  0.0073, -0.0120,  ...,  0.0013, -0.0030,  0.0349]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4902, -7.5781,  8.0625,  ..., -4.9766, -4.4844,  2.9746]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 05:18:07 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is enjoy, the past form is enjoyed
If the present form is announce, the past form is announced
If the present form is consider, the past form is considered
If the present form is create, the past form is created
If the present form is ask, the past form is asked
If the present form is describe, the past form is described
If the present form is decide, the past form is decided
If the present form is ensure, the past form is
2024-07-26 05:18:07 root INFO     [order_1_approx] starting weight calculation for If the present form is enjoy, the past form is enjoyed
If the present form is decide, the past form is decided
If the present form is describe, the past form is described
If the present form is ask, the past form is asked
If the present form is ensure, the past form is ensured
If the present form is create, the past form is created
If the present form is consider, the past form is considered
If the present form is announce, the past form is
2024-07-26 05:18:07 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 05:20:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6094,  1.4062, -4.1250,  ...,  1.0156,  0.6016,  0.1016],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 1.3594, -4.8750, -0.2539,  ..., -6.5625, -7.1562, -4.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.2095e-02, -3.4790e-03,  1.3580e-03,  ...,  4.7302e-04,
          1.0681e-04,  8.2779e-04],
        [ 8.4305e-04,  1.3916e-02,  8.1787e-03,  ..., -3.4790e-03,
         -2.4414e-03,  1.0376e-03],
        [ 4.2725e-03,  2.3804e-03,  6.4941e-02,  ..., -1.4267e-03,
          7.0801e-03, -7.3853e-03],
        ...,
        [ 4.6082e-03, -1.5106e-03, -3.4180e-03,  ...,  1.1047e-02,
         -3.4332e-04, -4.8828e-03],
        [ 1.9302e-03, -1.0681e-03, -2.9449e-03,  ...,  1.4191e-03,
          2.1729e-02,  1.5259e-05],
        [-1.8463e-03, -6.2943e-05, -9.2773e-03,  ..., -1.3580e-03,
         -8.2397e-04,  2.6611e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1748, -5.6875,  0.8125,  ..., -7.0508, -8.1016, -6.1016]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 05:20:55 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is enjoy, the past form is enjoyed
If the present form is decide, the past form is decided
If the present form is describe, the past form is described
If the present form is ask, the past form is asked
If the present form is ensure, the past form is ensured
If the present form is create, the past form is created
If the present form is consider, the past form is considered
If the present form is announce, the past form is
2024-07-26 05:20:55 root INFO     [order_1_approx] starting weight calculation for If the present form is decide, the past form is decided
If the present form is ask, the past form is asked
If the present form is announce, the past form is announced
If the present form is describe, the past form is described
If the present form is create, the past form is created
If the present form is ensure, the past form is ensured
If the present form is consider, the past form is considered
If the present form is enjoy, the past form is
2024-07-26 05:20:56 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 05:23:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2871, -1.2891,  0.6484,  ...,  0.3984,  0.6797,  3.3906],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 0.9688, -3.6875, 15.0000,  ..., -3.6250, -1.9141, -2.1406],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.1973e-02,  4.9591e-04,  2.0599e-03,  ...,  1.4114e-03,
         -1.5259e-03, -3.0823e-03],
        [ 2.0294e-03,  1.6724e-02,  4.3945e-03,  ..., -1.0498e-02,
          2.4719e-03,  1.9684e-03],
        [-2.8801e-04,  5.0049e-03,  7.1289e-02,  ..., -4.6692e-03,
          1.4191e-03, -1.8234e-03],
        ...,
        [ 6.4392e-03, -3.5858e-03, -1.3062e-02,  ...,  8.3008e-03,
         -6.8970e-03, -3.2654e-03],
        [ 5.6763e-03,  2.2278e-03,  5.3101e-03,  ..., -2.9755e-03,
          2.5146e-02,  6.1035e-05],
        [ 3.6621e-03,  6.2256e-03, -6.9580e-03,  ...,  7.9727e-04,
          5.3406e-04,  3.3203e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7412, -3.1270, 14.5078,  ..., -2.3164, -1.4395, -1.9404]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 05:23:44 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is decide, the past form is decided
If the present form is ask, the past form is asked
If the present form is announce, the past form is announced
If the present form is describe, the past form is described
If the present form is create, the past form is created
If the present form is ensure, the past form is ensured
If the present form is consider, the past form is considered
If the present form is enjoy, the past form is
2024-07-26 05:23:44 root INFO     total operator prediction time: 1345.918897151947 seconds
2024-07-26 05:23:44 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on verb_inf - 3pSg
2024-07-26 05:23:44 root INFO     building operator verb_inf - 3pSg
2024-07-26 05:23:44 root INFO     [order_1_approx] starting weight calculation for I promote, he promotes
I occur, he occurs
I seem, he seems
I happen, he happens
I add, he adds
I send, he sends
I identify, he identifies
I operate, he
2024-07-26 05:23:44 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 05:26:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6250,  1.0312,  5.6562,  ..., -3.6562, -1.5625,  0.0381],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([20.7500,  2.4375, 12.1875,  ..., -7.2812,  5.8125, -4.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0276,  0.0027, -0.0009,  ...,  0.0032,  0.0016,  0.0011],
        [ 0.0152,  0.0165,  0.0101,  ..., -0.0085,  0.0084,  0.0072],
        [ 0.0023,  0.0025,  0.0737,  ...,  0.0009, -0.0031,  0.0007],
        ...,
        [ 0.0036,  0.0037, -0.0067,  ...,  0.0168,  0.0040, -0.0038],
        [ 0.0033,  0.0069,  0.0076,  ..., -0.0001,  0.0280, -0.0017],
        [-0.0020,  0.0048,  0.0027,  ..., -0.0020,  0.0011,  0.0327]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[19.4688,  4.4805, 12.9688,  ..., -7.0312,  7.5977, -4.5742]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 05:26:31 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for I promote, he promotes
I occur, he occurs
I seem, he seems
I happen, he happens
I add, he adds
I send, he sends
I identify, he identifies
I operate, he
2024-07-26 05:26:31 root INFO     [order_1_approx] starting weight calculation for I operate, he operates
I promote, he promotes
I send, he sends
I occur, he occurs
I add, he adds
I seem, he seems
I identify, he identifies
I happen, he
2024-07-26 05:26:31 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 05:29:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([1.0469, 0.8320, 5.8438,  ..., 1.1328, 1.6719, 0.5078], device='cuda:0',
       dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 17.7500,  -5.4688,  10.8750,  ...,   1.3906,  -0.2500, -13.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.1118e-02,  7.0801e-03,  5.8289e-03,  ..., -3.5553e-03,
          1.2207e-04, -1.2970e-04],
        [ 1.0437e-02,  2.8381e-03, -7.3853e-03,  ...,  5.8594e-03,
          4.8218e-03,  6.4697e-03],
        [ 2.4872e-03,  4.6997e-03,  7.9590e-02,  ...,  6.1646e-03,
          1.0223e-03, -6.7444e-03],
        ...,
        [ 7.6294e-05,  2.5940e-03,  1.0452e-03,  ...,  1.7578e-02,
         -2.4872e-03, -3.9062e-03],
        [ 7.4768e-03,  9.0790e-04, -1.2894e-03,  ...,  3.7003e-04,
          2.7222e-02,  4.1962e-05],
        [-1.1597e-03,  4.9744e-03, -7.1411e-03,  ..., -2.1057e-03,
         -3.5858e-03,  2.3926e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 14.9219,  -1.9414,  12.0859,  ...,   2.4688,   0.3784, -14.0547]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 05:29:18 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for I operate, he operates
I promote, he promotes
I send, he sends
I occur, he occurs
I add, he adds
I seem, he seems
I identify, he identifies
I happen, he
2024-07-26 05:29:18 root INFO     [order_1_approx] starting weight calculation for I send, he sends
I promote, he promotes
I happen, he happens
I operate, he operates
I add, he adds
I seem, he seems
I identify, he identifies
I occur, he
2024-07-26 05:29:18 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 05:32:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.3477, 1.1094, 6.4688,  ..., 1.7500, 0.5273, 0.1562], device='cuda:0',
       dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 19.8750, -11.1875,   7.1875,  ...,  -3.5625,   9.3750,  -6.3438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0198,  0.0045, -0.0013,  ...,  0.0065, -0.0089, -0.0053],
        [ 0.0164,  0.0105, -0.0064,  ..., -0.0131,  0.0248,  0.0027],
        [ 0.0045,  0.0021,  0.0962,  ..., -0.0007,  0.0035, -0.0019],
        ...,
        [ 0.0021,  0.0009, -0.0013,  ...,  0.0153,  0.0043, -0.0068],
        [ 0.0026,  0.0031,  0.0027,  ..., -0.0028,  0.0282,  0.0009],
        [-0.0022,  0.0036, -0.0045,  ...,  0.0003, -0.0043,  0.0282]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[18.0625, -6.8320,  6.8906,  ..., -2.7754, 10.0391, -9.0469]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 05:32:05 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for I send, he sends
I promote, he promotes
I happen, he happens
I operate, he operates
I add, he adds
I seem, he seems
I identify, he identifies
I occur, he
2024-07-26 05:32:05 root INFO     [order_1_approx] starting weight calculation for I promote, he promotes
I send, he sends
I occur, he occurs
I seem, he seems
I operate, he operates
I happen, he happens
I identify, he identifies
I add, he
2024-07-26 05:32:05 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 05:34:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2773, -0.5078,  6.5938,  ...,  0.1699,  1.5234, -2.2031],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 21.7500, -13.8750,   8.1875,  ..., -11.8750,   5.2500, -10.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.2217e-02, -2.5635e-03,  1.8188e-02,  ...,  2.2583e-03,
          1.8311e-03,  1.5259e-03],
        [ 1.1108e-02,  1.9775e-02, -8.9722e-03,  ..., -5.9814e-03,
          2.1210e-03,  4.7607e-03],
        [ 1.0529e-03, -9.5367e-05,  8.9844e-02,  ..., -2.4109e-03,
         -6.7139e-03, -8.9722e-03],
        ...,
        [-2.2888e-04, -7.4768e-04, -1.7700e-02,  ...,  1.9897e-02,
          1.9073e-03, -3.6011e-03],
        [ 5.0049e-03,  6.1035e-03, -6.3324e-04,  ...,  1.6403e-03,
          3.7109e-02,  5.4169e-04],
        [-5.0049e-03,  1.6022e-03,  7.9346e-04,  ...,  2.6093e-03,
         -3.1128e-03,  4.0039e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 16.0938,  -7.9531,   9.3516,  ..., -12.2344,   6.5352, -12.9297]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 05:34:53 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for I promote, he promotes
I send, he sends
I occur, he occurs
I seem, he seems
I operate, he operates
I happen, he happens
I identify, he identifies
I add, he
2024-07-26 05:34:53 root INFO     [order_1_approx] starting weight calculation for I identify, he identifies
I add, he adds
I promote, he promotes
I happen, he happens
I occur, he occurs
I seem, he seems
I operate, he operates
I send, he
2024-07-26 05:34:53 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 05:37:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9297,  2.6250,  2.2500,  ..., -0.8555, -2.6875,  0.1177],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 31.8750, -15.3750,   9.3125,  ...,  -2.6875,  10.1875, -16.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.2339e-02, -1.5869e-03, -4.8828e-03,  ..., -6.1035e-04,
         -2.3041e-03,  2.2125e-03],
        [ 3.5400e-03,  1.7334e-02,  3.0518e-05,  ..., -4.2725e-03,
          4.9133e-03,  2.4414e-03],
        [ 7.2479e-04, -3.4485e-03,  8.2031e-02,  ...,  7.0190e-03,
         -6.8054e-03, -9.3994e-03],
        ...,
        [ 1.7548e-03, -1.6098e-03, -9.6436e-03,  ...,  1.9653e-02,
          7.3242e-04, -6.4468e-04],
        [ 6.1035e-03,  5.9509e-03, -6.5918e-03,  ...,  2.4872e-03,
          2.9907e-02,  1.1597e-03],
        [-5.8365e-04, -4.3945e-03, -7.2021e-03,  ...,  4.7607e-03,
         -2.5024e-03,  3.2959e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 31.3438, -14.2109,   9.5938,  ...,  -2.2305,  10.7812, -17.7969]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 05:37:40 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for I identify, he identifies
I add, he adds
I promote, he promotes
I happen, he happens
I occur, he occurs
I seem, he seems
I operate, he operates
I send, he
2024-07-26 05:37:40 root INFO     [order_1_approx] starting weight calculation for I add, he adds
I occur, he occurs
I promote, he promotes
I seem, he seems
I happen, he happens
I operate, he operates
I send, he sends
I identify, he
2024-07-26 05:37:40 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 05:40:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1016,  2.5156,  3.7656,  ..., -1.3672, -1.5156, -1.8672],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([12.6250, -7.3125, 14.6875,  ..., -4.2188, 12.2500, -6.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0220,  0.0067, -0.0082,  ..., -0.0003,  0.0027, -0.0054],
        [ 0.0079,  0.0248, -0.0010,  ...,  0.0002,  0.0071, -0.0003],
        [ 0.0035, -0.0011,  0.0952,  ...,  0.0023, -0.0009, -0.0071],
        ...,
        [-0.0012,  0.0001, -0.0068,  ...,  0.0214,  0.0006, -0.0041],
        [ 0.0066,  0.0046,  0.0027,  ..., -0.0023,  0.0275,  0.0049],
        [-0.0036,  0.0019, -0.0062,  ...,  0.0019, -0.0059,  0.0342]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[12.5078, -6.8633, 15.1875,  ..., -3.6836, 13.0156, -7.8281]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 05:40:27 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for I add, he adds
I occur, he occurs
I promote, he promotes
I seem, he seems
I happen, he happens
I operate, he operates
I send, he sends
I identify, he
2024-07-26 05:40:27 root INFO     [order_1_approx] starting weight calculation for I seem, he seems
I occur, he occurs
I happen, he happens
I add, he adds
I identify, he identifies
I operate, he operates
I send, he sends
I promote, he
2024-07-26 05:40:27 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 05:43:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.4375, -0.1328,  0.3516,  ..., -3.3750, -2.7031, -1.8984],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([25.3750, -1.0000,  3.9219,  ..., -5.8125,  5.1875,  0.6484],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0264,  0.0027, -0.0052,  ...,  0.0023, -0.0033, -0.0139],
        [ 0.0076,  0.0217, -0.0070,  ..., -0.0004,  0.0128,  0.0146],
        [ 0.0047,  0.0056,  0.0815,  ...,  0.0035,  0.0046, -0.0006],
        ...,
        [-0.0014,  0.0022, -0.0023,  ...,  0.0167,  0.0025, -0.0014],
        [ 0.0026,  0.0012,  0.0041,  ..., -0.0038,  0.0281, -0.0017],
        [-0.0017,  0.0015, -0.0038,  ..., -0.0042,  0.0109,  0.0439]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[26.9531,  1.1465,  5.6445,  ..., -4.9688,  6.0352, -1.0420]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 05:43:14 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for I seem, he seems
I occur, he occurs
I happen, he happens
I add, he adds
I identify, he identifies
I operate, he operates
I send, he sends
I promote, he
2024-07-26 05:43:14 root INFO     [order_1_approx] starting weight calculation for I send, he sends
I promote, he promotes
I add, he adds
I identify, he identifies
I occur, he occurs
I operate, he operates
I happen, he happens
I seem, he
2024-07-26 05:43:14 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 05:46:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.0781,  2.3750, -0.0703,  ...,  0.0273, -0.2188, -0.4023],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([16.5000, -8.5625,  6.6875,  ..., -9.5625, 11.3125, -5.3125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0262,  0.0065,  0.0048,  ..., -0.0005,  0.0058, -0.0008],
        [ 0.0081,  0.0154,  0.0003,  ...,  0.0048, -0.0029,  0.0034],
        [ 0.0114, -0.0041,  0.0889,  ...,  0.0034,  0.0060, -0.0057],
        ...,
        [-0.0023, -0.0007, -0.0132,  ...,  0.0168, -0.0032, -0.0083],
        [ 0.0079,  0.0037,  0.0001,  ..., -0.0010,  0.0310, -0.0007],
        [-0.0009,  0.0115, -0.0125,  ...,  0.0067, -0.0081,  0.0376]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[16.1250, -7.7695,  7.9258,  ..., -9.0156, 11.4531, -5.6875]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 05:46:01 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for I send, he sends
I promote, he promotes
I add, he adds
I identify, he identifies
I occur, he occurs
I operate, he operates
I happen, he happens
I seem, he
2024-07-26 05:46:01 root INFO     total operator prediction time: 1337.748595714569 seconds
2024-07-26 05:46:01 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on verb_Ving - 3pSg
2024-07-26 05:46:01 root INFO     building operator verb_Ving - 3pSg
2024-07-26 05:46:02 root INFO     [order_1_approx] starting weight calculation for When something is continuing, it continues
When something is happening, it happens
When something is thanking, it thanks
When something is asking, it asks
When something is referring, it refers
When something is applying, it applies
When something is hearing, it hears
When something is managing, it
2024-07-26 05:46:02 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 05:48:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1562,  0.6484,  2.2031,  ..., -1.0312, -0.6602, -0.1484],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 17.2500,   2.7812,  10.3125,  ..., -21.3750,  -1.0781, -11.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0317,  0.0020,  0.0030,  ...,  0.0007,  0.0025,  0.0019],
        [ 0.0074,  0.0188,  0.0084,  ..., -0.0009, -0.0017,  0.0001],
        [-0.0019,  0.0029,  0.0889,  ..., -0.0011, -0.0014, -0.0023],
        ...,
        [-0.0010,  0.0059,  0.0003,  ...,  0.0237, -0.0028, -0.0028],
        [ 0.0046,  0.0042, -0.0063,  ...,  0.0029,  0.0310, -0.0002],
        [-0.0026,  0.0047,  0.0001,  ..., -0.0004, -0.0013,  0.0308]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 17.5781,   5.3906,  10.9531,  ..., -22.1250,  -0.1968, -11.9609]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 05:48:51 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When something is continuing, it continues
When something is happening, it happens
When something is thanking, it thanks
When something is asking, it asks
When something is referring, it refers
When something is applying, it applies
When something is hearing, it hears
When something is managing, it
2024-07-26 05:48:52 root INFO     [order_1_approx] starting weight calculation for When something is asking, it asks
When something is hearing, it hears
When something is happening, it happens
When something is continuing, it continues
When something is thanking, it thanks
When something is referring, it refers
When something is managing, it manages
When something is applying, it
2024-07-26 05:48:52 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 05:51:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.3438, -1.2109,  5.2188,  ..., -0.4492,  1.7812,  0.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 24.1250, -12.1250,  16.6250,  ...,  -5.8125,   3.2031,  -2.4062],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0435, -0.0121, -0.0018,  ...,  0.0162,  0.0059, -0.0030],
        [-0.0109,  0.0317, -0.0024,  ..., -0.0219, -0.0010,  0.0082],
        [ 0.0010, -0.0007,  0.0874,  ..., -0.0027, -0.0002, -0.0010],
        ...,
        [ 0.0059,  0.0008, -0.0029,  ...,  0.0261,  0.0058,  0.0031],
        [ 0.0013,  0.0010, -0.0057,  ..., -0.0034,  0.0396,  0.0002],
        [ 0.0035, -0.0023,  0.0031,  ...,  0.0103, -0.0041,  0.0374]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 24.8438, -12.1719,  17.3750,  ...,  -5.1328,   2.3398,  -3.1699]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 05:51:42 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When something is asking, it asks
When something is hearing, it hears
When something is happening, it happens
When something is continuing, it continues
When something is thanking, it thanks
When something is referring, it refers
When something is managing, it manages
When something is applying, it
2024-07-26 05:51:42 root INFO     [order_1_approx] starting weight calculation for When something is continuing, it continues
When something is applying, it applies
When something is managing, it manages
When something is referring, it refers
When something is hearing, it hears
When something is thanking, it thanks
When something is happening, it happens
When something is asking, it
2024-07-26 05:51:42 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 05:54:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.5312, -2.0938,  3.0781,  ..., -0.1250,  1.8906, -1.5156],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([16.0000, -9.3750,  5.5312,  ..., -4.5000, 11.3750, -5.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.2583e-02, -1.5640e-03,  2.3193e-03,  ...,  3.0670e-03,
          3.4180e-03,  8.3923e-05],
        [ 3.1433e-03,  1.6846e-02, -5.0354e-03,  ..., -2.1057e-03,
         -2.4719e-03,  5.4932e-03],
        [ 2.0599e-03,  3.4180e-03,  7.8125e-02,  ..., -1.2360e-03,
         -1.7090e-03, -1.0986e-03],
        ...,
        [ 2.2888e-03,  1.7853e-03, -2.4567e-03,  ...,  1.5137e-02,
          5.2643e-04,  4.7302e-04],
        [ 3.6011e-03,  4.7913e-03, -2.3956e-03,  ...,  7.6294e-05,
          3.1982e-02, -2.2583e-03],
        [-3.8147e-06,  6.1798e-04, -2.1362e-03,  ...,  4.0436e-04,
         -2.3804e-03,  2.9785e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[14.0625, -7.2656,  6.1680,  ..., -2.4551,  9.8828, -5.3281]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 05:54:32 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When something is continuing, it continues
When something is applying, it applies
When something is managing, it manages
When something is referring, it refers
When something is hearing, it hears
When something is thanking, it thanks
When something is happening, it happens
When something is asking, it
2024-07-26 05:54:32 root INFO     [order_1_approx] starting weight calculation for When something is thanking, it thanks
When something is happening, it happens
When something is hearing, it hears
When something is asking, it asks
When something is continuing, it continues
When something is applying, it applies
When something is managing, it manages
When something is referring, it
2024-07-26 05:54:32 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 05:57:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.6406,  0.6172,  2.6719,  ...,  0.7773, -0.6406, -1.2812],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 17.5000,   0.7969,  11.0000,  ..., -11.5625,  13.6250, -11.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.8809e-02, -9.9182e-04, -1.1139e-03,  ..., -2.6550e-03,
          4.0588e-03,  4.5776e-05],
        [ 5.7373e-03,  1.9287e-02,  2.1515e-03,  ...,  1.0986e-03,
         -1.6022e-03,  1.6785e-03],
        [ 2.1973e-03, -2.3193e-03,  8.8867e-02,  ...,  1.6785e-04,
          1.6861e-03, -1.1215e-03],
        ...,
        [ 1.0757e-03,  2.0981e-04, -5.7983e-03,  ...,  2.0996e-02,
          1.0452e-03,  1.7319e-03],
        [ 8.4229e-03, -1.4648e-03,  1.5869e-03,  ...,  2.7466e-03,
          3.5400e-02, -8.0109e-04],
        [-2.1515e-03,  2.8534e-03, -3.4943e-03,  ...,  3.1586e-03,
         -1.7319e-03,  3.4668e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 16.7031,   0.3682,  11.6094,  ..., -11.7031,  15.4844, -13.8594]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 05:57:22 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When something is thanking, it thanks
When something is happening, it happens
When something is hearing, it hears
When something is asking, it asks
When something is continuing, it continues
When something is applying, it applies
When something is managing, it manages
When something is referring, it
2024-07-26 05:57:22 root INFO     [order_1_approx] starting weight calculation for When something is happening, it happens
When something is applying, it applies
When something is referring, it refers
When something is hearing, it hears
When something is asking, it asks
When something is managing, it manages
When something is continuing, it continues
When something is thanking, it
2024-07-26 05:57:22 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 06:00:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2969, -0.1328,  2.5312,  ...,  0.5508,  0.0625,  1.0469],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 16.2500,  -3.8438,   5.2500,  ...,  -4.6875,   8.7500, -10.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.6001e-02,  1.2436e-03,  3.2349e-03,  ..., -8.6975e-04,
          5.1270e-03,  3.2349e-03],
        [ 2.1515e-03,  1.6357e-02, -7.6294e-05,  ...,  1.4496e-03,
         -2.8229e-04,  3.5858e-03],
        [ 3.1586e-03, -3.2425e-04,  8.6914e-02,  ...,  2.0294e-03,
         -2.6550e-03, -1.6022e-03],
        ...,
        [ 5.6839e-04, -1.8921e-03, -4.4556e-03,  ...,  1.7334e-02,
          3.0518e-03,  1.9455e-04],
        [ 4.0283e-03, -4.5776e-03, -5.2795e-03,  ...,  5.1880e-04,
          3.0273e-02,  4.4556e-03],
        [-7.0953e-04,  1.8387e-03,  9.4604e-04,  ...,  9.1553e-04,
          2.0447e-03,  2.9053e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 16.7500,  -3.9316,   5.2227,  ...,  -4.9766,   8.1719, -12.4141]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 06:00:12 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When something is happening, it happens
When something is applying, it applies
When something is referring, it refers
When something is hearing, it hears
When something is asking, it asks
When something is managing, it manages
When something is continuing, it continues
When something is thanking, it
2024-07-26 06:00:12 root INFO     [order_1_approx] starting weight calculation for When something is thanking, it thanks
When something is happening, it happens
When something is continuing, it continues
When something is asking, it asks
When something is referring, it refers
When something is managing, it manages
When something is applying, it applies
When something is hearing, it
2024-07-26 06:00:12 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 06:03:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0547, -0.6016,  7.1875,  ...,  0.3047,  3.7344,  1.0938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 12.3125,  -1.7109,  15.7500,  ...,  -6.8125,   4.0625, -14.9375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0273, -0.0009,  0.0025,  ..., -0.0008,  0.0057,  0.0006],
        [ 0.0030,  0.0156,  0.0036,  ..., -0.0012, -0.0028,  0.0017],
        [-0.0009, -0.0010,  0.0796,  ...,  0.0011, -0.0007, -0.0018],
        ...,
        [ 0.0010,  0.0017,  0.0039,  ...,  0.0197, -0.0036,  0.0004],
        [ 0.0068,  0.0022, -0.0060,  ..., -0.0013,  0.0295, -0.0006],
        [-0.0009, -0.0001, -0.0051,  ...,  0.0041,  0.0047,  0.0262]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 13.0312,  -1.7461,  16.5938,  ...,  -6.1953,   4.8984, -15.1797]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 06:03:02 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When something is thanking, it thanks
When something is happening, it happens
When something is continuing, it continues
When something is asking, it asks
When something is referring, it refers
When something is managing, it manages
When something is applying, it applies
When something is hearing, it
2024-07-26 06:03:02 root INFO     [order_1_approx] starting weight calculation for When something is thanking, it thanks
When something is happening, it happens
When something is asking, it asks
When something is applying, it applies
When something is hearing, it hears
When something is managing, it manages
When something is referring, it refers
When something is continuing, it
2024-07-26 06:03:02 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 06:05:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.2188, -1.9688,  4.2812,  ...,  1.6328,  0.0811,  1.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 13.9375,   5.0625,  16.1250,  ..., -13.0625,  -5.7812, -12.8125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.4170e-02, -7.3624e-04, -1.3046e-03,  ...,  1.8768e-03,
          5.0659e-03, -3.9673e-03],
        [ 2.7313e-03,  1.2756e-02,  9.7656e-03,  ..., -6.6528e-03,
          7.1716e-03,  8.3008e-03],
        [-1.9836e-03, -2.7161e-03,  8.2031e-02,  ..., -7.6294e-05,
          3.0975e-03,  6.2866e-03],
        ...,
        [ 3.4485e-03,  2.8381e-03, -1.1368e-03,  ...,  2.3682e-02,
         -4.2725e-03, -3.8757e-03],
        [-1.7242e-03,  1.3733e-03, -6.2866e-03,  ..., -1.1215e-03,
          3.0396e-02,  2.8229e-04],
        [-1.2512e-03,  6.7139e-03, -5.3711e-03,  ...,  2.2278e-03,
          2.6245e-03,  2.5391e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 14.2422,   6.5508,  16.2500,  ..., -12.4688,  -6.7812, -13.9609]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 06:05:52 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When something is thanking, it thanks
When something is happening, it happens
When something is asking, it asks
When something is applying, it applies
When something is hearing, it hears
When something is managing, it manages
When something is referring, it refers
When something is continuing, it
2024-07-26 06:05:52 root INFO     [order_1_approx] starting weight calculation for When something is managing, it manages
When something is continuing, it continues
When something is asking, it asks
When something is applying, it applies
When something is hearing, it hears
When something is referring, it refers
When something is thanking, it thanks
When something is happening, it
2024-07-26 06:05:52 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 06:08:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7656,  0.1855,  3.7344,  ...,  0.5625,  1.9688,  3.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 13.1250,  -2.5781,  12.8750,  ...,  -2.9531,  -4.2500, -11.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.2227e-02, -2.9907e-03, -6.5918e-03,  ..., -1.9073e-05,
          5.0659e-03,  1.1826e-03],
        [ 5.6458e-03,  1.6113e-02,  2.0447e-03,  ...,  2.7466e-03,
         -2.4414e-03,  5.9891e-04],
        [-8.9722e-03,  6.1035e-05,  1.2598e-01,  ...,  1.6327e-03,
         -2.2125e-03, -2.7771e-03],
        ...,
        [ 3.3722e-03,  2.1362e-03, -1.0300e-03,  ...,  2.3926e-02,
          2.0905e-03,  1.0071e-03],
        [ 7.1716e-03, -2.6703e-03, -8.7280e-03,  ..., -2.8992e-03,
          4.3457e-02,  2.0447e-03],
        [-3.7079e-03,  4.7913e-03, -3.2654e-03,  ..., -1.9073e-03,
         -6.9580e-03,  3.6133e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 11.7266,  -0.4023,  13.3672,  ...,  -3.0234,  -4.5859, -12.8281]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 06:08:41 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When something is managing, it manages
When something is continuing, it continues
When something is asking, it asks
When something is applying, it applies
When something is hearing, it hears
When something is referring, it refers
When something is thanking, it thanks
When something is happening, it
2024-07-26 06:08:41 root INFO     total operator prediction time: 1359.7395765781403 seconds
2024-07-26 06:08:41 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on noun - plural_reg
2024-07-26 06:08:41 root INFO     building operator noun - plural_reg
2024-07-26 06:08:41 root INFO     [order_1_approx] starting weight calculation for The plural form of development is developments
The plural form of hour is hours
The plural form of language is languages
The plural form of example is examples
The plural form of science is sciences
The plural form of song is songs
The plural form of death is deaths
The plural form of customer is
2024-07-26 06:08:41 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 06:11:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1875, -1.0234,  0.9961,  ..., -0.2930, -0.6836,  0.2539],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 2.1719,  0.4688, -3.0938,  ..., -5.0938, -1.6719, -1.3438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0359,  0.0023, -0.0029,  ...,  0.0020,  0.0002, -0.0030],
        [ 0.0052,  0.0186,  0.0031,  ..., -0.0025,  0.0008,  0.0026],
        [ 0.0074,  0.0051,  0.1025,  ..., -0.0042,  0.0058, -0.0058],
        ...,
        [ 0.0070, -0.0022, -0.0066,  ...,  0.0223, -0.0076, -0.0047],
        [ 0.0025,  0.0049, -0.0005,  ..., -0.0018,  0.0349, -0.0029],
        [-0.0041,  0.0023, -0.0141,  ..., -0.0082,  0.0051,  0.0437]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.6562, -0.3799, -2.5234,  ..., -5.4453, -1.9482, -0.1260]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 06:11:31 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of development is developments
The plural form of hour is hours
The plural form of language is languages
The plural form of example is examples
The plural form of science is sciences
The plural form of song is songs
The plural form of death is deaths
The plural form of customer is
2024-07-26 06:11:31 root INFO     [order_1_approx] starting weight calculation for The plural form of science is sciences
The plural form of death is deaths
The plural form of language is languages
The plural form of development is developments
The plural form of example is examples
The plural form of customer is customers
The plural form of hour is hours
The plural form of song is
2024-07-26 06:11:31 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 06:14:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1094, -0.6875, -0.5898,  ..., -0.3164, -0.5938,  0.3926],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 24.0000, -10.6875,   6.6562,  ...,   1.4219,   3.5469,  -6.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.8931e-02, -1.7166e-03,  3.4424e-02,  ..., -6.6528e-03,
          6.9275e-03,  6.3171e-03],
        [ 8.0566e-03,  2.0752e-02, -6.6223e-03,  ..., -1.8845e-03,
         -3.1433e-03, -5.9891e-04],
        [ 2.5024e-03, -1.0605e-03,  1.2109e-01,  ..., -5.3406e-03,
          1.0681e-04, -1.8311e-04],
        ...,
        [ 1.2512e-03, -6.4697e-03,  1.4221e-02,  ...,  2.2705e-02,
         -4.3030e-03, -2.6093e-03],
        [-4.3640e-03,  3.1281e-03,  2.2461e-02,  ..., -6.1035e-04,
          3.8574e-02, -2.8534e-03],
        [ 1.3580e-03, -6.3477e-03,  8.7280e-03,  ..., -8.5449e-03,
          4.3945e-03,  4.1016e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[21.2500, -9.7500,  4.7109,  ..., -0.8379,  3.3535, -8.2031]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 06:14:21 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of science is sciences
The plural form of death is deaths
The plural form of language is languages
The plural form of development is developments
The plural form of example is examples
The plural form of customer is customers
The plural form of hour is hours
The plural form of song is
2024-07-26 06:14:21 root INFO     [order_1_approx] starting weight calculation for The plural form of language is languages
The plural form of example is examples
The plural form of hour is hours
The plural form of song is songs
The plural form of death is deaths
The plural form of customer is customers
The plural form of science is sciences
The plural form of development is
2024-07-26 06:14:21 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 06:17:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.5234, -2.0781, -0.0103,  ..., -0.3828, -0.3555, -1.6016],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  7.3750,   1.6328,   2.3594,  ..., -10.0625,  -2.6562,   0.0938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0304, -0.0034,  0.0047,  ...,  0.0068, -0.0014, -0.0001],
        [ 0.0047,  0.0181,  0.0070,  ..., -0.0025, -0.0042,  0.0012],
        [ 0.0015,  0.0045,  0.1123,  ...,  0.0003,  0.0029,  0.0043],
        ...,
        [ 0.0024, -0.0023, -0.0123,  ...,  0.0267,  0.0016, -0.0012],
        [ 0.0006,  0.0079,  0.0035,  ...,  0.0007,  0.0371,  0.0013],
        [-0.0005,  0.0007, -0.0065,  ..., -0.0070,  0.0085,  0.0410]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 7.4453,  1.5742,  3.7461,  ..., -9.8672, -0.7334,  0.5430]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 06:17:08 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of language is languages
The plural form of example is examples
The plural form of hour is hours
The plural form of song is songs
The plural form of death is deaths
The plural form of customer is customers
The plural form of science is sciences
The plural form of development is
2024-07-26 06:17:08 root INFO     [order_1_approx] starting weight calculation for The plural form of science is sciences
The plural form of development is developments
The plural form of song is songs
The plural form of customer is customers
The plural form of death is deaths
The plural form of hour is hours
The plural form of example is examples
The plural form of language is
2024-07-26 06:17:09 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 06:19:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-3.3125, -1.9688, -1.9453,  ..., -0.6992, -1.7891, -0.7383],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 10.8750, -11.6250,   5.7812,  ...,  -7.5000,  -0.4688,  -4.8125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0273,  0.0054, -0.0055,  ...,  0.0052,  0.0042, -0.0030],
        [ 0.0038,  0.0198,  0.0045,  ...,  0.0005, -0.0062,  0.0026],
        [ 0.0029,  0.0009,  0.1079,  ..., -0.0004, -0.0040, -0.0017],
        ...,
        [ 0.0047, -0.0080,  0.0015,  ...,  0.0225, -0.0065, -0.0023],
        [ 0.0009,  0.0036,  0.0025,  ..., -0.0016,  0.0359, -0.0006],
        [ 0.0048, -0.0044,  0.0013,  ..., -0.0013, -0.0019,  0.0425]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 10.8281, -11.1328,   6.1445,  ...,  -7.6133,   1.3604,  -3.8906]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 06:19:58 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of science is sciences
The plural form of development is developments
The plural form of song is songs
The plural form of customer is customers
The plural form of death is deaths
The plural form of hour is hours
The plural form of example is examples
The plural form of language is
2024-07-26 06:19:58 root INFO     [order_1_approx] starting weight calculation for The plural form of science is sciences
The plural form of development is developments
The plural form of hour is hours
The plural form of language is languages
The plural form of song is songs
The plural form of customer is customers
The plural form of death is deaths
The plural form of example is
2024-07-26 06:19:58 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 06:22:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8516, -1.6875,  4.5312,  ...,  0.2500, -1.2969,  3.2969],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 17.0000, -13.3750,   7.7812,  ...,  -1.4531,  -0.9414,  -0.0938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0444,  0.0112,  0.0083,  ...,  0.0139,  0.0026, -0.0140],
        [-0.0120,  0.0028,  0.0225,  ..., -0.0320, -0.0005,  0.0215],
        [ 0.0264,  0.0183,  0.0889,  ...,  0.0242, -0.0038, -0.0234],
        ...,
        [-0.0052, -0.0177,  0.0045,  ...,  0.0208, -0.0031,  0.0085],
        [ 0.0052,  0.0023, -0.0026,  ...,  0.0051,  0.0403, -0.0025],
        [ 0.0111,  0.0124, -0.0162,  ...,  0.0151,  0.0072,  0.0320]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 15.0859, -12.9297,   5.8477,  ...,  -0.8638,  -1.7188,   0.5537]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 06:22:48 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of science is sciences
The plural form of development is developments
The plural form of hour is hours
The plural form of language is languages
The plural form of song is songs
The plural form of customer is customers
The plural form of death is deaths
The plural form of example is
2024-07-26 06:22:48 root INFO     [order_1_approx] starting weight calculation for The plural form of song is songs
The plural form of hour is hours
The plural form of example is examples
The plural form of customer is customers
The plural form of development is developments
The plural form of death is deaths
The plural form of language is languages
The plural form of science is
2024-07-26 06:22:48 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 06:25:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1953, -2.0469, -0.9375,  ..., -0.7422, -0.3242, -1.9219],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([13.2500, -6.0000,  0.3594,  ...,  3.7969, -2.1094, -7.0312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0221, -0.0040,  0.0036,  ...,  0.0038,  0.0018, -0.0026],
        [-0.0008,  0.0154,  0.0070,  ..., -0.0046, -0.0041, -0.0027],
        [ 0.0009, -0.0034,  0.0996,  ..., -0.0032, -0.0044, -0.0068],
        ...,
        [ 0.0069, -0.0034,  0.0046,  ...,  0.0217, -0.0014, -0.0017],
        [-0.0004,  0.0025,  0.0059,  ..., -0.0003,  0.0280,  0.0006],
        [-0.0025, -0.0085, -0.0069,  ..., -0.0039,  0.0023,  0.0376]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[13.5234, -6.4766,  1.5645,  ...,  4.4609, -0.3633, -6.9102]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 06:25:36 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of song is songs
The plural form of hour is hours
The plural form of example is examples
The plural form of customer is customers
The plural form of development is developments
The plural form of death is deaths
The plural form of language is languages
The plural form of science is
2024-07-26 06:25:37 root INFO     [order_1_approx] starting weight calculation for The plural form of song is songs
The plural form of customer is customers
The plural form of language is languages
The plural form of development is developments
The plural form of hour is hours
The plural form of science is sciences
The plural form of example is examples
The plural form of death is
2024-07-26 06:25:37 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 06:28:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5312, -0.5469, -0.2002,  ..., -1.3281,  0.4414, -1.8125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-0.0391,  0.3125,  2.4219,  ..., -3.7969, -3.8125, -5.7188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0229,  0.0017, -0.0050,  ..., -0.0002, -0.0064,  0.0026],
        [ 0.0024,  0.0096,  0.0137,  ..., -0.0025, -0.0099, -0.0005],
        [ 0.0019,  0.0057,  0.0996,  ..., -0.0021,  0.0112, -0.0013],
        ...,
        [ 0.0046, -0.0038, -0.0066,  ...,  0.0232, -0.0060,  0.0019],
        [-0.0010,  0.0035,  0.0015,  ...,  0.0019,  0.0371,  0.0017],
        [ 0.0003, -0.0018, -0.0098,  ..., -0.0041,  0.0041,  0.0410]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5630, -1.2451,  4.0117,  ..., -3.7227, -3.3223, -4.7227]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 06:28:26 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of song is songs
The plural form of customer is customers
The plural form of language is languages
The plural form of development is developments
The plural form of hour is hours
The plural form of science is sciences
The plural form of example is examples
The plural form of death is
2024-07-26 06:28:26 root INFO     [order_1_approx] starting weight calculation for The plural form of example is examples
The plural form of language is languages
The plural form of death is deaths
The plural form of song is songs
The plural form of customer is customers
The plural form of science is sciences
The plural form of development is developments
The plural form of hour is
2024-07-26 06:28:27 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 06:31:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3906, -2.5938, -2.3281,  ..., -1.6875,  0.4531,  1.4219],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  9.7500,  -8.4375,  -8.3125,  ...,   1.4062,  -4.9375, -17.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.5156e-02, -1.5259e-03, -9.5367e-05,  ...,  1.1902e-02,
          6.2256e-03, -2.8992e-03],
        [ 8.7891e-03,  2.1973e-02, -1.7624e-03,  ..., -3.0518e-05,
         -5.8289e-03,  3.2196e-03],
        [ 4.3335e-03,  3.1738e-03,  1.0791e-01,  ...,  1.1963e-02,
          1.3062e-02, -9.5215e-03],
        ...,
        [-1.3199e-03, -7.2632e-03, -1.9836e-04,  ...,  2.3560e-02,
         -1.0315e-02,  1.6556e-03],
        [-4.9744e-03,  7.1411e-03, -1.0925e-02,  ..., -4.8065e-04,
          4.1748e-02, -5.6458e-03],
        [-7.2632e-03, -3.1662e-04, -1.5991e-02,  ...,  4.5471e-03,
          1.2695e-02,  3.9551e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  9.1250,  -6.3711, -10.4844,  ...,   3.1016,  -5.6719, -17.6094]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 06:31:16 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of example is examples
The plural form of language is languages
The plural form of death is deaths
The plural form of song is songs
The plural form of customer is customers
The plural form of science is sciences
The plural form of development is developments
The plural form of hour is
2024-07-26 06:31:16 root INFO     total operator prediction time: 1355.3577930927277 seconds
2024-07-26 06:31:16 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on verb_3pSg - Ved
2024-07-26 06:31:16 root INFO     building operator verb_3pSg - Ved
2024-07-26 06:31:17 root INFO     [order_1_approx] starting weight calculation for When he manages something, something has been managed
When he tells something, something has been told
When he agrees something, something has been agreed
When he applies something, something has been applied
When he involves something, something has been involved
When he consists something, something has been consisted
When he sends something, something has been sent
When he loses something, something has been
2024-07-26 06:31:17 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 06:34:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1328,  3.1562,  1.8906,  ...,  0.0859, -1.0469, -1.4609],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 17.0000, -17.7500,   3.4531,  ...,  -5.3125,   5.3125,  -8.9375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0227,  0.0018,  0.0118,  ..., -0.0010, -0.0063, -0.0030],
        [ 0.0074,  0.0145, -0.0080,  ..., -0.0042,  0.0092,  0.0033],
        [-0.0007,  0.0040,  0.0850,  ..., -0.0020, -0.0030, -0.0084],
        ...,
        [-0.0028, -0.0001, -0.0061,  ...,  0.0182,  0.0025, -0.0049],
        [ 0.0064,  0.0043, -0.0140,  ..., -0.0050,  0.0332,  0.0004],
        [-0.0020,  0.0066, -0.0101,  ..., -0.0010, -0.0051,  0.0272]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 17.5156, -16.8906,   3.7676,  ...,  -4.1797,   6.3906,  -9.4062]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 06:34:04 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When he manages something, something has been managed
When he tells something, something has been told
When he agrees something, something has been agreed
When he applies something, something has been applied
When he involves something, something has been involved
When he consists something, something has been consisted
When he sends something, something has been sent
When he loses something, something has been
2024-07-26 06:34:04 root INFO     [order_1_approx] starting weight calculation for When he agrees something, something has been agreed
When he tells something, something has been told
When he manages something, something has been managed
When he loses something, something has been lost
When he applies something, something has been applied
When he sends something, something has been sent
When he involves something, something has been involved
When he consists something, something has been
2024-07-26 06:34:04 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 06:36:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5664,  2.0469,  1.6406,  ..., -1.3281,  0.6250,  1.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 6.6250, -3.5000,  8.7500,  ..., -9.8750,  4.5938,  4.8125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0293,  0.0026,  0.0027,  ...,  0.0068, -0.0110, -0.0085],
        [ 0.0084,  0.0100,  0.0052,  ..., -0.0051,  0.0152,  0.0123],
        [ 0.0042, -0.0054,  0.0850,  ..., -0.0075,  0.0109,  0.0020],
        ...,
        [ 0.0038, -0.0041, -0.0009,  ...,  0.0197,  0.0046,  0.0002],
        [-0.0057,  0.0002, -0.0036,  ..., -0.0025,  0.0304,  0.0039],
        [-0.0004,  0.0017, -0.0009,  ...,  0.0006, -0.0030,  0.0383]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.5703,  0.3691,  9.8750,  ..., -7.8281,  4.6094,  3.0586]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 06:36:52 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When he agrees something, something has been agreed
When he tells something, something has been told
When he manages something, something has been managed
When he loses something, something has been lost
When he applies something, something has been applied
When he sends something, something has been sent
When he involves something, something has been involved
When he consists something, something has been
2024-07-26 06:36:52 root INFO     [order_1_approx] starting weight calculation for When he tells something, something has been told
When he agrees something, something has been agreed
When he loses something, something has been lost
When he involves something, something has been involved
When he applies something, something has been applied
When he sends something, something has been sent
When he consists something, something has been consisted
When he manages something, something has been
2024-07-26 06:36:52 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 06:39:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1875,  1.4531,  5.5312,  ..., -0.2461, -1.0547,  0.4297],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 13.0000,  -3.6719,   7.7500,  ..., -18.6250,  -2.0625,  -3.7812],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.2095e-02, -4.6158e-04,  2.1820e-03,  ..., -3.2425e-04,
          3.5858e-04,  6.3705e-04],
        [ 4.3030e-03,  1.3733e-02,  2.4567e-03,  ..., -2.6703e-03,
          1.9302e-03,  5.0354e-03],
        [ 1.0834e-03,  3.0060e-03,  6.3965e-02,  ..., -2.2583e-03,
         -1.1444e-03, -9.3079e-04],
        ...,
        [ 1.3123e-03,  1.8005e-03, -3.5095e-03,  ...,  1.2939e-02,
          1.7929e-03,  1.4191e-03],
        [ 3.6163e-03,  3.1891e-03, -5.6076e-04,  ..., -1.1444e-05,
          2.6367e-02,  1.4496e-04],
        [ 3.2425e-04,  1.4191e-03, -1.0010e-02,  ..., -1.5411e-03,
         -4.6997e-03,  2.5879e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 12.5391,  -2.0898,   8.1562,  ..., -18.0156,  -1.4570,  -2.8164]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 06:39:39 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When he tells something, something has been told
When he agrees something, something has been agreed
When he loses something, something has been lost
When he involves something, something has been involved
When he applies something, something has been applied
When he sends something, something has been sent
When he consists something, something has been consisted
When he manages something, something has been
2024-07-26 06:39:39 root INFO     [order_1_approx] starting weight calculation for When he loses something, something has been lost
When he agrees something, something has been agreed
When he consists something, something has been consisted
When he tells something, something has been told
When he manages something, something has been managed
When he sends something, something has been sent
When he involves something, something has been involved
When he applies something, something has been
2024-07-26 06:39:39 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 06:42:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9570,  0.8359,  6.4688,  ...,  0.8945,  2.1719,  0.3730],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 24.5000, -15.1875,  12.3750,  ...,  -7.2500,   0.8789,   1.2969],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.1729e-02, -4.3030e-03, -6.8359e-03,  ...,  3.5095e-04,
         -1.5564e-03, -4.9973e-04],
        [ 2.2888e-05,  1.5381e-02, -1.2329e-02,  ..., -6.8359e-03,
          1.1292e-03,  6.3477e-03],
        [-1.4038e-03, -8.3923e-05,  6.3477e-02,  ...,  2.3499e-03,
         -4.2114e-03, -3.0823e-03],
        ...,
        [ 4.1504e-03,  1.3733e-03, -9.4986e-04,  ...,  1.5625e-02,
          6.6528e-03,  5.4626e-03],
        [ 1.4877e-03, -1.6403e-04, -4.9133e-03,  ...,  2.6245e-03,
          2.3071e-02, -4.3335e-03],
        [ 1.2207e-04, -7.6294e-04,  1.8997e-03,  ...,  3.2349e-03,
         -3.4485e-03,  2.8320e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 23.1562, -13.9922,  13.1875,  ...,  -6.4219,  -0.0728,  -0.9531]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 06:42:26 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When he loses something, something has been lost
When he agrees something, something has been agreed
When he consists something, something has been consisted
When he tells something, something has been told
When he manages something, something has been managed
When he sends something, something has been sent
When he involves something, something has been involved
When he applies something, something has been
2024-07-26 06:42:26 root INFO     [order_1_approx] starting weight calculation for When he manages something, something has been managed
When he involves something, something has been involved
When he sends something, something has been sent
When he loses something, something has been lost
When he agrees something, something has been agreed
When he consists something, something has been consisted
When he applies something, something has been applied
When he tells something, something has been
2024-07-26 06:42:26 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 06:45:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1348,  2.8594,  5.9375,  ...,  0.9258,  1.0625, -0.7188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 24.5000, -10.7500,   8.0625,  ..., -11.6875,   2.2500,  -4.9375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0261, -0.0012, -0.0013,  ...,  0.0021,  0.0020, -0.0026],
        [ 0.0006,  0.0140, -0.0112,  ..., -0.0031,  0.0006,  0.0056],
        [ 0.0004,  0.0016,  0.0762,  ..., -0.0010,  0.0007, -0.0009],
        ...,
        [ 0.0040, -0.0004, -0.0118,  ...,  0.0166,  0.0021,  0.0017],
        [ 0.0071,  0.0032, -0.0090,  ...,  0.0008,  0.0347, -0.0014],
        [ 0.0026,  0.0043, -0.0048,  ...,  0.0008,  0.0003,  0.0293]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 25.6562, -10.6328,   8.8438,  ..., -11.8281,   1.6973,  -5.2266]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 06:45:13 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When he manages something, something has been managed
When he involves something, something has been involved
When he sends something, something has been sent
When he loses something, something has been lost
When he agrees something, something has been agreed
When he consists something, something has been consisted
When he applies something, something has been applied
When he tells something, something has been
2024-07-26 06:45:13 root INFO     [order_1_approx] starting weight calculation for When he loses something, something has been lost
When he applies something, something has been applied
When he sends something, something has been sent
When he manages something, something has been managed
When he consists something, something has been consisted
When he involves something, something has been involved
When he tells something, something has been told
When he agrees something, something has been
2024-07-26 06:45:13 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 06:48:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1328,  1.7422,  5.5625,  ...,  0.5078, -0.2227, -0.0508],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 18.6250, -15.6875,   3.5938,  ..., -13.7500,  -1.1328, -13.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.7822e-02,  2.3499e-03, -1.6785e-03,  ..., -2.2888e-05,
          6.1035e-05, -5.1270e-03],
        [ 6.0654e-04,  1.2085e-02,  5.4932e-04,  ..., -4.3335e-03,
         -1.1368e-03,  1.0223e-03],
        [ 2.6855e-03,  1.4305e-04,  6.3965e-02,  ..., -3.6621e-04,
          4.5776e-04, -1.4496e-04],
        ...,
        [ 1.2207e-03, -1.2894e-03, -1.4954e-03,  ...,  1.1230e-02,
          1.2360e-03,  2.5330e-03],
        [ 2.8076e-03,  3.0060e-03, -6.8970e-03,  ..., -2.4414e-03,
          2.1362e-02, -2.0447e-03],
        [-6.8665e-04,  2.8381e-03, -2.1057e-03,  ..., -1.2512e-03,
         -1.7853e-03,  2.1973e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 18.8438, -15.2656,   4.5391,  ..., -11.9844,  -1.8105, -14.5625]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 06:48:00 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When he loses something, something has been lost
When he applies something, something has been applied
When he sends something, something has been sent
When he manages something, something has been managed
When he consists something, something has been consisted
When he involves something, something has been involved
When he tells something, something has been told
When he agrees something, something has been
2024-07-26 06:48:00 root INFO     [order_1_approx] starting weight calculation for When he consists something, something has been consisted
When he loses something, something has been lost
When he manages something, something has been managed
When he applies something, something has been applied
When he tells something, something has been told
When he involves something, something has been involved
When he agrees something, something has been agreed
When he sends something, something has been
2024-07-26 06:48:01 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 06:50:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.1562,  0.7852,  3.2344,  ...,  0.5117, -1.7344,  2.1719],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 26.2500, -21.1250,   6.5625,  ...,  -9.7500,   0.9219,  -8.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.7090e-02, -2.5787e-03, -7.4768e-03,  ..., -5.9814e-03,
         -2.9449e-03, -1.7853e-03],
        [ 7.2632e-03,  7.2937e-03, -5.9509e-03,  ..., -3.5248e-03,
          1.4343e-03, -1.2207e-04],
        [-5.7602e-04,  2.4414e-03,  6.3477e-02,  ...,  2.1744e-04,
         -4.2114e-03, -1.9531e-03],
        ...,
        [ 5.6763e-03, -8.2016e-04, -6.3477e-03,  ...,  1.3428e-02,
          5.4016e-03,  2.0142e-03],
        [ 7.1716e-03,  2.1362e-03, -1.1108e-02,  ..., -9.1553e-04,
          2.4414e-02, -1.6861e-03],
        [-1.0681e-03, -4.8828e-04, -6.8359e-03,  ...,  3.0518e-05,
         -2.9602e-03,  2.3071e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 25.0312, -21.2656,   6.0430,  ..., -10.8672,   1.5146, -10.0859]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 06:50:48 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When he consists something, something has been consisted
When he loses something, something has been lost
When he manages something, something has been managed
When he applies something, something has been applied
When he tells something, something has been told
When he involves something, something has been involved
When he agrees something, something has been agreed
When he sends something, something has been
2024-07-26 06:50:48 root INFO     [order_1_approx] starting weight calculation for When he manages something, something has been managed
When he tells something, something has been told
When he consists something, something has been consisted
When he agrees something, something has been agreed
When he applies something, something has been applied
When he sends something, something has been sent
When he loses something, something has been lost
When he involves something, something has been
2024-07-26 06:50:48 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 06:53:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6484, -1.3828,  3.4219,  ...,  0.5547, -0.6953,  0.6602],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  9.6875, -15.5625,   9.5625,  ..., -14.1250,   9.6875,  -0.7188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0295, -0.0006, -0.0145,  ..., -0.0002, -0.0040, -0.0037],
        [ 0.0012,  0.0164,  0.0044,  ..., -0.0017,  0.0042,  0.0045],
        [ 0.0030,  0.0022,  0.0889,  ..., -0.0031,  0.0056, -0.0018],
        ...,
        [ 0.0015, -0.0032, -0.0050,  ...,  0.0250,  0.0029, -0.0013],
        [ 0.0016, -0.0003,  0.0023,  ..., -0.0031,  0.0364,  0.0001],
        [-0.0043,  0.0014,  0.0028,  ...,  0.0001, -0.0014,  0.0361]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  8.9531, -16.6875,  12.4766,  ..., -13.1172,   9.2500,  -2.6172]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 06:53:35 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When he manages something, something has been managed
When he tells something, something has been told
When he consists something, something has been consisted
When he agrees something, something has been agreed
When he applies something, something has been applied
When he sends something, something has been sent
When he loses something, something has been lost
When he involves something, something has been
2024-07-26 06:53:35 root INFO     total operator prediction time: 1338.365448474884 seconds
2024-07-26 06:53:35 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on adj - superlative
2024-07-26 06:53:35 root INFO     building operator adj - superlative
2024-07-26 06:53:35 root INFO     [order_1_approx] starting weight calculation for If something is the most rare, it is rarest
If something is the most strange, it is strangest
If something is the most dense, it is densest
If something is the most weird, it is weirdest
If something is the most happy, it is happiest
If something is the most huge, it is hugest
If something is the most pure, it is purest
If something is the most nasty, it is
2024-07-26 06:53:35 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 06:56:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0908,  1.9609,  2.9219,  ...,  0.7930, -0.1641,  0.1719],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([14.0000, 22.3750, -4.9375,  ...,  2.7344, -7.3125,  8.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0244,  0.0029, -0.0093,  ..., -0.0033,  0.0033, -0.0052],
        [ 0.0054,  0.0183,  0.0023,  ...,  0.0052,  0.0045,  0.0033],
        [-0.0010,  0.0041,  0.0771,  ..., -0.0006, -0.0061, -0.0004],
        ...,
        [ 0.0014, -0.0024, -0.0034,  ...,  0.0165, -0.0049, -0.0072],
        [ 0.0046,  0.0069, -0.0063,  ..., -0.0028,  0.0359,  0.0003],
        [-0.0081,  0.0064,  0.0054,  ..., -0.0065, -0.0110,  0.0267]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[12.7344, 19.7344, -6.0469,  ...,  1.4121, -9.9062, 10.6797]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 06:56:21 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most rare, it is rarest
If something is the most strange, it is strangest
If something is the most dense, it is densest
If something is the most weird, it is weirdest
If something is the most happy, it is happiest
If something is the most huge, it is hugest
If something is the most pure, it is purest
If something is the most nasty, it is
2024-07-26 06:56:21 root INFO     [order_1_approx] starting weight calculation for If something is the most rare, it is rarest
If something is the most strange, it is strangest
If something is the most happy, it is happiest
If something is the most dense, it is densest
If something is the most huge, it is hugest
If something is the most pure, it is purest
If something is the most nasty, it is nastiest
If something is the most weird, it is
2024-07-26 06:56:21 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 06:59:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2969, -0.5000, -0.6602,  ..., -1.5469,  2.6250,  0.1641],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 25.6250,  14.9375,  -2.6562,  ..., -12.7500,   2.6562,  -1.7656],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0176, -0.0011, -0.0022,  ...,  0.0018,  0.0047, -0.0051],
        [ 0.0034,  0.0116, -0.0014,  ..., -0.0004,  0.0076, -0.0046],
        [ 0.0042,  0.0093,  0.0732,  ..., -0.0012,  0.0036, -0.0051],
        ...,
        [ 0.0024,  0.0014, -0.0028,  ...,  0.0130, -0.0065, -0.0042],
        [-0.0005,  0.0027,  0.0087,  ..., -0.0031,  0.0145,  0.0011],
        [-0.0060,  0.0020,  0.0042,  ..., -0.0031, -0.0088,  0.0209]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 23.7812,  14.1094,  -3.9492,  ..., -13.4141,   5.4453,  -0.0596]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 06:59:07 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most rare, it is rarest
If something is the most strange, it is strangest
If something is the most happy, it is happiest
If something is the most dense, it is densest
If something is the most huge, it is hugest
If something is the most pure, it is purest
If something is the most nasty, it is nastiest
If something is the most weird, it is
2024-07-26 06:59:08 root INFO     [order_1_approx] starting weight calculation for If something is the most huge, it is hugest
If something is the most pure, it is purest
If something is the most strange, it is strangest
If something is the most happy, it is happiest
If something is the most weird, it is weirdest
If something is the most nasty, it is nastiest
If something is the most rare, it is rarest
If something is the most dense, it is
2024-07-26 06:59:08 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 07:01:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5703, -0.1836,  2.5156,  ..., -1.0781, -1.4922, -2.0469],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 16.1250,  18.1250,   4.9062,  ..., -13.6875,  -8.4375,  -0.8203],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0233, -0.0014, -0.0003,  ...,  0.0060,  0.0058, -0.0018],
        [ 0.0090,  0.0173,  0.0057,  ..., -0.0053, -0.0030,  0.0016],
        [ 0.0035,  0.0008,  0.0674,  ..., -0.0044,  0.0007,  0.0011],
        ...,
        [ 0.0081, -0.0015,  0.0008,  ...,  0.0182, -0.0047, -0.0079],
        [ 0.0002,  0.0029, -0.0044,  ..., -0.0059,  0.0280,  0.0069],
        [-0.0024,  0.0038, -0.0087,  ..., -0.0078, -0.0064,  0.0249]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 17.2344,  19.5156,   3.2031,  ..., -13.4844,  -7.8398,  -1.1523]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 07:01:54 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most huge, it is hugest
If something is the most pure, it is purest
If something is the most strange, it is strangest
If something is the most happy, it is happiest
If something is the most weird, it is weirdest
If something is the most nasty, it is nastiest
If something is the most rare, it is rarest
If something is the most dense, it is
2024-07-26 07:01:54 root INFO     [order_1_approx] starting weight calculation for If something is the most strange, it is strangest
If something is the most dense, it is densest
If something is the most pure, it is purest
If something is the most nasty, it is nastiest
If something is the most rare, it is rarest
If something is the most weird, it is weirdest
If something is the most huge, it is hugest
If something is the most happy, it is
2024-07-26 07:01:54 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 07:04:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0000,  1.9062,  3.6406,  ...,  0.8867, -1.5234,  0.6289],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([11.9375, 20.7500,  6.9688,  ..., -3.7656, -6.9375, -3.0781],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0205, -0.0035,  0.0069,  ...,  0.0008,  0.0013, -0.0036],
        [ 0.0038,  0.0177,  0.0032,  ...,  0.0018,  0.0020,  0.0051],
        [ 0.0082,  0.0028,  0.0664,  ..., -0.0034, -0.0037, -0.0029],
        ...,
        [ 0.0017, -0.0014, -0.0006,  ...,  0.0164, -0.0040, -0.0074],
        [ 0.0040,  0.0017, -0.0047,  ...,  0.0011,  0.0228,  0.0035],
        [ 0.0033,  0.0035, -0.0063,  ..., -0.0034, -0.0026,  0.0278]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[12.2188, 20.6562,  6.8281,  ..., -3.4688, -7.5156, -2.3535]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 07:04:40 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most strange, it is strangest
If something is the most dense, it is densest
If something is the most pure, it is purest
If something is the most nasty, it is nastiest
If something is the most rare, it is rarest
If something is the most weird, it is weirdest
If something is the most huge, it is hugest
If something is the most happy, it is
2024-07-26 07:04:40 root INFO     [order_1_approx] starting weight calculation for If something is the most nasty, it is nastiest
If something is the most strange, it is strangest
If something is the most huge, it is hugest
If something is the most happy, it is happiest
If something is the most weird, it is weirdest
If something is the most rare, it is rarest
If something is the most dense, it is densest
If something is the most pure, it is
2024-07-26 07:04:40 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 07:07:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1328, -0.2656,  0.3594,  ...,  1.0938,  1.9844, -1.7969],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([16.3750, 16.7500,  0.6484,  ..., -9.3750, -7.6562,  3.7344],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0190, -0.0028,  0.0073,  ...,  0.0002,  0.0030, -0.0018],
        [ 0.0014,  0.0168, -0.0014,  ..., -0.0018, -0.0016,  0.0041],
        [ 0.0043,  0.0011,  0.0605,  ..., -0.0032, -0.0044,  0.0015],
        ...,
        [-0.0018, -0.0053, -0.0066,  ...,  0.0183, -0.0003, -0.0014],
        [-0.0006,  0.0017, -0.0089,  ..., -0.0047,  0.0260,  0.0051],
        [-0.0004,  0.0035,  0.0005,  ..., -0.0071, -0.0025,  0.0189]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[14.9219, 17.5781,  1.9980,  ..., -9.9062, -6.9258,  5.4883]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 07:07:26 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most nasty, it is nastiest
If something is the most strange, it is strangest
If something is the most huge, it is hugest
If something is the most happy, it is happiest
If something is the most weird, it is weirdest
If something is the most rare, it is rarest
If something is the most dense, it is densest
If something is the most pure, it is
2024-07-26 07:07:27 root INFO     [order_1_approx] starting weight calculation for If something is the most weird, it is weirdest
If something is the most happy, it is happiest
If something is the most dense, it is densest
If something is the most pure, it is purest
If something is the most nasty, it is nastiest
If something is the most strange, it is strangest
If something is the most rare, it is rarest
If something is the most huge, it is
2024-07-26 07:07:27 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 07:10:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.5469, -0.2578,  0.0879,  ...,  1.1094, -0.6484,  2.2188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 16.8750,  13.9375,  -4.4375,  ...,  -5.5000, -16.3750,  -5.8438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.7334e-02,  5.9128e-04,  9.0332e-03,  ...,  1.8539e-03,
          1.9073e-03, -9.2163e-03],
        [ 8.7891e-03,  2.0752e-02, -8.6670e-03,  ..., -7.6294e-06,
          1.0437e-02,  3.3112e-03],
        [ 4.0588e-03,  7.1411e-03,  7.0801e-02,  ...,  2.2507e-04,
          3.5400e-03, -2.3193e-03],
        ...,
        [-9.1553e-05, -3.0823e-03,  4.6082e-03,  ...,  1.3428e-02,
         -7.6599e-03, -1.1719e-02],
        [ 3.8757e-03,  4.4250e-03, -7.0190e-03,  ..., -4.1199e-03,
          2.3804e-02,  2.6550e-03],
        [ 6.1798e-04,  7.2632e-03, -1.9043e-02,  ..., -2.7313e-03,
         -5.0659e-03,  2.1851e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 15.9688,  13.8438,  -5.4062,  ...,  -5.4375, -17.5781,  -5.9219]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 07:10:13 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most weird, it is weirdest
If something is the most happy, it is happiest
If something is the most dense, it is densest
If something is the most pure, it is purest
If something is the most nasty, it is nastiest
If something is the most strange, it is strangest
If something is the most rare, it is rarest
If something is the most huge, it is
2024-07-26 07:10:13 root INFO     [order_1_approx] starting weight calculation for If something is the most dense, it is densest
If something is the most nasty, it is nastiest
If something is the most weird, it is weirdest
If something is the most pure, it is purest
If something is the most huge, it is hugest
If something is the most happy, it is happiest
If something is the most rare, it is rarest
If something is the most strange, it is
2024-07-26 07:10:13 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 07:12:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6953, -0.9219, -0.1172,  ..., -2.5156,  2.7500,  0.6016],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 23.0000,  20.2500,  -0.1719,  ..., -13.8125, -10.8125,  -1.2344],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.5503e-02,  1.6632e-03,  8.3008e-03,  ..., -8.0109e-04,
          8.3008e-03, -1.3733e-03],
        [ 4.4250e-03,  1.3855e-02, -8.1177e-03,  ..., -2.4261e-03,
          6.1646e-03,  3.8757e-03],
        [ 1.0147e-03,  7.0496e-03,  7.1289e-02,  ..., -3.9673e-03,
          2.1362e-03, -1.1444e-03],
        ...,
        [-1.8692e-04, -8.3923e-05, -1.7166e-03,  ...,  1.0071e-02,
         -1.9531e-03, -4.1504e-03],
        [ 6.4697e-03,  1.3275e-03, -8.8501e-03,  ..., -4.6997e-03,
          1.5503e-02, -1.3351e-03],
        [-8.9111e-03,  6.8359e-03, -2.8381e-03,  ..., -5.5542e-03,
         -5.8594e-03,  2.2583e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.2438e+01,  1.8406e+01,  5.6152e-03,  ..., -1.5141e+01,
         -9.5391e+00, -3.7549e-01]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SubBackward0>) 

                    
2024-07-26 07:12:59 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most dense, it is densest
If something is the most nasty, it is nastiest
If something is the most weird, it is weirdest
If something is the most pure, it is purest
If something is the most huge, it is hugest
If something is the most happy, it is happiest
If something is the most rare, it is rarest
If something is the most strange, it is
2024-07-26 07:13:00 root INFO     [order_1_approx] starting weight calculation for If something is the most huge, it is hugest
If something is the most happy, it is happiest
If something is the most nasty, it is nastiest
If something is the most weird, it is weirdest
If something is the most strange, it is strangest
If something is the most pure, it is purest
If something is the most dense, it is densest
If something is the most rare, it is
2024-07-26 07:13:00 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 07:15:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3516, -0.7344,  2.1875,  ..., -0.6016,  1.5938,  0.3203],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 23.6250,  21.3750, -13.2500,  ..., -11.8125,  -2.0625,  -4.4688],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0206, -0.0043,  0.0060,  ...,  0.0012,  0.0043, -0.0019],
        [ 0.0060,  0.0184,  0.0062,  ..., -0.0039,  0.0019,  0.0034],
        [ 0.0085,  0.0078,  0.0610,  ..., -0.0038, -0.0032,  0.0007],
        ...,
        [ 0.0026, -0.0061, -0.0096,  ...,  0.0161, -0.0025, -0.0018],
        [ 0.0001,  0.0053,  0.0016,  ..., -0.0044,  0.0190, -0.0011],
        [-0.0025,  0.0079,  0.0023,  ..., -0.0075, -0.0059,  0.0208]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 23.1406,  22.0156, -13.0391,  ..., -12.7734,  -2.1602,  -5.4727]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 07:15:46 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most huge, it is hugest
If something is the most happy, it is happiest
If something is the most nasty, it is nastiest
If something is the most weird, it is weirdest
If something is the most strange, it is strangest
If something is the most pure, it is purest
If something is the most dense, it is densest
If something is the most rare, it is
2024-07-26 07:15:46 root INFO     total operator prediction time: 1331.187752008438 seconds
2024-07-26 07:15:46 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on verb+er_irreg
2024-07-26 07:15:46 root INFO     building operator verb+er_irreg
2024-07-26 07:15:46 root INFO     [order_1_approx] starting weight calculation for If you bake something, you are a baker
If you organise something, you are a organiser
If you announce something, you are a announcer
If you suffer something, you are a sufferer
If you explore something, you are a explorer
If you defend something, you are a defender
If you tell something, you are a teller
If you teach something, you are a
2024-07-26 07:15:46 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 07:18:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5547, -1.5781,  4.3438,  ..., -0.4375, -1.4062,  2.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([18.0000, -3.2969,  6.3125,  ..., -5.9688,  1.8594,  2.9375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0155,  0.0010,  0.0019,  ...,  0.0024,  0.0078, -0.0040],
        [ 0.0002,  0.0088,  0.0037,  ...,  0.0012,  0.0002,  0.0026],
        [-0.0015, -0.0019,  0.0591,  ...,  0.0014, -0.0043,  0.0120],
        ...,
        [ 0.0009,  0.0049, -0.0066,  ...,  0.0125, -0.0024,  0.0004],
        [ 0.0004,  0.0017, -0.0067,  ...,  0.0006,  0.0178,  0.0038],
        [ 0.0005, -0.0002, -0.0108,  ...,  0.0013, -0.0023,  0.0299]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[17.3438, -2.8281,  8.6875,  ..., -5.5039,  3.6582,  5.3281]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 07:18:27 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you bake something, you are a baker
If you organise something, you are a organiser
If you announce something, you are a announcer
If you suffer something, you are a sufferer
If you explore something, you are a explorer
If you defend something, you are a defender
If you tell something, you are a teller
If you teach something, you are a
2024-07-26 07:18:27 root INFO     [order_1_approx] starting weight calculation for If you explore something, you are a explorer
If you bake something, you are a baker
If you announce something, you are a announcer
If you teach something, you are a teacher
If you tell something, you are a teller
If you suffer something, you are a sufferer
If you organise something, you are a organiser
If you defend something, you are a
2024-07-26 07:18:27 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 07:21:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1875, -2.2969,  1.8438,  ..., -0.9102,  0.0781,  3.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -2.6875,   2.6406,  15.3750,  ..., -12.5000,   5.3750,   2.0938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.1362e-02, -5.1880e-04,  1.1292e-03,  ...,  4.3640e-03,
          4.8828e-03, -5.9814e-03],
        [ 2.3804e-03,  1.2512e-02,  2.3651e-03,  ..., -1.2207e-04,
          2.1820e-03,  7.5378e-03],
        [ 5.0354e-03, -1.2589e-03,  8.3984e-02,  ...,  5.9128e-05,
          2.3499e-03, -1.8387e-03],
        ...,
        [ 3.1090e-04,  5.3406e-05, -1.6251e-03,  ...,  1.2573e-02,
         -2.2736e-03,  3.0975e-03],
        [-1.6632e-03,  5.0354e-03,  6.8970e-03,  ..., -9.9945e-04,
          2.7588e-02,  1.0300e-03],
        [-1.6098e-03,  2.6550e-03, -6.3477e-03,  ..., -5.7373e-03,
         -2.3956e-03,  2.9785e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -3.3125,   2.3125,  16.2344,  ..., -12.4219,   5.4961,   0.7646]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 07:21:12 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you explore something, you are a explorer
If you bake something, you are a baker
If you announce something, you are a announcer
If you teach something, you are a teacher
If you tell something, you are a teller
If you suffer something, you are a sufferer
If you organise something, you are a organiser
If you defend something, you are a
2024-07-26 07:21:12 root INFO     [order_1_approx] starting weight calculation for If you tell something, you are a teller
If you defend something, you are a defender
If you organise something, you are a organiser
If you bake something, you are a baker
If you explore something, you are a explorer
If you teach something, you are a teacher
If you suffer something, you are a sufferer
If you announce something, you are a
2024-07-26 07:21:13 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 07:23:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4141,  0.4980, -0.4375,  ...,  1.6797, -1.4922,  4.4375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-2.5781,  9.8750,  3.7656,  ..., -3.4375, -7.1250, 10.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.9785e-02,  1.6479e-03,  2.3804e-03,  ...,  3.8147e-06,
          6.1340e-03, -4.4250e-03],
        [-1.3580e-03,  1.3611e-02,  5.3406e-03,  ..., -2.3499e-03,
          1.6403e-04,  5.3711e-03],
        [ 6.1646e-03, -2.7924e-03,  9.7656e-02,  ...,  1.0986e-03,
          6.0120e-03, -8.5449e-03],
        ...,
        [ 1.1673e-03,  2.1973e-03, -3.8147e-03,  ...,  2.0020e-02,
         -1.5945e-03,  2.1362e-03],
        [ 4.1504e-03,  4.5166e-03,  2.0142e-03,  ..., -4.4861e-03,
          3.2227e-02, -5.4932e-03],
        [-2.2430e-03, -9.2697e-04, -6.3171e-03,  ..., -1.1673e-03,
         -8.4839e-03,  3.2227e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.4141,  9.0391,  5.0781,  ..., -4.1602, -8.0078, 10.5000]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 07:24:00 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you tell something, you are a teller
If you defend something, you are a defender
If you organise something, you are a organiser
If you bake something, you are a baker
If you explore something, you are a explorer
If you teach something, you are a teacher
If you suffer something, you are a sufferer
If you announce something, you are a
2024-07-26 07:24:00 root INFO     [order_1_approx] starting weight calculation for If you explore something, you are a explorer
If you organise something, you are a organiser
If you defend something, you are a defender
If you bake something, you are a baker
If you suffer something, you are a sufferer
If you announce something, you are a announcer
If you teach something, you are a teacher
If you tell something, you are a
2024-07-26 07:24:00 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 07:26:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.3984,  1.1953,  3.6406,  ...,  0.5859,  1.2188,  0.5586],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([15.0625,  0.1562, 12.5625,  ...,  1.3281,  1.4609,  9.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0156, -0.0003, -0.0034,  ...,  0.0033,  0.0064, -0.0006],
        [-0.0011,  0.0107,  0.0002,  ..., -0.0024, -0.0036,  0.0043],
        [-0.0023, -0.0007,  0.0723,  ...,  0.0048,  0.0043,  0.0005],
        ...,
        [ 0.0003,  0.0010, -0.0079,  ...,  0.0157, -0.0029,  0.0020],
        [ 0.0001,  0.0034, -0.0008,  ..., -0.0027,  0.0243,  0.0021],
        [ 0.0024,  0.0046, -0.0081,  ..., -0.0042, -0.0024,  0.0242]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[15.9375, -0.7222, 12.3359,  ...,  0.4990,  0.5249,  9.8594]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 07:26:47 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you explore something, you are a explorer
If you organise something, you are a organiser
If you defend something, you are a defender
If you bake something, you are a baker
If you suffer something, you are a sufferer
If you announce something, you are a announcer
If you teach something, you are a teacher
If you tell something, you are a
2024-07-26 07:26:47 root INFO     [order_1_approx] starting weight calculation for If you announce something, you are a announcer
If you teach something, you are a teacher
If you bake something, you are a baker
If you tell something, you are a teller
If you defend something, you are a defender
If you organise something, you are a organiser
If you explore something, you are a explorer
If you suffer something, you are a
2024-07-26 07:26:48 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 07:29:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.5000,  0.9414,  3.8750,  ..., -0.8516, -1.7969,  0.9844],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  9.6250,   6.5938,  19.7500,  ...,  -9.1250, -12.6875,  -6.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0135,  0.0009,  0.0007,  ...,  0.0011,  0.0010, -0.0021],
        [ 0.0008,  0.0145,  0.0051,  ..., -0.0009,  0.0008,  0.0049],
        [ 0.0017,  0.0007,  0.0713,  ..., -0.0044,  0.0037, -0.0002],
        ...,
        [-0.0011,  0.0037, -0.0067,  ...,  0.0138, -0.0005, -0.0038],
        [ 0.0084,  0.0015, -0.0022,  ..., -0.0060,  0.0216, -0.0052],
        [-0.0002,  0.0036, -0.0061,  ..., -0.0053, -0.0033,  0.0248]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  8.9062,   7.1055,  21.1562,  ...,  -9.4531, -13.3672,  -5.9609]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 07:29:35 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you announce something, you are a announcer
If you teach something, you are a teacher
If you bake something, you are a baker
If you tell something, you are a teller
If you defend something, you are a defender
If you organise something, you are a organiser
If you explore something, you are a explorer
If you suffer something, you are a
2024-07-26 07:29:35 root INFO     [order_1_approx] starting weight calculation for If you bake something, you are a baker
If you teach something, you are a teacher
If you announce something, you are a announcer
If you tell something, you are a teller
If you suffer something, you are a sufferer
If you explore something, you are a explorer
If you defend something, you are a defender
If you organise something, you are a
2024-07-26 07:29:35 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 07:32:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6250, -1.4766, -0.9531,  ...,  0.4824, -3.0000,  6.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 5.5938,  2.8438,  3.9375,  ..., -4.2500,  3.1094, 10.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0244, -0.0020,  0.0099,  ..., -0.0026,  0.0084, -0.0007],
        [ 0.0029,  0.0137,  0.0009,  ...,  0.0011,  0.0007,  0.0017],
        [ 0.0012, -0.0033,  0.0918,  ...,  0.0024, -0.0031, -0.0027],
        ...,
        [ 0.0038,  0.0027, -0.0099,  ...,  0.0181, -0.0008,  0.0003],
        [-0.0010,  0.0033, -0.0025,  ..., -0.0023,  0.0291, -0.0033],
        [-0.0022, -0.0021, -0.0054,  ..., -0.0013, -0.0040,  0.0347]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.0703,  2.1152,  6.3672,  ..., -5.2344,  3.7852, 11.0859]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 07:32:21 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you bake something, you are a baker
If you teach something, you are a teacher
If you announce something, you are a announcer
If you tell something, you are a teller
If you suffer something, you are a sufferer
If you explore something, you are a explorer
If you defend something, you are a defender
If you organise something, you are a
2024-07-26 07:32:21 root INFO     [order_1_approx] starting weight calculation for If you bake something, you are a baker
If you announce something, you are a announcer
If you suffer something, you are a sufferer
If you tell something, you are a teller
If you organise something, you are a organiser
If you defend something, you are a defender
If you teach something, you are a teacher
If you explore something, you are a
2024-07-26 07:32:21 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 07:35:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.9609, -2.4375,  5.0938,  ..., -1.1016, -0.9766,  4.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 9.5000, -0.3906, 14.5000,  ..., -9.5000,  8.2500,  9.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0228,  0.0007,  0.0025,  ..., -0.0007,  0.0098,  0.0005],
        [ 0.0071,  0.0122,  0.0081,  ..., -0.0001,  0.0033,  0.0056],
        [-0.0010, -0.0051,  0.0957,  ...,  0.0001,  0.0022,  0.0026],
        ...,
        [ 0.0020, -0.0003, -0.0024,  ...,  0.0151, -0.0009,  0.0054],
        [ 0.0001,  0.0028,  0.0002,  ..., -0.0020,  0.0243, -0.0081],
        [-0.0046,  0.0007,  0.0029,  ..., -0.0065, -0.0013,  0.0339]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  9.1641,  -1.0723,  15.7656,  ..., -10.2578,   8.0156,   7.0078]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 07:35:08 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you bake something, you are a baker
If you announce something, you are a announcer
If you suffer something, you are a sufferer
If you tell something, you are a teller
If you organise something, you are a organiser
If you defend something, you are a defender
If you teach something, you are a teacher
If you explore something, you are a
2024-07-26 07:35:08 root INFO     [order_1_approx] starting weight calculation for If you announce something, you are a announcer
If you teach something, you are a teacher
If you suffer something, you are a sufferer
If you tell something, you are a teller
If you explore something, you are a explorer
If you defend something, you are a defender
If you organise something, you are a organiser
If you bake something, you are a
2024-07-26 07:35:08 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 07:37:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.5000,  1.8750,  1.7500,  ...,  0.9297,  0.4805,  1.3516],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 4.4375,  2.0000,  9.0000,  ...,  2.5781,  7.2188, 10.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.3804e-02, -2.3193e-03,  3.5858e-03,  ...,  1.7319e-03,
          1.2695e-02, -8.4305e-04],
        [ 3.2997e-04,  1.8799e-02,  1.1475e-02,  ..., -1.8005e-03,
          2.6855e-03, -6.4087e-04],
        [ 3.1433e-03,  2.6855e-03,  7.0312e-02,  ...,  9.1553e-05,
         -8.5068e-04, -1.1520e-03],
        ...,
        [ 1.1139e-03,  1.1063e-03, -1.6357e-02,  ...,  1.4160e-02,
         -6.1646e-03,  1.0223e-03],
        [-1.5411e-03,  2.1667e-03, -7.2937e-03,  ..., -2.9907e-03,
          2.0508e-02, -2.8229e-03],
        [ 2.6550e-03,  4.2725e-03,  5.0354e-04,  ..., -3.1281e-03,
         -2.4414e-03,  2.4902e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.5469,  1.5830,  9.2188,  ...,  2.6445,  8.1094, 10.4844]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 07:37:54 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you announce something, you are a announcer
If you teach something, you are a teacher
If you suffer something, you are a sufferer
If you tell something, you are a teller
If you explore something, you are a explorer
If you defend something, you are a defender
If you organise something, you are a organiser
If you bake something, you are a
2024-07-26 07:37:54 root INFO     total operator prediction time: 1328.3484330177307 seconds
2024-07-26 07:37:54 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on over+adj_reg
2024-07-26 07:37:54 root INFO     building operator over+adj_reg
2024-07-26 07:37:55 root INFO     [order_1_approx] starting weight calculation for If something is too subscribed, it is oversubscribed
If something is too stimulated, it is overstimulated
If something is too excited, it is overexcited
If something is too cooked, it is overcooked
If something is too paid, it is overpaid
If something is too arching, it is overarching
If something is too ambitious, it is overambitious
If something is too dressed, it is
2024-07-26 07:37:55 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 07:40:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1719,  1.5156, -1.6875,  ...,  0.2617, -0.3203,  1.2109],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-1.6172, -5.9688, -6.5625,  ..., -7.3438, 16.6250,  0.8555],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.4902e-02, -4.2419e-03,  3.0060e-03,  ..., -8.0490e-04,
         -6.5613e-04,  1.8768e-03],
        [ 3.3569e-03,  1.5747e-02, -3.5095e-04,  ..., -2.5330e-03,
         -4.6539e-04,  5.9814e-03],
        [ 4.4250e-03, -5.0964e-03,  1.0010e-01,  ..., -8.1177e-03,
          1.5259e-05, -5.7373e-03],
        ...,
        [ 7.1716e-03,  6.1035e-03, -5.3101e-03,  ...,  9.8877e-03,
         -2.8687e-03, -4.9744e-03],
        [ 1.8311e-03, -8.8501e-03,  8.3618e-03,  ..., -1.3580e-03,
          3.1006e-02, -3.6163e-03],
        [ 6.2180e-04,  2.4719e-03, -7.1106e-03,  ..., -3.4142e-04,
         -2.9755e-04,  3.1494e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.6133, -6.6211, -6.7773,  ..., -5.4727, 15.4453,  0.5977]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 07:40:39 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is too subscribed, it is oversubscribed
If something is too stimulated, it is overstimulated
If something is too excited, it is overexcited
If something is too cooked, it is overcooked
If something is too paid, it is overpaid
If something is too arching, it is overarching
If something is too ambitious, it is overambitious
If something is too dressed, it is
2024-07-26 07:40:39 root INFO     [order_1_approx] starting weight calculation for If something is too dressed, it is overdressed
If something is too stimulated, it is overstimulated
If something is too excited, it is overexcited
If something is too paid, it is overpaid
If something is too subscribed, it is oversubscribed
If something is too arching, it is overarching
If something is too cooked, it is overcooked
If something is too ambitious, it is
2024-07-26 07:40:39 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 07:43:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5977, -1.5000,  3.4375,  ..., -0.1328, -0.6016,  3.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  6.8125,  -6.8750, -10.5625,  ...,  -7.7500,  -8.5625,  -7.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0442, -0.0054, -0.0049,  ..., -0.0035,  0.0030, -0.0053],
        [-0.0039,  0.0194,  0.0162,  ..., -0.0055,  0.0070,  0.0056],
        [-0.0015, -0.0048,  0.1328,  ..., -0.0064,  0.0083, -0.0125],
        ...,
        [ 0.0049,  0.0042,  0.0107,  ...,  0.0347,  0.0017, -0.0116],
        [ 0.0068, -0.0049, -0.0200,  ..., -0.0052,  0.0613, -0.0013],
        [ 0.0117,  0.0092, -0.0148,  ...,  0.0010, -0.0068,  0.0542]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  5.3672,  -8.0859, -10.2422,  ...,  -6.8984,  -8.2656,  -8.0000]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 07:43:22 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is too dressed, it is overdressed
If something is too stimulated, it is overstimulated
If something is too excited, it is overexcited
If something is too paid, it is overpaid
If something is too subscribed, it is oversubscribed
If something is too arching, it is overarching
If something is too cooked, it is overcooked
If something is too ambitious, it is
2024-07-26 07:43:22 root INFO     [order_1_approx] starting weight calculation for If something is too subscribed, it is oversubscribed
If something is too paid, it is overpaid
If something is too excited, it is overexcited
If something is too dressed, it is overdressed
If something is too arching, it is overarching
If something is too ambitious, it is overambitious
If something is too stimulated, it is overstimulated
If something is too cooked, it is
2024-07-26 07:43:22 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 07:46:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.5000, -0.2988, -2.0469,  ...,  0.7617,  1.5078,  0.7969],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  1.5625,  -9.0000, -15.0000,  ...,  -6.5312,  11.6875,  -2.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.7100e-02,  6.1035e-05, -5.6076e-04,  ..., -2.1820e-03,
         -3.0518e-03,  8.4229e-03],
        [-8.8501e-04,  2.0508e-02, -1.0559e-02,  ...,  1.0376e-03,
         -7.5912e-04,  1.1414e-02],
        [ 8.1787e-03, -2.8076e-03,  9.0820e-02,  ...,  1.1539e-04,
         -5.7220e-05, -8.1177e-03],
        ...,
        [ 6.6833e-03, -4.2725e-04,  2.1240e-02,  ...,  1.3794e-02,
         -3.6316e-03, -1.2451e-02],
        [-2.6550e-03,  2.3499e-03, -8.6670e-03,  ...,  2.3651e-03,
          3.8086e-02,  9.3079e-04],
        [ 5.7983e-03,  1.2207e-03, -2.8381e-03,  ..., -2.9144e-03,
         -4.7607e-03,  3.7598e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  2.4355,  -7.2227, -14.3125,  ...,  -5.3672,  11.7500,  -2.7832]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 07:46:08 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is too subscribed, it is oversubscribed
If something is too paid, it is overpaid
If something is too excited, it is overexcited
If something is too dressed, it is overdressed
If something is too arching, it is overarching
If something is too ambitious, it is overambitious
If something is too stimulated, it is overstimulated
If something is too cooked, it is
2024-07-26 07:46:08 root INFO     [order_1_approx] starting weight calculation for If something is too ambitious, it is overambitious
If something is too paid, it is overpaid
If something is too excited, it is overexcited
If something is too dressed, it is overdressed
If something is too subscribed, it is oversubscribed
If something is too arching, it is overarching
If something is too cooked, it is overcooked
If something is too stimulated, it is
2024-07-26 07:46:08 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 07:48:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.6875, -1.0156,  3.3750,  ...,  0.9531, -0.2275,  2.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 4.6875, -1.0234, -4.9688,  ..., -6.5000,  9.7500,  3.6094],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0371, -0.0056,  0.0072,  ..., -0.0023,  0.0122,  0.0002],
        [ 0.0066,  0.0242,  0.0205,  ..., -0.0037,  0.0083,  0.0143],
        [-0.0053,  0.0009,  0.1357,  ..., -0.0094, -0.0026, -0.0053],
        ...,
        [-0.0074,  0.0055, -0.0175,  ...,  0.0182,  0.0154, -0.0052],
        [ 0.0037,  0.0103, -0.0337,  ..., -0.0039,  0.0522, -0.0006],
        [ 0.0067, -0.0042, -0.0060,  ..., -0.0086, -0.0234,  0.0635]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.5547, -2.0547, -8.7031,  ..., -6.6562,  9.6797,  1.8467]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 07:48:53 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is too ambitious, it is overambitious
If something is too paid, it is overpaid
If something is too excited, it is overexcited
If something is too dressed, it is overdressed
If something is too subscribed, it is oversubscribed
If something is too arching, it is overarching
If something is too cooked, it is overcooked
If something is too stimulated, it is
2024-07-26 07:48:53 root INFO     [order_1_approx] starting weight calculation for If something is too dressed, it is overdressed
If something is too ambitious, it is overambitious
If something is too excited, it is overexcited
If something is too cooked, it is overcooked
If something is too paid, it is overpaid
If something is too subscribed, it is oversubscribed
If something is too stimulated, it is overstimulated
If something is too arching, it is
2024-07-26 07:48:53 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 07:51:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.0781,  1.0703, -2.9531,  ...,  1.1797,  0.0469,  0.8047],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-1.4688, -5.3750,  3.2188,  ...,  4.3438,  6.0938, -0.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0422, -0.0092,  0.0018,  ..., -0.0024, -0.0034,  0.0029],
        [ 0.0068,  0.0137,  0.0067,  ...,  0.0020,  0.0019,  0.0077],
        [-0.0091, -0.0070,  0.1260,  ...,  0.0025,  0.0112, -0.0018],
        ...,
        [ 0.0082, -0.0088, -0.0018,  ...,  0.0215,  0.0073, -0.0043],
        [ 0.0125, -0.0083, -0.0020,  ..., -0.0007,  0.0474,  0.0017],
        [ 0.0101, -0.0020, -0.0160,  ...,  0.0010,  0.0069,  0.0454]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.1680, -2.8848,  1.4600,  ...,  6.4141,  8.7969, -1.7568]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 07:51:37 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is too dressed, it is overdressed
If something is too ambitious, it is overambitious
If something is too excited, it is overexcited
If something is too cooked, it is overcooked
If something is too paid, it is overpaid
If something is too subscribed, it is oversubscribed
If something is too stimulated, it is overstimulated
If something is too arching, it is
2024-07-26 07:51:38 root INFO     [order_1_approx] starting weight calculation for If something is too ambitious, it is overambitious
If something is too paid, it is overpaid
If something is too dressed, it is overdressed
If something is too cooked, it is overcooked
If something is too arching, it is overarching
If something is too excited, it is overexcited
If something is too stimulated, it is overstimulated
If something is too subscribed, it is
2024-07-26 07:51:38 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 07:54:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.5078, -1.7969, -0.4453,  ...,  1.5078, -0.0938,  2.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 6.2500,  0.0801, -6.3750,  ...,  0.6094,  5.9375, -9.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0344, -0.0062,  0.0082,  ..., -0.0035, -0.0031, -0.0043],
        [ 0.0079,  0.0114,  0.0151,  ..., -0.0069,  0.0093,  0.0193],
        [ 0.0038, -0.0019,  0.1064,  ..., -0.0084,  0.0128, -0.0071],
        ...,
        [ 0.0006,  0.0020, -0.0190,  ...,  0.0206, -0.0041, -0.0073],
        [ 0.0055,  0.0013,  0.0030,  ...,  0.0021,  0.0415, -0.0016],
        [ 0.0059,  0.0078, -0.0171,  ..., -0.0020, -0.0015,  0.0527]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 7.5664, -0.4016, -8.6875,  ...,  0.9668,  7.6094, -9.1250]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 07:54:24 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is too ambitious, it is overambitious
If something is too paid, it is overpaid
If something is too dressed, it is overdressed
If something is too cooked, it is overcooked
If something is too arching, it is overarching
If something is too excited, it is overexcited
If something is too stimulated, it is overstimulated
If something is too subscribed, it is
2024-07-26 07:54:24 root INFO     [order_1_approx] starting weight calculation for If something is too excited, it is overexcited
If something is too dressed, it is overdressed
If something is too subscribed, it is oversubscribed
If something is too stimulated, it is overstimulated
If something is too cooked, it is overcooked
If something is too arching, it is overarching
If something is too ambitious, it is overambitious
If something is too paid, it is
2024-07-26 07:54:24 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 07:57:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.2969, 2.6875, 0.7812,  ..., 1.2812, 0.9102, 2.6250], device='cuda:0',
       dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-0.4688, -7.6250, -3.3125,  ..., -2.5469,  8.9375,  0.1484],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0405, -0.0026,  0.0053,  ..., -0.0005, -0.0045, -0.0086],
        [ 0.0073,  0.0181,  0.0043,  ..., -0.0056,  0.0035,  0.0008],
        [ 0.0017,  0.0008,  0.1455,  ...,  0.0004,  0.0244, -0.0002],
        ...,
        [ 0.0103,  0.0023, -0.0029,  ...,  0.0269, -0.0030, -0.0032],
        [-0.0020, -0.0018,  0.0053,  ...,  0.0016,  0.0562, -0.0061],
        [ 0.0020,  0.0028, -0.0181,  ...,  0.0012,  0.0041,  0.0610]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4482, -9.1484, -2.3223,  ..., -1.6270,  7.9844,  0.0833]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 07:57:11 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is too excited, it is overexcited
If something is too dressed, it is overdressed
If something is too subscribed, it is oversubscribed
If something is too stimulated, it is overstimulated
If something is too cooked, it is overcooked
If something is too arching, it is overarching
If something is too ambitious, it is overambitious
If something is too paid, it is
2024-07-26 07:57:11 root INFO     [order_1_approx] starting weight calculation for If something is too cooked, it is overcooked
If something is too subscribed, it is oversubscribed
If something is too dressed, it is overdressed
If something is too stimulated, it is overstimulated
If something is too ambitious, it is overambitious
If something is too arching, it is overarching
If something is too paid, it is overpaid
If something is too excited, it is
2024-07-26 07:57:11 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 07:59:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4688, -2.2188,  3.8281,  ...,  0.2988, -1.6016,  2.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-1.6641, -3.6562, -6.3125,  ..., -2.8750, -2.8594, -0.5938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0339, -0.0025,  0.0056,  ..., -0.0116,  0.0080,  0.0033],
        [ 0.0121,  0.0139,  0.0189,  ..., -0.0034,  0.0025,  0.0046],
        [-0.0006,  0.0008,  0.1094,  ..., -0.0054, -0.0026, -0.0090],
        ...,
        [ 0.0019,  0.0037, -0.0177,  ...,  0.0133, -0.0027, -0.0031],
        [ 0.0065,  0.0055, -0.0060,  ..., -0.0052,  0.0417, -0.0037],
        [ 0.0038, -0.0015, -0.0093,  ..., -0.0059, -0.0049,  0.0391]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0010, -5.2266, -5.7812,  ..., -2.4062, -0.7812, -0.0488]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 07:59:58 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is too cooked, it is overcooked
If something is too subscribed, it is oversubscribed
If something is too dressed, it is overdressed
If something is too stimulated, it is overstimulated
If something is too ambitious, it is overambitious
If something is too arching, it is overarching
If something is too paid, it is overpaid
If something is too excited, it is
2024-07-26 07:59:58 root INFO     total operator prediction time: 1323.267672777176 seconds
2024-07-26 07:59:58 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on adj+ly_reg
2024-07-26 07:59:58 root INFO     building operator adj+ly_reg
2024-07-26 07:59:58 root INFO     [order_1_approx] starting weight calculation for The adjective form of legal is legally
The adjective form of immediate is immediately
The adjective form of similar is similarly
The adjective form of actual is actually
The adjective form of international is internationally
The adjective form of subsequent is subsequently
The adjective form of creative is creatively
The adjective form of internal is
2024-07-26 07:59:58 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 08:02:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-3.1719, -2.8125, -0.1045,  ...,  2.3281,  0.7773,  0.7070],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 1.4688,  4.6875,  7.8438,  ...,  0.2754,  5.4062, 14.3125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0239,  0.0067,  0.0009,  ...,  0.0032,  0.0021, -0.0088],
        [ 0.0129,  0.0182,  0.0006,  ..., -0.0073,  0.0115,  0.0130],
        [ 0.0042, -0.0058,  0.0737,  ..., -0.0066, -0.0053,  0.0111],
        ...,
        [ 0.0008,  0.0034, -0.0088,  ...,  0.0234, -0.0099, -0.0007],
        [-0.0015,  0.0037, -0.0008,  ..., -0.0034,  0.0295,  0.0016],
        [-0.0073,  0.0013,  0.0124,  ..., -0.0134,  0.0075,  0.0349]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7354,  6.5195,  6.2734,  ..., -0.5557,  3.6699, 12.3359]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 08:02:47 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of legal is legally
The adjective form of immediate is immediately
The adjective form of similar is similarly
The adjective form of actual is actually
The adjective form of international is internationally
The adjective form of subsequent is subsequently
The adjective form of creative is creatively
The adjective form of internal is
2024-07-26 08:02:47 root INFO     [order_1_approx] starting weight calculation for The adjective form of internal is internally
The adjective form of creative is creatively
The adjective form of similar is similarly
The adjective form of immediate is immediately
The adjective form of legal is legally
The adjective form of actual is actually
The adjective form of international is internationally
The adjective form of subsequent is
2024-07-26 08:02:47 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 08:05:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5312, -1.1953,  5.2500,  ...,  0.9531,  1.0625,  1.8047],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 4.3125,  2.5312, 12.1875,  ...,  6.7500,  3.9531,  3.0938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0332,  0.0029,  0.0048,  ...,  0.0062, -0.0028, -0.0096],
        [ 0.0012,  0.0238,  0.0076,  ..., -0.0079,  0.0054,  0.0032],
        [ 0.0067, -0.0007,  0.0947,  ..., -0.0101, -0.0008, -0.0005],
        ...,
        [-0.0024,  0.0016,  0.0052,  ...,  0.0212, -0.0088, -0.0021],
        [ 0.0007, -0.0022, -0.0164,  ...,  0.0009,  0.0344, -0.0070],
        [-0.0070,  0.0005, -0.0024,  ..., -0.0066,  0.0030,  0.0300]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.4961,  3.3594, 12.3281,  ...,  8.8125,  4.9727,  4.3672]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 08:05:35 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of internal is internally
The adjective form of creative is creatively
The adjective form of similar is similarly
The adjective form of immediate is immediately
The adjective form of legal is legally
The adjective form of actual is actually
The adjective form of international is internationally
The adjective form of subsequent is
2024-07-26 08:05:36 root INFO     [order_1_approx] starting weight calculation for The adjective form of immediate is immediately
The adjective form of legal is legally
The adjective form of creative is creatively
The adjective form of similar is similarly
The adjective form of internal is internally
The adjective form of subsequent is subsequently
The adjective form of actual is actually
The adjective form of international is
2024-07-26 08:05:36 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 08:08:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.5000, -1.7656,  1.2578,  ...,  2.6875,  0.0278,  2.5938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([4.1562, 1.0156, 5.5312,  ..., 2.5000, 6.4688, 8.3750], device='cuda:0',
       dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0258,  0.0010,  0.0020,  ...,  0.0023,  0.0022, -0.0011],
        [ 0.0063,  0.0129,  0.0047,  ..., -0.0145,  0.0178,  0.0081],
        [ 0.0018, -0.0042,  0.0854,  ..., -0.0017, -0.0083,  0.0022],
        ...,
        [-0.0009,  0.0043, -0.0041,  ...,  0.0228, -0.0070, -0.0014],
        [ 0.0026,  0.0042,  0.0086,  ...,  0.0018,  0.0293, -0.0002],
        [-0.0072, -0.0035, -0.0042,  ..., -0.0085,  0.0120,  0.0283]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[4.6133, 4.7539, 4.3672,  ..., 1.1631, 7.6719, 8.5781]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 08:08:25 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of immediate is immediately
The adjective form of legal is legally
The adjective form of creative is creatively
The adjective form of similar is similarly
The adjective form of internal is internally
The adjective form of subsequent is subsequently
The adjective form of actual is actually
The adjective form of international is
2024-07-26 08:08:25 root INFO     [order_1_approx] starting weight calculation for The adjective form of legal is legally
The adjective form of immediate is immediately
The adjective form of creative is creatively
The adjective form of international is internationally
The adjective form of subsequent is subsequently
The adjective form of internal is internally
The adjective form of similar is similarly
The adjective form of actual is
2024-07-26 08:08:25 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 08:11:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.7344,  0.5273, -0.2070,  ...,  2.0938,  0.4336,  0.4766],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 9.2500,  5.1875,  4.1875,  ..., -2.9688, -1.6562,  3.2188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.7222e-02, -1.2207e-04,  4.6539e-04,  ...,  7.0190e-03,
         -2.5024e-03, -5.0049e-03],
        [ 6.4392e-03,  2.0996e-02,  4.4556e-03,  ..., -1.1353e-02,
          7.7209e-03,  1.0498e-02],
        [ 1.0498e-02, -6.4697e-03,  8.8867e-02,  ..., -2.2278e-03,
         -5.8899e-03,  7.5684e-03],
        ...,
        [ 1.5259e-05,  5.2490e-03,  5.6458e-03,  ...,  2.6611e-02,
         -1.5991e-02, -4.2114e-03],
        [-2.9297e-03,  3.5095e-03, -9.5215e-03,  ...,  1.1749e-03,
          2.9175e-02,  1.7242e-03],
        [-1.0010e-02, -3.2043e-04, -3.8147e-03,  ..., -1.1108e-02,
          1.0254e-02,  2.9907e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[10.0391,  6.9062,  4.3516,  ..., -3.4902, -3.0879,  3.4961]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 08:11:15 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of legal is legally
The adjective form of immediate is immediately
The adjective form of creative is creatively
The adjective form of international is internationally
The adjective form of subsequent is subsequently
The adjective form of internal is internally
The adjective form of similar is similarly
The adjective form of actual is
2024-07-26 08:11:15 root INFO     [order_1_approx] starting weight calculation for The adjective form of immediate is immediately
The adjective form of subsequent is subsequently
The adjective form of actual is actually
The adjective form of legal is legally
The adjective form of internal is internally
The adjective form of similar is similarly
The adjective form of international is internationally
The adjective form of creative is
2024-07-26 08:11:15 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 08:14:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.4531, -1.3672,  0.8750,  ...,  2.2969, -0.3984,  0.0391],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 1.9062,  6.5938,  5.6250,  ..., -6.9062, -2.8750,  3.8125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0271, -0.0020, -0.0046,  ...,  0.0036,  0.0029, -0.0042],
        [ 0.0036,  0.0249,  0.0139,  ..., -0.0092,  0.0032,  0.0036],
        [ 0.0070,  0.0034,  0.0762,  ..., -0.0008,  0.0016,  0.0025],
        ...,
        [ 0.0076,  0.0037, -0.0063,  ...,  0.0192, -0.0067,  0.0008],
        [ 0.0042,  0.0029, -0.0112,  ...,  0.0035,  0.0299,  0.0027],
        [-0.0053,  0.0037, -0.0040,  ..., -0.0058,  0.0105,  0.0322]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.8906,  6.8945,  5.5586,  ..., -8.2734, -3.7949,  3.9023]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 08:14:03 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of immediate is immediately
The adjective form of subsequent is subsequently
The adjective form of actual is actually
The adjective form of legal is legally
The adjective form of internal is internally
The adjective form of similar is similarly
The adjective form of international is internationally
The adjective form of creative is
2024-07-26 08:14:03 root INFO     [order_1_approx] starting weight calculation for The adjective form of actual is actually
The adjective form of similar is similarly
The adjective form of subsequent is subsequently
The adjective form of creative is creatively
The adjective form of legal is legally
The adjective form of internal is internally
The adjective form of international is internationally
The adjective form of immediate is
2024-07-26 08:14:03 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 08:16:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1719, -0.7109, -0.8281,  ...,  0.4023, -1.0469,  2.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 2.5781,  1.3125,  1.4688,  ..., -8.5000,  7.9375, 17.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0374, -0.0019,  0.0025,  ...,  0.0058,  0.0041, -0.0067],
        [ 0.0067,  0.0129,  0.0056,  ..., -0.0101,  0.0137,  0.0122],
        [ 0.0097,  0.0041,  0.0938,  ..., -0.0015, -0.0079,  0.0011],
        ...,
        [ 0.0027,  0.0002, -0.0117,  ...,  0.0228, -0.0103,  0.0032],
        [ 0.0055,  0.0083, -0.0057,  ..., -0.0071,  0.0337,  0.0010],
        [-0.0037,  0.0022, -0.0079,  ..., -0.0151,  0.0135,  0.0327]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4414,  3.0039,  1.0439,  ..., -9.1875,  5.8750, 19.0938]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 08:16:52 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of actual is actually
The adjective form of similar is similarly
The adjective form of subsequent is subsequently
The adjective form of creative is creatively
The adjective form of legal is legally
The adjective form of internal is internally
The adjective form of international is internationally
The adjective form of immediate is
2024-07-26 08:16:53 root INFO     [order_1_approx] starting weight calculation for The adjective form of legal is legally
The adjective form of subsequent is subsequently
The adjective form of immediate is immediately
The adjective form of international is internationally
The adjective form of internal is internally
The adjective form of creative is creatively
The adjective form of actual is actually
The adjective form of similar is
2024-07-26 08:16:53 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 08:19:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.7500, -3.1094, -0.6328,  ..., -1.6094,  1.7578, -0.2305],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([14.8125,  0.0000, -6.3750,  ..., -4.9062, -1.2891,  9.4375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.6123e-02,  6.9427e-04,  8.5449e-03,  ...,  5.3711e-03,
         -3.7689e-03, -1.9531e-03],
        [ 1.0437e-02,  1.9043e-02,  9.8877e-03,  ..., -1.3794e-02,
          9.1553e-03,  7.3547e-03],
        [ 1.3611e-02,  7.0801e-03,  7.2266e-02,  ...,  2.0294e-03,
         -1.0010e-02,  3.4790e-03],
        ...,
        [-6.3324e-04,  5.6152e-03, -2.1667e-03,  ...,  2.1484e-02,
         -1.2207e-02, -1.4343e-03],
        [-1.7242e-03,  6.1035e-03, -1.3611e-02,  ...,  4.5776e-03,
          1.8921e-02, -3.4027e-03],
        [-1.0376e-02, -7.6294e-05,  2.6855e-03,  ..., -1.0620e-02,
          1.6861e-03,  2.2949e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[17.2188,  2.2168, -7.3555,  ..., -5.8906, -3.0586, 10.3594]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 08:19:42 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of legal is legally
The adjective form of subsequent is subsequently
The adjective form of immediate is immediately
The adjective form of international is internationally
The adjective form of internal is internally
The adjective form of creative is creatively
The adjective form of actual is actually
The adjective form of similar is
2024-07-26 08:19:43 root INFO     [order_1_approx] starting weight calculation for The adjective form of international is internationally
The adjective form of internal is internally
The adjective form of creative is creatively
The adjective form of immediate is immediately
The adjective form of similar is similarly
The adjective form of subsequent is subsequently
The adjective form of actual is actually
The adjective form of legal is
2024-07-26 08:19:43 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 08:22:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.5391,  1.3438,  4.5000,  ...,  1.6562,  1.8203, -0.1445],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 3.9375,  5.7812, -2.0625,  ..., -6.9375, -1.9531, -0.8594],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.3071e-02, -7.8583e-04,  4.5776e-03,  ...,  2.3346e-03,
          1.3123e-03, -6.1646e-03],
        [ 3.5248e-03,  1.4465e-02,  4.9744e-03,  ..., -5.3406e-03,
          5.9814e-03,  1.2939e-02],
        [ 4.4556e-03, -1.8692e-03,  7.9102e-02,  ...,  9.0027e-04,
         -3.6316e-03,  1.9302e-03],
        ...,
        [ 7.2327e-03,  6.4697e-03, -4.5166e-03,  ...,  2.0508e-02,
         -9.2773e-03, -2.1820e-03],
        [-1.7700e-03,  5.8746e-04,  9.9182e-05,  ...,  3.0060e-03,
          2.2827e-02, -4.0283e-03],
        [-8.4839e-03,  2.6093e-03, -7.0801e-03,  ..., -4.1504e-03,
          1.4572e-03,  2.3071e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.0781,  7.3047, -2.3105,  ..., -7.5312, -2.6504, -1.9922]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 08:22:32 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of international is internationally
The adjective form of internal is internally
The adjective form of creative is creatively
The adjective form of immediate is immediately
The adjective form of similar is similarly
The adjective form of subsequent is subsequently
The adjective form of actual is actually
The adjective form of legal is
2024-07-26 08:22:32 root INFO     total operator prediction time: 1354.436386346817 seconds
2024-07-26 08:22:32 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on verb+tion_irreg
2024-07-26 08:22:32 root INFO     building operator verb+tion_irreg
2024-07-26 08:22:32 root INFO     [order_1_approx] starting weight calculation for To compute results in computation
To admire results in admiration
To starve results in starvation
To imagine results in imagination
To modernize results in modernization
To privatize results in privatization
To examine results in examination
To condense results in
2024-07-26 08:22:32 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 08:25:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7344, -2.0469,  0.7734,  ..., -1.2422, -1.9375,  1.3594],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 14.1250,  16.2500,  -4.3125,  ...,   1.2812, -14.1875,  -2.8125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0356,  0.0023,  0.0060,  ...,  0.0057, -0.0020, -0.0013],
        [ 0.0046,  0.0269,  0.0131,  ..., -0.0065,  0.0095,  0.0067],
        [ 0.0081, -0.0006,  0.1157,  ..., -0.0061,  0.0018, -0.0016],
        ...,
        [ 0.0041,  0.0029,  0.0027,  ...,  0.0269, -0.0015,  0.0015],
        [-0.0006,  0.0017, -0.0062,  ..., -0.0065,  0.0557,  0.0074],
        [-0.0026,  0.0040, -0.0173,  ..., -0.0023,  0.0052,  0.0605]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 14.7891,  19.5469,  -4.4023,  ...,   0.5273, -15.4531,  -2.9863]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 08:25:19 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To compute results in computation
To admire results in admiration
To starve results in starvation
To imagine results in imagination
To modernize results in modernization
To privatize results in privatization
To examine results in examination
To condense results in
2024-07-26 08:25:19 root INFO     [order_1_approx] starting weight calculation for To compute results in computation
To examine results in examination
To condense results in condensation
To imagine results in imagination
To privatize results in privatization
To admire results in admiration
To modernize results in modernization
To starve results in
2024-07-26 08:25:19 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 08:28:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.6328,  0.6992,  0.2461,  ...,  0.2344,  0.9219,  2.0156],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 6.6250, 11.3125, -3.6406,  ...,  2.4688, -5.4062, -5.4062],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.8330e-02,  1.0681e-04, -1.5869e-03,  ...,  7.2021e-03,
         -1.2512e-03, -1.5030e-03],
        [ 2.2125e-03,  2.4902e-02,  9.7656e-03,  ...,  5.7983e-03,
          8.4686e-04,  8.4839e-03],
        [ 5.4626e-03,  3.6316e-03,  9.4727e-02,  ...,  8.8882e-04,
         -5.4932e-03, -2.6703e-03],
        ...,
        [ 7.8125e-03,  7.9346e-03, -5.6458e-04,  ...,  3.0273e-02,
         -5.0354e-03,  2.6245e-03],
        [-8.5068e-04, -2.2888e-05, -2.1851e-02,  ..., -1.2970e-03,
          4.2969e-02,  6.4697e-03],
        [-2.3193e-03,  1.3733e-03, -1.2451e-02,  ..., -3.9062e-03,
         -1.9264e-04,  4.1992e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 7.3906, 10.3984, -5.7500,  ...,  0.7197, -3.9160, -5.6992]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 08:28:06 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To compute results in computation
To examine results in examination
To condense results in condensation
To imagine results in imagination
To privatize results in privatization
To admire results in admiration
To modernize results in modernization
To starve results in
2024-07-26 08:28:06 root INFO     [order_1_approx] starting weight calculation for To imagine results in imagination
To admire results in admiration
To privatize results in privatization
To condense results in condensation
To starve results in starvation
To compute results in computation
To examine results in examination
To modernize results in
2024-07-26 08:28:06 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 08:30:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-4.0938,  0.9531,  0.1152,  ...,  2.0938, -0.6484,  3.1406],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([12.8750, 18.1250, -9.6250,  ...,  6.8438, -4.0938,  3.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.8818e-02, -1.0376e-03,  1.3733e-03,  ...,  3.7079e-03,
          1.4191e-03,  1.2817e-03],
        [ 1.2054e-03,  2.4902e-02,  6.5613e-03,  ..., -3.4485e-03,
         -3.8147e-04,  8.3008e-03],
        [ 2.5940e-04, -4.2725e-03,  1.2354e-01,  ..., -1.5259e-05,
         -3.3875e-03, -4.4556e-03],
        ...,
        [ 4.9438e-03,  6.0425e-03, -4.1962e-04,  ...,  2.2705e-02,
         -5.0659e-03,  6.3477e-03],
        [-2.6703e-05,  1.1444e-03,  7.7209e-03,  ..., -4.5776e-03,
          4.6387e-02,  9.2773e-03],
        [ 3.8757e-03,  1.1902e-03, -1.2451e-02,  ..., -3.0365e-03,
          2.2583e-03,  5.2246e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[13.9844, 18.5000, -8.7656,  ...,  5.6719, -4.8477,  2.2617]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 08:30:53 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To imagine results in imagination
To admire results in admiration
To privatize results in privatization
To condense results in condensation
To starve results in starvation
To compute results in computation
To examine results in examination
To modernize results in
2024-07-26 08:30:54 root INFO     [order_1_approx] starting weight calculation for To imagine results in imagination
To condense results in condensation
To compute results in computation
To examine results in examination
To modernize results in modernization
To admire results in admiration
To starve results in starvation
To privatize results in
2024-07-26 08:30:54 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 08:33:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.8438,  2.1875, -2.0469,  ...,  2.7344,  0.6719,  2.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([20.8750, 15.7500, -2.7344,  ..., 12.3750, -4.0625, -0.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.4648e-02, -1.3046e-03, -1.8539e-03,  ..., -3.5095e-03,
          4.1962e-04,  2.8687e-03],
        [ 4.5013e-04,  1.0986e-02,  6.3477e-03,  ..., -2.8381e-03,
          2.0294e-03,  6.0120e-03],
        [-1.9226e-03,  4.3869e-04,  6.2500e-02,  ...,  1.3123e-03,
         -3.5667e-04, -4.5471e-03],
        ...,
        [ 1.7700e-03,  4.9210e-04, -8.3618e-03,  ...,  9.0332e-03,
          5.2643e-04, -1.0300e-03],
        [-1.4038e-03,  1.2779e-04,  3.5858e-03,  ..., -2.5635e-03,
          1.9897e-02,  1.7395e-03],
        [ 9.1553e-05,  2.8687e-03, -6.5918e-03,  ..., -8.2779e-04,
         -1.0376e-03,  1.8066e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[21.4688, 16.0625, -2.0625,  ..., 11.9141, -4.4375, -1.3516]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 08:33:40 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To imagine results in imagination
To condense results in condensation
To compute results in computation
To examine results in examination
To modernize results in modernization
To admire results in admiration
To starve results in starvation
To privatize results in
2024-07-26 08:33:41 root INFO     [order_1_approx] starting weight calculation for To admire results in admiration
To modernize results in modernization
To starve results in starvation
To privatize results in privatization
To imagine results in imagination
To condense results in condensation
To examine results in examination
To compute results in
2024-07-26 08:33:41 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 08:36:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.2656,  0.8281, 10.0000,  ...,  0.0547, -0.6602,  2.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 10.6250,   9.6875,  -6.4375,  ...,   6.0000, -20.6250,  -1.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0244,  0.0030, -0.0081,  ...,  0.0075, -0.0104,  0.0054],
        [ 0.0019,  0.0220,  0.0109,  ..., -0.0085,  0.0071,  0.0030],
        [-0.0010,  0.0005,  0.0864,  ..., -0.0110,  0.0032, -0.0031],
        ...,
        [ 0.0019,  0.0054, -0.0118,  ...,  0.0182,  0.0011,  0.0049],
        [ 0.0019,  0.0017,  0.0039,  ...,  0.0009,  0.0405, -0.0049],
        [-0.0053,  0.0013, -0.0135,  ..., -0.0067,  0.0029,  0.0415]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  9.8203,   9.7188,  -9.3047,  ...,   5.3672, -19.9219,  -2.1914]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 08:36:28 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To admire results in admiration
To modernize results in modernization
To starve results in starvation
To privatize results in privatization
To imagine results in imagination
To condense results in condensation
To examine results in examination
To compute results in
2024-07-26 08:36:28 root INFO     [order_1_approx] starting weight calculation for To examine results in examination
To admire results in admiration
To modernize results in modernization
To compute results in computation
To condense results in condensation
To starve results in starvation
To privatize results in privatization
To imagine results in
2024-07-26 08:36:28 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
2024-07-26 08:39:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9414,  2.0312,  1.9453,  ..., -0.3320,  0.3047,  0.4160],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 13.2500,  18.7500, -18.6250,  ...,  10.0000, -15.8125, -11.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.5889e-02,  5.9204e-03,  9.4604e-04,  ...,  7.9956e-03,
         -1.3657e-03,  8.9264e-04],
        [ 4.8523e-03,  1.9531e-02,  5.4932e-03,  ..., -7.9346e-03,
         -2.1057e-03,  1.1047e-02],
        [ 2.8992e-03, -1.9226e-03,  1.0352e-01,  ..., -1.0010e-02,
         -1.3733e-04, -8.6670e-03],
        ...,
        [ 1.2512e-02, -2.2888e-05, -1.0742e-02,  ...,  2.0874e-02,
         -6.6528e-03,  2.8076e-03],
        [-1.0376e-03, -6.2943e-04, -7.5684e-03,  ...,  8.9264e-04,
          5.1758e-02, -1.8311e-03],
        [-2.1362e-03,  5.6458e-03, -5.5542e-03,  ..., -3.8757e-03,
          2.4414e-03,  4.1016e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 14.5000,  21.4062, -18.4531,  ...,  11.3281, -17.3750, -10.6953]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-26 08:39:15 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To examine results in examination
To admire results in admiration
To modernize results in modernization
To compute results in computation
To condense results in condensation
To starve results in starvation
To privatize results in privatization
To imagine results in
2024-07-26 08:39:16 root INFO     [order_1_approx] starting weight calculation for To privatize results in privatization
To modernize results in modernization
To compute results in computation
To condense results in condensation
To imagine results in imagination
To starve results in starvation
To admire results in admiration
To examine results in
2024-07-26 08:39:16 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.24
