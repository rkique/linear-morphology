2024-07-23 22:15:59 root INFO     loading model + tokenizer
2024-07-23 22:16:16 root INFO     model + tokenizer loaded
2024-07-23 22:16:16 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on UK_city - county
2024-07-23 22:16:16 root INFO     building operator UK_city - county
2024-07-23 22:16:16 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of salisbury is in the county of wiltshire
In the United Kingdom, the city of oxford is in the county of oxfordshire
In the United Kingdom, the city of norwich is in the county of norfolk
In the United Kingdom, the city of nottingham is in the county of nottinghamshire
In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of ely is in the county of cambridgeshire
In the United Kingdom, the city of exeter is in the county of devon
In the United Kingdom, the city of belfast is in the county of
2024-07-23 22:16:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 22:20:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.8105,  0.4902, -0.3340,  ...,  0.7949, -1.5078,  0.3716],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.1641,  1.8008, -0.7090,  ..., -3.8828, -3.1328,  0.5352],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.8748e-02, -2.1347e-02, -1.5488e-02,  ...,  5.3978e-03,
         -3.2997e-03,  6.1836e-03],
        [-5.0430e-03, -2.6398e-02, -1.2215e-02,  ..., -3.9139e-03,
         -7.6637e-03,  2.0027e-03],
        [-2.7466e-03, -6.1035e-03, -1.8814e-02,  ...,  1.3229e-02,
          2.9182e-04, -4.6921e-03],
        ...,
        [-8.4915e-03, -9.4147e-03,  8.8501e-04,  ..., -4.9248e-03,
         -6.3591e-03,  8.5831e-03],
        [ 2.9945e-03,  9.7504e-03, -1.8280e-02,  ..., -1.4336e-02,
         -2.4872e-02,  8.8654e-03],
        [-2.1240e-02, -2.2736e-02,  3.0937e-03,  ...,  1.6754e-02,
         -8.1062e-05, -1.5778e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.8965,  1.3467,  0.6748,  ..., -4.2422, -3.7637,  1.4131]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:20:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of salisbury is in the county of wiltshire
In the United Kingdom, the city of oxford is in the county of oxfordshire
In the United Kingdom, the city of norwich is in the county of norfolk
In the United Kingdom, the city of nottingham is in the county of nottinghamshire
In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of ely is in the county of cambridgeshire
In the United Kingdom, the city of exeter is in the county of devon
In the United Kingdom, the city of belfast is in the county of
2024-07-23 22:20:04 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of salisbury is in the county of wiltshire
In the United Kingdom, the city of belfast is in the county of antrim
In the United Kingdom, the city of nottingham is in the county of nottinghamshire
In the United Kingdom, the city of exeter is in the county of devon
In the United Kingdom, the city of norwich is in the county of norfolk
In the United Kingdom, the city of oxford is in the county of oxfordshire
In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of ely is in the county of
2024-07-23 22:20:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 22:23:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5293,  1.2520,  1.3389,  ...,  0.6826, -0.6245, -0.5586],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.9785, -4.3516, -3.1973,  ..., -3.0508,  2.2637,  1.1270],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0403, -0.0004, -0.0069,  ...,  0.0033,  0.0006,  0.0137],
        [ 0.0037, -0.0207,  0.0027,  ...,  0.0098, -0.0096, -0.0252],
        [ 0.0106, -0.0003, -0.0493,  ...,  0.0136, -0.0051,  0.0180],
        ...,
        [ 0.0119, -0.0172, -0.0202,  ...,  0.0104,  0.0038, -0.0065],
        [-0.0087,  0.0064,  0.0156,  ..., -0.0126, -0.0207,  0.0269],
        [-0.0086, -0.0044,  0.0036,  ..., -0.0073,  0.0042, -0.0471]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.0840, -3.5469, -2.5664,  ..., -2.7422,  1.9326,  2.7051]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:23:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of salisbury is in the county of wiltshire
In the United Kingdom, the city of belfast is in the county of antrim
In the United Kingdom, the city of nottingham is in the county of nottinghamshire
In the United Kingdom, the city of exeter is in the county of devon
In the United Kingdom, the city of norwich is in the county of norfolk
In the United Kingdom, the city of oxford is in the county of oxfordshire
In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of ely is in the county of
2024-07-23 22:23:45 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of salisbury is in the county of wiltshire
In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of oxford is in the county of oxfordshire
In the United Kingdom, the city of belfast is in the county of antrim
In the United Kingdom, the city of ely is in the county of cambridgeshire
In the United Kingdom, the city of norwich is in the county of norfolk
In the United Kingdom, the city of nottingham is in the county of nottinghamshire
In the United Kingdom, the city of exeter is in the county of
2024-07-23 22:23:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 22:27:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4561,  0.3901, -0.1821,  ...,  0.7925, -0.0803,  1.0762],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7617,  1.6172, -5.7109,  ..., -3.8457,  5.2070, -0.7310],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0646, -0.0098, -0.0135,  ..., -0.0119, -0.0075,  0.0028],
        [-0.0014, -0.0208, -0.0087,  ..., -0.0163, -0.0144,  0.0088],
        [-0.0049, -0.0055, -0.0111,  ..., -0.0065, -0.0004, -0.0057],
        ...,
        [-0.0190, -0.0194, -0.0244,  ..., -0.0379, -0.0240,  0.0114],
        [-0.0274,  0.0056, -0.0152,  ..., -0.0127, -0.0174,  0.0227],
        [-0.0073,  0.0026, -0.0005,  ..., -0.0077, -0.0015, -0.0326]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3945,  2.7969, -6.6836,  ..., -4.6484,  6.2656, -1.5742]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:27:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of salisbury is in the county of wiltshire
In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of oxford is in the county of oxfordshire
In the United Kingdom, the city of belfast is in the county of antrim
In the United Kingdom, the city of ely is in the county of cambridgeshire
In the United Kingdom, the city of norwich is in the county of norfolk
In the United Kingdom, the city of nottingham is in the county of nottinghamshire
In the United Kingdom, the city of exeter is in the county of
2024-07-23 22:27:31 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of exeter is in the county of devon
In the United Kingdom, the city of belfast is in the county of antrim
In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of nottingham is in the county of nottinghamshire
In the United Kingdom, the city of salisbury is in the county of wiltshire
In the United Kingdom, the city of ely is in the county of cambridgeshire
In the United Kingdom, the city of oxford is in the county of oxfordshire
In the United Kingdom, the city of norwich is in the county of
2024-07-23 22:27:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 22:31:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2002, -0.2258,  1.1592,  ..., -1.7275, -0.1887,  0.7090],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7642, -1.1240, -4.8750,  ..., -1.7188,  3.5312,  3.1621],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0102, -0.0167, -0.0082,  ..., -0.0050,  0.0211,  0.0123],
        [-0.0074, -0.0280, -0.0111,  ...,  0.0132, -0.0075, -0.0206],
        [ 0.0082,  0.0037, -0.0266,  ..., -0.0133, -0.0043,  0.0078],
        ...,
        [-0.0077, -0.0258,  0.0179,  ...,  0.0021,  0.0211,  0.0123],
        [ 0.0007,  0.0333, -0.0223,  ..., -0.0069, -0.0027,  0.0036],
        [-0.0144, -0.0289,  0.0076,  ...,  0.0176,  0.0153, -0.0460]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4409, -0.2266, -4.3125,  ..., -3.8125,  4.4297,  2.1758]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:31:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of exeter is in the county of devon
In the United Kingdom, the city of belfast is in the county of antrim
In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of nottingham is in the county of nottinghamshire
In the United Kingdom, the city of salisbury is in the county of wiltshire
In the United Kingdom, the city of ely is in the county of cambridgeshire
In the United Kingdom, the city of oxford is in the county of oxfordshire
In the United Kingdom, the city of norwich is in the county of
2024-07-23 22:31:18 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of salisbury is in the county of wiltshire
In the United Kingdom, the city of ely is in the county of cambridgeshire
In the United Kingdom, the city of norwich is in the county of norfolk
In the United Kingdom, the city of exeter is in the county of devon
In the United Kingdom, the city of belfast is in the county of antrim
In the United Kingdom, the city of oxford is in the county of oxfordshire
In the United Kingdom, the city of nottingham is in the county of
2024-07-23 22:31:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 22:35:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6763, -0.1046,  0.9351,  ...,  0.3774,  1.1602,  0.7051],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6504, -4.0664, -4.1758,  ...,  5.1289,  2.5234, -0.8887],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0216, -0.0176,  0.0092,  ...,  0.0097,  0.0106, -0.0161],
        [-0.0092, -0.0089, -0.0127,  ...,  0.0214, -0.0019, -0.0111],
        [ 0.0181,  0.0271, -0.0185,  ..., -0.0137, -0.0118,  0.0076],
        ...,
        [-0.0553, -0.0340,  0.0034,  ...,  0.0241,  0.0099,  0.0050],
        [ 0.0159,  0.0114,  0.0030,  ..., -0.0159, -0.0143, -0.0067],
        [-0.0153, -0.0273, -0.0074,  ...,  0.0049,  0.0032, -0.0128]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5850, -3.1680, -3.8730,  ...,  3.1289,  2.9277, -1.2627]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:35:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of salisbury is in the county of wiltshire
In the United Kingdom, the city of ely is in the county of cambridgeshire
In the United Kingdom, the city of norwich is in the county of norfolk
In the United Kingdom, the city of exeter is in the county of devon
In the United Kingdom, the city of belfast is in the county of antrim
In the United Kingdom, the city of oxford is in the county of oxfordshire
In the United Kingdom, the city of nottingham is in the county of
2024-07-23 22:35:06 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of salisbury is in the county of wiltshire
In the United Kingdom, the city of nottingham is in the county of nottinghamshire
In the United Kingdom, the city of norwich is in the county of norfolk
In the United Kingdom, the city of exeter is in the county of devon
In the United Kingdom, the city of ely is in the county of cambridgeshire
In the United Kingdom, the city of belfast is in the county of antrim
In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of oxford is in the county of
2024-07-23 22:35:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 22:38:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3809,  0.3643,  0.7842,  ..., -1.0459, -0.9609, -0.0359],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.3535, -3.7734, -4.9609,  ...,  2.4180,  3.2012,  0.0518],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0188, -0.0089, -0.0008,  ..., -0.0023,  0.0027, -0.0094],
        [ 0.0002, -0.0114, -0.0047,  ...,  0.0102,  0.0056, -0.0024],
        [ 0.0032,  0.0096, -0.0240,  ...,  0.0021, -0.0079,  0.0107],
        ...,
        [-0.0127, -0.0110,  0.0113,  ..., -0.0113, -0.0150,  0.0064],
        [ 0.0071,  0.0166, -0.0139,  ...,  0.0080, -0.0140,  0.0093],
        [ 0.0098, -0.0171,  0.0075,  ..., -0.0013,  0.0095, -0.0201]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.5352, -3.3457, -5.4883,  ...,  2.2422,  3.2422, -0.7007]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:38:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of salisbury is in the county of wiltshire
In the United Kingdom, the city of nottingham is in the county of nottinghamshire
In the United Kingdom, the city of norwich is in the county of norfolk
In the United Kingdom, the city of exeter is in the county of devon
In the United Kingdom, the city of ely is in the county of cambridgeshire
In the United Kingdom, the city of belfast is in the county of antrim
In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of oxford is in the county of
2024-07-23 22:38:50 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of oxford is in the county of oxfordshire
In the United Kingdom, the city of norwich is in the county of norfolk
In the United Kingdom, the city of exeter is in the county of devon
In the United Kingdom, the city of nottingham is in the county of nottinghamshire
In the United Kingdom, the city of belfast is in the county of antrim
In the United Kingdom, the city of salisbury is in the county of wiltshire
In the United Kingdom, the city of ely is in the county of cambridgeshire
In the United Kingdom, the city of salford is in the county of
2024-07-23 22:38:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 22:42:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0660, -0.3445,  0.6426,  ..., -0.5659, -1.0303,  0.1282],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4800, -3.3262, -3.7148,  ..., -1.6777,  4.5039, -1.2852],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0307, -0.0016,  0.0195,  ...,  0.0106, -0.0039, -0.0381],
        [ 0.0044, -0.0028, -0.0316,  ...,  0.0398,  0.0030,  0.0199],
        [-0.0177,  0.0023, -0.0124,  ..., -0.0037,  0.0104,  0.0059],
        ...,
        [-0.0559, -0.0153,  0.0414,  ..., -0.0091,  0.0006,  0.0020],
        [ 0.0186, -0.0256, -0.0010,  ..., -0.0114, -0.0169,  0.0097],
        [-0.0629, -0.0220,  0.0146,  ...,  0.0260, -0.0018, -0.0187]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0176, -3.3574, -2.8555,  ..., -1.9111,  4.1172, -2.7148]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:42:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of oxford is in the county of oxfordshire
In the United Kingdom, the city of norwich is in the county of norfolk
In the United Kingdom, the city of exeter is in the county of devon
In the United Kingdom, the city of nottingham is in the county of nottinghamshire
In the United Kingdom, the city of belfast is in the county of antrim
In the United Kingdom, the city of salisbury is in the county of wiltshire
In the United Kingdom, the city of ely is in the county of cambridgeshire
In the United Kingdom, the city of salford is in the county of
2024-07-23 22:42:36 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of exeter is in the county of devon
In the United Kingdom, the city of norwich is in the county of norfolk
In the United Kingdom, the city of nottingham is in the county of nottinghamshire
In the United Kingdom, the city of oxford is in the county of oxfordshire
In the United Kingdom, the city of ely is in the county of cambridgeshire
In the United Kingdom, the city of belfast is in the county of antrim
In the United Kingdom, the city of salisbury is in the county of
2024-07-23 22:42:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 22:46:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6860, -1.9629, -0.7695,  ...,  0.0854, -0.0396,  0.8306],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6768, -1.1270, -3.8750,  ..., -3.5117,  1.3818,  2.1699],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0663, -0.0267, -0.0089,  ..., -0.0029, -0.0073, -0.0049],
        [ 0.0013,  0.0054, -0.0267,  ...,  0.0468,  0.0088, -0.0368],
        [-0.0137,  0.0045, -0.0246,  ..., -0.0256,  0.0037,  0.0111],
        ...,
        [-0.0217, -0.0190, -0.0027,  ...,  0.0217,  0.0203,  0.0107],
        [ 0.0087,  0.0240, -0.0197,  ..., -0.0151, -0.0092,  0.0213],
        [-0.0073,  0.0014,  0.0127,  ..., -0.0061,  0.0071, -0.0366]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8311,  0.4023, -3.9277,  ..., -2.6367,  2.2109,  1.4473]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:46:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of exeter is in the county of devon
In the United Kingdom, the city of norwich is in the county of norfolk
In the United Kingdom, the city of nottingham is in the county of nottinghamshire
In the United Kingdom, the city of oxford is in the county of oxfordshire
In the United Kingdom, the city of ely is in the county of cambridgeshire
In the United Kingdom, the city of belfast is in the county of antrim
In the United Kingdom, the city of salisbury is in the county of
2024-07-23 22:46:33 root INFO     total operator prediction time: 1817.9637324810028 seconds
2024-07-23 22:46:33 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on animal - youth
2024-07-23 22:46:33 root INFO     building operator animal - youth
2024-07-23 22:46:34 root INFO     [order_1_approx] starting weight calculation for The offspring of a horse is referred to as a foal
The offspring of a ferret is referred to as a kit
The offspring of a fox is referred to as a cub
The offspring of a lion is referred to as a cub
The offspring of a cat is referred to as a kitten
The offspring of a muskrat is referred to as a kit
The offspring of a panda is referred to as a cub
The offspring of a beetle is referred to as a
2024-07-23 22:46:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 22:50:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4009, -0.5171, -0.6357,  ..., -0.6201,  0.3638,  0.3635],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7891, -1.7188, -4.0391,  ...,  2.0859, -2.1289,  1.0098],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0036, -0.0032,  0.0074,  ...,  0.0164, -0.0096,  0.0046],
        [ 0.0074, -0.0221,  0.0067,  ...,  0.0226, -0.0100,  0.0086],
        [-0.0247,  0.0029, -0.0115,  ..., -0.0120,  0.0039,  0.0042],
        ...,
        [ 0.0078, -0.0099,  0.0080,  ..., -0.0019, -0.0063, -0.0127],
        [ 0.0008, -0.0140,  0.0075,  ..., -0.0078, -0.0064, -0.0102],
        [-0.0041, -0.0024,  0.0116,  ..., -0.0076,  0.0031, -0.0081]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3379, -3.0098, -4.8047,  ...,  2.1172, -2.2031,  0.5000]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:50:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a horse is referred to as a foal
The offspring of a ferret is referred to as a kit
The offspring of a fox is referred to as a cub
The offspring of a lion is referred to as a cub
The offspring of a cat is referred to as a kitten
The offspring of a muskrat is referred to as a kit
The offspring of a panda is referred to as a cub
The offspring of a beetle is referred to as a
2024-07-23 22:50:22 root INFO     [order_1_approx] starting weight calculation for The offspring of a panda is referred to as a cub
The offspring of a muskrat is referred to as a kit
The offspring of a horse is referred to as a foal
The offspring of a lion is referred to as a cub
The offspring of a beetle is referred to as a larva
The offspring of a ferret is referred to as a kit
The offspring of a fox is referred to as a cub
The offspring of a cat is referred to as a
2024-07-23 22:50:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 22:54:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4219, -0.5957, -0.3677,  ...,  0.6597, -0.1453,  0.8276],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0664,  0.8096, -1.2480,  ...,  0.9473, -2.4355,  0.8643],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-3.0212e-03, -5.6839e-04, -1.0147e-03,  ...,  1.3939e-02,
          3.8738e-03, -6.9618e-03],
        [-1.8692e-03, -1.0025e-02,  3.2425e-04,  ...,  3.8605e-03,
          1.2024e-02,  9.9030e-03],
        [-1.2558e-02,  2.1454e-02, -1.0147e-02,  ...,  5.5923e-03,
          2.8839e-02,  6.3515e-03],
        ...,
        [ 1.4954e-03, -8.9569e-03,  7.6523e-03,  ...,  1.5244e-02,
         -2.1759e-02,  7.0839e-03],
        [-8.1329e-03,  1.7151e-02,  8.2397e-04,  ..., -6.1035e-05,
         -8.4610e-03,  1.4214e-02],
        [ 3.5095e-03, -8.5754e-03, -1.2360e-03,  ...,  6.9580e-03,
          4.1237e-03,  9.7198e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5854,  0.7974, -1.6152,  ...,  0.4316, -2.5918,  1.2988]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:54:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a panda is referred to as a cub
The offspring of a muskrat is referred to as a kit
The offspring of a horse is referred to as a foal
The offspring of a lion is referred to as a cub
The offspring of a beetle is referred to as a larva
The offspring of a ferret is referred to as a kit
The offspring of a fox is referred to as a cub
The offspring of a cat is referred to as a
2024-07-23 22:54:09 root INFO     [order_1_approx] starting weight calculation for The offspring of a horse is referred to as a foal
The offspring of a fox is referred to as a cub
The offspring of a beetle is referred to as a larva
The offspring of a panda is referred to as a cub
The offspring of a lion is referred to as a cub
The offspring of a muskrat is referred to as a kit
The offspring of a cat is referred to as a kitten
The offspring of a ferret is referred to as a
2024-07-23 22:54:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 22:57:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2605,  0.2368, -1.7109,  ..., -0.0642, -0.9424,  1.0234],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6133, -0.1646, -2.6992,  ..., -1.0088, -1.1504,  2.4980],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0115, -0.0326,  0.0163,  ...,  0.0212,  0.0103, -0.0173],
        [ 0.0159, -0.0115,  0.0102,  ...,  0.0237,  0.0048,  0.0036],
        [ 0.0034,  0.0322,  0.0164,  ..., -0.0049,  0.0113,  0.0110],
        ...,
        [-0.0213, -0.0099, -0.0078,  ...,  0.0147,  0.0063,  0.0228],
        [-0.0109,  0.0025, -0.0146,  ..., -0.0124, -0.0095,  0.0141],
        [ 0.0096,  0.0368, -0.0132,  ...,  0.0016,  0.0141,  0.0183]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6274,  0.3516, -2.8262,  ..., -1.2920, -0.7642,  2.0312]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:57:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a horse is referred to as a foal
The offspring of a fox is referred to as a cub
The offspring of a beetle is referred to as a larva
The offspring of a panda is referred to as a cub
The offspring of a lion is referred to as a cub
The offspring of a muskrat is referred to as a kit
The offspring of a cat is referred to as a kitten
The offspring of a ferret is referred to as a
2024-07-23 22:57:57 root INFO     [order_1_approx] starting weight calculation for The offspring of a cat is referred to as a kitten
The offspring of a beetle is referred to as a larva
The offspring of a ferret is referred to as a kit
The offspring of a lion is referred to as a cub
The offspring of a horse is referred to as a foal
The offspring of a muskrat is referred to as a kit
The offspring of a panda is referred to as a cub
The offspring of a fox is referred to as a
2024-07-23 22:57:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 23:01:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2246, -0.8140, -1.3086,  ...,  0.5479, -0.4270,  0.8086],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8784, -3.7207, -4.0195,  ...,  0.7026,  1.5059,  3.2305],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.1509e-03, -1.5137e-02, -3.0632e-03,  ...,  2.4658e-02,
         -1.8997e-03, -1.4282e-02],
        [-5.6648e-03, -1.6968e-02,  1.5190e-02,  ...,  1.4473e-02,
         -1.5343e-02, -2.3132e-02],
        [ 2.8191e-03, -6.7673e-03,  1.4832e-02,  ..., -5.3864e-03,
          2.0065e-03, -3.3855e-05],
        ...,
        [ 1.5450e-03,  1.0162e-02, -1.4397e-02,  ...,  1.0712e-02,
         -7.5722e-03,  1.7052e-03],
        [-7.1487e-03, -9.0714e-03, -2.0706e-02,  ...,  1.7929e-02,
         -8.6975e-03,  2.2079e-02],
        [ 7.6675e-03,  3.9368e-03,  1.9550e-03,  ..., -3.9825e-03,
          1.6510e-02, -7.1716e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0654, -4.4453, -3.6621,  ...,  0.0991,  1.5410,  2.1719]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:01:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a cat is referred to as a kitten
The offspring of a beetle is referred to as a larva
The offspring of a ferret is referred to as a kit
The offspring of a lion is referred to as a cub
The offspring of a horse is referred to as a foal
The offspring of a muskrat is referred to as a kit
The offspring of a panda is referred to as a cub
The offspring of a fox is referred to as a
2024-07-23 23:01:48 root INFO     [order_1_approx] starting weight calculation for The offspring of a ferret is referred to as a kit
The offspring of a beetle is referred to as a larva
The offspring of a panda is referred to as a cub
The offspring of a cat is referred to as a kitten
The offspring of a lion is referred to as a cub
The offspring of a muskrat is referred to as a kit
The offspring of a fox is referred to as a cub
The offspring of a horse is referred to as a
2024-07-23 23:01:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 23:05:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3838, -0.3386, -0.1426,  ...,  0.6064, -0.4937,  0.6001],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5137, -2.3750, -5.5312,  ..., -1.1572,  0.0236,  5.0391],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0049, -0.0021,  0.0189,  ...,  0.0105, -0.0017, -0.0063],
        [-0.0089,  0.0050,  0.0017,  ...,  0.0062,  0.0050, -0.0025],
        [-0.0040,  0.0101, -0.0037,  ...,  0.0112,  0.0193,  0.0009],
        ...,
        [-0.0132,  0.0035, -0.0071,  ..., -0.0051,  0.0013, -0.0053],
        [-0.0027,  0.0091,  0.0042,  ...,  0.0090, -0.0016,  0.0069],
        [-0.0117, -0.0116,  0.0024,  ...,  0.0080,  0.0098, -0.0047]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5186, -2.4883, -5.6758,  ..., -0.7915,  0.2803,  4.7812]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:05:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a ferret is referred to as a kit
The offspring of a beetle is referred to as a larva
The offspring of a panda is referred to as a cub
The offspring of a cat is referred to as a kitten
The offspring of a lion is referred to as a cub
The offspring of a muskrat is referred to as a kit
The offspring of a fox is referred to as a cub
The offspring of a horse is referred to as a
2024-07-23 23:05:34 root INFO     [order_1_approx] starting weight calculation for The offspring of a muskrat is referred to as a kit
The offspring of a fox is referred to as a cub
The offspring of a horse is referred to as a foal
The offspring of a panda is referred to as a cub
The offspring of a cat is referred to as a kitten
The offspring of a ferret is referred to as a kit
The offspring of a beetle is referred to as a larva
The offspring of a lion is referred to as a
2024-07-23 23:05:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 23:09:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6592, -1.2695, -0.3831,  ...,  1.4873,  0.7207,  0.6396],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9043, -3.1836, -2.7754,  ..., -0.0591, -3.3516,  3.9746],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.0849e-02,  2.2049e-03,  5.5313e-05,  ..., -3.5553e-03,
          7.3624e-04, -9.3002e-03],
        [ 1.0437e-02,  8.4839e-03, -8.4496e-04,  ...,  1.1459e-02,
          1.2314e-02, -2.2491e-02],
        [-1.4801e-02,  1.1337e-02,  8.9798e-03,  ..., -3.3989e-03,
          1.3535e-02,  6.0577e-03],
        ...,
        [ 4.3221e-03, -5.4092e-03,  6.7940e-03,  ...,  8.9722e-03,
         -1.2665e-02,  9.0942e-03],
        [ 8.6746e-03,  3.9673e-03, -1.6418e-02,  ..., -6.5079e-03,
         -1.0185e-03,  1.0506e-02],
        [-1.3702e-02, -6.7291e-03, -4.8828e-03,  ...,  4.1847e-03,
          5.1231e-03,  6.2599e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.1289, -3.0645, -3.5957,  ...,  0.6279, -3.4082,  4.0430]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:09:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a muskrat is referred to as a kit
The offspring of a fox is referred to as a cub
The offspring of a horse is referred to as a foal
The offspring of a panda is referred to as a cub
The offspring of a cat is referred to as a kitten
The offspring of a ferret is referred to as a kit
The offspring of a beetle is referred to as a larva
The offspring of a lion is referred to as a
2024-07-23 23:09:23 root INFO     [order_1_approx] starting weight calculation for The offspring of a beetle is referred to as a larva
The offspring of a lion is referred to as a cub
The offspring of a ferret is referred to as a kit
The offspring of a horse is referred to as a foal
The offspring of a fox is referred to as a cub
The offspring of a cat is referred to as a kitten
The offspring of a panda is referred to as a cub
The offspring of a muskrat is referred to as a
2024-07-23 23:09:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 23:13:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5259, -0.9551, -0.1896,  ...,  0.4851, -0.6406,  0.9014],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3330,  0.8657, -3.3535,  ...,  2.0254,  0.8677,  1.2656],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0109, -0.0157,  0.0105,  ...,  0.0239,  0.0077, -0.0118],
        [-0.0232, -0.0007,  0.0186,  ...,  0.0162, -0.0013,  0.0026],
        [-0.0101,  0.0192,  0.0011,  ..., -0.0009,  0.0098,  0.0058],
        ...,
        [ 0.0043, -0.0022, -0.0010,  ...,  0.0053, -0.0211,  0.0014],
        [-0.0087, -0.0048, -0.0006,  ..., -0.0101,  0.0073, -0.0120],
        [ 0.0170,  0.0030, -0.0024,  ..., -0.0004,  0.0129,  0.0056]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7559,  0.2271, -3.2520,  ...,  2.7109,  1.1064,  2.3672]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:13:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a beetle is referred to as a larva
The offspring of a lion is referred to as a cub
The offspring of a ferret is referred to as a kit
The offspring of a horse is referred to as a foal
The offspring of a fox is referred to as a cub
The offspring of a cat is referred to as a kitten
The offspring of a panda is referred to as a cub
The offspring of a muskrat is referred to as a
2024-07-23 23:13:09 root INFO     [order_1_approx] starting weight calculation for The offspring of a cat is referred to as a kitten
The offspring of a ferret is referred to as a kit
The offspring of a fox is referred to as a cub
The offspring of a muskrat is referred to as a kit
The offspring of a horse is referred to as a foal
The offspring of a lion is referred to as a cub
The offspring of a beetle is referred to as a larva
The offspring of a panda is referred to as a
2024-07-23 23:13:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 23:16:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.9580,  0.9541, -0.6680,  ...,  0.3564,  0.7856,  1.2676],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7090,  0.3799, -4.0859,  ...,  0.3452,  1.8203,  2.6895],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.2894e-02, -1.1414e-02, -1.7395e-02,  ...,  2.1698e-02,
          2.5406e-03, -8.4229e-03],
        [ 6.3705e-03,  5.4092e-03,  1.7761e-02,  ...,  5.5695e-03,
          1.7578e-02, -6.3095e-03],
        [ 4.5586e-03, -2.1191e-03, -7.8049e-03,  ..., -3.1235e-02,
          2.4948e-02,  2.4307e-02],
        ...,
        [ 1.7166e-02,  1.0353e-02, -1.1856e-02,  ...,  7.3471e-03,
          2.4796e-05,  1.8738e-02],
        [ 8.9111e-03, -9.8419e-03, -9.7809e-03,  ..., -6.2561e-04,
         -2.0866e-03, -4.6539e-03],
        [ 2.8839e-03,  1.3161e-04,  2.1118e-02,  ...,  8.0185e-03,
          7.1678e-03, -1.5053e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.8467,  0.2306, -4.3438,  ...,  1.2695,  1.3613,  2.5312]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:16:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a cat is referred to as a kitten
The offspring of a ferret is referred to as a kit
The offspring of a fox is referred to as a cub
The offspring of a muskrat is referred to as a kit
The offspring of a horse is referred to as a foal
The offspring of a lion is referred to as a cub
The offspring of a beetle is referred to as a larva
The offspring of a panda is referred to as a
2024-07-23 23:16:57 root INFO     total operator prediction time: 1823.5917687416077 seconds
2024-07-23 23:16:57 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on animal - sound
2024-07-23 23:16:57 root INFO     building operator animal - sound
2024-07-23 23:16:57 root INFO     [order_1_approx] starting weight calculation for The sound that a mule makes is called a bray
The sound that a deer makes is called a bellow
The sound that a lion makes is called a roar
The sound that a seal makes is called a bark
The sound that a monkey makes is called a chatter
The sound that a mouse makes is called a squeak
The sound that a songbird makes is called a chirrup
The sound that a chimpanzee makes is called a
2024-07-23 23:16:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 23:20:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3250, -0.2074,  0.1306,  ...,  0.1150,  0.4268,  1.4277],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6533,  0.7642, -1.7188,  ..., -1.2871, -1.8311, -3.0020],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0017, -0.0090,  0.0080,  ...,  0.0051,  0.0049, -0.0058],
        [-0.0070,  0.0086,  0.0009,  ...,  0.0013,  0.0010, -0.0033],
        [ 0.0022, -0.0063,  0.0035,  ..., -0.0047,  0.0032,  0.0111],
        ...,
        [-0.0046, -0.0024, -0.0031,  ..., -0.0030, -0.0086,  0.0026],
        [-0.0051,  0.0055, -0.0021,  ...,  0.0100, -0.0021,  0.0153],
        [ 0.0022,  0.0060, -0.0006,  ...,  0.0097, -0.0009,  0.0077]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1758,  0.9678, -1.9229,  ..., -1.0928, -1.9795, -2.7363]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:20:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a mule makes is called a bray
The sound that a deer makes is called a bellow
The sound that a lion makes is called a roar
The sound that a seal makes is called a bark
The sound that a monkey makes is called a chatter
The sound that a mouse makes is called a squeak
The sound that a songbird makes is called a chirrup
The sound that a chimpanzee makes is called a
2024-07-23 23:20:41 root INFO     [order_1_approx] starting weight calculation for The sound that a mouse makes is called a squeak
The sound that a lion makes is called a roar
The sound that a monkey makes is called a chatter
The sound that a chimpanzee makes is called a scream
The sound that a songbird makes is called a chirrup
The sound that a mule makes is called a bray
The sound that a seal makes is called a bark
The sound that a deer makes is called a
2024-07-23 23:20:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 23:24:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7617,  0.5962, -0.0967,  ...,  0.4580,  0.1312,  0.7827],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.3594,  3.5000, -4.1953,  ..., -3.3340, -2.1250,  0.3765],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 6.8855e-03,  4.2419e-03,  3.0365e-03,  ..., -3.4752e-03,
         -5.9509e-03, -1.0284e-02],
        [ 1.5121e-02,  3.4485e-03, -6.3896e-05,  ..., -4.0283e-03,
          1.4099e-02, -1.2802e-02],
        [-2.3346e-03, -1.4160e-02,  2.1759e-02,  ..., -1.1955e-02,
         -7.3547e-03,  2.2614e-02],
        ...,
        [ 3.0251e-03, -8.3923e-03, -1.2741e-02,  ...,  1.0254e-02,
         -2.7771e-02,  2.7252e-02],
        [-4.5357e-03, -8.8272e-03, -2.3071e-02,  ..., -4.6310e-03,
          1.0666e-02,  1.0956e-02],
        [-1.1894e-02, -3.3607e-03, -1.8738e-02,  ..., -6.6528e-03,
          8.7509e-03,  2.3994e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1094,  2.1719, -3.6211,  ..., -3.1211, -1.9746, -0.0359]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:24:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a mouse makes is called a squeak
The sound that a lion makes is called a roar
The sound that a monkey makes is called a chatter
The sound that a chimpanzee makes is called a scream
The sound that a songbird makes is called a chirrup
The sound that a mule makes is called a bray
The sound that a seal makes is called a bark
The sound that a deer makes is called a
2024-07-23 23:24:28 root INFO     [order_1_approx] starting weight calculation for The sound that a mouse makes is called a squeak
The sound that a seal makes is called a bark
The sound that a monkey makes is called a chatter
The sound that a chimpanzee makes is called a scream
The sound that a songbird makes is called a chirrup
The sound that a deer makes is called a bellow
The sound that a mule makes is called a bray
The sound that a lion makes is called a
2024-07-23 23:24:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 23:28:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7207, -0.8750,  0.5923,  ...,  1.2188,  0.7974,  1.3828],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4150,  1.7686,  1.7266,  ..., -5.4062, -6.9609, -0.7729],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0006, -0.0034, -0.0103,  ...,  0.0075,  0.0112,  0.0015],
        [ 0.0036,  0.0108, -0.0106,  ..., -0.0110, -0.0022, -0.0065],
        [-0.0086, -0.0108, -0.0062,  ..., -0.0013, -0.0021,  0.0199],
        ...,
        [-0.0016, -0.0233,  0.0325,  ..., -0.0006, -0.0093,  0.0211],
        [-0.0066, -0.0062,  0.0109,  ..., -0.0095,  0.0085, -0.0057],
        [-0.0104,  0.0129,  0.0040,  ...,  0.0044,  0.0057,  0.0160]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-6.3477e-03,  1.9688e+00,  1.4580e+00,  ..., -5.7227e+00,
         -6.6055e+00, -1.7773e+00]], device='cuda:1', dtype=torch.float16,
       grad_fn=<SubBackward0>) 

                    
2024-07-23 23:28:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a mouse makes is called a squeak
The sound that a seal makes is called a bark
The sound that a monkey makes is called a chatter
The sound that a chimpanzee makes is called a scream
The sound that a songbird makes is called a chirrup
The sound that a deer makes is called a bellow
The sound that a mule makes is called a bray
The sound that a lion makes is called a
2024-07-23 23:28:14 root INFO     [order_1_approx] starting weight calculation for The sound that a deer makes is called a bellow
The sound that a chimpanzee makes is called a scream
The sound that a mouse makes is called a squeak
The sound that a seal makes is called a bark
The sound that a mule makes is called a bray
The sound that a songbird makes is called a chirrup
The sound that a lion makes is called a roar
The sound that a monkey makes is called a
2024-07-23 23:28:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 23:32:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4194, -1.4121,  0.4792,  ...,  0.1866, -0.1794,  0.5044],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3809,  1.8115, -1.9541,  ..., -0.0378, -1.6777, -4.1211],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0045,  0.0033, -0.0013,  ..., -0.0022,  0.0289, -0.0290],
        [-0.0154, -0.0053, -0.0033,  ...,  0.0059,  0.0061, -0.0058],
        [ 0.0020, -0.0091, -0.0085,  ...,  0.0035, -0.0069,  0.0046],
        ...,
        [-0.0114,  0.0032, -0.0057,  ...,  0.0022, -0.0057, -0.0163],
        [-0.0034, -0.0064, -0.0105,  ...,  0.0011, -0.0037,  0.0060],
        [ 0.0176,  0.0082, -0.0214,  ...,  0.0069,  0.0032, -0.0048]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1270,  1.6084, -2.7188,  ...,  0.6348, -2.1230, -3.8105]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:32:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a deer makes is called a bellow
The sound that a chimpanzee makes is called a scream
The sound that a mouse makes is called a squeak
The sound that a seal makes is called a bark
The sound that a mule makes is called a bray
The sound that a songbird makes is called a chirrup
The sound that a lion makes is called a roar
The sound that a monkey makes is called a
2024-07-23 23:32:03 root INFO     [order_1_approx] starting weight calculation for The sound that a deer makes is called a bellow
The sound that a seal makes is called a bark
The sound that a monkey makes is called a chatter
The sound that a mule makes is called a bray
The sound that a songbird makes is called a chirrup
The sound that a lion makes is called a roar
The sound that a chimpanzee makes is called a scream
The sound that a mouse makes is called a
2024-07-23 23:32:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 23:35:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5459,  0.0852,  1.0303,  ..., -0.0664, -0.7734,  1.4590],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.9805,  0.4629,  2.5703,  ..., -1.7305, -3.4844, -7.4844],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 6.3400e-03,  2.4967e-03,  3.7231e-03,  ..., -5.5161e-03,
          1.0269e-02,  7.9041e-03],
        [-4.1580e-03,  3.0518e-03, -1.2398e-03,  ...,  3.7251e-03,
         -7.0724e-03, -1.8082e-02],
        [-2.4414e-03, -3.3569e-03,  1.9016e-03,  ...,  9.7046e-03,
          1.6159e-02,  1.0872e-02],
        ...,
        [-9.5367e-05, -3.3150e-03, -4.2763e-03,  ...,  9.5215e-03,
         -2.6184e-02,  1.7395e-02],
        [-6.0196e-03, -9.7504e-03, -3.9043e-03,  ..., -1.0399e-02,
          9.9030e-03,  2.4567e-03],
        [ 1.4664e-02, -5.8365e-03, -7.6294e-04,  ...,  6.6414e-03,
         -7.5722e-03,  4.9896e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.8359,  0.1155,  2.8340,  ..., -0.5088, -2.9805, -8.0078]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:35:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a deer makes is called a bellow
The sound that a seal makes is called a bark
The sound that a monkey makes is called a chatter
The sound that a mule makes is called a bray
The sound that a songbird makes is called a chirrup
The sound that a lion makes is called a roar
The sound that a chimpanzee makes is called a scream
The sound that a mouse makes is called a
2024-07-23 23:35:45 root INFO     [order_1_approx] starting weight calculation for The sound that a monkey makes is called a chatter
The sound that a deer makes is called a bellow
The sound that a mouse makes is called a squeak
The sound that a seal makes is called a bark
The sound that a chimpanzee makes is called a scream
The sound that a songbird makes is called a chirrup
The sound that a lion makes is called a roar
The sound that a mule makes is called a
2024-07-23 23:35:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 23:39:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2334, -0.8428,  0.3228,  ...,  0.7666,  0.9902,  1.4824],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.2480, -1.1562, -5.9883,  ..., -5.6055,  1.8643,  0.0251],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0118, -0.0192, -0.0090,  ...,  0.0135, -0.0097,  0.0021],
        [ 0.0028,  0.0214, -0.0045,  ...,  0.0015,  0.0044, -0.0047],
        [ 0.0110,  0.0118,  0.0015,  ...,  0.0094, -0.0035,  0.0054],
        ...,
        [-0.0051, -0.0026, -0.0035,  ...,  0.0025, -0.0039,  0.0084],
        [ 0.0059, -0.0109, -0.0141,  ..., -0.0032,  0.0150,  0.0061],
        [-0.0017,  0.0066, -0.0112,  ...,  0.0089,  0.0026, -0.0030]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.8037, -1.6006, -6.1484,  ..., -4.7930,  2.0352, -0.0285]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:39:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a monkey makes is called a chatter
The sound that a deer makes is called a bellow
The sound that a mouse makes is called a squeak
The sound that a seal makes is called a bark
The sound that a chimpanzee makes is called a scream
The sound that a songbird makes is called a chirrup
The sound that a lion makes is called a roar
The sound that a mule makes is called a
2024-07-23 23:39:33 root INFO     [order_1_approx] starting weight calculation for The sound that a deer makes is called a bellow
The sound that a mouse makes is called a squeak
The sound that a lion makes is called a roar
The sound that a monkey makes is called a chatter
The sound that a songbird makes is called a chirrup
The sound that a chimpanzee makes is called a scream
The sound that a mule makes is called a bray
The sound that a seal makes is called a
2024-07-23 23:39:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 23:43:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2515,  0.8555,  0.5132,  ...,  0.5142, -0.5752,  1.6729],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2366,  3.3613, -2.3906,  ..., -0.3125, -2.3027, -4.1797],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0004,  0.0074,  0.0073,  ...,  0.0055, -0.0010,  0.0104],
        [-0.0186,  0.0166,  0.0020,  ...,  0.0032,  0.0126,  0.0033],
        [-0.0187,  0.0143,  0.0174,  ...,  0.0056,  0.0039,  0.0052],
        ...,
        [-0.0198, -0.0115, -0.0052,  ..., -0.0107, -0.0044,  0.0063],
        [-0.0017, -0.0299, -0.0009,  ..., -0.0106,  0.0157,  0.0013],
        [-0.0150, -0.0147, -0.0021,  ...,  0.0024,  0.0041, -0.0082]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7725,  2.1523, -2.2012,  ...,  0.7197, -1.5430, -3.9883]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:43:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a deer makes is called a bellow
The sound that a mouse makes is called a squeak
The sound that a lion makes is called a roar
The sound that a monkey makes is called a chatter
The sound that a songbird makes is called a chirrup
The sound that a chimpanzee makes is called a scream
The sound that a mule makes is called a bray
The sound that a seal makes is called a
2024-07-23 23:43:20 root INFO     [order_1_approx] starting weight calculation for The sound that a lion makes is called a roar
The sound that a seal makes is called a bark
The sound that a mule makes is called a bray
The sound that a monkey makes is called a chatter
The sound that a mouse makes is called a squeak
The sound that a deer makes is called a bellow
The sound that a chimpanzee makes is called a scream
The sound that a songbird makes is called a
2024-07-23 23:43:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 23:47:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2788, -0.3970,  0.6602,  ...,  0.5576, -0.1138,  0.0288],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.5938,  3.8398,  2.1660,  ...,  1.0713, -2.9023, -8.9766],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0096, -0.0044, -0.0079,  ...,  0.0007, -0.0042,  0.0030],
        [-0.0035,  0.0048,  0.0078,  ..., -0.0020,  0.0003, -0.0080],
        [-0.0055, -0.0047,  0.0060,  ..., -0.0058,  0.0032, -0.0013],
        ...,
        [-0.0010, -0.0094, -0.0093,  ...,  0.0067, -0.0148,  0.0062],
        [-0.0033, -0.0077, -0.0218,  ...,  0.0008, -0.0018,  0.0015],
        [-0.0050, -0.0141, -0.0144,  ...,  0.0102, -0.0071,  0.0168]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7520,  3.6445,  2.3203,  ...,  1.4619, -3.6797, -9.0000]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:47:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a lion makes is called a roar
The sound that a seal makes is called a bark
The sound that a mule makes is called a bray
The sound that a monkey makes is called a chatter
The sound that a mouse makes is called a squeak
The sound that a deer makes is called a bellow
The sound that a chimpanzee makes is called a scream
The sound that a songbird makes is called a
2024-07-23 23:47:08 root INFO     total operator prediction time: 1811.3990397453308 seconds
2024-07-23 23:47:09 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on things - color
2024-07-23 23:47:09 root INFO     building operator things - color
2024-07-23 23:47:09 root INFO     [order_1_approx] starting weight calculation for The pepper is colored black
The ruby is colored red
The cabbage is colored green
The peony is colored red
The celery is colored green
The sapphire is colored blue
The milk is colored white
The apple is colored
2024-07-23 23:47:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 23:50:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6494, -0.2314,  1.5449,  ..., -0.4595, -0.9238,  0.7197],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1348,  4.7969, -1.3789,  ..., -1.4385,  4.0195, -1.7031],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0101,  0.0052, -0.0022,  ...,  0.0025,  0.0023, -0.0020],
        [-0.0018, -0.0010, -0.0088,  ...,  0.0150,  0.0078, -0.0078],
        [ 0.0048,  0.0082,  0.0036,  ..., -0.0093, -0.0114,  0.0120],
        ...,
        [ 0.0027, -0.0099, -0.0040,  ...,  0.0043, -0.0081,  0.0063],
        [-0.0018, -0.0020,  0.0022,  ...,  0.0057, -0.0019, -0.0001],
        [ 0.0031,  0.0032, -0.0010,  ..., -0.0116,  0.0074, -0.0006]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0879,  4.5898, -1.3389,  ..., -1.3271,  3.7891, -1.8486]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:50:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The pepper is colored black
The ruby is colored red
The cabbage is colored green
The peony is colored red
The celery is colored green
The sapphire is colored blue
The milk is colored white
The apple is colored
2024-07-23 23:50:59 root INFO     [order_1_approx] starting weight calculation for The sapphire is colored blue
The apple is colored red
The pepper is colored black
The peony is colored red
The celery is colored green
The ruby is colored red
The milk is colored white
The cabbage is colored
2024-07-23 23:50:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 23:54:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7383, -1.5400,  0.9756,  ...,  0.2103, -0.0977, -1.1299],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9121,  4.2344,  0.3477,  ..., -2.3750,  6.8594, -3.7617],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0129,  0.0035, -0.0001,  ...,  0.0040,  0.0041, -0.0039],
        [-0.0045, -0.0004,  0.0078,  ...,  0.0016,  0.0087, -0.0114],
        [ 0.0186,  0.0040, -0.0036,  ...,  0.0154, -0.0200,  0.0023],
        ...,
        [ 0.0124,  0.0021, -0.0120,  ...,  0.0162, -0.0140,  0.0038],
        [-0.0113, -0.0078, -0.0009,  ...,  0.0116,  0.0091, -0.0054],
        [ 0.0057,  0.0051,  0.0093,  ..., -0.0093, -0.0064,  0.0078]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4717,  4.0000,  0.1456,  ..., -2.7988,  6.1836, -4.0625]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:54:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sapphire is colored blue
The apple is colored red
The pepper is colored black
The peony is colored red
The celery is colored green
The ruby is colored red
The milk is colored white
The cabbage is colored
2024-07-23 23:54:48 root INFO     [order_1_approx] starting weight calculation for The cabbage is colored green
The peony is colored red
The ruby is colored red
The apple is colored red
The milk is colored white
The pepper is colored black
The sapphire is colored blue
The celery is colored
2024-07-23 23:54:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-23 23:58:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0391,  0.6040,  1.2598,  ...,  0.7852,  0.5967, -0.1897],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6079,  2.9648,  1.0293,  ..., -3.0020,  6.5703, -3.7910],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0150,  0.0047, -0.0014,  ...,  0.0090,  0.0026,  0.0005],
        [ 0.0007, -0.0038,  0.0062,  ..., -0.0029,  0.0125, -0.0089],
        [ 0.0018,  0.0040, -0.0009,  ...,  0.0056, -0.0083,  0.0039],
        ...,
        [ 0.0091,  0.0108, -0.0144,  ...,  0.0249, -0.0023,  0.0023],
        [ 0.0033, -0.0005, -0.0042,  ...,  0.0017,  0.0068, -0.0098],
        [ 0.0059,  0.0073, -0.0006,  ..., -0.0123,  0.0061,  0.0124]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6680,  2.7266,  1.0742,  ..., -2.8145,  6.5000, -3.6504]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:58:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The cabbage is colored green
The peony is colored red
The ruby is colored red
The apple is colored red
The milk is colored white
The pepper is colored black
The sapphire is colored blue
The celery is colored
2024-07-23 23:58:37 root INFO     [order_1_approx] starting weight calculation for The sapphire is colored blue
The cabbage is colored green
The peony is colored red
The celery is colored green
The pepper is colored black
The ruby is colored red
The apple is colored red
The milk is colored
2024-07-23 23:58:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 00:02:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8232,  0.4971,  0.4209,  ...,  0.3591,  0.5830,  0.1313],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8984,  6.4102,  0.9414,  ...,  1.4805, -0.2964, -2.4062],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0049,  0.0030, -0.0056,  ..., -0.0054, -0.0101,  0.0036],
        [-0.0040, -0.0113,  0.0081,  ..., -0.0014,  0.0008, -0.0051],
        [-0.0034,  0.0094,  0.0045,  ..., -0.0021, -0.0072,  0.0117],
        ...,
        [-0.0163, -0.0110,  0.0012,  ...,  0.0103, -0.0154,  0.0067],
        [ 0.0052,  0.0063,  0.0139,  ...,  0.0005,  0.0103, -0.0014],
        [-0.0098,  0.0085,  0.0046,  ...,  0.0021,  0.0097,  0.0053]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3750,  6.2891,  1.1445,  ...,  1.1191, -0.6963, -2.9453]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:02:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sapphire is colored blue
The cabbage is colored green
The peony is colored red
The celery is colored green
The pepper is colored black
The ruby is colored red
The apple is colored red
The milk is colored
2024-07-24 00:02:26 root INFO     [order_1_approx] starting weight calculation for The apple is colored red
The milk is colored white
The celery is colored green
The sapphire is colored blue
The ruby is colored red
The cabbage is colored green
The pepper is colored black
The peony is colored
2024-07-24 00:02:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 00:06:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1333,  0.7314,  0.8452,  ..., -1.2910, -1.0352, -0.5908],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5469,  6.0898,  1.1543,  ..., -0.8096,  4.8945, -2.2773],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0016, -0.0060, -0.0021,  ...,  0.0067,  0.0033, -0.0046],
        [-0.0026, -0.0065,  0.0003,  ..., -0.0005,  0.0119, -0.0072],
        [-0.0049,  0.0009, -0.0031,  ..., -0.0028,  0.0009,  0.0066],
        ...,
        [-0.0054, -0.0011, -0.0073,  ...,  0.0094, -0.0097,  0.0050],
        [ 0.0014,  0.0053,  0.0016,  ...,  0.0009, -0.0035, -0.0004],
        [-0.0019, -0.0003,  0.0013,  ..., -0.0070,  0.0030,  0.0090]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4805,  5.7578,  1.2295,  ..., -0.8286,  4.5938, -2.2676]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:06:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The apple is colored red
The milk is colored white
The celery is colored green
The sapphire is colored blue
The ruby is colored red
The cabbage is colored green
The pepper is colored black
The peony is colored
2024-07-24 00:06:17 root INFO     [order_1_approx] starting weight calculation for The milk is colored white
The peony is colored red
The apple is colored red
The cabbage is colored green
The ruby is colored red
The sapphire is colored blue
The celery is colored green
The pepper is colored
2024-07-24 00:06:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 00:10:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7378,  0.9370,  1.5254,  ...,  0.2988, -0.3491,  0.0879],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9932,  4.2344,  0.9805,  ...,  0.4980,  7.6641, -1.0391],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.8494e-02,  2.3041e-03, -4.3373e-03,  ...,  4.5929e-03,
          9.1553e-03, -1.1108e-02],
        [ 5.6534e-03, -2.5940e-04,  7.0953e-04,  ...,  8.8806e-03,
         -6.7902e-03, -2.8839e-03],
        [-4.5090e-03,  1.1734e-02, -2.6875e-03,  ..., -7.7133e-03,
         -7.7152e-04,  1.0086e-02],
        ...,
        [ 3.4332e-05, -5.6076e-03, -9.4452e-03,  ...,  1.3672e-02,
         -1.4420e-02,  1.9779e-03],
        [-1.0101e-02, -1.0841e-02,  1.0452e-02,  ...,  2.8362e-03,
         -4.0588e-03, -4.8332e-03],
        [ 1.2398e-03,  3.0174e-03,  2.0409e-03,  ..., -8.9798e-03,
          5.1384e-03,  8.6517e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0967,  4.4727,  1.0244,  ...,  0.7642,  7.4961, -1.1680]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:10:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The milk is colored white
The peony is colored red
The apple is colored red
The cabbage is colored green
The ruby is colored red
The sapphire is colored blue
The celery is colored green
The pepper is colored
2024-07-24 00:10:06 root INFO     [order_1_approx] starting weight calculation for The apple is colored red
The celery is colored green
The cabbage is colored green
The pepper is colored black
The sapphire is colored blue
The peony is colored red
The milk is colored white
The ruby is colored
2024-07-24 00:10:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 00:13:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5039, -0.0991,  0.7324,  ..., -0.3281,  0.3477, -0.1687],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7246,  4.0586, -0.0215,  ..., -2.2617,  2.4551, -0.0898],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0015, -0.0073, -0.0028,  ..., -0.0037,  0.0022, -0.0043],
        [ 0.0042,  0.0009, -0.0015,  ...,  0.0114,  0.0048, -0.0072],
        [-0.0017,  0.0045, -0.0011,  ..., -0.0014,  0.0027,  0.0019],
        ...,
        [-0.0077, -0.0132, -0.0133,  ...,  0.0142, -0.0043,  0.0087],
        [-0.0049, -0.0062, -0.0002,  ..., -0.0075, -0.0065,  0.0073],
        [ 0.0043,  0.0104,  0.0069,  ..., -0.0072, -0.0003,  0.0025]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0576,  4.1484,  0.0770,  ..., -2.2852,  2.2344, -0.3616]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:13:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The apple is colored red
The celery is colored green
The cabbage is colored green
The pepper is colored black
The sapphire is colored blue
The peony is colored red
The milk is colored white
The ruby is colored
2024-07-24 00:13:51 root INFO     [order_1_approx] starting weight calculation for The peony is colored red
The celery is colored green
The milk is colored white
The ruby is colored red
The apple is colored red
The pepper is colored black
The cabbage is colored green
The sapphire is colored
2024-07-24 00:13:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 00:17:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1250, -1.2285,  1.4238,  ..., -0.1931,  1.2461, -0.7017],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5967,  4.9102, -0.9395,  ..., -1.9434,  2.9609, -0.0635],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0042, -0.0023, -0.0096,  ..., -0.0066,  0.0031,  0.0023],
        [ 0.0040, -0.0044,  0.0044,  ...,  0.0024,  0.0002, -0.0102],
        [ 0.0071,  0.0118,  0.0002,  ..., -0.0028,  0.0073,  0.0072],
        ...,
        [-0.0002,  0.0005, -0.0093,  ...,  0.0056, -0.0056,  0.0107],
        [-0.0007, -0.0114,  0.0087,  ...,  0.0032, -0.0077, -0.0034],
        [ 0.0011,  0.0006, -0.0002,  ..., -0.0200,  0.0017,  0.0126]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6089,  5.1523, -0.9756,  ..., -2.0254,  3.0898, -0.1755]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:17:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The peony is colored red
The celery is colored green
The milk is colored white
The ruby is colored red
The apple is colored red
The pepper is colored black
The cabbage is colored green
The sapphire is colored
2024-07-24 00:17:40 root INFO     total operator prediction time: 1831.549278974533 seconds
2024-07-24 00:17:40 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on country - capital
2024-07-24 00:17:40 root INFO     building operator country - capital
2024-07-24 00:17:40 root INFO     [order_1_approx] starting weight calculation for The country with santiago as its capital is known as chile
The country with rome as its capital is known as italy
The country with zagreb as its capital is known as croatia
The country with beijing as its capital is known as china
The country with hanoi as its capital is known as vietnam
The country with madrid as its capital is known as spain
The country with tokyo as its capital is known as japan
The country with bangkok as its capital is known as
2024-07-24 00:17:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 00:21:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6011,  0.0923,  0.1627,  ...,  0.7671, -0.0122, -0.9219],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.4463,  1.9990, -2.2031,  ...,  1.3535, -4.0234, -5.5742],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0120, -0.0174, -0.0040,  ...,  0.0186,  0.0116, -0.0097],
        [-0.0020, -0.0148, -0.0012,  ..., -0.0027,  0.0071,  0.0089],
        [ 0.0123,  0.0353,  0.0027,  ...,  0.0017,  0.0077,  0.0058],
        ...,
        [-0.0305, -0.0079, -0.0019,  ...,  0.0124, -0.0023,  0.0077],
        [ 0.0066,  0.0155, -0.0009,  ..., -0.0047, -0.0054,  0.0009],
        [ 0.0025,  0.0160, -0.0175,  ...,  0.0025,  0.0186, -0.0050]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6499,  1.7822, -2.6914,  ...,  1.3564, -3.5898, -4.9883]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:21:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with santiago as its capital is known as chile
The country with rome as its capital is known as italy
The country with zagreb as its capital is known as croatia
The country with beijing as its capital is known as china
The country with hanoi as its capital is known as vietnam
The country with madrid as its capital is known as spain
The country with tokyo as its capital is known as japan
The country with bangkok as its capital is known as
2024-07-24 00:21:21 root INFO     [order_1_approx] starting weight calculation for The country with bangkok as its capital is known as thailand
The country with tokyo as its capital is known as japan
The country with zagreb as its capital is known as croatia
The country with rome as its capital is known as italy
The country with hanoi as its capital is known as vietnam
The country with madrid as its capital is known as spain
The country with santiago as its capital is known as chile
The country with beijing as its capital is known as
2024-07-24 00:21:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 00:25:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1250, -0.0530,  0.4653,  ..., -1.5596, -1.4160, -0.1813],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.8379, -3.2734,  0.4272,  ..., -4.5195,  0.2612, -2.7188],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0100,  0.0160, -0.0030,  ...,  0.0169,  0.0206, -0.0119],
        [ 0.0026,  0.0119, -0.0034,  ..., -0.0057,  0.0088, -0.0160],
        [ 0.0168, -0.0084, -0.0126,  ...,  0.0077,  0.0008,  0.0131],
        ...,
        [-0.0091,  0.0033,  0.0006,  ...,  0.0145, -0.0094, -0.0157],
        [-0.0087,  0.0079, -0.0063,  ...,  0.0034, -0.0355,  0.0183],
        [ 0.0018, -0.0393, -0.0189,  ..., -0.0121,  0.0203,  0.0067]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4766, -3.1250,  1.5742,  ..., -5.5820,  0.3167, -2.2266]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:25:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with bangkok as its capital is known as thailand
The country with tokyo as its capital is known as japan
The country with zagreb as its capital is known as croatia
The country with rome as its capital is known as italy
The country with hanoi as its capital is known as vietnam
The country with madrid as its capital is known as spain
The country with santiago as its capital is known as chile
The country with beijing as its capital is known as
2024-07-24 00:25:01 root INFO     [order_1_approx] starting weight calculation for The country with zagreb as its capital is known as croatia
The country with rome as its capital is known as italy
The country with santiago as its capital is known as chile
The country with beijing as its capital is known as china
The country with tokyo as its capital is known as japan
The country with bangkok as its capital is known as thailand
The country with madrid as its capital is known as spain
The country with hanoi as its capital is known as
2024-07-24 00:25:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 00:28:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4792, -0.0898,  0.5938,  ..., -1.2373, -0.8945, -0.1858],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 9.0156, -3.5039,  2.0059,  ..., -5.7109, -0.8120, -1.4941],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0114, -0.0223,  0.0155,  ..., -0.0058,  0.0062, -0.0345],
        [ 0.0042, -0.0104,  0.0037,  ..., -0.0003, -0.0048, -0.0190],
        [-0.0074,  0.0211, -0.0228,  ..., -0.0108, -0.0050, -0.0148],
        ...,
        [ 0.0171,  0.0096, -0.0188,  ...,  0.0070,  0.0029,  0.0137],
        [-0.0057,  0.0027,  0.0031,  ...,  0.0032, -0.0270,  0.0220],
        [ 0.0178, -0.0147, -0.0185,  ...,  0.0113,  0.0035, -0.0113]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 9.6250, -3.0137,  1.3193,  ..., -5.5469, -0.9019, -1.6328]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:28:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with zagreb as its capital is known as croatia
The country with rome as its capital is known as italy
The country with santiago as its capital is known as chile
The country with beijing as its capital is known as china
The country with tokyo as its capital is known as japan
The country with bangkok as its capital is known as thailand
The country with madrid as its capital is known as spain
The country with hanoi as its capital is known as
2024-07-24 00:28:31 root INFO     [order_1_approx] starting weight calculation for The country with rome as its capital is known as italy
The country with zagreb as its capital is known as croatia
The country with hanoi as its capital is known as vietnam
The country with bangkok as its capital is known as thailand
The country with beijing as its capital is known as china
The country with tokyo as its capital is known as japan
The country with santiago as its capital is known as chile
The country with madrid as its capital is known as
2024-07-24 00:28:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 00:32:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8740,  0.5342,  1.0820,  ..., -0.8589,  0.3496, -0.2144],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7012,  0.6094,  1.9932,  ..., -1.9141, -6.3633,  0.6655],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0072,  0.0062, -0.0047,  ...,  0.0101,  0.0014, -0.0164],
        [-0.0031, -0.0237, -0.0022,  ...,  0.0046,  0.0131,  0.0003],
        [-0.0153,  0.0051, -0.0082,  ..., -0.0006, -0.0098, -0.0016],
        ...,
        [-0.0101,  0.0031,  0.0037,  ...,  0.0050, -0.0138,  0.0103],
        [ 0.0120,  0.0160,  0.0012,  ..., -0.0155, -0.0228, -0.0143],
        [ 0.0037, -0.0122, -0.0040,  ..., -0.0220,  0.0131, -0.0167]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3284,  1.0908,  2.1133,  ..., -2.2051, -6.7930,  0.5132]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:32:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with rome as its capital is known as italy
The country with zagreb as its capital is known as croatia
The country with hanoi as its capital is known as vietnam
The country with bangkok as its capital is known as thailand
The country with beijing as its capital is known as china
The country with tokyo as its capital is known as japan
The country with santiago as its capital is known as chile
The country with madrid as its capital is known as
2024-07-24 00:32:15 root INFO     [order_1_approx] starting weight calculation for The country with madrid as its capital is known as spain
The country with bangkok as its capital is known as thailand
The country with tokyo as its capital is known as japan
The country with beijing as its capital is known as china
The country with santiago as its capital is known as chile
The country with zagreb as its capital is known as croatia
The country with hanoi as its capital is known as vietnam
The country with rome as its capital is known as
2024-07-24 00:32:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 00:35:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1055, -0.4380,  0.7266,  ...,  0.4253, -0.5112, -1.1299],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8047,  1.5840, -0.4951,  ..., -1.0059, -1.6689, -2.2070],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0021, -0.0072, -0.0042,  ..., -0.0017, -0.0070, -0.0112],
        [-0.0016, -0.0249, -0.0001,  ...,  0.0034,  0.0088, -0.0133],
        [-0.0049,  0.0122, -0.0156,  ..., -0.0072, -0.0126, -0.0043],
        ...,
        [ 0.0053,  0.0019, -0.0039,  ..., -0.0093,  0.0036, -0.0134],
        [-0.0008,  0.0039, -0.0009,  ..., -0.0034, -0.0242,  0.0076],
        [-0.0076, -0.0170,  0.0008,  ..., -0.0127,  0.0106, -0.0041]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2891,  2.1855, -1.6396,  ..., -0.7485, -1.5049, -2.3984]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:35:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with madrid as its capital is known as spain
The country with bangkok as its capital is known as thailand
The country with tokyo as its capital is known as japan
The country with beijing as its capital is known as china
The country with santiago as its capital is known as chile
The country with zagreb as its capital is known as croatia
The country with hanoi as its capital is known as vietnam
The country with rome as its capital is known as
2024-07-24 00:35:52 root INFO     [order_1_approx] starting weight calculation for The country with bangkok as its capital is known as thailand
The country with madrid as its capital is known as spain
The country with beijing as its capital is known as china
The country with zagreb as its capital is known as croatia
The country with hanoi as its capital is known as vietnam
The country with tokyo as its capital is known as japan
The country with rome as its capital is known as italy
The country with santiago as its capital is known as
2024-07-24 00:35:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 00:39:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3748,  0.3044,  0.3518,  ..., -0.6011, -0.5337, -0.0732],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.5664, -1.5557,  0.3916,  ..., -1.5703, -2.6094,  1.2080],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.1230e-02,  9.4452e-03, -1.7578e-02,  ...,  1.4130e-02,
          3.6652e-02, -7.2479e-03],
        [-1.6556e-03,  1.2375e-02, -8.5297e-03,  ...,  6.1760e-03,
          4.6387e-03,  9.6359e-03],
        [ 4.1580e-03,  1.1215e-02, -1.0971e-02,  ...,  5.8365e-03,
          1.0704e-02,  4.3488e-03],
        ...,
        [-2.3346e-02,  2.0409e-04,  6.6757e-05,  ...,  2.7313e-02,
         -2.5101e-02, -7.9880e-03],
        [-2.9907e-03,  7.1564e-03, -2.4948e-02,  ...,  3.2532e-02,
         -2.5360e-02,  2.9877e-02],
        [ 1.6998e-02,  8.6517e-03, -1.5030e-03,  ...,  6.6452e-03,
          1.4908e-02, -1.4221e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.1016, -0.6221,  0.0066,  ..., -0.5234, -2.0996,  1.6758]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:39:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with bangkok as its capital is known as thailand
The country with madrid as its capital is known as spain
The country with beijing as its capital is known as china
The country with zagreb as its capital is known as croatia
The country with hanoi as its capital is known as vietnam
The country with tokyo as its capital is known as japan
The country with rome as its capital is known as italy
The country with santiago as its capital is known as
2024-07-24 00:39:37 root INFO     [order_1_approx] starting weight calculation for The country with rome as its capital is known as italy
The country with beijing as its capital is known as china
The country with zagreb as its capital is known as croatia
The country with santiago as its capital is known as chile
The country with hanoi as its capital is known as vietnam
The country with madrid as its capital is known as spain
The country with bangkok as its capital is known as thailand
The country with tokyo as its capital is known as
2024-07-24 00:39:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 00:43:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.5498,  1.7432,  2.1738,  ...,  0.2200,  0.3362, -1.0869],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5386, -2.5430,  0.3389,  ..., -4.5312, -2.6562, -1.5811],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0040,  0.0020, -0.0126,  ...,  0.0022,  0.0172, -0.0066],
        [ 0.0119, -0.0064, -0.0029,  ...,  0.0069,  0.0008, -0.0172],
        [ 0.0178,  0.0199, -0.0125,  ...,  0.0239, -0.0093, -0.0035],
        ...,
        [ 0.0016,  0.0022, -0.0124,  ...,  0.0064, -0.0101,  0.0084],
        [ 0.0001,  0.0067, -0.0123,  ...,  0.0041, -0.0234,  0.0241],
        [ 0.0090, -0.0135,  0.0012,  ..., -0.0059,  0.0205,  0.0056]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0952, -1.4912,  0.9004,  ..., -4.8125, -2.5723, -1.5850]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:43:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with rome as its capital is known as italy
The country with beijing as its capital is known as china
The country with zagreb as its capital is known as croatia
The country with santiago as its capital is known as chile
The country with hanoi as its capital is known as vietnam
The country with madrid as its capital is known as spain
The country with bangkok as its capital is known as thailand
The country with tokyo as its capital is known as
2024-07-24 00:43:18 root INFO     [order_1_approx] starting weight calculation for The country with bangkok as its capital is known as thailand
The country with beijing as its capital is known as china
The country with madrid as its capital is known as spain
The country with santiago as its capital is known as chile
The country with rome as its capital is known as italy
The country with hanoi as its capital is known as vietnam
The country with tokyo as its capital is known as japan
The country with zagreb as its capital is known as
2024-07-24 00:43:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 00:46:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1807,  0.3889,  0.4731,  ..., -0.1748,  0.2314,  0.1377],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0527, -2.8711,  3.3633,  ..., -0.6357, -4.8438, -3.0684],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.1118e-02, -2.0859e-02, -2.1095e-03,  ...,  1.9653e-02,
         -1.5097e-03, -1.2383e-02],
        [ 9.7198e-03, -1.8616e-02,  2.2156e-02,  ...,  8.6823e-03,
          2.6733e-02, -6.7596e-03],
        [-8.5297e-03,  5.3177e-03, -2.5009e-02,  ...,  1.3954e-02,
         -1.1116e-02, -6.2523e-03],
        ...,
        [ 8.1177e-03,  1.6260e-04, -4.1656e-03,  ...,  1.5802e-03,
         -6.6757e-05, -1.3809e-03],
        [ 1.8402e-02,  1.9409e-02, -3.0457e-02,  ..., -1.1841e-02,
         -4.4647e-02,  1.0071e-02],
        [ 1.3474e-02,  9.2545e-03, -3.0289e-02,  ..., -1.6373e-02,
         -2.1622e-02,  5.8556e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4424, -2.7031,  2.8887,  ..., -0.6445, -6.9727, -3.8008]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:46:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with bangkok as its capital is known as thailand
The country with beijing as its capital is known as china
The country with madrid as its capital is known as spain
The country with santiago as its capital is known as chile
The country with rome as its capital is known as italy
The country with hanoi as its capital is known as vietnam
The country with tokyo as its capital is known as japan
The country with zagreb as its capital is known as
2024-07-24 00:46:53 root INFO     total operator prediction time: 1753.1530718803406 seconds
2024-07-24 00:46:53 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on name - occupation
2024-07-24 00:46:53 root INFO     building operator name - occupation
2024-07-24 00:46:53 root INFO     [order_1_approx] starting weight calculation for edison was known for their work as a  inventor
darwin was known for their work as a  naturalist
haydn was known for their work as a  composer
pacino was known for their work as a  actor
plato was known for their work as a  philosopher
raphael was known for their work as a  painter
picasso was known for their work as a  painter
caesar was known for their work as a 
2024-07-24 00:46:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 00:50:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7651, -0.7817, -0.5732,  ...,  1.1680,  0.1721,  0.3667],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6343,  0.5898, -4.6680,  ..., -2.7715,  0.1523, -0.9004],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-7.3013e-03, -2.8706e-03,  6.2141e-03,  ..., -4.0474e-03,
          2.7588e-02,  8.9340e-03],
        [-1.7654e-02,  8.6823e-03, -4.3106e-04,  ...,  5.8937e-03,
          8.6060e-03, -6.2370e-03],
        [ 8.7738e-03,  7.8344e-04, -8.1329e-03,  ..., -8.3313e-03,
          1.2070e-02, -1.4000e-02],
        ...,
        [ 1.1711e-03, -1.5480e-02, -1.5602e-03,  ...,  6.7368e-03,
         -4.8676e-03, -1.0323e-02],
        [-1.5640e-03, -5.7411e-03,  1.4381e-03,  ...,  3.7746e-03,
         -9.1553e-05, -4.1046e-03],
        [-1.2291e-02, -1.0834e-03, -7.2289e-03,  ..., -8.1329e-03,
          1.0063e-02,  1.4877e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7822,  0.9365, -5.6523,  ..., -2.4453, -0.3608, -1.1602]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:50:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for edison was known for their work as a  inventor
darwin was known for their work as a  naturalist
haydn was known for their work as a  composer
pacino was known for their work as a  actor
plato was known for their work as a  philosopher
raphael was known for their work as a  painter
picasso was known for their work as a  painter
caesar was known for their work as a 
2024-07-24 00:50:42 root INFO     [order_1_approx] starting weight calculation for raphael was known for their work as a  painter
pacino was known for their work as a  actor
picasso was known for their work as a  painter
caesar was known for their work as a  emperor
plato was known for their work as a  philosopher
haydn was known for their work as a  composer
edison was known for their work as a  inventor
darwin was known for their work as a 
2024-07-24 00:50:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 00:54:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3472, -0.3906,  0.0479,  ...,  0.5073, -0.8135,  0.7358],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3516, -4.3398, -1.5293,  ..., -2.0703, -0.9336, -1.7109],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.1654e-03, -6.6376e-04, -1.4084e-02,  ...,  4.5776e-05,
         -8.2092e-03, -9.9945e-03],
        [-9.4528e-03,  5.5504e-04, -1.1162e-02,  ...,  5.3864e-03,
          9.4757e-03,  4.6158e-03],
        [ 4.3831e-03, -4.4861e-03, -2.1866e-02,  ..., -8.9264e-03,
         -2.7637e-03, -5.8823e-03],
        ...,
        [-1.1589e-02,  1.1833e-02, -8.4877e-04,  ...,  4.5872e-04,
         -1.1086e-02, -4.3945e-03],
        [ 9.7504e-03, -3.5019e-03,  7.3318e-03,  ...,  1.3428e-02,
          1.0748e-03, -7.4005e-03],
        [ 8.1062e-06,  1.1024e-03,  3.3798e-03,  ..., -1.4524e-03,
         -1.3382e-02,  2.2697e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.8428, -3.5469, -1.8096,  ..., -2.0898, -1.1367, -1.5371]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:54:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for raphael was known for their work as a  painter
pacino was known for their work as a  actor
picasso was known for their work as a  painter
caesar was known for their work as a  emperor
plato was known for their work as a  philosopher
haydn was known for their work as a  composer
edison was known for their work as a  inventor
darwin was known for their work as a 
2024-07-24 00:54:28 root INFO     [order_1_approx] starting weight calculation for caesar was known for their work as a  emperor
darwin was known for their work as a  naturalist
plato was known for their work as a  philosopher
picasso was known for their work as a  painter
raphael was known for their work as a  painter
pacino was known for their work as a  actor
haydn was known for their work as a  composer
edison was known for their work as a 
2024-07-24 00:54:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 00:58:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2664,  0.7109,  1.5029,  ...,  1.7520, -0.4287,  0.5537],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9556, -0.4175, -2.4551,  ..., -3.6523, -5.5078, -2.7051],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0089, -0.0117, -0.0031,  ...,  0.0072,  0.0191, -0.0071],
        [-0.0077,  0.0011, -0.0015,  ...,  0.0133, -0.0172, -0.0017],
        [-0.0049,  0.0129, -0.0127,  ..., -0.0071, -0.0036,  0.0035],
        ...,
        [ 0.0004,  0.0278,  0.0004,  ..., -0.0076, -0.0359,  0.0020],
        [-0.0034, -0.0054, -0.0022,  ...,  0.0130,  0.0028,  0.0036],
        [-0.0232,  0.0062, -0.0056,  ...,  0.0087,  0.0188,  0.0071]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7139, -0.1984, -3.3535,  ..., -3.7129, -5.2461, -2.8730]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:58:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for caesar was known for their work as a  emperor
darwin was known for their work as a  naturalist
plato was known for their work as a  philosopher
picasso was known for their work as a  painter
raphael was known for their work as a  painter
pacino was known for their work as a  actor
haydn was known for their work as a  composer
edison was known for their work as a 
2024-07-24 00:58:03 root INFO     [order_1_approx] starting weight calculation for pacino was known for their work as a  actor
caesar was known for their work as a  emperor
darwin was known for their work as a  naturalist
plato was known for their work as a  philosopher
raphael was known for their work as a  painter
edison was known for their work as a  inventor
picasso was known for their work as a  painter
haydn was known for their work as a 
2024-07-24 00:58:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 01:01:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5283, -0.1465, -0.2190,  ...,  1.0137, -0.6265,  0.9014],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5000, -0.8442, -0.4888,  ..., -1.6387,  1.6582,  3.0801],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0151,  0.0016,  0.0025,  ..., -0.0030,  0.0030, -0.0139],
        [-0.0129,  0.0105,  0.0167,  ...,  0.0103,  0.0038, -0.0033],
        [ 0.0012,  0.0039, -0.0021,  ...,  0.0147, -0.0018, -0.0087],
        ...,
        [ 0.0064, -0.0091, -0.0101,  ..., -0.0034, -0.0046, -0.0001],
        [-0.0139, -0.0063, -0.0051,  ...,  0.0094, -0.0199,  0.0091],
        [-0.0197,  0.0064,  0.0030,  ...,  0.0092,  0.0187,  0.0143]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8052, -0.4661, -0.2798,  ..., -2.1992,  1.6426,  3.4648]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:01:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for pacino was known for their work as a  actor
caesar was known for their work as a  emperor
darwin was known for their work as a  naturalist
plato was known for their work as a  philosopher
raphael was known for their work as a  painter
edison was known for their work as a  inventor
picasso was known for their work as a  painter
haydn was known for their work as a 
2024-07-24 01:01:51 root INFO     [order_1_approx] starting weight calculation for raphael was known for their work as a  painter
edison was known for their work as a  inventor
plato was known for their work as a  philosopher
picasso was known for their work as a  painter
caesar was known for their work as a  emperor
haydn was known for their work as a  composer
darwin was known for their work as a  naturalist
pacino was known for their work as a 
2024-07-24 01:01:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 01:05:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2336, -0.1487,  1.9102,  ...,  1.2354, -0.3726,  1.0234],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0195, -2.1543,  1.7607,  ..., -4.1367,  2.1992,  2.7500],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0083,  0.0063, -0.0157,  ...,  0.0135,  0.0084,  0.0040],
        [-0.0174, -0.0047, -0.0074,  ...,  0.0181, -0.0273, -0.0091],
        [-0.0154,  0.0177, -0.0050,  ...,  0.0066,  0.0037, -0.0209],
        ...,
        [-0.0108, -0.0120, -0.0021,  ..., -0.0133, -0.0224, -0.0059],
        [ 0.0078, -0.0008,  0.0030,  ..., -0.0105, -0.0027, -0.0011],
        [ 0.0004,  0.0025, -0.0019,  ..., -0.0010,  0.0212,  0.0227]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4004, -1.5967,  1.5684,  ..., -4.2383,  1.7285,  2.1309]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:05:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for raphael was known for their work as a  painter
edison was known for their work as a  inventor
plato was known for their work as a  philosopher
picasso was known for their work as a  painter
caesar was known for their work as a  emperor
haydn was known for their work as a  composer
darwin was known for their work as a  naturalist
pacino was known for their work as a 
2024-07-24 01:05:39 root INFO     [order_1_approx] starting weight calculation for plato was known for their work as a  philosopher
raphael was known for their work as a  painter
darwin was known for their work as a  naturalist
caesar was known for their work as a  emperor
pacino was known for their work as a  actor
edison was known for their work as a  inventor
haydn was known for their work as a  composer
picasso was known for their work as a 
2024-07-24 01:05:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 01:09:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6895, -0.0342, -0.5771,  ...,  1.5967,  0.5591, -0.0371],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4800,  0.2812, -2.7461,  ..., -0.8911, -3.3594,  1.0312],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0103,  0.0021,  0.0014,  ..., -0.0066,  0.0216, -0.0091],
        [-0.0040,  0.0094,  0.0113,  ...,  0.0194,  0.0231, -0.0025],
        [-0.0039,  0.0145,  0.0046,  ..., -0.0077, -0.0035, -0.0154],
        ...,
        [-0.0069,  0.0111,  0.0030,  ...,  0.0022, -0.0100,  0.0016],
        [ 0.0052,  0.0068,  0.0020,  ...,  0.0087, -0.0084, -0.0076],
        [-0.0111, -0.0105, -0.0041,  ..., -0.0141,  0.0123,  0.0075]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7109,  0.8755, -3.0488,  ..., -0.4404, -3.7812,  0.6758]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:09:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for plato was known for their work as a  philosopher
raphael was known for their work as a  painter
darwin was known for their work as a  naturalist
caesar was known for their work as a  emperor
pacino was known for their work as a  actor
edison was known for their work as a  inventor
haydn was known for their work as a  composer
picasso was known for their work as a 
2024-07-24 01:09:18 root INFO     [order_1_approx] starting weight calculation for darwin was known for their work as a  naturalist
haydn was known for their work as a  composer
caesar was known for their work as a  emperor
pacino was known for their work as a  actor
edison was known for their work as a  inventor
picasso was known for their work as a  painter
raphael was known for their work as a  painter
plato was known for their work as a 
2024-07-24 01:09:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 01:12:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4958,  0.6260,  1.2549,  ...,  0.3362,  0.1533,  0.3394],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9648, -0.1367,  0.2915,  ..., -5.4531, -0.8008,  1.0020],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-7.6294e-05,  4.2992e-03,  9.3384e-03,  ..., -1.8738e-02,
          1.9684e-02, -7.4005e-04],
        [-8.5678e-03,  7.5455e-03,  1.5736e-03,  ..., -9.0027e-04,
          1.5213e-02, -3.2921e-03],
        [ 1.7868e-02,  1.5259e-03, -9.6893e-03,  ...,  7.6103e-03,
          1.5869e-02, -8.8959e-03],
        ...,
        [-1.1078e-02,  7.6218e-03, -1.3382e-02,  ..., -1.2360e-02,
         -3.0708e-03, -1.1883e-03],
        [ 6.6528e-03, -9.3613e-03,  1.0727e-02,  ...,  1.7212e-02,
         -2.0885e-03,  6.6605e-03],
        [-3.4668e-02, -1.2184e-02,  1.1459e-02,  ...,  1.7715e-02,
          1.2093e-02,  9.7580e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3057, -0.5659, -0.1013,  ..., -5.7656, -1.6162,  0.1553]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:12:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for darwin was known for their work as a  naturalist
haydn was known for their work as a  composer
caesar was known for their work as a  emperor
pacino was known for their work as a  actor
edison was known for their work as a  inventor
picasso was known for their work as a  painter
raphael was known for their work as a  painter
plato was known for their work as a 
2024-07-24 01:12:59 root INFO     [order_1_approx] starting weight calculation for darwin was known for their work as a  naturalist
plato was known for their work as a  philosopher
haydn was known for their work as a  composer
caesar was known for their work as a  emperor
picasso was known for their work as a  painter
pacino was known for their work as a  actor
edison was known for their work as a  inventor
raphael was known for their work as a 
2024-07-24 01:12:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 01:16:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.3174, -1.3574, -0.3430,  ...,  1.0996,  1.0986, -0.6714],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0137, -0.5293, -4.3477,  ..., -2.3496, -1.5215,  3.4023],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0011,  0.0094, -0.0055,  ..., -0.0067,  0.0171, -0.0049],
        [ 0.0047,  0.0070, -0.0062,  ...,  0.0081,  0.0114, -0.0045],
        [-0.0028, -0.0036, -0.0079,  ...,  0.0047,  0.0005, -0.0067],
        ...,
        [-0.0030,  0.0028, -0.0069,  ...,  0.0032, -0.0042,  0.0095],
        [-0.0029, -0.0035, -0.0027,  ...,  0.0098, -0.0208,  0.0048],
        [-0.0200, -0.0122, -0.0016,  ...,  0.0094, -0.0054,  0.0043]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.8867, -0.2153, -4.4297,  ..., -2.8418, -1.6514,  3.5332]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:16:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for darwin was known for their work as a  naturalist
plato was known for their work as a  philosopher
haydn was known for their work as a  composer
caesar was known for their work as a  emperor
picasso was known for their work as a  painter
pacino was known for their work as a  actor
edison was known for their work as a  inventor
raphael was known for their work as a 
2024-07-24 01:16:44 root INFO     total operator prediction time: 1790.2937545776367 seconds
2024-07-24 01:16:44 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on male - female
2024-07-24 01:16:44 root INFO     building operator male - female
2024-07-24 01:16:44 root INFO     [order_1_approx] starting weight calculation for A female stepfather is known as a stepmother
A female waiter is known as a waitress
A female emperor is known as a empress
A female fisherman is known as a fisherwoman
A female sculptor is known as a sculptress
A female grandfather is known as a grandmother
A female superman is known as a superwoman
A female bull is known as a
2024-07-24 01:16:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 01:20:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3330, -0.8574, -0.7285,  ...,  2.3066, -0.2375, -0.5439],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.3750,  1.4023, -4.6641,  ..., -3.1035,  4.1758, -1.7695],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0161, -0.0126,  0.0024,  ...,  0.0179, -0.0022, -0.0206],
        [-0.0128, -0.0094, -0.0043,  ...,  0.0003, -0.0130, -0.0202],
        [ 0.0162,  0.0182,  0.0008,  ...,  0.0002,  0.0015,  0.0075],
        ...,
        [-0.0203, -0.0036,  0.0047,  ..., -0.0160, -0.0212, -0.0103],
        [ 0.0142, -0.0119, -0.0039,  ...,  0.0088, -0.0130,  0.0021],
        [ 0.0019,  0.0030, -0.0016,  ..., -0.0164,  0.0056, -0.0043]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.4375,  1.8750, -5.5586,  ..., -2.3809,  5.0547, -1.8359]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:20:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female stepfather is known as a stepmother
A female waiter is known as a waitress
A female emperor is known as a empress
A female fisherman is known as a fisherwoman
A female sculptor is known as a sculptress
A female grandfather is known as a grandmother
A female superman is known as a superwoman
A female bull is known as a
2024-07-24 01:20:30 root INFO     [order_1_approx] starting weight calculation for A female bull is known as a cow
A female waiter is known as a waitress
A female sculptor is known as a sculptress
A female fisherman is known as a fisherwoman
A female superman is known as a superwoman
A female grandfather is known as a grandmother
A female stepfather is known as a stepmother
A female emperor is known as a
2024-07-24 01:20:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 01:24:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0654, -0.7402,  0.1011,  ..., -0.5181,  0.9165, -0.6162],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3750,  0.1294, -3.6035,  ..., -1.9961, -0.1975, -4.4688],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0123,  0.0135, -0.0015,  ..., -0.0008,  0.0237, -0.0129],
        [ 0.0065, -0.0272, -0.0068,  ...,  0.0118, -0.0055, -0.0014],
        [ 0.0454,  0.0010,  0.0070,  ..., -0.0197,  0.0085,  0.0134],
        ...,
        [-0.0155, -0.0012,  0.0020,  ...,  0.0005, -0.0199, -0.0036],
        [-0.0037, -0.0338, -0.0073,  ...,  0.0192, -0.0510,  0.0043],
        [ 0.0134,  0.0264,  0.0234,  ..., -0.0177,  0.0105, -0.0064]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.2480, -0.7407, -4.8398,  ..., -2.7305,  0.6328, -4.8125]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:24:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female bull is known as a cow
A female waiter is known as a waitress
A female sculptor is known as a sculptress
A female fisherman is known as a fisherwoman
A female superman is known as a superwoman
A female grandfather is known as a grandmother
A female stepfather is known as a stepmother
A female emperor is known as a
2024-07-24 01:24:16 root INFO     [order_1_approx] starting weight calculation for A female sculptor is known as a sculptress
A female grandfather is known as a grandmother
A female stepfather is known as a stepmother
A female waiter is known as a waitress
A female bull is known as a cow
A female superman is known as a superwoman
A female emperor is known as a empress
A female fisherman is known as a
2024-07-24 01:24:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 01:28:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5820,  0.0615,  0.6094,  ...,  0.6250, -0.3643,  1.0068],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.5938,  3.3281, -4.9844,  ...,  2.3555,  1.6621,  2.1934],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0029, -0.0071,  0.0055,  ...,  0.0133,  0.0026, -0.0166],
        [-0.0074, -0.0344, -0.0187,  ...,  0.0141,  0.0041,  0.0123],
        [-0.0229,  0.0023,  0.0082,  ..., -0.0173, -0.0057,  0.0354],
        ...,
        [-0.0159, -0.0248, -0.0137,  ...,  0.0034, -0.0124, -0.0065],
        [ 0.0184, -0.0083,  0.0036,  ...,  0.0037, -0.0190,  0.0099],
        [-0.0105,  0.0162,  0.0049,  ...,  0.0241, -0.0063, -0.0225]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.3828,  3.3516, -4.4453,  ...,  2.6836,  2.7930,  1.0029]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:28:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female sculptor is known as a sculptress
A female grandfather is known as a grandmother
A female stepfather is known as a stepmother
A female waiter is known as a waitress
A female bull is known as a cow
A female superman is known as a superwoman
A female emperor is known as a empress
A female fisherman is known as a
2024-07-24 01:28:01 root INFO     [order_1_approx] starting weight calculation for A female emperor is known as a empress
A female sculptor is known as a sculptress
A female waiter is known as a waitress
A female bull is known as a cow
A female fisherman is known as a fisherwoman
A female superman is known as a superwoman
A female stepfather is known as a stepmother
A female grandfather is known as a
2024-07-24 01:28:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 01:31:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2083, -1.0635,  0.8936,  ..., -0.4331,  0.8403, -0.5381],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 8.1484, -3.9121, -1.2715,  ..., -0.7769,  2.9805, -3.9766],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0382, -0.0069,  0.0242,  ...,  0.0115, -0.0080, -0.0421],
        [-0.0078,  0.0069,  0.0250,  ...,  0.0115, -0.0086, -0.0099],
        [ 0.0088,  0.0030, -0.0004,  ..., -0.0106,  0.0136,  0.0084],
        ...,
        [-0.0048, -0.0083, -0.0110,  ..., -0.0022,  0.0031,  0.0005],
        [ 0.0083,  0.0053,  0.0053,  ...,  0.0074, -0.0165, -0.0040],
        [ 0.0211, -0.0003,  0.0014,  ...,  0.0091,  0.0349,  0.0104]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 8.6406, -3.6797, -0.7319,  ..., -1.2695,  2.8750, -5.0156]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:31:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female emperor is known as a empress
A female sculptor is known as a sculptress
A female waiter is known as a waitress
A female bull is known as a cow
A female fisherman is known as a fisherwoman
A female superman is known as a superwoman
A female stepfather is known as a stepmother
A female grandfather is known as a
2024-07-24 01:31:47 root INFO     [order_1_approx] starting weight calculation for A female bull is known as a cow
A female stepfather is known as a stepmother
A female grandfather is known as a grandmother
A female fisherman is known as a fisherwoman
A female superman is known as a superwoman
A female emperor is known as a empress
A female waiter is known as a waitress
A female sculptor is known as a
2024-07-24 01:31:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 01:35:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5908, -1.4043,  0.3418,  ..., -0.9766,  0.8662,  0.5293],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.3125,  2.5957, -4.6211,  ..., -1.2266, -3.5391, -1.7451],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0080,  0.0009,  0.0039,  ..., -0.0138, -0.0033, -0.0134],
        [-0.0101, -0.0195,  0.0017,  ...,  0.0053,  0.0067,  0.0006],
        [ 0.0305,  0.0143, -0.0017,  ..., -0.0028, -0.0003, -0.0091],
        ...,
        [-0.0038, -0.0013, -0.0047,  ..., -0.0208, -0.0096, -0.0028],
        [-0.0086, -0.0025,  0.0100,  ...,  0.0113, -0.0299, -0.0024],
        [-0.0163, -0.0027, -0.0025,  ...,  0.0098,  0.0091, -0.0082]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.6992,  3.4121, -5.9258,  ..., -0.9868, -3.5312, -2.2070]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:35:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female bull is known as a cow
A female stepfather is known as a stepmother
A female grandfather is known as a grandmother
A female fisherman is known as a fisherwoman
A female superman is known as a superwoman
A female emperor is known as a empress
A female waiter is known as a waitress
A female sculptor is known as a
2024-07-24 01:35:30 root INFO     [order_1_approx] starting weight calculation for A female sculptor is known as a sculptress
A female superman is known as a superwoman
A female grandfather is known as a grandmother
A female bull is known as a cow
A female fisherman is known as a fisherwoman
A female emperor is known as a empress
A female waiter is known as a waitress
A female stepfather is known as a
2024-07-24 01:35:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 01:39:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3516,  0.4202,  0.7773,  ...,  0.3413,  0.6318, -0.9131],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.6094, -6.1758,  1.0381,  ..., -0.7090,  2.0742, -1.9707],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0228, -0.0008,  0.0194,  ...,  0.0042,  0.0160, -0.0301],
        [ 0.0083,  0.0014,  0.0082,  ..., -0.0058, -0.0008,  0.0203],
        [ 0.0023,  0.0102, -0.0086,  ..., -0.0065,  0.0123, -0.0112],
        ...,
        [-0.0168, -0.0035, -0.0271,  ..., -0.0065, -0.0114,  0.0239],
        [ 0.0006, -0.0038, -0.0018,  ...,  0.0085, -0.0144,  0.0201],
        [ 0.0054,  0.0074,  0.0023,  ..., -0.0043,  0.0170, -0.0135]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.4473, -6.0547,  0.5840,  ..., -0.5684,  2.2891, -2.0293]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:39:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female sculptor is known as a sculptress
A female superman is known as a superwoman
A female grandfather is known as a grandmother
A female bull is known as a cow
A female fisherman is known as a fisherwoman
A female emperor is known as a empress
A female waiter is known as a waitress
A female stepfather is known as a
2024-07-24 01:39:14 root INFO     [order_1_approx] starting weight calculation for A female sculptor is known as a sculptress
A female fisherman is known as a fisherwoman
A female grandfather is known as a grandmother
A female waiter is known as a waitress
A female bull is known as a cow
A female stepfather is known as a stepmother
A female emperor is known as a empress
A female superman is known as a
2024-07-24 01:39:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 01:43:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 7.3242e-04, -4.3628e-01,  6.3330e-01,  ..., -5.4443e-02,
        -1.0664e+00, -5.9814e-01], device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6680, -1.2559, -3.9004,  ..., -2.4199, -3.6250,  1.7695],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0056, -0.0089,  0.0076,  ..., -0.0098,  0.0001, -0.0258],
        [-0.0105, -0.0221, -0.0050,  ...,  0.0255, -0.0063,  0.0190],
        [ 0.0065,  0.0197, -0.0178,  ...,  0.0001,  0.0182, -0.0110],
        ...,
        [-0.0138, -0.0122,  0.0038,  ..., -0.0030, -0.0217,  0.0022],
        [ 0.0022,  0.0092,  0.0035,  ...,  0.0049, -0.0260,  0.0048],
        [-0.0201,  0.0114, -0.0038,  ...,  0.0076,  0.0085, -0.0145]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0879, -0.6543, -4.5820,  ..., -1.9746, -3.1465,  1.8242]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:43:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female sculptor is known as a sculptress
A female fisherman is known as a fisherwoman
A female grandfather is known as a grandmother
A female waiter is known as a waitress
A female bull is known as a cow
A female stepfather is known as a stepmother
A female emperor is known as a empress
A female superman is known as a
2024-07-24 01:43:04 root INFO     [order_1_approx] starting weight calculation for A female fisherman is known as a fisherwoman
A female superman is known as a superwoman
A female emperor is known as a empress
A female stepfather is known as a stepmother
A female grandfather is known as a grandmother
A female bull is known as a cow
A female sculptor is known as a sculptress
A female waiter is known as a
2024-07-24 01:43:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 01:46:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6504, -1.0488,  0.6479,  ...,  0.7554,  0.5244,  0.4463],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0576,  1.8594, -1.3594,  ...,  3.0508,  2.2188,  0.3232],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0027, -0.0099,  0.0137,  ...,  0.0101,  0.0097, -0.0255],
        [-0.0176,  0.0065, -0.0087,  ...,  0.0020,  0.0026,  0.0038],
        [ 0.0122, -0.0001, -0.0162,  ...,  0.0249,  0.0018,  0.0164],
        ...,
        [-0.0260, -0.0168, -0.0076,  ..., -0.0009, -0.0163, -0.0038],
        [ 0.0018, -0.0021, -0.0049,  ...,  0.0163, -0.0199,  0.0222],
        [ 0.0001, -0.0032, -0.0007,  ..., -0.0008,  0.0002,  0.0093]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2881,  2.6250, -1.9336,  ...,  2.9141,  2.6426, -0.1924]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:46:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female fisherman is known as a fisherwoman
A female superman is known as a superwoman
A female emperor is known as a empress
A female stepfather is known as a stepmother
A female grandfather is known as a grandmother
A female bull is known as a cow
A female sculptor is known as a sculptress
A female waiter is known as a
2024-07-24 01:46:55 root INFO     total operator prediction time: 1811.1842279434204 seconds
2024-07-24 01:46:55 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on animal - shelter
2024-07-24 01:46:55 root INFO     building operator animal - shelter
2024-07-24 01:46:55 root INFO     [order_1_approx] starting weight calculation for The place hamster lives in is called nest
The place snake lives in is called nest
The place crow lives in is called nest
The place dog lives in is called doghouse
The place fly lives in is called nest
The place raven lives in is called nest
The place wasp lives in is called nest
The place cockroach lives in is called
2024-07-24 01:46:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 01:50:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.4141, -1.1895,  0.3074,  ...,  0.5547, -0.6689, -0.4639],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6777, -3.8320,  0.7090,  ..., -3.5898,  0.0762,  2.3516],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0104,  0.0020, -0.0052,  ...,  0.0067,  0.0035,  0.0072],
        [-0.0060,  0.0050,  0.0068,  ...,  0.0044,  0.0039, -0.0051],
        [-0.0024,  0.0085, -0.0064,  ..., -0.0051,  0.0047,  0.0120],
        ...,
        [-0.0034, -0.0070, -0.0032,  ...,  0.0076, -0.0073,  0.0072],
        [-0.0087,  0.0046, -0.0020,  ..., -0.0085,  0.0024,  0.0088],
        [-0.0034, -0.0204, -0.0039,  ...,  0.0025,  0.0072,  0.0053]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.0820, -4.1328,  0.1738,  ..., -3.6270,  1.1543,  2.9902]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:50:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place hamster lives in is called nest
The place snake lives in is called nest
The place crow lives in is called nest
The place dog lives in is called doghouse
The place fly lives in is called nest
The place raven lives in is called nest
The place wasp lives in is called nest
The place cockroach lives in is called
2024-07-24 01:50:46 root INFO     [order_1_approx] starting weight calculation for The place fly lives in is called nest
The place wasp lives in is called nest
The place hamster lives in is called nest
The place dog lives in is called doghouse
The place snake lives in is called nest
The place cockroach lives in is called nest
The place raven lives in is called nest
The place crow lives in is called
2024-07-24 01:50:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 01:54:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2236, -1.0898, -0.2649,  ...,  1.0430, -1.0908,  0.3320],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.2051, -0.2559,  0.6602,  ..., -1.0664,  2.3945,  0.9590],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0173,  0.0028,  0.0012,  ...,  0.0184,  0.0098,  0.0062],
        [-0.0150, -0.0003,  0.0090,  ...,  0.0041,  0.0035, -0.0033],
        [-0.0118,  0.0266,  0.0084,  ...,  0.0069,  0.0096,  0.0065],
        ...,
        [-0.0224, -0.0018,  0.0048,  ...,  0.0015,  0.0042,  0.0158],
        [-0.0329, -0.0032,  0.0021,  ..., -0.0015,  0.0031,  0.0132],
        [-0.0195, -0.0154, -0.0118,  ..., -0.0130,  0.0023, -0.0079]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9805, -0.3491,  0.5791,  ..., -0.5728,  2.3633,  0.7788]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:54:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place fly lives in is called nest
The place wasp lives in is called nest
The place hamster lives in is called nest
The place dog lives in is called doghouse
The place snake lives in is called nest
The place cockroach lives in is called nest
The place raven lives in is called nest
The place crow lives in is called
2024-07-24 01:54:37 root INFO     [order_1_approx] starting weight calculation for The place crow lives in is called nest
The place hamster lives in is called nest
The place raven lives in is called nest
The place cockroach lives in is called nest
The place fly lives in is called nest
The place wasp lives in is called nest
The place snake lives in is called nest
The place dog lives in is called
2024-07-24 01:54:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 01:58:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6099, -0.4487,  0.2632,  ...,  0.6172, -1.2773,  0.4043],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4902, -6.5820,  0.4873,  ..., -6.7539,  3.8125,  5.0977],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.0536e-02, -9.2010e-03,  7.0534e-03,  ...,  5.3482e-03,
          9.0790e-04,  4.2725e-04],
        [ 9.4986e-03,  4.1962e-03,  8.8501e-04,  ..., -8.4925e-04,
          1.3710e-02, -1.1833e-02],
        [-2.3403e-03,  1.8646e-02, -1.3504e-02,  ..., -7.8049e-03,
          2.1057e-02, -9.5444e-03],
        ...,
        [-8.4305e-03, -5.9814e-03,  3.2043e-04,  ...,  1.8668e-04,
          8.1024e-03,  9.3689e-03],
        [-1.3672e-02,  3.0251e-03,  1.1826e-02,  ..., -1.6079e-03,
         -1.1955e-02, -8.5831e-06],
        [-8.7280e-03, -1.2268e-02,  3.9749e-03,  ...,  2.1973e-02,
         -4.7493e-03,  1.2779e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3708, -5.2031,  0.8896,  ..., -6.4688,  3.5508,  4.9102]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:58:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place crow lives in is called nest
The place hamster lives in is called nest
The place raven lives in is called nest
The place cockroach lives in is called nest
The place fly lives in is called nest
The place wasp lives in is called nest
The place snake lives in is called nest
The place dog lives in is called
2024-07-24 01:58:20 root INFO     [order_1_approx] starting weight calculation for The place cockroach lives in is called nest
The place wasp lives in is called nest
The place snake lives in is called nest
The place raven lives in is called nest
The place crow lives in is called nest
The place hamster lives in is called nest
The place dog lives in is called doghouse
The place fly lives in is called
2024-07-24 01:58:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 02:02:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2534,  0.7554,  0.6870,  ...,  0.7158, -1.4844,  1.1221],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3770, -2.4961,  1.2832,  ..., -2.3145,  0.4280,  1.6035],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 6.6833e-03,  3.9482e-03,  4.8447e-03,  ...,  1.8646e-02,
         -1.5125e-03,  7.3814e-03],
        [-1.3113e-03, -1.0551e-02,  1.3489e-02,  ...,  1.5289e-02,
         -9.2850e-03,  1.4427e-02],
        [-8.5526e-03,  7.1449e-03, -2.4395e-03,  ..., -6.2294e-03,
          3.9673e-03,  3.4561e-03],
        ...,
        [ 3.1204e-03, -1.0193e-02, -7.5150e-03,  ...,  4.8141e-03,
         -5.6038e-03,  1.7807e-02],
        [-3.3360e-03, -2.5177e-03, -2.9564e-05,  ..., -6.1836e-03,
          1.8702e-03,  6.4621e-03],
        [-1.4885e-02, -2.3712e-02,  8.7738e-03,  ..., -1.0574e-02,
         -1.6918e-03,  4.1237e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0635, -2.9766,  0.6060,  ..., -2.1328,  0.7363,  1.9961]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:02:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place cockroach lives in is called nest
The place wasp lives in is called nest
The place snake lives in is called nest
The place raven lives in is called nest
The place crow lives in is called nest
The place hamster lives in is called nest
The place dog lives in is called doghouse
The place fly lives in is called
2024-07-24 02:02:09 root INFO     [order_1_approx] starting weight calculation for The place crow lives in is called nest
The place snake lives in is called nest
The place fly lives in is called nest
The place cockroach lives in is called nest
The place dog lives in is called doghouse
The place raven lives in is called nest
The place wasp lives in is called nest
The place hamster lives in is called
2024-07-24 02:02:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 02:05:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1843, -0.3391, -0.0573,  ...,  0.2856, -0.6177,  1.4355],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.0801, -2.0645, -0.2373,  ..., -7.3359,  0.4580,  5.0625],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0157, -0.0115, -0.0030,  ...,  0.0080, -0.0100, -0.0074],
        [-0.0065, -0.0008,  0.0092,  ...,  0.0090,  0.0077, -0.0068],
        [ 0.0031,  0.0066,  0.0045,  ..., -0.0006,  0.0125,  0.0036],
        ...,
        [-0.0017, -0.0014, -0.0107,  ...,  0.0035, -0.0060,  0.0143],
        [ 0.0017,  0.0007,  0.0079,  ..., -0.0033,  0.0012,  0.0014],
        [-0.0073, -0.0058,  0.0053,  ...,  0.0007,  0.0064,  0.0129]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.7852, -2.0703, -0.4209,  ..., -7.5547,  0.1206,  5.6172]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:05:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place crow lives in is called nest
The place snake lives in is called nest
The place fly lives in is called nest
The place cockroach lives in is called nest
The place dog lives in is called doghouse
The place raven lives in is called nest
The place wasp lives in is called nest
The place hamster lives in is called
2024-07-24 02:05:53 root INFO     [order_1_approx] starting weight calculation for The place dog lives in is called doghouse
The place fly lives in is called nest
The place crow lives in is called nest
The place snake lives in is called nest
The place hamster lives in is called nest
The place wasp lives in is called nest
The place cockroach lives in is called nest
The place raven lives in is called
2024-07-24 02:05:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 02:09:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6104, -0.9409, -0.2485,  ...,  0.6934, -0.5981, -0.2983],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-5.1328, -1.2344,  1.3477,  ..., -2.1445,  2.2812,  1.3809],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.0710e-02, -1.0925e-02, -2.2202e-03,  ...,  1.8509e-02,
          9.1553e-03, -2.6703e-03],
        [ 4.0054e-03,  3.2623e-02,  5.4779e-03,  ...,  3.2654e-03,
          1.0223e-02,  6.6605e-03],
        [-2.0103e-03,  6.3210e-03,  5.7259e-03,  ...,  1.0590e-02,
          1.0406e-02, -1.8219e-02],
        ...,
        [-8.2397e-03,  1.3924e-02,  1.3687e-02,  ...,  1.9379e-02,
          2.2415e-02,  3.3936e-02],
        [-2.0538e-02,  5.9128e-05, -6.9580e-03,  ...,  5.3692e-04,
          7.9269e-03,  2.2568e-02],
        [-2.2995e-02, -1.8585e-02, -3.7384e-03,  ..., -5.0964e-03,
         -2.2507e-03,  9.9335e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.3203, -1.3262,  0.7559,  ..., -2.8066,  2.8184,  1.5391]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:09:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place dog lives in is called doghouse
The place fly lives in is called nest
The place crow lives in is called nest
The place snake lives in is called nest
The place hamster lives in is called nest
The place wasp lives in is called nest
The place cockroach lives in is called nest
The place raven lives in is called
2024-07-24 02:09:30 root INFO     [order_1_approx] starting weight calculation for The place hamster lives in is called nest
The place cockroach lives in is called nest
The place crow lives in is called nest
The place raven lives in is called nest
The place fly lives in is called nest
The place dog lives in is called doghouse
The place wasp lives in is called nest
The place snake lives in is called
2024-07-24 02:09:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 02:13:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2820,  0.6333, -0.0215,  ...,  0.7373, -0.4536,  0.3101],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.2383, -3.9570,  0.6768,  ..., -3.6953,  3.0234,  3.0410],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0279,  0.0201, -0.0109,  ...,  0.0246,  0.0228, -0.0019],
        [ 0.0098,  0.0170,  0.0065,  ...,  0.0192, -0.0008, -0.0161],
        [-0.0100, -0.0022, -0.0063,  ...,  0.0055,  0.0141,  0.0115],
        ...,
        [-0.0047,  0.0016,  0.0017,  ...,  0.0140, -0.0050,  0.0188],
        [-0.0093, -0.0122, -0.0027,  ..., -0.0050,  0.0047,  0.0125],
        [-0.0101, -0.0093,  0.0035,  ...,  0.0009, -0.0041,  0.0044]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.3789, -4.6602,  0.5161,  ..., -3.6699,  2.9199,  2.9199]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:13:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place hamster lives in is called nest
The place cockroach lives in is called nest
The place crow lives in is called nest
The place raven lives in is called nest
The place fly lives in is called nest
The place dog lives in is called doghouse
The place wasp lives in is called nest
The place snake lives in is called
2024-07-24 02:13:21 root INFO     [order_1_approx] starting weight calculation for The place fly lives in is called nest
The place cockroach lives in is called nest
The place crow lives in is called nest
The place hamster lives in is called nest
The place raven lives in is called nest
The place dog lives in is called doghouse
The place snake lives in is called nest
The place wasp lives in is called
2024-07-24 02:13:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 02:17:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2097,  0.1790, -1.1973,  ...,  0.8633, -0.1045,  0.7188],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.8721, -1.7861,  0.8906,  ..., -2.8047,  5.4180,  3.2422],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.9989e-02, -3.8300e-03, -1.3634e-02,  ...,  9.6130e-03,
          1.2960e-03,  6.8378e-04],
        [-9.3002e-03,  1.0902e-02,  1.1314e-02,  ...,  1.4076e-03,
         -1.5083e-02, -1.6357e-02],
        [-4.0779e-03,  4.1237e-03, -1.4664e-02,  ..., -9.0332e-03,
          1.5190e-02,  6.6681e-03],
        ...,
        [ 8.3008e-03, -8.7204e-03, -8.1177e-03,  ...,  7.7972e-03,
         -9.6893e-03,  5.8479e-03],
        [-2.0447e-02, -2.2888e-05,  9.7580e-03,  ..., -6.1874e-03,
         -1.6470e-03, -4.6043e-03],
        [-2.5864e-02, -6.3744e-03,  5.4741e-03,  ..., -3.1376e-03,
          4.8447e-03, -3.7432e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.8506, -1.6875,  0.6182,  ..., -2.6543,  5.5508,  2.7285]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:17:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place fly lives in is called nest
The place cockroach lives in is called nest
The place crow lives in is called nest
The place hamster lives in is called nest
The place raven lives in is called nest
The place dog lives in is called doghouse
The place snake lives in is called nest
The place wasp lives in is called
2024-07-24 02:17:07 root INFO     total operator prediction time: 1812.7665166854858 seconds
2024-07-24 02:17:07 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on country - language
2024-07-24 02:17:07 root INFO     building operator country - language
2024-07-24 02:17:08 root INFO     [order_1_approx] starting weight calculation for The country of denmark primarily speaks the language of danish
The country of colombia primarily speaks the language of spanish
The country of israel primarily speaks the language of hebrew
The country of ecuador primarily speaks the language of spanish
The country of guam primarily speaks the language of english
The country of usa primarily speaks the language of english
The country of guadeloupe primarily speaks the language of french
The country of bolivia primarily speaks the language of
2024-07-24 02:17:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 02:20:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6699, -0.8311, -0.7051,  ..., -0.2021,  1.5137,  0.0560],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.1406, -2.4551, -6.2812,  ..., -0.5918,  0.3203, -3.4102],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.2505e-02, -1.1349e-03,  4.2801e-03,  ..., -1.3100e-02,
         -9.5520e-03, -7.7438e-03],
        [-1.3245e-02,  6.2943e-05,  3.5439e-03,  ...,  1.9516e-02,
          1.4893e-02,  1.3399e-03],
        [ 2.0294e-02,  2.9282e-02, -1.8906e-02,  ..., -3.3325e-02,
         -1.7014e-02, -1.3908e-02],
        ...,
        [-1.1108e-02, -1.3542e-02, -3.2349e-03,  ...,  1.5564e-02,
          7.0381e-03,  5.5618e-03],
        [ 4.5395e-03,  1.9255e-03,  3.3169e-03,  ..., -5.5542e-03,
         -1.5129e-02, -2.4014e-03],
        [ 5.6953e-03,  7.5417e-03, -2.1229e-03,  ..., -1.0391e-02,
          9.2468e-03,  3.5095e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3184, -1.0166, -7.6016,  ..., -0.1367, -0.1160, -3.7070]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:20:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of denmark primarily speaks the language of danish
The country of colombia primarily speaks the language of spanish
The country of israel primarily speaks the language of hebrew
The country of ecuador primarily speaks the language of spanish
The country of guam primarily speaks the language of english
The country of usa primarily speaks the language of english
The country of guadeloupe primarily speaks the language of french
The country of bolivia primarily speaks the language of
2024-07-24 02:20:55 root INFO     [order_1_approx] starting weight calculation for The country of guam primarily speaks the language of english
The country of ecuador primarily speaks the language of spanish
The country of israel primarily speaks the language of hebrew
The country of bolivia primarily speaks the language of spanish
The country of usa primarily speaks the language of english
The country of guadeloupe primarily speaks the language of french
The country of denmark primarily speaks the language of danish
The country of colombia primarily speaks the language of
2024-07-24 02:20:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 02:24:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0566,  0.3589, -0.3506,  ...,  0.8330,  0.6040,  0.0012],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3066, -3.4414, -3.5547,  ..., -5.0820, -1.4463, -1.5039],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0133, -0.0058, -0.0109,  ..., -0.0107, -0.0028, -0.0116],
        [-0.0074, -0.0150,  0.0055,  ...,  0.0048,  0.0089, -0.0059],
        [ 0.0070,  0.0102, -0.0216,  ..., -0.0094, -0.0041,  0.0012],
        ...,
        [ 0.0015,  0.0030, -0.0058,  ..., -0.0057, -0.0044, -0.0048],
        [ 0.0107,  0.0108, -0.0102,  ..., -0.0028, -0.0113, -0.0062],
        [ 0.0009, -0.0039, -0.0066,  ..., -0.0198,  0.0044, -0.0100]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2676, -3.9277, -3.6445,  ..., -5.6367, -2.0938, -2.0430]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:24:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of guam primarily speaks the language of english
The country of ecuador primarily speaks the language of spanish
The country of israel primarily speaks the language of hebrew
The country of bolivia primarily speaks the language of spanish
The country of usa primarily speaks the language of english
The country of guadeloupe primarily speaks the language of french
The country of denmark primarily speaks the language of danish
The country of colombia primarily speaks the language of
2024-07-24 02:24:42 root INFO     [order_1_approx] starting weight calculation for The country of guadeloupe primarily speaks the language of french
The country of ecuador primarily speaks the language of spanish
The country of colombia primarily speaks the language of spanish
The country of bolivia primarily speaks the language of spanish
The country of guam primarily speaks the language of english
The country of israel primarily speaks the language of hebrew
The country of usa primarily speaks the language of english
The country of denmark primarily speaks the language of
2024-07-24 02:24:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 02:28:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5991,  0.3516,  0.1256,  ..., -0.4954, -0.7783,  0.5078],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6338, -5.8984, -2.6270,  ..., -5.8789, -2.1230,  0.9014],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0005,  0.0013, -0.0034,  ..., -0.0071, -0.0147, -0.0037],
        [ 0.0128, -0.0013, -0.0033,  ..., -0.0003,  0.0120, -0.0116],
        [ 0.0045,  0.0043, -0.0153,  ...,  0.0066,  0.0053,  0.0056],
        ...,
        [ 0.0156, -0.0053,  0.0011,  ...,  0.0015, -0.0117,  0.0072],
        [-0.0049,  0.0024, -0.0039,  ...,  0.0051, -0.0221,  0.0015],
        [ 0.0029, -0.0057, -0.0097,  ..., -0.0060, -0.0038, -0.0038]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4170, -5.6992, -2.7539,  ..., -6.1406, -2.1699,  1.2227]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:28:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of guadeloupe primarily speaks the language of french
The country of ecuador primarily speaks the language of spanish
The country of colombia primarily speaks the language of spanish
The country of bolivia primarily speaks the language of spanish
The country of guam primarily speaks the language of english
The country of israel primarily speaks the language of hebrew
The country of usa primarily speaks the language of english
The country of denmark primarily speaks the language of
2024-07-24 02:28:24 root INFO     [order_1_approx] starting weight calculation for The country of usa primarily speaks the language of english
The country of guadeloupe primarily speaks the language of french
The country of colombia primarily speaks the language of spanish
The country of israel primarily speaks the language of hebrew
The country of bolivia primarily speaks the language of spanish
The country of denmark primarily speaks the language of danish
The country of guam primarily speaks the language of english
The country of ecuador primarily speaks the language of
2024-07-24 02:28:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 02:31:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3303, -0.6421, -1.3096,  ...,  0.0150,  0.3579,  0.5352],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8984, -2.8496, -3.7422,  ..., -0.7139,  1.6650, -2.8320],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 6.2637e-03, -1.2413e-02,  1.4877e-03,  ..., -6.4354e-03,
         -2.3441e-03, -3.7575e-04],
        [-7.1716e-04, -3.5934e-03, -2.1629e-03,  ..., -7.6370e-03,
          1.0071e-02, -3.4714e-04],
        [ 5.4932e-03,  9.6130e-03, -2.6718e-02,  ..., -7.1716e-03,
          1.3847e-03, -6.6109e-03],
        ...,
        [ 5.0125e-03, -9.5062e-03, -5.8441e-03,  ...,  6.4850e-05,
          5.6839e-03, -6.1893e-04],
        [ 3.8853e-03, -4.6692e-03,  6.9771e-03,  ..., -3.0708e-04,
         -8.0109e-03,  1.1742e-02],
        [ 8.0566e-03,  1.2184e-02, -7.6523e-03,  ..., -2.5063e-03,
          9.6588e-03, -9.1705e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0332, -2.2793, -4.3984,  ..., -1.2441,  1.3867, -3.1543]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:31:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of usa primarily speaks the language of english
The country of guadeloupe primarily speaks the language of french
The country of colombia primarily speaks the language of spanish
The country of israel primarily speaks the language of hebrew
The country of bolivia primarily speaks the language of spanish
The country of denmark primarily speaks the language of danish
The country of guam primarily speaks the language of english
The country of ecuador primarily speaks the language of
2024-07-24 02:31:59 root INFO     [order_1_approx] starting weight calculation for The country of colombia primarily speaks the language of spanish
The country of ecuador primarily speaks the language of spanish
The country of denmark primarily speaks the language of danish
The country of bolivia primarily speaks the language of spanish
The country of usa primarily speaks the language of english
The country of israel primarily speaks the language of hebrew
The country of guam primarily speaks the language of english
The country of guadeloupe primarily speaks the language of
2024-07-24 02:31:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 02:35:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0635, -1.0068, -1.4639,  ...,  0.1538,  0.1704,  0.1377],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.2031, -4.1289, -9.1875,  ..., -4.5000, -0.7427,  0.0950],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0096, -0.0102,  0.0153,  ..., -0.0172,  0.0176, -0.0042],
        [-0.0060,  0.0066, -0.0065,  ..., -0.0096,  0.0197, -0.0082],
        [ 0.0032,  0.0186, -0.0317,  ...,  0.0054, -0.0329,  0.0019],
        ...,
        [-0.0056, -0.0117,  0.0011,  ...,  0.0032, -0.0086,  0.0093],
        [-0.0192, -0.0042,  0.0001,  ...,  0.0144, -0.0188,  0.0027],
        [-0.0009, -0.0006,  0.0040,  ...,  0.0063,  0.0011, -0.0028]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.7871, -4.4453, -9.4219,  ..., -5.1562, -0.9341,  0.1716]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:35:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of colombia primarily speaks the language of spanish
The country of ecuador primarily speaks the language of spanish
The country of denmark primarily speaks the language of danish
The country of bolivia primarily speaks the language of spanish
The country of usa primarily speaks the language of english
The country of israel primarily speaks the language of hebrew
The country of guam primarily speaks the language of english
The country of guadeloupe primarily speaks the language of
2024-07-24 02:35:46 root INFO     [order_1_approx] starting weight calculation for The country of colombia primarily speaks the language of spanish
The country of denmark primarily speaks the language of danish
The country of israel primarily speaks the language of hebrew
The country of ecuador primarily speaks the language of spanish
The country of usa primarily speaks the language of english
The country of bolivia primarily speaks the language of spanish
The country of guadeloupe primarily speaks the language of french
The country of guam primarily speaks the language of
2024-07-24 02:35:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 02:39:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1931, -1.2148, -0.5381,  ..., -0.3809,  0.3232, -0.2773],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8047, -3.7109, -5.4414,  ...,  0.3022,  4.0586, -0.3701],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.7136e-02, -1.8234e-03,  7.9193e-03,  ..., -4.8790e-03,
          8.7585e-03, -5.4703e-03],
        [-8.5907e-03,  8.7357e-04,  2.4414e-03,  ...,  1.1559e-03,
          1.8463e-02,  1.1673e-02],
        [ 2.7344e-02,  4.0161e-02, -1.8860e-02,  ...,  6.9580e-03,
         -2.5970e-02, -7.3242e-03],
        ...,
        [-1.1272e-03, -1.6754e-02,  6.8398e-03,  ...,  2.9755e-03,
         -2.5196e-03,  8.7023e-06],
        [-1.4984e-02, -8.6975e-03, -4.7302e-03,  ...,  6.9351e-03,
          1.0048e-02,  2.1992e-03],
        [ 1.2398e-02,  5.2376e-03,  6.4240e-03,  ...,  1.2772e-02,
         -2.6703e-04, -1.1475e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3320, -3.6016, -5.9023,  ..., -0.2935,  4.7695,  0.0479]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:39:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of colombia primarily speaks the language of spanish
The country of denmark primarily speaks the language of danish
The country of israel primarily speaks the language of hebrew
The country of ecuador primarily speaks the language of spanish
The country of usa primarily speaks the language of english
The country of bolivia primarily speaks the language of spanish
The country of guadeloupe primarily speaks the language of french
The country of guam primarily speaks the language of
2024-07-24 02:39:35 root INFO     [order_1_approx] starting weight calculation for The country of colombia primarily speaks the language of spanish
The country of denmark primarily speaks the language of danish
The country of guam primarily speaks the language of english
The country of guadeloupe primarily speaks the language of french
The country of usa primarily speaks the language of english
The country of bolivia primarily speaks the language of spanish
The country of ecuador primarily speaks the language of spanish
The country of israel primarily speaks the language of
2024-07-24 02:39:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 02:43:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3083, -0.1704, -0.8604,  ...,  0.5044,  0.3865,  0.0848],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.1406, -4.6953, -5.3984,  ..., -3.9727, -0.3665, -1.6396],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0015,  0.0037, -0.0003,  ..., -0.0056, -0.0004, -0.0062],
        [ 0.0039,  0.0090, -0.0040,  ...,  0.0072,  0.0105, -0.0026],
        [ 0.0019, -0.0102,  0.0005,  ...,  0.0025, -0.0143,  0.0110],
        ...,
        [ 0.0023, -0.0182,  0.0025,  ...,  0.0127,  0.0022,  0.0034],
        [-0.0063, -0.0022, -0.0017,  ...,  0.0056, -0.0062,  0.0003],
        [ 0.0026, -0.0088, -0.0005,  ...,  0.0021,  0.0060,  0.0038]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.9824, -4.6133, -5.5820,  ..., -4.8203, -0.6553, -1.9326]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:43:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of colombia primarily speaks the language of spanish
The country of denmark primarily speaks the language of danish
The country of guam primarily speaks the language of english
The country of guadeloupe primarily speaks the language of french
The country of usa primarily speaks the language of english
The country of bolivia primarily speaks the language of spanish
The country of ecuador primarily speaks the language of spanish
The country of israel primarily speaks the language of
2024-07-24 02:43:19 root INFO     [order_1_approx] starting weight calculation for The country of guadeloupe primarily speaks the language of french
The country of ecuador primarily speaks the language of spanish
The country of bolivia primarily speaks the language of spanish
The country of israel primarily speaks the language of hebrew
The country of colombia primarily speaks the language of spanish
The country of guam primarily speaks the language of english
The country of denmark primarily speaks the language of danish
The country of usa primarily speaks the language of
2024-07-24 02:43:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 02:47:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3635,  0.8633,  0.4346,  ...,  0.2859,  0.0068, -0.1287],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2246, -2.7168, -1.5791,  ..., -2.2891,  3.4648, -0.7305],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0055, -0.0091, -0.0058,  ..., -0.0082, -0.0018, -0.0071],
        [ 0.0062,  0.0100, -0.0060,  ...,  0.0110,  0.0002,  0.0009],
        [-0.0064, -0.0091, -0.0049,  ..., -0.0037,  0.0062, -0.0030],
        ...,
        [ 0.0021, -0.0098, -0.0065,  ...,  0.0051,  0.0020, -0.0041],
        [ 0.0024, -0.0050, -0.0073,  ...,  0.0152, -0.0010,  0.0075],
        [ 0.0022, -0.0017, -0.0106,  ...,  0.0056,  0.0075,  0.0029]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3057, -2.1016, -1.5078,  ..., -2.8711,  3.0645, -1.0771]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:47:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of guadeloupe primarily speaks the language of french
The country of ecuador primarily speaks the language of spanish
The country of bolivia primarily speaks the language of spanish
The country of israel primarily speaks the language of hebrew
The country of colombia primarily speaks the language of spanish
The country of guam primarily speaks the language of english
The country of denmark primarily speaks the language of danish
The country of usa primarily speaks the language of
2024-07-24 02:47:07 root INFO     total operator prediction time: 1799.9742805957794 seconds
2024-07-24 02:47:07 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on name - nationality
2024-07-24 02:47:07 root INFO     building operator name - nationality
2024-07-24 02:47:08 root INFO     [order_1_approx] starting weight calculation for caesar was roman
newton was english
wagner was german
strauss was austrian
hitler was german
edison was american
copernicus was polish
beethoven was
2024-07-24 02:47:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 02:50:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1631,  1.2256,  0.3164,  ...,  1.1592,  0.3171,  0.0951],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.1484, -2.7695, -6.6406,  ..., -0.2817, -4.8477, -5.7852],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0258, -0.0283,  0.0021,  ...,  0.0071,  0.0465, -0.0114],
        [ 0.0200,  0.0174, -0.0116,  ..., -0.0028, -0.0149, -0.0028],
        [ 0.0056, -0.0028, -0.0056,  ..., -0.0063, -0.0169,  0.0011],
        ...,
        [-0.0155,  0.0044,  0.0121,  ...,  0.0043, -0.0013, -0.0033],
        [-0.0039, -0.0066, -0.0265,  ...,  0.0006, -0.0209,  0.0143],
        [-0.0002,  0.0111,  0.0089,  ..., -0.0187,  0.0065, -0.0203]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.3828, -2.6387, -7.2031,  ...,  0.3257, -5.6562, -6.0703]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:50:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for caesar was roman
newton was english
wagner was german
strauss was austrian
hitler was german
edison was american
copernicus was polish
beethoven was
2024-07-24 02:50:57 root INFO     [order_1_approx] starting weight calculation for strauss was austrian
wagner was german
copernicus was polish
beethoven was german
hitler was german
edison was american
newton was english
caesar was
2024-07-24 02:50:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 02:54:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8003, -0.4077, -0.6025,  ...,  0.3101, -0.0688,  0.4897],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.4395,  0.4319, -7.0664,  ..., -0.5229, -5.4258, -5.2539],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0215, -0.0194,  0.0002,  ..., -0.0177,  0.0125, -0.0128],
        [-0.0021,  0.0196,  0.0195,  ...,  0.0214, -0.0095, -0.0100],
        [ 0.0003, -0.0038, -0.0023,  ...,  0.0018, -0.0105,  0.0025],
        ...,
        [-0.0144, -0.0069,  0.0056,  ...,  0.0201,  0.0059, -0.0011],
        [-0.0083, -0.0062, -0.0176,  ..., -0.0099, -0.0045,  0.0131],
        [ 0.0039, -0.0077, -0.0079,  ..., -0.0020, -0.0070,  0.0095]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.4375,  0.4094, -7.6484,  ..., -0.1240, -5.5312, -5.8828]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:54:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for strauss was austrian
wagner was german
copernicus was polish
beethoven was german
hitler was german
edison was american
newton was english
caesar was
2024-07-24 02:54:47 root INFO     [order_1_approx] starting weight calculation for caesar was roman
hitler was german
beethoven was german
edison was american
strauss was austrian
newton was english
wagner was german
copernicus was
2024-07-24 02:54:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 02:58:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0474,  1.2188, -0.9404,  ..., -0.0515,  0.7832,  1.1406],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7974, -4.9297, -6.1250,  ..., -1.5078, -4.2578, -0.1729],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0062, -0.0207,  0.0208,  ..., -0.0146,  0.0129, -0.0063],
        [ 0.0033,  0.0151,  0.0079,  ...,  0.0006, -0.0119,  0.0008],
        [ 0.0045,  0.0092, -0.0022,  ..., -0.0126, -0.0084,  0.0074],
        ...,
        [-0.0267, -0.0042,  0.0008,  ...,  0.0253, -0.0185,  0.0004],
        [ 0.0003, -0.0119, -0.0105,  ..., -0.0035,  0.0067,  0.0228],
        [ 0.0002,  0.0208,  0.0034,  ..., -0.0112,  0.0050, -0.0029]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3799, -4.8633, -6.4766,  ..., -1.7373, -4.3359,  0.4238]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:58:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for caesar was roman
hitler was german
beethoven was german
edison was american
strauss was austrian
newton was english
wagner was german
copernicus was
2024-07-24 02:58:38 root INFO     [order_1_approx] starting weight calculation for newton was english
copernicus was polish
wagner was german
beethoven was german
hitler was german
caesar was roman
strauss was austrian
edison was
2024-07-24 02:58:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 03:02:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0630,  1.1729,  1.2715,  ...,  1.0898, -0.2107,  0.3640],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3945, -1.5449, -3.7461,  ..., -2.4434, -0.1475, -2.1719],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0178, -0.0179,  0.0002,  ..., -0.0033,  0.0029,  0.0051],
        [ 0.0082,  0.0177, -0.0084,  ..., -0.0005,  0.0037, -0.0057],
        [-0.0028,  0.0092, -0.0034,  ...,  0.0016,  0.0062, -0.0132],
        ...,
        [-0.0203,  0.0037,  0.0041,  ...,  0.0081, -0.0051,  0.0134],
        [ 0.0031, -0.0167, -0.0005,  ..., -0.0063, -0.0009,  0.0150],
        [-0.0087, -0.0139,  0.0049,  ..., -0.0174,  0.0162,  0.0016]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2002, -1.7119, -4.6758,  ..., -3.3438, -0.6875, -2.8887]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:02:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for newton was english
copernicus was polish
wagner was german
beethoven was german
hitler was german
caesar was roman
strauss was austrian
edison was
2024-07-24 03:02:26 root INFO     [order_1_approx] starting weight calculation for edison was american
wagner was german
caesar was roman
beethoven was german
strauss was austrian
copernicus was polish
newton was english
hitler was
2024-07-24 03:02:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 03:06:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6582,  1.0254, -0.9209,  ..., -0.2329, -0.4121, -0.5029],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2676, -1.1641, -8.6562,  ..., -3.1133, -7.0273, -4.8984],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 8.2016e-05, -1.8959e-03,  1.9531e-03,  ..., -1.3809e-02,
          1.3489e-02,  5.3024e-04],
        [-3.9840e-04,  1.1551e-02, -4.3488e-03,  ...,  1.2009e-02,
         -2.7695e-03,  1.3344e-02],
        [-1.8921e-03,  8.7433e-03, -9.8133e-04,  ..., -1.7288e-02,
         -2.1011e-02, -1.8616e-02],
        ...,
        [-3.0289e-03,  7.5607e-03,  2.0885e-03,  ...,  5.0583e-03,
         -7.9956e-03, -9.8801e-04],
        [ 2.9259e-03, -2.1458e-03, -3.3379e-03,  ..., -6.5613e-03,
         -5.0125e-03, -8.9340e-03],
        [-5.8632e-03,  7.5960e-04,  4.5891e-03,  ..., -1.6327e-03,
         -4.8676e-03, -1.7166e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9121, -1.0918, -9.7109,  ..., -3.6504, -6.9180, -5.5312]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:06:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for edison was american
wagner was german
caesar was roman
beethoven was german
strauss was austrian
copernicus was polish
newton was english
hitler was
2024-07-24 03:06:15 root INFO     [order_1_approx] starting weight calculation for hitler was german
edison was american
wagner was german
copernicus was polish
beethoven was german
strauss was austrian
caesar was roman
newton was
2024-07-24 03:06:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 03:10:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6978,  1.0654, -0.0503,  ...,  0.3501, -1.0400,  0.8003],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3496, -1.5605, -5.6250,  ...,  0.1367, -1.0713, -0.3892],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0168, -0.0265, -0.0041,  ..., -0.0092, -0.0133, -0.0213],
        [ 0.0128,  0.0140,  0.0045,  ...,  0.0036, -0.0037,  0.0041],
        [ 0.0100,  0.0067, -0.0080,  ..., -0.0065,  0.0090, -0.0084],
        ...,
        [-0.0210,  0.0030,  0.0055,  ...,  0.0148,  0.0060,  0.0072],
        [-0.0103,  0.0046, -0.0003,  ..., -0.0091,  0.0013,  0.0123],
        [-0.0145, -0.0005,  0.0152,  ..., -0.0189,  0.0033,  0.0099]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8945, -1.1514, -6.3711,  ..., -0.0768, -0.4814,  0.0066]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:10:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for hitler was german
edison was american
wagner was german
copernicus was polish
beethoven was german
strauss was austrian
caesar was roman
newton was
2024-07-24 03:10:01 root INFO     [order_1_approx] starting weight calculation for wagner was german
edison was american
newton was english
copernicus was polish
hitler was german
caesar was roman
beethoven was german
strauss was
2024-07-24 03:10:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 03:13:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1423,  0.7188,  0.3672,  ...,  1.5078,  0.6035, -0.1716],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0654, -2.1621, -6.8516,  ..., -6.4297, -4.0547, -2.5918],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 6.6757e-04, -6.6757e-03, -1.1124e-02,  ...,  6.6757e-03,
         -3.5934e-03, -2.3727e-03],
        [ 1.0483e-02,  6.8321e-03, -2.1782e-03,  ...,  1.9608e-03,
          1.2352e-02, -5.4169e-03],
        [-1.5392e-03, -9.5367e-05, -8.4610e-03,  ..., -6.8054e-03,
          1.1963e-02, -1.0414e-03],
        ...,
        [-5.2299e-03,  2.4395e-03, -2.4414e-04,  ..., -2.0456e-04,
         -1.3588e-02, -1.0658e-02],
        [-8.2245e-03, -1.0818e-02,  4.9629e-03,  ..., -5.8136e-03,
         -4.6654e-03, -8.0566e-03],
        [-3.3646e-03, -4.1122e-03, -1.7471e-03,  ..., -9.7427e-03,
         -2.0981e-03, -6.0043e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4661, -1.6270, -7.4609,  ..., -6.7539, -4.1953, -2.9570]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:13:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for wagner was german
edison was american
newton was english
copernicus was polish
hitler was german
caesar was roman
beethoven was german
strauss was
2024-07-24 03:13:47 root INFO     [order_1_approx] starting weight calculation for beethoven was german
newton was english
copernicus was polish
edison was american
strauss was austrian
caesar was roman
hitler was german
wagner was
2024-07-24 03:13:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 03:17:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0128,  0.7954,  0.7246,  ...,  0.0929, -0.1169, -0.2769],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0791, -2.6191, -7.8125,  ..., -1.7598, -1.6777, -3.5508],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0005, -0.0060, -0.0072,  ..., -0.0028,  0.0028, -0.0110],
        [ 0.0065,  0.0104, -0.0025,  ...,  0.0043, -0.0021, -0.0052],
        [ 0.0040, -0.0032, -0.0118,  ..., -0.0133, -0.0017,  0.0030],
        ...,
        [-0.0199,  0.0140, -0.0030,  ..., -0.0039, -0.0074, -0.0038],
        [ 0.0126, -0.0019,  0.0082,  ...,  0.0041,  0.0138,  0.0030],
        [-0.0096,  0.0022, -0.0061,  ..., -0.0229, -0.0032, -0.0069]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1819, -2.6426, -8.1562,  ..., -2.2324, -1.8887, -4.3906]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:17:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for beethoven was german
newton was english
copernicus was polish
edison was american
strauss was austrian
caesar was roman
hitler was german
wagner was
2024-07-24 03:17:36 root INFO     total operator prediction time: 1828.5707895755768 seconds
2024-07-24 03:17:36 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+ment_irreg
2024-07-24 03:17:36 root INFO     building operator verb+ment_irreg
2024-07-24 03:17:36 root INFO     [order_1_approx] starting weight calculation for To excite results in a excitement
To disagree results in a disagreement
To agree results in a agreement
To impair results in a impairment
To manage results in a management
To replace results in a replacement
To engage results in a engagement
To achieve results in a
2024-07-24 03:17:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 03:21:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4382,  0.4724,  1.4775,  ..., -0.1179, -0.5454, -0.6270],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.3926,  4.3750, -1.8555,  ..., -3.1074,  4.6055,  1.8809],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0116, -0.0169,  0.0044,  ...,  0.0039,  0.0024, -0.0146],
        [-0.0082, -0.0208,  0.0048,  ...,  0.0083, -0.0087, -0.0087],
        [-0.0089,  0.0029, -0.0144,  ...,  0.0106, -0.0006, -0.0082],
        ...,
        [ 0.0003,  0.0024, -0.0016,  ..., -0.0061,  0.0056,  0.0096],
        [-0.0011,  0.0052,  0.0048,  ..., -0.0090, -0.0128,  0.0037],
        [ 0.0135,  0.0069,  0.0063,  ...,  0.0144,  0.0216, -0.0032]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.9648,  4.7695, -2.2812,  ..., -3.6387,  4.6211,  1.7383]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:21:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To excite results in a excitement
To disagree results in a disagreement
To agree results in a agreement
To impair results in a impairment
To manage results in a management
To replace results in a replacement
To engage results in a engagement
To achieve results in a
2024-07-24 03:21:21 root INFO     [order_1_approx] starting weight calculation for To replace results in a replacement
To disagree results in a disagreement
To achieve results in a achievement
To impair results in a impairment
To excite results in a excitement
To manage results in a management
To engage results in a engagement
To agree results in a
2024-07-24 03:21:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 03:25:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1792,  0.3901,  1.0420,  ..., -0.3696,  0.0881,  0.4280],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4207, -0.3123,  0.3774,  ..., -0.5713, -0.6973,  1.9160],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0021, -0.0119, -0.0014,  ..., -0.0115, -0.0041, -0.0115],
        [-0.0166, -0.0127, -0.0024,  ...,  0.0020, -0.0168, -0.0020],
        [-0.0046, -0.0058, -0.0007,  ..., -0.0062,  0.0077, -0.0322],
        ...,
        [-0.0142, -0.0221, -0.0064,  ...,  0.0021,  0.0040,  0.0288],
        [-0.0065,  0.0153, -0.0142,  ..., -0.0011,  0.0041,  0.0036],
        [ 0.0161,  0.0131, -0.0037,  ..., -0.0269,  0.0177, -0.0175]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2078, -0.6890, -0.3076,  ..., -0.7998, -0.9995,  1.7012]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:25:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To replace results in a replacement
To disagree results in a disagreement
To achieve results in a achievement
To impair results in a impairment
To excite results in a excitement
To manage results in a management
To engage results in a engagement
To agree results in a
2024-07-24 03:25:07 root INFO     [order_1_approx] starting weight calculation for To impair results in a impairment
To achieve results in a achievement
To replace results in a replacement
To engage results in a engagement
To manage results in a management
To agree results in a agreement
To excite results in a excitement
To disagree results in a
2024-07-24 03:25:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 03:28:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5127, -1.0684,  2.1582,  ...,  0.0498,  0.3511,  0.4802],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1545, -3.1426, -1.4004,  ...,  0.2412,  3.6191,  1.6689],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-4.8752e-03,  4.1389e-03,  5.6725e-03,  ..., -2.6894e-03,
         -8.7166e-04, -1.1024e-02],
        [-4.6883e-03, -1.5915e-02, -5.9814e-03,  ..., -1.7776e-03,
          2.2316e-03, -1.5381e-02],
        [-1.4520e-04, -4.4022e-03, -1.4732e-02,  ...,  1.0254e-02,
          1.3161e-03, -2.3102e-02],
        ...,
        [-1.2955e-02, -5.4626e-03, -5.5618e-03,  ..., -9.3536e-03,
          1.2955e-02, -7.3395e-03],
        [-1.4534e-02,  1.7685e-02, -2.1381e-03,  ..., -9.7275e-05,
         -1.3680e-02,  1.5678e-03],
        [-9.1095e-03,  2.6489e-02,  6.6147e-03,  ..., -1.2711e-02,
         -3.8986e-03, -1.1154e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3762, -2.1172, -1.4053,  ...,  0.1199,  2.9648,  1.2090]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:28:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To impair results in a impairment
To achieve results in a achievement
To replace results in a replacement
To engage results in a engagement
To manage results in a management
To agree results in a agreement
To excite results in a excitement
To disagree results in a
2024-07-24 03:28:56 root INFO     [order_1_approx] starting weight calculation for To replace results in a replacement
To impair results in a impairment
To disagree results in a disagreement
To manage results in a management
To achieve results in a achievement
To excite results in a excitement
To agree results in a agreement
To engage results in a
2024-07-24 03:28:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 03:32:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3242, -0.0334,  0.0790,  ...,  0.8496, -1.1182,  0.7329],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0513,  0.4380, -0.3975,  ..., -0.3311,  0.3228,  4.0742],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.9073e-03, -2.8305e-03,  1.1988e-03,  ...,  1.5732e-02,
          1.1383e-02, -7.4615e-03],
        [-1.4381e-03, -1.0460e-02,  6.9466e-03,  ...,  1.2894e-03,
         -5.0659e-03, -6.7673e-03],
        [-1.5717e-03, -2.7428e-03, -1.4038e-03,  ..., -2.0981e-03,
         -1.4687e-04, -1.0719e-02],
        ...,
        [-1.3405e-02,  3.0518e-05,  3.9139e-03,  ..., -2.1763e-03,
          4.1962e-04, -8.9493e-03],
        [-1.5335e-03,  9.5673e-03, -3.9597e-03,  ..., -4.0531e-06,
         -1.3817e-02, -3.1204e-03],
        [-7.1106e-03,  2.2247e-02, -7.0419e-03,  ..., -4.5433e-03,
          1.6418e-02,  6.9504e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0807,  0.5845, -0.9873,  ..., -0.6475,  0.3611,  4.1719]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:32:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To replace results in a replacement
To impair results in a impairment
To disagree results in a disagreement
To manage results in a management
To achieve results in a achievement
To excite results in a excitement
To agree results in a agreement
To engage results in a
2024-07-24 03:32:43 root INFO     [order_1_approx] starting weight calculation for To achieve results in a achievement
To agree results in a agreement
To engage results in a engagement
To impair results in a impairment
To disagree results in a disagreement
To manage results in a management
To replace results in a replacement
To excite results in a
2024-07-24 03:32:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 03:36:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3105, -0.3608,  1.0732,  ..., -0.2971,  0.4353,  1.0830],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5649, -0.1971, -0.5522,  ..., -1.2461, -0.1411,  2.4121],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.6240e-03, -1.5152e-02, -1.0452e-02,  ..., -8.4877e-04,
         -1.1597e-03, -1.0201e-02],
        [-6.8016e-03, -5.8937e-03,  5.2643e-03,  ...,  6.6147e-03,
         -7.7896e-03,  1.1806e-03],
        [ 7.9651e-03,  3.6488e-03, -5.7755e-03,  ...,  4.6997e-03,
         -3.7651e-03, -2.6131e-04],
        ...,
        [-1.9180e-02,  4.0054e-05, -2.0218e-04,  ..., -8.8577e-03,
          8.0948e-03, -2.5349e-03],
        [ 5.2691e-04, -3.6812e-03,  3.4599e-03,  ..., -3.0231e-04,
         -9.4604e-04, -5.8365e-03],
        [-4.7226e-03,  1.1353e-02,  4.6196e-03,  ...,  1.3790e-03,
         -7.7438e-04, -1.0040e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1624,  0.4463, -0.9834,  ..., -1.3291,  0.0137,  2.3184]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:36:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To achieve results in a achievement
To agree results in a agreement
To engage results in a engagement
To impair results in a impairment
To disagree results in a disagreement
To manage results in a management
To replace results in a replacement
To excite results in a
2024-07-24 03:36:32 root INFO     [order_1_approx] starting weight calculation for To replace results in a replacement
To engage results in a engagement
To excite results in a excitement
To achieve results in a achievement
To manage results in a management
To agree results in a agreement
To disagree results in a disagreement
To impair results in a
2024-07-24 03:36:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 03:40:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4773, -0.0774, -0.1385,  ...,  0.7002, -0.5171, -0.5303],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0039, -1.4355, -3.1328,  ...,  1.1191, -0.1182,  2.7344],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0062,  0.0033,  0.0015,  ..., -0.0038,  0.0043, -0.0075],
        [ 0.0050, -0.0014, -0.0034,  ..., -0.0129, -0.0011,  0.0045],
        [ 0.0003, -0.0023, -0.0004,  ...,  0.0174, -0.0021, -0.0157],
        ...,
        [-0.0091, -0.0044, -0.0012,  ...,  0.0037,  0.0004,  0.0050],
        [ 0.0046, -0.0119,  0.0107,  ..., -0.0108, -0.0179, -0.0003],
        [-0.0016,  0.0166, -0.0115,  ..., -0.0125,  0.0111, -0.0127]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2397, -0.6187, -3.8203,  ...,  0.0244, -0.8120,  2.5977]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:40:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To replace results in a replacement
To engage results in a engagement
To excite results in a excitement
To achieve results in a achievement
To manage results in a management
To agree results in a agreement
To disagree results in a disagreement
To impair results in a
2024-07-24 03:40:19 root INFO     [order_1_approx] starting weight calculation for To impair results in a impairment
To replace results in a replacement
To excite results in a excitement
To disagree results in a disagreement
To agree results in a agreement
To engage results in a engagement
To achieve results in a achievement
To manage results in a
2024-07-24 03:40:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 03:44:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3623,  0.0544,  0.9434,  ..., -0.0195,  0.0168, -0.0725],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3003, -0.5850, -4.8711,  ...,  1.6162,  1.1846,  7.2578],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0010, -0.0016, -0.0007,  ..., -0.0044,  0.0027, -0.0184],
        [ 0.0090, -0.0124, -0.0025,  ..., -0.0012,  0.0100,  0.0014],
        [-0.0012, -0.0078,  0.0052,  ...,  0.0072,  0.0005, -0.0022],
        ...,
        [-0.0134, -0.0139, -0.0103,  ...,  0.0078,  0.0054, -0.0005],
        [-0.0112,  0.0014, -0.0039,  ..., -0.0114, -0.0062,  0.0098],
        [ 0.0039,  0.0031,  0.0046,  ...,  0.0048,  0.0088, -0.0073]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5430, -0.2795, -4.8633,  ...,  1.3076,  1.6152,  7.7852]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:44:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To impair results in a impairment
To replace results in a replacement
To excite results in a excitement
To disagree results in a disagreement
To agree results in a agreement
To engage results in a engagement
To achieve results in a achievement
To manage results in a
2024-07-24 03:44:09 root INFO     [order_1_approx] starting weight calculation for To engage results in a engagement
To manage results in a management
To disagree results in a disagreement
To impair results in a impairment
To excite results in a excitement
To achieve results in a achievement
To agree results in a agreement
To replace results in a
2024-07-24 03:44:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 03:47:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5645,  0.2455,  0.9600,  ...,  0.5625, -0.0347, -0.4702],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.5508,  0.4897, -2.5488,  ...,  2.4668,  1.2061,  7.0859],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0050, -0.0037,  0.0032,  ..., -0.0086, -0.0022, -0.0003],
        [-0.0057, -0.0135, -0.0079,  ...,  0.0003, -0.0041,  0.0025],
        [ 0.0049,  0.0061,  0.0012,  ...,  0.0049,  0.0078, -0.0032],
        ...,
        [-0.0280, -0.0213, -0.0165,  ..., -0.0031,  0.0080,  0.0035],
        [-0.0248, -0.0011,  0.0054,  ..., -0.0073, -0.0003,  0.0081],
        [-0.0038,  0.0162,  0.0084,  ..., -0.0021,  0.0023, -0.0172]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9214,  1.2686, -2.9453,  ...,  2.2441,  0.7861,  6.6055]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:47:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To engage results in a engagement
To manage results in a management
To disagree results in a disagreement
To impair results in a impairment
To excite results in a excitement
To achieve results in a achievement
To agree results in a agreement
To replace results in a
2024-07-24 03:47:56 root INFO     total operator prediction time: 1819.7619669437408 seconds
2024-07-24 03:47:56 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on noun+less_reg
2024-07-24 03:47:56 root INFO     building operator noun+less_reg
2024-07-24 03:47:56 root INFO     [order_1_approx] starting weight calculation for Something without god is godless
Something without guilt is guiltless
Something without gender is genderless
Something without defence is defenceless
Something without window is windowless
Something without penny is penniless
Something without error is errorless
Something without art is
2024-07-24 03:47:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 03:51:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4622,  0.1277, -0.2666,  ...,  0.4229,  0.0862,  1.3633],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2783,  2.5977, -4.4844,  ...,  1.2891, -0.2642,  0.9219],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.4496e-02, -1.7517e-02,  3.4218e-03,  ...,  2.1744e-04,
         -2.4399e-02, -2.3438e-02],
        [-9.8267e-03, -2.3956e-02,  9.1553e-05,  ..., -5.0011e-03,
         -6.9275e-03, -9.5901e-03],
        [-2.9373e-04,  7.4387e-05, -4.2999e-02,  ...,  1.5373e-03,
          2.9221e-03, -1.4046e-02],
        ...,
        [ 5.4169e-03,  1.6876e-02, -5.7907e-03,  ..., -2.9297e-02,
          1.4069e-02, -1.1597e-02],
        [-1.6525e-02,  2.8954e-03,  1.2718e-02,  ..., -3.3989e-03,
         -3.8391e-02,  7.2861e-03],
        [-1.6953e-02,  1.9562e-02, -1.0414e-02,  ..., -2.4376e-03,
          1.5175e-02, -3.4912e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4209,  2.5371, -4.4492,  ...,  1.0557, -0.4849,  0.6157]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:51:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without god is godless
Something without guilt is guiltless
Something without gender is genderless
Something without defence is defenceless
Something without window is windowless
Something without penny is penniless
Something without error is errorless
Something without art is
2024-07-24 03:51:33 root INFO     [order_1_approx] starting weight calculation for Something without gender is genderless
Something without window is windowless
Something without penny is penniless
Something without god is godless
Something without guilt is guiltless
Something without error is errorless
Something without art is artless
Something without defence is
2024-07-24 03:51:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 03:55:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2109,  0.0767, -0.2671,  ...,  0.5034, -0.0371,  1.8691],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.6855,  0.0317, -3.7148,  ..., -2.7070,  3.3906, -1.3428],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0049, -0.0089, -0.0100,  ...,  0.0162, -0.0098,  0.0001],
        [ 0.0022, -0.0061, -0.0124,  ..., -0.0055, -0.0004, -0.0006],
        [ 0.0019,  0.0126, -0.0021,  ...,  0.0287, -0.0011, -0.0009],
        ...,
        [ 0.0073, -0.0198, -0.0126,  ..., -0.0345, -0.0043, -0.0025],
        [-0.0160, -0.0154,  0.0156,  ..., -0.0081, -0.0423, -0.0017],
        [-0.0099,  0.0138,  0.0261,  ...,  0.0151, -0.0110, -0.0352]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.8223, -0.1697, -4.2188,  ..., -1.9141,  3.3477, -0.5093]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:55:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without gender is genderless
Something without window is windowless
Something without penny is penniless
Something without god is godless
Something without guilt is guiltless
Something without error is errorless
Something without art is artless
Something without defence is
2024-07-24 03:55:15 root INFO     [order_1_approx] starting weight calculation for Something without defence is defenceless
Something without art is artless
Something without gender is genderless
Something without god is godless
Something without penny is penniless
Something without window is windowless
Something without guilt is guiltless
Something without error is
2024-07-24 03:55:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 03:59:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5117, -1.2012,  0.0636,  ..., -0.3389, -0.1575,  0.1952],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.2344, -0.8267,  2.9863,  ..., -1.1934,  4.2383,  2.4883],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.8646e-02,  7.6141e-03,  1.3237e-02,  ..., -1.2421e-02,
         -4.8904e-03, -3.0518e-05],
        [ 4.4556e-03, -4.2328e-02, -1.4786e-02,  ..., -2.9678e-03,
         -1.0277e-02, -1.1276e-02],
        [ 1.4938e-02,  2.3544e-02, -2.2888e-02,  ...,  1.2321e-02,
         -6.4163e-03,  4.0283e-03],
        ...,
        [-7.3090e-03, -3.7117e-03, -4.7874e-03,  ..., -1.5701e-02,
          2.0721e-02,  1.5305e-02],
        [-5.2032e-03, -9.3079e-03,  1.2520e-02,  ...,  5.1384e-03,
         -3.2959e-02,  8.7280e-03],
        [-1.8860e-02,  3.3691e-02, -4.4174e-03,  ..., -1.2985e-02,
          3.7170e-02, -4.4769e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.9668, -0.5151,  2.5820,  ..., -0.4766,  4.0078,  1.8535]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:59:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without defence is defenceless
Something without art is artless
Something without gender is genderless
Something without god is godless
Something without penny is penniless
Something without window is windowless
Something without guilt is guiltless
Something without error is
2024-07-24 03:59:06 root INFO     [order_1_approx] starting weight calculation for Something without error is errorless
Something without god is godless
Something without art is artless
Something without penny is penniless
Something without defence is defenceless
Something without window is windowless
Something without guilt is guiltless
Something without gender is
2024-07-24 03:59:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 04:02:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6050, -0.0942, -0.3494,  ..., -1.3867,  1.3418,  1.2188],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.2207,  1.0684, -4.5547,  ..., -0.5503,  6.2734,  1.7617],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0084, -0.0281,  0.0114,  ..., -0.0071, -0.0260,  0.0101],
        [-0.0105, -0.0358, -0.0058,  ..., -0.0134,  0.0004, -0.0124],
        [ 0.0101, -0.0022, -0.0522,  ...,  0.0212,  0.0119,  0.0150],
        ...,
        [-0.0116, -0.0108,  0.0052,  ..., -0.0142,  0.0239,  0.0039],
        [-0.0059,  0.0031,  0.0275,  ...,  0.0089, -0.0434,  0.0008],
        [-0.0019,  0.0175, -0.0047,  ..., -0.0100,  0.0070, -0.0432]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1836,  1.6191, -4.0352,  ..., -0.7769,  5.9805,  1.7959]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 04:02:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without error is errorless
Something without god is godless
Something without art is artless
Something without penny is penniless
Something without defence is defenceless
Something without window is windowless
Something without guilt is guiltless
Something without gender is
2024-07-24 04:02:56 root INFO     [order_1_approx] starting weight calculation for Something without defence is defenceless
Something without error is errorless
Something without guilt is guiltless
Something without art is artless
Something without window is windowless
Something without gender is genderless
Something without penny is penniless
Something without god is
2024-07-24 04:02:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 04:06:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9268, -0.7607, -0.9243,  ..., -0.1952,  0.4333,  0.2476],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8530,  0.8726, -3.7285,  ..., -2.9258,  2.1777, -1.7480],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0292, -0.0056,  0.0027,  ..., -0.0034, -0.0306,  0.0025],
        [-0.0048, -0.0126, -0.0051,  ...,  0.0072, -0.0027, -0.0104],
        [ 0.0029,  0.0112, -0.0389,  ..., -0.0047, -0.0159, -0.0003],
        ...,
        [ 0.0041,  0.0034,  0.0079,  ..., -0.0122,  0.0037,  0.0069],
        [-0.0119, -0.0053,  0.0060,  ..., -0.0005, -0.0649,  0.0338],
        [ 0.0157,  0.0091, -0.0005,  ..., -0.0272,  0.0110, -0.0559]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7500,  0.4226, -3.7871,  ..., -3.0098,  2.2461, -1.3115]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 04:06:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without defence is defenceless
Something without error is errorless
Something without guilt is guiltless
Something without art is artless
Something without window is windowless
Something without gender is genderless
Something without penny is penniless
Something without god is
2024-07-24 04:06:39 root INFO     [order_1_approx] starting weight calculation for Something without window is windowless
Something without god is godless
Something without gender is genderless
Something without defence is defenceless
Something without art is artless
Something without error is errorless
Something without penny is penniless
Something without guilt is
2024-07-24 04:06:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 04:10:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4521, -0.7607, -0.4590,  ..., -0.2859,  0.7344,  0.6338],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.5371,  0.4614, -3.1133,  ...,  1.9141,  1.2559, -0.7471],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0317, -0.0179,  0.0122,  ...,  0.0063, -0.0403, -0.0116],
        [-0.0066, -0.0195, -0.0005,  ...,  0.0130, -0.0168,  0.0069],
        [-0.0017,  0.0136, -0.0184,  ...,  0.0231,  0.0187,  0.0055],
        ...,
        [-0.0068, -0.0247, -0.0087,  ..., -0.0408,  0.0287,  0.0130],
        [-0.0007, -0.0008,  0.0281,  ..., -0.0211, -0.0464,  0.0176],
        [-0.0201,  0.0371,  0.0111,  ..., -0.0196,  0.0103, -0.0498]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.0781,  0.7427, -2.7852,  ...,  1.7598, -0.2383, -1.5547]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 04:10:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without window is windowless
Something without god is godless
Something without gender is genderless
Something without defence is defenceless
Something without art is artless
Something without error is errorless
Something without penny is penniless
Something without guilt is
2024-07-24 04:10:26 root INFO     [order_1_approx] starting weight calculation for Something without guilt is guiltless
Something without art is artless
Something without gender is genderless
Something without window is windowless
Something without error is errorless
Something without defence is defenceless
Something without god is godless
Something without penny is
2024-07-24 04:10:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 04:14:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3179, -0.7773, -0.9229,  ..., -0.4082,  0.2418,  2.2461],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9805, -1.6660, -3.5391,  ...,  2.3887,  0.1807,  2.5527],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0063, -0.0175,  0.0079,  ...,  0.0133, -0.0095, -0.0031],
        [-0.0180, -0.0049,  0.0102,  ...,  0.0002,  0.0084, -0.0026],
        [-0.0014,  0.0162, -0.0417,  ..., -0.0089,  0.0021,  0.0189],
        ...,
        [-0.0092, -0.0126,  0.0182,  ..., -0.0146, -0.0016, -0.0125],
        [ 0.0158, -0.0069,  0.0219,  ...,  0.0032, -0.0211, -0.0091],
        [-0.0050,  0.0076,  0.0180,  ..., -0.0194,  0.0139, -0.0297]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0391, -1.0020, -3.1562,  ...,  2.1152, -1.4307,  1.6582]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 04:14:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without guilt is guiltless
Something without art is artless
Something without gender is genderless
Something without window is windowless
Something without error is errorless
Something without defence is defenceless
Something without god is godless
Something without penny is
2024-07-24 04:14:12 root INFO     [order_1_approx] starting weight calculation for Something without gender is genderless
Something without guilt is guiltless
Something without penny is penniless
Something without art is artless
Something without error is errorless
Something without god is godless
Something without defence is defenceless
Something without window is
2024-07-24 04:14:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 04:18:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3081,  0.0984, -0.9863,  ..., -0.1370,  1.1748,  0.7344],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5664,  2.1211, -0.9844,  ...,  0.3875, -0.9707,  4.2227],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0074, -0.0072,  0.0134,  ..., -0.0033, -0.0150,  0.0007],
        [-0.0033, -0.0157,  0.0018,  ..., -0.0034,  0.0272, -0.0180],
        [ 0.0057,  0.0110, -0.0285,  ..., -0.0123, -0.0003,  0.0174],
        ...,
        [-0.0048,  0.0012,  0.0169,  ...,  0.0051,  0.0211, -0.0087],
        [-0.0042, -0.0007,  0.0035,  ...,  0.0090, -0.0135, -0.0136],
        [-0.0062,  0.0143,  0.0044,  ..., -0.0006,  0.0131, -0.0388]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.8223,  1.5820, -0.7314,  ..., -0.1965, -1.0098,  4.1406]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 04:18:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without gender is genderless
Something without guilt is guiltless
Something without penny is penniless
Something without art is artless
Something without error is errorless
Something without god is godless
Something without defence is defenceless
Something without window is
2024-07-24 04:18:02 root INFO     total operator prediction time: 1806.4424443244934 seconds
2024-07-24 04:18:02 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on adj+ness_reg
2024-07-24 04:18:02 root INFO     building operator adj+ness_reg
2024-07-24 04:18:03 root INFO     [order_1_approx] starting weight calculation for The state of being reasonable is reasonableness
The state of being huge is hugeness
The state of being strange is strangeness
The state of being helpful is helpfulness
The state of being same is sameness
The state of being directed is directedness
The state of being impressive is impressiveness
The state of being competitive is
2024-07-24 04:18:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 04:21:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2979,  0.0786,  1.3086,  ..., -0.5225,  1.2861,  0.4253],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-5.2500, -0.8994, -2.6367,  ...,  0.0161,  2.6465,  4.5000],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0045, -0.0165,  0.0139,  ..., -0.0403, -0.0103,  0.0007],
        [-0.0122, -0.0083,  0.0005,  ..., -0.0006,  0.0061, -0.0173],
        [-0.0022, -0.0015, -0.0206,  ..., -0.0126,  0.0075,  0.0042],
        ...,
        [-0.0286, -0.0271,  0.0142,  ..., -0.0386,  0.0075, -0.0170],
        [-0.0089, -0.0018,  0.0185,  ..., -0.0124, -0.0220,  0.0224],
        [-0.0201,  0.0385,  0.0113,  ..., -0.0047,  0.0171, -0.0352]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.8867,  0.0527, -2.7871,  ..., -0.3899,  2.7188,  4.9453]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 04:21:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being reasonable is reasonableness
The state of being huge is hugeness
The state of being strange is strangeness
The state of being helpful is helpfulness
The state of being same is sameness
The state of being directed is directedness
The state of being impressive is impressiveness
The state of being competitive is
2024-07-24 04:21:52 root INFO     [order_1_approx] starting weight calculation for The state of being same is sameness
The state of being huge is hugeness
The state of being reasonable is reasonableness
The state of being helpful is helpfulness
The state of being competitive is competitiveness
The state of being strange is strangeness
The state of being impressive is impressiveness
The state of being directed is
2024-07-24 04:21:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 04:25:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.7832, -0.0716,  0.1531,  ..., -0.3582,  1.0000, -0.3740],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1504, -1.6855,  0.7305,  ..., -3.2695,  4.6133,  3.7129],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0029, -0.0141,  0.0036,  ..., -0.0223, -0.0219, -0.0038],
        [-0.0073, -0.0097,  0.0032,  ..., -0.0002, -0.0056,  0.0016],
        [ 0.0012, -0.0146, -0.0096,  ..., -0.0026, -0.0091,  0.0026],
        ...,
        [-0.0356, -0.0073, -0.0024,  ..., -0.0120,  0.0136, -0.0071],
        [-0.0021,  0.0123,  0.0015,  ..., -0.0119, -0.0079,  0.0073],
        [ 0.0021,  0.0097, -0.0038,  ..., -0.0048,  0.0210,  0.0035]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.1787e+00, -1.6660e+00,  2.9297e-03,  ..., -3.0000e+00,
          4.6172e+00,  3.4941e+00]], device='cuda:1', dtype=torch.float16,
       grad_fn=<SubBackward0>) 

                    
2024-07-24 04:25:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being same is sameness
The state of being huge is hugeness
The state of being reasonable is reasonableness
The state of being helpful is helpfulness
The state of being competitive is competitiveness
The state of being strange is strangeness
The state of being impressive is impressiveness
The state of being directed is
2024-07-24 04:25:39 root INFO     [order_1_approx] starting weight calculation for The state of being same is sameness
The state of being directed is directedness
The state of being reasonable is reasonableness
The state of being impressive is impressiveness
The state of being huge is hugeness
The state of being competitive is competitiveness
The state of being strange is strangeness
The state of being helpful is
2024-07-24 04:25:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 04:29:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6484,  0.4551, -0.3232,  ..., -0.0752,  1.0039,  0.7529],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.1855,  1.2441, -0.3438,  ..., -1.2070,  0.3735,  7.1562],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0181, -0.0167,  0.0191,  ..., -0.0077, -0.0006, -0.0206],
        [-0.0107, -0.0194, -0.0023,  ..., -0.0044,  0.0019,  0.0057],
        [-0.0043, -0.0013, -0.0142,  ...,  0.0052,  0.0084,  0.0118],
        ...,
        [-0.0138, -0.0146, -0.0032,  ..., -0.0122,  0.0038, -0.0174],
        [-0.0004,  0.0240,  0.0098,  ..., -0.0112, -0.0254, -0.0036],
        [-0.0125,  0.0096,  0.0005,  ..., -0.0129, -0.0011, -0.0372]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.0840,  1.7773, -0.5879,  ..., -0.8330, -0.0652,  8.2109]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 04:29:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being same is sameness
The state of being directed is directedness
The state of being reasonable is reasonableness
The state of being impressive is impressiveness
The state of being huge is hugeness
The state of being competitive is competitiveness
The state of being strange is strangeness
The state of being helpful is
2024-07-24 04:29:24 root INFO     [order_1_approx] starting weight calculation for The state of being impressive is impressiveness
The state of being same is sameness
The state of being directed is directedness
The state of being helpful is helpfulness
The state of being competitive is competitiveness
The state of being strange is strangeness
The state of being reasonable is reasonableness
The state of being huge is
2024-07-24 04:29:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 04:33:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2329, -0.0125,  0.4124,  ..., -0.1646,  1.0342,  0.1621],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4104,  0.9917, -2.4766,  ..., -2.6094, -0.9199,  4.8047],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.2474e-02, -9.3613e-03, -1.0864e-02,  ..., -5.8823e-03,
         -2.3117e-02, -2.0203e-02],
        [ 7.8201e-05, -2.7573e-02, -8.1177e-03,  ...,  1.4286e-03,
         -4.2000e-03,  1.7471e-03],
        [ 1.1505e-02,  7.3967e-03, -4.4189e-02,  ..., -1.8311e-03,
          1.0559e-02,  9.3689e-03],
        ...,
        [-4.6349e-03, -1.9485e-02,  1.0178e-02,  ..., -4.0375e-02,
          1.6968e-02, -2.0508e-02],
        [ 5.3596e-03,  6.1111e-03,  1.1826e-02,  ..., -2.8732e-02,
         -4.3518e-02, -3.0441e-03],
        [-1.7639e-02,  9.1705e-03, -1.2138e-02,  ..., -2.3926e-02,
         -9.7885e-03, -3.4943e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0193,  1.4570, -2.7441,  ..., -2.7930, -0.7861,  4.0469]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 04:33:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being impressive is impressiveness
The state of being same is sameness
The state of being directed is directedness
The state of being helpful is helpfulness
The state of being competitive is competitiveness
The state of being strange is strangeness
The state of being reasonable is reasonableness
The state of being huge is
2024-07-24 04:33:09 root INFO     [order_1_approx] starting weight calculation for The state of being competitive is competitiveness
The state of being reasonable is reasonableness
The state of being same is sameness
The state of being strange is strangeness
The state of being huge is hugeness
The state of being helpful is helpfulness
The state of being directed is directedness
The state of being impressive is
2024-07-24 04:33:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 04:36:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.6504, 0.1621, 0.5425,  ..., 1.3320, 1.5781, 0.9785], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.8750, -0.2188, -2.4766,  ...,  0.5103, -3.0605,  2.1113],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0005, -0.0011,  0.0027,  ..., -0.0233, -0.0224, -0.0072],
        [ 0.0066, -0.0075,  0.0107,  ..., -0.0053,  0.0131,  0.0158],
        [-0.0019,  0.0004, -0.0313,  ...,  0.0142,  0.0081,  0.0013],
        ...,
        [-0.0224, -0.0308, -0.0156,  ..., -0.0353,  0.0262, -0.0101],
        [ 0.0171, -0.0100,  0.0243,  ..., -0.0188, -0.0289, -0.0008],
        [-0.0092,  0.0324, -0.0222,  ..., -0.0101, -0.0221, -0.0032]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.4199, -0.4731, -3.1660,  ..., -0.0811, -3.5078,  3.0781]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 04:36:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being competitive is competitiveness
The state of being reasonable is reasonableness
The state of being same is sameness
The state of being strange is strangeness
The state of being huge is hugeness
The state of being helpful is helpfulness
The state of being directed is directedness
The state of being impressive is
2024-07-24 04:36:58 root INFO     [order_1_approx] starting weight calculation for The state of being huge is hugeness
The state of being competitive is competitiveness
The state of being same is sameness
The state of being impressive is impressiveness
The state of being helpful is helpfulness
The state of being strange is strangeness
The state of being directed is directedness
The state of being reasonable is
2024-07-24 04:36:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 04:40:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5732, -0.7368,  1.1699,  ..., -0.4097,  0.5806,  0.6025],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4385,  3.4805, -2.2617,  ..., -1.2012,  2.1348,  4.7031],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0175, -0.0051,  0.0110,  ..., -0.0024, -0.0125, -0.0188],
        [-0.0061, -0.0154, -0.0143,  ..., -0.0080,  0.0111,  0.0047],
        [-0.0058,  0.0058, -0.0125,  ...,  0.0027,  0.0033,  0.0009],
        ...,
        [-0.0062,  0.0001, -0.0171,  ..., -0.0231,  0.0286, -0.0027],
        [ 0.0081,  0.0030, -0.0001,  ..., -0.0171, -0.0256,  0.0023],
        [-0.0106,  0.0223, -0.0085,  ..., -0.0123,  0.0014, -0.0328]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3555,  3.0859, -2.8926,  ..., -1.4199,  2.5566,  4.0352]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 04:40:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being huge is hugeness
The state of being competitive is competitiveness
The state of being same is sameness
The state of being impressive is impressiveness
The state of being helpful is helpfulness
The state of being strange is strangeness
The state of being directed is directedness
The state of being reasonable is
2024-07-24 04:40:45 root INFO     [order_1_approx] starting weight calculation for The state of being huge is hugeness
The state of being strange is strangeness
The state of being helpful is helpfulness
The state of being reasonable is reasonableness
The state of being competitive is competitiveness
The state of being impressive is impressiveness
The state of being directed is directedness
The state of being same is
2024-07-24 04:40:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 04:44:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3486,  0.9775,  0.9336,  ..., -1.2637,  0.4209, -0.4939],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8149,  4.3945, -2.1152,  ...,  1.0010,  1.3301,  2.3516],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.9587e-02, -1.2039e-02,  8.4686e-03,  ..., -6.9580e-03,
         -4.7485e-02, -2.8076e-02],
        [-2.9846e-02, -3.7079e-02,  3.3140e-04,  ..., -5.9509e-04,
          1.0414e-02, -1.4343e-02],
        [ 2.7832e-02,  3.4695e-03, -3.5370e-02,  ...,  5.8899e-03,
          5.0964e-03,  5.9700e-03],
        ...,
        [-1.4542e-02, -8.8043e-03,  1.3741e-02,  ..., -8.3313e-03,
          1.3790e-03,  1.2718e-02],
        [-1.0063e-02,  1.6724e-02, -4.9591e-05,  ..., -4.1313e-03,
         -1.5930e-02, -6.9695e-03],
        [-1.3710e-02,  1.1505e-02, -6.1188e-03,  ..., -2.7237e-02,
          1.3474e-02, -2.2491e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9697,  5.0391, -2.2383,  ...,  0.5337,  1.5771,  2.8477]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 04:44:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being huge is hugeness
The state of being strange is strangeness
The state of being helpful is helpfulness
The state of being reasonable is reasonableness
The state of being competitive is competitiveness
The state of being impressive is impressiveness
The state of being directed is directedness
The state of being same is
2024-07-24 04:44:30 root INFO     [order_1_approx] starting weight calculation for The state of being directed is directedness
The state of being huge is hugeness
The state of being reasonable is reasonableness
The state of being competitive is competitiveness
The state of being helpful is helpfulness
The state of being same is sameness
The state of being impressive is impressiveness
The state of being strange is
2024-07-24 04:44:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 04:48:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7314, -0.6260, -0.3088,  ...,  0.2646,  0.9321, -0.1218],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.6152,  0.2441,  0.4912,  ...,  2.4062,  2.6211,  8.3750],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0084, -0.0137,  0.0087,  ..., -0.0187, -0.0166, -0.0248],
        [ 0.0006, -0.0059, -0.0067,  ..., -0.0063, -0.0143, -0.0065],
        [ 0.0027,  0.0006, -0.0445,  ...,  0.0314, -0.0018,  0.0061],
        ...,
        [-0.0050, -0.0231,  0.0010,  ..., -0.0260, -0.0027, -0.0153],
        [-0.0125,  0.0219,  0.0102,  ..., -0.0147, -0.0270, -0.0014],
        [-0.0387,  0.0046,  0.0101,  ..., -0.0424, -0.0141, -0.0475]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.2461,  0.1160,  0.3965,  ...,  2.4219,  2.5742,  8.7734]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 04:48:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being directed is directedness
The state of being huge is hugeness
The state of being reasonable is reasonableness
The state of being competitive is competitiveness
The state of being helpful is helpfulness
The state of being same is sameness
The state of being impressive is impressiveness
The state of being strange is
2024-07-24 04:48:19 root INFO     total operator prediction time: 1816.2936046123505 seconds
2024-07-24 04:48:19 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on re+verb_reg
2024-07-24 04:48:19 root INFO     building operator re+verb_reg
2024-07-24 04:48:19 root INFO     [order_1_approx] starting weight calculation for To calculate again is to recalculate
To organize again is to reorganize
To configure again is to reconfigure
To send again is to resend
To deem again is to redeem
To investigate again is to reinvestigate
To engage again is to reengage
To adjust again is to
2024-07-24 04:48:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 04:52:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.6777, 0.0291, 1.0820,  ..., 0.2639, 0.2415, 0.0332], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7168,  2.2598, -7.5938,  ..., -1.1504,  0.2156,  3.7734],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0370, -0.0013,  0.0122,  ..., -0.0122, -0.0166,  0.0013],
        [-0.0029,  0.0038, -0.0019,  ...,  0.0006,  0.0179, -0.0209],
        [ 0.0520,  0.0030, -0.0059,  ...,  0.0100, -0.0205, -0.0056],
        ...,
        [-0.0474, -0.0209, -0.0060,  ..., -0.0046,  0.0050, -0.0112],
        [-0.0032, -0.0122,  0.0095,  ..., -0.0111, -0.0073, -0.0047],
        [ 0.0016,  0.0200,  0.0014,  ..., -0.0045,  0.0156, -0.0162]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5664,  2.9277, -7.5625,  ..., -0.7363, -0.5576,  3.5625]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 04:52:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To calculate again is to recalculate
To organize again is to reorganize
To configure again is to reconfigure
To send again is to resend
To deem again is to redeem
To investigate again is to reinvestigate
To engage again is to reengage
To adjust again is to
2024-07-24 04:52:08 root INFO     [order_1_approx] starting weight calculation for To send again is to resend
To engage again is to reengage
To deem again is to redeem
To configure again is to reconfigure
To adjust again is to readjust
To investigate again is to reinvestigate
To organize again is to reorganize
To calculate again is to
2024-07-24 04:52:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 04:55:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1780,  0.9209, -0.0317,  ...,  0.2330, -0.5391,  0.9551],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.8125,  0.9893, -3.0840,  ..., -4.0195,  1.4727, -0.7866],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0351,  0.0124, -0.0004,  ...,  0.0041, -0.0030,  0.0007],
        [-0.0121, -0.0099,  0.0121,  ..., -0.0023,  0.0015, -0.0131],
        [ 0.0190,  0.0140, -0.0034,  ...,  0.0210, -0.0006,  0.0136],
        ...,
        [-0.0179, -0.0139, -0.0123,  ..., -0.0128, -0.0065,  0.0157],
        [-0.0022, -0.0011,  0.0239,  ...,  0.0047, -0.0173, -0.0025],
        [ 0.0069,  0.0168,  0.0229,  ...,  0.0056,  0.0063,  0.0241]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.0527,  0.9126, -3.4922,  ..., -4.4570,  1.4600, -1.3633]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 04:55:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To send again is to resend
To engage again is to reengage
To deem again is to redeem
To configure again is to reconfigure
To adjust again is to readjust
To investigate again is to reinvestigate
To organize again is to reorganize
To calculate again is to
2024-07-24 04:55:57 root INFO     [order_1_approx] starting weight calculation for To send again is to resend
To deem again is to redeem
To organize again is to reorganize
To investigate again is to reinvestigate
To engage again is to reengage
To calculate again is to recalculate
To adjust again is to readjust
To configure again is to
2024-07-24 04:55:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 04:59:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1836,  0.8599,  0.3423,  ...,  0.3123, -0.7520, -0.4778],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.4297,  0.6704, -6.0508,  ..., -2.4199,  0.3403, -0.0566],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0112, -0.0051,  0.0072,  ...,  0.0054, -0.0094, -0.0103],
        [-0.0067, -0.0112,  0.0049,  ..., -0.0169,  0.0129, -0.0090],
        [ 0.0291,  0.0056, -0.0164,  ...,  0.0050, -0.0014, -0.0021],
        ...,
        [-0.0192, -0.0172, -0.0020,  ...,  0.0023, -0.0182,  0.0284],
        [ 0.0046, -0.0018, -0.0072,  ...,  0.0026, -0.0198, -0.0073],
        [ 0.0024,  0.0085,  0.0048,  ..., -0.0046,  0.0175, -0.0022]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.0586,  1.7734, -6.7188,  ..., -3.2227,  0.1665,  0.2053]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 04:59:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To send again is to resend
To deem again is to redeem
To organize again is to reorganize
To investigate again is to reinvestigate
To engage again is to reengage
To calculate again is to recalculate
To adjust again is to readjust
To configure again is to
2024-07-24 04:59:45 root INFO     [order_1_approx] starting weight calculation for To investigate again is to reinvestigate
To calculate again is to recalculate
To configure again is to reconfigure
To engage again is to reengage
To send again is to resend
To organize again is to reorganize
To adjust again is to readjust
To deem again is to
2024-07-24 04:59:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 05:03:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5918, -0.1252,  0.5518,  ..., -0.3318, -0.8711,  1.0820],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.3594, -0.1040, -2.7031,  ..., -1.6953, -2.4824,  0.6504],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0073,  0.0013,  0.0195,  ...,  0.0130, -0.0130, -0.0102],
        [-0.0058, -0.0023,  0.0078,  ..., -0.0014,  0.0031,  0.0033],
        [ 0.0042,  0.0047, -0.0167,  ..., -0.0027,  0.0046, -0.0115],
        ...,
        [-0.0052, -0.0053,  0.0039,  ...,  0.0048, -0.0030,  0.0214],
        [ 0.0094, -0.0072, -0.0126,  ..., -0.0141, -0.0059,  0.0112],
        [ 0.0021,  0.0047,  0.0071,  ...,  0.0118,  0.0043,  0.0088]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.8047,  0.5317, -2.4609,  ..., -2.3047, -2.5664,  0.5771]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 05:03:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To investigate again is to reinvestigate
To calculate again is to recalculate
To configure again is to reconfigure
To engage again is to reengage
To send again is to resend
To organize again is to reorganize
To adjust again is to readjust
To deem again is to
2024-07-24 05:03:34 root INFO     [order_1_approx] starting weight calculation for To send again is to resend
To adjust again is to readjust
To calculate again is to recalculate
To deem again is to redeem
To investigate again is to reinvestigate
To configure again is to reconfigure
To organize again is to reorganize
To engage again is to
2024-07-24 05:03:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 05:07:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5288, -0.0219,  0.4810,  ...,  1.1055, -1.5225,  0.7119],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5190, -0.9766, -2.0586,  ..., -1.2910, -0.4072,  0.5703],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0175, -0.0022, -0.0047,  ..., -0.0062, -0.0124, -0.0118],
        [ 0.0052,  0.0110, -0.0107,  ...,  0.0007,  0.0032,  0.0046],
        [ 0.0043,  0.0059,  0.0033,  ..., -0.0102, -0.0028, -0.0004],
        ...,
        [-0.0179, -0.0105, -0.0025,  ...,  0.0085, -0.0015, -0.0155],
        [ 0.0132, -0.0061,  0.0105,  ...,  0.0122,  0.0031,  0.0082],
        [-0.0014,  0.0183,  0.0005,  ...,  0.0010,  0.0099, -0.0015]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8037, -0.7646, -2.4785,  ..., -1.9248, -1.4717,  0.2812]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 05:07:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To send again is to resend
To adjust again is to readjust
To calculate again is to recalculate
To deem again is to redeem
To investigate again is to reinvestigate
To configure again is to reconfigure
To organize again is to reorganize
To engage again is to
2024-07-24 05:07:24 root INFO     [order_1_approx] starting weight calculation for To configure again is to reconfigure
To deem again is to redeem
To calculate again is to recalculate
To adjust again is to readjust
To send again is to resend
To organize again is to reorganize
To engage again is to reengage
To investigate again is to
2024-07-24 05:07:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 05:11:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6006, -0.4573,  0.7554,  ...,  0.0919, -0.5522,  1.0801],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6035,  2.3457, -4.3633,  ...,  0.0332, -0.8760, -1.0176],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.0518e-05, -6.3286e-03,  2.3460e-03,  ..., -2.1835e-02,
         -3.2425e-03,  5.2223e-03],
        [ 2.6627e-03, -3.6850e-03,  1.8387e-03,  ...,  1.9569e-03,
          1.5106e-02, -1.9455e-04],
        [ 2.6733e-02,  8.4610e-03,  6.6910e-03,  ...,  1.7487e-02,
          4.1046e-03, -3.1052e-03],
        ...,
        [-2.6917e-02, -1.9958e-02, -9.1019e-03,  ..., -2.5040e-02,
         -6.0081e-05,  2.8496e-03],
        [ 6.1378e-03, -7.3547e-03,  1.3794e-02,  ...,  2.0370e-03,
         -7.1106e-03, -2.5330e-03],
        [-2.1172e-03,  2.1332e-02,  1.6144e-02,  ..., -5.1537e-03,
          4.4861e-03,  1.8654e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7676,  3.2773, -5.4570,  ...,  0.1558, -0.8760, -1.7227]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 05:11:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To configure again is to reconfigure
To deem again is to redeem
To calculate again is to recalculate
To adjust again is to readjust
To send again is to resend
To organize again is to reorganize
To engage again is to reengage
To investigate again is to
2024-07-24 05:11:12 root INFO     [order_1_approx] starting weight calculation for To send again is to resend
To investigate again is to reinvestigate
To adjust again is to readjust
To engage again is to reengage
To calculate again is to recalculate
To configure again is to reconfigure
To deem again is to redeem
To organize again is to
2024-07-24 05:11:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 05:15:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2416,  0.1343,  0.6953,  ..., -0.4863,  0.4641,  0.3506],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7739, -1.2012, -2.5352,  ...,  2.1152, -0.2383,  0.9893],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0047, -0.0266,  0.0081,  ...,  0.0045, -0.0005, -0.0225],
        [ 0.0044, -0.0114,  0.0060,  ..., -0.0106,  0.0138, -0.0098],
        [ 0.0386, -0.0022, -0.0202,  ...,  0.0071,  0.0007, -0.0206],
        ...,
        [-0.0376, -0.0361,  0.0101,  ...,  0.0065, -0.0193,  0.0138],
        [-0.0116, -0.0056, -0.0086,  ...,  0.0056, -0.0233,  0.0120],
        [ 0.0022, -0.0080, -0.0011,  ..., -0.0076,  0.0226,  0.0047]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0591,  0.2246, -3.1875,  ...,  2.3086, -1.2178,  1.0088]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 05:15:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To send again is to resend
To investigate again is to reinvestigate
To adjust again is to readjust
To engage again is to reengage
To calculate again is to recalculate
To configure again is to reconfigure
To deem again is to redeem
To organize again is to
2024-07-24 05:15:01 root INFO     [order_1_approx] starting weight calculation for To deem again is to redeem
To engage again is to reengage
To adjust again is to readjust
To investigate again is to reinvestigate
To configure again is to reconfigure
To calculate again is to recalculate
To organize again is to reorganize
To send again is to
2024-07-24 05:15:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 05:18:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.2979, 0.0229, 0.5581,  ..., 0.4734, 0.4004, 0.5811], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.6562,  2.7422, -0.3926,  ...,  3.5762, -1.6504,  1.7500],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0045,  0.0104,  0.0017,  ..., -0.0040, -0.0117,  0.0004],
        [-0.0026, -0.0092,  0.0060,  ...,  0.0188,  0.0004,  0.0152],
        [ 0.0022, -0.0030,  0.0003,  ...,  0.0138, -0.0033, -0.0030],
        ...,
        [-0.0193, -0.0318,  0.0035,  ..., -0.0255, -0.0024,  0.0043],
        [ 0.0054,  0.0115, -0.0012,  ..., -0.0100, -0.0300,  0.0103],
        [ 0.0012,  0.0103,  0.0056,  ..., -0.0096,  0.0118, -0.0154]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.2383,  4.1055, -0.7988,  ...,  3.0820, -1.5879,  1.1748]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 05:18:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To deem again is to redeem
To engage again is to reengage
To adjust again is to readjust
To investigate again is to reinvestigate
To configure again is to reconfigure
To calculate again is to recalculate
To organize again is to reorganize
To send again is to
2024-07-24 05:18:49 root INFO     total operator prediction time: 1829.9942908287048 seconds
2024-07-24 05:18:49 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on un+adj_reg
2024-07-24 05:18:49 root INFO     building operator un+adj_reg
2024-07-24 05:18:49 root INFO     [order_1_approx] starting weight calculation for The opposite of realistic is unrealistic
The opposite of suitable is unsuitable
The opposite of published is unpublished
The opposite of known is unknown
The opposite of expected is unexpected
The opposite of resolved is unresolved
The opposite of predictable is unpredictable
The opposite of desirable is
2024-07-24 05:18:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 05:22:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2124,  0.4053, -0.3418,  ..., -0.9219,  1.9824, -0.5732],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.1992,  1.2383,  2.9102,  ..., -2.3281,  2.3047,  2.0703],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0027, -0.0291, -0.0106,  ..., -0.0088, -0.0146, -0.0179],
        [-0.0017, -0.0240, -0.0192,  ..., -0.0266, -0.0180,  0.0191],
        [-0.0018, -0.0094, -0.0393,  ..., -0.0095, -0.0065,  0.0179],
        ...,
        [-0.0119, -0.0103, -0.0036,  ..., -0.0177, -0.0031, -0.0023],
        [-0.0265, -0.0172,  0.0162,  ...,  0.0060, -0.0225, -0.0149],
        [-0.0239,  0.0021,  0.0045,  ...,  0.0114,  0.0291, -0.0097]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.5723,  1.3008,  1.8350,  ..., -1.5391,  1.3994,  2.4512]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 05:22:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of realistic is unrealistic
The opposite of suitable is unsuitable
The opposite of published is unpublished
The opposite of known is unknown
The opposite of expected is unexpected
The opposite of resolved is unresolved
The opposite of predictable is unpredictable
The opposite of desirable is
2024-07-24 05:22:38 root INFO     [order_1_approx] starting weight calculation for The opposite of published is unpublished
The opposite of desirable is undesirable
The opposite of suitable is unsuitable
The opposite of resolved is unresolved
The opposite of predictable is unpredictable
The opposite of realistic is unrealistic
The opposite of known is unknown
The opposite of expected is
2024-07-24 05:22:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 05:26:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9922,  0.7144,  0.4248,  ...,  0.2137,  2.6367, -0.3169],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3203, -1.0059,  0.2510,  ...,  1.9961,  2.8418,  1.8945],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.7302e-03, -2.2232e-02,  7.2174e-03,  ..., -8.8882e-04,
         -1.1826e-02, -3.2310e-03],
        [-1.1459e-02,  1.2245e-03, -1.3908e-02,  ..., -1.3405e-02,
         -2.5803e-02, -1.1192e-02],
        [ 3.9291e-03, -2.1240e-02, -3.0045e-02,  ..., -2.3712e-02,
          1.9951e-03,  8.4534e-03],
        ...,
        [-3.2013e-02, -2.2583e-02, -1.5259e-04,  ...,  8.1100e-03,
         -1.3260e-02,  3.1452e-03],
        [-2.5139e-03, -1.2611e-02,  7.1983e-03,  ...,  6.8665e-05,
         -3.8338e-03, -1.2260e-02],
        [ 7.1030e-03, -1.7715e-02, -1.3657e-02,  ...,  6.1417e-04,
          2.4841e-02, -8.1329e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4922, -1.3721,  0.0078,  ...,  1.8105,  3.1055,  2.4258]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 05:26:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of published is unpublished
The opposite of desirable is undesirable
The opposite of suitable is unsuitable
The opposite of resolved is unresolved
The opposite of predictable is unpredictable
The opposite of realistic is unrealistic
The opposite of known is unknown
The opposite of expected is
2024-07-24 05:26:27 root INFO     [order_1_approx] starting weight calculation for The opposite of published is unpublished
The opposite of suitable is unsuitable
The opposite of resolved is unresolved
The opposite of desirable is undesirable
The opposite of predictable is unpredictable
The opposite of expected is unexpected
The opposite of realistic is unrealistic
The opposite of known is
2024-07-24 05:26:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 05:30:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.7617, 1.3877, 1.1855,  ..., 0.2830, 1.2812, 0.2637], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5449, -1.5586,  3.3652,  ...,  0.8018,  1.5693,  0.2275],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.5083e-02, -3.2684e-02,  2.7390e-03,  ..., -6.6948e-03,
         -3.3531e-03, -8.1406e-03],
        [ 8.3008e-03, -2.2888e-04, -2.4231e-02,  ...,  2.0752e-02,
         -1.6724e-02,  1.6327e-02],
        [-2.1957e-02, -5.0392e-03, -3.7262e-02,  ..., -1.9684e-03,
          1.5030e-03,  9.7122e-03],
        ...,
        [-1.1597e-02, -3.9398e-02,  6.2180e-03,  ..., -1.6876e-02,
         -2.2354e-03,  1.7319e-03],
        [-7.5607e-03,  3.8719e-03,  9.1858e-03,  ...,  7.2479e-05,
         -1.6266e-02, -5.5466e-03],
        [-9.9869e-03, -7.7324e-03, -2.8992e-03,  ..., -8.4305e-03,
          1.9913e-02, -2.3773e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1304, -1.3125,  2.9863,  ...,  0.6274,  2.4062,  0.7793]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 05:30:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of published is unpublished
The opposite of suitable is unsuitable
The opposite of resolved is unresolved
The opposite of desirable is undesirable
The opposite of predictable is unpredictable
The opposite of expected is unexpected
The opposite of realistic is unrealistic
The opposite of known is
2024-07-24 05:30:14 root INFO     [order_1_approx] starting weight calculation for The opposite of desirable is undesirable
The opposite of resolved is unresolved
The opposite of known is unknown
The opposite of realistic is unrealistic
The opposite of published is unpublished
The opposite of suitable is unsuitable
The opposite of expected is unexpected
The opposite of predictable is
2024-07-24 05:30:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 05:34:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8223, -0.2183,  1.1338,  ..., -0.7598,  2.1758, -0.3555],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1523, -3.0449, -1.1777,  ...,  1.8984,  3.7539,  4.4922],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0059, -0.0282,  0.0273,  ...,  0.0090,  0.0018, -0.0124],
        [ 0.0017, -0.0016, -0.0276,  ..., -0.0230, -0.0185, -0.0148],
        [ 0.0012, -0.0323, -0.0068,  ..., -0.0013, -0.0029,  0.0210],
        ...,
        [-0.0325, -0.0187, -0.0217,  ...,  0.0052,  0.0066,  0.0025],
        [-0.0179, -0.0019, -0.0029,  ...,  0.0019, -0.0298, -0.0106],
        [ 0.0197,  0.0059, -0.0026,  ..., -0.0154,  0.0234, -0.0132]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8652, -3.0371, -1.5293,  ...,  2.0352,  2.9219,  3.8672]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 05:34:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of desirable is undesirable
The opposite of resolved is unresolved
The opposite of known is unknown
The opposite of realistic is unrealistic
The opposite of published is unpublished
The opposite of suitable is unsuitable
The opposite of expected is unexpected
The opposite of predictable is
2024-07-24 05:34:05 root INFO     [order_1_approx] starting weight calculation for The opposite of realistic is unrealistic
The opposite of desirable is undesirable
The opposite of resolved is unresolved
The opposite of known is unknown
The opposite of suitable is unsuitable
The opposite of expected is unexpected
The opposite of predictable is unpredictable
The opposite of published is
2024-07-24 05:34:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 05:37:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.3076, 0.5913, 1.3262,  ..., 1.2969, 1.4121, 0.6665], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2920, -3.7812,  2.8125,  ...,  1.0430,  3.1289, -2.3379],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0104, -0.0196,  0.0160,  ..., -0.0115, -0.0165, -0.0217],
        [-0.0053,  0.0099, -0.0219,  ...,  0.0091, -0.0382, -0.0099],
        [-0.0162, -0.0056, -0.0379,  ..., -0.0161, -0.0107, -0.0042],
        ...,
        [-0.0144, -0.0085,  0.0063,  ..., -0.0183,  0.0009, -0.0046],
        [-0.0141,  0.0225,  0.0044,  ...,  0.0006, -0.0040,  0.0061],
        [ 0.0189,  0.0171, -0.0167,  ..., -0.0179,  0.0201, -0.0246]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.2441, -3.6641,  1.6104,  ...,  1.2754,  3.4258, -3.5000]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 05:37:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of realistic is unrealistic
The opposite of desirable is undesirable
The opposite of resolved is unresolved
The opposite of known is unknown
The opposite of suitable is unsuitable
The opposite of expected is unexpected
The opposite of predictable is unpredictable
The opposite of published is
2024-07-24 05:37:53 root INFO     [order_1_approx] starting weight calculation for The opposite of resolved is unresolved
The opposite of desirable is undesirable
The opposite of suitable is unsuitable
The opposite of predictable is unpredictable
The opposite of published is unpublished
The opposite of known is unknown
The opposite of expected is unexpected
The opposite of realistic is
2024-07-24 05:37:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 05:41:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2321, -1.0830,  0.4016,  ...,  0.1177,  2.4844,  0.0862],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.3340, -2.8086, -2.1719,  ..., -2.3516,  3.1719,  2.8848],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0053, -0.0153,  0.0150,  ...,  0.0136, -0.0217, -0.0043],
        [ 0.0074, -0.0012, -0.0022,  ...,  0.0028, -0.0271, -0.0017],
        [-0.0160, -0.0237, -0.0145,  ...,  0.0060,  0.0031,  0.0010],
        ...,
        [-0.0289, -0.0223, -0.0172,  ..., -0.0151,  0.0168, -0.0004],
        [-0.0115, -0.0214,  0.0420,  ...,  0.0015, -0.0222,  0.0012],
        [-0.0060,  0.0061, -0.0164,  ..., -0.0068,  0.0334, -0.0259]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5977, -2.9648, -3.1562,  ..., -2.3086,  3.6348,  2.8125]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 05:41:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of resolved is unresolved
The opposite of desirable is undesirable
The opposite of suitable is unsuitable
The opposite of predictable is unpredictable
The opposite of published is unpublished
The opposite of known is unknown
The opposite of expected is unexpected
The opposite of realistic is
2024-07-24 05:41:42 root INFO     [order_1_approx] starting weight calculation for The opposite of desirable is undesirable
The opposite of expected is unexpected
The opposite of predictable is unpredictable
The opposite of known is unknown
The opposite of published is unpublished
The opposite of suitable is unsuitable
The opposite of realistic is unrealistic
The opposite of resolved is
2024-07-24 05:41:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 05:45:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2166,  0.5859,  0.8223,  ...,  1.0352,  0.7007, -0.4224],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5469, -2.4199, -3.2930,  ...,  1.8311,  2.6367, -2.0293],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0123, -0.0176,  0.0026,  ...,  0.0016, -0.0288, -0.0078],
        [-0.0218,  0.0008, -0.0134,  ..., -0.0293, -0.0287, -0.0149],
        [ 0.0068, -0.0358, -0.0092,  ..., -0.0251, -0.0035, -0.0026],
        ...,
        [-0.0293, -0.0034, -0.0077,  ..., -0.0307,  0.0081,  0.0086],
        [-0.0065, -0.0165,  0.0192,  ...,  0.0134,  0.0002, -0.0305],
        [-0.0190,  0.0036,  0.0094,  ..., -0.0003,  0.0041, -0.0042]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5771, -3.7305, -2.9531,  ...,  2.0723,  2.8125, -2.2012]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 05:45:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of desirable is undesirable
The opposite of expected is unexpected
The opposite of predictable is unpredictable
The opposite of known is unknown
The opposite of published is unpublished
The opposite of suitable is unsuitable
The opposite of realistic is unrealistic
The opposite of resolved is
2024-07-24 05:45:31 root INFO     [order_1_approx] starting weight calculation for The opposite of expected is unexpected
The opposite of resolved is unresolved
The opposite of desirable is undesirable
The opposite of realistic is unrealistic
The opposite of known is unknown
The opposite of published is unpublished
The opposite of predictable is unpredictable
The opposite of suitable is
2024-07-24 05:45:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 05:49:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2461,  0.0940,  0.8350,  ..., -0.4458,  1.2217,  0.1689],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.8027,  1.5234,  0.2422,  ..., -1.0820,  1.2598,  1.8047],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0126, -0.0026,  0.0051,  ..., -0.0007, -0.0261, -0.0100],
        [ 0.0103,  0.0027, -0.0283,  ..., -0.0102, -0.0306, -0.0067],
        [ 0.0207, -0.0031, -0.0224,  ..., -0.0085, -0.0149,  0.0212],
        ...,
        [-0.0122, -0.0137, -0.0194,  ..., -0.0176,  0.0114,  0.0113],
        [-0.0289, -0.0217,  0.0349,  ...,  0.0059, -0.0225, -0.0086],
        [-0.0011,  0.0322, -0.0148,  ..., -0.0268,  0.0231, -0.0142]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.3086,  1.6895,  0.8330,  ..., -0.5713,  0.3286,  1.7627]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 05:49:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of expected is unexpected
The opposite of resolved is unresolved
The opposite of desirable is undesirable
The opposite of realistic is unrealistic
The opposite of known is unknown
The opposite of published is unpublished
The opposite of predictable is unpredictable
The opposite of suitable is
2024-07-24 05:49:19 root INFO     total operator prediction time: 1830.0341169834137 seconds
2024-07-24 05:49:19 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+able_reg
2024-07-24 05:49:19 root INFO     building operator verb+able_reg
2024-07-24 05:49:19 root INFO     [order_1_approx] starting weight calculation for If you can understand something, that thing is understandable
If you can expect something, that thing is expectable
If you can avoid something, that thing is avoidable
If you can predict something, that thing is predictable
If you can expand something, that thing is expandable
If you can renew something, that thing is renewable
If you can discover something, that thing is discoverable
If you can achieve something, that thing is
2024-07-24 05:49:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 05:53:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5615,  1.1035,  0.6289,  ..., -0.3887,  0.4011,  0.0437],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0303,  3.6445, -1.5449,  ..., -7.9727,  5.2930, -0.4497],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0155, -0.0040, -0.0020,  ...,  0.0284,  0.0016, -0.0089],
        [ 0.0069, -0.0109,  0.0007,  ..., -0.0061, -0.0055,  0.0002],
        [-0.0059,  0.0003, -0.0225,  ...,  0.0158,  0.0039, -0.0015],
        ...,
        [-0.0005, -0.0118, -0.0144,  ..., -0.0292, -0.0139,  0.0107],
        [ 0.0048, -0.0017,  0.0019,  ..., -0.0193, -0.0297,  0.0142],
        [-0.0088,  0.0125,  0.0151,  ..., -0.0078,  0.0030, -0.0335]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7646,  3.7520, -1.0977,  ..., -8.1484,  6.0703, -1.2617]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 05:53:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can understand something, that thing is understandable
If you can expect something, that thing is expectable
If you can avoid something, that thing is avoidable
If you can predict something, that thing is predictable
If you can expand something, that thing is expandable
If you can renew something, that thing is renewable
If you can discover something, that thing is discoverable
If you can achieve something, that thing is
2024-07-24 05:53:07 root INFO     [order_1_approx] starting weight calculation for If you can understand something, that thing is understandable
If you can predict something, that thing is predictable
If you can expect something, that thing is expectable
If you can expand something, that thing is expandable
If you can achieve something, that thing is achieveable
If you can discover something, that thing is discoverable
If you can renew something, that thing is renewable
If you can avoid something, that thing is
2024-07-24 05:53:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 05:56:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4480,  0.5859, -1.2891,  ..., -0.4854, -0.3052, -0.0458],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.7266,  2.1172, -3.4414,  ...,  1.1309,  7.0977,  2.2715],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0046, -0.0005,  0.0041,  ...,  0.0086, -0.0084,  0.0015],
        [-0.0028, -0.0138,  0.0044,  ..., -0.0056, -0.0176, -0.0124],
        [ 0.0084,  0.0114, -0.0155,  ...,  0.0038,  0.0165,  0.0116],
        ...,
        [-0.0300, -0.0134, -0.0196,  ..., -0.0117,  0.0075,  0.0067],
        [-0.0019, -0.0063,  0.0117,  ..., -0.0195, -0.0338,  0.0031],
        [-0.0141,  0.0090, -0.0033,  ..., -0.0148,  0.0045, -0.0365]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.8652,  1.8926, -3.5430,  ...,  1.6719,  7.8320,  2.3418]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 05:56:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can understand something, that thing is understandable
If you can predict something, that thing is predictable
If you can expect something, that thing is expectable
If you can expand something, that thing is expandable
If you can achieve something, that thing is achieveable
If you can discover something, that thing is discoverable
If you can renew something, that thing is renewable
If you can avoid something, that thing is
2024-07-24 05:56:56 root INFO     [order_1_approx] starting weight calculation for If you can expect something, that thing is expectable
If you can expand something, that thing is expandable
If you can understand something, that thing is understandable
If you can avoid something, that thing is avoidable
If you can renew something, that thing is renewable
If you can achieve something, that thing is achieveable
If you can predict something, that thing is predictable
If you can discover something, that thing is
2024-07-24 05:56:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 06:00:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0239,  0.1616, -0.7432,  ..., -0.2043,  0.9233,  0.0508],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7881,  1.0352, -2.6387,  ..., -4.0234,  1.4727,  1.6855],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0144, -0.0065,  0.0050,  ...,  0.0127, -0.0027, -0.0053],
        [-0.0067, -0.0062, -0.0012,  ..., -0.0079,  0.0183, -0.0050],
        [ 0.0047,  0.0005, -0.0160,  ..., -0.0095,  0.0121, -0.0061],
        ...,
        [-0.0136, -0.0068, -0.0120,  ..., -0.0123,  0.0003,  0.0020],
        [-0.0079,  0.0027,  0.0098,  ..., -0.0180, -0.0113,  0.0108],
        [-0.0031, -0.0079,  0.0017,  ..., -0.0301,  0.0174, -0.0330]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.1172,  0.8062, -2.6152,  ..., -3.1914,  1.5586,  2.1816]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 06:00:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can expect something, that thing is expectable
If you can expand something, that thing is expandable
If you can understand something, that thing is understandable
If you can avoid something, that thing is avoidable
If you can renew something, that thing is renewable
If you can achieve something, that thing is achieveable
If you can predict something, that thing is predictable
If you can discover something, that thing is
2024-07-24 06:00:46 root INFO     [order_1_approx] starting weight calculation for If you can predict something, that thing is predictable
If you can renew something, that thing is renewable
If you can discover something, that thing is discoverable
If you can understand something, that thing is understandable
If you can achieve something, that thing is achieveable
If you can expect something, that thing is expectable
If you can avoid something, that thing is avoidable
If you can expand something, that thing is
2024-07-24 06:00:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 06:04:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1929,  1.0410,  0.0961,  ..., -0.2153,  1.0322, -0.1517],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.3320,  0.4351, -3.5000,  ..., -2.0723,  2.3672,  1.2529],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-7.0763e-03, -9.3842e-04, -3.4218e-03,  ...,  1.2192e-02,
         -1.8707e-02, -2.5330e-03],
        [ 9.7990e-05, -1.5442e-02,  5.1117e-03,  ...,  2.8229e-03,
          8.1253e-04, -1.9875e-03],
        [ 9.6130e-04,  4.5662e-03, -1.4442e-02,  ..., -2.5063e-03,
          5.7831e-03, -1.0386e-03],
        ...,
        [-2.5238e-02, -8.1024e-03, -1.4236e-02,  ..., -1.8005e-02,
         -3.4752e-03,  2.5291e-03],
        [-1.5366e-02, -2.9068e-03,  7.9117e-03,  ..., -1.8372e-02,
         -2.4139e-02,  4.1351e-03],
        [-4.7836e-03, -6.7444e-03, -4.3106e-04,  ..., -1.4626e-02,
          2.7313e-02, -3.0365e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.0449,  0.2015, -3.5938,  ..., -1.1641,  2.3984,  0.5137]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 06:04:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can predict something, that thing is predictable
If you can renew something, that thing is renewable
If you can discover something, that thing is discoverable
If you can understand something, that thing is understandable
If you can achieve something, that thing is achieveable
If you can expect something, that thing is expectable
If you can avoid something, that thing is avoidable
If you can expand something, that thing is
2024-07-24 06:04:32 root INFO     [order_1_approx] starting weight calculation for If you can renew something, that thing is renewable
If you can discover something, that thing is discoverable
If you can achieve something, that thing is achieveable
If you can understand something, that thing is understandable
If you can predict something, that thing is predictable
If you can expand something, that thing is expandable
If you can avoid something, that thing is avoidable
If you can expect something, that thing is
2024-07-24 06:04:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 06:08:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0779,  1.7617, -0.6338,  ...,  0.3076,  2.2695,  0.8447],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4775,  2.8945, -2.7695,  ..., -0.7856,  4.6680,  2.2285],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.7548e-02, -6.5727e-03, -1.2665e-03,  ...,  7.5493e-03,
         -2.1484e-02, -9.8572e-03],
        [-1.6098e-03, -1.9684e-02,  1.0605e-03,  ..., -8.6594e-04,
         -8.9264e-03,  2.5177e-03],
        [-1.1444e-05,  3.2330e-03, -1.2650e-02,  ...,  6.2027e-03,
          1.8826e-03, -4.6120e-03],
        ...,
        [-1.4542e-02, -1.5549e-02, -1.4587e-02,  ..., -1.3657e-02,
         -1.4069e-02, -6.1569e-03],
        [-3.2616e-04, -5.4703e-03, -4.1580e-04,  ..., -1.9150e-02,
         -3.4882e-02,  9.0103e-03],
        [ 3.9940e-03,  8.5297e-03,  4.1771e-03,  ..., -1.3275e-02,
          2.0386e-02, -2.4261e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5049,  3.0234, -2.2578,  ..., -0.6064,  5.4609,  1.4893]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 06:08:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can renew something, that thing is renewable
If you can discover something, that thing is discoverable
If you can achieve something, that thing is achieveable
If you can understand something, that thing is understandable
If you can predict something, that thing is predictable
If you can expand something, that thing is expandable
If you can avoid something, that thing is avoidable
If you can expect something, that thing is
2024-07-24 06:08:13 root INFO     [order_1_approx] starting weight calculation for If you can expand something, that thing is expandable
If you can renew something, that thing is renewable
If you can avoid something, that thing is avoidable
If you can understand something, that thing is understandable
If you can discover something, that thing is discoverable
If you can expect something, that thing is expectable
If you can achieve something, that thing is achieveable
If you can predict something, that thing is
2024-07-24 06:08:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 06:11:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3506,  0.8154,  0.3967,  ..., -0.6689,  0.9165,  0.2090],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6152,  0.3882, -2.3086,  ..., -2.6953,  4.2773,  1.6406],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0190, -0.0069,  0.0312,  ...,  0.0126, -0.0204, -0.0076],
        [-0.0169, -0.0289, -0.0024,  ..., -0.0074,  0.0038, -0.0014],
        [ 0.0003,  0.0051, -0.0087,  ...,  0.0136,  0.0079, -0.0096],
        ...,
        [-0.0253, -0.0282, -0.0201,  ..., -0.0250, -0.0048, -0.0156],
        [ 0.0037, -0.0213,  0.0097,  ..., -0.0179, -0.0270,  0.0120],
        [-0.0190,  0.0160, -0.0106,  ..., -0.0238,  0.0241, -0.0536]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0430,  0.5610, -2.2168,  ..., -2.3809,  5.0859,  1.1572]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 06:11:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can expand something, that thing is expandable
If you can renew something, that thing is renewable
If you can avoid something, that thing is avoidable
If you can understand something, that thing is understandable
If you can discover something, that thing is discoverable
If you can expect something, that thing is expectable
If you can achieve something, that thing is achieveable
If you can predict something, that thing is
2024-07-24 06:11:55 root INFO     [order_1_approx] starting weight calculation for If you can expect something, that thing is expectable
If you can understand something, that thing is understandable
If you can expand something, that thing is expandable
If you can predict something, that thing is predictable
If you can discover something, that thing is discoverable
If you can avoid something, that thing is avoidable
If you can achieve something, that thing is achieveable
If you can renew something, that thing is
2024-07-24 06:11:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 06:15:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3677,  0.2581,  0.1707,  ..., -0.6460,  0.3406,  0.5752],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2100,  0.3701, -4.1875,  ..., -0.1279,  1.1641,  1.7422],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.2062e-02, -6.1874e-03,  7.0877e-03,  ...,  5.5389e-03,
         -4.0512e-03, -3.1662e-03],
        [ 4.0054e-03, -1.2421e-02, -1.1124e-02,  ...,  4.6234e-03,
          1.4099e-02, -2.0733e-03],
        [ 1.3206e-02,  7.7972e-03, -5.1041e-03,  ...,  7.1030e-03,
          1.2184e-02, -9.1705e-03],
        ...,
        [-1.7792e-02,  1.2207e-04, -6.1417e-03,  ..., -2.0187e-02,
          1.2207e-02, -1.6449e-02],
        [-9.5978e-03, -2.0370e-02,  1.1078e-02,  ..., -1.8219e-02,
         -1.7349e-02,  1.8509e-02],
        [-5.7373e-03,  3.4332e-05,  1.3275e-03,  ..., -2.0325e-02,
          6.7520e-03, -4.2206e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.6826,  0.6826, -4.0703,  ...,  0.8560,  1.2939,  1.3818]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 06:15:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can expect something, that thing is expectable
If you can understand something, that thing is understandable
If you can expand something, that thing is expandable
If you can predict something, that thing is predictable
If you can discover something, that thing is discoverable
If you can avoid something, that thing is avoidable
If you can achieve something, that thing is achieveable
If you can renew something, that thing is
2024-07-24 06:15:36 root INFO     [order_1_approx] starting weight calculation for If you can discover something, that thing is discoverable
If you can achieve something, that thing is achieveable
If you can expand something, that thing is expandable
If you can expect something, that thing is expectable
If you can renew something, that thing is renewable
If you can avoid something, that thing is avoidable
If you can predict something, that thing is predictable
If you can understand something, that thing is
2024-07-24 06:15:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 06:19:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7646,  0.0249, -0.1036,  ..., -0.9150,  1.2686,  0.0086],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4482,  2.6211, -4.6641,  ..., -2.6230,  4.5234,  1.7871],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0133, -0.0268, -0.0024,  ...,  0.0085, -0.0184, -0.0162],
        [-0.0033, -0.0119,  0.0120,  ...,  0.0021,  0.0047, -0.0006],
        [ 0.0075,  0.0069, -0.0086,  ...,  0.0081,  0.0176, -0.0140],
        ...,
        [-0.0271, -0.0105, -0.0058,  ..., -0.0107,  0.0081, -0.0186],
        [-0.0104, -0.0065, -0.0002,  ..., -0.0224, -0.0367,  0.0047],
        [ 0.0076,  0.0106, -0.0091,  ..., -0.0372,  0.0263, -0.0340]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3408,  2.5977, -4.8125,  ..., -2.8203,  4.6484,  1.4678]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 06:19:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can discover something, that thing is discoverable
If you can achieve something, that thing is achieveable
If you can expand something, that thing is expandable
If you can expect something, that thing is expectable
If you can renew something, that thing is renewable
If you can avoid something, that thing is avoidable
If you can predict something, that thing is predictable
If you can understand something, that thing is
2024-07-24 06:19:22 root INFO     total operator prediction time: 1803.78604388237 seconds
2024-07-24 06:19:22 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+tion_irreg
2024-07-24 06:19:22 root INFO     building operator verb+tion_irreg
2024-07-24 06:19:23 root INFO     [order_1_approx] starting weight calculation for To standardize results in standardization
To maximize results in maximization
To visualize results in visualization
To prepare results in preparation
To organize results in organization
To imagine results in imagination
To privatize results in privatization
To determine results in
2024-07-24 06:19:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 06:23:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3740,  0.3167, -0.3403,  ..., -0.2573, -0.5693,  0.0287],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.8242,  0.3623, -1.8135,  ..., -4.5273,  1.3398,  4.2812],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0047,  0.0025,  0.0019,  ..., -0.0036,  0.0079, -0.0275],
        [ 0.0058, -0.0051,  0.0091,  ..., -0.0135, -0.0145, -0.0007],
        [-0.0158, -0.0032, -0.0194,  ...,  0.0222,  0.0178, -0.0010],
        ...,
        [-0.0163,  0.0102, -0.0110,  ...,  0.0009,  0.0024,  0.0004],
        [-0.0098,  0.0166,  0.0134,  ..., -0.0059, -0.0162,  0.0075],
        [-0.0190,  0.0010,  0.0091,  ..., -0.0035,  0.0190, -0.0039]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6445,  0.0251, -1.6973,  ..., -4.6484,  2.5859,  4.8672]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 06:23:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To standardize results in standardization
To maximize results in maximization
To visualize results in visualization
To prepare results in preparation
To organize results in organization
To imagine results in imagination
To privatize results in privatization
To determine results in
2024-07-24 06:23:08 root INFO     [order_1_approx] starting weight calculation for To visualize results in visualization
To prepare results in preparation
To privatize results in privatization
To organize results in organization
To determine results in determination
To standardize results in standardization
To maximize results in maximization
To imagine results in
2024-07-24 06:23:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 06:26:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2021, -0.2786, -0.0297,  ..., -0.3274,  0.3857, -0.2939],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6182,  1.3516, -3.8809,  ...,  0.9043, -2.6621, -0.4258],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0014, -0.0085, -0.0009,  ..., -0.0111, -0.0024, -0.0157],
        [-0.0159, -0.0088,  0.0074,  ...,  0.0039, -0.0109, -0.0114],
        [ 0.0070,  0.0113, -0.0195,  ...,  0.0166,  0.0069,  0.0057],
        ...,
        [-0.0133, -0.0064, -0.0106,  ..., -0.0113, -0.0024, -0.0038],
        [-0.0017,  0.0212, -0.0167,  ..., -0.0064, -0.0066, -0.0097],
        [-0.0059,  0.0308, -0.0122,  ..., -0.0070, -0.0069, -0.0154]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8545,  1.5645, -4.2266,  ...,  0.5195, -2.3320,  0.5469]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 06:26:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To visualize results in visualization
To prepare results in preparation
To privatize results in privatization
To organize results in organization
To determine results in determination
To standardize results in standardization
To maximize results in maximization
To imagine results in
2024-07-24 06:26:54 root INFO     [order_1_approx] starting weight calculation for To privatize results in privatization
To organize results in organization
To standardize results in standardization
To determine results in determination
To visualize results in visualization
To prepare results in preparation
To imagine results in imagination
To maximize results in
2024-07-24 06:26:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 06:30:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0859, -0.0688,  0.3577,  ..., -0.9648, -0.5474, -0.8135],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.4258,  1.9102, -3.8711,  ...,  0.4775,  0.4932,  2.4512],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0109, -0.0324,  0.0014,  ..., -0.0063, -0.0100, -0.0214],
        [-0.0036, -0.0100,  0.0097,  ...,  0.0259, -0.0018,  0.0057],
        [-0.0050, -0.0077, -0.0096,  ...,  0.0050, -0.0050,  0.0064],
        ...,
        [-0.0129, -0.0189, -0.0060,  ...,  0.0087,  0.0005, -0.0078],
        [-0.0036,  0.0060,  0.0110,  ..., -0.0182,  0.0012,  0.0197],
        [-0.0073,  0.0021,  0.0117,  ...,  0.0035,  0.0184, -0.0048]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.7734,  2.4707, -4.8008,  ...,  0.1804,  0.4111,  2.2988]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 06:30:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To privatize results in privatization
To organize results in organization
To standardize results in standardization
To determine results in determination
To visualize results in visualization
To prepare results in preparation
To imagine results in imagination
To maximize results in
2024-07-24 06:30:43 root INFO     [order_1_approx] starting weight calculation for To determine results in determination
To maximize results in maximization
To imagine results in imagination
To prepare results in preparation
To standardize results in standardization
To privatize results in privatization
To visualize results in visualization
To organize results in
2024-07-24 06:30:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 06:34:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2208,  0.6289,  0.4758,  ..., -0.6631,  0.8154, -0.4131],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0625,  0.2891, -0.7397,  ..., -0.6494, -2.2617,  3.4570],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0069, -0.0122,  0.0007,  ...,  0.0088, -0.0052, -0.0172],
        [-0.0023, -0.0042,  0.0010,  ...,  0.0149,  0.0004, -0.0159],
        [-0.0070, -0.0128, -0.0209,  ..., -0.0072, -0.0052, -0.0121],
        ...,
        [-0.0126, -0.0071, -0.0086,  ..., -0.0095, -0.0149, -0.0046],
        [-0.0100,  0.0051,  0.0024,  ..., -0.0118, -0.0099,  0.0043],
        [ 0.0028,  0.0203,  0.0058,  ...,  0.0093,  0.0256, -0.0127]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0801,  0.1864, -1.0752,  ..., -0.3970, -2.6875,  4.3281]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 06:34:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To determine results in determination
To maximize results in maximization
To imagine results in imagination
To prepare results in preparation
To standardize results in standardization
To privatize results in privatization
To visualize results in visualization
To organize results in
2024-07-24 06:34:32 root INFO     [order_1_approx] starting weight calculation for To standardize results in standardization
To privatize results in privatization
To visualize results in visualization
To determine results in determination
To imagine results in imagination
To organize results in organization
To maximize results in maximization
To prepare results in
2024-07-24 06:34:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 06:38:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3047,  0.4275, -0.0809,  ...,  0.0977,  0.2505, -1.4023],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8423,  0.4741, -1.5605,  ..., -2.2227, -0.9131,  4.4414],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-6.5155e-03, -2.8305e-03,  2.4300e-03,  ..., -3.4809e-04,
          1.4648e-03, -1.9745e-02],
        [ 4.5853e-03, -7.6370e-03,  1.6985e-03,  ...,  1.2634e-02,
          3.6240e-05, -1.1158e-03],
        [-6.2180e-03,  2.8858e-03, -2.6703e-04,  ..., -2.8419e-04,
         -4.3564e-03,  1.4381e-03],
        ...,
        [-1.6693e-02, -1.4969e-02, -5.0507e-03,  ..., -8.3389e-03,
         -4.5776e-03, -6.8359e-03],
        [-8.1711e-03, -6.8665e-04, -9.6893e-04,  ..., -9.7809e-03,
         -1.7593e-02,  8.5449e-03],
        [ 8.1711e-03, -2.1935e-05,  1.5511e-02,  ..., -4.4022e-03,
          1.6174e-02, -1.1215e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3379,  0.6206, -1.8389,  ..., -2.0918, -1.5889,  4.7852]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 06:38:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To standardize results in standardization
To privatize results in privatization
To visualize results in visualization
To determine results in determination
To imagine results in imagination
To organize results in organization
To maximize results in maximization
To prepare results in
2024-07-24 06:38:18 root INFO     [order_1_approx] starting weight calculation for To prepare results in preparation
To determine results in determination
To organize results in organization
To imagine results in imagination
To standardize results in standardization
To visualize results in visualization
To maximize results in maximization
To privatize results in
2024-07-24 06:38:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 06:42:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0571, -0.3647,  0.8271,  ...,  0.2161,  0.1609,  0.3547],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3164,  0.3799, -0.8853,  ...,  0.2366, -2.5781,  1.7314],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-9.3689e-03, -1.3580e-02,  2.4796e-03,  ...,  6.4926e-03,
          1.4496e-03, -1.0323e-02],
        [ 1.0506e-02, -5.0430e-03,  2.3403e-03,  ...,  1.5961e-02,
         -1.3115e-02, -2.2831e-03],
        [ 2.3060e-03, -6.9427e-04, -1.9318e-02,  ...,  2.7714e-03,
         -4.7913e-03,  4.4479e-03],
        ...,
        [-1.0834e-02, -1.1497e-02, -1.3641e-02,  ...,  9.5081e-04,
         -7.7209e-03, -4.6768e-03],
        [-6.5575e-03,  1.4175e-02,  1.3550e-02,  ..., -1.0437e-02,
         -1.0536e-02,  1.2112e-03],
        [-9.5367e-05,  7.5798e-03,  2.7466e-04,  ..., -9.3613e-03,
          5.2757e-03, -4.9820e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2257, -0.0569, -0.7236,  ...,  0.2629, -3.3789,  2.0762]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 06:42:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To prepare results in preparation
To determine results in determination
To organize results in organization
To imagine results in imagination
To standardize results in standardization
To visualize results in visualization
To maximize results in maximization
To privatize results in
2024-07-24 06:42:03 root INFO     [order_1_approx] starting weight calculation for To imagine results in imagination
To organize results in organization
To visualize results in visualization
To determine results in determination
To prepare results in preparation
To privatize results in privatization
To maximize results in maximization
To standardize results in
2024-07-24 06:42:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 06:45:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8013,  0.2842,  0.6372,  ..., -0.1415, -0.6250, -0.3643],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0352,  1.1807, -2.7422,  ...,  0.6157,  3.4043,  4.8047],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.7962e-03, -1.0590e-02, -1.6861e-03,  ..., -1.6365e-03,
         -2.4033e-03, -9.9335e-03],
        [ 1.4984e-02, -1.9913e-02,  2.8591e-03,  ...,  9.7046e-03,
         -6.1874e-03, -9.7046e-03],
        [ 5.7907e-03, -1.2848e-02, -1.9054e-03,  ..., -1.4629e-03,
         -4.9934e-03,  3.7861e-03],
        ...,
        [-1.2421e-02,  5.9814e-03, -1.8112e-02,  ..., -1.1986e-02,
          2.0046e-03,  1.9951e-03],
        [-6.8283e-03, -7.3433e-05, -1.2188e-03,  ...,  2.0409e-03,
         -9.2621e-03,  1.7862e-03],
        [-1.2474e-03,  7.2441e-03, -4.5853e-03,  ...,  2.6436e-03,
         -4.6082e-03,  2.2087e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7363,  1.2686, -2.3750,  ...,  0.1074,  3.6992,  4.6406]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 06:45:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To imagine results in imagination
To organize results in organization
To visualize results in visualization
To determine results in determination
To prepare results in preparation
To privatize results in privatization
To maximize results in maximization
To standardize results in
2024-07-24 06:45:49 root INFO     [order_1_approx] starting weight calculation for To organize results in organization
To imagine results in imagination
To determine results in determination
To standardize results in standardization
To prepare results in preparation
To privatize results in privatization
To maximize results in maximization
To visualize results in
2024-07-24 06:45:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 06:49:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4978,  0.0952,  0.8203,  ..., -0.3569,  0.5576,  0.3684],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1016,  3.9297, -1.2500,  ..., -2.4551, -6.8672, -2.3789],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0091, -0.0137, -0.0025,  ..., -0.0144, -0.0013, -0.0112],
        [-0.0137,  0.0007, -0.0023,  ...,  0.0026, -0.0095, -0.0017],
        [-0.0042,  0.0117, -0.0204,  ...,  0.0098, -0.0058, -0.0052],
        ...,
        [-0.0141, -0.0171,  0.0006,  ..., -0.0134,  0.0022,  0.0097],
        [ 0.0033,  0.0350, -0.0099,  ..., -0.0032, -0.0131,  0.0110],
        [ 0.0105,  0.0308,  0.0021,  ..., -0.0114,  0.0092,  0.0016]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5420,  5.0742, -1.3301,  ..., -2.7910, -7.9258, -2.5840]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 06:49:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To organize results in organization
To imagine results in imagination
To determine results in determination
To standardize results in standardization
To prepare results in preparation
To privatize results in privatization
To maximize results in maximization
To visualize results in
2024-07-24 06:49:39 root INFO     total operator prediction time: 1816.7151062488556 seconds
2024-07-24 06:49:39 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on adj+ly_reg
2024-07-24 06:49:39 root INFO     building operator adj+ly_reg
2024-07-24 06:49:39 root INFO     [order_1_approx] starting weight calculation for The adjective form of strong is strongly
The adjective form of obvious is obviously
The adjective form of political is politically
The adjective form of global is globally
The adjective form of huge is hugely
The adjective form of mental is mentally
The adjective form of regional is regionally
The adjective form of clinical is
2024-07-24 06:49:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 06:53:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6729,  0.7969, -0.0745,  ...,  0.3738,  0.2949,  0.0605],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5767, -0.4131,  0.9746,  ..., -1.3887,  0.7656,  0.7192],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0227,  0.0141,  0.0149,  ..., -0.0103, -0.0043, -0.0071],
        [-0.0095, -0.0228,  0.0176,  ...,  0.0054, -0.0075,  0.0142],
        [-0.0378, -0.0138,  0.0032,  ...,  0.0122, -0.0015,  0.0255],
        ...,
        [-0.0186, -0.0031, -0.0090,  ..., -0.0213,  0.0070, -0.0120],
        [-0.0032,  0.0080,  0.0012,  ..., -0.0145, -0.0244,  0.0101],
        [-0.0011, -0.0038,  0.0085,  ..., -0.0074,  0.0130, -0.0384]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2520, -1.3164,  0.3628,  ..., -1.2900,  1.1338,  0.2266]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 06:53:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of strong is strongly
The adjective form of obvious is obviously
The adjective form of political is politically
The adjective form of global is globally
The adjective form of huge is hugely
The adjective form of mental is mentally
The adjective form of regional is regionally
The adjective form of clinical is
2024-07-24 06:53:29 root INFO     [order_1_approx] starting weight calculation for The adjective form of strong is strongly
The adjective form of political is politically
The adjective form of mental is mentally
The adjective form of clinical is clinically
The adjective form of huge is hugely
The adjective form of regional is regionally
The adjective form of obvious is obviously
The adjective form of global is
2024-07-24 06:53:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 06:57:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4199,  0.6611,  1.0166,  ..., -0.8940,  0.5508,  0.3250],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5654,  2.6113,  0.5586,  ...,  1.3975, -1.2363,  0.9355],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0318, -0.0048,  0.0075,  ..., -0.0201,  0.0052, -0.0100],
        [-0.0041, -0.0157,  0.0096,  ...,  0.0042, -0.0130,  0.0154],
        [-0.0088,  0.0071, -0.0186,  ...,  0.0179, -0.0109,  0.0371],
        ...,
        [-0.0171, -0.0132,  0.0067,  ..., -0.0081,  0.0054, -0.0092],
        [-0.0132,  0.0026,  0.0017,  ..., -0.0042, -0.0136, -0.0100],
        [-0.0058,  0.0092, -0.0006,  ..., -0.0045,  0.0095, -0.0289]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3906,  3.2090,  0.0142,  ...,  0.9707, -1.3848,  1.6270]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 06:57:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of strong is strongly
The adjective form of political is politically
The adjective form of mental is mentally
The adjective form of clinical is clinically
The adjective form of huge is hugely
The adjective form of regional is regionally
The adjective form of obvious is obviously
The adjective form of global is
2024-07-24 06:57:17 root INFO     [order_1_approx] starting weight calculation for The adjective form of mental is mentally
The adjective form of regional is regionally
The adjective form of clinical is clinically
The adjective form of political is politically
The adjective form of strong is strongly
The adjective form of global is globally
The adjective form of obvious is obviously
The adjective form of huge is
2024-07-24 06:57:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 07:00:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1504,  0.0381,  0.2201,  ..., -0.0106,  0.9766,  0.7783],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7812,  0.4741,  0.4785,  ..., -0.9062,  2.0703,  4.9727],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0327,  0.0028,  0.0045,  ..., -0.0107, -0.0280, -0.0071],
        [-0.0002, -0.0180,  0.0003,  ...,  0.0129, -0.0298,  0.0021],
        [ 0.0047, -0.0024, -0.0258,  ...,  0.0072,  0.0021,  0.0211],
        ...,
        [-0.0103, -0.0150,  0.0120,  ..., -0.0220, -0.0007, -0.0141],
        [-0.0217,  0.0054,  0.0066,  ..., -0.0145, -0.0022, -0.0283],
        [-0.0153,  0.0116, -0.0097,  ..., -0.0022, -0.0047, -0.0282]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1152,  0.1353, -0.7852,  ..., -0.5728,  3.1191,  5.5430]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 07:00:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of mental is mentally
The adjective form of regional is regionally
The adjective form of clinical is clinically
The adjective form of political is politically
The adjective form of strong is strongly
The adjective form of global is globally
The adjective form of obvious is obviously
The adjective form of huge is
2024-07-24 07:00:58 root INFO     [order_1_approx] starting weight calculation for The adjective form of strong is strongly
The adjective form of obvious is obviously
The adjective form of regional is regionally
The adjective form of huge is hugely
The adjective form of political is politically
The adjective form of clinical is clinically
The adjective form of global is globally
The adjective form of mental is
2024-07-24 07:00:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 07:04:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5693,  0.4512, -0.5562,  ...,  0.4636,  0.5913,  0.1333],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3320, -0.0764, -1.2031,  ...,  1.0928,  2.3203,  1.6367],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.6113e-02, -4.7607e-03,  1.8311e-02,  ..., -2.6215e-02,
          3.9368e-03, -2.6398e-03],
        [-4.5776e-03,  7.8430e-03,  9.2239e-03,  ..., -5.3406e-05,
          1.7090e-03,  1.7532e-02],
        [ 5.9586e-03,  8.7128e-03, -2.2400e-02,  ...,  2.1942e-02,
          3.4332e-03,  1.1917e-02],
        ...,
        [-3.5614e-02, -1.1230e-02, -1.6281e-02,  ..., -2.0081e-02,
          1.3962e-03, -2.1591e-02],
        [-1.6113e-02, -1.7670e-02,  2.7130e-02,  ..., -1.4862e-02,
         -3.5095e-02,  5.2528e-03],
        [ 2.0294e-03,  1.4458e-03,  1.4372e-03,  ..., -2.3758e-02,
          2.5696e-02, -3.4790e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.8379, -0.8164, -1.6016,  ...,  0.1338,  2.6758,  2.0488]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 07:04:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of strong is strongly
The adjective form of obvious is obviously
The adjective form of regional is regionally
The adjective form of huge is hugely
The adjective form of political is politically
The adjective form of clinical is clinically
The adjective form of global is globally
The adjective form of mental is
2024-07-24 07:04:43 root INFO     [order_1_approx] starting weight calculation for The adjective form of strong is strongly
The adjective form of mental is mentally
The adjective form of huge is hugely
The adjective form of political is politically
The adjective form of global is globally
The adjective form of clinical is clinically
The adjective form of regional is regionally
The adjective form of obvious is
2024-07-24 07:04:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 07:08:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1514, -0.5947,  0.2098,  ..., -0.7197, -0.1738,  0.3337],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 7.7734,  5.0703, -3.0000,  ...,  0.8403, -1.4668,  2.7402],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0204, -0.0226,  0.0118,  ..., -0.0203, -0.0027,  0.0025],
        [-0.0145, -0.0224,  0.0084,  ...,  0.0013,  0.0030,  0.0012],
        [-0.0066, -0.0144, -0.0294,  ...,  0.0145,  0.0033,  0.0049],
        ...,
        [-0.0047, -0.0261, -0.0196,  ..., -0.0378, -0.0091, -0.0191],
        [-0.0130,  0.0078, -0.0010,  ...,  0.0065, -0.0241,  0.0014],
        [-0.0093,  0.0192, -0.0131,  ...,  0.0039,  0.0073, -0.0535]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 7.5391,  4.5977, -3.1973,  ...,  0.9009, -1.5068,  2.8398]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 07:08:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of strong is strongly
The adjective form of mental is mentally
The adjective form of huge is hugely
The adjective form of political is politically
The adjective form of global is globally
The adjective form of clinical is clinically
The adjective form of regional is regionally
The adjective form of obvious is
2024-07-24 07:08:31 root INFO     [order_1_approx] starting weight calculation for The adjective form of clinical is clinically
The adjective form of huge is hugely
The adjective form of strong is strongly
The adjective form of mental is mentally
The adjective form of regional is regionally
The adjective form of global is globally
The adjective form of obvious is obviously
The adjective form of political is
2024-07-24 07:08:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 07:12:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2842,  0.7505,  0.1367,  ...,  0.1074, -0.2607,  0.2268],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.1328, -2.1367,  2.2441,  ..., -1.9121, -3.0078,  1.5459],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0167, -0.0102,  0.0093,  ..., -0.0143, -0.0060, -0.0060],
        [ 0.0041, -0.0099,  0.0015,  ..., -0.0014, -0.0124, -0.0025],
        [-0.0091, -0.0080, -0.0156,  ...,  0.0070, -0.0192,  0.0272],
        ...,
        [-0.0286, -0.0126,  0.0004,  ..., -0.0330,  0.0086, -0.0133],
        [-0.0071,  0.0067,  0.0045,  ..., -0.0131, -0.0175,  0.0163],
        [-0.0211,  0.0082, -0.0025,  ..., -0.0123,  0.0239, -0.0359]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.0938, -1.9268,  2.2656,  ..., -2.0996, -2.7422,  1.6465]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 07:12:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of clinical is clinically
The adjective form of huge is hugely
The adjective form of strong is strongly
The adjective form of mental is mentally
The adjective form of regional is regionally
The adjective form of global is globally
The adjective form of obvious is obviously
The adjective form of political is
2024-07-24 07:12:15 root INFO     [order_1_approx] starting weight calculation for The adjective form of global is globally
The adjective form of mental is mentally
The adjective form of huge is hugely
The adjective form of obvious is obviously
The adjective form of political is politically
The adjective form of clinical is clinically
The adjective form of strong is strongly
The adjective form of regional is
2024-07-24 07:12:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 07:15:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1565,  0.9014, -0.4895,  ..., -0.4792,  0.8682,  0.6245],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8306,  1.2236, -2.1309,  ...,  2.5547,  0.0078,  3.3574],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.8738e-02,  7.4120e-03,  9.7046e-03,  ..., -1.4999e-02,
         -6.1798e-03, -5.3978e-03],
        [-7.7209e-03,  5.2490e-03,  7.1487e-03,  ..., -2.2354e-03,
          5.4588e-03,  1.0475e-02],
        [-9.4681e-03, -1.1726e-02, -4.4289e-03,  ...,  2.0233e-02,
          2.2888e-05,  1.2001e-02],
        ...,
        [-4.4342e-02, -1.3771e-02, -2.2766e-02,  ..., -2.4857e-02,
          1.9684e-02, -1.0391e-02],
        [-5.9013e-03,  1.2802e-02,  1.0262e-02,  ..., -2.6855e-02,
         -3.3081e-02,  1.9180e-02],
        [ 8.0109e-03,  5.3787e-04, -5.8594e-03,  ..., -1.5320e-02,
          1.5244e-02, -5.0354e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5127,  1.2295, -1.4844,  ...,  2.7949,  0.0735,  4.0859]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 07:15:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of global is globally
The adjective form of mental is mentally
The adjective form of huge is hugely
The adjective form of obvious is obviously
The adjective form of political is politically
The adjective form of clinical is clinically
The adjective form of strong is strongly
The adjective form of regional is
2024-07-24 07:16:00 root INFO     [order_1_approx] starting weight calculation for The adjective form of global is globally
The adjective form of political is politically
The adjective form of mental is mentally
The adjective form of regional is regionally
The adjective form of huge is hugely
The adjective form of clinical is clinically
The adjective form of obvious is obviously
The adjective form of strong is
2024-07-24 07:16:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 07:19:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1230, -0.3521,  0.7178,  ...,  0.5317,  0.7168,  0.0620],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8320, -0.7788,  0.7119,  ...,  2.1621, -1.2812,  0.2891],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0174, -0.0020,  0.0011,  ..., -0.0137, -0.0110, -0.0109],
        [-0.0019, -0.0136,  0.0038,  ..., -0.0009, -0.0160,  0.0060],
        [-0.0090, -0.0048,  0.0050,  ...,  0.0052,  0.0154,  0.0243],
        ...,
        [-0.0187, -0.0251,  0.0018,  ..., -0.0196, -0.0152,  0.0046],
        [-0.0054,  0.0159,  0.0074,  ..., -0.0011, -0.0134,  0.0010],
        [-0.0081,  0.0169,  0.0037,  ..., -0.0167,  0.0052, -0.0276]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1055, -1.2773, -0.1729,  ...,  2.1934, -0.5317,  0.0632]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 07:19:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of global is globally
The adjective form of political is politically
The adjective form of mental is mentally
The adjective form of regional is regionally
The adjective form of huge is hugely
The adjective form of clinical is clinically
The adjective form of obvious is obviously
The adjective form of strong is
2024-07-24 07:19:45 root INFO     total operator prediction time: 1805.8393263816833 seconds
2024-07-24 07:19:45 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on over+adj_reg
2024-07-24 07:19:45 root INFO     building operator over+adj_reg
2024-07-24 07:19:45 root INFO     [order_1_approx] starting weight calculation for If something is too stressed, it is overstressed
If something is too turned, it is overturned
If something is too heated, it is overheated
If something is too spent, it is overspent
If something is too enthusiastic, it is overenthusiastic
If something is too optimistic, it is overoptimistic
If something is too represented, it is overrepresented
If something is too confident, it is
2024-07-24 07:19:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 07:23:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6162,  0.1173, -0.8711,  ..., -0.4409,  0.9434, -0.4663],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8633, -2.3242, -3.4629,  ..., -0.9131,  1.1016, -0.2441],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0153, -0.0147,  0.0175,  ..., -0.0049,  0.0008, -0.0139],
        [ 0.0046, -0.0068,  0.0067,  ...,  0.0132,  0.0083,  0.0018],
        [-0.0086,  0.0171, -0.0192,  ...,  0.0016,  0.0080,  0.0133],
        ...,
        [-0.0081, -0.0307, -0.0222,  ..., -0.0165, -0.0031, -0.0099],
        [ 0.0111, -0.0069, -0.0106,  ..., -0.0226, -0.0250,  0.0258],
        [ 0.0045,  0.0038,  0.0182,  ..., -0.0083, -0.0032, -0.0380]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0820, -3.0586, -4.0117,  ...,  0.3877,  0.5371,  0.0156]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 07:23:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too stressed, it is overstressed
If something is too turned, it is overturned
If something is too heated, it is overheated
If something is too spent, it is overspent
If something is too enthusiastic, it is overenthusiastic
If something is too optimistic, it is overoptimistic
If something is too represented, it is overrepresented
If something is too confident, it is
2024-07-24 07:23:29 root INFO     [order_1_approx] starting weight calculation for If something is too confident, it is overconfident
If something is too heated, it is overheated
If something is too spent, it is overspent
If something is too stressed, it is overstressed
If something is too optimistic, it is overoptimistic
If something is too represented, it is overrepresented
If something is too turned, it is overturned
If something is too enthusiastic, it is
2024-07-24 07:23:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 07:27:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1348,  0.1011,  1.0078,  ..., -0.2876,  1.8076,  0.1342],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.2676, -1.5928, -0.1807,  ...,  1.0186,  4.2422,  0.3438],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0036, -0.0417,  0.0170,  ...,  0.0178, -0.0113, -0.0108],
        [-0.0190, -0.0200,  0.0089,  ...,  0.0215, -0.0045,  0.0091],
        [ 0.0070, -0.0034, -0.0294,  ..., -0.0178,  0.0143,  0.0021],
        ...,
        [-0.0318, -0.0386, -0.0115,  ...,  0.0014, -0.0004, -0.0022],
        [-0.0061, -0.0084, -0.0019,  ..., -0.0023, -0.0306, -0.0176],
        [ 0.0103,  0.0030,  0.0268,  ..., -0.0052, -0.0020, -0.0206]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.9688, -1.8506, -0.6826,  ...,  2.1777,  4.2227,  0.5176]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 07:27:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too confident, it is overconfident
If something is too heated, it is overheated
If something is too spent, it is overspent
If something is too stressed, it is overstressed
If something is too optimistic, it is overoptimistic
If something is too represented, it is overrepresented
If something is too turned, it is overturned
If something is too enthusiastic, it is
2024-07-24 07:27:19 root INFO     [order_1_approx] starting weight calculation for If something is too turned, it is overturned
If something is too enthusiastic, it is overenthusiastic
If something is too stressed, it is overstressed
If something is too represented, it is overrepresented
If something is too spent, it is overspent
If something is too confident, it is overconfident
If something is too optimistic, it is overoptimistic
If something is too heated, it is
2024-07-24 07:27:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 07:31:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8984,  0.9058,  0.6255,  ..., -0.8721,  1.7520,  0.6050],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4863, -0.2729, -1.8867,  ..., -0.1709,  2.9980,  3.4375],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0063, -0.0120,  0.0153,  ..., -0.0051, -0.0190, -0.0210],
        [-0.0139,  0.0042, -0.0041,  ...,  0.0100, -0.0014, -0.0036],
        [-0.0126,  0.0170,  0.0075,  ...,  0.0172,  0.0186,  0.0009],
        ...,
        [-0.0159, -0.0111, -0.0122,  ..., -0.0008,  0.0066,  0.0129],
        [-0.0003,  0.0109,  0.0106,  ..., -0.0061, -0.0385, -0.0036],
        [-0.0107,  0.0003,  0.0013,  ..., -0.0188, -0.0070, -0.0310]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.5449,  0.1541, -3.0625,  ...,  0.4478,  2.2969,  3.8750]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 07:31:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too turned, it is overturned
If something is too enthusiastic, it is overenthusiastic
If something is too stressed, it is overstressed
If something is too represented, it is overrepresented
If something is too spent, it is overspent
If something is too confident, it is overconfident
If something is too optimistic, it is overoptimistic
If something is too heated, it is
2024-07-24 07:31:01 root INFO     [order_1_approx] starting weight calculation for If something is too spent, it is overspent
If something is too heated, it is overheated
If something is too enthusiastic, it is overenthusiastic
If something is too stressed, it is overstressed
If something is too confident, it is overconfident
If something is too represented, it is overrepresented
If something is too turned, it is overturned
If something is too optimistic, it is
2024-07-24 07:31:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 07:34:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1399, -0.2681,  0.2634,  ..., -0.7266,  2.1914,  0.3918],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.8281, -0.6689, -1.9648,  ...,  0.2061, -0.5557,  0.8740],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0013, -0.0404,  0.0140,  ...,  0.0070, -0.0142, -0.0116],
        [-0.0107, -0.0267,  0.0209,  ...,  0.0148,  0.0004,  0.0053],
        [ 0.0037,  0.0230, -0.0319,  ...,  0.0026, -0.0015,  0.0318],
        ...,
        [-0.0132, -0.0394, -0.0118,  ..., -0.0032, -0.0129, -0.0166],
        [-0.0066, -0.0102, -0.0196,  ..., -0.0259, -0.0336,  0.0193],
        [ 0.0045,  0.0098,  0.0158,  ..., -0.0152,  0.0070, -0.0490]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.0664, -0.3447, -2.3789,  ...,  1.2354, -0.7080,  0.9390]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 07:34:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too spent, it is overspent
If something is too heated, it is overheated
If something is too enthusiastic, it is overenthusiastic
If something is too stressed, it is overstressed
If something is too confident, it is overconfident
If something is too represented, it is overrepresented
If something is too turned, it is overturned
If something is too optimistic, it is
2024-07-24 07:34:38 root INFO     [order_1_approx] starting weight calculation for If something is too enthusiastic, it is overenthusiastic
If something is too optimistic, it is overoptimistic
If something is too spent, it is overspent
If something is too confident, it is overconfident
If something is too stressed, it is overstressed
If something is too turned, it is overturned
If something is too heated, it is overheated
If something is too represented, it is
2024-07-24 07:34:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 07:38:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0013,  0.4329,  0.4026,  ..., -0.7681,  1.1035,  0.2360],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0303, -0.3950, -2.6055,  ...,  0.8906,  1.9980,  0.5254],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0044, -0.0084, -0.0003,  ..., -0.0197, -0.0052, -0.0060],
        [ 0.0058, -0.0065,  0.0182,  ..., -0.0067, -0.0031, -0.0107],
        [ 0.0068,  0.0138, -0.0118,  ..., -0.0179,  0.0007, -0.0016],
        ...,
        [-0.0129, -0.0072,  0.0010,  ..., -0.0053, -0.0142, -0.0047],
        [-0.0080, -0.0193, -0.0048,  ..., -0.0264, -0.0068, -0.0056],
        [ 0.0075,  0.0073, -0.0077,  ..., -0.0092,  0.0065, -0.0033]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5347, -0.8398, -1.7480,  ...,  1.2129,  2.2637,  0.5581]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 07:38:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too enthusiastic, it is overenthusiastic
If something is too optimistic, it is overoptimistic
If something is too spent, it is overspent
If something is too confident, it is overconfident
If something is too stressed, it is overstressed
If something is too turned, it is overturned
If something is too heated, it is overheated
If something is too represented, it is
2024-07-24 07:38:16 root INFO     [order_1_approx] starting weight calculation for If something is too represented, it is overrepresented
If something is too stressed, it is overstressed
If something is too heated, it is overheated
If something is too enthusiastic, it is overenthusiastic
If something is too turned, it is overturned
If something is too optimistic, it is overoptimistic
If something is too confident, it is overconfident
If something is too spent, it is
2024-07-24 07:38:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 07:41:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7910,  0.3259,  0.3350,  ..., -0.4373,  0.5811, -0.2517],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8174,  0.4280, -0.1211,  ...,  1.1240,  1.1328, -0.2856],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0106, -0.0206,  0.0119,  ...,  0.0089, -0.0199, -0.0132],
        [-0.0002, -0.0004,  0.0049,  ...,  0.0079, -0.0045,  0.0090],
        [ 0.0115,  0.0231, -0.0105,  ...,  0.0099,  0.0044,  0.0100],
        ...,
        [-0.0145, -0.0126, -0.0104,  ..., -0.0088, -0.0001, -0.0129],
        [-0.0040, -0.0105,  0.0191,  ...,  0.0137, -0.0060,  0.0106],
        [-0.0093,  0.0202,  0.0068,  ..., -0.0112,  0.0194, -0.0219]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4766,  1.7969, -0.8433,  ...,  0.4619,  1.0156,  0.1755]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 07:41:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too represented, it is overrepresented
If something is too stressed, it is overstressed
If something is too heated, it is overheated
If something is too enthusiastic, it is overenthusiastic
If something is too turned, it is overturned
If something is too optimistic, it is overoptimistic
If something is too confident, it is overconfident
If something is too spent, it is
2024-07-24 07:41:59 root INFO     [order_1_approx] starting weight calculation for If something is too turned, it is overturned
If something is too enthusiastic, it is overenthusiastic
If something is too optimistic, it is overoptimistic
If something is too spent, it is overspent
If something is too heated, it is overheated
If something is too confident, it is overconfident
If something is too represented, it is overrepresented
If something is too stressed, it is
2024-07-24 07:41:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 07:45:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2869,  1.4375,  1.6348,  ...,  0.2793,  1.3320, -0.0942],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4341, -0.7822, -1.4824,  ..., -0.8408,  3.8223,  2.6191],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-5.2872e-03, -5.6000e-03,  1.1787e-02,  ...,  1.1719e-02,
         -1.4290e-02, -2.9373e-03],
        [-1.1196e-03,  1.7090e-03, -3.9291e-04,  ...,  3.1586e-03,
         -1.2711e-02,  1.5259e-05],
        [ 3.1357e-03,  2.2491e-02,  7.6981e-03,  ...,  1.4870e-02,
         -1.0986e-03,  8.3618e-03],
        ...,
        [-2.5070e-02, -1.2100e-02, -1.8921e-02,  ..., -2.4582e-02,
          2.0599e-03, -6.9656e-03],
        [-1.8021e-02,  5.2795e-03, -5.2910e-03,  ...,  4.6654e-03,
         -2.7405e-02,  2.4757e-03],
        [ 3.7537e-03, -8.6060e-03, -2.0962e-03,  ..., -2.7878e-02,
         -1.2955e-02, -1.9180e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1650, -0.5654, -2.5547,  ..., -0.6724,  4.0547,  2.8535]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 07:45:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too turned, it is overturned
If something is too enthusiastic, it is overenthusiastic
If something is too optimistic, it is overoptimistic
If something is too spent, it is overspent
If something is too heated, it is overheated
If something is too confident, it is overconfident
If something is too represented, it is overrepresented
If something is too stressed, it is
2024-07-24 07:45:37 root INFO     [order_1_approx] starting weight calculation for If something is too represented, it is overrepresented
If something is too stressed, it is overstressed
If something is too spent, it is overspent
If something is too optimistic, it is overoptimistic
If something is too enthusiastic, it is overenthusiastic
If something is too confident, it is overconfident
If something is too heated, it is overheated
If something is too turned, it is
2024-07-24 07:45:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 07:49:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3169,  0.9355,  0.5430,  ...,  0.7900,  1.2988, -0.7075],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1953, -0.5962, -1.3184,  ...,  0.3896,  3.2598, -0.4839],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-6.8550e-03,  9.7198e-03,  1.4477e-03,  ...,  2.4300e-03,
         -4.8447e-03, -1.0063e-02],
        [-9.6970e-03,  2.4433e-03,  1.2619e-02,  ..., -5.7907e-03,
          5.9967e-03,  9.4528e-03],
        [-6.0921e-03,  6.4011e-03, -2.5711e-02,  ..., -1.4473e-02,
          2.1622e-02,  3.1128e-02],
        ...,
        [ 3.1528e-03, -2.7527e-02, -3.1261e-03,  ...,  3.3531e-03,
         -3.0746e-03,  7.2479e-05],
        [-1.6907e-02,  6.6147e-03,  1.4404e-02,  ...,  2.7657e-03,
         -1.0620e-02,  1.2833e-02],
        [-7.7248e-03,  1.3199e-02,  1.0902e-02,  ..., -1.1879e-02,
         -1.4488e-02, -2.8839e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2954, -0.8008, -1.2051,  ...,  0.3811,  4.2930, -0.1685]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 07:49:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too represented, it is overrepresented
If something is too stressed, it is overstressed
If something is too spent, it is overspent
If something is too optimistic, it is overoptimistic
If something is too enthusiastic, it is overenthusiastic
If something is too confident, it is overconfident
If something is too heated, it is overheated
If something is too turned, it is
2024-07-24 07:49:23 root INFO     total operator prediction time: 1777.507064819336 seconds
2024-07-24 07:49:23 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+er_irreg
2024-07-24 07:49:23 root INFO     building operator verb+er_irreg
2024-07-24 07:49:23 root INFO     [order_1_approx] starting weight calculation for If you determine something, you are a determiner
If you promote something, you are a promoter
If you offend something, you are a offender
If you tell something, you are a teller
If you provide something, you are a provider
If you listen something, you are a listener
If you borrow something, you are a borrower
If you announce something, you are a
2024-07-24 07:49:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 07:53:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.6191,  0.8398,  1.2158,  ..., -0.1792, -0.4629,  1.4160],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0177,  1.1240, -4.7031,  ..., -1.0215,  8.0000,  6.0117],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0054,  0.0066,  0.0072,  ...,  0.0261, -0.0006, -0.0071],
        [-0.0161, -0.0125,  0.0081,  ...,  0.0114, -0.0037,  0.0001],
        [ 0.0196,  0.0249, -0.0011,  ...,  0.0279,  0.0015,  0.0172],
        ...,
        [-0.0028, -0.0122,  0.0033,  ..., -0.0152,  0.0059, -0.0079],
        [ 0.0076,  0.0002,  0.0077,  ..., -0.0018, -0.0210,  0.0061],
        [ 0.0106, -0.0045,  0.0056,  ..., -0.0014,  0.0094, -0.0031]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1052,  1.1748, -5.5234,  ..., -0.0615,  8.5859,  6.1797]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 07:53:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you determine something, you are a determiner
If you promote something, you are a promoter
If you offend something, you are a offender
If you tell something, you are a teller
If you provide something, you are a provider
If you listen something, you are a listener
If you borrow something, you are a borrower
If you announce something, you are a
2024-07-24 07:53:03 root INFO     [order_1_approx] starting weight calculation for If you announce something, you are a announcer
If you provide something, you are a provider
If you tell something, you are a teller
If you offend something, you are a offender
If you listen something, you are a listener
If you promote something, you are a promoter
If you determine something, you are a determiner
If you borrow something, you are a
2024-07-24 07:53:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 07:56:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9326, -1.4277,  0.5518,  ..., -0.2026, -1.2217,  1.0967],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 6.4961,  0.4043, -1.8711,  ..., -2.2031,  0.8408,  5.6602],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.6251e-03,  7.9117e-03,  2.6611e-02,  ...,  7.4081e-03,
          7.3090e-03, -2.0630e-02],
        [-5.2414e-03,  9.3269e-04,  5.9662e-03,  ...,  2.1553e-04,
         -1.1802e-04,  1.6832e-03],
        [-1.5030e-02,  1.1398e-02, -5.3749e-03,  ...,  1.0254e-02,
         -1.0994e-02,  1.8066e-02],
        ...,
        [ 5.7220e-04, -1.1101e-02, -7.8125e-03,  ..., -1.2207e-02,
          1.2070e-02, -7.3624e-04],
        [ 9.6893e-03, -8.0872e-03, -9.1934e-04,  ...,  8.4229e-03,
         -2.1744e-02,  3.2158e-03],
        [-3.4332e-05, -1.4908e-02,  2.1248e-03,  ..., -1.0269e-02,
          1.9653e-02, -1.5274e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.2422,  0.4731, -1.7500,  ..., -2.4102,  1.0049,  5.6211]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 07:56:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you announce something, you are a announcer
If you provide something, you are a provider
If you tell something, you are a teller
If you offend something, you are a offender
If you listen something, you are a listener
If you promote something, you are a promoter
If you determine something, you are a determiner
If you borrow something, you are a
2024-07-24 07:56:50 root INFO     [order_1_approx] starting weight calculation for If you offend something, you are a offender
If you borrow something, you are a borrower
If you promote something, you are a promoter
If you tell something, you are a teller
If you provide something, you are a provider
If you announce something, you are a announcer
If you listen something, you are a listener
If you determine something, you are a
2024-07-24 07:56:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 08:00:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6270,  0.4805, -0.4258,  ..., -0.1514, -0.8481,  0.8169],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 6.1797, -2.7598, -3.2109,  ..., -7.6836,  2.3789,  3.6426],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-7.8888e-03, -5.5161e-03,  1.5656e-02,  ...,  2.0065e-03,
         -2.6703e-04, -1.0811e-02],
        [-8.3618e-03,  6.8817e-03,  8.1024e-03,  ...,  1.4481e-02,
         -3.6049e-03,  3.7155e-03],
        [-4.5624e-03,  3.5522e-02, -9.7504e-03,  ...,  8.8882e-04,
          1.8196e-03,  1.4275e-02],
        ...,
        [-1.6876e-02, -1.2207e-04, -9.5520e-03,  ..., -1.2344e-02,
          3.8223e-03,  5.0735e-03],
        [ 2.0981e-05, -2.2259e-03,  8.7738e-04,  ..., -9.8267e-03,
         -2.4628e-02,  6.8588e-03],
        [ 1.1124e-02, -7.2517e-03,  8.0109e-05,  ..., -2.1400e-03,
          7.4234e-03, -6.8092e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.6133, -2.3379, -3.0254,  ..., -6.9648,  2.4160,  3.0410]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 08:00:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you offend something, you are a offender
If you borrow something, you are a borrower
If you promote something, you are a promoter
If you tell something, you are a teller
If you provide something, you are a provider
If you announce something, you are a announcer
If you listen something, you are a listener
If you determine something, you are a
2024-07-24 08:00:35 root INFO     [order_1_approx] starting weight calculation for If you announce something, you are a announcer
If you tell something, you are a teller
If you determine something, you are a determiner
If you provide something, you are a provider
If you offend something, you are a offender
If you promote something, you are a promoter
If you borrow something, you are a borrower
If you listen something, you are a
2024-07-24 08:00:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 08:04:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8828, -0.2419,  1.2246,  ..., -0.6035,  0.2183,  0.7119],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4739,  3.8398, -2.0938,  ..., -6.0352,  6.8164,  3.2461],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0107, -0.0005,  0.0069,  ..., -0.0042, -0.0111, -0.0138],
        [ 0.0008, -0.0122,  0.0049,  ..., -0.0020,  0.0081, -0.0186],
        [-0.0066,  0.0110,  0.0095,  ...,  0.0044, -0.0075,  0.0071],
        ...,
        [-0.0190, -0.0067, -0.0072,  ..., -0.0048, -0.0051, -0.0005],
        [ 0.0064, -0.0051,  0.0033,  ...,  0.0126, -0.0250, -0.0024],
        [-0.0004,  0.0182, -0.0057,  ..., -0.0210, -0.0013,  0.0197]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7627,  4.0820, -2.1230,  ..., -5.7695,  6.9062,  2.9668]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 08:04:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you announce something, you are a announcer
If you tell something, you are a teller
If you determine something, you are a determiner
If you provide something, you are a provider
If you offend something, you are a offender
If you promote something, you are a promoter
If you borrow something, you are a borrower
If you listen something, you are a
2024-07-24 08:04:23 root INFO     [order_1_approx] starting weight calculation for If you listen something, you are a listener
If you provide something, you are a provider
If you announce something, you are a announcer
If you borrow something, you are a borrower
If you tell something, you are a teller
If you promote something, you are a promoter
If you determine something, you are a determiner
If you offend something, you are a
2024-07-24 08:04:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 08:08:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8286, -0.5259,  0.5068,  ..., -0.1265,  0.4756,  0.6372],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4382,  2.2930, -2.2070,  ..., -0.7300,  5.6094,  0.2920],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 5.7068e-03,  8.4686e-04,  1.3138e-02,  ...,  1.1612e-02,
         -3.5954e-04,  8.5678e-03],
        [-3.0880e-03, -7.9727e-03,  8.0109e-05,  ...,  8.5297e-03,
         -7.2479e-03,  3.7231e-03],
        [ 8.4972e-04,  1.6403e-02, -7.7896e-03,  ...,  1.5808e-02,
         -5.6076e-03, -6.0959e-03],
        ...,
        [-5.2414e-03, -1.0330e-02, -1.8005e-03,  ..., -1.4572e-02,
          1.1787e-02, -2.3766e-03],
        [-2.4872e-03,  2.9335e-03,  1.1208e-02,  ...,  1.6632e-03,
         -2.8107e-02,  4.1962e-03],
        [ 2.4757e-03,  9.8343e-03, -9.1553e-04,  ..., -1.4618e-02,
          7.6828e-03, -1.3252e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4583,  2.3730, -2.5059,  ..., -0.8584,  5.6016,  0.4736]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 08:08:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you listen something, you are a listener
If you provide something, you are a provider
If you announce something, you are a announcer
If you borrow something, you are a borrower
If you tell something, you are a teller
If you promote something, you are a promoter
If you determine something, you are a determiner
If you offend something, you are a
2024-07-24 08:08:11 root INFO     [order_1_approx] starting weight calculation for If you offend something, you are a offender
If you borrow something, you are a borrower
If you determine something, you are a determiner
If you listen something, you are a listener
If you announce something, you are a announcer
If you provide something, you are a provider
If you tell something, you are a teller
If you promote something, you are a
2024-07-24 08:08:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 08:12:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7969,  0.4353,  0.0226,  ..., -0.4668, -0.5479,  0.6040],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.2188, -0.5220, -4.2461,  ..., -1.9727,  0.6377,  2.3242],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0109, -0.0140,  0.0078,  ...,  0.0001, -0.0027, -0.0127],
        [-0.0072, -0.0079,  0.0169,  ...,  0.0126, -0.0008,  0.0062],
        [ 0.0234,  0.0047,  0.0070,  ...,  0.0089,  0.0180,  0.0135],
        ...,
        [ 0.0019, -0.0110, -0.0071,  ..., -0.0142, -0.0027,  0.0073],
        [ 0.0019,  0.0041, -0.0052,  ..., -0.0238, -0.0226,  0.0209],
        [ 0.0182, -0.0065,  0.0112,  ..., -0.0064,  0.0094, -0.0068]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.9766, -0.4741, -4.3398,  ..., -2.3340, -0.3467,  1.9502]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 08:12:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you offend something, you are a offender
If you borrow something, you are a borrower
If you determine something, you are a determiner
If you listen something, you are a listener
If you announce something, you are a announcer
If you provide something, you are a provider
If you tell something, you are a teller
If you promote something, you are a
2024-07-24 08:12:02 root INFO     [order_1_approx] starting weight calculation for If you offend something, you are a offender
If you borrow something, you are a borrower
If you announce something, you are a announcer
If you tell something, you are a teller
If you listen something, you are a listener
If you promote something, you are a promoter
If you determine something, you are a determiner
If you provide something, you are a
2024-07-24 08:12:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 08:15:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5938,  1.0342, -0.6670,  ..., -0.4744,  0.0615,  0.7212],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.5938,  1.6982, -0.0349,  ..., -7.4023,  0.7568,  0.8135],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0010, -0.0040,  0.0220,  ...,  0.0141, -0.0081, -0.0087],
        [-0.0012, -0.0058,  0.0108,  ...,  0.0008,  0.0019, -0.0073],
        [-0.0062,  0.0071, -0.0032,  ...,  0.0076,  0.0035, -0.0167],
        ...,
        [ 0.0007,  0.0201, -0.0253,  ..., -0.0046,  0.0057,  0.0113],
        [ 0.0193,  0.0129, -0.0181,  ...,  0.0007, -0.0203,  0.0147],
        [ 0.0078,  0.0116, -0.0007,  ..., -0.0111,  0.0041, -0.0079]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.8770,  2.2324,  0.3564,  ..., -7.2500,  0.7129,  0.4751]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 08:15:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you offend something, you are a offender
If you borrow something, you are a borrower
If you announce something, you are a announcer
If you tell something, you are a teller
If you listen something, you are a listener
If you promote something, you are a promoter
If you determine something, you are a determiner
If you provide something, you are a
2024-07-24 08:15:51 root INFO     [order_1_approx] starting weight calculation for If you announce something, you are a announcer
If you borrow something, you are a borrower
If you determine something, you are a determiner
If you promote something, you are a promoter
If you provide something, you are a provider
If you listen something, you are a listener
If you offend something, you are a offender
If you tell something, you are a
2024-07-24 08:15:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 08:19:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1162,  0.4309,  0.4216,  ...,  0.3452, -0.2073,  0.5122],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.4746,  2.1641, -2.3125,  ..., -1.9180,  2.8086,  3.2344],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0157, -0.0038,  0.0129,  ..., -0.0050, -0.0071, -0.0007],
        [-0.0047, -0.0142,  0.0306,  ...,  0.0083,  0.0168,  0.0014],
        [ 0.0039,  0.0041, -0.0076,  ...,  0.0085,  0.0156,  0.0047],
        ...,
        [-0.0088, -0.0101, -0.0016,  ..., -0.0226,  0.0238,  0.0044],
        [-0.0087,  0.0125, -0.0158,  ...,  0.0047, -0.0283, -0.0049],
        [ 0.0062,  0.0158,  0.0056,  ..., -0.0125,  0.0105, -0.0078]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.4883,  1.9404, -2.8125,  ..., -1.2969,  2.7012,  3.6270]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 08:19:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you announce something, you are a announcer
If you borrow something, you are a borrower
If you determine something, you are a determiner
If you promote something, you are a promoter
If you provide something, you are a provider
If you listen something, you are a listener
If you offend something, you are a offender
If you tell something, you are a
2024-07-24 08:19:40 root INFO     total operator prediction time: 1817.584254026413 seconds
2024-07-24 08:19:40 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on adj - superlative
2024-07-24 08:19:40 root INFO     building operator adj - superlative
2024-07-24 08:19:40 root INFO     [order_1_approx] starting weight calculation for If something is the most wealthy, it is wealthiest
If something is the most shiny, it is shiniest
If something is the most happy, it is happiest
If something is the most merry, it is merriest
If something is the most noisy, it is noisiest
If something is the most mild, it is mildest
If something is the most hardy, it is hardiest
If something is the most able, it is
2024-07-24 08:19:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 08:23:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5410, -0.4678,  0.6025,  ...,  0.7354,  0.1978, -0.4565],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7266,  1.0654, -3.0039,  ...,  0.7021,  0.8955,  1.1924],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0154,  0.0017,  0.0033,  ..., -0.0224,  0.0050, -0.0217],
        [-0.0064, -0.0171,  0.0041,  ..., -0.0086, -0.0151, -0.0132],
        [ 0.0069,  0.0281,  0.0089,  ...,  0.0189, -0.0093, -0.0055],
        ...,
        [-0.0148, -0.0043, -0.0066,  ..., -0.0285, -0.0145, -0.0049],
        [ 0.0091,  0.0026,  0.0053,  ...,  0.0077, -0.0211,  0.0122],
        [-0.0115,  0.0212,  0.0055,  ...,  0.0019, -0.0089, -0.0139]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7192,  1.5088, -2.2480,  ...,  0.5601,  0.5688,  1.6504]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 08:23:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most wealthy, it is wealthiest
If something is the most shiny, it is shiniest
If something is the most happy, it is happiest
If something is the most merry, it is merriest
If something is the most noisy, it is noisiest
If something is the most mild, it is mildest
If something is the most hardy, it is hardiest
If something is the most able, it is
2024-07-24 08:23:24 root INFO     [order_1_approx] starting weight calculation for If something is the most merry, it is merriest
If something is the most noisy, it is noisiest
If something is the most hardy, it is hardiest
If something is the most mild, it is mildest
If something is the most able, it is ablest
If something is the most wealthy, it is wealthiest
If something is the most shiny, it is shiniest
If something is the most happy, it is
2024-07-24 08:23:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 08:27:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1597,  1.2441,  0.6816,  ...,  0.5083,  1.5518,  0.1560],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4414, -1.2090, -4.5273,  ..., -1.1875,  3.5254,  5.0781],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0122, -0.0120,  0.0083,  ..., -0.0018, -0.0078, -0.0135],
        [ 0.0078, -0.0068,  0.0017,  ...,  0.0029, -0.0062, -0.0055],
        [ 0.0071,  0.0146,  0.0062,  ...,  0.0109, -0.0025,  0.0051],
        ...,
        [-0.0030,  0.0055,  0.0020,  ..., -0.0199, -0.0040, -0.0001],
        [-0.0093, -0.0173,  0.0006,  ..., -0.0140, -0.0193,  0.0002],
        [-0.0256, -0.0558,  0.0186,  ...,  0.0208,  0.0247, -0.0337]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2656, -2.1934, -4.6406,  ..., -0.1299,  3.6816,  4.5781]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 08:27:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most merry, it is merriest
If something is the most noisy, it is noisiest
If something is the most hardy, it is hardiest
If something is the most mild, it is mildest
If something is the most able, it is ablest
If something is the most wealthy, it is wealthiest
If something is the most shiny, it is shiniest
If something is the most happy, it is
2024-07-24 08:27:13 root INFO     [order_1_approx] starting weight calculation for If something is the most mild, it is mildest
If something is the most noisy, it is noisiest
If something is the most shiny, it is shiniest
If something is the most happy, it is happiest
If something is the most able, it is ablest
If something is the most wealthy, it is wealthiest
If something is the most merry, it is merriest
If something is the most hardy, it is
2024-07-24 08:27:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 08:31:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.8379,  0.6445,  0.7861,  ...,  0.1792, -0.7549,  1.0605],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.5059, -2.6367, -2.6602,  ..., -2.1914,  2.0410,  8.4141],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0089, -0.0249, -0.0017,  ...,  0.0050, -0.0083, -0.0126],
        [ 0.0094, -0.0094,  0.0039,  ..., -0.0015,  0.0023, -0.0026],
        [ 0.0029,  0.0125, -0.0123,  ..., -0.0027,  0.0023,  0.0002],
        ...,
        [-0.0005,  0.0070, -0.0044,  ..., -0.0201,  0.0048, -0.0039],
        [ 0.0021, -0.0018,  0.0088,  ..., -0.0152, -0.0236,  0.0002],
        [-0.0419, -0.0201,  0.0283,  ..., -0.0062,  0.0007, -0.0353]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.7031, -2.7305, -2.4473,  ..., -2.6504,  0.9922,  9.8750]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 08:31:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most mild, it is mildest
If something is the most noisy, it is noisiest
If something is the most shiny, it is shiniest
If something is the most happy, it is happiest
If something is the most able, it is ablest
If something is the most wealthy, it is wealthiest
If something is the most merry, it is merriest
If something is the most hardy, it is
2024-07-24 08:31:02 root INFO     [order_1_approx] starting weight calculation for If something is the most shiny, it is shiniest
If something is the most wealthy, it is wealthiest
If something is the most mild, it is mildest
If something is the most noisy, it is noisiest
If something is the most happy, it is happiest
If something is the most able, it is ablest
If something is the most hardy, it is hardiest
If something is the most merry, it is
2024-07-24 08:31:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 08:34:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2410,  0.7598,  0.3398,  ...,  1.8350,  1.0781,  0.9834],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.8105, -0.0762, -6.8398,  ...,  0.7236,  2.3496,  4.0625],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-7.0152e-03, -4.5090e-03,  1.2329e-02,  ..., -1.8997e-03,
          4.0512e-03, -9.4604e-03],
        [ 1.9836e-03, -5.4817e-03, -1.1139e-02,  ...,  9.9792e-03,
         -1.2650e-02, -5.4512e-03],
        [ 1.3489e-02,  1.5625e-02, -6.1913e-03,  ...,  9.0485e-03,
          1.1787e-02,  7.5188e-03],
        ...,
        [-5.6839e-03,  8.0109e-05,  1.2901e-02,  ..., -1.4107e-02,
          1.8883e-03,  8.4686e-04],
        [-1.3790e-03, -5.9357e-03,  1.3260e-02,  ..., -2.6245e-03,
         -2.3392e-02, -4.8180e-03],
        [-1.6663e-02, -2.2293e-02,  8.8806e-03,  ..., -5.4970e-03,
          2.4475e-02, -4.1931e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.4746, -0.1530, -6.4023,  ...,  0.3174,  2.8047,  4.2344]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 08:34:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most shiny, it is shiniest
If something is the most wealthy, it is wealthiest
If something is the most mild, it is mildest
If something is the most noisy, it is noisiest
If something is the most happy, it is happiest
If something is the most able, it is ablest
If something is the most hardy, it is hardiest
If something is the most merry, it is
2024-07-24 08:34:51 root INFO     [order_1_approx] starting weight calculation for If something is the most hardy, it is hardiest
If something is the most merry, it is merriest
If something is the most able, it is ablest
If something is the most wealthy, it is wealthiest
If something is the most noisy, it is noisiest
If something is the most happy, it is happiest
If something is the most shiny, it is shiniest
If something is the most mild, it is
2024-07-24 08:34:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 08:38:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2893, -0.1146,  1.1367,  ...,  0.0251,  0.1304,  0.8091],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7998,  0.3198, -6.7773,  ..., -0.0214, -1.6816,  6.2305],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0078, -0.0147,  0.0234,  ..., -0.0024, -0.0201,  0.0009],
        [-0.0154, -0.0078, -0.0013,  ..., -0.0173,  0.0074, -0.0107],
        [-0.0040,  0.0194, -0.0037,  ...,  0.0155, -0.0113,  0.0023],
        ...,
        [-0.0187, -0.0218,  0.0112,  ..., -0.0117, -0.0150,  0.0106],
        [ 0.0111,  0.0040,  0.0104,  ..., -0.0030, -0.0235,  0.0171],
        [-0.0067, -0.0115, -0.0014,  ..., -0.0167,  0.0184, -0.0536]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0342,  0.7646, -7.4062,  ...,  0.3516, -1.9502,  6.1836]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 08:38:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most hardy, it is hardiest
If something is the most merry, it is merriest
If something is the most able, it is ablest
If something is the most wealthy, it is wealthiest
If something is the most noisy, it is noisiest
If something is the most happy, it is happiest
If something is the most shiny, it is shiniest
If something is the most mild, it is
2024-07-24 08:38:39 root INFO     [order_1_approx] starting weight calculation for If something is the most merry, it is merriest
If something is the most able, it is ablest
If something is the most hardy, it is hardiest
If something is the most mild, it is mildest
If something is the most shiny, it is shiniest
If something is the most happy, it is happiest
If something is the most wealthy, it is wealthiest
If something is the most noisy, it is
2024-07-24 08:38:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 08:42:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0775,  0.5195,  0.7451,  ..., -0.2062,  0.8301,  0.2593],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0925, -2.6152, -3.1582,  ...,  3.5312,  2.9102,  3.9668],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0042, -0.0106, -0.0007,  ...,  0.0177, -0.0168, -0.0033],
        [-0.0036,  0.0014,  0.0028,  ...,  0.0050, -0.0187, -0.0023],
        [-0.0052,  0.0208, -0.0033,  ..., -0.0034,  0.0105,  0.0063],
        ...,
        [-0.0005, -0.0160, -0.0018,  ..., -0.0211, -0.0107, -0.0083],
        [ 0.0025,  0.0013,  0.0055,  ..., -0.0037, -0.0346,  0.0123],
        [-0.0139, -0.0239,  0.0008,  ..., -0.0261,  0.0048, -0.0381]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9014, -1.9961, -3.6094,  ...,  3.8574,  2.6387,  3.5703]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 08:42:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most merry, it is merriest
If something is the most able, it is ablest
If something is the most hardy, it is hardiest
If something is the most mild, it is mildest
If something is the most shiny, it is shiniest
If something is the most happy, it is happiest
If something is the most wealthy, it is wealthiest
If something is the most noisy, it is
2024-07-24 08:42:28 root INFO     [order_1_approx] starting weight calculation for If something is the most hardy, it is hardiest
If something is the most able, it is ablest
If something is the most noisy, it is noisiest
If something is the most merry, it is merriest
If something is the most wealthy, it is wealthiest
If something is the most happy, it is happiest
If something is the most mild, it is mildest
If something is the most shiny, it is
2024-07-24 08:42:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 08:46:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1226,  0.4529,  0.8916,  ..., -0.5376,  1.5684,  0.5078],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.0586,  0.3352, -1.7480,  ...,  5.0156, -7.1328,  4.5312],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0024, -0.0128,  0.0044,  ...,  0.0177,  0.0120, -0.0108],
        [ 0.0010, -0.0060,  0.0069,  ..., -0.0043, -0.0144, -0.0033],
        [-0.0026, -0.0041,  0.0008,  ...,  0.0067,  0.0029, -0.0124],
        ...,
        [-0.0122, -0.0188, -0.0076,  ..., -0.0089, -0.0087,  0.0005],
        [ 0.0109, -0.0059,  0.0034,  ..., -0.0012, -0.0107,  0.0097],
        [-0.0144, -0.0031, -0.0083,  ..., -0.0002,  0.0118, -0.0141]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7363,  0.5137, -1.5322,  ...,  6.0273, -7.4180,  4.6836]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 08:46:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most hardy, it is hardiest
If something is the most able, it is ablest
If something is the most noisy, it is noisiest
If something is the most merry, it is merriest
If something is the most wealthy, it is wealthiest
If something is the most happy, it is happiest
If something is the most mild, it is mildest
If something is the most shiny, it is
2024-07-24 08:46:14 root INFO     [order_1_approx] starting weight calculation for If something is the most shiny, it is shiniest
If something is the most noisy, it is noisiest
If something is the most merry, it is merriest
If something is the most able, it is ablest
If something is the most happy, it is happiest
If something is the most hardy, it is hardiest
If something is the most mild, it is mildest
If something is the most wealthy, it is
2024-07-24 08:46:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 08:50:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.5923, 0.8828, 0.5576,  ..., 0.3276, 1.7676, 0.4880], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9951, -1.8301, -1.0576,  ..., -2.7676, -1.7168,  2.1816],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0113, -0.0136, -0.0099,  ..., -0.0136, -0.0078,  0.0004],
        [-0.0038, -0.0086, -0.0146,  ...,  0.0051, -0.0018, -0.0089],
        [ 0.0157,  0.0111, -0.0216,  ...,  0.0240,  0.0012,  0.0095],
        ...,
        [-0.0078, -0.0119, -0.0206,  ..., -0.0435, -0.0100, -0.0152],
        [ 0.0053, -0.0170, -0.0101,  ..., -0.0261, -0.0735,  0.0310],
        [-0.0329,  0.0261,  0.0136,  ...,  0.0263,  0.0255, -0.0618]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5000, -1.1426, -1.0996,  ..., -2.9570, -2.8359,  2.2637]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 08:50:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most shiny, it is shiniest
If something is the most noisy, it is noisiest
If something is the most merry, it is merriest
If something is the most able, it is ablest
If something is the most happy, it is happiest
If something is the most hardy, it is hardiest
If something is the most mild, it is mildest
If something is the most wealthy, it is
2024-07-24 08:50:01 root INFO     total operator prediction time: 1821.1505365371704 seconds
2024-07-24 08:50:01 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_3pSg - Ved
2024-07-24 08:50:01 root INFO     building operator verb_3pSg - Ved
2024-07-24 08:50:01 root INFO     [order_1_approx] starting weight calculation for When he manages something, something has been managed
When he follows something, something has been followed
When he intends something, something has been intended
When he appears something, something has been appeared
When he applies something, something has been applied
When he becomes something, something has been became
When he believes something, something has been believed
When he allows something, something has been
2024-07-24 08:50:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 08:53:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5215, -0.2603,  1.3203,  ...,  0.2512,  0.4678,  0.5786],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7109, -0.6411,  1.5664,  ...,  1.2549, -0.7173,  0.2275],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-3.6697e-03,  1.8585e-02, -3.3798e-03,  ...,  2.1667e-02,
          5.7907e-03, -1.8768e-02],
        [-5.7840e-04, -2.0447e-02,  3.2082e-03,  ..., -1.1292e-02,
         -6.4697e-03, -5.1880e-03],
        [ 3.1357e-03,  2.6741e-03, -3.2928e-02,  ...,  6.7368e-03,
          5.1994e-03,  1.3809e-02],
        ...,
        [-3.2013e-02,  1.5631e-03,  4.1809e-03,  ..., -1.3847e-02,
         -1.6546e-03, -1.0735e-02],
        [-2.2888e-05, -5.1498e-03,  3.0670e-03,  ..., -2.3087e-02,
         -3.1555e-02, -9.0218e-04],
        [-7.2289e-03, -1.7792e-02,  9.9945e-03,  ...,  3.5782e-03,
         -7.2746e-03, -3.2715e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9834, -0.7524,  2.1895,  ...,  0.7617, -0.0811, -0.0194]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 08:53:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he manages something, something has been managed
When he follows something, something has been followed
When he intends something, something has been intended
When he appears something, something has been appeared
When he applies something, something has been applied
When he becomes something, something has been became
When he believes something, something has been believed
When he allows something, something has been
2024-07-24 08:53:49 root INFO     [order_1_approx] starting weight calculation for When he applies something, something has been applied
When he allows something, something has been allowed
When he intends something, something has been intended
When he believes something, something has been believed
When he follows something, something has been followed
When he manages something, something has been managed
When he becomes something, something has been became
When he appears something, something has been
2024-07-24 08:53:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 08:57:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.3457,  0.5098,  1.0342,  ...,  0.0603,  0.2427, -0.1814],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.1055, -0.9238,  1.1699,  ..., -0.4131, -2.3887, -3.5605],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0030,  0.0005, -0.0067,  ...,  0.0173,  0.0047, -0.0131],
        [-0.0097, -0.0045,  0.0156,  ..., -0.0006, -0.0128,  0.0054],
        [ 0.0137,  0.0054, -0.0105,  ...,  0.0054, -0.0016,  0.0033],
        ...,
        [-0.0092,  0.0020,  0.0022,  ...,  0.0027,  0.0010, -0.0082],
        [ 0.0087,  0.0041, -0.0059,  ..., -0.0126, -0.0137, -0.0117],
        [-0.0184,  0.0152, -0.0061,  ...,  0.0060, -0.0032, -0.0238]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0039,  0.0396,  0.6562,  ..., -0.9409, -1.5859, -4.2109]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 08:57:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he applies something, something has been applied
When he allows something, something has been allowed
When he intends something, something has been intended
When he believes something, something has been believed
When he follows something, something has been followed
When he manages something, something has been managed
When he becomes something, something has been became
When he appears something, something has been
2024-07-24 08:57:32 root INFO     [order_1_approx] starting weight calculation for When he allows something, something has been allowed
When he appears something, something has been appeared
When he follows something, something has been followed
When he becomes something, something has been became
When he intends something, something has been intended
When he believes something, something has been believed
When he manages something, something has been managed
When he applies something, something has been
2024-07-24 08:57:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 09:01:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0449,  0.5669,  1.3564,  ..., -0.4331,  0.5117,  0.6206],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6572,  2.1348,  1.4736,  ...,  1.6445, -0.7783,  0.6162],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0097,  0.0080,  0.0060,  ...,  0.0184, -0.0135, -0.0059],
        [-0.0116, -0.0061,  0.0021,  ..., -0.0038, -0.0042,  0.0029],
        [ 0.0220,  0.0240, -0.0065,  ...,  0.0098,  0.0002, -0.0062],
        ...,
        [-0.0188,  0.0047, -0.0040,  ..., -0.0214,  0.0095, -0.0130],
        [ 0.0107, -0.0106,  0.0024,  ..., -0.0118, -0.0152, -0.0058],
        [ 0.0151,  0.0187,  0.0030,  ...,  0.0083,  0.0062, -0.0208]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2656,  2.1387,  1.9688,  ...,  1.8105, -0.6938,  0.0781]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 09:01:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he allows something, something has been allowed
When he appears something, something has been appeared
When he follows something, something has been followed
When he becomes something, something has been became
When he intends something, something has been intended
When he believes something, something has been believed
When he manages something, something has been managed
When he applies something, something has been
2024-07-24 09:01:19 root INFO     [order_1_approx] starting weight calculation for When he allows something, something has been allowed
When he believes something, something has been believed
When he appears something, something has been appeared
When he manages something, something has been managed
When he intends something, something has been intended
When he follows something, something has been followed
When he applies something, something has been applied
When he becomes something, something has been
2024-07-24 09:01:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 09:04:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1875,  0.6538,  1.2168,  ...,  0.7734,  0.5176, -0.2137],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7861,  1.2832,  0.5820,  ..., -0.3740,  1.2197, -1.4531],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0141, -0.0084,  0.0160,  ...,  0.0172,  0.0017, -0.0205],
        [-0.0001, -0.0068, -0.0022,  ..., -0.0047,  0.0028, -0.0004],
        [ 0.0002, -0.0052, -0.0145,  ...,  0.0051, -0.0181,  0.0015],
        ...,
        [-0.0023, -0.0141,  0.0014,  ..., -0.0172, -0.0013,  0.0127],
        [ 0.0142, -0.0007, -0.0003,  ..., -0.0130, -0.0119, -0.0080],
        [ 0.0063,  0.0097, -0.0014,  ..., -0.0007,  0.0035, -0.0259]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7422,  0.4019,  1.3193,  ..., -0.0042,  1.8164, -0.8120]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 09:04:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he allows something, something has been allowed
When he believes something, something has been believed
When he appears something, something has been appeared
When he manages something, something has been managed
When he intends something, something has been intended
When he follows something, something has been followed
When he applies something, something has been applied
When he becomes something, something has been
2024-07-24 09:04:58 root INFO     [order_1_approx] starting weight calculation for When he appears something, something has been appeared
When he intends something, something has been intended
When he follows something, something has been followed
When he allows something, something has been allowed
When he becomes something, something has been became
When he manages something, something has been managed
When he applies something, something has been applied
When he believes something, something has been
2024-07-24 09:04:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 09:08:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4565,  0.6904,  0.0318,  ...,  0.0107,  1.1758,  0.3813],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9238, -0.1520,  0.5801,  ...,  1.3545, -4.0039, -0.0078],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0205, -0.0166,  0.0054,  ...,  0.0048,  0.0076, -0.0205],
        [-0.0071, -0.0003,  0.0087,  ..., -0.0050,  0.0049,  0.0013],
        [ 0.0099,  0.0076, -0.0073,  ...,  0.0061,  0.0031,  0.0164],
        ...,
        [-0.0333, -0.0081,  0.0057,  ..., -0.0179,  0.0026, -0.0061],
        [-0.0061,  0.0043, -0.0147,  ..., -0.0216, -0.0301, -0.0005],
        [ 0.0004,  0.0233, -0.0105,  ...,  0.0114,  0.0052, -0.0368]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.2129,  0.2257,  0.6152,  ...,  1.6279, -3.7832,  0.3196]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 09:08:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he appears something, something has been appeared
When he intends something, something has been intended
When he follows something, something has been followed
When he allows something, something has been allowed
When he becomes something, something has been became
When he manages something, something has been managed
When he applies something, something has been applied
When he believes something, something has been
2024-07-24 09:08:47 root INFO     [order_1_approx] starting weight calculation for When he allows something, something has been allowed
When he believes something, something has been believed
When he intends something, something has been intended
When he becomes something, something has been became
When he manages something, something has been managed
When he appears something, something has been appeared
When he applies something, something has been applied
When he follows something, something has been
2024-07-24 09:08:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 09:12:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4851,  0.2769,  1.7852,  ...,  0.1332, -0.1340,  1.4375],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8477, -1.1211,  1.2588,  ..., -3.1035,  1.2363,  2.3379],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0042, -0.0032,  0.0129,  ..., -0.0018, -0.0252, -0.0206],
        [ 0.0029,  0.0015,  0.0042,  ...,  0.0090, -0.0018,  0.0079],
        [-0.0019,  0.0082, -0.0022,  ..., -0.0064, -0.0199,  0.0143],
        ...,
        [-0.0216, -0.0084, -0.0056,  ..., -0.0023,  0.0052, -0.0017],
        [-0.0042, -0.0095,  0.0061,  ..., -0.0148, -0.0161, -0.0017],
        [-0.0054,  0.0114, -0.0036,  ...,  0.0016,  0.0121, -0.0248]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5605, -1.1543,  1.3965,  ..., -3.1582,  0.9482,  2.3418]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 09:12:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he allows something, something has been allowed
When he believes something, something has been believed
When he intends something, something has been intended
When he becomes something, something has been became
When he manages something, something has been managed
When he appears something, something has been appeared
When he applies something, something has been applied
When he follows something, something has been
2024-07-24 09:12:32 root INFO     [order_1_approx] starting weight calculation for When he follows something, something has been followed
When he manages something, something has been managed
When he allows something, something has been allowed
When he becomes something, something has been became
When he believes something, something has been believed
When he applies something, something has been applied
When he appears something, something has been appeared
When he intends something, something has been
2024-07-24 09:12:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 09:16:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0103,  1.7734, -0.7202,  ..., -0.2795,  0.6387, -0.2061],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.6426,  1.7012,  0.1816,  ...,  2.3633, -1.8955,  1.8740],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.6525e-02, -5.5351e-03,  1.7670e-02,  ...,  2.9373e-04,
         -2.9716e-03, -1.1864e-02],
        [-1.0818e-02,  3.3207e-03, -2.9564e-03,  ..., -7.0801e-03,
         -1.3535e-02,  1.6769e-02],
        [ 8.4229e-03,  2.2461e-02, -1.6510e-02,  ...,  7.8888e-03,
         -1.1520e-02,  3.9368e-03],
        ...,
        [-2.3743e-02, -2.0905e-02,  4.5013e-03,  ..., -3.3051e-02,
          4.9057e-03, -1.0841e-02],
        [ 2.9922e-02,  4.5166e-03, -5.1193e-03,  ..., -1.4664e-02,
         -2.7405e-02, -4.5509e-03],
        [-2.9221e-03,  3.2990e-02, -1.5507e-03,  ..., -3.8147e-06,
          3.5610e-03, -4.3823e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.5977,  3.3945,  1.3115,  ...,  2.1582, -1.9629,  1.2500]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 09:16:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he follows something, something has been followed
When he manages something, something has been managed
When he allows something, something has been allowed
When he becomes something, something has been became
When he believes something, something has been believed
When he applies something, something has been applied
When he appears something, something has been appeared
When he intends something, something has been
2024-07-24 09:16:16 root INFO     [order_1_approx] starting weight calculation for When he becomes something, something has been became
When he applies something, something has been applied
When he follows something, something has been followed
When he intends something, something has been intended
When he believes something, something has been believed
When he appears something, something has been appeared
When he allows something, something has been allowed
When he manages something, something has been
2024-07-24 09:16:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 09:20:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6631,  0.5342,  0.8477,  ..., -0.2086, -0.5127, -0.1240],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5723, -0.9751, -3.8438,  ...,  0.6611,  1.0918,  1.0068],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 5.7716e-03, -5.2414e-03,  6.3477e-03,  ..., -1.3252e-02,
         -1.1948e-02, -7.5455e-03],
        [-7.6904e-03,  3.2940e-03,  1.9531e-03,  ...,  1.3523e-03,
          2.1286e-03,  8.0109e-03],
        [ 1.0521e-02,  5.3673e-03, -1.0864e-02,  ...,  9.5901e-03,
          2.8496e-03,  4.5509e-03],
        ...,
        [-2.2675e-02, -9.2621e-03, -1.4061e-02,  ..., -1.7487e-02,
          8.3771e-03, -2.9373e-03],
        [ 7.6294e-06, -5.2643e-03, -2.3708e-03,  ..., -1.8585e-02,
         -2.1072e-02, -4.9400e-04],
        [-4.5013e-04,  1.6708e-02,  1.5381e-02,  ...,  7.1793e-03,
          6.3591e-03, -2.7130e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5010, -0.3350, -3.7871,  ...,  0.2251,  1.3555,  0.9531]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 09:20:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he becomes something, something has been became
When he applies something, something has been applied
When he follows something, something has been followed
When he intends something, something has been intended
When he believes something, something has been believed
When he appears something, something has been appeared
When he allows something, something has been allowed
When he manages something, something has been
2024-07-24 09:20:04 root INFO     total operator prediction time: 1802.2806582450867 seconds
2024-07-24 09:20:04 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on noun - plural_reg
2024-07-24 09:20:04 root INFO     building operator noun - plural_reg
2024-07-24 09:20:04 root INFO     [order_1_approx] starting weight calculation for The plural form of product is products
The plural form of period is periods
The plural form of year is years
The plural form of death is deaths
The plural form of customer is customers
The plural form of member is members
The plural form of office is offices
The plural form of area is
2024-07-24 09:20:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 09:23:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0117,  0.2434, -1.0146,  ..., -1.0234,  0.5513,  0.1143],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1152, -1.3359,  1.9590,  ..., -2.4219,  3.9609,  3.4785],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0216, -0.0094,  0.0122,  ...,  0.0060, -0.0146, -0.0069],
        [ 0.0006, -0.0129,  0.0026,  ..., -0.0073, -0.0008,  0.0065],
        [ 0.0004,  0.0101, -0.0141,  ..., -0.0049, -0.0036, -0.0059],
        ...,
        [-0.0216,  0.0059, -0.0045,  ..., -0.0152,  0.0198,  0.0048],
        [ 0.0042, -0.0047, -0.0011,  ..., -0.0030, -0.0284,  0.0221],
        [ 0.0086, -0.0024,  0.0010,  ..., -0.0190,  0.0068, -0.0135]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3748, -0.7847,  1.3379,  ..., -2.1309,  4.1055,  3.7637]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 09:23:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of product is products
The plural form of period is periods
The plural form of year is years
The plural form of death is deaths
The plural form of customer is customers
The plural form of member is members
The plural form of office is offices
The plural form of area is
2024-07-24 09:23:51 root INFO     [order_1_approx] starting weight calculation for The plural form of product is products
The plural form of year is years
The plural form of death is deaths
The plural form of member is members
The plural form of office is offices
The plural form of area is areas
The plural form of period is periods
The plural form of customer is
2024-07-24 09:23:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 09:27:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.3677, 0.0518, 0.4204,  ..., 1.9775, 0.2012, 1.1211], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.2480,  1.2119,  1.5127,  ..., -3.1172, -0.8330, -0.6240],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0031, -0.0224,  0.0180,  ...,  0.0020, -0.0101, -0.0175],
        [-0.0050, -0.0282, -0.0015,  ...,  0.0022,  0.0134,  0.0093],
        [ 0.0076, -0.0127, -0.0155,  ...,  0.0024,  0.0115,  0.0119],
        ...,
        [-0.0199, -0.0027,  0.0028,  ..., -0.0195,  0.0140,  0.0044],
        [-0.0121, -0.0102,  0.0154,  ..., -0.0198, -0.0110, -0.0049],
        [-0.0281, -0.0033, -0.0101,  ..., -0.0020,  0.0159, -0.0194]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.2031,  2.1289,  2.1230,  ..., -3.8926, -1.5039, -0.1514]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 09:27:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of product is products
The plural form of year is years
The plural form of death is deaths
The plural form of member is members
The plural form of office is offices
The plural form of area is areas
The plural form of period is periods
The plural form of customer is
2024-07-24 09:27:34 root INFO     [order_1_approx] starting weight calculation for The plural form of area is areas
The plural form of member is members
The plural form of product is products
The plural form of period is periods
The plural form of customer is customers
The plural form of office is offices
The plural form of year is years
The plural form of death is
2024-07-24 09:27:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 09:32:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2695, -0.1119,  0.2169,  ...,  1.1279, -0.3652,  0.1429],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.2891, -2.6289, -1.6660,  ..., -3.2910, -1.6885,  2.0430],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0241,  0.0051,  0.0291,  ...,  0.0023, -0.0043, -0.0052],
        [-0.0359, -0.0223, -0.0118,  ..., -0.0123,  0.0130,  0.0132],
        [ 0.0051,  0.0361, -0.0332,  ..., -0.0029,  0.0277, -0.0118],
        ...,
        [-0.0375, -0.0117, -0.0152,  ..., -0.0135,  0.0042, -0.0084],
        [-0.0168, -0.0083,  0.0090,  ...,  0.0112, -0.0371,  0.0105],
        [-0.0101,  0.0031,  0.0143,  ..., -0.0054,  0.0037, -0.0226]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.8789, -1.7881, -2.7734,  ..., -4.2188, -2.2656,  1.1465]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 09:32:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of area is areas
The plural form of member is members
The plural form of product is products
The plural form of period is periods
The plural form of customer is customers
The plural form of office is offices
The plural form of year is years
The plural form of death is
2024-07-24 09:32:28 root INFO     [order_1_approx] starting weight calculation for The plural form of death is deaths
The plural form of office is offices
The plural form of customer is customers
The plural form of year is years
The plural form of period is periods
The plural form of product is products
The plural form of area is areas
The plural form of member is
2024-07-24 09:32:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 09:38:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.2734,  1.0586, -0.0873,  ...,  0.5684,  0.2139,  1.3496],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6157,  1.7734,  0.9922,  ...,  1.6250, -0.2876,  1.9346],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0111, -0.0065,  0.0196,  ...,  0.0049,  0.0032, -0.0115],
        [ 0.0022, -0.0328, -0.0175,  ...,  0.0025, -0.0050, -0.0031],
        [ 0.0046,  0.0065, -0.0052,  ...,  0.0093,  0.0084, -0.0059],
        ...,
        [ 0.0033, -0.0059, -0.0071,  ..., -0.0249,  0.0035, -0.0102],
        [-0.0017, -0.0116,  0.0003,  ...,  0.0069, -0.0170, -0.0073],
        [-0.0008,  0.0015,  0.0110,  ..., -0.0201,  0.0132, -0.0070]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[0.4521, 1.3105, 1.0088,  ..., 1.4004, 0.1426, 2.5801]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 09:38:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of death is deaths
The plural form of office is offices
The plural form of customer is customers
The plural form of year is years
The plural form of period is periods
The plural form of product is products
The plural form of area is areas
The plural form of member is
2024-07-24 09:38:15 root INFO     [order_1_approx] starting weight calculation for The plural form of customer is customers
The plural form of member is members
The plural form of product is products
The plural form of period is periods
The plural form of year is years
The plural form of death is deaths
The plural form of area is areas
The plural form of office is
2024-07-24 09:38:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 09:43:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.0625, -0.6504,  0.5410,  ..., -0.1685,  0.1595,  0.2603],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6880, -0.1465, -2.6406,  ...,  0.8994,  0.1006,  2.6250],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.1501e-03, -3.0457e-02,  1.5045e-02,  ..., -7.2670e-03,
          6.0387e-03, -2.0416e-02],
        [-1.1276e-02, -1.6907e-02, -1.7105e-02,  ..., -6.4011e-03,
         -6.3934e-03,  6.1188e-03],
        [ 3.3905e-02, -8.8654e-03, -9.2621e-03,  ...,  7.4615e-03,
          2.9144e-03,  8.9722e-03],
        ...,
        [ 2.1286e-03,  5.6458e-04, -9.1400e-03,  ..., -6.1302e-03,
          1.5411e-02, -1.7899e-02],
        [ 7.6294e-04, -1.3542e-02, -7.6294e-05,  ..., -9.7504e-03,
         -1.3077e-02,  6.1874e-03],
        [ 1.1856e-02,  1.2024e-02, -1.1311e-03,  ...,  6.8207e-03,
          6.7329e-03,  3.5973e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1162, -0.0798, -2.0859,  ..., -0.0088,  0.4353,  3.4863]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 09:43:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of customer is customers
The plural form of member is members
The plural form of product is products
The plural form of period is periods
The plural form of year is years
The plural form of death is deaths
The plural form of area is areas
The plural form of office is
2024-07-24 09:43:35 root INFO     [order_1_approx] starting weight calculation for The plural form of year is years
The plural form of office is offices
The plural form of area is areas
The plural form of member is members
The plural form of customer is customers
The plural form of product is products
The plural form of death is deaths
The plural form of period is
2024-07-24 09:43:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 09:48:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0127,  0.5498, -0.4814,  ..., -1.4785,  0.3428,  0.2419],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4314, -0.5225, -1.7920,  ...,  3.9941,  1.4229,  2.7578],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0127, -0.0067,  0.0093,  ..., -0.0052, -0.0126, -0.0211],
        [-0.0191, -0.0140, -0.0172,  ...,  0.0113,  0.0042, -0.0043],
        [-0.0082,  0.0119, -0.0216,  ..., -0.0125,  0.0080,  0.0083],
        ...,
        [-0.0217, -0.0051,  0.0090,  ..., -0.0037,  0.0107, -0.0071],
        [-0.0014, -0.0116, -0.0065,  ..., -0.0083, -0.0375,  0.0239],
        [-0.0077,  0.0074,  0.0223,  ...,  0.0074,  0.0190, -0.0204]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1985,  0.5430, -0.6475,  ...,  3.2051,  0.6328,  2.7012]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 09:48:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of year is years
The plural form of office is offices
The plural form of area is areas
The plural form of member is members
The plural form of customer is customers
The plural form of product is products
The plural form of death is deaths
The plural form of period is
2024-07-24 09:48:56 root INFO     [order_1_approx] starting weight calculation for The plural form of area is areas
The plural form of customer is customers
The plural form of period is periods
The plural form of office is offices
The plural form of member is members
The plural form of death is deaths
The plural form of year is years
The plural form of product is
2024-07-24 09:48:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 09:54:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1934,  0.7783,  1.0986,  ...,  0.3755,  0.0767, -0.2405],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.7539, -2.9160,  1.3301,  ...,  2.5098,  0.3965,  2.6250],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0187,  0.0012,  0.0233,  ...,  0.0018, -0.0143, -0.0230],
        [ 0.0060, -0.0202, -0.0037,  ...,  0.0060,  0.0072, -0.0077],
        [ 0.0238,  0.0097, -0.0240,  ...,  0.0030, -0.0014,  0.0078],
        ...,
        [-0.0119, -0.0005, -0.0035,  ..., -0.0201,  0.0160,  0.0090],
        [-0.0005,  0.0042, -0.0005,  ..., -0.0018, -0.0394,  0.0088],
        [-0.0122,  0.0019, -0.0066,  ..., -0.0061,  0.0204, -0.0198]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.7188, -2.8535,  1.4102,  ...,  1.8398,  0.9561,  2.9492]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 09:54:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of area is areas
The plural form of customer is customers
The plural form of period is periods
The plural form of office is offices
The plural form of member is members
The plural form of death is deaths
The plural form of year is years
The plural form of product is
2024-07-24 09:54:18 root INFO     [order_1_approx] starting weight calculation for The plural form of member is members
The plural form of area is areas
The plural form of death is deaths
The plural form of office is offices
The plural form of product is products
The plural form of customer is customers
The plural form of period is periods
The plural form of year is
2024-07-24 09:54:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 09:59:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.9258,  0.3481, -0.7510,  ..., -0.5776, -0.7036,  1.2559],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8477, -3.8535, -0.7295,  ...,  2.6641,  3.2559,  2.7383],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0156, -0.0150, -0.0026,  ...,  0.0102,  0.0016,  0.0064],
        [-0.0128, -0.0066, -0.0031,  ..., -0.0018, -0.0039,  0.0079],
        [-0.0024, -0.0056, -0.0222,  ...,  0.0003,  0.0085, -0.0069],
        ...,
        [-0.0022, -0.0082, -0.0180,  ..., -0.0068,  0.0027,  0.0090],
        [-0.0004, -0.0027, -0.0034,  ..., -0.0158, -0.0254,  0.0104],
        [-0.0048,  0.0020,  0.0038,  ...,  0.0022,  0.0066, -0.0206]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9785, -3.6602, -0.0640,  ...,  2.3398,  3.7188,  3.5234]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 09:59:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of member is members
The plural form of area is areas
The plural form of death is deaths
The plural form of office is offices
The plural form of product is products
The plural form of customer is customers
The plural form of period is periods
The plural form of year is
2024-07-24 09:59:50 root INFO     total operator prediction time: 2386.0678129196167 seconds
2024-07-24 09:59:50 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_Ving - 3pSg
2024-07-24 09:59:50 root INFO     building operator verb_Ving - 3pSg
2024-07-24 09:59:50 root INFO     [order_1_approx] starting weight calculation for When something is performing, it performs
When something is happening, it happens
When something is publishing, it publishes
When something is becoming, it becomes
When something is continuing, it continues
When something is receiving, it receives
When something is referring, it refers
When something is appearing, it
2024-07-24 09:59:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 10:05:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1598, -0.6357,  1.2432,  ..., -0.0378,  0.9893,  0.0592],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.8027,  2.3711, -0.7002,  ...,  4.5625, -1.9238, -0.1582],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0053, -0.0236,  0.0072,  ...,  0.0095, -0.0042, -0.0145],
        [-0.0120, -0.0134,  0.0137,  ..., -0.0050, -0.0273,  0.0192],
        [-0.0034,  0.0206, -0.0322,  ...,  0.0323, -0.0144, -0.0070],
        ...,
        [-0.0305, -0.0078, -0.0087,  ..., -0.0110, -0.0047, -0.0013],
        [ 0.0030,  0.0052,  0.0094,  ..., -0.0320, -0.0096,  0.0235],
        [-0.0166,  0.0034,  0.0165,  ..., -0.0115,  0.0039, -0.0098]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.0195,  4.0430, -1.6338,  ...,  4.7969, -2.1602, -1.3057]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 10:05:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is performing, it performs
When something is happening, it happens
When something is publishing, it publishes
When something is becoming, it becomes
When something is continuing, it continues
When something is receiving, it receives
When something is referring, it refers
When something is appearing, it
2024-07-24 10:05:35 root INFO     [order_1_approx] starting weight calculation for When something is referring, it refers
When something is performing, it performs
When something is appearing, it appears
When something is publishing, it publishes
When something is continuing, it continues
When something is receiving, it receives
When something is happening, it happens
When something is becoming, it
2024-07-24 10:05:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 10:11:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0743,  0.4011,  0.8750,  ...,  1.3887,  0.3269, -0.3423],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([2.8770, 0.2075, 1.1045,  ..., 1.2500, 0.4243, 0.2070], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0347, -0.0454,  0.0119,  ..., -0.0221,  0.0002, -0.0100],
        [ 0.0024, -0.0104, -0.0126,  ..., -0.0038, -0.0140, -0.0210],
        [ 0.0064, -0.0189, -0.0202,  ...,  0.0159, -0.0078,  0.0219],
        ...,
        [ 0.0006,  0.0043,  0.0021,  ..., -0.0585, -0.0271, -0.0017],
        [-0.0097,  0.0129,  0.0106,  ..., -0.0070, -0.0460,  0.0044],
        [-0.0055,  0.0028, -0.0050,  ..., -0.0016,  0.0088, -0.0272]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[2.3438, 0.9570, 0.8711,  ..., 1.5713, 0.2573, 0.2441]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 10:11:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is referring, it refers
When something is performing, it performs
When something is appearing, it appears
When something is publishing, it publishes
When something is continuing, it continues
When something is receiving, it receives
When something is happening, it happens
When something is becoming, it
2024-07-24 10:11:07 root INFO     [order_1_approx] starting weight calculation for When something is appearing, it appears
When something is publishing, it publishes
When something is receiving, it receives
When something is happening, it happens
When something is performing, it performs
When something is becoming, it becomes
When something is referring, it refers
When something is continuing, it
2024-07-24 10:11:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 10:16:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3809, -0.3770,  0.7329,  ...,  0.6543,  1.1143, -0.0096],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8159, -1.3789, -0.3193,  ...,  2.7285, -1.1016,  2.1055],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0216, -0.0150, -0.0044,  ..., -0.0036, -0.0109, -0.0190],
        [ 0.0079,  0.0011, -0.0104,  ..., -0.0030, -0.0012,  0.0050],
        [ 0.0188, -0.0016, -0.0368,  ...,  0.0124,  0.0004,  0.0087],
        ...,
        [-0.0222,  0.0054, -0.0114,  ..., -0.0100,  0.0045, -0.0039],
        [ 0.0031, -0.0003,  0.0017,  ..., -0.0084, -0.0284,  0.0154],
        [-0.0017,  0.0049,  0.0056,  ..., -0.0131,  0.0067, -0.0387]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3203, -0.7344, -0.0576,  ...,  3.2266, -1.2861,  1.7402]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 10:16:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is appearing, it appears
When something is publishing, it publishes
When something is receiving, it receives
When something is happening, it happens
When something is performing, it performs
When something is becoming, it becomes
When something is referring, it refers
When something is continuing, it
2024-07-24 10:16:36 root INFO     [order_1_approx] starting weight calculation for When something is continuing, it continues
When something is publishing, it publishes
When something is receiving, it receives
When something is becoming, it becomes
When something is referring, it refers
When something is appearing, it appears
When something is performing, it performs
When something is happening, it
2024-07-24 10:16:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 10:21:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4456, -0.4929,  1.4277,  ...,  0.4019,  1.3672, -0.8540],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8232,  0.6206, -2.5215,  ...,  2.0977,  2.8730,  4.7969],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0251, -0.0280,  0.0148,  ...,  0.0282, -0.0156, -0.0103],
        [-0.0112, -0.0390, -0.0118,  ...,  0.0109, -0.0075,  0.0049],
        [-0.0014,  0.0065, -0.0409,  ...,  0.0255, -0.0043,  0.0206],
        ...,
        [-0.0163, -0.0139, -0.0160,  ..., -0.0489, -0.0022, -0.0204],
        [-0.0038,  0.0202,  0.0072,  ..., -0.0153, -0.0666,  0.0227],
        [-0.0165, -0.0114,  0.0057,  ...,  0.0070,  0.0093, -0.0506]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5840,  1.9551, -2.9668,  ...,  1.7109,  2.2637,  3.4883]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 10:21:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is continuing, it continues
When something is publishing, it publishes
When something is receiving, it receives
When something is becoming, it becomes
When something is referring, it refers
When something is appearing, it appears
When something is performing, it performs
When something is happening, it
2024-07-24 10:21:52 root INFO     [order_1_approx] starting weight calculation for When something is continuing, it continues
When something is appearing, it appears
When something is becoming, it becomes
When something is publishing, it publishes
When something is referring, it refers
When something is receiving, it receives
When something is happening, it happens
When something is performing, it
2024-07-24 10:21:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 10:27:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1432, -0.1211,  0.1985,  ..., -0.4150,  0.9961, -0.5269],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.3184,  0.6753,  0.5967,  ...,  4.1367, -0.4341,  1.9902],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0002, -0.0302,  0.0023,  ..., -0.0009, -0.0159, -0.0211],
        [ 0.0091, -0.0120, -0.0064,  ..., -0.0030, -0.0091,  0.0051],
        [ 0.0088, -0.0049, -0.0077,  ...,  0.0246, -0.0008,  0.0006],
        ...,
        [-0.0224, -0.0081, -0.0092,  ..., -0.0230,  0.0060, -0.0014],
        [-0.0078,  0.0111, -0.0086,  ..., -0.0028, -0.0455,  0.0122],
        [-0.0144,  0.0019, -0.0002,  ...,  0.0002, -0.0028, -0.0189]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7109,  0.9688,  0.5386,  ...,  4.6875, -0.9204,  1.8857]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 10:27:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is continuing, it continues
When something is appearing, it appears
When something is becoming, it becomes
When something is publishing, it publishes
When something is referring, it refers
When something is receiving, it receives
When something is happening, it happens
When something is performing, it
2024-07-24 10:27:17 root INFO     [order_1_approx] starting weight calculation for When something is becoming, it becomes
When something is continuing, it continues
When something is performing, it performs
When something is appearing, it appears
When something is happening, it happens
When something is referring, it refers
When something is receiving, it receives
When something is publishing, it
2024-07-24 10:27:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 10:32:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0488, -1.4424,  1.8535,  ...,  1.4756,  0.9678,  0.0865],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6211,  0.8359, -1.3037,  ...,  4.4141, -2.0176,  1.5713],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0079, -0.0157,  0.0095,  ...,  0.0196,  0.0013, -0.0192],
        [ 0.0011, -0.0021, -0.0033,  ...,  0.0110, -0.0086, -0.0011],
        [ 0.0073,  0.0011, -0.0109,  ...,  0.0217, -0.0241,  0.0004],
        ...,
        [-0.0080, -0.0062, -0.0157,  ..., -0.0366, -0.0186, -0.0037],
        [-0.0077,  0.0143,  0.0025,  ..., -0.0051, -0.0363,  0.0136],
        [-0.0145, -0.0044, -0.0038,  ..., -0.0105,  0.0095, -0.0288]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3828,  1.4365, -0.8960,  ...,  4.1250, -2.9238,  1.6045]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 10:32:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is becoming, it becomes
When something is continuing, it continues
When something is performing, it performs
When something is appearing, it appears
When something is happening, it happens
When something is referring, it refers
When something is receiving, it receives
When something is publishing, it
2024-07-24 10:32:21 root INFO     [order_1_approx] starting weight calculation for When something is continuing, it continues
When something is performing, it performs
When something is referring, it refers
When something is happening, it happens
When something is appearing, it appears
When something is becoming, it becomes
When something is publishing, it publishes
When something is receiving, it
2024-07-24 10:32:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 10:37:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9746, -1.3477,  0.6719,  ...,  0.3198,  0.7725,  1.0742],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([3.8262, 1.1182, 0.9707,  ..., 3.5332, 0.4922, 4.7969], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0074, -0.0035, -0.0123,  ..., -0.0052, -0.0172, -0.0024],
        [ 0.0086,  0.0055,  0.0186,  ..., -0.0033, -0.0048,  0.0010],
        [ 0.0061,  0.0082, -0.0102,  ...,  0.0092, -0.0261,  0.0083],
        ...,
        [-0.0021, -0.0133, -0.0132,  ..., -0.0236,  0.0118, -0.0025],
        [ 0.0012,  0.0111, -0.0073,  ..., -0.0205, -0.0315,  0.0159],
        [ 0.0067, -0.0088, -0.0004,  ..., -0.0169,  0.0202, -0.0185]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[2.5117, 1.6982, 0.9238,  ..., 3.2578, 0.2080, 4.2344]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 10:37:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is continuing, it continues
When something is performing, it performs
When something is referring, it refers
When something is happening, it happens
When something is appearing, it appears
When something is becoming, it becomes
When something is publishing, it publishes
When something is receiving, it
2024-07-24 10:37:04 root INFO     [order_1_approx] starting weight calculation for When something is continuing, it continues
When something is happening, it happens
When something is becoming, it becomes
When something is performing, it performs
When something is publishing, it publishes
When something is receiving, it receives
When something is appearing, it appears
When something is referring, it
2024-07-24 10:37:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 10:42:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8081, -0.8340,  0.2104,  ...,  0.0973,  1.1338, -0.5029],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3557,  2.2129,  1.1816,  ...,  6.0781,  1.2051,  1.9180],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0106, -0.0236,  0.0144,  ..., -0.0045, -0.0136,  0.0221],
        [-0.0057, -0.0099, -0.0107,  ..., -0.0073,  0.0052, -0.0177],
        [ 0.0091,  0.0191, -0.0272,  ...,  0.0225, -0.0094,  0.0058],
        ...,
        [-0.0012, -0.0158, -0.0048,  ..., -0.0309, -0.0038, -0.0193],
        [ 0.0002,  0.0023,  0.0250,  ..., -0.0096, -0.0491,  0.0105],
        [-0.0032,  0.0141,  0.0075,  ..., -0.0009,  0.0012, -0.0205]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4214,  3.2520,  0.7930,  ...,  5.6211,  1.2588,  1.5859]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 10:42:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is continuing, it continues
When something is happening, it happens
When something is becoming, it becomes
When something is performing, it performs
When something is publishing, it publishes
When something is receiving, it receives
When something is appearing, it appears
When something is referring, it
2024-07-24 10:42:37 root INFO     total operator prediction time: 2567.125725030899 seconds
2024-07-24 10:42:37 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_inf - 3pSg
2024-07-24 10:42:37 root INFO     building operator verb_inf - 3pSg
2024-07-24 10:42:37 root INFO     [order_1_approx] starting weight calculation for I include, he includes
I apply, he applies
I prevent, he prevents
I receive, he receives
I contain, he contains
I believe, he believes
I promote, he promotes
I achieve, he
2024-07-24 10:42:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 10:47:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5640,  0.5869,  1.9824,  ..., -0.4077, -0.4607,  0.7305],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8633,  1.3477, -0.0127,  ..., -3.1953, -0.1030, -0.3564],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0152, -0.0091, -0.0040,  ..., -0.0067, -0.0086, -0.0230],
        [-0.0080, -0.0101, -0.0064,  ..., -0.0139, -0.0085,  0.0038],
        [-0.0021, -0.0089, -0.0381,  ...,  0.0205,  0.0139, -0.0022],
        ...,
        [ 0.0054, -0.0035, -0.0210,  ..., -0.0319, -0.0197, -0.0053],
        [-0.0068,  0.0082,  0.0214,  ..., -0.0101, -0.0319,  0.0075],
        [-0.0014,  0.0215,  0.0020,  ..., -0.0128,  0.0044, -0.0125]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.2715,  2.1191, -0.8345,  ..., -4.0352,  0.5381, -0.8906]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 10:47:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I include, he includes
I apply, he applies
I prevent, he prevents
I receive, he receives
I contain, he contains
I believe, he believes
I promote, he promotes
I achieve, he
2024-07-24 10:47:33 root INFO     [order_1_approx] starting weight calculation for I receive, he receives
I include, he includes
I achieve, he achieves
I prevent, he prevents
I contain, he contains
I promote, he promotes
I believe, he believes
I apply, he
2024-07-24 10:47:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 10:51:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8667, -0.3389,  1.6816,  ..., -0.4780,  0.0929,  1.4746],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.1055,  0.3970,  1.2051,  ...,  3.0879, -1.6641,  5.4805],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0192, -0.0053,  0.0009,  ...,  0.0072, -0.0073, -0.0037],
        [ 0.0008, -0.0083,  0.0070,  ..., -0.0091,  0.0086,  0.0089],
        [ 0.0059,  0.0097, -0.0204,  ...,  0.0126, -0.0019, -0.0168],
        ...,
        [-0.0067, -0.0192, -0.0262,  ..., -0.0130,  0.0057, -0.0011],
        [-0.0187,  0.0016,  0.0261,  ..., -0.0107, -0.0269,  0.0166],
        [ 0.0081,  0.0239,  0.0070,  ..., -0.0042, -0.0010, -0.0175]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.3359,  0.5703,  0.5254,  ...,  3.2754, -0.4980,  4.8906]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 10:51:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I receive, he receives
I include, he includes
I achieve, he achieves
I prevent, he prevents
I contain, he contains
I promote, he promotes
I believe, he believes
I apply, he
2024-07-24 10:51:23 root INFO     [order_1_approx] starting weight calculation for I include, he includes
I prevent, he prevents
I achieve, he achieves
I contain, he contains
I promote, he promotes
I apply, he applies
I receive, he receives
I believe, he
2024-07-24 10:51:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 10:55:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8096, -0.2744,  0.1119,  ...,  0.2334,  0.8789,  0.4036],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.2715, -0.8042, -1.1816,  ...,  2.8105, -4.0234,  3.0742],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0160, -0.0302,  0.0163,  ...,  0.0101,  0.0158, -0.0128],
        [-0.0050, -0.0188,  0.0101,  ..., -0.0094,  0.0242,  0.0020],
        [ 0.0006, -0.0005, -0.0197,  ...,  0.0212,  0.0048, -0.0017],
        ...,
        [-0.0103, -0.0101, -0.0162,  ..., -0.0255, -0.0041,  0.0138],
        [ 0.0079, -0.0053,  0.0197,  ..., -0.0060, -0.0371,  0.0069],
        [ 0.0100,  0.0104, -0.0156,  ...,  0.0014, -0.0087, -0.0267]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6133, -0.7349, -0.7515,  ...,  2.5137, -3.6562,  2.4785]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 10:55:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I include, he includes
I prevent, he prevents
I achieve, he achieves
I contain, he contains
I promote, he promotes
I apply, he applies
I receive, he receives
I believe, he
2024-07-24 10:55:12 root INFO     [order_1_approx] starting weight calculation for I achieve, he achieves
I receive, he receives
I believe, he believes
I prevent, he prevents
I include, he includes
I promote, he promotes
I apply, he applies
I contain, he
2024-07-24 10:55:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 10:59:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0619,  0.0435,  0.7378,  ..., -0.4626, -0.3677,  0.9702],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9219, -2.4160, -1.1641,  ...,  1.3389, -3.0078,  3.3574],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0169, -0.0007, -0.0121,  ..., -0.0026,  0.0008, -0.0148],
        [ 0.0011, -0.0044,  0.0069,  ...,  0.0074,  0.0064, -0.0115],
        [ 0.0018,  0.0081, -0.0194,  ...,  0.0026,  0.0050, -0.0124],
        ...,
        [-0.0148, -0.0117, -0.0189,  ..., -0.0162,  0.0166,  0.0235],
        [-0.0100,  0.0073,  0.0113,  ..., -0.0117, -0.0179,  0.0138],
        [-0.0070,  0.0008,  0.0097,  ..., -0.0122,  0.0023, -0.0013]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9126, -2.6016, -1.6016,  ...,  1.9551, -2.7715,  3.5781]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 10:59:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I achieve, he achieves
I receive, he receives
I believe, he believes
I prevent, he prevents
I include, he includes
I promote, he promotes
I apply, he applies
I contain, he
2024-07-24 10:59:02 root INFO     [order_1_approx] starting weight calculation for I contain, he contains
I believe, he believes
I promote, he promotes
I receive, he receives
I apply, he applies
I prevent, he prevents
I achieve, he achieves
I include, he
2024-07-24 10:59:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 11:02:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.8872, 0.3999, 1.3906,  ..., 0.0337, 0.0136, 1.7324], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2510, -2.2871, -0.8809,  ...,  2.8184, -2.5332,  1.1562],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0256,  0.0169,  0.0004,  ...,  0.0014, -0.0005,  0.0083],
        [ 0.0001, -0.0063,  0.0027,  ...,  0.0063,  0.0061,  0.0031],
        [-0.0047, -0.0112, -0.0101,  ...,  0.0098, -0.0029, -0.0047],
        ...,
        [-0.0119,  0.0015, -0.0095,  ...,  0.0006, -0.0238,  0.0153],
        [ 0.0105,  0.0244, -0.0034,  ..., -0.0212, -0.0183,  0.0227],
        [ 0.0065,  0.0159,  0.0103,  ..., -0.0145,  0.0044, -0.0115]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0420, -1.7598, -0.5415,  ...,  2.8418, -2.6660,  1.0654]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 11:02:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I contain, he contains
I believe, he believes
I promote, he promotes
I receive, he receives
I apply, he applies
I prevent, he prevents
I achieve, he achieves
I include, he
2024-07-24 11:02:46 root INFO     [order_1_approx] starting weight calculation for I receive, he receives
I apply, he applies
I contain, he contains
I believe, he believes
I include, he includes
I promote, he promotes
I achieve, he achieves
I prevent, he
2024-07-24 11:02:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 11:06:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4495,  0.4182,  1.0713,  ..., -0.0651,  0.0837, -0.1565],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6133, -4.0352,  0.8945,  ...,  1.9443,  1.1729,  5.1484],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0094, -0.0196,  0.0111,  ...,  0.0016,  0.0015, -0.0419],
        [-0.0061, -0.0023,  0.0078,  ..., -0.0042,  0.0103, -0.0140],
        [-0.0168, -0.0002, -0.0361,  ..., -0.0051,  0.0129, -0.0091],
        ...,
        [-0.0208, -0.0210, -0.0091,  ...,  0.0181, -0.0121, -0.0004],
        [ 0.0138,  0.0161,  0.0049,  ..., -0.0294, -0.0266, -0.0062],
        [-0.0201,  0.0224,  0.0034,  ..., -0.0054,  0.0067, -0.0207]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3086, -3.9902,  0.7471,  ...,  1.3672,  1.8848,  4.6953]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 11:06:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I receive, he receives
I apply, he applies
I contain, he contains
I believe, he believes
I include, he includes
I promote, he promotes
I achieve, he achieves
I prevent, he
2024-07-24 11:06:32 root INFO     [order_1_approx] starting weight calculation for I receive, he receives
I contain, he contains
I apply, he applies
I prevent, he prevents
I achieve, he achieves
I believe, he believes
I include, he includes
I promote, he
2024-07-24 11:06:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 11:10:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9834,  0.1045,  1.3906,  ..., -0.0235, -0.5503,  0.6265],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4014, -3.9785,  0.5518,  ...,  4.3125, -2.3242,  4.3320],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0001, -0.0336,  0.0031,  ..., -0.0032, -0.0187, -0.0265],
        [-0.0045,  0.0014,  0.0027,  ...,  0.0050,  0.0078, -0.0070],
        [-0.0111, -0.0097, -0.0188,  ...,  0.0060, -0.0110,  0.0090],
        ...,
        [-0.0094, -0.0091, -0.0076,  ..., -0.0060, -0.0081,  0.0090],
        [ 0.0015,  0.0077,  0.0088,  ..., -0.0199, -0.0392,  0.0081],
        [-0.0056,  0.0168,  0.0133,  ...,  0.0067, -0.0030, -0.0093]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1375, -3.6719,  0.8374,  ...,  5.1836, -2.5117,  3.7070]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 11:10:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I receive, he receives
I contain, he contains
I apply, he applies
I prevent, he prevents
I achieve, he achieves
I believe, he believes
I include, he includes
I promote, he
2024-07-24 11:10:18 root INFO     [order_1_approx] starting weight calculation for I contain, he contains
I achieve, he achieves
I promote, he promotes
I prevent, he prevents
I include, he includes
I apply, he applies
I believe, he believes
I receive, he
2024-07-24 11:10:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 11:14:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0552, -1.2764,  1.2129,  ..., -0.2159, -0.0098,  1.4746],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3398, -1.2881,  1.7383,  ...,  2.4609, -0.7700,  5.7109],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0088, -0.0124, -0.0084,  ...,  0.0055, -0.0159, -0.0151],
        [ 0.0040, -0.0153,  0.0120,  ..., -0.0025,  0.0135, -0.0024],
        [ 0.0120,  0.0091, -0.0224,  ...,  0.0116,  0.0006, -0.0038],
        ...,
        [ 0.0095, -0.0261, -0.0136,  ..., -0.0199, -0.0062, -0.0013],
        [-0.0043,  0.0055, -0.0020,  ..., -0.0072, -0.0331,  0.0113],
        [ 0.0022, -0.0125, -0.0141,  ..., -0.0081, -0.0072, -0.0315]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5518, -0.8018,  1.5039,  ...,  2.9961, -0.4243,  5.8125]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 11:14:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I contain, he contains
I achieve, he achieves
I promote, he promotes
I prevent, he prevents
I include, he includes
I apply, he applies
I believe, he believes
I receive, he
2024-07-24 11:14:06 root INFO     total operator prediction time: 1889.3260145187378 seconds
2024-07-24 11:14:06 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_inf - Ved
2024-07-24 11:14:06 root INFO     building operator verb_inf - Ved
2024-07-24 11:14:06 root INFO     [order_1_approx] starting weight calculation for If the present form is receive, the past form is received
If the present form is refer, the past form is referred
If the present form is believe, the past form is believed
If the present form is provide, the past form is provided
If the present form is remain, the past form is remained
If the present form is perform, the past form is performed
If the present form is discover, the past form is discovered
If the present form is add, the past form is
2024-07-24 11:14:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 11:17:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4785,  0.3269,  1.1494,  ...,  0.1359, -0.5454, -0.1754],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6948,  0.3848,  4.8711,  ..., -2.5781, -1.6230, -0.1255],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-7.7667e-03,  2.7027e-03, -4.1466e-03,  ...,  1.7252e-03,
         -1.4954e-02,  2.2736e-03],
        [ 2.3437e-04, -1.7426e-02, -1.0605e-02,  ...,  4.2191e-03,
          2.9869e-03, -8.2855e-03],
        [ 5.0278e-03, -1.3561e-03, -1.9226e-02,  ...,  1.0254e-02,
          1.7166e-04, -1.8829e-02],
        ...,
        [-2.1576e-02,  8.2855e-03,  5.1155e-03,  ..., -1.5419e-02,
          3.3379e-04,  6.2828e-03],
        [ 6.5651e-03,  2.2903e-02,  1.8890e-02,  ..., -1.2466e-02,
         -1.4542e-02, -1.1543e-02],
        [-4.1389e-03, -3.5286e-05, -8.4381e-03,  ..., -5.1727e-03,
          7.8087e-03, -7.2250e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3232,  0.0474,  4.6250,  ..., -1.8379, -2.5117, -0.7734]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 11:17:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is receive, the past form is received
If the present form is refer, the past form is referred
If the present form is believe, the past form is believed
If the present form is provide, the past form is provided
If the present form is remain, the past form is remained
If the present form is perform, the past form is performed
If the present form is discover, the past form is discovered
If the present form is add, the past form is
2024-07-24 11:17:53 root INFO     [order_1_approx] starting weight calculation for If the present form is receive, the past form is received
If the present form is perform, the past form is performed
If the present form is discover, the past form is discovered
If the present form is add, the past form is added
If the present form is provide, the past form is provided
If the present form is refer, the past form is referred
If the present form is remain, the past form is remained
If the present form is believe, the past form is
2024-07-24 11:17:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 11:21:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5400,  0.3352,  0.3210,  ...,  0.6538,  0.4619, -0.2622],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3418,  0.3271,  0.1758,  ...,  2.5801, -4.7617,  0.7109],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.8677e-02, -2.3407e-02,  5.7030e-03,  ..., -1.2413e-02,
          5.1689e-03, -1.4801e-02],
        [-1.2634e-02, -2.6428e-02, -1.5625e-02,  ...,  8.1539e-04,
          1.8829e-02, -1.6190e-02],
        [-9.4376e-03,  3.3150e-03, -1.0277e-02,  ...,  1.8585e-02,
         -1.8196e-03, -6.4850e-04],
        ...,
        [-3.4599e-03,  2.5082e-03, -7.2060e-03,  ..., -2.1194e-02,
         -3.9577e-05,  1.0071e-02],
        [-1.0147e-03,  8.7509e-03,  1.6876e-02,  ..., -2.2980e-02,
         -3.9917e-02,  2.4986e-03],
        [-1.0529e-03,  1.5236e-02, -5.3101e-03,  ...,  1.0574e-02,
         -1.9836e-03, -3.8177e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5879,  0.2136,  0.3789,  ...,  3.0469, -4.9922,  1.0723]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 11:21:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is receive, the past form is received
If the present form is perform, the past form is performed
If the present form is discover, the past form is discovered
If the present form is add, the past form is added
If the present form is provide, the past form is provided
If the present form is refer, the past form is referred
If the present form is remain, the past form is remained
If the present form is believe, the past form is
2024-07-24 11:21:40 root INFO     [order_1_approx] starting weight calculation for If the present form is remain, the past form is remained
If the present form is believe, the past form is believed
If the present form is perform, the past form is performed
If the present form is add, the past form is added
If the present form is refer, the past form is referred
If the present form is receive, the past form is received
If the present form is provide, the past form is provided
If the present form is discover, the past form is
2024-07-24 11:21:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 11:25:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4409,  0.0654,  0.1047,  ...,  0.4922,  0.3516, -0.1841],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.8105,  1.1289,  0.1152,  ..., -3.1777, -4.3594,  1.3145],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0049, -0.0049, -0.0009,  ..., -0.0042, -0.0053, -0.0028],
        [-0.0127, -0.0220, -0.0098,  ...,  0.0046,  0.0272, -0.0008],
        [-0.0148, -0.0121, -0.0284,  ..., -0.0007,  0.0051, -0.0131],
        ...,
        [-0.0107,  0.0089, -0.0168,  ..., -0.0146,  0.0047,  0.0066],
        [ 0.0108,  0.0115,  0.0168,  ..., -0.0024, -0.0363,  0.0027],
        [-0.0092, -0.0043, -0.0052,  ..., -0.0157,  0.0171, -0.0357]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1602,  1.4512,  0.0827,  ..., -2.2422, -4.6562,  1.1660]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 11:25:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is remain, the past form is remained
If the present form is believe, the past form is believed
If the present form is perform, the past form is performed
If the present form is add, the past form is added
If the present form is refer, the past form is referred
If the present form is receive, the past form is received
If the present form is provide, the past form is provided
If the present form is discover, the past form is
2024-07-24 11:25:28 root INFO     [order_1_approx] starting weight calculation for If the present form is provide, the past form is provided
If the present form is discover, the past form is discovered
If the present form is receive, the past form is received
If the present form is refer, the past form is referred
If the present form is remain, the past form is remained
If the present form is add, the past form is added
If the present form is believe, the past form is believed
If the present form is perform, the past form is
2024-07-24 11:25:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 11:29:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0825,  0.8481,  1.4648,  ..., -0.3181,  0.3313, -0.0767],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7207, -0.3335,  0.1094,  ...,  2.5898,  0.2686,  0.8096],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0241, -0.0191,  0.0009,  ...,  0.0005,  0.0047, -0.0237],
        [-0.0015, -0.0311, -0.0015,  ..., -0.0017, -0.0013, -0.0062],
        [ 0.0065, -0.0052, -0.0162,  ...,  0.0071, -0.0009,  0.0023],
        ...,
        [-0.0267,  0.0066, -0.0073,  ..., -0.0210, -0.0008,  0.0101],
        [-0.0030,  0.0080,  0.0168,  ..., -0.0136, -0.0154,  0.0151],
        [-0.0156,  0.0102, -0.0019,  ..., -0.0015, -0.0012, -0.0144]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.8047,  0.1670, -0.1602,  ...,  2.8965, -0.0828,  0.8354]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 11:29:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is provide, the past form is provided
If the present form is discover, the past form is discovered
If the present form is receive, the past form is received
If the present form is refer, the past form is referred
If the present form is remain, the past form is remained
If the present form is add, the past form is added
If the present form is believe, the past form is believed
If the present form is perform, the past form is
2024-07-24 11:29:15 root INFO     [order_1_approx] starting weight calculation for If the present form is add, the past form is added
If the present form is remain, the past form is remained
If the present form is receive, the past form is received
If the present form is discover, the past form is discovered
If the present form is believe, the past form is believed
If the present form is refer, the past form is referred
If the present form is perform, the past form is performed
If the present form is provide, the past form is
2024-07-24 11:29:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 11:33:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7378,  0.2375, -0.7559,  ...,  0.5552,  0.9253,  0.0742],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6367,  1.1914, -1.0762,  ...,  1.2783, -0.6177, -0.2998],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0077, -0.0015, -0.0027,  ..., -0.0036,  0.0035, -0.0027],
        [-0.0038, -0.0155, -0.0049,  ...,  0.0011,  0.0022, -0.0082],
        [ 0.0089, -0.0056, -0.0059,  ..., -0.0027,  0.0024, -0.0061],
        ...,
        [-0.0203,  0.0258, -0.0131,  ..., -0.0058, -0.0012,  0.0097],
        [ 0.0154,  0.0015,  0.0113,  ..., -0.0246, -0.0184,  0.0230],
        [-0.0115, -0.0046,  0.0070,  ..., -0.0011,  0.0145, -0.0265]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1055,  1.7070, -1.1836,  ...,  1.2949, -1.2168, -0.6265]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 11:33:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is add, the past form is added
If the present form is remain, the past form is remained
If the present form is receive, the past form is received
If the present form is discover, the past form is discovered
If the present form is believe, the past form is believed
If the present form is refer, the past form is referred
If the present form is perform, the past form is performed
If the present form is provide, the past form is
2024-07-24 11:33:56 root INFO     [order_1_approx] starting weight calculation for If the present form is believe, the past form is believed
If the present form is refer, the past form is referred
If the present form is provide, the past form is provided
If the present form is perform, the past form is performed
If the present form is remain, the past form is remained
If the present form is discover, the past form is discovered
If the present form is add, the past form is added
If the present form is receive, the past form is
2024-07-24 11:33:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 11:39:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3323, -1.1064,  0.5986,  ...,  0.2463, -0.1091,  1.0781],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.8477, -0.4482, -1.4404,  ...,  2.0137,  1.6523,  1.4639],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.6022e-02, -1.9226e-03, -1.5388e-02,  ..., -7.1030e-03,
         -4.8065e-04, -1.2352e-02],
        [ 9.2983e-05, -1.4664e-02,  2.3327e-03,  ..., -2.2736e-03,
          1.0345e-02, -1.5106e-03],
        [ 3.5763e-03, -4.2496e-03, -2.1454e-02,  ..., -2.1534e-03,
          3.8872e-03,  6.4125e-03],
        ...,
        [ 8.1730e-04,  3.7766e-04, -1.3184e-02,  ..., -1.1948e-02,
          6.4430e-03,  7.2708e-03],
        [ 6.5346e-03, -8.9493e-03,  6.9466e-03,  ..., -1.0742e-02,
         -3.0136e-02,  5.8289e-03],
        [-3.3875e-03,  6.8283e-03, -1.8730e-03,  ..., -1.1505e-02,
          9.9945e-04, -3.2440e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.9863,  0.2715, -1.2676,  ...,  1.6211,  0.7734,  1.5000]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 11:39:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is believe, the past form is believed
If the present form is refer, the past form is referred
If the present form is provide, the past form is provided
If the present form is perform, the past form is performed
If the present form is remain, the past form is remained
If the present form is discover, the past form is discovered
If the present form is add, the past form is added
If the present form is receive, the past form is
2024-07-24 11:39:36 root INFO     [order_1_approx] starting weight calculation for If the present form is receive, the past form is received
If the present form is discover, the past form is discovered
If the present form is add, the past form is added
If the present form is perform, the past form is performed
If the present form is believe, the past form is believed
If the present form is remain, the past form is remained
If the present form is provide, the past form is provided
If the present form is refer, the past form is
2024-07-24 11:39:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 11:43:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0312,  0.7954,  0.4055,  ..., -0.6895,  0.1809, -0.2303],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([1.1221, 0.1077, 2.2324,  ..., 4.5469, 2.1953, 0.0215], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0168, -0.0156,  0.0030,  ..., -0.0094, -0.0004,  0.0017],
        [-0.0104, -0.0177,  0.0027,  ...,  0.0035,  0.0153, -0.0137],
        [-0.0002, -0.0058, -0.0240,  ..., -0.0010, -0.0041, -0.0111],
        ...,
        [-0.0249, -0.0034, -0.0097,  ..., -0.0309,  0.0088, -0.0017],
        [-0.0071, -0.0026,  0.0214,  ..., -0.0145, -0.0274, -0.0014],
        [-0.0075,  0.0091,  0.0131,  ..., -0.0057,  0.0069, -0.0216]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[0.8877, 0.3655, 2.3652,  ..., 4.3867, 2.2773, 0.5107]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 11:43:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is receive, the past form is received
If the present form is discover, the past form is discovered
If the present form is add, the past form is added
If the present form is perform, the past form is performed
If the present form is believe, the past form is believed
If the present form is remain, the past form is remained
If the present form is provide, the past form is provided
If the present form is refer, the past form is
2024-07-24 11:43:57 root INFO     [order_1_approx] starting weight calculation for If the present form is receive, the past form is received
If the present form is perform, the past form is performed
If the present form is provide, the past form is provided
If the present form is discover, the past form is discovered
If the present form is believe, the past form is believed
If the present form is add, the past form is added
If the present form is refer, the past form is referred
If the present form is remain, the past form is
2024-07-24 11:43:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 11:47:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1365, -0.2913,  1.7734,  ...,  1.2256, -0.2375,  0.2622],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.6328,  1.5801, -2.2383,  ...,  2.2031,  1.1562,  0.3901],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0153, -0.0005, -0.0052,  ...,  0.0083,  0.0035, -0.0217],
        [ 0.0014, -0.0245,  0.0001,  ...,  0.0023,  0.0144, -0.0041],
        [-0.0027,  0.0126, -0.0334,  ...,  0.0088,  0.0123,  0.0002],
        ...,
        [-0.0169, -0.0020, -0.0015,  ..., -0.0127, -0.0135,  0.0068],
        [ 0.0030, -0.0077,  0.0055,  ..., -0.0127, -0.0184,  0.0139],
        [-0.0194, -0.0018, -0.0170,  ..., -0.0086, -0.0038, -0.0272]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.2734,  1.8047, -1.9141,  ...,  2.5605,  0.8057, -0.1396]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 11:47:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is receive, the past form is received
If the present form is perform, the past form is performed
If the present form is provide, the past form is provided
If the present form is discover, the past form is discovered
If the present form is believe, the past form is believed
If the present form is add, the past form is added
If the present form is refer, the past form is referred
If the present form is remain, the past form is
2024-07-24 11:47:46 root INFO     total operator prediction time: 2020.3086078166962 seconds
2024-07-24 11:47:46 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_Ving - Ved
2024-07-24 11:47:46 root INFO     building operator verb_Ving - Ved
2024-07-24 11:47:47 root INFO     [order_1_approx] starting weight calculation for After something is telling, it has told
After something is replacing, it has replaced
After something is receiving, it has received
After something is following, it has followed
After something is asking, it has asked
After something is requiring, it has required
After something is considering, it has considered
After something is allowing, it has
2024-07-24 11:47:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 11:51:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1279, -0.4932,  1.1953,  ...,  0.0633,  0.1306,  0.2429],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0957, -0.0437,  1.3584,  ...,  1.9736, -2.0918,  4.0859],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0113,  0.0009,  0.0031,  ...,  0.0075,  0.0046, -0.0081],
        [ 0.0028, -0.0378,  0.0077,  ...,  0.0044,  0.0084, -0.0028],
        [-0.0005, -0.0214, -0.0273,  ...,  0.0088,  0.0008, -0.0160],
        ...,
        [-0.0403, -0.0249, -0.0111,  ..., -0.0224,  0.0120, -0.0002],
        [ 0.0118, -0.0150,  0.0125,  ..., -0.0114, -0.0211,  0.0301],
        [-0.0043, -0.0058, -0.0023,  ...,  0.0100, -0.0149, -0.0071]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4424,  0.2410,  1.2998,  ...,  1.3828, -2.4180,  3.3613]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 11:51:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is telling, it has told
After something is replacing, it has replaced
After something is receiving, it has received
After something is following, it has followed
After something is asking, it has asked
After something is requiring, it has required
After something is considering, it has considered
After something is allowing, it has
2024-07-24 11:51:37 root INFO     [order_1_approx] starting weight calculation for After something is considering, it has considered
After something is requiring, it has required
After something is following, it has followed
After something is telling, it has told
After something is allowing, it has allowed
After something is receiving, it has received
After something is replacing, it has replaced
After something is asking, it has
2024-07-24 11:51:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 11:55:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.6465,  0.1025,  0.4785,  ..., -0.8296,  1.2256,  0.6211],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([1.8271, 2.0000, 2.9414,  ..., 3.3906, 0.1123, 3.6172], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0184, -0.0197,  0.0020,  ...,  0.0043, -0.0195, -0.0054],
        [-0.0089, -0.0180, -0.0100,  ...,  0.0069,  0.0181, -0.0070],
        [ 0.0125,  0.0071, -0.0197,  ...,  0.0152, -0.0045,  0.0015],
        ...,
        [-0.0216, -0.0156, -0.0076,  ..., -0.0350,  0.0239, -0.0062],
        [ 0.0033, -0.0023,  0.0114,  ..., -0.0096, -0.0449,  0.0231],
        [-0.0037,  0.0082,  0.0054,  ...,  0.0129,  0.0203, -0.0199]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3496,  1.4023,  2.7207,  ...,  3.0254, -0.1633,  2.1758]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 11:55:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is considering, it has considered
After something is requiring, it has required
After something is following, it has followed
After something is telling, it has told
After something is allowing, it has allowed
After something is receiving, it has received
After something is replacing, it has replaced
After something is asking, it has
2024-07-24 11:55:30 root INFO     [order_1_approx] starting weight calculation for After something is replacing, it has replaced
After something is receiving, it has received
After something is requiring, it has required
After something is telling, it has told
After something is allowing, it has allowed
After something is following, it has followed
After something is asking, it has asked
After something is considering, it has
2024-07-24 11:55:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 11:59:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2871, -0.1885, -0.1768,  ...,  0.4185, -0.0200,  1.2295],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8242,  2.8379,  0.5879,  ...,  1.3857, -4.7773, -0.9053],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0215, -0.0075,  0.0155,  ...,  0.0005, -0.0170, -0.0037],
        [-0.0168, -0.0199, -0.0127,  ..., -0.0005,  0.0220,  0.0008],
        [-0.0175, -0.0015, -0.0209,  ...,  0.0070,  0.0037, -0.0215],
        ...,
        [-0.0140, -0.0182, -0.0001,  ..., -0.0238,  0.0213, -0.0074],
        [ 0.0130, -0.0108,  0.0005,  ..., -0.0286, -0.0273,  0.0300],
        [ 0.0061,  0.0126, -0.0156,  ...,  0.0159,  0.0011, -0.0312]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4136,  2.9707,  0.3770,  ...,  2.0254, -4.8320, -0.6792]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 11:59:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is replacing, it has replaced
After something is receiving, it has received
After something is requiring, it has required
After something is telling, it has told
After something is allowing, it has allowed
After something is following, it has followed
After something is asking, it has asked
After something is considering, it has
2024-07-24 11:59:24 root INFO     [order_1_approx] starting weight calculation for After something is allowing, it has allowed
After something is considering, it has considered
After something is asking, it has asked
After something is requiring, it has required
After something is receiving, it has received
After something is telling, it has told
After something is replacing, it has replaced
After something is following, it has
2024-07-24 11:59:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 12:03:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1230, -0.5156,  1.9160,  ...,  0.2798, -0.2925,  1.2988],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6494,  0.4507, -0.9121,  ..., -1.4307,  1.2607,  3.4688],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0056, -0.0099,  0.0130,  ...,  0.0063, -0.0339, -0.0254],
        [ 0.0012, -0.0187,  0.0084,  ...,  0.0049, -0.0038, -0.0129],
        [-0.0009, -0.0109,  0.0019,  ...,  0.0103, -0.0162,  0.0105],
        ...,
        [-0.0202, -0.0161,  0.0031,  ..., -0.0086,  0.0085,  0.0066],
        [ 0.0012,  0.0069,  0.0058,  ...,  0.0025, -0.0251,  0.0119],
        [-0.0063,  0.0166, -0.0026,  ..., -0.0100,  0.0188, -0.0282]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1055,  0.9106, -0.9292,  ..., -0.9746,  2.2793,  2.3867]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 12:03:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is allowing, it has allowed
After something is considering, it has considered
After something is asking, it has asked
After something is requiring, it has required
After something is receiving, it has received
After something is telling, it has told
After something is replacing, it has replaced
After something is following, it has
2024-07-24 12:03:17 root INFO     [order_1_approx] starting weight calculation for After something is requiring, it has required
After something is telling, it has told
After something is allowing, it has allowed
After something is replacing, it has replaced
After something is considering, it has considered
After something is following, it has followed
After something is asking, it has asked
After something is receiving, it has
2024-07-24 12:03:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 12:07:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7930, -0.5088,  1.0898,  ...,  0.0295,  0.3706,  1.0781],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([2.6484, 2.0664, 0.7559,  ..., 2.7812, 0.0630, 4.2266], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0343, -0.0180, -0.0046,  ..., -0.0103, -0.0290, -0.0316],
        [ 0.0146, -0.0175, -0.0041,  ...,  0.0040,  0.0117,  0.0153],
        [ 0.0226, -0.0102, -0.0173,  ...,  0.0029,  0.0124,  0.0028],
        ...,
        [-0.0095, -0.0263, -0.0331,  ..., -0.0219,  0.0050,  0.0060],
        [ 0.0336,  0.0070, -0.0197,  ..., -0.0190, -0.0469,  0.0524],
        [-0.0013, -0.0105,  0.0069,  ..., -0.0119,  0.0088, -0.0362]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1992,  2.4043,  0.7754,  ...,  3.1426, -0.9468,  3.7188]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 12:07:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is requiring, it has required
After something is telling, it has told
After something is allowing, it has allowed
After something is replacing, it has replaced
After something is considering, it has considered
After something is following, it has followed
After something is asking, it has asked
After something is receiving, it has
2024-07-24 12:07:08 root INFO     [order_1_approx] starting weight calculation for After something is asking, it has asked
After something is considering, it has considered
After something is receiving, it has received
After something is following, it has followed
After something is requiring, it has required
After something is telling, it has told
After something is allowing, it has allowed
After something is replacing, it has
2024-07-24 12:07:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 12:10:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5850,  0.1443,  1.0146,  ..., -0.1641,  0.2031, -0.7339],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.1699,  3.0312, -0.3491,  ...,  3.2852, -0.7715,  5.1680],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0221, -0.0106,  0.0010,  ..., -0.0036, -0.0204, -0.0088],
        [-0.0047, -0.0191, -0.0087,  ...,  0.0099,  0.0109,  0.0052],
        [ 0.0096, -0.0025, -0.0165,  ..., -0.0039, -0.0066, -0.0036],
        ...,
        [-0.0225, -0.0106, -0.0303,  ..., -0.0147,  0.0244,  0.0019],
        [-0.0116,  0.0055, -0.0063,  ..., -0.0035, -0.0408,  0.0090],
        [ 0.0174,  0.0073, -0.0021,  ..., -0.0212, -0.0142, -0.0388]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.4512,  2.9824, -0.4641,  ...,  2.9336, -1.5137,  5.2500]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 12:10:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is asking, it has asked
After something is considering, it has considered
After something is receiving, it has received
After something is following, it has followed
After something is requiring, it has required
After something is telling, it has told
After something is allowing, it has allowed
After something is replacing, it has
2024-07-24 12:10:56 root INFO     [order_1_approx] starting weight calculation for After something is telling, it has told
After something is following, it has followed
After something is asking, it has asked
After something is replacing, it has replaced
After something is considering, it has considered
After something is allowing, it has allowed
After something is receiving, it has received
After something is requiring, it has
2024-07-24 12:10:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 12:14:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2148, -0.3994,  0.9707,  ...,  0.2617,  0.8369,  0.9404],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.6484, -0.6465,  0.5303,  ...,  2.5664,  0.0830,  2.8984],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0242, -0.0061,  0.0025,  ..., -0.0086, -0.0191, -0.0045],
        [ 0.0001, -0.0055, -0.0210,  ...,  0.0056, -0.0049, -0.0013],
        [ 0.0214, -0.0054, -0.0176,  ...,  0.0258, -0.0153, -0.0105],
        ...,
        [-0.0380, -0.0285, -0.0331,  ..., -0.0256,  0.0032,  0.0052],
        [-0.0040, -0.0171,  0.0244,  ..., -0.0231, -0.0206,  0.0210],
        [ 0.0159, -0.0189,  0.0140,  ...,  0.0165, -0.0024, -0.0320]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.5703, -0.1118,  0.6851,  ...,  2.0820,  0.1134,  1.9961]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 12:14:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is telling, it has told
After something is following, it has followed
After something is asking, it has asked
After something is replacing, it has replaced
After something is considering, it has considered
After something is allowing, it has allowed
After something is receiving, it has received
After something is requiring, it has
2024-07-24 12:14:49 root INFO     [order_1_approx] starting weight calculation for After something is considering, it has considered
After something is replacing, it has replaced
After something is allowing, it has allowed
After something is requiring, it has required
After something is receiving, it has received
After something is following, it has followed
After something is asking, it has asked
After something is telling, it has
2024-07-24 12:14:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 12:19:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1523, -0.1309,  1.6777,  ...,  0.6553, -0.0710,  0.1674],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([2.0020, 1.3984, 3.2148,  ..., 2.7598, 1.0508, 1.6455], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.6495e-02, -3.3875e-02,  8.3618e-03,  ..., -2.6131e-03,
         -2.9053e-02, -1.7487e-02],
        [-5.9166e-03, -2.4323e-02,  2.8839e-03,  ...,  2.5482e-02,
          1.0704e-02,  1.6769e-02],
        [ 1.6098e-02,  3.0518e-05, -7.3242e-03,  ..., -1.9836e-03,
          3.9940e-03,  2.5055e-02],
        ...,
        [-2.2675e-02, -1.6693e-02, -3.3875e-02,  ..., -2.1469e-02,
          2.2125e-02, -3.7842e-03],
        [ 1.4969e-02, -1.0330e-02,  7.4883e-03,  ..., -1.5747e-02,
         -3.9978e-02,  2.5391e-02],
        [ 1.1765e-02,  2.4109e-02,  1.5366e-02,  ...,  1.4214e-02,
         -1.0109e-03, -1.1307e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[2.4434, 1.8311, 2.8242,  ..., 4.6914, 0.9463, 0.5156]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 12:19:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is considering, it has considered
After something is replacing, it has replaced
After something is allowing, it has allowed
After something is requiring, it has required
After something is receiving, it has received
After something is following, it has followed
After something is asking, it has asked
After something is telling, it has
2024-07-24 12:19:28 root INFO     total operator prediction time: 1901.302238225937 seconds
2024-07-24 12:19:28 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on Ving - verb_inf
2024-07-24 12:19:28 root INFO     building operator Ving - verb_inf
2024-07-24 12:19:28 root INFO     [order_1_approx] starting weight calculation for establishing is the active form of establish
operating is the active form of operate
teaching is the active form of teach
involving is the active form of involve
following is the active form of follow
improving is the active form of improve
reducing is the active form of reduce
enjoying is the active form of
2024-07-24 12:19:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
2024-07-24 12:24:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3203, -0.5742,  0.4766,  ..., -0.7334,  1.0469,  0.1089],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.0020,  1.3320,  1.9238,  ..., -1.4004,  1.9463,  3.3086],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0050, -0.0104,  0.0136,  ...,  0.0007, -0.0154, -0.0146],
        [-0.0017, -0.0112,  0.0123,  ...,  0.0041, -0.0042,  0.0004],
        [ 0.0069,  0.0083, -0.0109,  ..., -0.0143,  0.0067, -0.0088],
        ...,
        [-0.0269, -0.0041,  0.0055,  ..., -0.0193,  0.0027,  0.0039],
        [ 0.0040,  0.0036, -0.0041,  ...,  0.0065, -0.0164,  0.0214],
        [ 0.0109, -0.0067,  0.0036,  ..., -0.0184,  0.0122, -0.0228]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.8516,  1.7441,  2.3672,  ..., -1.3926,  1.4609,  3.0957]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 12:24:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for establishing is the active form of establish
operating is the active form of operate
teaching is the active form of teach
involving is the active form of involve
following is the active form of follow
improving is the active form of improve
reducing is the active form of reduce
enjoying is the active form of
2024-07-24 12:24:03 root INFO     [order_1_approx] starting weight calculation for teaching is the active form of teach
reducing is the active form of reduce
improving is the active form of improve
operating is the active form of operate
following is the active form of follow
involving is the active form of involve
enjoying is the active form of enjoy
establishing is the active form of
2024-07-24 12:24:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.6
