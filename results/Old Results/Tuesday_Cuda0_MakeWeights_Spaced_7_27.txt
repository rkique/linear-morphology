2024-07-23 09:20:21 root INFO     loading model + tokenizer
2024-07-23 09:21:26 root INFO     loading model + tokenizer
2024-07-23 09:21:43 root INFO     model + tokenizer loaded
2024-07-23 09:21:43 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - part
2024-07-23 09:21:43 root INFO     building operator meronyms - part
2024-07-23 09:21:43 root INFO     total operator prediction time: 0.00030875205993652344 seconds
2024-07-23 09:21:43 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on synonyms - exact
2024-07-23 09:21:43 root INFO     building operator synonyms - exact
2024-07-23 09:21:43 root INFO     [order_1_approx] starting weight calculation for Another word for new is modern
Another word for baby is infant
Another word for homogeneous is uniform
Another word for lazy is indolent
Another word for villain is scoundrel
Another word for snake is serpent
Another word for obsolete is outdated
Another word for auto is
2024-07-23 09:21:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 09:25:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0596,  0.5107, -0.4678,  ...,  0.8789, -0.8071,  0.6709],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.3555, -2.3711, -2.3223,  ..., -1.3125, -0.2605, -0.3281],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0039, -0.0171, -0.0133,  ..., -0.0004,  0.0109,  0.0046],
        [ 0.0001, -0.0014,  0.0038,  ...,  0.0003,  0.0106,  0.0090],
        [-0.0027,  0.0033, -0.0177,  ..., -0.0238,  0.0246,  0.0519],
        ...,
        [-0.0122,  0.0170,  0.0143,  ...,  0.0117, -0.0086,  0.0152],
        [ 0.0185, -0.0092, -0.0169,  ..., -0.0053,  0.0064, -0.0088],
        [-0.0032, -0.0029, -0.0033,  ..., -0.0058, -0.0002, -0.0005]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.7598, -1.5264, -1.9648,  ..., -0.9580,  0.4568,  0.2021]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 09:25:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for new is modern
Another word for baby is infant
Another word for homogeneous is uniform
Another word for lazy is indolent
Another word for villain is scoundrel
Another word for snake is serpent
Another word for obsolete is outdated
Another word for auto is
2024-07-23 09:25:27 root INFO     [order_1_approx] starting weight calculation for Another word for homogeneous is uniform
Another word for villain is scoundrel
Another word for new is modern
Another word for obsolete is outdated
Another word for auto is car
Another word for lazy is indolent
Another word for snake is serpent
Another word for baby is
2024-07-23 09:25:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 09:29:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6738,  0.6021, -1.1875,  ...,  0.5063, -0.8770, -0.4229],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4758, -1.9199, -3.6074,  ...,  0.1121,  1.9043,  0.3662],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0215,  0.0002,  0.0110,  ...,  0.0021,  0.0034, -0.0139],
        [-0.0014, -0.0126, -0.0027,  ...,  0.0066, -0.0104, -0.0162],
        [-0.0053,  0.0237, -0.0154,  ..., -0.0079,  0.0207,  0.0156],
        ...,
        [-0.0198,  0.0017,  0.0034,  ..., -0.0022, -0.0133, -0.0045],
        [ 0.0012, -0.0048, -0.0213,  ..., -0.0040, -0.0284, -0.0053],
        [-0.0155, -0.0131, -0.0038,  ..., -0.0019,  0.0001,  0.0067]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0039, -1.6689, -3.1367,  ...,  0.5093,  2.2129,  0.8486]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 09:29:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for homogeneous is uniform
Another word for villain is scoundrel
Another word for new is modern
Another word for obsolete is outdated
Another word for auto is car
Another word for lazy is indolent
Another word for snake is serpent
Another word for baby is
2024-07-23 09:29:11 root INFO     [order_1_approx] starting weight calculation for Another word for obsolete is outdated
Another word for new is modern
Another word for villain is scoundrel
Another word for baby is infant
Another word for snake is serpent
Another word for lazy is indolent
Another word for auto is car
Another word for homogeneous is
2024-07-23 09:29:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 09:32:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1104,  1.1133,  1.4619,  ..., -0.3406,  0.7646,  1.1562],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3438,  2.3242, -5.2539,  ...,  2.7285,  0.6548, -2.2695],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0065, -0.0078,  0.0063,  ..., -0.0012, -0.0035, -0.0133],
        [ 0.0016,  0.0047, -0.0093,  ...,  0.0004, -0.0039, -0.0051],
        [ 0.0018, -0.0113, -0.0125,  ...,  0.0008,  0.0008,  0.0013],
        ...,
        [ 0.0033, -0.0126,  0.0016,  ...,  0.0093,  0.0036,  0.0165],
        [-0.0105, -0.0007,  0.0059,  ..., -0.0039,  0.0111,  0.0034],
        [ 0.0048,  0.0018, -0.0017,  ..., -0.0054,  0.0120,  0.0168]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.8403,  2.6230, -5.0234,  ...,  3.2441,  0.6211, -2.0312]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 09:32:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for obsolete is outdated
Another word for new is modern
Another word for villain is scoundrel
Another word for baby is infant
Another word for snake is serpent
Another word for lazy is indolent
Another word for auto is car
Another word for homogeneous is
2024-07-23 09:32:56 root INFO     [order_1_approx] starting weight calculation for Another word for new is modern
Another word for auto is car
Another word for baby is infant
Another word for snake is serpent
Another word for homogeneous is uniform
Another word for obsolete is outdated
Another word for villain is scoundrel
Another word for lazy is
2024-07-23 09:32:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 09:36:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1406,  0.0259, -0.0814,  ...,  1.0518, -0.0183,  1.3818],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6836, -0.4539, -1.6562,  ..., -2.7891, -4.9141, -0.9038],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0044, -0.0172,  0.0113,  ..., -0.0078, -0.0063,  0.0061],
        [ 0.0114,  0.0023, -0.0077,  ...,  0.0120, -0.0005,  0.0056],
        [-0.0188,  0.0086, -0.0133,  ..., -0.0096, -0.0081,  0.0030],
        ...,
        [ 0.0094,  0.0039, -0.0004,  ...,  0.0216, -0.0096,  0.0064],
        [ 0.0073, -0.0116,  0.0056,  ..., -0.0053,  0.0060, -0.0120],
        [-0.0025,  0.0006,  0.0062,  ...,  0.0034,  0.0045,  0.0194]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5586, -0.9170, -1.4961,  ..., -2.9688, -4.2109, -1.0908]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 09:36:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for new is modern
Another word for auto is car
Another word for baby is infant
Another word for snake is serpent
Another word for homogeneous is uniform
Another word for obsolete is outdated
Another word for villain is scoundrel
Another word for lazy is
2024-07-23 09:36:40 root INFO     [order_1_approx] starting weight calculation for Another word for snake is serpent
Another word for homogeneous is uniform
Another word for lazy is indolent
Another word for villain is scoundrel
Another word for obsolete is outdated
Another word for baby is infant
Another word for auto is car
Another word for new is
2024-07-23 09:36:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 09:40:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4092,  0.3223, -0.6943,  ..., -0.0831,  0.2001,  0.2681],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0469, -4.8125, -2.9648,  ..., -1.8877, -0.6924,  0.6201],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0026, -0.0273, -0.0005,  ..., -0.0190,  0.0046, -0.0072],
        [-0.0043,  0.0153, -0.0129,  ...,  0.0018,  0.0025,  0.0064],
        [ 0.0065,  0.0094,  0.0022,  ...,  0.0065, -0.0076,  0.0132],
        ...,
        [ 0.0031, -0.0236,  0.0035,  ..., -0.0085,  0.0038,  0.0065],
        [ 0.0077,  0.0041,  0.0051,  ..., -0.0151, -0.0130, -0.0062],
        [-0.0036, -0.0183, -0.0083,  ...,  0.0052,  0.0024,  0.0057]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1543, -4.6094, -1.6543,  ..., -1.9512,  0.2017,  0.7915]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 09:40:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for snake is serpent
Another word for homogeneous is uniform
Another word for lazy is indolent
Another word for villain is scoundrel
Another word for obsolete is outdated
Another word for baby is infant
Another word for auto is car
Another word for new is
2024-07-23 09:40:24 root INFO     [order_1_approx] starting weight calculation for Another word for auto is car
Another word for new is modern
Another word for homogeneous is uniform
Another word for snake is serpent
Another word for lazy is indolent
Another word for baby is infant
Another word for villain is scoundrel
Another word for obsolete is
2024-07-23 09:40:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 09:44:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3201,  0.4089,  0.8706,  ..., -0.8735,  0.7104,  1.2666],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.1230, -2.2090, -3.1895,  ..., -2.7852,  1.7930,  1.4219],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0152, -0.0072,  0.0099,  ...,  0.0126, -0.0181, -0.0165],
        [-0.0076, -0.0003, -0.0120,  ...,  0.0085, -0.0088, -0.0069],
        [ 0.0061,  0.0019, -0.0122,  ...,  0.0062, -0.0040, -0.0020],
        ...,
        [ 0.0055, -0.0126,  0.0066,  ...,  0.0014, -0.0101, -0.0054],
        [ 0.0163, -0.0085, -0.0070,  ...,  0.0216,  0.0004, -0.0107],
        [-0.0089, -0.0104,  0.0044,  ...,  0.0102,  0.0079,  0.0309]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.3926, -1.9854, -2.9980,  ..., -2.0156,  1.6416,  1.3291]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 09:44:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for auto is car
Another word for new is modern
Another word for homogeneous is uniform
Another word for snake is serpent
Another word for lazy is indolent
Another word for baby is infant
Another word for villain is scoundrel
Another word for obsolete is
2024-07-23 09:44:11 root INFO     [order_1_approx] starting weight calculation for Another word for obsolete is outdated
Another word for auto is car
Another word for homogeneous is uniform
Another word for lazy is indolent
Another word for baby is infant
Another word for new is modern
Another word for villain is scoundrel
Another word for snake is
2024-07-23 09:44:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 09:47:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1592, -0.4739, -1.6650,  ...,  0.9033, -0.2029,  0.6230],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.6523,  0.0754, -1.4287,  ...,  1.4365, -0.0276, -2.5586],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.2550e-03, -9.6989e-04,  1.1002e-02,  ...,  1.6632e-02,
         -1.6098e-02, -3.2837e-02],
        [-1.1307e-02, -6.1951e-03, -2.6245e-03,  ...,  9.3079e-03,
         -2.2659e-02, -1.0254e-02],
        [-1.4290e-02, -9.6817e-03,  2.4414e-03,  ..., -7.4844e-03,
          1.4458e-02,  3.7720e-02],
        ...,
        [ 9.5367e-06, -1.2573e-02,  1.4610e-02,  ...,  1.1612e-02,
         -1.5129e-02,  1.6357e-02],
        [ 9.3460e-03, -4.6722e-02,  1.2711e-02,  ...,  1.5450e-03,
          1.0574e-02,  1.7731e-02],
        [-2.0126e-02, -1.6724e-02,  9.6588e-03,  ...,  2.6550e-03,
         -4.4060e-04,  2.1286e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.8359,  0.4138, -1.2061,  ...,  2.0449,  1.2578, -2.9727]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 09:47:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for obsolete is outdated
Another word for auto is car
Another word for homogeneous is uniform
Another word for lazy is indolent
Another word for baby is infant
Another word for new is modern
Another word for villain is scoundrel
Another word for snake is
2024-07-23 09:47:56 root INFO     [order_1_approx] starting weight calculation for Another word for snake is serpent
Another word for obsolete is outdated
Another word for lazy is indolent
Another word for new is modern
Another word for baby is infant
Another word for homogeneous is uniform
Another word for auto is car
Another word for villain is
2024-07-23 09:47:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 09:51:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1641, -1.0713, -0.0216,  ...,  0.7847,  0.5039,  0.5732],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.5371, -2.7227, -1.8594,  ...,  1.0576,  1.9883, -0.0254],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0052, -0.0163, -0.0137,  ..., -0.0142,  0.0036, -0.0188],
        [ 0.0069, -0.0163, -0.0032,  ..., -0.0084, -0.0039, -0.0081],
        [-0.0064,  0.0040, -0.0040,  ..., -0.0049,  0.0142,  0.0103],
        ...,
        [-0.0074,  0.0050,  0.0016,  ...,  0.0130, -0.0132,  0.0049],
        [ 0.0135, -0.0050,  0.0132,  ...,  0.0095,  0.0066,  0.0084],
        [-0.0030, -0.0078,  0.0111,  ...,  0.0087, -0.0012,  0.0258]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.8711, -2.4570, -1.1514,  ...,  1.1494,  1.3652,  0.1820]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 09:51:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for snake is serpent
Another word for obsolete is outdated
Another word for lazy is indolent
Another word for new is modern
Another word for baby is infant
Another word for homogeneous is uniform
Another word for auto is car
Another word for villain is
2024-07-23 09:51:40 root INFO     total operator prediction time: 1797.6441087722778 seconds
2024-07-23 09:51:40 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on hypernyms - misc
2024-07-23 09:51:40 root INFO     building operator hypernyms - misc
2024-07-23 09:51:41 root INFO     [order_1_approx] starting weight calculation for The vase falls into the category of jar
The toaster falls into the category of appliance
The hamburger falls into the category of sandwich
The sweater falls into the category of clothes
The croissant falls into the category of pastry
The bracelet falls into the category of jewelry
The blender falls into the category of appliance
The armchair falls into the category of
2024-07-23 09:51:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 09:55:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1436, -0.7275,  0.6196,  ...,  0.3235, -0.3086, -1.2725],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1064, -2.7539, -1.9756,  ..., -2.4570, -1.7344, -2.6777],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0141,  0.0048, -0.0040,  ...,  0.0052, -0.0007,  0.0017],
        [-0.0009,  0.0132,  0.0030,  ...,  0.0044, -0.0002, -0.0027],
        [-0.0073,  0.0038,  0.0039,  ...,  0.0050,  0.0021,  0.0050],
        ...,
        [-0.0035, -0.0016,  0.0011,  ...,  0.0048, -0.0090,  0.0069],
        [-0.0063,  0.0072,  0.0023,  ...,  0.0003, -0.0022, -0.0008],
        [ 0.0021, -0.0027,  0.0005,  ...,  0.0021,  0.0159,  0.0125]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7554, -2.7363, -2.2031,  ..., -2.2520, -1.8174, -2.8496]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 09:55:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The vase falls into the category of jar
The toaster falls into the category of appliance
The hamburger falls into the category of sandwich
The sweater falls into the category of clothes
The croissant falls into the category of pastry
The bracelet falls into the category of jewelry
The blender falls into the category of appliance
The armchair falls into the category of
2024-07-23 09:55:25 root INFO     [order_1_approx] starting weight calculation for The croissant falls into the category of pastry
The bracelet falls into the category of jewelry
The hamburger falls into the category of sandwich
The toaster falls into the category of appliance
The sweater falls into the category of clothes
The vase falls into the category of jar
The armchair falls into the category of chair
The blender falls into the category of
2024-07-23 09:55:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 09:59:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0557,  0.5381, -1.9121,  ..., -0.1279, -0.4031,  0.8882],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.4277,  1.4131, -0.3979,  ...,  2.0059, -0.5215, -1.6982],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.8107e-02, -8.0681e-04, -1.0025e-02,  ...,  5.0278e-03,
          4.3945e-03, -9.5673e-03],
        [-6.1760e-03,  1.9043e-02,  3.0899e-04,  ..., -3.4409e-03,
         -6.6910e-03,  5.5847e-03],
        [-1.8921e-03,  5.7068e-03,  1.5518e-02,  ..., -6.7062e-03,
          4.9782e-04, -1.5831e-03],
        ...,
        [-6.6605e-03, -2.1317e-02,  9.3842e-04,  ...,  2.5635e-02,
         -2.3788e-02,  1.1948e-02],
        [-5.7144e-03,  7.2250e-03,  1.0445e-02,  ..., -1.5259e-05,
         -1.6727e-03,  2.1255e-02],
        [ 9.5520e-03, -6.1646e-03, -5.6686e-03,  ...,  1.7757e-03,
          1.3573e-02,  2.6794e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.3926,  2.2070,  0.1362,  ...,  2.2129, -0.3359, -1.4551]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 09:59:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The croissant falls into the category of pastry
The bracelet falls into the category of jewelry
The hamburger falls into the category of sandwich
The toaster falls into the category of appliance
The sweater falls into the category of clothes
The vase falls into the category of jar
The armchair falls into the category of chair
The blender falls into the category of
2024-07-23 09:59:11 root INFO     [order_1_approx] starting weight calculation for The vase falls into the category of jar
The croissant falls into the category of pastry
The sweater falls into the category of clothes
The armchair falls into the category of chair
The blender falls into the category of appliance
The hamburger falls into the category of sandwich
The toaster falls into the category of appliance
The bracelet falls into the category of
2024-07-23 09:59:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 10:02:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4907, -0.8745, -0.4236,  ...,  0.0408,  0.2603,  0.5522],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-5.9375, -1.6582, -0.9390,  ..., -4.1641, -2.8633,  2.3594],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0060,  0.0157, -0.0135,  ..., -0.0038,  0.0042,  0.0143],
        [-0.0136,  0.0096, -0.0067,  ...,  0.0088, -0.0002,  0.0059],
        [ 0.0007,  0.0137,  0.0024,  ...,  0.0136,  0.0041,  0.0028],
        ...,
        [ 0.0024, -0.0016,  0.0007,  ...,  0.0146, -0.0213,  0.0028],
        [-0.0047,  0.0119, -0.0046,  ...,  0.0032, -0.0170,  0.0179],
        [-0.0007, -0.0177,  0.0047,  ...,  0.0018,  0.0197,  0.0091]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-6.6641, -1.4863, -1.4551,  ..., -4.4141, -3.3379,  2.7129]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 10:02:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The vase falls into the category of jar
The croissant falls into the category of pastry
The sweater falls into the category of clothes
The armchair falls into the category of chair
The blender falls into the category of appliance
The hamburger falls into the category of sandwich
The toaster falls into the category of appliance
The bracelet falls into the category of
2024-07-23 10:02:55 root INFO     [order_1_approx] starting weight calculation for The toaster falls into the category of appliance
The vase falls into the category of jar
The bracelet falls into the category of jewelry
The armchair falls into the category of chair
The blender falls into the category of appliance
The hamburger falls into the category of sandwich
The sweater falls into the category of clothes
The croissant falls into the category of
2024-07-23 10:02:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 10:06:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7598, -1.2461, -0.6792,  ...,  0.1731, -1.7305, -0.8013],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-5.2461,  2.0469, -0.9844,  ...,  0.2744, -1.5381,  1.2148],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.5045e-02, -4.1428e-03, -2.9373e-04,  ..., -3.9825e-03,
          7.2937e-03,  8.8882e-03],
        [-3.7651e-03,  3.9978e-03,  3.0231e-03,  ...,  3.6983e-03,
          6.8207e-03, -5.3329e-03],
        [ 1.8463e-03,  6.4507e-03, -9.1476e-03,  ...,  3.2558e-03,
          4.1199e-03,  5.6381e-03],
        ...,
        [-8.2855e-03, -7.6294e-03,  3.8090e-03,  ...,  7.4196e-03,
         -9.9945e-03, -5.6343e-03],
        [ 2.4796e-04, -6.6280e-05, -3.4523e-03,  ...,  7.8678e-04,
         -3.9902e-03,  4.6844e-03],
        [-8.3466e-03, -2.6360e-03,  2.5291e-03,  ..., -5.6915e-03,
         -7.6485e-04,  2.8801e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-6.0000,  2.2734, -0.7983,  ...,  0.6387, -1.5098,  0.8457]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 10:06:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The toaster falls into the category of appliance
The vase falls into the category of jar
The bracelet falls into the category of jewelry
The armchair falls into the category of chair
The blender falls into the category of appliance
The hamburger falls into the category of sandwich
The sweater falls into the category of clothes
The croissant falls into the category of
2024-07-23 10:06:37 root INFO     [order_1_approx] starting weight calculation for The toaster falls into the category of appliance
The blender falls into the category of appliance
The croissant falls into the category of pastry
The vase falls into the category of jar
The armchair falls into the category of chair
The bracelet falls into the category of jewelry
The sweater falls into the category of clothes
The hamburger falls into the category of
2024-07-23 10:06:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 10:10:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0680,  1.3965, -0.1785,  ...,  0.1005, -0.4817,  0.6353],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3262,  1.9199, -1.0674,  ..., -1.0645, -0.5801,  0.0352],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.4297e-02, -8.2397e-03, -3.6335e-03,  ...,  3.2539e-03,
         -8.0948e-03,  6.4850e-04],
        [-1.1002e-02, -2.7847e-03,  1.3342e-03,  ...,  8.5602e-03,
          2.2774e-03, -3.4332e-05],
        [-2.8934e-03,  3.1738e-03, -9.6512e-03,  ..., -8.4686e-04,
          5.5389e-03, -4.9782e-04],
        ...,
        [-1.0712e-02, -9.3937e-04, -4.1389e-03,  ...,  4.2114e-03,
         -5.3482e-03,  7.5569e-03],
        [ 5.5923e-03,  4.5776e-03,  2.4490e-03,  ..., -7.4577e-03,
         -2.1381e-03,  2.6836e-03],
        [-1.4877e-02,  2.1496e-03, -3.7918e-03,  ..., -1.5526e-03,
          7.2021e-03,  5.5389e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.1289,  2.3457, -1.9844,  ..., -1.6152, -0.9834,  0.5186]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 10:10:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The toaster falls into the category of appliance
The blender falls into the category of appliance
The croissant falls into the category of pastry
The vase falls into the category of jar
The armchair falls into the category of chair
The bracelet falls into the category of jewelry
The sweater falls into the category of clothes
The hamburger falls into the category of
2024-07-23 10:10:21 root INFO     [order_1_approx] starting weight calculation for The armchair falls into the category of chair
The toaster falls into the category of appliance
The croissant falls into the category of pastry
The hamburger falls into the category of sandwich
The bracelet falls into the category of jewelry
The blender falls into the category of appliance
The vase falls into the category of jar
The sweater falls into the category of
2024-07-23 10:10:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 10:14:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1892,  0.4683, -0.5493,  ...,  0.7544, -0.0073, -0.2588],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6484, -0.9863, -0.0879,  ..., -3.6875, -2.1582, -5.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0171, -0.0029, -0.0187,  ..., -0.0007,  0.0056,  0.0129],
        [-0.0112,  0.0056,  0.0089,  ...,  0.0089, -0.0084, -0.0062],
        [-0.0084,  0.0119, -0.0022,  ..., -0.0069,  0.0022,  0.0076],
        ...,
        [-0.0012,  0.0070, -0.0022,  ...,  0.0091, -0.0126,  0.0075],
        [ 0.0020,  0.0123, -0.0047,  ..., -0.0030,  0.0016,  0.0034],
        [ 0.0062, -0.0032,  0.0023,  ..., -0.0005,  0.0172,  0.0258]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.7930, -0.8467, -0.7661,  ..., -3.1855, -2.8926, -5.1602]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 10:14:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The armchair falls into the category of chair
The toaster falls into the category of appliance
The croissant falls into the category of pastry
The hamburger falls into the category of sandwich
The bracelet falls into the category of jewelry
The blender falls into the category of appliance
The vase falls into the category of jar
The sweater falls into the category of
2024-07-23 10:14:04 root INFO     [order_1_approx] starting weight calculation for The croissant falls into the category of pastry
The vase falls into the category of jar
The blender falls into the category of appliance
The sweater falls into the category of clothes
The hamburger falls into the category of sandwich
The armchair falls into the category of chair
The bracelet falls into the category of jewelry
The toaster falls into the category of
2024-07-23 10:14:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 10:17:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0605,  0.7529, -0.2563,  ...,  1.2217,  0.7490,  0.9258],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-7.2031,  0.9126,  0.2090,  ...,  1.7021, -1.2305, -0.1680],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0134,  0.0049, -0.0041,  ..., -0.0082, -0.0063,  0.0027],
        [-0.0054,  0.0090, -0.0023,  ...,  0.0095,  0.0034,  0.0043],
        [-0.0029, -0.0008,  0.0121,  ...,  0.0035,  0.0001, -0.0007],
        ...,
        [-0.0096, -0.0061, -0.0069,  ...,  0.0170,  0.0019,  0.0082],
        [ 0.0044,  0.0041,  0.0072,  ..., -0.0074, -0.0021,  0.0045],
        [-0.0117, -0.0022, -0.0015,  ..., -0.0077,  0.0015,  0.0098]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-7.0508,  0.8892, -0.3179,  ...,  1.4043, -1.4443,  0.2134]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 10:17:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The croissant falls into the category of pastry
The vase falls into the category of jar
The blender falls into the category of appliance
The sweater falls into the category of clothes
The hamburger falls into the category of sandwich
The armchair falls into the category of chair
The bracelet falls into the category of jewelry
The toaster falls into the category of
2024-07-23 10:17:48 root INFO     [order_1_approx] starting weight calculation for The armchair falls into the category of chair
The croissant falls into the category of pastry
The bracelet falls into the category of jewelry
The sweater falls into the category of clothes
The blender falls into the category of appliance
The hamburger falls into the category of sandwich
The toaster falls into the category of appliance
The vase falls into the category of
2024-07-23 10:17:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 10:21:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3413,  0.4985,  1.0771,  ..., -0.2827, -0.6499,  0.5459],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8174, -2.5547, -1.5039,  ..., -0.5059, -0.7334,  0.3379],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0225,  0.0021, -0.0084,  ...,  0.0005,  0.0033, -0.0058],
        [-0.0028,  0.0144, -0.0053,  ...,  0.0102,  0.0053,  0.0055],
        [ 0.0036,  0.0121,  0.0061,  ...,  0.0027,  0.0036,  0.0023],
        ...,
        [-0.0024, -0.0233,  0.0006,  ...,  0.0201, -0.0094,  0.0012],
        [-0.0004, -0.0050,  0.0029,  ..., -0.0003,  0.0020,  0.0035],
        [-0.0056, -0.0105,  0.0007,  ..., -0.0035,  0.0085,  0.0219]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4241, -2.0879, -1.3525,  ..., -0.4434, -0.4614,  0.5645]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 10:21:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The armchair falls into the category of chair
The croissant falls into the category of pastry
The bracelet falls into the category of jewelry
The sweater falls into the category of clothes
The blender falls into the category of appliance
The hamburger falls into the category of sandwich
The toaster falls into the category of appliance
The vase falls into the category of
2024-07-23 10:21:32 root INFO     total operator prediction time: 1792.1543443202972 seconds
2024-07-23 10:21:33 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - substance
2024-07-23 10:21:33 root INFO     building operator meronyms - substance
2024-07-23 10:21:33 root INFO     [order_1_approx] starting weight calculation for A lens is made up of glass
A plastic is made up of polymer
A spoon is made up of aluminium
A diamond is made up of carbon
A penny is made up of metal
A clothing is made up of fabric
A snow is made up of water
A beard is made up of
2024-07-23 10:21:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 10:25:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0176, -0.3406, -0.7979,  ..., -0.7153, -0.9966,  1.9785],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6982, -1.7285, -1.5000,  ...,  0.7881, -2.4531, -1.9814],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0108, -0.0100,  0.0016,  ...,  0.0003,  0.0082, -0.0146],
        [-0.0082, -0.0006,  0.0069,  ...,  0.0121,  0.0066, -0.0104],
        [ 0.0143, -0.0025, -0.0092,  ...,  0.0030,  0.0007, -0.0051],
        ...,
        [-0.0088, -0.0153,  0.0002,  ...,  0.0135, -0.0114,  0.0057],
        [ 0.0052,  0.0009, -0.0052,  ...,  0.0090, -0.0071,  0.0004],
        [ 0.0051,  0.0056, -0.0067,  ..., -0.0135,  0.0071,  0.0009]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2778, -1.4922, -2.1406,  ...,  0.4885, -2.7070, -1.8750]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 10:25:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A lens is made up of glass
A plastic is made up of polymer
A spoon is made up of aluminium
A diamond is made up of carbon
A penny is made up of metal
A clothing is made up of fabric
A snow is made up of water
A beard is made up of
2024-07-23 10:25:16 root INFO     [order_1_approx] starting weight calculation for A plastic is made up of polymer
A penny is made up of metal
A snow is made up of water
A beard is made up of hair
A diamond is made up of carbon
A spoon is made up of aluminium
A lens is made up of glass
A clothing is made up of
2024-07-23 10:25:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 10:28:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2051,  0.2827,  0.1733,  ...,  0.7627, -0.1804,  0.3330],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7480, -1.9102,  1.1143,  ...,  1.2910, -0.2129, -2.7617],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.8027e-03, -2.2141e-02, -4.4365e-03,  ...,  8.7738e-05,
          4.6730e-04, -1.5326e-03],
        [ 4.8828e-03, -5.2147e-03, -2.8267e-03,  ...,  8.2550e-03,
         -4.1809e-03, -1.0712e-02],
        [ 3.9711e-03, -4.0359e-03, -1.2398e-02,  ...,  1.2226e-03,
          1.5078e-03,  6.7177e-03],
        ...,
        [-8.8806e-03,  8.2932e-03,  4.8714e-03,  ..., -1.0742e-02,
          1.5602e-03,  8.9569e-03],
        [-3.3455e-03, -6.8359e-03,  6.8588e-03,  ...,  6.3248e-03,
         -8.8959e-03,  3.8338e-04],
        [-6.2866e-03,  3.7403e-03, -4.6158e-03,  ..., -7.4844e-03,
          1.4038e-02,  3.4466e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2480, -1.6348,  1.1045,  ...,  1.5840, -0.7793, -2.9121]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 10:28:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A plastic is made up of polymer
A penny is made up of metal
A snow is made up of water
A beard is made up of hair
A diamond is made up of carbon
A spoon is made up of aluminium
A lens is made up of glass
A clothing is made up of
2024-07-23 10:28:59 root INFO     [order_1_approx] starting weight calculation for A spoon is made up of aluminium
A snow is made up of water
A clothing is made up of fabric
A beard is made up of hair
A plastic is made up of polymer
A lens is made up of glass
A penny is made up of metal
A diamond is made up of
2024-07-23 10:29:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 10:32:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4297, -0.1421, -0.2710,  ..., -1.2168,  0.7314, -0.3689],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.4492,  1.9365,  4.8672,  ...,  0.1782, -0.2944,  2.7227],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0065, -0.0094, -0.0029,  ..., -0.0054, -0.0010,  0.0019],
        [-0.0042, -0.0016,  0.0045,  ...,  0.0012, -0.0054, -0.0082],
        [ 0.0042, -0.0010, -0.0061,  ...,  0.0025,  0.0026, -0.0101],
        ...,
        [ 0.0087,  0.0056,  0.0067,  ..., -0.0071, -0.0104,  0.0079],
        [-0.0009, -0.0014, -0.0034,  ...,  0.0018, -0.0050, -0.0206],
        [ 0.0088,  0.0026, -0.0045,  ...,  0.0042,  0.0114,  0.0021]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.5000,  1.8115,  4.5312,  ...,  0.3901, -0.5903,  2.3340]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 10:32:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A spoon is made up of aluminium
A snow is made up of water
A clothing is made up of fabric
A beard is made up of hair
A plastic is made up of polymer
A lens is made up of glass
A penny is made up of metal
A diamond is made up of
2024-07-23 10:32:43 root INFO     [order_1_approx] starting weight calculation for A penny is made up of metal
A snow is made up of water
A diamond is made up of carbon
A spoon is made up of aluminium
A clothing is made up of fabric
A beard is made up of hair
A plastic is made up of polymer
A lens is made up of
2024-07-23 10:32:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 10:36:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2266, -0.2798, -1.0928,  ...,  1.2119, -0.3176,  0.6416],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7197,  2.8418,  0.7861,  ..., -1.3926, -1.7227,  0.8481],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0020,  0.0020, -0.0019,  ..., -0.0075,  0.0008, -0.0022],
        [-0.0072,  0.0013,  0.0040,  ...,  0.0088, -0.0016, -0.0005],
        [ 0.0009, -0.0024, -0.0060,  ...,  0.0084,  0.0111,  0.0100],
        ...,
        [ 0.0017,  0.0020,  0.0028,  ..., -0.0113, -0.0091,  0.0102],
        [ 0.0011, -0.0011,  0.0006,  ..., -0.0005, -0.0067, -0.0067],
        [ 0.0012,  0.0054,  0.0018,  ..., -0.0019,  0.0193,  0.0010]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.4180,  3.4238,  0.3835,  ..., -1.1934, -2.3711,  0.7637]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 10:36:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A penny is made up of metal
A snow is made up of water
A diamond is made up of carbon
A spoon is made up of aluminium
A clothing is made up of fabric
A beard is made up of hair
A plastic is made up of polymer
A lens is made up of
2024-07-23 10:36:27 root INFO     [order_1_approx] starting weight calculation for A lens is made up of glass
A snow is made up of water
A diamond is made up of carbon
A plastic is made up of polymer
A clothing is made up of fabric
A beard is made up of hair
A spoon is made up of aluminium
A penny is made up of
2024-07-23 10:36:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 10:40:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8242, -0.3955, -1.0781,  ..., -0.1956, -0.0146,  2.0508],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6113, -0.1105, -1.4883,  ..., -2.4727, -0.6807,  0.5156],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0124, -0.0164,  0.0074,  ..., -0.0108, -0.0065, -0.0097],
        [-0.0056,  0.0002,  0.0106,  ...,  0.0004,  0.0023,  0.0013],
        [ 0.0009, -0.0079, -0.0109,  ..., -0.0031,  0.0021, -0.0023],
        ...,
        [ 0.0038, -0.0061, -0.0127,  ..., -0.0047, -0.0025,  0.0134],
        [ 0.0015,  0.0023,  0.0027,  ...,  0.0054, -0.0027, -0.0017],
        [-0.0070,  0.0056,  0.0082,  ...,  0.0023,  0.0115, -0.0033]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7324, -0.1095, -1.0645,  ..., -2.3320, -0.7114,  0.3965]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 10:40:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A lens is made up of glass
A snow is made up of water
A diamond is made up of carbon
A plastic is made up of polymer
A clothing is made up of fabric
A beard is made up of hair
A spoon is made up of aluminium
A penny is made up of
2024-07-23 10:40:12 root INFO     [order_1_approx] starting weight calculation for A snow is made up of water
A lens is made up of glass
A diamond is made up of carbon
A penny is made up of metal
A spoon is made up of aluminium
A clothing is made up of fabric
A beard is made up of hair
A plastic is made up of
2024-07-23 10:40:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 10:43:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4939, -0.0126, -0.2517,  ...,  0.8369, -0.4121, -0.8125],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.9570, -1.0723,  1.9570,  ..., -0.4841, -0.6401,  1.2471],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0182, -0.0051, -0.0077,  ..., -0.0016, -0.0024, -0.0015],
        [-0.0044,  0.0023, -0.0021,  ..., -0.0021, -0.0002,  0.0039],
        [-0.0009, -0.0047, -0.0053,  ...,  0.0008,  0.0044,  0.0036],
        ...,
        [ 0.0080, -0.0071,  0.0044,  ...,  0.0131, -0.0005, -0.0104],
        [ 0.0031, -0.0051, -0.0005,  ...,  0.0165, -0.0060, -0.0058],
        [-0.0024, -0.0042,  0.0030,  ..., -0.0072,  0.0027,  0.0058]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.4629, -1.2061,  1.7744,  ..., -0.1318, -0.8340,  1.1641]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 10:43:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A snow is made up of water
A lens is made up of glass
A diamond is made up of carbon
A penny is made up of metal
A spoon is made up of aluminium
A clothing is made up of fabric
A beard is made up of hair
A plastic is made up of
2024-07-23 10:43:57 root INFO     [order_1_approx] starting weight calculation for A penny is made up of metal
A beard is made up of hair
A lens is made up of glass
A plastic is made up of polymer
A diamond is made up of carbon
A spoon is made up of aluminium
A clothing is made up of fabric
A snow is made up of
2024-07-23 10:43:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 10:47:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9893,  1.1172, -0.3579,  ...,  0.9678, -0.3743,  0.1421],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([1.4297, 2.1641, 1.3027,  ..., 1.5820, 0.6729, 0.6133], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0026, -0.0002,  0.0013,  ...,  0.0011,  0.0012,  0.0037],
        [-0.0073, -0.0140,  0.0056,  ..., -0.0035, -0.0078, -0.0213],
        [ 0.0104, -0.0033, -0.0042,  ..., -0.0003,  0.0103, -0.0054],
        ...,
        [ 0.0047,  0.0017, -0.0028,  ..., -0.0010, -0.0044, -0.0011],
        [ 0.0057, -0.0020, -0.0096,  ..., -0.0043,  0.0037,  0.0055],
        [ 0.0135, -0.0004,  0.0028,  ..., -0.0026,  0.0124, -0.0036]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[1.4277, 2.1426, 1.0635,  ..., 1.4941, 0.0923, 0.3440]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 10:47:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A penny is made up of metal
A beard is made up of hair
A lens is made up of glass
A plastic is made up of polymer
A diamond is made up of carbon
A spoon is made up of aluminium
A clothing is made up of fabric
A snow is made up of
2024-07-23 10:47:41 root INFO     [order_1_approx] starting weight calculation for A penny is made up of metal
A plastic is made up of polymer
A snow is made up of water
A beard is made up of hair
A lens is made up of glass
A diamond is made up of carbon
A clothing is made up of fabric
A spoon is made up of
2024-07-23 10:47:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 10:51:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5381,  0.6719, -1.0078,  ...,  1.5000, -0.7842,  0.3862],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.5000,  1.7305, -0.4150,  ...,  2.0527,  3.2871,  2.3320],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0074, -0.0061, -0.0016,  ..., -0.0183, -0.0040,  0.0055],
        [-0.0101,  0.0165,  0.0036,  ...,  0.0055,  0.0041,  0.0086],
        [ 0.0037, -0.0119,  0.0127,  ...,  0.0014, -0.0044, -0.0030],
        ...,
        [-0.0007, -0.0190, -0.0005,  ...,  0.0093, -0.0119,  0.0061],
        [-0.0006, -0.0126,  0.0143,  ...,  0.0081, -0.0028, -0.0088],
        [-0.0007,  0.0022,  0.0010,  ...,  0.0016,  0.0076,  0.0082]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.6055,  2.1133, -0.8828,  ...,  1.9844,  2.5859,  2.7168]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 10:51:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A penny is made up of metal
A plastic is made up of polymer
A snow is made up of water
A beard is made up of hair
A lens is made up of glass
A diamond is made up of carbon
A clothing is made up of fabric
A spoon is made up of
2024-07-23 10:51:23 root INFO     total operator prediction time: 1790.5147352218628 seconds
2024-07-23 10:51:23 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on synonyms - intensity
2024-07-23 10:51:23 root INFO     building operator synonyms - intensity
2024-07-23 10:51:23 root INFO     [order_1_approx] starting weight calculation for A more intense word for doze is sleep
A more intense word for house is palace
A more intense word for pony is horse
A more intense word for giggle is laugh
A more intense word for dislike is hate
A more intense word for unfortunate is tragic
A more intense word for rain is deluge
A more intense word for angry is
2024-07-23 10:51:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 10:55:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([1.5000, 0.4502, 0.6816,  ..., 1.5723, 0.4487, 1.4551], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7988, -5.0469, -2.2793,  ...,  3.1367,  6.8125, -0.5635],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0041, -0.0091,  0.0053,  ..., -0.0051, -0.0042, -0.0063],
        [ 0.0028,  0.0110, -0.0028,  ..., -0.0133,  0.0096, -0.0187],
        [ 0.0082,  0.0243,  0.0044,  ..., -0.0020,  0.0087,  0.0044],
        ...,
        [-0.0190, -0.0092,  0.0120,  ...,  0.0040, -0.0056,  0.0042],
        [ 0.0048, -0.0320, -0.0017,  ...,  0.0132, -0.0041,  0.0044],
        [ 0.0055,  0.0004, -0.0137,  ..., -0.0081,  0.0178,  0.0009]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6582, -4.4961, -1.9023,  ...,  3.1680,  6.0859, -0.4741]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 10:55:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for doze is sleep
A more intense word for house is palace
A more intense word for pony is horse
A more intense word for giggle is laugh
A more intense word for dislike is hate
A more intense word for unfortunate is tragic
A more intense word for rain is deluge
A more intense word for angry is
2024-07-23 10:55:09 root INFO     [order_1_approx] starting weight calculation for A more intense word for rain is deluge
A more intense word for house is palace
A more intense word for giggle is laugh
A more intense word for unfortunate is tragic
A more intense word for angry is furious
A more intense word for doze is sleep
A more intense word for pony is horse
A more intense word for dislike is
2024-07-23 10:55:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 10:58:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3730, -0.7402,  0.6523,  ...,  0.3989,  0.0309,  0.8760],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9736, -0.7881,  0.0039,  ...,  3.7383,  1.3555,  2.4160],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0086, -0.0063,  0.0257,  ..., -0.0056, -0.0013, -0.0177],
        [-0.0027,  0.0060, -0.0109,  ...,  0.0020,  0.0116, -0.0042],
        [ 0.0301, -0.0196,  0.0013,  ..., -0.0060,  0.0134,  0.0015],
        ...,
        [-0.0073, -0.0105,  0.0081,  ...,  0.0156, -0.0320,  0.0007],
        [ 0.0332, -0.0119,  0.0193,  ...,  0.0066,  0.0080,  0.0009],
        [-0.0138,  0.0046, -0.0080,  ...,  0.0153, -0.0085,  0.0101]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1758, -0.6890,  0.3979,  ...,  3.8047,  1.2695,  2.4414]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 10:58:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for rain is deluge
A more intense word for house is palace
A more intense word for giggle is laugh
A more intense word for unfortunate is tragic
A more intense word for angry is furious
A more intense word for doze is sleep
A more intense word for pony is horse
A more intense word for dislike is
2024-07-23 10:58:55 root INFO     [order_1_approx] starting weight calculation for A more intense word for angry is furious
A more intense word for unfortunate is tragic
A more intense word for pony is horse
A more intense word for dislike is hate
A more intense word for house is palace
A more intense word for giggle is laugh
A more intense word for rain is deluge
A more intense word for doze is
2024-07-23 10:58:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 11:02:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([1.2793, 0.0557, 0.3445,  ..., 0.6709, 1.2285, 0.4951], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.4805, -0.1797,  0.1074,  ..., -5.0430, -1.3506,  0.1660],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0096, -0.0059,  0.0122,  ..., -0.0010, -0.0091,  0.0075],
        [ 0.0117, -0.0063,  0.0020,  ..., -0.0020,  0.0019,  0.0050],
        [-0.0027,  0.0091,  0.0061,  ...,  0.0003,  0.0029,  0.0018],
        ...,
        [-0.0131, -0.0191,  0.0002,  ...,  0.0016,  0.0037,  0.0137],
        [-0.0005,  0.0009,  0.0093,  ..., -0.0095,  0.0092,  0.0027],
        [ 0.0031,  0.0028,  0.0013,  ..., -0.0009,  0.0029,  0.0113]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.8164,  0.2195,  0.0060,  ..., -4.1719, -1.8379, -0.2129]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 11:02:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for angry is furious
A more intense word for unfortunate is tragic
A more intense word for pony is horse
A more intense word for dislike is hate
A more intense word for house is palace
A more intense word for giggle is laugh
A more intense word for rain is deluge
A more intense word for doze is
2024-07-23 11:02:41 root INFO     [order_1_approx] starting weight calculation for A more intense word for dislike is hate
A more intense word for pony is horse
A more intense word for house is palace
A more intense word for rain is deluge
A more intense word for unfortunate is tragic
A more intense word for angry is furious
A more intense word for doze is sleep
A more intense word for giggle is
2024-07-23 11:02:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 11:59:04 root INFO     loading model + tokenizer
2024-07-23 11:59:36 root INFO     loading model + tokenizer
2024-07-23 11:59:53 root INFO     model + tokenizer loaded
2024-07-23 11:59:53 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on hypernyms - animals
2024-07-23 11:59:53 root INFO     building operator hypernyms - animals
2024-07-23 11:59:53 root INFO     [order_1_approx] starting weight calculation for The jackal falls into the category of canine
The turkey falls into the category of fowl
The beaver falls into the category of rodent
The mamba falls into the category of snake
The viper falls into the category of snake
The eagle falls into the category of raptor
The wolf falls into the category of canine
The anaconda falls into the category of
2024-07-23 11:59:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 12:03:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4121,  1.3008, -0.1680,  ...,  1.0811, -0.0771,  0.4468],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2529,  0.6450, -1.8613,  ...,  1.1377, -3.0195,  2.4512],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0084,  0.0072, -0.0046,  ..., -0.0024,  0.0022, -0.0057],
        [ 0.0014,  0.0016,  0.0010,  ...,  0.0109,  0.0058, -0.0037],
        [ 0.0016,  0.0053,  0.0045,  ..., -0.0084,  0.0030,  0.0115],
        ...,
        [-0.0082, -0.0032, -0.0111,  ...,  0.0166,  0.0045,  0.0043],
        [ 0.0015, -0.0133, -0.0085,  ..., -0.0097,  0.0095,  0.0154],
        [-0.0110, -0.0025,  0.0042,  ..., -0.0011, -0.0003,  0.0072]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1858,  0.8770, -2.5430,  ...,  1.4395, -3.7969,  3.1094]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 12:03:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The jackal falls into the category of canine
The turkey falls into the category of fowl
The beaver falls into the category of rodent
The mamba falls into the category of snake
The viper falls into the category of snake
The eagle falls into the category of raptor
The wolf falls into the category of canine
The anaconda falls into the category of
2024-07-23 12:03:35 root INFO     [order_1_approx] starting weight calculation for The wolf falls into the category of canine
The mamba falls into the category of snake
The jackal falls into the category of canine
The viper falls into the category of snake
The turkey falls into the category of fowl
The eagle falls into the category of raptor
The anaconda falls into the category of snake
The beaver falls into the category of
2024-07-23 12:03:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 12:07:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1887, -0.9917, -2.0898,  ...,  1.3447, -0.3242,  0.7793],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.0938,  3.1426, -3.7793,  ...,  1.9395, -0.1621,  3.1641],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0055, -0.0079, -0.0022,  ...,  0.0013, -0.0003, -0.0047],
        [-0.0132, -0.0084,  0.0154,  ...,  0.0135,  0.0121, -0.0118],
        [ 0.0098, -0.0032,  0.0067,  ..., -0.0141,  0.0065,  0.0085],
        ...,
        [-0.0150, -0.0085, -0.0009,  ...,  0.0154, -0.0170,  0.0073],
        [-0.0001, -0.0104, -0.0055,  ..., -0.0067,  0.0007,  0.0065],
        [ 0.0052, -0.0007, -0.0023,  ..., -0.0017,  0.0055,  0.0082]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.9766,  3.2891, -3.9395,  ...,  1.8398, -0.0890,  3.0859]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 12:07:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The wolf falls into the category of canine
The mamba falls into the category of snake
The jackal falls into the category of canine
The viper falls into the category of snake
The turkey falls into the category of fowl
The eagle falls into the category of raptor
The anaconda falls into the category of snake
The beaver falls into the category of
2024-07-23 12:07:15 root INFO     [order_1_approx] starting weight calculation for The turkey falls into the category of fowl
The mamba falls into the category of snake
The jackal falls into the category of canine
The viper falls into the category of snake
The wolf falls into the category of canine
The beaver falls into the category of rodent
The anaconda falls into the category of snake
The eagle falls into the category of
2024-07-23 12:07:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 12:10:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3218, -1.6533, -0.3127,  ..., -0.2183,  0.0511,  0.3293],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5312,  0.7471, -2.2227,  ..., -0.1960, -1.1758, -0.8252],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0115, -0.0064, -0.0070,  ..., -0.0010,  0.0035,  0.0036],
        [ 0.0050,  0.0123,  0.0105,  ...,  0.0117, -0.0038, -0.0050],
        [ 0.0031, -0.0190, -0.0061,  ..., -0.0029,  0.0092,  0.0036],
        ...,
        [ 0.0002, -0.0080,  0.0081,  ...,  0.0082, -0.0023,  0.0133],
        [-0.0019, -0.0092, -0.0022,  ..., -0.0091,  0.0011,  0.0028],
        [-0.0041, -0.0069,  0.0029,  ...,  0.0016,  0.0054,  0.0176]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9131,  1.6562, -3.5938,  ..., -0.4805, -1.5781, -0.5381]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 12:10:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The turkey falls into the category of fowl
The mamba falls into the category of snake
The jackal falls into the category of canine
The viper falls into the category of snake
The wolf falls into the category of canine
The beaver falls into the category of rodent
The anaconda falls into the category of snake
The eagle falls into the category of
2024-07-23 12:10:59 root INFO     [order_1_approx] starting weight calculation for The eagle falls into the category of raptor
The viper falls into the category of snake
The mamba falls into the category of snake
The beaver falls into the category of rodent
The wolf falls into the category of canine
The turkey falls into the category of fowl
The anaconda falls into the category of snake
The jackal falls into the category of
2024-07-23 12:10:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 12:14:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4546, -1.9492, -0.6206,  ...,  0.7188, -0.6592, -0.6270],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2490, -0.5117,  0.2803,  ..., -2.1875, -1.2578, -0.2515],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 9.1934e-03, -7.8506e-03,  1.7509e-03,  ..., -5.6381e-03,
         -3.4142e-03, -1.0643e-02],
        [ 3.8548e-03,  4.0283e-03,  9.4376e-03,  ...,  1.7670e-02,
          9.0714e-03, -4.5967e-04],
        [ 4.7302e-04, -1.1505e-02,  9.5367e-04,  ..., -3.7994e-03,
          2.2984e-03,  1.1253e-02],
        ...,
        [-7.7744e-03, -4.4212e-03, -6.1035e-05,  ...,  2.1744e-02,
         -2.1973e-03,  1.2634e-02],
        [-6.8588e-03, -5.3291e-03, -2.1210e-03,  ...,  3.4332e-03,
          4.5853e-03,  6.0081e-03],
        [-6.9885e-03, -1.8606e-03,  1.5991e-02,  ..., -1.1921e-03,
          2.7599e-03,  3.7270e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4795, -0.2063,  0.3254,  ..., -2.5938, -1.1172, -0.2311]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 12:14:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The eagle falls into the category of raptor
The viper falls into the category of snake
The mamba falls into the category of snake
The beaver falls into the category of rodent
The wolf falls into the category of canine
The turkey falls into the category of fowl
The anaconda falls into the category of snake
The jackal falls into the category of
2024-07-23 12:14:41 root INFO     [order_1_approx] starting weight calculation for The beaver falls into the category of rodent
The viper falls into the category of snake
The eagle falls into the category of raptor
The turkey falls into the category of fowl
The jackal falls into the category of canine
The wolf falls into the category of canine
The anaconda falls into the category of snake
The mamba falls into the category of
2024-07-23 12:14:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 12:18:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1689, -0.0911,  0.1700,  ...,  0.5420,  1.5195, -0.3804],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5527, -1.2363, -1.5762,  ...,  0.8906, -0.9941, -0.1040],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0049, -0.0041, -0.0166,  ..., -0.0032, -0.0071, -0.0105],
        [ 0.0026,  0.0119,  0.0053,  ...,  0.0137,  0.0090, -0.0049],
        [-0.0153, -0.0020,  0.0018,  ..., -0.0080, -0.0058,  0.0086],
        ...,
        [-0.0126, -0.0063, -0.0043,  ...,  0.0233, -0.0003,  0.0136],
        [-0.0098, -0.0092, -0.0026,  ...,  0.0012,  0.0231,  0.0134],
        [ 0.0032,  0.0081,  0.0013,  ..., -0.0017, -0.0063,  0.0098]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3809, -0.8516, -2.4531,  ...,  0.8213, -1.2695,  1.2188]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 12:18:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The beaver falls into the category of rodent
The viper falls into the category of snake
The eagle falls into the category of raptor
The turkey falls into the category of fowl
The jackal falls into the category of canine
The wolf falls into the category of canine
The anaconda falls into the category of snake
The mamba falls into the category of
2024-07-23 12:18:24 root INFO     [order_1_approx] starting weight calculation for The viper falls into the category of snake
The wolf falls into the category of canine
The anaconda falls into the category of snake
The eagle falls into the category of raptor
The beaver falls into the category of rodent
The mamba falls into the category of snake
The jackal falls into the category of canine
The turkey falls into the category of
2024-07-23 12:18:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 12:22:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3325, -1.1162,  0.5176,  ...,  0.1785, -0.7124,  1.1768],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4326,  2.3477, -2.5000,  ..., -1.2871,  1.3086, -1.1650],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0027, -0.0121,  0.0015,  ...,  0.0016,  0.0099,  0.0041],
        [-0.0034,  0.0047,  0.0002,  ...,  0.0022, -0.0076, -0.0060],
        [ 0.0062,  0.0087, -0.0030,  ...,  0.0032,  0.0040,  0.0101],
        ...,
        [-0.0171, -0.0079,  0.0140,  ...,  0.0141, -0.0093,  0.0172],
        [ 0.0030, -0.0121, -0.0052,  ...,  0.0009,  0.0064, -0.0005],
        [-0.0042, -0.0040, -0.0006,  ..., -0.0094,  0.0069,  0.0129]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.6152,  2.9336, -2.7207,  ..., -1.2100,  1.8428, -0.6465]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 12:22:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The viper falls into the category of snake
The wolf falls into the category of canine
The anaconda falls into the category of snake
The eagle falls into the category of raptor
The beaver falls into the category of rodent
The mamba falls into the category of snake
The jackal falls into the category of canine
The turkey falls into the category of
2024-07-23 12:22:07 root INFO     [order_1_approx] starting weight calculation for The eagle falls into the category of raptor
The turkey falls into the category of fowl
The jackal falls into the category of canine
The anaconda falls into the category of snake
The mamba falls into the category of snake
The beaver falls into the category of rodent
The wolf falls into the category of canine
The viper falls into the category of
2024-07-23 12:22:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 12:25:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1934, -0.2808, -1.4482,  ...,  0.6270, -0.1616,  0.5449],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.9648, -0.3755, -3.2559,  ...,  0.8838, -1.2090, -0.7744],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0190, -0.0029, -0.0063,  ..., -0.0051,  0.0049, -0.0027],
        [-0.0032, -0.0029,  0.0087,  ...,  0.0114,  0.0030, -0.0069],
        [ 0.0009,  0.0015, -0.0078,  ..., -0.0167, -0.0010,  0.0086],
        ...,
        [-0.0084, -0.0043,  0.0060,  ...,  0.0216,  0.0003,  0.0044],
        [-0.0007, -0.0127,  0.0026,  ..., -0.0048,  0.0073,  0.0136],
        [ 0.0016, -0.0023, -0.0025,  ..., -0.0069, -0.0052,  0.0049]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.0918, -0.2502, -4.1055,  ...,  1.1367, -1.3906,  0.1611]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 12:25:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The eagle falls into the category of raptor
The turkey falls into the category of fowl
The jackal falls into the category of canine
The anaconda falls into the category of snake
The mamba falls into the category of snake
The beaver falls into the category of rodent
The wolf falls into the category of canine
The viper falls into the category of
2024-07-23 12:25:49 root INFO     [order_1_approx] starting weight calculation for The beaver falls into the category of rodent
The jackal falls into the category of canine
The viper falls into the category of snake
The mamba falls into the category of snake
The eagle falls into the category of raptor
The turkey falls into the category of fowl
The anaconda falls into the category of snake
The wolf falls into the category of
2024-07-23 12:25:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 12:29:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5400, -0.9990, -0.9575,  ...,  0.5835, -0.7656,  0.4480],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9814, -0.7266, -0.7573,  ..., -2.8730, -0.0791,  2.5469],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0077, -0.0290, -0.0050,  ..., -0.0039,  0.0054,  0.0073],
        [ 0.0004,  0.0063,  0.0133,  ...,  0.0142, -0.0038, -0.0035],
        [-0.0039, -0.0087,  0.0019,  ..., -0.0037, -0.0092,  0.0320],
        ...,
        [-0.0045, -0.0068,  0.0094,  ...,  0.0242, -0.0057,  0.0144],
        [ 0.0004, -0.0105, -0.0038,  ...,  0.0075,  0.0142,  0.0103],
        [-0.0042, -0.0028,  0.0175,  ..., -0.0018,  0.0020,  0.0218]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5820,  0.2095, -0.9502,  ..., -2.7129, -0.5078,  2.7422]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 12:29:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The beaver falls into the category of rodent
The jackal falls into the category of canine
The viper falls into the category of snake
The mamba falls into the category of snake
The eagle falls into the category of raptor
The turkey falls into the category of fowl
The anaconda falls into the category of snake
The wolf falls into the category of
2024-07-23 12:29:31 root INFO     total operator prediction time: 1778.571337223053 seconds
2024-07-23 12:29:31 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on hyponyms - misc
2024-07-23 12:29:31 root INFO     building operator hyponyms - misc
2024-07-23 12:29:31 root INFO     [order_1_approx] starting weight calculation for A more specific term for a sweater is turtleneck
A more specific term for a drum is tambourine
A more specific term for a spice is pepper
A more specific term for a toy is doll
A more specific term for a gun is rifle
A more specific term for a weapon is gun
A more specific term for a cushion is pincushion
A more specific term for a book is
2024-07-23 12:29:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 12:33:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4297, -0.4565, -0.4585,  ..., -0.1538, -0.5415,  0.5088],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.1172, -3.0078, -2.1133,  ...,  0.8042,  1.4932, -1.6523],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.6174e-03, -3.3016e-03, -1.1978e-02,  ...,  1.1826e-03,
          9.6054e-03,  1.5335e-03],
        [ 1.2672e-02,  1.1871e-02,  8.0948e-03,  ...,  4.6997e-03,
          9.9945e-03,  1.0162e-02],
        [-3.8338e-04, -3.4714e-04, -3.1719e-03,  ...,  1.1742e-02,
         -7.3624e-04,  2.5040e-02],
        ...,
        [-3.8872e-03, -1.0536e-02,  1.0406e-02,  ...,  9.5367e-03,
         -2.6703e-05,  2.1271e-02],
        [-8.4686e-04,  1.9104e-02,  1.2108e-02,  ...,  3.1128e-03,
         -1.9211e-02, -5.4016e-03],
        [-1.7357e-03,  2.1912e-02,  4.3907e-03,  ...,  4.4327e-03,
          1.4862e-02,  1.5747e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5273, -2.5781, -1.1152,  ...,  1.2646,  1.6592, -2.2109]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 12:33:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a sweater is turtleneck
A more specific term for a drum is tambourine
A more specific term for a spice is pepper
A more specific term for a toy is doll
A more specific term for a gun is rifle
A more specific term for a weapon is gun
A more specific term for a cushion is pincushion
A more specific term for a book is
2024-07-23 12:33:15 root INFO     [order_1_approx] starting weight calculation for A more specific term for a book is paperback
A more specific term for a drum is tambourine
A more specific term for a sweater is turtleneck
A more specific term for a gun is rifle
A more specific term for a toy is doll
A more specific term for a weapon is gun
A more specific term for a spice is pepper
A more specific term for a cushion is
2024-07-23 12:33:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 12:36:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6797, -0.4941, -1.3203,  ...,  0.3579, -0.9492,  0.1006],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9355, -4.0547, -4.7500,  ...,  0.1201,  0.2742, -1.7695],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0069, -0.0047,  0.0089,  ...,  0.0052, -0.0100, -0.0027],
        [ 0.0072, -0.0047,  0.0092,  ..., -0.0011, -0.0087,  0.0068],
        [ 0.0011,  0.0179, -0.0018,  ..., -0.0140,  0.0001,  0.0231],
        ...,
        [-0.0006, -0.0185,  0.0023,  ...,  0.0103, -0.0124,  0.0168],
        [ 0.0008, -0.0035,  0.0081,  ..., -0.0090,  0.0060,  0.0081],
        [ 0.0188,  0.0014, -0.0031,  ..., -0.0005,  0.0183,  0.0264]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5977, -4.1367, -3.2734,  ...,  1.0117,  0.8662, -1.5215]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 12:36:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a book is paperback
A more specific term for a drum is tambourine
A more specific term for a sweater is turtleneck
A more specific term for a gun is rifle
A more specific term for a toy is doll
A more specific term for a weapon is gun
A more specific term for a spice is pepper
A more specific term for a cushion is
2024-07-23 12:36:57 root INFO     [order_1_approx] starting weight calculation for A more specific term for a gun is rifle
A more specific term for a sweater is turtleneck
A more specific term for a toy is doll
A more specific term for a spice is pepper
A more specific term for a cushion is pincushion
A more specific term for a book is paperback
A more specific term for a weapon is gun
A more specific term for a drum is
2024-07-23 12:36:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 12:40:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1387,  0.5522, -0.0117,  ..., -0.4302, -0.8306, -0.7666],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.6367,  3.3145,  0.6387,  ..., -2.7148,  1.4434, -2.8789],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0036, -0.0036, -0.0038,  ...,  0.0034,  0.0045,  0.0132],
        [ 0.0082, -0.0060, -0.0010,  ...,  0.0082,  0.0012, -0.0153],
        [-0.0163,  0.0036, -0.0117,  ..., -0.0161,  0.0232,  0.0135],
        ...,
        [-0.0063, -0.0384,  0.0064,  ...,  0.0123, -0.0164, -0.0022],
        [-0.0161,  0.0004, -0.0095,  ..., -0.0082,  0.0059,  0.0289],
        [-0.0134,  0.0103,  0.0144,  ..., -0.0107,  0.0183,  0.0371]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.0039,  3.1699,  0.8760,  ..., -3.2891,  2.3750, -3.7754]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 12:40:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a gun is rifle
A more specific term for a sweater is turtleneck
A more specific term for a toy is doll
A more specific term for a spice is pepper
A more specific term for a cushion is pincushion
A more specific term for a book is paperback
A more specific term for a weapon is gun
A more specific term for a drum is
2024-07-23 12:40:39 root INFO     [order_1_approx] starting weight calculation for A more specific term for a spice is pepper
A more specific term for a drum is tambourine
A more specific term for a cushion is pincushion
A more specific term for a weapon is gun
A more specific term for a sweater is turtleneck
A more specific term for a book is paperback
A more specific term for a toy is doll
A more specific term for a gun is
2024-07-23 12:40:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 12:44:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1348, -0.0819,  0.2098,  ...,  0.6475, -0.3364, -0.3730],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4919,  0.5435, -2.3477,  ..., -2.1973,  2.7422,  2.8809],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.8463e-03,  1.4374e-02, -1.1536e-02,  ..., -5.3215e-04,
          5.0240e-03,  1.2268e-02],
        [ 1.1826e-03, -4.2877e-03,  7.6141e-03,  ...,  1.0803e-02,
         -1.1826e-03, -2.2259e-03],
        [ 1.3275e-02,  4.9744e-03,  1.0788e-02,  ..., -1.5327e-02,
          1.2093e-03,  4.8065e-04],
        ...,
        [-7.5645e-03, -2.0020e-02,  1.2062e-02,  ...,  1.1497e-02,
         -4.4823e-05,  5.9891e-03],
        [-1.3069e-02,  1.5640e-03, -1.2787e-02,  ..., -6.2256e-03,
          1.1627e-02,  7.8735e-03],
        [-7.3242e-03, -1.3954e-02,  1.3824e-02,  ..., -5.6744e-04,
          3.1090e-03,  3.2806e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0098,  0.0405, -2.3613,  ..., -1.0459,  2.4082,  2.6504]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 12:44:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a spice is pepper
A more specific term for a drum is tambourine
A more specific term for a cushion is pincushion
A more specific term for a weapon is gun
A more specific term for a sweater is turtleneck
A more specific term for a book is paperback
A more specific term for a toy is doll
A more specific term for a gun is
2024-07-23 12:44:23 root INFO     [order_1_approx] starting weight calculation for A more specific term for a cushion is pincushion
A more specific term for a toy is doll
A more specific term for a gun is rifle
A more specific term for a weapon is gun
A more specific term for a sweater is turtleneck
A more specific term for a drum is tambourine
A more specific term for a book is paperback
A more specific term for a spice is
2024-07-23 12:44:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 12:48:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.6406,  0.1111, -0.3770,  ...,  0.0186, -0.3184, -0.0217],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1289,  1.5977,  0.3887,  ...,  0.8320,  3.4043,  0.1949],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.6003e-03, -9.1782e-03,  4.5586e-04,  ...,  2.8591e-03,
          4.5395e-04,  3.2673e-03],
        [-1.0399e-02, -2.8553e-03, -1.3161e-03,  ..., -1.4374e-02,
         -2.0844e-02, -3.8910e-04],
        [-5.2910e-03,  7.1983e-03,  1.1406e-03,  ...,  8.7509e-03,
          2.0790e-03,  1.0368e-02],
        ...,
        [-1.2871e-02,  1.1711e-03,  4.7684e-03,  ...,  2.1698e-02,
         -9.4070e-03,  9.3222e-04],
        [-4.6310e-03, -3.1433e-03, -4.5128e-03,  ...,  5.7220e-05,
         -2.5692e-03,  8.2245e-03],
        [-2.7809e-03,  1.4740e-02, -4.9782e-03,  ..., -8.8730e-03,
          2.4841e-02,  2.1118e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7959,  1.6943,  0.3975,  ...,  1.2812,  3.7266,  0.8447]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 12:48:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a cushion is pincushion
A more specific term for a toy is doll
A more specific term for a gun is rifle
A more specific term for a weapon is gun
A more specific term for a sweater is turtleneck
A more specific term for a drum is tambourine
A more specific term for a book is paperback
A more specific term for a spice is
2024-07-23 12:48:07 root INFO     [order_1_approx] starting weight calculation for A more specific term for a drum is tambourine
A more specific term for a toy is doll
A more specific term for a weapon is gun
A more specific term for a cushion is pincushion
A more specific term for a gun is rifle
A more specific term for a book is paperback
A more specific term for a spice is pepper
A more specific term for a sweater is
2024-07-23 12:48:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 12:51:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8271,  0.3091, -1.1982,  ...,  1.5137,  0.7466,  0.1387],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4590, -0.1333,  5.1484,  ...,  0.2070, -1.2949, -4.8398],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0104, -0.0195, -0.0029,  ...,  0.0029, -0.0031,  0.0162],
        [ 0.0021, -0.0045,  0.0098,  ...,  0.0071,  0.0039, -0.0048],
        [-0.0084, -0.0003, -0.0071,  ..., -0.0135,  0.0023,  0.0152],
        ...,
        [-0.0005,  0.0095, -0.0050,  ...,  0.0137, -0.0191, -0.0013],
        [-0.0025,  0.0013,  0.0082,  ..., -0.0110,  0.0101,  0.0030],
        [-0.0032, -0.0008,  0.0056,  ...,  0.0003,  0.0132,  0.0182]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2876,  0.2717,  5.5859,  ...,  1.4248, -1.9902, -4.2578]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 12:51:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a drum is tambourine
A more specific term for a toy is doll
A more specific term for a weapon is gun
A more specific term for a cushion is pincushion
A more specific term for a gun is rifle
A more specific term for a book is paperback
A more specific term for a spice is pepper
A more specific term for a sweater is
2024-07-23 12:51:49 root INFO     [order_1_approx] starting weight calculation for A more specific term for a gun is rifle
A more specific term for a drum is tambourine
A more specific term for a spice is pepper
A more specific term for a weapon is gun
A more specific term for a cushion is pincushion
A more specific term for a book is paperback
A more specific term for a sweater is turtleneck
A more specific term for a toy is
2024-07-23 12:51:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 12:55:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6592,  1.0596, -0.7549,  ..., -0.2549, -0.2686,  1.0576],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-5.6797, -3.4414,  2.0586,  ..., -2.0117,  6.9727,  6.6562],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.3895e-02,  7.5378e-03, -2.3071e-02,  ...,  1.0208e-02,
          4.1809e-03,  1.1765e-02],
        [ 3.9749e-03,  1.4923e-02,  8.1177e-03,  ...,  7.0343e-03,
          4.0131e-03,  4.1924e-03],
        [-1.0468e-02,  5.1880e-03, -1.6281e-02,  ..., -2.7252e-02,
          9.6512e-04,  8.5144e-03],
        ...,
        [-6.4735e-03,  1.0468e-02, -1.0147e-03,  ...,  1.0849e-02,
         -1.0872e-02,  2.5940e-04],
        [-6.9084e-03, -8.8196e-03, -5.2109e-03,  ...,  3.8147e-06,
         -8.7452e-04,  1.0452e-02],
        [-7.1335e-04, -1.9501e-02,  1.9989e-02,  ..., -1.4023e-02,
          6.0310e-03,  2.2964e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.6641, -3.8828,  3.2422,  ..., -1.6895,  6.8008,  6.1250]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 12:55:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a gun is rifle
A more specific term for a drum is tambourine
A more specific term for a spice is pepper
A more specific term for a weapon is gun
A more specific term for a cushion is pincushion
A more specific term for a book is paperback
A more specific term for a sweater is turtleneck
A more specific term for a toy is
2024-07-23 12:55:34 root INFO     [order_1_approx] starting weight calculation for A more specific term for a gun is rifle
A more specific term for a spice is pepper
A more specific term for a book is paperback
A more specific term for a sweater is turtleneck
A more specific term for a toy is doll
A more specific term for a drum is tambourine
A more specific term for a cushion is pincushion
A more specific term for a weapon is
2024-07-23 12:55:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 12:59:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8638,  0.4663,  0.5854,  ..., -0.1333, -0.0229,  0.2876],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.7402, -1.6621, -2.7578,  ..., -1.6250,  4.4609,  0.1455],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0215,  0.0016, -0.0033,  ..., -0.0066,  0.0041, -0.0019],
        [ 0.0077,  0.0115, -0.0059,  ...,  0.0069, -0.0019,  0.0010],
        [ 0.0104,  0.0042,  0.0197,  ..., -0.0118,  0.0008,  0.0019],
        ...,
        [-0.0340, -0.0075, -0.0043,  ...,  0.0137, -0.0038,  0.0077],
        [-0.0168,  0.0005, -0.0037,  ..., -0.0139,  0.0035,  0.0028],
        [-0.0093, -0.0145, -0.0016,  ...,  0.0164,  0.0223,  0.0215]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.7910, -1.7119, -2.8301,  ..., -1.4551,  4.3906, -0.3721]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 12:59:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a gun is rifle
A more specific term for a spice is pepper
A more specific term for a book is paperback
A more specific term for a sweater is turtleneck
A more specific term for a toy is doll
A more specific term for a drum is tambourine
A more specific term for a cushion is pincushion
A more specific term for a weapon is
2024-07-23 12:59:18 root INFO     total operator prediction time: 1786.3517489433289 seconds
2024-07-23 12:59:18 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on antonyms - binary
2024-07-23 12:59:18 root INFO     building operator antonyms - binary
2024-07-23 12:59:18 root INFO     [order_1_approx] starting weight calculation for The opposite of downslope is upslope
The opposite of drop is lift
The opposite of uphill is downhill
The opposite of rise is sink
The opposite of west is east
The opposite of in is out
The opposite of under is over
The opposite of beginning is
2024-07-23 12:59:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 13:41:13 root INFO     loading model + tokenizer
2024-07-23 13:41:29 root INFO     model + tokenizer loaded
2024-07-23 13:41:29 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on antonyms - binary
2024-07-23 13:41:29 root INFO     building operator antonyms - binary
2024-07-23 13:41:30 root INFO     [order_1_approx] starting weight calculation for The opposite of under is over
The opposite of downslope is upslope
The opposite of uphill is downhill
The opposite of in is out
The opposite of rise is sink
The opposite of drop is lift
The opposite of west is east
The opposite of beginning is
2024-07-23 13:41:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 13:45:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1011,  0.0591,  0.2144,  ...,  0.1417,  1.1602,  0.7637],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.4277, -2.0840, -0.2285,  ...,  4.6641,  1.2002, -0.2139],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.5375e-02, -1.9958e-02, -1.3981e-03,  ...,  4.5204e-03,
         -1.2794e-02,  6.7978e-03],
        [-4.1046e-03, -2.6886e-02, -1.2947e-02,  ..., -1.1459e-02,
          1.2245e-02,  2.2293e-02],
        [-1.5251e-02,  2.3499e-02, -2.0370e-02,  ...,  3.6621e-03,
         -7.5684e-03,  1.1337e-02],
        ...,
        [-4.4212e-03, -1.6937e-02, -1.0971e-02,  ..., -5.7449e-03,
          4.8828e-03,  6.3286e-03],
        [-3.8147e-06, -7.6370e-03,  2.6932e-02,  ..., -1.4868e-03,
          7.4997e-03, -4.2572e-03],
        [-2.1229e-03,  4.1885e-03, -5.6839e-03,  ..., -1.7624e-03,
          1.9684e-02, -2.5391e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.8389, -3.2285, -0.1824,  ...,  3.8633,  1.5254, -0.3970]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 13:45:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of under is over
The opposite of downslope is upslope
The opposite of uphill is downhill
The opposite of in is out
The opposite of rise is sink
The opposite of drop is lift
The opposite of west is east
The opposite of beginning is
2024-07-23 13:45:17 root INFO     [order_1_approx] starting weight calculation for The opposite of under is over
The opposite of drop is lift
The opposite of uphill is downhill
The opposite of rise is sink
The opposite of beginning is end
The opposite of west is east
The opposite of in is out
The opposite of downslope is
2024-07-23 13:45:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 13:48:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6328,  0.0864,  0.7139,  ..., -0.7988,  0.6851, -0.4116],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.0371, -2.6738,  2.5723,  ...,  0.7725,  2.7031,  0.3748],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0105, -0.0161,  0.0159,  ..., -0.0057,  0.0033, -0.0130],
        [ 0.0013, -0.0076, -0.0122,  ...,  0.0088,  0.0019,  0.0188],
        [-0.0071, -0.0011, -0.0087,  ..., -0.0233,  0.0071, -0.0120],
        ...,
        [-0.0011, -0.0123, -0.0068,  ..., -0.0198, -0.0048,  0.0035],
        [-0.0107,  0.0095,  0.0107,  ..., -0.0122, -0.0088,  0.0098],
        [-0.0002, -0.0014, -0.0045,  ..., -0.0092,  0.0136, -0.0071]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.3164, -2.6738,  3.1016,  ...,  1.2627,  3.3926, -0.7939]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 13:48:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of under is over
The opposite of drop is lift
The opposite of uphill is downhill
The opposite of rise is sink
The opposite of beginning is end
The opposite of west is east
The opposite of in is out
The opposite of downslope is
2024-07-23 13:48:59 root INFO     [order_1_approx] starting weight calculation for The opposite of uphill is downhill
The opposite of in is out
The opposite of under is over
The opposite of west is east
The opposite of rise is sink
The opposite of downslope is upslope
The opposite of beginning is end
The opposite of drop is
2024-07-23 13:48:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 13:52:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7959, -0.0018,  0.9810,  ...,  0.7085,  0.3513,  0.2122],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1401, -3.2793, -2.5625,  ..., -1.4521, -0.7100,  3.1270],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0038, -0.0264, -0.0136,  ...,  0.0251,  0.0179, -0.0327],
        [-0.0120, -0.0278, -0.0023,  ..., -0.0065, -0.0246,  0.0026],
        [ 0.0096, -0.0158, -0.0372,  ..., -0.0018,  0.0226,  0.0159],
        ...,
        [-0.0018, -0.0084, -0.0269,  ...,  0.0056, -0.0166,  0.0163],
        [ 0.0047,  0.0040,  0.0105,  ..., -0.0166,  0.0079, -0.0183],
        [-0.0305, -0.0198,  0.0165,  ...,  0.0109, -0.0196,  0.0113]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0349, -3.0566, -3.1602,  ..., -0.3398, -1.0918,  3.2539]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 13:52:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of uphill is downhill
The opposite of in is out
The opposite of under is over
The opposite of west is east
The opposite of rise is sink
The opposite of downslope is upslope
The opposite of beginning is end
The opposite of drop is
2024-07-23 13:52:39 root INFO     [order_1_approx] starting weight calculation for The opposite of west is east
The opposite of rise is sink
The opposite of uphill is downhill
The opposite of beginning is end
The opposite of drop is lift
The opposite of under is over
The opposite of downslope is upslope
The opposite of in is
2024-07-23 13:52:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 13:56:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6323,  0.4019, -0.3242,  ...,  0.3687,  0.3247, -0.1836],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.3359, -5.3516,  2.1641,  ...,  0.4226,  2.8574, -4.1797],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0217,  0.0059,  0.0264,  ...,  0.0019, -0.0302, -0.0290],
        [ 0.0071,  0.0484, -0.0244,  ...,  0.0274, -0.0011,  0.0180],
        [-0.0079, -0.0134, -0.0446,  ..., -0.0060,  0.0068,  0.0267],
        ...,
        [ 0.0008, -0.0107,  0.0038,  ...,  0.0111, -0.0236,  0.0191],
        [-0.0104,  0.0166,  0.0140,  ..., -0.0098, -0.0128, -0.0075],
        [ 0.0367,  0.0018, -0.0263,  ...,  0.0092,  0.0500,  0.0404]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.8047, -5.0000,  2.4531,  ..., -0.1760,  2.3730, -2.2598]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 13:56:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of west is east
The opposite of rise is sink
The opposite of uphill is downhill
The opposite of beginning is end
The opposite of drop is lift
The opposite of under is over
The opposite of downslope is upslope
The opposite of in is
2024-07-23 13:56:27 root INFO     [order_1_approx] starting weight calculation for The opposite of west is east
The opposite of beginning is end
The opposite of under is over
The opposite of drop is lift
The opposite of uphill is downhill
The opposite of downslope is upslope
The opposite of in is out
The opposite of rise is
2024-07-23 13:56:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 14:00:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3359, -0.7446,  0.2051,  ...,  0.6914, -0.2074, -0.2092],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.4141, -3.8184,  3.0039,  ..., -2.6914,  1.9160,  3.3887],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.0689e-02, -1.8921e-03, -1.8753e-02,  ...,  1.6098e-03,
         -7.5493e-03,  2.0447e-02],
        [ 1.7487e-02,  1.1177e-02, -2.9739e-02,  ..., -1.2999e-03,
         -1.2451e-02, -1.9188e-03],
        [-2.0309e-02, -1.1482e-02, -2.9266e-02,  ...,  5.6038e-03,
          3.5820e-03,  2.3163e-02],
        ...,
        [ 5.8899e-03, -1.1856e-02, -1.5091e-02,  ..., -4.5776e-05,
         -1.6571e-02,  4.7340e-03],
        [-2.7466e-04,  1.6541e-02,  7.6294e-05,  ..., -3.0914e-02,
         -1.1337e-02, -4.1389e-04],
        [ 9.6588e-03, -1.5259e-05, -1.5640e-03,  ...,  7.4768e-03,
          1.0849e-02, -1.9501e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.6230, -3.3730,  3.7891,  ..., -1.4775,  2.5547,  2.8203]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 14:00:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of west is east
The opposite of beginning is end
The opposite of under is over
The opposite of drop is lift
The opposite of uphill is downhill
The opposite of downslope is upslope
The opposite of in is out
The opposite of rise is
2024-07-23 14:00:15 root INFO     [order_1_approx] starting weight calculation for The opposite of rise is sink
The opposite of west is east
The opposite of in is out
The opposite of drop is lift
The opposite of downslope is upslope
The opposite of beginning is end
The opposite of uphill is downhill
The opposite of under is
2024-07-23 14:00:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 14:04:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6914, -0.9551,  0.0408,  ...,  0.7637,  0.4302,  1.2734],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0137, -7.1250, -0.3838,  ...,  0.6753,  2.6094, -0.3633],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0071, -0.0275,  0.0044,  ..., -0.0044,  0.0025, -0.0076],
        [-0.0283,  0.0062, -0.0028,  ...,  0.0119,  0.0141,  0.0133],
        [ 0.0137, -0.0201, -0.0459,  ...,  0.0032,  0.0269,  0.0130],
        ...,
        [ 0.0114,  0.0076,  0.0029,  ..., -0.0172, -0.0034,  0.0022],
        [-0.0052, -0.0034,  0.0320,  ..., -0.0310, -0.0046,  0.0141],
        [-0.0120,  0.0007, -0.0279,  ..., -0.0243,  0.0170, -0.0304]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3047, -6.4023, -0.4556,  ...,  0.7041,  3.5000,  1.0664]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 14:04:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of rise is sink
The opposite of west is east
The opposite of in is out
The opposite of drop is lift
The opposite of downslope is upslope
The opposite of beginning is end
The opposite of uphill is downhill
The opposite of under is
2024-07-23 14:04:03 root INFO     [order_1_approx] starting weight calculation for The opposite of rise is sink
The opposite of under is over
The opposite of downslope is upslope
The opposite of drop is lift
The opposite of in is out
The opposite of west is east
The opposite of beginning is end
The opposite of uphill is
2024-07-23 14:04:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 14:07:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2249, -0.7529, -0.8516,  ..., -0.4055,  0.5601, -0.2030],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2112,  0.2075,  1.4629,  ..., -3.2070,  2.0820,  2.8398],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0302, -0.0278,  0.0242,  ...,  0.0224, -0.0049, -0.0017],
        [-0.0064, -0.0016, -0.0328,  ...,  0.0105, -0.0195,  0.0117],
        [-0.0264,  0.0025, -0.0241,  ..., -0.0083, -0.0067,  0.0014],
        ...,
        [ 0.0236, -0.0056, -0.0161,  ..., -0.0137, -0.0084, -0.0127],
        [-0.0350,  0.0201,  0.0347,  ..., -0.0135, -0.0143, -0.0154],
        [-0.0040,  0.0023, -0.0003,  ..., -0.0090, -0.0075, -0.0038]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9990,  0.7583,  2.4160,  ..., -2.6426,  2.2227,  2.6816]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 14:07:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of rise is sink
The opposite of under is over
The opposite of downslope is upslope
The opposite of drop is lift
The opposite of in is out
The opposite of west is east
The opposite of beginning is end
The opposite of uphill is
2024-07-23 14:07:52 root INFO     [order_1_approx] starting weight calculation for The opposite of under is over
The opposite of uphill is downhill
The opposite of downslope is upslope
The opposite of drop is lift
The opposite of beginning is end
The opposite of rise is sink
The opposite of in is out
The opposite of west is
2024-07-23 14:07:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 14:11:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6582, -0.9194, -0.8262,  ..., -0.0488,  2.0254,  0.5649],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.0664, -2.9082,  1.8496,  ..., -2.4180, -0.3779, -0.0332],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0149, -0.0124,  0.0033,  ...,  0.0029, -0.0262,  0.0149],
        [-0.0162, -0.0106, -0.0077,  ...,  0.0067,  0.0001,  0.0070],
        [-0.0036,  0.0064, -0.0122,  ..., -0.0201,  0.0039,  0.0056],
        ...,
        [ 0.0224, -0.0121, -0.0074,  ..., -0.0199,  0.0006,  0.0105],
        [ 0.0265,  0.0050,  0.0224,  ..., -0.0082, -0.0146,  0.0107],
        [-0.0096, -0.0067, -0.0053,  ...,  0.0094,  0.0248, -0.0018]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.4883, -3.3809,  1.5742,  ..., -2.2031,  0.5020,  0.6255]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 14:11:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of under is over
The opposite of uphill is downhill
The opposite of downslope is upslope
The opposite of drop is lift
The opposite of beginning is end
The opposite of rise is sink
The opposite of in is out
The opposite of west is
2024-07-23 14:11:41 root INFO     total operator prediction time: 1811.319447517395 seconds
2024-07-23 14:11:41 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - member
2024-07-23 14:11:41 root INFO     building operator meronyms - member
2024-07-23 14:11:41 root INFO     [order_1_approx] starting weight calculation for A elephant is a member of a herd
A spouse is a member of a couple
A singer is a member of a choir
A employee is a member of a staff
A citizen is a member of a citizenry
A parishioner is a member of a parish
A student is a member of a class
A bird is a member of a
2024-07-23 14:11:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 14:15:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2061,  0.0278,  1.0186,  ...,  1.2188, -0.9648, -0.1693],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1230e+00,  1.0166e+00, -3.0898e+00,  ...,  3.8262e+00,
        -1.4648e-03, -5.1318e-01], device='cuda:0', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0131, -0.0053,  0.0098,  ...,  0.0006,  0.0019, -0.0100],
        [ 0.0116, -0.0096,  0.0032,  ...,  0.0083,  0.0045,  0.0073],
        [ 0.0128,  0.0077,  0.0005,  ...,  0.0089, -0.0029,  0.0016],
        ...,
        [-0.0082,  0.0083, -0.0033,  ...,  0.0129, -0.0124,  0.0050],
        [-0.0091, -0.0053, -0.0006,  ...,  0.0044, -0.0085,  0.0048],
        [ 0.0090, -0.0077, -0.0026,  ..., -0.0115,  0.0126,  0.0077]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9170,  1.2539, -3.5938,  ...,  4.4648, -0.0198, -0.2852]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 14:15:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A elephant is a member of a herd
A spouse is a member of a couple
A singer is a member of a choir
A employee is a member of a staff
A citizen is a member of a citizenry
A parishioner is a member of a parish
A student is a member of a class
A bird is a member of a
2024-07-23 14:15:31 root INFO     [order_1_approx] starting weight calculation for A spouse is a member of a couple
A employee is a member of a staff
A parishioner is a member of a parish
A bird is a member of a flock
A singer is a member of a choir
A elephant is a member of a herd
A student is a member of a class
A citizen is a member of a
2024-07-23 14:15:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 14:19:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6543, -0.0479,  0.0437,  ...,  0.7388,  0.6416, -0.0540],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 6.0742,  0.0068,  1.1104,  ..., -0.7485,  1.5137,  1.7842],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0053, -0.0003,  0.0050,  ..., -0.0058, -0.0032,  0.0042],
        [ 0.0003,  0.0056,  0.0083,  ...,  0.0077,  0.0014,  0.0001],
        [ 0.0144, -0.0013,  0.0067,  ...,  0.0022,  0.0054, -0.0034],
        ...,
        [-0.0002, -0.0135, -0.0008,  ..., -0.0008, -0.0190,  0.0089],
        [-0.0019, -0.0143,  0.0063,  ...,  0.0020, -0.0058, -0.0167],
        [-0.0013,  0.0078,  0.0055,  ...,  0.0073,  0.0144,  0.0034]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.9492, -0.2476,  1.3594,  ..., -0.6675,  1.5049,  1.0195]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 14:19:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A spouse is a member of a couple
A employee is a member of a staff
A parishioner is a member of a parish
A bird is a member of a flock
A singer is a member of a choir
A elephant is a member of a herd
A student is a member of a class
A citizen is a member of a
2024-07-23 14:19:21 root INFO     [order_1_approx] starting weight calculation for A employee is a member of a staff
A parishioner is a member of a parish
A student is a member of a class
A bird is a member of a flock
A spouse is a member of a couple
A citizen is a member of a citizenry
A singer is a member of a choir
A elephant is a member of a
2024-07-23 14:19:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 14:23:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2343, -0.7788,  0.0272,  ...,  1.4004,  0.1163,  0.4868],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5352,  3.1973,  0.8784,  ..., -1.5039, -1.6250,  2.8008],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0061, -0.0095,  0.0061,  ..., -0.0077, -0.0071, -0.0134],
        [-0.0077, -0.0060,  0.0142,  ...,  0.0069,  0.0074,  0.0041],
        [ 0.0087, -0.0060, -0.0043,  ..., -0.0007,  0.0038, -0.0038],
        ...,
        [-0.0033, -0.0058, -0.0094,  ...,  0.0081,  0.0039,  0.0105],
        [ 0.0058, -0.0067,  0.0021,  ..., -0.0009, -0.0115, -0.0131],
        [ 0.0072, -0.0127,  0.0018,  ..., -0.0077,  0.0065, -0.0005]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7598,  3.0586,  1.4268,  ..., -1.1172, -1.9844,  3.6367]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 14:23:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A employee is a member of a staff
A parishioner is a member of a parish
A student is a member of a class
A bird is a member of a flock
A spouse is a member of a couple
A citizen is a member of a citizenry
A singer is a member of a choir
A elephant is a member of a
2024-07-23 14:23:10 root INFO     [order_1_approx] starting weight calculation for A parishioner is a member of a parish
A singer is a member of a choir
A citizen is a member of a citizenry
A spouse is a member of a couple
A student is a member of a class
A bird is a member of a flock
A elephant is a member of a herd
A employee is a member of a
2024-07-23 14:23:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 14:27:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([1.3232, 0.1409, 1.6289,  ..., 1.0996, 0.0221, 0.1941], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0488, -2.4805,  0.8125,  ...,  1.1270,  1.4199,  2.3574],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 7.1144e-04, -7.8506e-03,  5.1880e-03,  ..., -3.5152e-03,
          2.5177e-03, -1.0628e-02],
        [-1.1612e-02,  7.8773e-04,  7.9393e-04,  ..., -6.1321e-04,
         -6.4850e-05, -9.3842e-04],
        [-3.4885e-03,  1.1467e-02, -2.7313e-03,  ...,  5.4054e-03,
          1.2875e-03,  4.4632e-03],
        ...,
        [-2.4776e-03, -4.0855e-03,  9.6130e-04,  ...,  9.9335e-03,
         -1.3542e-03,  1.3206e-02],
        [ 6.0654e-03, -7.1831e-03,  4.8923e-04,  ..., -3.9062e-03,
          5.8784e-03, -7.0839e-03],
        [-1.3885e-03,  2.5539e-03,  7.4997e-03,  ...,  2.0142e-03,
          1.8906e-02,  1.3412e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.8037, -2.4434,  0.5337,  ...,  1.1240,  1.1602,  1.7832]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 14:27:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A parishioner is a member of a parish
A singer is a member of a choir
A citizen is a member of a citizenry
A spouse is a member of a couple
A student is a member of a class
A bird is a member of a flock
A elephant is a member of a herd
A employee is a member of a
2024-07-23 14:27:01 root INFO     [order_1_approx] starting weight calculation for A elephant is a member of a herd
A singer is a member of a choir
A citizen is a member of a citizenry
A spouse is a member of a couple
A bird is a member of a flock
A student is a member of a class
A employee is a member of a staff
A parishioner is a member of a
2024-07-23 14:27:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 14:30:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0576, -1.3516,  1.3398,  ...,  1.7148,  0.5518, -0.1387],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([1.8867, 1.6465, 2.9961,  ..., 0.4697, 0.2339, 2.4316], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 6.8665e-05,  8.2779e-04,  3.5954e-03,  ..., -3.9139e-03,
          3.2020e-04, -1.5259e-03],
        [ 3.1776e-03,  3.8681e-03,  9.5749e-03,  ...,  5.9700e-04,
          1.8969e-03, -6.2180e-03],
        [-2.3460e-03,  2.8706e-03, -2.2526e-03,  ..., -7.6485e-04,
          6.3744e-03,  6.4316e-03],
        ...,
        [-5.6686e-03,  2.1515e-03, -1.8730e-03,  ...,  2.8515e-03,
         -4.4670e-03,  7.2021e-03],
        [-6.8054e-03, -8.5754e-03,  5.1270e-03,  ...,  2.9793e-03,
         -7.4863e-04, -3.7613e-03],
        [-1.4610e-03, -5.3368e-03, -5.7220e-06,  ...,  6.0043e-03,
          1.1398e-02,  5.8632e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[1.5293, 1.4014, 2.7656,  ..., 0.1228, 0.2947, 2.0391]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 14:30:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A elephant is a member of a herd
A singer is a member of a choir
A citizen is a member of a citizenry
A spouse is a member of a couple
A bird is a member of a flock
A student is a member of a class
A employee is a member of a staff
A parishioner is a member of a
2024-07-23 14:30:51 root INFO     [order_1_approx] starting weight calculation for A elephant is a member of a herd
A student is a member of a class
A citizen is a member of a citizenry
A spouse is a member of a couple
A employee is a member of a staff
A bird is a member of a flock
A parishioner is a member of a parish
A singer is a member of a
2024-07-23 14:30:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 14:34:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5254,  0.7837,  0.4592,  ...,  0.5234,  0.5107, -0.8154],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.2578,  2.3945,  1.2100,  ...,  2.4570, -0.0830, -0.7910],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0062,  0.0002, -0.0031,  ...,  0.0021,  0.0027, -0.0088],
        [ 0.0003,  0.0103,  0.0181,  ...,  0.0106, -0.0151, -0.0003],
        [-0.0031, -0.0050, -0.0088,  ...,  0.0166,  0.0037,  0.0159],
        ...,
        [-0.0094, -0.0008, -0.0172,  ..., -0.0041, -0.0175,  0.0176],
        [ 0.0004, -0.0041, -0.0029,  ...,  0.0047, -0.0269, -0.0063],
        [ 0.0108, -0.0011,  0.0068,  ..., -0.0051,  0.0175, -0.0003]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6597,  2.5703,  0.6855,  ...,  3.0156, -0.1007, -1.5117]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 14:34:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A elephant is a member of a herd
A student is a member of a class
A citizen is a member of a citizenry
A spouse is a member of a couple
A employee is a member of a staff
A bird is a member of a flock
A parishioner is a member of a parish
A singer is a member of a
2024-07-23 14:34:46 root INFO     [order_1_approx] starting weight calculation for A citizen is a member of a citizenry
A elephant is a member of a herd
A student is a member of a class
A bird is a member of a flock
A parishioner is a member of a parish
A singer is a member of a choir
A employee is a member of a staff
A spouse is a member of a
2024-07-23 14:34:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 14:38:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.7402,  0.7207,  0.7275,  ...,  0.1327, -0.5420, -0.6924],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8037,  0.1777,  2.1641,  ..., -2.7891,  0.8486,  1.6025],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0014,  0.0071,  0.0090,  ..., -0.0051,  0.0239, -0.0028],
        [-0.0028,  0.0002,  0.0048,  ..., -0.0054,  0.0007, -0.0105],
        [ 0.0002,  0.0017, -0.0144,  ...,  0.0105, -0.0039, -0.0032],
        ...,
        [-0.0086, -0.0176,  0.0007,  ..., -0.0082, -0.0043,  0.0051],
        [ 0.0020,  0.0133,  0.0068,  ...,  0.0080, -0.0137,  0.0040],
        [-0.0050, -0.0045, -0.0026,  ...,  0.0047,  0.0223,  0.0064]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4783,  0.0929,  1.7598,  ..., -2.7324,  0.4504,  1.3867]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 14:38:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A citizen is a member of a citizenry
A elephant is a member of a herd
A student is a member of a class
A bird is a member of a flock
A parishioner is a member of a parish
A singer is a member of a choir
A employee is a member of a staff
A spouse is a member of a
2024-07-23 14:38:35 root INFO     [order_1_approx] starting weight calculation for A citizen is a member of a citizenry
A employee is a member of a staff
A singer is a member of a choir
A elephant is a member of a herd
A spouse is a member of a couple
A parishioner is a member of a parish
A bird is a member of a flock
A student is a member of a
2024-07-23 14:38:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 14:42:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([1.4102, 0.3208, 1.0693,  ..., 1.2578, 0.3711, 0.5737], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3506,  3.3066, -1.5303,  ...,  3.1738,  3.4336,  2.2969],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0035,  0.0075,  0.0122,  ...,  0.0061, -0.0066, -0.0001],
        [ 0.0013,  0.0210,  0.0134,  ...,  0.0038, -0.0005, -0.0103],
        [-0.0109,  0.0061,  0.0059,  ...,  0.0065, -0.0011,  0.0028],
        ...,
        [-0.0024,  0.0072,  0.0044,  ...,  0.0082, -0.0020,  0.0099],
        [ 0.0124, -0.0301,  0.0043,  ...,  0.0076, -0.0167, -0.0069],
        [ 0.0084, -0.0016, -0.0019,  ...,  0.0049,  0.0120,  0.0051]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3877,  4.1562, -2.0723,  ...,  3.2441,  3.6133,  1.9150]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 14:42:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A citizen is a member of a citizenry
A employee is a member of a staff
A singer is a member of a choir
A elephant is a member of a herd
A spouse is a member of a couple
A parishioner is a member of a parish
A bird is a member of a flock
A student is a member of a
2024-07-23 14:42:27 root INFO     total operator prediction time: 1846.5949482917786 seconds
2024-07-23 14:42:27 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on noun - plural_irreg
2024-07-23 14:42:27 root INFO     building operator noun - plural_irreg
2024-07-23 14:42:28 root INFO     [order_1_approx] starting weight calculation for The plural form of library is libraries
The plural form of property is properties
The plural form of opportunity is opportunities
The plural form of secretary is secretaries
The plural form of wife is wives
The plural form of story is stories
The plural form of responsibility is responsibilities
The plural form of family is
2024-07-23 14:42:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 14:46:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5615,  0.7422,  0.8374,  ..., -0.3474,  0.6997,  0.3989],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0322, -0.7627, -1.3643,  ..., -4.0859,  1.2168,  3.5898],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0172, -0.0095,  0.0143,  ..., -0.0017,  0.0140, -0.0255],
        [ 0.0121, -0.0008, -0.0058,  ...,  0.0064, -0.0018, -0.0054],
        [ 0.0085, -0.0095, -0.0077,  ..., -0.0077,  0.0189, -0.0134],
        ...,
        [-0.0112, -0.0126,  0.0032,  ..., -0.0092,  0.0076, -0.0231],
        [-0.0018, -0.0039,  0.0172,  ..., -0.0177, -0.0144,  0.0058],
        [-0.0063,  0.0016, -0.0129,  ..., -0.0086,  0.0167, -0.0020]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9443, -0.1079, -1.9922,  ..., -5.0977,  0.6343,  3.8516]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 14:46:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of library is libraries
The plural form of property is properties
The plural form of opportunity is opportunities
The plural form of secretary is secretaries
The plural form of wife is wives
The plural form of story is stories
The plural form of responsibility is responsibilities
The plural form of family is
2024-07-23 14:46:16 root INFO     [order_1_approx] starting weight calculation for The plural form of family is families
The plural form of story is stories
The plural form of responsibility is responsibilities
The plural form of opportunity is opportunities
The plural form of secretary is secretaries
The plural form of property is properties
The plural form of wife is wives
The plural form of library is
2024-07-23 14:46:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 14:50:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.6445,  0.9326, -0.2576,  ..., -0.2656, -0.4373,  0.2920],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9092,  0.4292, -2.4922,  ...,  0.1465, -0.6914,  1.0938],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0033, -0.0185,  0.0051,  ..., -0.0035,  0.0102, -0.0142],
        [ 0.0085, -0.0124,  0.0013,  ...,  0.0026, -0.0090, -0.0059],
        [ 0.0107,  0.0139,  0.0013,  ..., -0.0012,  0.0117,  0.0153],
        ...,
        [-0.0035, -0.0106,  0.0024,  ..., -0.0053,  0.0113, -0.0035],
        [-0.0019,  0.0021,  0.0061,  ..., -0.0077, -0.0176, -0.0026],
        [-0.0015,  0.0106, -0.0003,  ..., -0.0045,  0.0142, -0.0026]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9849,  0.6592, -2.5918,  ..., -0.1060, -0.0977,  1.0986]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 14:50:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of family is families
The plural form of story is stories
The plural form of responsibility is responsibilities
The plural form of opportunity is opportunities
The plural form of secretary is secretaries
The plural form of property is properties
The plural form of wife is wives
The plural form of library is
2024-07-23 14:50:05 root INFO     [order_1_approx] starting weight calculation for The plural form of library is libraries
The plural form of wife is wives
The plural form of property is properties
The plural form of secretary is secretaries
The plural form of story is stories
The plural form of family is families
The plural form of responsibility is responsibilities
The plural form of opportunity is
2024-07-23 14:50:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 14:53:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0400, -0.8789,  1.2979,  ..., -0.6270,  0.6123, -0.5840],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.0176,  1.7227, -0.6177,  ...,  0.9639,  1.5312,  1.1152],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0049, -0.0200,  0.0067,  ..., -0.0077,  0.0092, -0.0204],
        [-0.0020,  0.0053, -0.0173,  ...,  0.0133,  0.0087,  0.0104],
        [ 0.0052, -0.0093, -0.0094,  ...,  0.0045,  0.0080, -0.0107],
        ...,
        [-0.0032, -0.0202, -0.0045,  ..., -0.0261,  0.0045,  0.0072],
        [ 0.0020,  0.0052,  0.0074,  ..., -0.0168, -0.0177,  0.0020],
        [ 0.0051,  0.0109,  0.0035,  ..., -0.0044,  0.0243, -0.0103]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.9746,  1.8994, -1.1025,  ...,  0.5146,  0.9976,  1.3633]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 14:53:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of library is libraries
The plural form of wife is wives
The plural form of property is properties
The plural form of secretary is secretaries
The plural form of story is stories
The plural form of family is families
The plural form of responsibility is responsibilities
The plural form of opportunity is
2024-07-23 14:53:54 root INFO     [order_1_approx] starting weight calculation for The plural form of opportunity is opportunities
The plural form of library is libraries
The plural form of wife is wives
The plural form of secretary is secretaries
The plural form of responsibility is responsibilities
The plural form of family is families
The plural form of story is stories
The plural form of property is
2024-07-23 14:53:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 14:57:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2656, -0.4824, -0.0704,  ...,  0.8662, -0.3208, -0.0583],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1182, -3.6289, -0.1001,  ..., -1.1338, -3.7305,  1.0645],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0105,  0.0007,  0.0111,  ..., -0.0084,  0.0012, -0.0143],
        [-0.0071, -0.0129, -0.0007,  ...,  0.0105, -0.0036, -0.0028],
        [ 0.0169, -0.0031, -0.0057,  ..., -0.0070,  0.0067,  0.0104],
        ...,
        [ 0.0080, -0.0091, -0.0014,  ..., -0.0108,  0.0056, -0.0139],
        [ 0.0042,  0.0045,  0.0117,  ..., -0.0133, -0.0275,  0.0016],
        [-0.0102,  0.0014, -0.0095,  ..., -0.0104,  0.0244, -0.0085]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5488, -3.2188, -0.6650,  ..., -1.1074, -3.6934,  1.3613]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 14:57:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of opportunity is opportunities
The plural form of library is libraries
The plural form of wife is wives
The plural form of secretary is secretaries
The plural form of responsibility is responsibilities
The plural form of family is families
The plural form of story is stories
The plural form of property is
2024-07-23 14:57:43 root INFO     [order_1_approx] starting weight calculation for The plural form of opportunity is opportunities
The plural form of library is libraries
The plural form of property is properties
The plural form of secretary is secretaries
The plural form of wife is wives
The plural form of family is families
The plural form of story is stories
The plural form of responsibility is
2024-07-23 14:57:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 15:01:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0759, -0.1482,  1.5000,  ..., -0.4231, -0.7178,  0.0688],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-5.2773,  1.6445,  1.9600,  ...,  0.9551,  0.7271,  1.5283],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0023, -0.0016,  0.0211,  ..., -0.0128,  0.0033, -0.0175],
        [ 0.0013, -0.0029, -0.0169,  ..., -0.0004,  0.0077,  0.0048],
        [-0.0040, -0.0093, -0.0039,  ..., -0.0033,  0.0148,  0.0095],
        ...,
        [-0.0008, -0.0276, -0.0100,  ..., -0.0228,  0.0184,  0.0004],
        [ 0.0041, -0.0027,  0.0201,  ..., -0.0160, -0.0203,  0.0064],
        [-0.0097,  0.0202,  0.0022,  ..., -0.0018,  0.0253,  0.0102]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.5977,  2.0312,  1.4756,  ...,  1.4043,  0.5015,  2.2578]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 15:01:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of opportunity is opportunities
The plural form of library is libraries
The plural form of property is properties
The plural form of secretary is secretaries
The plural form of wife is wives
The plural form of family is families
The plural form of story is stories
The plural form of responsibility is
2024-07-23 15:01:33 root INFO     [order_1_approx] starting weight calculation for The plural form of opportunity is opportunities
The plural form of responsibility is responsibilities
The plural form of family is families
The plural form of story is stories
The plural form of wife is wives
The plural form of library is libraries
The plural form of property is properties
The plural form of secretary is
2024-07-23 15:01:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 15:05:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.8887,  0.1107, -0.2354,  ...,  1.1426, -0.4292,  0.2798],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-8.1797,  4.3633,  2.8262,  ...,  2.3262, -4.0859,  5.2930],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0085, -0.0079,  0.0176,  ...,  0.0035, -0.0065, -0.0150],
        [-0.0001, -0.0077, -0.0249,  ..., -0.0219,  0.0014,  0.0234],
        [ 0.0156,  0.0025,  0.0056,  ...,  0.0023,  0.0130,  0.0110],
        ...,
        [-0.0127, -0.0271, -0.0114,  ..., -0.0107, -0.0001, -0.0111],
        [ 0.0100, -0.0243,  0.0007,  ..., -0.0029, -0.0092,  0.0087],
        [-0.0042,  0.0115,  0.0096,  ..., -0.0011,  0.0047,  0.0037]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-8.1562,  5.1641,  3.2461,  ...,  2.0508, -3.9453,  5.5273]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 15:05:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of opportunity is opportunities
The plural form of responsibility is responsibilities
The plural form of family is families
The plural form of story is stories
The plural form of wife is wives
The plural form of library is libraries
The plural form of property is properties
The plural form of secretary is
2024-07-23 15:05:20 root INFO     [order_1_approx] starting weight calculation for The plural form of family is families
The plural form of responsibility is responsibilities
The plural form of property is properties
The plural form of secretary is secretaries
The plural form of opportunity is opportunities
The plural form of wife is wives
The plural form of library is libraries
The plural form of story is
2024-07-23 15:05:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 15:09:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([1.0312, 0.2498, 0.7773,  ..., 0.3740, 0.6514, 0.3674], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0430,  2.3555,  1.4824,  ...,  1.8047, -6.4336,  2.4453],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0112,  0.0015,  0.0061,  ..., -0.0046,  0.0227, -0.0131],
        [-0.0210, -0.0098, -0.0096,  ..., -0.0117, -0.0158,  0.0067],
        [ 0.0002, -0.0122, -0.0226,  ..., -0.0058,  0.0187,  0.0074],
        ...,
        [-0.0029, -0.0102,  0.0058,  ..., -0.0177,  0.0005, -0.0119],
        [ 0.0044,  0.0147,  0.0104,  ...,  0.0038, -0.0262, -0.0067],
        [-0.0069,  0.0241, -0.0012,  ..., -0.0105,  0.0044, -0.0107]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.1016,  2.4609,  1.6768,  ...,  1.7139, -6.3086,  2.5762]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 15:09:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of family is families
The plural form of responsibility is responsibilities
The plural form of property is properties
The plural form of secretary is secretaries
The plural form of opportunity is opportunities
The plural form of wife is wives
The plural form of library is libraries
The plural form of story is
2024-07-23 15:09:07 root INFO     [order_1_approx] starting weight calculation for The plural form of responsibility is responsibilities
The plural form of secretary is secretaries
The plural form of property is properties
The plural form of opportunity is opportunities
The plural form of story is stories
The plural form of library is libraries
The plural form of family is families
The plural form of wife is
2024-07-23 15:09:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 15:12:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.1328,  0.4507, -0.4751,  ...,  0.3257, -1.1055,  0.6226],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.6094, -0.9463,  0.7051,  ..., -3.3789,  2.5762,  1.1406],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0111, -0.0020,  0.0300,  ..., -0.0127,  0.0035, -0.0077],
        [ 0.0033, -0.0124, -0.0153,  ...,  0.0027,  0.0008,  0.0097],
        [ 0.0089,  0.0004, -0.0139,  ..., -0.0139,  0.0249, -0.0080],
        ...,
        [-0.0071, -0.0024,  0.0019,  ..., -0.0130, -0.0113, -0.0059],
        [ 0.0045,  0.0002, -0.0031,  ..., -0.0266, -0.0093, -0.0113],
        [-0.0154, -0.0052, -0.0156,  ..., -0.0097,  0.0095,  0.0216]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.7793, -0.7568,  0.3140,  ..., -3.4473,  2.6875,  1.6143]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 15:12:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of responsibility is responsibilities
The plural form of secretary is secretaries
The plural form of property is properties
The plural form of opportunity is opportunities
The plural form of story is stories
The plural form of library is libraries
The plural form of family is families
The plural form of wife is
2024-07-23 15:12:54 root INFO     total operator prediction time: 1826.7397804260254 seconds
2024-07-23 15:12:54 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on Ving - verb_inf
2024-07-23 15:12:54 root INFO     building operator Ving - verb_inf
2024-07-23 15:12:54 root INFO     [order_1_approx] starting weight calculation for maintaining is the active form of maintain
performing is the active form of perform
sitting is the active form of sit
telling is the active form of tell
improving is the active form of improve
referring is the active form of refer
expecting is the active form of expect
existing is the active form of
2024-07-23 15:12:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 15:16:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2119, -0.4297,  2.0488,  ...,  0.6685,  2.4043, -0.4141],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.5508,  0.3848,  4.1797,  ..., -3.5996,  1.2793, -0.2805],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0027, -0.0103,  0.0117,  ..., -0.0084,  0.0052, -0.0113],
        [-0.0012, -0.0172,  0.0082,  ...,  0.0059, -0.0087,  0.0043],
        [-0.0157,  0.0053, -0.0170,  ...,  0.0058,  0.0014, -0.0158],
        ...,
        [-0.0157, -0.0122, -0.0107,  ..., -0.0182, -0.0074, -0.0002],
        [-0.0026, -0.0199,  0.0202,  ..., -0.0098, -0.0188,  0.0032],
        [ 0.0117,  0.0035,  0.0203,  ...,  0.0005,  0.0204, -0.0125]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.2871,  1.9209,  4.6133,  ..., -3.7402,  0.9258, -0.8057]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 15:16:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for maintaining is the active form of maintain
performing is the active form of perform
sitting is the active form of sit
telling is the active form of tell
improving is the active form of improve
referring is the active form of refer
expecting is the active form of expect
existing is the active form of
2024-07-23 15:16:44 root INFO     [order_1_approx] starting weight calculation for referring is the active form of refer
maintaining is the active form of maintain
telling is the active form of tell
performing is the active form of perform
improving is the active form of improve
existing is the active form of exist
sitting is the active form of sit
expecting is the active form of
2024-07-23 15:16:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 15:20:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4453, -0.4358,  0.0591,  ..., -0.0181,  2.5508,  0.7515],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 6.2578,  0.8799,  4.0039,  ..., -2.9023,  0.4570,  0.8013],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0026,  0.0012,  0.0046,  ..., -0.0020, -0.0031, -0.0019],
        [ 0.0017, -0.0072,  0.0047,  ..., -0.0017, -0.0074,  0.0059],
        [ 0.0193,  0.0154,  0.0018,  ..., -0.0044,  0.0083,  0.0034],
        ...,
        [-0.0189, -0.0241,  0.0111,  ..., -0.0017, -0.0010, -0.0032],
        [-0.0023, -0.0071,  0.0079,  ..., -0.0136, -0.0218,  0.0005],
        [ 0.0085,  0.0214,  0.0064,  ..., -0.0137,  0.0043, -0.0013]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.9531,  1.7070,  3.5000,  ..., -3.4512,  0.1570,  0.7368]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 15:20:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for referring is the active form of refer
maintaining is the active form of maintain
telling is the active form of tell
performing is the active form of perform
improving is the active form of improve
existing is the active form of exist
sitting is the active form of sit
expecting is the active form of
2024-07-23 15:20:35 root INFO     [order_1_approx] starting weight calculation for sitting is the active form of sit
referring is the active form of refer
maintaining is the active form of maintain
existing is the active form of exist
telling is the active form of tell
expecting is the active form of expect
performing is the active form of perform
improving is the active form of
2024-07-23 15:20:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 15:24:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3857, -1.2617,  0.4951,  ..., -0.3557,  1.9346, -0.2529],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([12.4531,  2.7266, -1.4590,  ..., -0.0757, -0.4727,  0.7314],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0099,  0.0026,  0.0141,  ..., -0.0220, -0.0119, -0.0122],
        [ 0.0019, -0.0033,  0.0048,  ...,  0.0130, -0.0069,  0.0075],
        [ 0.0090, -0.0058, -0.0070,  ..., -0.0128,  0.0336, -0.0013],
        ...,
        [-0.0396, -0.0119, -0.0103,  ...,  0.0006, -0.0051, -0.0182],
        [-0.0181, -0.0074,  0.0035,  ..., -0.0138, -0.0153,  0.0115],
        [-0.0090,  0.0162,  0.0059,  ..., -0.0003,  0.0132, -0.0092]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[13.9766,  3.5039, -1.3408,  ...,  0.2458, -0.1487, -0.1353]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 15:24:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for sitting is the active form of sit
referring is the active form of refer
maintaining is the active form of maintain
existing is the active form of exist
telling is the active form of tell
expecting is the active form of expect
performing is the active form of perform
improving is the active form of
2024-07-23 15:24:20 root INFO     [order_1_approx] starting weight calculation for expecting is the active form of expect
referring is the active form of refer
sitting is the active form of sit
performing is the active form of perform
improving is the active form of improve
existing is the active form of exist
telling is the active form of tell
maintaining is the active form of
2024-07-23 15:24:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 15:28:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3457, -0.9971,  1.0449,  ...,  0.2510,  0.3008, -0.0620],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.9219, -1.4824, -2.7695,  ..., -1.0293,  1.3994,  5.1523],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0034, -0.0041,  0.0189,  ..., -0.0115,  0.0120, -0.0142],
        [-0.0009,  0.0007,  0.0073,  ...,  0.0040,  0.0023,  0.0051],
        [-0.0008,  0.0135, -0.0001,  ..., -0.0009, -0.0038, -0.0069],
        ...,
        [-0.0120, -0.0107, -0.0099,  ..., -0.0074,  0.0213, -0.0091],
        [-0.0064,  0.0095,  0.0031,  ..., -0.0170, -0.0133,  0.0070],
        [ 0.0038,  0.0068,  0.0059,  ..., -0.0137,  0.0091, -0.0078]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.9219, -1.2324, -2.8301,  ..., -0.4546,  1.3242,  5.0547]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 15:28:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for expecting is the active form of expect
referring is the active form of refer
sitting is the active form of sit
performing is the active form of perform
improving is the active form of improve
existing is the active form of exist
telling is the active form of tell
maintaining is the active form of
2024-07-23 15:28:12 root INFO     [order_1_approx] starting weight calculation for expecting is the active form of expect
improving is the active form of improve
sitting is the active form of sit
telling is the active form of tell
existing is the active form of exist
referring is the active form of refer
maintaining is the active form of maintain
performing is the active form of
2024-07-23 15:28:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 15:32:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7905,  0.0596,  1.0586,  ..., -1.0215,  2.1426, -0.8350],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 8.5938, -0.2969,  1.4297,  ..., -1.0771,  2.0605,  1.2422],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0089, -0.0142,  0.0169,  ..., -0.0096, -0.0084, -0.0163],
        [ 0.0006, -0.0176, -0.0043,  ...,  0.0030, -0.0067, -0.0103],
        [-0.0077, -0.0073, -0.0130,  ..., -0.0036,  0.0106, -0.0181],
        ...,
        [-0.0207, -0.0142,  0.0015,  ..., -0.0143,  0.0005, -0.0166],
        [-0.0188, -0.0018,  0.0196,  ..., -0.0236, -0.0144,  0.0067],
        [ 0.0039,  0.0215,  0.0155,  ..., -0.0078,  0.0006, -0.0069]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 9.3047, -0.0680,  0.9097,  ..., -0.4194,  1.5957,  0.9014]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 15:32:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for expecting is the active form of expect
improving is the active form of improve
sitting is the active form of sit
telling is the active form of tell
existing is the active form of exist
referring is the active form of refer
maintaining is the active form of maintain
performing is the active form of
2024-07-23 15:32:02 root INFO     [order_1_approx] starting weight calculation for existing is the active form of exist
expecting is the active form of expect
improving is the active form of improve
telling is the active form of tell
sitting is the active form of sit
performing is the active form of perform
maintaining is the active form of maintain
referring is the active form of
2024-07-23 15:32:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 15:35:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0820, -1.1992,  0.7339,  ..., -0.3218,  1.9688, -0.6538],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4932, -1.0664, -0.4883,  ...,  1.8164,  0.4058,  1.2803],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0034, -0.0167,  0.0058,  ..., -0.0045,  0.0172, -0.0049],
        [ 0.0029, -0.0051,  0.0100,  ...,  0.0114,  0.0047,  0.0057],
        [-0.0051, -0.0048, -0.0084,  ..., -0.0020,  0.0030, -0.0042],
        ...,
        [-0.0095, -0.0104,  0.0005,  ..., -0.0217, -0.0031,  0.0074],
        [-0.0163,  0.0068,  0.0080,  ...,  0.0006, -0.0282,  0.0101],
        [ 0.0098,  0.0183,  0.0103,  ..., -0.0083,  0.0026, -0.0238]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1162, -0.4819, -0.3013,  ...,  1.3750,  1.0039,  1.3760]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 15:35:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for existing is the active form of exist
expecting is the active form of expect
improving is the active form of improve
telling is the active form of tell
sitting is the active form of sit
performing is the active form of perform
maintaining is the active form of maintain
referring is the active form of
2024-07-23 15:35:52 root INFO     [order_1_approx] starting weight calculation for maintaining is the active form of maintain
referring is the active form of refer
performing is the active form of perform
existing is the active form of exist
expecting is the active form of expect
telling is the active form of tell
improving is the active form of improve
sitting is the active form of
2024-07-23 15:35:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 15:39:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3340, -2.0820,  1.2148,  ...,  1.0215,  1.7275,  0.4517],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.9727, -0.0439,  3.5645,  ..., -0.1721, -2.8809,  2.1309],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0046,  0.0041,  0.0065,  ..., -0.0222,  0.0038, -0.0158],
        [-0.0007, -0.0056,  0.0069,  ...,  0.0055, -0.0043,  0.0065],
        [ 0.0065,  0.0096, -0.0119,  ..., -0.0077,  0.0062, -0.0088],
        ...,
        [-0.0196, -0.0169,  0.0099,  ...,  0.0004, -0.0130,  0.0111],
        [ 0.0070, -0.0075,  0.0139,  ...,  0.0111, -0.0010, -0.0005],
        [ 0.0037,  0.0021, -0.0013,  ...,  0.0030,  0.0245, -0.0182]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.2695,  0.2037,  3.9121,  ...,  0.5127, -2.9473,  1.2793]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 15:39:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for maintaining is the active form of maintain
referring is the active form of refer
performing is the active form of perform
existing is the active form of exist
expecting is the active form of expect
telling is the active form of tell
improving is the active form of improve
sitting is the active form of
2024-07-23 15:39:42 root INFO     [order_1_approx] starting weight calculation for maintaining is the active form of maintain
expecting is the active form of expect
existing is the active form of exist
referring is the active form of refer
performing is the active form of perform
sitting is the active form of sit
improving is the active form of improve
telling is the active form of
2024-07-23 15:39:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 15:43:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2275, -0.4463,  0.5791,  ...,  1.1855,  0.4788, -0.2131],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.6895,  2.3281,  1.9268,  ..., -1.1572,  3.8828,  2.7578],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0026, -0.0240,  0.0148,  ..., -0.0226,  0.0045, -0.0115],
        [ 0.0053, -0.0055,  0.0059,  ...,  0.0006, -0.0238,  0.0086],
        [ 0.0054,  0.0045, -0.0042,  ..., -0.0151,  0.0220, -0.0036],
        ...,
        [-0.0212, -0.0283, -0.0113,  ..., -0.0150, -0.0077, -0.0100],
        [-0.0171, -0.0026,  0.0023,  ..., -0.0136, -0.0258,  0.0145],
        [ 0.0415,  0.0254,  0.0181,  ..., -0.0070,  0.0131, -0.0082]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[4.1758, 2.8027, 2.0977,  ..., 0.3926, 4.5000, 2.4336]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 15:43:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for maintaining is the active form of maintain
expecting is the active form of expect
existing is the active form of exist
referring is the active form of refer
performing is the active form of perform
sitting is the active form of sit
improving is the active form of improve
telling is the active form of
2024-07-23 15:43:29 root INFO     total operator prediction time: 1835.3044648170471 seconds
2024-07-23 15:43:29 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_Ving - Ved
2024-07-23 15:43:29 root INFO     building operator verb_Ving - Ved
2024-07-23 15:43:30 root INFO     [order_1_approx] starting weight calculation for After something is teaching, it has taught
After something is operating, it has operated
After something is allowing, it has allowed
After something is establishing, it has established
After something is appointing, it has appointed
After something is improving, it has improved
After something is requiring, it has required
After something is agreeing, it has
2024-07-23 15:43:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 15:47:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5356, -0.0718,  0.3762,  ..., -0.0640, -0.6104, -0.0862],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8120,  2.6758,  3.1992,  ...,  0.0582, -4.8281,  0.5078],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0381, -0.0190,  0.0109,  ..., -0.0041,  0.0018, -0.0231],
        [ 0.0047, -0.0198, -0.0083,  ...,  0.0044,  0.0030,  0.0002],
        [-0.0089, -0.0084, -0.0192,  ..., -0.0082, -0.0081,  0.0163],
        ...,
        [-0.0176, -0.0105, -0.0086,  ..., -0.0079, -0.0092,  0.0003],
        [-0.0087, -0.0030,  0.0163,  ..., -0.0205, -0.0394,  0.0150],
        [-0.0110,  0.0162,  0.0007,  ..., -0.0031,  0.0061, -0.0407]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3906,  2.8672,  3.5996,  ...,  0.6616, -4.0703,  0.3740]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 15:47:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is teaching, it has taught
After something is operating, it has operated
After something is allowing, it has allowed
After something is establishing, it has established
After something is appointing, it has appointed
After something is improving, it has improved
After something is requiring, it has required
After something is agreeing, it has
2024-07-23 15:47:19 root INFO     [order_1_approx] starting weight calculation for After something is improving, it has improved
After something is teaching, it has taught
After something is agreeing, it has agreed
After something is operating, it has operated
After something is appointing, it has appointed
After something is requiring, it has required
After something is establishing, it has established
After something is allowing, it has
2024-07-23 15:47:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 15:51:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9136, -0.2397,  1.0303,  ...,  0.9639,  0.3638,  0.0938],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3535, -0.4097,  0.8945,  ...,  1.9717, -1.6299,  3.9727],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0153, -0.0096,  0.0120,  ...,  0.0129,  0.0112, -0.0052],
        [ 0.0016, -0.0234,  0.0059,  ..., -0.0050, -0.0052,  0.0024],
        [-0.0113, -0.0134, -0.0102,  ..., -0.0072,  0.0074, -0.0034],
        ...,
        [-0.0203, -0.0129, -0.0152,  ..., -0.0244, -0.0046, -0.0014],
        [ 0.0083, -0.0061,  0.0215,  ..., -0.0153, -0.0358,  0.0187],
        [-0.0091, -0.0037, -0.0027,  ...,  0.0026, -0.0125, -0.0078]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0657, -0.3064,  1.3457,  ...,  2.4062, -0.7129,  3.6895]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 15:51:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is improving, it has improved
After something is teaching, it has taught
After something is agreeing, it has agreed
After something is operating, it has operated
After something is appointing, it has appointed
After something is requiring, it has required
After something is establishing, it has established
After something is allowing, it has
2024-07-23 15:51:09 root INFO     [order_1_approx] starting weight calculation for After something is operating, it has operated
After something is establishing, it has established
After something is teaching, it has taught
After something is allowing, it has allowed
After something is improving, it has improved
After something is requiring, it has required
After something is agreeing, it has agreed
After something is appointing, it has
2024-07-23 15:51:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 15:55:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6514, -0.8325,  0.6265,  ...,  0.8604,  0.0149,  0.6582],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.8682,  0.1580,  1.6445,  ...,  3.8691, -0.7725,  2.1953],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0295, -0.0169,  0.0043,  ...,  0.0081, -0.0073, -0.0147],
        [ 0.0107, -0.0075, -0.0006,  ..., -0.0206,  0.0057,  0.0026],
        [-0.0120,  0.0043, -0.0081,  ...,  0.0094,  0.0090, -0.0132],
        ...,
        [-0.0176, -0.0138, -0.0447,  ..., -0.0031, -0.0046,  0.0187],
        [-0.0060,  0.0018,  0.0144,  ..., -0.0227, -0.0280,  0.0267],
        [-0.0092,  0.0152,  0.0020,  ..., -0.0138,  0.0049, -0.0446]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3496,  0.4424,  2.0898,  ...,  3.2812, -0.5137,  2.7051]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 15:55:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is operating, it has operated
After something is establishing, it has established
After something is teaching, it has taught
After something is allowing, it has allowed
After something is improving, it has improved
After something is requiring, it has required
After something is agreeing, it has agreed
After something is appointing, it has
2024-07-23 15:55:01 root INFO     [order_1_approx] starting weight calculation for After something is operating, it has operated
After something is agreeing, it has agreed
After something is improving, it has improved
After something is appointing, it has appointed
After something is allowing, it has allowed
After something is teaching, it has taught
After something is requiring, it has required
After something is establishing, it has
2024-07-23 15:55:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 15:58:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0316,  0.7114,  0.3652,  ...,  1.0322, -0.0063, -0.1077],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9165,  0.1030,  2.0840,  ...,  3.0352, -1.9082,  4.5391],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0302, -0.0284,  0.0116,  ..., -0.0017, -0.0035, -0.0176],
        [ 0.0034, -0.0094, -0.0074,  ...,  0.0024, -0.0121,  0.0022],
        [-0.0094,  0.0009, -0.0076,  ...,  0.0037,  0.0198,  0.0109],
        ...,
        [-0.0085, -0.0132, -0.0145,  ..., -0.0158,  0.0008,  0.0010],
        [-0.0099, -0.0004,  0.0055,  ..., -0.0192, -0.0325,  0.0261],
        [ 0.0034,  0.0143,  0.0134,  ..., -0.0087,  0.0092, -0.0164]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3608,  0.2969,  2.8418,  ...,  2.8203, -1.6318,  4.8750]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 15:58:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is operating, it has operated
After something is agreeing, it has agreed
After something is improving, it has improved
After something is appointing, it has appointed
After something is allowing, it has allowed
After something is teaching, it has taught
After something is requiring, it has required
After something is establishing, it has
2024-07-23 15:58:49 root INFO     [order_1_approx] starting weight calculation for After something is operating, it has operated
After something is establishing, it has established
After something is teaching, it has taught
After something is requiring, it has required
After something is agreeing, it has agreed
After something is allowing, it has allowed
After something is appointing, it has appointed
After something is improving, it has
2024-07-23 15:58:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 16:02:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8066,  0.0389,  0.6406,  ..., -0.3542,  0.9180, -0.1992],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.8984,  0.9746,  0.7705,  ...,  3.8398, -1.7637,  2.4375],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0087, -0.0149, -0.0040,  ..., -0.0087,  0.0039, -0.0086],
        [ 0.0080, -0.0092, -0.0041,  ..., -0.0003, -0.0176, -0.0013],
        [-0.0044, -0.0106, -0.0091,  ..., -0.0089,  0.0146,  0.0104],
        ...,
        [-0.0131, -0.0184,  0.0025,  ..., -0.0085, -0.0023,  0.0092],
        [-0.0013, -0.0158, -0.0010,  ..., -0.0224, -0.0436,  0.0012],
        [-0.0052,  0.0055,  0.0066,  ...,  0.0038, -0.0054, -0.0264]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.6719,  1.2793,  0.8091,  ...,  4.2227, -2.1035,  2.3672]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 16:02:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is operating, it has operated
After something is establishing, it has established
After something is teaching, it has taught
After something is requiring, it has required
After something is agreeing, it has agreed
After something is allowing, it has allowed
After something is appointing, it has appointed
After something is improving, it has
2024-07-23 16:02:38 root INFO     [order_1_approx] starting weight calculation for After something is teaching, it has taught
After something is requiring, it has required
After something is agreeing, it has agreed
After something is appointing, it has appointed
After something is improving, it has improved
After something is establishing, it has established
After something is allowing, it has allowed
After something is operating, it has
2024-07-23 16:02:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 16:06:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5864,  0.1321,  0.8696,  ...,  0.5054,  1.0020, -0.3774],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6519, -1.1680,  1.9727,  ...,  0.8242,  0.1919,  4.0664],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0246, -0.0090, -0.0044,  ...,  0.0127, -0.0013, -0.0095],
        [ 0.0020, -0.0107, -0.0068,  ...,  0.0050, -0.0093,  0.0188],
        [-0.0016,  0.0043, -0.0049,  ...,  0.0107,  0.0119,  0.0169],
        ...,
        [-0.0145, -0.0109, -0.0033,  ..., -0.0238, -0.0163,  0.0041],
        [ 0.0087,  0.0041, -0.0099,  ..., -0.0183, -0.0306,  0.0083],
        [ 0.0098,  0.0111, -0.0085,  ..., -0.0176,  0.0051, -0.0221]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5469, -1.2383,  1.7734,  ...,  0.8862, -0.3374,  2.9258]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 16:06:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is teaching, it has taught
After something is requiring, it has required
After something is agreeing, it has agreed
After something is appointing, it has appointed
After something is improving, it has improved
After something is establishing, it has established
After something is allowing, it has allowed
After something is operating, it has
2024-07-23 16:06:27 root INFO     [order_1_approx] starting weight calculation for After something is operating, it has operated
After something is teaching, it has taught
After something is agreeing, it has agreed
After something is establishing, it has established
After something is appointing, it has appointed
After something is improving, it has improved
After something is allowing, it has allowed
After something is requiring, it has
2024-07-23 16:06:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 16:10:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5889, -0.0123,  0.6138,  ...,  1.4326,  0.5938,  0.5117],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6855,  0.0801, -0.8398,  ...,  1.4014, -1.0312,  3.6523],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.5879e-02, -1.5182e-02, -4.6082e-03,  ..., -9.8114e-03,
         -7.8964e-03, -7.0648e-03],
        [-1.5812e-03, -1.2001e-02, -1.0483e-02,  ..., -1.5732e-02,
         -4.6692e-03, -1.9608e-03],
        [-1.0628e-02,  4.8141e-03,  3.4447e-03,  ...,  2.4376e-03,
         -5.5771e-03,  5.9357e-03],
        ...,
        [-1.5404e-02, -2.0279e-02, -1.8143e-02,  ..., -2.8961e-02,
          5.6686e-03,  2.5574e-02],
        [-1.6136e-03, -1.3550e-02,  1.4275e-02,  ..., -1.6617e-02,
         -1.6495e-02,  1.6510e-02],
        [ 1.0422e-02,  8.9073e-04, -2.2888e-05,  ..., -2.5730e-03,
          7.7744e-03, -3.3020e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3027,  0.3638,  0.0811,  ...,  1.2451, -1.3281,  2.8242]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 16:10:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is operating, it has operated
After something is teaching, it has taught
After something is agreeing, it has agreed
After something is establishing, it has established
After something is appointing, it has appointed
After something is improving, it has improved
After something is allowing, it has allowed
After something is requiring, it has
2024-07-23 16:10:16 root INFO     [order_1_approx] starting weight calculation for After something is appointing, it has appointed
After something is operating, it has operated
After something is improving, it has improved
After something is requiring, it has required
After something is allowing, it has allowed
After something is agreeing, it has agreed
After something is establishing, it has established
After something is teaching, it has
2024-07-23 16:10:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 16:14:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.1152,  0.8457,  1.5684,  ...,  0.4395,  0.4072, -0.1204],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5312,  0.7637,  0.4629,  ...,  3.0039,  0.6050,  2.1484],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0038, -0.0222, -0.0032,  ..., -0.0018, -0.0041, -0.0007],
        [-0.0021, -0.0086, -0.0093,  ...,  0.0099, -0.0159,  0.0062],
        [ 0.0010,  0.0135, -0.0048,  ..., -0.0060,  0.0207,  0.0109],
        ...,
        [-0.0132,  0.0013, -0.0199,  ..., -0.0113,  0.0020,  0.0043],
        [-0.0082, -0.0035,  0.0109,  ..., -0.0098, -0.0326,  0.0067],
        [-0.0057,  0.0164, -0.0047,  ..., -0.0160,  0.0062, -0.0356]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7686,  1.1152,  1.0293,  ...,  3.3926,  0.2236,  2.0215]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 16:14:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is appointing, it has appointed
After something is operating, it has operated
After something is improving, it has improved
After something is requiring, it has required
After something is allowing, it has allowed
After something is agreeing, it has agreed
After something is establishing, it has established
After something is teaching, it has
2024-07-23 16:14:06 root INFO     total operator prediction time: 1836.4590001106262 seconds
2024-07-23 16:14:06 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_inf - Ved
2024-07-23 16:14:06 root INFO     building operator verb_inf - Ved
2024-07-23 16:14:06 root INFO     [order_1_approx] starting weight calculation for If the present form is refer, the past form is referred
If the present form is receive, the past form is received
If the present form is follow, the past form is followed
If the present form is discover, the past form is discovered
If the present form is decide, the past form is decided
If the present form is consider, the past form is considered
If the present form is describe, the past form is described
If the present form is believe, the past form is
2024-07-23 16:14:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 16:17:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9473,  0.0393,  0.9722,  ...,  1.3145,  0.9248, -0.5137],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4785, -0.1416, -0.2695,  ...,  2.0684, -3.6309,  1.0693],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0123, -0.0267,  0.0089,  ..., -0.0157,  0.0060, -0.0119],
        [-0.0122, -0.0087,  0.0037,  ...,  0.0074,  0.0036, -0.0122],
        [-0.0072,  0.0099, -0.0150,  ..., -0.0019, -0.0004, -0.0063],
        ...,
        [-0.0059,  0.0030, -0.0011,  ..., -0.0207, -0.0063,  0.0005],
        [ 0.0050,  0.0068,  0.0063,  ..., -0.0260, -0.0324,  0.0079],
        [-0.0099,  0.0230, -0.0075,  ...,  0.0104,  0.0008, -0.0333]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5859, -0.0483,  0.0630,  ...,  2.4805, -3.3047,  1.6328]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 16:17:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is refer, the past form is referred
If the present form is receive, the past form is received
If the present form is follow, the past form is followed
If the present form is discover, the past form is discovered
If the present form is decide, the past form is decided
If the present form is consider, the past form is considered
If the present form is describe, the past form is described
If the present form is believe, the past form is
2024-07-23 16:17:53 root INFO     [order_1_approx] starting weight calculation for If the present form is decide, the past form is decided
If the present form is believe, the past form is believed
If the present form is refer, the past form is referred
If the present form is discover, the past form is discovered
If the present form is describe, the past form is described
If the present form is receive, the past form is received
If the present form is follow, the past form is followed
If the present form is consider, the past form is
2024-07-23 16:17:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 16:21:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7407,  1.0098,  1.3750,  ...,  0.7627,  0.8135, -0.3950],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.9121,  1.0381, -0.3896,  ...,  1.0254, -1.5996, -2.9336],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0162,  0.0043,  0.0091,  ...,  0.0028, -0.0034, -0.0099],
        [-0.0150, -0.0083,  0.0033,  ...,  0.0022,  0.0127, -0.0127],
        [-0.0068, -0.0062, -0.0069,  ..., -0.0051, -0.0053, -0.0062],
        ...,
        [-0.0066,  0.0022, -0.0076,  ..., -0.0142, -0.0033,  0.0072],
        [ 0.0079,  0.0034, -0.0010,  ..., -0.0133, -0.0144,  0.0062],
        [-0.0082,  0.0077,  0.0031,  ..., -0.0023,  0.0097, -0.0097]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5938,  1.2461, -0.3198,  ...,  1.2520, -1.0137, -2.7383]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 16:21:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is decide, the past form is decided
If the present form is believe, the past form is believed
If the present form is refer, the past form is referred
If the present form is discover, the past form is discovered
If the present form is describe, the past form is described
If the present form is receive, the past form is received
If the present form is follow, the past form is followed
If the present form is consider, the past form is
2024-07-23 16:21:40 root INFO     [order_1_approx] starting weight calculation for If the present form is refer, the past form is referred
If the present form is follow, the past form is followed
If the present form is consider, the past form is considered
If the present form is believe, the past form is believed
If the present form is describe, the past form is described
If the present form is discover, the past form is discovered
If the present form is receive, the past form is received
If the present form is decide, the past form is
2024-07-23 16:21:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 16:25:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1133,  0.1958,  1.2305,  ...,  0.9941, -0.1011, -0.2612],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6016,  0.9653,  0.6465,  ...,  0.6802,  0.7197, -0.3691],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.7588e-02,  2.8191e-03, -6.3591e-03,  ..., -7.8888e-03,
         -5.7564e-03, -1.6754e-02],
        [-1.9348e-02, -3.4981e-03, -7.2136e-03,  ...,  2.5482e-03,
          1.2062e-02, -1.3275e-02],
        [ 3.5362e-03,  1.0628e-02, -6.2141e-03,  ..., -8.9645e-04,
          9.1705e-03, -3.1033e-03],
        ...,
        [-1.4061e-02, -4.6616e-03,  2.6703e-05,  ..., -2.9160e-02,
          1.4221e-02,  3.5591e-03],
        [ 1.6464e-02,  4.4861e-03,  1.5717e-03,  ..., -2.1805e-02,
         -3.4302e-02,  1.1421e-02],
        [-1.0101e-02,  1.4534e-02,  7.3853e-03,  ...,  1.4397e-02,
          3.9825e-03, -2.3636e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3809,  1.9785,  1.1602,  ...,  0.4050,  0.4644, -0.8340]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 16:25:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is refer, the past form is referred
If the present form is follow, the past form is followed
If the present form is consider, the past form is considered
If the present form is believe, the past form is believed
If the present form is describe, the past form is described
If the present form is discover, the past form is discovered
If the present form is receive, the past form is received
If the present form is decide, the past form is
2024-07-23 16:25:27 root INFO     [order_1_approx] starting weight calculation for If the present form is consider, the past form is considered
If the present form is decide, the past form is decided
If the present form is believe, the past form is believed
If the present form is receive, the past form is received
If the present form is follow, the past form is followed
If the present form is discover, the past form is discovered
If the present form is refer, the past form is referred
If the present form is describe, the past form is
2024-07-23 16:25:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 16:29:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0879,  0.7480,  2.3965,  ...,  1.0400, -0.4165, -1.1729],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6816, -1.6562,  1.2646,  ...,  2.3594,  0.0371, -0.3203],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0176, -0.0174,  0.0047,  ..., -0.0079,  0.0009, -0.0062],
        [-0.0131, -0.0095,  0.0157,  ..., -0.0080, -0.0070, -0.0131],
        [-0.0087,  0.0028, -0.0206,  ...,  0.0050,  0.0084,  0.0035],
        ...,
        [-0.0302, -0.0109, -0.0071,  ..., -0.0173,  0.0041,  0.0005],
        [ 0.0048, -0.0116,  0.0003,  ..., -0.0114, -0.0265,  0.0126],
        [-0.0231,  0.0011,  0.0204,  ...,  0.0001,  0.0138, -0.0183]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8652, -1.8301,  1.7139,  ...,  2.6133, -0.2351, -0.3823]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 16:29:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is consider, the past form is considered
If the present form is decide, the past form is decided
If the present form is believe, the past form is believed
If the present form is receive, the past form is received
If the present form is follow, the past form is followed
If the present form is discover, the past form is discovered
If the present form is refer, the past form is referred
If the present form is describe, the past form is
2024-07-23 16:29:14 root INFO     [order_1_approx] starting weight calculation for If the present form is receive, the past form is received
If the present form is describe, the past form is described
If the present form is decide, the past form is decided
If the present form is consider, the past form is considered
If the present form is believe, the past form is believed
If the present form is follow, the past form is followed
If the present form is refer, the past form is referred
If the present form is discover, the past form is
2024-07-23 16:29:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 16:33:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3831,  0.3787,  0.5015,  ...,  1.6562,  0.4351, -0.1664],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6895,  0.8701, -0.6797,  ..., -2.0469, -4.5664,  0.5391],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0197, -0.0048, -0.0061,  ..., -0.0081, -0.0055, -0.0167],
        [-0.0173, -0.0095, -0.0018,  ...,  0.0011,  0.0150,  0.0007],
        [-0.0169, -0.0038, -0.0333,  ..., -0.0055,  0.0117, -0.0092],
        ...,
        [-0.0040,  0.0064, -0.0120,  ..., -0.0087,  0.0088,  0.0024],
        [ 0.0115,  0.0091,  0.0050,  ...,  0.0017, -0.0293,  0.0135],
        [-0.0183,  0.0058, -0.0019,  ..., -0.0121,  0.0163, -0.0293]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.6602,  1.0098, -0.5371,  ..., -1.7461, -4.3047,  0.7959]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 16:33:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is receive, the past form is received
If the present form is describe, the past form is described
If the present form is decide, the past form is decided
If the present form is consider, the past form is considered
If the present form is believe, the past form is believed
If the present form is follow, the past form is followed
If the present form is refer, the past form is referred
If the present form is discover, the past form is
2024-07-23 16:33:02 root INFO     [order_1_approx] starting weight calculation for If the present form is receive, the past form is received
If the present form is discover, the past form is discovered
If the present form is describe, the past form is described
If the present form is consider, the past form is considered
If the present form is believe, the past form is believed
If the present form is refer, the past form is referred
If the present form is decide, the past form is decided
If the present form is follow, the past form is
2024-07-23 16:33:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 16:36:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0645,  1.0752,  2.9629,  ...,  1.9883, -0.0674,  1.1602],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8105, -2.0723, -1.0830,  ..., -0.9409,  0.2822,  0.6787],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0188, -0.0134,  0.0057,  ...,  0.0011, -0.0074, -0.0142],
        [-0.0035, -0.0116,  0.0038,  ...,  0.0049,  0.0058, -0.0025],
        [-0.0075, -0.0011, -0.0003,  ..., -0.0009, -0.0092,  0.0039],
        ...,
        [-0.0033, -0.0066, -0.0006,  ..., -0.0012,  0.0041, -0.0011],
        [ 0.0004, -0.0068,  0.0064,  ..., -0.0027, -0.0166, -0.0056],
        [-0.0144,  0.0126,  0.0077,  ..., -0.0009,  0.0096, -0.0170]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5986, -2.1074, -0.3218,  ..., -0.9268,  0.0110,  0.6714]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 16:36:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is receive, the past form is received
If the present form is discover, the past form is discovered
If the present form is describe, the past form is described
If the present form is consider, the past form is considered
If the present form is believe, the past form is believed
If the present form is refer, the past form is referred
If the present form is decide, the past form is decided
If the present form is follow, the past form is
2024-07-23 16:36:50 root INFO     [order_1_approx] starting weight calculation for If the present form is refer, the past form is referred
If the present form is believe, the past form is believed
If the present form is decide, the past form is decided
If the present form is consider, the past form is considered
If the present form is follow, the past form is followed
If the present form is discover, the past form is discovered
If the present form is describe, the past form is described
If the present form is receive, the past form is
2024-07-23 16:36:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 16:40:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4172, -0.6807,  1.1562,  ...,  1.5518,  0.1079,  0.9072],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([2.6914, 0.7271, 0.1855,  ..., 2.6133, 1.5898, 2.6973], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0106, -0.0095, -0.0125,  ..., -0.0033,  0.0039, -0.0199],
        [-0.0049, -0.0035,  0.0076,  ..., -0.0055, -0.0019,  0.0018],
        [-0.0023,  0.0038, -0.0266,  ..., -0.0054,  0.0027,  0.0041],
        ...,
        [-0.0025,  0.0097, -0.0184,  ..., -0.0135,  0.0028,  0.0033],
        [ 0.0089,  0.0021,  0.0093,  ..., -0.0098, -0.0248,  0.0135],
        [-0.0121, -0.0010, -0.0079,  ..., -0.0110,  0.0040, -0.0275]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[2.4590, 0.9536, 0.5059,  ..., 2.4160, 0.8647, 2.9629]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 16:40:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is refer, the past form is referred
If the present form is believe, the past form is believed
If the present form is decide, the past form is decided
If the present form is consider, the past form is considered
If the present form is follow, the past form is followed
If the present form is discover, the past form is discovered
If the present form is describe, the past form is described
If the present form is receive, the past form is
2024-07-23 16:40:38 root INFO     [order_1_approx] starting weight calculation for If the present form is believe, the past form is believed
If the present form is follow, the past form is followed
If the present form is discover, the past form is discovered
If the present form is describe, the past form is described
If the present form is decide, the past form is decided
If the present form is consider, the past form is considered
If the present form is receive, the past form is received
If the present form is refer, the past form is
2024-07-23 16:40:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 16:44:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3516,  0.6357,  0.9053,  ...,  0.3682,  0.7725, -0.5776],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4941, -0.0580,  2.0898,  ...,  5.8320,  1.9355, -0.1416],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0256, -0.0210,  0.0107,  ..., -0.0016,  0.0009, -0.0014],
        [-0.0056, -0.0195,  0.0048,  ...,  0.0022,  0.0067, -0.0145],
        [ 0.0018, -0.0185, -0.0195,  ..., -0.0059,  0.0004, -0.0086],
        ...,
        [-0.0176, -0.0074, -0.0088,  ..., -0.0325,  0.0061, -0.0053],
        [-0.0030,  0.0095,  0.0136,  ..., -0.0140, -0.0317, -0.0005],
        [-0.0050,  0.0141,  0.0109,  ..., -0.0074,  0.0123, -0.0176]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[1.4258, 0.7104, 1.8242,  ..., 5.8594, 2.3555, 0.1833]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 16:44:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is believe, the past form is believed
If the present form is follow, the past form is followed
If the present form is discover, the past form is discovered
If the present form is describe, the past form is described
If the present form is decide, the past form is decided
If the present form is consider, the past form is considered
If the present form is receive, the past form is received
If the present form is refer, the past form is
2024-07-23 16:44:25 root INFO     total operator prediction time: 1819.0266330242157 seconds
2024-07-23 16:44:25 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_inf - 3pSg
2024-07-23 16:44:25 root INFO     building operator verb_inf - 3pSg
2024-07-23 16:44:25 root INFO     [order_1_approx] starting weight calculation for I create, he creates
I prevent, he prevents
I become, he becomes
I maintain, he maintains
I learn, he learns
I provide, he provides
I understand, he understands
I agree, he
2024-07-23 16:44:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 16:48:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3345, -0.8179,  1.1211,  ...,  0.6504, -0.1744,  0.1472],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6768, -1.1807,  3.6113,  ..., -1.2852, -3.2930,  0.2344],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0236, -0.0129,  0.0066,  ...,  0.0161,  0.0134, -0.0197],
        [ 0.0024, -0.0122, -0.0157,  ..., -0.0046, -0.0096, -0.0132],
        [-0.0048, -0.0034, -0.0256,  ...,  0.0095, -0.0063,  0.0057],
        ...,
        [-0.0076, -0.0168, -0.0098,  ..., -0.0120, -0.0041,  0.0005],
        [-0.0024,  0.0082,  0.0204,  ..., -0.0096, -0.0307,  0.0105],
        [-0.0063,  0.0191,  0.0044,  ..., -0.0068, -0.0004, -0.0279]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0122, -1.3828,  3.6816,  ..., -1.2295, -1.4570, -0.1506]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 16:48:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I create, he creates
I prevent, he prevents
I become, he becomes
I maintain, he maintains
I learn, he learns
I provide, he provides
I understand, he understands
I agree, he
2024-07-23 16:48:16 root INFO     [order_1_approx] starting weight calculation for I prevent, he prevents
I provide, he provides
I understand, he understands
I maintain, he maintains
I create, he creates
I learn, he learns
I agree, he agrees
I become, he
2024-07-23 16:48:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 16:52:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7314, -0.3081,  2.1426,  ...,  0.7876,  0.8813,  0.1838],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1738, -1.9551,  0.7012,  ...,  0.3320,  1.0820,  2.0195],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0411, -0.0270,  0.0154,  ..., -0.0049, -0.0030, -0.0268],
        [-0.0177, -0.0240, -0.0137,  ...,  0.0033,  0.0208, -0.0011],
        [ 0.0047, -0.0066, -0.0486,  ...,  0.0207,  0.0055, -0.0068],
        ...,
        [-0.0063, -0.0118, -0.0167,  ..., -0.0430, -0.0125,  0.0242],
        [-0.0117, -0.0092,  0.0192,  ..., -0.0207, -0.0141,  0.0185],
        [ 0.0041,  0.0078,  0.0016,  ..., -0.0042,  0.0051, -0.0336]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0352, -1.9092,  0.7993,  ...,  0.8120,  1.7412,  2.0430]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 16:52:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I prevent, he prevents
I provide, he provides
I understand, he understands
I maintain, he maintains
I create, he creates
I learn, he learns
I agree, he agrees
I become, he
2024-07-23 16:52:06 root INFO     [order_1_approx] starting weight calculation for I agree, he agrees
I understand, he understands
I learn, he learns
I maintain, he maintains
I provide, he provides
I prevent, he prevents
I become, he becomes
I create, he
2024-07-23 16:52:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 16:55:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4619, -1.1426,  1.0039,  ...,  0.7324,  0.3159, -0.1772],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1953, -3.8945, -0.2930,  ..., -0.4180, -1.1367,  0.2480],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0249, -0.0040,  0.0106,  ...,  0.0074, -0.0178, -0.0183],
        [-0.0186, -0.0160,  0.0045,  ..., -0.0101,  0.0112, -0.0023],
        [-0.0253, -0.0009, -0.0222,  ...,  0.0032,  0.0196, -0.0015],
        ...,
        [-0.0176, -0.0121, -0.0204,  ..., -0.0072, -0.0118, -0.0065],
        [-0.0089,  0.0082,  0.0170,  ..., -0.0176, -0.0205,  0.0185],
        [-0.0007,  0.0145,  0.0063,  ..., -0.0118,  0.0068, -0.0291]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0591, -4.4688, -0.5396,  ..., -1.0469, -0.9365, -0.4302]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 16:55:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I agree, he agrees
I understand, he understands
I learn, he learns
I maintain, he maintains
I provide, he provides
I prevent, he prevents
I become, he becomes
I create, he
2024-07-23 16:55:56 root INFO     [order_1_approx] starting weight calculation for I agree, he agrees
I provide, he provides
I create, he creates
I prevent, he prevents
I maintain, he maintains
I become, he becomes
I understand, he understands
I learn, he
2024-07-23 16:55:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 16:59:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8613, -0.5371,  2.4336,  ..., -0.6416,  1.5215,  0.6323],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.2832, -0.0674, -2.6875,  ..., -0.1724, -2.4844,  3.1230],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0366, -0.0051,  0.0097,  ...,  0.0048, -0.0066, -0.0236],
        [-0.0139, -0.0255, -0.0070,  ..., -0.0022, -0.0017, -0.0030],
        [-0.0032, -0.0004, -0.0281,  ...,  0.0080,  0.0127, -0.0003],
        ...,
        [-0.0051, -0.0091, -0.0172,  ..., -0.0294, -0.0145, -0.0011],
        [ 0.0043,  0.0002,  0.0184,  ..., -0.0111, -0.0370,  0.0039],
        [ 0.0087, -0.0005, -0.0127,  ..., -0.0103,  0.0213, -0.0371]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.2617, -0.7280, -3.4805,  ...,  0.2656, -1.2920,  2.5312]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 16:59:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I agree, he agrees
I provide, he provides
I create, he creates
I prevent, he prevents
I maintain, he maintains
I become, he becomes
I understand, he understands
I learn, he
2024-07-23 16:59:46 root INFO     [order_1_approx] starting weight calculation for I learn, he learns
I agree, he agrees
I create, he creates
I provide, he provides
I become, he becomes
I understand, he understands
I prevent, he prevents
I maintain, he
2024-07-23 16:59:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 17:03:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8018,  0.2246,  1.6074,  ...,  0.1848,  0.1562,  1.2480],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0225, -4.4883, -4.9414,  ...,  1.0596,  1.1201,  5.3398],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0038, -0.0076,  0.0079,  ...,  0.0041,  0.0116, -0.0096],
        [-0.0060,  0.0049,  0.0092,  ..., -0.0029,  0.0038,  0.0021],
        [ 0.0032, -0.0017, -0.0144,  ...,  0.0074,  0.0141, -0.0178],
        ...,
        [-0.0138, -0.0067, -0.0111,  ..., -0.0212, -0.0024, -0.0007],
        [-0.0027,  0.0088,  0.0044,  ..., -0.0115, -0.0264,  0.0225],
        [-0.0028,  0.0096, -0.0036,  ...,  0.0020,  0.0156, -0.0221]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4961, -4.3711, -5.4180,  ...,  0.9800,  2.1895,  5.1094]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 17:03:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I learn, he learns
I agree, he agrees
I create, he creates
I provide, he provides
I become, he becomes
I understand, he understands
I prevent, he prevents
I maintain, he
2024-07-23 17:03:33 root INFO     [order_1_approx] starting weight calculation for I understand, he understands
I agree, he agrees
I learn, he learns
I create, he creates
I maintain, he maintains
I become, he becomes
I provide, he provides
I prevent, he
2024-07-23 17:03:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 17:07:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2125,  0.0316,  0.9316,  ...,  0.5562,  0.6758, -0.0131],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.0977, -2.9922,  0.5161,  ...,  1.2383,  0.7842,  5.2227],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0005, -0.0104,  0.0211,  ...,  0.0046, -0.0112, -0.0327],
        [-0.0169, -0.0063,  0.0081,  ..., -0.0121,  0.0009,  0.0026],
        [-0.0280, -0.0244, -0.0178,  ..., -0.0068,  0.0180, -0.0010],
        ...,
        [-0.0207, -0.0209, -0.0255,  ..., -0.0051, -0.0070, -0.0097],
        [ 0.0158,  0.0155,  0.0069,  ..., -0.0199, -0.0165,  0.0033],
        [-0.0138,  0.0193,  0.0046,  ..., -0.0089,  0.0014, -0.0218]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.5996, -2.9512,  0.0967,  ...,  0.1602,  1.1816,  4.2109]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 17:07:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I understand, he understands
I agree, he agrees
I learn, he learns
I create, he creates
I maintain, he maintains
I become, he becomes
I provide, he provides
I prevent, he
2024-07-23 17:07:24 root INFO     [order_1_approx] starting weight calculation for I agree, he agrees
I prevent, he prevents
I create, he creates
I understand, he understands
I become, he becomes
I learn, he learns
I maintain, he maintains
I provide, he
2024-07-23 17:07:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 17:11:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1416, -0.3933,  0.1631,  ...,  0.1096,  0.5986,  0.6172],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8018, -1.4004, -0.6660,  ..., -0.8994, -3.3555,  2.4570],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0066, -0.0149,  0.0030,  ..., -0.0184, -0.0101, -0.0230],
        [-0.0023, -0.0039,  0.0043,  ..., -0.0149,  0.0016,  0.0130],
        [-0.0091, -0.0117, -0.0273,  ..., -0.0011,  0.0032,  0.0069],
        ...,
        [-0.0144, -0.0027, -0.0145,  ..., -0.0076, -0.0152, -0.0006],
        [-0.0101,  0.0008,  0.0242,  ..., -0.0191, -0.0083,  0.0176],
        [-0.0010,  0.0198, -0.0010,  ..., -0.0028,  0.0272, -0.0226]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2559, -1.0127, -0.0664,  ..., -1.0791, -2.7480,  1.7510]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 17:11:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I agree, he agrees
I prevent, he prevents
I create, he creates
I understand, he understands
I become, he becomes
I learn, he learns
I maintain, he maintains
I provide, he
2024-07-23 17:11:13 root INFO     [order_1_approx] starting weight calculation for I provide, he provides
I learn, he learns
I agree, he agrees
I prevent, he prevents
I become, he becomes
I create, he creates
I maintain, he maintains
I understand, he
2024-07-23 17:11:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 17:15:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8091, -1.4990,  0.9048,  ..., -0.4563,  1.5303,  0.3857],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.5664, -2.2930,  0.9648,  ...,  0.1730, -1.2451,  3.2227],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.9745e-02, -4.5547e-03,  1.0696e-02,  ..., -3.5362e-03,
         -8.8730e-03, -1.7487e-02],
        [-2.1545e-02, -1.9608e-02,  1.5884e-02,  ...,  3.1853e-04,
          4.0741e-03, -9.9182e-03],
        [-2.0294e-02,  1.2388e-03, -2.7298e-02,  ...,  8.2169e-03,
          7.5302e-03,  5.0583e-03],
        ...,
        [-1.1272e-03, -1.4639e-03,  1.8597e-03,  ..., -3.3550e-03,
         -1.4442e-02, -1.0071e-02],
        [ 5.2643e-03, -2.8992e-04,  1.8539e-02,  ..., -5.8899e-03,
         -2.3346e-02, -7.6294e-05],
        [ 1.4603e-02,  9.1248e-03, -1.5320e-02,  ..., -1.4954e-03,
          2.4689e-02, -2.8503e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.8809, -2.7832,  1.7900,  ...,  0.3921, -0.5332,  2.9648]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 17:15:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I provide, he provides
I learn, he learns
I agree, he agrees
I prevent, he prevents
I become, he becomes
I create, he creates
I maintain, he maintains
I understand, he
2024-07-23 17:15:02 root INFO     total operator prediction time: 1836.648987531662 seconds
2024-07-23 17:15:02 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_Ving - 3pSg
2024-07-23 17:15:02 root INFO     building operator verb_Ving - 3pSg
2024-07-23 17:15:02 root INFO     [order_1_approx] starting weight calculation for When something is improving, it improves
When something is believing, it believes
When something is asking, it asks
When something is describing, it describes
When something is promoting, it promotes
When something is enabling, it enables
When something is telling, it tells
When something is applying, it
2024-07-23 17:15:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 17:18:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0068, -0.5146,  0.9385,  ...,  0.1235,  0.7480,  0.3665],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.3506, -0.5073,  1.5176,  ...,  4.6445, -2.6836,  3.6719],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0162, -0.0142,  0.0060,  ...,  0.0066,  0.0103, -0.0132],
        [-0.0044, -0.0053, -0.0021,  ..., -0.0109, -0.0061,  0.0003],
        [-0.0065,  0.0147, -0.0121,  ...,  0.0117, -0.0068,  0.0138],
        ...,
        [-0.0168, -0.0142, -0.0105,  ..., -0.0162,  0.0033, -0.0018],
        [ 0.0092,  0.0001,  0.0119,  ..., -0.0173, -0.0079,  0.0028],
        [-0.0108,  0.0038,  0.0094,  ..., -0.0084,  0.0147, -0.0106]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.8457,  0.0552,  0.8208,  ...,  4.8867, -2.1113,  3.3711]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 17:18:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is improving, it improves
When something is believing, it believes
When something is asking, it asks
When something is describing, it describes
When something is promoting, it promotes
When something is enabling, it enables
When something is telling, it tells
When something is applying, it
2024-07-23 17:18:50 root INFO     [order_1_approx] starting weight calculation for When something is believing, it believes
When something is promoting, it promotes
When something is enabling, it enables
When something is improving, it improves
When something is telling, it tells
When something is describing, it describes
When something is applying, it applies
When something is asking, it
2024-07-23 17:18:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 17:22:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3350, -0.2378,  0.3992,  ..., -0.5508,  1.5000,  0.4448],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1040,  0.3428,  2.2871,  ...,  1.7119, -0.1445,  3.1738],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0195, -0.0176,  0.0003,  ...,  0.0079, -0.0224,  0.0062],
        [-0.0105, -0.0167,  0.0000,  ..., -0.0129, -0.0028, -0.0086],
        [ 0.0045,  0.0159, -0.0199,  ...,  0.0107,  0.0043,  0.0156],
        ...,
        [-0.0136, -0.0040, -0.0049,  ..., -0.0141,  0.0036,  0.0100],
        [ 0.0089,  0.0114,  0.0076,  ..., -0.0127, -0.0316,  0.0065],
        [-0.0058,  0.0088, -0.0109,  ...,  0.0006,  0.0107, -0.0169]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5400,  0.8286,  2.1836,  ...,  2.0801, -0.1108,  2.7754]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 17:22:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is believing, it believes
When something is promoting, it promotes
When something is enabling, it enables
When something is improving, it improves
When something is telling, it tells
When something is describing, it describes
When something is applying, it applies
When something is asking, it
2024-07-23 17:22:37 root INFO     [order_1_approx] starting weight calculation for When something is promoting, it promotes
When something is enabling, it enables
When something is improving, it improves
When something is telling, it tells
When something is asking, it asks
When something is describing, it describes
When something is applying, it applies
When something is believing, it
2024-07-23 17:22:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 17:26:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.4204, 0.1538, 0.6504,  ..., 1.2461, 1.2314, 0.1140], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.5566,  0.9551,  0.7920,  ...,  1.4141, -2.4238,  3.4375],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.9053e-02, -1.5640e-02,  1.3931e-02,  ...,  1.9409e-02,
         -2.3041e-03, -1.4938e-02],
        [ 1.0010e-02, -1.1902e-02,  1.3092e-02,  ...,  2.1210e-03,
         -3.6602e-03,  9.8801e-04],
        [ 3.5553e-03,  1.7899e-02, -2.6932e-02,  ...,  2.6340e-03,
          7.8506e-03,  6.2904e-03],
        ...,
        [-1.8860e-02, -2.1729e-02, -8.7585e-03,  ..., -2.5253e-02,
          1.7796e-03, -2.6703e-05],
        [-2.4891e-04, -8.0109e-04,  5.4398e-03,  ..., -6.9771e-03,
         -2.9190e-02,  1.3580e-03],
        [-6.3477e-03,  1.7883e-02,  5.3253e-03,  ...,  8.9264e-03,
          6.6147e-03, -4.3182e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9180,  1.8145,  0.9888,  ...,  1.7910, -2.1855,  2.7031]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 17:26:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is promoting, it promotes
When something is enabling, it enables
When something is improving, it improves
When something is telling, it tells
When something is asking, it asks
When something is describing, it describes
When something is applying, it applies
When something is believing, it
2024-07-23 17:26:27 root INFO     [order_1_approx] starting weight calculation for When something is applying, it applies
When something is asking, it asks
When something is enabling, it enables
When something is telling, it tells
When something is improving, it improves
When something is promoting, it promotes
When something is believing, it believes
When something is describing, it
2024-07-23 17:26:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 17:30:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3176, -0.0981,  1.1309,  ..., -0.2522, -0.0513, -1.6758],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7651, -1.7402,  1.9170,  ...,  1.6523, -1.9141,  1.3691],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0089, -0.0253,  0.0114,  ...,  0.0092, -0.0025,  0.0027],
        [-0.0046, -0.0078, -0.0043,  ..., -0.0181, -0.0127, -0.0082],
        [-0.0049,  0.0090, -0.0139,  ...,  0.0051,  0.0066, -0.0061],
        ...,
        [-0.0352, -0.0381,  0.0010,  ..., -0.0160, -0.0058, -0.0156],
        [ 0.0063,  0.0233,  0.0273,  ..., -0.0277, -0.0187,  0.0159],
        [ 0.0017,  0.0187,  0.0193,  ..., -0.0181,  0.0113, -0.0343]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.4258, -2.1992,  0.9668,  ...,  1.5508, -1.5723,  0.4424]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 17:30:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is applying, it applies
When something is asking, it asks
When something is enabling, it enables
When something is telling, it tells
When something is improving, it improves
When something is promoting, it promotes
When something is believing, it believes
When something is describing, it
2024-07-23 17:30:17 root INFO     [order_1_approx] starting weight calculation for When something is promoting, it promotes
When something is improving, it improves
When something is asking, it asks
When something is telling, it tells
When something is believing, it believes
When something is describing, it describes
When something is applying, it applies
When something is enabling, it
2024-07-23 17:30:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 17:34:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3467,  0.4209,  1.0342,  ...,  1.1250,  0.6689, -0.9238],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2754, -0.0964,  3.1504,  ...,  1.3418, -2.5000,  4.6445],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0082, -0.0111, -0.0018,  ...,  0.0122,  0.0128, -0.0149],
        [ 0.0044, -0.0090, -0.0173,  ..., -0.0201, -0.0089, -0.0021],
        [-0.0018,  0.0036, -0.0040,  ..., -0.0021,  0.0084,  0.0059],
        ...,
        [-0.0145, -0.0112, -0.0213,  ..., -0.0156, -0.0059, -0.0097],
        [ 0.0071, -0.0145,  0.0066,  ..., -0.0191, -0.0069, -0.0044],
        [ 0.0001,  0.0094, -0.0089,  ..., -0.0076,  0.0008,  0.0050]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5547,  0.1501,  3.1797,  ...,  1.2129, -2.1094,  4.0586]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 17:34:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is promoting, it promotes
When something is improving, it improves
When something is asking, it asks
When something is telling, it tells
When something is believing, it believes
When something is describing, it describes
When something is applying, it applies
When something is enabling, it
2024-07-23 17:34:05 root INFO     [order_1_approx] starting weight calculation for When something is asking, it asks
When something is describing, it describes
When something is enabling, it enables
When something is promoting, it promotes
When something is telling, it tells
When something is applying, it applies
When something is believing, it believes
When something is improving, it
2024-07-23 17:34:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 17:37:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1394,  0.0591,  0.1489,  ...,  0.3179,  1.2607, -0.4963],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 8.2891,  0.2798,  2.2344,  ...,  3.3652, -0.9761,  2.0195],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.4374e-02, -1.0010e-02, -6.8665e-03,  ..., -1.2215e-02,
         -5.1918e-03, -2.0538e-02],
        [-5.7907e-03, -1.3435e-02, -1.6174e-02,  ..., -2.2583e-02,
         -1.5137e-02, -1.2192e-02],
        [-6.6910e-03, -3.2158e-03, -8.9569e-03,  ...,  7.5989e-03,
          2.7847e-03,  9.1171e-03],
        ...,
        [ 4.5204e-03,  1.1368e-03, -4.9667e-03,  ..., -2.2491e-02,
          1.6222e-03, -1.0475e-02],
        [-1.5282e-02, -5.2223e-03, -5.8098e-03,  ..., -1.7761e-02,
         -2.6901e-02, -4.0436e-04],
        [-5.6992e-03,  1.3565e-02, -1.7166e-05,  ...,  1.0080e-03,
         -2.7809e-03, -1.8143e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 8.7344,  0.5449,  2.2852,  ...,  3.5234, -1.0635,  1.7637]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 17:37:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is asking, it asks
When something is describing, it describes
When something is enabling, it enables
When something is promoting, it promotes
When something is telling, it tells
When something is applying, it applies
When something is believing, it believes
When something is improving, it
2024-07-23 17:37:55 root INFO     [order_1_approx] starting weight calculation for When something is telling, it tells
When something is believing, it believes
When something is asking, it asks
When something is improving, it improves
When something is applying, it applies
When something is enabling, it enables
When something is describing, it describes
When something is promoting, it
2024-07-23 17:37:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 17:41:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2939, -0.7656,  0.1133,  ...,  0.3091,  0.2871, -0.5225],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9751, -1.7148,  2.4668,  ...,  3.3047, -2.3262,  1.5117],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.7384e-03, -3.1143e-02,  4.4022e-03,  ..., -7.4196e-03,
         -9.7122e-03, -1.9058e-02],
        [-4.5776e-04, -1.1444e-05, -1.0231e-02,  ...,  1.0223e-03,
          1.6785e-04, -7.8583e-03],
        [ 4.2648e-03,  9.3384e-03, -3.8757e-03,  ...,  8.6517e-03,
          1.2032e-02,  1.6754e-02],
        ...,
        [-1.3260e-02, -1.8494e-02, -1.0826e-02,  ..., -1.7807e-02,
         -2.0920e-02,  1.2245e-03],
        [-6.3820e-03, -1.0071e-03,  1.9470e-02,  ..., -2.3071e-02,
         -2.5665e-02,  1.5182e-03],
        [-2.9984e-03,  3.5934e-03, -3.3379e-03,  ...,  7.4120e-03,
          1.4534e-02, -1.2337e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7646, -1.4619,  3.0664,  ...,  3.1602, -3.2617,  1.1875]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 17:41:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is telling, it tells
When something is believing, it believes
When something is asking, it asks
When something is improving, it improves
When something is applying, it applies
When something is enabling, it enables
When something is describing, it describes
When something is promoting, it
2024-07-23 17:41:44 root INFO     [order_1_approx] starting weight calculation for When something is improving, it improves
When something is applying, it applies
When something is promoting, it promotes
When something is enabling, it enables
When something is believing, it believes
When something is asking, it asks
When something is describing, it describes
When something is telling, it
2024-07-23 17:41:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 17:45:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7383, -0.0718,  0.9043,  ...,  1.1357,  0.1777, -0.3584],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9966, -0.4170,  2.1094,  ...,  2.3535,  0.7627,  1.6611],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0162, -0.0161,  0.0124,  ...,  0.0055, -0.0091, -0.0145],
        [-0.0063, -0.0016, -0.0052,  ..., -0.0150,  0.0167,  0.0136],
        [-0.0146,  0.0277,  0.0027,  ...,  0.0034,  0.0089,  0.0163],
        ...,
        [-0.0380, -0.0137, -0.0172,  ..., -0.0341,  0.0018, -0.0029],
        [-0.0083, -0.0172,  0.0065,  ..., -0.0130, -0.0556,  0.0007],
        [-0.0031,  0.0363,  0.0201,  ..., -0.0041,  0.0185, -0.0217]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5947, -0.2478,  1.7871,  ...,  2.7363,  0.6665,  0.9507]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 17:45:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is improving, it improves
When something is applying, it applies
When something is promoting, it promotes
When something is enabling, it enables
When something is believing, it believes
When something is asking, it asks
When something is describing, it describes
When something is telling, it
2024-07-23 17:45:32 root INFO     total operator prediction time: 1830.673404932022 seconds
2024-07-23 17:45:32 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on noun - plural_reg
2024-07-23 17:45:32 root INFO     building operator noun - plural_reg
2024-07-23 17:45:33 root INFO     [order_1_approx] starting weight calculation for The plural form of solution is solutions
The plural form of population is populations
The plural form of village is villages
The plural form of god is gods
The plural form of thing is things
The plural form of customer is customers
The plural form of river is rivers
The plural form of council is
2024-07-23 17:45:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 17:49:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9312,  0.1887, -0.8042,  ...,  0.8076,  0.9541,  0.3613],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.0703,  2.9551, -1.2305,  ..., -4.0078, -2.5430,  1.1436],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-5.9700e-03, -6.1836e-03,  1.0162e-02,  ..., -7.0572e-03,
         -1.4038e-03, -1.2260e-02],
        [-6.1035e-05, -8.4152e-03, -5.7869e-03,  ...,  1.9531e-03,
         -6.2180e-03,  1.5793e-02],
        [ 6.3248e-03, -1.2230e-02,  4.7646e-03,  ...,  1.2703e-03,
          9.5367e-03, -3.8376e-03],
        ...,
        [ 1.1539e-03, -1.7929e-03, -6.0272e-04,  ..., -3.6507e-03,
          7.2708e-03, -3.1624e-03],
        [-6.5155e-03, -1.2909e-02,  8.7738e-03,  ..., -1.0361e-02,
         -2.3224e-02, -1.4572e-03],
        [ 5.3978e-03,  2.1881e-02,  8.6517e-03,  ..., -4.2191e-03,
          1.9928e-02, -2.6581e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.6797,  2.9043, -0.5562,  ..., -4.1992, -2.3477,  1.8242]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 17:49:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of solution is solutions
The plural form of population is populations
The plural form of village is villages
The plural form of god is gods
The plural form of thing is things
The plural form of customer is customers
The plural form of river is rivers
The plural form of council is
2024-07-23 17:49:22 root INFO     [order_1_approx] starting weight calculation for The plural form of village is villages
The plural form of population is populations
The plural form of council is councils
The plural form of river is rivers
The plural form of solution is solutions
The plural form of god is gods
The plural form of thing is things
The plural form of customer is
2024-07-23 17:49:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 17:53:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8477,  0.1495, -0.6978,  ...,  1.8965, -0.8105,  1.1240],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.0957,  2.1816,  1.2695,  ..., -4.5234, -2.5879, -0.2031],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0066, -0.0140,  0.0191,  ...,  0.0032,  0.0053, -0.0065],
        [-0.0020, -0.0148, -0.0022,  ...,  0.0080, -0.0011, -0.0051],
        [ 0.0161, -0.0070, -0.0085,  ..., -0.0072,  0.0231,  0.0167],
        ...,
        [-0.0090, -0.0041,  0.0046,  ...,  0.0008, -0.0043, -0.0153],
        [-0.0024,  0.0023,  0.0203,  ..., -0.0173, -0.0292, -0.0165],
        [-0.0317,  0.0059, -0.0100,  ..., -0.0117,  0.0269, -0.0089]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.7891,  2.5664,  1.3076,  ..., -5.5586, -2.3516, -0.5308]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 17:53:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of village is villages
The plural form of population is populations
The plural form of council is councils
The plural form of river is rivers
The plural form of solution is solutions
The plural form of god is gods
The plural form of thing is things
The plural form of customer is
2024-07-23 17:53:08 root INFO     [order_1_approx] starting weight calculation for The plural form of solution is solutions
The plural form of river is rivers
The plural form of village is villages
The plural form of thing is things
The plural form of population is populations
The plural form of council is councils
The plural form of customer is customers
The plural form of god is
2024-07-23 17:53:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 17:56:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.6064, -0.3691,  0.3228,  ..., -0.2446, -0.2651, -0.4656],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.5840, -0.6758,  0.4980,  ...,  0.7363, -0.7891,  1.2783],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0068, -0.0087,  0.0087,  ...,  0.0039,  0.0032, -0.0056],
        [ 0.0066, -0.0195, -0.0044,  ...,  0.0105,  0.0021,  0.0064],
        [ 0.0166, -0.0086, -0.0197,  ..., -0.0045, -0.0073, -0.0097],
        ...,
        [-0.0039, -0.0008, -0.0076,  ..., -0.0286,  0.0169, -0.0066],
        [-0.0124,  0.0014,  0.0101,  ..., -0.0237, -0.0248, -0.0197],
        [-0.0171,  0.0088, -0.0093,  ..., -0.0002,  0.0054, -0.0072]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.5898, -0.6152,  0.7822,  ...,  0.1260, -0.7026,  1.6035]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 17:56:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of solution is solutions
The plural form of river is rivers
The plural form of village is villages
The plural form of thing is things
The plural form of population is populations
The plural form of council is councils
The plural form of customer is customers
The plural form of god is
2024-07-23 17:56:56 root INFO     [order_1_approx] starting weight calculation for The plural form of thing is things
The plural form of god is gods
The plural form of village is villages
The plural form of council is councils
The plural form of customer is customers
The plural form of solution is solutions
The plural form of river is rivers
The plural form of population is
2024-07-23 17:56:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 18:00:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9619,  0.3755, -0.1599,  ...,  0.1487,  0.3821,  0.2008],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.8184, -2.2754,  1.6270,  ...,  0.0669, -1.1504,  4.0508],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0212, -0.0215,  0.0232,  ..., -0.0024, -0.0075, -0.0180],
        [ 0.0026, -0.0040, -0.0030,  ...,  0.0135,  0.0015, -0.0005],
        [ 0.0123, -0.0181, -0.0114,  ..., -0.0095,  0.0227, -0.0059],
        ...,
        [-0.0236, -0.0020, -0.0127,  ..., -0.0127,  0.0010, -0.0354],
        [-0.0100, -0.0034,  0.0149,  ..., -0.0074, -0.0199, -0.0105],
        [-0.0240,  0.0076, -0.0048,  ..., -0.0148,  0.0178, -0.0128]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.0820, -2.4609,  1.5615,  ..., -0.1774, -1.0977,  5.1406]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 18:00:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of thing is things
The plural form of god is gods
The plural form of village is villages
The plural form of council is councils
The plural form of customer is customers
The plural form of solution is solutions
The plural form of river is rivers
The plural form of population is
2024-07-23 18:00:43 root INFO     [order_1_approx] starting weight calculation for The plural form of thing is things
The plural form of village is villages
The plural form of solution is solutions
The plural form of council is councils
The plural form of customer is customers
The plural form of population is populations
The plural form of god is gods
The plural form of river is
2024-07-23 18:00:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 18:04:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2793, -0.4668,  0.0208,  ...,  0.6758, -0.2837,  1.1553],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3472,  2.4219, -0.5244,  ...,  4.2383, -2.5312,  1.3965],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0100, -0.0072,  0.0111,  ..., -0.0061, -0.0084, -0.0028],
        [ 0.0039, -0.0054,  0.0019,  ...,  0.0126, -0.0072,  0.0126],
        [-0.0057,  0.0019, -0.0050,  ..., -0.0047,  0.0129, -0.0035],
        ...,
        [ 0.0032, -0.0042,  0.0025,  ...,  0.0021, -0.0038,  0.0002],
        [ 0.0007, -0.0108,  0.0160,  ...,  0.0073, -0.0132,  0.0069],
        [-0.0071,  0.0149, -0.0073,  ..., -0.0151,  0.0061, -0.0020]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1519,  2.0781, -0.2024,  ...,  3.5586, -2.4492,  2.5293]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 18:04:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of thing is things
The plural form of village is villages
The plural form of solution is solutions
The plural form of council is councils
The plural form of customer is customers
The plural form of population is populations
The plural form of god is gods
The plural form of river is
2024-07-23 18:04:30 root INFO     [order_1_approx] starting weight calculation for The plural form of council is councils
The plural form of village is villages
The plural form of thing is things
The plural form of customer is customers
The plural form of god is gods
The plural form of population is populations
The plural form of river is rivers
The plural form of solution is
2024-07-23 18:04:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 18:08:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([1.7598, 0.9980, 0.3403,  ..., 0.0781, 0.2529, 0.7520], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.2734,  2.2871,  1.8389,  ...,  1.5361, -2.9922,  1.7129],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0147,  0.0050,  0.0049,  ..., -0.0058, -0.0137, -0.0183],
        [ 0.0140, -0.0199, -0.0068,  ...,  0.0041, -0.0106, -0.0105],
        [-0.0289, -0.0129, -0.0165,  ..., -0.0010,  0.0079,  0.0003],
        ...,
        [-0.0065,  0.0022,  0.0059,  ..., -0.0109, -0.0039, -0.0109],
        [-0.0105,  0.0021,  0.0084,  ..., -0.0057, -0.0334, -0.0050],
        [-0.0158,  0.0199, -0.0097,  ..., -0.0083,  0.0111, -0.0132]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.2715,  2.8613,  2.1875,  ...,  0.4424, -3.0684,  2.1426]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 18:08:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of council is councils
The plural form of village is villages
The plural form of thing is things
The plural form of customer is customers
The plural form of god is gods
The plural form of population is populations
The plural form of river is rivers
The plural form of solution is
2024-07-23 18:08:19 root INFO     [order_1_approx] starting weight calculation for The plural form of council is councils
The plural form of village is villages
The plural form of population is populations
The plural form of god is gods
The plural form of solution is solutions
The plural form of customer is customers
The plural form of river is rivers
The plural form of thing is
2024-07-23 18:08:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 18:12:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8330,  0.4707, -0.0520,  ..., -0.1636, -0.2676,  0.1582],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1904,  0.5796, -0.5625,  ...,  0.3906,  0.1106,  0.3008],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0173, -0.0185,  0.0194,  ..., -0.0029,  0.0032, -0.0024],
        [-0.0049, -0.0256, -0.0085,  ...,  0.0027,  0.0014, -0.0081],
        [ 0.0057,  0.0095, -0.0302,  ..., -0.0043,  0.0106,  0.0109],
        ...,
        [ 0.0114, -0.0004,  0.0053,  ..., -0.0324,  0.0069, -0.0029],
        [-0.0079, -0.0039,  0.0023,  ..., -0.0139, -0.0307, -0.0006],
        [-0.0055,  0.0193,  0.0042,  ..., -0.0127,  0.0041, -0.0091]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.1729,  0.4834, -0.5508,  ...,  0.6421, -0.4954,  0.7959]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 18:12:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of council is councils
The plural form of village is villages
The plural form of population is populations
The plural form of god is gods
The plural form of solution is solutions
The plural form of customer is customers
The plural form of river is rivers
The plural form of thing is
2024-07-23 18:12:07 root INFO     [order_1_approx] starting weight calculation for The plural form of population is populations
The plural form of customer is customers
The plural form of river is rivers
The plural form of council is councils
The plural form of thing is things
The plural form of solution is solutions
The plural form of god is gods
The plural form of village is
2024-07-23 18:12:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 18:15:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2480,  0.0000,  0.0728,  ...,  0.2593, -0.1287,  0.5254],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.5039,  1.2578, -1.6045,  ..., -1.8896,  2.2656, -0.5410],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0279, -0.0140,  0.0162,  ...,  0.0084,  0.0073, -0.0066],
        [-0.0090, -0.0082, -0.0062,  ...,  0.0113, -0.0152,  0.0077],
        [ 0.0222, -0.0115, -0.0077,  ..., -0.0118,  0.0228, -0.0097],
        ...,
        [-0.0035, -0.0219, -0.0162,  ..., -0.0183,  0.0007, -0.0045],
        [-0.0073, -0.0068,  0.0037,  ..., -0.0068, -0.0177, -0.0111],
        [-0.0140,  0.0088,  0.0007,  ..., -0.0199,  0.0081, -0.0083]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.9590,  1.4902, -0.9849,  ..., -2.4727,  2.5117, -0.2090]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 18:15:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of population is populations
The plural form of customer is customers
The plural form of river is rivers
The plural form of council is councils
The plural form of thing is things
The plural form of solution is solutions
The plural form of god is gods
The plural form of village is
2024-07-23 18:15:55 root INFO     total operator prediction time: 1822.5873351097107 seconds
2024-07-23 18:15:55 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_3pSg - Ved
2024-07-23 18:15:55 root INFO     building operator verb_3pSg - Ved
2024-07-23 18:15:55 root INFO     [order_1_approx] starting weight calculation for When he contains something, something has been contained
When he fails something, something has been failed
When he introduces something, something has been introduced
When he requires something, something has been required
When he follows something, something has been followed
When he intends something, something has been intended
When he believes something, something has been believed
When he agrees something, something has been
2024-07-23 18:15:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 18:19:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9326,  0.4434,  1.0498,  ...,  1.3525, -0.7769,  0.2939],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.0430,  4.2539,  4.2852,  ..., -1.0605, -4.3945, -2.1602],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0003,  0.0030, -0.0101,  ..., -0.0082,  0.0141, -0.0208],
        [-0.0143,  0.0021, -0.0075,  ...,  0.0006,  0.0042, -0.0074],
        [-0.0053,  0.0035, -0.0185,  ..., -0.0090, -0.0002,  0.0020],
        ...,
        [-0.0124, -0.0074,  0.0006,  ..., -0.0037, -0.0056, -0.0040],
        [-0.0022,  0.0002,  0.0010,  ..., -0.0114, -0.0275, -0.0036],
        [ 0.0120,  0.0099, -0.0045,  ...,  0.0104,  0.0195, -0.0317]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.5273,  4.7383,  4.9297,  ..., -0.8882, -3.9531, -2.3848]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 18:19:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he contains something, something has been contained
When he fails something, something has been failed
When he introduces something, something has been introduced
When he requires something, something has been required
When he follows something, something has been followed
When he intends something, something has been intended
When he believes something, something has been believed
When he agrees something, something has been
2024-07-23 18:19:44 root INFO     [order_1_approx] starting weight calculation for When he contains something, something has been contained
When he intends something, something has been intended
When he introduces something, something has been introduced
When he fails something, something has been failed
When he requires something, something has been required
When he follows something, something has been followed
When he agrees something, something has been agreed
When he believes something, something has been
2024-07-23 18:19:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 18:23:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2197,  0.5928,  0.3567,  ...,  0.5215,  1.1660, -0.4175],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.1465,  1.1016,  1.2012,  ...,  0.3799, -5.6602, -0.1289],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0071, -0.0224,  0.0035,  ..., -0.0047,  0.0048, -0.0134],
        [-0.0084, -0.0089, -0.0026,  ..., -0.0015, -0.0024, -0.0021],
        [-0.0041,  0.0113, -0.0139,  ...,  0.0090,  0.0061,  0.0155],
        ...,
        [-0.0194, -0.0001,  0.0067,  ..., -0.0034, -0.0002, -0.0069],
        [-0.0119,  0.0035,  0.0014,  ..., -0.0077, -0.0217, -0.0074],
        [-0.0035,  0.0231, -0.0144,  ...,  0.0051,  0.0197, -0.0347]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.4199,  1.2559,  1.2539,  ...,  0.6094, -5.5156, -0.1375]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 18:23:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he contains something, something has been contained
When he intends something, something has been intended
When he introduces something, something has been introduced
When he fails something, something has been failed
When he requires something, something has been required
When he follows something, something has been followed
When he agrees something, something has been agreed
When he believes something, something has been
2024-07-23 18:23:34 root INFO     [order_1_approx] starting weight calculation for When he intends something, something has been intended
When he introduces something, something has been introduced
When he follows something, something has been followed
When he fails something, something has been failed
When he believes something, something has been believed
When he requires something, something has been required
When he agrees something, something has been agreed
When he contains something, something has been
2024-07-23 18:23:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 18:27:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9580,  0.6655,  0.9233,  ...,  0.1243, -0.1278,  0.7056],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5781,  3.0039,  0.2695,  ..., -1.8545, -1.9736,  0.3743],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0148,  0.0010, -0.0013,  ...,  0.0079,  0.0032, -0.0241],
        [-0.0118, -0.0055,  0.0072,  ..., -0.0126, -0.0061,  0.0021],
        [-0.0086,  0.0008, -0.0015,  ..., -0.0092, -0.0015,  0.0070],
        ...,
        [-0.0101,  0.0038, -0.0074,  ..., -0.0027,  0.0027, -0.0007],
        [-0.0066,  0.0158,  0.0103,  ..., -0.0041,  0.0055,  0.0022],
        [-0.0053, -0.0028, -0.0122,  ...,  0.0062, -0.0037, -0.0069]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6885,  3.5391,  0.5801,  ..., -2.0625, -2.3340,  0.5869]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 18:27:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he intends something, something has been intended
When he introduces something, something has been introduced
When he follows something, something has been followed
When he fails something, something has been failed
When he believes something, something has been believed
When he requires something, something has been required
When he agrees something, something has been agreed
When he contains something, something has been
2024-07-23 18:27:23 root INFO     [order_1_approx] starting weight calculation for When he agrees something, something has been agreed
When he introduces something, something has been introduced
When he follows something, something has been followed
When he intends something, something has been intended
When he believes something, something has been believed
When he requires something, something has been required
When he contains something, something has been contained
When he fails something, something has been
2024-07-23 18:27:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 18:31:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.5781,  0.6978,  1.3359,  ...,  0.6514,  0.4897, -0.8389],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1241,  2.0781,  2.4941,  ..., -0.2642, -2.7812, -1.1670],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0099, -0.0021, -0.0037,  ...,  0.0041, -0.0006, -0.0128],
        [-0.0043, -0.0040, -0.0043,  ..., -0.0016,  0.0042, -0.0013],
        [-0.0076,  0.0039, -0.0101,  ..., -0.0139,  0.0082, -0.0002],
        ...,
        [-0.0108,  0.0001, -0.0018,  ..., -0.0092, -0.0116, -0.0067],
        [ 0.0020, -0.0031, -0.0025,  ...,  0.0014, -0.0049,  0.0039],
        [ 0.0047,  0.0066,  0.0030,  ..., -0.0032, -0.0019, -0.0122]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1442,  1.9775,  2.9414,  ..., -0.0928, -2.6035, -1.4033]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 18:31:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he agrees something, something has been agreed
When he introduces something, something has been introduced
When he follows something, something has been followed
When he intends something, something has been intended
When he believes something, something has been believed
When he requires something, something has been required
When he contains something, something has been contained
When he fails something, something has been
2024-07-23 18:31:12 root INFO     [order_1_approx] starting weight calculation for When he fails something, something has been failed
When he contains something, something has been contained
When he believes something, something has been believed
When he agrees something, something has been agreed
When he requires something, something has been required
When he intends something, something has been intended
When he introduces something, something has been introduced
When he follows something, something has been
2024-07-23 18:31:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 18:35:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5371,  0.2212,  1.4277,  ...,  0.7266, -0.5298,  0.9487],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.8086,  0.1868,  0.7227,  ..., -2.9746,  1.6826,  0.6973],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0093, -0.0061,  0.0107,  ..., -0.0053, -0.0062, -0.0116],
        [ 0.0112,  0.0060,  0.0052,  ...,  0.0107, -0.0016,  0.0011],
        [-0.0059,  0.0038, -0.0099,  ..., -0.0053, -0.0115,  0.0096],
        ...,
        [-0.0097, -0.0099, -0.0070,  ...,  0.0060,  0.0050, -0.0085],
        [-0.0116, -0.0033,  0.0042,  ..., -0.0079, -0.0197, -0.0021],
        [ 0.0088,  0.0272, -0.0016,  ...,  0.0008,  0.0058, -0.0204]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6172,  0.2588,  0.3777,  ..., -3.1406,  1.8584,  0.5771]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 18:35:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he fails something, something has been failed
When he contains something, something has been contained
When he believes something, something has been believed
When he agrees something, something has been agreed
When he requires something, something has been required
When he intends something, something has been intended
When he introduces something, something has been introduced
When he follows something, something has been
2024-07-23 18:35:01 root INFO     [order_1_approx] starting weight calculation for When he fails something, something has been failed
When he believes something, something has been believed
When he contains something, something has been contained
When he introduces something, something has been introduced
When he follows something, something has been followed
When he requires something, something has been required
When he agrees something, something has been agreed
When he intends something, something has been
2024-07-23 18:35:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 18:38:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2058,  2.3887, -0.9014,  ...,  0.0220,  1.0410, -0.9224],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1404,  3.4492,  0.1035,  ...,  0.6763, -4.1719,  0.9863],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0003, -0.0089,  0.0071,  ...,  0.0105,  0.0085, -0.0158],
        [-0.0161, -0.0037,  0.0116,  ..., -0.0022, -0.0105,  0.0040],
        [ 0.0018,  0.0097, -0.0129,  ...,  0.0044, -0.0039,  0.0156],
        ...,
        [-0.0226, -0.0005, -0.0064,  ..., -0.0082, -0.0052, -0.0021],
        [ 0.0049,  0.0126, -0.0104,  ..., -0.0046, -0.0126, -0.0086],
        [ 0.0042,  0.0275, -0.0021,  ..., -0.0013, -0.0001, -0.0327]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1018,  4.4258,  0.7368,  ...,  0.8320, -4.3164,  0.5425]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 18:38:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he fails something, something has been failed
When he believes something, something has been believed
When he contains something, something has been contained
When he introduces something, something has been introduced
When he follows something, something has been followed
When he requires something, something has been required
When he agrees something, something has been agreed
When he intends something, something has been
2024-07-23 18:38:50 root INFO     [order_1_approx] starting weight calculation for When he follows something, something has been followed
When he requires something, something has been required
When he fails something, something has been failed
When he agrees something, something has been agreed
When he intends something, something has been intended
When he contains something, something has been contained
When he believes something, something has been believed
When he introduces something, something has been
2024-07-23 18:38:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 18:42:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7344,  0.5576,  0.8589,  ...,  0.0745, -0.4375, -0.2173],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.5811, -0.9771,  0.8965,  ...,  2.9746,  0.2443,  3.5859],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0016, -0.0117, -0.0016,  ...,  0.0125,  0.0012, -0.0120],
        [-0.0093,  0.0001,  0.0036,  ...,  0.0143, -0.0049,  0.0111],
        [-0.0016,  0.0003,  0.0011,  ..., -0.0094, -0.0073, -0.0115],
        ...,
        [-0.0026, -0.0082, -0.0045,  ..., -0.0077, -0.0031,  0.0050],
        [-0.0047,  0.0112, -0.0093,  ..., -0.0142, -0.0186, -0.0076],
        [ 0.0010,  0.0182, -0.0024,  ...,  0.0065,  0.0045, -0.0266]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9922, -0.5693,  1.7227,  ...,  2.2266,  0.6626,  3.4961]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 18:42:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he follows something, something has been followed
When he requires something, something has been required
When he fails something, something has been failed
When he agrees something, something has been agreed
When he intends something, something has been intended
When he contains something, something has been contained
When he believes something, something has been believed
When he introduces something, something has been
2024-07-23 18:42:39 root INFO     [order_1_approx] starting weight calculation for When he intends something, something has been intended
When he believes something, something has been believed
When he fails something, something has been failed
When he follows something, something has been followed
When he contains something, something has been contained
When he agrees something, something has been agreed
When he introduces something, something has been introduced
When he requires something, something has been
2024-07-23 18:42:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 18:46:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9619, -0.0574,  0.0425,  ...,  0.6309,  1.0312, -0.1792],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.8633, -0.3535, -0.6621,  ...,  0.2837, -1.0527,  2.2344],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0037, -0.0043, -0.0036,  ..., -0.0014, -0.0037, -0.0098],
        [-0.0027,  0.0110,  0.0025,  ..., -0.0086, -0.0072,  0.0042],
        [-0.0099,  0.0066,  0.0070,  ..., -0.0005, -0.0095,  0.0167],
        ...,
        [-0.0144, -0.0010, -0.0062,  ...,  0.0001,  0.0026, -0.0028],
        [ 0.0101,  0.0094,  0.0060,  ..., -0.0099, -0.0048,  0.0059],
        [-0.0075,  0.0114,  0.0004,  ..., -0.0045,  0.0033, -0.0215]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1973,  0.0515, -0.0757,  ...,  0.0146, -1.1152,  2.1328]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 18:46:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he intends something, something has been intended
When he believes something, something has been believed
When he fails something, something has been failed
When he follows something, something has been followed
When he contains something, something has been contained
When he agrees something, something has been agreed
When he introduces something, something has been introduced
When he requires something, something has been
2024-07-23 18:46:23 root INFO     total operator prediction time: 1828.5146067142487 seconds
2024-07-23 18:46:23 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on adj - superlative
2024-07-23 18:46:23 root INFO     building operator adj - superlative
2024-07-23 18:46:24 root INFO     [order_1_approx] starting weight calculation for If something is the most nice, it is nicest
If something is the most subtle, it is subtlest
If something is the most tricky, it is trickiest
If something is the most shiny, it is shiniest
If something is the most ugly, it is ugliest
If something is the most mild, it is mildest
If something is the most rare, it is rarest
If something is the most lucky, it is
2024-07-23 18:46:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 18:50:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2786,  0.8643,  0.1511,  ...,  0.4863,  0.3762, -0.8018],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.8398,  1.1738, -5.7422,  ..., -2.4668,  3.4414,  4.7617],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0047, -0.0140,  0.0097,  ..., -0.0035, -0.0064, -0.0024],
        [ 0.0008, -0.0026, -0.0012,  ...,  0.0063,  0.0077, -0.0038],
        [ 0.0100,  0.0088, -0.0046,  ...,  0.0055,  0.0011,  0.0068],
        ...,
        [ 0.0007, -0.0033, -0.0025,  ..., -0.0176, -0.0095, -0.0046],
        [-0.0002, -0.0111, -0.0016,  ..., -0.0114, -0.0222,  0.0057],
        [-0.0061,  0.0157,  0.0106,  ...,  0.0084,  0.0037, -0.0050]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0020,  1.3809, -5.2773,  ..., -2.4980,  3.5215,  4.8242]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 18:50:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most nice, it is nicest
If something is the most subtle, it is subtlest
If something is the most tricky, it is trickiest
If something is the most shiny, it is shiniest
If something is the most ugly, it is ugliest
If something is the most mild, it is mildest
If something is the most rare, it is rarest
If something is the most lucky, it is
2024-07-23 18:50:11 root INFO     [order_1_approx] starting weight calculation for If something is the most subtle, it is subtlest
If something is the most shiny, it is shiniest
If something is the most nice, it is nicest
If something is the most ugly, it is ugliest
If something is the most rare, it is rarest
If something is the most tricky, it is trickiest
If something is the most lucky, it is luckiest
If something is the most mild, it is
2024-07-23 18:50:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 18:53:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2886, -0.2615,  0.5132,  ...,  0.5020,  0.2993,  0.0327],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6152,  0.4502, -5.0234,  ..., -0.5264, -2.3613,  5.0078],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0119, -0.0234,  0.0047,  ...,  0.0007, -0.0102, -0.0021],
        [-0.0029, -0.0071, -0.0086,  ..., -0.0100, -0.0085, -0.0018],
        [ 0.0040, -0.0002, -0.0111,  ...,  0.0006,  0.0058,  0.0062],
        ...,
        [-0.0046, -0.0133,  0.0008,  ..., -0.0198,  0.0015,  0.0019],
        [ 0.0046,  0.0144,  0.0108,  ..., -0.0013, -0.0226,  0.0039],
        [-0.0220, -0.0062,  0.0113,  ..., -0.0068,  0.0132, -0.0297]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0171,  0.8838, -5.6445,  ..., -0.2715, -2.6719,  4.6484]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 18:53:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most subtle, it is subtlest
If something is the most shiny, it is shiniest
If something is the most nice, it is nicest
If something is the most ugly, it is ugliest
If something is the most rare, it is rarest
If something is the most tricky, it is trickiest
If something is the most lucky, it is luckiest
If something is the most mild, it is
2024-07-23 18:53:59 root INFO     [order_1_approx] starting weight calculation for If something is the most mild, it is mildest
If something is the most rare, it is rarest
If something is the most lucky, it is luckiest
If something is the most tricky, it is trickiest
If something is the most shiny, it is shiniest
If something is the most subtle, it is subtlest
If something is the most ugly, it is ugliest
If something is the most nice, it is
2024-07-23 18:53:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 18:57:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0039, -0.8022, -0.1891,  ...,  0.0513,  0.6924, -0.5098],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.2363,  0.6138, -0.2505,  ...,  0.8096, -1.4307,  5.5508],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0045, -0.0294,  0.0039,  ...,  0.0211, -0.0009, -0.0006],
        [-0.0124, -0.0120, -0.0069,  ..., -0.0003, -0.0094, -0.0113],
        [ 0.0033, -0.0067, -0.0014,  ...,  0.0049,  0.0076,  0.0119],
        ...,
        [-0.0122, -0.0040,  0.0169,  ..., -0.0139, -0.0089, -0.0062],
        [ 0.0019,  0.0013,  0.0067,  ..., -0.0054, -0.0230,  0.0218],
        [-0.0106,  0.0038,  0.0098,  ..., -0.0118, -0.0097, -0.0386]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.2266,  0.1555,  0.0461,  ...,  0.9434, -1.7637,  5.5703]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 18:57:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most mild, it is mildest
If something is the most rare, it is rarest
If something is the most lucky, it is luckiest
If something is the most tricky, it is trickiest
If something is the most shiny, it is shiniest
If something is the most subtle, it is subtlest
If something is the most ugly, it is ugliest
If something is the most nice, it is
2024-07-23 18:57:45 root INFO     [order_1_approx] starting weight calculation for If something is the most subtle, it is subtlest
If something is the most lucky, it is luckiest
If something is the most tricky, it is trickiest
If something is the most nice, it is nicest
If something is the most ugly, it is ugliest
If something is the most shiny, it is shiniest
If something is the most mild, it is mildest
If something is the most rare, it is
2024-07-23 18:57:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 19:01:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5391,  0.2163, -0.4573,  ..., -0.2278,  1.7754, -1.3184],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.9727, -1.2461, -0.9907,  ...,  3.6602,  1.4023,  3.9922],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0037, -0.0168,  0.0044,  ..., -0.0090, -0.0059, -0.0018],
        [-0.0023, -0.0095,  0.0010,  ...,  0.0020, -0.0105, -0.0021],
        [-0.0068, -0.0002, -0.0057,  ..., -0.0066,  0.0033,  0.0080],
        ...,
        [-0.0129, -0.0109,  0.0049,  ..., -0.0156,  0.0003, -0.0108],
        [ 0.0018, -0.0180,  0.0127,  ..., -0.0104, -0.0143,  0.0039],
        [-0.0141,  0.0023,  0.0085,  ..., -0.0132,  0.0052, -0.0189]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.4473, -1.1006, -0.8345,  ...,  2.9980,  0.8149,  3.5859]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 19:01:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most subtle, it is subtlest
If something is the most lucky, it is luckiest
If something is the most tricky, it is trickiest
If something is the most nice, it is nicest
If something is the most ugly, it is ugliest
If something is the most shiny, it is shiniest
If something is the most mild, it is mildest
If something is the most rare, it is
2024-07-23 19:01:33 root INFO     [order_1_approx] starting weight calculation for If something is the most ugly, it is ugliest
If something is the most lucky, it is luckiest
If something is the most nice, it is nicest
If something is the most tricky, it is trickiest
If something is the most rare, it is rarest
If something is the most mild, it is mildest
If something is the most subtle, it is subtlest
If something is the most shiny, it is
2024-07-23 19:01:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 19:05:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0449, -0.0679,  0.8262,  ...,  0.0527,  2.1875, -0.2258],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.3633,  0.9121, -2.7285,  ...,  4.6055, -5.8398,  3.5820],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-8.3828e-04, -1.8646e-02,  3.4466e-03,  ...,  1.2932e-03,
          3.5739e-04, -1.7624e-03],
        [ 3.6182e-03, -6.5994e-04,  5.8594e-03,  ..., -2.8534e-03,
         -1.6571e-02, -2.3460e-03],
        [ 2.7275e-04, -2.6703e-05,  2.4242e-03,  ...,  3.3035e-03,
          8.4763e-03,  7.8964e-03],
        ...,
        [-7.1297e-03, -5.8136e-03, -7.6447e-03,  ..., -1.0162e-02,
         -1.3268e-02, -6.7978e-03],
        [ 5.6915e-03, -6.4812e-03, -5.1270e-03,  ..., -6.4278e-03,
         -2.3590e-02,  9.0790e-03],
        [-1.2375e-02,  1.2085e-02, -8.2016e-03,  ..., -5.3482e-03,
          1.3733e-02, -1.6891e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.4492,  0.9653, -2.7070,  ...,  4.5508, -6.3164,  3.8281]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 19:05:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most ugly, it is ugliest
If something is the most lucky, it is luckiest
If something is the most nice, it is nicest
If something is the most tricky, it is trickiest
If something is the most rare, it is rarest
If something is the most mild, it is mildest
If something is the most subtle, it is subtlest
If something is the most shiny, it is
2024-07-23 19:05:21 root INFO     [order_1_approx] starting weight calculation for If something is the most ugly, it is ugliest
If something is the most mild, it is mildest
If something is the most rare, it is rarest
If something is the most tricky, it is trickiest
If something is the most lucky, it is luckiest
If something is the most shiny, it is shiniest
If something is the most nice, it is nicest
If something is the most subtle, it is
2024-07-23 19:05:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 19:09:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2632, -0.9023, -0.1589,  ...,  0.9883,  0.6055, -1.5430],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.1426,  3.3203, -0.6602,  ...,  6.0820, -5.7969,  3.8945],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0003, -0.0094, -0.0053,  ..., -0.0007,  0.0023, -0.0022],
        [-0.0018,  0.0127,  0.0002,  ..., -0.0053, -0.0169, -0.0016],
        [ 0.0046,  0.0025, -0.0041,  ...,  0.0083, -0.0113, -0.0008],
        ...,
        [-0.0166, -0.0179,  0.0087,  ..., -0.0156,  0.0033, -0.0162],
        [ 0.0005, -0.0033,  0.0092,  ..., -0.0034, -0.0101, -0.0054],
        [-0.0107,  0.0122,  0.0035,  ..., -0.0200,  0.0099, -0.0281]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.2793,  3.8613, -0.2737,  ...,  5.6797, -6.2930,  3.6367]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 19:09:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most ugly, it is ugliest
If something is the most mild, it is mildest
If something is the most rare, it is rarest
If something is the most tricky, it is trickiest
If something is the most lucky, it is luckiest
If something is the most shiny, it is shiniest
If something is the most nice, it is nicest
If something is the most subtle, it is
2024-07-23 19:09:07 root INFO     [order_1_approx] starting weight calculation for If something is the most lucky, it is luckiest
If something is the most ugly, it is ugliest
If something is the most subtle, it is subtlest
If something is the most shiny, it is shiniest
If something is the most rare, it is rarest
If something is the most nice, it is nicest
If something is the most mild, it is mildest
If something is the most tricky, it is
2024-07-23 19:09:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 19:12:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2832,  0.1487, -0.6250,  ...,  0.2551,  0.4495, -0.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-8.3984, -0.0757,  1.7676,  ...,  3.9766,  0.9316,  4.7578],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-4.3793e-03, -7.2479e-05,  1.0666e-02,  ...,  7.0038e-03,
         -9.3460e-03, -1.5854e-02],
        [-3.4485e-03, -6.5536e-03,  7.6447e-03,  ..., -1.1425e-03,
          6.2466e-04, -1.5564e-03],
        [-1.0422e-02,  3.3321e-03, -3.4103e-03,  ...,  3.6640e-03,
          1.4839e-02, -1.4229e-03],
        ...,
        [-6.5880e-03, -2.0874e-02, -8.3771e-03,  ..., -2.8259e-02,
          3.5038e-03, -1.0895e-02],
        [-1.2779e-03, -1.3447e-03,  1.6006e-02,  ..., -1.9714e-02,
         -2.9175e-02,  6.5155e-03],
        [-3.0731e-02,  2.2308e-02, -1.9207e-03,  ..., -1.0719e-02,
          9.3079e-04, -3.5339e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-8.2500,  0.3032,  1.8828,  ...,  4.1406,  0.6543,  4.5859]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 19:12:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most lucky, it is luckiest
If something is the most ugly, it is ugliest
If something is the most subtle, it is subtlest
If something is the most shiny, it is shiniest
If something is the most rare, it is rarest
If something is the most nice, it is nicest
If something is the most mild, it is mildest
If something is the most tricky, it is
2024-07-23 19:12:55 root INFO     [order_1_approx] starting weight calculation for If something is the most shiny, it is shiniest
If something is the most rare, it is rarest
If something is the most subtle, it is subtlest
If something is the most nice, it is nicest
If something is the most mild, it is mildest
If something is the most tricky, it is trickiest
If something is the most lucky, it is luckiest
If something is the most ugly, it is
2024-07-23 19:12:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 19:16:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7632, -0.5391,  0.0557,  ..., -0.0034,  2.0586, -0.9751],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.2832, -1.0801, -3.2734,  ...,  1.0889, -3.2207,  1.5840],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0056, -0.0105,  0.0033,  ..., -0.0004, -0.0044, -0.0218],
        [ 0.0051, -0.0034, -0.0069,  ..., -0.0106, -0.0135, -0.0090],
        [ 0.0029,  0.0114, -0.0074,  ...,  0.0030, -0.0046,  0.0060],
        ...,
        [-0.0085, -0.0016,  0.0077,  ..., -0.0163, -0.0085, -0.0055],
        [ 0.0012,  0.0036,  0.0019,  ..., -0.0023, -0.0181, -0.0024],
        [-0.0210, -0.0181,  0.0032,  ..., -0.0115, -0.0130, -0.0150]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9121, -1.2051, -3.4336,  ...,  1.0859, -3.1543,  1.1230]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 19:16:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most shiny, it is shiniest
If something is the most rare, it is rarest
If something is the most subtle, it is subtlest
If something is the most nice, it is nicest
If something is the most mild, it is mildest
If something is the most tricky, it is trickiest
If something is the most lucky, it is luckiest
If something is the most ugly, it is
2024-07-23 19:16:43 root INFO     total operator prediction time: 1819.3358736038208 seconds
2024-07-23 19:16:43 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+er_irreg
2024-07-23 19:16:43 root INFO     building operator verb+er_irreg
2024-07-23 19:16:43 root INFO     [order_1_approx] starting weight calculation for If you determine something, you are a determiner
If you promote something, you are a promoter
If you molest something, you are a molester
If you destroy something, you are a destroyer
If you organise something, you are a organiser
If you mourn something, you are a mourner
If you receive something, you are a receiver
If you advertise something, you are a
2024-07-23 19:16:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 19:20:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2178,  1.0576,  0.3813,  ..., -0.3457,  0.9062,  0.6963],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3887, -0.3467, -6.1602,  ..., -0.9585,  0.9512,  2.1523],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-6.0577e-03,  1.7548e-03,  1.3443e-02,  ..., -2.9507e-03,
          1.7471e-03, -6.6795e-03],
        [-6.4774e-03, -6.1531e-03,  5.1651e-03,  ...,  2.6684e-03,
         -9.8991e-04, -5.7907e-03],
        [ 1.6113e-02,  2.3865e-02,  1.0925e-02,  ..., -8.6517e-03,
          1.4267e-02,  1.7914e-02],
        ...,
        [-1.0681e-02, -1.4420e-03, -2.9907e-03,  ..., -5.3444e-03,
          4.9133e-03,  8.4686e-03],
        [ 2.7180e-04,  3.0861e-03,  5.7449e-03,  ..., -2.7122e-03,
         -1.1444e-02,  5.9204e-03],
        [ 8.3618e-03,  3.3436e-03,  9.0241e-05,  ..., -7.6218e-03,
          1.4816e-02,  1.2192e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1914, -0.1930, -6.8750,  ..., -0.8735,  0.7334,  2.3457]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 19:20:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you determine something, you are a determiner
If you promote something, you are a promoter
If you molest something, you are a molester
If you destroy something, you are a destroyer
If you organise something, you are a organiser
If you mourn something, you are a mourner
If you receive something, you are a receiver
If you advertise something, you are a
2024-07-23 19:20:29 root INFO     [order_1_approx] starting weight calculation for If you organise something, you are a organiser
If you determine something, you are a determiner
If you receive something, you are a receiver
If you advertise something, you are a advertiser
If you mourn something, you are a mourner
If you molest something, you are a molester
If you promote something, you are a promoter
If you destroy something, you are a
2024-07-23 19:20:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 19:24:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2695,  0.1511,  0.7334,  ..., -0.2549, -0.2156, -0.2142],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.9727, -2.4922, -4.1094,  ..., -0.8364,  5.5273,  6.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0024,  0.0064,  0.0083,  ..., -0.0047, -0.0037, -0.0080],
        [ 0.0068,  0.0069,  0.0034,  ..., -0.0032,  0.0022,  0.0023],
        [ 0.0030,  0.0078, -0.0035,  ...,  0.0064,  0.0074,  0.0069],
        ...,
        [-0.0133, -0.0166, -0.0014,  ...,  0.0045, -0.0061,  0.0015],
        [ 0.0024, -0.0027,  0.0011,  ..., -0.0045, -0.0012, -0.0035],
        [-0.0025, -0.0022, -0.0002,  ..., -0.0173,  0.0085,  0.0179]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.3789, -2.5020, -3.9785,  ..., -0.8110,  5.0234,  6.3047]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 19:24:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you organise something, you are a organiser
If you determine something, you are a determiner
If you receive something, you are a receiver
If you advertise something, you are a advertiser
If you mourn something, you are a mourner
If you molest something, you are a molester
If you promote something, you are a promoter
If you destroy something, you are a
2024-07-23 19:24:15 root INFO     [order_1_approx] starting weight calculation for If you organise something, you are a organiser
If you receive something, you are a receiver
If you molest something, you are a molester
If you destroy something, you are a destroyer
If you promote something, you are a promoter
If you advertise something, you are a advertiser
If you mourn something, you are a mourner
If you determine something, you are a
2024-07-23 19:24:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 19:28:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-6.2012e-02,  9.0674e-01, -7.3364e-02,  ...,  8.5449e-04,
        -5.9229e-01,  1.4795e-01], device='cuda:0', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.2305, -3.1191, -5.3672,  ..., -4.8711,  2.8984,  4.3398],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.0880e-02,  1.0582e-02,  4.8714e-03,  ..., -1.9226e-02,
         -4.4098e-03,  2.3460e-03],
        [-1.8692e-03,  6.7520e-03,  1.2352e-02,  ..., -1.1978e-03,
          2.4796e-05,  1.7738e-03],
        [ 3.7155e-03,  1.3077e-02, -1.2344e-02,  ..., -1.6212e-04,
         -5.4979e-04,  2.1362e-03],
        ...,
        [-1.2688e-02, -2.3804e-03, -5.7220e-04,  ..., -3.5801e-03,
          4.3602e-03,  1.2379e-03],
        [ 6.6614e-04,  3.5858e-04, -7.9193e-03,  ..., -9.2545e-03,
         -1.4709e-02, -1.3123e-03],
        [-5.7259e-03,  5.2948e-03,  9.0332e-03,  ..., -3.4676e-03,
          1.3947e-02,  1.0090e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.1562, -3.0137, -5.6406,  ..., -3.9180,  3.0352,  4.3047]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 19:28:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you organise something, you are a organiser
If you receive something, you are a receiver
If you molest something, you are a molester
If you destroy something, you are a destroyer
If you promote something, you are a promoter
If you advertise something, you are a advertiser
If you mourn something, you are a mourner
If you determine something, you are a
2024-07-23 19:28:01 root INFO     [order_1_approx] starting weight calculation for If you receive something, you are a receiver
If you advertise something, you are a advertiser
If you mourn something, you are a mourner
If you determine something, you are a determiner
If you organise something, you are a organiser
If you destroy something, you are a destroyer
If you promote something, you are a promoter
If you molest something, you are a
2024-07-23 19:28:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 19:31:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4121,  0.1821,  2.0898,  ..., -0.3442,  0.3462,  0.6406],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8340,  0.4358, -3.5059,  ...,  0.0273,  4.1406,  5.8164],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0053, -0.0021,  0.0009,  ...,  0.0006, -0.0030, -0.0017],
        [ 0.0063, -0.0021, -0.0033,  ...,  0.0042,  0.0028, -0.0051],
        [ 0.0106,  0.0076, -0.0097,  ...,  0.0107, -0.0011,  0.0052],
        ...,
        [-0.0123, -0.0080, -0.0026,  ..., -0.0080,  0.0013, -0.0006],
        [ 0.0117,  0.0028,  0.0049,  ..., -0.0051, -0.0122,  0.0090],
        [ 0.0056,  0.0051, -0.0023,  ..., -0.0022,  0.0150, -0.0012]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1211,  0.4915, -3.3945,  ...,  0.0928,  4.6172,  5.8477]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 19:31:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you receive something, you are a receiver
If you advertise something, you are a advertiser
If you mourn something, you are a mourner
If you determine something, you are a determiner
If you organise something, you are a organiser
If you destroy something, you are a destroyer
If you promote something, you are a promoter
If you molest something, you are a
2024-07-23 19:31:47 root INFO     [order_1_approx] starting weight calculation for If you receive something, you are a receiver
If you advertise something, you are a advertiser
If you destroy something, you are a destroyer
If you promote something, you are a promoter
If you organise something, you are a organiser
If you determine something, you are a determiner
If you molest something, you are a molester
If you mourn something, you are a
2024-07-23 19:31:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 19:35:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6099, -0.6509,  0.7310,  ...,  0.9336, -0.0356,  0.1152],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.3477,  3.4570, -6.7070,  ..., -1.8633,  4.8281,  4.4375],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0012,  0.0022,  0.0185,  ..., -0.0024, -0.0134, -0.0041],
        [-0.0022, -0.0140,  0.0145,  ...,  0.0092, -0.0089, -0.0007],
        [ 0.0315,  0.0234, -0.0153,  ...,  0.0214,  0.0338,  0.0048],
        ...,
        [-0.0013, -0.0186, -0.0065,  ..., -0.0076,  0.0133,  0.0091],
        [-0.0110, -0.0103, -0.0070,  ..., -0.0177, -0.0298, -0.0060],
        [-0.0046,  0.0075,  0.0023,  ..., -0.0061,  0.0179, -0.0130]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.6523,  3.5410, -8.2188,  ..., -2.0332,  6.0312,  4.1797]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 19:35:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you receive something, you are a receiver
If you advertise something, you are a advertiser
If you destroy something, you are a destroyer
If you promote something, you are a promoter
If you organise something, you are a organiser
If you determine something, you are a determiner
If you molest something, you are a molester
If you mourn something, you are a
2024-07-23 19:35:34 root INFO     [order_1_approx] starting weight calculation for If you determine something, you are a determiner
If you promote something, you are a promoter
If you mourn something, you are a mourner
If you receive something, you are a receiver
If you advertise something, you are a advertiser
If you destroy something, you are a destroyer
If you molest something, you are a molester
If you organise something, you are a
2024-07-23 19:35:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 19:39:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9590,  0.5522,  0.5352,  ...,  0.4102, -0.0348,  0.9844],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4971,  0.2350, -1.9980,  ...,  1.2969,  2.3789,  6.0273],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0092,  0.0008,  0.0005,  ...,  0.0014,  0.0109, -0.0048],
        [ 0.0005,  0.0038,  0.0024,  ...,  0.0025,  0.0090, -0.0081],
        [ 0.0090,  0.0073,  0.0012,  ..., -0.0011,  0.0015,  0.0036],
        ...,
        [-0.0100, -0.0088, -0.0011,  ...,  0.0028,  0.0005,  0.0118],
        [ 0.0078,  0.0077, -0.0096,  ..., -0.0071,  0.0058,  0.0066],
        [-0.0050,  0.0043,  0.0093,  ..., -0.0177,  0.0167, -0.0052]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2402, -0.2563, -1.8682,  ...,  1.5625,  2.1914,  5.7148]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 19:39:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you determine something, you are a determiner
If you promote something, you are a promoter
If you mourn something, you are a mourner
If you receive something, you are a receiver
If you advertise something, you are a advertiser
If you destroy something, you are a destroyer
If you molest something, you are a molester
If you organise something, you are a
2024-07-23 19:39:20 root INFO     [order_1_approx] starting weight calculation for If you mourn something, you are a mourner
If you destroy something, you are a destroyer
If you advertise something, you are a advertiser
If you determine something, you are a determiner
If you receive something, you are a receiver
If you organise something, you are a organiser
If you molest something, you are a molester
If you promote something, you are a
2024-07-23 19:39:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 19:43:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1869,  0.5078,  0.4729,  ..., -0.2764,  0.1527,  0.4226],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.2891, -1.1582, -5.2188,  ...,  0.5781,  1.5918,  2.5156],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-7.2861e-03, -6.7520e-04,  1.5137e-02,  ...,  1.3008e-03,
          5.0850e-03, -5.5313e-03],
        [-2.3384e-03, -3.7193e-04,  8.4686e-03,  ...,  1.2817e-02,
          2.2335e-03, -1.0576e-03],
        [ 1.1925e-02,  2.3758e-02,  3.2787e-03,  ..., -1.1414e-02,
          1.4893e-02,  5.4131e-03],
        ...,
        [-9.5673e-03, -1.2054e-02, -1.6266e-02,  ...,  4.6921e-03,
         -4.1122e-03,  1.0712e-02],
        [ 1.4381e-02,  1.3321e-02, -4.4479e-03,  ..., -1.6800e-02,
         -9.9182e-05,  5.5847e-03],
        [ 1.8082e-02,  1.1711e-03,  4.4556e-03,  ..., -1.8372e-02,
          1.2138e-02,  9.7504e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.2773, -0.9229, -4.7578,  ...,  0.2478,  0.7983,  2.6445]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 19:43:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you mourn something, you are a mourner
If you destroy something, you are a destroyer
If you advertise something, you are a advertiser
If you determine something, you are a determiner
If you receive something, you are a receiver
If you organise something, you are a organiser
If you molest something, you are a molester
If you promote something, you are a
2024-07-23 19:43:07 root INFO     [order_1_approx] starting weight calculation for If you destroy something, you are a destroyer
If you molest something, you are a molester
If you advertise something, you are a advertiser
If you mourn something, you are a mourner
If you determine something, you are a determiner
If you organise something, you are a organiser
If you promote something, you are a promoter
If you receive something, you are a
2024-07-23 19:43:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 19:46:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0742, -0.0269, -0.4741,  ..., -0.1897, -0.3887,  0.7275],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7109, -1.1094, -1.5859,  ..., -2.4727,  2.5215,  2.3320],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.6665e-03,  1.5488e-03,  5.4703e-03,  ..., -1.1154e-02,
          3.6755e-03, -9.0637e-03],
        [ 8.7204e-03,  7.4768e-03,  1.0323e-02,  ..., -2.9716e-03,
         -6.0806e-03, -8.2321e-03],
        [-2.3460e-03,  6.3210e-03, -4.9362e-03,  ...,  1.2680e-02,
          7.0572e-05,  4.3182e-03],
        ...,
        [-1.2848e-02, -1.0574e-02,  8.3618e-03,  ...,  5.4321e-03,
          5.5199e-03, -5.0507e-03],
        [ 1.1833e-02,  3.5858e-03, -3.5400e-03,  ..., -1.0681e-03,
         -6.3400e-03,  1.0841e-02],
        [ 1.0452e-03,  1.0750e-02, -4.1924e-03,  ..., -7.4387e-03,
          1.8372e-02,  1.5259e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4043, -0.9165, -1.1553,  ..., -2.8652,  2.3379,  2.7773]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 19:46:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you destroy something, you are a destroyer
If you molest something, you are a molester
If you advertise something, you are a advertiser
If you mourn something, you are a mourner
If you determine something, you are a determiner
If you organise something, you are a organiser
If you promote something, you are a promoter
If you receive something, you are a
2024-07-23 19:46:54 root INFO     total operator prediction time: 1811.5790152549744 seconds
2024-07-23 19:46:54 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on over+adj_reg
2024-07-23 19:46:54 root INFO     building operator over+adj_reg
2024-07-23 19:46:55 root INFO     [order_1_approx] starting weight calculation for If something is too dressed, it is overdressed
If something is too shadowed, it is overshadowed
If something is too paid, it is overpaid
If something is too exposed, it is overexposed
If something is too protected, it is overprotected
If something is too loaded, it is overloaded
If something is too represented, it is overrepresented
If something is too ambitious, it is
2024-07-23 19:46:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 19:50:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1301, -0.9121,  0.0312,  ..., -0.6680, -0.3262, -0.2576],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.7383,  1.4463, -0.1475,  ...,  1.3184,  2.0312,  2.8203],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0174, -0.0206,  0.0215,  ..., -0.0119, -0.0115, -0.0053],
        [-0.0010,  0.0013,  0.0008,  ..., -0.0010, -0.0066, -0.0014],
        [ 0.0122,  0.0174, -0.0080,  ...,  0.0033,  0.0105,  0.0171],
        ...,
        [-0.0132, -0.0183, -0.0078,  ..., -0.0070, -0.0089, -0.0099],
        [-0.0076,  0.0014,  0.0107,  ..., -0.0011, -0.0158, -0.0056],
        [-0.0014,  0.0092,  0.0271,  ...,  0.0047, -0.0009, -0.0178]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.8047,  1.8066, -0.9810,  ...,  1.5117,  1.3057,  2.6719]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 19:50:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too dressed, it is overdressed
If something is too shadowed, it is overshadowed
If something is too paid, it is overpaid
If something is too exposed, it is overexposed
If something is too protected, it is overprotected
If something is too loaded, it is overloaded
If something is too represented, it is overrepresented
If something is too ambitious, it is
2024-07-23 19:50:42 root INFO     [order_1_approx] starting weight calculation for If something is too paid, it is overpaid
If something is too ambitious, it is overambitious
If something is too loaded, it is overloaded
If something is too shadowed, it is overshadowed
If something is too exposed, it is overexposed
If something is too protected, it is overprotected
If something is too represented, it is overrepresented
If something is too dressed, it is
2024-07-23 19:50:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 19:54:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2412,  0.4014,  1.5352,  ...,  1.3535,  1.1611, -0.6963],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2769, -2.6484, -0.0938,  ..., -5.5664,  1.2109,  4.9258],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0051, -0.0397, -0.0086,  ...,  0.0124, -0.0017, -0.0114],
        [ 0.0013, -0.0004,  0.0149,  ...,  0.0198,  0.0006, -0.0118],
        [ 0.0302, -0.0020, -0.0072,  ...,  0.0114,  0.0202, -0.0024],
        ...,
        [-0.0183, -0.0054,  0.0022,  ..., -0.0139, -0.0092,  0.0097],
        [-0.0131,  0.0054,  0.0051,  ..., -0.0061, -0.0230,  0.0070],
        [-0.0357, -0.0034,  0.0137,  ..., -0.0064, -0.0047, -0.0140]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.8247, -1.8496, -0.0806,  ..., -6.3789,  0.8203,  4.8633]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 19:54:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too paid, it is overpaid
If something is too ambitious, it is overambitious
If something is too loaded, it is overloaded
If something is too shadowed, it is overshadowed
If something is too exposed, it is overexposed
If something is too protected, it is overprotected
If something is too represented, it is overrepresented
If something is too dressed, it is
2024-07-23 19:54:31 root INFO     [order_1_approx] starting weight calculation for If something is too ambitious, it is overambitious
If something is too represented, it is overrepresented
If something is too dressed, it is overdressed
If something is too protected, it is overprotected
If something is too paid, it is overpaid
If something is too loaded, it is overloaded
If something is too shadowed, it is overshadowed
If something is too exposed, it is
2024-07-23 19:54:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 19:58:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9902,  0.1265,  1.7168,  ..., -0.2257,  0.5137, -0.4277],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.6055,  3.1465, -1.3857,  ..., -0.2686, -0.4109, -1.5137],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0009, -0.0148, -0.0002,  ..., -0.0045, -0.0116, -0.0128],
        [-0.0072,  0.0061,  0.0140,  ..., -0.0006, -0.0079, -0.0069],
        [ 0.0178,  0.0090, -0.0025,  ...,  0.0087,  0.0075, -0.0010],
        ...,
        [-0.0188, -0.0102,  0.0066,  ..., -0.0068, -0.0061, -0.0046],
        [ 0.0138, -0.0182, -0.0041,  ...,  0.0162, -0.0142,  0.0106],
        [-0.0098,  0.0113,  0.0173,  ..., -0.0067, -0.0010, -0.0073]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.8828,  3.3262, -1.9824,  ..., -0.1199, -0.6499, -1.6777]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 19:58:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too ambitious, it is overambitious
If something is too represented, it is overrepresented
If something is too dressed, it is overdressed
If something is too protected, it is overprotected
If something is too paid, it is overpaid
If something is too loaded, it is overloaded
If something is too shadowed, it is overshadowed
If something is too exposed, it is
2024-07-23 19:58:18 root INFO     [order_1_approx] starting weight calculation for If something is too protected, it is overprotected
If something is too shadowed, it is overshadowed
If something is too dressed, it is overdressed
If something is too represented, it is overrepresented
If something is too paid, it is overpaid
If something is too ambitious, it is overambitious
If something is too exposed, it is overexposed
If something is too loaded, it is
2024-07-23 19:58:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 20:02:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6304,  0.2603,  0.1248,  ...,  0.4521,  0.2520, -0.5068],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6973, -0.1332, -2.0117,  ..., -0.1562,  0.1252,  4.3711],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0104, -0.0218,  0.0121,  ..., -0.0045, -0.0031,  0.0049],
        [-0.0036,  0.0125, -0.0020,  ...,  0.0057, -0.0121, -0.0096],
        [-0.0085,  0.0040, -0.0092,  ...,  0.0043, -0.0077,  0.0012],
        ...,
        [-0.0300, -0.0065,  0.0133,  ..., -0.0163,  0.0054, -0.0106],
        [-0.0107, -0.0121,  0.0141,  ...,  0.0077, -0.0276, -0.0276],
        [-0.0173,  0.0055,  0.0117,  ..., -0.0228, -0.0107, -0.0218]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4932, -0.4097, -1.7822,  ...,  0.3647,  0.5728,  4.8945]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 20:02:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too protected, it is overprotected
If something is too shadowed, it is overshadowed
If something is too dressed, it is overdressed
If something is too represented, it is overrepresented
If something is too paid, it is overpaid
If something is too ambitious, it is overambitious
If something is too exposed, it is overexposed
If something is too loaded, it is
2024-07-23 20:02:07 root INFO     [order_1_approx] starting weight calculation for If something is too dressed, it is overdressed
If something is too shadowed, it is overshadowed
If something is too ambitious, it is overambitious
If something is too represented, it is overrepresented
If something is too loaded, it is overloaded
If something is too exposed, it is overexposed
If something is too protected, it is overprotected
If something is too paid, it is
2024-07-23 20:02:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 20:05:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.3574, -0.5181,  0.1040,  ...,  0.4910,  0.4617,  0.1990],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.1777, -1.5547,  0.1611,  ...,  0.3906,  1.2627,  3.4043],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.7357e-03, -2.3972e-02,  1.1574e-02,  ..., -1.7624e-03,
         -1.4854e-02, -3.7384e-03],
        [-9.7046e-03,  3.2921e-03,  1.4107e-02,  ..., -3.1700e-03,
          1.1024e-03, -5.3406e-05],
        [ 7.1983e-03,  1.4328e-02, -1.0094e-02,  ..., -4.4174e-03,
          6.4888e-03,  3.6297e-03],
        ...,
        [-7.6904e-03, -5.8060e-03, -8.3542e-03,  ...,  3.8948e-03,
          4.8141e-03, -1.5228e-02],
        [-5.5008e-03, -1.2314e-02, -7.7963e-04,  ..., -2.4796e-03,
         -1.7548e-02,  1.6190e-02],
        [ 1.6632e-03,  6.1417e-04,  8.7051e-03,  ..., -2.5177e-03,
          5.0316e-03, -3.4657e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.0098, -0.9438,  0.2484,  ...,  0.1018,  2.0566,  3.4453]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 20:05:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too dressed, it is overdressed
If something is too shadowed, it is overshadowed
If something is too ambitious, it is overambitious
If something is too represented, it is overrepresented
If something is too loaded, it is overloaded
If something is too exposed, it is overexposed
If something is too protected, it is overprotected
If something is too paid, it is
2024-07-23 20:05:55 root INFO     [order_1_approx] starting weight calculation for If something is too represented, it is overrepresented
If something is too dressed, it is overdressed
If something is too paid, it is overpaid
If something is too shadowed, it is overshadowed
If something is too loaded, it is overloaded
If something is too exposed, it is overexposed
If something is too ambitious, it is overambitious
If something is too protected, it is
2024-07-23 20:05:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 20:09:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3301, -0.1569,  1.1660,  ...,  1.3535,  1.6641,  0.3689],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5381,  0.3555, -0.8857,  ...,  1.6846, -0.1763,  2.1602],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0113, -0.0309,  0.0020,  ..., -0.0095, -0.0047, -0.0116],
        [ 0.0052,  0.0046,  0.0067,  ...,  0.0079,  0.0077, -0.0019],
        [ 0.0064,  0.0071,  0.0032,  ..., -0.0055,  0.0229,  0.0215],
        ...,
        [-0.0163,  0.0025, -0.0144,  ..., -0.0069, -0.0085, -0.0064],
        [-0.0109, -0.0262, -0.0107,  ...,  0.0015, -0.0135,  0.0155],
        [-0.0002,  0.0295,  0.0085,  ..., -0.0202, -0.0066, -0.0049]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1299,  0.7119, -0.1489,  ...,  1.8066, -1.0400,  3.0039]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 20:09:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too represented, it is overrepresented
If something is too dressed, it is overdressed
If something is too paid, it is overpaid
If something is too shadowed, it is overshadowed
If something is too loaded, it is overloaded
If something is too exposed, it is overexposed
If something is too ambitious, it is overambitious
If something is too protected, it is
2024-07-23 20:09:39 root INFO     [order_1_approx] starting weight calculation for If something is too ambitious, it is overambitious
If something is too shadowed, it is overshadowed
If something is too paid, it is overpaid
If something is too dressed, it is overdressed
If something is too exposed, it is overexposed
If something is too loaded, it is overloaded
If something is too protected, it is overprotected
If something is too represented, it is
2024-07-23 20:09:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 20:13:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4768, -0.4331,  0.1045,  ..., -0.4839,  0.7324, -0.7207],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6143, -1.3643, -0.6621,  ...,  0.8496,  1.7822,  2.8340],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 8.6670e-03, -1.9547e-02, -8.0490e-04,  ...,  7.4387e-03,
         -1.4362e-03, -1.1444e-03],
        [-6.6986e-03, -7.7591e-03,  1.5747e-02,  ...,  3.6774e-03,
         -2.0981e-03, -9.9030e-03],
        [ 1.9012e-02,  7.6294e-04, -2.2400e-02,  ..., -5.7888e-04,
          3.1052e-02,  9.1705e-03],
        ...,
        [-3.2310e-03, -1.6317e-03,  2.8610e-05,  ..., -1.0864e-02,
          6.7902e-03,  3.3607e-03],
        [-4.0321e-03, -5.7487e-03,  1.4496e-03,  ..., -3.1471e-05,
         -3.4332e-02, -4.4022e-03],
        [-8.3313e-03, -1.8635e-03,  2.8152e-03,  ..., -1.0948e-02,
         -6.1226e-04, -5.4817e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0195, -1.9883, -0.6143,  ...,  1.2490,  1.9795,  3.2598]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 20:13:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too ambitious, it is overambitious
If something is too shadowed, it is overshadowed
If something is too paid, it is overpaid
If something is too dressed, it is overdressed
If something is too exposed, it is overexposed
If something is too loaded, it is overloaded
If something is too protected, it is overprotected
If something is too represented, it is
2024-07-23 20:13:25 root INFO     [order_1_approx] starting weight calculation for If something is too loaded, it is overloaded
If something is too represented, it is overrepresented
If something is too dressed, it is overdressed
If something is too ambitious, it is overambitious
If something is too protected, it is overprotected
If something is too exposed, it is overexposed
If something is too paid, it is overpaid
If something is too shadowed, it is
2024-07-23 20:13:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 20:17:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1729, -0.7979,  1.0928,  ...,  1.1787,  0.3899,  0.3215],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4277,  0.9390,  0.5576,  ..., -0.2686, -1.9805,  1.9385],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-3.8414e-03, -1.5747e-02, -1.3885e-03,  ..., -1.9493e-03,
         -6.7253e-03, -5.7487e-03],
        [-5.3825e-03, -3.6888e-03,  2.6417e-03,  ..., -1.3103e-03,
         -5.4474e-03,  9.4452e-03],
        [-1.2856e-03,  3.3264e-03, -1.3474e-02,  ...,  1.1581e-02,
          1.2894e-02, -1.3351e-04],
        ...,
        [-1.3474e-02, -7.6523e-03, -1.5450e-03,  ...,  3.1776e-03,
          4.7722e-03,  1.4648e-03],
        [-4.2267e-03, -7.6294e-06,  5.0545e-05,  ..., -4.5929e-03,
         -2.5024e-02,  1.2062e-02],
        [-2.4834e-03,  1.4542e-02,  1.6518e-03,  ...,  9.3269e-04,
         -3.0479e-03,  2.7008e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.8271,  1.0420,  0.7178,  ...,  0.4170, -3.2695,  2.1523]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 20:17:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too loaded, it is overloaded
If something is too represented, it is overrepresented
If something is too dressed, it is overdressed
If something is too ambitious, it is overambitious
If something is too protected, it is overprotected
If something is too exposed, it is overexposed
If something is too paid, it is overpaid
If something is too shadowed, it is
2024-07-23 20:17:04 root INFO     total operator prediction time: 1809.6137564182281 seconds
2024-07-23 20:17:04 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on adj+ly_reg
2024-07-23 20:17:04 root INFO     building operator adj+ly_reg
2024-07-23 20:17:04 root INFO     [order_1_approx] starting weight calculation for The adjective form of typical is typically
The adjective form of additional is additionally
The adjective form of popular is popularly
The adjective form of financial is financially
The adjective form of similar is similarly
The adjective form of digital is digitally
The adjective form of apparent is apparently
The adjective form of actual is
2024-07-23 20:17:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 20:20:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5283,  1.0107,  1.1484,  ...,  0.2783,  0.9448, -0.1904],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9023,  2.8164,  3.6133,  ..., -3.2676,  3.4648,  0.9766],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0113, -0.0021,  0.0050,  ..., -0.0147, -0.0025, -0.0147],
        [ 0.0028,  0.0016,  0.0087,  ...,  0.0192, -0.0309,  0.0081],
        [-0.0006, -0.0117,  0.0078,  ..., -0.0045,  0.0082,  0.0051],
        ...,
        [-0.0154,  0.0026,  0.0040,  ..., -0.0031,  0.0048, -0.0226],
        [-0.0102, -0.0121,  0.0007,  ..., -0.0107, -0.0212,  0.0028],
        [-0.0112,  0.0145,  0.0039,  ..., -0.0123,  0.0189, -0.0011]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3516,  1.7227,  2.5391,  ..., -3.1348,  4.4922,  1.9141]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 20:20:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of typical is typically
The adjective form of additional is additionally
The adjective form of popular is popularly
The adjective form of financial is financially
The adjective form of similar is similarly
The adjective form of digital is digitally
The adjective form of apparent is apparently
The adjective form of actual is
2024-07-23 20:20:46 root INFO     [order_1_approx] starting weight calculation for The adjective form of typical is typically
The adjective form of popular is popularly
The adjective form of apparent is apparently
The adjective form of similar is similarly
The adjective form of financial is financially
The adjective form of digital is digitally
The adjective form of actual is actually
The adjective form of additional is
2024-07-23 20:20:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 20:24:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6514,  0.4839,  0.8340,  ..., -0.5225,  0.0380,  0.8999],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6440, -1.3174,  1.2461,  ..., -2.3633,  2.6641,  0.8418],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0012, -0.0147,  0.0033,  ..., -0.0206, -0.0040, -0.0149],
        [ 0.0003, -0.0125,  0.0117,  ...,  0.0086, -0.0184, -0.0046],
        [-0.0059,  0.0173, -0.0191,  ...,  0.0079,  0.0008, -0.0050],
        ...,
        [-0.0198,  0.0173, -0.0147,  ..., -0.0152,  0.0017, -0.0213],
        [-0.0053, -0.0219,  0.0052,  ..., -0.0135, -0.0210,  0.0036],
        [ 0.0083,  0.0205, -0.0162,  ..., -0.0035,  0.0105, -0.0135]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6367, -1.0820,  1.9580,  ..., -2.4863,  3.2324,  1.4707]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 20:24:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of typical is typically
The adjective form of popular is popularly
The adjective form of apparent is apparently
The adjective form of similar is similarly
The adjective form of financial is financially
The adjective form of digital is digitally
The adjective form of actual is actually
The adjective form of additional is
2024-07-23 20:24:35 root INFO     [order_1_approx] starting weight calculation for The adjective form of popular is popularly
The adjective form of actual is actually
The adjective form of additional is additionally
The adjective form of financial is financially
The adjective form of similar is similarly
The adjective form of digital is digitally
The adjective form of typical is typically
The adjective form of apparent is
2024-07-23 20:24:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 20:28:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4893,  0.9717, -0.1294,  ...,  0.5513,  0.6631, -0.1770],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.1523,  1.0127, -0.1592,  ...,  0.5361, -2.5898, -1.6797],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.0674e-02, -1.8539e-02,  2.0027e-03,  ..., -2.2842e-02,
         -8.1635e-03,  3.4752e-03],
        [-4.9210e-03,  4.8141e-03,  1.7548e-03,  ...,  3.3989e-03,
         -2.3743e-02, -1.1396e-03],
        [-1.6052e-02, -1.6022e-02, -1.1721e-03,  ..., -8.3542e-03,
          9.3842e-03,  1.6388e-02],
        ...,
        [-1.5274e-02, -5.7220e-04, -1.1421e-02,  ..., -5.9700e-03,
         -1.1215e-03, -1.1284e-02],
        [ 1.6079e-03, -1.8494e-02,  1.1162e-02,  ...,  9.9182e-04,
          4.3869e-05, -1.2451e-02],
        [ 2.1629e-03,  4.7150e-03,  3.3264e-03,  ..., -9.2926e-03,
          2.8290e-02, -1.2939e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.8945,  0.5000, -0.3032,  ...,  1.0107, -2.5098, -1.3193]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 20:28:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of popular is popularly
The adjective form of actual is actually
The adjective form of additional is additionally
The adjective form of financial is financially
The adjective form of similar is similarly
The adjective form of digital is digitally
The adjective form of typical is typically
The adjective form of apparent is
2024-07-23 20:28:24 root INFO     [order_1_approx] starting weight calculation for The adjective form of typical is typically
The adjective form of financial is financially
The adjective form of actual is actually
The adjective form of apparent is apparently
The adjective form of popular is popularly
The adjective form of similar is similarly
The adjective form of additional is additionally
The adjective form of digital is
2024-07-23 20:28:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 20:32:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5986, -0.4287,  0.2369,  ...,  1.2568,  0.0244, -0.3896],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2305,  1.2637, -0.7305,  ..., -0.9399,  1.7881,  2.0234],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0109, -0.0075, -0.0048,  ..., -0.0187,  0.0087,  0.0032],
        [-0.0141, -0.0213,  0.0062,  ...,  0.0064, -0.0155, -0.0108],
        [-0.0105, -0.0125,  0.0005,  ...,  0.0066,  0.0132, -0.0047],
        ...,
        [-0.0140, -0.0053, -0.0021,  ...,  0.0114, -0.0012,  0.0001],
        [ 0.0012, -0.0106,  0.0024,  ..., -0.0086,  0.0018, -0.0122],
        [ 0.0037,  0.0069, -0.0046,  ...,  0.0010,  0.0136, -0.0031]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1423,  0.8398, -1.0078,  ..., -1.2852,  2.2832,  2.7422]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 20:32:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of typical is typically
The adjective form of financial is financially
The adjective form of actual is actually
The adjective form of apparent is apparently
The adjective form of popular is popularly
The adjective form of similar is similarly
The adjective form of additional is additionally
The adjective form of digital is
2024-07-23 20:32:12 root INFO     [order_1_approx] starting weight calculation for The adjective form of digital is digitally
The adjective form of actual is actually
The adjective form of typical is typically
The adjective form of popular is popularly
The adjective form of apparent is apparently
The adjective form of additional is additionally
The adjective form of similar is similarly
The adjective form of financial is
2024-07-23 20:32:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 20:35:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.8486, 1.0557, 0.1858,  ..., 0.5410, 0.3882, 0.4434], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3672,  2.9004, -1.4062,  ..., -1.4395,  2.5234,  0.7344],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0210, -0.0144,  0.0068,  ..., -0.0066,  0.0060, -0.0146],
        [ 0.0043, -0.0181,  0.0172,  ...,  0.0087, -0.0465, -0.0045],
        [-0.0008, -0.0023, -0.0016,  ...,  0.0054,  0.0143,  0.0096],
        ...,
        [-0.0017, -0.0071, -0.0012,  ...,  0.0011,  0.0010, -0.0271],
        [ 0.0112, -0.0091,  0.0074,  ..., -0.0111,  0.0046,  0.0011],
        [-0.0082,  0.0098, -0.0196,  ..., -0.0116,  0.0254, -0.0049]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3027,  1.7021, -1.3887,  ..., -1.2734,  2.5840,  1.0625]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 20:35:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of digital is digitally
The adjective form of actual is actually
The adjective form of typical is typically
The adjective form of popular is popularly
The adjective form of apparent is apparently
The adjective form of additional is additionally
The adjective form of similar is similarly
The adjective form of financial is
2024-07-23 20:35:59 root INFO     [order_1_approx] starting weight calculation for The adjective form of similar is similarly
The adjective form of apparent is apparently
The adjective form of typical is typically
The adjective form of financial is financially
The adjective form of additional is additionally
The adjective form of digital is digitally
The adjective form of actual is actually
The adjective form of popular is
2024-07-23 20:35:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 20:39:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3127,  1.4102,  0.7056,  ...,  0.7080,  0.9160,  0.3772],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5977, -0.0764,  0.3574,  ...,  1.6533,  1.1982, -4.7930],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0048, -0.0119,  0.0208,  ..., -0.0060, -0.0017, -0.0040],
        [-0.0032, -0.0046,  0.0140,  ...,  0.0030, -0.0175, -0.0014],
        [-0.0109, -0.0233,  0.0009,  ...,  0.0066,  0.0121, -0.0004],
        ...,
        [-0.0157, -0.0144,  0.0049,  ..., -0.0077, -0.0121,  0.0020],
        [-0.0106,  0.0018, -0.0028,  ..., -0.0059,  0.0006, -0.0099],
        [ 0.0013,  0.0190, -0.0053,  ..., -0.0037,  0.0243, -0.0158]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.8447, -0.0659, -0.6328,  ...,  0.9341,  1.3643, -4.9766]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 20:39:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of similar is similarly
The adjective form of apparent is apparently
The adjective form of typical is typically
The adjective form of financial is financially
The adjective form of additional is additionally
The adjective form of digital is digitally
The adjective form of actual is actually
The adjective form of popular is
2024-07-23 20:39:47 root INFO     [order_1_approx] starting weight calculation for The adjective form of typical is typically
The adjective form of popular is popularly
The adjective form of actual is actually
The adjective form of additional is additionally
The adjective form of digital is digitally
The adjective form of apparent is apparently
The adjective form of financial is financially
The adjective form of similar is
2024-07-23 20:39:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 20:43:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6069,  1.1084,  0.9629,  ...,  0.3408,  0.8496,  0.1765],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9531,  2.9082, -0.4033,  ...,  2.4805, -2.2168, -2.0391],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0111, -0.0126,  0.0109,  ..., -0.0153,  0.0011, -0.0179],
        [-0.0211,  0.0076, -0.0018,  ..., -0.0024, -0.0187,  0.0072],
        [ 0.0045, -0.0222, -0.0144,  ...,  0.0083,  0.0207, -0.0032],
        ...,
        [-0.0245, -0.0085, -0.0034,  ..., -0.0122, -0.0070,  0.0020],
        [ 0.0128, -0.0005,  0.0148,  ..., -0.0080, -0.0241, -0.0168],
        [ 0.0058,  0.0242, -0.0044,  ..., -0.0072,  0.0255, -0.0298]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4209,  3.1191, -0.0974,  ...,  2.2109, -2.1445, -1.7822]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 20:43:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of typical is typically
The adjective form of popular is popularly
The adjective form of actual is actually
The adjective form of additional is additionally
The adjective form of digital is digitally
The adjective form of apparent is apparently
The adjective form of financial is financially
The adjective form of similar is
2024-07-23 20:43:35 root INFO     [order_1_approx] starting weight calculation for The adjective form of actual is actually
The adjective form of apparent is apparently
The adjective form of additional is additionally
The adjective form of similar is similarly
The adjective form of digital is digitally
The adjective form of financial is financially
The adjective form of popular is popularly
The adjective form of typical is
2024-07-23 20:43:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 20:47:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7061,  1.9814, -0.1188,  ...,  0.2593,  1.2422,  0.8843],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6963,  1.0176,  3.4531,  ...,  3.6016, -0.4922,  1.1738],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0058, -0.0177, -0.0008,  ..., -0.0069,  0.0076, -0.0095],
        [-0.0166, -0.0074,  0.0023,  ...,  0.0020, -0.0208,  0.0140],
        [ 0.0056, -0.0199, -0.0051,  ..., -0.0143, -0.0071,  0.0171],
        ...,
        [-0.0139,  0.0011,  0.0076,  ..., -0.0129, -0.0031,  0.0040],
        [ 0.0168, -0.0017,  0.0078,  ...,  0.0083, -0.0063, -0.0319],
        [ 0.0126,  0.0128, -0.0173,  ...,  0.0032,  0.0346, -0.0327]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4766,  1.3184,  3.3496,  ...,  3.2578, -1.0986,  1.5732]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 20:47:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of actual is actually
The adjective form of apparent is apparently
The adjective form of additional is additionally
The adjective form of similar is similarly
The adjective form of digital is digitally
The adjective form of financial is financially
The adjective form of popular is popularly
The adjective form of typical is
2024-07-23 20:47:24 root INFO     total operator prediction time: 1819.5721242427826 seconds
2024-07-23 20:47:24 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+tion_irreg
2024-07-23 20:47:24 root INFO     building operator verb+tion_irreg
2024-07-23 20:47:24 root INFO     [order_1_approx] starting weight calculation for To minimize results in minimization
To characterize results in characterization
To stabilize results in stabilization
To expire results in expiration
To optimize results in optimization
To standardize results in standardization
To privatize results in privatization
To admire results in
2024-07-23 20:47:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 20:51:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5166, -0.6826,  0.4077,  ..., -0.7383,  0.7573, -0.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.5449,  1.6562, -3.1602,  ..., -0.2520, -0.2476,  4.0234],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0053, -0.0094,  0.0077,  ..., -0.0106, -0.0039, -0.0210],
        [-0.0047, -0.0101,  0.0019,  ...,  0.0099, -0.0043,  0.0082],
        [ 0.0032,  0.0191, -0.0201,  ...,  0.0066,  0.0080,  0.0050],
        ...,
        [-0.0273, -0.0244, -0.0075,  ..., -0.0189, -0.0007, -0.0258],
        [ 0.0051,  0.0038,  0.0011,  ...,  0.0046, -0.0217, -0.0028],
        [-0.0127,  0.0232,  0.0124,  ..., -0.0130,  0.0023,  0.0027]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4492,  2.7930, -4.0430,  ..., -0.1609, -0.1107,  4.8672]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 20:51:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To minimize results in minimization
To characterize results in characterization
To stabilize results in stabilization
To expire results in expiration
To optimize results in optimization
To standardize results in standardization
To privatize results in privatization
To admire results in
2024-07-23 20:51:15 root INFO     [order_1_approx] starting weight calculation for To stabilize results in stabilization
To standardize results in standardization
To privatize results in privatization
To minimize results in minimization
To admire results in admiration
To optimize results in optimization
To expire results in expiration
To characterize results in
2024-07-23 20:51:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 20:55:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.6465, -0.3384,  0.1968,  ..., -0.3577, -0.3938, -0.2656],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.0117, -0.2451, -3.9141,  ..., -0.1895, -1.0840,  4.3672],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0085, -0.0167, -0.0029,  ..., -0.0071,  0.0013, -0.0145],
        [-0.0043, -0.0121,  0.0086,  ..., -0.0123, -0.0071, -0.0067],
        [-0.0005, -0.0034, -0.0149,  ..., -0.0055,  0.0030, -0.0039],
        ...,
        [-0.0132, -0.0238, -0.0226,  ..., -0.0126, -0.0004, -0.0162],
        [-0.0047,  0.0091,  0.0116,  ..., -0.0159, -0.0077,  0.0089],
        [-0.0192,  0.0111, -0.0055,  ..., -0.0031,  0.0042,  0.0060]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.6172, -0.5293, -4.6797,  ..., -0.6094, -1.3877,  4.5430]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 20:55:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To stabilize results in stabilization
To standardize results in standardization
To privatize results in privatization
To minimize results in minimization
To admire results in admiration
To optimize results in optimization
To expire results in expiration
To characterize results in
2024-07-23 20:55:06 root INFO     [order_1_approx] starting weight calculation for To minimize results in minimization
To standardize results in standardization
To characterize results in characterization
To privatize results in privatization
To admire results in admiration
To optimize results in optimization
To stabilize results in stabilization
To expire results in
2024-07-23 20:55:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 20:58:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.8760, -1.1074, -0.0518,  ..., -0.0801,  1.6172,  1.1934],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9307,  0.9854, -2.7363,  ..., -1.1348,  1.3320,  2.0195],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0042, -0.0095,  0.0013,  ..., -0.0089, -0.0174, -0.0269],
        [ 0.0093, -0.0090,  0.0183,  ...,  0.0006,  0.0014, -0.0012],
        [-0.0096, -0.0045, -0.0137,  ..., -0.0001, -0.0036,  0.0005],
        ...,
        [ 0.0016, -0.0208, -0.0191,  ..., -0.0122, -0.0130, -0.0156],
        [-0.0191,  0.0043,  0.0142,  ...,  0.0137, -0.0175, -0.0075],
        [-0.0052,  0.0209,  0.0114,  ..., -0.0056, -0.0049, -0.0205]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2871,  1.5000, -3.1250,  ..., -1.7852,  1.0488,  2.5859]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 20:58:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To minimize results in minimization
To standardize results in standardization
To characterize results in characterization
To privatize results in privatization
To admire results in admiration
To optimize results in optimization
To stabilize results in stabilization
To expire results in
2024-07-23 20:58:55 root INFO     [order_1_approx] starting weight calculation for To characterize results in characterization
To standardize results in standardization
To optimize results in optimization
To privatize results in privatization
To expire results in expiration
To admire results in admiration
To stabilize results in stabilization
To minimize results in
2024-07-23 20:58:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 21:02:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5806, -0.8418,  0.5049,  ..., -1.5654, -0.3428, -0.8291],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.0547, -0.8413, -5.3906,  ...,  0.6050,  1.1191,  5.6211],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0172, -0.0099,  0.0078,  ..., -0.0002,  0.0042, -0.0142],
        [ 0.0009,  0.0115,  0.0192,  ...,  0.0177, -0.0033,  0.0177],
        [ 0.0034,  0.0038,  0.0157,  ..., -0.0058,  0.0023, -0.0062],
        ...,
        [-0.0294, -0.0067,  0.0048,  ...,  0.0044, -0.0081, -0.0084],
        [ 0.0044,  0.0075,  0.0171,  ..., -0.0112, -0.0187,  0.0224],
        [ 0.0016,  0.0216, -0.0033,  ..., -0.0052,  0.0044,  0.0110]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.9727, -0.5259, -6.1367,  ...,  0.2590,  1.1855,  5.2070]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 21:02:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To characterize results in characterization
To standardize results in standardization
To optimize results in optimization
To privatize results in privatization
To expire results in expiration
To admire results in admiration
To stabilize results in stabilization
To minimize results in
2024-07-23 21:02:43 root INFO     [order_1_approx] starting weight calculation for To stabilize results in stabilization
To standardize results in standardization
To minimize results in minimization
To privatize results in privatization
To expire results in expiration
To characterize results in characterization
To admire results in admiration
To optimize results in
2024-07-23 21:02:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 21:06:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 9.9414e-01,  3.0420e-01, -9.7656e-04,  ..., -6.1035e-01,
        -2.5928e-01, -1.2441e+00], device='cuda:0', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.1484,  0.0303, -5.1445,  ...,  0.9595, -0.9878,  4.3281],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0022, -0.0014, -0.0013,  ..., -0.0166, -0.0014, -0.0042],
        [-0.0068, -0.0105,  0.0001,  ...,  0.0092, -0.0035,  0.0067],
        [-0.0051,  0.0063, -0.0047,  ..., -0.0084,  0.0173, -0.0059],
        ...,
        [-0.0119, -0.0096, -0.0121,  ...,  0.0004, -0.0009, -0.0161],
        [-0.0153, -0.0006,  0.0139,  ..., -0.0079, -0.0175, -0.0009],
        [-0.0104,  0.0105,  0.0056,  ..., -0.0016,  0.0065, -0.0017]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.8047,  0.5845, -6.0859,  ...,  0.3550, -1.1006,  4.1055]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 21:06:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To stabilize results in stabilization
To standardize results in standardization
To minimize results in minimization
To privatize results in privatization
To expire results in expiration
To characterize results in characterization
To admire results in admiration
To optimize results in
2024-07-23 21:06:33 root INFO     [order_1_approx] starting weight calculation for To minimize results in minimization
To standardize results in standardization
To characterize results in characterization
To expire results in expiration
To optimize results in optimization
To admire results in admiration
To stabilize results in stabilization
To privatize results in
2024-07-23 21:06:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 21:10:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0706, -1.0039,  0.6846,  ...,  0.3923,  0.0430,  0.1865],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2393, -0.1797, -1.8672,  ...,  0.9658, -0.8267,  1.1650],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0046, -0.0121,  0.0088,  ...,  0.0048,  0.0013, -0.0114],
        [ 0.0104,  0.0022,  0.0095,  ...,  0.0112, -0.0105, -0.0035],
        [ 0.0005,  0.0095, -0.0130,  ..., -0.0005,  0.0032, -0.0028],
        ...,
        [-0.0085, -0.0151, -0.0073,  ...,  0.0022, -0.0046, -0.0041],
        [-0.0111,  0.0032,  0.0130,  ..., -0.0092, -0.0069,  0.0041],
        [-0.0086,  0.0132,  0.0031,  ..., -0.0049,  0.0129, -0.0035]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1062, -0.5010, -1.7920,  ...,  0.8423, -0.9512,  1.5752]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 21:10:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To minimize results in minimization
To standardize results in standardization
To characterize results in characterization
To expire results in expiration
To optimize results in optimization
To admire results in admiration
To stabilize results in stabilization
To privatize results in
2024-07-23 21:10:24 root INFO     [order_1_approx] starting weight calculation for To privatize results in privatization
To expire results in expiration
To admire results in admiration
To optimize results in optimization
To standardize results in standardization
To characterize results in characterization
To minimize results in minimization
To stabilize results in
2024-07-23 21:10:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 21:14:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7241,  0.3242,  0.5308,  ..., -0.2339, -0.9385, -0.8247],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.3770,  1.8066, -2.7910,  ...,  0.2271,  1.9756,  5.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0030, -0.0103, -0.0080,  ...,  0.0022,  0.0056, -0.0164],
        [ 0.0125,  0.0042, -0.0021,  ...,  0.0047, -0.0195,  0.0031],
        [-0.0033, -0.0047, -0.0069,  ...,  0.0019,  0.0083,  0.0026],
        ...,
        [-0.0222, -0.0091, -0.0162,  ..., -0.0046,  0.0008, -0.0111],
        [-0.0207, -0.0100,  0.0015,  ..., -0.0073, -0.0061, -0.0017],
        [ 0.0047,  0.0224, -0.0042,  ..., -0.0059,  0.0027, -0.0043]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.3613,  1.8340, -3.0996,  ..., -0.1711,  1.7549,  5.0156]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 21:14:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To privatize results in privatization
To expire results in expiration
To admire results in admiration
To optimize results in optimization
To standardize results in standardization
To characterize results in characterization
To minimize results in minimization
To stabilize results in
2024-07-23 21:14:14 root INFO     [order_1_approx] starting weight calculation for To expire results in expiration
To stabilize results in stabilization
To minimize results in minimization
To characterize results in characterization
To privatize results in privatization
To admire results in admiration
To optimize results in optimization
To standardize results in
2024-07-23 21:14:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 21:18:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9355, -0.0159,  0.5176,  ...,  0.4465, -0.0513, -0.4170],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0508,  0.7876, -2.0645,  ...,  0.2451,  5.3516,  5.4258],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0005, -0.0129,  0.0064,  ...,  0.0034, -0.0012, -0.0132],
        [-0.0075, -0.0068,  0.0049,  ...,  0.0040, -0.0024, -0.0006],
        [ 0.0023, -0.0087,  0.0014,  ..., -0.0057,  0.0027,  0.0034],
        ...,
        [-0.0045, -0.0105, -0.0126,  ...,  0.0041, -0.0026, -0.0037],
        [-0.0142, -0.0005,  0.0134,  ..., -0.0034, -0.0057,  0.0008],
        [-0.0121,  0.0195,  0.0122,  ...,  0.0004, -0.0041, -0.0015]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.0996,  1.1113, -1.5762,  ...,  0.1049,  5.7344,  5.4844]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 21:18:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To expire results in expiration
To stabilize results in stabilization
To minimize results in minimization
To characterize results in characterization
To privatize results in privatization
To admire results in admiration
To optimize results in optimization
To standardize results in
2024-07-23 21:18:03 root INFO     total operator prediction time: 1839.6929950714111 seconds
2024-07-23 21:18:03 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+able_reg
2024-07-23 21:18:03 root INFO     building operator verb+able_reg
2024-07-23 21:18:04 root INFO     [order_1_approx] starting weight calculation for If you can renew something, that thing is renewable
If you can learn something, that thing is learnable
If you can expand something, that thing is expandable
If you can represent something, that thing is representable
If you can survive something, that thing is survivable
If you can prevent something, that thing is preventable
If you can sustain something, that thing is sustainable
If you can discover something, that thing is
2024-07-23 21:18:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 21:21:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0074,  0.6914, -0.8970,  ...,  0.1266,  1.3936,  0.1841],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.4961,  1.5889, -2.4219,  ..., -3.6953,  0.9668,  3.7422],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0028, -0.0088, -0.0005,  ...,  0.0052,  0.0020, -0.0019],
        [-0.0060, -0.0002,  0.0028,  ..., -0.0071,  0.0041,  0.0080],
        [ 0.0061,  0.0016, -0.0067,  ...,  0.0090,  0.0133, -0.0073],
        ...,
        [-0.0018, -0.0071, -0.0084,  ..., -0.0043, -0.0056, -0.0060],
        [-0.0141,  0.0063,  0.0107,  ..., -0.0051, -0.0178,  0.0004],
        [-0.0033,  0.0052,  0.0020,  ..., -0.0076,  0.0242, -0.0258]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.8770,  1.3809, -2.7305,  ..., -3.1133,  1.1455,  3.8809]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 21:21:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can renew something, that thing is renewable
If you can learn something, that thing is learnable
If you can expand something, that thing is expandable
If you can represent something, that thing is representable
If you can survive something, that thing is survivable
If you can prevent something, that thing is preventable
If you can sustain something, that thing is sustainable
If you can discover something, that thing is
2024-07-23 21:21:48 root INFO     [order_1_approx] starting weight calculation for If you can discover something, that thing is discoverable
If you can survive something, that thing is survivable
If you can learn something, that thing is learnable
If you can represent something, that thing is representable
If you can sustain something, that thing is sustainable
If you can renew something, that thing is renewable
If you can prevent something, that thing is preventable
If you can expand something, that thing is
2024-07-23 21:21:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 21:25:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5508,  0.6582, -0.0459,  ..., -0.0269,  0.7393,  0.0679],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.7754,  1.3936, -3.9688,  ..., -2.6211,  2.1836,  0.9502],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0012, -0.0097, -0.0005,  ...,  0.0038, -0.0049, -0.0055],
        [ 0.0019, -0.0040,  0.0037,  ..., -0.0024, -0.0016, -0.0056],
        [ 0.0014,  0.0039, -0.0051,  ...,  0.0080,  0.0146, -0.0060],
        ...,
        [-0.0160, -0.0077, -0.0070,  ..., -0.0087,  0.0050, -0.0009],
        [-0.0163, -0.0057, -0.0028,  ..., -0.0015, -0.0224,  0.0056],
        [-0.0010,  0.0030,  0.0072,  ..., -0.0192,  0.0153, -0.0256]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.4980,  1.4727, -3.8926,  ..., -2.6113,  2.0645,  0.5420]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 21:25:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can discover something, that thing is discoverable
If you can survive something, that thing is survivable
If you can learn something, that thing is learnable
If you can represent something, that thing is representable
If you can sustain something, that thing is sustainable
If you can renew something, that thing is renewable
If you can prevent something, that thing is preventable
If you can expand something, that thing is
2024-07-23 21:25:36 root INFO     [order_1_approx] starting weight calculation for If you can expand something, that thing is expandable
If you can discover something, that thing is discoverable
If you can sustain something, that thing is sustainable
If you can represent something, that thing is representable
If you can prevent something, that thing is preventable
If you can survive something, that thing is survivable
If you can renew something, that thing is renewable
If you can learn something, that thing is
2024-07-23 21:25:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 21:29:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2915,  0.5181,  0.7192,  ..., -0.3484,  1.5352, -0.0281],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.7969,  5.5469, -4.3828,  ..., -1.4795,  2.1230,  0.4128],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0030, -0.0100, -0.0045,  ...,  0.0074, -0.0007, -0.0057],
        [ 0.0021, -0.0110,  0.0040,  ..., -0.0142,  0.0064,  0.0052],
        [-0.0148,  0.0178, -0.0100,  ...,  0.0082,  0.0016, -0.0051],
        ...,
        [-0.0118, -0.0039, -0.0088,  ..., -0.0072, -0.0059, -0.0065],
        [ 0.0040, -0.0029, -0.0009,  ..., -0.0161, -0.0174, -0.0028],
        [-0.0019,  0.0078, -0.0055,  ..., -0.0131,  0.0121, -0.0222]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.1758,  4.9531, -4.0156,  ..., -0.7017,  2.1426,  0.6006]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 21:29:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can expand something, that thing is expandable
If you can discover something, that thing is discoverable
If you can sustain something, that thing is sustainable
If you can represent something, that thing is representable
If you can prevent something, that thing is preventable
If you can survive something, that thing is survivable
If you can renew something, that thing is renewable
If you can learn something, that thing is
2024-07-23 21:29:24 root INFO     [order_1_approx] starting weight calculation for If you can discover something, that thing is discoverable
If you can sustain something, that thing is sustainable
If you can expand something, that thing is expandable
If you can survive something, that thing is survivable
If you can renew something, that thing is renewable
If you can represent something, that thing is representable
If you can learn something, that thing is learnable
If you can prevent something, that thing is
2024-07-23 21:29:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 21:33:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2988,  1.5918, -0.3237,  ...,  0.7852,  0.8281, -0.2886],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1719,  0.8555, -5.2344,  ...,  2.2051,  5.9023,  5.1133],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0041, -0.0092,  0.0163,  ...,  0.0135, -0.0068, -0.0075],
        [-0.0051, -0.0006,  0.0001,  ..., -0.0106, -0.0068, -0.0132],
        [-0.0018,  0.0033, -0.0041,  ...,  0.0061, -0.0023,  0.0063],
        ...,
        [-0.0266, -0.0223, -0.0012,  ..., -0.0034,  0.0045, -0.0145],
        [ 0.0033, -0.0013,  0.0020,  ..., -0.0241, -0.0397,  0.0032],
        [-0.0152,  0.0103,  0.0139,  ..., -0.0121,  0.0024, -0.0536]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.8438,  1.0186, -5.2070,  ...,  2.3516,  5.7930,  4.8672]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 21:33:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can discover something, that thing is discoverable
If you can sustain something, that thing is sustainable
If you can expand something, that thing is expandable
If you can survive something, that thing is survivable
If you can renew something, that thing is renewable
If you can represent something, that thing is representable
If you can learn something, that thing is learnable
If you can prevent something, that thing is
2024-07-23 21:33:11 root INFO     [order_1_approx] starting weight calculation for If you can represent something, that thing is representable
If you can discover something, that thing is discoverable
If you can learn something, that thing is learnable
If you can survive something, that thing is survivable
If you can sustain something, that thing is sustainable
If you can expand something, that thing is expandable
If you can prevent something, that thing is preventable
If you can renew something, that thing is
2024-07-23 21:33:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 21:36:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5176, -0.0555,  0.3369,  ..., -0.2271,  0.6240,  0.2153],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0605,  0.9492, -5.8125,  ..., -0.1455,  2.1660,  3.7246],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0102, -0.0136, -0.0016,  ...,  0.0014,  0.0027, -0.0095],
        [-0.0037,  0.0028, -0.0087,  ..., -0.0009,  0.0079,  0.0038],
        [ 0.0042,  0.0106, -0.0060,  ...,  0.0004,  0.0165, -0.0031],
        ...,
        [-0.0019, -0.0046, -0.0049,  ..., -0.0098,  0.0120, -0.0163],
        [-0.0125, -0.0121,  0.0015,  ..., -0.0128, -0.0143,  0.0156],
        [-0.0106,  0.0031,  0.0143,  ..., -0.0220,  0.0158, -0.0350]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9531,  1.4072, -5.2656,  ..., -0.4658,  2.2656,  3.8848]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 21:36:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can represent something, that thing is representable
If you can discover something, that thing is discoverable
If you can learn something, that thing is learnable
If you can survive something, that thing is survivable
If you can sustain something, that thing is sustainable
If you can expand something, that thing is expandable
If you can prevent something, that thing is preventable
If you can renew something, that thing is
2024-07-23 21:36:57 root INFO     [order_1_approx] starting weight calculation for If you can survive something, that thing is survivable
If you can expand something, that thing is expandable
If you can prevent something, that thing is preventable
If you can discover something, that thing is discoverable
If you can learn something, that thing is learnable
If you can renew something, that thing is renewable
If you can sustain something, that thing is sustainable
If you can represent something, that thing is
2024-07-23 21:36:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 21:40:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1453,  0.2822,  0.1063,  ..., -0.9092,  0.9043,  0.6768],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7939,  2.1582, -3.4375,  ..., -1.7676,  3.1445,  3.3105],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0071, -0.0098, -0.0051,  ...,  0.0089,  0.0019, -0.0034],
        [ 0.0023, -0.0001,  0.0027,  ..., -0.0112, -0.0033, -0.0007],
        [ 0.0012,  0.0046, -0.0071,  ...,  0.0066,  0.0050, -0.0025],
        ...,
        [-0.0107, -0.0150,  0.0053,  ...,  0.0033, -0.0022, -0.0090],
        [-0.0133, -0.0130, -0.0051,  ..., -0.0155, -0.0253,  0.0014],
        [-0.0070,  0.0034, -0.0062,  ..., -0.0131,  0.0033, -0.0203]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.8408,  2.1094, -3.6016,  ..., -1.2119,  3.0176,  2.9609]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 21:40:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can survive something, that thing is survivable
If you can expand something, that thing is expandable
If you can prevent something, that thing is preventable
If you can discover something, that thing is discoverable
If you can learn something, that thing is learnable
If you can renew something, that thing is renewable
If you can sustain something, that thing is sustainable
If you can represent something, that thing is
2024-07-23 21:40:44 root INFO     [order_1_approx] starting weight calculation for If you can learn something, that thing is learnable
If you can expand something, that thing is expandable
If you can sustain something, that thing is sustainable
If you can renew something, that thing is renewable
If you can discover something, that thing is discoverable
If you can represent something, that thing is representable
If you can prevent something, that thing is preventable
If you can survive something, that thing is
2024-07-23 21:40:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 21:44:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5464,  1.2295,  0.9634,  ...,  1.9639,  1.4043,  0.0190],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.5059,  3.0312, -2.0137,  ..., -2.9883,  3.2852,  5.5938],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0057, -0.0066, -0.0058,  ..., -0.0058, -0.0056, -0.0039],
        [-0.0001, -0.0015,  0.0022,  ..., -0.0010, -0.0075,  0.0132],
        [ 0.0091, -0.0026, -0.0010,  ...,  0.0086,  0.0098,  0.0001],
        ...,
        [-0.0011,  0.0028, -0.0010,  ..., -0.0037,  0.0050, -0.0035],
        [-0.0070, -0.0078, -0.0066,  ..., -0.0012, -0.0316,  0.0138],
        [-0.0080,  0.0109,  0.0081,  ..., -0.0129,  0.0137, -0.0353]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.1387,  2.9590, -1.5312,  ..., -3.2324,  2.6406,  5.4414]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 21:44:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can learn something, that thing is learnable
If you can expand something, that thing is expandable
If you can sustain something, that thing is sustainable
If you can renew something, that thing is renewable
If you can discover something, that thing is discoverable
If you can represent something, that thing is representable
If you can prevent something, that thing is preventable
If you can survive something, that thing is
2024-07-23 21:44:31 root INFO     [order_1_approx] starting weight calculation for If you can discover something, that thing is discoverable
If you can prevent something, that thing is preventable
If you can survive something, that thing is survivable
If you can expand something, that thing is expandable
If you can renew something, that thing is renewable
If you can represent something, that thing is representable
If you can learn something, that thing is learnable
If you can sustain something, that thing is
2024-07-23 21:44:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 21:48:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0680,  0.9604,  0.5942,  ...,  1.0098,  1.3682, -0.5674],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.0879,  2.7051, -3.6055,  ...,  1.2051,  0.9824,  5.4180],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.4872e-03, -5.3406e-05, -1.4587e-02,  ...,  2.1172e-03,
          9.2163e-03, -1.5884e-02],
        [-2.9354e-03, -1.7281e-03, -1.1673e-02,  ..., -1.7258e-02,
          4.7913e-03, -1.0590e-02],
        [ 5.3253e-03,  1.0452e-02, -1.0254e-02,  ...,  1.1459e-02,
          6.6109e-03, -5.0545e-03],
        ...,
        [-2.6535e-02, -9.4223e-03, -4.4479e-03,  ..., -1.2573e-02,
          6.9733e-03, -8.3923e-05],
        [-2.3651e-03, -1.3268e-02, -7.1106e-03,  ..., -4.0359e-03,
         -1.6144e-02,  8.9874e-03],
        [-9.8877e-03,  1.8417e-02,  2.5368e-03,  ..., -1.5251e-02,
          2.3743e-02, -3.8940e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.0059,  2.1914, -3.4805,  ...,  1.1836,  0.1934,  5.9805]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 21:48:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can discover something, that thing is discoverable
If you can prevent something, that thing is preventable
If you can survive something, that thing is survivable
If you can expand something, that thing is expandable
If you can renew something, that thing is renewable
If you can represent something, that thing is representable
If you can learn something, that thing is learnable
If you can sustain something, that thing is
2024-07-23 21:48:19 root INFO     total operator prediction time: 1815.9979319572449 seconds
2024-07-23 21:48:19 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on un+adj_reg
2024-07-23 21:48:19 root INFO     building operator un+adj_reg
2024-07-23 21:48:19 root INFO     [order_1_approx] starting weight calculation for The opposite of related is unrelated
The opposite of reliable is unreliable
The opposite of suitable is unsuitable
The opposite of pleasant is unpleasant
The opposite of successful is unsuccessful
The opposite of sustainable is unsustainable
The opposite of intended is unintended
The opposite of conscious is
2024-07-23 21:48:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 21:52:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.7959,  1.3711, -0.3877,  ...,  1.0166,  1.6914, -1.4570],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.9688, -0.9878, -0.0874,  ..., -3.3438,  1.7246, -1.3691],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0019,  0.0060, -0.0016,  ..., -0.0114, -0.0034, -0.0183],
        [ 0.0070, -0.0139,  0.0137,  ...,  0.0028, -0.0045,  0.0066],
        [-0.0083, -0.0004, -0.0403,  ...,  0.0089, -0.0026,  0.0155],
        ...,
        [ 0.0049, -0.0137, -0.0125,  ..., -0.0371,  0.0129,  0.0246],
        [ 0.0041, -0.0056,  0.0145,  ..., -0.0128, -0.0247, -0.0029],
        [-0.0055,  0.0086, -0.0032,  ..., -0.0063,  0.0074, -0.0196]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.7031, -1.0596, -0.2009,  ..., -2.7637,  0.8911, -1.7900]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 21:52:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of related is unrelated
The opposite of reliable is unreliable
The opposite of suitable is unsuitable
The opposite of pleasant is unpleasant
The opposite of successful is unsuccessful
The opposite of sustainable is unsustainable
The opposite of intended is unintended
The opposite of conscious is
2024-07-23 21:52:11 root INFO     [order_1_approx] starting weight calculation for The opposite of reliable is unreliable
The opposite of conscious is unconscious
The opposite of successful is unsuccessful
The opposite of related is unrelated
The opposite of pleasant is unpleasant
The opposite of sustainable is unsustainable
The opposite of suitable is unsuitable
The opposite of intended is
2024-07-23 21:52:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 21:56:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9023,  1.4775, -0.9233,  ...,  0.4226,  1.6680, -0.2104],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6660, -0.3560, -0.6221,  ...,  3.3535, -0.8926,  1.6289],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-4.7455e-03,  7.9498e-03, -3.7251e-03,  ..., -1.1536e-02,
         -8.7585e-03, -1.5617e-02],
        [-1.6571e-02, -1.7197e-02,  5.2032e-03,  ..., -1.3557e-02,
         -9.6359e-03, -1.9150e-03],
        [-6.1035e-05, -1.8768e-03, -2.6215e-02,  ..., -1.1627e-02,
         -1.0353e-02,  1.3657e-02],
        ...,
        [-1.0422e-02, -7.5073e-03,  7.4425e-03,  ..., -2.2308e-02,
          2.7084e-03,  2.3468e-02],
        [-1.1223e-02, -7.2174e-03,  1.5030e-02,  ...,  3.6469e-03,
         -1.0651e-02, -8.0261e-03],
        [-3.8605e-03,  8.7128e-03, -2.4612e-02,  ..., -2.3285e-02,
          1.1536e-02, -2.2217e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.4189, -0.3477,  0.1260,  ...,  2.9531, -0.3887,  1.3047]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 21:56:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of reliable is unreliable
The opposite of conscious is unconscious
The opposite of successful is unsuccessful
The opposite of related is unrelated
The opposite of pleasant is unpleasant
The opposite of sustainable is unsustainable
The opposite of suitable is unsuitable
The opposite of intended is
2024-07-23 21:56:02 root INFO     [order_1_approx] starting weight calculation for The opposite of reliable is unreliable
The opposite of intended is unintended
The opposite of sustainable is unsustainable
The opposite of conscious is unconscious
The opposite of suitable is unsuitable
The opposite of related is unrelated
The opposite of successful is unsuccessful
The opposite of pleasant is
2024-07-23 21:56:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 21:59:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2617, -0.1313, -2.2031,  ..., -0.3721,  2.0508, -0.4067],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.9531, -1.0586, -1.2168,  ...,  3.9023, -0.4375,  1.1230],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0014, -0.0030, -0.0138,  ...,  0.0032, -0.0009, -0.0161],
        [ 0.0065, -0.0253, -0.0006,  ..., -0.0097, -0.0032, -0.0181],
        [ 0.0111, -0.0075, -0.0325,  ..., -0.0026,  0.0168,  0.0235],
        ...,
        [-0.0054, -0.0106,  0.0093,  ...,  0.0043, -0.0090,  0.0104],
        [-0.0206, -0.0071, -0.0007,  ..., -0.0126, -0.0142, -0.0141],
        [-0.0015, -0.0057,  0.0043,  ...,  0.0045,  0.0129, -0.0064]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.5078, -0.7603, -1.4688,  ...,  2.8398, -1.2012,  1.6084]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 21:59:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of reliable is unreliable
The opposite of intended is unintended
The opposite of sustainable is unsustainable
The opposite of conscious is unconscious
The opposite of suitable is unsuitable
The opposite of related is unrelated
The opposite of successful is unsuccessful
The opposite of pleasant is
2024-07-23 21:59:52 root INFO     [order_1_approx] starting weight calculation for The opposite of intended is unintended
The opposite of reliable is unreliable
The opposite of successful is unsuccessful
The opposite of pleasant is unpleasant
The opposite of sustainable is unsustainable
The opposite of suitable is unsuitable
The opposite of conscious is unconscious
The opposite of related is
2024-07-23 21:59:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 22:03:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1465,  0.8120,  0.0916,  ..., -0.3430,  1.1475, -0.4224],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.6055, -3.1113, -1.7490,  ...,  0.4883,  3.5781,  0.4043],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0208, -0.0186,  0.0177,  ..., -0.0144, -0.0080, -0.0121],
        [ 0.0009, -0.0062, -0.0119,  ...,  0.0019, -0.0005, -0.0124],
        [ 0.0013, -0.0018, -0.0259,  ..., -0.0199, -0.0125,  0.0054],
        ...,
        [ 0.0032, -0.0006,  0.0035,  ..., -0.0085,  0.0101,  0.0390],
        [-0.0183, -0.0187, -0.0004,  ..., -0.0021, -0.0118,  0.0160],
        [ 0.0079,  0.0023, -0.0124,  ..., -0.0172,  0.0662, -0.0434]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.5391, -2.5625, -1.5469,  ..., -1.1357,  2.7539,  1.4971]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:03:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of intended is unintended
The opposite of reliable is unreliable
The opposite of successful is unsuccessful
The opposite of pleasant is unpleasant
The opposite of sustainable is unsustainable
The opposite of suitable is unsuitable
The opposite of conscious is unconscious
The opposite of related is
2024-07-23 22:03:41 root INFO     [order_1_approx] starting weight calculation for The opposite of suitable is unsuitable
The opposite of conscious is unconscious
The opposite of pleasant is unpleasant
The opposite of sustainable is unsustainable
The opposite of successful is unsuccessful
The opposite of related is unrelated
The opposite of intended is unintended
The opposite of reliable is
2024-07-23 22:03:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 22:07:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7866, -0.1169,  0.1990,  ...,  1.3887,  1.7441,  0.1824],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7227, -3.2188, -2.4961,  ...,  0.0054,  3.1562,  1.3232],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0088, -0.0118,  0.0049,  ..., -0.0267, -0.0032, -0.0079],
        [-0.0074, -0.0240, -0.0111,  ..., -0.0024, -0.0144, -0.0074],
        [-0.0087, -0.0204, -0.0173,  ..., -0.0060,  0.0010,  0.0205],
        ...,
        [-0.0239, -0.0151, -0.0087,  ..., -0.0082,  0.0128,  0.0037],
        [-0.0154, -0.0109,  0.0047,  ...,  0.0161, -0.0273, -0.0037],
        [ 0.0137, -0.0039, -0.0060,  ..., -0.0099,  0.0231, -0.0101]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.9395, -2.7637, -1.7979,  ...,  0.5288,  3.7539,  0.0254]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:07:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of suitable is unsuitable
The opposite of conscious is unconscious
The opposite of pleasant is unpleasant
The opposite of sustainable is unsustainable
The opposite of successful is unsuccessful
The opposite of related is unrelated
The opposite of intended is unintended
The opposite of reliable is
2024-07-23 22:07:31 root INFO     [order_1_approx] starting weight calculation for The opposite of suitable is unsuitable
The opposite of conscious is unconscious
The opposite of sustainable is unsustainable
The opposite of related is unrelated
The opposite of intended is unintended
The opposite of pleasant is unpleasant
The opposite of reliable is unreliable
The opposite of successful is
2024-07-23 22:07:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 22:11:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3535,  0.3770,  0.2065,  ...,  0.9141,  0.9478, -0.0908],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9443,  0.5684,  0.2383,  ...,  0.3699, -0.0852, -0.1680],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0042, -0.0037,  0.0062,  ...,  0.0157, -0.0254, -0.0225],
        [ 0.0064, -0.0255,  0.0034,  ..., -0.0068, -0.0288, -0.0091],
        [-0.0061,  0.0078, -0.0160,  ..., -0.0216,  0.0035,  0.0051],
        ...,
        [ 0.0012, -0.0031, -0.0191,  ..., -0.0302,  0.0120, -0.0079],
        [-0.0063, -0.0069,  0.0025,  ..., -0.0037, -0.0204, -0.0064],
        [-0.0193,  0.0008, -0.0020,  ..., -0.0027,  0.0228, -0.0121]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2441,  0.3013,  0.3928,  ...,  1.1123,  0.2104,  0.8154]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:11:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of suitable is unsuitable
The opposite of conscious is unconscious
The opposite of sustainable is unsustainable
The opposite of related is unrelated
The opposite of intended is unintended
The opposite of pleasant is unpleasant
The opposite of reliable is unreliable
The opposite of successful is
2024-07-23 22:11:20 root INFO     [order_1_approx] starting weight calculation for The opposite of conscious is unconscious
The opposite of reliable is unreliable
The opposite of intended is unintended
The opposite of related is unrelated
The opposite of pleasant is unpleasant
The opposite of successful is unsuccessful
The opposite of sustainable is unsustainable
The opposite of suitable is
2024-07-23 22:11:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 22:15:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1152,  0.0254, -0.1516,  ...,  0.2979,  1.6250,  0.5688],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6094,  0.9263, -0.3779,  ...,  1.2617,  2.8711,  1.2031],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0147, -0.0108,  0.0116,  ..., -0.0148, -0.0276, -0.0175],
        [ 0.0203, -0.0030, -0.0199,  ..., -0.0167, -0.0173, -0.0330],
        [-0.0015, -0.0211, -0.0231,  ..., -0.0272, -0.0015,  0.0006],
        ...,
        [-0.0007, -0.0174, -0.0140,  ..., -0.0218,  0.0120,  0.0066],
        [-0.0247, -0.0032,  0.0188,  ...,  0.0151, -0.0075,  0.0038],
        [-0.0050, -0.0040, -0.0090,  ..., -0.0352,  0.0122, -0.0049]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2021,  1.4375,  0.0879,  ...,  2.3945,  2.5078,  1.4297]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:15:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of conscious is unconscious
The opposite of reliable is unreliable
The opposite of intended is unintended
The opposite of related is unrelated
The opposite of pleasant is unpleasant
The opposite of successful is unsuccessful
The opposite of sustainable is unsustainable
The opposite of suitable is
2024-07-23 22:15:08 root INFO     [order_1_approx] starting weight calculation for The opposite of pleasant is unpleasant
The opposite of successful is unsuccessful
The opposite of suitable is unsuitable
The opposite of conscious is unconscious
The opposite of related is unrelated
The opposite of reliable is unreliable
The opposite of intended is unintended
The opposite of sustainable is
2024-07-23 22:15:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 22:18:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.0859, -0.5752,  1.4102,  ...,  0.3137,  2.7852, -0.9150],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.4102, -2.0762, -1.8213,  ...,  2.3301,  2.1621,  2.1016],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0239,  0.0020, -0.0033,  ...,  0.0015,  0.0082, -0.0186],
        [-0.0180, -0.0067,  0.0065,  ..., -0.0149, -0.0087, -0.0086],
        [-0.0083, -0.0047, -0.0293,  ..., -0.0108, -0.0021,  0.0138],
        ...,
        [ 0.0023, -0.0103, -0.0126,  ..., -0.0312, -0.0001,  0.0009],
        [-0.0083, -0.0115,  0.0044,  ..., -0.0067, -0.0106, -0.0080],
        [-0.0066,  0.0018,  0.0024,  ..., -0.0090, -0.0176, -0.0187]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.3203, -1.1602, -2.5781,  ...,  2.2754,  1.5918,  1.7285]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:18:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of pleasant is unpleasant
The opposite of successful is unsuccessful
The opposite of suitable is unsuitable
The opposite of conscious is unconscious
The opposite of related is unrelated
The opposite of reliable is unreliable
The opposite of intended is unintended
The opposite of sustainable is
2024-07-23 22:18:59 root INFO     total operator prediction time: 1839.3541569709778 seconds
2024-07-23 22:18:59 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on re+verb_reg
2024-07-23 22:18:59 root INFO     building operator re+verb_reg
2024-07-23 22:18:59 root INFO     [order_1_approx] starting weight calculation for To assign again is to reassign
To engage again is to reengage
To upload again is to reupload
To connect again is to reconnect
To grow again is to regrow
To appear again is to reappear
To deem again is to redeem
To acquire again is to
2024-07-23 22:18:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 22:22:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2573, -0.1947,  1.0898,  ..., -0.0019, -0.0469,  0.6299],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6289,  1.0527, -5.8828,  ..., -1.3828, -2.8926,  2.9434],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0009, -0.0067, -0.0032,  ..., -0.0076,  0.0109, -0.0219],
        [-0.0016,  0.0018,  0.0069,  ...,  0.0008, -0.0068,  0.0098],
        [ 0.0152,  0.0096, -0.0240,  ...,  0.0082,  0.0211, -0.0064],
        ...,
        [-0.0091, -0.0117,  0.0023,  ..., -0.0091, -0.0019, -0.0169],
        [ 0.0027, -0.0098, -0.0159,  ..., -0.0122,  0.0038, -0.0424],
        [-0.0047, -0.0032,  0.0023,  ...,  0.0062,  0.0015,  0.0052]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.1094,  1.4658, -6.6172,  ..., -1.6660, -3.4883,  3.2988]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:22:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To assign again is to reassign
To engage again is to reengage
To upload again is to reupload
To connect again is to reconnect
To grow again is to regrow
To appear again is to reappear
To deem again is to redeem
To acquire again is to
2024-07-23 22:22:48 root INFO     [order_1_approx] starting weight calculation for To engage again is to reengage
To assign again is to reassign
To grow again is to regrow
To deem again is to redeem
To acquire again is to reacquire
To upload again is to reupload
To connect again is to reconnect
To appear again is to
2024-07-23 22:22:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 22:26:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4790, -0.5552,  1.1436,  ...,  0.0396, -0.3533, -0.0077],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8701, -0.6729, -1.8164,  ...,  4.1680, -5.2969, -0.3945],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0012, -0.0001, -0.0070,  ..., -0.0004,  0.0029, -0.0088],
        [ 0.0015,  0.0005,  0.0120,  ..., -0.0121, -0.0031,  0.0017],
        [ 0.0138,  0.0057, -0.0038,  ...,  0.0076, -0.0029,  0.0008],
        ...,
        [-0.0190,  0.0016,  0.0103,  ...,  0.0101, -0.0009,  0.0015],
        [-0.0087, -0.0134, -0.0072,  ...,  0.0051, -0.0118, -0.0042],
        [ 0.0083,  0.0143,  0.0125,  ..., -0.0129,  0.0154, -0.0203]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7061,  0.0190, -2.3398,  ...,  3.7070, -5.6094, -1.5557]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:26:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To engage again is to reengage
To assign again is to reassign
To grow again is to regrow
To deem again is to redeem
To acquire again is to reacquire
To upload again is to reupload
To connect again is to reconnect
To appear again is to
2024-07-23 22:26:35 root INFO     [order_1_approx] starting weight calculation for To appear again is to reappear
To acquire again is to reacquire
To connect again is to reconnect
To engage again is to reengage
To grow again is to regrow
To deem again is to redeem
To upload again is to reupload
To assign again is to
2024-07-23 22:26:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 22:30:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9800, -1.0400,  0.9766,  ...,  0.5483, -0.0621,  0.0070],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.6055,  2.3809, -3.3672,  ..., -0.4316, -1.8955,  2.5742],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0205, -0.0066,  0.0011,  ..., -0.0030, -0.0092, -0.0105],
        [-0.0014,  0.0105,  0.0040,  ..., -0.0150,  0.0031,  0.0166],
        [ 0.0101, -0.0098, -0.0034,  ..., -0.0116,  0.0058, -0.0092],
        ...,
        [-0.0078, -0.0081, -0.0065,  ...,  0.0076, -0.0055,  0.0012],
        [-0.0009, -0.0020,  0.0102,  ..., -0.0026, -0.0105, -0.0155],
        [ 0.0022,  0.0148,  0.0011,  ...,  0.0028,  0.0096, -0.0013]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.7461,  2.9199, -3.4316,  ..., -0.6895, -1.7744,  2.1523]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:30:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To appear again is to reappear
To acquire again is to reacquire
To connect again is to reconnect
To engage again is to reengage
To grow again is to regrow
To deem again is to redeem
To upload again is to reupload
To assign again is to
2024-07-23 22:30:22 root INFO     [order_1_approx] starting weight calculation for To appear again is to reappear
To engage again is to reengage
To acquire again is to reacquire
To grow again is to regrow
To upload again is to reupload
To assign again is to reassign
To deem again is to redeem
To connect again is to
2024-07-23 22:30:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 22:34:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6152, -0.6250,  0.7881,  ...,  1.4053,  0.2734,  0.9780],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.4746, -0.2139, -4.5430,  ...,  2.1719, -4.5234,  3.1484],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0151,  0.0004, -0.0078,  ...,  0.0146, -0.0014, -0.0286],
        [ 0.0130,  0.0042,  0.0110,  ...,  0.0065, -0.0048,  0.0183],
        [ 0.0106,  0.0080, -0.0117,  ...,  0.0056,  0.0098, -0.0237],
        ...,
        [-0.0109, -0.0081, -0.0005,  ...,  0.0049, -0.0092,  0.0022],
        [ 0.0096, -0.0067, -0.0127,  ...,  0.0122,  0.0031, -0.0258],
        [ 0.0022,  0.0049,  0.0027,  ..., -0.0117, -0.0069,  0.0019]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.1016,  1.3545, -6.3828,  ...,  1.2285, -6.6719,  3.4199]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:34:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To appear again is to reappear
To engage again is to reengage
To acquire again is to reacquire
To grow again is to regrow
To upload again is to reupload
To assign again is to reassign
To deem again is to redeem
To connect again is to
2024-07-23 22:34:11 root INFO     [order_1_approx] starting weight calculation for To connect again is to reconnect
To assign again is to reassign
To engage again is to reengage
To grow again is to regrow
To appear again is to reappear
To upload again is to reupload
To acquire again is to reacquire
To deem again is to
2024-07-23 22:34:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 22:37:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([1.5820, 0.1017, 1.1641,  ..., 0.3562, 0.0837, 1.1416], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.2539, -0.0247, -2.0605,  ..., -1.7070, -2.2500,  1.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-3.7956e-03, -8.1024e-03,  7.0610e-03,  ..., -6.6986e-03,
         -2.9240e-03, -3.4332e-05],
        [-5.7650e-04, -3.0937e-03,  1.2787e-02,  ..., -8.5602e-03,
          8.4229e-03,  7.9422e-03],
        [ 2.1172e-04,  1.1841e-02, -1.4771e-02,  ..., -1.5745e-03,
          6.6566e-04, -1.2604e-02],
        ...,
        [-1.4908e-02, -1.7609e-02,  1.8127e-02,  ...,  8.3771e-03,
         -4.9515e-03,  1.2627e-02],
        [ 2.7161e-03, -1.3794e-02, -7.0724e-03,  ..., -7.9651e-03,
         -1.4175e-02, -5.7487e-03],
        [ 4.9629e-03,  8.2855e-03, -3.9673e-03,  ...,  7.5417e-03,
         -5.2834e-03, -2.0180e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.2305,  0.4297, -2.9785,  ..., -2.2031, -2.1191,  0.9043]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:38:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To connect again is to reconnect
To assign again is to reassign
To engage again is to reengage
To grow again is to regrow
To appear again is to reappear
To upload again is to reupload
To acquire again is to reacquire
To deem again is to
2024-07-23 22:38:00 root INFO     [order_1_approx] starting weight calculation for To deem again is to redeem
To grow again is to regrow
To acquire again is to reacquire
To assign again is to reassign
To upload again is to reupload
To appear again is to reappear
To connect again is to reconnect
To engage again is to
2024-07-23 22:38:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 22:41:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0322, -0.4172,  0.3823,  ...,  1.1172, -1.5693,  0.8188],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0898, -1.1914, -2.4453,  ...,  0.9004, -0.1846,  1.5674],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0255,  0.0005, -0.0114,  ...,  0.0117,  0.0062, -0.0089],
        [-0.0122,  0.0057,  0.0069,  ..., -0.0065, -0.0044,  0.0020],
        [ 0.0068,  0.0081, -0.0003,  ..., -0.0041,  0.0011,  0.0131],
        ...,
        [ 0.0049, -0.0006, -0.0017,  ...,  0.0100,  0.0009, -0.0084],
        [ 0.0157, -0.0153, -0.0098,  ...,  0.0139,  0.0009, -0.0165],
        [ 0.0007,  0.0175, -0.0033,  ..., -0.0007,  0.0087,  0.0050]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3467, -0.2920, -2.6855,  ..., -0.3047, -1.0635,  1.2891]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:41:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To deem again is to redeem
To grow again is to regrow
To acquire again is to reacquire
To assign again is to reassign
To upload again is to reupload
To appear again is to reappear
To connect again is to reconnect
To engage again is to
2024-07-23 22:41:48 root INFO     [order_1_approx] starting weight calculation for To assign again is to reassign
To appear again is to reappear
To connect again is to reconnect
To acquire again is to reacquire
To deem again is to redeem
To engage again is to reengage
To upload again is to reupload
To grow again is to
2024-07-23 22:41:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 22:45:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5527, -0.3833,  1.3086,  ...,  0.3933,  0.0126,  0.3350],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5967, -0.0205, -5.5391,  ..., -2.1914,  1.9736, -2.3926],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0088, -0.0107, -0.0017,  ..., -0.0084, -0.0005, -0.0125],
        [ 0.0046, -0.0095,  0.0048,  ...,  0.0037,  0.0113,  0.0017],
        [-0.0019,  0.0060, -0.0145,  ..., -0.0043, -0.0053, -0.0083],
        ...,
        [-0.0054,  0.0027, -0.0069,  ...,  0.0077, -0.0022,  0.0050],
        [-0.0067, -0.0039, -0.0122,  ...,  0.0064, -0.0078, -0.0125],
        [ 0.0105,  0.0001,  0.0105,  ...,  0.0013,  0.0092,  0.0019]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.8628,  0.8506, -5.5312,  ..., -3.1094,  1.6143, -2.7773]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:45:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To assign again is to reassign
To appear again is to reappear
To connect again is to reconnect
To acquire again is to reacquire
To deem again is to redeem
To engage again is to reengage
To upload again is to reupload
To grow again is to
2024-07-23 22:45:36 root INFO     [order_1_approx] starting weight calculation for To grow again is to regrow
To connect again is to reconnect
To acquire again is to reacquire
To appear again is to reappear
To deem again is to redeem
To assign again is to reassign
To engage again is to reengage
To upload again is to
2024-07-23 22:45:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 22:49:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8906, -0.8467,  0.3647,  ...,  1.3018, -0.4094,  0.7900],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3555,  1.3242, -5.0234,  ...,  2.3672, -4.0859, -0.2012],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0037, -0.0072, -0.0105,  ..., -0.0065,  0.0019, -0.0037],
        [ 0.0102,  0.0096, -0.0005,  ..., -0.0051, -0.0079, -0.0029],
        [ 0.0004,  0.0037, -0.0055,  ...,  0.0013,  0.0004, -0.0060],
        ...,
        [-0.0012, -0.0061, -0.0049,  ..., -0.0120, -0.0061,  0.0137],
        [ 0.0007, -0.0082,  0.0091,  ..., -0.0046, -0.0190, -0.0065],
        [-0.0032,  0.0047, -0.0031,  ...,  0.0015,  0.0090,  0.0054]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1895,  2.2617, -5.3906,  ...,  2.2422, -4.1328, -0.1171]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:49:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To grow again is to regrow
To connect again is to reconnect
To acquire again is to reacquire
To appear again is to reappear
To deem again is to redeem
To assign again is to reassign
To engage again is to reengage
To upload again is to
2024-07-23 22:49:25 root INFO     total operator prediction time: 1826.1726129055023 seconds
2024-07-23 22:49:25 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on adj+ness_reg
2024-07-23 22:49:25 root INFO     building operator adj+ness_reg
2024-07-23 22:49:25 root INFO     [order_1_approx] starting weight calculation for The state of being serious is seriousness
The state of being hidden is hiddenness
The state of being sacred is sacredness
The state of being innovative is innovativeness
The state of being marked is markedness
The state of being interesting is interestingness
The state of being reasonable is reasonableness
The state of being directed is
2024-07-23 22:49:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 22:53:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.7764, -0.1167,  0.5400,  ...,  0.2546,  1.5918, -0.1855],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.2695, -1.5488,  2.0156,  ..., -3.3672,  3.9199,  2.0605],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 5.1994e-03,  1.6327e-03,  6.6528e-03,  ..., -9.2010e-03,
          6.2294e-03, -1.4420e-02],
        [-9.0714e-03, -1.5686e-02,  1.6689e-03,  ..., -3.7537e-03,
         -1.0597e-02,  9.9182e-04],
        [ 8.2932e-03, -6.4583e-03, -7.4768e-03,  ...,  6.5613e-04,
          3.5095e-04, -7.7248e-05],
        ...,
        [-2.6245e-02, -1.0773e-02,  1.4057e-03,  ..., -1.2589e-02,
          6.5269e-03,  4.5929e-03],
        [ 1.3924e-04,  4.1962e-04,  3.6430e-03,  ..., -2.3483e-02,
         -1.0941e-02,  9.9411e-03],
        [-4.2534e-03,  1.1734e-02,  1.2369e-03,  ...,  4.6616e-03,
          1.2184e-02,  2.5082e-04]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5244, -2.0020,  2.2520,  ..., -3.0156,  4.4336,  2.1562]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:53:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being serious is seriousness
The state of being hidden is hiddenness
The state of being sacred is sacredness
The state of being innovative is innovativeness
The state of being marked is markedness
The state of being interesting is interestingness
The state of being reasonable is reasonableness
The state of being directed is
2024-07-23 22:53:14 root INFO     [order_1_approx] starting weight calculation for The state of being serious is seriousness
The state of being innovative is innovativeness
The state of being directed is directedness
The state of being reasonable is reasonableness
The state of being sacred is sacredness
The state of being marked is markedness
The state of being interesting is interestingness
The state of being hidden is
2024-07-23 22:53:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 22:57:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3237, -0.3862,  1.4473,  ...,  1.3340,  0.7637,  0.4795],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6201,  3.5938,  0.3774,  ..., -0.7334, -1.8379,  4.9688],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.3327e-03, -8.6823e-03,  1.2314e-02,  ...,  1.7548e-03,
         -4.4785e-03, -2.2644e-02],
        [-8.6117e-04, -1.2619e-02,  8.7452e-04,  ..., -2.8610e-03,
         -1.8854e-03, -4.5776e-04],
        [-5.2032e-03,  5.2261e-04, -5.3024e-03,  ..., -4.5242e-03,
         -7.6561e-03, -1.3638e-03],
        ...,
        [-9.6359e-03, -1.9257e-02,  3.6240e-03,  ..., -7.3624e-03,
          2.1439e-03, -3.9673e-03],
        [-1.3371e-03,  4.9171e-03,  6.0272e-03,  ..., -2.2125e-02,
         -2.4384e-02, -4.3869e-05],
        [-8.1940e-03, -1.3771e-03,  2.0599e-03,  ..., -1.5915e-02,
          1.1978e-02, -2.0569e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4980,  3.7070,  0.4229,  ..., -0.2925, -1.5977,  5.4688]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 22:57:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being serious is seriousness
The state of being innovative is innovativeness
The state of being directed is directedness
The state of being reasonable is reasonableness
The state of being sacred is sacredness
The state of being marked is markedness
The state of being interesting is interestingness
The state of being hidden is
2024-07-23 22:57:04 root INFO     [order_1_approx] starting weight calculation for The state of being directed is directedness
The state of being reasonable is reasonableness
The state of being sacred is sacredness
The state of being marked is markedness
The state of being serious is seriousness
The state of being hidden is hiddenness
The state of being interesting is interestingness
The state of being innovative is
2024-07-23 22:57:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 23:00:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.9844, -0.8203,  1.2148,  ...,  0.4497,  1.4443,  0.5264],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.2734, -0.6157, -1.1396,  ...,  0.8218, -1.1836,  3.1270],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0003, -0.0210,  0.0056,  ...,  0.0038,  0.0006, -0.0077],
        [-0.0164, -0.0031,  0.0125,  ...,  0.0030,  0.0148,  0.0062],
        [-0.0053, -0.0102, -0.0200,  ..., -0.0031,  0.0062,  0.0015],
        ...,
        [-0.0115, -0.0204, -0.0095,  ..., -0.0216, -0.0036, -0.0136],
        [ 0.0050, -0.0119,  0.0050,  ..., -0.0212, -0.0305, -0.0068],
        [-0.0025,  0.0155,  0.0081,  ..., -0.0059,  0.0018, -0.0345]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.5527,  0.2368, -1.1885,  ...,  1.5352, -1.2871,  2.8262]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:00:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being directed is directedness
The state of being reasonable is reasonableness
The state of being sacred is sacredness
The state of being marked is markedness
The state of being serious is seriousness
The state of being hidden is hiddenness
The state of being interesting is interestingness
The state of being innovative is
2024-07-23 23:00:53 root INFO     [order_1_approx] starting weight calculation for The state of being serious is seriousness
The state of being reasonable is reasonableness
The state of being directed is directedness
The state of being marked is markedness
The state of being innovative is innovativeness
The state of being hidden is hiddenness
The state of being sacred is sacredness
The state of being interesting is
2024-07-23 23:00:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 23:04:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6333,  0.6353,  0.5410,  ...,  1.6982,  0.9492, -0.2341],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4434,  2.7070,  1.3252,  ...,  1.4258,  1.4932,  6.8516],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0131, -0.0092,  0.0048,  ...,  0.0082, -0.0094, -0.0265],
        [ 0.0076, -0.0012,  0.0228,  ...,  0.0111, -0.0054,  0.0069],
        [-0.0076, -0.0125, -0.0183,  ...,  0.0014,  0.0006,  0.0018],
        ...,
        [-0.0078, -0.0055,  0.0016,  ..., -0.0085, -0.0080, -0.0028],
        [ 0.0014, -0.0009,  0.0064,  ..., -0.0144, -0.0174, -0.0006],
        [-0.0095,  0.0229, -0.0048,  ..., -0.0195, -0.0058, -0.0320]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9927,  2.8887,  1.0342,  ...,  1.4150,  1.9336,  7.5625]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:04:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being serious is seriousness
The state of being reasonable is reasonableness
The state of being directed is directedness
The state of being marked is markedness
The state of being innovative is innovativeness
The state of being hidden is hiddenness
The state of being sacred is sacredness
The state of being interesting is
2024-07-23 23:04:42 root INFO     [order_1_approx] starting weight calculation for The state of being directed is directedness
The state of being interesting is interestingness
The state of being reasonable is reasonableness
The state of being innovative is innovativeness
The state of being hidden is hiddenness
The state of being serious is seriousness
The state of being sacred is sacredness
The state of being marked is
2024-07-23 23:04:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 23:08:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6201,  0.0938,  0.8315,  ...,  2.1074,  0.5527,  0.3228],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.5000,  1.3984,  0.0605,  ..., -1.3535,  0.2878,  6.4883],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0057, -0.0069,  0.0040,  ...,  0.0050, -0.0020, -0.0119],
        [-0.0218, -0.0194,  0.0034,  ..., -0.0039,  0.0077, -0.0109],
        [ 0.0068, -0.0030, -0.0148,  ...,  0.0020, -0.0123,  0.0130],
        ...,
        [-0.0131, -0.0289, -0.0090,  ..., -0.0163, -0.0076, -0.0091],
        [-0.0009,  0.0167,  0.0006,  ..., -0.0005, -0.0112,  0.0123],
        [-0.0202, -0.0019, -0.0026,  ..., -0.0168,  0.0087, -0.0320]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.6250,  1.3838,  1.0947,  ..., -1.1797,  0.5244,  6.7969]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:08:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being directed is directedness
The state of being interesting is interestingness
The state of being reasonable is reasonableness
The state of being innovative is innovativeness
The state of being hidden is hiddenness
The state of being serious is seriousness
The state of being sacred is sacredness
The state of being marked is
2024-07-23 23:08:30 root INFO     [order_1_approx] starting weight calculation for The state of being interesting is interestingness
The state of being directed is directedness
The state of being sacred is sacredness
The state of being serious is seriousness
The state of being innovative is innovativeness
The state of being marked is markedness
The state of being hidden is hiddenness
The state of being reasonable is
2024-07-23 23:08:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 23:12:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5791, -0.8525,  1.2275,  ...,  0.0073,  1.4395, -0.0615],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1670,  4.0312, -1.4980,  ..., -0.2998,  2.8398,  2.6797],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0138, -0.0193,  0.0115,  ..., -0.0027, -0.0084, -0.0179],
        [-0.0062,  0.0018, -0.0068,  ..., -0.0058,  0.0056,  0.0032],
        [-0.0058,  0.0007, -0.0064,  ...,  0.0016,  0.0148,  0.0119],
        ...,
        [-0.0054, -0.0133, -0.0079,  ..., -0.0154,  0.0148,  0.0010],
        [-0.0028,  0.0006,  0.0049,  ..., -0.0162, -0.0162, -0.0074],
        [-0.0102,  0.0177,  0.0153,  ..., -0.0185, -0.0027, -0.0164]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8208,  4.3945, -1.4922,  ..., -0.0349,  3.2812,  2.4062]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:12:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being interesting is interestingness
The state of being directed is directedness
The state of being sacred is sacredness
The state of being serious is seriousness
The state of being innovative is innovativeness
The state of being marked is markedness
The state of being hidden is hiddenness
The state of being reasonable is
2024-07-23 23:12:17 root INFO     [order_1_approx] starting weight calculation for The state of being serious is seriousness
The state of being interesting is interestingness
The state of being hidden is hiddenness
The state of being directed is directedness
The state of being innovative is innovativeness
The state of being reasonable is reasonableness
The state of being marked is markedness
The state of being sacred is
2024-07-23 23:12:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 23:16:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3105, -1.1279,  1.0234,  ...,  0.5674,  1.6064, -0.2966],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.6387,  6.9688,  1.6758,  ..., -4.3789,  0.1191,  3.6230],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0125,  0.0010,  0.0151,  ..., -0.0039, -0.0064, -0.0218],
        [-0.0170, -0.0253, -0.0004,  ..., -0.0084, -0.0092,  0.0155],
        [ 0.0106,  0.0074, -0.0187,  ..., -0.0021,  0.0042,  0.0034],
        ...,
        [-0.0070, -0.0161,  0.0006,  ..., -0.0274,  0.0011, -0.0258],
        [-0.0090, -0.0020,  0.0081,  ..., -0.0107, -0.0196, -0.0066],
        [-0.0267,  0.0014,  0.0014,  ..., -0.0266,  0.0040, -0.0122]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.0312,  7.1602,  2.2812,  ..., -3.5391, -0.4048,  3.9258]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:16:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being serious is seriousness
The state of being interesting is interestingness
The state of being hidden is hiddenness
The state of being directed is directedness
The state of being innovative is innovativeness
The state of being reasonable is reasonableness
The state of being marked is markedness
The state of being sacred is
2024-07-23 23:16:05 root INFO     [order_1_approx] starting weight calculation for The state of being directed is directedness
The state of being interesting is interestingness
The state of being sacred is sacredness
The state of being marked is markedness
The state of being reasonable is reasonableness
The state of being innovative is innovativeness
The state of being hidden is hiddenness
The state of being serious is
2024-07-23 23:16:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 23:19:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([1.1426, 0.6216, 0.9497,  ..., 0.2417, 1.5312, 0.2051], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0674,  4.3164,  3.3633,  ..., -1.2441, -2.1973,  0.2627],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0140, -0.0107,  0.0174,  ...,  0.0014, -0.0301, -0.0054],
        [ 0.0007, -0.0147, -0.0015,  ..., -0.0001, -0.0139,  0.0061],
        [-0.0045,  0.0033, -0.0197,  ...,  0.0035,  0.0100,  0.0187],
        ...,
        [-0.0083, -0.0294,  0.0145,  ..., -0.0260,  0.0055, -0.0071],
        [-0.0116,  0.0054,  0.0141,  ..., -0.0084, -0.0240, -0.0018],
        [-0.0117,  0.0135,  0.0034,  ..., -0.0143,  0.0181, -0.0343]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8062,  4.8750,  4.3984,  ..., -0.7520, -2.0527, -0.0193]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:19:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being directed is directedness
The state of being interesting is interestingness
The state of being sacred is sacredness
The state of being marked is markedness
The state of being reasonable is reasonableness
The state of being innovative is innovativeness
The state of being hidden is hiddenness
The state of being serious is
2024-07-23 23:19:54 root INFO     total operator prediction time: 1829.1650130748749 seconds
2024-07-23 23:19:54 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on noun+less_reg
2024-07-23 23:19:54 root INFO     building operator noun+less_reg
2024-07-23 23:19:54 root INFO     [order_1_approx] starting weight calculation for Something without death is deathless
Something without child is childless
Something without talent is talentless
Something without meat is meatless
Something without remorse is remorseless
Something without wit is witless
Something without sensor is sensorless
Something without arm is
2024-07-23 23:19:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 23:23:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1699,  0.7754, -0.2054,  ...,  0.6455, -0.3628,  0.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4062,  0.7124,  0.7441,  ..., -1.7217,  0.7695,  3.4570],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-7.5760e-03, -1.1353e-02,  2.8427e-02,  ...,  1.2619e-02,
         -9.2392e-03, -1.4526e-02],
        [-1.5823e-02, -2.4704e-02, -1.9989e-03,  ...,  4.4250e-04,
          1.1040e-02, -1.3409e-03],
        [ 7.3242e-03,  1.5213e-02, -2.7466e-02,  ..., -1.1017e-02,
         -3.2368e-03, -1.0651e-02],
        ...,
        [ 9.8572e-03,  2.5589e-02,  9.9564e-04,  ..., -7.0343e-03,
         -6.2943e-03, -1.4168e-02],
        [-1.5259e-05, -9.7656e-03,  1.5099e-02,  ..., -8.2855e-03,
         -1.7792e-02, -2.8839e-03],
        [ 8.3542e-04,  1.9760e-02, -8.4152e-03,  ...,  2.0523e-03,
          2.0199e-03, -1.8936e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9482,  0.7881,  1.0488,  ..., -1.7061,  1.1963,  3.6816]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:23:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without death is deathless
Something without child is childless
Something without talent is talentless
Something without meat is meatless
Something without remorse is remorseless
Something without wit is witless
Something without sensor is sensorless
Something without arm is
2024-07-23 23:23:42 root INFO     [order_1_approx] starting weight calculation for Something without meat is meatless
Something without talent is talentless
Something without death is deathless
Something without remorse is remorseless
Something without wit is witless
Something without sensor is sensorless
Something without arm is armless
Something without child is
2024-07-23 23:23:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 23:27:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3301,  0.0134, -0.2939,  ..., -0.1201,  0.0195,  0.5840],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1936, -2.6172, -5.9062,  ..., -0.3115,  6.0195,  0.3242],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.7471e-02, -1.7029e-02,  1.4046e-02,  ..., -2.2717e-03,
         -2.4734e-02, -1.2924e-02],
        [-6.4697e-03, -1.1902e-02, -4.3907e-03,  ...,  4.4346e-05,
         -8.2626e-03, -9.2468e-03],
        [-6.7635e-03,  7.2289e-03, -2.6703e-02,  ..., -1.2833e-02,
          6.4240e-03,  1.5106e-03],
        ...,
        [ 1.6155e-03, -4.2610e-03, -3.1853e-03,  ..., -2.0203e-02,
         -7.5760e-03, -1.0567e-02],
        [-5.7716e-03, -4.1580e-04,  9.8267e-03,  ..., -4.5242e-03,
         -2.3682e-02,  1.4137e-02],
        [-1.2688e-02,  1.0017e-02,  6.5727e-03,  ...,  3.0346e-03,
          7.7629e-03, -2.3575e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4231, -1.8086, -6.1133,  ..., -0.6489,  5.7930, -0.5459]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:27:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without meat is meatless
Something without talent is talentless
Something without death is deathless
Something without remorse is remorseless
Something without wit is witless
Something without sensor is sensorless
Something without arm is armless
Something without child is
2024-07-23 23:27:31 root INFO     [order_1_approx] starting weight calculation for Something without remorse is remorseless
Something without meat is meatless
Something without sensor is sensorless
Something without arm is armless
Something without talent is talentless
Something without child is childless
Something without wit is witless
Something without death is
2024-07-23 23:27:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 23:31:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4854,  0.9023, -1.1260,  ...,  0.1344,  1.0068, -0.1907],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.5078, -1.3027,  0.1982,  ..., -4.3320, -1.1523,  1.8057],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0190, -0.0131,  0.0251,  ..., -0.0008, -0.0127, -0.0148],
        [-0.0072, -0.0337, -0.0015,  ..., -0.0018, -0.0032, -0.0018],
        [-0.0022,  0.0058, -0.0337,  ..., -0.0167, -0.0070, -0.0045],
        ...,
        [ 0.0090,  0.0019, -0.0052,  ..., -0.0256, -0.0035, -0.0024],
        [ 0.0010, -0.0168,  0.0131,  ...,  0.0081, -0.0327, -0.0036],
        [-0.0054, -0.0003,  0.0153,  ..., -0.0090,  0.0063, -0.0219]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.9922, -0.6489,  0.6406,  ..., -4.2188, -0.8320,  1.4346]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:31:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without remorse is remorseless
Something without meat is meatless
Something without sensor is sensorless
Something without arm is armless
Something without talent is talentless
Something without child is childless
Something without wit is witless
Something without death is
2024-07-23 23:31:21 root INFO     [order_1_approx] starting weight calculation for Something without arm is armless
Something without sensor is sensorless
Something without wit is witless
Something without remorse is remorseless
Something without child is childless
Something without death is deathless
Something without talent is talentless
Something without meat is
2024-07-23 23:31:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 23:35:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1812, -0.7188, -1.0020,  ...,  0.6606,  0.4807, -0.1025],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.2812, -1.6191,  0.2598,  ...,  2.1523, -1.1006,  1.1084],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0065, -0.0109,  0.0083,  ...,  0.0014, -0.0226, -0.0169],
        [-0.0074, -0.0202,  0.0012,  ...,  0.0031, -0.0057, -0.0075],
        [-0.0011,  0.0181, -0.0344,  ...,  0.0067,  0.0057, -0.0202],
        ...,
        [ 0.0031,  0.0028,  0.0033,  ..., -0.0063, -0.0105,  0.0029],
        [ 0.0015, -0.0028,  0.0244,  ...,  0.0108, -0.0237,  0.0044],
        [ 0.0020,  0.0109, -0.0021,  ...,  0.0095,  0.0035, -0.0324]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.4297, -1.1514, -0.2749,  ...,  1.4873, -1.5977,  0.6494]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:35:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without arm is armless
Something without sensor is sensorless
Something without wit is witless
Something without remorse is remorseless
Something without child is childless
Something without death is deathless
Something without talent is talentless
Something without meat is
2024-07-23 23:35:11 root INFO     [order_1_approx] starting weight calculation for Something without child is childless
Something without arm is armless
Something without meat is meatless
Something without wit is witless
Something without death is deathless
Something without talent is talentless
Something without sensor is sensorless
Something without remorse is
2024-07-23 23:35:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 23:38:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1660, -0.6504, -0.0902,  ...,  0.0929,  1.3691, -0.1128],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3979, -0.7656, -4.7695,  ...,  0.3022, -1.9609,  2.3711],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0062, -0.0191,  0.0022,  ..., -0.0026, -0.0120, -0.0067],
        [ 0.0025, -0.0121, -0.0259,  ..., -0.0087,  0.0122, -0.0007],
        [-0.0057,  0.0094, -0.0399,  ...,  0.0064,  0.0195, -0.0022],
        ...,
        [-0.0053, -0.0314, -0.0169,  ..., -0.0229,  0.0065,  0.0087],
        [ 0.0104,  0.0078,  0.0114,  ..., -0.0107, -0.0220,  0.0106],
        [-0.0110,  0.0140,  0.0192,  ..., -0.0064, -0.0015, -0.0311]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5469, -1.3965, -5.2500,  ...,  0.6465, -2.1055,  2.1699]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:38:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without child is childless
Something without arm is armless
Something without meat is meatless
Something without wit is witless
Something without death is deathless
Something without talent is talentless
Something without sensor is sensorless
Something without remorse is
2024-07-23 23:38:59 root INFO     [order_1_approx] starting weight calculation for Something without wit is witless
Something without talent is talentless
Something without child is childless
Something without remorse is remorseless
Something without meat is meatless
Something without death is deathless
Something without arm is armless
Something without sensor is
2024-07-23 23:38:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 23:42:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6562,  0.5439, -0.8652,  ...,  0.0154,  1.1318,  0.3188],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.5742,  0.3193,  4.5781,  ..., -0.8535, -4.1836,  5.1328],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0133, -0.0082,  0.0052,  ..., -0.0105, -0.0060,  0.0022],
        [-0.0040, -0.0184, -0.0090,  ...,  0.0002, -0.0031, -0.0033],
        [-0.0025, -0.0017, -0.0141,  ..., -0.0057, -0.0057,  0.0028],
        ...,
        [-0.0011,  0.0054,  0.0022,  ..., -0.0092,  0.0020,  0.0048],
        [ 0.0113, -0.0105,  0.0092,  ...,  0.0079, -0.0042, -0.0064],
        [-0.0060,  0.0252, -0.0079,  ..., -0.0136, -0.0015, -0.0258]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.1504,  0.2683,  4.5469,  ..., -1.4277, -4.3633,  5.6055]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:42:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without wit is witless
Something without talent is talentless
Something without child is childless
Something without remorse is remorseless
Something without meat is meatless
Something without death is deathless
Something without arm is armless
Something without sensor is
2024-07-23 23:42:49 root INFO     [order_1_approx] starting weight calculation for Something without remorse is remorseless
Something without sensor is sensorless
Something without child is childless
Something without wit is witless
Something without death is deathless
Something without meat is meatless
Something without arm is armless
Something without talent is
2024-07-23 23:42:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 23:46:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8252, -0.1471,  0.1646,  ..., -0.0969,  0.6113, -0.1606],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([0.0874, 1.5029, 2.8359,  ..., 6.4180, 0.9624, 2.1973], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0147,  0.0011,  0.0067,  ...,  0.0031,  0.0016, -0.0082],
        [-0.0260, -0.0153,  0.0006,  ...,  0.0121, -0.0085, -0.0170],
        [-0.0054, -0.0173, -0.0243,  ..., -0.0132, -0.0004,  0.0126],
        ...,
        [-0.0166, -0.0120,  0.0082,  ..., -0.0165, -0.0047, -0.0094],
        [-0.0042, -0.0151,  0.0166,  ..., -0.0206, -0.0139,  0.0051],
        [-0.0122,  0.0233,  0.0059,  ..., -0.0089,  0.0024, -0.0425]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[0.1209, 1.5459, 3.0586,  ..., 6.1602, 0.9106, 1.2793]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:46:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without remorse is remorseless
Something without sensor is sensorless
Something without child is childless
Something without wit is witless
Something without death is deathless
Something without meat is meatless
Something without arm is armless
Something without talent is
2024-07-23 23:46:40 root INFO     [order_1_approx] starting weight calculation for Something without talent is talentless
Something without child is childless
Something without remorse is remorseless
Something without meat is meatless
Something without death is deathless
Something without arm is armless
Something without sensor is sensorless
Something without wit is
2024-07-23 23:46:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 23:50:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.3945, -0.3948, -0.3096,  ...,  1.2891,  0.5693,  1.9004],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.8438, -1.5586, -1.7393,  ...,  5.6133,  5.2656, -1.1719],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0084, -0.0173,  0.0111,  ..., -0.0013,  0.0028, -0.0127],
        [-0.0055, -0.0142,  0.0026,  ...,  0.0007,  0.0025, -0.0131],
        [-0.0106,  0.0062, -0.0201,  ..., -0.0066,  0.0031, -0.0116],
        ...,
        [ 0.0105,  0.0071,  0.0072,  ..., -0.0156, -0.0034, -0.0130],
        [-0.0020, -0.0012,  0.0142,  ..., -0.0018, -0.0235,  0.0080],
        [-0.0112,  0.0038,  0.0063,  ..., -0.0188, -0.0033, -0.0347]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.3594, -1.3477, -2.9766,  ...,  5.1875,  5.8242, -1.3359]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:50:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without talent is talentless
Something without child is childless
Something without remorse is remorseless
Something without meat is meatless
Something without death is deathless
Something without arm is armless
Something without sensor is sensorless
Something without wit is
2024-07-23 23:50:30 root INFO     total operator prediction time: 1836.0018508434296 seconds
2024-07-23 23:50:30 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+ment_irreg
2024-07-23 23:50:30 root INFO     building operator verb+ment_irreg
2024-07-23 23:50:30 root INFO     [order_1_approx] starting weight calculation for To enforce results in a enforcement
To develop results in a development
To invest results in a investment
To manage results in a management
To encourage results in a encouragement
To reimburse results in a reimbursement
To advertise results in a advertisement
To accomplish results in a
2024-07-23 23:50:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 23:54:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1584,  1.3467,  1.2998,  ..., -0.4229,  0.2410, -0.9888],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5303,  0.6929, -7.5117,  ..., -2.3086,  1.3359,  2.6328],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.3245e-02, -1.6947e-03,  3.9902e-03,  ..., -1.0376e-02,
          8.0566e-03, -1.5518e-02],
        [-1.0162e-02, -2.1591e-03,  6.9885e-03,  ..., -4.1237e-03,
          8.7643e-04, -3.7670e-05],
        [ 1.4305e-03,  6.7101e-03, -1.7670e-02,  ..., -9.0790e-03,
          7.5989e-03, -7.0648e-03],
        ...,
        [ 2.7103e-03, -2.5826e-03, -6.5994e-03,  ..., -4.5319e-03,
         -2.2697e-03, -3.4409e-03],
        [-1.8044e-03,  1.0918e-02,  6.1111e-03,  ...,  5.4703e-03,
         -6.4011e-03,  5.2261e-04],
        [ 1.5199e-04,  2.1042e-02, -7.5989e-03,  ..., -5.8784e-03,
          7.4234e-03, -3.6144e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9941,  0.9932, -8.2344,  ..., -2.4199,  0.8389,  2.8203]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:54:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To enforce results in a enforcement
To develop results in a development
To invest results in a investment
To manage results in a management
To encourage results in a encouragement
To reimburse results in a reimbursement
To advertise results in a advertisement
To accomplish results in a
2024-07-23 23:54:20 root INFO     [order_1_approx] starting weight calculation for To enforce results in a enforcement
To accomplish results in a accomplishment
To develop results in a development
To manage results in a management
To reimburse results in a reimbursement
To invest results in a investment
To encourage results in a encouragement
To advertise results in a
2024-07-23 23:54:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-23 23:58:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3306,  0.1870, -0.0666,  ...,  0.3108,  0.3345,  0.6353],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7871, -0.7393, -5.4531,  ..., -1.5996, -0.7607,  1.8789],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.5793e-03, -8.1863e-03,  2.5311e-03,  ..., -1.0094e-02,
         -8.6975e-04, -6.0387e-03],
        [-6.1569e-03, -1.0391e-02,  3.5534e-03,  ...,  5.4550e-03,
         -1.7151e-02,  6.6414e-03],
        [-2.4796e-05,  9.1782e-03, -2.0695e-03,  ...,  4.6692e-03,
          3.4714e-04,  3.7537e-03],
        ...,
        [-1.1200e-02, -6.8436e-03,  9.6512e-04,  ..., -6.0081e-04,
         -2.3193e-03, -7.2403e-03],
        [-6.4087e-03,  2.7943e-04,  5.8098e-03,  ..., -5.3635e-03,
         -1.6251e-02,  1.3580e-02],
        [-3.0174e-03,  2.3926e-02,  1.4563e-03,  ..., -1.3971e-03,
         -1.7662e-03, -3.7231e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.5918, -0.6841, -5.3125,  ..., -1.8926, -0.5537,  1.8760]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-23 23:58:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To enforce results in a enforcement
To accomplish results in a accomplishment
To develop results in a development
To manage results in a management
To reimburse results in a reimbursement
To invest results in a investment
To encourage results in a encouragement
To advertise results in a
2024-07-23 23:58:10 root INFO     [order_1_approx] starting weight calculation for To enforce results in a enforcement
To invest results in a investment
To manage results in a management
To advertise results in a advertisement
To encourage results in a encouragement
To accomplish results in a accomplishment
To reimburse results in a reimbursement
To develop results in a
2024-07-23 23:58:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 00:01:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4678, -0.2000, -0.0757,  ...,  0.9448, -0.2349, -0.3240],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8076, -1.0742, -3.1992,  ..., -2.6992,  2.2559,  3.4570],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0107, -0.0089, -0.0005,  ...,  0.0018,  0.0133, -0.0132],
        [-0.0054, -0.0121, -0.0037,  ..., -0.0009, -0.0013,  0.0049],
        [-0.0029,  0.0141, -0.0154,  ...,  0.0021,  0.0002, -0.0063],
        ...,
        [-0.0188, -0.0189, -0.0120,  ..., -0.0031,  0.0115,  0.0020],
        [ 0.0066,  0.0014,  0.0020,  ..., -0.0143, -0.0098, -0.0040],
        [-0.0024,  0.0274, -0.0008,  ...,  0.0103,  0.0025, -0.0005]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9766, -0.6733, -3.5312,  ..., -3.3027,  1.8379,  3.5137]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:02:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To enforce results in a enforcement
To invest results in a investment
To manage results in a management
To advertise results in a advertisement
To encourage results in a encouragement
To accomplish results in a accomplishment
To reimburse results in a reimbursement
To develop results in a
2024-07-24 00:02:00 root INFO     [order_1_approx] starting weight calculation for To develop results in a development
To invest results in a investment
To accomplish results in a accomplishment
To manage results in a management
To reimburse results in a reimbursement
To advertise results in a advertisement
To enforce results in a enforcement
To encourage results in a
2024-07-24 00:02:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 00:05:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0425, -0.3179, -0.1646,  ...,  0.4050,  1.1758,  0.7905],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9580, -1.7490, -2.3770,  ..., -0.6172,  0.6333,  1.0361],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0136, -0.0117,  0.0054,  ..., -0.0012, -0.0008, -0.0146],
        [-0.0157, -0.0065, -0.0027,  ..., -0.0009, -0.0010, -0.0043],
        [ 0.0113, -0.0084, -0.0015,  ...,  0.0053,  0.0071, -0.0146],
        ...,
        [ 0.0064, -0.0078, -0.0079,  ..., -0.0055,  0.0167, -0.0182],
        [ 0.0115,  0.0137,  0.0006,  ..., -0.0145, -0.0092,  0.0188],
        [-0.0266,  0.0388, -0.0109,  ...,  0.0323,  0.0088,  0.0131]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0508, -1.1270, -2.7949,  ..., -0.8115,  0.9053,  0.0820]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:05:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To develop results in a development
To invest results in a investment
To accomplish results in a accomplishment
To manage results in a management
To reimburse results in a reimbursement
To advertise results in a advertisement
To enforce results in a enforcement
To encourage results in a
2024-07-24 00:05:49 root INFO     [order_1_approx] starting weight calculation for To encourage results in a encouragement
To invest results in a investment
To accomplish results in a accomplishment
To develop results in a development
To reimburse results in a reimbursement
To advertise results in a advertisement
To manage results in a management
To enforce results in a
2024-07-24 00:05:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 00:09:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2087,  1.9229,  0.0273,  ...,  0.1338, -0.6699,  1.2959],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0293, -1.9414, -3.2969,  ..., -1.5371,  0.0425,  7.8594],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0020, -0.0044, -0.0059,  ..., -0.0056,  0.0005, -0.0108],
        [ 0.0062, -0.0016, -0.0071,  ..., -0.0062, -0.0081,  0.0002],
        [-0.0121,  0.0065, -0.0103,  ...,  0.0101,  0.0085, -0.0027],
        ...,
        [-0.0080, -0.0044, -0.0139,  ...,  0.0014,  0.0037, -0.0173],
        [-0.0055,  0.0115, -0.0033,  ..., -0.0090, -0.0148,  0.0072],
        [-0.0161,  0.0185,  0.0055,  ..., -0.0073,  0.0015, -0.0011]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9053, -0.8828, -3.8633,  ..., -1.5752, -0.0664,  7.5391]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:09:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To encourage results in a encouragement
To invest results in a investment
To accomplish results in a accomplishment
To develop results in a development
To reimburse results in a reimbursement
To advertise results in a advertisement
To manage results in a management
To enforce results in a
2024-07-24 00:09:39 root INFO     [order_1_approx] starting weight calculation for To accomplish results in a accomplishment
To develop results in a development
To reimburse results in a reimbursement
To enforce results in a enforcement
To manage results in a management
To advertise results in a advertisement
To encourage results in a encouragement
To invest results in a
2024-07-24 00:09:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 00:13:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0391,  1.0605,  1.6660,  ..., -0.1838, -0.9204, -0.0273],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5664,  2.0078, -3.2812,  ..., -0.6323,  1.7100,  4.6719],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0010, -0.0129,  0.0060,  ...,  0.0001,  0.0035, -0.0076],
        [-0.0086,  0.0063,  0.0172,  ...,  0.0022, -0.0119,  0.0185],
        [ 0.0011,  0.0098, -0.0155,  ..., -0.0040,  0.0021, -0.0092],
        ...,
        [-0.0037, -0.0112, -0.0028,  ..., -0.0041, -0.0053, -0.0082],
        [ 0.0023, -0.0048, -0.0093,  ..., -0.0047, -0.0205,  0.0120],
        [-0.0130,  0.0165, -0.0073,  ..., -0.0004, -0.0017, -0.0201]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4268,  2.3672, -3.8555,  ..., -1.4023,  1.7617,  4.0156]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:13:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To accomplish results in a accomplishment
To develop results in a development
To reimburse results in a reimbursement
To enforce results in a enforcement
To manage results in a management
To advertise results in a advertisement
To encourage results in a encouragement
To invest results in a
2024-07-24 00:13:29 root INFO     [order_1_approx] starting weight calculation for To advertise results in a advertisement
To encourage results in a encouragement
To invest results in a investment
To accomplish results in a accomplishment
To reimburse results in a reimbursement
To develop results in a development
To enforce results in a enforcement
To manage results in a
2024-07-24 00:13:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 00:17:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2461,  0.5322,  0.0933,  ...,  0.5498,  0.1476, -0.2759],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3127, -2.8750, -4.2070,  ...,  2.0508,  1.6250,  6.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-5.0621e-03, -7.2861e-03, -7.5073e-03,  ..., -1.0376e-03,
         -7.6675e-03, -7.4615e-03],
        [ 2.4452e-03, -8.6517e-03,  5.6305e-03,  ..., -2.3460e-04,
         -5.1804e-03, -6.1684e-03],
        [ 3.3913e-03, -4.4937e-03, -6.6910e-03,  ..., -1.1444e-05,
          2.8667e-03,  4.4632e-04],
        ...,
        [-1.9684e-03, -1.6937e-02, -8.2016e-03,  ..., -2.1439e-03,
         -2.3079e-03, -3.6678e-03],
        [-4.0283e-03, -2.2583e-03,  3.2883e-03,  ..., -1.3702e-02,
         -1.4702e-02,  8.4991e-03],
        [-5.8899e-03,  1.5556e-02,  3.7212e-03,  ..., -2.0523e-03,
          5.8060e-03, -6.9809e-04]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2815, -2.5625, -4.8086,  ...,  1.7480,  2.0020,  5.5977]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:17:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To advertise results in a advertisement
To encourage results in a encouragement
To invest results in a investment
To accomplish results in a accomplishment
To reimburse results in a reimbursement
To develop results in a development
To enforce results in a enforcement
To manage results in a
2024-07-24 00:17:18 root INFO     [order_1_approx] starting weight calculation for To encourage results in a encouragement
To accomplish results in a accomplishment
To invest results in a investment
To enforce results in a enforcement
To develop results in a development
To manage results in a management
To advertise results in a advertisement
To reimburse results in a
2024-07-24 00:17:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 00:21:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0715, -0.5166,  0.5078,  ..., -0.1685,  0.1201,  0.0950],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1554,  1.0928, -7.8633,  ...,  2.3906,  1.1582,  4.6523],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.0506e-02, -2.2568e-02,  1.6012e-03,  ..., -7.0381e-03,
          1.6357e-02, -1.3901e-02],
        [-9.4032e-04, -4.4212e-03, -1.9550e-05,  ..., -4.2152e-03,
         -7.4997e-03,  5.8784e-03],
        [-5.9700e-03,  4.3526e-03, -4.8370e-03,  ..., -1.6785e-03,
         -1.2062e-02, -9.1934e-04],
        ...,
        [ 6.6147e-03, -2.2339e-02, -1.3420e-02,  ..., -1.1864e-03,
         -3.1452e-03, -9.3155e-03],
        [-3.8948e-03,  6.4011e-03,  1.1978e-03,  ...,  2.9926e-03,
         -9.7961e-03,  1.0094e-02],
        [ 3.3951e-04,  5.3520e-03, -6.7711e-05,  ...,  1.4706e-03,
          1.9264e-03,  3.9825e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4048,  1.4160, -8.1562,  ...,  2.6719,  1.2275,  4.8984]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:21:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To encourage results in a encouragement
To accomplish results in a accomplishment
To invest results in a investment
To enforce results in a enforcement
To develop results in a development
To manage results in a management
To advertise results in a advertisement
To reimburse results in a
2024-07-24 00:21:07 root INFO     total operator prediction time: 1836.8096170425415 seconds
2024-07-24 00:21:07 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on name - nationality
2024-07-24 00:21:07 root INFO     building operator name - nationality
2024-07-24 00:21:07 root INFO     [order_1_approx] starting weight calculation for newton was english
lennon was english
lenin was soviet
locke was english
darwin was english
pascal was french
hegel was german
confucius was
2024-07-24 00:21:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 00:24:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4285, -0.7666, -2.0000,  ..., -0.2395,  0.3157,  1.6836],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6074, -4.0625, -8.1250,  ..., -2.0332, -0.7632, -4.4336],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0144, -0.0043,  0.0042,  ..., -0.0033,  0.0055,  0.0011],
        [ 0.0067,  0.0055,  0.0007,  ..., -0.0050, -0.0038,  0.0036],
        [-0.0058, -0.0076, -0.0067,  ..., -0.0123, -0.0091, -0.0055],
        ...,
        [-0.0095,  0.0034, -0.0085,  ...,  0.0173, -0.0056, -0.0046],
        [-0.0016,  0.0009,  0.0068,  ..., -0.0100,  0.0074,  0.0024],
        [-0.0108,  0.0020, -0.0053,  ..., -0.0124, -0.0047,  0.0048]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2344, -4.0977, -7.4648,  ..., -1.4238, -0.6982, -4.0625]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:24:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for newton was english
lennon was english
lenin was soviet
locke was english
darwin was english
pascal was french
hegel was german
confucius was
2024-07-24 00:24:55 root INFO     [order_1_approx] starting weight calculation for lenin was soviet
hegel was german
pascal was french
lennon was english
newton was english
locke was english
confucius was chinese
darwin was
2024-07-24 00:24:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 00:28:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2026, -0.6060, -0.3992,  ...,  0.2039,  0.0469,  0.4146],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8984, -2.3555, -2.7012,  ..., -3.8477, -0.0854, -4.1289],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.4748e-02, -1.3870e-02, -3.2654e-03,  ..., -7.8430e-03,
         -1.6003e-03, -5.4779e-03],
        [ 2.4166e-03,  8.6670e-03, -2.1820e-03,  ...,  7.4768e-04,
         -4.0550e-03,  4.9553e-03],
        [ 7.6294e-05, -3.3417e-03,  8.8882e-04,  ..., -1.5686e-02,
          7.3471e-03,  3.1586e-03],
        ...,
        [-3.0022e-03,  4.3831e-03,  1.6870e-03,  ...,  1.4893e-02,
         -9.7504e-03, -1.0986e-03],
        [-5.9052e-03, -1.1551e-02,  1.0796e-02,  ...,  2.7275e-03,
          1.7548e-02,  1.1879e-02],
        [-5.6458e-03,  1.7029e-02,  5.7068e-03,  ..., -7.0267e-03,
         -7.3509e-03,  1.0376e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.6162, -3.1367, -2.6680,  ..., -3.7656,  0.4336, -4.3242]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:28:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for lenin was soviet
hegel was german
pascal was french
lennon was english
newton was english
locke was english
confucius was chinese
darwin was
2024-07-24 00:28:43 root INFO     [order_1_approx] starting weight calculation for confucius was chinese
pascal was french
darwin was english
newton was english
lennon was english
lenin was soviet
locke was english
hegel was
2024-07-24 00:28:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 00:32:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.4219,  0.1606, -0.0707,  ...,  0.1152, -0.3369,  1.1855],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1523, -4.1406, -6.3672,  ..., -6.9219, -4.9102, -3.1914],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 5.5542e-03, -2.1286e-03,  1.9569e-03,  ..., -4.8370e-03,
          6.3171e-03, -8.4686e-03],
        [ 1.1665e-02,  6.4926e-03, -3.9673e-03,  ...,  9.5520e-03,
          5.0697e-03, -1.6384e-03],
        [-1.5945e-03, -1.2535e-02, -3.5620e-04,  ..., -1.8570e-02,
         -5.8403e-03,  4.6539e-03],
        ...,
        [-9.7561e-04,  1.8692e-04, -9.0561e-03,  ...,  1.2302e-03,
         -3.4676e-03, -2.9488e-03],
        [ 4.7989e-03, -7.8430e-03, -3.8147e-05,  ..., -3.8071e-03,
          9.7809e-03,  4.8790e-03],
        [-4.6730e-03, -5.1270e-03, -4.0779e-03,  ..., -9.3231e-03,
          2.1515e-03,  1.2840e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1147, -4.5078, -7.0391,  ..., -6.9844, -4.8516, -4.2422]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:32:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for confucius was chinese
pascal was french
darwin was english
newton was english
lennon was english
lenin was soviet
locke was english
hegel was
2024-07-24 00:32:29 root INFO     [order_1_approx] starting weight calculation for confucius was chinese
pascal was french
lennon was english
newton was english
hegel was german
darwin was english
locke was english
lenin was
2024-07-24 00:32:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 00:36:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0161,  0.9883, -0.3799,  ..., -0.5205,  0.2856,  1.0215],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.3047, -2.9590, -3.1992,  ..., -9.0312, -4.3203, -0.3450],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0224, -0.0039,  0.0050,  ..., -0.0103,  0.0155, -0.0019],
        [ 0.0091,  0.0116,  0.0010,  ...,  0.0168,  0.0044, -0.0016],
        [-0.0031,  0.0041,  0.0048,  ..., -0.0051, -0.0098, -0.0011],
        ...,
        [-0.0134, -0.0068,  0.0079,  ...,  0.0182, -0.0109,  0.0033],
        [ 0.0131, -0.0075,  0.0040,  ...,  0.0022,  0.0121,  0.0096],
        [-0.0013,  0.0039,  0.0088,  ..., -0.0084,  0.0140,  0.0168]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.9023, -3.0742, -3.4297,  ..., -9.4844, -3.3496, -0.4592]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:36:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for confucius was chinese
pascal was french
lennon was english
newton was english
hegel was german
darwin was english
locke was english
lenin was
2024-07-24 00:36:17 root INFO     [order_1_approx] starting weight calculation for pascal was french
locke was english
darwin was english
hegel was german
confucius was chinese
newton was english
lenin was soviet
lennon was
2024-07-24 00:36:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 00:40:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5586,  1.0459,  0.9370,  ...,  1.2939,  0.5903,  0.2700],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2793, -1.8125, -4.9062,  ..., -1.6885, -0.0632, -6.2344],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0053, -0.0030,  0.0011,  ..., -0.0024,  0.0130,  0.0020],
        [ 0.0042,  0.0053, -0.0078,  ...,  0.0076,  0.0077, -0.0054],
        [-0.0070, -0.0078, -0.0022,  ..., -0.0229, -0.0034, -0.0048],
        ...,
        [-0.0131, -0.0010,  0.0175,  ..., -0.0009, -0.0169, -0.0158],
        [ 0.0084, -0.0019,  0.0201,  ..., -0.0106,  0.0006, -0.0115],
        [ 0.0043, -0.0042,  0.0046,  ..., -0.0174, -0.0164,  0.0016]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1489, -1.3105, -4.6523,  ..., -2.3164, -0.7842, -5.6016]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:40:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for pascal was french
locke was english
darwin was english
hegel was german
confucius was chinese
newton was english
lenin was soviet
lennon was
2024-07-24 00:40:06 root INFO     [order_1_approx] starting weight calculation for pascal was french
darwin was english
newton was english
lennon was english
confucius was chinese
lenin was soviet
hegel was german
locke was
2024-07-24 00:40:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 00:43:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2852, -0.3013, -0.1051,  ...,  1.7168, -1.3135,  0.8354],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0957, -3.0098, -6.8359,  ..., -4.6953, -1.1514, -3.9824],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.9087e-03,  2.4796e-05,  2.5082e-03,  ...,  2.0981e-04,
         -1.7967e-03,  6.6147e-03],
        [-9.3079e-04,  1.0391e-02,  4.6082e-03,  ...,  1.1864e-02,
         -1.1536e-02,  2.8000e-03],
        [ 1.6220e-02, -1.3428e-02, -1.4664e-02,  ..., -1.8280e-02,
          3.6774e-03,  1.5125e-03],
        ...,
        [ 6.2943e-03, -1.9440e-02,  4.5776e-03,  ...,  1.7204e-03,
         -1.2760e-03, -9.4070e-03],
        [-7.7934e-03,  1.1444e-03, -5.6362e-04,  ...,  3.6278e-03,
         -5.3711e-03,  3.0899e-03],
        [ 7.1640e-03, -9.1095e-03,  1.4744e-03,  ..., -2.2369e-02,
         -7.5645e-03, -4.6730e-05]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3145, -3.5234, -7.1172,  ..., -5.8867, -1.5918, -4.4844]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:43:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for pascal was french
darwin was english
newton was english
lennon was english
confucius was chinese
lenin was soviet
hegel was german
locke was
2024-07-24 00:43:55 root INFO     [order_1_approx] starting weight calculation for locke was english
confucius was chinese
lenin was soviet
lennon was english
darwin was english
pascal was french
hegel was german
newton was
2024-07-24 00:43:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 00:47:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7437,  0.7046,  0.2532,  ...,  0.7520, -1.6582,  0.8926],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4971, -2.7051, -3.7480,  ...,  1.1143, -1.6426, -0.3943],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0123, -0.0143,  0.0060,  ..., -0.0050,  0.0032, -0.0023],
        [ 0.0010,  0.0205, -0.0010,  ...,  0.0001, -0.0061,  0.0044],
        [ 0.0005,  0.0014,  0.0062,  ..., -0.0045,  0.0091,  0.0015],
        ...,
        [-0.0107, -0.0043,  0.0098,  ...,  0.0078,  0.0096, -0.0017],
        [-0.0121, -0.0118,  0.0008,  ..., -0.0076,  0.0142, -0.0006],
        [-0.0017, -0.0001,  0.0094,  ..., -0.0169,  0.0108,  0.0133]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2002, -2.1816, -4.2266,  ...,  0.7236, -0.9155, -0.9658]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:47:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for locke was english
confucius was chinese
lenin was soviet
lennon was english
darwin was english
pascal was french
hegel was german
newton was
2024-07-24 00:47:44 root INFO     [order_1_approx] starting weight calculation for hegel was german
confucius was chinese
darwin was english
newton was english
lenin was soviet
locke was english
lennon was english
pascal was
2024-07-24 00:47:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 00:51:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5156, -0.4934, -0.9551,  ...,  0.4629,  0.0620,  2.1289],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8828, -2.4238, -6.6055,  ..., -3.2500, -4.5781, -0.7124],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0067, -0.0136,  0.0080,  ..., -0.0028, -0.0036, -0.0102],
        [ 0.0061,  0.0039,  0.0029,  ...,  0.0014, -0.0073, -0.0030],
        [-0.0055,  0.0098, -0.0117,  ..., -0.0161, -0.0037, -0.0035],
        ...,
        [-0.0099, -0.0082, -0.0060,  ..., -0.0076, -0.0104, -0.0103],
        [-0.0028, -0.0026, -0.0027,  ...,  0.0004,  0.0044,  0.0034],
        [-0.0080,  0.0146,  0.0020,  ..., -0.0217, -0.0018,  0.0039]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1123, -2.5742, -7.2422,  ..., -3.3145, -4.4023, -0.2634]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:51:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for hegel was german
confucius was chinese
darwin was english
newton was english
lenin was soviet
locke was english
lennon was english
pascal was
2024-07-24 00:51:33 root INFO     total operator prediction time: 1826.3505103588104 seconds
2024-07-24 00:51:33 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on country - language
2024-07-24 00:51:33 root INFO     building operator country - language
2024-07-24 00:51:33 root INFO     [order_1_approx] starting weight calculation for The country of philippines primarily speaks the language of tagalog
The country of mozambique primarily speaks the language of portuguese
The country of peru primarily speaks the language of spanish
The country of brazil primarily speaks the language of portuguese
The country of morocco primarily speaks the language of berber
The country of guadeloupe primarily speaks the language of french
The country of venezuela primarily speaks the language of spanish
The country of argentina primarily speaks the language of
2024-07-24 00:51:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 00:55:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6313,  0.0431,  0.7749,  ...,  0.2263,  0.7695, -0.4258],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2139, -2.9688, -4.7188,  ..., -1.5312,  1.1445, -5.7109],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-3.0479e-03, -2.4891e-03,  3.5477e-04,  ..., -7.5951e-03,
         -7.4615e-03, -6.3057e-03],
        [-1.3428e-03, -9.6283e-03,  1.0208e-02,  ...,  9.5825e-03,
          2.9411e-03,  6.0225e-04],
        [ 3.1433e-03,  2.4521e-02, -3.7109e-02,  ..., -1.3138e-02,
         -7.1526e-04, -9.4910e-03],
        ...,
        [-4.9515e-03, -4.0321e-03, -1.3733e-02,  ..., -5.0507e-03,
         -1.0818e-02, -1.7080e-03],
        [ 4.3297e-04,  4.8523e-03, -1.6815e-02,  ..., -9.0866e-03,
         -1.1574e-02, -9.4833e-03],
        [ 2.9068e-03, -4.5815e-03, -1.2878e-02,  ..., -9.9640e-03,
          1.8120e-05, -6.5765e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3223, -2.4844, -4.8867,  ..., -2.2871,  0.6821, -5.6758]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:55:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of philippines primarily speaks the language of tagalog
The country of mozambique primarily speaks the language of portuguese
The country of peru primarily speaks the language of spanish
The country of brazil primarily speaks the language of portuguese
The country of morocco primarily speaks the language of berber
The country of guadeloupe primarily speaks the language of french
The country of venezuela primarily speaks the language of spanish
The country of argentina primarily speaks the language of
2024-07-24 00:55:22 root INFO     [order_1_approx] starting weight calculation for The country of venezuela primarily speaks the language of spanish
The country of philippines primarily speaks the language of tagalog
The country of argentina primarily speaks the language of spanish
The country of guadeloupe primarily speaks the language of french
The country of peru primarily speaks the language of spanish
The country of morocco primarily speaks the language of berber
The country of mozambique primarily speaks the language of portuguese
The country of brazil primarily speaks the language of
2024-07-24 00:55:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 00:59:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1484,  0.4639, -0.4331,  ..., -0.3618, -0.0778, -1.0342],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.2031, -3.3652, -5.4023,  ..., -2.3750,  0.8184, -2.9102],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-7.7629e-03, -2.1687e-03,  6.0959e-03,  ...,  6.4850e-05,
          1.4458e-02, -6.8474e-04],
        [ 4.2267e-03,  1.3481e-02, -1.3321e-02,  ...,  5.1498e-03,
         -2.0256e-03,  1.9321e-03],
        [-4.3259e-03,  2.6741e-03, -9.7504e-03,  ..., -6.7234e-04,
          5.0125e-03, -2.1782e-03],
        ...,
        [ 9.4070e-03,  3.3450e-04, -3.5419e-03,  ...,  6.4240e-03,
         -1.4221e-02, -3.2234e-03],
        [ 3.5477e-03, -7.8583e-04, -1.2367e-02,  ..., -1.6270e-03,
         -1.4572e-02, -7.8583e-03],
        [-1.6327e-03, -8.7280e-03,  3.2120e-03,  ...,  4.9210e-04,
          1.8501e-03,  4.3297e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.8086, -3.0645, -5.1133,  ..., -2.2070,  0.6528, -3.2578]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 00:59:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of venezuela primarily speaks the language of spanish
The country of philippines primarily speaks the language of tagalog
The country of argentina primarily speaks the language of spanish
The country of guadeloupe primarily speaks the language of french
The country of peru primarily speaks the language of spanish
The country of morocco primarily speaks the language of berber
The country of mozambique primarily speaks the language of portuguese
The country of brazil primarily speaks the language of
2024-07-24 00:59:11 root INFO     [order_1_approx] starting weight calculation for The country of peru primarily speaks the language of spanish
The country of argentina primarily speaks the language of spanish
The country of venezuela primarily speaks the language of spanish
The country of morocco primarily speaks the language of berber
The country of mozambique primarily speaks the language of portuguese
The country of brazil primarily speaks the language of portuguese
The country of philippines primarily speaks the language of tagalog
The country of guadeloupe primarily speaks the language of
2024-07-24 00:59:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 01:02:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5488, -0.5566, -1.9697,  ...,  0.5430, -0.1802, -0.0991],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.9414, -5.0391, -9.2266,  ..., -3.9980, -0.4263,  0.6309],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0033, -0.0109,  0.0143,  ..., -0.0090,  0.0149, -0.0238],
        [-0.0012,  0.0098,  0.0041,  ..., -0.0026,  0.0181, -0.0096],
        [ 0.0028,  0.0156, -0.0216,  ...,  0.0003, -0.0152,  0.0134],
        ...,
        [-0.0033, -0.0130,  0.0004,  ...,  0.0129, -0.0071,  0.0065],
        [-0.0108, -0.0107, -0.0065,  ...,  0.0167, -0.0104,  0.0022],
        [ 0.0017,  0.0041,  0.0056,  ..., -0.0003,  0.0043, -0.0005]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.7051e+00, -4.8281e+00, -9.1094e+00,  ..., -5.1250e+00,
         -1.0762e+00, -7.3242e-03]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SubBackward0>) 

                    
2024-07-24 01:02:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of peru primarily speaks the language of spanish
The country of argentina primarily speaks the language of spanish
The country of venezuela primarily speaks the language of spanish
The country of morocco primarily speaks the language of berber
The country of mozambique primarily speaks the language of portuguese
The country of brazil primarily speaks the language of portuguese
The country of philippines primarily speaks the language of tagalog
The country of guadeloupe primarily speaks the language of
2024-07-24 01:03:00 root INFO     [order_1_approx] starting weight calculation for The country of brazil primarily speaks the language of portuguese
The country of guadeloupe primarily speaks the language of french
The country of mozambique primarily speaks the language of portuguese
The country of argentina primarily speaks the language of spanish
The country of venezuela primarily speaks the language of spanish
The country of peru primarily speaks the language of spanish
The country of philippines primarily speaks the language of tagalog
The country of morocco primarily speaks the language of
2024-07-24 01:03:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 01:06:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0869, -0.7979, -0.3518,  ...,  0.3857, -1.0244,  1.0430],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9746, -3.6230, -4.4336,  ..., -3.8770, -3.5586, -1.0986],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.5259e-05, -6.5002e-03, -4.4746e-03,  ...,  1.4009e-03,
          7.4921e-03,  3.8624e-05],
        [ 1.2016e-03, -4.6997e-03,  2.2850e-03,  ..., -5.5351e-03,
          1.3367e-02, -7.0877e-03],
        [ 1.2131e-03,  1.3680e-02, -1.1894e-02,  ...,  7.1983e-03,
         -5.0316e-03, -5.9128e-04],
        ...,
        [ 1.0239e-02,  1.1833e-02, -6.8626e-03,  ...,  3.0251e-03,
         -1.7490e-03,  5.1498e-03],
        [-8.7509e-03,  1.2718e-02, -9.3689e-03,  ...,  1.0490e-04,
         -1.6891e-02,  1.4191e-02],
        [-1.6022e-04, -6.5994e-03,  1.8005e-03,  ...,  1.2760e-03,
          1.2619e-02,  4.2572e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1758, -3.2676, -5.0586,  ..., -3.9355, -4.1172, -1.8223]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:06:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of brazil primarily speaks the language of portuguese
The country of guadeloupe primarily speaks the language of french
The country of mozambique primarily speaks the language of portuguese
The country of argentina primarily speaks the language of spanish
The country of venezuela primarily speaks the language of spanish
The country of peru primarily speaks the language of spanish
The country of philippines primarily speaks the language of tagalog
The country of morocco primarily speaks the language of
2024-07-24 01:06:48 root INFO     [order_1_approx] starting weight calculation for The country of argentina primarily speaks the language of spanish
The country of venezuela primarily speaks the language of spanish
The country of brazil primarily speaks the language of portuguese
The country of morocco primarily speaks the language of berber
The country of guadeloupe primarily speaks the language of french
The country of philippines primarily speaks the language of tagalog
The country of peru primarily speaks the language of spanish
The country of mozambique primarily speaks the language of
2024-07-24 01:06:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 01:10:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0352, -1.1797, -0.7090,  ...,  0.4470,  1.0957,  0.6509],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.5625, -6.3594, -7.9375,  ...,  1.6084, -0.9224, -9.0469],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-6.1874e-03, -4.4403e-03,  6.6872e-03,  ..., -7.1049e-04,
         -4.6272e-03, -9.2392e-03],
        [-7.2479e-05, -1.3905e-03,  8.3847e-03,  ...,  1.0361e-02,
          2.5902e-03,  4.4060e-04],
        [ 9.4681e-03,  1.0681e-02, -8.8196e-03,  ..., -5.4092e-03,
         -3.1624e-03, -1.3857e-03],
        ...,
        [-2.0340e-02, -1.7767e-03,  1.1726e-02,  ...,  1.0216e-02,
         -9.4147e-03,  1.0666e-02],
        [-4.1046e-03, -4.8676e-03, -1.0674e-02,  ...,  4.5872e-04,
         -5.6763e-03, -2.3785e-03],
        [ 1.2749e-02,  8.3160e-04, -1.2863e-02,  ..., -1.1818e-02,
          1.3947e-02,  1.1959e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.4961, -5.8789, -8.1875,  ...,  0.9414, -0.5781, -9.7734]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:10:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of argentina primarily speaks the language of spanish
The country of venezuela primarily speaks the language of spanish
The country of brazil primarily speaks the language of portuguese
The country of morocco primarily speaks the language of berber
The country of guadeloupe primarily speaks the language of french
The country of philippines primarily speaks the language of tagalog
The country of peru primarily speaks the language of spanish
The country of mozambique primarily speaks the language of
2024-07-24 01:10:36 root INFO     [order_1_approx] starting weight calculation for The country of venezuela primarily speaks the language of spanish
The country of philippines primarily speaks the language of tagalog
The country of mozambique primarily speaks the language of portuguese
The country of morocco primarily speaks the language of berber
The country of argentina primarily speaks the language of spanish
The country of brazil primarily speaks the language of portuguese
The country of guadeloupe primarily speaks the language of french
The country of peru primarily speaks the language of
2024-07-24 01:10:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 01:14:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6309, -0.2581, -2.1992,  ...,  0.8315,  1.3320,  0.3792],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.3242,  0.3579, -8.8438,  ..., -1.8652,  3.6934, -2.6680],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0061, -0.0058, -0.0047,  ...,  0.0009,  0.0038,  0.0069],
        [-0.0075,  0.0053,  0.0085,  ...,  0.0128,  0.0125,  0.0094],
        [ 0.0149, -0.0002, -0.0298,  ..., -0.0147, -0.0142, -0.0075],
        ...,
        [-0.0093, -0.0059, -0.0072,  ...,  0.0014, -0.0104,  0.0041],
        [ 0.0084, -0.0104, -0.0032,  ...,  0.0018,  0.0021,  0.0026],
        [ 0.0063, -0.0058, -0.0036,  ..., -0.0087, -0.0004, -0.0005]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1191,  1.0547, -9.0234,  ..., -2.4570,  2.9297, -3.1895]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:14:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of venezuela primarily speaks the language of spanish
The country of philippines primarily speaks the language of tagalog
The country of mozambique primarily speaks the language of portuguese
The country of morocco primarily speaks the language of berber
The country of argentina primarily speaks the language of spanish
The country of brazil primarily speaks the language of portuguese
The country of guadeloupe primarily speaks the language of french
The country of peru primarily speaks the language of
2024-07-24 01:14:25 root INFO     [order_1_approx] starting weight calculation for The country of guadeloupe primarily speaks the language of french
The country of venezuela primarily speaks the language of spanish
The country of brazil primarily speaks the language of portuguese
The country of argentina primarily speaks the language of spanish
The country of morocco primarily speaks the language of berber
The country of peru primarily speaks the language of spanish
The country of mozambique primarily speaks the language of portuguese
The country of philippines primarily speaks the language of
2024-07-24 01:14:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 01:18:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.4785, -0.0337, -0.0532,  ...,  1.3174,  0.7090,  0.4460],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4736, -3.2656, -8.0469,  ..., -2.7305,  2.6406, -2.7363],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 7.1526e-04, -5.9090e-03, -1.4105e-03,  ..., -2.8934e-03,
          3.9978e-03, -4.5052e-03],
        [ 2.1553e-03, -7.9632e-04,  7.9346e-03,  ...,  6.8855e-03,
         -1.7815e-03,  7.8735e-03],
        [ 3.9825e-03,  1.0353e-02, -1.3458e-02,  ..., -7.8583e-04,
          1.9703e-03, -5.2261e-03],
        ...,
        [-1.8860e-02,  8.3351e-04,  4.0894e-03,  ...,  7.9269e-03,
         -1.9531e-03,  4.0054e-04],
        [-9.5520e-03, -9.1248e-03, -1.7667e-04,  ..., -8.4534e-03,
         -4.6387e-03, -2.1782e-03],
        [ 7.1716e-04, -8.3637e-04, -1.6060e-03,  ...,  7.9036e-05,
          9.7046e-03, -3.6125e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8159, -2.5684, -7.9570,  ..., -2.9180,  2.6699, -3.2148]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:18:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of guadeloupe primarily speaks the language of french
The country of venezuela primarily speaks the language of spanish
The country of brazil primarily speaks the language of portuguese
The country of argentina primarily speaks the language of spanish
The country of morocco primarily speaks the language of berber
The country of peru primarily speaks the language of spanish
The country of mozambique primarily speaks the language of portuguese
The country of philippines primarily speaks the language of
2024-07-24 01:18:14 root INFO     [order_1_approx] starting weight calculation for The country of philippines primarily speaks the language of tagalog
The country of brazil primarily speaks the language of portuguese
The country of morocco primarily speaks the language of berber
The country of guadeloupe primarily speaks the language of french
The country of argentina primarily speaks the language of spanish
The country of peru primarily speaks the language of spanish
The country of mozambique primarily speaks the language of portuguese
The country of venezuela primarily speaks the language of
2024-07-24 01:18:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 01:22:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6421,  0.3452, -0.9810,  ..., -0.5674,  0.1874, -0.1414],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4795, -3.5938, -3.7305,  ..., -2.9844,  0.7852, -0.4038],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0023, -0.0027, -0.0042,  ..., -0.0020,  0.0030, -0.0009],
        [ 0.0044,  0.0008, -0.0007,  ..., -0.0017, -0.0014,  0.0031],
        [-0.0040,  0.0020, -0.0002,  ...,  0.0022,  0.0027, -0.0021],
        ...,
        [-0.0026,  0.0028,  0.0065,  ...,  0.0031, -0.0008,  0.0048],
        [-0.0025,  0.0006, -0.0027,  ...,  0.0012, -0.0068, -0.0014],
        [ 0.0004, -0.0056,  0.0067,  ...,  0.0021,  0.0012,  0.0064]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3962, -3.5996, -3.9297,  ..., -2.7188,  0.9814, -0.2454]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:22:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of philippines primarily speaks the language of tagalog
The country of brazil primarily speaks the language of portuguese
The country of morocco primarily speaks the language of berber
The country of guadeloupe primarily speaks the language of french
The country of argentina primarily speaks the language of spanish
The country of peru primarily speaks the language of spanish
The country of mozambique primarily speaks the language of portuguese
The country of venezuela primarily speaks the language of
2024-07-24 01:22:03 root INFO     total operator prediction time: 1829.3897998332977 seconds
2024-07-24 01:22:03 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on animal - shelter
2024-07-24 01:22:03 root INFO     building operator animal - shelter
2024-07-24 01:22:03 root INFO     [order_1_approx] starting weight calculation for The place woodchuck lives in is called hole
The place mallard lives in is called nest
The place lion lives in is called den
The place rat lives in is called nest
The place dog lives in is called doghouse
The place goldfish lives in is called pond
The place locust lives in is called nest
The place chinchilla lives in is called
2024-07-24 01:22:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 01:25:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4160, -1.0957, -1.1934,  ...,  0.0596, -1.2656,  0.3315],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8711, -4.4219, -2.5195,  ..., -2.8750,  2.4199,  1.5449],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0004, -0.0129, -0.0102,  ...,  0.0012, -0.0086,  0.0010],
        [ 0.0099,  0.0049, -0.0036,  ..., -0.0005,  0.0206, -0.0067],
        [-0.0061,  0.0049, -0.0080,  ..., -0.0081, -0.0008,  0.0073],
        ...,
        [ 0.0006, -0.0099, -0.0043,  ...,  0.0070, -0.0064,  0.0192],
        [-0.0075,  0.0008,  0.0013,  ..., -0.0016, -0.0045,  0.0208],
        [-0.0037,  0.0013,  0.0066,  ..., -0.0083, -0.0022,  0.0111]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9478, -4.6914, -2.7461,  ..., -2.4648,  2.2891,  1.1250]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:25:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place woodchuck lives in is called hole
The place mallard lives in is called nest
The place lion lives in is called den
The place rat lives in is called nest
The place dog lives in is called doghouse
The place goldfish lives in is called pond
The place locust lives in is called nest
The place chinchilla lives in is called
2024-07-24 01:25:52 root INFO     [order_1_approx] starting weight calculation for The place woodchuck lives in is called hole
The place goldfish lives in is called pond
The place mallard lives in is called nest
The place lion lives in is called den
The place locust lives in is called nest
The place chinchilla lives in is called nest
The place rat lives in is called nest
The place dog lives in is called
2024-07-24 01:25:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 01:29:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7622, -0.7432,  0.0531,  ...,  0.9531, -1.0615,  0.8110],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8994, -7.9180, -0.9546,  ..., -5.8555,  4.3359,  3.4219],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0114, -0.0128,  0.0003,  ...,  0.0058, -0.0090, -0.0040],
        [ 0.0144,  0.0162,  0.0056,  ...,  0.0028,  0.0194, -0.0181],
        [ 0.0100,  0.0144,  0.0007,  ...,  0.0037,  0.0170,  0.0061],
        ...,
        [-0.0119, -0.0052,  0.0129,  ...,  0.0118, -0.0019,  0.0214],
        [-0.0088,  0.0047,  0.0111,  ...,  0.0006, -0.0092,  0.0071],
        [ 0.0009, -0.0052, -0.0045,  ...,  0.0032,  0.0009,  0.0004]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4758, -6.5547, -0.7754,  ..., -5.6406,  4.8711,  3.1602]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:29:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place woodchuck lives in is called hole
The place goldfish lives in is called pond
The place mallard lives in is called nest
The place lion lives in is called den
The place locust lives in is called nest
The place chinchilla lives in is called nest
The place rat lives in is called nest
The place dog lives in is called
2024-07-24 01:29:42 root INFO     [order_1_approx] starting weight calculation for The place rat lives in is called nest
The place woodchuck lives in is called hole
The place dog lives in is called doghouse
The place chinchilla lives in is called nest
The place lion lives in is called den
The place locust lives in is called nest
The place mallard lives in is called nest
The place goldfish lives in is called
2024-07-24 01:29:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 01:33:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3687, -0.0195,  0.2288,  ..., -0.4290, -0.6069,  0.3853],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.4648, -1.1045, -2.0098,  ..., -3.8398, -3.8320,  2.8125],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0107,  0.0005, -0.0118,  ..., -0.0016,  0.0052, -0.0034],
        [-0.0022,  0.0021,  0.0129,  ...,  0.0072,  0.0030, -0.0140],
        [-0.0010,  0.0134, -0.0053,  ..., -0.0099,  0.0122,  0.0140],
        ...,
        [-0.0056, -0.0074,  0.0061,  ...,  0.0049, -0.0056,  0.0037],
        [-0.0007, -0.0030, -0.0115,  ..., -0.0090,  0.0155,  0.0155],
        [ 0.0033, -0.0070,  0.0043,  ...,  0.0009,  0.0017,  0.0116]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.2891, -1.1592, -2.0547,  ..., -3.6074, -3.8691,  2.8926]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:33:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place rat lives in is called nest
The place woodchuck lives in is called hole
The place dog lives in is called doghouse
The place chinchilla lives in is called nest
The place lion lives in is called den
The place locust lives in is called nest
The place mallard lives in is called nest
The place goldfish lives in is called
2024-07-24 01:33:31 root INFO     [order_1_approx] starting weight calculation for The place rat lives in is called nest
The place woodchuck lives in is called hole
The place chinchilla lives in is called nest
The place dog lives in is called doghouse
The place locust lives in is called nest
The place mallard lives in is called nest
The place goldfish lives in is called pond
The place lion lives in is called
2024-07-24 01:33:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 01:37:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2659, -0.6816,  0.2258,  ...,  0.9951, -0.0625,  0.7549],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5312, -3.9336,  1.7080,  ..., -3.6113, -0.6533,  2.8594],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0024, -0.0103, -0.0075,  ...,  0.0135,  0.0002, -0.0008],
        [ 0.0099,  0.0024,  0.0020,  ...,  0.0049,  0.0097, -0.0154],
        [-0.0038,  0.0137,  0.0036,  ..., -0.0006,  0.0036,  0.0113],
        ...,
        [-0.0069, -0.0087,  0.0028,  ...,  0.0103, -0.0005,  0.0209],
        [ 0.0010, -0.0030,  0.0107,  ..., -0.0015, -0.0111, -0.0037],
        [-0.0059, -0.0063,  0.0077,  ...,  0.0079, -0.0019,  0.0209]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9473, -2.9648,  1.6104,  ..., -3.4297, -0.4746,  2.5410]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:37:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place rat lives in is called nest
The place woodchuck lives in is called hole
The place chinchilla lives in is called nest
The place dog lives in is called doghouse
The place locust lives in is called nest
The place mallard lives in is called nest
The place goldfish lives in is called pond
The place lion lives in is called
2024-07-24 01:37:19 root INFO     [order_1_approx] starting weight calculation for The place woodchuck lives in is called hole
The place lion lives in is called den
The place mallard lives in is called nest
The place chinchilla lives in is called nest
The place rat lives in is called nest
The place goldfish lives in is called pond
The place dog lives in is called doghouse
The place locust lives in is called
2024-07-24 01:37:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 01:41:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7920,  0.3489,  0.4663,  ...,  1.5547, -1.1748,  0.6963],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3613, -1.5312,  0.3584,  ..., -3.5156,  1.1807,  1.3506],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-6.4774e-03, -8.1100e-03, -1.5945e-03,  ...,  5.0430e-03,
          9.4604e-03, -7.2861e-04],
        [ 1.0071e-03,  1.7578e-02,  1.0063e-02,  ...,  6.6109e-03,
          8.0261e-03,  1.4381e-03],
        [ 7.0381e-04,  1.1110e-03,  3.2997e-04,  ..., -5.1003e-03,
         -4.9019e-04,  1.1581e-02],
        ...,
        [ 6.4850e-05, -1.5774e-03, -1.5965e-03,  ...,  4.3373e-03,
         -2.5711e-03,  7.5874e-03],
        [ 8.4915e-03,  8.6975e-04, -3.3283e-03,  ..., -7.4768e-03,
          1.4420e-03,  1.1887e-02],
        [-2.2415e-02, -1.3214e-02,  5.0354e-03,  ..., -1.4809e-02,
          1.5427e-02,  1.1734e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5771, -2.1641, -0.0796,  ..., -3.1289,  1.3145,  1.7881]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:41:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place woodchuck lives in is called hole
The place lion lives in is called den
The place mallard lives in is called nest
The place chinchilla lives in is called nest
The place rat lives in is called nest
The place goldfish lives in is called pond
The place dog lives in is called doghouse
The place locust lives in is called
2024-07-24 01:41:08 root INFO     [order_1_approx] starting weight calculation for The place woodchuck lives in is called hole
The place goldfish lives in is called pond
The place locust lives in is called nest
The place lion lives in is called den
The place dog lives in is called doghouse
The place chinchilla lives in is called nest
The place rat lives in is called nest
The place mallard lives in is called
2024-07-24 01:41:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 01:44:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4468, -0.7021, -0.2065,  ...,  1.2676, -1.9668, -0.4297],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5205, -0.5156, -3.7656,  ...,  0.2805, -0.2515,  0.4248],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0250, -0.0086,  0.0029,  ..., -0.0012,  0.0058, -0.0015],
        [ 0.0065,  0.0139,  0.0089,  ...,  0.0059, -0.0046, -0.0095],
        [ 0.0019,  0.0011, -0.0005,  ..., -0.0114,  0.0181,  0.0166],
        ...,
        [-0.0022, -0.0102,  0.0023,  ...,  0.0030, -0.0041,  0.0017],
        [-0.0119, -0.0011, -0.0063,  ..., -0.0017,  0.0178,  0.0146],
        [-0.0054, -0.0113,  0.0016,  ..., -0.0041,  0.0180,  0.0098]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.9414,  0.6436, -4.6289,  ...,  0.2383, -0.9463,  0.8608]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:44:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place woodchuck lives in is called hole
The place goldfish lives in is called pond
The place locust lives in is called nest
The place lion lives in is called den
The place dog lives in is called doghouse
The place chinchilla lives in is called nest
The place rat lives in is called nest
The place mallard lives in is called
2024-07-24 01:44:57 root INFO     [order_1_approx] starting weight calculation for The place woodchuck lives in is called hole
The place chinchilla lives in is called nest
The place dog lives in is called doghouse
The place locust lives in is called nest
The place mallard lives in is called nest
The place lion lives in is called den
The place goldfish lives in is called pond
The place rat lives in is called
2024-07-24 01:44:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 01:48:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8228,  0.1174, -0.1996,  ...,  0.3701, -1.8467,  0.5718],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.1934, -2.3984,  0.4980,  ..., -4.2383, -1.3281,  0.6348],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0141, -0.0189, -0.0062,  ...,  0.0152, -0.0027,  0.0086],
        [ 0.0036, -0.0005,  0.0133,  ...,  0.0127,  0.0063, -0.0048],
        [-0.0040,  0.0090, -0.0016,  ...,  0.0143,  0.0054,  0.0035],
        ...,
        [-0.0030, -0.0070,  0.0051,  ...,  0.0147, -0.0057,  0.0170],
        [-0.0064,  0.0049,  0.0109,  ..., -0.0010, -0.0040, -0.0036],
        [-0.0006, -0.0024, -0.0162,  ..., -0.0128, -0.0008, -0.0039]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.4629, -3.0547, -0.0396,  ..., -4.3477, -0.9868,  1.2520]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:48:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place woodchuck lives in is called hole
The place chinchilla lives in is called nest
The place dog lives in is called doghouse
The place locust lives in is called nest
The place mallard lives in is called nest
The place lion lives in is called den
The place goldfish lives in is called pond
The place rat lives in is called
2024-07-24 01:48:46 root INFO     [order_1_approx] starting weight calculation for The place lion lives in is called den
The place rat lives in is called nest
The place dog lives in is called doghouse
The place locust lives in is called nest
The place chinchilla lives in is called nest
The place mallard lives in is called nest
The place goldfish lives in is called pond
The place woodchuck lives in is called
2024-07-24 01:48:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 01:52:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3291,  0.2358, -0.3389,  ..., -0.2253, -0.5098,  0.6133],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7520, -1.4707, -4.0273,  ..., -2.8770,  2.4883,  2.6543],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0018,  0.0008, -0.0020,  ...,  0.0128,  0.0158,  0.0062],
        [-0.0029,  0.0012,  0.0067,  ...,  0.0059,  0.0036, -0.0062],
        [-0.0066,  0.0083,  0.0010,  ..., -0.0061,  0.0068,  0.0072],
        ...,
        [-0.0050, -0.0070,  0.0004,  ...,  0.0142,  0.0019,  0.0054],
        [-0.0123,  0.0080, -0.0072,  ..., -0.0106,  0.0022,  0.0129],
        [ 0.0090,  0.0035, -0.0089,  ..., -0.0075,  0.0050,  0.0065]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3677, -1.4072, -4.8398,  ..., -2.8066,  1.9883,  2.6406]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:52:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place lion lives in is called den
The place rat lives in is called nest
The place dog lives in is called doghouse
The place locust lives in is called nest
The place chinchilla lives in is called nest
The place mallard lives in is called nest
The place goldfish lives in is called pond
The place woodchuck lives in is called
2024-07-24 01:52:35 root INFO     total operator prediction time: 1832.4057993888855 seconds
2024-07-24 01:52:35 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on male - female
2024-07-24 01:52:35 root INFO     building operator male - female
2024-07-24 01:52:35 root INFO     [order_1_approx] starting weight calculation for A female dad is known as a mom
A female sculptor is known as a sculptress
A female murderer is known as a murderess
A female heir is known as a heiress
A female hero is known as a heroine
A female husband is known as a wife
A female lion is known as a lioness
A female boar is known as a
2024-07-24 01:52:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 01:56:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8359, -0.5151, -0.7256,  ...,  0.3657, -0.3904, -0.0874],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.2441,  0.5703, -1.8623,  ..., -4.1562,  0.3320, -0.0420],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-8.2703e-03, -7.7667e-03,  3.0065e-04,  ...,  1.2451e-02,
          6.8665e-04, -8.4610e-03],
        [-5.3558e-03, -9.5062e-03, -6.8283e-04,  ...,  1.0437e-02,
          7.4081e-03,  1.3992e-02],
        [ 1.3374e-02,  8.9111e-03, -7.0152e-03,  ..., -6.0883e-03,
          6.5269e-03,  3.8967e-03],
        ...,
        [ 1.2665e-03,  7.6294e-05,  1.4496e-03,  ..., -1.6296e-02,
          4.6349e-04,  8.8730e-03],
        [-7.7438e-03, -1.5533e-02,  2.3441e-03,  ...,  3.8948e-03,
         -1.0101e-02, -1.5907e-03],
        [-1.0277e-02,  1.1871e-02, -4.3144e-03,  ..., -1.3084e-02,
          1.3336e-02, -2.8214e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.3418,  0.3164, -1.7793,  ..., -3.3535,  0.1238, -0.0574]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 01:56:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female dad is known as a mom
A female sculptor is known as a sculptress
A female murderer is known as a murderess
A female heir is known as a heiress
A female hero is known as a heroine
A female husband is known as a wife
A female lion is known as a lioness
A female boar is known as a
2024-07-24 01:56:24 root INFO     [order_1_approx] starting weight calculation for A female boar is known as a sow
A female hero is known as a heroine
A female heir is known as a heiress
A female murderer is known as a murderess
A female sculptor is known as a sculptress
A female husband is known as a wife
A female lion is known as a lioness
A female dad is known as a
2024-07-24 01:56:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 02:00:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8511, -0.1010, -0.2167,  ..., -0.3042,  0.0993,  0.2173],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.2656, -7.6250, -2.1016,  ..., -1.7412,  6.2695, -1.7275],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.1955e-02, -5.0125e-03,  2.4300e-03,  ..., -3.8967e-03,
          1.4603e-02, -1.5533e-02],
        [-2.4872e-03, -2.0920e-02,  8.0109e-04,  ..., -3.8300e-02,
          1.3275e-03,  1.1650e-02],
        [-2.6169e-03,  6.4774e-03, -3.5583e-02,  ..., -2.8641e-02,
          1.6129e-02, -5.7220e-03],
        ...,
        [ 6.1035e-05, -8.0414e-03,  2.6989e-03,  ...,  3.2349e-03,
         -1.9058e-02,  8.6288e-03],
        [ 1.2589e-02,  6.2294e-03,  1.1665e-02,  ...,  1.3794e-02,
         -3.4698e-02, -7.1487e-03],
        [ 9.4604e-03,  2.0279e-02, -4.4785e-03,  ..., -8.6060e-03,
          2.0416e-02, -1.1734e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.3594, -7.7266, -1.2988,  ..., -2.8164,  6.4141, -1.3301]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:00:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female boar is known as a sow
A female hero is known as a heroine
A female heir is known as a heiress
A female murderer is known as a murderess
A female sculptor is known as a sculptress
A female husband is known as a wife
A female lion is known as a lioness
A female dad is known as a
2024-07-24 02:00:13 root INFO     [order_1_approx] starting weight calculation for A female dad is known as a mom
A female lion is known as a lioness
A female sculptor is known as a sculptress
A female hero is known as a heroine
A female murderer is known as a murderess
A female boar is known as a sow
A female husband is known as a wife
A female heir is known as a
2024-07-24 02:00:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 02:04:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.0612, 0.4646, 0.4783,  ..., 2.2539, 1.2881, 0.2363], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2134, -0.4160, -2.7461,  ...,  0.0947, -3.9727, -1.4727],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0065,  0.0033,  0.0095,  ..., -0.0081, -0.0102, -0.0239],
        [-0.0017, -0.0095, -0.0061,  ...,  0.0013,  0.0054, -0.0038],
        [ 0.0051,  0.0067, -0.0116,  ...,  0.0036,  0.0051, -0.0052],
        ...,
        [-0.0078, -0.0109, -0.0019,  ...,  0.0060, -0.0040,  0.0022],
        [ 0.0013,  0.0018,  0.0066,  ...,  0.0098, -0.0101,  0.0018],
        [-0.0022,  0.0094,  0.0177,  ...,  0.0033,  0.0157, -0.0026]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.8955,  0.3066, -2.8379,  ...,  0.8647, -4.0312, -2.1387]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:04:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female dad is known as a mom
A female lion is known as a lioness
A female sculptor is known as a sculptress
A female hero is known as a heroine
A female murderer is known as a murderess
A female boar is known as a sow
A female husband is known as a wife
A female heir is known as a
2024-07-24 02:04:02 root INFO     [order_1_approx] starting weight calculation for A female husband is known as a wife
A female boar is known as a sow
A female heir is known as a heiress
A female dad is known as a mom
A female lion is known as a lioness
A female sculptor is known as a sculptress
A female murderer is known as a murderess
A female hero is known as a
2024-07-24 02:04:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 02:07:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0510,  0.9121,  1.5889,  ...,  0.3936,  0.3284, -0.6982],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.7773, -1.1309, -4.5547,  ..., -3.2656,  2.8320,  2.4277],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-6.2561e-03, -1.2131e-02, -6.7949e-04,  ..., -3.8681e-03,
         -6.1035e-04, -4.7646e-03],
        [ 6.1951e-03, -2.7710e-02, -1.6769e-02,  ..., -5.4092e-03,
         -1.6956e-03,  1.5392e-03],
        [ 1.9623e-02,  1.1047e-02, -9.1248e-03,  ..., -9.8610e-04,
          9.7351e-03, -1.2535e-02],
        ...,
        [ 8.1635e-04,  2.8896e-04,  9.0637e-03,  ...,  5.7220e-05,
         -5.5084e-03, -1.6190e-02],
        [-8.1787e-03,  9.5978e-03,  3.9825e-03,  ...,  2.0813e-02,
         -1.7792e-02,  5.7869e-03],
        [-1.9379e-03, -1.1787e-02,  9.1858e-03,  ..., -8.9035e-03,
          7.2784e-03, -3.2921e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.7676, -0.9229, -4.8477,  ..., -3.1523,  3.0410,  1.8574]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:07:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female husband is known as a wife
A female boar is known as a sow
A female heir is known as a heiress
A female dad is known as a mom
A female lion is known as a lioness
A female sculptor is known as a sculptress
A female murderer is known as a murderess
A female hero is known as a
2024-07-24 02:07:51 root INFO     [order_1_approx] starting weight calculation for A female heir is known as a heiress
A female lion is known as a lioness
A female boar is known as a sow
A female sculptor is known as a sculptress
A female dad is known as a mom
A female hero is known as a heroine
A female murderer is known as a murderess
A female husband is known as a
2024-07-24 02:07:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 02:11:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.6719, -0.1837,  0.6538,  ...,  1.0010,  0.4255,  0.3804],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.8066, -2.9512, -1.0322,  ..., -4.8984,  1.3408,  0.1006],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-4.0817e-03,  6.0730e-03,  2.1210e-02,  ...,  3.0518e-05,
          1.8173e-02, -5.6381e-03],
        [-4.9515e-03, -1.1429e-02,  5.5504e-03,  ..., -4.3869e-04,
         -7.8773e-04, -7.8583e-03],
        [-2.9716e-03,  4.8370e-03, -1.1581e-02,  ...,  5.4092e-03,
         -5.2490e-03, -3.2005e-03],
        ...,
        [-1.4359e-02, -1.8845e-03,  1.4267e-02,  ..., -1.8616e-03,
         -2.0157e-02,  1.1017e-02],
        [ 1.0956e-02,  7.5912e-03,  8.3923e-04,  ...,  5.5923e-03,
         -1.6441e-03,  1.1063e-02],
        [-2.8992e-04, -6.2256e-03,  1.1734e-02,  ...,  7.2632e-03,
          3.4088e-02,  1.0056e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.9609, -2.8457, -1.1924,  ..., -3.7305,  1.2002, -0.0756]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:11:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female heir is known as a heiress
A female lion is known as a lioness
A female boar is known as a sow
A female sculptor is known as a sculptress
A female dad is known as a mom
A female hero is known as a heroine
A female murderer is known as a murderess
A female husband is known as a
2024-07-24 02:11:39 root INFO     [order_1_approx] starting weight calculation for A female dad is known as a mom
A female murderer is known as a murderess
A female hero is known as a heroine
A female boar is known as a sow
A female husband is known as a wife
A female heir is known as a heiress
A female sculptor is known as a sculptress
A female lion is known as a
2024-07-24 02:11:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 02:15:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3562, -0.9189,  0.3755,  ...,  0.5962,  0.6387,  0.3853],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.6328,  0.8691,  0.8066,  ..., -3.6504, -2.5781,  0.3616],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0123, -0.0049,  0.0066,  ..., -0.0005,  0.0088, -0.0164],
        [-0.0026, -0.0135, -0.0048,  ...,  0.0151,  0.0002,  0.0003],
        [ 0.0120,  0.0110,  0.0003,  ...,  0.0088,  0.0129,  0.0155],
        ...,
        [ 0.0021, -0.0142, -0.0075,  ..., -0.0046, -0.0026,  0.0061],
        [ 0.0081, -0.0175,  0.0044,  ...,  0.0023, -0.0249,  0.0030],
        [-0.0328, -0.0049, -0.0159,  ...,  0.0024,  0.0104,  0.0038]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1875,  1.0801,  0.0532,  ..., -3.1094, -2.3789,  0.6123]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:15:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female dad is known as a mom
A female murderer is known as a murderess
A female hero is known as a heroine
A female boar is known as a sow
A female husband is known as a wife
A female heir is known as a heiress
A female sculptor is known as a sculptress
A female lion is known as a
2024-07-24 02:15:29 root INFO     [order_1_approx] starting weight calculation for A female boar is known as a sow
A female husband is known as a wife
A female hero is known as a heroine
A female heir is known as a heiress
A female lion is known as a lioness
A female sculptor is known as a sculptress
A female dad is known as a mom
A female murderer is known as a
2024-07-24 02:15:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 02:19:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8140, -0.3018,  0.3218,  ...,  0.4265, -0.4072, -0.1572],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6035, -3.9766, -0.2891,  ...,  0.2510,  3.1816, -1.9346],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0063, -0.0214, -0.0156,  ...,  0.0063,  0.0069, -0.0262],
        [ 0.0115,  0.0020, -0.0035,  ...,  0.0056, -0.0009,  0.0068],
        [-0.0065,  0.0095, -0.0045,  ..., -0.0050,  0.0054,  0.0192],
        ...,
        [-0.0323, -0.0231,  0.0105,  ...,  0.0038, -0.0199,  0.0073],
        [ 0.0089, -0.0103,  0.0163,  ...,  0.0015, -0.0134,  0.0139],
        [-0.0001,  0.0117, -0.0135,  ..., -0.0098,  0.0121, -0.0089]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1680, -3.3457, -0.8174,  ...,  0.6367,  3.7461, -2.5859]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:19:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female boar is known as a sow
A female husband is known as a wife
A female hero is known as a heroine
A female heir is known as a heiress
A female lion is known as a lioness
A female sculptor is known as a sculptress
A female dad is known as a mom
A female murderer is known as a
2024-07-24 02:19:18 root INFO     [order_1_approx] starting weight calculation for A female dad is known as a mom
A female lion is known as a lioness
A female heir is known as a heiress
A female hero is known as a heroine
A female murderer is known as a murderess
A female husband is known as a wife
A female boar is known as a sow
A female sculptor is known as a
2024-07-24 02:19:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 02:23:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4414, -1.6768,  0.1533,  ..., -0.8931,  1.2676,  0.4478],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.9492,  2.6777, -4.0859,  ..., -1.5107, -3.5215, -2.3340],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0020,  0.0014, -0.0034,  ..., -0.0058,  0.0003, -0.0081],
        [-0.0086, -0.0101,  0.0021,  ...,  0.0113,  0.0123,  0.0037],
        [ 0.0223,  0.0007, -0.0006,  ..., -0.0049,  0.0110,  0.0045],
        ...,
        [-0.0059,  0.0007, -0.0006,  ..., -0.0051, -0.0033,  0.0050],
        [-0.0055, -0.0032,  0.0096,  ...,  0.0049, -0.0189, -0.0018],
        [-0.0063,  0.0029,  0.0004,  ...,  0.0018,  0.0048,  0.0064]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.1953,  2.8164, -5.2891,  ..., -1.5186, -3.0410, -2.0410]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:23:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female dad is known as a mom
A female lion is known as a lioness
A female heir is known as a heiress
A female hero is known as a heroine
A female murderer is known as a murderess
A female husband is known as a wife
A female boar is known as a sow
A female sculptor is known as a
2024-07-24 02:23:07 root INFO     total operator prediction time: 1832.1559610366821 seconds
2024-07-24 02:23:07 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on name - occupation
2024-07-24 02:23:07 root INFO     building operator name - occupation
2024-07-24 02:23:07 root INFO     [order_1_approx] starting weight calculation for truman was known for their work as a  president
hitler was known for their work as a  dictator
hawking was known for their work as a  physicist
marx was known for their work as a  philosopher
euler was known for their work as a  mathematician
napoleon was known for their work as a  emperor
hegel was known for their work as a  philosopher
dante was known for their work as a 
2024-07-24 02:23:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 02:26:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6304, -0.4255, -0.0737,  ...,  0.5107, -0.5195, -0.3862],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2217,  1.7920, -6.7344,  ..., -4.0820,  2.6816, -1.5322],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0097,  0.0020, -0.0040,  ..., -0.0064,  0.0087,  0.0001],
        [ 0.0062,  0.0090,  0.0093,  ...,  0.0093, -0.0049, -0.0050],
        [-0.0005, -0.0105, -0.0100,  ..., -0.0065,  0.0070,  0.0036],
        ...,
        [-0.0092,  0.0035, -0.0013,  ..., -0.0064,  0.0069,  0.0010],
        [ 0.0023,  0.0014,  0.0086,  ...,  0.0108, -0.0029, -0.0025],
        [-0.0077, -0.0125,  0.0002,  ...,  0.0060,  0.0061,  0.0026]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.8809,  1.7412, -6.6484,  ..., -3.5977,  2.8262, -2.1953]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:26:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for truman was known for their work as a  president
hitler was known for their work as a  dictator
hawking was known for their work as a  physicist
marx was known for their work as a  philosopher
euler was known for their work as a  mathematician
napoleon was known for their work as a  emperor
hegel was known for their work as a  philosopher
dante was known for their work as a 
2024-07-24 02:26:56 root INFO     [order_1_approx] starting weight calculation for hegel was known for their work as a  philosopher
truman was known for their work as a  president
marx was known for their work as a  philosopher
hawking was known for their work as a  physicist
hitler was known for their work as a  dictator
dante was known for their work as a  poet
napoleon was known for their work as a  emperor
euler was known for their work as a 
2024-07-24 02:26:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 02:30:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1091,  0.4004,  0.2788,  ...,  1.1191, -0.2480,  0.4836],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2080,  0.3459, -0.0425,  ..., -1.5693, -5.0898,  2.0195],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0074, -0.0060, -0.0098,  ..., -0.0046,  0.0014, -0.0062],
        [-0.0200, -0.0009,  0.0081,  ...,  0.0047,  0.0097, -0.0100],
        [ 0.0076,  0.0068, -0.0119,  ...,  0.0061,  0.0107, -0.0091],
        ...,
        [ 0.0059,  0.0002, -0.0125,  ..., -0.0018, -0.0168,  0.0066],
        [ 0.0041, -0.0212, -0.0115,  ...,  0.0013, -0.0040, -0.0006],
        [-0.0107,  0.0033, -0.0057,  ...,  0.0011,  0.0098,  0.0057]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9902,  0.7305, -0.4929,  ..., -1.4189, -4.8906,  2.2949]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:30:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for hegel was known for their work as a  philosopher
truman was known for their work as a  president
marx was known for their work as a  philosopher
hawking was known for their work as a  physicist
hitler was known for their work as a  dictator
dante was known for their work as a  poet
napoleon was known for their work as a  emperor
euler was known for their work as a 
2024-07-24 02:30:45 root INFO     [order_1_approx] starting weight calculation for hegel was known for their work as a  philosopher
dante was known for their work as a  poet
hitler was known for their work as a  dictator
euler was known for their work as a  mathematician
truman was known for their work as a  president
marx was known for their work as a  philosopher
napoleon was known for their work as a  emperor
hawking was known for their work as a 
2024-07-24 02:30:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 02:34:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6182,  1.1904, -1.2295,  ...,  0.4934,  0.2634,  0.5156],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9932, -0.2620, -2.3066,  ..., -0.4033,  0.4375, -0.7852],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0053, -0.0023,  0.0046,  ..., -0.0114,  0.0175,  0.0045],
        [ 0.0028, -0.0052,  0.0066,  ...,  0.0212, -0.0009,  0.0034],
        [ 0.0202, -0.0173, -0.0066,  ...,  0.0022,  0.0032, -0.0079],
        ...,
        [-0.0123, -0.0078, -0.0118,  ..., -0.0049, -0.0154,  0.0014],
        [ 0.0053, -0.0183, -0.0093,  ...,  0.0141, -0.0029, -0.0166],
        [-0.0075, -0.0068, -0.0083,  ..., -0.0107,  0.0092,  0.0118]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5811, -0.0582, -2.6016,  ..., -1.0332, -0.2271, -1.1025]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:34:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for hegel was known for their work as a  philosopher
dante was known for their work as a  poet
hitler was known for their work as a  dictator
euler was known for their work as a  mathematician
truman was known for their work as a  president
marx was known for their work as a  philosopher
napoleon was known for their work as a  emperor
hawking was known for their work as a 
2024-07-24 02:34:34 root INFO     [order_1_approx] starting weight calculation for euler was known for their work as a  mathematician
napoleon was known for their work as a  emperor
marx was known for their work as a  philosopher
hitler was known for their work as a  dictator
dante was known for their work as a  poet
truman was known for their work as a  president
hawking was known for their work as a  physicist
hegel was known for their work as a 
2024-07-24 02:34:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 02:38:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.6641, -0.1919, -0.1542,  ...,  1.0137,  0.2520,  1.3418],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8262, -4.6875, -2.5273,  ..., -6.2266, -1.8047, -3.1836],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-9.0790e-04, -2.0428e-03,  2.3975e-03,  ..., -6.2408e-03,
          4.5319e-03, -1.0666e-02],
        [ 1.4130e-02,  2.4929e-03, -5.4169e-03,  ...,  1.9012e-02,
          1.7719e-03, -1.5762e-02],
        [-7.6523e-03, -5.6725e-03, -1.6617e-02,  ..., -1.3580e-02,
          6.9351e-03,  8.3351e-04],
        ...,
        [-9.8877e-03,  4.1313e-03, -4.5662e-03,  ..., -4.1695e-03,
         -9.9182e-05,  1.4477e-03],
        [-1.7643e-03, -9.9106e-03, -2.6436e-03,  ...,  8.4534e-03,
         -3.5610e-03,  1.2589e-03],
        [-1.5404e-02,  1.8177e-03,  1.4973e-03,  ..., -1.0330e-02,
          5.1422e-03,  1.6037e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2686, -5.0859, -2.6035,  ..., -6.6641, -1.6455, -2.8574]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:38:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for euler was known for their work as a  mathematician
napoleon was known for their work as a  emperor
marx was known for their work as a  philosopher
hitler was known for their work as a  dictator
dante was known for their work as a  poet
truman was known for their work as a  president
hawking was known for their work as a  physicist
hegel was known for their work as a 
2024-07-24 02:38:23 root INFO     [order_1_approx] starting weight calculation for marx was known for their work as a  philosopher
hegel was known for their work as a  philosopher
euler was known for their work as a  mathematician
hawking was known for their work as a  physicist
truman was known for their work as a  president
napoleon was known for their work as a  emperor
dante was known for their work as a  poet
hitler was known for their work as a 
2024-07-24 02:38:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 02:42:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6787,  0.8394, -1.0674,  ...,  0.1760, -1.0596, -1.2139],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7578, -2.8281, -6.3555,  ..., -1.2188, -2.0410, -2.3457],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.0239e-02, -1.4885e-02, -1.0162e-02,  ..., -7.8583e-03,
          6.8054e-03, -1.7796e-03],
        [-1.0147e-02,  4.1122e-03, -1.2672e-02,  ...,  1.6785e-02,
          2.6093e-03, -1.0544e-02],
        [ 2.9755e-04, -1.3084e-03, -1.2833e-02,  ...,  1.6632e-03,
          8.9417e-03, -1.8143e-02],
        ...,
        [-3.7479e-03,  4.3221e-03,  2.3880e-03,  ...,  1.0742e-02,
         -2.4109e-03,  8.6069e-05],
        [ 8.8425e-03, -3.4657e-03, -9.1858e-03,  ...,  3.2425e-03,
         -1.1337e-02, -4.7302e-04],
        [-2.1286e-03, -1.0986e-03,  9.2697e-04,  ..., -5.8975e-03,
          2.0081e-02, -1.2589e-04]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.8975, -2.7344, -6.6172,  ..., -2.6133, -2.5859, -2.4141]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:42:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for marx was known for their work as a  philosopher
hegel was known for their work as a  philosopher
euler was known for their work as a  mathematician
hawking was known for their work as a  physicist
truman was known for their work as a  president
napoleon was known for their work as a  emperor
dante was known for their work as a  poet
hitler was known for their work as a 
2024-07-24 02:42:12 root INFO     [order_1_approx] starting weight calculation for truman was known for their work as a  president
napoleon was known for their work as a  emperor
dante was known for their work as a  poet
hegel was known for their work as a  philosopher
hawking was known for their work as a  physicist
hitler was known for their work as a  dictator
euler was known for their work as a  mathematician
marx was known for their work as a 
2024-07-24 02:42:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 02:46:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7124,  0.0864, -0.0334,  ..., -0.5332, -1.0234, -0.3167],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6543, -3.3594, -6.2461,  ..., -4.7344, -1.7949, -1.4395],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 7.1640e-03, -1.0052e-03, -2.5043e-03,  ..., -7.7896e-03,
         -1.3027e-03,  1.4553e-03],
        [ 4.8752e-03, -1.8215e-03, -2.2888e-05,  ...,  4.8370e-03,
          3.3283e-03, -1.1139e-03],
        [ 7.2861e-03,  7.8583e-03, -9.0408e-03,  ..., -1.6129e-02,
         -6.5384e-03, -1.0254e-02],
        ...,
        [-4.0512e-03,  0.0000e+00, -5.0125e-03,  ...,  2.9640e-03,
         -5.1346e-03, -6.6032e-03],
        [ 3.0708e-03, -3.6068e-03,  3.6392e-03,  ...,  8.8348e-03,
          1.3256e-04, -1.4915e-03],
        [-6.6757e-03, -4.1542e-03,  9.6054e-03,  ..., -2.3899e-03,
         -2.3403e-03,  1.2660e-04]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2947, -3.6016, -6.1602,  ..., -4.7617, -1.2090, -1.2881]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:46:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for truman was known for their work as a  president
napoleon was known for their work as a  emperor
dante was known for their work as a  poet
hegel was known for their work as a  philosopher
hawking was known for their work as a  physicist
hitler was known for their work as a  dictator
euler was known for their work as a  mathematician
marx was known for their work as a 
2024-07-24 02:46:02 root INFO     [order_1_approx] starting weight calculation for truman was known for their work as a  president
hitler was known for their work as a  dictator
hawking was known for their work as a  physicist
hegel was known for their work as a  philosopher
dante was known for their work as a  poet
marx was known for their work as a  philosopher
euler was known for their work as a  mathematician
napoleon was known for their work as a 
2024-07-24 02:46:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 02:49:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2715, -0.5474,  0.4736,  ...,  0.1516, -0.2476,  0.5674],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.0469, -1.7490, -3.7383,  ..., -2.5957, -0.5864, -1.7520],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.5516e-03, -3.4790e-03, -3.1242e-03,  ..., -9.7656e-04,
          8.0490e-03, -1.3687e-02],
        [ 7.1640e-03,  5.5199e-03,  4.1809e-03,  ...,  1.4076e-03,
         -7.5073e-03, -5.2185e-03],
        [-3.1357e-03,  1.2344e-02, -4.9057e-03,  ..., -1.2405e-02,
          9.4528e-03, -1.3016e-02],
        ...,
        [-5.1575e-03, -6.4430e-03,  4.0741e-03,  ..., -6.9618e-05,
          6.2752e-03,  7.6828e-03],
        [ 5.1613e-03,  7.6752e-03,  4.1199e-04,  ..., -7.4959e-04,
         -2.1935e-03, -9.4910e-03],
        [ 4.2439e-05, -1.1467e-02,  1.5907e-03,  ...,  1.6508e-03,
          1.5163e-03,  6.0081e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.7402, -2.1523, -3.7715,  ..., -2.5352, -0.1077, -1.6250]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:49:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for truman was known for their work as a  president
hitler was known for their work as a  dictator
hawking was known for their work as a  physicist
hegel was known for their work as a  philosopher
dante was known for their work as a  poet
marx was known for their work as a  philosopher
euler was known for their work as a  mathematician
napoleon was known for their work as a 
2024-07-24 02:49:51 root INFO     [order_1_approx] starting weight calculation for hegel was known for their work as a  philosopher
hitler was known for their work as a  dictator
dante was known for their work as a  poet
euler was known for their work as a  mathematician
marx was known for their work as a  philosopher
napoleon was known for their work as a  emperor
hawking was known for their work as a  physicist
truman was known for their work as a 
2024-07-24 02:49:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 02:53:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9688,  0.4521, -0.1531,  ...,  1.1484, -0.4517, -0.4741],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0312, -0.1893, -0.5000,  ..., -2.8848,  2.0449, -1.6016],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.5945e-02,  5.9166e-03, -2.8877e-03,  ..., -1.0551e-02,
          6.7596e-03, -3.0956e-03],
        [-4.4403e-03, -2.9945e-03, -1.9150e-03,  ...,  6.8855e-03,
          7.0930e-06, -5.7793e-03],
        [-2.8648e-03,  3.1986e-03, -3.7117e-03,  ...,  4.4441e-03,
         -2.1992e-03, -8.6212e-03],
        ...,
        [-3.0823e-03, -7.7896e-03,  3.7098e-03,  ..., -3.4790e-03,
         -1.1047e-02,  6.9962e-03],
        [ 5.8556e-03, -4.6043e-03,  9.5291e-03,  ...,  1.5778e-02,
         -4.0932e-03, -1.1444e-02],
        [ 1.4992e-03,  9.6321e-05,  1.3962e-03,  ..., -3.6411e-03,
          9.6283e-03,  1.9531e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2080,  0.2194, -0.7153,  ..., -2.7969,  2.7871, -1.8652]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:53:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for hegel was known for their work as a  philosopher
hitler was known for their work as a  dictator
dante was known for their work as a  poet
euler was known for their work as a  mathematician
marx was known for their work as a  philosopher
napoleon was known for their work as a  emperor
hawking was known for their work as a  physicist
truman was known for their work as a 
2024-07-24 02:53:40 root INFO     total operator prediction time: 1832.742716550827 seconds
2024-07-24 02:53:40 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on country - capital
2024-07-24 02:53:40 root INFO     building operator country - capital
2024-07-24 02:53:40 root INFO     [order_1_approx] starting weight calculation for The country with nairobi as its capital is known as kenya
The country with warsaw as its capital is known as poland
The country with bern as its capital is known as switzerland
The country with jakarta as its capital is known as indonesia
The country with dublin as its capital is known as ireland
The country with canberra as its capital is known as australia
The country with sofia as its capital is known as bulgaria
The country with amman as its capital is known as
2024-07-24 02:53:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 02:57:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9966, -1.4268,  0.0764,  ..., -0.9624, -1.2695, -0.4929],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7773,  1.3994, -0.5073,  ..., -2.0254, -0.1875, -3.3125],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0034, -0.0160, -0.0077,  ...,  0.0163,  0.0033, -0.0086],
        [-0.0085, -0.0113,  0.0132,  ...,  0.0104, -0.0039, -0.0020],
        [ 0.0048,  0.0023,  0.0154,  ..., -0.0131, -0.0018,  0.0129],
        ...,
        [-0.0224,  0.0025, -0.0006,  ...,  0.0036, -0.0035,  0.0177],
        [-0.0067, -0.0034, -0.0163,  ..., -0.0047,  0.0030,  0.0153],
        [-0.0042, -0.0015, -0.0106,  ...,  0.0048,  0.0059, -0.0037]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1553,  1.4893, -0.6279,  ..., -1.5693, -0.2343, -3.1562]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 02:57:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with nairobi as its capital is known as kenya
The country with warsaw as its capital is known as poland
The country with bern as its capital is known as switzerland
The country with jakarta as its capital is known as indonesia
The country with dublin as its capital is known as ireland
The country with canberra as its capital is known as australia
The country with sofia as its capital is known as bulgaria
The country with amman as its capital is known as
2024-07-24 02:57:30 root INFO     [order_1_approx] starting weight calculation for The country with jakarta as its capital is known as indonesia
The country with dublin as its capital is known as ireland
The country with amman as its capital is known as jordan
The country with nairobi as its capital is known as kenya
The country with warsaw as its capital is known as poland
The country with sofia as its capital is known as bulgaria
The country with canberra as its capital is known as australia
The country with bern as its capital is known as
2024-07-24 02:57:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 03:01:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9941,  0.1311, -1.6123,  ...,  1.0234, -0.1328, -0.4976],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5000,  2.4434, -3.7480,  ..., -2.9492, -4.1172, -2.9141],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0056,  0.0011, -0.0024,  ...,  0.0024,  0.0044,  0.0007],
        [ 0.0029, -0.0076,  0.0057,  ...,  0.0039,  0.0040, -0.0092],
        [ 0.0059,  0.0059, -0.0072,  ..., -0.0071, -0.0111, -0.0008],
        ...,
        [-0.0055, -0.0080, -0.0003,  ...,  0.0109, -0.0060, -0.0105],
        [-0.0062, -0.0048, -0.0007,  ...,  0.0006, -0.0016,  0.0173],
        [ 0.0005, -0.0033, -0.0050,  ...,  0.0023,  0.0090,  0.0064]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2559,  2.8477, -4.1719,  ..., -2.4375, -4.2305, -2.5820]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:01:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with jakarta as its capital is known as indonesia
The country with dublin as its capital is known as ireland
The country with amman as its capital is known as jordan
The country with nairobi as its capital is known as kenya
The country with warsaw as its capital is known as poland
The country with sofia as its capital is known as bulgaria
The country with canberra as its capital is known as australia
The country with bern as its capital is known as
2024-07-24 03:01:19 root INFO     [order_1_approx] starting weight calculation for The country with dublin as its capital is known as ireland
The country with nairobi as its capital is known as kenya
The country with jakarta as its capital is known as indonesia
The country with bern as its capital is known as switzerland
The country with sofia as its capital is known as bulgaria
The country with warsaw as its capital is known as poland
The country with amman as its capital is known as jordan
The country with canberra as its capital is known as
2024-07-24 03:01:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 03:05:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6426, -0.8311, -1.0176,  ...,  0.4302,  0.5508,  0.5840],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6064,  0.5078,  0.0762,  ..., -3.5000, -0.4414, -7.0195],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-6.4201e-03, -5.5122e-04, -7.4615e-03,  ...,  1.0468e-02,
          8.2779e-04,  2.2125e-04],
        [-6.6223e-03, -1.0818e-02,  1.2360e-02,  ...,  6.7024e-03,
          9.5215e-03, -4.2114e-03],
        [-1.3294e-03,  8.7967e-03,  6.1035e-04,  ...,  8.4839e-03,
         -5.3062e-03,  3.7537e-03],
        ...,
        [-9.1248e-03, -5.2567e-03,  7.4959e-04,  ...,  8.2092e-03,
         -1.3756e-02,  2.3918e-03],
        [-1.1238e-02,  6.6757e-05, -6.3438e-03,  ...,  1.1787e-03,
         -1.9348e-02,  1.0963e-02],
        [-2.9888e-03, -8.6441e-03, -7.4120e-03,  ...,  6.6757e-04,
         -1.3533e-03,  1.0147e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8042,  1.1553,  0.2296,  ..., -3.7129, -1.0801, -6.3984]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:05:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with dublin as its capital is known as ireland
The country with nairobi as its capital is known as kenya
The country with jakarta as its capital is known as indonesia
The country with bern as its capital is known as switzerland
The country with sofia as its capital is known as bulgaria
The country with warsaw as its capital is known as poland
The country with amman as its capital is known as jordan
The country with canberra as its capital is known as
2024-07-24 03:05:08 root INFO     [order_1_approx] starting weight calculation for The country with canberra as its capital is known as australia
The country with sofia as its capital is known as bulgaria
The country with warsaw as its capital is known as poland
The country with bern as its capital is known as switzerland
The country with jakarta as its capital is known as indonesia
The country with amman as its capital is known as jordan
The country with nairobi as its capital is known as kenya
The country with dublin as its capital is known as
2024-07-24 03:05:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 03:08:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0078,  0.0654, -0.9756,  ..., -0.0376, -0.7070,  0.8203],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5073,  1.3203,  2.3340,  ..., -1.1250,  2.0039, -0.9727],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-9.3689e-03, -5.3711e-03, -9.1553e-03,  ...,  1.3580e-02,
          1.6069e-03,  2.5520e-03],
        [-2.6474e-03, -3.1357e-03,  1.0696e-02,  ...,  5.5237e-03,
         -5.0278e-03,  7.5340e-03],
        [-7.2632e-03,  4.3716e-03, -5.3177e-03,  ...,  6.6223e-03,
         -6.9351e-03, -6.5269e-03],
        ...,
        [-1.9798e-03,  6.1264e-03, -5.8784e-03,  ...,  5.6458e-04,
         -1.1238e-02, -9.0599e-05],
        [-4.5433e-03, -1.5869e-03, -9.7198e-03,  ..., -9.0027e-03,
         -5.8441e-03, -2.3003e-03],
        [-8.5640e-04, -2.9945e-03, -3.1509e-03,  ...,  1.6861e-03,
          8.6365e-03, -8.5068e-04]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0129,  1.6875,  2.1582,  ..., -1.5439,  1.0439, -0.1821]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:08:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with canberra as its capital is known as australia
The country with sofia as its capital is known as bulgaria
The country with warsaw as its capital is known as poland
The country with bern as its capital is known as switzerland
The country with jakarta as its capital is known as indonesia
The country with amman as its capital is known as jordan
The country with nairobi as its capital is known as kenya
The country with dublin as its capital is known as
2024-07-24 03:08:56 root INFO     [order_1_approx] starting weight calculation for The country with dublin as its capital is known as ireland
The country with sofia as its capital is known as bulgaria
The country with amman as its capital is known as jordan
The country with bern as its capital is known as switzerland
The country with canberra as its capital is known as australia
The country with nairobi as its capital is known as kenya
The country with warsaw as its capital is known as poland
The country with jakarta as its capital is known as
2024-07-24 03:08:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 03:12:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0007,  0.2396,  0.2383,  ..., -0.1426,  0.0415,  0.3018],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6104, -2.2852, -1.4102,  ..., -4.8711, -0.2598, -1.1084],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0028, -0.0025, -0.0111,  ...,  0.0009, -0.0010, -0.0028],
        [-0.0029, -0.0085,  0.0200,  ...,  0.0083,  0.0139, -0.0091],
        [ 0.0064,  0.0052, -0.0072,  ..., -0.0015,  0.0023, -0.0087],
        ...,
        [ 0.0016,  0.0006, -0.0164,  ...,  0.0003, -0.0044,  0.0017],
        [ 0.0075, -0.0046, -0.0260,  ...,  0.0030, -0.0074, -0.0013],
        [-0.0109, -0.0038, -0.0107,  ..., -0.0103,  0.0035,  0.0003]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5620, -1.8887, -1.9424,  ..., -4.8242, -0.7363, -1.1416]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:12:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with dublin as its capital is known as ireland
The country with sofia as its capital is known as bulgaria
The country with amman as its capital is known as jordan
The country with bern as its capital is known as switzerland
The country with canberra as its capital is known as australia
The country with nairobi as its capital is known as kenya
The country with warsaw as its capital is known as poland
The country with jakarta as its capital is known as
2024-07-24 03:12:46 root INFO     [order_1_approx] starting weight calculation for The country with jakarta as its capital is known as indonesia
The country with warsaw as its capital is known as poland
The country with dublin as its capital is known as ireland
The country with canberra as its capital is known as australia
The country with bern as its capital is known as switzerland
The country with sofia as its capital is known as bulgaria
The country with amman as its capital is known as jordan
The country with nairobi as its capital is known as
2024-07-24 03:12:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 03:16:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5181, -1.2354, -0.8218,  ...,  0.0063,  1.4473,  0.8765],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8770,  5.5156,  3.5254,  ...,  1.1260,  1.3418, -2.9902],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0022,  0.0076, -0.0143,  ..., -0.0014,  0.0032, -0.0084],
        [-0.0149, -0.0102,  0.0210,  ...,  0.0239,  0.0119,  0.0097],
        [-0.0014, -0.0043, -0.0077,  ...,  0.0133, -0.0046,  0.0059],
        ...,
        [-0.0154,  0.0066, -0.0057,  ...,  0.0096, -0.0085,  0.0085],
        [-0.0108,  0.0012, -0.0078,  ..., -0.0087, -0.0124,  0.0092],
        [-0.0109, -0.0127, -0.0068,  ..., -0.0090, -0.0039,  0.0008]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.4180,  5.9453,  3.3770,  ...,  1.7734,  0.9761, -2.4961]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:16:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with jakarta as its capital is known as indonesia
The country with warsaw as its capital is known as poland
The country with dublin as its capital is known as ireland
The country with canberra as its capital is known as australia
The country with bern as its capital is known as switzerland
The country with sofia as its capital is known as bulgaria
The country with amman as its capital is known as jordan
The country with nairobi as its capital is known as
2024-07-24 03:16:35 root INFO     [order_1_approx] starting weight calculation for The country with bern as its capital is known as switzerland
The country with warsaw as its capital is known as poland
The country with jakarta as its capital is known as indonesia
The country with nairobi as its capital is known as kenya
The country with canberra as its capital is known as australia
The country with dublin as its capital is known as ireland
The country with amman as its capital is known as jordan
The country with sofia as its capital is known as
2024-07-24 03:16:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 03:20:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1875, -0.6240, -0.2754,  ...,  0.7197, -0.4082,  0.7485],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.2422,  2.6602,  3.3809,  ...,  3.7949, -9.4453, -2.4688],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0057, -0.0091, -0.0215,  ...,  0.0033,  0.0068, -0.0032],
        [-0.0081, -0.0130,  0.0031,  ...,  0.0211,  0.0036, -0.0086],
        [-0.0098, -0.0231, -0.0176,  ...,  0.0043, -0.0061, -0.0022],
        ...,
        [-0.0220, -0.0005,  0.0174,  ...,  0.0215, -0.0121,  0.0018],
        [-0.0086, -0.0276, -0.0307,  ..., -0.0132, -0.0406,  0.0502],
        [-0.0083, -0.0338, -0.0209,  ..., -0.0168,  0.0046, -0.0044]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.9854,  3.1660,  2.7598,  ...,  4.1836, -8.6328, -2.5156]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:20:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with bern as its capital is known as switzerland
The country with warsaw as its capital is known as poland
The country with jakarta as its capital is known as indonesia
The country with nairobi as its capital is known as kenya
The country with canberra as its capital is known as australia
The country with dublin as its capital is known as ireland
The country with amman as its capital is known as jordan
The country with sofia as its capital is known as
2024-07-24 03:20:25 root INFO     [order_1_approx] starting weight calculation for The country with dublin as its capital is known as ireland
The country with canberra as its capital is known as australia
The country with amman as its capital is known as jordan
The country with nairobi as its capital is known as kenya
The country with sofia as its capital is known as bulgaria
The country with bern as its capital is known as switzerland
The country with jakarta as its capital is known as indonesia
The country with warsaw as its capital is known as
2024-07-24 03:20:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.7
2024-07-24 03:24:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.4258, -0.1194, -0.0859,  ...,  0.1906,  1.3789,  1.0176],
       device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([0.5967, 0.9170, 0.8125,  ..., 1.8809, 3.0625, 5.4023], device='cuda:0',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0238, -0.0071,  0.0079,  ...,  0.0027,  0.0014, -0.0148],
        [ 0.0062, -0.0131,  0.0142,  ...,  0.0110,  0.0079,  0.0057],
        [ 0.0153, -0.0079, -0.0270,  ...,  0.0004, -0.0115, -0.0221],
        ...,
        [-0.0149, -0.0026,  0.0080,  ...,  0.0102, -0.0243,  0.0166],
        [-0.0368, -0.0055, -0.0247,  ..., -0.0125, -0.0483,  0.0131],
        [-0.0353, -0.0176,  0.0189,  ...,  0.0052,  0.0139,  0.0051]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[0.6479, 2.5410, 1.0811,  ..., 1.9570, 1.9434, 6.7734]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 03:24:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with dublin as its capital is known as ireland
The country with canberra as its capital is known as australia
The country with amman as its capital is known as jordan
The country with nairobi as its capital is known as kenya
The country with sofia as its capital is known as bulgaria
The country with bern as its capital is known as switzerland
The country with jakarta as its capital is known as indonesia
The country with warsaw as its capital is known as
2024-07-24 03:24:14 root INFO     total operator prediction time: 1833.639987230301 seconds
2024-07-24 03:24:14 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on things - color
2024-07-24 03:24:14 root INFO     building operator things - color
2024-07-24 03:24:14 root INFO     total operator prediction time: 0.021877050399780273 seconds
2024-07-24 03:24:14 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on animal - sound
2024-07-24 03:24:14 root INFO     building operator animal - sound
2024-07-24 03:24:14 root INFO     total operator prediction time: 0.014870643615722656 seconds
2024-07-24 03:24:14 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on animal - youth
2024-07-24 03:24:14 root INFO     building operator animal - youth
2024-07-24 03:24:14 root INFO     total operator prediction time: 0.01322793960571289 seconds
2024-07-24 03:24:14 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on UK_city - county
2024-07-24 03:24:14 root INFO     building operator UK_city - county
2024-07-24 03:24:14 root INFO     total operator prediction time: 0.0137481689453125 seconds
