2024-07-15 16:56:00 root INFO     loading model + tokenizer
2024-07-15 16:56:17 root INFO     model + tokenizer loaded
2024-07-15 16:56:17 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - part
2024-07-15 16:56:17 root INFO     building operator meronyms - part
2024-07-15 16:56:18 root INFO     [order_1_approx] starting weight calculation for movie scene
railcar suspension
seafront harbor
gigabit megabit
litre millilitre
castle donjon
tripod leg
byte
2024-07-15 16:56:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 17:00:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.3047e-01, -6.9629e-01, -1.3770e+00,  ..., -1.0681e-03,
        -1.5264e+00,  1.7705e+00], device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3584, -3.6816, -2.8555,  ..., -4.1484, -2.8535,  0.5381],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 5.6055e-01, -1.1139e-02,  1.4038e-03,  ..., -3.3447e-02,
          8.1177e-03, -7.3364e-02],
        [ 3.6804e-02,  5.1172e-01, -1.6006e-02,  ..., -4.5593e-02,
          3.8086e-02,  5.3101e-03],
        [ 3.6240e-04, -5.7831e-03,  5.1611e-01,  ...,  6.1401e-02,
          3.3875e-03,  2.8412e-02],
        ...,
        [-9.6130e-03, -1.3763e-02, -3.5217e-02,  ...,  5.3564e-01,
          1.7807e-02, -1.5205e-02],
        [ 3.8696e-02,  2.9572e-02, -3.0396e-02,  ..., -5.5923e-03,
          5.2100e-01, -2.6703e-02],
        [-3.6316e-03,  3.8208e-02, -2.6184e-02,  ..., -2.0706e-02,
          3.6072e-02,  5.7373e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1768, -0.8926,  0.6074,  ..., -3.0137,  1.0098, -2.5742]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 17:00:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for movie scene
railcar suspension
seafront harbor
gigabit megabit
litre millilitre
castle donjon
tripod leg
byte
2024-07-15 17:00:24 root INFO     [order_1_approx] starting weight calculation for gigabit megabit
byte bit
castle donjon
tripod leg
seafront harbor
litre millilitre
movie scene
railcar
2024-07-15 17:00:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 17:04:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0034, -0.3462, -0.0417,  ...,  0.3550,  0.3254,  0.3447],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0410, -0.6445, -3.3711,  ..., -3.3457,  1.8662,  0.1982],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.8257e-01, -3.1738e-02,  5.9875e-02,  ...,  7.0648e-03,
          2.9297e-03,  5.9204e-03],
        [ 8.5831e-03,  3.7354e-01,  1.3008e-03,  ..., -4.2236e-02,
          2.1317e-02,  8.5983e-03],
        [-1.4137e-02, -1.1673e-02,  4.1260e-01,  ...,  1.7044e-02,
          2.5940e-04, -1.0216e-02],
        ...,
        [-2.4841e-02,  5.2551e-02, -1.3947e-02,  ...,  3.3813e-01,
          3.1528e-03,  5.5695e-04],
        [ 1.6678e-02,  1.8402e-02, -1.6403e-03,  ...,  1.4915e-03,
          3.7305e-01, -5.8594e-03],
        [-2.9144e-02, -6.0921e-03, -9.6970e-03,  ..., -3.5339e-02,
          1.9493e-03,  3.8916e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9668, -1.5791, -2.2539,  ...,  0.7988,  2.5039,  2.0117]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 17:04:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for gigabit megabit
byte bit
castle donjon
tripod leg
seafront harbor
litre millilitre
movie scene
railcar
2024-07-15 17:04:29 root INFO     [order_1_approx] starting weight calculation for movie scene
litre millilitre
seafront harbor
castle donjon
gigabit megabit
byte bit
railcar suspension
tripod
2024-07-15 17:04:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 17:08:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8760,  0.1755, -1.4121,  ...,  0.6924, -0.8594, -0.7295],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5859,  0.7749, -0.6191,  ...,  1.8691, -3.6445, -2.1934],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.8438e-01,  2.8839e-03, -1.6785e-04,  ..., -1.9855e-03,
         -1.8906e-02, -7.5912e-03],
        [ 9.0408e-03,  4.8682e-01,  2.6505e-02,  ...,  2.2675e-02,
          4.3304e-02,  3.6560e-02],
        [-4.7974e-02, -9.0942e-03,  4.9341e-01,  ...,  2.7374e-02,
         -2.1652e-02, -1.7624e-03],
        ...,
        [ 1.2207e-02,  2.0233e-02,  8.8806e-03,  ...,  4.7949e-01,
         -2.0050e-02, -1.1307e-02],
        [ 3.9429e-02, -4.5776e-05, -2.0676e-03,  ...,  2.6474e-03,
          4.9585e-01,  1.6632e-02],
        [ 3.4271e-02, -7.9956e-03, -2.5192e-02,  ..., -2.4353e-02,
         -1.9867e-02,  4.8193e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3594, -0.2134,  0.9248,  ...,  2.8574, -3.7969, -0.6943]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 17:08:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for movie scene
litre millilitre
seafront harbor
castle donjon
gigabit megabit
byte bit
railcar suspension
tripod
2024-07-15 17:08:35 root INFO     [order_1_approx] starting weight calculation for movie scene
railcar suspension
byte bit
tripod leg
gigabit megabit
seafront harbor
castle donjon
litre
2024-07-15 17:08:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 17:12:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2019,  0.4780,  0.1094,  ..., -0.2173,  0.0883, -0.0315],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.3359, -1.3389, -5.0312,  ..., -2.2773,  0.2400, -2.8828],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.5396, -0.0380, -0.0104,  ..., -0.0576,  0.0126, -0.0624],
        [ 0.0141,  0.4473,  0.0688,  ..., -0.0331, -0.0195,  0.0533],
        [ 0.0228,  0.0415,  0.4866,  ...,  0.0217, -0.0045, -0.0262],
        ...,
        [ 0.0283,  0.0119, -0.0351,  ...,  0.4558,  0.0329, -0.0049],
        [ 0.0884, -0.0068, -0.0464,  ..., -0.0314,  0.4988, -0.0479],
        [ 0.0396, -0.0044, -0.0079,  ..., -0.0488,  0.0209,  0.4192]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.4453, -0.3037, -2.1445,  ..., -1.2588,  0.5488, -3.6562]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 17:12:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for movie scene
railcar suspension
byte bit
tripod leg
gigabit megabit
seafront harbor
castle donjon
litre
2024-07-15 17:12:42 root INFO     [order_1_approx] starting weight calculation for movie scene
tripod leg
seafront harbor
byte bit
railcar suspension
litre millilitre
gigabit megabit
castle
2024-07-15 17:12:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 17:16:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2031, -0.8413, -0.0840,  ..., -0.1870, -0.1731,  0.6982],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3750, -0.5703, -2.0078,  ..., -0.2144, -1.4629, -1.9746],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4717,  0.0066, -0.0379,  ..., -0.0440,  0.0349, -0.0332],
        [ 0.0444,  0.4243, -0.0075,  ..., -0.0006, -0.0006,  0.0066],
        [-0.0106, -0.0025,  0.5020,  ...,  0.0114, -0.0125, -0.0052],
        ...,
        [ 0.0417,  0.0031, -0.0510,  ...,  0.4387,  0.0398, -0.0259],
        [ 0.0228, -0.0167, -0.0507,  ..., -0.0042,  0.4470, -0.0239],
        [ 0.0174,  0.0137,  0.0049,  ...,  0.0017, -0.0081,  0.4497]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.5586,  0.0479, -0.0684,  ..., -0.4722, -2.0195, -1.9688]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 17:16:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for movie scene
tripod leg
seafront harbor
byte bit
railcar suspension
litre millilitre
gigabit megabit
castle
2024-07-15 17:16:46 root INFO     [order_1_approx] starting weight calculation for castle donjon
tripod leg
byte bit
seafront harbor
litre millilitre
gigabit megabit
railcar suspension
movie
2024-07-15 17:16:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 17:20:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6445,  0.6670,  0.0320,  ...,  0.1221,  0.3071,  0.4521],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.6719, -3.3828, -2.2500,  ..., -1.8320, -0.3564,  2.2051],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4272, -0.0108,  0.0007,  ..., -0.0222,  0.0084, -0.0297],
        [ 0.0137,  0.3992, -0.0215,  ..., -0.0011,  0.0026, -0.0069],
        [ 0.0082,  0.0237,  0.4998,  ...,  0.0020, -0.0111,  0.0065],
        ...,
        [-0.0009,  0.0055, -0.0101,  ...,  0.4724,  0.0239,  0.0080],
        [ 0.0159, -0.0045, -0.0154,  ..., -0.0043,  0.3975, -0.0034],
        [-0.0350,  0.0037,  0.0117,  ..., -0.0082,  0.0196,  0.4519]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.8594, -2.6523, -1.9238,  ..., -0.9258, -0.9697,  1.4785]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 17:20:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for castle donjon
tripod leg
byte bit
seafront harbor
litre millilitre
gigabit megabit
railcar suspension
movie
2024-07-15 17:20:52 root INFO     [order_1_approx] starting weight calculation for seafront harbor
movie scene
tripod leg
castle donjon
litre millilitre
railcar suspension
byte bit
gigabit
2024-07-15 17:20:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 17:24:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9307, -0.7021,  0.0830,  ..., -1.1270,  0.5542,  0.3389],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9185,  0.3975, -0.6328,  ...,  1.1055,  3.1621, -3.0586],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.7656e-01,  4.5105e-02,  2.5421e-02,  ...,  2.6642e-02,
         -2.7237e-02,  1.3412e-02],
        [ 2.1713e-02,  4.7363e-01,  1.3474e-02,  ..., -4.5700e-03,
          2.3499e-03,  5.3314e-02],
        [-2.0874e-02, -1.6296e-02,  5.0488e-01,  ...,  2.4918e-02,
         -1.2085e-02, -2.7405e-02],
        ...,
        [-3.8025e-02, -3.7659e-02,  4.3823e-02,  ...,  5.3809e-01,
         -8.9569e-03,  4.1008e-03],
        [-4.7684e-04,  2.9106e-03, -1.3504e-02,  ..., -2.1332e-02,
          5.0146e-01, -1.4935e-03],
        [ 2.2888e-03,  1.0773e-02, -1.8631e-02,  ..., -8.8272e-03,
          1.4420e-03,  4.3408e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6406,  1.9775,  2.4160,  ...,  1.7754,  2.3359, -1.3311]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 17:24:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for seafront harbor
movie scene
tripod leg
castle donjon
litre millilitre
railcar suspension
byte bit
gigabit
2024-07-15 17:24:56 root INFO     [order_1_approx] starting weight calculation for railcar suspension
tripod leg
castle donjon
gigabit megabit
byte bit
litre millilitre
movie scene
seafront
2024-07-15 17:24:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 17:29:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7217, -0.1807,  0.2168,  ..., -0.2949,  0.3562,  0.9219],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5039,  1.3145, -1.0254,  ..., -0.3960, -3.6816, -1.9238],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4392, -0.0168,  0.0329,  ..., -0.0131,  0.0297, -0.0102],
        [-0.0148,  0.4089,  0.0239,  ...,  0.0076,  0.0010,  0.0233],
        [ 0.0035, -0.0196,  0.4424,  ...,  0.0526,  0.0039, -0.0014],
        ...,
        [ 0.0048,  0.0236,  0.0313,  ...,  0.4404,  0.0175, -0.0109],
        [ 0.0086,  0.0207,  0.0158,  ..., -0.0097,  0.4080,  0.0104],
        [ 0.0375, -0.0217, -0.0278,  ..., -0.0211, -0.0041,  0.4114]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1006, -0.4453, -1.3438,  ..., -1.5859, -4.1523, -0.9683]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 17:29:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for railcar suspension
tripod leg
castle donjon
gigabit megabit
byte bit
litre millilitre
movie scene
seafront
2024-07-15 17:29:02 root INFO     total operator prediction time: 1964.3555421829224 seconds
2024-07-15 17:29:02 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on synonyms - exact
2024-07-15 17:29:02 root INFO     building operator synonyms - exact
2024-07-15 17:29:02 root INFO     [order_1_approx] starting weight calculation for baby infant
phone telephone
homogeneous uniform
lady madam
sofa couch
organized arranged
lad chap
sweets
2024-07-15 17:29:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 17:33:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0358,  0.3088, -0.0356,  ...,  0.2156,  0.8833, -0.2703],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.3633, -0.6885, -2.1309,  ..., -1.9629, -1.3564,  0.6240],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 5.1465e-01, -1.4015e-02, -2.4124e-02,  ..., -3.6850e-03,
          2.8854e-02, -4.9744e-02],
        [-1.0223e-02,  3.8770e-01,  1.9592e-02,  ...,  2.8046e-02,
          7.7972e-03, -1.8768e-02],
        [ 4.3701e-02, -1.1070e-02,  4.4458e-01,  ..., -5.0354e-04,
          2.0187e-02, -3.4241e-02],
        ...,
        [-1.3527e-02, -2.0599e-03,  4.1046e-03,  ...,  4.4922e-01,
          3.1586e-02, -6.8779e-03],
        [ 3.0472e-02, -6.2599e-03,  8.0948e-03,  ..., -7.8125e-03,
          4.0186e-01, -2.6489e-02],
        [-3.3264e-02, -1.9348e-02,  3.9551e-02,  ..., -1.2817e-02,
         -1.0124e-02,  4.1016e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-6.2227, -1.0547, -1.6826,  ..., -0.6748, -1.9746,  3.2031]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 17:33:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for baby infant
phone telephone
homogeneous uniform
lady madam
sofa couch
organized arranged
lad chap
sweets
2024-07-15 17:33:09 root INFO     [order_1_approx] starting weight calculation for lad chap
sweets confectionery
phone telephone
lady madam
sofa couch
homogeneous uniform
organized arranged
baby
2024-07-15 17:33:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 17:37:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0361,  0.2404, -0.5371,  ..., -0.2920, -0.6606, -1.0537],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.9541, -2.4766, -4.4766,  ..., -1.7744,  4.3125, -3.8828],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.1675e-01, -7.0190e-03, -1.6785e-04,  ..., -6.0577e-03,
          4.4708e-03,  4.2305e-03],
        [-9.2468e-03,  3.7500e-01,  1.8433e-02,  ...,  2.0508e-02,
          7.3929e-03,  9.8953e-03],
        [-6.8970e-03, -3.7498e-03,  3.9990e-01,  ..., -4.6997e-03,
          5.8441e-03, -3.7750e-02],
        ...,
        [-3.4943e-02, -2.9205e-02,  2.0157e-02,  ...,  4.6582e-01,
         -1.6983e-02, -8.5449e-03],
        [ 1.9058e-02, -2.4872e-03, -6.6681e-03,  ..., -5.0583e-03,
          4.2749e-01,  2.3346e-03],
        [ 3.2318e-02,  2.2614e-02, -2.0935e-02,  ...,  2.1591e-02,
          4.7119e-02,  4.2383e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.1953, -3.0781, -2.0781,  ..., -1.1562,  3.2715, -2.2812]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 17:37:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for lad chap
sweets confectionery
phone telephone
lady madam
sofa couch
homogeneous uniform
organized arranged
baby
2024-07-15 17:37:16 root INFO     [order_1_approx] starting weight calculation for sofa couch
baby infant
homogeneous uniform
phone telephone
sweets confectionery
lady madam
organized arranged
lad
2024-07-15 17:37:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 17:41:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1665, -0.3333, -0.3887,  ..., -0.5967,  0.4922, -0.5166],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.2656, -1.3926, -2.8086,  ..., -3.1367,  0.4666, -1.9102],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 5.2344e-01, -2.0966e-02, -1.9882e-02,  ..., -1.0056e-02,
          1.7365e-02, -7.1487e-03],
        [ 2.4033e-04,  4.5044e-01,  1.3924e-03,  ...,  2.5192e-02,
          2.1301e-02,  1.3428e-02],
        [-5.1422e-03, -1.3382e-02,  5.4297e-01,  ..., -4.0222e-02,
          1.3855e-02, -1.7914e-02],
        ...,
        [-1.1139e-02, -2.2079e-02,  9.2392e-03,  ...,  5.4688e-01,
          1.1383e-02, -3.7994e-02],
        [ 9.8343e-03, -3.7384e-03, -1.7563e-02,  ...,  1.7166e-04,
          5.2979e-01,  1.5640e-02],
        [ 2.4307e-02,  2.2858e-02, -5.1514e-02,  ..., -4.8645e-02,
          1.3664e-02,  5.0244e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.7422, -1.6387, -0.9219,  ..., -1.0938,  1.5205, -2.2246]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 17:41:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for sofa couch
baby infant
homogeneous uniform
phone telephone
sweets confectionery
lady madam
organized arranged
lad
2024-07-15 17:41:24 root INFO     [order_1_approx] starting weight calculation for homogeneous uniform
sweets confectionery
sofa couch
lad chap
baby infant
phone telephone
organized arranged
lady
2024-07-15 17:41:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 17:45:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.3545,  0.3513,  0.2336,  ..., -0.0911, -0.2292, -0.0563],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0254, -2.3379, -3.5430,  ..., -4.0430,  1.5352, -4.3906],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4556,  0.0093, -0.0148,  ..., -0.0273,  0.0099, -0.0284],
        [ 0.0277,  0.3818, -0.0250,  ...,  0.0014,  0.0022, -0.0018],
        [ 0.0377,  0.0116,  0.4387,  ..., -0.0234, -0.0023, -0.0263],
        ...,
        [-0.0156, -0.0011,  0.0044,  ...,  0.4229,  0.0070, -0.0206],
        [-0.0100,  0.0134, -0.0044,  ...,  0.0290,  0.4075,  0.0017],
        [ 0.0371,  0.0027, -0.0385,  ..., -0.0014, -0.0107,  0.4177]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.8447, -0.5449, -4.1992,  ..., -4.0195,  0.7588, -3.8496]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 17:45:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for homogeneous uniform
sweets confectionery
sofa couch
lad chap
baby infant
phone telephone
organized arranged
lady
2024-07-15 17:45:30 root INFO     [order_1_approx] starting weight calculation for sweets confectionery
organized arranged
sofa couch
lad chap
baby infant
lady madam
phone telephone
homogeneous
2024-07-15 17:45:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 17:49:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4758,  1.0801,  1.2441,  ..., -0.7231, -0.3193,  0.6396],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6172,  1.3994, -1.7910,  ...,  1.3652,  3.1426,  3.1875],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.5728e-01,  2.7695e-02, -3.4088e-02,  ...,  1.3351e-03,
         -1.3435e-02,  1.7319e-02],
        [-3.7231e-03,  4.5459e-01, -1.0963e-02,  ...,  3.5980e-02,
          1.8311e-02, -8.8692e-04],
        [ 4.3335e-02, -2.7130e-02,  4.9731e-01,  ..., -3.1036e-02,
          2.4185e-03, -4.8737e-02],
        ...,
        [-5.1941e-02, -2.0416e-02, -1.7151e-02,  ...,  4.7607e-01,
          1.2077e-02,  1.8326e-02],
        [ 1.2199e-02, -3.9673e-04, -1.1589e-02,  ...,  9.9945e-03,
          4.4214e-01, -1.5793e-03],
        [ 3.2776e-02,  3.3905e-02, -2.8801e-03,  ..., -6.0806e-03,
         -1.0712e-02,  4.5410e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.2090,  2.1562, -1.1172,  ..., -0.5469,  2.3438,  3.6602]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 17:49:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for sweets confectionery
organized arranged
sofa couch
lad chap
baby infant
lady madam
phone telephone
homogeneous
2024-07-15 17:49:38 root INFO     [order_1_approx] starting weight calculation for lad chap
sweets confectionery
lady madam
homogeneous uniform
phone telephone
organized arranged
baby infant
sofa
2024-07-15 17:49:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 17:53:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0906, -0.1572,  0.0826,  ...,  1.0127,  1.5820, -0.3667],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4814, -2.9980, -2.4277,  ..., -2.6914, -2.9258, -2.3672],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4731, -0.0347,  0.0053,  ..., -0.0018, -0.0171, -0.0489],
        [-0.0072,  0.4631, -0.0059,  ...,  0.0363,  0.0205, -0.0069],
        [-0.0097,  0.0100,  0.4568,  ...,  0.0256, -0.0292, -0.0180],
        ...,
        [-0.0071, -0.0070,  0.0114,  ...,  0.4707,  0.0113, -0.0206],
        [ 0.0421,  0.0014,  0.0022,  ...,  0.0092,  0.4475, -0.0192],
        [ 0.0040, -0.0041, -0.0060,  ...,  0.0328, -0.0044,  0.4663]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3096, -3.2305, -0.0410,  ..., -3.1270, -3.9082, -1.7969]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 17:53:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for lad chap
sweets confectionery
lady madam
homogeneous uniform
phone telephone
organized arranged
baby infant
sofa
2024-07-15 17:53:45 root INFO     [order_1_approx] starting weight calculation for sofa couch
sweets confectionery
baby infant
phone telephone
lady madam
homogeneous uniform
lad chap
organized
2024-07-15 17:53:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 17:57:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6392,  0.5156, -0.0021,  ..., -0.1665,  1.1406, -0.2742],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.2490, -3.2754, -1.6621,  ..., -1.7637,  3.3047,  0.2197],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.5996e-01,  6.7749e-03, -1.8356e-02,  ..., -6.9885e-03,
          1.8600e-02, -1.1810e-02],
        [ 8.4381e-03,  3.8110e-01, -1.7303e-02,  ..., -1.0757e-03,
          2.5925e-02,  1.6205e-02],
        [ 1.1566e-02,  1.0612e-02,  4.6167e-01,  ..., -2.9755e-04,
         -2.3178e-02,  5.1498e-05],
        ...,
        [-3.8834e-03,  2.4109e-02,  4.1771e-03,  ...,  4.8901e-01,
         -2.1149e-02, -3.0670e-02],
        [ 2.6001e-02,  1.4984e-02, -9.9182e-05,  ...,  2.4986e-03,
          4.5337e-01,  1.5732e-02],
        [-6.0501e-03,  9.3231e-03, -5.6839e-03,  ...,  2.0203e-02,
         -1.2650e-02,  4.0503e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8052, -3.8633,  0.6973,  ...,  1.2109,  3.5234, -0.2576]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 17:57:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for sofa couch
sweets confectionery
baby infant
phone telephone
lady madam
homogeneous uniform
lad chap
organized
2024-07-15 17:57:51 root INFO     [order_1_approx] starting weight calculation for organized arranged
sweets confectionery
lady madam
lad chap
sofa couch
baby infant
homogeneous uniform
phone
2024-07-15 17:57:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 18:01:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6538, -0.0629, -0.6680,  ...,  0.4885,  0.4929, -0.2041],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6729,  0.4255, -1.1123,  ..., -3.0703,  4.2422,  0.3408],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4304, -0.0073,  0.0170,  ..., -0.0069,  0.0088, -0.0388],
        [-0.0064,  0.3682,  0.0219,  ...,  0.0089,  0.0046, -0.0040],
        [ 0.0014, -0.0092,  0.4458,  ...,  0.0093, -0.0239, -0.0286],
        ...,
        [ 0.0005,  0.0086,  0.0035,  ...,  0.4282,  0.0421, -0.0161],
        [ 0.0028,  0.0192,  0.0124,  ...,  0.0184,  0.4319,  0.0294],
        [ 0.0298, -0.0114, -0.0479,  ...,  0.0068, -0.0140,  0.4072]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2627,  2.3105, -0.9941,  ..., -4.1523,  4.0352, -0.2710]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 18:01:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for organized arranged
sweets confectionery
lady madam
lad chap
sofa couch
baby infant
homogeneous uniform
phone
2024-07-15 18:01:58 root INFO     total operator prediction time: 1976.37575173378 seconds
2024-07-15 18:01:58 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on hypernyms - misc
2024-07-15 18:01:58 root INFO     building operator hypernyms - misc
2024-07-15 18:01:58 root INFO     [order_1_approx] starting weight calculation for vase jar
notepad pad
dishwasher appliance
jeans trousers
tub container
desk furniture
blender appliance
lemon
2024-07-15 18:01:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 18:06:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0703, -0.6021,  1.2002,  ...,  0.1016,  0.8555,  0.6753],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.5781, -0.8311,  0.2529,  ..., -2.6270,  2.9062,  0.5771],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.9634e-01, -4.6730e-04, -3.4149e-02,  ..., -1.4328e-02,
         -8.2321e-03, -1.7532e-02],
        [ 9.0637e-03,  4.3213e-01,  1.5152e-02,  ..., -3.9902e-03,
          2.9526e-03,  2.6596e-02],
        [ 2.2736e-02,  2.5574e-02,  4.3750e-01,  ..., -8.4915e-03,
         -1.6006e-02, -8.3389e-03],
        ...,
        [ 3.2120e-03,  2.0554e-02, -1.5305e-02,  ...,  4.9438e-01,
          1.9806e-02, -5.5389e-03],
        [ 9.8572e-03, -3.8147e-03, -1.5823e-02,  ...,  2.9419e-02,
          4.8877e-01, -1.5572e-02],
        [ 2.7786e-02,  4.4159e-02,  4.1931e-02,  ..., -2.3251e-03,
          2.9846e-02,  4.6313e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.1289, -1.0479, -2.3359,  ..., -3.9023,  2.3906, -1.6514]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 18:06:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for vase jar
notepad pad
dishwasher appliance
jeans trousers
tub container
desk furniture
blender appliance
lemon
2024-07-15 18:06:05 root INFO     [order_1_approx] starting weight calculation for jeans trousers
tub container
vase jar
desk furniture
notepad pad
lemon citrus
dishwasher appliance
blender
2024-07-15 18:06:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 18:10:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7822,  0.0869, -1.9277,  ...,  0.3164,  0.1672,  0.3047],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.5762,  0.2188,  2.1504,  ...,  3.6562,  0.9922, -0.4785],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4971, -0.0053, -0.0396,  ..., -0.0012,  0.0091, -0.0102],
        [-0.0179,  0.3958,  0.0146,  ...,  0.0506, -0.0261,  0.0093],
        [-0.0179,  0.0184,  0.4746,  ...,  0.0173, -0.0067,  0.0124],
        ...,
        [-0.0220,  0.0245,  0.0412,  ...,  0.5176, -0.0257, -0.0111],
        [ 0.0107, -0.0211,  0.0048,  ...,  0.0314,  0.4331,  0.0296],
        [ 0.0059,  0.0224, -0.0150,  ...,  0.0193,  0.0134,  0.4634]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5938, -0.2034,  2.0898,  ...,  2.3711,  1.4951,  0.3306]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 18:10:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for jeans trousers
tub container
vase jar
desk furniture
notepad pad
lemon citrus
dishwasher appliance
blender
2024-07-15 18:10:12 root INFO     [order_1_approx] starting weight calculation for dishwasher appliance
vase jar
lemon citrus
notepad pad
tub container
blender appliance
desk furniture
jeans
2024-07-15 18:10:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 18:14:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0129, -0.4939, -0.9795,  ..., -0.4653, -0.3726,  0.8774],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9619, -4.3281,  0.5176,  ..., -6.5312, -2.3457, -5.6641],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4404, -0.0176, -0.0172,  ..., -0.0270,  0.0115, -0.0251],
        [-0.0224,  0.3984, -0.0065,  ..., -0.0048, -0.0113,  0.0082],
        [-0.0095,  0.0228,  0.4614,  ..., -0.0262,  0.0030, -0.0006],
        ...,
        [-0.0379,  0.0081,  0.0327,  ...,  0.4561, -0.0364,  0.0073],
        [ 0.0477,  0.0078,  0.0143,  ...,  0.0142,  0.4258, -0.0259],
        [ 0.0499,  0.0020, -0.0180,  ..., -0.0139,  0.0125,  0.4380]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3105, -5.2891,  1.9512,  ..., -6.0781, -0.3066, -4.0156]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 18:14:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for dishwasher appliance
vase jar
lemon citrus
notepad pad
tub container
blender appliance
desk furniture
jeans
2024-07-15 18:14:18 root INFO     [order_1_approx] starting weight calculation for notepad pad
lemon citrus
vase jar
blender appliance
tub container
dishwasher appliance
jeans trousers
desk
2024-07-15 18:14:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 18:18:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-3.9868e-01,  8.7036e-02,  8.3618e-02,  ..., -8.7158e-02,
        -1.9189e-01,  6.1035e-05], device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.5742, -1.1143,  2.1016,  ...,  0.3237,  1.5625, -0.5215],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4646,  0.0096, -0.0147,  ...,  0.0072,  0.0031, -0.0051],
        [-0.0040,  0.4221,  0.0087,  ..., -0.0068,  0.0040, -0.0474],
        [-0.0164,  0.0064,  0.4258,  ..., -0.0032, -0.0147, -0.0077],
        ...,
        [ 0.0279,  0.0262, -0.0071,  ...,  0.4292,  0.0029, -0.0017],
        [ 0.0315, -0.0073, -0.0242,  ..., -0.0321,  0.4275, -0.0069],
        [ 0.0068,  0.0076, -0.0011,  ..., -0.0041,  0.0082,  0.4216]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4336, -0.0176,  0.9199,  ...,  1.2363,  0.8623, -0.1548]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 18:18:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for notepad pad
lemon citrus
vase jar
blender appliance
tub container
dishwasher appliance
jeans trousers
desk
2024-07-15 18:18:25 root INFO     [order_1_approx] starting weight calculation for notepad pad
blender appliance
lemon citrus
tub container
dishwasher appliance
desk furniture
jeans trousers
vase
2024-07-15 18:18:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 18:22:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1211,  1.0391, -0.1133,  ...,  0.1257, -0.0117,  1.0938],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0938, -1.0117,  0.7734,  ...,  1.5723,  1.7051,  3.5703],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.7705e-01, -1.7242e-02,  3.6621e-02,  ...,  1.6556e-02,
         -1.3107e-02, -2.7695e-02],
        [-2.6184e-02,  4.0942e-01,  2.3117e-02,  ..., -2.1362e-02,
         -3.7422e-03, -1.5869e-02],
        [ 1.2657e-02,  1.2894e-02,  4.7290e-01,  ..., -5.3291e-03,
          9.1248e-03,  2.5940e-04],
        ...,
        [-1.5175e-02, -3.6255e-02,  8.6823e-03,  ...,  4.5435e-01,
          2.3560e-02, -3.1433e-03],
        [ 4.1901e-02,  1.1887e-02, -7.2060e-03,  ...,  2.3193e-02,
          4.5337e-01, -3.0853e-02],
        [ 3.0994e-03,  2.7817e-02,  1.9745e-02,  ...,  7.9651e-03,
          3.0937e-03,  4.3213e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6450, -1.1328,  1.0244,  ...,  1.8799,  0.4902,  4.0977]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 18:22:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for notepad pad
blender appliance
lemon citrus
tub container
dishwasher appliance
desk furniture
jeans trousers
vase
2024-07-15 18:22:32 root INFO     [order_1_approx] starting weight calculation for tub container
vase jar
jeans trousers
desk furniture
notepad pad
lemon citrus
blender appliance
dishwasher
2024-07-15 18:22:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 18:26:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0973,  0.2427,  0.6963,  ...,  1.0234, -0.0754, -1.1074],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.9180, -1.6855,  2.8750,  ..., -1.7812, -0.8848,  2.5156],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.5825e-01,  2.6794e-02, -5.1880e-02,  ...,  1.7349e-02,
          4.4983e-02, -4.6692e-02],
        [-1.6068e-02,  4.1992e-01, -8.7128e-03,  ...,  2.8656e-02,
         -9.8877e-03, -1.4885e-02],
        [-2.5574e-02,  2.6215e-02,  4.4165e-01,  ..., -8.8215e-05,
         -2.7206e-02,  4.5624e-03],
        ...,
        [-5.3291e-03,  1.1383e-02, -2.0844e-02,  ...,  4.7095e-01,
          1.4893e-02, -5.0293e-02],
        [ 6.5994e-03,  2.3300e-02, -1.4465e-02,  ..., -3.8567e-03,
          4.2944e-01,  1.4702e-02],
        [-1.0712e-02,  1.2299e-02, -3.2745e-02,  ..., -1.8997e-02,
          5.5161e-03,  4.0723e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.1445, -1.8066,  0.1211,  ..., -2.5703, -1.0684,  3.2578]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 18:26:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for tub container
vase jar
jeans trousers
desk furniture
notepad pad
lemon citrus
blender appliance
dishwasher
2024-07-15 18:26:39 root INFO     [order_1_approx] starting weight calculation for blender appliance
jeans trousers
lemon citrus
vase jar
desk furniture
dishwasher appliance
notepad pad
tub
2024-07-15 18:26:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 18:30:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0479,  0.8364, -0.6841,  ..., -0.8447,  0.2588, -0.6426],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.9189,  0.2046, -2.0781,  ..., -3.1758,  1.4443,  2.3066],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.7949e-01,  5.9128e-04,  2.1782e-03,  ..., -2.8534e-02,
          2.2797e-02, -3.3936e-02],
        [ 3.7231e-03,  4.4653e-01, -2.2469e-03,  ..., -3.7842e-03,
          9.5367e-03, -5.6953e-03],
        [-8.4610e-03,  5.3444e-03,  5.1562e-01,  ..., -2.0050e-02,
         -4.6539e-04,  2.5864e-02],
        ...,
        [-7.6752e-03,  1.9318e-02, -2.1210e-03,  ...,  5.0244e-01,
          2.4536e-02, -6.0181e-02],
        [ 6.6223e-03,  2.1240e-02,  2.9755e-03,  ...,  4.0436e-02,
          4.8608e-01,  1.7059e-02],
        [ 1.9531e-02, -4.6444e-04,  1.9363e-02,  ..., -1.9867e-02,
          3.8849e-02,  4.9414e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0088,  0.5801, -0.1348,  ..., -2.4531,  0.0068,  2.4473]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 18:30:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for blender appliance
jeans trousers
lemon citrus
vase jar
desk furniture
dishwasher appliance
notepad pad
tub
2024-07-15 18:30:45 root INFO     [order_1_approx] starting weight calculation for dishwasher appliance
blender appliance
desk furniture
lemon citrus
vase jar
jeans trousers
tub container
notepad
2024-07-15 18:30:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 18:34:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0889,  1.0361, -1.5820,  ...,  0.1929,  0.8384,  0.3552],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4160,  0.3594,  2.4160,  ...,  2.8809,  4.1055, -0.6348],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4260,  0.0040,  0.0128,  ..., -0.0192,  0.0033, -0.0127],
        [-0.0176,  0.3989,  0.0298,  ..., -0.0072,  0.0014,  0.0162],
        [-0.0071, -0.0016,  0.4006,  ...,  0.0136, -0.0072,  0.0120],
        ...,
        [ 0.0156,  0.0058,  0.0195,  ...,  0.4768,  0.0092,  0.0351],
        [ 0.0284, -0.0006,  0.0103,  ...,  0.0091,  0.4236, -0.0045],
        [ 0.0082,  0.0127,  0.0233,  ...,  0.0116, -0.0400,  0.3899]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5146, -2.6543,  2.4805,  ...,  1.0918,  2.0918,  1.0029]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 18:34:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for dishwasher appliance
blender appliance
desk furniture
lemon citrus
vase jar
jeans trousers
tub container
notepad
2024-07-15 18:34:52 root INFO     total operator prediction time: 1973.7311618328094 seconds
2024-07-15 18:34:52 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - substance
2024-07-15 18:34:52 root INFO     building operator meronyms - substance
2024-07-15 18:34:52 root INFO     [order_1_approx] starting weight calculation for money paper
ocean water
wire metal
sea water
clothing fabric
box cardboard
doorknob metal
jam
2024-07-15 18:34:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 18:38:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3196,  0.9229,  0.0107,  ..., -0.7861, -0.6172, -0.8535],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3262, -1.2520, -1.4043,  ..., -5.6680, -3.9492,  2.7793],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.7217e-01,  2.5696e-02,  7.0190e-03,  ..., -5.5542e-02,
          7.6218e-03, -5.3589e-02],
        [ 1.6434e-02,  4.3555e-01,  1.4435e-02,  ..., -6.7978e-03,
         -5.8556e-03, -1.3428e-02],
        [ 1.1559e-02, -2.0813e-02,  4.8608e-01,  ..., -2.5848e-02,
         -3.6682e-02, -4.1580e-03],
        ...,
        [-1.4572e-02, -5.6610e-03, -2.2888e-04,  ...,  4.9658e-01,
          1.7487e-02, -5.1056e-02],
        [ 1.2604e-02, -6.8283e-03,  1.8997e-02,  ...,  1.9485e-02,
          4.7266e-01, -1.3443e-02],
        [-4.1389e-03,  2.5406e-03, -2.8809e-02,  ...,  1.9882e-02,
          7.4081e-03,  4.9121e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.1230, -3.3164,  1.6484,  ..., -2.7793, -3.5000,  2.2285]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 18:39:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for money paper
ocean water
wire metal
sea water
clothing fabric
box cardboard
doorknob metal
jam
2024-07-15 18:39:00 root INFO     [order_1_approx] starting weight calculation for ocean water
wire metal
money paper
jam fruit
clothing fabric
box cardboard
doorknob metal
sea
2024-07-15 18:39:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 18:43:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6978,  0.8716,  0.3296,  ..., -0.9238, -0.6602, -0.7183],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5474,  0.6147,  1.5127,  ..., -4.3203,  2.7227,  0.6543],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.7021e-01, -2.7676e-03,  9.7961e-03,  ..., -7.6904e-03,
         -1.1749e-03, -6.7902e-03],
        [ 1.1993e-02,  4.0283e-01,  8.4610e-03,  ...,  3.8635e-02,
          2.4292e-02,  3.7079e-03],
        [ 3.1471e-03, -2.8809e-02,  4.5361e-01,  ..., -3.1319e-03,
         -1.0956e-02, -3.0624e-02],
        ...,
        [-1.6556e-03,  1.2520e-02,  1.2062e-02,  ...,  4.9878e-01,
          1.2154e-02, -2.2720e-02],
        [-1.6403e-04,  6.8817e-03,  1.2444e-02,  ...,  1.9806e-02,
          4.3799e-01,  4.0054e-04],
        [ 1.3306e-02,  1.9951e-03, -2.8610e-03,  ...,  4.9591e-04,
          7.1564e-03,  4.2114e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3918,  0.2847,  1.2227,  ..., -2.3672, -0.1758,  1.2217]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 18:43:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for ocean water
wire metal
money paper
jam fruit
clothing fabric
box cardboard
doorknob metal
sea
2024-07-15 18:43:09 root INFO     [order_1_approx] starting weight calculation for clothing fabric
money paper
ocean water
doorknob metal
sea water
jam fruit
box cardboard
wire
2024-07-15 18:43:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 18:47:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1738, -0.1379, -0.7710,  ...,  0.7725,  0.4861,  1.0420],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0186, -0.8867, -4.5820,  ..., -2.6035,  0.2810,  3.3086],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.6680e-01,  4.2953e-03, -2.5482e-02,  ..., -2.4231e-02,
         -2.4796e-05, -1.7136e-02],
        [-1.0101e-02,  4.5728e-01,  1.8280e-02,  ..., -2.9297e-03,
         -8.4229e-03,  1.0521e-02],
        [ 1.6998e-02, -6.4545e-03,  4.9268e-01,  ..., -8.2703e-03,
         -4.1534e-02,  9.2468e-03],
        ...,
        [ 9.7046e-03,  8.9455e-04,  1.3371e-03,  ...,  4.8975e-01,
         -1.9592e-02, -1.8021e-02],
        [ 7.1030e-03, -8.0261e-03, -2.2064e-02,  ..., -1.8188e-02,
          4.7314e-01, -5.2681e-03],
        [ 4.1885e-03,  2.6093e-03, -7.5073e-03,  ..., -8.2550e-03,
          2.6360e-03,  4.6289e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0039, -1.6680, -1.8711,  ..., -1.5244,  0.5366,  3.5723]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 18:47:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for clothing fabric
money paper
ocean water
doorknob metal
sea water
jam fruit
box cardboard
wire
2024-07-15 18:47:11 root INFO     [order_1_approx] starting weight calculation for doorknob metal
money paper
wire metal
jam fruit
ocean water
sea water
box cardboard
clothing
2024-07-15 18:47:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 18:51:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1484,  0.1562, -0.0115,  ...,  0.2720,  0.3118,  0.3381],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.8711, -2.8438,  2.1172,  ..., -4.5469, -0.3738, -5.2969],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4116, -0.0062, -0.0085,  ..., -0.0057,  0.0168, -0.0252],
        [ 0.0396,  0.3679, -0.0025,  ..., -0.0041,  0.0346,  0.0197],
        [-0.0322,  0.0091,  0.4380,  ..., -0.0178, -0.0136,  0.0160],
        ...,
        [ 0.0079, -0.0020,  0.0069,  ...,  0.4355, -0.0090, -0.0101],
        [ 0.0019, -0.0028, -0.0216,  ..., -0.0397,  0.4194,  0.0169],
        [ 0.0177, -0.0259, -0.0049,  ..., -0.0010, -0.0038,  0.4124]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.2500, -3.7051,  1.9229,  ..., -4.1641,  1.9854, -5.1289]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 18:51:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for doorknob metal
money paper
wire metal
jam fruit
ocean water
sea water
box cardboard
clothing
2024-07-15 18:51:18 root INFO     [order_1_approx] starting weight calculation for clothing fabric
wire metal
doorknob metal
ocean water
money paper
sea water
jam fruit
box
2024-07-15 18:51:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 18:55:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0459,  0.2133, -0.4729,  ...,  0.1412, -0.4009,  0.3547],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0977, -1.3965, -3.4531,  ..., -0.2603, -0.2422,  1.0732],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.7144e-01,  1.5457e-02, -3.9001e-02,  ...,  1.5282e-02,
         -3.4103e-03, -7.2144e-02],
        [-5.3864e-03,  4.0991e-01, -1.0597e-02,  ..., -1.5900e-02,
          1.3245e-02,  3.8727e-02],
        [ 4.5929e-03, -8.1253e-03,  4.6167e-01,  ...,  3.6201e-03,
          1.1196e-03, -3.9444e-03],
        ...,
        [-1.0422e-02, -2.8419e-04,  2.0676e-03,  ...,  4.5435e-01,
          1.3260e-02, -2.3499e-03],
        [ 2.8564e-02,  2.2583e-03, -3.5156e-02,  ..., -4.8065e-03,
          4.7217e-01,  2.6703e-02],
        [ 2.1622e-02, -3.4103e-03, -3.7567e-02,  ..., -3.7346e-03,
          5.3741e-02,  4.3066e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7129, -1.7373, -3.4023,  ...,  0.4922,  1.9902,  0.0342]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 18:55:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for clothing fabric
wire metal
doorknob metal
ocean water
money paper
sea water
jam fruit
box
2024-07-15 18:55:27 root INFO     [order_1_approx] starting weight calculation for box cardboard
jam fruit
clothing fabric
doorknob metal
money paper
wire metal
sea water
ocean
2024-07-15 18:55:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 18:59:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0195,  0.0602,  0.2939,  ..., -2.1699,  0.7256, -0.1686],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.3223, -2.1797, -0.0352,  ..., -5.6094,  1.7715, -1.0957],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.7241e-01, -1.0765e-02,  1.4679e-02,  ..., -2.7252e-02,
          3.3539e-02, -7.2250e-03],
        [ 1.8143e-02,  3.9185e-01, -1.2970e-02,  ...,  8.5831e-03,
         -1.9836e-02,  8.5831e-05],
        [ 3.4302e-02, -1.8463e-02,  4.6045e-01,  ...,  1.0338e-02,
         -6.1531e-03, -3.3508e-02],
        ...,
        [-1.7899e-02,  7.8087e-03, -8.3618e-03,  ...,  4.7803e-01,
          2.2240e-03,  8.8959e-03],
        [-6.2866e-03,  5.3711e-03, -2.1820e-02,  ..., -3.3112e-02,
          4.6704e-01, -1.2970e-04],
        [ 2.5696e-02, -8.1482e-03, -1.5526e-03,  ..., -3.5305e-03,
          8.1024e-03,  4.2285e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.7930, -1.9658, -0.3704,  ..., -3.8516,  0.0791, -1.0742]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 18:59:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for box cardboard
jam fruit
clothing fabric
doorknob metal
money paper
wire metal
sea water
ocean
2024-07-15 18:59:35 root INFO     [order_1_approx] starting weight calculation for doorknob metal
ocean water
clothing fabric
wire metal
jam fruit
sea water
box cardboard
money
2024-07-15 18:59:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 19:03:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3584, -0.1626,  0.1934,  ..., -0.0742,  0.5913,  0.2544],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.7734, -0.4951, -0.1348,  ..., -1.8516,  4.2578,  0.4199],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.5044e-01, -3.6011e-02, -9.1705e-03,  ...,  3.0136e-02,
          6.1989e-04, -3.1677e-02],
        [ 4.8645e-02,  3.9771e-01, -2.7466e-04,  ..., -1.4801e-02,
         -6.3782e-03,  8.3618e-03],
        [ 1.3062e-02,  6.2714e-03,  4.4165e-01,  ..., -3.2684e-02,
         -5.0049e-03,  1.0628e-02],
        ...,
        [-7.9422e-03,  2.2522e-02, -4.7913e-03,  ...,  4.4385e-01,
         -6.5079e-03,  2.3193e-02],
        [-8.4229e-03,  2.5497e-02, -2.5482e-02,  ..., -2.6978e-02,
          4.0698e-01,  1.7181e-02],
        [-5.5313e-03, -3.1921e-02, -2.0172e-02,  ..., -1.6083e-02,
          8.6136e-03,  4.1406e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.4375,  0.6875,  1.0137,  ..., -1.8525,  2.3828, -0.1714]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 19:03:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for doorknob metal
ocean water
clothing fabric
wire metal
jam fruit
sea water
box cardboard
money
2024-07-15 19:03:43 root INFO     [order_1_approx] starting weight calculation for clothing fabric
wire metal
ocean water
jam fruit
sea water
money paper
box cardboard
doorknob
2024-07-15 19:03:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 19:07:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4868,  0.3604, -1.0400,  ...,  0.4170,  0.2441,  0.1188],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7876,  0.0654,  0.3311,  ..., -2.7578,  1.2725,  2.8359],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4314,  0.0020,  0.0157,  ..., -0.0046, -0.0322, -0.0312],
        [ 0.0069,  0.4089, -0.0052,  ...,  0.0183,  0.0119,  0.0007],
        [ 0.0255,  0.0224,  0.4407,  ..., -0.0077, -0.0231,  0.0265],
        ...,
        [ 0.0088,  0.0197,  0.0130,  ...,  0.4341, -0.0080,  0.0058],
        [ 0.0080, -0.0097,  0.0066,  ..., -0.0167,  0.4231, -0.0150],
        [ 0.0171, -0.0033,  0.0192,  ..., -0.0341,  0.0122,  0.4087]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9927, -1.5684, -1.0762,  ...,  0.6055,  3.3711,  4.1719]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 19:07:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for clothing fabric
wire metal
ocean water
jam fruit
sea water
money paper
box cardboard
doorknob
2024-07-15 19:07:52 root INFO     total operator prediction time: 1980.1898665428162 seconds
2024-07-15 19:07:52 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on synonyms - intensity
2024-07-15 19:07:52 root INFO     building operator synonyms - intensity
2024-07-15 19:07:52 root INFO     [order_1_approx] starting weight calculation for cry scream
house palace
dislike hate
soon immediately
love adore
chuckle laugh
creative ingenious
tired
2024-07-15 19:07:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 19:12:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0762, -0.4468, -0.5010,  ..., -0.4380, -0.4263, -0.4009],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.5508, -3.3164, -2.9336,  ..., -3.5176,  1.9863,  0.5732],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4426,  0.0130,  0.0107,  ..., -0.0101,  0.0069, -0.0497],
        [-0.0072,  0.3862, -0.0032,  ..., -0.0100,  0.0181,  0.0005],
        [-0.0330,  0.0469,  0.4692,  ..., -0.0014, -0.0075,  0.0099],
        ...,
        [-0.0312,  0.0027, -0.0045,  ...,  0.4473,  0.0311,  0.0221],
        [ 0.0039,  0.0137,  0.0042,  ..., -0.0230,  0.4438,  0.0099],
        [ 0.0074, -0.0257, -0.0164,  ..., -0.0425, -0.0116,  0.4102]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.1719,  0.6191, -2.2109,  ..., -4.0117,  3.4375,  2.4297]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 19:12:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for cry scream
house palace
dislike hate
soon immediately
love adore
chuckle laugh
creative ingenious
tired
2024-07-15 19:12:01 root INFO     [order_1_approx] starting weight calculation for love adore
soon immediately
tired exhausted
house palace
creative ingenious
chuckle laugh
dislike hate
cry
2024-07-15 19:12:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 19:16:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0293, -0.9326,  1.2617,  ...,  0.0159,  0.3943,  0.2317],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([  4.8398, -10.1406,  -1.4209,  ...,  -2.4492,   2.9883,  -4.9688],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4885, -0.0644, -0.0372,  ..., -0.0387, -0.0029, -0.0879],
        [ 0.0107,  0.3845, -0.0105,  ...,  0.0036,  0.0202, -0.0262],
        [ 0.0113,  0.0421,  0.4783,  ...,  0.0083, -0.0222,  0.0122],
        ...,
        [ 0.0336, -0.0131, -0.0110,  ...,  0.4563,  0.0349, -0.0137],
        [ 0.0082, -0.0023,  0.0149,  ...,  0.0097,  0.4172,  0.0124],
        [ 0.0396,  0.0093, -0.0021,  ..., -0.0297,  0.0266,  0.3972]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.1719, -8.8906, -0.2139,  ..., -3.2734,  3.9961, -5.5117]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 19:16:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for love adore
soon immediately
tired exhausted
house palace
creative ingenious
chuckle laugh
dislike hate
cry
2024-07-15 19:16:10 root INFO     [order_1_approx] starting weight calculation for house palace
cry scream
creative ingenious
love adore
soon immediately
chuckle laugh
tired exhausted
dislike
2024-07-15 19:16:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 19:20:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1064, -0.1631,  1.6846,  ...,  0.0110,  0.4807,  1.0479],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.4238, -0.6621, -0.9277,  ..., -1.0215,  2.4531, -0.9155],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.0723e-01, -2.0050e-02, -3.1433e-02,  ..., -1.2863e-02,
          1.5198e-02, -7.8278e-03],
        [ 1.9531e-02,  3.8794e-01,  7.4043e-03,  ...,  7.0572e-03,
         -1.9440e-02,  2.2049e-02],
        [ 3.7903e-02, -9.7961e-03,  3.9331e-01,  ..., -1.4839e-02,
          2.4460e-02,  4.0436e-04],
        ...,
        [-1.3779e-02,  4.7882e-02,  1.5427e-02,  ...,  4.4238e-01,
          6.9809e-03, -8.7891e-03],
        [ 4.7729e-02,  4.4952e-02,  1.3260e-02,  ...,  9.6283e-03,
          4.0601e-01,  3.7292e-02],
        [ 1.9440e-02, -4.0665e-03, -3.2196e-02,  ..., -6.4468e-03,
          1.8295e-02,  4.0723e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.2402, -1.5967, -0.2192,  ..., -0.2349,  2.3359, -0.8887]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 19:20:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for house palace
cry scream
creative ingenious
love adore
soon immediately
chuckle laugh
tired exhausted
dislike
2024-07-15 19:20:18 root INFO     [order_1_approx] starting weight calculation for tired exhausted
house palace
chuckle laugh
dislike hate
soon immediately
creative ingenious
cry scream
love
2024-07-15 19:20:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 19:24:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3530, -0.3364,  0.5557,  ...,  0.2434, -0.4897,  0.2456],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 6.4844, -0.4541, -1.7285,  ..., -4.6484,  2.1777, -2.5664],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.0112e-01, -7.4005e-03,  6.9923e-03,  ...,  1.3298e-02,
          1.0422e-02, -7.7534e-04],
        [ 1.2283e-02,  3.8013e-01, -1.5121e-02,  ...,  1.5411e-03,
         -1.7548e-03,  1.1368e-03],
        [ 1.1139e-02, -1.2817e-02,  3.8770e-01,  ..., -8.4991e-03,
         -1.9333e-02, -2.4857e-02],
        ...,
        [-5.2032e-03, -3.5591e-03, -8.3847e-03,  ...,  4.5483e-01,
          2.5909e-02, -2.5055e-02],
        [ 2.0981e-04, -1.4801e-02,  1.2627e-02,  ..., -8.9798e-03,
          3.7134e-01,  2.0111e-02],
        [ 4.3823e-02, -1.7670e-02, -8.0109e-03,  ...,  9.4452e-03,
         -7.0343e-03,  3.7329e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.2461, -1.1016, -0.6348,  ..., -4.2734,  1.5361, -1.7520]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 19:24:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for tired exhausted
house palace
chuckle laugh
dislike hate
soon immediately
creative ingenious
cry scream
love
2024-07-15 19:24:26 root INFO     [order_1_approx] starting weight calculation for dislike hate
creative ingenious
cry scream
house palace
love adore
chuckle laugh
tired exhausted
soon
2024-07-15 19:24:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 19:28:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2432, -0.0044,  0.4717,  ...,  0.2656, -0.1311, -0.2179],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7676,  1.0049, -4.9219,  ...,  0.2861,  1.6924, -1.9902],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.4238e-01, -3.0502e-02,  4.4220e-02,  ..., -1.3687e-02,
          3.0502e-02,  1.4908e-02],
        [-3.9673e-04,  4.4019e-01,  7.6447e-03,  ...,  1.2634e-02,
         -1.8768e-02,  1.6113e-02],
        [ 1.7838e-02,  4.8599e-03,  4.3604e-01,  ...,  2.1561e-02,
          2.6108e-02, -2.6108e-02],
        ...,
        [-8.9417e-03, -2.8305e-03,  8.5449e-04,  ...,  4.5337e-01,
         -1.9409e-02,  9.8419e-03],
        [ 1.1215e-02,  1.1841e-02,  3.4302e-02,  ...,  2.2507e-03,
          4.5483e-01, -1.2085e-02],
        [ 1.7746e-02,  6.8474e-03, -2.0142e-03,  ..., -4.2297e-02,
         -2.5604e-02,  4.3994e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4521,  0.0786, -3.4902,  ...,  0.2849,  1.5703, -2.3047]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 19:28:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for dislike hate
creative ingenious
cry scream
house palace
love adore
chuckle laugh
tired exhausted
soon
2024-07-15 19:28:34 root INFO     [order_1_approx] starting weight calculation for cry scream
dislike hate
house palace
love adore
creative ingenious
soon immediately
tired exhausted
chuckle
2024-07-15 19:28:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 19:32:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3857,  0.1323, -0.1194,  ..., -0.4121,  1.7119,  0.3140],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2754, -1.8477,  0.0742,  ..., -0.0244,  4.4062, -4.5547],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4175,  0.0304, -0.0096,  ..., -0.0213,  0.0064,  0.0104],
        [ 0.0294,  0.3970,  0.0251,  ...,  0.0277, -0.0088, -0.0096],
        [-0.0191,  0.0396,  0.4478,  ...,  0.0105, -0.0210,  0.0095],
        ...,
        [-0.0595,  0.0210,  0.0521,  ...,  0.4609, -0.0031,  0.0416],
        [-0.0087, -0.0276, -0.0026,  ...,  0.0151,  0.4272,  0.0234],
        [ 0.0540,  0.0432, -0.0376,  ..., -0.0547,  0.0153,  0.3962]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7925, -2.0762, -0.1090,  ...,  0.9585,  1.7773, -4.0820]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 19:32:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for cry scream
dislike hate
house palace
love adore
creative ingenious
soon immediately
tired exhausted
chuckle
2024-07-15 19:32:43 root INFO     [order_1_approx] starting weight calculation for soon immediately
tired exhausted
house palace
love adore
cry scream
chuckle laugh
dislike hate
creative
2024-07-15 19:32:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 19:36:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5059, -1.2188, -0.2900,  ...,  0.5303,  0.6133, -1.2148],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3838, -1.3281, -0.0137,  ..., -1.5342,  3.4355, -2.1777],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4517, -0.0030, -0.0191,  ...,  0.0068,  0.0037, -0.0203],
        [ 0.0273,  0.3918,  0.0127,  ..., -0.0170, -0.0088, -0.0020],
        [ 0.0192, -0.0103,  0.4775,  ..., -0.0110, -0.0124, -0.0559],
        ...,
        [-0.0144, -0.0233,  0.0066,  ...,  0.4534,  0.0074, -0.0078],
        [ 0.0198,  0.0057, -0.0216,  ..., -0.0214,  0.4509, -0.0218],
        [-0.0331, -0.0058, -0.0029,  ...,  0.0063,  0.0218,  0.4500]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5322, -0.9502, -1.2910,  ..., -3.4805,  2.7441, -2.9922]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 19:36:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for soon immediately
tired exhausted
house palace
love adore
cry scream
chuckle laugh
dislike hate
creative
2024-07-15 19:36:51 root INFO     [order_1_approx] starting weight calculation for soon immediately
creative ingenious
dislike hate
tired exhausted
chuckle laugh
love adore
cry scream
house
2024-07-15 19:36:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 19:40:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0659, -0.5547, -0.7856,  ...,  1.2793, -0.1047, -0.8311],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7246, -1.8711, -3.2031,  ..., -0.1206,  1.9551,  0.5273],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4275, -0.0137, -0.0097,  ..., -0.0179,  0.0278, -0.0387],
        [-0.0038,  0.3652,  0.0091,  ..., -0.0141, -0.0091,  0.0066],
        [ 0.0350,  0.0144,  0.4385,  ..., -0.0099, -0.0109, -0.0110],
        ...,
        [-0.0201, -0.0175,  0.0027,  ...,  0.4072,  0.0122, -0.0242],
        [ 0.0036,  0.0010,  0.0081,  ...,  0.0309,  0.4106,  0.0109],
        [ 0.0149, -0.0051, -0.0085,  ..., -0.0216, -0.0114,  0.3911]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1074, -1.5977, -1.7705,  ..., -0.0945,  0.7598, -1.6699]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 19:40:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for soon immediately
creative ingenious
dislike hate
tired exhausted
chuckle laugh
love adore
cry scream
house
2024-07-15 19:40:59 root INFO     total operator prediction time: 1987.4450507164001 seconds
2024-07-15 19:41:00 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on hypernyms - animals
2024-07-15 19:41:00 root INFO     building operator hypernyms - animals
2024-07-15 19:41:00 root INFO     [order_1_approx] starting weight calculation for mouse rodent
chimpanzee primate
cobra snake
viper snake
triceratops dinosaur
cow bovid
owl raptor
chicken
2024-07-15 19:41:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 19:45:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1830,  0.3521, -1.1006,  ...,  0.9141, -0.7773,  0.9937],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0625,  0.5078, -1.9033,  ..., -2.1680,  0.1230,  1.6094],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4900,  0.0010, -0.0042,  ..., -0.0071, -0.0128, -0.0162],
        [ 0.0013,  0.4275,  0.0154,  ..., -0.0208,  0.0106,  0.0118],
        [-0.0009, -0.0177,  0.5088,  ...,  0.0059,  0.0109,  0.0142],
        ...,
        [ 0.0379, -0.0160, -0.0344,  ...,  0.4922,  0.0244, -0.0069],
        [ 0.0297, -0.0097, -0.0450,  ..., -0.0096,  0.4756,  0.0090],
        [ 0.0191, -0.0247,  0.0266,  ...,  0.0259, -0.0022,  0.4558]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1758,  0.7163, -1.6270,  ..., -3.8184,  0.4458,  0.3457]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 19:45:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for mouse rodent
chimpanzee primate
cobra snake
viper snake
triceratops dinosaur
cow bovid
owl raptor
chicken
2024-07-15 19:45:06 root INFO     [order_1_approx] starting weight calculation for chimpanzee primate
chicken fowl
triceratops dinosaur
cobra snake
mouse rodent
cow bovid
owl raptor
viper
2024-07-15 19:45:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 19:49:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4658,  0.8916, -0.5112,  ..., -0.1174,  0.1156,  1.6016],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.2344, -1.6387, -1.6699,  ...,  0.7607,  0.3760,  4.5625],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 5.4297e-01,  8.5068e-03, -2.7100e-02,  ...,  2.4185e-03,
          2.4223e-03,  5.7373e-03],
        [-4.0527e-02,  4.7705e-01,  1.1330e-02,  ...,  4.6921e-03,
         -1.2512e-03,  2.3697e-02],
        [ 2.1408e-02,  1.2047e-02,  4.9854e-01,  ..., -1.0139e-02,
         -1.1421e-02, -6.8207e-03],
        ...,
        [ 7.7057e-03,  6.6376e-04, -1.4389e-02,  ...,  4.7998e-01,
         -4.1199e-02,  1.8143e-02],
        [ 1.7670e-02, -4.6204e-02,  9.2926e-03,  ...,  6.0501e-03,
          5.0684e-01,  1.2207e-04],
        [-4.1885e-03, -1.1894e-02, -1.6403e-02,  ...,  1.1124e-02,
          1.6022e-03,  4.6484e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.8203, -3.0312,  0.6680,  ...,  1.5811, -1.5039,  5.1172]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 19:49:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for chimpanzee primate
chicken fowl
triceratops dinosaur
cobra snake
mouse rodent
cow bovid
owl raptor
viper
2024-07-15 19:49:13 root INFO     [order_1_approx] starting weight calculation for mouse rodent
viper snake
chimpanzee primate
chicken fowl
triceratops dinosaur
owl raptor
cobra snake
cow
2024-07-15 19:49:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 19:53:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8223, -0.3716, -0.4746,  ...,  0.2002,  0.2401,  0.8477],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.5117,  0.8335, -0.5732,  ..., -4.7930,  1.6934, -2.1953],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.5405, -0.0025,  0.0051,  ...,  0.0192,  0.0097,  0.0040],
        [ 0.0021,  0.4348, -0.0209,  ..., -0.0370,  0.0124, -0.0031],
        [-0.0078,  0.0423,  0.5288,  ...,  0.0044, -0.0363,  0.0457],
        ...,
        [ 0.0059, -0.0279, -0.0206,  ...,  0.5024,  0.0285, -0.0472],
        [ 0.0215, -0.0008, -0.0255,  ...,  0.0008,  0.4854, -0.0139],
        [ 0.0334, -0.0125, -0.0104,  ..., -0.0217,  0.0122,  0.4663]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.8652,  2.4062,  0.2344,  ..., -2.3379,  1.5674, -2.2793]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 19:53:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for mouse rodent
viper snake
chimpanzee primate
chicken fowl
triceratops dinosaur
owl raptor
cobra snake
cow
2024-07-15 19:53:20 root INFO     [order_1_approx] starting weight calculation for triceratops dinosaur
cow bovid
mouse rodent
owl raptor
viper snake
chimpanzee primate
chicken fowl
cobra
2024-07-15 19:53:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 19:57:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1885,  0.2383, -1.5713,  ...,  0.7271, -0.3774, -0.6367],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1953, -2.9023,  0.9648,  ..., -0.7168, -2.2285,  0.6758],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.7998e-01,  1.2627e-02, -2.2125e-02,  ...,  1.7944e-02,
          3.7781e-02, -3.3646e-03],
        [-4.6265e-02,  4.2310e-01,  9.3994e-03,  ...,  8.7585e-03,
         -3.7384e-04,  1.3237e-02],
        [ 3.1738e-02,  1.6357e-02,  4.9487e-01,  ..., -1.3229e-02,
         -8.0872e-04, -2.3499e-03],
        ...,
        [-4.0436e-03, -9.8343e-03,  2.2507e-03,  ...,  4.5972e-01,
         -4.1504e-02,  2.8534e-02],
        [-1.7105e-02, -2.5665e-02, -5.0140e-02,  ...,  3.7689e-03,
          5.2100e-01,  1.1948e-02],
        [ 3.1250e-02, -1.5930e-02, -6.9046e-03,  ..., -4.3915e-02,
         -6.6406e-02,  4.4727e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.4883, -1.6709,  1.7969,  ...,  0.6855, -1.6797,  3.0039]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 19:57:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for triceratops dinosaur
cow bovid
mouse rodent
owl raptor
viper snake
chimpanzee primate
chicken fowl
cobra
2024-07-15 19:57:26 root INFO     [order_1_approx] starting weight calculation for chicken fowl
viper snake
cobra snake
mouse rodent
triceratops dinosaur
chimpanzee primate
cow bovid
owl
2024-07-15 19:57:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 20:01:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3179, -0.4023, -0.6724,  ..., -0.9111,  0.1873,  0.2108],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.0000,  3.4219,  2.2812,  ...,  3.6699,  1.3184, -0.5488],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.6143e-01,  6.6071e-03,  1.5039e-03,  ..., -2.1805e-02,
          8.9493e-03, -1.1490e-02],
        [-2.1393e-02,  4.7241e-01,  5.7449e-03,  ...,  1.5812e-03,
         -1.7227e-02, -7.5302e-03],
        [-8.3237e-03, -1.4496e-03,  4.9585e-01,  ..., -8.0261e-03,
          2.7725e-02,  3.1281e-04],
        ...,
        [ 7.7133e-03, -9.9030e-03, -2.1835e-02,  ...,  4.9878e-01,
         -4.9820e-03,  7.8354e-03],
        [ 1.8631e-02, -2.9663e-02, -1.4511e-02,  ...,  1.4671e-02,
          4.9585e-01,  2.3026e-02],
        [ 1.0674e-02,  1.4099e-02,  9.7656e-03,  ..., -2.0859e-02,
          8.7891e-03,  4.2773e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4141,  4.3555,  0.6074,  ...,  5.1719,  0.5229, -2.0977]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 20:01:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for chicken fowl
viper snake
cobra snake
mouse rodent
triceratops dinosaur
chimpanzee primate
cow bovid
owl
2024-07-15 20:01:34 root INFO     [order_1_approx] starting weight calculation for chicken fowl
cow bovid
mouse rodent
cobra snake
owl raptor
triceratops dinosaur
viper snake
chimpanzee
2024-07-15 20:01:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 20:05:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4973, -0.2181, -0.9209,  ..., -0.0513,  0.3318,  0.6772],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 9.2109,  0.7852,  4.8203,  ..., -4.5859, -0.1736,  1.3730],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4434, -0.0115,  0.0519,  ..., -0.0123,  0.0130,  0.0205],
        [-0.0068,  0.4446,  0.0332,  ..., -0.0016,  0.0134,  0.0183],
        [-0.0153, -0.0208,  0.5083,  ..., -0.0025,  0.0304,  0.0465],
        ...,
        [-0.0342, -0.0118, -0.0693,  ...,  0.4797, -0.0059, -0.0181],
        [ 0.0425,  0.0120, -0.0599,  ..., -0.0167,  0.4502, -0.0224],
        [ 0.0316,  0.0255, -0.0187,  ...,  0.0035,  0.0182,  0.4580]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.6953, -0.1899,  2.4863,  ..., -0.9355,  2.4121,  2.1895]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 20:05:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for chicken fowl
cow bovid
mouse rodent
cobra snake
owl raptor
triceratops dinosaur
viper snake
chimpanzee
2024-07-15 20:05:41 root INFO     [order_1_approx] starting weight calculation for cobra snake
owl raptor
chicken fowl
mouse rodent
chimpanzee primate
cow bovid
viper snake
triceratops
2024-07-15 20:05:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 20:09:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3169,  1.7979, -1.0234,  ...,  0.8691, -0.1025,  0.5586],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.6875,  0.0068,  4.5039,  ...,  1.6426, -1.4062,  1.2686],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 5.3125e-01, -2.1423e-02,  2.8000e-02,  ...,  8.7357e-04,
          1.4618e-02,  7.5912e-03],
        [-2.4963e-02,  4.9731e-01,  1.8311e-02,  ...,  4.1534e-02,
          3.0384e-03, -3.1662e-04],
        [ 2.2736e-02,  7.0763e-03,  5.2344e-01,  ...,  3.2349e-03,
         -3.9215e-02,  2.1454e-02],
        ...,
        [-2.1988e-02,  8.8120e-03, -2.5085e-02,  ...,  5.3809e-01,
         -6.7024e-03, -3.5278e-02],
        [-2.0569e-02,  4.2267e-02, -4.6417e-02,  ..., -1.0414e-02,
          5.1221e-01, -2.8671e-02],
        [ 5.4199e-02, -1.3931e-02,  1.8005e-02,  ..., -4.1290e-02,
         -6.2866e-03,  4.9976e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.3750, -3.7031,  6.3477,  ...,  1.6133, -2.5938,  2.1523]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 20:09:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for cobra snake
owl raptor
chicken fowl
mouse rodent
chimpanzee primate
cow bovid
viper snake
triceratops
2024-07-15 20:09:48 root INFO     [order_1_approx] starting weight calculation for chimpanzee primate
triceratops dinosaur
cow bovid
viper snake
chicken fowl
cobra snake
owl raptor
mouse
2024-07-15 20:09:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 20:13:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6357,  0.4932, -0.4744,  ...,  0.0091, -0.7534,  0.0734],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.9414,  4.2930,  0.9824,  ..., -5.1328, -1.2246,  2.3691],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.5459e-01,  3.9711e-03,  1.4420e-03,  ...,  3.8147e-05,
         -8.9569e-03, -1.8784e-02],
        [-4.5700e-03,  4.2822e-01,  3.3325e-02,  ...,  3.7308e-03,
         -2.5894e-02,  2.5803e-02],
        [ 1.2566e-02, -9.8648e-03,  4.5996e-01,  ..., -3.4714e-03,
          1.6037e-02,  3.6926e-03],
        ...,
        [ 1.0529e-02,  1.8860e-02, -1.1795e-02,  ...,  4.6924e-01,
          2.5543e-02, -3.4485e-02],
        [ 5.4092e-03,  1.2329e-02, -7.4196e-03,  ...,  3.7384e-03,
          4.4336e-01,  2.3682e-02],
        [-4.9934e-03, -5.9052e-03, -1.1200e-02,  ..., -1.1230e-02,
         -8.1329e-03,  4.4531e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.3594,  5.8359,  0.6211,  ..., -3.2578,  0.0771,  2.7129]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 20:13:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for chimpanzee primate
triceratops dinosaur
cow bovid
viper snake
chicken fowl
cobra snake
owl raptor
mouse
2024-07-15 20:13:48 root INFO     total operator prediction time: 1968.84987616539 seconds
2024-07-15 20:13:48 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on hyponyms - misc
2024-07-15 20:13:48 root INFO     building operator hyponyms - misc
2024-07-15 20:13:49 root INFO     [order_1_approx] starting weight calculation for sofa divan
shirt polo
sweater turtleneck
poem haiku
dress gown
drum tambourine
dessert cake
gun
2024-07-15 20:13:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 20:17:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.7471,  0.5098, -0.0232,  ...,  0.4341, -0.0630, -0.2255],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.8672,  1.1816,  0.8027,  ..., -2.3203,  2.8711, -0.1665],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.7852e-01,  2.3453e-02, -2.0996e-02,  ..., -4.5929e-03,
         -8.8959e-03, -1.0414e-02],
        [-2.6855e-03,  4.3994e-01, -4.6120e-03,  ...,  1.7654e-02,
         -1.1139e-02,  1.5945e-02],
        [ 6.5689e-03, -2.4414e-04,  4.8535e-01,  ..., -2.0782e-02,
          6.6833e-03,  1.4595e-02],
        ...,
        [ 4.6921e-04, -1.1879e-02, -4.2725e-02,  ...,  4.9878e-01,
          1.8158e-03, -2.6779e-02],
        [-3.1872e-03,  2.0416e-02, -1.3191e-02,  ...,  3.9253e-03,
          4.4727e-01,  2.9480e-02],
        [ 6.6452e-03,  7.1106e-03,  7.7400e-03,  ...,  4.1924e-03,
          2.3060e-03,  4.5459e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7959,  0.5601,  0.5381,  ..., -1.3799,  3.3652, -0.1486]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 20:17:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for sofa divan
shirt polo
sweater turtleneck
poem haiku
dress gown
drum tambourine
dessert cake
gun
2024-07-15 20:17:56 root INFO     [order_1_approx] starting weight calculation for sweater turtleneck
dress gown
poem haiku
shirt polo
drum tambourine
sofa divan
gun rifle
dessert
2024-07-15 20:17:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 20:22:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4194,  0.3594,  0.0547,  ...,  1.1699,  0.1599,  0.2600],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.6953,  1.8096,  1.9336,  ..., -0.5142, -0.7808,  0.9434],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 5.1172e-01, -2.0477e-02, -3.5950e-02,  ..., -1.5106e-02,
         -3.5187e-02, -6.4758e-02],
        [-1.4877e-04,  4.4238e-01,  1.7792e-02,  ...,  1.0040e-02,
         -2.2163e-03,  3.1555e-02],
        [ 1.0689e-02,  1.5625e-02,  4.8633e-01,  ...,  5.8289e-03,
         -1.6113e-02, -1.3657e-03],
        ...,
        [ 9.3155e-03,  3.4576e-02, -1.2672e-02,  ...,  4.9683e-01,
          1.0424e-03, -2.6031e-02],
        [ 1.8188e-02, -1.6098e-02, -7.0343e-03,  ...,  1.7181e-02,
          4.4922e-01,  1.8967e-02],
        [ 3.5980e-02, -3.5278e-02,  2.4567e-02,  ...,  7.7553e-03,
          8.7357e-04,  4.6729e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.2227, -0.5127,  2.4375,  ..., -1.7031, -2.3203,  0.7461]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 20:22:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for sweater turtleneck
dress gown
poem haiku
shirt polo
drum tambourine
sofa divan
gun rifle
dessert
2024-07-15 20:22:02 root INFO     [order_1_approx] starting weight calculation for sofa divan
dessert cake
poem haiku
gun rifle
drum tambourine
shirt polo
dress gown
sweater
2024-07-15 20:22:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 20:26:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4900,  0.6069, -0.8467,  ...,  0.6816,  0.9492,  0.5190],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.1133,  0.3628,  1.5117,  ..., -5.5664,  0.9175, -3.9805],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4810, -0.0065, -0.0161,  ..., -0.0195,  0.0284, -0.0118],
        [ 0.0387,  0.4858,  0.0057,  ...,  0.0016,  0.0403, -0.0046],
        [ 0.0370, -0.0075,  0.4832,  ...,  0.0190, -0.0047, -0.0056],
        ...,
        [ 0.0047,  0.0088,  0.0294,  ...,  0.4968, -0.0318,  0.0077],
        [-0.0146, -0.0361, -0.0037,  ..., -0.0143,  0.4490,  0.0498],
        [ 0.0187, -0.0171, -0.0013,  ..., -0.0022,  0.0178,  0.4607]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1562, -2.3633,  1.5898,  ..., -5.6055, -1.4043, -2.6387]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 20:26:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for sofa divan
dessert cake
poem haiku
gun rifle
drum tambourine
shirt polo
dress gown
sweater
2024-07-15 20:26:08 root INFO     [order_1_approx] starting weight calculation for sofa divan
sweater turtleneck
dessert cake
gun rifle
drum tambourine
dress gown
poem haiku
shirt
2024-07-15 20:26:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 20:30:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2314, -0.0085, -0.5254,  ..., -0.0160,  0.5859,  0.5205],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0723, -0.0217, -1.2832,  ..., -7.7734,  2.2539, -1.0352],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4771, -0.0015,  0.0027,  ..., -0.0026, -0.0071, -0.0242],
        [ 0.0187,  0.4119, -0.0077,  ..., -0.0113,  0.0056, -0.0194],
        [ 0.0264, -0.0361,  0.4626,  ...,  0.0364,  0.0173, -0.0140],
        ...,
        [-0.0189, -0.0271, -0.0038,  ...,  0.4824, -0.0091, -0.0023],
        [-0.0286, -0.0188, -0.0136,  ..., -0.0204,  0.3884,  0.0489],
        [ 0.0244,  0.0055, -0.0188,  ...,  0.0248,  0.0229,  0.4443]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9082,  0.3518,  1.3789,  ..., -5.9609,  0.2793,  0.2217]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 20:30:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for sofa divan
sweater turtleneck
dessert cake
gun rifle
drum tambourine
dress gown
poem haiku
shirt
2024-07-15 20:30:14 root INFO     [order_1_approx] starting weight calculation for sofa divan
dress gown
sweater turtleneck
dessert cake
shirt polo
gun rifle
poem haiku
drum
2024-07-15 20:30:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 20:34:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.5400,  0.6353, -0.6978,  ...,  0.4392, -1.4395, -0.3691],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.3799,  0.6094, -0.9512,  ..., -7.3594, -0.7168, -3.7910],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 5.0879e-01, -1.1566e-02, -4.5837e-02,  ..., -1.7151e-02,
          2.9633e-02, -4.5471e-02],
        [ 8.6212e-03,  4.6606e-01, -7.4158e-03,  ...,  2.5711e-02,
          1.8463e-03,  1.8738e-02],
        [ 1.1642e-02,  6.1493e-03,  5.2441e-01,  ..., -2.1149e-02,
          2.5558e-04, -1.7212e-02],
        ...,
        [ 3.4363e-02, -4.7264e-03, -8.5327e-02,  ...,  5.0781e-01,
          6.8054e-03, -7.7332e-02],
        [-3.3531e-03,  1.2878e-02, -1.9043e-02,  ...,  1.8692e-02,
          4.9194e-01,  1.8677e-02],
        [ 3.3844e-02, -2.1606e-02, -6.6833e-02,  ...,  4.1138e-02,
         -3.4828e-03,  4.5752e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0449, -0.0210,  0.5693,  ..., -4.2031,  1.4121, -7.8203]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 20:34:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for sofa divan
dress gown
sweater turtleneck
dessert cake
shirt polo
gun rifle
poem haiku
drum
2024-07-15 20:34:18 root INFO     [order_1_approx] starting weight calculation for dessert cake
drum tambourine
sofa divan
poem haiku
shirt polo
sweater turtleneck
gun rifle
dress
2024-07-15 20:34:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 20:38:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6704, -0.0237,  0.4653,  ...,  0.1008, -0.6235, -0.9756],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.7148,  0.5205,  2.8457,  ..., -5.2812, -1.0234, -3.8359],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.4995e-01,  1.1932e-02,  1.3542e-04,  ..., -5.4871e-02,
          1.2558e-02, -9.5139e-03],
        [-2.3621e-02,  4.2236e-01,  1.9211e-02,  ...,  1.4496e-02,
         -4.4403e-03,  1.1444e-03],
        [-2.9793e-03,  2.3422e-02,  4.6533e-01,  ...,  1.4511e-02,
         -1.4374e-02,  2.2186e-02],
        ...,
        [-2.4780e-02, -3.9062e-03,  4.9286e-03,  ...,  4.6509e-01,
          1.9257e-02, -1.5839e-02],
        [ 4.8065e-03, -3.8208e-02,  4.7379e-03,  ..., -1.8120e-03,
          3.8867e-01,  9.3231e-03],
        [ 2.5101e-02,  6.6910e-03,  2.4612e-02,  ...,  7.7209e-03,
          2.0618e-03,  4.2847e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.6797, -1.4189,  2.9824,  ..., -6.3477,  1.1250, -2.9062]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 20:38:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for dessert cake
drum tambourine
sofa divan
poem haiku
shirt polo
sweater turtleneck
gun rifle
dress
2024-07-15 20:38:25 root INFO     [order_1_approx] starting weight calculation for shirt polo
dessert cake
gun rifle
sweater turtleneck
drum tambourine
dress gown
sofa divan
poem
2024-07-15 20:38:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 20:42:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5161,  0.2695,  0.2480,  ..., -1.1572,  0.3193,  0.0939],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7812, -0.9814, -5.5156,  ..., -3.8320,  3.0020, -6.0820],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.6582e-01,  3.3798e-03,  1.9836e-02,  ..., -2.2583e-02,
          1.8402e-02, -5.5908e-02],
        [ 1.7776e-03,  3.9722e-01,  6.1264e-03,  ..., -1.9684e-02,
         -1.0834e-02,  2.7122e-03],
        [ 1.5839e-02,  2.5635e-03,  4.1455e-01,  ..., -2.0981e-04,
          2.3193e-03, -4.1885e-03],
        ...,
        [-1.1978e-03,  6.1035e-04, -2.5757e-02,  ...,  4.2163e-01,
         -4.6883e-03, -2.3315e-02],
        [-4.1237e-03,  9.2621e-03, -5.4474e-03,  ...,  3.8086e-02,
          3.7280e-01,  2.1637e-02],
        [ 1.7517e-02,  7.0267e-03,  2.3483e-02,  ..., -1.7761e-02,
         -1.0696e-02,  4.1162e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.8789, -1.6641, -5.5625,  ..., -2.1445,  1.4902, -5.4453]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 20:42:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for shirt polo
dessert cake
gun rifle
sweater turtleneck
drum tambourine
dress gown
sofa divan
poem
2024-07-15 20:42:30 root INFO     [order_1_approx] starting weight calculation for dress gown
poem haiku
shirt polo
drum tambourine
gun rifle
sweater turtleneck
dessert cake
sofa
2024-07-15 20:42:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 20:46:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1439,  0.6987,  0.2186,  ...,  0.6997,  1.8291, -0.1791],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4834, -4.8125, -1.0420,  ..., -3.0977, -3.4648, -1.9844],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.4043e-01, -2.9419e-02, -2.0157e-02,  ..., -4.5624e-03,
         -1.4496e-04, -2.3315e-02],
        [ 2.4109e-02,  3.9868e-01, -3.9886e-02,  ...,  9.2392e-03,
          5.8708e-03, -4.6783e-02],
        [ 8.6899e-03,  2.7054e-02,  4.4189e-01,  ...,  4.7264e-03,
         -1.0361e-02, -1.9455e-02],
        ...,
        [ 9.3460e-05,  7.6294e-06, -2.1500e-02,  ...,  4.5557e-01,
         -8.1177e-03, -2.6260e-02],
        [ 1.7685e-02, -2.0035e-02, -1.2657e-02,  ...,  1.2581e-02,
          4.3384e-01, -1.5289e-02],
        [ 1.3443e-02, -9.6970e-03, -2.9251e-02,  ..., -7.0953e-04,
         -3.4332e-05,  4.2847e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9707, -5.1992, -1.3174,  ..., -1.8633, -4.1172,  0.6191]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 20:46:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for dress gown
poem haiku
shirt polo
drum tambourine
gun rifle
sweater turtleneck
dessert cake
sofa
2024-07-15 20:46:35 root INFO     total operator prediction time: 1966.9208686351776 seconds
2024-07-15 20:46:35 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on antonyms - binary
2024-07-15 20:46:35 root INFO     building operator antonyms - binary
2024-07-15 20:46:36 root INFO     [order_1_approx] starting weight calculation for under over
internal external
in out
input output
fall rise
climb descend
inbound outbound
before
2024-07-15 20:46:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 20:50:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6953,  0.6616, -0.0820,  ...,  0.0146,  0.1877, -0.1868],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.2500, -3.0117, -0.7666,  ...,  0.9492,  5.5312,  0.8418],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.1138e-01, -6.8817e-03,  8.5449e-04,  ..., -7.5188e-03,
         -4.1504e-02, -3.6713e-02],
        [ 6.1646e-03,  3.7842e-01, -5.7411e-03,  ..., -1.4008e-02,
          9.7046e-03,  2.4811e-02],
        [ 1.6464e-02, -5.7602e-04,  4.3091e-01,  ...,  1.6174e-02,
         -1.6727e-03, -3.3325e-02],
        ...,
        [-3.9551e-02,  4.0619e-02,  1.2917e-02,  ...,  4.3506e-01,
         -1.1749e-03, -3.0960e-02],
        [ 7.0572e-05,  2.7756e-02,  3.7872e-02,  ..., -1.5472e-02,
          4.3164e-01,  2.9938e-02],
        [ 1.5472e-02, -7.3242e-03, -5.3497e-02,  ..., -3.0975e-02,
          5.5237e-03,  3.8330e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0430, -2.4746,  0.4717,  ...,  0.2305,  3.0625,  0.2485]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 20:50:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for under over
internal external
in out
input output
fall rise
climb descend
inbound outbound
before
2024-07-15 20:50:42 root INFO     [order_1_approx] starting weight calculation for inbound outbound
fall rise
internal external
before after
input output
under over
climb descend
in
2024-07-15 20:50:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 20:54:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8506,  0.0205, -0.1500,  ..., -0.5459,  0.8525, -0.1866],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8809, -4.8164, -4.2500,  ..., -3.6504,  4.5312, -2.4727],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4260,  0.0079, -0.0016,  ...,  0.0025,  0.0013, -0.0241],
        [-0.0092,  0.3831, -0.0016,  ..., -0.0079, -0.0087,  0.0179],
        [ 0.0117, -0.0097,  0.4658,  ..., -0.0179, -0.0051, -0.0055],
        ...,
        [ 0.0231, -0.0264,  0.0178,  ...,  0.4944,  0.0236, -0.0173],
        [-0.0048, -0.0055, -0.0105,  ...,  0.0034,  0.4265,  0.0295],
        [-0.0200, -0.0008, -0.0258,  ...,  0.0032, -0.0069,  0.3982]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9917, -6.5547, -3.6250,  ..., -4.1211,  3.4121, -3.4590]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 20:54:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for inbound outbound
fall rise
internal external
before after
input output
under over
climb descend
in
2024-07-15 20:54:49 root INFO     [order_1_approx] starting weight calculation for in out
before after
climb descend
under over
input output
fall rise
inbound outbound
internal
2024-07-15 20:54:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 20:58:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2100,  1.2666,  0.0223,  ..., -0.0439,  0.0024,  0.9629],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5449, -1.3320,  1.0938,  ..., -0.2817,  3.7480,  3.0742],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.1699e-01, -9.0027e-03,  7.4844e-03,  ...,  1.5442e-02,
         -4.2725e-03, -7.6065e-03],
        [-7.0381e-04,  4.2432e-01, -6.7024e-03,  ..., -4.1199e-03,
          6.0463e-04,  2.3254e-02],
        [ 3.5583e-02, -2.6978e-02,  4.3018e-01,  ..., -4.2419e-02,
         -6.1798e-03,  2.2545e-03],
        ...,
        [-2.5177e-04, -2.4429e-02,  1.4664e-02,  ...,  4.6338e-01,
          1.9684e-02,  1.1971e-02],
        [-1.0460e-02,  1.1200e-02,  9.4986e-03,  ..., -8.2397e-03,
          4.2310e-01,  8.5754e-03],
        [-3.4637e-03,  4.8065e-04, -1.0124e-02,  ..., -6.9199e-03,
         -2.9678e-03,  3.9990e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.2539, -2.7051,  3.7266,  ..., -1.4277,  4.0547,  1.6123]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 20:58:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for in out
before after
climb descend
under over
input output
fall rise
inbound outbound
internal
2024-07-15 20:58:56 root INFO     [order_1_approx] starting weight calculation for input output
under over
climb descend
internal external
before after
in out
fall rise
inbound
2024-07-15 20:58:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 21:03:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7993,  0.4336, -0.1047,  ...,  0.3064, -0.7690, -0.4270],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4385, -2.6699, -0.1973,  ..., -0.1797,  5.8594, -3.9844],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4307,  0.0054,  0.0111,  ..., -0.0237,  0.0059, -0.0245],
        [ 0.0172,  0.4414,  0.0353,  ..., -0.0024, -0.0378,  0.0127],
        [ 0.0090,  0.0091,  0.4314,  ...,  0.0414,  0.0095, -0.0411],
        ...,
        [-0.0174, -0.0438, -0.0294,  ...,  0.4739, -0.0066,  0.0172],
        [ 0.0017, -0.0165, -0.0082,  ..., -0.0271,  0.4507, -0.0084],
        [ 0.0616, -0.0216,  0.0012,  ..., -0.0064,  0.0390,  0.4409]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2510, -1.3828, -2.3047,  ..., -0.4390,  5.3750, -4.2109]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 21:03:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for input output
under over
climb descend
internal external
before after
in out
fall rise
inbound
2024-07-15 21:03:03 root INFO     [order_1_approx] starting weight calculation for before after
internal external
in out
under over
climb descend
input output
inbound outbound
fall
2024-07-15 21:03:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 21:07:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.8711,  0.2822, -0.1884,  ..., -0.3313, -0.3865,  0.1152],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1748, -2.8008, -1.9727,  ..., -1.1484, -0.3169, -0.1904],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.2920e-01,  1.8997e-03, -1.5053e-02,  ..., -2.0599e-04,
         -2.2545e-03, -9.3231e-03],
        [ 2.7649e-02,  3.9233e-01, -5.0888e-03,  ..., -1.0017e-02,
          3.6621e-04, -5.6686e-03],
        [ 1.8707e-02, -2.0599e-03,  4.3408e-01,  ..., -2.8717e-02,
          5.4855e-03,  7.3318e-03],
        ...,
        [-3.2215e-03,  1.2512e-03,  7.1602e-03,  ...,  4.3799e-01,
         -2.4414e-03,  8.1635e-04],
        [ 8.4839e-03, -1.9112e-03, -6.2637e-03,  ...,  2.1683e-02,
          4.2871e-01,  5.4169e-03],
        [ 2.8671e-02,  8.6594e-03, -1.7914e-02,  ..., -7.3700e-03,
          8.2550e-03,  3.8989e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0332, -4.9922,  0.3750,  ..., -2.6719,  0.2212,  0.2764]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 21:07:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for before after
internal external
in out
under over
climb descend
input output
inbound outbound
fall
2024-07-15 21:07:10 root INFO     [order_1_approx] starting weight calculation for internal external
climb descend
input output
fall rise
before after
inbound outbound
in out
under
2024-07-15 21:07:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 21:11:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1189,  0.1467,  0.0480,  ..., -0.1663,  0.5518, -0.1146],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.9961, -2.6973,  1.5771,  ..., -2.0664,  5.4297, -2.3809],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4482, -0.0274, -0.0006,  ..., -0.0066,  0.0023, -0.0357],
        [ 0.0106,  0.4170,  0.0037,  ..., -0.0159, -0.0294,  0.0457],
        [-0.0114, -0.0137,  0.4080,  ...,  0.0043, -0.0131, -0.0051],
        ...,
        [ 0.0136, -0.0234,  0.0337,  ...,  0.4275, -0.0039,  0.0194],
        [-0.0045,  0.0107, -0.0093,  ...,  0.0187,  0.4082,  0.0203],
        [ 0.0681, -0.0142,  0.0041,  ..., -0.0184, -0.0089,  0.4114]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.9883, -4.2188,  1.3965,  ..., -2.4199,  3.9102, -1.6299]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 21:11:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for internal external
climb descend
input output
fall rise
before after
inbound outbound
in out
under
2024-07-15 21:11:16 root INFO     [order_1_approx] starting weight calculation for climb descend
internal external
before after
inbound outbound
under over
in out
fall rise
input
2024-07-15 21:11:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 21:15:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0654, -0.0535,  0.3171,  ..., -0.0939, -0.8262,  0.5137],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.1094, -3.3633,  2.0098,  ..., -0.3384,  2.5449,  0.1953],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4409, -0.0061, -0.0097,  ...,  0.0155, -0.0225, -0.0132],
        [-0.0197,  0.3999, -0.0047,  ...,  0.0137,  0.0094,  0.0090],
        [ 0.0233, -0.0276,  0.4578,  ...,  0.0079,  0.0012,  0.0084],
        ...,
        [-0.0015, -0.0488,  0.0100,  ...,  0.4849, -0.0206, -0.0129],
        [-0.0123,  0.0323, -0.0211,  ...,  0.0138,  0.4177,  0.0074],
        [ 0.0132,  0.0030, -0.0111,  ..., -0.0266, -0.0087,  0.4150]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.5664, -2.8438,  0.6641,  ..., -1.9746,  5.9648,  0.3359]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 21:15:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for climb descend
internal external
before after
inbound outbound
under over
in out
fall rise
input
2024-07-15 21:15:23 root INFO     [order_1_approx] starting weight calculation for inbound outbound
before after
in out
internal external
fall rise
input output
under over
climb
2024-07-15 21:15:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 21:19:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3774, -0.9082, -0.0596,  ..., -0.6670,  0.9028,  0.0114],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.7812, -4.1406,  1.2363,  ..., -2.0371,  2.4863, -2.4961],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4380,  0.0253,  0.0087,  ...,  0.0273, -0.0190, -0.0102],
        [ 0.0198,  0.4253, -0.0068,  ...,  0.0081,  0.0046, -0.0008],
        [ 0.0016, -0.0059,  0.4431,  ...,  0.0160, -0.0396, -0.0007],
        ...,
        [-0.0165, -0.0075,  0.0098,  ...,  0.5337, -0.0246,  0.0096],
        [-0.0157, -0.0016,  0.0194,  ...,  0.0088,  0.4771, -0.0179],
        [ 0.0471, -0.0046, -0.0113,  ...,  0.0149, -0.0031,  0.4587]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.5547, -3.4141,  0.6782,  ..., -0.6748,  2.6836, -4.3516]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 21:19:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for inbound outbound
before after
in out
internal external
fall rise
input output
under over
climb
2024-07-15 21:19:29 root INFO     total operator prediction time: 1974.050089597702 seconds
2024-07-15 21:19:29 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - member
2024-07-15 21:19:29 root INFO     building operator meronyms - member
2024-07-15 21:19:30 root INFO     [order_1_approx] starting weight calculation for bee swarm
letter alphabet
nomad horde
senator senate
person society
song album
musician orchestra
cattle
2024-07-15 21:19:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 21:23:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0879, -0.0732, -0.9590,  ...,  0.9717, -0.5234,  0.0563],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.1875,  0.6533, -2.3203,  ..., -3.0039,  1.6924, -4.0078],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.5259, -0.0013, -0.0175,  ..., -0.0072,  0.0199, -0.0327],
        [-0.0042,  0.4497,  0.0057,  ..., -0.0112, -0.0081,  0.0161],
        [ 0.0180,  0.0224,  0.5264,  ..., -0.0228, -0.0297, -0.0088],
        ...,
        [-0.0028,  0.0041, -0.0188,  ...,  0.5200,  0.0063,  0.0066],
        [-0.0024,  0.0026, -0.0224,  ...,  0.0016,  0.4819,  0.0146],
        [ 0.0182,  0.0285,  0.0249,  ..., -0.0122, -0.0098,  0.4612]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.3047, -1.1748, -0.3311,  ..., -2.3652,  1.1475, -4.6172]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 21:23:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for bee swarm
letter alphabet
nomad horde
senator senate
person society
song album
musician orchestra
cattle
2024-07-15 21:23:39 root INFO     [order_1_approx] starting weight calculation for bee swarm
song album
musician orchestra
person society
letter alphabet
cattle herd
nomad horde
senator
2024-07-15 21:23:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 21:27:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8960,  0.0070,  0.4084,  ...,  1.1006,  0.9307, -0.8091],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 7.2227, -0.6694,  0.0684,  ...,  2.4004,  2.8008, -1.4355],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 5.0586e-01,  1.5106e-02, -1.5656e-02,  ..., -2.4582e-02,
          1.5671e-02, -2.4734e-02],
        [ 4.6692e-03,  4.3506e-01,  3.5858e-04,  ...,  4.1046e-03,
          6.3019e-03, -6.6452e-03],
        [-5.7526e-03, -2.7481e-02,  4.5093e-01,  ...,  1.3535e-02,
         -7.4348e-03,  7.5607e-03],
        ...,
        [-1.4763e-02, -1.3542e-02, -1.1482e-02,  ...,  4.9634e-01,
         -2.0386e-02, -2.0004e-02],
        [ 3.5583e-02, -1.3161e-02, -1.5053e-02,  ..., -9.3536e-03,
          4.5288e-01,  3.3539e-02],
        [ 4.4067e-02, -5.7907e-03, -2.7618e-03,  ..., -2.1530e-02,
          1.2711e-02,  5.0586e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.0703,  0.3940,  3.0000,  ...,  2.0078,  1.0254, -2.0605]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 21:27:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for bee swarm
song album
musician orchestra
person society
letter alphabet
cattle herd
nomad horde
senator
2024-07-15 21:27:47 root INFO     [order_1_approx] starting weight calculation for bee swarm
musician orchestra
person society
senator senate
song album
nomad horde
cattle herd
letter
2024-07-15 21:27:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 21:31:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3149,  0.2332,  0.7061,  ..., -0.5527, -0.2452,  0.1061],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.9141, -0.9893,  1.6797,  ..., -4.7500,  1.8389,  0.9429],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4800, -0.0453, -0.0080,  ..., -0.0257,  0.0070, -0.0272],
        [ 0.0298,  0.4519,  0.0116,  ..., -0.0260,  0.0052, -0.0017],
        [-0.0007, -0.0200,  0.4673,  ...,  0.0207,  0.0015,  0.0377],
        ...,
        [ 0.0177, -0.0204, -0.0111,  ...,  0.4888,  0.0264,  0.0074],
        [ 0.0130,  0.0007, -0.0074,  ..., -0.0164,  0.4551, -0.0027],
        [ 0.0053,  0.0032, -0.0143,  ..., -0.0031,  0.0250,  0.4663]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.3242,  0.6416,  3.9961,  ..., -3.0938,  1.9062,  1.3457]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 21:31:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for bee swarm
musician orchestra
person society
senator senate
song album
nomad horde
cattle herd
letter
2024-07-15 21:31:56 root INFO     [order_1_approx] starting weight calculation for musician orchestra
song album
bee swarm
letter alphabet
person society
senator senate
cattle herd
nomad
2024-07-15 21:31:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 21:36:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3440, -1.3477, -0.5317,  ..., -0.4631, -0.4604, -1.1895],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3286, -1.0039, -3.4355,  ..., -3.2812,  1.6680, -3.3809],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4688, -0.0170,  0.0261,  ..., -0.0141,  0.0121,  0.0029],
        [-0.0070,  0.4529,  0.0139,  ...,  0.0674,  0.0196,  0.0104],
        [ 0.0518,  0.0016,  0.4978,  ..., -0.0089,  0.0256, -0.0322],
        ...,
        [ 0.0127, -0.0063,  0.0328,  ...,  0.4670,  0.0353,  0.0038],
        [ 0.0290, -0.0107, -0.0063,  ..., -0.0476,  0.4475, -0.0205],
        [ 0.0240, -0.0030, -0.0063,  ..., -0.0050, -0.0100,  0.4678]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8110, -1.8027, -2.1445,  ..., -2.1270,  1.1250, -2.2188]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 21:36:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for musician orchestra
song album
bee swarm
letter alphabet
person society
senator senate
cattle herd
nomad
2024-07-15 21:36:05 root INFO     [order_1_approx] starting weight calculation for letter alphabet
senator senate
song album
cattle herd
bee swarm
nomad horde
musician orchestra
person
2024-07-15 21:36:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 21:40:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2588, -0.2322, -0.3472,  ...,  0.1221,  0.1748, -0.3174],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2334, -3.5625, -2.0312,  ..., -5.7070,  2.1973,  2.7227],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4937, -0.0327,  0.0274,  ..., -0.0206,  0.0144, -0.0223],
        [-0.0043,  0.4319, -0.0182,  ..., -0.0043, -0.0100, -0.0154],
        [-0.0085,  0.0047,  0.4602,  ..., -0.0153, -0.0053,  0.0110],
        ...,
        [ 0.0117, -0.0069, -0.0050,  ...,  0.4604, -0.0062, -0.0067],
        [ 0.0091, -0.0237, -0.0093,  ...,  0.0265,  0.4402,  0.0126],
        [ 0.0369,  0.0201, -0.0026,  ..., -0.0082, -0.0129,  0.4563]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7354, -3.1992, -1.6387,  ..., -5.6094,  0.3867,  4.0664]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 21:40:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for letter alphabet
senator senate
song album
cattle herd
bee swarm
nomad horde
musician orchestra
person
2024-07-15 21:40:14 root INFO     [order_1_approx] starting weight calculation for person society
senator senate
cattle herd
letter alphabet
nomad horde
song album
bee swarm
musician
2024-07-15 21:40:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 21:44:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2627, -0.0566,  0.9082,  ..., -0.2098,  0.2693, -1.1416],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.1758, -3.0977,  0.5137,  ..., -5.1875,  3.9160, -5.2070],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.3457e-01, -4.6448e-02,  1.0895e-02,  ..., -4.5586e-03,
          2.5879e-02, -4.5502e-02],
        [ 6.1569e-03,  3.9307e-01,  1.5961e-02,  ..., -1.3489e-02,
          1.8829e-02, -3.0930e-02],
        [ 1.4061e-02, -2.5558e-02,  4.5020e-01,  ..., -3.0785e-03,
         -4.8676e-03, -9.8114e-03],
        ...,
        [-5.7220e-05, -6.8359e-02, -2.6489e-02,  ...,  4.4604e-01,
          1.0242e-03, -1.7944e-02],
        [ 1.8234e-03, -4.2152e-03,  1.9928e-02,  ...,  1.5900e-02,
          4.3042e-01, -1.2108e-02],
        [ 1.3222e-02, -1.6388e-02, -2.3376e-02,  ..., -2.7328e-02,
         -2.0203e-02,  3.9185e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1162, -4.4102, -2.5215,  ..., -2.7812,  1.1777, -3.6875]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 21:44:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for person society
senator senate
cattle herd
letter alphabet
nomad horde
song album
bee swarm
musician
2024-07-15 21:44:22 root INFO     [order_1_approx] starting weight calculation for letter alphabet
person society
musician orchestra
cattle herd
nomad horde
senator senate
song album
bee
2024-07-15 21:44:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 21:48:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1006,  0.1753, -1.2070,  ..., -0.8320, -0.0850,  0.7417],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6318,  2.8242, -4.1250,  ..., -3.1543,  3.5977,  2.0801],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.9585e-01, -1.8936e-02,  8.1940e-03,  ...,  6.7253e-03,
          1.2756e-02, -3.3875e-02],
        [ 2.4200e-02,  4.4629e-01,  1.4938e-02,  ...,  1.5793e-03,
          3.7537e-03, -1.6815e-02],
        [-2.8595e-02,  3.4256e-03,  5.0049e-01,  ..., -8.3237e-03,
          5.1079e-03, -1.8356e-02],
        ...,
        [ 2.1362e-04, -2.5726e-02,  2.1103e-02,  ...,  5.3174e-01,
         -1.9016e-03, -9.7580e-03],
        [ 3.5706e-02, -3.3455e-03, -2.1454e-02,  ...,  7.4348e-03,
          4.9829e-01,  3.0609e-02],
        [ 2.4017e-02,  8.3771e-03,  2.4338e-03,  ...,  4.4098e-03,
         -2.7428e-03,  4.9438e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.6875,  2.2344, -4.1055,  ..., -0.9062,  3.8672,  1.8594]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 21:48:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for letter alphabet
person society
musician orchestra
cattle herd
nomad horde
senator senate
song album
bee
2024-07-15 21:48:31 root INFO     [order_1_approx] starting weight calculation for bee swarm
person society
cattle herd
letter alphabet
nomad horde
senator senate
musician orchestra
song
2024-07-15 21:48:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 21:52:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5630, -0.3008,  0.4382,  ..., -0.7900,  0.1301, -1.2988],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.4531,  2.2773, -1.6846,  ..., -2.6504,  2.1719, -6.2969],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.9487e-01,  1.0223e-02, -2.6550e-02,  ...,  2.2697e-03,
         -5.3673e-03, -4.2938e-02],
        [ 2.5497e-02,  4.4458e-01, -8.6975e-03,  ..., -1.2451e-02,
         -1.1200e-02, -4.4212e-03],
        [ 2.8702e-02,  9.9945e-03,  4.6362e-01,  ...,  1.8768e-02,
          1.8845e-02, -9.6436e-03],
        ...,
        [ 1.7548e-04,  1.6434e-02, -2.1423e-02,  ...,  4.7510e-01,
         -1.0223e-02,  3.0518e-02],
        [ 1.7654e-02, -3.7506e-02, -3.9368e-02,  ...,  6.8283e-04,
          4.4556e-01,  6.5804e-03],
        [ 1.7929e-02, -1.6724e-02,  1.0246e-02,  ...,  1.8997e-02,
         -2.4490e-03,  4.5337e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.7031,  4.6328,  0.1396,  ..., -1.4395,  1.0352, -4.0820]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 21:52:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for bee swarm
person society
cattle herd
letter alphabet
nomad horde
senator senate
musician orchestra
song
2024-07-15 21:52:40 root INFO     total operator prediction time: 1990.568112373352 seconds
2024-07-15 21:52:40 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on noun - plural_irreg
2024-07-15 21:52:40 root INFO     building operator noun - plural_irreg
2024-07-15 21:52:40 root INFO     [order_1_approx] starting weight calculation for category categories
series series
county counties
industry industries
library libraries
community communities
city cities
majority
2024-07-15 21:52:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 21:56:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3882,  0.5820,  1.1514,  ..., -1.4658, -0.3398, -0.5825],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.5312, -2.2793,  4.3828,  ..., -0.2349, -0.0967,  0.0347],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4910,  0.0122, -0.0019,  ...,  0.0103,  0.0325,  0.0198],
        [-0.0047,  0.4343, -0.0128,  ...,  0.0219,  0.0145, -0.0073],
        [-0.0058,  0.0257,  0.4509,  ...,  0.0239, -0.0522,  0.0044],
        ...,
        [-0.0292, -0.0100,  0.0346,  ...,  0.5078,  0.0161, -0.0155],
        [ 0.0307,  0.0060, -0.0137,  ..., -0.0044,  0.4780,  0.0014],
        [ 0.0269,  0.0109, -0.0167,  ...,  0.0139, -0.0108,  0.4434]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.1133, -0.9365,  6.5703,  ...,  0.6816, -0.8672,  2.4824]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 21:56:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for category categories
series series
county counties
industry industries
library libraries
community communities
city cities
majority
2024-07-15 21:56:47 root INFO     [order_1_approx] starting weight calculation for city cities
majority majorities
library libraries
community communities
county counties
series series
category categories
industry
2024-07-15 21:56:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 22:00:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2429,  0.7593,  1.3994,  ..., -0.3035,  0.1902, -0.5361],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6299, -3.7266,  2.2109,  ..., -0.5771, -1.9141,  1.1924],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4644,  0.0284, -0.0341,  ..., -0.0367,  0.0291, -0.0398],
        [-0.0169,  0.4216,  0.0021,  ..., -0.0006, -0.0122,  0.0254],
        [ 0.0210,  0.0277,  0.4424,  ..., -0.0133, -0.0224,  0.0052],
        ...,
        [ 0.0233, -0.0420,  0.0048,  ...,  0.4626,  0.0293, -0.0363],
        [ 0.0345,  0.0875, -0.0719,  ..., -0.0412,  0.4409, -0.0290],
        [-0.0030, -0.0120,  0.0231,  ..., -0.0183,  0.0068,  0.4448]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3062, -3.5527,  1.5576,  ..., -1.5137, -1.8389,  0.4907]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 22:00:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for city cities
majority majorities
library libraries
community communities
county counties
series series
category categories
industry
2024-07-15 22:00:54 root INFO     [order_1_approx] starting weight calculation for majority majorities
series series
county counties
city cities
community communities
library libraries
industry industries
category
2024-07-15 22:00:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 22:04:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5293,  0.2639,  0.2100,  ..., -0.6924,  0.2988, -0.3000],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.1016, -2.6602,  3.4082,  ...,  0.1919, -1.4277, -1.1191],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4268,  0.0171,  0.0027,  ..., -0.0229,  0.0406,  0.0101],
        [-0.0085,  0.3909,  0.0063,  ...,  0.0259, -0.0179, -0.0157],
        [ 0.0175, -0.0125,  0.4458,  ..., -0.0068, -0.0156, -0.0189],
        ...,
        [-0.0078, -0.0210,  0.0079,  ...,  0.4451,  0.0375,  0.0219],
        [ 0.0146,  0.0335, -0.0221,  ..., -0.0039,  0.4175,  0.0258],
        [ 0.0334,  0.0170, -0.0069,  ..., -0.0273,  0.0108,  0.4077]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.7598, -2.5410,  2.9883,  ..., -0.9019, -0.4131, -0.4004]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 22:04:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for majority majorities
series series
county counties
city cities
community communities
library libraries
industry industries
category
2024-07-15 22:04:59 root INFO     [order_1_approx] starting weight calculation for city cities
industry industries
county counties
community communities
category categories
majority majorities
series series
library
2024-07-15 22:04:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 22:09:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9370, -0.2578, -0.8081,  ...,  0.1665, -0.0370, -0.1724],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8159, -3.1973,  0.3975,  ..., -2.5352,  1.6455,  1.9395],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4468, -0.0330,  0.0038,  ...,  0.0093,  0.0380, -0.0279],
        [ 0.0272,  0.4023,  0.0305,  ..., -0.0103,  0.0291,  0.0231],
        [-0.0405,  0.0172,  0.4456,  ...,  0.0013, -0.0186,  0.0020],
        ...,
        [-0.0014, -0.0272,  0.0360,  ...,  0.4512,  0.0306,  0.0017],
        [ 0.0146,  0.0498,  0.0178,  ...,  0.0039,  0.3936, -0.0073],
        [-0.0048,  0.0374, -0.0295,  ..., -0.0018,  0.0131,  0.4160]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5142, -3.5859,  0.9351,  ..., -1.5098,  0.4717,  2.0137]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 22:09:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for city cities
industry industries
county counties
community communities
category categories
majority majorities
series series
library
2024-07-15 22:09:06 root INFO     [order_1_approx] starting weight calculation for industry industries
city cities
majority majorities
county counties
category categories
library libraries
series series
community
2024-07-15 22:09:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 22:13:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7427,  0.6582, -0.1969,  ...,  0.1255,  0.0205, -0.2749],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2100, -2.1270,  2.0078,  ..., -2.4062, -1.5166,  0.9326],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4561, -0.0159, -0.0075,  ..., -0.0270,  0.0082, -0.0145],
        [ 0.0058,  0.4141,  0.0068,  ..., -0.0250,  0.0378, -0.0187],
        [ 0.0140,  0.0118,  0.4639,  ...,  0.0015,  0.0362,  0.0331],
        ...,
        [ 0.0422,  0.0061, -0.0091,  ...,  0.4497,  0.0338,  0.0136],
        [ 0.0448,  0.0450, -0.0137,  ...,  0.0099,  0.4033,  0.0164],
        [ 0.0037,  0.0078, -0.0140,  ..., -0.0229,  0.0087,  0.3972]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2949, -3.9648,  0.2939,  ..., -4.8555,  2.1797, -0.1045]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 22:13:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for industry industries
city cities
majority majorities
county counties
category categories
library libraries
series series
community
2024-07-15 22:13:13 root INFO     [order_1_approx] starting weight calculation for industry industries
city cities
category categories
community communities
county counties
majority majorities
library libraries
series
2024-07-15 22:13:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 22:17:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5483,  0.7896,  0.6982,  ...,  1.1396,  0.4785,  0.2854],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6758, -2.7852,  3.6719,  ...,  0.2324, -8.0312,  1.3145],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4341,  0.0269,  0.0187,  ...,  0.0088,  0.0264, -0.0274],
        [-0.0299,  0.4089,  0.0053,  ...,  0.0144, -0.0301, -0.0122],
        [ 0.0440,  0.0323,  0.4246,  ...,  0.0216,  0.0302,  0.0038],
        ...,
        [ 0.0102, -0.0257,  0.0062,  ...,  0.4634,  0.0025, -0.0279],
        [ 0.0051,  0.0552,  0.0047,  ...,  0.0417,  0.4277,  0.0172],
        [-0.0294,  0.0596, -0.0328,  ..., -0.0475,  0.0252,  0.4272]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.0586, -3.4160,  5.6641,  ..., -1.9395, -4.9922, -1.4688]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 22:17:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for industry industries
city cities
category categories
community communities
county counties
majority majorities
library libraries
series
2024-07-15 22:17:20 root INFO     [order_1_approx] starting weight calculation for community communities
series series
category categories
library libraries
industry industries
county counties
majority majorities
city
2024-07-15 22:17:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 22:21:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4834,  0.5166, -0.8164,  ..., -0.4121, -0.1743, -0.7729],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0469, -1.1406, -1.7178,  ..., -4.5508,  1.8682,  0.8496],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.9795e-01,  2.6215e-02, -3.7933e-02,  ..., -3.0304e-02,
          1.6937e-02, -3.1738e-02],
        [ 1.0674e-02,  3.9941e-01,  2.4841e-02,  ...,  1.4877e-03,
         -3.0533e-02,  4.0527e-02],
        [-8.0109e-04, -3.4218e-03,  4.3945e-01,  ..., -1.0307e-02,
          2.3102e-02,  9.5062e-03],
        ...,
        [ 2.3300e-02, -3.4313e-03, -1.3733e-04,  ...,  4.3994e-01,
          2.3468e-02, -2.4536e-02],
        [ 1.0941e-02,  2.0996e-02,  5.4817e-03,  ..., -1.0445e-02,
          3.8062e-01,  2.5803e-02],
        [ 1.5778e-02,  2.9495e-02, -2.1347e-02,  ..., -3.7689e-02,
          2.5879e-02,  3.9551e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7324, -0.3926,  1.1904,  ..., -2.1484,  1.8994,  2.9219]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 22:21:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for community communities
series series
category categories
library libraries
industry industries
county counties
majority majorities
city
2024-07-15 22:21:27 root INFO     [order_1_approx] starting weight calculation for city cities
majority majorities
library libraries
category categories
community communities
industry industries
series series
county
2024-07-15 22:21:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 22:25:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4912,  0.2600, -0.8252,  ...,  0.8340, -0.4355,  1.0703],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0723,  0.0688, -1.6943,  ..., -4.8164,  0.7080, -1.0693],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4060,  0.0517, -0.0382,  ..., -0.0271, -0.0327, -0.0089],
        [-0.0138,  0.3931,  0.0332,  ...,  0.0064,  0.0277,  0.0061],
        [-0.0043, -0.0175,  0.4746,  ..., -0.0557,  0.0110,  0.0081],
        ...,
        [-0.0130, -0.0297,  0.0169,  ...,  0.4619, -0.0044, -0.0079],
        [-0.0022,  0.0318, -0.0090,  ...,  0.0281,  0.4016, -0.0054],
        [-0.0101,  0.0035, -0.0247,  ..., -0.0302,  0.0070,  0.4143]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9453, -0.9312, -1.0498,  ..., -4.7461,  1.2148, -0.1592]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 22:25:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for city cities
majority majorities
library libraries
category categories
community communities
industry industries
series series
county
2024-07-15 22:25:33 root INFO     total operator prediction time: 1973.068276643753 seconds
2024-07-15 22:25:33 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on Ving - verb_inf
2024-07-15 22:25:33 root INFO     building operator Ving - verb_inf
2024-07-15 22:25:33 root INFO     [order_1_approx] starting weight calculation for continuing continue
understanding understand
believing believe
remaining remain
receiving receive
protecting protect
involving involve
reducing
2024-07-15 22:25:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 22:29:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3979, -0.1937,  0.7500,  ..., -0.7896, -0.0020,  0.2659],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2969, -1.5811, -0.6172,  ..., -1.4766,  0.4675,  2.7773],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4177,  0.0233,  0.0105,  ...,  0.0188,  0.0061,  0.0027],
        [-0.0528,  0.3696, -0.0064,  ..., -0.0050, -0.0197,  0.0068],
        [-0.0078,  0.0035,  0.4172,  ..., -0.0074,  0.0007, -0.0050],
        ...,
        [-0.0031,  0.0177,  0.0219,  ...,  0.4209,  0.0436, -0.0175],
        [ 0.0019,  0.0342,  0.0041,  ..., -0.0040,  0.3901,  0.0215],
        [-0.0017,  0.0171, -0.0248,  ..., -0.0269, -0.0129,  0.3977]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5107,  0.1309, -1.8066,  ...,  0.1846, -2.8613,  3.6660]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 22:29:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for continuing continue
understanding understand
believing believe
remaining remain
receiving receive
protecting protect
involving involve
reducing
2024-07-15 22:29:36 root INFO     [order_1_approx] starting weight calculation for believing believe
reducing reduce
protecting protect
involving involve
remaining remain
understanding understand
receiving receive
continuing
2024-07-15 22:29:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 22:33:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1267, -0.6670,  0.9922,  ..., -0.7686,  1.4854,  0.9844],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1816, -4.9961, -2.1543,  ...,  2.3906,  1.1084,  4.0156],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4263, -0.0143, -0.0051,  ..., -0.0026,  0.0194, -0.0230],
        [-0.0063,  0.4275, -0.0125,  ..., -0.0092, -0.0153,  0.0070],
        [-0.0104,  0.0455,  0.4060,  ..., -0.0061,  0.0178, -0.0394],
        ...,
        [-0.0057,  0.0012, -0.0046,  ...,  0.4771,  0.0029, -0.0255],
        [-0.0306, -0.0199,  0.0012,  ..., -0.0461,  0.4453,  0.0009],
        [ 0.0073,  0.0066, -0.0135,  ..., -0.0186, -0.0138,  0.4316]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3491, -3.4336, -2.6758,  ...,  3.0840, -1.8467,  4.1953]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 22:33:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for believing believe
reducing reduce
protecting protect
involving involve
remaining remain
understanding understand
receiving receive
continuing
2024-07-15 22:33:41 root INFO     [order_1_approx] starting weight calculation for protecting protect
remaining remain
receiving receive
believing believe
reducing reduce
continuing continue
understanding understand
involving
2024-07-15 22:33:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 22:37:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9053, -0.6758,  0.8477,  ..., -0.0793,  1.5605,  0.9146],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2908, -3.6309, -2.3809,  ...,  4.4922,  0.7363,  3.3672],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4670,  0.0203, -0.0032,  ..., -0.0161,  0.0362,  0.0243],
        [ 0.0150,  0.4595,  0.0173,  ...,  0.0119, -0.0045, -0.0219],
        [ 0.0121,  0.0182,  0.4878,  ..., -0.0277,  0.0113, -0.0284],
        ...,
        [-0.0137,  0.0172,  0.0435,  ...,  0.5112, -0.0173, -0.0286],
        [ 0.0146, -0.0060,  0.0023,  ..., -0.0018,  0.4531,  0.0273],
        [ 0.0307, -0.0424, -0.0536,  ..., -0.0114,  0.0255,  0.4805]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.0566, -3.1094, -4.0352,  ...,  4.6719,  1.2197,  5.1523]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 22:37:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for protecting protect
remaining remain
receiving receive
believing believe
reducing reduce
continuing continue
understanding understand
involving
2024-07-15 22:37:46 root INFO     [order_1_approx] starting weight calculation for continuing continue
receiving receive
involving involve
protecting protect
reducing reduce
believing believe
understanding understand
remaining
2024-07-15 22:37:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 22:41:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0811,  0.7158,  0.5762,  ..., -0.0627,  0.7686, -0.3364],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.4688,  1.6777, -0.8291,  ...,  2.4102,  1.6416,  4.2344],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.2065e-01, -4.8279e-02, -1.3557e-02,  ...,  7.1716e-03,
          1.9135e-02, -3.9520e-03],
        [-9.5367e-05,  4.3579e-01,  1.4107e-02,  ...,  2.9160e-02,
         -2.7618e-02,  1.3649e-02],
        [ 1.5625e-02,  4.3793e-02,  4.6094e-01,  ..., -9.9373e-04,
         -1.2802e-02, -1.9180e-02],
        ...,
        [-1.0475e-02,  4.8950e-02, -2.2827e-02,  ...,  4.5947e-01,
          2.4460e-02,  7.4005e-04],
        [ 1.6953e-02,  5.5542e-02,  3.8116e-02,  ...,  2.5253e-03,
          4.2603e-01,  2.4811e-02],
        [ 4.6005e-03,  2.9785e-02,  5.7831e-03,  ..., -1.1444e-02,
         -3.9520e-02,  4.2456e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1777,  1.6670, -3.4336,  ...,  3.4492, -1.5986,  4.0742]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 22:41:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for continuing continue
receiving receive
involving involve
protecting protect
reducing reduce
believing believe
understanding understand
remaining
2024-07-15 22:41:48 root INFO     [order_1_approx] starting weight calculation for reducing reduce
believing believe
understanding understand
involving involve
receiving receive
continuing continue
remaining remain
protecting
2024-07-15 22:41:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 22:45:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.6123, 0.4253, 0.6670,  ..., 0.5122, 0.3496, 0.2480], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3047,  0.1377, -0.9761,  ...,  1.3340,  0.1389,  5.5586],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.8940e-01, -2.6112e-03,  1.7242e-02,  ..., -1.1444e-02,
          2.1881e-02,  3.4485e-03],
        [-1.3016e-02,  3.8965e-01,  8.3923e-05,  ...,  1.7258e-02,
         -1.3695e-02,  5.5618e-03],
        [ 4.1779e-02,  1.0300e-02,  4.1821e-01,  ..., -2.3308e-03,
          1.9562e-02, -1.8082e-03],
        ...,
        [-2.3285e-02,  1.4511e-02,  1.5236e-02,  ...,  4.3115e-01,
          1.1147e-02, -3.0121e-02],
        [ 2.5452e-02, -1.3832e-02,  9.6664e-03,  ..., -3.2745e-02,
          4.0112e-01,  8.6746e-03],
        [ 1.4580e-02,  4.8676e-03, -3.1250e-02,  ..., -7.3242e-03,
          1.6434e-02,  4.0625e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4180,  0.9009, -1.8379,  ...,  1.6348, -3.1113,  7.8281]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 22:45:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for reducing reduce
believing believe
understanding understand
involving involve
receiving receive
continuing continue
remaining remain
protecting
2024-07-15 22:45:53 root INFO     [order_1_approx] starting weight calculation for involving involve
remaining remain
reducing reduce
protecting protect
continuing continue
understanding understand
believing believe
receiving
2024-07-15 22:45:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 22:49:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3162, -1.0947,  0.7329,  ..., -0.0847,  1.1055,  0.1431],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.8594, -4.8281,  1.8096,  ...,  2.6836,  3.9336,  3.9570],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.3774, -0.0123, -0.0080,  ..., -0.0163,  0.0255, -0.0065],
        [ 0.0174,  0.3792,  0.0207,  ...,  0.0090,  0.0017, -0.0284],
        [-0.0091,  0.0175,  0.4514,  ...,  0.0088, -0.0316, -0.0020],
        ...,
        [-0.0094, -0.0097,  0.0250,  ...,  0.4429,  0.0179, -0.0052],
        [ 0.0049, -0.0023, -0.0077,  ..., -0.0137,  0.4141,  0.0169],
        [ 0.0254, -0.0077, -0.0346,  ..., -0.0281,  0.0053,  0.3821]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.8711, -3.6875,  1.1855,  ...,  3.4570,  0.7734,  3.4883]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 22:49:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for involving involve
remaining remain
reducing reduce
protecting protect
continuing continue
understanding understand
believing believe
receiving
2024-07-15 22:49:58 root INFO     [order_1_approx] starting weight calculation for reducing reduce
receiving receive
continuing continue
protecting protect
involving involve
remaining remain
understanding understand
believing
2024-07-15 22:49:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 22:54:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0967, -0.4041,  0.4612,  ..., -0.2053,  1.6475, -0.2683],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.3809, -0.7827,  1.9141,  ...,  2.0879,  1.8906, -1.1523],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.1235e-01,  2.0752e-02, -2.0355e-02,  ...,  2.4673e-02,
         -5.9471e-03, -3.1860e-02],
        [-1.2779e-02,  3.9355e-01,  2.3697e-02,  ...,  4.2267e-03,
          3.4332e-04,  4.5280e-03],
        [-1.1040e-02,  1.8433e-02,  4.3481e-01,  ...,  1.6541e-02,
          2.2552e-02, -2.9816e-02],
        ...,
        [ 2.1332e-02,  1.3962e-02, -3.8818e-02,  ...,  4.4043e-01,
          4.0161e-02, -6.2561e-04],
        [ 2.6337e-02, -1.8250e-02,  1.7761e-02,  ..., -3.2593e-02,
          4.2554e-01,  3.9307e-02],
        [ 3.8643e-03, -1.0529e-02, -3.2898e-02,  ..., -3.7964e-02,
          6.5269e-03,  3.7939e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.2305, -1.4707,  0.7061,  ...,  3.4297, -1.5273,  0.9258]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 22:54:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for reducing reduce
receiving receive
continuing continue
protecting protect
involving involve
remaining remain
understanding understand
believing
2024-07-15 22:54:03 root INFO     [order_1_approx] starting weight calculation for receiving receive
involving involve
continuing continue
protecting protect
remaining remain
believing believe
reducing reduce
understanding
2024-07-15 22:54:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 22:58:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2476, -0.6685,  0.7632,  ..., -0.7041,  0.9697,  0.9312],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4609, -2.5957,  1.9092,  ..., -1.5869,  5.0547,  1.9980],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4194,  0.0088,  0.0041,  ..., -0.0023,  0.0007,  0.0015],
        [-0.0067,  0.3823,  0.0111,  ..., -0.0025, -0.0038, -0.0048],
        [-0.0170,  0.0175,  0.4570,  ..., -0.0031, -0.0137, -0.0046],
        ...,
        [ 0.0091, -0.0008,  0.0345,  ...,  0.4602,  0.0580, -0.0449],
        [ 0.0027,  0.0282, -0.0062,  ..., -0.0112,  0.3857,  0.0579],
        [ 0.0333,  0.0112,  0.0258,  ..., -0.0382,  0.0291,  0.4497]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[0.7334, 0.5020, 0.3555,  ..., 1.9717, 2.1152, 3.0117]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 22:58:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for receiving receive
involving involve
continuing continue
protecting protect
remaining remain
believing believe
reducing reduce
understanding
2024-07-15 22:58:08 root INFO     total operator prediction time: 1954.571223974228 seconds
2024-07-15 22:58:08 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_Ving - Ved
2024-07-15 22:58:08 root INFO     building operator verb_Ving - Ved
2024-07-15 22:58:08 root INFO     [order_1_approx] starting weight calculation for describing described
representing represented
uniting united
following followed
providing provided
involving involved
spending spent
understanding
2024-07-15 22:58:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 23:02:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2729, -0.6260,  0.8311,  ..., -0.5552,  0.9951,  0.3982],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.1270,  0.4736,  1.4336,  ..., -3.2930,  4.2930,  1.3320],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.1235e-01,  5.8899e-03, -1.0567e-02,  ..., -3.0365e-02,
          1.9714e-02, -3.6011e-03],
        [ 4.4250e-04,  3.9185e-01, -1.2360e-02,  ...,  3.1586e-02,
          7.6523e-03, -3.9215e-02],
        [ 1.3409e-03,  1.9592e-02,  4.3506e-01,  ...,  6.3248e-03,
         -8.9264e-03, -4.4342e-02],
        ...,
        [ 8.4763e-03,  3.3508e-02,  2.6367e-02,  ...,  4.3848e-01,
          1.4481e-02, -3.7109e-02],
        [ 8.2550e-03, -1.1345e-02, -7.8125e-03,  ...,  1.7120e-02,
          4.4385e-01,  8.5266e-02],
        [ 2.8687e-02, -7.7209e-03,  5.7983e-04,  ..., -3.2532e-02,
          3.4515e-02,  4.4751e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6914,  2.3867,  0.6729,  ..., -0.4102,  2.7734,  1.8691]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 23:02:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for describing described
representing represented
uniting united
following followed
providing provided
involving involved
spending spent
understanding
2024-07-15 23:02:15 root INFO     [order_1_approx] starting weight calculation for involving involved
uniting united
spending spent
understanding understood
following followed
describing described
representing represented
providing
2024-07-15 23:02:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 23:06:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4521, -0.4729,  0.6133,  ..., -0.0714,  1.0625,  0.2084],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7666, -2.0273,  2.2812,  ...,  1.0234,  1.2559,  0.9043],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.3872e-01, -3.2928e-02,  1.6663e-02,  ...,  3.7384e-02,
          1.7975e-02,  3.2623e-02],
        [-5.8861e-03,  3.9160e-01, -4.6753e-02,  ..., -1.9852e-02,
          2.6825e-02,  1.4977e-02],
        [ 4.4250e-03,  1.9867e-02,  4.7656e-01,  ..., -1.8967e-02,
         -2.7969e-02,  4.9438e-03],
        ...,
        [ 1.7410e-02, -1.4984e-02,  1.1932e-02,  ...,  4.4165e-01,
          4.9866e-02, -2.3697e-02],
        [ 4.9072e-02, -3.4313e-03,  8.3618e-03,  ..., -3.5431e-02,
          4.1699e-01, -1.0595e-03],
        [-1.8585e-02,  1.3031e-02,  3.8910e-04,  ..., -9.7427e-03,
         -5.1758e-02,  4.0625e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8076, -1.5166,  1.6973,  ...,  3.4668,  0.5542, -1.7168]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 23:06:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for involving involved
uniting united
spending spent
understanding understood
following followed
describing described
representing represented
providing
2024-07-15 23:06:23 root INFO     [order_1_approx] starting weight calculation for spending spent
understanding understood
describing described
following followed
providing provided
representing represented
involving involved
uniting
2024-07-15 23:06:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 23:10:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.7236, 0.4258, 0.8740,  ..., 0.6045, 0.5674, 0.7925], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9873,  0.8643, -0.8672,  ...,  2.1641,  1.4746,  1.4199],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.6436e-01, -1.2901e-02,  1.0948e-02,  ...,  2.7802e-02,
         -8.8120e-03,  3.2349e-02],
        [-3.6865e-02,  3.9893e-01,  5.1727e-03,  ...,  2.2278e-02,
          2.2659e-02, -1.3092e-02],
        [ 4.2084e-02,  1.6418e-02,  5.0928e-01,  ...,  3.0289e-02,
         -6.2790e-03, -1.9913e-02],
        ...,
        [ 2.6642e-02, -3.8483e-02,  3.7445e-02,  ...,  4.8145e-01,
          5.8929e-02,  3.1433e-03],
        [ 1.1909e-02,  1.7899e-02,  1.6785e-04,  ..., -2.1515e-02,
          4.5337e-01,  9.8114e-03],
        [ 6.5613e-02, -7.0305e-03, -5.1208e-02,  ..., -3.7109e-02,
          3.0640e-02,  5.0635e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.9990,  2.5840, -1.6318,  ...,  2.2949, -0.6777,  0.8027]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 23:10:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for spending spent
understanding understood
describing described
following followed
providing provided
representing represented
involving involved
uniting
2024-07-15 23:10:31 root INFO     [order_1_approx] starting weight calculation for uniting united
following followed
spending spent
representing represented
involving involved
understanding understood
providing provided
describing
2024-07-15 23:10:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 23:14:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1523, -0.2881,  0.4280,  ..., -0.6504,  0.9312,  0.6763],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2305, -1.4414,  0.9531,  ..., -0.1973, -1.3838,  0.0547],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.3945e-01, -3.5591e-03, -4.0710e-02,  ...,  1.6899e-03,
          2.2110e-02,  3.7445e-02],
        [-2.4658e-02,  4.0649e-01,  1.9562e-02,  ...,  4.9469e-02,
          1.7410e-02,  6.1951e-03],
        [-1.5053e-02,  6.9153e-02,  4.2871e-01,  ...,  4.1504e-03,
          1.8539e-02, -1.8978e-03],
        ...,
        [-3.1158e-02,  5.4626e-02,  2.1973e-02,  ...,  5.0732e-01,
         -1.0246e-02, -3.7415e-02],
        [-1.9135e-02,  9.0561e-03,  1.9119e-02,  ..., -5.8655e-02,
          4.3140e-01,  1.0052e-03],
        [-2.2697e-04, -1.6632e-03,  6.6223e-03,  ..., -1.9653e-02,
          4.4136e-03,  4.2773e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.0039, -3.1484, -2.0469,  ..., -1.8398, -3.4961,  1.2441]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 23:14:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for uniting united
following followed
spending spent
representing represented
involving involved
understanding understood
providing provided
describing
2024-07-15 23:14:38 root INFO     [order_1_approx] starting weight calculation for spending spent
describing described
uniting united
following followed
representing represented
providing provided
understanding understood
involving
2024-07-15 23:14:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 23:18:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6982, -0.7607,  0.8804,  ...,  0.4146,  1.7715,  1.0820],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0332, -2.3691, -3.1055,  ...,  6.4570,  1.9131,  2.0410],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.8730e-01, -1.8787e-03, -1.6632e-02,  ...,  5.5389e-03,
          1.5167e-02,  4.4098e-02],
        [-4.4250e-03,  4.5654e-01,  8.6975e-03,  ..., -5.1117e-03,
         -7.2784e-03,  1.9646e-03],
        [ 2.9068e-02,  3.5156e-02,  4.9756e-01,  ...,  2.2003e-02,
          3.4363e-02, -1.2772e-02],
        ...,
        [-2.4139e-02,  1.0643e-03,  3.3997e-02,  ...,  5.2441e-01,
         -5.8861e-03, -2.3911e-02],
        [-6.1035e-04,  2.7786e-02, -1.1002e-02,  ..., -1.7731e-02,
          4.3555e-01, -2.8362e-03],
        [ 2.6901e-02,  2.1744e-04, -3.9459e-02,  ..., -1.9043e-02,
          2.5757e-02,  4.9170e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7832, -1.6787, -5.9219,  ...,  6.4648,  1.8389,  3.0625]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 23:18:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for spending spent
describing described
uniting united
following followed
representing represented
providing provided
understanding understood
involving
2024-07-15 23:18:46 root INFO     [order_1_approx] starting weight calculation for uniting united
involving involved
describing described
representing represented
spending spent
providing provided
understanding understood
following
2024-07-15 23:18:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 23:22:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5254, -0.2827,  1.3281,  ..., -0.1382,  0.6406,  0.6406],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.2188, -2.5410,  3.5801,  ...,  1.8320,  1.4814,  2.7129],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.9697e-01,  1.4954e-03, -2.8168e-02,  ..., -4.0192e-02,
          1.3672e-02,  1.5015e-02],
        [-4.8828e-02,  3.8403e-01,  4.6021e-02,  ..., -2.1500e-02,
         -3.9062e-02, -9.2926e-03],
        [-9.8877e-03,  3.5736e-02,  4.3237e-01,  ...,  2.6321e-04,
         -6.4278e-04, -5.8441e-03],
        ...,
        [-1.7822e-02,  2.1408e-02,  4.1870e-02,  ...,  4.2236e-01,
          1.0162e-02, -1.5617e-02],
        [ 1.4069e-02, -2.8687e-02, -2.2095e-02,  ...,  4.3945e-02,
          4.6411e-01,  4.4098e-03],
        [ 1.2215e-02, -1.3657e-03, -2.1622e-02,  ...,  1.3176e-02,
         -1.6327e-02,  3.5278e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.3242, -4.4805,  2.2109,  ...,  2.9570,  1.1846,  3.2578]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 23:22:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for uniting united
involving involved
describing described
representing represented
spending spent
providing provided
understanding understood
following
2024-07-15 23:22:53 root INFO     [order_1_approx] starting weight calculation for understanding understood
spending spent
describing described
involving involved
providing provided
uniting united
following followed
representing
2024-07-15 23:22:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 23:26:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.6787, -0.8896,  0.2993,  ..., -0.2842,  0.9707,  0.5527],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.5137, -2.3262, -2.1953,  ...,  1.1553,  1.0312,  4.2383],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4504,  0.0057, -0.0349,  ...,  0.0082,  0.0357, -0.0334],
        [-0.0314,  0.4043,  0.0025,  ...,  0.0038,  0.0155, -0.0016],
        [ 0.0107, -0.0079,  0.4426,  ...,  0.0143, -0.0012, -0.0297],
        ...,
        [ 0.0099,  0.0423,  0.0041,  ...,  0.4275,  0.0497, -0.0273],
        [-0.0257,  0.0120, -0.0096,  ..., -0.0384,  0.4155,  0.0164],
        [ 0.0039,  0.0201,  0.0135,  ..., -0.0026, -0.0380,  0.4436]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5410, -2.7070, -0.5479,  ...,  2.1445, -2.0312,  2.7539]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 23:27:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for understanding understood
spending spent
describing described
involving involved
providing provided
uniting united
following followed
representing
2024-07-15 23:27:00 root INFO     [order_1_approx] starting weight calculation for describing described
understanding understood
following followed
involving involved
representing represented
uniting united
providing provided
spending
2024-07-15 23:27:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 23:30:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1406,  0.6797, -0.0553,  ..., -0.0652,  1.2725, -0.6279],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1239, -0.3779,  4.2891,  ...,  1.9990, -1.3047,  2.2090],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4390, -0.0056,  0.0171,  ..., -0.0015, -0.0338,  0.0327],
        [ 0.0235,  0.4119,  0.0097,  ..., -0.0245, -0.0010, -0.0045],
        [ 0.0559,  0.0033,  0.4507,  ..., -0.0286, -0.0110,  0.0015],
        ...,
        [ 0.0036,  0.0191, -0.0006,  ...,  0.4504,  0.0200, -0.0351],
        [-0.0068,  0.0042, -0.0025,  ..., -0.0135,  0.4539,  0.0126],
        [-0.0059,  0.0247,  0.0110,  ..., -0.0331, -0.0085,  0.4236]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7871,  0.2461,  4.9609,  ...,  2.5098, -5.0703,  0.7021]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 23:31:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for describing described
understanding understood
following followed
involving involved
representing represented
uniting united
providing provided
spending
2024-07-15 23:31:00 root INFO     total operator prediction time: 1972.0415551662445 seconds
2024-07-15 23:31:00 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_inf - Ved
2024-07-15 23:31:00 root INFO     building operator verb_inf - Ved
2024-07-15 23:31:00 root INFO     [order_1_approx] starting weight calculation for accept accepted
spend spent
lose lost
involve involved
marry married
add added
believe believed
perform
2024-07-15 23:31:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 23:35:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5049,  0.2661,  1.0410,  ..., -0.5259,  0.6660, -0.4229],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0811, -0.8340,  2.2656,  ..., -2.1094,  1.6299,  2.0117],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.3091e-01, -2.2888e-05,  1.7288e-02,  ...,  1.5205e-02,
          2.8610e-02, -3.4241e-02],
        [ 2.6978e-02,  3.8892e-01, -1.9577e-02,  ..., -2.8366e-02,
          8.1482e-03,  1.0735e-02],
        [ 2.3300e-02,  4.5700e-03,  3.9819e-01,  ...,  3.2349e-03,
         -1.6937e-02, -6.6071e-03],
        ...,
        [-2.1332e-02,  1.0353e-02,  8.3618e-03,  ...,  4.5044e-01,
          2.7733e-03,  1.1551e-02],
        [-7.8278e-03,  1.6464e-02,  1.7258e-02,  ..., -6.3438e-03,
          4.1089e-01,  5.7068e-03],
        [-2.1515e-02,  2.3407e-02, -1.4755e-02,  ...,  1.2054e-03,
          3.0823e-03,  3.9819e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.1855,  0.2861,  3.0039,  ..., -2.7012, -3.1250,  0.7607]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 23:35:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for accept accepted
spend spent
lose lost
involve involved
marry married
add added
believe believed
perform
2024-07-15 23:35:06 root INFO     [order_1_approx] starting weight calculation for believe believed
marry married
involve involved
spend spent
perform performed
add added
accept accepted
lose
2024-07-15 23:35:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 23:39:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1727,  0.1897,  1.0752,  ...,  0.2139,  0.2769,  0.4302],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.1172, -0.2266, -3.1289,  ..., -1.4746, -3.2559,  4.4688],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4309,  0.0199,  0.0462,  ...,  0.0159,  0.0288, -0.0023],
        [ 0.0062,  0.3789,  0.0071,  ...,  0.0112, -0.0039,  0.0261],
        [ 0.0246,  0.0033,  0.4238,  ..., -0.0181, -0.0085,  0.0092],
        ...,
        [ 0.0013,  0.0276,  0.0103,  ...,  0.4309, -0.0125,  0.0217],
        [-0.0164,  0.0140, -0.0328,  ...,  0.0074,  0.4104,  0.0187],
        [-0.0201,  0.0153,  0.0174,  ..., -0.0353,  0.0301,  0.3906]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.8301, -0.1838, -3.8008,  ..., -2.8496, -4.6523,  5.7617]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 23:39:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for believe believed
marry married
involve involved
spend spent
perform performed
add added
accept accepted
lose
2024-07-15 23:39:13 root INFO     [order_1_approx] starting weight calculation for spend spent
lose lost
add added
accept accepted
marry married
believe believed
perform performed
involve
2024-07-15 23:39:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 23:43:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1879,  0.1970,  0.7153,  ..., -0.2144,  0.4634,  1.7021],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.2598,  0.8818, -1.2510,  ...,  1.0762,  3.6230,  2.6367],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.4873e-01, -1.3153e-02,  4.5471e-02,  ...,  2.1210e-02,
          1.6754e-02,  1.0437e-02],
        [ 2.5368e-03,  4.3115e-01,  9.2773e-03,  ..., -1.9409e-02,
          2.5864e-03, -1.8433e-02],
        [ 4.2664e-02,  3.6163e-03,  4.7485e-01,  ..., -3.0899e-04,
          4.6570e-02, -2.0447e-02],
        ...,
        [-3.5278e-02, -6.5689e-03,  2.7649e-02,  ...,  4.9707e-01,
         -6.0486e-02, -5.2948e-02],
        [ 1.3504e-02,  1.1864e-03, -3.4821e-02,  ..., -2.2324e-02,
          4.5679e-01,  4.9042e-02],
        [ 8.5602e-03, -6.4621e-03, -5.8960e-02,  ..., -5.5328e-02,
          2.3712e-02,  4.5215e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.9961,  2.5312, -1.5879,  ...,  0.6279,  2.9062,  2.6719]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 23:43:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for spend spent
lose lost
add added
accept accepted
marry married
believe believed
perform performed
involve
2024-07-15 23:43:19 root INFO     [order_1_approx] starting weight calculation for lose lost
spend spent
perform performed
involve involved
accept accepted
add added
marry married
believe
2024-07-15 23:43:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 23:47:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0244, -0.4456, -0.3564,  ...,  0.4775,  0.5195, -0.5605],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.8672, -0.8384,  0.9023,  ...,  2.3105,  0.9424, -0.7305],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.7744e-01, -1.9283e-03,  4.0588e-03,  ..., -2.1530e-02,
          4.4022e-03, -2.9022e-02],
        [ 1.7212e-02,  3.8135e-01,  2.7514e-04,  ...,  7.0190e-04,
         -1.1444e-03,  1.9474e-03],
        [ 1.1856e-02, -2.3926e-02,  4.1504e-01,  ...,  2.6398e-02,
          1.6129e-02, -2.8503e-02],
        ...,
        [-9.1553e-04,  1.2619e-02,  1.0529e-02,  ...,  4.3066e-01,
          2.3407e-02, -1.5884e-02],
        [ 2.4948e-02,  1.6220e-02,  2.9633e-02,  ...,  5.2910e-03,
          3.9478e-01,  1.1894e-02],
        [ 3.7231e-03,  1.3580e-03, -3.6621e-02,  ..., -3.2898e-02,
         -1.7059e-02,  3.8892e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.2812,  0.3511,  3.1855,  ...,  2.0840, -1.4521, -0.6621]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 23:47:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for lose lost
spend spent
perform performed
involve involved
accept accepted
add added
marry married
believe
2024-07-15 23:47:26 root INFO     [order_1_approx] starting weight calculation for believe believed
add added
accept accepted
spend spent
perform performed
lose lost
involve involved
marry
2024-07-15 23:47:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 23:51:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3765, -0.5054,  1.1777,  ..., -0.4849,  0.3569, -0.2026],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.4102, -4.2617, -2.7539,  ..., -3.3242,  2.3848,  2.6484],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.2090e-01, -1.3847e-02,  6.4583e-03,  ...,  1.4336e-02,
          1.2833e-02, -2.4612e-02],
        [-8.1177e-03,  4.0088e-01,  1.3626e-02,  ...,  4.0016e-03,
         -6.6605e-03,  7.7896e-03],
        [ 1.3748e-02, -1.8280e-02,  4.1284e-01,  ...,  1.2398e-02,
          1.0223e-02, -3.2806e-02],
        ...,
        [-1.9745e-02, -1.3641e-02,  2.5558e-04,  ...,  4.5215e-01,
          8.9722e-03, -9.0103e-03],
        [ 2.7969e-02,  7.2365e-03,  2.3376e-02,  ...,  1.2344e-02,
          4.1431e-01,  3.5248e-02],
        [ 3.9902e-03,  3.2532e-02, -5.0751e-02,  ..., -1.9089e-02,
          7.5684e-03,  3.8843e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.7656, -1.4961, -2.0508,  ..., -1.6494,  0.7559,  2.1348]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 23:51:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for believe believed
add added
accept accepted
spend spent
perform performed
lose lost
involve involved
marry
2024-07-15 23:51:33 root INFO     [order_1_approx] starting weight calculation for marry married
involve involved
lose lost
believe believed
accept accepted
perform performed
add added
spend
2024-07-15 23:51:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 23:55:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0422,  0.4377, -0.2074,  ..., -0.7026,  1.4062, -0.6064],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0957,  3.1211,  1.1133,  ..., -0.5913,  1.8857,  3.6855],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.9551e-01, -4.6539e-03,  3.1647e-02,  ...,  7.4387e-03,
         -2.0248e-02,  2.8534e-02],
        [ 6.1722e-03,  3.9990e-01, -3.6926e-03,  ..., -9.7198e-03,
         -1.0239e-02,  1.3779e-02],
        [ 2.1973e-02, -4.6616e-03,  4.4629e-01,  ...,  6.4888e-03,
         -1.4801e-02,  4.3396e-02],
        ...,
        [-2.8641e-02, -1.1948e-02, -6.2714e-03,  ...,  4.1650e-01,
          1.6541e-02, -1.1703e-02],
        [ 8.2855e-03,  4.5776e-05,  2.4139e-02,  ..., -6.4621e-03,
          4.0356e-01,  1.7197e-02],
        [ 8.0185e-03,  1.4923e-02,  6.3858e-03,  ..., -5.1941e-02,
         -1.3794e-02,  4.1968e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.1016,  2.9941,  0.7290,  ...,  0.1924, -0.8682,  3.1816]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 23:55:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for marry married
involve involved
lose lost
believe believed
accept accepted
perform performed
add added
spend
2024-07-15 23:55:40 root INFO     [order_1_approx] starting weight calculation for perform performed
spend spent
accept accepted
believe believed
lose lost
marry married
involve involved
add
2024-07-15 23:55:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-15 23:59:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0420, -0.3408,  0.2275,  ..., -0.0638, -0.0854, -0.2502],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0342,  3.2695,  1.5381,  ...,  0.3047, -1.9824, -0.5742],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4458,  0.0024,  0.0033,  ...,  0.0094,  0.0242,  0.0076],
        [ 0.0166,  0.3992,  0.0063,  ...,  0.0254,  0.0114, -0.0010],
        [ 0.0064, -0.0238,  0.4272,  ..., -0.0028,  0.0057,  0.0042],
        ...,
        [-0.0140, -0.0127, -0.0153,  ...,  0.4380,  0.0161,  0.0064],
        [-0.0107,  0.0150,  0.0077,  ..., -0.0029,  0.3804,  0.0366],
        [ 0.0429,  0.0022, -0.0233,  ...,  0.0257, -0.0207,  0.4114]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3398,  3.4961,  2.8203,  ...,  0.2021, -1.3789,  1.2061]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-15 23:59:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for perform performed
spend spent
accept accepted
believe believed
lose lost
marry married
involve involved
add
2024-07-15 23:59:47 root INFO     [order_1_approx] starting weight calculation for spend spent
add added
perform performed
involve involved
marry married
believe believed
lose lost
accept
2024-07-15 23:59:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 00:03:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2227, -0.3738,  0.7002,  ..., -0.1582,  0.5972,  0.8979],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.7207, -0.8032,  1.0215,  ...,  0.7949, -0.4956,  1.3770],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4805,  0.0044, -0.0018,  ..., -0.0009,  0.0468,  0.0065],
        [ 0.0133,  0.4189, -0.0123,  ...,  0.0194, -0.0193,  0.0144],
        [ 0.0066, -0.0327,  0.4319,  ...,  0.0186,  0.0079,  0.0086],
        ...,
        [-0.0151, -0.0181, -0.0040,  ...,  0.4614,  0.0333, -0.0557],
        [-0.0189, -0.0189,  0.0011,  ..., -0.0211,  0.4282,  0.0024],
        [-0.0279, -0.0315, -0.0240,  ..., -0.0230, -0.0039,  0.4150]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.8633, -1.1426,  1.5020,  ..., -0.0527, -1.8828,  1.0996]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 00:03:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for spend spent
add added
perform performed
involve involved
marry married
believe believed
lose lost
accept
2024-07-16 00:03:53 root INFO     total operator prediction time: 1972.9852855205536 seconds
2024-07-16 00:03:53 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_inf - 3pSg
2024-07-16 00:03:53 root INFO     building operator verb_inf - 3pSg
2024-07-16 00:03:53 root INFO     [order_1_approx] starting weight calculation for explain explains
provide provides
include includes
maintain maintains
happen happens
allow allows
involve involves
describe
2024-07-16 00:03:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 00:07:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7646, -0.1768,  1.6543,  ..., -0.0676, -0.5747, -0.0287],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-9.9707e-01,  9.7656e-04,  1.9189e+00,  ...,  1.0254e+00,
         2.3193e-01,  2.2051e+00], device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.6729e-01,  1.0445e-02,  1.3832e-02,  ...,  2.3407e-02,
          1.0147e-02,  2.6093e-02],
        [ 2.5497e-02,  3.9453e-01, -3.9276e-02,  ...,  1.6052e-02,
          4.8706e-02, -7.8735e-03],
        [ 2.8656e-02,  2.3193e-03,  4.5093e-01,  ...,  3.0182e-02,
          1.8311e-04, -5.3101e-03],
        ...,
        [-1.7242e-02,  1.1566e-02,  1.2772e-02,  ...,  4.6436e-01,
          2.6855e-03, -4.5410e-02],
        [ 4.2343e-03, -2.8290e-02,  2.2766e-02,  ..., -1.6785e-02,
          4.0186e-01,  1.9211e-02],
        [ 2.9907e-03,  5.9143e-02, -6.9214e-02,  ..., -3.6926e-02,
          5.0446e-02,  4.5117e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.5664,  0.0255,  2.0273,  ...,  2.4336, -0.4917,  2.6465]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 00:08:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for explain explains
provide provides
include includes
maintain maintains
happen happens
allow allows
involve involves
describe
2024-07-16 00:08:00 root INFO     [order_1_approx] starting weight calculation for happen happens
maintain maintains
explain explains
include includes
allow allows
provide provides
describe describes
involve
2024-07-16 00:08:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 00:12:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7090, -0.8887,  1.0537,  ...,  0.1370,  0.8442,  2.2031],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9551, -3.7910,  1.3984,  ...,  1.5469,  2.0176,  2.3789],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.5078,  0.0056,  0.0017,  ...,  0.0252,  0.0047,  0.0444],
        [ 0.0168,  0.4624,  0.0022,  ...,  0.0093,  0.0191, -0.0203],
        [ 0.0225, -0.0098,  0.4951,  ..., -0.0083,  0.0167, -0.0083],
        ...,
        [-0.0108, -0.0220,  0.0175,  ...,  0.5435,  0.0014, -0.0176],
        [ 0.0110,  0.0179, -0.0077,  ...,  0.0184,  0.4912,  0.0557],
        [-0.0021,  0.0150, -0.0222,  ..., -0.0072, -0.0042,  0.5322]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.2773, -2.7148,  2.2051,  ...,  2.1484,  0.6035,  1.6543]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 00:12:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for happen happens
maintain maintains
explain explains
include includes
allow allows
provide provides
describe describes
involve
2024-07-16 00:12:08 root INFO     [order_1_approx] starting weight calculation for explain explains
describe describes
provide provides
maintain maintains
allow allows
happen happens
involve involves
include
2024-07-16 00:12:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 00:16:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3459, -0.1624,  0.6860,  ...,  0.2224,  1.2725,  1.9473],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4453, -3.0840, -4.0469,  ...,  4.8516,  1.1133,  2.4805],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4561,  0.0154,  0.0240,  ..., -0.0039,  0.0064,  0.0764],
        [ 0.0105,  0.3733,  0.0009,  ...,  0.0221,  0.0049,  0.0187],
        [ 0.0156,  0.0528,  0.4158,  ..., -0.0129,  0.0047, -0.0445],
        ...,
        [-0.0302,  0.0207, -0.0188,  ...,  0.4536, -0.0136,  0.0056],
        [ 0.0193,  0.0007, -0.0302,  ..., -0.0054,  0.3762,  0.0240],
        [ 0.0117,  0.0566, -0.0492,  ..., -0.0290,  0.0357,  0.4238]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6123, -2.8945, -0.4648,  ...,  2.3848,  0.5435,  4.2773]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 00:16:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for explain explains
describe describes
provide provides
maintain maintains
allow allows
happen happens
involve involves
include
2024-07-16 00:16:15 root INFO     [order_1_approx] starting weight calculation for include includes
allow allows
maintain maintains
provide provides
explain explains
involve involves
describe describes
happen
2024-07-16 00:16:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 00:20:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8940,  0.2637,  0.4248,  ...,  0.0518,  0.7012, -0.2625],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4106, -1.3389, -2.3867,  ..., -1.8262,  3.7422,  5.8555],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.5010, -0.0236,  0.0054,  ..., -0.0059,  0.0413, -0.0144],
        [-0.0041,  0.4553,  0.0098,  ...,  0.0168,  0.0481,  0.0128],
        [ 0.0795,  0.0193,  0.4487,  ...,  0.0390, -0.0117,  0.0064],
        ...,
        [ 0.0307, -0.0742, -0.0025,  ...,  0.4722,  0.0380, -0.0183],
        [-0.0184, -0.0037, -0.0331,  ...,  0.0080,  0.4768,  0.0138],
        [-0.0392,  0.0103, -0.0274,  ..., -0.0524, -0.0149,  0.4663]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2876, -0.1416, -2.1191,  ..., -4.2344,  0.9922,  5.7812]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 00:20:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for include includes
allow allows
maintain maintains
provide provides
explain explains
involve involves
describe describes
happen
2024-07-16 00:20:23 root INFO     [order_1_approx] starting weight calculation for describe describes
happen happens
explain explains
maintain maintains
allow allows
involve involves
include includes
provide
2024-07-16 00:20:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 00:24:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0581, -0.4126, -0.3423,  ..., -0.7778,  0.5103,  0.4854],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5117, -4.4688,  1.8037,  ...,  0.2461,  1.6406,  2.9023],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4766, -0.0190,  0.0028,  ...,  0.0192, -0.0147,  0.0076],
        [-0.0064,  0.3999,  0.0211,  ..., -0.0031, -0.0188,  0.0135],
        [ 0.0213,  0.0039,  0.4497,  ..., -0.0072,  0.0018,  0.0150],
        ...,
        [-0.0484, -0.0256, -0.0190,  ...,  0.4634,  0.0373, -0.0080],
        [ 0.0158,  0.0221, -0.0058,  ...,  0.0127,  0.4109,  0.0147],
        [-0.0157,  0.0197, -0.0372,  ..., -0.0418,  0.0388,  0.3870]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2314, -1.6641,  3.0352,  ...,  2.2305,  1.4482,  4.0312]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 00:24:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for describe describes
happen happens
explain explains
maintain maintains
allow allows
involve involves
include includes
provide
2024-07-16 00:24:31 root INFO     [order_1_approx] starting weight calculation for provide provides
describe describes
explain explains
maintain maintains
involve involves
include includes
happen happens
allow
2024-07-16 00:24:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 00:28:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0010, -0.5718, -0.3850,  ..., -1.0078,  0.1830, -0.1123],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7075, -1.6943,  2.8770,  ..., -0.6230,  2.2637, -0.6943],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.8560e-01,  3.1708e-02, -2.4521e-02,  ..., -4.6417e-02,
          5.0888e-03, -1.0773e-02],
        [ 6.1035e-05,  4.2139e-01, -1.1528e-02,  ..., -8.8730e-03,
          8.2397e-04,  8.8501e-03],
        [ 2.6031e-02, -6.7139e-04,  4.3872e-01,  ..., -7.2021e-03,
          2.1698e-02,  1.1826e-02],
        ...,
        [-7.0648e-03, -3.2990e-02, -3.0945e-02,  ...,  4.7510e-01,
          1.2413e-02, -3.8666e-02],
        [-6.8932e-03,  4.9744e-02,  4.0314e-02,  ...,  1.5312e-02,
          4.5508e-01, -1.6968e-02],
        [ 1.3008e-02,  3.2715e-02,  1.1673e-02,  ...,  1.6327e-03,
          1.0788e-02,  4.5825e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.1719, -1.2021,  4.6914,  ..., -0.2068,  0.4902, -0.4143]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 00:28:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for provide provides
describe describes
explain explains
maintain maintains
involve involves
include includes
happen happens
allow
2024-07-16 00:28:38 root INFO     [order_1_approx] starting weight calculation for explain explains
allow allows
include includes
provide provides
describe describes
involve involves
happen happens
maintain
2024-07-16 00:28:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 00:32:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2881, -0.4243,  0.3413,  ..., -0.1368,  0.4556,  0.6143],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7402, -2.4395, -0.9785,  ..., -1.3799,  0.1157,  2.8887],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.5034,  0.0076,  0.0093,  ...,  0.0107,  0.0142,  0.0175],
        [ 0.0076,  0.4277, -0.0181,  ..., -0.0064,  0.0371, -0.0139],
        [ 0.0215,  0.0045,  0.4575,  ..., -0.0199,  0.0124, -0.0245],
        ...,
        [ 0.0034, -0.0114,  0.0142,  ...,  0.5078,  0.0020, -0.0309],
        [-0.0122,  0.0240, -0.0068,  ..., -0.0269,  0.4219,  0.0218],
        [ 0.0213,  0.0027, -0.0070,  ..., -0.0432,  0.0545,  0.4453]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.4551, -0.3945, -0.3071,  ..., -1.7764, -1.4590,  1.5723]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 00:32:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for explain explains
allow allows
include includes
provide provides
describe describes
involve involves
happen happens
maintain
2024-07-16 00:32:46 root INFO     [order_1_approx] starting weight calculation for allow allows
maintain maintains
provide provides
involve involves
include includes
happen happens
describe describes
explain
2024-07-16 00:32:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 00:36:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.4121, -1.0664,  0.9971,  ..., -0.2920,  0.1064, -1.0537],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.8594, -2.3438,  2.4453,  ..., -0.3643, -0.1259,  0.9492],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.2871e-01, -8.3923e-05, -1.3557e-02,  ...,  3.7109e-02,
         -1.5175e-02,  1.5121e-02],
        [-1.1520e-03,  3.9941e-01,  1.1063e-04,  ...,  3.7354e-02,
          4.8523e-03,  5.5603e-02],
        [ 1.4511e-02, -4.9973e-03,  4.2798e-01,  ..., -1.3382e-02,
         -3.2379e-02, -3.2135e-02],
        ...,
        [-2.5482e-03, -3.8879e-02, -6.6681e-03,  ...,  4.5215e-01,
          2.3163e-02, -1.4816e-02],
        [ 6.8321e-03,  3.4576e-02,  1.4168e-02,  ..., -2.9465e-02,
          4.0210e-01,  5.6992e-03],
        [ 9.7351e-03,  1.8204e-02, -7.7057e-03,  ..., -2.6825e-02,
         -6.9351e-03,  4.2456e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.0801, -2.4102,  3.5195,  ..., -0.6685, -0.9048,  3.2324]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 00:36:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for allow allows
maintain maintains
provide provides
involve involves
include includes
happen happens
describe describes
explain
2024-07-16 00:36:53 root INFO     total operator prediction time: 1980.6614253520966 seconds
2024-07-16 00:36:53 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_Ving - 3pSg
2024-07-16 00:36:53 root INFO     building operator verb_Ving - 3pSg
2024-07-16 00:36:54 root INFO     [order_1_approx] starting weight calculation for creating creates
promoting promotes
managing manages
appearing appears
thanking thanks
happening happens
sitting sits
spending
2024-07-16 00:36:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 00:41:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2406,  0.5742,  0.1562,  ..., -0.4082,  1.5527, -1.0234],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.0527,  1.3467,  2.6641,  ...,  0.6035, -1.2139,  1.7637],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.0723e-01, -2.4841e-02,  4.5898e-02,  ..., -5.1422e-03,
          1.4488e-02,  7.0343e-03],
        [-2.3163e-02,  3.8599e-01,  9.9945e-03,  ...,  1.4397e-02,
          2.7451e-02, -6.0272e-04],
        [ 2.6764e-02, -2.0798e-02,  3.9307e-01,  ...,  1.2755e-04,
          3.1403e-02,  1.2131e-02],
        ...,
        [-2.1149e-02,  2.1057e-02,  5.0354e-02,  ...,  3.9819e-01,
         -1.2505e-02, -3.7262e-02],
        [ 5.7068e-03, -8.0566e-03,  1.1253e-03,  ...,  8.0414e-03,
          4.3042e-01,  2.8244e-02],
        [ 1.3962e-03,  3.5461e-02, -3.3264e-02,  ..., -4.7516e-02,
          3.5339e-02,  3.8721e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.2422,  1.2793,  3.7031,  ...,  0.6152, -4.1836,  4.2891]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 00:41:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for creating creates
promoting promotes
managing manages
appearing appears
thanking thanks
happening happens
sitting sits
spending
2024-07-16 00:41:01 root INFO     [order_1_approx] starting weight calculation for promoting promotes
happening happens
spending spends
thanking thanks
appearing appears
managing manages
creating creates
sitting
2024-07-16 00:41:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 00:45:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2091, -1.7422,  0.7593,  ...,  0.7148,  0.6592,  0.7344],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.2695, -4.1836,  4.3750,  ...,  2.0625, -4.6797,  1.3154],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4604,  0.0037,  0.0210,  ..., -0.0190,  0.0051, -0.0120],
        [ 0.0196,  0.3733, -0.0219,  ...,  0.0137,  0.0170,  0.0220],
        [ 0.0015,  0.0141,  0.4600,  ...,  0.0174,  0.0043, -0.0253],
        ...,
        [-0.0532,  0.0302,  0.0443,  ...,  0.4519, -0.0243, -0.0168],
        [ 0.0484, -0.0034, -0.0564,  ..., -0.0410,  0.4736, -0.0087],
        [ 0.0058,  0.0115, -0.0445,  ..., -0.0137,  0.0067,  0.3948]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.5195,  0.2031,  3.3457,  ..., -0.7539, -3.6289,  2.3457]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 00:45:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for promoting promotes
happening happens
spending spends
thanking thanks
appearing appears
managing manages
creating creates
sitting
2024-07-16 00:45:08 root INFO     [order_1_approx] starting weight calculation for managing manages
happening happens
sitting sits
promoting promotes
appearing appears
thanking thanks
spending spends
creating
2024-07-16 00:45:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 00:49:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7290, -0.3223, -0.0365,  ...,  0.4487,  0.4011, -0.6548],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.2617, -4.0586,  1.5586,  ...,  2.4531, -0.4990, -0.7715],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4321,  0.0298,  0.0075,  ...,  0.0074,  0.0169, -0.0309],
        [ 0.0010,  0.3701, -0.0181,  ..., -0.0085,  0.0310,  0.0113],
        [ 0.0053,  0.0206,  0.4341,  ...,  0.0311, -0.0146, -0.0168],
        ...,
        [-0.0136,  0.0213, -0.0052,  ...,  0.4028,  0.0085, -0.0059],
        [ 0.0108, -0.0125, -0.0391,  ..., -0.0201,  0.4121, -0.0010],
        [ 0.0301,  0.0052,  0.0093,  ..., -0.0222, -0.0312,  0.3965]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.3828, -2.2480,  3.0625,  ...,  4.9609, -1.3359,  0.0396]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 00:49:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for managing manages
happening happens
sitting sits
promoting promotes
appearing appears
thanking thanks
spending spends
creating
2024-07-16 00:49:15 root INFO     [order_1_approx] starting weight calculation for spending spends
managing manages
happening happens
creating creates
appearing appears
sitting sits
promoting promotes
thanking
2024-07-16 00:49:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 00:53:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.4209, 0.1924, 1.3877,  ..., 0.3579, 1.3164, 1.1055], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4365,  1.7773,  1.3008,  ..., -1.8789,  1.6992,  7.2969],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4622, -0.0297, -0.0131,  ...,  0.0496, -0.0258, -0.0017],
        [ 0.0273,  0.4133, -0.0044,  ...,  0.0303,  0.0526,  0.0012],
        [ 0.0323,  0.0289,  0.4385,  ...,  0.0172,  0.0208, -0.0065],
        ...,
        [-0.0307, -0.0005,  0.0570,  ...,  0.4524, -0.0148,  0.0136],
        [-0.0226, -0.0108,  0.0044,  ..., -0.0287,  0.3938,  0.0136],
        [-0.0064,  0.0521, -0.0623,  ..., -0.0651,  0.0324,  0.4026]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2090,  1.3398,  2.2500,  ...,  0.0254, -1.7188,  8.7734]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 00:53:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for spending spends
managing manages
happening happens
creating creates
appearing appears
sitting sits
promoting promotes
thanking
2024-07-16 00:53:23 root INFO     [order_1_approx] starting weight calculation for managing manages
happening happens
spending spends
thanking thanks
appearing appears
creating creates
sitting sits
promoting
2024-07-16 00:53:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 00:57:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2488,  0.0789,  0.5537,  ...,  0.3367,  0.6055,  0.4819],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.0195, -2.2617,  6.3164,  ...,  7.2266, -1.9492,  0.3096],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4402, -0.0183,  0.0385,  ...,  0.0130,  0.0099, -0.0134],
        [ 0.0078,  0.3711, -0.0104,  ..., -0.0298, -0.0080,  0.0297],
        [ 0.0236, -0.0034,  0.4080,  ..., -0.0157,  0.0131, -0.0147],
        ...,
        [-0.0246, -0.0255, -0.0014,  ...,  0.4116, -0.0251,  0.0251],
        [ 0.0237,  0.0236, -0.0220,  ..., -0.0320,  0.3691, -0.0102],
        [ 0.0208,  0.0262, -0.0307,  ..., -0.0590,  0.0011,  0.3948]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.0312, -0.8271,  7.6484,  ...,  4.5898, -3.2656,  2.4453]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 00:57:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for managing manages
happening happens
spending spends
thanking thanks
appearing appears
creating creates
sitting sits
promoting
2024-07-16 00:57:29 root INFO     [order_1_approx] starting weight calculation for sitting sits
happening happens
thanking thanks
creating creates
promoting promotes
spending spends
appearing appears
managing
2024-07-16 00:57:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 01:01:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4927, -0.1482,  0.0518,  ..., -0.2095,  1.1035, -0.5479],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9199,  0.9062, -2.3906,  ...,  3.4434,  2.5586,  3.2344],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.3433e-01,  3.8548e-03,  7.4120e-03,  ..., -3.8147e-03,
          1.2054e-03,  1.4091e-02],
        [ 1.3618e-02,  3.8110e-01,  8.5449e-04,  ...,  3.3203e-02,
          1.2573e-02,  1.9806e-02],
        [ 2.1835e-02,  4.0344e-02,  4.3628e-01,  ..., -6.1340e-03,
         -7.4081e-03, -4.9866e-02],
        ...,
        [-9.7809e-03,  7.8430e-03,  4.3518e-02,  ...,  4.9365e-01,
         -4.7379e-03,  8.4610e-03],
        [-8.0948e-03,  2.5574e-02,  3.9673e-04,  ...,  3.2532e-02,
          4.3774e-01,  2.1240e-02],
        [ 3.5492e-02, -3.9337e-02, -3.9917e-02,  ..., -4.2480e-02,
          4.8294e-03,  4.1309e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.3965,  1.0283, -1.1279,  ...,  1.3984,  1.4824,  5.6250]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 01:01:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for sitting sits
happening happens
thanking thanks
creating creates
promoting promotes
spending spends
appearing appears
managing
2024-07-16 01:01:36 root INFO     [order_1_approx] starting weight calculation for thanking thanks
sitting sits
creating creates
managing manages
promoting promotes
spending spends
happening happens
appearing
2024-07-16 01:01:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 01:05:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0803,  0.3716,  1.0322,  ...,  0.4956,  0.4404, -0.7480],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.9180,  0.4111,  0.2617,  ...,  2.1699, -3.3633,  1.6445],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4429,  0.0257,  0.0029,  ..., -0.0234,  0.0499, -0.0346],
        [-0.0469,  0.3672, -0.0016,  ...,  0.0431, -0.0226,  0.0007],
        [ 0.0215, -0.0159,  0.4561,  ...,  0.0565,  0.0081, -0.0178],
        ...,
        [-0.0812, -0.0095,  0.0390,  ...,  0.4758, -0.0377,  0.0438],
        [-0.0149,  0.0614,  0.0149,  ..., -0.0622,  0.4263,  0.0147],
        [ 0.0115, -0.0067, -0.0283,  ..., -0.0823, -0.0097,  0.4038]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6758,  1.6914, -0.4858,  ...,  0.5684, -5.3867,  2.0605]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 01:05:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for thanking thanks
sitting sits
creating creates
managing manages
promoting promotes
spending spends
happening happens
appearing
2024-07-16 01:05:44 root INFO     [order_1_approx] starting weight calculation for promoting promotes
spending spends
thanking thanks
appearing appears
managing manages
creating creates
sitting sits
happening
2024-07-16 01:05:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 01:09:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1639,  0.3674,  1.1143,  ..., -0.5415,  1.1729, -0.8813],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.0664,  0.0518, -0.5713,  ...,  2.4082,  3.8633,  4.5000],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.3926,  0.0344,  0.0271,  ..., -0.0060,  0.0074,  0.0213],
        [ 0.0334,  0.4077, -0.0118,  ...,  0.0180,  0.0504, -0.0084],
        [ 0.0507,  0.0160,  0.4514,  ...,  0.0310,  0.0225, -0.0197],
        ...,
        [-0.0806,  0.0204,  0.0300,  ...,  0.4590, -0.0040, -0.0176],
        [ 0.1169,  0.0253, -0.0590,  ...,  0.0017,  0.4333, -0.0175],
        [ 0.0201, -0.0410, -0.0334,  ..., -0.0374,  0.0276,  0.4172]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.6133,  0.6948, -0.9531,  ...,  0.5859,  4.8125,  5.5391]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 01:09:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for promoting promotes
spending spends
thanking thanks
appearing appears
managing manages
creating creates
sitting sits
happening
2024-07-16 01:09:51 root INFO     total operator prediction time: 1978.0257799625397 seconds
2024-07-16 01:09:51 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on noun - plural_reg
2024-07-16 01:09:51 root INFO     building operator noun - plural_reg
2024-07-16 01:09:52 root INFO     [order_1_approx] starting weight calculation for street streets
player players
song songs
government governments
product products
night nights
area areas
thing
2024-07-16 01:09:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 01:13:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4062,  0.7803,  0.4546,  ..., -0.3015,  0.1738, -0.7056],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2773,  0.9507,  2.2383,  ..., -0.4375,  3.0781,  4.0039],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4739, -0.0485, -0.0043,  ..., -0.0350,  0.0485, -0.0377],
        [-0.0150,  0.4502, -0.0337,  ...,  0.0193, -0.0079,  0.0218],
        [ 0.0119,  0.0285,  0.4917,  ..., -0.0156, -0.0384, -0.0099],
        ...,
        [ 0.0005, -0.0095,  0.0210,  ...,  0.4680,  0.0079,  0.0057],
        [ 0.0248,  0.0117,  0.0039,  ..., -0.0236,  0.4521,  0.0325],
        [-0.0054,  0.0266, -0.0046,  ..., -0.0114, -0.0145,  0.4976]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[1.2842, 2.0742, 3.3574,  ..., 0.6172, 3.3105, 2.9980]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 01:13:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for street streets
player players
song songs
government governments
product products
night nights
area areas
thing
2024-07-16 01:13:57 root INFO     [order_1_approx] starting weight calculation for night nights
player players
thing things
street streets
area areas
product products
song songs
government
2024-07-16 01:13:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 01:18:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0786, -0.0502, -0.6924,  ..., -0.2059, -0.7759,  0.2588],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5547, -0.1396,  2.1055,  ...,  2.7773,  0.8066, -2.6484],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4275, -0.0460,  0.0204,  ..., -0.0242, -0.0035, -0.0099],
        [-0.0075,  0.3726,  0.0165,  ..., -0.0071,  0.0223, -0.0323],
        [ 0.0093,  0.0254,  0.4741,  ..., -0.0180, -0.0234, -0.0105],
        ...,
        [ 0.0007,  0.0023,  0.0075,  ...,  0.4675,  0.0332,  0.0072],
        [-0.0184,  0.0177, -0.0283,  ..., -0.0040,  0.4160,  0.0107],
        [ 0.0269,  0.0468, -0.0306,  ..., -0.0281,  0.0483,  0.4385]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.3652, -0.5010,  2.3398,  ...,  3.2422,  0.8984, -2.0410]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 01:18:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for night nights
player players
thing things
street streets
area areas
product products
song songs
government
2024-07-16 01:18:05 root INFO     [order_1_approx] starting weight calculation for player players
night nights
government governments
thing things
song songs
street streets
product products
area
2024-07-16 01:18:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 01:22:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0215,  0.2275, -0.7266,  ..., -0.5850, -0.9072, -0.3013],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6807, -2.4141, -1.0889,  ..., -0.0479,  4.1367,  0.4863],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4937, -0.0152, -0.0406,  ..., -0.0479, -0.0159,  0.0032],
        [ 0.0070,  0.4436, -0.0315,  ...,  0.0146, -0.0178,  0.0397],
        [ 0.0381, -0.0308,  0.4985,  ..., -0.0142, -0.0256, -0.0277],
        ...,
        [ 0.0032, -0.0131, -0.0042,  ...,  0.5244,  0.0148,  0.0042],
        [-0.0089,  0.0237, -0.0304,  ..., -0.0212,  0.4363,  0.0243],
        [-0.0087, -0.0019, -0.0316,  ..., -0.0254,  0.0368,  0.4753]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6260, -2.2598,  1.7178,  ...,  0.7412,  4.2461, -0.4399]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 01:22:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for player players
night nights
government governments
thing things
song songs
street streets
product products
area
2024-07-16 01:22:12 root INFO     [order_1_approx] starting weight calculation for thing things
product products
player players
night nights
street streets
government governments
area areas
song
2024-07-16 01:22:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 01:26:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4949, -0.4944,  0.2930,  ...,  0.4158,  0.3022, -0.9902],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8789, -4.6680, -0.4492,  ...,  1.8643, -0.8184, -1.6875],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.7021e-01, -4.2648e-03, -3.0487e-02,  ..., -4.1595e-02,
          3.8177e-02, -3.2745e-02],
        [ 1.2459e-02,  4.2798e-01, -1.2756e-02,  ...,  7.0572e-05,
          2.7847e-03, -1.2299e-02],
        [ 6.8140e-04,  2.0294e-02,  4.7852e-01,  ...,  9.0256e-03,
         -1.9073e-02, -1.7090e-03],
        ...,
        [-1.0155e-02,  2.0721e-02,  1.1108e-02,  ...,  5.0732e-01,
         -1.3046e-03, -2.3193e-02],
        [ 2.6550e-02,  1.2100e-02, -4.9164e-02,  ..., -2.5024e-02,
          4.1724e-01, -2.0233e-02],
        [ 1.1108e-02,  1.7563e-02,  4.5128e-03,  ...,  1.6327e-02,
         -8.3923e-03,  4.6265e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.3906, -1.7891,  0.4858,  ...,  3.5215, -1.3291, -4.1328]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 01:26:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for thing things
product products
player players
night nights
street streets
government governments
area areas
song
2024-07-16 01:26:20 root INFO     [order_1_approx] starting weight calculation for player players
night nights
thing things
product products
song songs
government governments
area areas
street
2024-07-16 01:26:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 01:30:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6641, -0.6675, -0.8164,  ...,  1.9600, -0.6704, -2.5938],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4211, -2.2734, -0.3701,  ...,  0.4648,  1.6562, -0.4727],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.7681e-01, -3.0518e-05, -1.6129e-02,  ..., -1.2527e-02,
          1.5549e-02, -2.4399e-02],
        [ 1.0246e-02,  4.1162e-01, -1.0193e-02,  ..., -4.5776e-05,
          2.2980e-02,  1.9852e-02],
        [-1.2802e-02, -1.6785e-02,  4.6924e-01,  ..., -1.8234e-03,
         -7.0915e-03, -5.9586e-03],
        ...,
        [-1.1429e-02,  1.6918e-03,  3.9406e-03,  ...,  4.9365e-01,
          3.2104e-02,  1.5541e-02],
        [-1.0414e-03,  4.5319e-03,  9.0027e-03,  ...,  8.7509e-03,
          4.5605e-01,  1.5396e-02],
        [ 4.5593e-02,  7.5378e-03, -1.6647e-02,  ...,  2.5208e-02,
         -3.7842e-03,  4.7705e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3159, -1.6660,  1.7822,  ..., -0.9971,  1.6211, -0.5410]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 01:30:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for player players
night nights
thing things
product products
song songs
government governments
area areas
street
2024-07-16 01:30:28 root INFO     [order_1_approx] starting weight calculation for song songs
government governments
thing things
area areas
street streets
night nights
product products
player
2024-07-16 01:30:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 01:34:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8975, -0.4792,  0.0305,  ...,  0.6602,  0.1685, -0.3157],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.9922,  0.2202,  1.5127,  ...,  0.8223, -0.7876,  2.2754],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4009,  0.0020, -0.0025,  ...,  0.0050, -0.0024, -0.0416],
        [ 0.0022,  0.3882,  0.0180,  ...,  0.0101,  0.0051, -0.0095],
        [ 0.0148, -0.0335,  0.4219,  ...,  0.0068, -0.0207,  0.0175],
        ...,
        [-0.0162, -0.0222,  0.0212,  ...,  0.4473,  0.0066, -0.0211],
        [-0.0107,  0.0155, -0.0230,  ...,  0.0170,  0.4219,  0.0249],
        [ 0.0180, -0.0151,  0.0062,  ..., -0.0134,  0.0235,  0.4329]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3340,  0.0964,  2.1152,  ...,  0.0068, -0.9268,  1.2002]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 01:34:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for song songs
government governments
thing things
area areas
street streets
night nights
product products
player
2024-07-16 01:34:35 root INFO     [order_1_approx] starting weight calculation for song songs
government governments
player players
thing things
street streets
night nights
area areas
product
2024-07-16 01:34:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 01:38:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0142,  0.4082,  1.0850,  ...,  0.4014,  0.2185, -0.3911],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0967, -2.4883,  3.8633,  ...,  0.1191, -2.8906, -0.0166],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.5508e-01, -1.3039e-02,  1.9653e-02,  ..., -1.0727e-02,
          2.0203e-02, -5.2719e-03],
        [ 4.2572e-03,  4.2041e-01, -2.5513e-02,  ...,  2.5604e-02,
          5.4626e-03, -1.5625e-02],
        [-5.3024e-03,  2.0126e-02,  4.7778e-01,  ..., -3.5431e-02,
         -1.9516e-02, -1.1581e-02],
        ...,
        [ 6.1798e-03, -4.7188e-03,  5.4131e-03,  ...,  4.7803e-01,
          3.1319e-03,  1.4648e-03],
        [ 2.6566e-02,  5.3467e-02,  1.0956e-02,  ..., -2.4887e-02,
          4.2334e-01,  3.2043e-04],
        [ 1.8051e-02,  1.8921e-02,  7.9803e-03,  ..., -1.7670e-02,
          2.2659e-02,  4.6509e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9360, -2.2109,  4.0039,  ..., -2.1816, -2.2695, -0.2313]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 01:38:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for song songs
government governments
player players
thing things
street streets
night nights
area areas
product
2024-07-16 01:38:42 root INFO     [order_1_approx] starting weight calculation for government governments
street streets
area areas
player players
song songs
product products
thing things
night
2024-07-16 01:38:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 01:42:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2462, -0.1733, -0.2057,  ..., -0.6006,  0.4463, -0.8325],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3789, -4.1094,  2.0410,  ..., -0.7490,  2.2793,  1.3447],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4880, -0.0137,  0.0147,  ..., -0.0132, -0.0200, -0.0308],
        [-0.0446,  0.4312,  0.0019,  ...,  0.0040,  0.0056,  0.0020],
        [ 0.0111,  0.0045,  0.4717,  ..., -0.0076, -0.0069, -0.0242],
        ...,
        [-0.0081, -0.0170,  0.0073,  ...,  0.4778, -0.0067, -0.0071],
        [ 0.0103,  0.0154,  0.0225,  ..., -0.0017,  0.4355, -0.0172],
        [ 0.0243,  0.0278, -0.0134,  ..., -0.0157,  0.0007,  0.4900]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.2129, -2.5801,  4.0625,  ...,  1.5830,  1.9443,  0.7583]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 01:42:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for government governments
street streets
area areas
player players
song songs
product products
thing things
night
2024-07-16 01:42:49 root INFO     total operator prediction time: 1977.7667956352234 seconds
2024-07-16 01:42:49 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_3pSg - Ved
2024-07-16 01:42:49 root INFO     building operator verb_3pSg - Ved
2024-07-16 01:42:49 root INFO     [order_1_approx] starting weight calculation for appears appeared
tells told
refers referred
replaces replaced
involves involved
remains remained
occurs occurred
develops
2024-07-16 01:42:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 01:46:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6660,  0.5522,  0.8560,  ..., -0.1704,  0.5684,  1.0957],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.8105, -3.1289,  0.7148,  ..., -0.7554,  5.1641,  0.0059],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.5068e-01,  3.6102e-02, -3.1281e-02,  ..., -3.8872e-03,
          1.8768e-02,  3.5095e-04],
        [-9.3307e-03,  3.9111e-01, -3.3417e-02,  ...,  1.6830e-02,
          1.2375e-02,  5.7182e-03],
        [ 5.1758e-02, -2.4872e-02,  4.1187e-01,  ...,  2.8549e-02,
          1.2291e-02,  1.1017e-02],
        ...,
        [-6.7062e-03,  2.8748e-02, -1.8692e-02,  ...,  4.6680e-01,
          1.9398e-03, -1.8311e-02],
        [ 5.2834e-03,  2.9205e-02,  2.2598e-02,  ..., -1.3298e-02,
          4.3506e-01, -2.2278e-02],
        [ 1.6647e-02, -2.5955e-02,  6.5842e-03,  ..., -1.4191e-02,
         -3.5950e-02,  3.9624e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.9199, -3.7129,  1.3984,  ..., -1.7695,  4.9883, -0.5894]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 01:46:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for appears appeared
tells told
refers referred
replaces replaced
involves involved
remains remained
occurs occurred
develops
2024-07-16 01:46:56 root INFO     [order_1_approx] starting weight calculation for refers referred
occurs occurred
remains remained
develops developed
appears appeared
involves involved
replaces replaced
tells
2024-07-16 01:46:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 01:51:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.7412, 0.7114, 1.4697,  ..., 0.3037, 0.1666, 0.9258], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([0.2417, 1.2812, 4.7969,  ..., 3.7305, 1.3047, 2.2227], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4841,  0.0316, -0.0005,  ...,  0.0087,  0.0128, -0.0327],
        [-0.0190,  0.4431,  0.0146,  ...,  0.0182,  0.0027, -0.0318],
        [-0.0074,  0.0059,  0.4155,  ...,  0.0155, -0.0485, -0.0213],
        ...,
        [ 0.0190,  0.0094,  0.0021,  ...,  0.4636,  0.0521, -0.0394],
        [-0.0166,  0.0094, -0.0244,  ..., -0.0267,  0.4268,  0.0198],
        [ 0.0089, -0.0488,  0.0391,  ..., -0.0235, -0.0115,  0.3843]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[0.4180, 2.0312, 2.2207,  ..., 2.6973, 1.2344, 0.5898]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 01:51:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for refers referred
occurs occurred
remains remained
develops developed
appears appeared
involves involved
replaces replaced
tells
2024-07-16 01:51:03 root INFO     [order_1_approx] starting weight calculation for refers referred
remains remained
replaces replaced
occurs occurred
tells told
develops developed
involves involved
appears
2024-07-16 01:51:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 01:55:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2418,  0.6021,  1.6133,  ...,  1.1504,  0.4434,  0.3311],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9268, -0.0293,  1.6113,  ...,  3.2500, -0.1406, -1.6016],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4580, -0.0013,  0.0041,  ..., -0.0243,  0.0256,  0.0152],
        [-0.0049,  0.4082, -0.0208,  ..., -0.0043,  0.0026, -0.0089],
        [ 0.0361,  0.0044,  0.4478,  ...,  0.0058, -0.0194, -0.0091],
        ...,
        [-0.0289,  0.0082,  0.0142,  ...,  0.4788,  0.0123, -0.0210],
        [-0.0094,  0.0148, -0.0102,  ...,  0.0033,  0.4324,  0.0354],
        [ 0.0126, -0.0257, -0.0295,  ..., -0.0079, -0.0161,  0.4443]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.6484,  0.0504,  0.0059,  ...,  1.6289, -0.7915, -2.0332]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 01:55:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for refers referred
remains remained
replaces replaced
occurs occurred
tells told
develops developed
involves involved
appears
2024-07-16 01:55:11 root INFO     [order_1_approx] starting weight calculation for tells told
replaces replaced
involves involved
appears appeared
remains remained
refers referred
develops developed
occurs
2024-07-16 01:55:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 01:59:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1216, -0.5537,  1.3145,  ..., -0.0781,  0.6548,  0.6772],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.1914, -4.2617, -6.3086,  ..., -0.1545,  4.7227,  0.5078],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4529, -0.0359,  0.0106,  ...,  0.0218, -0.0017,  0.0012],
        [-0.0009,  0.3965,  0.0018,  ...,  0.0085, -0.0027,  0.0229],
        [ 0.1115,  0.1083,  0.3867,  ..., -0.0169,  0.0384, -0.0523],
        ...,
        [ 0.0409,  0.0041, -0.0111,  ...,  0.4836,  0.0144, -0.0339],
        [ 0.0214,  0.0238,  0.0092,  ...,  0.0643,  0.4106,  0.0433],
        [ 0.0485, -0.0334, -0.0465,  ..., -0.0684,  0.0534,  0.3877]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.3359, -6.2969,  2.9180,  ..., -3.3008,  4.1758,  2.8125]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 01:59:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for tells told
replaces replaced
involves involved
appears appeared
remains remained
refers referred
develops developed
occurs
2024-07-16 01:59:17 root INFO     [order_1_approx] starting weight calculation for occurs occurred
appears appeared
remains remained
develops developed
replaces replaced
involves involved
tells told
refers
2024-07-16 01:59:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 02:03:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0327, -0.2700,  0.5283,  ..., -0.1411,  0.6914,  1.0547],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9619, -1.3545,  1.8828,  ...,  1.6484,  1.7715,  4.7578],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.5093, -0.0079, -0.0125,  ...,  0.0051,  0.0246,  0.0145],
        [-0.0182,  0.4619, -0.0222,  ...,  0.0145, -0.0071, -0.0139],
        [-0.0078,  0.0334,  0.4727,  ..., -0.0077, -0.0760,  0.0079],
        ...,
        [-0.0163, -0.0062,  0.0161,  ...,  0.5049,  0.0141, -0.0138],
        [ 0.0159,  0.0301, -0.0346,  ...,  0.0129,  0.4924,  0.0303],
        [ 0.0082,  0.0148, -0.0318,  ..., -0.0255,  0.0025,  0.4978]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0352, -3.0508,  1.5459,  ..., -1.7695,  2.4375,  2.3965]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 02:03:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for occurs occurred
appears appeared
remains remained
develops developed
replaces replaced
involves involved
tells told
refers
2024-07-16 02:03:24 root INFO     [order_1_approx] starting weight calculation for tells told
occurs occurred
replaces replaced
appears appeared
refers referred
develops developed
involves involved
remains
2024-07-16 02:03:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 02:07:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.0708, 1.3467, 1.0342,  ..., 1.1680, 0.0149, 0.0207], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.2520,  0.4961, -0.6436,  ...,  2.9180,  2.7812,  3.7031],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.2725e-01, -2.3499e-02,  5.1041e-03,  ...,  2.0096e-02,
          3.9062e-02,  4.2496e-03],
        [ 2.4902e-02,  4.0454e-01, -2.5711e-02,  ...,  1.5411e-02,
          2.5467e-02, -2.1240e-02],
        [ 1.7197e-02,  1.5480e-02,  4.0723e-01,  ..., -2.3331e-02,
         -2.2461e-02, -2.6489e-02],
        ...,
        [ 1.3237e-02,  1.8555e-02,  2.1286e-03,  ...,  4.4946e-01,
         -5.5313e-03, -2.0340e-02],
        [ 1.2665e-03,  7.0572e-03, -8.7585e-03,  ..., -2.9373e-04,
          4.2090e-01,  1.3496e-02],
        [ 1.7746e-02, -1.2283e-02, -8.6365e-03,  ..., -1.5442e-02,
         -1.2062e-02,  4.1479e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.5039,  0.7207, -0.5752,  ...,  0.9268,  2.0332,  1.8057]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 02:07:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for tells told
occurs occurred
replaces replaced
appears appeared
refers referred
develops developed
involves involved
remains
2024-07-16 02:07:30 root INFO     [order_1_approx] starting weight calculation for occurs occurred
tells told
remains remained
replaces replaced
develops developed
refers referred
appears appeared
involves
2024-07-16 02:07:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 02:11:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3774, -0.5552,  0.6421,  ...,  0.5205,  0.9526,  0.8613],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.9951, -1.3525, -3.4297,  ...,  3.6660,  6.2188,  4.8867],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.9805e-01,  2.8667e-03,  2.9068e-03,  ..., -1.0452e-02,
          3.4393e-02,  3.0304e-02],
        [ 1.2123e-02,  4.7461e-01,  3.0640e-02,  ...,  1.1368e-03,
          1.8768e-02, -1.0941e-02],
        [-9.0408e-04,  1.0872e-02,  5.1758e-01,  ...,  3.0823e-03,
         -1.2794e-02, -1.2695e-02],
        ...,
        [-1.0880e-02,  1.4809e-02,  5.1636e-02,  ...,  5.4590e-01,
         -3.2043e-04, -2.1164e-02],
        [-1.5579e-02,  4.0527e-02, -2.5238e-02,  ...,  1.6983e-02,
          4.9243e-01,  4.8401e-02],
        [ 5.1346e-03,  9.1095e-03, -1.8570e-02,  ..., -5.2109e-03,
         -1.9379e-02,  5.1367e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.2812, -0.0850, -2.3223,  ...,  2.4219,  5.1758,  4.4961]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 02:11:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for occurs occurred
tells told
remains remained
replaces replaced
develops developed
refers referred
appears appeared
involves
2024-07-16 02:11:36 root INFO     [order_1_approx] starting weight calculation for develops developed
tells told
remains remained
appears appeared
involves involved
occurs occurred
refers referred
replaces
2024-07-16 02:11:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 02:15:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2505,  0.1577,  0.7285,  ..., -0.4224,  0.2671,  0.1647],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.3359, -1.0029,  1.7061,  ...,  3.8730,  1.6055,  5.5859],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4263, -0.0037, -0.0142,  ...,  0.0013,  0.0117,  0.0374],
        [-0.0029,  0.3862,  0.0421,  ...,  0.0147, -0.0031, -0.0193],
        [ 0.0195,  0.0195,  0.4089,  ..., -0.0052, -0.0329, -0.0038],
        ...,
        [ 0.0175,  0.0191,  0.0060,  ...,  0.4434, -0.0078, -0.0113],
        [ 0.0090,  0.0084, -0.0150,  ...,  0.0189,  0.4385,  0.0141],
        [ 0.0116,  0.0060, -0.0082,  ...,  0.0057,  0.0043,  0.3735]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.4746, -1.6279,  1.3320,  ...,  5.4023,  0.7983,  4.8047]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 02:15:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for develops developed
tells told
remains remained
appears appeared
involves involved
occurs occurred
refers referred
replaces
2024-07-16 02:15:43 root INFO     total operator prediction time: 1973.4361805915833 seconds
2024-07-16 02:15:43 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on adj - superlative
2024-07-16 02:15:43 root INFO     building operator adj - superlative
2024-07-16 02:15:43 root INFO     [order_1_approx] starting weight calculation for neat neatest
shiny shiniest
tiny tiniest
polite politest
nice nicest
able ablest
lengthy lengthiest
tricky
2024-07-16 02:15:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 02:19:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2954, -0.1810, -0.0784,  ...,  0.3655, -0.2179,  0.6343],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-6.3086, -4.0000,  0.1875,  ...,  6.0195,  2.0762,  3.0625],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4937, -0.0133,  0.0019,  ..., -0.0049, -0.0135,  0.0063],
        [-0.0031,  0.4912,  0.0596,  ...,  0.0060,  0.0169,  0.0706],
        [ 0.0271,  0.0358,  0.4758,  ..., -0.0063, -0.0070, -0.0256],
        ...,
        [-0.0200, -0.0362,  0.0287,  ...,  0.5020,  0.0062, -0.0363],
        [-0.0114,  0.0058,  0.0238,  ..., -0.0073,  0.4749,  0.0111],
        [ 0.0123,  0.0092,  0.0240,  ..., -0.0066, -0.0225,  0.4863]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-6.2031, -8.0547,  0.7207,  ...,  4.3320,  1.4678, -0.0820]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 02:19:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for neat neatest
shiny shiniest
tiny tiniest
polite politest
nice nicest
able ablest
lengthy lengthiest
tricky
2024-07-16 02:19:50 root INFO     [order_1_approx] starting weight calculation for able ablest
polite politest
lengthy lengthiest
shiny shiniest
tricky trickiest
tiny tiniest
nice nicest
neat
2024-07-16 02:19:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 02:23:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3184, -0.8438, -0.4268,  ..., -0.1118, -0.2822,  0.6777],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5918,  1.8564, -3.7031,  ...,  0.8389, -0.1145,  6.3281],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.8535e-01, -3.1281e-04,  7.1487e-03,  ..., -1.3092e-02,
         -7.5150e-04, -8.9340e-03],
        [ 2.3071e-02,  4.7827e-01, -2.9049e-03,  ...,  2.1019e-03,
         -9.3842e-03,  4.3488e-04],
        [ 2.6001e-02,  1.0437e-02,  5.0537e-01,  ..., -1.3840e-02,
         -1.2802e-02, -1.7746e-02],
        ...,
        [-2.3865e-02,  3.6865e-02, -4.0375e-02,  ...,  5.2100e-01,
         -2.2781e-02, -2.5482e-03],
        [ 2.7649e-02,  2.0981e-02,  8.5678e-03,  ...,  1.2909e-02,
          4.5288e-01, -2.5040e-02],
        [ 1.6861e-02, -3.2166e-02,  1.2733e-02,  ...,  1.5564e-03,
         -6.7017e-02,  5.0635e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5898,  4.8125, -3.1973,  ...,  5.8281, -0.5596,  3.2715]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 02:23:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for able ablest
polite politest
lengthy lengthiest
shiny shiniest
tricky trickiest
tiny tiniest
nice nicest
neat
2024-07-16 02:23:57 root INFO     [order_1_approx] starting weight calculation for lengthy lengthiest
tricky trickiest
able ablest
shiny shiniest
neat neatest
polite politest
tiny tiniest
nice
2024-07-16 02:23:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 02:28:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0276, -0.7344, -0.6797,  ..., -0.5771, -0.2212,  0.4482],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4417, -0.0732, -5.5000,  ...,  2.5254,  5.1406,  4.8672],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.3359e-01, -2.7176e-02,  4.8103e-03,  ..., -1.8906e-02,
         -3.5767e-02, -1.5854e-02],
        [-8.3847e-03,  3.7329e-01,  2.8839e-02,  ...,  3.3813e-02,
         -3.3813e-02,  4.7302e-02],
        [ 3.9558e-03,  2.2614e-02,  4.1797e-01,  ..., -1.1993e-02,
         -8.3771e-03,  4.9057e-03],
        ...,
        [-1.9928e-02, -3.4058e-02,  5.1994e-03,  ...,  4.2603e-01,
         -4.4800e-02,  2.2430e-02],
        [ 7.3013e-03,  2.4658e-02,  1.2939e-02,  ...,  1.0544e-02,
          3.7305e-01,  1.6022e-04],
        [-1.1826e-02,  1.6159e-02, -3.2043e-02,  ..., -3.7384e-02,
         -1.8021e-02,  3.8281e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.4316,  0.5869, -5.3086,  ...,  2.8184,  3.6133,  3.9375]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 02:28:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for lengthy lengthiest
tricky trickiest
able ablest
shiny shiniest
neat neatest
polite politest
tiny tiniest
nice
2024-07-16 02:28:04 root INFO     [order_1_approx] starting weight calculation for tricky trickiest
nice nicest
neat neatest
polite politest
able ablest
tiny tiniest
shiny shiniest
lengthy
2024-07-16 02:28:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 02:32:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3093,  0.9390,  0.1875,  ...,  0.6514, -0.0938,  0.9277],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.2812, -2.6055, -4.6953,  ...,  2.4531,  4.1406,  5.9688],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.3958,  0.0408, -0.0063,  ..., -0.0110,  0.0108, -0.0081],
        [-0.0190,  0.3325, -0.0188,  ..., -0.0089, -0.0157,  0.0552],
        [ 0.0032,  0.0102,  0.4170,  ..., -0.0012,  0.0235, -0.0066],
        ...,
        [-0.0278,  0.0184,  0.0007,  ...,  0.4336,  0.0149,  0.0024],
        [-0.0129,  0.0220,  0.0132,  ...,  0.0049,  0.3994,  0.0127],
        [-0.0008, -0.0036, -0.0108,  ...,  0.0084, -0.0018,  0.4333]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7734, -3.6035, -5.0977,  ...,  3.6172,  3.2109,  4.8008]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 02:32:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for tricky trickiest
nice nicest
neat neatest
polite politest
able ablest
tiny tiniest
shiny shiniest
lengthy
2024-07-16 02:32:11 root INFO     [order_1_approx] starting weight calculation for lengthy lengthiest
nice nicest
neat neatest
polite politest
shiny shiniest
tricky trickiest
tiny tiniest
able
2024-07-16 02:32:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 02:36:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1079, -0.6709,  0.2659,  ..., -0.4744, -0.7075, -0.2925],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.8906,  1.2930, -3.8105,  ..., -1.8379,  4.7734,  1.7891],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.8853e-01,  6.7101e-03, -1.1187e-03,  ...,  2.5978e-03,
          8.6212e-03, -2.8946e-02],
        [-3.0975e-02,  5.0244e-01, -1.7349e-02,  ...,  3.9368e-02,
          2.3727e-02,  3.2349e-02],
        [-3.4485e-02,  3.0884e-02,  5.4492e-01,  ..., -1.9287e-02,
         -6.1455e-03, -3.8223e-03],
        ...,
        [ 1.2817e-02, -4.9408e-02,  1.6998e-02,  ...,  5.6787e-01,
         -3.8239e-02,  8.3923e-05],
        [ 6.6376e-03,  1.7731e-02, -3.8025e-02,  ..., -1.6388e-02,
          4.9902e-01,  8.5449e-04],
        [ 5.0049e-02, -7.6752e-03, -5.1994e-03,  ..., -4.6509e-02,
          3.9185e-02,  4.9707e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.5469, -0.4756, -2.7832,  ...,  1.2500,  3.0781,  1.4355]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 02:36:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for lengthy lengthiest
nice nicest
neat neatest
polite politest
shiny shiniest
tricky trickiest
tiny tiniest
able
2024-07-16 02:36:18 root INFO     [order_1_approx] starting weight calculation for tricky trickiest
able ablest
tiny tiniest
shiny shiniest
nice nicest
neat neatest
lengthy lengthiest
polite
2024-07-16 02:36:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 02:40:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4951, -1.4697, -0.1246,  ...,  0.8657,  0.1516,  2.0293],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 6.6406, -5.6445, -4.3711,  ...,  2.5234, -0.3198,  3.8711],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.3945e-01, -5.0110e-02, -2.5940e-04,  ...,  3.0334e-02,
         -5.8746e-04,  1.6968e-02],
        [ 9.0942e-03,  4.0088e-01, -1.5488e-02,  ...,  3.9825e-02,
          2.1255e-02,  2.4582e-02],
        [ 2.8992e-02,  1.6708e-02,  4.1016e-01,  ...,  4.3755e-03,
         -2.5909e-02, -2.0294e-02],
        ...,
        [-6.5369e-02,  1.6327e-03,  3.0575e-03,  ...,  4.3774e-01,
         -3.8483e-02, -2.0523e-03],
        [ 9.3689e-03,  7.5836e-03, -1.1490e-02,  ..., -5.0446e-02,
          4.1357e-01,  2.9663e-02],
        [-1.5175e-02,  8.2626e-03,  8.3771e-03,  ..., -4.1046e-02,
         -2.4872e-03,  3.9453e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7188, -8.8281, -8.2188,  ...,  1.5820, -0.8340,  3.8281]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 02:40:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for tricky trickiest
able ablest
tiny tiniest
shiny shiniest
nice nicest
neat neatest
lengthy lengthiest
polite
2024-07-16 02:40:24 root INFO     [order_1_approx] starting weight calculation for nice nicest
polite politest
shiny shiniest
lengthy lengthiest
neat neatest
tricky trickiest
able ablest
tiny
2024-07-16 02:40:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 02:44:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1396,  0.6040,  0.9531,  ...,  0.2993, -0.2095, -0.3838],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.1602,  1.6045, -0.5742,  ...,  3.0273,  0.8125,  2.5254],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4119, -0.0327, -0.0334,  ..., -0.0165, -0.0331, -0.0028],
        [-0.0520,  0.3838,  0.0531,  ..., -0.0175, -0.0095,  0.0487],
        [ 0.0292,  0.0321,  0.4072,  ..., -0.0166, -0.0134,  0.0201],
        ...,
        [-0.0513, -0.0103, -0.0050,  ...,  0.4314, -0.0221, -0.0083],
        [-0.0223,  0.0412,  0.0146,  ..., -0.0177,  0.3711,  0.0266],
        [-0.0006,  0.0279, -0.0138,  ..., -0.0447, -0.0365,  0.3704]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9502,  0.3369, -1.7236,  ..., -0.1543, -0.0342,  2.1035]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 02:44:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for nice nicest
polite politest
shiny shiniest
lengthy lengthiest
neat neatest
tricky trickiest
able ablest
tiny
2024-07-16 02:44:31 root INFO     [order_1_approx] starting weight calculation for lengthy lengthiest
nice nicest
polite politest
neat neatest
tiny tiniest
able ablest
tricky trickiest
shiny
2024-07-16 02:44:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 02:48:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0889,  0.5732, -1.2900,  ..., -0.1865,  1.5459,  0.9741],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3379, -1.3262, -0.8374,  ...,  4.2969,  2.2422,  5.7891],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.1943e-01, -1.0605e-03,  4.0245e-03,  ..., -1.0712e-02,
          2.6672e-02,  3.5065e-02],
        [-9.0332e-03,  4.3188e-01,  9.0485e-03,  ...,  2.0538e-02,
          5.8441e-03,  2.7420e-02],
        [-3.1982e-02,  5.0110e-02,  4.7021e-01,  ...,  2.3392e-02,
         -1.3943e-03,  1.1093e-02],
        ...,
        [-5.1422e-02, -1.2375e-02,  1.8936e-02,  ...,  4.5264e-01,
         -1.3908e-02, -1.9470e-02],
        [ 3.0090e-02, -1.8341e-02, -1.4801e-02,  ...,  1.2779e-04,
          4.2188e-01,  9.6817e-03],
        [-2.6016e-03,  3.1708e-02, -3.3508e-02,  ..., -1.7365e-02,
         -9.6970e-03,  4.3408e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4026, -3.0957,  1.4570,  ...,  5.1172,  0.2598,  4.4297]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 02:48:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for lengthy lengthiest
nice nicest
polite politest
neat neatest
tiny tiniest
able ablest
tricky trickiest
shiny
2024-07-16 02:48:38 root INFO     total operator prediction time: 1975.2504110336304 seconds
2024-07-16 02:48:38 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+er_irreg
2024-07-16 02:48:38 root INFO     building operator verb+er_irreg
2024-07-16 02:48:38 root INFO     [order_1_approx] starting weight calculation for discover discoverer
teach teacher
publish publisher
molest molester
achieve achiever
observe observer
entertain entertainer
interpret
2024-07-16 02:48:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 02:52:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0830,  0.2344,  0.0415,  ..., -0.9766,  0.5947,  0.3682],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2295, -0.2314, -3.6738,  ..., -2.1055,  2.1504,  0.6426],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4363, -0.0013, -0.0131,  ..., -0.0410,  0.0178,  0.0139],
        [-0.0241,  0.4082,  0.0150,  ...,  0.0305, -0.0088,  0.0210],
        [ 0.0119, -0.0184,  0.4719,  ...,  0.0215, -0.0029, -0.0368],
        ...,
        [-0.0243,  0.0041,  0.0097,  ...,  0.5156, -0.0059,  0.0161],
        [ 0.0148, -0.0054,  0.0025,  ..., -0.0023,  0.4082,  0.0056],
        [ 0.0039,  0.0271,  0.0085,  ..., -0.0204, -0.0068,  0.4102]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5361, -0.4814, -0.6953,  ..., -3.8828,  2.1836,  4.0898]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 02:52:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for discover discoverer
teach teacher
publish publisher
molest molester
achieve achiever
observe observer
entertain entertainer
interpret
2024-07-16 02:52:45 root INFO     [order_1_approx] starting weight calculation for observe observer
teach teacher
achieve achiever
molest molester
publish publisher
discover discoverer
interpret interpreter
entertain
2024-07-16 02:52:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 02:56:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.5234, -0.8037, -0.5269,  ..., -0.1300,  0.4976,  0.2524],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.8379,  0.4258, -1.8281,  ...,  1.4785,  3.9297,  4.9141],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.5312e-01, -2.8191e-03,  1.7319e-02,  ...,  7.0915e-03,
          1.9653e-02, -4.5868e-02],
        [ 1.0406e-02,  4.0869e-01,  3.7003e-03,  ...,  1.8730e-03,
         -1.6541e-02,  3.5553e-03],
        [ 3.0334e-02, -4.2915e-05,  4.5752e-01,  ...,  2.0370e-02,
          1.5900e-02, -3.6987e-02],
        ...,
        [-2.6596e-02, -5.1117e-03, -2.0218e-04,  ...,  4.4531e-01,
         -4.1229e-02, -1.5747e-02],
        [ 7.1526e-03, -1.9073e-06,  1.1353e-02,  ...,  4.7226e-03,
          4.0698e-01,  3.8147e-02],
        [-3.9673e-04,  1.8768e-02, -2.4643e-02,  ..., -2.7298e-02,
         -5.4810e-02,  4.5239e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6445,  3.7070, -0.0420,  ..., -1.4766,  3.5098,  3.8750]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 02:56:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for observe observer
teach teacher
achieve achiever
molest molester
publish publisher
discover discoverer
interpret interpreter
entertain
2024-07-16 02:56:50 root INFO     [order_1_approx] starting weight calculation for interpret interpreter
achieve achiever
teach teacher
publish publisher
molest molester
entertain entertainer
discover discoverer
observe
2024-07-16 02:56:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 03:00:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1743,  0.0627, -0.4507,  ..., -0.8491, -0.6973,  1.1387],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 7.4609,  0.3494, -4.9141,  ..., -2.8848,  1.8750,  3.6113],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4089,  0.0366,  0.0243,  ..., -0.0204, -0.0225,  0.0059],
        [ 0.0060,  0.3169,  0.0129,  ..., -0.0026, -0.0019, -0.0005],
        [-0.0172,  0.0068,  0.3933,  ...,  0.0185,  0.0151, -0.0341],
        ...,
        [-0.0367,  0.0321,  0.0254,  ...,  0.3787, -0.0042,  0.0089],
        [ 0.0280, -0.0104, -0.0251,  ..., -0.0228,  0.3779,  0.0293],
        [-0.0071,  0.0172,  0.0010,  ..., -0.0433, -0.0191,  0.4265]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4922,  1.9062, -4.0781,  ..., -1.8311,  2.8867,  4.8047]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 03:00:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for interpret interpreter
achieve achiever
teach teacher
publish publisher
molest molester
entertain entertainer
discover discoverer
observe
2024-07-16 03:00:57 root INFO     [order_1_approx] starting weight calculation for interpret interpreter
discover discoverer
observe observer
entertain entertainer
publish publisher
molest molester
achieve achiever
teach
2024-07-16 03:00:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 03:05:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0203,  1.3408,  1.3682,  ..., -0.8228,  0.1592,  0.1006],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.1797,  0.7739, -4.7031,  ...,  1.6875,  4.0391,  0.1006],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4727, -0.0380,  0.0056,  ..., -0.0260, -0.0083, -0.0446],
        [ 0.0287,  0.3687,  0.0008,  ..., -0.0206,  0.0079, -0.0210],
        [ 0.0292, -0.0159,  0.4026,  ...,  0.0455,  0.0446, -0.0299],
        ...,
        [ 0.0070, -0.0138,  0.0441,  ...,  0.4473,  0.0053,  0.0117],
        [-0.0141, -0.0045, -0.0087,  ...,  0.0055,  0.3716,  0.0097],
        [ 0.0725, -0.0067,  0.0109,  ..., -0.0214,  0.0144,  0.3828]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5254,  1.2480, -4.5273,  ...,  1.8105,  3.7051,  2.5625]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 03:05:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for interpret interpreter
discover discoverer
observe observer
entertain entertainer
publish publisher
molest molester
achieve achiever
teach
2024-07-16 03:05:04 root INFO     [order_1_approx] starting weight calculation for observe observer
interpret interpreter
achieve achiever
molest molester
discover discoverer
entertain entertainer
teach teacher
publish
2024-07-16 03:05:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 03:09:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1660, -0.1752,  0.7871,  ...,  0.7031,  0.4321,  0.8262],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 6.1992, -2.7148, -5.2422,  ...,  2.2129, -0.4917,  1.1963],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.8867e-01, -6.1226e-03,  3.1342e-02,  ..., -8.1558e-03,
          5.4970e-03, -4.5135e-02],
        [-1.4275e-02,  3.6792e-01,  3.9612e-02,  ..., -5.5313e-05,
         -3.0151e-02,  1.4175e-02],
        [ 7.0572e-03,  7.0114e-03,  4.3018e-01,  ...,  2.4414e-02,
         -5.2299e-03,  7.3242e-03],
        ...,
        [ 9.2926e-03, -1.6449e-02, -2.0630e-02,  ...,  4.3555e-01,
          3.2684e-02, -4.5563e-02],
        [ 6.6147e-03, -1.3428e-02, -3.2013e-02,  ..., -3.3264e-03,
          4.0088e-01,  1.3931e-02],
        [ 3.0151e-02, -1.0040e-02, -1.6724e-02,  ..., -8.1635e-03,
         -3.4607e-02,  3.9062e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.6172, -2.2031, -6.0312,  ...,  3.9395, -0.9229,  2.6289]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 03:09:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for observe observer
interpret interpreter
achieve achiever
molest molester
discover discoverer
entertain entertainer
teach teacher
publish
2024-07-16 03:09:10 root INFO     [order_1_approx] starting weight calculation for entertain entertainer
achieve achiever
teach teacher
molest molester
interpret interpreter
publish publisher
observe observer
discover
2024-07-16 03:09:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 03:13:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6523,  0.0544, -0.1487,  ..., -0.1863,  0.1328, -0.0942],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6953,  0.4434, -3.3848,  ..., -5.5078,  1.9561, -1.4805],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.3750e-01,  1.2810e-02, -6.1035e-05,  ...,  1.6403e-02,
          3.4790e-03, -2.7069e-02],
        [ 7.6904e-03,  3.6377e-01,  3.4851e-02,  ...,  1.5564e-02,
          4.1428e-03, -2.3041e-03],
        [ 1.0757e-02, -2.1301e-02,  4.0283e-01,  ...,  8.2703e-03,
         -1.4687e-03, -4.0100e-02],
        ...,
        [ 5.1117e-04, -3.5736e-02,  1.7738e-04,  ...,  4.3262e-01,
          1.4107e-02, -1.1063e-02],
        [ 2.3651e-03,  1.1742e-02,  2.6855e-02,  ..., -1.9897e-02,
          3.7646e-01,  2.1225e-02],
        [ 5.9509e-04,  1.4465e-02, -3.3112e-02,  ..., -1.1421e-02,
         -2.2507e-02,  4.1064e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3740, -0.6992, -2.7637,  ..., -4.4570, -1.0146,  0.7969]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 03:13:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for entertain entertainer
achieve achiever
teach teacher
molest molester
interpret interpreter
publish publisher
observe observer
discover
2024-07-16 03:13:16 root INFO     [order_1_approx] starting weight calculation for interpret interpreter
entertain entertainer
teach teacher
observe observer
discover discoverer
publish publisher
achieve achiever
molest
2024-07-16 03:13:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 03:17:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1083, -0.7656,  2.2383,  ...,  0.0323,  1.0342,  1.5117],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0664, -1.5703, -4.1875,  ..., -0.3037,  4.7500,  3.3633],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4575,  0.0257,  0.0076,  ..., -0.0445,  0.0303, -0.0203],
        [ 0.0198,  0.3782,  0.0082,  ..., -0.0024, -0.0159,  0.0219],
        [ 0.0069, -0.0138,  0.3931,  ...,  0.0198,  0.0351, -0.0313],
        ...,
        [-0.0143,  0.0109,  0.0460,  ...,  0.4666, -0.0050,  0.0148],
        [-0.0120,  0.0218,  0.0036,  ...,  0.0542,  0.4001, -0.0042],
        [-0.0038,  0.0104,  0.0171,  ..., -0.0141, -0.0098,  0.3953]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0459, -3.2656, -5.0078,  ...,  0.2842,  3.7266,  3.2266]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 03:17:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for interpret interpreter
entertain entertainer
teach teacher
observe observer
discover discoverer
publish publisher
achieve achiever
molest
2024-07-16 03:17:21 root INFO     [order_1_approx] starting weight calculation for publish publisher
discover discoverer
observe observer
interpret interpreter
molest molester
entertain entertainer
teach teacher
achieve
2024-07-16 03:17:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 03:21:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0859,  0.0524,  0.2571,  ..., -0.7051,  0.2490, -0.0867],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.9629,  1.3398, -3.2695,  ..., -4.7422,  2.3203, -1.8584],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4631,  0.0247,  0.0260,  ..., -0.0102,  0.0090,  0.0246],
        [ 0.0216,  0.4441,  0.0011,  ..., -0.0145,  0.0018, -0.0123],
        [ 0.0475, -0.0065,  0.4824,  ..., -0.0016,  0.0059, -0.0070],
        ...,
        [ 0.0256, -0.0377, -0.0132,  ...,  0.4812,  0.0059, -0.0129],
        [ 0.0169,  0.0224,  0.0036,  ..., -0.0182,  0.4585,  0.0221],
        [ 0.0229,  0.0017, -0.0133,  ..., -0.0448,  0.0260,  0.4600]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1055,  3.5215, -3.3672,  ..., -3.7695,  1.0938,  1.1904]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 03:21:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for publish publisher
discover discoverer
observe observer
interpret interpreter
molest molester
entertain entertainer
teach teacher
achieve
2024-07-16 03:21:26 root INFO     total operator prediction time: 1968.4805691242218 seconds
2024-07-16 03:21:26 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on over+adj_reg
2024-07-16 03:21:26 root INFO     building operator over+adj_reg
2024-07-16 03:21:27 root INFO     [order_1_approx] starting weight calculation for spent overspent
stressed overstressed
charged overcharged
excited overexcited
taken overtaken
stocked overstocked
arching overarching
stated
2024-07-16 03:21:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 03:25:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2324,  0.2249,  0.6372,  ...,  1.0342,  0.2375,  0.0088],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4453,  4.2539,  2.2051,  ...,  2.6641, -1.3125,  4.9805],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.3091e-01, -5.4932e-03,  1.4938e-02,  ...,  9.9487e-03,
          4.1016e-02, -2.0218e-02],
        [ 5.2917e-02,  3.7646e-01, -2.2476e-02,  ...,  4.3701e-02,
          1.5259e-02, -6.7749e-03],
        [ 2.3483e-02,  7.9193e-03,  4.5581e-01,  ...,  2.2354e-03,
          9.6130e-03, -4.3823e-02],
        ...,
        [ 5.2185e-03, -2.8595e-02, -2.0218e-04,  ...,  4.8242e-01,
          1.7426e-02, -2.9785e-02],
        [-8.3191e-02,  4.4250e-02,  2.4033e-02,  ..., -2.3071e-02,
          4.2139e-01,  6.1417e-04],
        [ 1.3290e-02, -3.8574e-02,  1.1436e-02,  ..., -1.1536e-02,
         -1.8265e-02,  4.3066e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.7656,  5.1055,  3.3867,  ...,  1.6377, -2.6074,  7.0000]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 03:25:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for spent overspent
stressed overstressed
charged overcharged
excited overexcited
taken overtaken
stocked overstocked
arching overarching
stated
2024-07-16 03:25:30 root INFO     [order_1_approx] starting weight calculation for arching overarching
stated overstated
taken overtaken
stressed overstressed
stocked overstocked
spent overspent
charged overcharged
excited
2024-07-16 03:25:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 03:29:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.5410, 0.5552, 1.4229,  ..., 0.5669, 0.1438, 0.5566], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.0938, -3.3242,  1.2305,  ...,  1.1855,  4.7344,  3.5586],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4209,  0.0170, -0.0223,  ..., -0.0208,  0.0063,  0.0168],
        [ 0.0194,  0.3547,  0.0072,  ...,  0.0269, -0.0265,  0.0038],
        [ 0.0161,  0.0154,  0.4153,  ..., -0.0259,  0.0134, -0.0569],
        ...,
        [-0.0070,  0.0052, -0.0013,  ...,  0.4663, -0.0253, -0.0197],
        [ 0.0075,  0.0047,  0.0154,  ...,  0.0253,  0.4006,  0.0528],
        [ 0.0207, -0.0618,  0.0064,  ..., -0.0333,  0.0035,  0.4258]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7930, -4.5312,  0.8501,  ...,  1.3770,  3.7715,  2.2871]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 03:29:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for arching overarching
stated overstated
taken overtaken
stressed overstressed
stocked overstocked
spent overspent
charged overcharged
excited
2024-07-16 03:29:35 root INFO     [order_1_approx] starting weight calculation for taken overtaken
charged overcharged
spent overspent
stocked overstocked
stressed overstressed
excited overexcited
stated overstated
arching
2024-07-16 03:29:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 03:33:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2163, -0.4854,  0.1550,  ..., -0.7798,  1.4902,  0.7773],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.6523, -1.3945, -0.4102,  ...,  2.3125,  1.1045,  3.7773],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.9731e-01,  1.4420e-03, -3.6945e-03,  ..., -4.0802e-02,
          3.4962e-03, -7.2403e-03],
        [-8.1253e-03,  4.9048e-01,  1.5259e-05,  ...,  3.7415e-02,
         -2.4933e-02,  2.3438e-02],
        [ 2.7481e-02,  3.9917e-02,  5.4492e-01,  ..., -2.8107e-02,
         -1.2901e-02,  6.7139e-03],
        ...,
        [-1.9531e-02,  2.6749e-02,  2.0813e-02,  ...,  5.8740e-01,
         -1.8036e-02,  5.7983e-02],
        [-1.6464e-02, -3.6850e-03,  2.0157e-02,  ..., -9.2239e-03,
          5.6836e-01,  1.1398e-02],
        [-1.3580e-03,  1.9135e-02, -2.8107e-02,  ..., -4.9683e-02,
         -3.4637e-02,  5.2539e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3301, -2.8828, -1.1445,  ...,  5.5742, -0.6221,  0.4258]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 03:33:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for taken overtaken
charged overcharged
spent overspent
stocked overstocked
stressed overstressed
excited overexcited
stated overstated
arching
2024-07-16 03:33:41 root INFO     [order_1_approx] starting weight calculation for stressed overstressed
spent overspent
charged overcharged
stated overstated
excited overexcited
stocked overstocked
arching overarching
taken
2024-07-16 03:33:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 03:37:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0044, -0.3718,  0.5068,  ...,  0.5913,  0.1262, -0.0743],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.3477, -3.1465,  1.5312,  ...,  2.7578,  3.1973,  3.9727],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.3701e-01, -1.4687e-03,  1.3611e-02,  ...,  5.9242e-03,
         -5.8975e-03, -1.1948e-02],
        [ 1.2482e-02,  4.2505e-01,  8.8120e-03,  ...,  3.4237e-03,
         -1.9684e-03,  5.5450e-02],
        [ 4.8065e-02,  2.0111e-02,  4.5898e-01,  ...,  9.4910e-03,
          4.3518e-02, -1.8143e-02],
        ...,
        [-2.9068e-02, -1.2352e-02,  3.3997e-02,  ...,  5.3125e-01,
          1.2909e-02, -1.5762e-02],
        [-3.3783e-02, -1.9348e-02, -1.7334e-02,  ..., -4.4861e-03,
          4.5190e-01,  3.4851e-02],
        [ 5.7251e-02, -8.2016e-05,  3.6621e-02,  ..., -3.7781e-02,
          7.5798e-03,  4.4824e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4570, -4.7969,  1.0312,  ...,  4.7031,  2.6367,  5.5000]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 03:37:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for stressed overstressed
spent overspent
charged overcharged
stated overstated
excited overexcited
stocked overstocked
arching overarching
taken
2024-07-16 03:37:48 root INFO     [order_1_approx] starting weight calculation for taken overtaken
stressed overstressed
charged overcharged
stated overstated
excited overexcited
stocked overstocked
arching overarching
spent
2024-07-16 03:37:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 03:41:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3350,  0.5474,  0.4238,  ..., -0.0667,  0.1949, -0.3252],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3955,  3.1562,  2.9785,  ...,  8.7891, -0.4399,  2.5156],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.3896e-01, -5.2643e-04,  2.4384e-02,  ..., -1.6449e-02,
         -6.2637e-03, -1.2383e-02],
        [-1.6006e-02,  4.1943e-01,  2.7466e-03,  ..., -4.3182e-03,
         -1.1269e-02,  3.1464e-02],
        [-2.0615e-02,  9.9030e-03,  4.1968e-01,  ..., -3.1174e-02,
         -1.3977e-02, -6.0303e-02],
        ...,
        [-2.9831e-02, -5.2032e-03,  3.2623e-02,  ...,  4.7852e-01,
          1.8036e-02, -5.2307e-02],
        [-2.7374e-02,  3.1891e-02,  3.6621e-04,  ...,  1.6449e-02,
          4.7412e-01,  2.3483e-02],
        [ 3.2854e-04, -2.6611e-02,  1.0300e-02,  ..., -6.6101e-02,
         -2.1469e-02,  4.9414e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3770,  2.4570,  3.4414,  ...,  9.0469, -0.2037,  0.4883]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 03:41:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for taken overtaken
stressed overstressed
charged overcharged
stated overstated
excited overexcited
stocked overstocked
arching overarching
spent
2024-07-16 03:41:55 root INFO     [order_1_approx] starting weight calculation for taken overtaken
charged overcharged
stated overstated
spent overspent
excited overexcited
stressed overstressed
arching overarching
stocked
2024-07-16 03:41:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 03:46:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3433,  1.3965,  0.0532,  ...,  1.1738, -0.2385, -0.2837],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.3242,  2.2539,  1.0996,  ...,  3.7012, -1.1055,  5.4453],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4583,  0.0101,  0.0092,  ..., -0.0108,  0.0380, -0.0364],
        [-0.0024,  0.4067,  0.0330,  ...,  0.0102, -0.0416,  0.0493],
        [-0.0143,  0.0125,  0.4753,  ..., -0.0247, -0.0437, -0.0399],
        ...,
        [-0.0086,  0.0031,  0.0071,  ...,  0.5010,  0.0562, -0.0102],
        [ 0.0181,  0.0227,  0.0033,  ..., -0.0020,  0.4666, -0.0070],
        [-0.0088, -0.0435,  0.0284,  ..., -0.0126, -0.0645,  0.4377]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.8574, -0.3184, -0.9863,  ...,  5.4609, -2.4297,  2.6250]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 03:46:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for taken overtaken
charged overcharged
stated overstated
spent overspent
excited overexcited
stressed overstressed
arching overarching
stocked
2024-07-16 03:46:02 root INFO     [order_1_approx] starting weight calculation for excited overexcited
stated overstated
stocked overstocked
taken overtaken
arching overarching
spent overspent
stressed overstressed
charged
2024-07-16 03:46:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 03:50:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9170,  0.4263,  0.8535,  ...,  1.1621, -0.0872, -0.0186],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([3.4883, 1.2217, 2.1133,  ..., 3.8203, 2.1445, 1.0518], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4863,  0.0038, -0.0061,  ..., -0.0205,  0.0035, -0.0067],
        [-0.0209,  0.4434, -0.0188,  ...,  0.0024, -0.0037,  0.0095],
        [-0.0106,  0.0190,  0.5029,  ...,  0.0145, -0.0179, -0.0251],
        ...,
        [-0.0207,  0.0044,  0.0228,  ...,  0.5356, -0.0061,  0.0020],
        [ 0.0105, -0.0137,  0.0331,  ..., -0.0263,  0.4519, -0.0068],
        [ 0.0174, -0.0335,  0.0107,  ..., -0.0406, -0.0104,  0.5381]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[2.7344, 2.0449, 2.4941,  ..., 5.7656, 0.5771, 0.8633]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 03:50:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for excited overexcited
stated overstated
stocked overstocked
taken overtaken
arching overarching
spent overspent
stressed overstressed
charged
2024-07-16 03:50:09 root INFO     [order_1_approx] starting weight calculation for charged overcharged
excited overexcited
taken overtaken
spent overspent
arching overarching
stocked overstocked
stated overstated
stressed
2024-07-16 03:50:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 03:54:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5098,  1.4072,  2.5723,  ...,  1.1162,  0.2390,  0.3599],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([1.3486, 0.7090, 2.4805,  ..., 4.6562, 1.0293, 3.9531], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4597,  0.0028, -0.0261,  ..., -0.0127,  0.0159, -0.0090],
        [ 0.0269,  0.4163,  0.0051,  ...,  0.0070,  0.0258, -0.0084],
        [-0.0079,  0.0189,  0.4673,  ...,  0.0016, -0.0186, -0.0162],
        ...,
        [ 0.0142,  0.0231,  0.0180,  ...,  0.5010,  0.0137, -0.0265],
        [-0.0112, -0.0133,  0.0179,  ..., -0.0172,  0.4421,  0.0651],
        [ 0.0010, -0.0097, -0.0185,  ..., -0.0222, -0.0093,  0.4534]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9395,  1.8086,  1.1553,  ...,  6.4688, -2.4551,  3.5527]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 03:54:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for charged overcharged
excited overexcited
taken overtaken
spent overspent
arching overarching
stocked overstocked
stated overstated
stressed
2024-07-16 03:54:16 root INFO     total operator prediction time: 1969.6634063720703 seconds
2024-07-16 03:54:16 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on adj+ly_reg
2024-07-16 03:54:16 root INFO     building operator adj+ly_reg
2024-07-16 03:54:16 root INFO     [order_1_approx] starting weight calculation for political politically
internal internally
virtual virtually
mental mentally
physical physically
nice nicely
increasing increasingly
serious
2024-07-16 03:54:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 03:58:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0793,  0.8066,  1.2412,  ..., -0.1039,  1.3730, -0.0352],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4238,  1.5977,  3.8984,  ...,  1.3818,  1.0039, -4.5195],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4268,  0.0127, -0.0222,  ..., -0.0072,  0.0156, -0.0338],
        [-0.0120,  0.3965,  0.0244,  ...,  0.0306, -0.0171,  0.0244],
        [ 0.0098,  0.0013,  0.4641,  ...,  0.0401, -0.0353,  0.0234],
        ...,
        [-0.0358,  0.0101,  0.0166,  ...,  0.4995,  0.0219,  0.0011],
        [ 0.0378,  0.0313,  0.0182,  ..., -0.0669,  0.4204, -0.0063],
        [ 0.0320,  0.0156, -0.0136,  ..., -0.0682,  0.0115,  0.4111]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.1719, -1.0234,  1.7031,  ...,  0.6885,  1.3281, -2.8516]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 03:58:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for political politically
internal internally
virtual virtually
mental mentally
physical physically
nice nicely
increasing increasingly
serious
2024-07-16 03:58:22 root INFO     [order_1_approx] starting weight calculation for physical physically
political politically
virtual virtually
nice nicely
serious seriously
internal internally
increasing increasingly
mental
2024-07-16 03:58:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 04:02:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0566,  0.7285, -0.8696,  ..., -0.0200,  0.5479,  0.4143],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9761,  0.5103, -3.1602,  ..., -1.7324,  3.4316, -3.1914],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.8682e-01, -4.3678e-03, -1.6617e-02,  ..., -3.8422e-02,
          3.4924e-03, -2.8534e-02],
        [ 3.1281e-04,  4.3237e-01, -2.3788e-02,  ..., -1.1963e-02,
         -9.1705e-03, -2.9984e-03],
        [-3.3386e-02,  2.3865e-02,  5.0488e-01,  ..., -5.1575e-03,
          1.8616e-02,  3.3966e-02],
        ...,
        [-1.1536e-02,  2.3071e-02,  1.0994e-02,  ...,  5.0879e-01,
          1.0162e-02,  7.7896e-03],
        [ 1.1833e-02, -2.6932e-03,  3.7498e-03,  ..., -1.9577e-02,
          4.8486e-01,  7.5264e-03],
        [ 3.6743e-02,  2.1622e-02, -2.0889e-02,  ..., -5.7037e-02,
          1.4053e-02,  4.8926e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0273,  0.8037, -0.1289,  ...,  0.0059,  2.8867,  0.7012]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 04:02:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for physical physically
political politically
virtual virtually
nice nicely
serious seriously
internal internally
increasing increasingly
mental
2024-07-16 04:02:29 root INFO     [order_1_approx] starting weight calculation for mental mentally
serious seriously
physical physically
virtual virtually
political politically
nice nicely
increasing increasingly
internal
2024-07-16 04:02:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 04:06:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7417,  1.1904,  0.3149,  ...,  0.1555, -0.2485,  1.3213],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5259,  0.5322,  2.4570,  ...,  1.2334,  1.4629,  0.8496],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4512, -0.0109, -0.0176,  ...,  0.0043,  0.0147, -0.0277],
        [-0.0225,  0.4216, -0.0216,  ...,  0.0055,  0.0108, -0.0071],
        [ 0.0242, -0.0080,  0.4832,  ..., -0.0347,  0.0327, -0.0036],
        ...,
        [-0.0090, -0.0258,  0.0136,  ...,  0.4780,  0.0146, -0.0050],
        [ 0.0320, -0.0434, -0.0063,  ..., -0.0127,  0.4565, -0.0111],
        [ 0.0080, -0.0169, -0.0040,  ..., -0.0499, -0.0063,  0.4270]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.1270,  0.9941,  2.2207,  ...,  0.6631,  5.2578,  0.7812]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 04:06:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for mental mentally
serious seriously
physical physically
virtual virtually
political politically
nice nicely
increasing increasingly
internal
2024-07-16 04:06:36 root INFO     [order_1_approx] starting weight calculation for serious seriously
mental mentally
virtual virtually
internal internally
physical physically
nice nicely
increasing increasingly
political
2024-07-16 04:06:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 04:10:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9961,  0.0486, -0.2324,  ...,  0.2593,  0.0066, -0.1692],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.7871, -3.4082,  2.0156,  ..., -1.0947, -1.5547, -0.3877],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.3359e-01, -1.7212e-02, -1.1421e-02,  ..., -9.9487e-03,
          1.5945e-02, -3.9948e-02],
        [-2.8839e-03,  3.8818e-01, -2.6993e-02,  ..., -9.1553e-04,
         -5.1117e-03, -4.5700e-03],
        [ 1.6541e-02,  8.6975e-04,  4.3384e-01,  ..., -4.9591e-04,
         -3.6163e-02, -2.8748e-02],
        ...,
        [-3.9749e-03,  1.6541e-02, -6.1798e-03,  ...,  4.3335e-01,
          2.9419e-02, -1.2207e-04],
        [ 7.7591e-03,  3.9459e-02,  2.2308e-02,  ..., -3.0777e-02,
          4.3555e-01,  1.5015e-02],
        [-9.7961e-03,  3.0472e-02, -1.1238e-02,  ..., -3.3936e-02,
         -1.4175e-02,  3.7671e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5488,  0.4434, -1.7285,  ..., -0.9976, -0.6499,  0.2515]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 04:10:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for serious seriously
mental mentally
virtual virtually
internal internally
physical physically
nice nicely
increasing increasingly
political
2024-07-16 04:10:42 root INFO     [order_1_approx] starting weight calculation for physical physically
internal internally
nice nicely
serious seriously
virtual virtually
mental mentally
political politically
increasing
2024-07-16 04:10:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 04:14:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9004, -0.2969, -0.4448,  ..., -0.1592,  1.6299, -0.4697],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.4219, -0.4819,  3.2441,  ...,  1.7021,  1.8701, -2.0312],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.3921e-01,  3.7384e-03,  1.2260e-02,  ...,  1.8585e-02,
         -4.5395e-04, -1.6357e-02],
        [ 1.2459e-02,  3.8110e-01, -3.9795e-02,  ...,  3.1372e-02,
         -4.9210e-03,  2.2049e-02],
        [ 1.7960e-02,  3.4199e-03,  4.5532e-01,  ..., -2.8515e-03,
          1.7395e-02,  5.2567e-03],
        ...,
        [ 4.3106e-03,  1.6647e-02, -2.0401e-02,  ...,  4.9072e-01,
          2.1484e-02,  2.7893e-02],
        [-1.5915e-02, -4.3716e-03, -1.4591e-04,  ..., -8.0185e-03,
          4.4678e-01, -3.8834e-03],
        [ 8.3771e-03, -6.3324e-04, -6.8054e-03,  ..., -3.5461e-02,
         -7.5874e-03,  4.2188e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.3945,  0.2666,  3.2188,  ...,  1.2637, -1.3389, -0.3906]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 04:14:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for physical physically
internal internally
nice nicely
serious seriously
virtual virtually
mental mentally
political politically
increasing
2024-07-16 04:14:49 root INFO     [order_1_approx] starting weight calculation for physical physically
serious seriously
political politically
internal internally
increasing increasingly
mental mentally
nice nicely
virtual
2024-07-16 04:14:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 04:18:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2109, -0.1914, -0.1588,  ...,  0.1323,  0.0249, -0.3569],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.7676, -2.4688,  2.9707,  ...,  2.6035,  0.0974, -4.0156],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4583,  0.0045, -0.0189,  ..., -0.0294,  0.0131, -0.0288],
        [-0.0201,  0.4045,  0.0104,  ..., -0.0128,  0.0049,  0.0077],
        [ 0.0139,  0.0038,  0.4468,  ...,  0.0282,  0.0048, -0.0079],
        ...,
        [-0.0131, -0.0040,  0.0179,  ...,  0.4590,  0.0093,  0.0075],
        [ 0.0056, -0.0038,  0.0043,  ..., -0.0279,  0.4600,  0.0033],
        [-0.0015,  0.0124, -0.0098,  ..., -0.0203,  0.0359,  0.4023]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2041, -3.1875,  2.5605,  ...,  0.5273,  0.4021, -2.5371]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 04:18:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for physical physically
serious seriously
political politically
internal internally
increasing increasingly
mental mentally
nice nicely
virtual
2024-07-16 04:18:56 root INFO     [order_1_approx] starting weight calculation for serious seriously
nice nicely
mental mentally
virtual virtually
political politically
increasing increasingly
internal internally
physical
2024-07-16 04:18:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 04:23:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0420,  1.1230,  0.6406,  ...,  0.6401,  1.0566, -0.4263],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-5.0430,  0.6182,  4.2695,  ..., -3.6426,  0.8545, -6.2812],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4553,  0.0112, -0.0206,  ..., -0.0056,  0.0162, -0.0205],
        [ 0.0194,  0.3896, -0.0303,  ..., -0.0037,  0.0231, -0.0298],
        [ 0.0198,  0.0051,  0.4468,  ...,  0.0064,  0.0027,  0.0056],
        ...,
        [-0.0123, -0.0102,  0.0159,  ...,  0.4163,  0.0221,  0.0110],
        [ 0.0260,  0.0532, -0.0225,  ...,  0.0015,  0.4094,  0.0135],
        [ 0.0424,  0.0084, -0.0085,  ..., -0.0330,  0.0079,  0.3975]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-6.7031, -0.4238,  5.3555,  ..., -3.4375,  0.1860, -2.1211]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 04:23:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for serious seriously
nice nicely
mental mentally
virtual virtually
political politically
increasing increasingly
internal internally
physical
2024-07-16 04:23:04 root INFO     [order_1_approx] starting weight calculation for serious seriously
mental mentally
political politically
increasing increasingly
physical physically
internal internally
virtual virtually
nice
2024-07-16 04:23:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 04:27:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6309, -0.7305, -1.2295,  ..., -0.1780, -0.1924,  0.8232],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0529, -2.5352, -1.1562,  ...,  2.6387,  2.0762,  2.0586],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4475, -0.0120, -0.0076,  ..., -0.0033,  0.0018, -0.0330],
        [ 0.0057,  0.4534,  0.0006,  ...,  0.0026, -0.0088,  0.0207],
        [-0.0045,  0.0234,  0.4814,  ...,  0.0281,  0.0086, -0.0309],
        ...,
        [-0.0089, -0.0024,  0.0217,  ...,  0.4934,  0.0014,  0.0098],
        [ 0.0083,  0.0481,  0.0053,  ...,  0.0292,  0.4573, -0.0372],
        [ 0.0114, -0.0435, -0.0413,  ..., -0.0164, -0.0326,  0.4570]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.1650, -1.4404, -0.2554,  ..., -0.6348,  1.5938,  4.7461]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 04:27:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for serious seriously
mental mentally
political politically
increasing increasingly
physical physically
internal internally
virtual virtually
nice
2024-07-16 04:27:11 root INFO     total operator prediction time: 1974.8798348903656 seconds
2024-07-16 04:27:11 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+tion_irreg
2024-07-16 04:27:11 root INFO     building operator verb+tion_irreg
2024-07-16 04:27:11 root INFO     [order_1_approx] starting weight calculation for examine examination
realize realization
stabilize stabilization
declare declaration
install installation
colonize colonization
inspire inspiration
condense
2024-07-16 04:27:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 04:31:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2925,  0.2361,  1.2549,  ..., -0.3538,  1.4785,  0.2734],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.3711, -1.7480, -0.3569,  ..., -0.4307,  0.3042,  6.9922],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4619,  0.0305,  0.0080,  ..., -0.0108,  0.0101,  0.0019],
        [-0.0114,  0.3806, -0.0094,  ...,  0.0325, -0.0135, -0.0247],
        [-0.0154,  0.0151,  0.4529,  ...,  0.0032, -0.0016,  0.0085],
        ...,
        [-0.0400,  0.0133,  0.0132,  ...,  0.4785,  0.0045, -0.0246],
        [ 0.0043,  0.0083,  0.0056,  ...,  0.0214,  0.4099,  0.0275],
        [ 0.0071,  0.0316,  0.0101,  ..., -0.0163,  0.0053,  0.4424]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.0469, -2.7227,  0.5176,  ..., -0.1147, -2.0469,  7.5742]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 04:31:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for examine examination
realize realization
stabilize stabilization
declare declaration
install installation
colonize colonization
inspire inspiration
condense
2024-07-16 04:31:19 root INFO     [order_1_approx] starting weight calculation for inspire inspiration
realize realization
install installation
examine examination
condense condensation
stabilize stabilization
colonize colonization
declare
2024-07-16 04:31:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 04:35:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.3945, -0.4458, -1.2041,  ..., -0.4272, -1.4902,  1.8008],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.4062, -3.1348, -6.8906,  ..., -1.3770,  2.5938,  1.5938],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.4556e-01,  2.3972e-02, -5.2643e-04,  ..., -2.1500e-02,
          2.6207e-03, -3.6530e-02],
        [ 3.1189e-02,  4.0649e-01,  1.2070e-02,  ...,  1.7029e-02,
         -1.3222e-02,  2.1133e-02],
        [ 1.7548e-04,  2.6093e-03,  4.4238e-01,  ..., -5.9509e-04,
         -2.7084e-04, -2.3254e-02],
        ...,
        [-6.3133e-04, -3.0136e-03,  1.3847e-02,  ...,  4.3286e-01,
          1.8860e-02, -1.3214e-02],
        [-1.1574e-02,  2.9984e-02, -3.4637e-02,  ...,  1.7273e-02,
          3.9551e-01, -1.2360e-03],
        [ 4.1718e-02, -8.8196e-03, -1.2589e-02,  ..., -3.4851e-02,
          4.0894e-03,  4.2920e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.7344, -1.7188, -3.9922,  ..., -0.8145,  0.9287,  1.9521]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 04:35:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for inspire inspiration
realize realization
install installation
examine examination
condense condensation
stabilize stabilization
colonize colonization
declare
2024-07-16 04:35:24 root INFO     [order_1_approx] starting weight calculation for examine examination
colonize colonization
install installation
inspire inspiration
condense condensation
declare declaration
stabilize stabilization
realize
2024-07-16 04:35:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 04:39:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8467, -0.5054,  1.0400,  ..., -1.1602,  0.4219,  0.0420],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3008,  1.2246, -3.1250,  ..., -4.2070,  0.5312,  4.6406],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.5430, -0.0089, -0.0073,  ...,  0.0072,  0.0252, -0.0092],
        [ 0.0099,  0.4492,  0.0035,  ...,  0.0172, -0.0501,  0.0104],
        [ 0.0158,  0.0086,  0.4778,  ..., -0.0178,  0.0555, -0.0023],
        ...,
        [-0.0020, -0.0111,  0.0089,  ...,  0.4961,  0.0258, -0.0216],
        [ 0.0232,  0.0202, -0.0215,  ..., -0.0491,  0.4160,  0.0275],
        [ 0.0228,  0.0331, -0.0011,  ..., -0.0072, -0.0316,  0.5137]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1992,  2.9512, -3.2539,  ..., -4.1992, -0.9004,  5.0078]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 04:39:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for examine examination
colonize colonization
install installation
inspire inspiration
condense condensation
declare declaration
stabilize stabilization
realize
2024-07-16 04:39:28 root INFO     [order_1_approx] starting weight calculation for inspire inspiration
stabilize stabilization
install installation
colonize colonization
realize realization
declare declaration
condense condensation
examine
2024-07-16 04:39:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 04:43:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0676, -0.4932, -0.1907,  ..., -1.5059,  0.3843,  0.5088],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.8828,  0.9058, -3.0703,  ...,  0.8486, -1.2207,  3.8320],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4460, -0.0012,  0.0122,  ..., -0.0053,  0.0242, -0.0313],
        [ 0.0144,  0.4241, -0.0082,  ...,  0.0385, -0.0525,  0.0194],
        [-0.0129,  0.0236,  0.4539,  ...,  0.0223,  0.0022, -0.0167],
        ...,
        [-0.0011, -0.0225,  0.0117,  ...,  0.4814, -0.0225, -0.0143],
        [ 0.0233,  0.0050, -0.0076,  ..., -0.0071,  0.4326,  0.0290],
        [-0.0231,  0.0108,  0.0036,  ..., -0.0582, -0.0336,  0.4297]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.7891,  0.4956, -3.9746,  ...,  3.6484, -2.0898,  0.7227]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 04:43:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for inspire inspiration
stabilize stabilization
install installation
colonize colonization
realize realization
declare declaration
condense condensation
examine
2024-07-16 04:43:32 root INFO     [order_1_approx] starting weight calculation for realize realization
inspire inspiration
install installation
condense condensation
declare declaration
colonize colonization
examine examination
stabilize
2024-07-16 04:43:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 04:47:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1562, -0.1270,  0.7715,  ...,  0.0861, -0.1260,  0.5771],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1650,  0.5635, -1.7090,  ...,  1.5127,  2.8066,  8.9375],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.1162e-01, -3.0518e-05, -2.5421e-02,  ..., -1.6342e-02,
          2.0508e-02,  6.5308e-03],
        [ 9.8801e-04,  4.0137e-01,  7.8812e-03,  ...,  3.7003e-03,
          1.8463e-02,  7.2517e-03],
        [ 6.6833e-03, -7.6752e-03,  4.0527e-01,  ..., -4.3564e-03,
         -1.4404e-02, -2.2491e-02],
        ...,
        [-1.8524e-02, -7.3242e-04,  6.7787e-03,  ...,  4.3872e-01,
         -5.5237e-03, -1.0651e-02],
        [-4.2877e-03,  2.8000e-02, -1.0452e-02,  ..., -7.5989e-03,
          3.9111e-01,  2.9480e-02],
        [ 2.1896e-02,  2.8290e-02, -2.0233e-02,  ..., -3.4943e-03,
         -3.7292e-02,  4.5264e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4688,  0.4482, -2.0918,  ...,  1.9023,  2.8594, 10.2734]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 04:47:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for realize realization
inspire inspiration
install installation
condense condensation
declare declaration
colonize colonization
examine examination
stabilize
2024-07-16 04:47:36 root INFO     [order_1_approx] starting weight calculation for stabilize stabilization
examine examination
install installation
condense condensation
colonize colonization
realize realization
declare declaration
inspire
2024-07-16 04:47:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 04:51:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3149, -1.2061, -0.0160,  ...,  0.8359,  1.2080, -0.9409],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.0312e+00, -4.5039e+00, -8.4219e+00,  ...,  5.3164e+00,
         3.9062e-03, -3.4434e+00], device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4614,  0.0209,  0.0185,  ..., -0.0203,  0.0095,  0.0027],
        [ 0.0471,  0.4475,  0.0146,  ..., -0.0109,  0.0027,  0.0105],
        [ 0.0263, -0.0119,  0.4316,  ..., -0.0101,  0.0173, -0.0048],
        ...,
        [-0.0199, -0.0543,  0.0210,  ...,  0.4978,  0.0061, -0.0182],
        [-0.0099, -0.0134, -0.0119,  ...,  0.0112,  0.4209,  0.0006],
        [ 0.0299, -0.0060, -0.0225,  ..., -0.0341,  0.0027,  0.4595]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.0625, -4.8828, -7.4062,  ...,  3.8223, -1.0469, -0.2090]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 04:51:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for stabilize stabilization
examine examination
install installation
condense condensation
colonize colonization
realize realization
declare declaration
inspire
2024-07-16 04:51:41 root INFO     [order_1_approx] starting weight calculation for examine examination
declare declaration
stabilize stabilization
realize realization
inspire inspiration
condense condensation
colonize colonization
install
2024-07-16 04:51:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 04:55:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9072, -0.5376, -0.0490,  ...,  0.0598,  0.1815,  0.5293],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9155,  0.3687, -0.7012,  ...,  1.0098,  2.5078,  2.4531],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.7607e-01,  3.2043e-02,  2.1458e-03,  ..., -4.0283e-03,
          2.0096e-02, -1.2466e-02],
        [ 2.0233e-02,  4.1064e-01, -6.5994e-04,  ..., -6.0425e-03,
         -5.6839e-03,  1.1917e-02],
        [-1.7563e-02,  2.6840e-02,  4.6289e-01,  ...,  2.0466e-03,
          7.4043e-03, -9.1553e-03],
        ...,
        [-1.1414e-02, -1.3664e-02,  8.5373e-03,  ...,  4.7461e-01,
          4.5013e-04,  7.5989e-03],
        [ 6.0425e-03,  1.6708e-02, -3.1090e-03,  ..., -1.5808e-02,
          4.2529e-01,  9.1553e-03],
        [ 2.1347e-02,  8.5907e-03,  5.5542e-03,  ..., -9.1476e-03,
         -7.4997e-03,  4.3018e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[1.8008, 1.6816, 0.5068,  ..., 2.1953, 3.6777, 0.7637]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 04:55:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for examine examination
declare declaration
stabilize stabilization
realize realization
inspire inspiration
condense condensation
colonize colonization
install
2024-07-16 04:55:47 root INFO     [order_1_approx] starting weight calculation for install installation
realize realization
stabilize stabilization
examine examination
inspire inspiration
condense condensation
declare declaration
colonize
2024-07-16 04:55:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 04:59:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4292, -0.1385,  0.5054,  ...,  0.6787,  1.1270,  0.1938],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0312, -0.5742, -2.6367,  ...,  0.1152,  0.3147,  6.7539],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4158, -0.0244,  0.0052,  ..., -0.0038,  0.0405, -0.0491],
        [ 0.0332,  0.3940, -0.0057,  ..., -0.0331,  0.0228,  0.0262],
        [ 0.0210,  0.0118,  0.3848,  ..., -0.0127,  0.0016,  0.0107],
        ...,
        [-0.0467,  0.0169, -0.0169,  ...,  0.4431,  0.0062,  0.0090],
        [ 0.0162,  0.0162,  0.0049,  ...,  0.0127,  0.4172,  0.0240],
        [-0.0222,  0.0417,  0.0273,  ...,  0.0024, -0.0051,  0.4153]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.2578,  0.3994, -0.7803,  ..., -0.4360,  0.7646,  5.9180]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 04:59:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for install installation
realize realization
stabilize stabilization
examine examination
inspire inspiration
condense condensation
declare declaration
colonize
2024-07-16 04:59:52 root INFO     total operator prediction time: 1960.9937670230865 seconds
2024-07-16 04:59:52 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+able_reg
2024-07-16 04:59:52 root INFO     building operator verb+able_reg
2024-07-16 04:59:52 root INFO     [order_1_approx] starting weight calculation for publish publishable
enjoy enjoyable
prevent preventable
inflate inflatable
advise advisable
renew renewable
consider considerable
foresee
2024-07-16 04:59:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 05:03:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2778,  0.6851, -0.8916,  ...,  0.2742,  1.5098, -0.3218],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7031,  1.4141, -5.8750,  ..., -1.5508,  1.9062,  0.5078],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4617,  0.0131, -0.0119,  ...,  0.0242,  0.0129,  0.0231],
        [ 0.0233,  0.4624,  0.0098,  ...,  0.0076,  0.0018,  0.0258],
        [ 0.0205, -0.0173,  0.4780,  ..., -0.0060,  0.0099, -0.0374],
        ...,
        [ 0.0008, -0.0230,  0.0147,  ...,  0.4849,  0.0587, -0.0267],
        [ 0.0190,  0.0009, -0.0281,  ..., -0.0069,  0.4900,  0.0190],
        [ 0.0032, -0.0025, -0.0310,  ...,  0.0173, -0.0188,  0.4695]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3516,  1.9434, -4.3164,  ..., -0.1260,  0.3730,  1.6895]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 05:03:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for publish publishable
enjoy enjoyable
prevent preventable
inflate inflatable
advise advisable
renew renewable
consider considerable
foresee
2024-07-16 05:03:59 root INFO     [order_1_approx] starting weight calculation for consider considerable
inflate inflatable
foresee foreseeable
renew renewable
publish publishable
prevent preventable
enjoy enjoyable
advise
2024-07-16 05:03:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 05:08:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4111, -0.2461,  0.3042,  ...,  1.0566, -0.4517,  0.6641],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.8203,  0.9150, -3.6895,  ...,  0.9922,  1.9023,  3.8750],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.6948e-01, -7.6294e-06,  2.0874e-02,  ...,  8.2092e-03,
          3.7903e-02, -4.4556e-02],
        [ 1.3924e-02,  3.7524e-01,  1.2108e-02,  ...,  5.2429e-02,
          1.4420e-03,  3.3630e-02],
        [-1.6190e-02,  5.0415e-02,  4.2188e-01,  ..., -1.2070e-02,
         -1.9623e-02, -1.3954e-02],
        ...,
        [-4.0985e-02,  7.6904e-03,  6.9427e-03,  ...,  4.3774e-01,
         -1.8646e-02, -2.1683e-02],
        [ 1.4870e-02, -3.3569e-02, -6.4087e-04,  ...,  6.9695e-03,
          4.1260e-01,  1.4969e-02],
        [ 4.8141e-03,  6.5536e-03, -2.0081e-02,  ...,  2.6123e-02,
         -4.2175e-02,  3.7622e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7676, -1.3896, -6.2969,  ..., -0.4805,  4.2734,  2.0996]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 05:08:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for consider considerable
inflate inflatable
foresee foreseeable
renew renewable
publish publishable
prevent preventable
enjoy enjoyable
advise
2024-07-16 05:08:05 root INFO     [order_1_approx] starting weight calculation for prevent preventable
consider considerable
publish publishable
inflate inflatable
foresee foreseeable
renew renewable
advise advisable
enjoy
2024-07-16 05:08:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 05:12:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9048,  0.0484, -0.2256,  ..., -0.3755, -0.3340,  0.5713],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7646,  2.5273,  0.1992,  ..., -0.9043,  0.8262,  0.4507],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4822,  0.0111, -0.0064,  ..., -0.0028,  0.0218, -0.0126],
        [ 0.0082,  0.4209,  0.0014,  ...,  0.0092, -0.0061,  0.0210],
        [ 0.0039,  0.0253,  0.4673,  ..., -0.0168,  0.0264, -0.0062],
        ...,
        [-0.0257,  0.0226, -0.0110,  ...,  0.5103,  0.0249,  0.0083],
        [ 0.0503, -0.0062,  0.0183,  ...,  0.0035,  0.4822,  0.0039],
        [-0.0103, -0.0131, -0.0057,  ..., -0.0061,  0.0018,  0.4734]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.4297,  3.4531, -2.3438,  ..., -1.6055, -0.4150, -0.6577]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 05:12:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for prevent preventable
consider considerable
publish publishable
inflate inflatable
foresee foreseeable
renew renewable
advise advisable
enjoy
2024-07-16 05:12:12 root INFO     [order_1_approx] starting weight calculation for prevent preventable
foresee foreseeable
advise advisable
enjoy enjoyable
renew renewable
publish publishable
inflate inflatable
consider
2024-07-16 05:12:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 05:16:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7202, -0.1328, -0.4070,  ..., -0.4907,  0.5371,  1.3525],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5996,  4.0547, -4.2812,  ...,  1.0352, -1.2793, -0.2197],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.1748e-01,  2.8870e-02,  2.2980e-02,  ..., -9.4604e-04,
          1.5320e-02,  9.9869e-03],
        [-1.0620e-02,  4.2139e-01, -9.8515e-04,  ...,  5.5450e-02,
         -2.3392e-02,  4.5532e-02],
        [-3.4637e-03,  2.1072e-02,  4.6924e-01,  ..., -7.9651e-03,
         -2.1286e-03,  1.5244e-02],
        ...,
        [ 2.1362e-04,  4.8294e-03,  2.2476e-02,  ...,  4.8511e-01,
          2.5806e-03,  2.8107e-02],
        [ 1.5076e-02, -3.7506e-02, -1.0422e-02,  ..., -3.2310e-03,
          4.2065e-01,  9.5825e-03],
        [ 4.0527e-02, -4.0375e-02, -1.7120e-02,  ..., -4.2664e-02,
         -3.5782e-03,  3.7012e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5137,  2.9414, -2.5527,  ..., -0.3506, -0.4731,  0.9834]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 05:16:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for prevent preventable
foresee foreseeable
advise advisable
enjoy enjoyable
renew renewable
publish publishable
inflate inflatable
consider
2024-07-16 05:16:18 root INFO     [order_1_approx] starting weight calculation for consider considerable
prevent preventable
foresee foreseeable
advise advisable
inflate inflatable
renew renewable
enjoy enjoyable
publish
2024-07-16 05:16:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 05:20:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4241, -0.2065,  0.8013,  ...,  1.1943,  0.4851,  0.8447],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0859, -1.2627, -4.7188,  ...,  2.3789, -2.3086,  0.6650],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4275,  0.0266, -0.0027,  ..., -0.0144,  0.0215, -0.0237],
        [ 0.0040,  0.3442, -0.0219,  ...,  0.0176,  0.0010,  0.0006],
        [-0.0143, -0.0249,  0.3987,  ...,  0.0361, -0.0252,  0.0115],
        ...,
        [-0.0042,  0.0030, -0.0043,  ...,  0.4270,  0.0243, -0.0237],
        [ 0.0427, -0.0165, -0.0326,  ...,  0.0172,  0.3931, -0.0057],
        [-0.0056, -0.0145, -0.0024,  ..., -0.0099, -0.0041,  0.3728]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1201,  0.2910, -3.8730,  ..., -0.4141, -1.4121, -1.2539]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 05:20:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for consider considerable
prevent preventable
foresee foreseeable
advise advisable
inflate inflatable
renew renewable
enjoy enjoyable
publish
2024-07-16 05:20:24 root INFO     [order_1_approx] starting weight calculation for publish publishable
foresee foreseeable
renew renewable
enjoy enjoyable
advise advisable
inflate inflatable
consider considerable
prevent
2024-07-16 05:20:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 05:24:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.5674, 0.7432, 0.5649,  ..., 0.3594, 0.4343, 0.2119], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5859,  1.6699, -3.9395,  ...,  1.3438,  4.2539, -1.9883],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4895,  0.0216, -0.0256,  ...,  0.0198,  0.0006, -0.0012],
        [ 0.0043,  0.4463, -0.0311,  ...,  0.0007, -0.0159,  0.0083],
        [ 0.0022, -0.0303,  0.5400,  ...,  0.0414, -0.0318,  0.0179],
        ...,
        [-0.0007,  0.0109, -0.0015,  ...,  0.4990,  0.0238, -0.0453],
        [ 0.0659, -0.0147, -0.0046,  ...,  0.0061,  0.5381,  0.0060],
        [-0.0101, -0.0029, -0.0083,  ...,  0.0177, -0.0426,  0.4802]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4404,  0.7646, -5.0391,  ...,  2.5039,  4.0859, -3.3164]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 05:24:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for publish publishable
foresee foreseeable
renew renewable
enjoy enjoyable
advise advisable
inflate inflatable
consider considerable
prevent
2024-07-16 05:24:29 root INFO     [order_1_approx] starting weight calculation for publish publishable
foresee foreseeable
renew renewable
prevent preventable
enjoy enjoyable
consider considerable
advise advisable
inflate
2024-07-16 05:24:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 05:28:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6802,  0.4485,  1.2461,  ..., -0.7295,  0.4692,  0.4873],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7627, -1.4082, -1.3164,  ..., -0.9922,  4.2266, -2.3281],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4512,  0.0057, -0.0175,  ..., -0.0022,  0.0147, -0.0236],
        [ 0.0307,  0.3982, -0.0090,  ..., -0.0160,  0.0052,  0.0168],
        [ 0.0118,  0.0524,  0.4197,  ..., -0.0128, -0.0096, -0.0226],
        ...,
        [-0.0464, -0.0122,  0.0233,  ...,  0.4243, -0.0088,  0.0028],
        [ 0.0356, -0.0083, -0.0336,  ...,  0.0051,  0.4382,  0.0074],
        [ 0.0013,  0.0433, -0.0171,  ..., -0.0144, -0.0010,  0.4302]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6221, -1.5186, -2.3594,  ...,  1.3203,  4.7383, -1.7402]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 05:28:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for publish publishable
foresee foreseeable
renew renewable
prevent preventable
enjoy enjoyable
consider considerable
advise advisable
inflate
2024-07-16 05:28:35 root INFO     [order_1_approx] starting weight calculation for prevent preventable
inflate inflatable
advise advisable
consider considerable
foresee foreseeable
enjoy enjoyable
publish publishable
renew
2024-07-16 05:28:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 05:32:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4089, -0.3064,  0.0830,  ...,  0.7168,  0.5459,  0.7100],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4956, -0.1250, -5.5625,  ...,  1.3311,  2.0332,  1.9551],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.6094e-01, -1.6998e-02, -3.5248e-02,  ..., -1.0048e-02,
          2.3636e-02, -1.5900e-02],
        [-7.7057e-03,  4.0283e-01,  3.3417e-03,  ..., -3.8223e-03,
         -4.1992e-02, -1.3718e-02],
        [ 2.0538e-02,  5.1117e-04,  4.8779e-01,  ..., -7.4654e-03,
          7.4768e-03, -3.3783e-02],
        ...,
        [-1.5457e-02,  1.8173e-02,  2.3346e-03,  ...,  4.5996e-01,
         -1.9073e-04, -2.9793e-03],
        [-1.8600e-02,  7.5302e-03,  1.2932e-02,  ...,  2.2411e-03,
          4.7021e-01,  2.8412e-02],
        [-9.1324e-03,  4.9530e-02,  2.5116e-02,  ..., -1.5656e-02,
         -2.1194e-02,  4.6704e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0151, -0.6724, -3.5820,  ..., -0.2852, -0.5645,  0.5459]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 05:32:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for prevent preventable
inflate inflatable
advise advisable
consider considerable
foresee foreseeable
enjoy enjoyable
publish publishable
renew
2024-07-16 05:32:41 root INFO     total operator prediction time: 1968.9831113815308 seconds
2024-07-16 05:32:41 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on un+adj_reg
2024-07-16 05:32:41 root INFO     building operator un+adj_reg
2024-07-16 05:32:41 root INFO     [order_1_approx] starting weight calculation for noticed unnoticed
affected unaffected
expected unexpected
pleasant unpleasant
successful unsuccessful
available unavailable
interrupted uninterrupted
healthy
2024-07-16 05:32:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 05:36:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0977,  0.4292, -0.2849,  ...,  0.2607, -0.1689,  0.9414],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([2.7031e+00, 1.7500e+00, 2.1582e-01,  ..., 2.5391e-01, 4.1016e+00,
        1.9531e-03], device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.4019e-01,  8.2626e-03,  1.2299e-02,  ..., -3.0022e-03,
         -4.5776e-05,  4.1504e-03],
        [ 1.7090e-02,  3.5229e-01, -1.7365e-02,  ...,  1.3840e-02,
          5.4077e-02,  8.0872e-03],
        [ 3.4676e-03, -9.1782e-03,  4.2578e-01,  ..., -1.8692e-02,
         -1.4297e-02, -3.4714e-03],
        ...,
        [-7.1793e-03,  1.9312e-03, -3.2043e-04,  ...,  4.8242e-01,
          8.8501e-04, -1.4481e-02],
        [ 1.8539e-02, -8.8425e-03,  3.3035e-03,  ...,  1.8692e-03,
          4.3140e-01,  3.5004e-02],
        [-2.4521e-02,  2.1820e-02, -1.6220e-02,  ...,  5.5542e-03,
         -2.3056e-02,  4.3140e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2520,  3.6250,  0.1028,  ...,  1.3516,  4.1484, -0.7559]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 05:36:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for noticed unnoticed
affected unaffected
expected unexpected
pleasant unpleasant
successful unsuccessful
available unavailable
interrupted uninterrupted
healthy
2024-07-16 05:36:49 root INFO     [order_1_approx] starting weight calculation for expected unexpected
successful unsuccessful
healthy unhealthy
affected unaffected
noticed unnoticed
available unavailable
pleasant unpleasant
interrupted
2024-07-16 05:36:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 05:40:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6016, -0.4966,  0.1008,  ...,  1.0996, -0.2734, -0.0781],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5801, -2.2031, -7.0234,  ...,  4.1328, -0.4600,  0.3115],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.2993e-01,  1.5518e-02, -8.2169e-03,  ..., -3.2074e-02,
          6.9466e-03, -3.0396e-02],
        [ 1.6464e-02,  3.8867e-01, -1.6602e-02,  ...,  6.2447e-03,
         -7.9727e-03,  1.2955e-02],
        [ 3.3569e-04,  1.2840e-02,  3.9600e-01,  ...,  3.5820e-03,
          2.6459e-02, -4.5441e-02],
        ...,
        [-3.3264e-02,  1.5160e-02,  4.5013e-02,  ...,  4.1870e-01,
         -5.4626e-03,  9.8190e-03],
        [-6.7825e-03,  1.0452e-02,  2.6031e-02,  ..., -3.2227e-02,
          4.6631e-01,  3.8147e-02],
        [ 3.6499e-02,  3.1189e-02, -1.7090e-02,  ..., -1.8616e-03,
          2.7046e-03,  3.8696e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.7617, -0.3594, -2.9531,  ...,  2.1270, -0.9648,  0.8037]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 05:40:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for expected unexpected
successful unsuccessful
healthy unhealthy
affected unaffected
noticed unnoticed
available unavailable
pleasant unpleasant
interrupted
2024-07-16 05:40:57 root INFO     [order_1_approx] starting weight calculation for interrupted uninterrupted
available unavailable
affected unaffected
successful unsuccessful
healthy unhealthy
noticed unnoticed
expected unexpected
pleasant
2024-07-16 05:40:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 05:45:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3706, -0.9243, -0.3953,  ...,  0.3193,  0.7979,  0.3303],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9980, -2.2344, -0.0811,  ...,  3.3633,  0.7959, -1.2891],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.2798e-01, -6.5002e-03,  3.5858e-03,  ...,  1.4099e-02,
         -5.1575e-03,  2.9480e-02],
        [ 3.1372e-02,  3.6572e-01, -4.6234e-03,  ..., -1.9104e-02,
          1.1581e-02,  1.0612e-02],
        [ 5.8594e-02,  2.5909e-02,  4.4629e-01,  ..., -3.3020e-02,
         -2.2705e-02, -8.5144e-03],
        ...,
        [-4.1016e-02,  1.5167e-02,  3.3081e-02,  ...,  4.2944e-01,
          1.2665e-02,  3.7079e-02],
        [-1.3618e-02,  4.9561e-02, -5.9204e-03,  ...,  1.0681e-04,
          4.4702e-01,  2.5543e-02],
        [ 1.0254e-02, -1.0071e-02, -4.4434e-02,  ...,  1.3184e-02,
          2.4948e-03,  4.0454e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9678, -1.7549,  0.2700,  ...,  6.8203, -1.0947, -0.7329]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 05:45:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for interrupted uninterrupted
available unavailable
affected unaffected
successful unsuccessful
healthy unhealthy
noticed unnoticed
expected unexpected
pleasant
2024-07-16 05:45:04 root INFO     [order_1_approx] starting weight calculation for pleasant unpleasant
available unavailable
interrupted uninterrupted
expected unexpected
noticed unnoticed
healthy unhealthy
affected unaffected
successful
2024-07-16 05:45:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 05:49:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.7158, 0.4028, 0.2070,  ..., 0.9009, 0.0952, 0.3816], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5078,  1.9688,  5.1406,  ...,  2.6289, -2.1621, -0.4443],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4690,  0.0235,  0.0325,  ..., -0.0072, -0.0356, -0.0145],
        [ 0.0140,  0.3752, -0.0006,  ...,  0.0136,  0.0366,  0.0032],
        [ 0.0054,  0.0057,  0.4434,  ..., -0.0088, -0.0081, -0.0126],
        ...,
        [ 0.0105, -0.0126, -0.0014,  ...,  0.4529, -0.0370,  0.0062],
        [-0.0046,  0.0164,  0.0027,  ...,  0.0030,  0.4858,  0.0233],
        [ 0.0011, -0.0092, -0.0359,  ..., -0.0573,  0.0184,  0.4399]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2383,  1.9922,  6.7500,  ...,  2.9629, -3.3828,  0.3696]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 05:49:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for pleasant unpleasant
available unavailable
interrupted uninterrupted
expected unexpected
noticed unnoticed
healthy unhealthy
affected unaffected
successful
2024-07-16 05:49:12 root INFO     [order_1_approx] starting weight calculation for pleasant unpleasant
healthy unhealthy
noticed unnoticed
successful unsuccessful
available unavailable
interrupted uninterrupted
affected unaffected
expected
2024-07-16 05:49:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 05:53:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 4.8828e-03,  5.4883e-01, -1.2207e-03,  ...,  5.9033e-01,
         1.2236e+00,  3.8525e-01], device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 7.0156, -0.9893, -0.6670,  ...,  2.5508,  5.3672,  0.5664],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4221,  0.0012,  0.0314,  ...,  0.0094, -0.0097,  0.0074],
        [ 0.0132,  0.3684, -0.0206,  ...,  0.0444,  0.0123,  0.0067],
        [ 0.0309,  0.0228,  0.4065,  ...,  0.0104, -0.0116, -0.0106],
        ...,
        [-0.0163,  0.0299, -0.0254,  ...,  0.4036,  0.0088, -0.0308],
        [ 0.0082, -0.0134,  0.0100,  ..., -0.0215,  0.3984, -0.0291],
        [-0.0133, -0.0056, -0.0065,  ..., -0.0294, -0.0292,  0.3994]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.7812,  0.5215, -0.2627,  ...,  1.9639,  3.7656, -0.0615]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 05:53:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for pleasant unpleasant
healthy unhealthy
noticed unnoticed
successful unsuccessful
available unavailable
interrupted uninterrupted
affected unaffected
expected
2024-07-16 05:53:19 root INFO     [order_1_approx] starting weight calculation for healthy unhealthy
affected unaffected
expected unexpected
interrupted uninterrupted
noticed unnoticed
successful unsuccessful
pleasant unpleasant
available
2024-07-16 05:53:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 05:57:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0273, -0.5532, -0.3879,  ...,  0.5825, -0.2036, -0.3005],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6328,  1.2500,  3.8828,  ..., -0.4331,  2.4199, -4.4648],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.3799e-01,  2.0416e-02, -1.6312e-02,  ...,  3.4485e-02,
          4.5929e-03, -2.8095e-03],
        [-7.0114e-03,  3.8037e-01, -1.5259e-05,  ..., -8.7280e-03,
          2.2446e-02,  1.6754e-02],
        [ 4.8950e-02, -5.3596e-03,  4.5215e-01,  ..., -9.4795e-04,
          3.2501e-03, -2.3483e-02],
        ...,
        [-1.1139e-03, -7.7248e-03,  1.3397e-02,  ...,  4.9023e-01,
          1.5152e-02, -2.9404e-02],
        [ 5.9662e-03,  2.4780e-02, -4.9683e-02,  ...,  1.6281e-02,
          4.1724e-01,  1.3565e-02],
        [ 2.1851e-02,  1.3878e-02, -5.8975e-03,  ...,  1.9745e-02,
         -2.3785e-03,  4.1650e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5742,  3.9316,  3.6465,  ..., -0.6807, -1.5664, -7.4062]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 05:57:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for healthy unhealthy
affected unaffected
expected unexpected
interrupted uninterrupted
noticed unnoticed
successful unsuccessful
pleasant unpleasant
available
2024-07-16 05:57:26 root INFO     [order_1_approx] starting weight calculation for healthy unhealthy
available unavailable
successful unsuccessful
interrupted uninterrupted
noticed unnoticed
pleasant unpleasant
expected unexpected
affected
2024-07-16 05:57:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 06:01:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7847, -0.4412,  0.2290,  ...,  1.0068, -0.3843,  0.7939],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4561,  0.7324,  0.1953,  ...,  0.1484,  6.4414,  1.0088],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4165,  0.0224,  0.0250,  ..., -0.0283,  0.0188, -0.0341],
        [ 0.0138,  0.3704, -0.0339,  ..., -0.0285,  0.0270, -0.0022],
        [ 0.0202,  0.0278,  0.4468,  ..., -0.0135, -0.0243, -0.0264],
        ...,
        [-0.0037,  0.0147, -0.0257,  ...,  0.4536,  0.0366,  0.0039],
        [ 0.0012, -0.0067,  0.0010,  ..., -0.0199,  0.4468,  0.0149],
        [-0.0149,  0.0035,  0.0293,  ..., -0.0064, -0.0008,  0.4016]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[0.8008, 0.7144, 2.2090,  ..., 0.0159, 7.9102, 4.6211]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 06:01:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for healthy unhealthy
available unavailable
successful unsuccessful
interrupted uninterrupted
noticed unnoticed
pleasant unpleasant
expected unexpected
affected
2024-07-16 06:01:33 root INFO     [order_1_approx] starting weight calculation for available unavailable
healthy unhealthy
interrupted uninterrupted
pleasant unpleasant
successful unsuccessful
expected unexpected
affected unaffected
noticed
2024-07-16 06:01:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 06:05:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4553,  0.1697, -0.2500,  ...,  0.0547, -0.1748,  0.2500],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5195, -0.0430,  0.5508,  ...,  2.1055,  0.8965,  2.3770],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4880,  0.0126,  0.0119,  ..., -0.0310,  0.0431, -0.0207],
        [-0.0005,  0.3921, -0.0232,  ...,  0.0442,  0.0151,  0.0208],
        [-0.0230,  0.0086,  0.4678,  ..., -0.0425, -0.0196, -0.0116],
        ...,
        [-0.0662,  0.0436,  0.0565,  ...,  0.5000, -0.0260, -0.0751],
        [ 0.0178,  0.0255, -0.0102,  ..., -0.0159,  0.4551,  0.0412],
        [-0.0143,  0.0098, -0.0065,  ..., -0.0176, -0.0298,  0.4216]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2549,  2.3555,  2.1328,  ...,  1.8887, -1.0352,  1.1064]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 06:05:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for available unavailable
healthy unhealthy
interrupted uninterrupted
pleasant unpleasant
successful unsuccessful
expected unexpected
affected unaffected
noticed
2024-07-16 06:05:40 root INFO     total operator prediction time: 1979.146785736084 seconds
2024-07-16 06:05:40 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on re+verb_reg
2024-07-16 06:05:40 root INFO     building operator re+verb_reg
2024-07-16 06:05:40 root INFO     [order_1_approx] starting weight calculation for distribute redistribute
negotiate renegotiate
grow regrow
arrange rearrange
integrate reintegrate
appear reappear
connect reconnect
invest
2024-07-16 06:05:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 06:09:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-5.5371e-01, -2.2949e-02,  1.4814e+00,  ..., -5.5225e-01,
        -1.2207e-03,  3.8257e-01], device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0176,  3.2500, -1.9199,  ..., -0.7588, -1.4014,  1.9414],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.5073, -0.0051,  0.0295,  ...,  0.0106,  0.0104, -0.0173],
        [ 0.0128,  0.4392,  0.0026,  ...,  0.0116,  0.0053,  0.0297],
        [ 0.0365, -0.0284,  0.5078,  ..., -0.0082,  0.0081, -0.0386],
        ...,
        [ 0.0407, -0.0169,  0.0134,  ...,  0.5015,  0.0239,  0.0089],
        [ 0.0201,  0.0170, -0.0058,  ...,  0.0113,  0.5254,  0.0192],
        [ 0.0231, -0.0294, -0.0401,  ..., -0.0329,  0.0222,  0.4697]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4551,  3.3945, -0.7041,  ...,  3.1211, -3.9102,  3.9219]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 06:09:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for distribute redistribute
negotiate renegotiate
grow regrow
arrange rearrange
integrate reintegrate
appear reappear
connect reconnect
invest
2024-07-16 06:09:47 root INFO     [order_1_approx] starting weight calculation for integrate reintegrate
appear reappear
arrange rearrange
connect reconnect
negotiate renegotiate
invest reinvest
distribute redistribute
grow
2024-07-16 06:09:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 06:13:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2874, -0.1954,  0.8521,  ...,  0.1460,  0.8521, -0.0474],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6787,  0.3198, -2.5352,  ..., -0.6338,  4.1523, -6.1016],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4446, -0.0018,  0.0295,  ...,  0.0143, -0.0153, -0.0058],
        [-0.0165,  0.4048,  0.0075,  ...,  0.0235,  0.0171,  0.0157],
        [ 0.0166,  0.0051,  0.4507,  ..., -0.0077,  0.0155,  0.0054],
        ...,
        [ 0.0014, -0.0606,  0.0008,  ...,  0.4436,  0.0101, -0.0043],
        [ 0.0169, -0.0133,  0.0139,  ...,  0.0266,  0.4224,  0.0087],
        [-0.0103,  0.0014, -0.0375,  ..., -0.0247,  0.0010,  0.3943]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.4023, -0.8599, -3.0254,  ..., -1.2051,  2.7598, -9.0000]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 06:13:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for integrate reintegrate
appear reappear
arrange rearrange
connect reconnect
negotiate renegotiate
invest reinvest
distribute redistribute
grow
2024-07-16 06:13:52 root INFO     [order_1_approx] starting weight calculation for appear reappear
arrange rearrange
distribute redistribute
negotiate renegotiate
grow regrow
invest reinvest
integrate reintegrate
connect
2024-07-16 06:13:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 06:17:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1037, -0.8672, -0.3530,  ...,  0.6699,  1.0605, -0.0645],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.1875,  0.2734, -4.4375,  ...,  2.0508, -2.1777,  1.9746],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.4971e-01, -1.0651e-02,  1.1017e-02,  ..., -3.0823e-03,
          2.1271e-02, -2.0050e-02],
        [-1.0651e-02,  4.1064e-01,  3.5057e-03,  ...,  1.6785e-03,
          3.4180e-03,  1.4236e-02],
        [ 4.0344e-02, -2.0218e-03,  4.5142e-01,  ...,  1.2894e-03,
         -7.5684e-03, -8.9264e-03],
        ...,
        [-2.0081e-02, -2.6840e-02,  6.6757e-03,  ...,  4.4653e-01,
          1.4740e-02, -2.5940e-04],
        [ 2.4521e-02, -3.0991e-02, -3.7384e-02,  ..., -3.5782e-03,
          4.4043e-01,  2.2064e-02],
        [ 1.3992e-02,  9.4452e-03, -2.5818e-02,  ..., -1.3199e-02,
         -1.0719e-02,  4.3604e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.9336, -0.4546, -3.5430,  ...,  1.4062, -1.6426,  0.7686]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 06:17:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for appear reappear
arrange rearrange
distribute redistribute
negotiate renegotiate
grow regrow
invest reinvest
integrate reintegrate
connect
2024-07-16 06:17:59 root INFO     [order_1_approx] starting weight calculation for connect reconnect
grow regrow
distribute redistribute
arrange rearrange
negotiate renegotiate
appear reappear
invest reinvest
integrate
2024-07-16 06:17:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 06:22:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4419, -0.2437,  0.2861,  ...,  0.3269, -0.1455,  0.0862],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.9844, -0.2588, -2.4648,  ...,  5.2305,  0.3105,  2.5703],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.9390e-01,  7.2632e-03,  4.7028e-02,  ...,  4.0817e-04,
         -7.2021e-03, -3.1776e-03],
        [-3.3386e-02,  3.5742e-01, -4.8218e-03,  ...,  7.0419e-03,
         -4.5288e-02, -2.7504e-03],
        [-6.4468e-04,  1.9775e-02,  4.0601e-01,  ...,  2.1973e-03,
         -2.2095e-02, -4.7638e-02],
        ...,
        [-4.1687e-02,  2.7332e-03, -4.2496e-03,  ...,  4.4678e-01,
          1.2817e-03,  2.3651e-02],
        [-2.8214e-02,  2.7954e-02, -1.8845e-02,  ...,  1.5266e-02,
          3.9380e-01,  3.0396e-02],
        [-4.6921e-03, -6.7596e-03, -1.2009e-02,  ..., -2.3926e-02,
          3.2349e-02,  4.4385e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.3516, -0.5083, -2.3906,  ...,  5.7227, -0.4990,  3.3613]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 06:22:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for connect reconnect
grow regrow
distribute redistribute
arrange rearrange
negotiate renegotiate
appear reappear
invest reinvest
integrate
2024-07-16 06:22:05 root INFO     [order_1_approx] starting weight calculation for negotiate renegotiate
invest reinvest
appear reappear
grow regrow
distribute redistribute
integrate reintegrate
connect reconnect
arrange
2024-07-16 06:22:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 06:26:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2864,  0.3120, -0.6714,  ..., -0.3120,  0.9048, -0.0060],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.5859,  0.0488, -3.2559,  ...,  1.3105,  3.3359,  1.2744],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.9756e-01,  2.1179e-02, -9.1553e-05,  ...,  7.7820e-03,
         -1.0712e-02, -5.0507e-03],
        [-4.5929e-02,  3.6841e-01,  7.2632e-03,  ...,  5.6763e-03,
         -1.2016e-02,  1.2833e-02],
        [ 4.8279e-02,  1.8631e-02,  4.4775e-01,  ...,  2.9572e-02,
         -4.3831e-03, -2.7802e-02],
        ...,
        [-4.8767e-02, -5.6488e-02,  2.3834e-02,  ...,  4.2676e-01,
          1.0986e-02,  2.2774e-03],
        [ 1.4229e-02,  4.1016e-02,  1.8677e-02,  ..., -2.9144e-02,
          4.0723e-01,  5.2185e-03],
        [ 1.4877e-02, -1.8250e-02, -1.4725e-02,  ..., -4.7760e-03,
          3.6163e-03,  4.2944e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-6.3984, -0.1677, -3.0742,  ...,  2.4434,  2.6211,  0.9795]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 06:26:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for negotiate renegotiate
invest reinvest
appear reappear
grow regrow
distribute redistribute
integrate reintegrate
connect reconnect
arrange
2024-07-16 06:26:10 root INFO     [order_1_approx] starting weight calculation for connect reconnect
negotiate renegotiate
grow regrow
distribute redistribute
integrate reintegrate
arrange rearrange
invest reinvest
appear
2024-07-16 06:26:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 06:30:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6113,  0.7812,  1.1719,  ...,  0.7695,  0.2695, -0.4185],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9658, -1.9932, -2.9570,  ...,  3.9355, -1.5195, -3.5625],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.8633e-01,  4.0131e-02,  4.0771e-02,  ..., -2.0355e-02,
          2.5543e-02,  1.3819e-03],
        [-8.0566e-03,  4.3481e-01,  6.8665e-05,  ...,  2.6154e-02,
          1.4244e-02,  1.2711e-02],
        [ 5.0262e-02, -4.2572e-03,  5.0098e-01,  ...,  7.1640e-03,
          6.0120e-03,  1.0269e-02],
        ...,
        [-3.2227e-02, -6.3660e-02,  6.8283e-03,  ...,  4.8633e-01,
          1.1932e-02, -2.1610e-03],
        [ 1.1665e-02,  2.6276e-02, -1.1192e-02,  ..., -2.4231e-02,
          4.6094e-01,  5.1498e-03],
        [ 2.1759e-02,  1.3252e-02, -5.1788e-02,  ..., -2.6520e-02,
          1.1612e-02,  4.3604e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4531, -2.7617, -1.4082,  ...,  2.6055, -1.2031, -2.0703]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 06:30:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for connect reconnect
negotiate renegotiate
grow regrow
distribute redistribute
integrate reintegrate
arrange rearrange
invest reinvest
appear
2024-07-16 06:30:16 root INFO     [order_1_approx] starting weight calculation for appear reappear
connect reconnect
integrate reintegrate
arrange rearrange
negotiate renegotiate
grow regrow
invest reinvest
distribute
2024-07-16 06:30:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 06:34:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2900, -0.1134,  0.8418,  ...,  0.4341,  0.2212, -0.2520],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.6797,  2.1992, -5.3828,  ...,  0.0547, -0.0420, -2.7773],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.4092e-01, -3.6659e-03,  1.4618e-02,  ...,  1.6418e-02,
          9.7198e-03,  2.7802e-02],
        [-1.8723e-02,  3.7622e-01, -4.7684e-03,  ..., -2.4872e-02,
          1.2283e-03, -8.5754e-03],
        [-2.2278e-03,  2.4231e-02,  4.6777e-01,  ...,  4.1748e-02,
         -1.6266e-02, -1.1444e-03],
        ...,
        [-3.0945e-02, -1.8463e-02,  3.8727e-02,  ...,  4.6948e-01,
          5.7220e-03,  1.3855e-02],
        [ 3.0426e-02, -7.1030e-03, -8.4686e-03,  ..., -1.5259e-04,
          4.5288e-01,  4.0817e-03],
        [ 5.0278e-03, -6.3324e-04, -4.5349e-02,  ..., -2.2400e-02,
          3.1555e-02,  4.3359e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.5547,  0.8633, -4.7930,  ..., -0.5273, -0.4827,  0.0449]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 06:34:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for appear reappear
connect reconnect
integrate reintegrate
arrange rearrange
negotiate renegotiate
grow regrow
invest reinvest
distribute
2024-07-16 06:34:18 root INFO     [order_1_approx] starting weight calculation for invest reinvest
distribute redistribute
arrange rearrange
integrate reintegrate
appear reappear
connect reconnect
grow regrow
negotiate
2024-07-16 06:34:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 06:38:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2473, -0.8530,  0.0447,  ..., -0.2866,  0.4036,  0.2981],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2350, -2.6562, -6.9141,  ..., -1.8066,  1.7012,  0.8906],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4504, -0.0017,  0.0300,  ..., -0.0083, -0.0112, -0.0470],
        [ 0.0222,  0.4246,  0.0012,  ...,  0.0079, -0.0344, -0.0053],
        [ 0.0196,  0.0372,  0.4863,  ..., -0.0121,  0.0162, -0.0273],
        ...,
        [-0.0393, -0.0123, -0.0195,  ...,  0.4614,  0.0144, -0.0300],
        [-0.0016,  0.0355,  0.0022,  ...,  0.0432,  0.4182,  0.0216],
        [ 0.0165, -0.0026, -0.0145,  ..., -0.0370, -0.0148,  0.4331]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2275, -2.2910, -6.6367,  ...,  0.3828,  1.8086,  5.4492]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 06:38:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for invest reinvest
distribute redistribute
arrange rearrange
integrate reintegrate
appear reappear
connect reconnect
grow regrow
negotiate
2024-07-16 06:38:23 root INFO     total operator prediction time: 1962.7748086452484 seconds
2024-07-16 06:38:23 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on adj+ness_reg
2024-07-16 06:38:23 root INFO     building operator adj+ness_reg
2024-07-16 06:38:23 root INFO     [order_1_approx] starting weight calculation for happy happiness
hidden hiddenness
broken brokenness
amazing amazingness
same sameness
strange strangeness
fixed fixedness
unique
2024-07-16 06:38:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 06:42:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4141,  0.3213, -0.0332,  ..., -0.2705,  0.5806, -0.0737],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.7812, -1.1982,  0.3633,  ...,  0.4697,  2.5371,  5.1875],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.9795e-01, -7.3395e-03,  4.0161e-02,  ..., -1.0651e-02,
         -1.7105e-02, -1.5091e-02],
        [-1.9547e-02,  3.7280e-01, -4.1595e-02,  ...,  2.2316e-03,
          4.0710e-02,  5.9814e-03],
        [-1.5640e-03,  1.1772e-02,  4.3848e-01,  ..., -1.7227e-02,
         -3.2288e-02, -4.5776e-05],
        ...,
        [-1.7334e-02, -9.4604e-03,  3.1006e-02,  ...,  4.4336e-01,
         -8.1863e-03, -3.0579e-02],
        [-1.3992e-02,  1.1444e-02,  5.9204e-03,  ..., -3.3508e-02,
          3.9697e-01, -1.9867e-02],
        [ 4.8065e-03,  1.9226e-02, -1.3138e-02,  ..., -8.4229e-03,
         -1.9150e-03,  4.1699e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7441, -1.1982,  1.4307,  ...,  0.4214,  0.6514,  5.5000]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 06:42:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for happy happiness
hidden hiddenness
broken brokenness
amazing amazingness
same sameness
strange strangeness
fixed fixedness
unique
2024-07-16 06:42:30 root INFO     [order_1_approx] starting weight calculation for hidden hiddenness
strange strangeness
fixed fixedness
same sameness
unique uniqueness
broken brokenness
amazing amazingness
happy
2024-07-16 06:42:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 06:46:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4722,  0.5200, -0.2203,  ...,  0.5342,  0.3198, -0.5078],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5703, -0.9561, -2.3438,  ..., -0.1299,  5.1484,  1.4414],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4141, -0.0009,  0.0447,  ..., -0.0088,  0.0143, -0.0311],
        [-0.0228,  0.3828, -0.0155,  ...,  0.0471,  0.0182,  0.0015],
        [ 0.0085,  0.0127,  0.4268,  ...,  0.0017,  0.0132,  0.0210],
        ...,
        [-0.0282,  0.0022,  0.0327,  ...,  0.4543, -0.0159,  0.0207],
        [ 0.0255,  0.0049,  0.0430,  ..., -0.0390,  0.4214,  0.0072],
        [ 0.0016,  0.0018, -0.0046,  ...,  0.0173,  0.0331,  0.3704]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3398, -2.8555, -0.5381,  ..., -0.9941,  5.5078,  1.0752]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 06:46:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for hidden hiddenness
strange strangeness
fixed fixedness
same sameness
unique uniqueness
broken brokenness
amazing amazingness
happy
2024-07-16 06:46:37 root INFO     [order_1_approx] starting weight calculation for strange strangeness
unique uniqueness
broken brokenness
happy happiness
amazing amazingness
hidden hiddenness
same sameness
fixed
2024-07-16 06:46:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 06:50:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0591,  0.7134, -0.4148,  ..., -0.1023, -0.0605, -0.2129],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7773, -1.3838, -0.1797,  ..., -1.1436, -1.1895,  4.3750],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.0991e-01,  1.1368e-03,  2.2888e-02,  ...,  6.0349e-03,
          2.4567e-03, -1.4870e-02],
        [-1.8295e-02,  4.0283e-01, -1.7319e-02,  ...,  2.5055e-02,
          6.0997e-03,  1.2619e-02],
        [ 1.9638e-02,  1.9745e-02,  4.4678e-01,  ..., -1.7586e-03,
         -5.7907e-03, -3.9673e-04],
        ...,
        [ 6.2943e-03, -5.7220e-04,  3.5431e-02,  ...,  4.6997e-01,
          1.3992e-02, -4.0771e-02],
        [-1.1154e-02,  9.2087e-03,  3.7872e-02,  ..., -2.7313e-02,
          4.1772e-01, -1.5617e-02],
        [ 2.8473e-02,  7.8506e-03, -1.5472e-02,  ..., -1.0895e-02,
          1.0826e-02,  4.2480e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.4785, -1.2686, -0.3838,  ..., -1.8936, -2.2031,  5.4844]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 06:50:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for strange strangeness
unique uniqueness
broken brokenness
happy happiness
amazing amazingness
hidden hiddenness
same sameness
fixed
2024-07-16 06:50:44 root INFO     [order_1_approx] starting weight calculation for hidden hiddenness
strange strangeness
unique uniqueness
fixed fixedness
broken brokenness
amazing amazingness
happy happiness
same
2024-07-16 06:50:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 06:54:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2365,  1.1904,  0.4312,  ..., -0.1335,  0.3989, -0.3398],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.4570, -0.5068,  0.8232,  ..., -2.4883,  0.2090,  4.6289],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.2993e-01, -2.0294e-02,  1.6159e-02,  ...,  5.2185e-03,
         -2.4185e-03, -2.2263e-02],
        [-2.9236e-02,  4.2383e-01,  9.6512e-03,  ...,  1.9073e-04,
         -2.7191e-02,  1.2360e-02],
        [ 9.7656e-03,  1.6418e-02,  4.3359e-01,  ..., -7.6637e-03,
         -8.8043e-03,  1.4458e-02],
        ...,
        [ 3.5950e-02, -1.8753e-02,  1.2833e-02,  ...,  4.7681e-01,
          2.0126e-02, -1.6670e-03],
        [ 8.5907e-03, -2.1362e-02,  3.0212e-03,  ..., -5.2643e-03,
          4.2383e-01, -1.1948e-02],
        [ 3.5065e-02,  2.1011e-02, -2.2079e-02,  ..., -1.7792e-02,
         -3.3752e-02,  4.4531e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.7852, -1.4365,  0.5308,  ..., -1.8867,  0.2313,  2.4902]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 06:54:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for hidden hiddenness
strange strangeness
unique uniqueness
fixed fixedness
broken brokenness
amazing amazingness
happy happiness
same
2024-07-16 06:54:52 root INFO     [order_1_approx] starting weight calculation for broken brokenness
same sameness
happy happiness
strange strangeness
fixed fixedness
amazing amazingness
unique uniqueness
hidden
2024-07-16 06:54:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 06:58:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.9434, -0.4712, -0.2417,  ...,  0.6196,  0.3018,  0.2145],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.9688, -0.8599,  1.4375,  ..., -3.2305, -0.2881,  3.8594],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4453,  0.0193,  0.0339,  ..., -0.0134, -0.0293, -0.0231],
        [-0.0335,  0.4121,  0.0106,  ...,  0.0040,  0.0076,  0.0250],
        [ 0.0358,  0.0127,  0.4875,  ...,  0.0282, -0.0054,  0.0207],
        ...,
        [-0.0260, -0.0112,  0.0154,  ...,  0.4785, -0.0081,  0.0161],
        [ 0.0275,  0.0238, -0.0210,  ..., -0.0317,  0.4585, -0.0285],
        [ 0.0080,  0.0207, -0.0234,  ..., -0.0514,  0.0256,  0.4512]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7637, -0.7271,  1.3096,  ..., -3.2812,  0.5518,  3.7285]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 06:59:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for broken brokenness
same sameness
happy happiness
strange strangeness
fixed fixedness
amazing amazingness
unique uniqueness
hidden
2024-07-16 06:59:00 root INFO     [order_1_approx] starting weight calculation for unique uniqueness
same sameness
broken brokenness
hidden hiddenness
strange strangeness
fixed fixedness
happy happiness
amazing
2024-07-16 06:59:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 07:03:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3813, -0.4697,  0.5376,  ...,  0.9424,  0.6108, -0.0651],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 6.7734,  0.2671, -0.1025,  ..., -3.2520,  0.6714,  5.2891],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.7388e-01, -2.0157e-02,  1.8127e-02,  ..., -1.0551e-02,
          1.1475e-02, -2.2278e-02],
        [ 3.3661e-02,  4.1211e-01,  1.2444e-02,  ..., -2.0180e-03,
         -1.0223e-02, -2.0996e-02],
        [ 3.3684e-03,  1.0269e-02,  4.3799e-01,  ..., -1.0529e-03,
          1.2054e-02, -3.4180e-02],
        ...,
        [-2.8992e-02,  2.2003e-02,  2.5299e-02,  ...,  4.7314e-01,
         -3.4912e-02,  1.4450e-02],
        [ 4.7760e-02, -5.3215e-04, -1.3290e-02,  ..., -3.8666e-02,
          4.2041e-01,  5.5542e-03],
        [ 2.1164e-02,  7.9041e-03, -2.5925e-02,  ..., -2.7466e-04,
         -1.8616e-02,  4.3750e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.8945, -0.8003, -1.1494,  ..., -2.9512,  3.1816,  3.6348]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 07:03:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for unique uniqueness
same sameness
broken brokenness
hidden hiddenness
strange strangeness
fixed fixedness
happy happiness
amazing
2024-07-16 07:03:07 root INFO     [order_1_approx] starting weight calculation for happy happiness
same sameness
fixed fixedness
hidden hiddenness
unique uniqueness
broken brokenness
amazing amazingness
strange
2024-07-16 07:03:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 07:07:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3987, -0.1324, -0.7959,  ...,  0.0845,  0.2515, -0.7559],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3203, -3.7695, -3.2812,  ...,  0.8384,  0.3628,  5.6406],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4629, -0.0073, -0.0072,  ..., -0.0259,  0.0231, -0.0014],
        [ 0.0439,  0.4355,  0.0007,  ..., -0.0172,  0.0288,  0.0164],
        [-0.0032,  0.0391,  0.4780,  ..., -0.0202, -0.0073, -0.0033],
        ...,
        [-0.0015,  0.0176,  0.0496,  ...,  0.4680, -0.0195, -0.0463],
        [-0.0059,  0.0058, -0.0107,  ..., -0.0201,  0.4355,  0.0072],
        [ 0.0286, -0.0025,  0.0049,  ..., -0.0154, -0.0154,  0.4512]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0264, -2.4785, -2.2305,  ...,  2.5020, -1.4707,  5.9102]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 07:07:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for happy happiness
same sameness
fixed fixedness
hidden hiddenness
unique uniqueness
broken brokenness
amazing amazingness
strange
2024-07-16 07:07:15 root INFO     [order_1_approx] starting weight calculation for same sameness
fixed fixedness
happy happiness
strange strangeness
unique uniqueness
hidden hiddenness
amazing amazingness
broken
2024-07-16 07:07:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 07:11:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.7754, -0.3516,  0.0391,  ...,  0.6509,  0.1138, -1.0918],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4683, -0.2520, -5.8867,  ..., -2.8496, -1.2168,  2.9414],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4448, -0.0108,  0.0190,  ..., -0.0101, -0.0235, -0.0368],
        [-0.0335,  0.3857, -0.0063,  ...,  0.0005, -0.0055, -0.0005],
        [ 0.0065, -0.0041,  0.4231,  ..., -0.0174,  0.0027,  0.0083],
        ...,
        [-0.0115, -0.0065, -0.0312,  ...,  0.3992,  0.0013, -0.0246],
        [ 0.0267,  0.0219, -0.0096,  ..., -0.0093,  0.4087, -0.0013],
        [ 0.0149,  0.0124, -0.0103,  ..., -0.0416, -0.0008,  0.3987]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3477, -1.5234, -6.8633,  ..., -1.5361, -1.4092,  3.9141]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 07:11:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for same sameness
fixed fixedness
happy happiness
strange strangeness
unique uniqueness
hidden hiddenness
amazing amazingness
broken
2024-07-16 07:11:22 root INFO     total operator prediction time: 1978.8569452762604 seconds
2024-07-16 07:11:22 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on noun+less_reg
2024-07-16 07:11:22 root INFO     building operator noun+less_reg
2024-07-16 07:11:22 root INFO     [order_1_approx] starting weight calculation for talent talentless
path pathless
friction frictionless
sensor sensorless
god godless
error errorless
carbon carbonless
ego
2024-07-16 07:11:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 07:15:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0635, -0.0730,  0.7119,  ..., -1.3662,  0.7471,  0.5161],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.0156, -1.9541,  3.2617,  ..., -1.3633,  1.1133, -1.0879],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4895,  0.0265, -0.0048,  ..., -0.0049,  0.0420, -0.0043],
        [ 0.0196,  0.4175,  0.0278,  ...,  0.0326, -0.0028, -0.0044],
        [-0.0041, -0.0237,  0.5181,  ...,  0.0579, -0.0100, -0.0394],
        ...,
        [-0.0369, -0.0057,  0.0080,  ...,  0.5361, -0.0374, -0.0420],
        [-0.0402,  0.0308, -0.0399,  ..., -0.0377,  0.4849,  0.0310],
        [ 0.0096, -0.0296,  0.0153,  ..., -0.0565, -0.0294,  0.5259]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5098, -3.1562,  2.3945,  ..., -0.1924,  1.7891, -2.0449]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 07:15:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for talent talentless
path pathless
friction frictionless
sensor sensorless
god godless
error errorless
carbon carbonless
ego
2024-07-16 07:15:28 root INFO     [order_1_approx] starting weight calculation for talent talentless
god godless
sensor sensorless
error errorless
carbon carbonless
friction frictionless
ego egoless
path
2024-07-16 07:15:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 07:19:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1414, -0.1970,  0.1938,  ...,  0.5957,  0.4272, -0.3350],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.2070, -5.6484, -0.6387,  ..., -2.8613,  2.7695, -0.6406],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.5532e-01,  1.5213e-02,  2.1942e-02,  ..., -1.6281e-02,
          2.0409e-04, -1.0052e-03],
        [-4.5395e-03,  4.0771e-01, -1.0971e-02,  ..., -1.7242e-03,
          2.0721e-02,  2.0447e-02],
        [ 2.8625e-02, -6.2065e-03,  4.5239e-01,  ..., -3.0472e-02,
          5.9509e-04, -1.1963e-02],
        ...,
        [ 1.6769e-02,  2.5299e-02,  1.8112e-02,  ...,  4.3311e-01,
         -1.7414e-03,  2.0508e-02],
        [ 7.3166e-03,  4.0039e-02,  7.7209e-03,  ..., -2.4460e-02,
          4.4141e-01, -3.8147e-03],
        [ 1.8143e-02,  1.9394e-02, -1.6510e-02,  ..., -1.1948e-02,
         -4.5776e-03,  4.1748e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1523, -6.3750, -0.2422,  ..., -1.8311,  2.7344, -1.3398]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 07:19:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for talent talentless
god godless
sensor sensorless
error errorless
carbon carbonless
friction frictionless
ego egoless
path
2024-07-16 07:19:33 root INFO     [order_1_approx] starting weight calculation for sensor sensorless
path pathless
carbon carbonless
god godless
ego egoless
error errorless
talent talentless
friction
2024-07-16 07:19:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 07:23:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9575,  0.1456, -0.6836,  ...,  0.3037,  0.1440,  1.0400],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-8.7578, -2.8438,  1.7266,  ..., -4.0703,  0.0986,  0.8652],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4617,  0.0722,  0.0076,  ..., -0.0452,  0.0012, -0.0151],
        [ 0.0030,  0.4158, -0.0052,  ..., -0.0033, -0.0237,  0.0094],
        [ 0.0085,  0.0044,  0.5356,  ...,  0.0164, -0.0121,  0.0133],
        ...,
        [ 0.0392, -0.0327,  0.0229,  ...,  0.5288,  0.0014, -0.0425],
        [ 0.0014, -0.0061, -0.0113,  ..., -0.0097,  0.4907,  0.0389],
        [ 0.0183,  0.0409, -0.0442,  ..., -0.0533,  0.0249,  0.4673]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-9.3359, -5.1172,  1.3164,  ..., -3.3633,  0.0937,  0.7974]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 07:23:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for sensor sensorless
path pathless
carbon carbonless
god godless
ego egoless
error errorless
talent talentless
friction
2024-07-16 07:23:39 root INFO     [order_1_approx] starting weight calculation for carbon carbonless
talent talentless
error errorless
sensor sensorless
path pathless
ego egoless
friction frictionless
god
2024-07-16 07:23:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 07:27:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0027,  0.2625,  0.2153,  ..., -0.1957, -0.4409, -1.1973],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.3711,  0.4434, -2.4219,  ..., -2.5410,  0.0377, -2.1270],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.5361e-01, -1.3542e-04, -9.7885e-03,  ..., -1.5160e-02,
          2.5909e-02, -6.6910e-03],
        [-3.7354e-02,  4.1870e-01,  6.0959e-03,  ...,  1.8616e-02,
         -1.1566e-02, -8.7357e-04],
        [ 1.1292e-03,  2.1484e-02,  4.5312e-01,  ...,  1.3237e-02,
         -3.1189e-02,  9.4070e-03],
        ...,
        [-2.1255e-02, -2.2736e-02,  3.4332e-02,  ...,  4.4287e-01,
         -3.8528e-03,  2.0218e-04],
        [ 3.6645e-04,  3.8361e-02, -2.7252e-02,  ...,  2.5921e-03,
          4.2480e-01, -2.4307e-02],
        [ 2.4338e-02, -1.2589e-04, -2.5467e-02,  ..., -1.1520e-02,
         -1.5297e-02,  4.2847e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2324,  1.0918, -2.3770,  ..., -1.2598, -0.8442, -0.1309]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 07:27:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for carbon carbonless
talent talentless
error errorless
sensor sensorless
path pathless
ego egoless
friction frictionless
god
2024-07-16 07:27:44 root INFO     [order_1_approx] starting weight calculation for talent talentless
path pathless
ego egoless
god godless
friction frictionless
sensor sensorless
error errorless
carbon
2024-07-16 07:27:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 07:31:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4602,  0.5405, -0.5483,  ...,  0.1780,  0.2256, -0.0788],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7559, -2.7852, -1.5273,  ..., -6.0547, -2.0273, -0.7900],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4727,  0.0015,  0.0007,  ...,  0.0098, -0.0036, -0.0238],
        [-0.0107,  0.4226,  0.0090,  ...,  0.0075, -0.0237, -0.0135],
        [ 0.0399,  0.0157,  0.4944,  ..., -0.0059,  0.0023, -0.0259],
        ...,
        [-0.0281, -0.0172, -0.0219,  ...,  0.4973,  0.0036, -0.0342],
        [ 0.0139,  0.0117, -0.0384,  ..., -0.0247,  0.4622,  0.0041],
        [ 0.0039, -0.0278, -0.0396,  ...,  0.0133,  0.0217,  0.4575]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1260, -2.4629, -2.3594,  ..., -4.9062, -2.3301, -1.2695]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 07:31:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for talent talentless
path pathless
ego egoless
god godless
friction frictionless
sensor sensorless
error errorless
carbon
2024-07-16 07:31:49 root INFO     [order_1_approx] starting weight calculation for carbon carbonless
god godless
ego egoless
path pathless
friction frictionless
talent talentless
sensor sensorless
error
2024-07-16 07:31:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 07:35:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7764,  0.1603,  0.3315,  ..., -0.2324, -0.7080, -0.0270],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4087, -2.3672,  5.2383,  ..., -4.7539, -1.5049,  3.1719],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4580,  0.0136,  0.0033,  ..., -0.0218,  0.0238, -0.0389],
        [-0.0080,  0.3733,  0.0124,  ...,  0.0134,  0.0065,  0.0164],
        [-0.0047,  0.0035,  0.4375,  ...,  0.0322,  0.0165,  0.0118],
        ...,
        [-0.0235,  0.0103, -0.0120,  ...,  0.4863, -0.0242, -0.0194],
        [ 0.0259,  0.0098, -0.0117,  ...,  0.0161,  0.4221,  0.0136],
        [ 0.0166, -0.0013, -0.0041,  ..., -0.0346, -0.0587,  0.4417]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5840, -2.2930,  3.6914,  ..., -2.6152, -1.3213, -0.5293]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 07:35:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for carbon carbonless
god godless
ego egoless
path pathless
friction frictionless
talent talentless
sensor sensorless
error
2024-07-16 07:35:54 root INFO     [order_1_approx] starting weight calculation for path pathless
sensor sensorless
error errorless
ego egoless
god godless
friction frictionless
carbon carbonless
talent
2024-07-16 07:35:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 07:39:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8413, -0.9170,  1.1465,  ..., -0.5850,  0.4954, -0.2986],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6006, -2.3281,  3.0801,  ..., -1.6621, -0.3643,  4.0859],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.5898e-01, -3.5477e-04,  1.1185e-02,  ..., -1.4252e-02,
          8.6288e-03, -1.0170e-02],
        [ 2.4460e-02,  4.3848e-01, -2.4323e-02,  ...,  1.4130e-02,
          3.7598e-02, -1.4496e-02],
        [ 4.1016e-02, -3.8574e-02,  4.8657e-01,  ...,  3.8666e-02,
          3.6865e-02,  1.6724e-02],
        ...,
        [-1.0178e-02, -3.4424e-02,  2.8839e-02,  ...,  5.0439e-01,
         -1.9592e-02, -1.1513e-02],
        [ 1.3504e-02,  3.8330e-02, -3.0518e-04,  ..., -2.6947e-02,
          4.5972e-01,  3.1586e-02],
        [ 7.8659e-03,  1.6968e-02, -3.3569e-02,  ..., -8.6365e-03,
         -1.4580e-02,  4.7192e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.8867, -3.4414,  0.1406,  ..., -0.7856, -0.5156,  2.6797]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 07:39:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for path pathless
sensor sensorless
error errorless
ego egoless
god godless
friction frictionless
carbon carbonless
talent
2024-07-16 07:40:00 root INFO     [order_1_approx] starting weight calculation for god godless
ego egoless
carbon carbonless
path pathless
error errorless
friction frictionless
talent talentless
sensor
2024-07-16 07:40:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 07:44:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4678,  0.1556,  0.3755,  ..., -0.2805,  0.7734,  0.1821],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1084, -2.5938,  4.1992,  ..., -4.3555, -3.6543,  2.4883],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.5288e-01,  2.2614e-02, -3.0518e-04,  ..., -1.1688e-02,
          4.6112e-02, -5.2109e-03],
        [ 2.4200e-02,  3.9697e-01,  1.1787e-02,  ...,  2.2842e-02,
          9.3384e-03, -1.3809e-02],
        [ 5.5237e-03, -4.6997e-03,  4.6924e-01,  ...,  3.3264e-02,
         -7.2479e-04, -1.0269e-02],
        ...,
        [-7.2021e-03, -1.9913e-02,  3.1281e-03,  ...,  4.8999e-01,
         -4.5395e-03, -1.0422e-02],
        [ 9.5673e-03, -3.0422e-03,  1.2405e-02,  ..., -4.6883e-03,
          4.5093e-01,  2.8503e-02],
        [-1.8082e-03,  2.7390e-03, -1.1436e-02,  ..., -2.5444e-03,
          4.0359e-03,  4.4873e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1309, -3.6094,  3.5469,  ..., -4.1914, -2.9160,  2.0742]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 07:44:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for god godless
ego egoless
carbon carbonless
path pathless
error errorless
friction frictionless
talent talentless
sensor
2024-07-16 07:44:05 root INFO     total operator prediction time: 1963.395997285843 seconds
2024-07-16 07:44:05 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+ment_irreg
2024-07-16 07:44:05 root INFO     building operator verb+ment_irreg
2024-07-16 07:44:05 root INFO     [order_1_approx] starting weight calculation for equip equipment
manage management
replace replacement
align alignment
displace displacement
impair impairment
involve involvement
enlighten
2024-07-16 07:44:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 07:48:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5649,  0.0383,  0.8042,  ..., -0.4458,  0.3479,  0.3682],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7549,  3.2891, -1.1426,  ..., -0.4131,  0.3994,  3.0352],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.4897e-01,  1.5579e-02, -8.6594e-03,  ...,  2.1179e-02,
          1.9958e-02, -2.0386e-02],
        [ 1.2596e-02,  4.0405e-01,  6.5079e-03,  ..., -4.3030e-03,
         -3.6682e-02, -1.3824e-02],
        [-1.2321e-03, -9.3536e-03,  4.1748e-01,  ..., -4.0131e-03,
          1.8051e-02, -2.0660e-02],
        ...,
        [-1.7715e-02,  2.4033e-04,  2.3407e-02,  ...,  4.1504e-01,
         -4.0771e-02,  1.7929e-02],
        [ 2.4353e-02,  1.3626e-02, -9.3155e-03,  ..., -2.2949e-02,
          3.8208e-01,  7.5836e-03],
        [ 3.2990e-02,  2.5806e-03, -1.6418e-02,  ..., -4.7760e-02,
          3.3936e-02,  4.0063e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6914,  0.7051,  1.3633,  ...,  0.0767,  1.0557,  3.8027]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 07:48:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for equip equipment
manage management
replace replacement
align alignment
displace displacement
impair impairment
involve involvement
enlighten
2024-07-16 07:48:13 root INFO     [order_1_approx] starting weight calculation for displace displacement
enlighten enlightenment
manage management
align alignment
impair impairment
equip equipment
replace replacement
involve
2024-07-16 07:48:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 07:52:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9639, -0.5459,  0.4751,  ..., -0.1721,  0.3152,  1.7275],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1025,  0.0322, -3.3867,  ...,  0.2472,  4.6406,  6.3906],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4651,  0.0119,  0.0448,  ...,  0.0033,  0.0188, -0.0110],
        [ 0.0254,  0.4514,  0.0074,  ...,  0.0453, -0.0145, -0.0071],
        [ 0.0377,  0.0158,  0.4888,  ..., -0.0086,  0.0343, -0.0459],
        ...,
        [-0.0099,  0.0139,  0.0115,  ...,  0.4983,  0.0137, -0.0479],
        [ 0.0101,  0.0256, -0.0267,  ..., -0.0063,  0.4150,  0.0676],
        [ 0.0439,  0.0157, -0.0347,  ..., -0.0589,  0.0445,  0.4619]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.8398, -0.5054, -2.5742,  ...,  1.3779,  3.9336,  5.2227]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 07:52:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for displace displacement
enlighten enlightenment
manage management
align alignment
impair impairment
equip equipment
replace replacement
involve
2024-07-16 07:52:20 root INFO     [order_1_approx] starting weight calculation for equip equipment
impair impairment
replace replacement
involve involvement
align alignment
enlighten enlightenment
displace displacement
manage
2024-07-16 07:52:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 07:56:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4700,  0.4207, -1.0254,  ..., -1.0215,  0.6401,  0.0144],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7461, -0.5210, -5.2891,  ...,  0.5747,  3.3633,  4.8555],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4612,  0.0249, -0.0034,  ...,  0.0094,  0.0208, -0.0350],
        [ 0.0358,  0.3823, -0.0045,  ...,  0.0104,  0.0072, -0.0141],
        [ 0.0278, -0.0028,  0.4507,  ..., -0.0117, -0.0198, -0.0179],
        ...,
        [ 0.0140,  0.0256,  0.0061,  ...,  0.4568, -0.0058, -0.0041],
        [ 0.0206,  0.0080,  0.0228,  ...,  0.0228,  0.4595,  0.0075],
        [ 0.0798,  0.0026, -0.0145,  ...,  0.0130,  0.0128,  0.4185]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7207, -0.6509, -5.8672,  ...,  3.7793,  2.9375,  7.8984]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 07:56:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for equip equipment
impair impairment
replace replacement
involve involvement
align alignment
enlighten enlightenment
displace displacement
manage
2024-07-16 07:56:28 root INFO     [order_1_approx] starting weight calculation for replace replacement
displace displacement
enlighten enlightenment
involve involvement
impair impairment
align alignment
manage management
equip
2024-07-16 07:56:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 08:00:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7568,  0.3384,  1.2148,  ..., -0.5889, -0.0381,  0.2883],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7500,  5.2070,  0.1045,  ...,  1.4141, -1.1055,  5.9492],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.8047e-01, -6.5804e-04,  5.7869e-03,  ..., -2.1072e-02,
          3.9398e-02, -1.2360e-02],
        [ 2.2049e-03,  4.4702e-01,  2.9831e-03,  ...,  5.5466e-03,
          1.6769e-02, -1.8097e-02],
        [ 4.2267e-02,  4.3411e-03,  4.8218e-01,  ...,  1.6968e-02,
         -2.0294e-02, -1.8707e-02],
        ...,
        [-2.7054e-02,  2.6230e-02, -1.6594e-04,  ...,  4.7168e-01,
          1.0925e-02,  2.3346e-02],
        [ 1.7761e-02,  2.6962e-02,  7.6637e-03,  ..., -2.7618e-03,
          4.4775e-01,  1.0262e-03],
        [ 6.8283e-03,  2.5803e-02, -2.6459e-02,  ..., -3.2654e-02,
          9.0179e-03,  4.8291e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5322,  5.4297,  0.4221,  ...,  1.5771, -1.7998,  6.0586]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 08:00:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for replace replacement
displace displacement
enlighten enlightenment
involve involvement
impair impairment
align alignment
manage management
equip
2024-07-16 08:00:35 root INFO     [order_1_approx] starting weight calculation for involve involvement
displace displacement
manage management
align alignment
enlighten enlightenment
replace replacement
equip equipment
impair
2024-07-16 08:00:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 08:04:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1239, -1.1191, -0.0332,  ..., -0.5649,  0.3472,  0.0538],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3076,  0.8140, -5.1484,  ..., -0.3931,  3.9297,  2.4141],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.4995e-01,  2.6413e-02,  9.6588e-03,  ...,  3.5400e-03,
          2.3468e-02, -2.1133e-02],
        [-2.5818e-02,  4.2163e-01,  1.6663e-02,  ...,  4.7119e-02,
         -1.0796e-02,  3.6896e-02],
        [ 4.8615e-02,  1.3535e-02,  4.6704e-01,  ...,  2.0447e-03,
          1.3237e-02, -2.7664e-02],
        ...,
        [ 1.6113e-02, -1.9501e-02,  3.8719e-04,  ...,  4.8389e-01,
          2.0905e-03,  2.5696e-02],
        [ 2.6688e-02,  1.4420e-03, -3.8147e-04,  ...,  4.7302e-03,
          4.3530e-01,  3.9612e-02],
        [ 3.3691e-02,  1.9196e-02, -2.3361e-02,  ..., -9.3079e-03,
          1.3596e-02,  4.9072e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8970,  2.3809, -2.9688,  ..., -2.0430,  4.9688,  3.5352]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 08:04:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for involve involvement
displace displacement
manage management
align alignment
enlighten enlightenment
replace replacement
equip equipment
impair
2024-07-16 08:04:43 root INFO     [order_1_approx] starting weight calculation for enlighten enlightenment
involve involvement
impair impairment
replace replacement
displace displacement
manage management
equip equipment
align
2024-07-16 08:04:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 08:08:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7490, -0.3164,  0.4502,  ..., -0.9941, -0.3279,  0.0533],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1582,  0.7241, -4.0352,  ..., -2.0293,  0.2275,  3.7109],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4346, -0.0029,  0.0071,  ..., -0.0267,  0.0034, -0.0276],
        [-0.0177,  0.4556, -0.0143,  ..., -0.0018, -0.0051, -0.0086],
        [ 0.0066,  0.0019,  0.4585,  ..., -0.0072, -0.0225, -0.0233],
        ...,
        [-0.0207,  0.0469,  0.0074,  ...,  0.4888,  0.0258, -0.0189],
        [-0.0198,  0.0015,  0.0141,  ...,  0.0017,  0.4153,  0.0471],
        [-0.0064,  0.0031,  0.0019,  ...,  0.0014,  0.0136,  0.4526]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.2168,  0.5210, -5.1445,  ..., -3.2422,  1.9443,  3.7910]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 08:08:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for enlighten enlightenment
involve involvement
impair impairment
replace replacement
displace displacement
manage management
equip equipment
align
2024-07-16 08:08:50 root INFO     [order_1_approx] starting weight calculation for manage management
impair impairment
replace replacement
involve involvement
align alignment
equip equipment
enlighten enlightenment
displace
2024-07-16 08:08:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 08:12:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7334, -0.1025,  0.7852,  ..., -0.2054, -0.2306,  0.1891],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1125,  0.3835, -2.1250,  ..., -4.1758,  4.3633,  0.9043],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4509,  0.0064,  0.0064,  ..., -0.0011,  0.0295,  0.0031],
        [ 0.0226,  0.4343,  0.0095,  ...,  0.0288, -0.0071,  0.0313],
        [ 0.0352,  0.0140,  0.4634,  ...,  0.0031,  0.0011, -0.0245],
        ...,
        [-0.0359,  0.0069, -0.0107,  ...,  0.4697,  0.0175, -0.0064],
        [ 0.0345,  0.0127, -0.0474,  ...,  0.0373,  0.4553, -0.0107],
        [ 0.0737,  0.0228,  0.0119,  ..., -0.0193,  0.0263,  0.4626]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2959,  0.2954, -0.6016,  ..., -4.0547,  4.5195,  3.6113]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 08:12:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for manage management
impair impairment
replace replacement
involve involvement
align alignment
equip equipment
enlighten enlightenment
displace
2024-07-16 08:12:57 root INFO     [order_1_approx] starting weight calculation for manage management
displace displacement
equip equipment
align alignment
enlighten enlightenment
involve involvement
impair impairment
replace
2024-07-16 08:12:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 08:17:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4717,  0.0747, -0.1641,  ...,  0.0272,  0.5522,  0.1876],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1826, -0.6719, -2.5840,  ...,  0.7163,  2.5527,  7.9297],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4526, -0.0131,  0.0507,  ...,  0.0158,  0.0133, -0.0481],
        [-0.0225,  0.3574, -0.0035,  ...,  0.0409, -0.0263, -0.0103],
        [ 0.0058,  0.0205,  0.4685,  ..., -0.0112,  0.0012, -0.0115],
        ...,
        [-0.0166,  0.0053,  0.0072,  ...,  0.3789,  0.0139, -0.0141],
        [ 0.0166,  0.0212, -0.0238,  ..., -0.0062,  0.3960,  0.0125],
        [ 0.0008, -0.0065, -0.0451,  ..., -0.0301, -0.0345,  0.4124]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.5195,  0.5938, -1.2734,  ...,  0.9565,  1.1191,  7.0352]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 08:17:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for manage management
displace displacement
equip equipment
align alignment
enlighten enlightenment
involve involvement
impair impairment
replace
2024-07-16 08:17:04 root INFO     total operator prediction time: 1978.5471050739288 seconds
2024-07-16 08:17:04 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on name - nationality
2024-07-16 08:17:04 root INFO     building operator name - nationality
2024-07-16 08:17:04 root INFO     [order_1_approx] starting weight calculation for homer greek
caesar roman
raphael italian
gorbachev soviet
dostoyevsky russian
balzac french
copernicus polish
stalin
2024-07-16 08:17:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 08:21:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7646,  1.8438,  0.1782,  ..., -0.0320,  1.3027,  0.0171],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.3789, -1.4434, -5.1094,  ..., -8.0156,  0.9946,  2.5410],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.5435e-01, -5.2643e-03,  1.4511e-02,  ..., -1.3191e-02,
         -9.4748e-04,  6.2256e-03],
        [ 5.8365e-03,  4.2847e-01,  2.0256e-03,  ...,  3.9368e-03,
          1.9836e-04, -1.3885e-02],
        [-2.6108e-02, -2.2659e-02,  4.5215e-01,  ..., -6.4850e-03,
         -2.7740e-02, -2.0828e-03],
        ...,
        [-1.0696e-02, -1.5869e-03, -1.4133e-03,  ...,  4.5386e-01,
         -9.0179e-03,  2.2598e-02],
        [ 4.4800e-02, -2.8336e-02, -1.7517e-02,  ..., -2.3056e-02,
          4.4531e-01, -8.1558e-03],
        [ 4.1443e-02,  2.4353e-02,  3.0823e-02,  ..., -3.7689e-03,
          2.8137e-02,  4.7461e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.4453, -5.5000, -1.2852,  ..., -5.0469, -0.6802,  1.9141]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 08:21:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for homer greek
caesar roman
raphael italian
gorbachev soviet
dostoyevsky russian
balzac french
copernicus polish
stalin
2024-07-16 08:21:12 root INFO     [order_1_approx] starting weight calculation for homer greek
balzac french
raphael italian
dostoyevsky russian
gorbachev soviet
stalin soviet
copernicus polish
caesar
2024-07-16 08:21:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 08:25:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5483,  0.1771, -0.9043,  ...,  0.3276, -0.2869, -0.1105],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5039, -1.1055, -9.2734,  ..., -3.9219, -1.9238, -7.7383],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.8071e-01, -2.8748e-02, -2.5818e-02,  ...,  1.7365e-02,
         -4.5776e-03, -2.6703e-04],
        [ 4.6021e-02,  4.8706e-01,  7.8964e-04,  ...,  2.7191e-02,
          2.0340e-02,  5.6091e-02],
        [ 2.1240e-02, -2.6581e-02,  4.3799e-01,  ..., -4.2603e-02,
          1.9440e-02, -2.0828e-02],
        ...,
        [-8.3160e-04, -9.8267e-03, -1.1024e-02,  ...,  4.6191e-01,
          3.1982e-02, -1.5259e-02],
        [ 2.1057e-02,  2.6581e-02, -4.5898e-02,  ..., -2.5604e-02,
          4.6899e-01,  5.0964e-03],
        [ 3.3844e-02,  1.5533e-02, -4.7455e-02,  ..., -4.1290e-02,
          1.8890e-02,  4.1357e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.5645, -1.3076, -5.2539,  ..., -3.0996, -2.1855, -8.9844]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 08:25:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for homer greek
balzac french
raphael italian
dostoyevsky russian
gorbachev soviet
stalin soviet
copernicus polish
caesar
2024-07-16 08:25:18 root INFO     [order_1_approx] starting weight calculation for homer greek
copernicus polish
stalin soviet
caesar roman
gorbachev soviet
balzac french
dostoyevsky russian
raphael
2024-07-16 08:25:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 08:29:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2627, -0.4514, -0.5649,  ...,  1.3115,  1.3799,  0.0127],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.5938, -3.3242, -4.1562,  ..., -0.0361, -1.3486, -1.4355],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4551,  0.0122,  0.0096,  ...,  0.0060, -0.0223,  0.0197],
        [ 0.0197,  0.4651,  0.0406,  ...,  0.0010,  0.0379,  0.0241],
        [-0.0389,  0.0061,  0.4595,  ...,  0.0124, -0.0464, -0.0433],
        ...,
        [ 0.0064, -0.0052, -0.0108,  ...,  0.4990,  0.0274, -0.0019],
        [ 0.0201,  0.0049, -0.0142,  ...,  0.0049,  0.4531, -0.0070],
        [ 0.0055, -0.0194, -0.0008,  ..., -0.0069,  0.0192,  0.4802]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7979, -4.1797, -0.9883,  ...,  0.1051, -4.0586, -2.0215]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 08:29:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for homer greek
copernicus polish
stalin soviet
caesar roman
gorbachev soviet
balzac french
dostoyevsky russian
raphael
2024-07-16 08:29:25 root INFO     [order_1_approx] starting weight calculation for gorbachev soviet
caesar roman
dostoyevsky russian
copernicus polish
raphael italian
balzac french
stalin soviet
homer
2024-07-16 08:29:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 08:33:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2131,  0.3604, -1.2031,  ...,  1.2109,  0.7861, -0.7705],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0684,  2.6582, -6.8984,  ..., -3.1797, -2.0742, -9.4922],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 5.0830e-01, -2.2354e-03, -1.9211e-02,  ..., -1.6052e-02,
          1.6663e-02, -2.8595e-02],
        [-1.9409e-02,  4.5459e-01,  2.5177e-02,  ...,  9.3231e-03,
          9.8572e-03,  2.0126e-02],
        [ 1.4862e-02,  5.6030e-02,  4.6484e-01,  ...,  1.4267e-02,
         -2.7222e-02, -2.1164e-02],
        ...,
        [ 3.1281e-04, -2.1667e-02, -1.4954e-03,  ...,  5.5322e-01,
          3.8391e-02, -1.2970e-02],
        [ 1.6129e-02, -2.1515e-03, -2.3010e-02,  ...,  1.8250e-02,
          5.1416e-01,  8.5526e-03],
        [ 3.0930e-02, -3.6285e-02, -5.4749e-02,  ...,  9.3307e-03,
          3.8025e-02,  4.4482e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.5635e-03,  2.2988e+00, -4.4141e+00,  ..., -4.5312e+00,
         -3.7539e+00, -7.5078e+00]], device='cuda:1', dtype=torch.float16,
       grad_fn=<SubBackward0>) 

                    
2024-07-16 08:33:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for gorbachev soviet
caesar roman
dostoyevsky russian
copernicus polish
raphael italian
balzac french
stalin soviet
homer
2024-07-16 08:33:32 root INFO     [order_1_approx] starting weight calculation for raphael italian
copernicus polish
stalin soviet
gorbachev soviet
balzac french
caesar roman
homer greek
dostoyevsky
2024-07-16 08:33:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 08:37:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3223, -1.0361,  1.6338,  ...,  0.2224,  0.3406,  0.3171],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.2949, -3.3418, -5.1562,  ..., -6.2578, -2.9492, -5.6875],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4211, -0.0335, -0.0171,  ..., -0.0061,  0.0259,  0.0021],
        [ 0.0082,  0.3848,  0.0120,  ..., -0.0019, -0.0027,  0.0086],
        [ 0.0505,  0.0135,  0.3818,  ...,  0.0248, -0.0323, -0.0197],
        ...,
        [ 0.0375, -0.0197, -0.0358,  ...,  0.4863, -0.0019,  0.0216],
        [ 0.0226,  0.0047, -0.0371,  ..., -0.0095,  0.4236, -0.0043],
        [ 0.0387, -0.0363, -0.0704,  ..., -0.0113,  0.0020,  0.4172]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7676, -3.8008, -4.5273,  ..., -2.8164, -2.5820, -3.8594]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 08:37:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for raphael italian
copernicus polish
stalin soviet
gorbachev soviet
balzac french
caesar roman
homer greek
dostoyevsky
2024-07-16 08:37:39 root INFO     [order_1_approx] starting weight calculation for homer greek
gorbachev soviet
dostoyevsky russian
stalin soviet
raphael italian
copernicus polish
caesar roman
balzac
2024-07-16 08:37:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 08:41:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8076,  0.7549, -0.9868,  ..., -0.2336, -0.5493,  1.0596],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6270, -0.2070, -6.3008,  ..., -6.4375, -8.2266, -7.1250],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.0283e-01, -1.7273e-02,  7.2746e-03,  ..., -2.9358e-02,
         -2.4139e-02,  1.6022e-04],
        [ 4.1077e-02,  4.8291e-01,  2.6901e-02,  ...,  3.3752e-02,
          3.9062e-03,  8.4991e-03],
        [ 2.7817e-02,  1.9028e-02,  4.4678e-01,  ..., -1.7288e-02,
          1.4015e-02, -2.7313e-02],
        ...,
        [ 5.3467e-02, -9.3842e-03, -1.0437e-02,  ...,  4.5923e-01,
          1.9470e-02,  8.4991e-03],
        [ 6.7139e-04,  6.1722e-03, -3.6713e-02,  ..., -2.5681e-02,
          4.4922e-01,  4.8447e-03],
        [-1.0529e-03,  1.7548e-02, -2.8305e-02,  ..., -9.2163e-02,
          1.5518e-02,  4.4873e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.7656, -1.5967, -6.1133,  ..., -3.2988, -7.5938, -6.7266]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 08:41:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for homer greek
gorbachev soviet
dostoyevsky russian
stalin soviet
raphael italian
copernicus polish
caesar roman
balzac
2024-07-16 08:41:45 root INFO     [order_1_approx] starting weight calculation for balzac french
stalin soviet
dostoyevsky russian
caesar roman
raphael italian
homer greek
gorbachev soviet
copernicus
2024-07-16 08:41:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 08:45:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3215,  1.1582, -1.2559,  ..., -0.4031,  0.3145,  0.8545],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.3789, -3.6133, -1.8164,  ..., -7.9648, -5.2500, -3.3984],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.4971e-01,  1.2802e-02,  1.9409e-02,  ...,  1.8036e-02,
         -7.5531e-03,  1.2535e-02],
        [ 1.4435e-02,  4.7461e-01,  3.4485e-02,  ...,  1.8265e-02,
          1.7761e-02,  1.1505e-02],
        [ 3.6987e-02, -2.0599e-02,  5.1123e-01,  ...,  1.6968e-02,
          2.7847e-03, -2.3556e-04],
        ...,
        [ 2.5894e-02, -2.2385e-02, -7.7087e-02,  ...,  4.6387e-01,
          3.3875e-02, -1.3939e-02],
        [ 7.6370e-03, -3.4294e-03, -6.1157e-02,  ..., -1.2505e-02,
          4.7656e-01, -4.5872e-04],
        [ 3.2471e-02, -3.1525e-02, -1.0052e-01,  ..., -6.9641e-02,
          3.7231e-02,  4.9731e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.3398, -5.4141, -1.6680,  ..., -1.7891, -2.7090,  2.3086]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 08:45:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for balzac french
stalin soviet
dostoyevsky russian
caesar roman
raphael italian
homer greek
gorbachev soviet
copernicus
2024-07-16 08:45:53 root INFO     [order_1_approx] starting weight calculation for homer greek
raphael italian
dostoyevsky russian
copernicus polish
stalin soviet
caesar roman
balzac french
gorbachev
2024-07-16 08:45:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 08:49:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3323,  0.3594, -0.7539,  ..., -0.1992,  0.6357,  0.6846],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7773, -1.8926, -2.4785,  ..., -7.2422,  2.0430,  0.8101],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4380, -0.0043, -0.0059,  ..., -0.0143,  0.0010, -0.0153],
        [ 0.0241,  0.3992,  0.0155,  ...,  0.0316, -0.0253, -0.0024],
        [ 0.0340,  0.0366,  0.4546,  ..., -0.0051, -0.0213,  0.0405],
        ...,
        [ 0.0087,  0.0467, -0.0036,  ...,  0.4768,  0.0035,  0.0173],
        [ 0.0486, -0.0307, -0.0203,  ...,  0.0322,  0.4001, -0.0046],
        [ 0.0751,  0.0033,  0.0029,  ..., -0.0219, -0.0007,  0.4324]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.0547, -1.6982,  0.0156,  ..., -4.3203,  1.3105,  2.6719]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 08:50:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for homer greek
raphael italian
dostoyevsky russian
copernicus polish
stalin soviet
caesar roman
balzac french
gorbachev
2024-07-16 08:50:00 root INFO     total operator prediction time: 1976.634430885315 seconds
2024-07-16 08:50:00 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on country - language
2024-07-16 08:50:00 root INFO     building operator country - language
2024-07-16 08:50:01 root INFO     [order_1_approx] starting weight calculation for syria arabic
australia english
bahamas english
andorra catalan
philippines tagalog
usa english
brazil portuguese
ethiopia
2024-07-16 08:50:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 08:54:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1349, -0.4136, -0.7461,  ..., -0.1726,  0.0639, -0.3508],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.4453, -2.1602, -5.9492,  ...,  0.9751, -2.5332, -8.4297],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.3989, -0.0518, -0.0082,  ..., -0.0321, -0.0337, -0.0188],
        [-0.0208,  0.3467,  0.0006,  ...,  0.0331,  0.0008,  0.0259],
        [-0.0154,  0.0127,  0.3855,  ..., -0.0022, -0.0212, -0.0055],
        ...,
        [-0.0600,  0.0111,  0.0271,  ...,  0.4275, -0.0444, -0.0224],
        [ 0.0137, -0.0051, -0.0155,  ..., -0.0069,  0.4104, -0.0010],
        [ 0.0444, -0.0571, -0.0372,  ..., -0.0448,  0.0123,  0.3135]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.8672, -1.8008, -3.2598,  ...,  0.2891,  0.8184, -3.9023]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 08:54:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for syria arabic
australia english
bahamas english
andorra catalan
philippines tagalog
usa english
brazil portuguese
ethiopia
2024-07-16 08:54:08 root INFO     [order_1_approx] starting weight calculation for bahamas english
syria arabic
philippines tagalog
brazil portuguese
andorra catalan
ethiopia amharic
australia english
usa
2024-07-16 08:54:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 08:58:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4158,  0.3501, -0.0715,  ...,  0.6064, -1.2637, -0.5137],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.3496, -3.5547, -5.1406,  ..., -4.2891,  3.0684, -3.0957],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4458,  0.0206, -0.0831,  ..., -0.0089,  0.0621, -0.0171],
        [ 0.0034,  0.4138, -0.0175,  ...,  0.0213, -0.0028,  0.0146],
        [ 0.0169, -0.0045,  0.5020,  ...,  0.0041,  0.0089,  0.0022],
        ...,
        [ 0.0133,  0.0058, -0.0308,  ...,  0.5029,  0.0807, -0.0457],
        [ 0.0098,  0.0223,  0.0024,  ..., -0.0073,  0.4014, -0.0045],
        [-0.0128, -0.0220, -0.0232,  ..., -0.0263,  0.0281,  0.4097]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.2812, -4.3359, -4.3242,  ..., -5.0781,  3.3281, -2.3711]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 08:58:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for bahamas english
syria arabic
philippines tagalog
brazil portuguese
andorra catalan
ethiopia amharic
australia english
usa
2024-07-16 08:58:15 root INFO     [order_1_approx] starting weight calculation for ethiopia amharic
andorra catalan
syria arabic
australia english
philippines tagalog
usa english
bahamas english
brazil
2024-07-16 08:58:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 09:02:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2578, -0.2139,  0.0092,  ...,  0.3496, -0.1620, -1.1299],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3594, -4.1836, -7.0547,  ..., -2.5430, -1.9941, -7.5781],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.6924e-01,  2.0599e-03,  2.8229e-03,  ..., -1.1925e-02,
         -1.1887e-02, -2.1088e-02],
        [ 2.9999e-02,  4.4214e-01,  7.4387e-04,  ..., -9.1019e-03,
          6.0577e-03,  1.6708e-02],
        [ 4.0588e-02, -7.5579e-05,  4.2651e-01,  ...,  1.0269e-02,
          1.2016e-02, -3.7964e-02],
        ...,
        [ 1.6327e-02, -5.2795e-03,  2.5177e-02,  ...,  4.8438e-01,
          2.8091e-02, -3.0121e-02],
        [ 5.7793e-03, -5.8670e-03,  4.4594e-03,  ..., -2.7817e-02,
          4.7144e-01,  2.7885e-03],
        [ 8.0811e-02,  1.9440e-02, -7.2021e-02,  ...,  2.1271e-02,
          3.3813e-02,  4.2285e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3066, -3.5996, -7.1094,  ..., -2.4590, -2.6758, -4.2461]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 09:02:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for ethiopia amharic
andorra catalan
syria arabic
australia english
philippines tagalog
usa english
bahamas english
brazil
2024-07-16 09:02:24 root INFO     [order_1_approx] starting weight calculation for bahamas english
brazil portuguese
ethiopia amharic
usa english
philippines tagalog
syria arabic
andorra catalan
australia
2024-07-16 09:02:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 09:06:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8877, -0.1221, -0.6113,  ..., -0.3989,  0.5586,  0.1599],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ -0.8115,  -3.1270,  -7.8203,  ...,  -6.5938,   2.1992, -11.1250],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4197, -0.0173, -0.0295,  ..., -0.0240,  0.0048, -0.0489],
        [ 0.0107,  0.3569, -0.0142,  ...,  0.0061, -0.0142,  0.0265],
        [ 0.0092, -0.0100,  0.3757,  ..., -0.0004, -0.0132, -0.0078],
        ...,
        [ 0.0008,  0.0050,  0.0038,  ...,  0.3840, -0.0155, -0.0172],
        [ 0.0124, -0.0061, -0.0060,  ..., -0.0274,  0.3345,  0.0311],
        [ 0.0833, -0.0324, -0.0305,  ..., -0.0308, -0.0042,  0.3232]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9351, -3.5645, -6.9453,  ..., -3.1094,  0.7871, -3.5117]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 09:06:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for bahamas english
brazil portuguese
ethiopia amharic
usa english
philippines tagalog
syria arabic
andorra catalan
australia
2024-07-16 09:06:31 root INFO     [order_1_approx] starting weight calculation for andorra catalan
brazil portuguese
usa english
philippines tagalog
syria arabic
australia english
ethiopia amharic
bahamas
2024-07-16 09:06:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 09:10:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7822, -0.0887, -0.9204,  ...,  0.5542,  1.1406,  0.0290],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2246, -1.7646, -9.0938,  ..., -3.7383,  0.9448, -0.7334],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.4629e-01, -3.2776e-02, -3.0579e-02,  ...,  4.1122e-03,
          2.4719e-03,  2.7771e-02],
        [ 3.5675e-02,  3.8574e-01,  2.2400e-02,  ...,  3.7262e-02,
         -2.8122e-02,  3.4058e-02],
        [ 2.1317e-02, -1.9577e-02,  3.6987e-01,  ..., -3.2883e-03,
          8.3984e-02, -7.0129e-02],
        ...,
        [-5.2155e-02, -3.0106e-02,  4.9706e-03,  ...,  5.0684e-01,
         -1.3000e-02,  1.7944e-02],
        [ 3.8727e-02, -2.9800e-02, -1.1063e-02,  ...,  9.6588e-03,
          4.1431e-01,  2.9083e-02],
        [-1.2207e-04, -5.6976e-02,  9.4604e-03,  ...,  2.3224e-02,
         -1.5427e-02,  4.2114e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.1191, -1.8701, -7.4297,  ..., -4.5273,  1.5459,  0.4775]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 09:10:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for andorra catalan
brazil portuguese
usa english
philippines tagalog
syria arabic
australia english
ethiopia amharic
bahamas
2024-07-16 09:10:39 root INFO     [order_1_approx] starting weight calculation for ethiopia amharic
syria arabic
usa english
brazil portuguese
bahamas english
andorra catalan
australia english
philippines
2024-07-16 09:10:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 09:14:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7056, -0.4038, -0.9297,  ...,  1.8291,  0.9639,  0.2184],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3242, -3.0801, -7.3125,  ..., -4.8047,  1.7549, -6.4297],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.7852e-01, -2.1576e-02, -2.5421e-02,  ..., -2.0737e-02,
          2.5879e-02, -3.2288e-02],
        [ 1.4908e-02,  4.0820e-01,  1.6537e-03,  ..., -1.3397e-02,
         -3.9307e-02,  2.0218e-04],
        [-1.7395e-03, -3.6526e-03,  4.0356e-01,  ..., -9.4910e-03,
          1.9806e-02, -3.2837e-02],
        ...,
        [-2.6798e-03,  2.1759e-02, -1.9119e-02,  ...,  4.5752e-01,
          1.3878e-02, -1.3947e-02],
        [-1.0757e-02,  1.6052e-02,  2.6550e-02,  ...,  1.7834e-04,
          4.2334e-01,  5.6534e-03],
        [ 1.6159e-02, -3.5034e-02, -1.2680e-02,  ...,  6.5422e-03,
         -5.5695e-03,  3.7549e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.8970, -0.3164, -3.5332,  ..., -4.5234, -0.6943, -4.6641]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 09:14:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for ethiopia amharic
syria arabic
usa english
brazil portuguese
bahamas english
andorra catalan
australia english
philippines
2024-07-16 09:14:47 root INFO     [order_1_approx] starting weight calculation for australia english
philippines tagalog
bahamas english
brazil portuguese
syria arabic
usa english
ethiopia amharic
andorra
2024-07-16 09:14:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 09:18:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2991, -0.3547, -1.4316,  ...,  0.4668,  0.2681,  0.9561],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.4463, -2.0078, -7.3281,  ..., -1.4551, -1.3867, -4.8438],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4214, -0.0068,  0.0060,  ..., -0.0321,  0.0392,  0.0055],
        [-0.0048,  0.4146, -0.0124,  ...,  0.0269, -0.0325,  0.0286],
        [ 0.0217,  0.0197,  0.4224,  ..., -0.0046, -0.0287, -0.0472],
        ...,
        [-0.0125,  0.0026,  0.0345,  ...,  0.4963, -0.0093, -0.0080],
        [-0.0264, -0.0022, -0.0108,  ...,  0.0129,  0.4688,  0.0348],
        [ 0.0367, -0.0364, -0.0054,  ..., -0.0230,  0.0183,  0.3438]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0371, -1.0527, -4.9609,  ..., -2.7188, -2.0742, -2.4980]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 09:18:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for australia english
philippines tagalog
bahamas english
brazil portuguese
syria arabic
usa english
ethiopia amharic
andorra
2024-07-16 09:18:56 root INFO     [order_1_approx] starting weight calculation for bahamas english
australia english
andorra catalan
philippines tagalog
brazil portuguese
usa english
ethiopia amharic
syria
2024-07-16 09:18:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 09:23:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1494, -1.2734,  0.1714,  ...,  0.0537, -0.6650, -0.2197],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5684, -5.0938, -1.6240,  ..., -6.7852, -2.6016, -2.5391],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.2236e-01, -5.0903e-02, -1.7624e-02,  ..., -1.7258e-02,
          8.5602e-03, -1.0666e-02],
        [ 1.9226e-03,  3.6084e-01,  9.6436e-03,  ...,  4.1962e-02,
         -2.0065e-03, -3.6621e-04],
        [-1.3420e-02,  1.8425e-03,  3.5791e-01,  ..., -7.8278e-03,
         -2.0294e-02, -2.6245e-03],
        ...,
        [-2.3743e-02, -7.6447e-03,  1.5823e-02,  ...,  4.3066e-01,
         -3.2776e-02, -5.1346e-03],
        [ 4.2633e-02, -3.8033e-03, -1.1826e-02,  ..., -1.7014e-02,
          4.0283e-01,  1.1539e-03],
        [ 1.8860e-02, -4.0833e-02,  8.7585e-03,  ..., -7.1564e-03,
          5.3253e-02,  3.6816e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0996, -3.5703,  1.4795,  ..., -5.8750, -2.8457, -3.8535]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 09:23:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for bahamas english
australia english
andorra catalan
philippines tagalog
brazil portuguese
usa english
ethiopia amharic
syria
2024-07-16 09:23:03 root INFO     total operator prediction time: 1982.9453864097595 seconds
2024-07-16 09:23:03 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on animal - shelter
2024-07-16 09:23:03 root INFO     building operator animal - shelter
2024-07-16 09:23:04 root INFO     [order_1_approx] starting weight calculation for hornet nest
duck pond
rat nest
crow nest
pig sty
dog doghouse
herring sea
baboon
2024-07-16 09:23:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 09:27:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2290, -0.6089, -0.9141,  ..., -0.8936,  0.9453,  0.1857],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7754,  0.3003, -0.4387,  ..., -1.3408,  1.7334,  1.6777],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4866,  0.0131, -0.0215,  ...,  0.0224,  0.0345, -0.0108],
        [-0.0055,  0.4333,  0.0383,  ...,  0.0411, -0.0136,  0.0355],
        [ 0.0049, -0.0266,  0.4536,  ...,  0.0169,  0.0245, -0.0125],
        ...,
        [-0.0271, -0.0318,  0.0040,  ...,  0.4946, -0.0266, -0.0152],
        [ 0.0028, -0.0191, -0.0344,  ...,  0.0240,  0.4683,  0.0371],
        [ 0.0875,  0.0049, -0.0096,  ..., -0.0288, -0.0124,  0.4331]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[1.6572, 1.0176, 0.1765,  ..., 0.3975, 1.2441, 1.9844]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 09:27:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for hornet nest
duck pond
rat nest
crow nest
pig sty
dog doghouse
herring sea
baboon
2024-07-16 09:27:10 root INFO     [order_1_approx] starting weight calculation for baboon grove
duck pond
dog doghouse
hornet nest
rat nest
crow nest
pig sty
herring
2024-07-16 09:27:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 09:31:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1445,  0.6851, -0.8271,  ...,  0.8516, -0.4343,  0.6865],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0095,  3.5605, -1.5020,  ...,  1.3398,  0.5264,  1.4111],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.8926e-01, -3.0861e-03, -1.9226e-02,  ..., -2.4319e-04,
         -1.7365e-02, -1.5099e-02],
        [-2.4948e-02,  4.7949e-01,  2.1759e-02,  ..., -8.4991e-03,
         -2.2141e-02,  4.8798e-02],
        [-1.3580e-02,  8.1482e-03,  5.0146e-01,  ...,  1.4999e-02,
          8.8577e-03, -3.4882e-02],
        ...,
        [-9.4299e-03,  2.9144e-02,  2.7481e-02,  ...,  5.1172e-01,
         -3.0258e-02, -1.0475e-02],
        [ 1.1082e-03,  1.0605e-03, -1.4626e-02,  ...,  4.9164e-02,
          4.9829e-01,  1.7731e-02],
        [ 3.3051e-02,  2.7496e-02, -5.5466e-03,  ..., -3.7476e-02,
          2.4597e-02,  4.7632e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[0.1351, 2.6777, 0.7441,  ..., 1.7246, 0.3418, 2.1484]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 09:31:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for baboon grove
duck pond
dog doghouse
hornet nest
rat nest
crow nest
pig sty
herring
2024-07-16 09:31:17 root INFO     [order_1_approx] starting weight calculation for herring sea
crow nest
pig sty
baboon grove
hornet nest
rat nest
dog doghouse
duck
2024-07-16 09:31:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 09:35:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7173, -0.8770, -0.6089,  ...,  0.6445, -1.5352,  0.4058],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 6.9238e-01,  8.0762e-01, -4.8477e+00,  ...,  9.8828e-01,
         3.2959e-03,  4.8125e+00], device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4910, -0.0044, -0.0115,  ...,  0.0046, -0.0052,  0.0187],
        [ 0.0047,  0.4534,  0.0324,  ...,  0.0064, -0.0370,  0.0317],
        [ 0.0112, -0.0104,  0.4868,  ..., -0.0145,  0.0233, -0.0203],
        ...,
        [-0.0031,  0.0068, -0.0052,  ...,  0.4917,  0.0187, -0.0355],
        [ 0.0079,  0.0098, -0.0124,  ...,  0.0312,  0.4773,  0.0248],
        [ 0.0227,  0.0180,  0.0039,  ...,  0.0201,  0.0131,  0.4932]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3228,  2.2949, -2.8281,  ...,  2.0273,  1.0908,  4.7422]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 09:35:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for herring sea
crow nest
pig sty
baboon grove
hornet nest
rat nest
dog doghouse
duck
2024-07-16 09:35:25 root INFO     [order_1_approx] starting weight calculation for baboon grove
crow nest
duck pond
pig sty
rat nest
hornet nest
herring sea
dog
2024-07-16 09:35:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 09:39:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1068, -0.2788, -0.9849,  ...,  0.3115, -0.2944,  1.0107],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3184,  0.9580, -2.3828,  ..., -0.1965,  0.2678,  2.4570],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4880,  0.0128, -0.0105,  ..., -0.0148, -0.0164, -0.0422],
        [-0.0245,  0.4177,  0.0254,  ...,  0.0058, -0.0044,  0.0305],
        [-0.0337, -0.0085,  0.4758,  ..., -0.0164,  0.0127, -0.0049],
        ...,
        [ 0.0123, -0.0045,  0.0103,  ...,  0.4614,  0.0171, -0.0252],
        [ 0.0128,  0.0137, -0.0246,  ...,  0.0294,  0.4519, -0.0080],
        [ 0.0383,  0.0186,  0.0057,  ..., -0.0145,  0.0117,  0.4468]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1455, -0.5723,  0.0234,  ...,  0.9160, -0.0344,  0.8877]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 09:39:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for baboon grove
crow nest
duck pond
pig sty
rat nest
hornet nest
herring sea
dog
2024-07-16 09:39:34 root INFO     [order_1_approx] starting weight calculation for crow nest
herring sea
duck pond
baboon grove
pig sty
rat nest
dog doghouse
hornet
2024-07-16 09:39:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 09:43:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0212,  0.7598, -1.0039,  ...,  1.0303,  0.2097,  1.6631],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6055,  2.6367,  0.1182,  ..., -1.3682,  4.1953,  4.2227],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.5195,  0.0012, -0.0721,  ..., -0.0010,  0.0243, -0.0519],
        [-0.0115,  0.4534,  0.0356,  ..., -0.0119, -0.0032,  0.0067],
        [-0.0265, -0.0135,  0.5361,  ...,  0.0088, -0.0130, -0.0022],
        ...,
        [ 0.0097, -0.0302, -0.0656,  ...,  0.5264, -0.0040, -0.0237],
        [-0.0047, -0.0155,  0.0070,  ..., -0.0094,  0.4885,  0.0088],
        [ 0.0547, -0.0103, -0.0338,  ..., -0.0247,  0.0089,  0.4888]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4277,  2.0293, -0.4185,  ...,  0.5830,  3.4531,  3.5820]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 09:43:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for crow nest
herring sea
duck pond
baboon grove
pig sty
rat nest
dog doghouse
hornet
2024-07-16 09:43:42 root INFO     [order_1_approx] starting weight calculation for duck pond
dog doghouse
crow nest
hornet nest
pig sty
baboon grove
herring sea
rat
2024-07-16 09:43:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 09:47:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0100,  0.1436, -1.5137,  ...,  0.0449, -1.1855,  0.3655],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.0586,  1.5137,  2.0195,  ..., -1.5498,  0.1255,  3.3555],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.9316e-01, -5.5656e-03, -3.3936e-02,  ..., -5.8670e-03,
         -9.8801e-03, -8.0414e-03],
        [-3.5000e-03,  4.6460e-01,  1.0170e-02,  ..., -2.4048e-02,
          9.9869e-03,  2.0920e-02],
        [-4.5013e-03, -2.9621e-03,  5.1074e-01,  ...,  1.8845e-02,
         -1.3176e-02,  3.4698e-02],
        ...,
        [ 1.1093e-02,  8.3542e-04, -2.1912e-02,  ...,  5.0000e-01,
          5.5695e-03, -3.9764e-02],
        [ 6.7253e-03, -9.1553e-05, -5.2414e-03,  ..., -5.0735e-04,
          5.0439e-01,  3.3173e-02],
        [ 2.7710e-02, -5.0659e-03,  9.7656e-04,  ..., -2.0416e-02,
          1.7151e-02,  4.9316e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7188,  3.5918,  3.3301,  ...,  1.5635, -0.8105,  3.1875]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 09:47:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for duck pond
dog doghouse
crow nest
hornet nest
pig sty
baboon grove
herring sea
rat
2024-07-16 09:47:48 root INFO     [order_1_approx] starting weight calculation for hornet nest
duck pond
crow nest
baboon grove
rat nest
herring sea
dog doghouse
pig
2024-07-16 09:47:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 09:51:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9014, -0.9258, -0.8594,  ..., -0.2317, -0.5586,  1.2295],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7617,  1.0469, -0.9609,  ..., -0.8594, -1.0830,  1.8760],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.3677e-01,  1.3985e-02, -1.5335e-03,  ...,  1.9577e-02,
          7.2098e-03,  4.4174e-03],
        [-5.8327e-03,  4.1089e-01, -5.4092e-03,  ...,  1.3229e-02,
         -8.9111e-03,  2.2659e-02],
        [-1.3985e-02, -1.6136e-03,  4.7363e-01,  ..., -1.3031e-02,
         -3.7231e-03,  1.8372e-02],
        ...,
        [-1.8524e-02,  1.3733e-04, -2.0737e-02,  ...,  4.4971e-01,
          8.6746e-03, -4.5509e-03],
        [-1.9516e-02, -5.3940e-03, -8.2016e-03,  ...,  3.2692e-03,
          4.2944e-01,  7.2060e-03],
        [ 6.7383e-02,  1.8143e-02, -2.5909e-02,  ..., -3.1494e-02,
          1.6922e-02,  4.1504e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9521,  0.8188, -0.7168,  ...,  0.8457, -1.4355,  1.1582]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 09:51:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for hornet nest
duck pond
crow nest
baboon grove
rat nest
herring sea
dog doghouse
pig
2024-07-16 09:51:55 root INFO     [order_1_approx] starting weight calculation for baboon grove
pig sty
herring sea
hornet nest
rat nest
dog doghouse
duck pond
crow
2024-07-16 09:51:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 09:56:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1602, -0.5420, -0.4114,  ...,  0.8359,  0.2058, -0.1792],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([3.2812, 1.6934, 0.3652,  ..., 0.3979, 1.2773, 0.3447], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 5.4199e-01, -7.5073e-03, -1.5121e-02,  ..., -2.8641e-02,
          4.8523e-03, -2.5711e-03],
        [-1.0254e-02,  4.8291e-01,  4.2175e-02,  ...,  4.5967e-03,
         -1.1681e-02,  4.9591e-03],
        [-2.8259e-02, -1.7487e-02,  5.5908e-01,  ..., -1.1177e-03,
         -2.2537e-02,  6.0486e-02],
        ...,
        [ 1.8845e-02,  1.4715e-03, -3.0807e-02,  ...,  5.4590e-01,
          5.0110e-02, -6.3858e-03],
        [ 8.3694e-03, -3.5095e-02, -2.3987e-02,  ...,  1.5884e-02,
          5.2051e-01, -2.1591e-02],
        [ 1.9440e-02, -9.5367e-03, -1.3138e-02,  ..., -2.4811e-02,
          2.3651e-04,  5.0098e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[2.7656, 1.7080, 0.7900,  ..., 1.0332, 0.1582, 1.6895]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 09:56:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for baboon grove
pig sty
herring sea
hornet nest
rat nest
dog doghouse
duck pond
crow
2024-07-16 09:56:03 root INFO     total operator prediction time: 1979.6312165260315 seconds
2024-07-16 09:56:03 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on male - female
2024-07-16 09:56:03 root INFO     building operator male - female
2024-07-16 09:56:03 root INFO     [order_1_approx] starting weight calculation for batman batwoman
businessman businesswoman
uncle aunt
boy girl
father mother
prince princess
webmaster webmistress
brother
2024-07-16 09:56:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 10:00:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3462, -0.1716,  0.5068,  ...,  0.8442,  1.2607, -0.4756],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.3711, -7.7969,  0.5977,  ..., -1.8223, -1.1777, -3.7617],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.3760, -0.0031,  0.0183,  ..., -0.0213, -0.0284, -0.0387],
        [ 0.0057,  0.3699,  0.0058,  ...,  0.0241,  0.0170, -0.0091],
        [-0.0033, -0.0249,  0.4277,  ...,  0.0012, -0.0325, -0.0171],
        ...,
        [-0.0566, -0.0073,  0.0062,  ...,  0.4490, -0.0223, -0.0138],
        [ 0.0014,  0.0251, -0.0064,  ...,  0.0097,  0.3911,  0.0112],
        [ 0.0341,  0.0257, -0.0232,  ...,  0.0031,  0.0314,  0.4104]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5859, -6.4336,  0.7695,  ...,  0.1201, -2.9883, -1.6582]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 10:00:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for batman batwoman
businessman businesswoman
uncle aunt
boy girl
father mother
prince princess
webmaster webmistress
brother
2024-07-16 10:00:10 root INFO     [order_1_approx] starting weight calculation for brother sister
father mother
uncle aunt
businessman businesswoman
prince princess
boy girl
webmaster webmistress
batman
2024-07-16 10:00:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 10:04:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0176,  0.5386, -0.9214,  ..., -0.0884, -0.6035, -0.2598],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0576, -0.4331, -0.9819,  ..., -2.6465, -1.8730,  0.0989],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.8135e-01, -3.8452e-02, -3.4821e-02,  ..., -4.3060e-02,
         -6.8665e-03, -4.3602e-03],
        [ 1.4473e-02,  4.1089e-01,  2.0615e-02,  ...,  4.0527e-02,
         -1.9211e-02, -1.7944e-02],
        [-4.5776e-05,  7.3814e-03,  4.0186e-01,  ..., -2.2812e-02,
          3.0914e-02,  1.0147e-03],
        ...,
        [-2.7740e-02,  1.7746e-02,  5.0812e-03,  ...,  4.2896e-01,
          9.2468e-03, -2.5604e-02],
        [-8.6365e-03, -4.9591e-04, -5.8350e-02,  ...,  1.2985e-02,
          3.8599e-01,  1.5495e-02],
        [ 3.7537e-02, -2.0004e-02, -4.6310e-03,  ...,  3.5706e-02,
          1.6937e-03,  3.8623e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0195, -1.2930, -0.8984,  ..., -3.3848,  0.1221,  0.6484]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 10:04:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for brother sister
father mother
uncle aunt
businessman businesswoman
prince princess
boy girl
webmaster webmistress
batman
2024-07-16 10:04:18 root INFO     [order_1_approx] starting weight calculation for prince princess
father mother
boy girl
webmaster webmistress
batman batwoman
uncle aunt
brother sister
businessman
2024-07-16 10:04:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 10:08:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0341, -0.4319, -0.1164,  ..., -0.2773, -0.4380, -1.2080],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5000, -6.8945, -3.0430,  ..., -3.1582,  1.0059,  1.1279],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4397,  0.0244, -0.0240,  ..., -0.0033,  0.0229, -0.0356],
        [ 0.0461,  0.3628, -0.0072,  ...,  0.0308,  0.0044, -0.0007],
        [ 0.0183,  0.0070,  0.3948,  ...,  0.0148, -0.0113, -0.0293],
        ...,
        [-0.0106,  0.0110, -0.0102,  ...,  0.3916, -0.0104, -0.0225],
        [ 0.0807,  0.0300, -0.0287,  ...,  0.0021,  0.3787, -0.0143],
        [ 0.0325, -0.0018, -0.0278,  ..., -0.0123, -0.0146,  0.3557]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0840, -6.5234, -2.9141,  ...,  0.1602,  4.0234,  4.9180]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 10:08:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for prince princess
father mother
boy girl
webmaster webmistress
batman batwoman
uncle aunt
brother sister
businessman
2024-07-16 10:08:25 root INFO     [order_1_approx] starting weight calculation for boy girl
brother sister
prince princess
father mother
businessman businesswoman
uncle aunt
batman batwoman
webmaster
2024-07-16 10:08:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 10:12:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.5522, 1.1826, 1.0586,  ..., 0.6260, 1.0996, 0.4106], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3208, -4.8906,  2.6133,  ...,  1.7598,  1.8271, -0.4077],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.2920e-01, -4.0588e-02, -1.2947e-02,  ..., -2.5314e-02,
          2.5864e-03,  7.4806e-03],
        [ 5.7068e-02,  3.7280e-01, -1.0529e-03,  ..., -3.9917e-02,
         -8.4381e-03,  1.7685e-02],
        [-3.2043e-04,  1.9348e-02,  4.4263e-01,  ...,  3.3936e-02,
          2.9449e-03, -3.9520e-03],
        ...,
        [-1.1444e-02, -2.5284e-02,  4.3259e-03,  ...,  4.6875e-01,
         -4.1656e-03,  1.2650e-02],
        [ 6.6147e-03,  3.2806e-03,  2.9968e-02,  ...,  5.8899e-03,
          3.9551e-01,  1.5457e-02],
        [ 1.6617e-02,  1.5396e-02, -3.3203e-02,  ..., -1.8661e-02,
          9.1324e-03,  3.8696e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1095, -4.9492,  3.0488,  ...,  2.2637, -0.6650,  3.2695]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 10:12:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for boy girl
brother sister
prince princess
father mother
businessman businesswoman
uncle aunt
batman batwoman
webmaster
2024-07-16 10:12:32 root INFO     [order_1_approx] starting weight calculation for boy girl
batman batwoman
webmaster webmistress
brother sister
uncle aunt
businessman businesswoman
prince princess
father
2024-07-16 10:12:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 10:16:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8047,  0.0900, -0.3281,  ...,  0.1738,  0.1904, -0.3853],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.4258, -6.4492, -0.2920,  ..., -3.0156,  2.3691, -1.1533],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4568,  0.0130, -0.0136,  ..., -0.0149, -0.0031,  0.0046],
        [-0.0006,  0.4346,  0.0495,  ...,  0.0349, -0.0314,  0.0171],
        [-0.0036,  0.0053,  0.4553,  ...,  0.0073, -0.0131,  0.0089],
        ...,
        [-0.0216,  0.0214,  0.0381,  ...,  0.4929, -0.0116, -0.0055],
        [-0.0436,  0.0174, -0.0247,  ...,  0.0039,  0.4663,  0.0133],
        [ 0.0097, -0.0018, -0.0327,  ..., -0.0099, -0.0149,  0.4626]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6641, -6.7695,  1.7256,  ..., -2.0527,  0.8975,  0.2080]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 10:16:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for boy girl
batman batwoman
webmaster webmistress
brother sister
uncle aunt
businessman businesswoman
prince princess
father
2024-07-16 10:16:38 root INFO     [order_1_approx] starting weight calculation for webmaster webmistress
prince princess
batman batwoman
father mother
brother sister
boy girl
businessman businesswoman
uncle
2024-07-16 10:16:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 10:20:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0226, -0.4150, -0.6221,  ...,  0.0460,  1.2510, -1.1807],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.1172, -7.4453,  0.4648,  ..., -3.3555, -2.7598, -3.5527],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.9795e-01, -1.2138e-02, -3.3264e-03,  ...,  1.6403e-02,
          1.1459e-02,  1.8585e-02],
        [-1.0986e-02,  3.7573e-01,  9.3842e-04,  ..., -3.5782e-03,
         -2.5116e-02,  1.8890e-02],
        [ 2.7863e-02,  2.0599e-02,  4.4775e-01,  ...,  3.7079e-03,
         -9.8190e-03, -4.5868e-02],
        ...,
        [-3.2166e-02, -3.6957e-02, -2.9526e-03,  ...,  4.4946e-01,
          1.6594e-03, -4.6051e-02],
        [-1.4709e-02, -2.1973e-03, -2.4414e-04,  ..., -1.2527e-02,
          4.0039e-01, -8.5449e-04],
        [-1.5450e-02,  5.7892e-02, -3.3691e-02,  ..., -5.7373e-03,
         -1.4709e-02,  4.1602e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1123, -6.1680, -0.0762,  ..., -2.7578, -4.8516,  1.5762]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 10:20:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for webmaster webmistress
prince princess
batman batwoman
father mother
brother sister
boy girl
businessman businesswoman
uncle
2024-07-16 10:20:45 root INFO     [order_1_approx] starting weight calculation for webmaster webmistress
prince princess
uncle aunt
father mother
brother sister
batman batwoman
businessman businesswoman
boy
2024-07-16 10:20:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 10:24:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7246, -0.2534, -0.7842,  ...,  0.0919,  0.9932, -0.9077],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.7734, -2.2324, -3.5781,  ..., -0.6211,  4.0703,  1.2178],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.8633e-01, -1.6823e-03, -7.3547e-03,  ...,  2.6226e-04,
          2.5299e-02, -8.3771e-03],
        [ 2.6764e-02,  3.5913e-01, -6.8054e-03,  ..., -1.4893e-02,
          1.0590e-02,  1.4221e-02],
        [ 1.8219e-02, -1.7944e-02,  4.2310e-01,  ..., -4.1885e-03,
          4.6043e-03, -3.0807e-02],
        ...,
        [-1.5541e-02, -3.4698e-02,  1.1139e-03,  ...,  4.4678e-01,
          2.8839e-02,  1.6327e-02],
        [ 1.4343e-03,  1.8921e-02,  2.8992e-04,  ...,  8.0414e-03,
          4.3506e-01,  4.5105e-02],
        [ 6.9199e-03,  8.3466e-03, -6.0425e-03,  ...,  1.0498e-02,
          1.5450e-02,  4.6265e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.2109, -0.8477, -2.4863,  ...,  0.8340,  3.2305,  2.0195]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 10:24:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for webmaster webmistress
prince princess
uncle aunt
father mother
brother sister
batman batwoman
businessman businesswoman
boy
2024-07-16 10:24:51 root INFO     [order_1_approx] starting weight calculation for father mother
batman batwoman
brother sister
webmaster webmistress
businessman businesswoman
uncle aunt
boy girl
prince
2024-07-16 10:24:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 10:28:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.4004, -0.7856,  0.5747,  ..., -0.4370,  1.2412, -0.2444],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.3291, -6.8867, -0.9990,  ..., -5.3516, -2.5742, -2.6348],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4600, -0.0400, -0.0110,  ..., -0.0065,  0.0282,  0.0013],
        [-0.0118,  0.3989,  0.0098,  ...,  0.0050,  0.0236,  0.0284],
        [ 0.0186,  0.0321,  0.4578,  ...,  0.0134, -0.0300, -0.0073],
        ...,
        [-0.0214, -0.0063,  0.0420,  ...,  0.4419,  0.0159, -0.0352],
        [ 0.0317,  0.0110, -0.0357,  ..., -0.0115,  0.4507, -0.0710],
        [ 0.0445,  0.0050, -0.0338,  ..., -0.0172,  0.0359,  0.4534]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.3203, -4.2031,  0.4443,  ..., -3.5820, -2.0762,  0.1543]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 10:28:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for father mother
batman batwoman
brother sister
webmaster webmistress
businessman businesswoman
uncle aunt
boy girl
prince
2024-07-16 10:28:58 root INFO     total operator prediction time: 1974.9462869167328 seconds
2024-07-16 10:28:58 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on name - occupation
2024-07-16 10:28:58 root INFO     building operator name - occupation
2024-07-16 10:28:58 root INFO     [order_1_approx] starting weight calculation for darwin naturalist
einstein physicist
moses prophet
schwarzenegger actor
kant philosopher
tolstoi novelist
newton scientist
maxwell
2024-07-16 10:28:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 10:33:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6416, -0.1235, -0.0818,  ..., -0.2161, -0.6270,  0.0571],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4160, -0.8755, -1.9580,  ..., -2.2168, -3.5039, -2.2969],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.5063,  0.0039, -0.0282,  ..., -0.0036, -0.0019,  0.0105],
        [-0.0013,  0.4768, -0.0158,  ...,  0.0309,  0.0092,  0.0263],
        [ 0.0154,  0.0021,  0.5483,  ..., -0.0247, -0.0126, -0.0148],
        ...,
        [-0.0096, -0.0111, -0.0288,  ...,  0.5635,  0.0028, -0.0162],
        [ 0.0475, -0.0131,  0.0172,  ..., -0.0008,  0.5220, -0.0249],
        [ 0.0301, -0.0178,  0.0217,  ...,  0.0089, -0.0174,  0.5342]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5137, -1.5430,  1.4385,  ...,  0.2559, -5.5078, -2.7930]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 10:33:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for darwin naturalist
einstein physicist
moses prophet
schwarzenegger actor
kant philosopher
tolstoi novelist
newton scientist
maxwell
2024-07-16 10:33:05 root INFO     [order_1_approx] starting weight calculation for maxwell physicist
darwin naturalist
schwarzenegger actor
einstein physicist
kant philosopher
moses prophet
tolstoi novelist
newton
2024-07-16 10:33:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 10:37:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2330,  0.7969,  0.0850,  ...,  0.4038, -0.7427,  0.3164],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.6484,  0.5107,  0.0371,  ..., -0.9517, -6.8008, -0.7734],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4392, -0.0320, -0.0231,  ..., -0.0045,  0.0111, -0.0067],
        [ 0.0009,  0.4312,  0.0275,  ...,  0.0175, -0.0257,  0.0061],
        [-0.0149, -0.0015,  0.5034,  ...,  0.0177, -0.0472,  0.0008],
        ...,
        [-0.0145, -0.0022, -0.0168,  ...,  0.5039, -0.0018,  0.0222],
        [ 0.0652,  0.0176, -0.0057,  ...,  0.0254,  0.4978, -0.0320],
        [-0.0169, -0.0318, -0.0088,  ..., -0.0367, -0.0692,  0.4756]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.8633,  0.8013,  1.8936,  ...,  0.6011, -4.4453, -0.3364]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 10:37:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for maxwell physicist
darwin naturalist
schwarzenegger actor
einstein physicist
kant philosopher
moses prophet
tolstoi novelist
newton
2024-07-16 10:37:09 root INFO     [order_1_approx] starting weight calculation for moses prophet
schwarzenegger actor
newton scientist
darwin naturalist
kant philosopher
einstein physicist
maxwell physicist
tolstoi
2024-07-16 10:37:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 10:41:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2439,  0.6040,  0.9844,  ...,  0.7368,  0.4092, -0.9395],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4629,  1.7754, -0.0747,  ..., -3.9062, -0.0043, -3.5879],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.3967, -0.0491,  0.0179,  ..., -0.0100, -0.0007, -0.0210],
        [-0.0427,  0.3923,  0.0208,  ...,  0.0502, -0.0318,  0.0215],
        [ 0.0260,  0.0552,  0.4067,  ...,  0.0074, -0.0485,  0.0038],
        ...,
        [ 0.0235,  0.0143,  0.0102,  ...,  0.4231, -0.0174,  0.0044],
        [ 0.0546, -0.0092, -0.0325,  ..., -0.0219,  0.3901, -0.0077],
        [ 0.0994, -0.0061, -0.0184,  ..., -0.0633,  0.0178,  0.3848]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.2891, -2.0176, -0.8481,  ..., -4.6914, -1.7158, -0.6191]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 10:41:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for moses prophet
schwarzenegger actor
newton scientist
darwin naturalist
kant philosopher
einstein physicist
maxwell physicist
tolstoi
2024-07-16 10:41:14 root INFO     [order_1_approx] starting weight calculation for kant philosopher
tolstoi novelist
schwarzenegger actor
moses prophet
newton scientist
maxwell physicist
einstein physicist
darwin
2024-07-16 10:41:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 10:45:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5244, -0.5532, -0.2158,  ...,  0.7720, -0.1967,  0.1892],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.0469, -2.3438,  0.7617,  ..., -2.6738, -2.6641, -1.8486],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.5117e-01, -1.2886e-02,  6.7749e-03,  ..., -1.2680e-02,
         -2.5620e-02, -1.9836e-02],
        [ 2.2980e-02,  4.6289e-01,  7.1716e-03,  ..., -2.9373e-04,
          2.5131e-02,  7.4158e-03],
        [ 3.5919e-02,  8.5297e-03,  5.0488e-01,  ..., -1.8402e-02,
         -5.3802e-02,  1.7944e-02],
        ...,
        [-3.1395e-03, -9.1400e-03,  2.2385e-02,  ...,  4.6069e-01,
         -1.0651e-02, -2.8534e-03],
        [ 6.8817e-03, -1.8494e-02,  6.2485e-03,  ...,  2.1317e-02,
          4.3066e-01, -2.3529e-02],
        [ 2.7039e-02,  1.3672e-02,  1.7960e-02,  ..., -3.5156e-02,
         -1.0689e-02,  4.4922e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.1016, -3.3750,  2.7109,  ..., -2.6543, -3.4121,  0.6865]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 10:45:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for kant philosopher
tolstoi novelist
schwarzenegger actor
moses prophet
newton scientist
maxwell physicist
einstein physicist
darwin
2024-07-16 10:45:20 root INFO     [order_1_approx] starting weight calculation for maxwell physicist
moses prophet
schwarzenegger actor
darwin naturalist
tolstoi novelist
kant philosopher
newton scientist
einstein
2024-07-16 10:45:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 10:49:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4990,  0.8960,  0.4399,  ..., -0.1086, -0.7520,  0.6602],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.8418, -2.8223,  2.1289,  ..., -2.2656, -5.0938, -2.9160],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4961, -0.0307,  0.0172,  ...,  0.0179, -0.0072,  0.0228],
        [ 0.0378,  0.4294,  0.0168,  ..., -0.0013,  0.0010, -0.0176],
        [ 0.0128,  0.0091,  0.5083,  ..., -0.0109, -0.0352,  0.0349],
        ...,
        [ 0.0154,  0.0344, -0.0108,  ...,  0.4788,  0.0060,  0.0325],
        [ 0.0637, -0.0122, -0.0126,  ..., -0.0118,  0.4053,  0.0089],
        [ 0.0209,  0.0029, -0.0120,  ..., -0.0331,  0.0209,  0.4524]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.5293, -3.9570,  2.7559,  ...,  1.0117, -4.8789, -1.9277]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 10:49:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for maxwell physicist
moses prophet
schwarzenegger actor
darwin naturalist
tolstoi novelist
kant philosopher
newton scientist
einstein
2024-07-16 10:49:25 root INFO     [order_1_approx] starting weight calculation for maxwell physicist
kant philosopher
schwarzenegger actor
newton scientist
tolstoi novelist
darwin naturalist
einstein physicist
moses
2024-07-16 10:49:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 10:53:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0620, -1.3691, -0.1367,  ...,  0.6797, -0.4985, -0.2632],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 6.8516, -1.4990, -1.7051,  ...,  0.7935, -1.5869, -1.9600],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4385,  0.0018, -0.0046,  ..., -0.0241, -0.0188, -0.0022],
        [ 0.0332,  0.4353,  0.0145,  ...,  0.0520, -0.0150,  0.0479],
        [ 0.0363,  0.0172,  0.4312,  ...,  0.0062, -0.0069, -0.0309],
        ...,
        [-0.0334,  0.0094,  0.0375,  ...,  0.4951,  0.0319, -0.0007],
        [ 0.0211, -0.0078,  0.0017,  ..., -0.0065,  0.4253,  0.0010],
        [ 0.0536, -0.0084,  0.0031,  ..., -0.0262, -0.0011,  0.4160]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.5820, -0.3418, -0.0615,  ..., -0.6743, -2.9883, -3.8164]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 10:53:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for maxwell physicist
kant philosopher
schwarzenegger actor
newton scientist
tolstoi novelist
darwin naturalist
einstein physicist
moses
2024-07-16 10:53:31 root INFO     [order_1_approx] starting weight calculation for moses prophet
tolstoi novelist
newton scientist
einstein physicist
schwarzenegger actor
maxwell physicist
darwin naturalist
kant
2024-07-16 10:53:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 10:57:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1270,  0.3484,  0.5752,  ..., -0.7266,  0.0015,  0.7124],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2383, -5.5273, -2.4492,  ..., -3.7324, -2.6387,  0.7754],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 5.3809e-01, -4.5654e-02,  5.1193e-03,  ..., -2.7176e-02,
          6.5308e-03,  9.3613e-03],
        [ 2.0294e-02,  4.9146e-01, -1.7395e-02,  ..., -1.4313e-02,
          3.4302e-02,  1.3748e-02],
        [ 3.3188e-04, -9.8877e-03,  5.1416e-01,  ..., -3.4058e-02,
         -2.0752e-02, -1.6464e-02],
        ...,
        [-4.5624e-03,  7.1869e-03, -2.8992e-03,  ...,  5.3613e-01,
         -3.1471e-03,  2.7634e-02],
        [ 1.5839e-02,  2.0218e-03,  3.3264e-03,  ...,  4.5563e-02,
          5.0830e-01, -8.4686e-04],
        [-3.7689e-03,  2.7649e-02,  3.3905e-02,  ..., -2.5497e-02,
         -4.8828e-02,  5.2930e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7910, -7.1406, -1.2227,  ..., -0.8477,  0.9414,  0.3801]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 10:57:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for moses prophet
tolstoi novelist
newton scientist
einstein physicist
schwarzenegger actor
maxwell physicist
darwin naturalist
kant
2024-07-16 10:57:37 root INFO     [order_1_approx] starting weight calculation for einstein physicist
newton scientist
moses prophet
tolstoi novelist
darwin naturalist
maxwell physicist
kant philosopher
schwarzenegger
2024-07-16 10:57:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 11:01:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.6543,  0.6475, -0.6230,  ...,  0.5176,  1.9434,  0.5376],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5352, -3.5684,  0.8130,  ..., -5.7578,  1.7754,  0.8955],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.1870e-01, -1.9211e-02,  4.6661e-02,  ...,  3.3905e-02,
         -5.2567e-03, -1.2253e-02],
        [ 5.9509e-04,  4.3970e-01,  1.7548e-04,  ...,  2.6947e-02,
         -1.4801e-03,  5.3635e-03],
        [ 1.2833e-02,  3.5645e-02,  4.4702e-01,  ...,  1.0071e-02,
         -1.9882e-02,  1.4877e-02],
        ...,
        [-1.2405e-02,  8.5449e-04, -2.0218e-02,  ...,  4.7754e-01,
          1.0818e-02, -5.4321e-03],
        [ 9.3842e-03,  2.7100e-02, -4.1626e-02,  ...,  1.9257e-02,
          4.6777e-01, -1.6006e-02],
        [ 9.2651e-02, -5.4199e-02, -2.6520e-02,  ..., -5.6732e-02,
         -3.1860e-02,  4.6704e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7266, -3.7832, -0.5728,  ..., -5.2812, -0.7930,  2.3691]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 11:01:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for einstein physicist
newton scientist
moses prophet
tolstoi novelist
darwin naturalist
maxwell physicist
kant philosopher
schwarzenegger
2024-07-16 11:01:44 root INFO     total operator prediction time: 1966.2823553085327 seconds
2024-07-16 11:01:44 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on country - capital
2024-07-16 11:01:44 root INFO     building operator country - capital
2024-07-16 11:01:44 root INFO     [order_1_approx] starting weight calculation for sofia bulgaria
beijing china
beirut lebanon
kiev ukraine
brussels belgium
conakry guinea
tehran iran
islamabad
2024-07-16 11:01:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 11:05:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5557, -0.5625,  0.3755,  ...,  1.0430, -0.8906,  0.7979],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6650, -4.7969,  0.9546,  ...,  0.1494,  2.4688, -1.3037],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.3384e-01, -2.2034e-02, -1.0605e-02,  ...,  1.7609e-02,
         -1.3153e-02,  1.1475e-02],
        [-7.9269e-03,  4.1113e-01, -3.2593e-02,  ...,  6.1401e-02,
          1.1566e-02,  6.0730e-03],
        [ 1.0101e-02,  1.6464e-02,  4.9609e-01,  ..., -3.8086e-02,
         -6.3599e-02,  6.6986e-03],
        ...,
        [ 2.8091e-02,  3.0029e-02,  3.3447e-02,  ...,  5.1172e-01,
         -4.2534e-04, -2.8824e-02],
        [ 3.3813e-02,  8.9844e-02,  3.7781e-02,  ...,  2.1942e-02,
          4.5654e-01,  2.4963e-02],
        [ 4.6631e-02, -1.0242e-01,  1.3031e-02,  ..., -7.1045e-02,
         -6.4880e-02,  3.8086e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4802, -2.9609, -0.3560,  ...,  0.0715,  1.1006, -2.0000]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 11:05:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for sofia bulgaria
beijing china
beirut lebanon
kiev ukraine
brussels belgium
conakry guinea
tehran iran
islamabad
2024-07-16 11:05:54 root INFO     [order_1_approx] starting weight calculation for conakry guinea
tehran iran
brussels belgium
kiev ukraine
beirut lebanon
islamabad pakistan
beijing china
sofia
2024-07-16 11:05:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 11:09:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.4307, -0.1274, -1.2949,  ...,  0.1681, -0.1274,  0.2363],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.8906,  0.0161,  2.7168,  ...,  4.6406, -7.5938, -0.7158],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4517, -0.0583, -0.0308,  ...,  0.0151,  0.0149, -0.0447],
        [-0.0137,  0.4373, -0.0060,  ...,  0.0579, -0.0294, -0.0157],
        [-0.0317, -0.0527,  0.4919,  ..., -0.0160, -0.0619, -0.0313],
        ...,
        [-0.0699, -0.0384,  0.0069,  ...,  0.4619, -0.0656, -0.0349],
        [ 0.0366,  0.0040, -0.0365,  ..., -0.0237,  0.4622,  0.0148],
        [-0.0355, -0.0020,  0.0068,  ..., -0.0516, -0.0342,  0.4377]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.4102,  1.0566,  2.2578,  ...,  5.7148, -2.6836, -2.8359]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 11:10:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for conakry guinea
tehran iran
brussels belgium
kiev ukraine
beirut lebanon
islamabad pakistan
beijing china
sofia
2024-07-16 11:10:00 root INFO     [order_1_approx] starting weight calculation for islamabad pakistan
beijing china
brussels belgium
kiev ukraine
tehran iran
beirut lebanon
sofia bulgaria
conakry
2024-07-16 11:10:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 11:14:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4639, -0.4419, -0.0288,  ...,  0.5854, -0.0393,  0.4626],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6104,  2.0488,  1.2236,  ...,  0.5547, -1.5303, -0.0703],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4238, -0.0053,  0.0519,  ...,  0.0314,  0.0107, -0.0460],
        [-0.0378,  0.4353,  0.0246,  ...,  0.0502, -0.0379, -0.0190],
        [-0.0006,  0.0160,  0.4697,  ..., -0.0131, -0.0347, -0.0256],
        ...,
        [-0.0319,  0.0369,  0.0412,  ...,  0.4854, -0.0378, -0.0061],
        [ 0.0061, -0.0183, -0.0034,  ...,  0.0530,  0.4722,  0.0335],
        [ 0.0180, -0.0241,  0.0081,  ..., -0.0717,  0.0190,  0.4175]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1484,  2.3027, -0.3232,  ..., -3.2812, -1.1426,  2.9805]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 11:14:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for islamabad pakistan
beijing china
brussels belgium
kiev ukraine
tehran iran
beirut lebanon
sofia bulgaria
conakry
2024-07-16 11:14:05 root INFO     [order_1_approx] starting weight calculation for tehran iran
conakry guinea
brussels belgium
kiev ukraine
sofia bulgaria
islamabad pakistan
beirut lebanon
beijing
2024-07-16 11:14:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 11:18:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.3447, -0.2793,  0.1036,  ..., -0.8447, -0.1687, -0.0201],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1611, -3.4512,  1.2402,  ..., -7.1484, -1.3896, -2.6133],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.0039e-01, -1.0681e-02,  4.9973e-04,  ..., -2.8114e-03,
          1.0742e-02, -3.3386e-02],
        [ 1.4481e-02,  3.8550e-01, -9.0361e-04,  ...,  1.1307e-02,
          2.7328e-02,  1.6922e-02],
        [ 9.7809e-03, -6.3019e-03,  3.8354e-01,  ..., -2.4536e-02,
         -3.5057e-03, -1.6693e-02],
        ...,
        [-2.0172e-02,  1.3992e-02,  5.6725e-03,  ...,  3.9722e-01,
          2.4246e-02, -3.4821e-02],
        [-1.1040e-02, -6.1646e-03,  7.0572e-03,  ..., -7.5989e-03,
          3.5791e-01,  4.8218e-03],
        [ 1.0391e-02, -4.4342e-02, -6.8665e-05,  ..., -4.1626e-02,
         -9.3079e-04,  3.2471e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6318, -5.1055,  0.5366,  ..., -6.9219,  0.6436, -1.8809]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 11:18:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for tehran iran
conakry guinea
brussels belgium
kiev ukraine
sofia bulgaria
islamabad pakistan
beirut lebanon
beijing
2024-07-16 11:18:10 root INFO     [order_1_approx] starting weight calculation for conakry guinea
kiev ukraine
islamabad pakistan
sofia bulgaria
tehran iran
beijing china
beirut lebanon
brussels
2024-07-16 11:18:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 11:22:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7910,  0.2754, -0.8135,  ...,  1.2734,  0.3823,  0.7417],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0137,  2.1055,  2.3496,  ..., -2.0156, -4.5742, -9.6016],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4221, -0.0157,  0.0297,  ..., -0.0056,  0.0011, -0.0201],
        [ 0.0033,  0.3677,  0.0199,  ...,  0.0051,  0.0161,  0.0058],
        [ 0.0237,  0.0247,  0.4631,  ..., -0.0228, -0.0734, -0.0061],
        ...,
        [-0.0106,  0.0295,  0.0124,  ...,  0.4709,  0.0399, -0.0408],
        [ 0.0122,  0.0032, -0.0392,  ..., -0.0175,  0.4138,  0.0074],
        [ 0.0240, -0.0317, -0.0321,  ..., -0.0290,  0.0219,  0.3848]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6953,  2.1172,  2.1133,  ..., -1.8672, -6.2578, -7.5430]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 11:22:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for conakry guinea
kiev ukraine
islamabad pakistan
sofia bulgaria
tehran iran
beijing china
beirut lebanon
brussels
2024-07-16 11:22:17 root INFO     [order_1_approx] starting weight calculation for beijing china
brussels belgium
sofia bulgaria
kiev ukraine
tehran iran
islamabad pakistan
conakry guinea
beirut
2024-07-16 11:22:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 11:26:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1455, -0.2917, -1.4746,  ...,  1.3076, -0.8657,  0.6172],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.6758, -0.4316,  1.2207,  ..., -0.7969, -4.0625, -2.9453],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4783,  0.0163, -0.0222,  ...,  0.0047,  0.0338, -0.0123],
        [-0.0136,  0.4380,  0.0263,  ...,  0.0538, -0.0148,  0.0132],
        [ 0.0045,  0.0166,  0.4587,  ..., -0.0034, -0.0080,  0.0023],
        ...,
        [ 0.0054, -0.0163,  0.0099,  ...,  0.4949, -0.0273,  0.0011],
        [ 0.0040, -0.0083, -0.0458,  ..., -0.0219,  0.4636,  0.0405],
        [ 0.0587, -0.0482, -0.0491,  ..., -0.0286,  0.0720,  0.3911]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.0195,  0.3506,  2.4180,  ..., -0.4451, -3.7871,  0.3086]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 11:26:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for beijing china
brussels belgium
sofia bulgaria
kiev ukraine
tehran iran
islamabad pakistan
conakry guinea
beirut
2024-07-16 11:26:23 root INFO     [order_1_approx] starting weight calculation for beirut lebanon
tehran iran
conakry guinea
beijing china
islamabad pakistan
sofia bulgaria
brussels belgium
kiev
2024-07-16 11:26:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 11:30:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1689,  0.1941, -0.0715,  ...,  2.4277,  0.6138,  1.3516],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-5.4531, -3.0762,  6.0625,  ...,  1.8643,  1.7041, -1.3135],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.2432e-01,  1.1566e-02, -2.0752e-03,  ..., -3.2959e-03,
          3.8818e-02, -3.3630e-02],
        [ 1.9470e-02,  3.7769e-01, -4.7798e-03,  ...,  1.4816e-02,
         -1.5808e-02, -1.2222e-02],
        [ 4.3976e-02,  1.8341e-02,  4.2725e-01,  ..., -1.0864e-02,
         -5.4321e-02, -1.6434e-02],
        ...,
        [-5.6061e-02,  2.2522e-02,  1.5640e-03,  ...,  4.6411e-01,
          2.5772e-02, -7.2098e-03],
        [-2.0447e-02, -5.3955e-02, -1.6556e-02,  ...,  2.2411e-04,
          3.4424e-01,  1.9913e-02],
        [ 2.0081e-02, -3.9062e-02, -1.6953e-02,  ..., -6.2103e-03,
          2.8961e-02,  3.4814e-01]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.5391, -3.1309,  7.7188,  ...,  0.2217,  2.3066,  1.6045]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 11:30:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for beirut lebanon
tehran iran
conakry guinea
beijing china
islamabad pakistan
sofia bulgaria
brussels belgium
kiev
2024-07-16 11:30:31 root INFO     [order_1_approx] starting weight calculation for conakry guinea
kiev ukraine
islamabad pakistan
sofia bulgaria
brussels belgium
beijing china
beirut lebanon
tehran
2024-07-16 11:30:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.5
2024-07-16 11:34:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6953,  0.0588, -0.6802,  ..., -0.7847,  0.3564,  0.1609],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1846,  1.9102,  5.2461,  ..., -1.6406, -1.2949,  0.2522],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.4053, -0.0136,  0.0128,  ..., -0.0074,  0.0010,  0.0174],
        [ 0.0135,  0.4072,  0.0485,  ...,  0.0111, -0.0250, -0.0157],
        [ 0.0408,  0.0021,  0.4993,  ...,  0.0217, -0.0729, -0.0071],
        ...,
        [ 0.0150,  0.0293, -0.0072,  ...,  0.4578, -0.0005, -0.0083],
        [ 0.0089, -0.0267,  0.0295,  ..., -0.0068,  0.4421,  0.0518],
        [ 0.0554, -0.0515, -0.0028,  ..., -0.0450,  0.0056,  0.3977]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9795,  0.8857,  4.0469,  ...,  0.4805, -1.1953,  1.7861]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-16 11:34:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for conakry guinea
kiev ukraine
islamabad pakistan
sofia bulgaria
brussels belgium
beijing china
beirut lebanon
tehran
2024-07-16 11:34:38 root INFO     total operator prediction time: 1974.2272708415985 seconds
