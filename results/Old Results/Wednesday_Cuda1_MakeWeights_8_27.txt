2024-07-17 17:11:40 root INFO     loading model + tokenizer
2024-07-17 17:11:57 root INFO     model + tokenizer loaded
2024-07-17 17:11:57 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - part
2024-07-17 17:11:57 root INFO     building operator meronyms - part
2024-07-17 17:11:58 root INFO     [order_1_approx] starting weight calculation for A part of a brush is a bristle
A part of a pie is a crust
A part of a gigabit is a megabit
A part of a gramm is a milligram
A part of a litre is a millilitre
A part of a byte is a bit
A part of a gun is a trigger
A part of a piano is a
2024-07-17 17:11:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 17:15:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1272, -0.5938,  0.6904,  ..., -0.0952, -0.8838, -0.3540],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1064, -1.9453, -2.6289,  ..., -4.4453,  0.3196, -1.8115],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0012, -0.0342,  0.0012,  ..., -0.0012,  0.0136, -0.0267],
        [-0.0198,  0.0153,  0.0064,  ...,  0.0062, -0.0143, -0.0148],
        [-0.0127, -0.0121, -0.0156,  ...,  0.0076,  0.0263,  0.0003],
        ...,
        [-0.0123, -0.0102,  0.0120,  ...,  0.0067, -0.0359,  0.0026],
        [-0.0068,  0.0208, -0.0143,  ..., -0.0008, -0.0212,  0.0075],
        [ 0.0085,  0.0158,  0.0211,  ...,  0.0040,  0.0121,  0.0128]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1846, -0.7070, -2.5469,  ..., -4.9609,  0.4946, -2.0195]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 17:15:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a brush is a bristle
A part of a pie is a crust
A part of a gigabit is a megabit
A part of a gramm is a milligram
A part of a litre is a millilitre
A part of a byte is a bit
A part of a gun is a trigger
A part of a piano is a
2024-07-17 17:15:31 root INFO     [order_1_approx] starting weight calculation for A part of a brush is a bristle
A part of a byte is a bit
A part of a pie is a crust
A part of a gigabit is a megabit
A part of a gramm is a milligram
A part of a litre is a millilitre
A part of a piano is a keyboard
A part of a gun is a
2024-07-17 17:15:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 17:19:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8765, -0.6426, -0.1152,  ...,  1.5469, -0.6519,  0.1575],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5879, -0.9814, -1.9961,  ..., -5.1992, -2.8887,  5.8320],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0104, -0.0094,  0.0021,  ..., -0.0052, -0.0019, -0.0039],
        [-0.0182,  0.0231,  0.0061,  ...,  0.0221, -0.0115,  0.0056],
        [-0.0134,  0.0117,  0.0088,  ..., -0.0147, -0.0095,  0.0035],
        ...,
        [-0.0162, -0.0042, -0.0040,  ...,  0.0262, -0.0253, -0.0089],
        [-0.0057,  0.0117,  0.0026,  ...,  0.0104, -0.0035,  0.0078],
        [-0.0168, -0.0112,  0.0025,  ..., -0.0173,  0.0223,  0.0263]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6406, -0.1553, -1.5518,  ..., -5.1055, -2.2188,  5.7383]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 17:19:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a brush is a bristle
A part of a byte is a bit
A part of a pie is a crust
A part of a gigabit is a megabit
A part of a gramm is a milligram
A part of a litre is a millilitre
A part of a piano is a keyboard
A part of a gun is a
2024-07-17 17:19:05 root INFO     [order_1_approx] starting weight calculation for A part of a pie is a crust
A part of a byte is a bit
A part of a brush is a bristle
A part of a piano is a keyboard
A part of a gun is a trigger
A part of a gramm is a milligram
A part of a gigabit is a megabit
A part of a litre is a
2024-07-17 17:19:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 17:22:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9453,  0.3540, -0.7471,  ..., -0.7075, -0.3145,  1.3174],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.5000,  1.6211, -1.0908,  ...,  0.0442, -0.9692, -0.2480],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.0627e-03, -6.1264e-03, -6.4545e-03,  ..., -3.4485e-03,
         -3.6888e-03,  1.2062e-02],
        [-5.1041e-03,  1.4706e-03, -2.7962e-03,  ..., -4.8523e-03,
         -1.2299e-02, -1.4809e-02],
        [-5.3215e-03, -1.6235e-02,  1.3145e-02,  ..., -4.7951e-03,
          3.3875e-03,  4.6082e-03],
        ...,
        [-1.8829e-02, -4.5776e-03,  7.7286e-03,  ...,  1.7380e-02,
         -8.9493e-03,  1.6403e-03],
        [ 2.4834e-03, -1.3351e-02, -9.4452e-03,  ...,  1.2085e-02,
          6.8359e-03,  2.7237e-03],
        [-8.9111e-03,  3.4142e-04, -1.9073e-06,  ..., -1.9714e-02,
          3.1830e-02,  9.6321e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7168,  1.2891, -1.6191,  ...,  0.3359, -0.8247,  0.5703]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 17:22:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a pie is a crust
A part of a byte is a bit
A part of a brush is a bristle
A part of a piano is a keyboard
A part of a gun is a trigger
A part of a gramm is a milligram
A part of a gigabit is a megabit
A part of a litre is a
2024-07-17 17:22:39 root INFO     [order_1_approx] starting weight calculation for A part of a litre is a millilitre
A part of a piano is a keyboard
A part of a brush is a bristle
A part of a gramm is a milligram
A part of a gigabit is a megabit
A part of a byte is a bit
A part of a gun is a trigger
A part of a pie is a
2024-07-17 17:22:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 17:26:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3242,  0.6973, -0.1440,  ...,  0.5859, -0.5908,  1.1738],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.7578,  6.2617, -1.5703,  ...,  1.1094,  1.7012, -1.1797],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0192, -0.0031,  0.0149,  ..., -0.0122, -0.0093, -0.0094],
        [-0.0196,  0.0167,  0.0013,  ...,  0.0054, -0.0038, -0.0085],
        [-0.0080,  0.0145, -0.0116,  ..., -0.0113,  0.0086,  0.0138],
        ...,
        [-0.0053,  0.0085, -0.0027,  ...,  0.0262, -0.0259,  0.0021],
        [ 0.0063, -0.0092, -0.0149,  ..., -0.0026, -0.0058,  0.0086],
        [-0.0346,  0.0057, -0.0232,  ...,  0.0056,  0.0049,  0.0130]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.3516,  6.1445, -0.8271,  ...,  1.5771,  0.7988, -0.2510]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 17:26:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a litre is a millilitre
A part of a piano is a keyboard
A part of a brush is a bristle
A part of a gramm is a milligram
A part of a gigabit is a megabit
A part of a byte is a bit
A part of a gun is a trigger
A part of a pie is a
2024-07-17 17:26:14 root INFO     [order_1_approx] starting weight calculation for A part of a gigabit is a megabit
A part of a gramm is a milligram
A part of a brush is a bristle
A part of a gun is a trigger
A part of a litre is a millilitre
A part of a piano is a keyboard
A part of a pie is a crust
A part of a byte is a
2024-07-17 17:26:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 17:29:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0195, -0.6250, -1.2500,  ..., -0.4050, -1.1084,  1.8594],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0625, -3.6133,  2.0273,  ..., -2.7344, -4.9922,  2.1211],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0155, -0.0013, -0.0051,  ...,  0.0176, -0.0012, -0.0103],
        [-0.0259, -0.0395, -0.0215,  ..., -0.0118, -0.0247, -0.0148],
        [-0.0118, -0.0091, -0.0031,  ..., -0.0052, -0.0032,  0.0165],
        ...,
        [-0.0127, -0.0233,  0.0065,  ...,  0.0052,  0.0025, -0.0072],
        [ 0.0180,  0.0035, -0.0067,  ...,  0.0058, -0.0140, -0.0038],
        [-0.0217,  0.0130,  0.0034,  ...,  0.0096,  0.0152, -0.0070]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.6631, -3.7754,  0.6768,  ..., -2.6836, -4.8711,  1.9932]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 17:29:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a gigabit is a megabit
A part of a gramm is a milligram
A part of a brush is a bristle
A part of a gun is a trigger
A part of a litre is a millilitre
A part of a piano is a keyboard
A part of a pie is a crust
A part of a byte is a
2024-07-17 17:29:49 root INFO     [order_1_approx] starting weight calculation for A part of a byte is a bit
A part of a litre is a millilitre
A part of a brush is a bristle
A part of a pie is a crust
A part of a piano is a keyboard
A part of a gun is a trigger
A part of a gramm is a milligram
A part of a gigabit is a
2024-07-17 17:29:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 17:33:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5391, -0.8286,  0.7227,  ..., -1.1104,  1.2891,  0.7627],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.5762,  1.0117, -1.8945,  ...,  0.7725, -2.5215,  3.2656],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0011, -0.0108, -0.0073,  ...,  0.0137, -0.0091, -0.0145],
        [-0.0111, -0.0069, -0.0028,  ...,  0.0101, -0.0163, -0.0186],
        [-0.0049, -0.0183, -0.0138,  ..., -0.0134,  0.0156,  0.0123],
        ...,
        [ 0.0054, -0.0131,  0.0165,  ...,  0.0087, -0.0003, -0.0169],
        [ 0.0122, -0.0132, -0.0100,  ...,  0.0107, -0.0122,  0.0101],
        [-0.0008,  0.0117,  0.0046,  ..., -0.0035,  0.0110, -0.0009]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5986,  1.6562, -1.6885,  ...,  0.8174, -1.7412,  2.7305]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 17:33:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a byte is a bit
A part of a litre is a millilitre
A part of a brush is a bristle
A part of a pie is a crust
A part of a piano is a keyboard
A part of a gun is a trigger
A part of a gramm is a milligram
A part of a gigabit is a
2024-07-17 17:33:22 root INFO     [order_1_approx] starting weight calculation for A part of a gigabit is a megabit
A part of a litre is a millilitre
A part of a pie is a crust
A part of a piano is a keyboard
A part of a gun is a trigger
A part of a gramm is a milligram
A part of a byte is a bit
A part of a brush is a
2024-07-17 17:33:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 17:36:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8867, -0.6670,  0.4106,  ...,  1.2480, -1.4727,  1.2188],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0234, -1.0928, -4.5938,  ..., -2.2500,  1.0400,  4.5273],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0230, -0.0026,  0.0131,  ...,  0.0215,  0.0039, -0.0146],
        [-0.0153,  0.0191,  0.0284,  ...,  0.0141, -0.0163, -0.0059],
        [-0.0016, -0.0201,  0.0189,  ..., -0.0037, -0.0050,  0.0107],
        ...,
        [ 0.0085,  0.0004, -0.0070,  ...,  0.0171, -0.0070, -0.0048],
        [ 0.0135, -0.0177, -0.0086,  ...,  0.0066,  0.0093, -0.0062],
        [-0.0002,  0.0051, -0.0023,  ...,  0.0109,  0.0102,  0.0157]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9062, -1.0518, -4.1055,  ..., -2.5762,  0.9023,  4.7500]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 17:36:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a gigabit is a megabit
A part of a litre is a millilitre
A part of a pie is a crust
A part of a piano is a keyboard
A part of a gun is a trigger
A part of a gramm is a milligram
A part of a byte is a bit
A part of a brush is a
2024-07-17 17:36:57 root INFO     [order_1_approx] starting weight calculation for A part of a piano is a keyboard
A part of a brush is a bristle
A part of a byte is a bit
A part of a gigabit is a megabit
A part of a litre is a millilitre
A part of a pie is a crust
A part of a gun is a trigger
A part of a gramm is a
2024-07-17 17:36:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 17:40:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4966,  0.3989,  0.8408,  ...,  0.1846, -0.3291,  0.4714],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.5352, -0.6519, -3.6172,  ...,  0.6074, -3.8105, -0.4656],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 8.8272e-03, -1.1055e-02, -1.0471e-03,  ..., -8.8196e-03,
         -3.8681e-03, -1.9436e-03],
        [ 4.1771e-03,  7.4425e-03, -1.5545e-03,  ..., -2.1896e-03,
         -2.9564e-03,  1.7090e-02],
        [ 2.5978e-03, -1.2909e-02, -8.4877e-04,  ..., -1.5945e-02,
         -2.3479e-03,  3.5381e-03],
        ...,
        [-8.7452e-04, -1.1246e-02,  6.3438e-03,  ...,  1.7487e-02,
         -1.8127e-02,  1.3275e-03],
        [ 1.3971e-03, -1.4191e-02,  4.5776e-05,  ...,  1.1360e-02,
          1.0300e-02,  3.9749e-03],
        [ 4.1962e-03, -4.5547e-03, -6.2828e-03,  ..., -2.9793e-03,
          3.1052e-03,  5.6534e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.1445, -0.3655, -2.8301,  ...,  0.8721, -4.0664, -1.1670]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 17:40:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a piano is a keyboard
A part of a brush is a bristle
A part of a byte is a bit
A part of a gigabit is a megabit
A part of a litre is a millilitre
A part of a pie is a crust
A part of a gun is a trigger
A part of a gramm is a
2024-07-17 17:40:29 root INFO     total operator prediction time: 1712.2314140796661 seconds
2024-07-17 17:40:30 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on synonyms - exact
2024-07-17 17:40:30 root INFO     building operator synonyms - exact
2024-07-17 17:40:30 root INFO     [order_1_approx] starting weight calculation for Another word for baby is infant
Another word for airplane is aeroplane
Another word for lad is chap
Another word for market is marketplace
Another word for style is manner
Another word for cloth is fabric
Another word for homogeneous is uniform
Another word for incorrect is
2024-07-17 17:40:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 17:44:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.2500, -0.4187,  0.1350,  ...,  0.0750,  0.8687,  0.9438],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3770,  1.6992, -2.3965,  ...,  1.2383,  5.5156, -0.8887],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0083, -0.0071,  0.0114,  ..., -0.0129,  0.0078, -0.0206],
        [ 0.0064, -0.0101, -0.0074,  ...,  0.0135, -0.0099,  0.0118],
        [ 0.0091, -0.0099,  0.0088,  ..., -0.0036,  0.0038,  0.0125],
        ...,
        [-0.0087, -0.0184,  0.0143,  ...,  0.0014, -0.0047,  0.0046],
        [-0.0015, -0.0124, -0.0142,  ...,  0.0014, -0.0052,  0.0048],
        [ 0.0066, -0.0023,  0.0091,  ...,  0.0064,  0.0161,  0.0063]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0415,  1.8252, -1.7793,  ...,  1.2207,  5.3867, -1.1504]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 17:44:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for baby is infant
Another word for airplane is aeroplane
Another word for lad is chap
Another word for market is marketplace
Another word for style is manner
Another word for cloth is fabric
Another word for homogeneous is uniform
Another word for incorrect is
2024-07-17 17:44:02 root INFO     [order_1_approx] starting weight calculation for Another word for cloth is fabric
Another word for style is manner
Another word for lad is chap
Another word for incorrect is wrong
Another word for market is marketplace
Another word for homogeneous is uniform
Another word for baby is infant
Another word for airplane is
2024-07-17 17:44:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 17:47:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.7793,  1.0732,  0.1512,  ..., -0.8750, -0.8936,  0.6240],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.6367, -1.1562,  0.6523,  ..., -2.9219,  1.0918, -4.0391],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0107, -0.0068, -0.0064,  ...,  0.0017, -0.0213,  0.0048],
        [ 0.0019, -0.0062,  0.0025,  ...,  0.0094,  0.0031,  0.0076],
        [-0.0040, -0.0009, -0.0240,  ..., -0.0158, -0.0029,  0.0042],
        ...,
        [ 0.0053,  0.0155,  0.0150,  ...,  0.0015, -0.0123,  0.0006],
        [ 0.0003, -0.0058, -0.0095,  ...,  0.0053,  0.0026,  0.0072],
        [-0.0021, -0.0184,  0.0090,  ...,  0.0019, -0.0112,  0.0150]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.8262,  0.8076,  0.6787,  ..., -2.2676,  1.6748, -4.4258]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 17:47:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for cloth is fabric
Another word for style is manner
Another word for lad is chap
Another word for incorrect is wrong
Another word for market is marketplace
Another word for homogeneous is uniform
Another word for baby is infant
Another word for airplane is
2024-07-17 17:47:37 root INFO     [order_1_approx] starting weight calculation for Another word for airplane is aeroplane
Another word for style is manner
Another word for homogeneous is uniform
Another word for baby is infant
Another word for incorrect is wrong
Another word for cloth is fabric
Another word for market is marketplace
Another word for lad is
2024-07-17 17:47:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 17:51:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6016,  0.3176, -1.2441,  ..., -0.8726,  0.3706,  0.5942],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.2812,  0.3535, -2.1367,  ..., -4.2852,  2.3281,  0.8486],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-6.1035e-03, -1.0170e-02, -3.5591e-03,  ..., -1.7258e-02,
         -1.8570e-02,  1.2802e-02],
        [ 7.9956e-03,  1.5434e-02, -1.7738e-03,  ...,  5.6267e-03,
          1.0681e-02,  1.1429e-02],
        [ 1.1688e-02, -2.1149e-02, -1.8215e-03,  ...,  2.7962e-03,
          6.1798e-04, -3.7689e-03],
        ...,
        [-6.1722e-03, -1.0689e-02,  4.0741e-03,  ...,  9.7847e-04,
         -9.2850e-03, -3.0518e-05],
        [-9.6436e-03, -6.7520e-03,  1.2039e-02,  ..., -1.5274e-02,
          7.7438e-03, -1.5717e-02],
        [-2.7832e-02, -1.7914e-02, -1.6441e-03,  ..., -7.7972e-03,
          3.5782e-03, -1.5144e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.1133,  0.7593, -2.5137,  ..., -4.6016,  1.9082,  1.1729]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 17:51:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for airplane is aeroplane
Another word for style is manner
Another word for homogeneous is uniform
Another word for baby is infant
Another word for incorrect is wrong
Another word for cloth is fabric
Another word for market is marketplace
Another word for lad is
2024-07-17 17:51:11 root INFO     [order_1_approx] starting weight calculation for Another word for style is manner
Another word for market is marketplace
Another word for lad is chap
Another word for baby is infant
Another word for cloth is fabric
Another word for incorrect is wrong
Another word for airplane is aeroplane
Another word for homogeneous is
2024-07-17 17:51:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 17:54:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.6211,  0.9600,  1.4102,  ..., -0.4990,  0.6133,  0.4688],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.2363,  3.0996, -6.9297,  ...,  2.9531,  1.6338, -0.0625],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.5248e-03, -3.8147e-03,  5.5962e-03,  ..., -4.1580e-03,
         -7.2594e-03, -1.8112e-02],
        [-3.0708e-03,  1.5221e-03, -1.2848e-02,  ...,  7.9880e-03,
         -1.9054e-03, -7.7057e-03],
        [-5.2872e-03, -1.6846e-02, -2.1973e-02,  ...,  2.0294e-03,
         -2.7409e-03,  3.3112e-03],
        ...,
        [-1.3237e-02, -1.5205e-02,  6.8512e-03,  ...,  2.0447e-02,
         -1.6663e-02,  7.8278e-03],
        [-2.2812e-03, -5.7220e-05,  1.5091e-02,  ..., -4.6310e-03,
          1.9073e-04,  3.5076e-03],
        [-7.1602e-03,  5.2185e-03, -6.9618e-03,  ..., -1.5373e-02,
          6.0272e-03,  1.5511e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.8164,  2.9043, -7.6641,  ...,  3.2266,  1.5107,  0.5229]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 17:54:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for style is manner
Another word for market is marketplace
Another word for lad is chap
Another word for baby is infant
Another word for cloth is fabric
Another word for incorrect is wrong
Another word for airplane is aeroplane
Another word for homogeneous is
2024-07-17 17:54:44 root INFO     [order_1_approx] starting weight calculation for Another word for lad is chap
Another word for market is marketplace
Another word for homogeneous is uniform
Another word for cloth is fabric
Another word for airplane is aeroplane
Another word for incorrect is wrong
Another word for baby is infant
Another word for style is
2024-07-17 17:54:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 17:58:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.4688,  0.0786, -0.5439,  ...,  1.1504,  0.0989, -0.6626],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7402, -3.1523, -4.9297,  ..., -0.2773, -1.2969,  0.9678],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0007, -0.0059,  0.0133,  ..., -0.0157, -0.0208, -0.0165],
        [ 0.0055, -0.0092, -0.0044,  ..., -0.0008, -0.0258, -0.0061],
        [ 0.0043, -0.0138, -0.0115,  ..., -0.0052, -0.0065,  0.0068],
        ...,
        [ 0.0018,  0.0006,  0.0145,  ...,  0.0138, -0.0085, -0.0068],
        [-0.0080,  0.0082,  0.0071,  ..., -0.0108,  0.0059, -0.0128],
        [ 0.0061, -0.0085,  0.0065,  ...,  0.0026,  0.0042,  0.0142]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4058, -2.4238, -5.2891,  ..., -0.1150, -0.5552,  1.1055]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 17:58:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for lad is chap
Another word for market is marketplace
Another word for homogeneous is uniform
Another word for cloth is fabric
Another word for airplane is aeroplane
Another word for incorrect is wrong
Another word for baby is infant
Another word for style is
2024-07-17 17:58:14 root INFO     [order_1_approx] starting weight calculation for Another word for airplane is aeroplane
Another word for homogeneous is uniform
Another word for style is manner
Another word for lad is chap
Another word for cloth is fabric
Another word for incorrect is wrong
Another word for baby is infant
Another word for market is
2024-07-17 17:58:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 18:01:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0630, -0.1482, -0.1565,  ...,  1.2363,  0.0917, -0.1682],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.5547, -2.2148, -6.1953,  ..., -0.1501,  0.8037,  2.5840],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0137, -0.0032,  0.0126,  ..., -0.0016, -0.0013, -0.0095],
        [-0.0046,  0.0142, -0.0014,  ...,  0.0208, -0.0009, -0.0029],
        [-0.0053, -0.0186,  0.0014,  ..., -0.0034,  0.0079,  0.0194],
        ...,
        [-0.0057, -0.0005,  0.0068,  ...,  0.0107, -0.0031,  0.0101],
        [ 0.0048,  0.0153, -0.0048,  ..., -0.0145,  0.0095, -0.0048],
        [-0.0241, -0.0214, -0.0064,  ...,  0.0065, -0.0032,  0.0267]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.3438, -0.7070, -5.4727,  ..., -1.0205,  2.1035,  2.4297]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 18:01:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for airplane is aeroplane
Another word for homogeneous is uniform
Another word for style is manner
Another word for lad is chap
Another word for cloth is fabric
Another word for incorrect is wrong
Another word for baby is infant
Another word for market is
2024-07-17 18:01:44 root INFO     [order_1_approx] starting weight calculation for Another word for lad is chap
Another word for homogeneous is uniform
Another word for incorrect is wrong
Another word for airplane is aeroplane
Another word for market is marketplace
Another word for baby is infant
Another word for style is manner
Another word for cloth is
2024-07-17 18:01:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 18:05:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.8066, -0.1384, -1.5342,  ...,  2.3418, -0.3364, -0.0729],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0518,  1.0391, -1.8564,  ..., -0.8130,  3.0488, -2.4062],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.7548e-03, -1.7609e-02, -6.7062e-03,  ..., -9.4910e-03,
         -2.2705e-02, -1.5656e-02],
        [-6.5765e-03, -1.1368e-02,  4.0865e-04,  ...,  1.5495e-02,
          1.1589e-02,  1.6724e-02],
        [-1.3115e-02, -9.2316e-03, -1.6983e-02,  ..., -5.9891e-03,
          1.0300e-03, -5.1346e-03],
        ...,
        [-3.1509e-03,  6.3515e-04, -4.6120e-03,  ...,  1.8066e-02,
         -1.1215e-02,  1.2299e-02],
        [ 8.0109e-05, -5.8746e-03,  5.4169e-03,  ...,  2.6894e-04,
          2.4078e-02,  2.3331e-02],
        [-1.7136e-02, -8.6975e-04,  2.1225e-02,  ...,  5.0735e-04,
          1.6724e-02,  1.1765e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0601,  1.5781, -1.6904,  ..., -0.6323,  2.6855, -2.3047]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 18:05:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for lad is chap
Another word for homogeneous is uniform
Another word for incorrect is wrong
Another word for airplane is aeroplane
Another word for market is marketplace
Another word for baby is infant
Another word for style is manner
Another word for cloth is
2024-07-17 18:05:15 root INFO     [order_1_approx] starting weight calculation for Another word for lad is chap
Another word for incorrect is wrong
Another word for style is manner
Another word for homogeneous is uniform
Another word for cloth is fabric
Another word for airplane is aeroplane
Another word for market is marketplace
Another word for baby is
2024-07-17 18:05:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 18:08:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5723,  0.5923, -1.0938,  ..., -0.0082, -1.2148, -0.2261],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5977, -2.0820, -5.0000,  ...,  1.8135,  2.8066,  0.2417],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0156, -0.0117,  0.0062,  ..., -0.0001, -0.0224, -0.0148],
        [-0.0030, -0.0008, -0.0086,  ...,  0.0006, -0.0093,  0.0006],
        [-0.0011,  0.0114, -0.0139,  ..., -0.0110,  0.0026,  0.0020],
        ...,
        [-0.0070, -0.0051, -0.0005,  ...,  0.0111, -0.0105, -0.0101],
        [ 0.0004,  0.0019, -0.0094,  ...,  0.0113, -0.0096, -0.0096],
        [-0.0076, -0.0172,  0.0009,  ..., -0.0046,  0.0012,  0.0070]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5083, -1.4238, -4.8477,  ...,  2.5742,  2.7246, -0.0544]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 18:08:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for lad is chap
Another word for incorrect is wrong
Another word for style is manner
Another word for homogeneous is uniform
Another word for cloth is fabric
Another word for airplane is aeroplane
Another word for market is marketplace
Another word for baby is
2024-07-17 18:08:48 root INFO     total operator prediction time: 1698.6116635799408 seconds
2024-07-17 18:08:48 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on hypernyms - misc
2024-07-17 18:08:48 root INFO     building operator hypernyms - misc
2024-07-17 18:08:49 root INFO     [order_1_approx] starting weight calculation for The shelf falls into the category of furniture
The jacket falls into the category of clothes
The gasoline falls into the category of fuel
The croissant falls into the category of pastry
The notebook falls into the category of book
The peach falls into the category of fruit
The lotion falls into the category of toiletry
The mascara falls into the category of
2024-07-17 18:08:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 18:12:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4204,  0.5220, -0.2188,  ..., -1.5625, -1.0439,  0.5400],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-6.3086, -4.3281, -1.1543,  ..., -1.8232, -8.0391,  1.8066],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-3.1395e-03,  9.5367e-05,  8.4381e-03,  ..., -6.6681e-03,
          2.4948e-03,  6.4926e-03],
        [-7.2327e-03,  2.8114e-03, -1.8559e-03,  ...,  1.8024e-03,
          5.2338e-03,  1.6449e-02],
        [ 2.2888e-04, -2.2926e-03,  7.1907e-03,  ...,  1.1587e-04,
         -6.4087e-04,  3.2597e-03],
        ...,
        [-9.0179e-03, -9.8991e-04, -2.3766e-03,  ...,  1.4168e-02,
         -1.1040e-02,  7.8201e-04],
        [-1.2407e-03, -1.4618e-02,  1.1230e-02,  ..., -6.5956e-03,
         -5.6305e-03,  5.4054e-03],
        [ 7.5760e-03, -4.0131e-03, -5.9509e-04,  ..., -8.5258e-04,
          8.6365e-03,  5.7831e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-7.0195, -4.8906, -0.7495,  ..., -2.5000, -8.6797,  1.9941]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 18:12:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The shelf falls into the category of furniture
The jacket falls into the category of clothes
The gasoline falls into the category of fuel
The croissant falls into the category of pastry
The notebook falls into the category of book
The peach falls into the category of fruit
The lotion falls into the category of toiletry
The mascara falls into the category of
2024-07-17 18:12:23 root INFO     [order_1_approx] starting weight calculation for The jacket falls into the category of clothes
The gasoline falls into the category of fuel
The peach falls into the category of fruit
The lotion falls into the category of toiletry
The shelf falls into the category of furniture
The croissant falls into the category of pastry
The mascara falls into the category of makeup
The notebook falls into the category of
2024-07-17 18:12:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 18:15:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1348, -0.4116, -1.1611,  ...,  0.6709, -0.0779,  1.1172],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7930, -0.1372, -1.2402,  ...,  3.7422,  0.4802, -0.9014],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0369, -0.0070, -0.0023,  ..., -0.0052,  0.0012,  0.0027],
        [ 0.0056,  0.0197, -0.0016,  ...,  0.0015, -0.0039, -0.0043],
        [ 0.0032, -0.0011,  0.0054,  ..., -0.0051, -0.0054,  0.0032],
        ...,
        [ 0.0008, -0.0079, -0.0014,  ...,  0.0072, -0.0107, -0.0019],
        [ 0.0070, -0.0121, -0.0005,  ...,  0.0012, -0.0017,  0.0082],
        [-0.0013, -0.0119, -0.0074,  ..., -0.0027,  0.0147,  0.0185]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.3672, -0.5039, -1.0391,  ...,  3.8574,  0.6997, -1.1348]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 18:16:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The jacket falls into the category of clothes
The gasoline falls into the category of fuel
The peach falls into the category of fruit
The lotion falls into the category of toiletry
The shelf falls into the category of furniture
The croissant falls into the category of pastry
The mascara falls into the category of makeup
The notebook falls into the category of
2024-07-17 18:16:00 root INFO     [order_1_approx] starting weight calculation for The peach falls into the category of fruit
The croissant falls into the category of pastry
The mascara falls into the category of makeup
The lotion falls into the category of toiletry
The gasoline falls into the category of fuel
The notebook falls into the category of book
The jacket falls into the category of clothes
The shelf falls into the category of
2024-07-17 18:16:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 18:19:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5630, -0.0239, -0.0464,  ..., -0.5933,  0.0858, -0.6294],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6621, -3.2051, -0.9297,  ..., -1.3760, -3.1094,  0.2993],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0149, -0.0005,  0.0015,  ..., -0.0010, -0.0001, -0.0004],
        [-0.0041,  0.0127, -0.0015,  ..., -0.0002, -0.0033,  0.0072],
        [ 0.0024,  0.0005,  0.0061,  ...,  0.0010,  0.0007,  0.0008],
        ...,
        [-0.0015, -0.0078,  0.0048,  ...,  0.0070, -0.0054,  0.0009],
        [-0.0028,  0.0024,  0.0034,  ...,  0.0006, -0.0004,  0.0011],
        [ 0.0027, -0.0045,  0.0022,  ..., -0.0055,  0.0052,  0.0089]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.2715, -3.3281, -1.1416,  ..., -1.6309, -3.3945, -0.1443]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 18:19:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The peach falls into the category of fruit
The croissant falls into the category of pastry
The mascara falls into the category of makeup
The lotion falls into the category of toiletry
The gasoline falls into the category of fuel
The notebook falls into the category of book
The jacket falls into the category of clothes
The shelf falls into the category of
2024-07-17 18:19:33 root INFO     [order_1_approx] starting weight calculation for The gasoline falls into the category of fuel
The shelf falls into the category of furniture
The mascara falls into the category of makeup
The notebook falls into the category of book
The lotion falls into the category of toiletry
The croissant falls into the category of pastry
The jacket falls into the category of clothes
The peach falls into the category of
2024-07-17 18:19:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 18:23:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2080, -0.4800, -0.1052,  ..., -1.1562, -0.4976,  0.1201],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.8906, -1.8828, -4.4570,  ..., -1.4365, -3.2305,  0.8374],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0222,  0.0040,  0.0013,  ..., -0.0175, -0.0014, -0.0020],
        [ 0.0022,  0.0131,  0.0022,  ...,  0.0101, -0.0045,  0.0064],
        [ 0.0004,  0.0063,  0.0113,  ...,  0.0025, -0.0007, -0.0011],
        ...,
        [ 0.0016, -0.0089,  0.0007,  ...,  0.0098, -0.0069, -0.0011],
        [ 0.0118,  0.0099, -0.0010,  ...,  0.0013,  0.0066, -0.0001],
        [-0.0170, -0.0052,  0.0008,  ...,  0.0020, -0.0014,  0.0146]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.8945, -1.5391, -4.2773,  ..., -1.5078, -3.1758,  0.5020]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 18:23:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The gasoline falls into the category of fuel
The shelf falls into the category of furniture
The mascara falls into the category of makeup
The notebook falls into the category of book
The lotion falls into the category of toiletry
The croissant falls into the category of pastry
The jacket falls into the category of clothes
The peach falls into the category of
2024-07-17 18:23:06 root INFO     [order_1_approx] starting weight calculation for The mascara falls into the category of makeup
The jacket falls into the category of clothes
The croissant falls into the category of pastry
The shelf falls into the category of furniture
The peach falls into the category of fruit
The lotion falls into the category of toiletry
The notebook falls into the category of book
The gasoline falls into the category of
2024-07-17 18:23:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 18:26:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5742,  1.0518, -1.1406,  ...,  0.1582,  0.7114,  0.0344],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8916, -2.9648, -2.8008,  ..., -1.2197,  1.1973, -0.0410],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 6.7253e-03, -3.0823e-03, -5.7640e-03,  ..., -9.8419e-04,
          9.3079e-04,  1.6060e-03],
        [-2.9869e-03,  1.6769e-02,  5.3406e-05,  ...,  9.6359e-03,
         -2.7046e-03, -6.5231e-03],
        [ 3.3875e-03,  4.4022e-03,  1.3113e-03,  ..., -4.9171e-03,
          5.5237e-03,  3.7060e-03],
        ...,
        [-3.6011e-03, -1.0582e-02,  2.2125e-04,  ...,  8.6975e-03,
         -1.9169e-03,  6.2408e-03],
        [ 2.8992e-04, -8.0032e-03,  5.9700e-04,  ..., -2.5444e-03,
         -2.7332e-03,  4.9629e-03],
        [-1.6449e-02, -4.6387e-03, -3.1662e-04,  ..., -1.1263e-03,
          7.1411e-03,  1.5160e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3750, -3.3203, -3.3457,  ..., -0.8608,  0.9473,  0.4038]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 18:26:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The mascara falls into the category of makeup
The jacket falls into the category of clothes
The croissant falls into the category of pastry
The shelf falls into the category of furniture
The peach falls into the category of fruit
The lotion falls into the category of toiletry
The notebook falls into the category of book
The gasoline falls into the category of
2024-07-17 18:26:40 root INFO     [order_1_approx] starting weight calculation for The jacket falls into the category of clothes
The gasoline falls into the category of fuel
The notebook falls into the category of book
The mascara falls into the category of makeup
The shelf falls into the category of furniture
The lotion falls into the category of toiletry
The peach falls into the category of fruit
The croissant falls into the category of
2024-07-17 18:26:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 18:30:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3333, -1.3359, -1.0781,  ..., -0.0607, -0.5684,  0.5679],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.5195,  0.9609,  0.7354,  ..., -0.2294, -3.5078,  1.0938],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.1002e-02,  1.8349e-03,  7.6866e-04,  ..., -6.4163e-03,
          4.7684e-04,  3.9902e-03],
        [ 7.7209e-03,  6.4087e-03, -4.0245e-04,  ...,  4.5242e-03,
          1.5192e-03,  3.3817e-03],
        [ 6.4850e-05,  1.1301e-03,  2.3460e-03,  ...,  5.5885e-04,
          1.2188e-03,  7.2384e-04],
        ...,
        [-5.3596e-03, -1.0384e-02,  2.9030e-03,  ...,  7.9803e-03,
         -8.8654e-03, -6.3324e-04],
        [ 2.3518e-03,  3.3569e-03,  6.6185e-04,  ..., -2.1133e-03,
          2.4204e-03,  7.7744e-03],
        [-1.0284e-02, -4.8256e-04, -8.8882e-04,  ..., -5.4216e-04,
          2.6073e-03,  5.2376e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.9180,  1.1104,  1.0186,  ..., -0.0277, -3.6387,  1.1289]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 18:30:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The jacket falls into the category of clothes
The gasoline falls into the category of fuel
The notebook falls into the category of book
The mascara falls into the category of makeup
The shelf falls into the category of furniture
The lotion falls into the category of toiletry
The peach falls into the category of fruit
The croissant falls into the category of
2024-07-17 18:30:14 root INFO     [order_1_approx] starting weight calculation for The croissant falls into the category of pastry
The notebook falls into the category of book
The jacket falls into the category of clothes
The peach falls into the category of fruit
The shelf falls into the category of furniture
The mascara falls into the category of makeup
The gasoline falls into the category of fuel
The lotion falls into the category of
2024-07-17 18:30:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 18:33:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4233,  0.6572, -0.9961,  ...,  0.1624,  0.3647,  0.5137],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7583, -4.7891,  0.5430,  ..., -1.6143, -2.4336, -0.1486],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.6647e-02, -3.0899e-04,  7.7248e-05,  ..., -5.4512e-03,
         -6.8378e-04, -1.3199e-03],
        [-2.5215e-03,  7.8278e-03,  1.7824e-03,  ...,  9.9182e-03,
          1.6747e-03,  6.4201e-03],
        [ 5.7831e-03, -2.4681e-03,  1.3962e-02,  ...,  1.6155e-03,
          9.0027e-04,  2.6894e-04],
        ...,
        [-8.8043e-03, -3.9482e-03, -8.4400e-04,  ...,  9.0485e-03,
         -6.9733e-03,  9.3994e-03],
        [ 1.6804e-03, -2.0428e-03, -3.3417e-03,  ...,  6.0654e-04,
          4.2419e-03,  3.2730e-03],
        [-7.8583e-03, -1.3981e-03,  6.9046e-03,  ..., -9.4910e-03,
          1.4320e-02,  1.0483e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.1963, -5.1445,  0.6357,  ..., -1.8730, -2.6660,  0.2676]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 18:33:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The croissant falls into the category of pastry
The notebook falls into the category of book
The jacket falls into the category of clothes
The peach falls into the category of fruit
The shelf falls into the category of furniture
The mascara falls into the category of makeup
The gasoline falls into the category of fuel
The lotion falls into the category of
2024-07-17 18:33:49 root INFO     [order_1_approx] starting weight calculation for The gasoline falls into the category of fuel
The peach falls into the category of fruit
The notebook falls into the category of book
The lotion falls into the category of toiletry
The mascara falls into the category of makeup
The shelf falls into the category of furniture
The croissant falls into the category of pastry
The jacket falls into the category of
2024-07-17 18:33:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 18:37:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4951, -0.5327, -0.7031,  ..., -0.2517, -1.2246,  1.2363],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9121, -2.3477,  0.7803,  ..., -1.1191, -1.5781, -2.1309],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.4656e-02,  1.8549e-03, -8.8043e-03,  ...,  2.9068e-03,
         -1.3451e-02,  8.2397e-03],
        [-1.1444e-03,  9.6970e-03,  5.7297e-03,  ...,  9.1705e-03,
         -3.1433e-03,  2.8419e-03],
        [-1.2665e-03,  7.1335e-03,  7.3090e-03,  ...,  1.2856e-03,
          4.3907e-03,  4.9667e-03],
        ...,
        [ 4.2572e-03, -6.7711e-03,  5.0964e-03,  ..., -6.1417e-04,
         -1.1398e-02,  2.8324e-03],
        [ 3.7403e-03,  1.3580e-02, -6.8665e-05,  ...,  8.5831e-04,
         -1.3046e-03,  5.5161e-03],
        [-8.7891e-03, -7.7248e-03, -6.2866e-03,  ..., -1.0780e-02,
          1.0071e-02,  1.5358e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.1348, -2.6426,  0.4468,  ..., -1.3916, -1.3535, -2.4590]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 18:37:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The gasoline falls into the category of fuel
The peach falls into the category of fruit
The notebook falls into the category of book
The lotion falls into the category of toiletry
The mascara falls into the category of makeup
The shelf falls into the category of furniture
The croissant falls into the category of pastry
The jacket falls into the category of
2024-07-17 18:37:24 root INFO     total operator prediction time: 1715.492380619049 seconds
2024-07-17 18:37:24 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - substance
2024-07-17 18:37:24 root INFO     building operator meronyms - substance
2024-07-17 18:37:24 root INFO     [order_1_approx] starting weight calculation for A plastic is made up of polymer
A bottle is made up of glass
A bronze is made up of copper
A bowl is made up of glass
A snow is made up of water
A omelette is made up of eggs
A pill is made up of medicine
A lawn is made up of
2024-07-17 18:37:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 18:40:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9160, -0.1135, -0.5298,  ...,  0.6284, -1.0664, -0.7695],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8564, -2.0879, -2.2520,  ...,  2.2422,  1.7803, -1.9902],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0137, -0.0142,  0.0079,  ...,  0.0022, -0.0128, -0.0055],
        [ 0.0010,  0.0076, -0.0036,  ...,  0.0093, -0.0048,  0.0037],
        [ 0.0061, -0.0072, -0.0022,  ...,  0.0023,  0.0010, -0.0050],
        ...,
        [ 0.0001,  0.0043, -0.0051,  ...,  0.0080, -0.0081, -0.0002],
        [ 0.0151, -0.0141, -0.0050,  ..., -0.0040,  0.0178, -0.0039],
        [ 0.0021,  0.0096,  0.0034,  ..., -0.0107,  0.0072,  0.0139]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.6738, -1.7148, -2.6465,  ...,  2.9023,  1.1621, -1.1953]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 18:40:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A plastic is made up of polymer
A bottle is made up of glass
A bronze is made up of copper
A bowl is made up of glass
A snow is made up of water
A omelette is made up of eggs
A pill is made up of medicine
A lawn is made up of
2024-07-17 18:40:56 root INFO     [order_1_approx] starting weight calculation for A plastic is made up of polymer
A pill is made up of medicine
A omelette is made up of eggs
A bottle is made up of glass
A lawn is made up of grass
A bowl is made up of glass
A snow is made up of water
A bronze is made up of
2024-07-17 18:40:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 18:44:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1121,  0.5615, -1.1367,  ..., -0.0508, -0.4094, -0.0220],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0449,  2.2324, -0.1504,  ..., -3.7383,  1.6523,  0.2275],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0048, -0.0119, -0.0043,  ..., -0.0121,  0.0009,  0.0004],
        [-0.0004, -0.0020, -0.0098,  ...,  0.0096, -0.0060,  0.0065],
        [ 0.0061,  0.0016,  0.0046,  ...,  0.0066,  0.0006,  0.0089],
        ...,
        [ 0.0136,  0.0010,  0.0007,  ..., -0.0097,  0.0064, -0.0081],
        [ 0.0082, -0.0005, -0.0072,  ...,  0.0056,  0.0099, -0.0003],
        [-0.0002,  0.0040,  0.0013,  ...,  0.0003,  0.0052,  0.0089]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3301,  1.4668,  0.1499,  ..., -3.7188,  1.3701,  0.5508]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 18:44:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A plastic is made up of polymer
A pill is made up of medicine
A omelette is made up of eggs
A bottle is made up of glass
A lawn is made up of grass
A bowl is made up of glass
A snow is made up of water
A bronze is made up of
2024-07-17 18:44:28 root INFO     [order_1_approx] starting weight calculation for A plastic is made up of polymer
A bottle is made up of glass
A bowl is made up of glass
A bronze is made up of copper
A omelette is made up of eggs
A pill is made up of medicine
A lawn is made up of grass
A snow is made up of
2024-07-17 18:44:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 18:47:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3613,  1.3848,  0.0456,  ...,  0.9746, -0.7119,  0.5337],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7666,  2.1270,  0.4619,  ..., -0.2256,  0.6973, -0.5132],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.1090e-03, -7.8888e-03,  3.4809e-03,  ...,  6.2637e-03,
         -1.8682e-03, -6.7635e-03],
        [ 4.7684e-05,  3.2673e-03,  1.1902e-03,  ...,  2.6703e-03,
          2.0065e-03, -3.7651e-03],
        [ 4.4899e-03, -2.0065e-03,  1.1101e-03,  ..., -8.8692e-04,
          8.1863e-03, -2.3289e-03],
        ...,
        [ 4.4670e-03,  4.2953e-03, -1.7290e-03,  ...,  5.8594e-03,
         -8.6212e-03, -6.7902e-04],
        [ 8.3008e-03, -1.2764e-02,  2.5892e-04,  ..., -1.1238e-02,
          1.9775e-02, -7.1144e-04],
        [ 1.7090e-03, -5.7602e-03,  2.0313e-03,  ...,  9.1248e-03,
          3.4695e-03, -1.2093e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1562,  1.7549,  0.3743,  ..., -0.1654,  0.1606, -0.2034]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 18:47:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A plastic is made up of polymer
A bottle is made up of glass
A bowl is made up of glass
A bronze is made up of copper
A omelette is made up of eggs
A pill is made up of medicine
A lawn is made up of grass
A snow is made up of
2024-07-17 18:47:59 root INFO     [order_1_approx] starting weight calculation for A bronze is made up of copper
A bottle is made up of glass
A plastic is made up of polymer
A lawn is made up of grass
A snow is made up of water
A bowl is made up of glass
A pill is made up of medicine
A omelette is made up of
2024-07-17 18:47:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 18:51:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1458, -0.0369,  0.5107,  ..., -0.1409,  1.3037,  1.2725],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.3125, -0.4653, -1.8223,  ...,  0.4802, -0.4673, -2.8516],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0082,  0.0046,  0.0076,  ..., -0.0050, -0.0071, -0.0054],
        [ 0.0012,  0.0025,  0.0052,  ...,  0.0020,  0.0004,  0.0084],
        [ 0.0024, -0.0040,  0.0016,  ..., -0.0041, -0.0024,  0.0084],
        ...,
        [-0.0040,  0.0032,  0.0026,  ...,  0.0036,  0.0032,  0.0011],
        [ 0.0079, -0.0018, -0.0013,  ..., -0.0026,  0.0062,  0.0094],
        [ 0.0037, -0.0016, -0.0037,  ..., -0.0080,  0.0097,  0.0044]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.8750, -0.4768, -1.7627,  ...,  0.6509, -0.6597, -2.5156]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 18:51:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A bronze is made up of copper
A bottle is made up of glass
A plastic is made up of polymer
A lawn is made up of grass
A snow is made up of water
A bowl is made up of glass
A pill is made up of medicine
A omelette is made up of
2024-07-17 18:51:30 root INFO     [order_1_approx] starting weight calculation for A pill is made up of medicine
A omelette is made up of eggs
A bowl is made up of glass
A plastic is made up of polymer
A bronze is made up of copper
A lawn is made up of grass
A snow is made up of water
A bottle is made up of
2024-07-17 18:51:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 18:55:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4009, -0.6934,  1.3320,  ...,  0.3477, -0.2478,  1.3926],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.9375, -0.2070,  0.5308,  ...,  0.8931,  0.3645,  0.2344],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0194, -0.0241,  0.0085,  ..., -0.0064, -0.0075, -0.0097],
        [-0.0052,  0.0109,  0.0001,  ...,  0.0013,  0.0120,  0.0149],
        [-0.0013, -0.0028,  0.0131,  ..., -0.0116,  0.0043,  0.0137],
        ...,
        [-0.0064, -0.0113,  0.0046,  ...,  0.0129, -0.0110,  0.0034],
        [-0.0035, -0.0057,  0.0077,  ...,  0.0034, -0.0014, -0.0024],
        [-0.0157, -0.0049,  0.0085,  ..., -0.0028,  0.0070,  0.0133]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.7324, -0.4629,  1.2002,  ...,  1.0879,  0.2803,  0.2451]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 18:55:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A pill is made up of medicine
A omelette is made up of eggs
A bowl is made up of glass
A plastic is made up of polymer
A bronze is made up of copper
A lawn is made up of grass
A snow is made up of water
A bottle is made up of
2024-07-17 18:55:01 root INFO     [order_1_approx] starting weight calculation for A bowl is made up of glass
A lawn is made up of grass
A pill is made up of medicine
A bottle is made up of glass
A omelette is made up of eggs
A snow is made up of water
A bronze is made up of copper
A plastic is made up of
2024-07-17 18:55:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 18:58:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4302, -0.1447,  0.1483,  ...,  0.3118, -0.4832, -0.8232],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9712, -2.6699,  0.2490,  ..., -0.8706, -1.3301,  1.1680],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0054, -0.0115, -0.0095,  ...,  0.0063,  0.0039, -0.0079],
        [ 0.0007, -0.0047, -0.0068,  ..., -0.0035, -0.0008, -0.0061],
        [ 0.0034,  0.0016, -0.0040,  ...,  0.0025, -0.0015,  0.0035],
        ...,
        [ 0.0104, -0.0015,  0.0069,  ...,  0.0120, -0.0059,  0.0033],
        [ 0.0033, -0.0064,  0.0036,  ...,  0.0051,  0.0031,  0.0009],
        [-0.0060, -0.0009, -0.0017,  ...,  0.0027,  0.0016,  0.0062]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8115, -2.7656,  0.1202,  ..., -0.7871, -1.3770,  0.9980]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 18:58:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A bowl is made up of glass
A lawn is made up of grass
A pill is made up of medicine
A bottle is made up of glass
A omelette is made up of eggs
A snow is made up of water
A bronze is made up of copper
A plastic is made up of
2024-07-17 18:58:32 root INFO     [order_1_approx] starting weight calculation for A lawn is made up of grass
A plastic is made up of polymer
A bottle is made up of glass
A snow is made up of water
A bowl is made up of glass
A omelette is made up of eggs
A bronze is made up of copper
A pill is made up of
2024-07-17 18:58:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 19:02:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8257, -0.0713, -1.6660,  ..., -0.9438, -0.1260,  0.7183],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.5469, -1.8047,  1.8623,  ..., -6.1250,  2.1621,  3.6836],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 8.3399e-04, -9.2697e-04, -5.4932e-04,  ..., -6.1111e-03,
         -5.8365e-03, -1.0117e-02],
        [ 9.6130e-04, -9.0790e-04,  1.7853e-03,  ...,  4.6616e-03,
         -2.6703e-05,  7.1411e-03],
        [-1.0872e-03, -9.7961e-03,  1.7548e-04,  ...,  5.5008e-03,
          4.3182e-03,  7.6523e-03],
        ...,
        [ 1.0483e-02, -1.1932e-02,  6.3667e-03,  ..., -1.0056e-02,
         -6.6910e-03, -2.4109e-03],
        [ 5.5389e-03, -4.0245e-03, -5.1880e-03,  ..., -3.6793e-03,
          7.8201e-04, -8.6136e-03],
        [-4.2458e-03, -1.0040e-02,  1.0040e-02,  ..., -2.5425e-03,
          4.9400e-03,  8.4839e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.3789, -1.7480,  1.8457,  ..., -6.4180,  2.0840,  3.9141]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 19:02:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A lawn is made up of grass
A plastic is made up of polymer
A bottle is made up of glass
A snow is made up of water
A bowl is made up of glass
A omelette is made up of eggs
A bronze is made up of copper
A pill is made up of
2024-07-17 19:02:04 root INFO     [order_1_approx] starting weight calculation for A pill is made up of medicine
A snow is made up of water
A bronze is made up of copper
A plastic is made up of polymer
A lawn is made up of grass
A bottle is made up of glass
A omelette is made up of eggs
A bowl is made up of
2024-07-17 19:02:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 19:05:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6372, -0.8467,  0.2673,  ...,  1.0312,  0.5513, -0.3401],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.5000,  3.4336, -1.3408,  ...,  5.5625,  0.7334, -0.2939],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0055, -0.0103,  0.0153,  ..., -0.0057, -0.0050, -0.0109],
        [-0.0100,  0.0210,  0.0094,  ...,  0.0073,  0.0059,  0.0047],
        [ 0.0007, -0.0128,  0.0162,  ..., -0.0050,  0.0067,  0.0022],
        ...,
        [ 0.0083, -0.0052, -0.0069,  ...,  0.0125, -0.0165, -0.0022],
        [-0.0069, -0.0137,  0.0087,  ..., -0.0019,  0.0027, -0.0052],
        [-0.0120, -0.0080, -0.0031,  ..., -0.0006,  0.0099,  0.0217]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.3516,  2.6992, -1.2617,  ...,  5.2578,  1.3750, -0.0654]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 19:05:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A pill is made up of medicine
A snow is made up of water
A bronze is made up of copper
A plastic is made up of polymer
A lawn is made up of grass
A bottle is made up of glass
A omelette is made up of eggs
A bowl is made up of
2024-07-17 19:05:36 root INFO     total operator prediction time: 1692.5250625610352 seconds
2024-07-17 19:05:36 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on synonyms - intensity
2024-07-17 19:05:36 root INFO     building operator synonyms - intensity
2024-07-17 19:05:36 root INFO     [order_1_approx] starting weight calculation for A more intense word for creative is ingenious
A more intense word for jog is run
A more intense word for sea is ocean
A more intense word for ask is beg
A more intense word for strong is powerful
A more intense word for house is palace
A more intense word for monkey is gorilla
A more intense word for dislike is
2024-07-17 19:05:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 19:09:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5352, -0.9189, -0.1388,  ..., -0.1460, -0.3965,  0.6963],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5371, -1.3223,  1.7598,  ...,  3.0859,  1.8262,  1.5781],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0078, -0.0102,  0.0147,  ..., -0.0099,  0.0067, -0.0031],
        [ 0.0082, -0.0008, -0.0014,  ...,  0.0054,  0.0170, -0.0043],
        [ 0.0228, -0.0109,  0.0033,  ..., -0.0001,  0.0106,  0.0120],
        ...,
        [-0.0005, -0.0121,  0.0058,  ...,  0.0090, -0.0226, -0.0087],
        [ 0.0220,  0.0065,  0.0037,  ...,  0.0033,  0.0150,  0.0043],
        [-0.0243, -0.0028, -0.0035,  ...,  0.0105, -0.0152, -0.0033]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7573, -1.4365,  1.4170,  ...,  2.8418,  2.5859,  1.0361]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 19:09:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for creative is ingenious
A more intense word for jog is run
A more intense word for sea is ocean
A more intense word for ask is beg
A more intense word for strong is powerful
A more intense word for house is palace
A more intense word for monkey is gorilla
A more intense word for dislike is
2024-07-17 19:09:10 root INFO     [order_1_approx] starting weight calculation for A more intense word for monkey is gorilla
A more intense word for creative is ingenious
A more intense word for ask is beg
A more intense word for house is palace
A more intense word for jog is run
A more intense word for dislike is hate
A more intense word for sea is ocean
A more intense word for strong is
2024-07-17 19:09:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 19:12:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3379, -0.6982,  0.8057,  ...,  0.2349,  0.4482, -0.1769],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1827, -2.4746, -1.9453,  ..., -4.7500, -0.4072, -2.4160],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0122, -0.0181,  0.0013,  ..., -0.0137,  0.0069,  0.0006],
        [ 0.0001,  0.0040,  0.0035,  ...,  0.0004, -0.0064,  0.0027],
        [ 0.0283,  0.0009,  0.0149,  ...,  0.0121,  0.0175,  0.0011],
        ...,
        [-0.0165, -0.0150,  0.0179,  ...,  0.0050,  0.0009, -0.0179],
        [-0.0051, -0.0016, -0.0047,  ..., -0.0013,  0.0194, -0.0018],
        [-0.0142, -0.0117,  0.0185,  ..., -0.0004, -0.0027,  0.0050]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6909, -2.1992, -0.9053,  ..., -4.4102, -0.0566, -2.1172]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 19:12:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for monkey is gorilla
A more intense word for creative is ingenious
A more intense word for ask is beg
A more intense word for house is palace
A more intense word for jog is run
A more intense word for dislike is hate
A more intense word for sea is ocean
A more intense word for strong is
2024-07-17 19:12:43 root INFO     [order_1_approx] starting weight calculation for A more intense word for sea is ocean
A more intense word for ask is beg
A more intense word for dislike is hate
A more intense word for strong is powerful
A more intense word for monkey is gorilla
A more intense word for jog is run
A more intense word for house is palace
A more intense word for creative is
2024-07-17 19:12:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 19:16:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.8906, -0.6958,  0.8394,  ...,  0.2007,  0.4089,  0.0466],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.9805, -4.2344, -2.3945,  ..., -2.1055,  0.9077, -0.6733],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0014,  0.0021,  0.0082,  ..., -0.0073, -0.0076, -0.0094],
        [-0.0013, -0.0072,  0.0006,  ..., -0.0065,  0.0022,  0.0040],
        [ 0.0127,  0.0055, -0.0102,  ..., -0.0004,  0.0004, -0.0008],
        ...,
        [-0.0195, -0.0114,  0.0215,  ...,  0.0154,  0.0057, -0.0091],
        [ 0.0047, -0.0019, -0.0040,  ...,  0.0188, -0.0110, -0.0005],
        [-0.0051, -0.0100,  0.0115,  ..., -0.0054, -0.0022,  0.0034]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.1094, -3.7109, -1.4141,  ..., -1.4980,  0.4248, -0.2419]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 19:16:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for sea is ocean
A more intense word for ask is beg
A more intense word for dislike is hate
A more intense word for strong is powerful
A more intense word for monkey is gorilla
A more intense word for jog is run
A more intense word for house is palace
A more intense word for creative is
2024-07-17 19:16:18 root INFO     [order_1_approx] starting weight calculation for A more intense word for monkey is gorilla
A more intense word for sea is ocean
A more intense word for dislike is hate
A more intense word for creative is ingenious
A more intense word for ask is beg
A more intense word for house is palace
A more intense word for strong is powerful
A more intense word for jog is
2024-07-17 19:16:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 19:19:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2451, -0.4985, -1.0068,  ...,  0.8462,  0.5352,  1.0078],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8945, -1.0635, -3.0547,  ..., -6.1602,  0.2196,  1.6113],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0222, -0.0051,  0.0015,  ...,  0.0034, -0.0129,  0.0140],
        [-0.0189,  0.0119, -0.0053,  ..., -0.0048,  0.0133,  0.0013],
        [-0.0072, -0.0047,  0.0081,  ..., -0.0152,  0.0004, -0.0003],
        ...,
        [-0.0114,  0.0139, -0.0077,  ...,  0.0215, -0.0016,  0.0007],
        [-0.0192, -0.0092,  0.0079,  ..., -0.0033,  0.0115,  0.0023],
        [ 0.0018, -0.0101, -0.0012,  ..., -0.0179, -0.0057,  0.0313]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0781, -1.6309, -2.4805,  ..., -6.0156, -0.3999,  2.3867]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 19:19:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for monkey is gorilla
A more intense word for sea is ocean
A more intense word for dislike is hate
A more intense word for creative is ingenious
A more intense word for ask is beg
A more intense word for house is palace
A more intense word for strong is powerful
A more intense word for jog is
2024-07-17 19:19:52 root INFO     [order_1_approx] starting weight calculation for A more intense word for jog is run
A more intense word for sea is ocean
A more intense word for dislike is hate
A more intense word for creative is ingenious
A more intense word for ask is beg
A more intense word for strong is powerful
A more intense word for house is palace
A more intense word for monkey is
2024-07-17 19:19:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 19:23:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7798, -2.2129, -0.8789,  ...,  0.7588, -0.5776,  1.0352],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7617, -2.0312,  1.0879,  ..., -1.6523,  0.7646, -0.3853],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 7.2975e-03, -2.7542e-02, -2.1759e-02,  ...,  5.3406e-03,
         -2.1973e-03, -1.1742e-02],
        [ 1.3184e-02, -1.7303e-02, -3.9253e-03,  ...,  1.4236e-02,
          3.2272e-03,  1.2184e-02],
        [ 4.7607e-03,  1.3885e-02,  1.2527e-02,  ..., -1.7471e-03,
          3.3665e-03,  2.0157e-02],
        ...,
        [-3.7479e-03, -1.2016e-03,  7.5226e-03,  ...,  1.0483e-02,
         -8.1024e-03, -6.1493e-03],
        [ 6.0768e-03, -7.4539e-03, -6.1035e-05,  ...,  1.2665e-02,
         -3.7766e-03, -6.6452e-03],
        [-3.5286e-03, -1.3466e-02, -4.7073e-03,  ..., -8.1635e-03,
          6.9962e-03,  1.2505e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7578, -2.5039,  1.6270,  ..., -0.5303,  1.0918, -0.6440]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 19:23:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for jog is run
A more intense word for sea is ocean
A more intense word for dislike is hate
A more intense word for creative is ingenious
A more intense word for ask is beg
A more intense word for strong is powerful
A more intense word for house is palace
A more intense word for monkey is
2024-07-17 19:23:27 root INFO     [order_1_approx] starting weight calculation for A more intense word for house is palace
A more intense word for dislike is hate
A more intense word for creative is ingenious
A more intense word for sea is ocean
A more intense word for monkey is gorilla
A more intense word for jog is run
A more intense word for strong is powerful
A more intense word for ask is
2024-07-17 19:23:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 19:27:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.2500,  0.2695, -0.0341,  ..., -0.2104,  1.1582,  2.1426],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.8125, -2.5430, -4.1406,  ..., -2.2676,  0.9346,  3.1055],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0041,  0.0029,  0.0066,  ...,  0.0012, -0.0057, -0.0022],
        [-0.0118,  0.0141, -0.0073,  ..., -0.0016, -0.0046, -0.0117],
        [ 0.0115, -0.0202,  0.0058,  ..., -0.0195, -0.0017,  0.0164],
        ...,
        [ 0.0021, -0.0109,  0.0025,  ..., -0.0114,  0.0080, -0.0018],
        [ 0.0222, -0.0030, -0.0122,  ..., -0.0066,  0.0067,  0.0082],
        [-0.0064, -0.0130,  0.0006,  ..., -0.0002,  0.0057, -0.0070]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.0352, -2.8281, -3.8750,  ..., -1.6963,  0.0913,  2.2812]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 19:27:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for house is palace
A more intense word for dislike is hate
A more intense word for creative is ingenious
A more intense word for sea is ocean
A more intense word for monkey is gorilla
A more intense word for jog is run
A more intense word for strong is powerful
A more intense word for ask is
2024-07-17 19:27:01 root INFO     [order_1_approx] starting weight calculation for A more intense word for jog is run
A more intense word for dislike is hate
A more intense word for creative is ingenious
A more intense word for strong is powerful
A more intense word for sea is ocean
A more intense word for ask is beg
A more intense word for monkey is gorilla
A more intense word for house is
2024-07-17 19:27:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 19:30:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2070, -0.6123, -0.2563,  ...,  1.3594, -0.4324,  0.6318],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3877, -1.0049, -0.7671,  ..., -1.9043, -0.5986,  0.0156],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0089, -0.0024, -0.0132,  ..., -0.0057,  0.0029, -0.0064],
        [-0.0023, -0.0023,  0.0113,  ...,  0.0062, -0.0030,  0.0052],
        [-0.0011, -0.0009,  0.0017,  ..., -0.0140,  0.0038,  0.0094],
        ...,
        [-0.0096, -0.0078,  0.0219,  ...,  0.0128,  0.0076,  0.0040],
        [ 0.0182, -0.0020, -0.0235,  ..., -0.0092, -0.0073, -0.0105],
        [-0.0177, -0.0112, -0.0028,  ..., -0.0135,  0.0099,  0.0052]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6152, -1.2842, -0.5850,  ..., -2.3438, -0.3142,  1.1680]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 19:30:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for jog is run
A more intense word for dislike is hate
A more intense word for creative is ingenious
A more intense word for strong is powerful
A more intense word for sea is ocean
A more intense word for ask is beg
A more intense word for monkey is gorilla
A more intense word for house is
2024-07-17 19:30:35 root INFO     [order_1_approx] starting weight calculation for A more intense word for house is palace
A more intense word for monkey is gorilla
A more intense word for ask is beg
A more intense word for creative is ingenious
A more intense word for dislike is hate
A more intense word for strong is powerful
A more intense word for jog is run
A more intense word for sea is
2024-07-17 19:30:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 19:34:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5488,  0.0999,  0.1346,  ..., -0.5713, -0.3794,  0.9927],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2324,  2.9316, -3.1602,  ..., -0.3684,  2.1895, -2.2539],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0039, -0.0035, -0.0059,  ..., -0.0084, -0.0035,  0.0100],
        [-0.0098, -0.0235, -0.0023,  ...,  0.0031, -0.0064, -0.0219],
        [ 0.0010,  0.0039, -0.0069,  ..., -0.0060,  0.0144,  0.0120],
        ...,
        [-0.0003, -0.0007,  0.0230,  ..., -0.0114, -0.0102, -0.0004],
        [ 0.0062,  0.0140, -0.0090,  ...,  0.0043,  0.0030, -0.0091],
        [-0.0072, -0.0063,  0.0110,  ...,  0.0027, -0.0136,  0.0042]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0195,  4.0664, -3.2852,  ..., -0.2029,  1.8555, -1.0605]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 19:34:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for house is palace
A more intense word for monkey is gorilla
A more intense word for ask is beg
A more intense word for creative is ingenious
A more intense word for dislike is hate
A more intense word for strong is powerful
A more intense word for jog is run
A more intense word for sea is
2024-07-17 19:34:09 root INFO     total operator prediction time: 1712.5506632328033 seconds
2024-07-17 19:34:09 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on hypernyms - animals
2024-07-17 19:34:09 root INFO     building operator hypernyms - animals
2024-07-17 19:34:09 root INFO     [order_1_approx] starting weight calculation for The velociraptor falls into the category of dinosaur
The porcupine falls into the category of rodent
The cow falls into the category of bovid
The squirrel falls into the category of rodent
The tyrannosaurus falls into the category of dinosaur
The triceratops falls into the category of dinosaur
The allosaurus falls into the category of dinosaur
The owl falls into the category of
2024-07-17 19:34:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 19:37:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4290, -0.4656, -0.5098,  ..., -0.8237, -0.7021,  1.9570],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0664,  2.9824,  0.3784,  ...,  1.9609, -2.4473,  0.2090],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 8.9569e-03, -1.6663e-02, -7.0267e-03,  ...,  5.0735e-04,
         -1.4038e-03,  4.4975e-03],
        [ 1.2146e-02,  2.2003e-02,  7.2174e-03,  ...,  1.6953e-02,
         -3.4275e-03, -7.4005e-04],
        [ 9.5062e-03, -1.3351e-02, -1.4519e-02,  ..., -2.3785e-03,
          1.1856e-02,  9.8267e-03],
        ...,
        [ 8.7690e-04, -3.2158e-03, -5.1498e-05,  ...,  1.4565e-02,
         -3.4771e-03,  4.2877e-03],
        [ 6.8130e-03, -8.3313e-03,  7.0114e-03,  ...,  1.0719e-03,
          2.3556e-03,  2.0733e-03],
        [-1.2848e-02, -3.4008e-03,  6.2408e-03,  ..., -2.7924e-03,
         -3.4332e-03,  2.3346e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5674,  4.0938,  0.1484,  ...,  2.6523, -2.1582,  0.8257]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 19:37:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The velociraptor falls into the category of dinosaur
The porcupine falls into the category of rodent
The cow falls into the category of bovid
The squirrel falls into the category of rodent
The tyrannosaurus falls into the category of dinosaur
The triceratops falls into the category of dinosaur
The allosaurus falls into the category of dinosaur
The owl falls into the category of
2024-07-17 19:37:42 root INFO     [order_1_approx] starting weight calculation for The squirrel falls into the category of rodent
The owl falls into the category of raptor
The porcupine falls into the category of rodent
The velociraptor falls into the category of dinosaur
The allosaurus falls into the category of dinosaur
The tyrannosaurus falls into the category of dinosaur
The cow falls into the category of bovid
The triceratops falls into the category of
2024-07-17 19:37:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 19:41:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8281,  1.7773, -3.1367,  ...,  0.6504, -0.5889,  2.5625],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.5625, -2.2344,  3.7891,  ...,  2.9980, -4.4727,  4.3906],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 7.5073e-03,  1.7738e-03, -3.7253e-05,  ..., -6.2828e-03,
          2.7161e-03, -5.0812e-03],
        [ 1.7452e-04,  5.4245e-03,  3.5896e-03,  ...,  6.5536e-03,
          5.7602e-04, -1.3847e-03],
        [-1.8749e-03, -8.1635e-03,  6.3324e-03,  ..., -5.0507e-03,
         -6.0349e-03, -2.3613e-03],
        ...,
        [-7.0267e-03,  6.4163e-03, -3.3913e-03,  ...,  9.8038e-03,
          6.8130e-03,  5.4207e-03],
        [ 4.6692e-03,  1.1032e-02, -5.4359e-03,  ..., -2.8687e-03,
          1.2417e-03,  1.1505e-02],
        [-6.8130e-03, -2.8839e-03,  4.7150e-03,  ..., -7.7391e-04,
          7.0343e-03, -4.5471e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.3652, -1.9043,  4.4102,  ...,  2.6055, -4.8125,  5.1992]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 19:41:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The squirrel falls into the category of rodent
The owl falls into the category of raptor
The porcupine falls into the category of rodent
The velociraptor falls into the category of dinosaur
The allosaurus falls into the category of dinosaur
The tyrannosaurus falls into the category of dinosaur
The cow falls into the category of bovid
The triceratops falls into the category of
2024-07-17 19:41:16 root INFO     [order_1_approx] starting weight calculation for The allosaurus falls into the category of dinosaur
The cow falls into the category of bovid
The porcupine falls into the category of rodent
The velociraptor falls into the category of dinosaur
The squirrel falls into the category of rodent
The owl falls into the category of raptor
The triceratops falls into the category of dinosaur
The tyrannosaurus falls into the category of
2024-07-17 19:41:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 19:44:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4111,  0.3438, -1.2021,  ..., -0.1893,  0.9238,  2.0977],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7012, -3.3203,  5.4453,  ...,  1.8584, -3.4355,  3.2070],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0060, -0.0006, -0.0024,  ..., -0.0025, -0.0011, -0.0027],
        [ 0.0045,  0.0033,  0.0031,  ...,  0.0070,  0.0055,  0.0054],
        [-0.0008,  0.0011,  0.0034,  ...,  0.0032,  0.0015,  0.0050],
        ...,
        [-0.0038, -0.0026, -0.0056,  ...,  0.0055, -0.0005,  0.0045],
        [-0.0014, -0.0028, -0.0039,  ...,  0.0004,  0.0036,  0.0076],
        [-0.0048, -0.0022,  0.0056,  ..., -0.0019,  0.0028,  0.0051]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1797, -2.7383,  6.1445,  ...,  1.6211, -4.5820,  3.5664]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 19:44:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The allosaurus falls into the category of dinosaur
The cow falls into the category of bovid
The porcupine falls into the category of rodent
The velociraptor falls into the category of dinosaur
The squirrel falls into the category of rodent
The owl falls into the category of raptor
The triceratops falls into the category of dinosaur
The tyrannosaurus falls into the category of
2024-07-17 19:44:49 root INFO     [order_1_approx] starting weight calculation for The porcupine falls into the category of rodent
The tyrannosaurus falls into the category of dinosaur
The triceratops falls into the category of dinosaur
The cow falls into the category of bovid
The owl falls into the category of raptor
The velociraptor falls into the category of dinosaur
The allosaurus falls into the category of dinosaur
The squirrel falls into the category of
2024-07-17 19:44:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 19:48:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8701, -1.0654, -1.6045,  ...,  0.4321,  0.7207, -0.0271],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([3.0898, 2.3340, 3.6113,  ..., 2.1035, 0.1730, 0.5957], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.5177e-03, -3.4466e-03,  4.6654e-03,  ..., -1.7805e-03,
         -1.0910e-03, -1.0063e-02],
        [ 1.6155e-03,  9.3689e-03,  4.1389e-04,  ...,  7.7553e-03,
          0.0000e+00,  5.7755e-03],
        [ 1.1017e-02, -1.3077e-02,  6.8665e-05,  ...,  1.3084e-03,
          2.5177e-04,  1.0757e-02],
        ...,
        [-6.2656e-04, -2.4780e-02, -8.1015e-04,  ...,  2.3453e-02,
         -1.2827e-03,  1.8570e-02],
        [-2.4128e-03,  2.9030e-03,  3.4943e-03,  ..., -1.1549e-03,
         -5.9509e-04,  7.7362e-03],
        [-1.3779e-02, -2.8667e-03, -6.6986e-03,  ..., -7.2594e-03,
          4.6310e-03,  1.4168e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[2.7441, 2.4961, 3.8848,  ..., 2.8145, 0.3792, 1.1377]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 19:48:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The porcupine falls into the category of rodent
The tyrannosaurus falls into the category of dinosaur
The triceratops falls into the category of dinosaur
The cow falls into the category of bovid
The owl falls into the category of raptor
The velociraptor falls into the category of dinosaur
The allosaurus falls into the category of dinosaur
The squirrel falls into the category of
2024-07-17 19:48:22 root INFO     [order_1_approx] starting weight calculation for The owl falls into the category of raptor
The porcupine falls into the category of rodent
The squirrel falls into the category of rodent
The cow falls into the category of bovid
The allosaurus falls into the category of dinosaur
The tyrannosaurus falls into the category of dinosaur
The triceratops falls into the category of dinosaur
The velociraptor falls into the category of
2024-07-17 19:48:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 19:51:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7383, -0.3538, -1.8877,  ..., -0.5693,  0.1838,  2.0918],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3770, -2.7598,  3.6680,  ..., -0.9443, -3.5977,  2.4727],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0273, -0.0069, -0.0084,  ..., -0.0111, -0.0012,  0.0030],
        [ 0.0033,  0.0086,  0.0052,  ...,  0.0149,  0.0063,  0.0011],
        [-0.0068, -0.0075,  0.0145,  ...,  0.0011, -0.0019, -0.0020],
        ...,
        [-0.0083, -0.0048, -0.0044,  ...,  0.0144,  0.0037,  0.0119],
        [ 0.0066, -0.0016, -0.0125,  ..., -0.0008,  0.0062,  0.0256],
        [-0.0074,  0.0022,  0.0082,  ..., -0.0025,  0.0044,  0.0025]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6748, -2.3223,  3.9961,  ..., -0.8647, -3.8516,  3.5000]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 19:51:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The owl falls into the category of raptor
The porcupine falls into the category of rodent
The squirrel falls into the category of rodent
The cow falls into the category of bovid
The allosaurus falls into the category of dinosaur
The tyrannosaurus falls into the category of dinosaur
The triceratops falls into the category of dinosaur
The velociraptor falls into the category of
2024-07-17 19:51:55 root INFO     [order_1_approx] starting weight calculation for The porcupine falls into the category of rodent
The velociraptor falls into the category of dinosaur
The tyrannosaurus falls into the category of dinosaur
The owl falls into the category of raptor
The squirrel falls into the category of rodent
The allosaurus falls into the category of dinosaur
The triceratops falls into the category of dinosaur
The cow falls into the category of
2024-07-17 19:51:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 19:55:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6699, -0.0497, -0.9941,  ...,  0.2568, -0.1538,  1.6025],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.7578,  0.3306, -0.0083,  ..., -1.6104, -2.1641, -0.0425],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0040, -0.0013, -0.0049,  ..., -0.0027,  0.0042, -0.0010],
        [ 0.0046,  0.0048, -0.0032,  ...,  0.0013,  0.0039, -0.0079],
        [ 0.0006, -0.0041, -0.0022,  ...,  0.0025,  0.0033,  0.0054],
        ...,
        [ 0.0008, -0.0055, -0.0008,  ...,  0.0042, -0.0058,  0.0128],
        [ 0.0005, -0.0024,  0.0017,  ..., -0.0016,  0.0005,  0.0064],
        [-0.0062, -0.0089,  0.0048,  ...,  0.0062, -0.0006,  0.0065]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.3320,  0.8721,  0.3132,  ..., -1.6943, -2.2715,  0.4519]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 19:55:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The porcupine falls into the category of rodent
The velociraptor falls into the category of dinosaur
The tyrannosaurus falls into the category of dinosaur
The owl falls into the category of raptor
The squirrel falls into the category of rodent
The allosaurus falls into the category of dinosaur
The triceratops falls into the category of dinosaur
The cow falls into the category of
2024-07-17 19:55:30 root INFO     [order_1_approx] starting weight calculation for The owl falls into the category of raptor
The velociraptor falls into the category of dinosaur
The tyrannosaurus falls into the category of dinosaur
The triceratops falls into the category of dinosaur
The squirrel falls into the category of rodent
The porcupine falls into the category of rodent
The cow falls into the category of bovid
The allosaurus falls into the category of
2024-07-17 19:55:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 19:59:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2808,  0.2465, -2.1348,  ..., -0.9692,  1.0410,  1.5430],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6377, -4.2070,  4.8594,  ...,  3.5176, -3.7422,  2.4336],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.2217e-02,  2.5578e-03,  2.7351e-03,  ..., -1.3374e-02,
         -6.3896e-03, -1.1467e-02],
        [ 9.0332e-03,  5.1613e-03,  2.8801e-03,  ...,  6.7787e-03,
          7.6370e-03,  4.4785e-03],
        [-6.0349e-03, -2.6947e-02,  7.3433e-05,  ...,  1.1894e-02,
          2.4776e-03, -6.1798e-03],
        ...,
        [-1.7502e-02, -1.6022e-02,  1.3018e-03,  ...,  2.1927e-02,
          4.6768e-03, -5.7297e-03],
        [-1.1883e-03,  1.2215e-02,  1.2789e-03,  ...,  2.0618e-03,
         -3.4218e-03,  8.3694e-03],
        [-1.3611e-02, -1.3672e-02, -4.5128e-03,  ...,  5.2452e-03,
          6.7978e-03,  2.4170e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3203, -3.7344,  5.1758,  ...,  4.5742, -4.7109,  3.2871]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 19:59:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The owl falls into the category of raptor
The velociraptor falls into the category of dinosaur
The tyrannosaurus falls into the category of dinosaur
The triceratops falls into the category of dinosaur
The squirrel falls into the category of rodent
The porcupine falls into the category of rodent
The cow falls into the category of bovid
The allosaurus falls into the category of
2024-07-17 19:59:03 root INFO     [order_1_approx] starting weight calculation for The cow falls into the category of bovid
The squirrel falls into the category of rodent
The triceratops falls into the category of dinosaur
The velociraptor falls into the category of dinosaur
The allosaurus falls into the category of dinosaur
The owl falls into the category of raptor
The tyrannosaurus falls into the category of dinosaur
The porcupine falls into the category of
2024-07-17 19:59:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 20:02:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2102, -1.1680, -0.6313,  ...,  0.0063,  0.3376,  0.6406],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.5391, -1.1914, -3.0371,  ...,  0.4109, -1.4043,  3.0840],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 8.6594e-03, -1.0071e-02,  6.0425e-03,  ...,  3.9520e-03,
          3.8147e-05, -4.0894e-03],
        [ 1.0483e-02,  2.8992e-03,  1.1635e-02,  ...,  1.4420e-02,
         -1.3161e-04,  3.9864e-04],
        [ 1.1959e-03, -2.7885e-03,  5.0125e-03,  ...,  2.8248e-03,
          6.5422e-03,  2.9049e-03],
        ...,
        [-4.3526e-03, -3.7823e-03, -3.9291e-03,  ...,  1.0315e-02,
         -5.6190e-03,  1.1169e-02],
        [-5.3253e-03,  2.3518e-03, -1.4210e-04,  ..., -5.1308e-03,
          3.0460e-03,  1.6890e-03],
        [-6.5880e-03, -8.7585e-03,  5.3825e-03,  ...,  5.9280e-03,
          1.1742e-02,  9.5520e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.7754, -1.0557, -2.5273,  ...,  0.3906, -1.8213,  3.7715]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 20:02:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The cow falls into the category of bovid
The squirrel falls into the category of rodent
The triceratops falls into the category of dinosaur
The velociraptor falls into the category of dinosaur
The allosaurus falls into the category of dinosaur
The owl falls into the category of raptor
The tyrannosaurus falls into the category of dinosaur
The porcupine falls into the category of
2024-07-17 20:02:37 root INFO     total operator prediction time: 1707.8077509403229 seconds
2024-07-17 20:02:37 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on hyponyms - misc
2024-07-17 20:02:37 root INFO     building operator hyponyms - misc
2024-07-17 20:02:37 root INFO     [order_1_approx] starting weight calculation for A more specific term for a shoes is sneakers
A more specific term for a spice is pepper
A more specific term for a shirt is polo
A more specific term for a flask is thermos
A more specific term for a railway is monorail
A more specific term for a mixer is blender
A more specific term for a trousers is jeans
A more specific term for a cup is
2024-07-17 20:02:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 20:06:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0627,  0.4624, -0.4971,  ..., -0.3779, -0.8760,  1.7979],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5654,  0.4819, -0.9150,  ..., -1.1475,  1.2197,  2.9648],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0097,  0.0113, -0.0038,  ..., -0.0090,  0.0051, -0.0085],
        [-0.0034,  0.0020,  0.0166,  ...,  0.0013, -0.0178,  0.0114],
        [ 0.0013, -0.0047,  0.0144,  ..., -0.0149, -0.0019,  0.0123],
        ...,
        [-0.0304,  0.0035,  0.0066,  ...,  0.0288, -0.0144, -0.0039],
        [-0.0069, -0.0102,  0.0032,  ..., -0.0061,  0.0050,  0.0116],
        [-0.0062, -0.0036,  0.0059,  ..., -0.0004,  0.0150,  0.0233]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6670,  0.2489, -0.3799,  ..., -0.6299,  1.0527,  2.3789]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 20:06:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a shoes is sneakers
A more specific term for a spice is pepper
A more specific term for a shirt is polo
A more specific term for a flask is thermos
A more specific term for a railway is monorail
A more specific term for a mixer is blender
A more specific term for a trousers is jeans
A more specific term for a cup is
2024-07-17 20:06:13 root INFO     [order_1_approx] starting weight calculation for A more specific term for a flask is thermos
A more specific term for a trousers is jeans
A more specific term for a cup is teacup
A more specific term for a shirt is polo
A more specific term for a railway is monorail
A more specific term for a mixer is blender
A more specific term for a spice is pepper
A more specific term for a shoes is
2024-07-17 20:06:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 20:09:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2979,  0.1991, -0.5474,  ...,  0.8574, -1.4795,  0.2527],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3330, -0.6855,  2.3398,  ..., -5.8125, -4.6289,  0.2009],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.0971e-02, -1.2840e-02, -3.0842e-03,  ..., -4.5128e-03,
         -3.8548e-03,  2.6360e-03],
        [-3.6850e-03,  1.4496e-02,  1.2207e-02,  ...,  2.6901e-02,
          8.0719e-03, -1.6815e-02],
        [-1.7738e-03, -8.1940e-03,  1.4114e-02,  ..., -1.3447e-04,
          5.8784e-03,  1.3809e-02],
        ...,
        [ 5.8746e-03,  9.1705e-03,  2.0142e-03,  ...,  1.4999e-02,
         -1.5732e-02,  2.6382e-02],
        [ 2.0161e-03,  1.8692e-02, -7.6981e-03,  ...,  6.8207e-03,
         -6.2332e-03,  6.9885e-03],
        [-1.6129e-02,  2.8534e-03, -4.5776e-05,  ..., -2.8114e-03,
          3.1250e-02,  4.1534e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9604, -0.2224,  2.9609,  ..., -5.3281, -6.1836,  0.4221]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 20:09:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a flask is thermos
A more specific term for a trousers is jeans
A more specific term for a cup is teacup
A more specific term for a shirt is polo
A more specific term for a railway is monorail
A more specific term for a mixer is blender
A more specific term for a spice is pepper
A more specific term for a shoes is
2024-07-17 20:09:47 root INFO     [order_1_approx] starting weight calculation for A more specific term for a spice is pepper
A more specific term for a shoes is sneakers
A more specific term for a shirt is polo
A more specific term for a cup is teacup
A more specific term for a mixer is blender
A more specific term for a flask is thermos
A more specific term for a trousers is jeans
A more specific term for a railway is
2024-07-17 20:09:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 20:13:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4375,  0.5391, -0.4709,  ..., -0.6685, -0.2324,  0.7217],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1865, -0.9873, -0.8965,  ..., -3.5215,  3.8848, -2.4863],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0192, -0.0144,  0.0054,  ..., -0.0026, -0.0033,  0.0134],
        [ 0.0043,  0.0164, -0.0068,  ...,  0.0206, -0.0018, -0.0021],
        [-0.0128,  0.0109,  0.0080,  ...,  0.0033,  0.0011,  0.0093],
        ...,
        [-0.0120, -0.0058, -0.0108,  ...,  0.0190, -0.0127,  0.0045],
        [ 0.0101, -0.0102,  0.0059,  ...,  0.0051,  0.0132,  0.0020],
        [-0.0131, -0.0119,  0.0106,  ..., -0.0041,  0.0014,  0.0259]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5996, -0.5967, -0.9199,  ..., -3.9180,  4.8164, -1.7412]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 20:13:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a spice is pepper
A more specific term for a shoes is sneakers
A more specific term for a shirt is polo
A more specific term for a cup is teacup
A more specific term for a mixer is blender
A more specific term for a flask is thermos
A more specific term for a trousers is jeans
A more specific term for a railway is
2024-07-17 20:13:22 root INFO     [order_1_approx] starting weight calculation for A more specific term for a railway is monorail
A more specific term for a trousers is jeans
A more specific term for a flask is thermos
A more specific term for a cup is teacup
A more specific term for a mixer is blender
A more specific term for a shoes is sneakers
A more specific term for a spice is pepper
A more specific term for a shirt is
2024-07-17 20:13:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 20:16:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4016, -0.4116, -0.0549,  ..., -0.5508, -0.4045,  0.9629],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1807,  0.6582, -1.5977,  ..., -4.3281,  2.4023, -3.8457],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0036, -0.0079, -0.0098,  ...,  0.0087, -0.0056,  0.0072],
        [-0.0017,  0.0063,  0.0011,  ...,  0.0091,  0.0043, -0.0044],
        [-0.0036,  0.0057,  0.0142,  ..., -0.0048,  0.0029,  0.0143],
        ...,
        [-0.0176, -0.0033,  0.0046,  ...,  0.0371, -0.0025, -0.0047],
        [ 0.0080, -0.0032, -0.0095,  ..., -0.0111,  0.0019,  0.0163],
        [-0.0129,  0.0089,  0.0060,  ..., -0.0212,  0.0164,  0.0071]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3252,  1.0283, -2.0293,  ..., -4.0938,  1.4746, -3.5547]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 20:16:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a railway is monorail
A more specific term for a trousers is jeans
A more specific term for a flask is thermos
A more specific term for a cup is teacup
A more specific term for a mixer is blender
A more specific term for a shoes is sneakers
A more specific term for a spice is pepper
A more specific term for a shirt is
2024-07-17 20:16:55 root INFO     [order_1_approx] starting weight calculation for A more specific term for a shirt is polo
A more specific term for a shoes is sneakers
A more specific term for a trousers is jeans
A more specific term for a spice is pepper
A more specific term for a railway is monorail
A more specific term for a cup is teacup
A more specific term for a flask is thermos
A more specific term for a mixer is
2024-07-17 20:16:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 20:20:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5278,  0.1365,  0.0095,  ...,  0.7627, -1.2129,  0.9111],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1650,  5.3555, -1.1348,  ..., -0.3191,  1.7031,  2.1523],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0201,  0.0015, -0.0099,  ...,  0.0141,  0.0030,  0.0039],
        [ 0.0073,  0.0103, -0.0180,  ..., -0.0070,  0.0134,  0.0167],
        [-0.0130, -0.0081,  0.0290,  ...,  0.0022,  0.0087,  0.0137],
        ...,
        [-0.0062,  0.0071,  0.0046,  ...,  0.0455,  0.0008,  0.0011],
        [-0.0115, -0.0069, -0.0019,  ..., -0.0112,  0.0158,  0.0128],
        [-0.0139,  0.0039,  0.0002,  ...,  0.0048,  0.0171,  0.0408]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4939,  5.4961, -0.9077,  ..., -0.4941,  2.2988,  2.8594]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 20:20:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a shirt is polo
A more specific term for a shoes is sneakers
A more specific term for a trousers is jeans
A more specific term for a spice is pepper
A more specific term for a railway is monorail
A more specific term for a cup is teacup
A more specific term for a flask is thermos
A more specific term for a mixer is
2024-07-17 20:20:28 root INFO     [order_1_approx] starting weight calculation for A more specific term for a shirt is polo
A more specific term for a mixer is blender
A more specific term for a cup is teacup
A more specific term for a railway is monorail
A more specific term for a trousers is jeans
A more specific term for a shoes is sneakers
A more specific term for a spice is pepper
A more specific term for a flask is
2024-07-17 20:20:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 20:23:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6602,  0.1685, -1.1611,  ...,  0.0310, -0.5483,  2.7891],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.1484, -0.5635,  0.2451,  ...,  2.9766, -1.1455,  2.9512],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0073,  0.0122, -0.0015,  ...,  0.0029,  0.0024, -0.0051],
        [-0.0014,  0.0025,  0.0028,  ...,  0.0093, -0.0067,  0.0037],
        [-0.0050, -0.0052,  0.0026,  ..., -0.0088,  0.0114,  0.0087],
        ...,
        [-0.0114, -0.0300, -0.0047,  ...,  0.0205, -0.0186, -0.0100],
        [ 0.0102,  0.0067, -0.0026,  ...,  0.0043,  0.0116,  0.0285],
        [-0.0185, -0.0033,  0.0046,  ...,  0.0090,  0.0170,  0.0324]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.6953, -0.3667,  1.0420,  ...,  4.4727, -1.1143,  3.2012]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 20:24:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a shirt is polo
A more specific term for a mixer is blender
A more specific term for a cup is teacup
A more specific term for a railway is monorail
A more specific term for a trousers is jeans
A more specific term for a shoes is sneakers
A more specific term for a spice is pepper
A more specific term for a flask is
2024-07-17 20:24:00 root INFO     [order_1_approx] starting weight calculation for A more specific term for a spice is pepper
A more specific term for a shirt is polo
A more specific term for a cup is teacup
A more specific term for a mixer is blender
A more specific term for a shoes is sneakers
A more specific term for a railway is monorail
A more specific term for a flask is thermos
A more specific term for a trousers is
2024-07-17 20:24:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 20:27:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7344, -1.0801, -0.8027,  ...,  0.7236, -1.7842,  1.3184],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1094, -1.3359, -1.1348,  ..., -7.7578, -0.0475, -5.3281],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0103,  0.0054, -0.0187,  ...,  0.0122, -0.0159, -0.0001],
        [-0.0062,  0.0026,  0.0038,  ...,  0.0053, -0.0026, -0.0114],
        [-0.0055, -0.0050,  0.0072,  ..., -0.0040, -0.0034, -0.0045],
        ...,
        [-0.0101,  0.0152,  0.0101,  ...,  0.0284,  0.0008, -0.0054],
        [ 0.0063,  0.0103, -0.0012,  ..., -0.0060,  0.0016,  0.0077],
        [-0.0105,  0.0039,  0.0145,  ..., -0.0051,  0.0101,  0.0148]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8530, -0.8506, -0.6484,  ..., -7.4180, -0.5933, -5.6602]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 20:27:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a spice is pepper
A more specific term for a shirt is polo
A more specific term for a cup is teacup
A more specific term for a mixer is blender
A more specific term for a shoes is sneakers
A more specific term for a railway is monorail
A more specific term for a flask is thermos
A more specific term for a trousers is
2024-07-17 20:27:34 root INFO     [order_1_approx] starting weight calculation for A more specific term for a mixer is blender
A more specific term for a flask is thermos
A more specific term for a trousers is jeans
A more specific term for a railway is monorail
A more specific term for a shoes is sneakers
A more specific term for a shirt is polo
A more specific term for a cup is teacup
A more specific term for a spice is
2024-07-17 20:27:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 20:31:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2695,  0.8706, -0.1086,  ..., -0.8413, -0.4736, -0.1094],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.0020,  1.3877,  0.3857,  ...,  0.7788,  2.4551, -0.3772],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0072, -0.0080,  0.0064,  ...,  0.0012, -0.0072, -0.0063],
        [-0.0167,  0.0043,  0.0008,  ..., -0.0074, -0.0085, -0.0093],
        [-0.0041, -0.0064,  0.0137,  ...,  0.0118, -0.0059,  0.0038],
        ...,
        [-0.0174, -0.0098, -0.0003,  ...,  0.0312, -0.0130, -0.0027],
        [-0.0052,  0.0051, -0.0056,  ...,  0.0006,  0.0085, -0.0026],
        [-0.0035,  0.0151, -0.0034,  ..., -0.0043,  0.0126,  0.0314]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.5156,  1.7734,  0.2546,  ...,  0.2334,  2.6699,  0.2595]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 20:31:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a mixer is blender
A more specific term for a flask is thermos
A more specific term for a trousers is jeans
A more specific term for a railway is monorail
A more specific term for a shoes is sneakers
A more specific term for a shirt is polo
A more specific term for a cup is teacup
A more specific term for a spice is
2024-07-17 20:31:04 root INFO     total operator prediction time: 1707.2080481052399 seconds
2024-07-17 20:31:04 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on antonyms - binary
2024-07-17 20:31:04 root INFO     building operator antonyms - binary
2024-07-17 20:31:04 root INFO     [order_1_approx] starting weight calculation for The opposite of toward is away
The opposite of over is under
The opposite of after is before
The opposite of drop is lift
The opposite of downslope is upslope
The opposite of inverse is reverse
The opposite of anterior is posterior
The opposite of down is
2024-07-17 20:31:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 20:34:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4673, -0.6680, -0.4614,  ..., -1.1602, -1.1025, -0.4329],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0781, -4.9102,  4.2500,  ..., -1.2754,  6.8828, -1.1357],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0049, -0.0129, -0.0061,  ..., -0.0085, -0.0051,  0.0044],
        [-0.0140,  0.0036, -0.0041,  ...,  0.0025,  0.0072,  0.0042],
        [-0.0140, -0.0003, -0.0157,  ..., -0.0120, -0.0035,  0.0187],
        ...,
        [-0.0058, -0.0146,  0.0175,  ..., -0.0078, -0.0082,  0.0085],
        [-0.0163, -0.0121,  0.0161,  ..., -0.0259, -0.0098,  0.0120],
        [ 0.0069,  0.0063,  0.0070,  ...,  0.0158, -0.0071, -0.0204]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3594, -4.5078,  3.5664,  ..., -2.1816,  8.6719, -1.8555]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 20:34:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of toward is away
The opposite of over is under
The opposite of after is before
The opposite of drop is lift
The opposite of downslope is upslope
The opposite of inverse is reverse
The opposite of anterior is posterior
The opposite of down is
2024-07-17 20:34:38 root INFO     [order_1_approx] starting weight calculation for The opposite of after is before
The opposite of anterior is posterior
The opposite of drop is lift
The opposite of inverse is reverse
The opposite of downslope is upslope
The opposite of toward is away
The opposite of down is up
The opposite of over is
2024-07-17 20:34:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 20:38:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4167, -0.2683,  0.4492,  ..., -0.0216,  0.7041,  0.8628],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0674, -1.3887, -1.1836,  ...,  0.3291,  5.5859, -1.6133],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.8854e-02, -2.5360e-02,  2.2354e-02,  ..., -2.5368e-03,
         -5.3406e-05, -1.4542e-02],
        [-2.1667e-02,  1.6098e-02, -6.9427e-03,  ...,  1.0330e-02,
          1.5335e-03,  2.1774e-02],
        [ 9.0714e-03, -4.3945e-03, -3.7384e-03,  ..., -5.6000e-03,
         -1.5259e-04,  3.2379e-02],
        ...,
        [-7.1335e-03,  2.2278e-03,  1.8749e-03,  ...,  6.8855e-03,
          2.9716e-03, -6.5460e-03],
        [-1.1627e-02,  8.1482e-03,  2.3918e-03,  ..., -1.8768e-02,
         -7.2289e-03,  5.6000e-03],
        [ 5.2261e-03, -3.0304e-02, -1.1932e-02,  ...,  9.3079e-03,
          4.8790e-03, -1.0956e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7910, -2.1895, -1.2783,  ...,  1.0000,  5.6211, -1.8789]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 20:38:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of after is before
The opposite of anterior is posterior
The opposite of drop is lift
The opposite of inverse is reverse
The opposite of downslope is upslope
The opposite of toward is away
The opposite of down is up
The opposite of over is
2024-07-17 20:38:12 root INFO     [order_1_approx] starting weight calculation for The opposite of down is up
The opposite of over is under
The opposite of drop is lift
The opposite of inverse is reverse
The opposite of downslope is upslope
The opposite of after is before
The opposite of anterior is posterior
The opposite of toward is
2024-07-17 20:38:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 20:41:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5605, -0.2500, -0.6943,  ...,  0.7441,  0.6792,  0.2434],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.4766,  1.0840,  2.1680,  ...,  0.4756,  7.5234, -1.2891],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0332, -0.0332,  0.0005,  ..., -0.0219, -0.0109,  0.0033],
        [-0.0217, -0.0230, -0.0303,  ..., -0.0003,  0.0057,  0.0176],
        [-0.0130, -0.0039, -0.0481,  ...,  0.0089,  0.0095,  0.0151],
        ...,
        [-0.0122, -0.0207,  0.0150,  ..., -0.0204,  0.0078,  0.0152],
        [-0.0322,  0.0018,  0.0245,  ...,  0.0136, -0.0365,  0.0140],
        [-0.0115, -0.0294,  0.0077,  ..., -0.0059,  0.0182, -0.0208]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.3223,  0.8892,  1.2559,  ...,  0.1809,  6.8750, -0.3647]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 20:41:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of down is up
The opposite of over is under
The opposite of drop is lift
The opposite of inverse is reverse
The opposite of downslope is upslope
The opposite of after is before
The opposite of anterior is posterior
The opposite of toward is
2024-07-17 20:41:45 root INFO     [order_1_approx] starting weight calculation for The opposite of anterior is posterior
The opposite of down is up
The opposite of toward is away
The opposite of over is under
The opposite of inverse is reverse
The opposite of drop is lift
The opposite of after is before
The opposite of downslope is
2024-07-17 20:41:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 20:45:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7964,  0.1672,  0.6660,  ..., -1.1602,  0.5249, -1.2686],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7324, -2.9258,  1.8242,  ...,  0.1172,  3.3945,  0.0239],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0073, -0.0099,  0.0052,  ...,  0.0057, -0.0133, -0.0154],
        [-0.0012,  0.0001, -0.0113,  ...,  0.0140,  0.0007,  0.0197],
        [ 0.0021,  0.0008,  0.0027,  ..., -0.0218,  0.0062, -0.0061],
        ...,
        [-0.0003, -0.0061,  0.0014,  ..., -0.0142, -0.0005,  0.0020],
        [-0.0058,  0.0092,  0.0092,  ..., -0.0015, -0.0034,  0.0055],
        [-0.0018, -0.0042, -0.0054,  ..., -0.0010, -0.0023, -0.0070]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.4375, -2.9512,  2.0078,  ...,  0.1381,  2.9141, -0.9434]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 20:45:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of anterior is posterior
The opposite of down is up
The opposite of toward is away
The opposite of over is under
The opposite of inverse is reverse
The opposite of drop is lift
The opposite of after is before
The opposite of downslope is
2024-07-17 20:45:17 root INFO     [order_1_approx] starting weight calculation for The opposite of inverse is reverse
The opposite of downslope is upslope
The opposite of over is under
The opposite of toward is away
The opposite of anterior is posterior
The opposite of down is up
The opposite of drop is lift
The opposite of after is
2024-07-17 20:45:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 20:48:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0750, -0.9424,  0.9722,  ...,  0.6343,  0.2539,  0.6738],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9199, -0.7920, -3.0957,  ..., -0.2666,  7.2539,  2.4746],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0180, -0.0092,  0.0124,  ...,  0.0030, -0.0110, -0.0029],
        [-0.0341,  0.0019, -0.0154,  ..., -0.0022, -0.0146,  0.0038],
        [ 0.0050, -0.0102, -0.0387,  ..., -0.0025,  0.0263,  0.0309],
        ...,
        [-0.0265, -0.0044, -0.0281,  ...,  0.0069, -0.0130,  0.0046],
        [-0.0073, -0.0039,  0.0266,  ..., -0.0025, -0.0151, -0.0159],
        [ 0.0047, -0.0189, -0.0014,  ...,  0.0040,  0.0080, -0.0107]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9980, -1.0078, -3.7559,  ..., -0.4731,  8.1875,  2.4141]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 20:48:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of inverse is reverse
The opposite of downslope is upslope
The opposite of over is under
The opposite of toward is away
The opposite of anterior is posterior
The opposite of down is up
The opposite of drop is lift
The opposite of after is
2024-07-17 20:48:50 root INFO     [order_1_approx] starting weight calculation for The opposite of drop is lift
The opposite of anterior is posterior
The opposite of after is before
The opposite of down is up
The opposite of downslope is upslope
The opposite of toward is away
The opposite of over is under
The opposite of inverse is
2024-07-17 20:48:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 20:52:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0225, -1.1094,  1.4951,  ..., -0.3247,  0.4939,  0.1294],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0459, -2.6367,  1.1484,  ..., -0.9785,  2.1875, -2.8633],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0321, -0.0170, -0.0052,  ..., -0.0064, -0.0232, -0.0075],
        [ 0.0098, -0.0118, -0.0295,  ..., -0.0154, -0.0070, -0.0046],
        [-0.0005,  0.0123, -0.0283,  ..., -0.0075, -0.0130,  0.0235],
        ...,
        [-0.0005, -0.0127, -0.0171,  ..., -0.0065,  0.0178,  0.0201],
        [-0.0065, -0.0016, -0.0058,  ..., -0.0006, -0.0118, -0.0188],
        [ 0.0386,  0.0078,  0.0239,  ...,  0.0186,  0.0049, -0.0022]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4482, -3.1777, -0.0566,  ..., -0.5371,  2.2227, -2.8047]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 20:52:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of drop is lift
The opposite of anterior is posterior
The opposite of after is before
The opposite of down is up
The opposite of downslope is upslope
The opposite of toward is away
The opposite of over is under
The opposite of inverse is
2024-07-17 20:52:23 root INFO     [order_1_approx] starting weight calculation for The opposite of toward is away
The opposite of over is under
The opposite of inverse is reverse
The opposite of downslope is upslope
The opposite of after is before
The opposite of anterior is posterior
The opposite of down is up
The opposite of drop is
2024-07-17 20:52:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 20:55:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6265, -0.0684, -0.0530,  ...,  0.0601,  0.0083,  0.6958],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3926, -3.7988, -2.9062,  ..., -0.3760, -0.4529,  4.1602],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0121, -0.0182, -0.0087,  ...,  0.0159, -0.0155, -0.0258],
        [ 0.0003, -0.0229, -0.0105,  ..., -0.0117, -0.0051,  0.0153],
        [ 0.0013, -0.0163, -0.0201,  ..., -0.0287, -0.0004,  0.0072],
        ...,
        [ 0.0080, -0.0206, -0.0237,  ..., -0.0138,  0.0057,  0.0053],
        [ 0.0142,  0.0046, -0.0110,  ..., -0.0168,  0.0083, -0.0032],
        [-0.0213, -0.0104,  0.0080,  ...,  0.0087,  0.0076, -0.0104]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6758, -3.7109, -3.4844,  ..., -0.2554, -1.0361,  4.6758]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 20:55:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of toward is away
The opposite of over is under
The opposite of inverse is reverse
The opposite of downslope is upslope
The opposite of after is before
The opposite of anterior is posterior
The opposite of down is up
The opposite of drop is
2024-07-17 20:55:56 root INFO     [order_1_approx] starting weight calculation for The opposite of inverse is reverse
The opposite of downslope is upslope
The opposite of after is before
The opposite of toward is away
The opposite of drop is lift
The opposite of down is up
The opposite of over is under
The opposite of anterior is
2024-07-17 20:55:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 20:59:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5273, -0.8447,  0.3481,  ...,  0.0122,  1.1719,  0.7148],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5703, -4.2812, -5.4141,  ...,  2.4668,  1.9863,  0.9751],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.3481e-02, -3.2425e-04,  1.2718e-02,  ..., -4.6539e-04,
         -1.2405e-02, -2.6047e-02],
        [-2.3651e-04,  3.5248e-03, -1.6891e-02,  ..., -6.8932e-03,
         -1.0941e-02,  1.2848e-02],
        [-3.8223e-03, -1.1917e-02, -3.5889e-02,  ...,  1.1444e-05,
          1.7456e-02, -1.3367e-02],
        ...,
        [-8.4839e-03,  2.1477e-03, -1.0529e-03,  ...,  9.5139e-03,
         -1.7376e-03,  9.2545e-03],
        [ 3.5248e-03,  4.0131e-03,  1.4221e-02,  ..., -1.0544e-02,
         -1.3756e-02, -4.6577e-03],
        [-7.9193e-03, -1.8280e-02,  4.2267e-03,  ..., -1.5617e-02,
         -1.6418e-02, -2.4090e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.2402, -3.5078, -6.3945,  ...,  2.4199,  1.6641,  0.6938]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 20:59:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of inverse is reverse
The opposite of downslope is upslope
The opposite of after is before
The opposite of toward is away
The opposite of drop is lift
The opposite of down is up
The opposite of over is under
The opposite of anterior is
2024-07-17 20:59:28 root INFO     total operator prediction time: 1704.4474070072174 seconds
2024-07-17 20:59:28 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - member
2024-07-17 20:59:28 root INFO     building operator meronyms - member
2024-07-17 20:59:29 root INFO     [order_1_approx] starting weight calculation for A tree is a member of a forest
A soldier is a member of a army
A kitten is a member of a litter
A employee is a member of a staff
A senator is a member of a senate
A listener is a member of a audience
A wolf is a member of a pack
A calf is a member of a
2024-07-17 20:59:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 21:03:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1787,  0.1921, -1.9990,  ...,  0.1919, -0.8628, -1.0205],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5801,  3.5645, -4.6016,  ..., -1.1641, -0.0337,  2.5039],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0046,  0.0001, -0.0008,  ..., -0.0094, -0.0060, -0.0104],
        [-0.0108,  0.0147,  0.0048,  ...,  0.0118, -0.0043,  0.0026],
        [ 0.0040, -0.0103,  0.0010,  ..., -0.0069,  0.0046, -0.0065],
        ...,
        [ 0.0131, -0.0080, -0.0054,  ...,  0.0044,  0.0085,  0.0012],
        [-0.0010,  0.0005,  0.0044,  ...,  0.0011, -0.0043,  0.0036],
        [-0.0014, -0.0023, -0.0049,  ..., -0.0053,  0.0106,  0.0058]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3164,  3.8555, -4.6562,  ..., -0.4541, -0.2883,  2.2617]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 21:03:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A tree is a member of a forest
A soldier is a member of a army
A kitten is a member of a litter
A employee is a member of a staff
A senator is a member of a senate
A listener is a member of a audience
A wolf is a member of a pack
A calf is a member of a
2024-07-17 21:03:03 root INFO     [order_1_approx] starting weight calculation for A tree is a member of a forest
A soldier is a member of a army
A senator is a member of a senate
A kitten is a member of a litter
A calf is a member of a cattle
A employee is a member of a staff
A wolf is a member of a pack
A listener is a member of a
2024-07-17 21:03:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 21:06:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4963, -0.9111,  0.1167,  ...,  0.6548,  0.8359,  0.2009],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1504,  3.9590, -1.3301,  ..., -2.1738,  2.6719,  2.8086],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.8646e-02,  2.4719e-03,  5.2452e-03,  ..., -4.2381e-03,
          1.1864e-03,  3.9062e-03],
        [ 4.7379e-03,  1.5701e-02,  1.6449e-02,  ...,  1.6785e-02,
          3.7975e-03,  4.8332e-03],
        [ 6.0349e-03, -8.2016e-03,  1.4099e-02,  ..., -3.6125e-03,
          3.7689e-03, -5.8136e-03],
        ...,
        [-1.3214e-02, -7.4291e-04, -1.6775e-03,  ...,  1.5472e-02,
         -8.1863e-03, -6.0654e-04],
        [ 1.6174e-03, -4.6196e-03,  4.9591e-05,  ...,  5.3406e-03,
         -1.1177e-03, -9.5215e-03],
        [-9.0637e-03,  5.1193e-03, -3.6030e-03,  ..., -5.1880e-03,
          1.2939e-02,  1.0162e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3447,  3.7520, -1.7344,  ..., -2.2559,  2.5449,  2.7578]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 21:06:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A tree is a member of a forest
A soldier is a member of a army
A senator is a member of a senate
A kitten is a member of a litter
A calf is a member of a cattle
A employee is a member of a staff
A wolf is a member of a pack
A listener is a member of a
2024-07-17 21:06:36 root INFO     [order_1_approx] starting weight calculation for A senator is a member of a senate
A tree is a member of a forest
A calf is a member of a cattle
A kitten is a member of a litter
A wolf is a member of a pack
A listener is a member of a audience
A employee is a member of a staff
A soldier is a member of a
2024-07-17 21:06:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 21:10:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2842,  0.0759,  0.1307,  ...,  0.5288,  0.6406, -0.0599],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6689,  1.6299, -1.1758,  ..., -3.5508,  1.5439, -0.7490],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0013, -0.0023,  0.0133,  ..., -0.0007, -0.0030, -0.0115],
        [-0.0053,  0.0183,  0.0084,  ...,  0.0094, -0.0048, -0.0004],
        [ 0.0125,  0.0038, -0.0052,  ...,  0.0041,  0.0114,  0.0051],
        ...,
        [-0.0065, -0.0047, -0.0082,  ..., -0.0017,  0.0008,  0.0008],
        [ 0.0004,  0.0071, -0.0026,  ...,  0.0156, -0.0069, -0.0004],
        [ 0.0063,  0.0038,  0.0039,  ..., -0.0066,  0.0175,  0.0035]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5410,  1.7754, -1.4824,  ..., -4.0469,  1.4492, -1.0732]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 21:10:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A senator is a member of a senate
A tree is a member of a forest
A calf is a member of a cattle
A kitten is a member of a litter
A wolf is a member of a pack
A listener is a member of a audience
A employee is a member of a staff
A soldier is a member of a
2024-07-17 21:10:09 root INFO     [order_1_approx] starting weight calculation for A listener is a member of a audience
A wolf is a member of a pack
A soldier is a member of a army
A employee is a member of a staff
A tree is a member of a forest
A kitten is a member of a litter
A calf is a member of a cattle
A senator is a member of a
2024-07-17 21:10:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 21:13:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.9043,  0.1252, -0.0669,  ...,  1.5713,  0.8452, -1.0898],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4414,  5.0508, -0.7334,  ...,  1.6201,  0.2837,  1.9141],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0010, -0.0072,  0.0055,  ..., -0.0054,  0.0007, -0.0048],
        [-0.0128, -0.0065, -0.0047,  ...,  0.0113,  0.0007,  0.0121],
        [ 0.0107, -0.0006, -0.0028,  ..., -0.0036,  0.0047,  0.0025],
        ...,
        [-0.0045, -0.0005, -0.0043,  ...,  0.0141, -0.0087,  0.0015],
        [ 0.0011, -0.0023,  0.0029,  ...,  0.0055, -0.0013, -0.0028],
        [-0.0106,  0.0050,  0.0038,  ...,  0.0010,  0.0088,  0.0111]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5098,  5.2734, -0.8076,  ...,  1.4014,  0.1802,  1.9482]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 21:13:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A listener is a member of a audience
A wolf is a member of a pack
A soldier is a member of a army
A employee is a member of a staff
A tree is a member of a forest
A kitten is a member of a litter
A calf is a member of a cattle
A senator is a member of a
2024-07-17 21:13:43 root INFO     [order_1_approx] starting weight calculation for A soldier is a member of a army
A wolf is a member of a pack
A calf is a member of a cattle
A kitten is a member of a litter
A listener is a member of a audience
A senator is a member of a senate
A employee is a member of a staff
A tree is a member of a
2024-07-17 21:13:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 21:17:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4600,  0.2168,  0.6353,  ...,  0.0161, -1.8926, -0.3337],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.7148, -0.9360, -3.1250,  ...,  2.4492,  2.0547,  0.8057],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0044,  0.0011,  0.0135,  ..., -0.0020, -0.0039,  0.0075],
        [ 0.0038,  0.0031,  0.0035,  ...,  0.0021, -0.0030,  0.0081],
        [-0.0022, -0.0022,  0.0014,  ..., -0.0107,  0.0030, -0.0046],
        ...,
        [ 0.0014, -0.0008,  0.0049,  ...,  0.0059, -0.0044,  0.0025],
        [-0.0003,  0.0050, -0.0072,  ...,  0.0010,  0.0057, -0.0030],
        [ 0.0051, -0.0031,  0.0056,  ..., -0.0130,  0.0098,  0.0035]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.3008, -1.2832, -3.3613,  ...,  2.1992,  1.6895,  0.5728]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 21:17:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A soldier is a member of a army
A wolf is a member of a pack
A calf is a member of a cattle
A kitten is a member of a litter
A listener is a member of a audience
A senator is a member of a senate
A employee is a member of a staff
A tree is a member of a
2024-07-17 21:17:18 root INFO     [order_1_approx] starting weight calculation for A senator is a member of a senate
A listener is a member of a audience
A soldier is a member of a army
A calf is a member of a cattle
A wolf is a member of a pack
A kitten is a member of a litter
A tree is a member of a forest
A employee is a member of a
2024-07-17 21:17:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 21:20:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1797,  0.1771,  0.4751,  ...,  1.2402, -0.1166,  0.1255],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4771, -3.0664,  1.3945,  ...,  0.1240,  2.9941,  1.9902],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 8.8959e-03, -2.1019e-03, -3.0441e-03,  ...,  4.6921e-04,
         -1.7538e-03,  4.0054e-03],
        [-4.2267e-03,  4.6005e-03, -1.6403e-04,  ...,  2.8000e-03,
          3.8643e-03, -4.2076e-03],
        [ 7.9803e-03,  7.2060e-03,  2.9831e-03,  ..., -4.5509e-03,
          8.3160e-04, -3.5763e-05],
        ...,
        [-9.0485e-03, -6.6910e-03, -8.5449e-03,  ...,  9.4147e-03,
         -4.5700e-03,  1.0433e-03],
        [ 2.9221e-03, -6.2981e-03,  1.5297e-03,  ...,  2.2507e-03,
          2.0523e-03, -2.8419e-03],
        [-1.1711e-02,  1.4992e-03,  3.2959e-03,  ...,  1.0773e-02,
          1.2215e-02,  1.0406e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4385, -2.5918,  1.4219,  ..., -0.3096,  2.7441,  1.6758]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 21:20:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A senator is a member of a senate
A listener is a member of a audience
A soldier is a member of a army
A calf is a member of a cattle
A wolf is a member of a pack
A kitten is a member of a litter
A tree is a member of a forest
A employee is a member of a
2024-07-17 21:20:52 root INFO     [order_1_approx] starting weight calculation for A senator is a member of a senate
A soldier is a member of a army
A tree is a member of a forest
A calf is a member of a cattle
A wolf is a member of a pack
A employee is a member of a staff
A listener is a member of a audience
A kitten is a member of a
2024-07-17 21:20:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 21:24:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0879,  0.0748, -1.9395,  ...,  0.0413, -1.0361, -0.1467],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5195,  1.6035, -0.0288,  ..., -1.0859, -0.2781,  0.9287],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0143, -0.0143,  0.0001,  ..., -0.0044, -0.0015,  0.0008],
        [ 0.0043,  0.0130,  0.0115,  ...,  0.0149, -0.0047,  0.0026],
        [-0.0018, -0.0077,  0.0025,  ...,  0.0022,  0.0026,  0.0088],
        ...,
        [-0.0006,  0.0025, -0.0041,  ...,  0.0120, -0.0019,  0.0065],
        [ 0.0098, -0.0054,  0.0168,  ..., -0.0100, -0.0041,  0.0054],
        [ 0.0016, -0.0064, -0.0086,  ..., -0.0041,  0.0079,  0.0141]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5142,  1.7520,  0.0088,  ..., -1.0703, -0.4607,  0.6064]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 21:24:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A senator is a member of a senate
A soldier is a member of a army
A tree is a member of a forest
A calf is a member of a cattle
A wolf is a member of a pack
A employee is a member of a staff
A listener is a member of a audience
A kitten is a member of a
2024-07-17 21:24:26 root INFO     [order_1_approx] starting weight calculation for A calf is a member of a cattle
A employee is a member of a staff
A soldier is a member of a army
A senator is a member of a senate
A kitten is a member of a litter
A listener is a member of a audience
A tree is a member of a forest
A wolf is a member of a
2024-07-17 21:24:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 21:27:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2871, -0.3125, -1.0859,  ...,  0.7441, -1.1348, -0.3267],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7178, -0.8794, -2.9570,  ...,  2.9961,  1.0225,  2.3047],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0205, -0.0137,  0.0004,  ..., -0.0022,  0.0004,  0.0034],
        [ 0.0026,  0.0070,  0.0064,  ...,  0.0143, -0.0051, -0.0055],
        [ 0.0114,  0.0043,  0.0182,  ..., -0.0034, -0.0092,  0.0049],
        ...,
        [-0.0096, -0.0046, -0.0007,  ...,  0.0165, -0.0002,  0.0131],
        [ 0.0011, -0.0108,  0.0034,  ...,  0.0002, -0.0025, -0.0065],
        [-0.0046, -0.0022,  0.0062,  ..., -0.0023,  0.0059,  0.0035]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5508, -1.0977, -2.7910,  ...,  3.1406,  0.7275,  2.2070]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 21:27:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A calf is a member of a cattle
A employee is a member of a staff
A soldier is a member of a army
A senator is a member of a senate
A kitten is a member of a litter
A listener is a member of a audience
A tree is a member of a forest
A wolf is a member of a
2024-07-17 21:27:59 root INFO     total operator prediction time: 1711.1867020130157 seconds
2024-07-17 21:27:59 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on noun - plural_irreg
2024-07-17 21:27:59 root INFO     building operator noun - plural_irreg
2024-07-17 21:28:00 root INFO     [order_1_approx] starting weight calculation for The plural form of variety is varieties
The plural form of academy is academies
The plural form of secretary is secretaries
The plural form of property is properties
The plural form of majority is majorities
The plural form of analysis is analyses
The plural form of activity is activities
The plural form of business is
2024-07-17 21:28:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 21:31:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5098,  0.7256,  0.7852,  ..., -0.2454, -0.0658,  0.0801],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-5.0781, -0.6045, -1.4941,  ..., -0.3828,  1.1064,  1.1787],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0126, -0.0089,  0.0141,  ..., -0.0117,  0.0117, -0.0261],
        [-0.0050, -0.0028, -0.0006,  ...,  0.0064, -0.0033,  0.0017],
        [ 0.0088, -0.0018, -0.0142,  ..., -0.0130,  0.0289,  0.0097],
        ...,
        [-0.0004, -0.0101, -0.0090,  ...,  0.0045,  0.0038, -0.0236],
        [ 0.0034, -0.0160,  0.0132,  ..., -0.0024, -0.0094,  0.0035],
        [-0.0121,  0.0078, -0.0012,  ..., -0.0188,  0.0002,  0.0076]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.3672, -0.2817, -1.2422,  ..., -0.1636,  0.9795,  1.3818]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 21:31:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of variety is varieties
The plural form of academy is academies
The plural form of secretary is secretaries
The plural form of property is properties
The plural form of majority is majorities
The plural form of analysis is analyses
The plural form of activity is activities
The plural form of business is
2024-07-17 21:31:32 root INFO     [order_1_approx] starting weight calculation for The plural form of academy is academies
The plural form of majority is majorities
The plural form of analysis is analyses
The plural form of business is businesses
The plural form of property is properties
The plural form of variety is varieties
The plural form of secretary is secretaries
The plural form of activity is
2024-07-17 21:31:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 21:35:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2725,  1.1250, -0.0117,  ...,  0.3555,  0.1361, -0.4775],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.3945,  2.4160,  1.5645,  ..., -1.5166,  5.0820,  2.9414],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-3.0441e-03, -7.3853e-03,  1.3069e-02,  ..., -7.2403e-03,
         -9.9716e-03, -2.3529e-02],
        [-5.6763e-03, -4.9210e-03, -1.3885e-02,  ...,  5.8060e-03,
         -8.2779e-03,  5.2414e-03],
        [ 1.7426e-02,  3.0918e-03, -1.8448e-02,  ..., -1.1215e-02,
          1.8372e-02, -4.1656e-03],
        ...,
        [-1.1444e-03, -1.8253e-03,  1.0765e-02,  ...,  4.0054e-05,
         -1.4057e-03, -8.2397e-03],
        [-1.5152e-02, -7.4463e-03,  1.0239e-02,  ..., -1.0147e-02,
         -1.9958e-02,  1.9608e-02],
        [-1.2230e-02,  1.9272e-02, -5.7831e-03,  ..., -1.4130e-02,
         -2.1820e-03, -1.9241e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.3281,  2.3672,  1.5635,  ..., -1.8105,  5.3281,  3.7344]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 21:35:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of academy is academies
The plural form of majority is majorities
The plural form of analysis is analyses
The plural form of business is businesses
The plural form of property is properties
The plural form of variety is varieties
The plural form of secretary is secretaries
The plural form of activity is
2024-07-17 21:35:04 root INFO     [order_1_approx] starting weight calculation for The plural form of analysis is analyses
The plural form of academy is academies
The plural form of business is businesses
The plural form of secretary is secretaries
The plural form of property is properties
The plural form of majority is majorities
The plural form of activity is activities
The plural form of variety is
2024-07-17 21:35:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 21:38:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.8438,  0.6992, -1.0938,  ..., -1.6211,  0.0862,  0.6494],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.7539,  0.5762, -3.2148,  ...,  1.2520,  2.8418,  2.4219],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0018, -0.0119,  0.0192,  ..., -0.0238, -0.0086, -0.0173],
        [-0.0153, -0.0046, -0.0003,  ...,  0.0152, -0.0086,  0.0004],
        [ 0.0201,  0.0014, -0.0154,  ...,  0.0012,  0.0282, -0.0071],
        ...,
        [-0.0027, -0.0004,  0.0127,  ...,  0.0030,  0.0008, -0.0102],
        [-0.0162, -0.0328,  0.0016,  ..., -0.0183, -0.0276,  0.0101],
        [-0.0104,  0.0361,  0.0089,  ..., -0.0170, -0.0130, -0.0147]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.6953,  0.8838, -2.9727,  ...,  0.6177,  3.3555,  2.2188]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 21:38:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of analysis is analyses
The plural form of academy is academies
The plural form of business is businesses
The plural form of secretary is secretaries
The plural form of property is properties
The plural form of majority is majorities
The plural form of activity is activities
The plural form of variety is
2024-07-17 21:38:38 root INFO     [order_1_approx] starting weight calculation for The plural form of majority is majorities
The plural form of secretary is secretaries
The plural form of business is businesses
The plural form of activity is activities
The plural form of variety is varieties
The plural form of analysis is analyses
The plural form of property is properties
The plural form of academy is
2024-07-17 21:38:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 21:42:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4023,  0.1935, -0.8477,  ...,  0.1449,  0.2957,  1.6133],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.9629,  1.9434, -5.1953,  ..., -4.9648, -4.6680,  3.7539],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-6.5575e-03, -1.1948e-02,  1.7685e-02,  ..., -1.8997e-03,
          5.1117e-04, -1.9638e-02],
        [-5.1498e-03,  4.3144e-03, -1.9043e-02,  ...,  1.9440e-02,
         -7.8011e-03,  3.9902e-03],
        [ 1.0719e-02,  1.3351e-02, -3.6469e-03,  ...,  3.6240e-04,
          2.0691e-02, -9.1095e-03],
        ...,
        [ 3.6011e-03,  1.6794e-03, -1.5182e-02,  ..., -5.2071e-03,
          4.7188e-03, -1.1703e-02],
        [ 3.0193e-03, -1.0605e-02,  1.7761e-02,  ..., -2.0638e-03,
         -1.5350e-02,  7.9880e-03],
        [-2.5787e-02,  1.4580e-02, -8.6784e-05,  ..., -1.4221e-02,
          2.0542e-03, -1.5594e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.5527,  2.3086, -5.1719,  ..., -4.8633, -4.6094,  3.1660]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 21:42:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of majority is majorities
The plural form of secretary is secretaries
The plural form of business is businesses
The plural form of activity is activities
The plural form of variety is varieties
The plural form of analysis is analyses
The plural form of property is properties
The plural form of academy is
2024-07-17 21:42:11 root INFO     [order_1_approx] starting weight calculation for The plural form of majority is majorities
The plural form of secretary is secretaries
The plural form of business is businesses
The plural form of variety is varieties
The plural form of activity is activities
The plural form of analysis is analyses
The plural form of academy is academies
The plural form of property is
2024-07-17 21:42:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 21:45:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.3633, -0.9873, -0.6943,  ...,  1.3174, -0.5059, -0.6035],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8799, -3.2441, -0.2197,  ..., -0.3730, -4.3203, -0.8369],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0068, -0.0065,  0.0276,  ...,  0.0033, -0.0018, -0.0181],
        [-0.0115, -0.0110, -0.0050,  ...,  0.0085, -0.0041, -0.0079],
        [ 0.0110,  0.0002, -0.0056,  ..., -0.0043,  0.0079,  0.0043],
        ...,
        [-0.0047, -0.0152, -0.0096,  ..., -0.0111, -0.0022, -0.0117],
        [ 0.0074,  0.0010,  0.0089,  ..., -0.0088, -0.0194,  0.0014],
        [-0.0198,  0.0037, -0.0006,  ..., -0.0094,  0.0088, -0.0031]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5381, -2.9492, -0.5732,  ..., -0.1953, -4.3203, -0.9673]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 21:45:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of majority is majorities
The plural form of secretary is secretaries
The plural form of business is businesses
The plural form of variety is varieties
The plural form of activity is activities
The plural form of analysis is analyses
The plural form of academy is academies
The plural form of property is
2024-07-17 21:45:45 root INFO     [order_1_approx] starting weight calculation for The plural form of property is properties
The plural form of business is businesses
The plural form of secretary is secretaries
The plural form of academy is academies
The plural form of majority is majorities
The plural form of variety is varieties
The plural form of activity is activities
The plural form of analysis is
2024-07-17 21:45:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 21:49:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8857, -0.0732,  0.3452,  ..., -1.3359,  0.0491,  0.7520],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1074,  0.7397, -0.3135,  ..., -1.1973,  1.8438,  3.7852],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0125, -0.0184,  0.0118,  ..., -0.0073, -0.0087, -0.0332],
        [-0.0166,  0.0016, -0.0030,  ...,  0.0207, -0.0059, -0.0026],
        [ 0.0099,  0.0138, -0.0199,  ...,  0.0056,  0.0183,  0.0103],
        ...,
        [ 0.0012,  0.0044,  0.0009,  ..., -0.0030,  0.0190, -0.0183],
        [-0.0069, -0.0224, -0.0014,  ..., -0.0149, -0.0275,  0.0044],
        [-0.0052,  0.0374, -0.0035,  ..., -0.0182,  0.0014, -0.0149]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2993,  0.8828, -0.5430,  ..., -1.0488,  1.9854,  3.4883]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 21:49:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of property is properties
The plural form of business is businesses
The plural form of secretary is secretaries
The plural form of academy is academies
The plural form of majority is majorities
The plural form of variety is varieties
The plural form of activity is activities
The plural form of analysis is
2024-07-17 21:49:18 root INFO     [order_1_approx] starting weight calculation for The plural form of academy is academies
The plural form of property is properties
The plural form of activity is activities
The plural form of variety is varieties
The plural form of business is businesses
The plural form of secretary is secretaries
The plural form of analysis is analyses
The plural form of majority is
2024-07-17 21:49:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 21:52:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0576,  1.0020, -0.5352,  ..., -2.5195, -0.4351,  0.8008],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8760, -1.0361, -1.9785,  ...,  1.4639, -4.3359,  0.6431],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0065, -0.0294,  0.0132,  ..., -0.0083, -0.0029, -0.0148],
        [-0.0035, -0.0071, -0.0021,  ...,  0.0253, -0.0056, -0.0035],
        [ 0.0087, -0.0048, -0.0287,  ...,  0.0102,  0.0222, -0.0063],
        ...,
        [-0.0126, -0.0132, -0.0060,  ..., -0.0087,  0.0011, -0.0162],
        [ 0.0168, -0.0101,  0.0159,  ..., -0.0084, -0.0185, -0.0075],
        [-0.0120,  0.0159,  0.0096,  ..., -0.0065,  0.0041, -0.0221]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7871, -0.9683, -1.1299,  ...,  2.0508, -3.7227,  0.6699]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 21:52:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of academy is academies
The plural form of property is properties
The plural form of activity is activities
The plural form of variety is varieties
The plural form of business is businesses
The plural form of secretary is secretaries
The plural form of analysis is analyses
The plural form of majority is
2024-07-17 21:52:51 root INFO     [order_1_approx] starting weight calculation for The plural form of academy is academies
The plural form of majority is majorities
The plural form of variety is varieties
The plural form of property is properties
The plural form of activity is activities
The plural form of analysis is analyses
The plural form of business is businesses
The plural form of secretary is
2024-07-17 21:52:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 21:56:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5430,  0.2852, -0.9473,  ...,  0.8535, -0.3003,  0.4302],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-8.3359,  4.2578,  2.0566,  ...,  1.3887, -4.5781,  5.3867],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0100, -0.0076,  0.0219,  ..., -0.0056, -0.0021, -0.0316],
        [-0.0038, -0.0142, -0.0195,  ..., -0.0020, -0.0128,  0.0101],
        [ 0.0171,  0.0127,  0.0005,  ..., -0.0172,  0.0211,  0.0102],
        ...,
        [-0.0147, -0.0378, -0.0089,  ..., -0.0090, -0.0020, -0.0086],
        [ 0.0033, -0.0358,  0.0009,  ..., -0.0101, -0.0149,  0.0146],
        [-0.0149,  0.0131,  0.0039,  ..., -0.0077, -0.0004, -0.0098]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-7.3008,  4.6094,  2.0273,  ...,  0.8354, -3.8906,  5.4922]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 21:56:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of academy is academies
The plural form of majority is majorities
The plural form of variety is varieties
The plural form of property is properties
The plural form of activity is activities
The plural form of analysis is analyses
The plural form of business is businesses
The plural form of secretary is
2024-07-17 21:56:24 root INFO     total operator prediction time: 1704.3529250621796 seconds
2024-07-17 21:56:24 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on Ving - verb_inf
2024-07-17 21:56:24 root INFO     building operator Ving - verb_inf
2024-07-17 21:56:24 root INFO     [order_1_approx] starting weight calculation for improving is the active form of improve
referring is the active form of refer
maintaining is the active form of maintain
allowing is the active form of allow
seeming is the active form of seem
receiving is the active form of receive
avoiding is the active form of avoid
understanding is the active form of
2024-07-17 21:56:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 21:59:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9990, -1.4912,  1.8184,  ..., -0.4038,  2.6445,  0.2917],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 6.4727,  1.9053, -0.8682,  ..., -1.6650,  2.7207,  4.1406],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0007, -0.0252,  0.0224,  ...,  0.0142,  0.0019,  0.0010],
        [-0.0154, -0.0076,  0.0047,  ...,  0.0031, -0.0062,  0.0054],
        [-0.0011,  0.0182, -0.0170,  ..., -0.0088, -0.0012, -0.0021],
        ...,
        [-0.0153, -0.0162,  0.0120,  ..., -0.0018,  0.0016, -0.0142],
        [ 0.0025, -0.0016, -0.0016,  ..., -0.0077, -0.0359,  0.0158],
        [-0.0024,  0.0182, -0.0012,  ..., -0.0027, -0.0058, -0.0123]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.5977,  2.8418, -0.7188,  ..., -1.6475,  2.0703,  4.2539]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 22:00:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for improving is the active form of improve
referring is the active form of refer
maintaining is the active form of maintain
allowing is the active form of allow
seeming is the active form of seem
receiving is the active form of receive
avoiding is the active form of avoid
understanding is the active form of
2024-07-17 22:00:00 root INFO     [order_1_approx] starting weight calculation for referring is the active form of refer
avoiding is the active form of avoid
understanding is the active form of understand
improving is the active form of improve
allowing is the active form of allow
receiving is the active form of receive
seeming is the active form of seem
maintaining is the active form of
2024-07-17 22:00:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 22:03:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8418, -0.6201,  1.1973,  ..., -0.0547,  1.6865, -0.0259],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.2383, -1.9570, -3.4551,  ...,  0.2505, -0.6670,  5.5938],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0072, -0.0009,  0.0131,  ..., -0.0044, -0.0050, -0.0175],
        [-0.0030,  0.0065, -0.0029,  ...,  0.0067,  0.0031,  0.0116],
        [ 0.0099,  0.0124,  0.0020,  ...,  0.0010,  0.0021,  0.0024],
        ...,
        [-0.0084, -0.0161,  0.0033,  ..., -0.0041, -0.0042, -0.0076],
        [-0.0095,  0.0071, -0.0038,  ..., -0.0078, -0.0258,  0.0068],
        [-0.0023,  0.0141,  0.0017,  ..., -0.0109,  0.0066, -0.0064]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.2461, -1.6875, -3.7969,  ...,  0.1787, -1.0137,  5.8789]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 22:03:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for referring is the active form of refer
avoiding is the active form of avoid
understanding is the active form of understand
improving is the active form of improve
allowing is the active form of allow
receiving is the active form of receive
seeming is the active form of seem
maintaining is the active form of
2024-07-17 22:03:35 root INFO     [order_1_approx] starting weight calculation for receiving is the active form of receive
allowing is the active form of allow
improving is the active form of improve
understanding is the active form of understand
maintaining is the active form of maintain
seeming is the active form of seem
avoiding is the active form of avoid
referring is the active form of
2024-07-17 22:03:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 22:07:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5996, -0.9619,  0.8354,  ..., -0.6431,  2.5938, -0.1072],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6016, -1.1426, -1.7051,  ...,  2.1953,  0.8203,  2.0820],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0267,  0.0043,  0.0295,  ..., -0.0047,  0.0018,  0.0102],
        [-0.0022, -0.0069,  0.0097,  ...,  0.0102,  0.0062,  0.0067],
        [-0.0056, -0.0162, -0.0130,  ..., -0.0033, -0.0110, -0.0172],
        ...,
        [-0.0203, -0.0095,  0.0019,  ..., -0.0278, -0.0060, -0.0033],
        [-0.0104,  0.0133,  0.0009,  ...,  0.0002, -0.0275,  0.0165],
        [-0.0004,  0.0132,  0.0059,  ..., -0.0025,  0.0152, -0.0211]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9023, -1.0039, -1.1406,  ...,  2.0273,  1.2148,  2.1562]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 22:07:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for receiving is the active form of receive
allowing is the active form of allow
improving is the active form of improve
understanding is the active form of understand
maintaining is the active form of maintain
seeming is the active form of seem
avoiding is the active form of avoid
referring is the active form of
2024-07-17 22:07:09 root INFO     [order_1_approx] starting weight calculation for maintaining is the active form of maintain
understanding is the active form of understand
referring is the active form of refer
seeming is the active form of seem
avoiding is the active form of avoid
receiving is the active form of receive
improving is the active form of improve
allowing is the active form of
2024-07-17 22:07:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 22:10:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.0957, -0.6504,  0.8550,  ...,  0.0833,  1.7275,  1.4180],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2363,  0.2500, -1.4111,  ..., -2.5078,  0.9424,  5.1875],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0085, -0.0008,  0.0201,  ..., -0.0048, -0.0063, -0.0156],
        [ 0.0020, -0.0021, -0.0076,  ..., -0.0046, -0.0048,  0.0098],
        [-0.0174, -0.0079, -0.0042,  ..., -0.0092, -0.0040,  0.0040],
        ...,
        [-0.0124, -0.0218, -0.0054,  ..., -0.0070,  0.0155, -0.0105],
        [-0.0039,  0.0185,  0.0262,  ...,  0.0041, -0.0151,  0.0153],
        [-0.0131,  0.0300,  0.0204,  ..., -0.0124, -0.0128, -0.0175]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7358,  1.1934, -0.9795,  ..., -2.9453, -0.3232,  4.2227]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 22:10:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for maintaining is the active form of maintain
understanding is the active form of understand
referring is the active form of refer
seeming is the active form of seem
avoiding is the active form of avoid
receiving is the active form of receive
improving is the active form of improve
allowing is the active form of
2024-07-17 22:10:43 root INFO     [order_1_approx] starting weight calculation for receiving is the active form of receive
avoiding is the active form of avoid
understanding is the active form of understand
seeming is the active form of seem
maintaining is the active form of maintain
referring is the active form of refer
allowing is the active form of allow
improving is the active form of
2024-07-17 22:10:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 22:14:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0781, -0.7305,  0.9453,  ..., -0.0656,  2.2441, -0.5679],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([10.6172,  1.3027,  0.1250,  ..., -0.3257, -1.5742,  2.0703],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0017, -0.0032,  0.0120,  ..., -0.0299, -0.0148, -0.0194],
        [-0.0048,  0.0002, -0.0080,  ...,  0.0122, -0.0052, -0.0002],
        [-0.0015,  0.0064, -0.0041,  ..., -0.0187,  0.0210, -0.0031],
        ...,
        [-0.0135, -0.0075,  0.0005,  ...,  0.0017, -0.0033, -0.0157],
        [-0.0044,  0.0027,  0.0071,  ..., -0.0061, -0.0273,  0.0164],
        [-0.0115,  0.0072,  0.0152,  ..., -0.0154,  0.0025, -0.0017]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[10.8281,  1.5303,  0.5225,  ..., -0.3074, -1.6201,  1.9326]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 22:14:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for receiving is the active form of receive
avoiding is the active form of avoid
understanding is the active form of understand
seeming is the active form of seem
maintaining is the active form of maintain
referring is the active form of refer
allowing is the active form of allow
improving is the active form of
2024-07-17 22:14:17 root INFO     [order_1_approx] starting weight calculation for referring is the active form of refer
seeming is the active form of seem
improving is the active form of improve
understanding is the active form of understand
allowing is the active form of allow
receiving is the active form of receive
maintaining is the active form of maintain
avoiding is the active form of
2024-07-17 22:14:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 22:17:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4219, -0.5630,  0.0064,  ..., -0.1338,  0.3279, -1.4512],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.2656,  0.6914, -0.1069,  ..., -1.4414,  2.0039,  2.8652],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0075,  0.0041,  0.0227,  ..., -0.0056, -0.0173,  0.0013],
        [ 0.0025,  0.0077, -0.0050,  ...,  0.0056,  0.0042,  0.0047],
        [ 0.0107,  0.0075,  0.0007,  ..., -0.0048,  0.0061,  0.0117],
        ...,
        [-0.0189, -0.0088,  0.0089,  ..., -0.0124, -0.0005,  0.0017],
        [-0.0080,  0.0195,  0.0020,  ..., -0.0118, -0.0228,  0.0205],
        [-0.0066,  0.0222,  0.0057,  ..., -0.0129,  0.0092, -0.0074]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.2773,  0.1956, -0.0407,  ..., -1.8223,  2.5312,  2.0527]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 22:17:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for referring is the active form of refer
seeming is the active form of seem
improving is the active form of improve
understanding is the active form of understand
allowing is the active form of allow
receiving is the active form of receive
maintaining is the active form of maintain
avoiding is the active form of
2024-07-17 22:17:51 root INFO     [order_1_approx] starting weight calculation for maintaining is the active form of maintain
avoiding is the active form of avoid
allowing is the active form of allow
referring is the active form of refer
understanding is the active form of understand
seeming is the active form of seem
improving is the active form of improve
receiving is the active form of
2024-07-17 22:17:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 22:21:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.9844, -1.7188,  1.9766,  ...,  0.3384,  1.7656,  0.7651],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.2305,  0.4580, -2.2539,  ..., -1.8057,  1.9922,  2.1348],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0043, -0.0052,  0.0056,  ..., -0.0077,  0.0040, -0.0051],
        [-0.0091, -0.0058, -0.0016,  ...,  0.0019, -0.0044,  0.0117],
        [-0.0008, -0.0084, -0.0117,  ..., -0.0065, -0.0040,  0.0024],
        ...,
        [-0.0082, -0.0177,  0.0030,  ..., -0.0044,  0.0074, -0.0213],
        [-0.0131,  0.0124,  0.0078,  ..., -0.0008, -0.0119,  0.0134],
        [-0.0040,  0.0093,  0.0132,  ..., -0.0081,  0.0038, -0.0074]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5273,  0.5601, -2.8242,  ..., -1.8018,  1.8896,  1.9570]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 22:21:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for maintaining is the active form of maintain
avoiding is the active form of avoid
allowing is the active form of allow
referring is the active form of refer
understanding is the active form of understand
seeming is the active form of seem
improving is the active form of improve
receiving is the active form of
2024-07-17 22:21:26 root INFO     [order_1_approx] starting weight calculation for understanding is the active form of understand
receiving is the active form of receive
maintaining is the active form of maintain
referring is the active form of refer
improving is the active form of improve
allowing is the active form of allow
avoiding is the active form of avoid
seeming is the active form of
2024-07-17 22:21:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 22:25:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2383, -0.0656,  1.0557,  ...,  0.6621,  0.7661, -0.4702],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1348,  2.5000,  2.7207,  ..., -1.2656, -4.7734,  4.9180],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0082, -0.0144,  0.0179,  ..., -0.0112, -0.0093,  0.0061],
        [-0.0201, -0.0254, -0.0044,  ...,  0.0012, -0.0042,  0.0003],
        [ 0.0114, -0.0091, -0.0116,  ...,  0.0015, -0.0059,  0.0013],
        ...,
        [-0.0173, -0.0152,  0.0023,  ..., -0.0093, -0.0091, -0.0210],
        [-0.0040,  0.0353, -0.0036,  ..., -0.0208, -0.0064,  0.0261],
        [-0.0172,  0.0153,  0.0146,  ..., -0.0155,  0.0040, -0.0154]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0117,  2.3750,  2.2871,  ..., -0.9438, -4.7031,  5.3164]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 22:25:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for understanding is the active form of understand
receiving is the active form of receive
maintaining is the active form of maintain
referring is the active form of refer
improving is the active form of improve
allowing is the active form of allow
avoiding is the active form of avoid
seeming is the active form of
2024-07-17 22:25:01 root INFO     total operator prediction time: 1716.775140285492 seconds
2024-07-17 22:25:01 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_Ving - Ved
2024-07-17 22:25:01 root INFO     building operator verb_Ving - Ved
2024-07-17 22:25:01 root INFO     [order_1_approx] starting weight calculation for After something is remaining, it has remained
After something is sending, it has sent
After something is including, it has included
After something is failing, it has failed
After something is following, it has followed
After something is relating, it has related
After something is appointing, it has appointed
After something is announcing, it has
2024-07-17 22:25:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 22:28:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1963, -0.3052,  0.9668,  ...,  0.7500,  0.5073,  0.6636],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.1523, -0.9307, -1.9238,  ...,  4.2305,  1.9443,  2.2051],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.5152e-02, -1.7899e-02,  8.6288e-03,  ...,  1.6846e-02,
         -5.5580e-03, -2.2278e-02],
        [-6.2485e-03, -9.4070e-03, -9.9869e-03,  ...,  2.4628e-02,
         -2.8763e-03,  1.1574e-02],
        [ 7.4997e-03,  5.5466e-03, -4.7379e-03,  ...,  1.2466e-02,
          2.6367e-02,  5.0240e-03],
        ...,
        [-2.1118e-02, -2.4933e-02, -1.8677e-02,  ..., -5.9776e-03,
         -2.3041e-02, -2.8610e-05],
        [ 4.3945e-03,  7.0229e-03,  1.2299e-02,  ..., -7.4692e-03,
         -3.6438e-02,  1.3298e-02],
        [ 4.0054e-03,  1.7822e-02, -7.8735e-03,  ..., -5.5695e-03,
         -6.8283e-04, -2.8229e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.4941, -0.7881, -0.7520,  ...,  3.8477,  2.4805,  1.9824]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 22:28:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is remaining, it has remained
After something is sending, it has sent
After something is including, it has included
After something is failing, it has failed
After something is following, it has followed
After something is relating, it has related
After something is appointing, it has appointed
After something is announcing, it has
2024-07-17 22:28:36 root INFO     [order_1_approx] starting weight calculation for After something is following, it has followed
After something is including, it has included
After something is appointing, it has appointed
After something is sending, it has sent
After something is announcing, it has announced
After something is failing, it has failed
After something is remaining, it has remained
After something is relating, it has
2024-07-17 22:28:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 22:32:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7544,  0.2203,  1.1299,  ..., -0.1472,  1.8535, -0.9175],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3147,  1.4590,  0.6592,  ...,  0.6431,  0.3467,  1.5566],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0163, -0.0179,  0.0088,  ..., -0.0036, -0.0123, -0.0147],
        [ 0.0047, -0.0114, -0.0141,  ...,  0.0032,  0.0042, -0.0021],
        [ 0.0090,  0.0006, -0.0160,  ...,  0.0051,  0.0100,  0.0008],
        ...,
        [-0.0351, -0.0103,  0.0014,  ..., -0.0168, -0.0012,  0.0095],
        [-0.0023, -0.0003, -0.0038,  ..., -0.0206, -0.0189, -0.0090],
        [-0.0032,  0.0205,  0.0132,  ..., -0.0222,  0.0054, -0.0159]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2319,  1.4688,  0.9956,  ...,  1.2031,  0.4294,  1.8330]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 22:32:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is following, it has followed
After something is including, it has included
After something is appointing, it has appointed
After something is sending, it has sent
After something is announcing, it has announced
After something is failing, it has failed
After something is remaining, it has remained
After something is relating, it has
2024-07-17 22:32:11 root INFO     [order_1_approx] starting weight calculation for After something is relating, it has related
After something is appointing, it has appointed
After something is announcing, it has announced
After something is following, it has followed
After something is remaining, it has remained
After something is including, it has included
After something is failing, it has failed
After something is sending, it has
2024-07-17 22:32:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 22:35:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.8750, -0.1367,  1.0879,  ...,  0.4546,  0.7534,  0.7871],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.8076,  2.3145,  2.8457,  ...,  2.8066, -1.7939,  2.7148],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0207, -0.0168, -0.0057,  ..., -0.0071,  0.0052, -0.0134],
        [-0.0024, -0.0172,  0.0018,  ...,  0.0001, -0.0238,  0.0101],
        [ 0.0063, -0.0099, -0.0126,  ..., -0.0041,  0.0095, -0.0004],
        ...,
        [-0.0276, -0.0089, -0.0030,  ..., -0.0190, -0.0061, -0.0013],
        [-0.0105,  0.0034,  0.0053,  ..., -0.0104, -0.0204,  0.0036],
        [-0.0014,  0.0072,  0.0023,  ..., -0.0012,  0.0143, -0.0273]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.9102,  1.8115,  2.7148,  ...,  3.0312, -2.0977,  3.2129]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 22:35:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is relating, it has related
After something is appointing, it has appointed
After something is announcing, it has announced
After something is following, it has followed
After something is remaining, it has remained
After something is including, it has included
After something is failing, it has failed
After something is sending, it has
2024-07-17 22:35:43 root INFO     [order_1_approx] starting weight calculation for After something is failing, it has failed
After something is including, it has included
After something is remaining, it has remained
After something is announcing, it has announced
After something is appointing, it has appointed
After something is sending, it has sent
After something is relating, it has related
After something is following, it has
2024-07-17 22:35:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 22:39:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0547, -0.8525,  1.9814,  ...,  0.8579,  0.8564,  0.7852],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0352, -0.4626, -0.8281,  ..., -1.6006,  1.0957,  1.4512],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0117, -0.0223,  0.0142,  ...,  0.0002, -0.0164, -0.0152],
        [-0.0041, -0.0070, -0.0051,  ...,  0.0038, -0.0120,  0.0101],
        [ 0.0076, -0.0048, -0.0023,  ..., -0.0080, -0.0049,  0.0196],
        ...,
        [-0.0159, -0.0088,  0.0116,  ...,  0.0148,  0.0119, -0.0013],
        [-0.0053,  0.0162, -0.0034,  ...,  0.0138, -0.0151,  0.0014],
        [-0.0141,  0.0226, -0.0117,  ..., -0.0109,  0.0124, -0.0243]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9619, -0.7739, -1.0020,  ..., -1.0332,  1.3838,  1.2588]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 22:39:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is failing, it has failed
After something is including, it has included
After something is remaining, it has remained
After something is announcing, it has announced
After something is appointing, it has appointed
After something is sending, it has sent
After something is relating, it has related
After something is following, it has
2024-07-17 22:39:16 root INFO     [order_1_approx] starting weight calculation for After something is relating, it has related
After something is remaining, it has remained
After something is sending, it has sent
After something is failing, it has failed
After something is including, it has included
After something is announcing, it has announced
After something is following, it has followed
After something is appointing, it has
2024-07-17 22:39:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 22:42:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1475, -1.1338,  0.3005,  ...,  0.2764,  0.3228,  0.4521],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.1230,  0.5527,  1.9121,  ...,  4.8906, -0.4751,  0.5352],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.0204e-03, -2.1881e-02,  1.4122e-02,  ..., -5.8212e-03,
         -1.2085e-02, -1.6724e-02],
        [ 2.1629e-03, -9.7961e-03, -1.7090e-02,  ..., -1.1719e-02,
          5.3596e-03,  3.8910e-03],
        [-1.1810e-02, -7.0419e-03, -5.0888e-03,  ...,  1.3596e-02,
          1.1925e-02,  8.4381e-03],
        ...,
        [-4.4464e-02, -5.3406e-03, -3.5156e-02,  ..., -7.1716e-03,
         -4.8294e-03, -9.1553e-05],
        [ 5.7068e-03,  2.1362e-02,  7.7515e-03,  ...,  1.1864e-03,
         -2.7573e-02,  1.1475e-02],
        [-3.9253e-03,  2.7100e-02, -1.2123e-02,  ..., -1.2627e-02,
         -1.4267e-02, -2.6886e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.0938,  1.0918,  2.2910,  ...,  5.3047, -0.6182,  1.1260]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 22:42:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is relating, it has related
After something is remaining, it has remained
After something is sending, it has sent
After something is failing, it has failed
After something is including, it has included
After something is announcing, it has announced
After something is following, it has followed
After something is appointing, it has
2024-07-17 22:42:50 root INFO     [order_1_approx] starting weight calculation for After something is relating, it has related
After something is announcing, it has announced
After something is sending, it has sent
After something is including, it has included
After something is appointing, it has appointed
After something is failing, it has failed
After something is following, it has followed
After something is remaining, it has
2024-07-17 22:42:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 22:46:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.3245, 0.9121, 0.6455,  ..., 0.4683, 1.0820, 0.3889], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.7383,  3.0488, -0.3770,  ...,  2.6035, -1.1172,  1.4824],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0127, -0.0075,  0.0164,  ..., -0.0002, -0.0126, -0.0226],
        [ 0.0004, -0.0147, -0.0042,  ...,  0.0015, -0.0106,  0.0056],
        [ 0.0021, -0.0027, -0.0190,  ..., -0.0102,  0.0062,  0.0059],
        ...,
        [-0.0209, -0.0008,  0.0111,  ..., -0.0207, -0.0132,  0.0068],
        [-0.0124,  0.0057,  0.0085,  ..., -0.0140, -0.0282,  0.0026],
        [-0.0098,  0.0041,  0.0002,  ...,  0.0007, -0.0041, -0.0231]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.9102,  3.5352, -0.2225,  ...,  2.8594, -1.3467,  1.9355]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 22:46:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is relating, it has related
After something is announcing, it has announced
After something is sending, it has sent
After something is including, it has included
After something is appointing, it has appointed
After something is failing, it has failed
After something is following, it has followed
After something is remaining, it has
2024-07-17 22:46:25 root INFO     [order_1_approx] starting weight calculation for After something is including, it has included
After something is appointing, it has appointed
After something is relating, it has related
After something is remaining, it has remained
After something is sending, it has sent
After something is announcing, it has announced
After something is following, it has followed
After something is failing, it has
2024-07-17 22:46:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 22:49:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3379,  0.8589,  0.9199,  ...,  0.4553,  1.7373, -0.1964],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.2852,  3.8594,  1.6973,  ..., -0.2876,  0.1265,  2.6250],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0258, -0.0175,  0.0065,  ...,  0.0094,  0.0103, -0.0002],
        [-0.0090, -0.0041, -0.0123,  ...,  0.0121, -0.0124,  0.0054],
        [ 0.0045, -0.0148, -0.0048,  ..., -0.0049,  0.0004,  0.0161],
        ...,
        [-0.0059, -0.0065, -0.0063,  ..., -0.0219, -0.0125, -0.0146],
        [-0.0044, -0.0052,  0.0029,  ..., -0.0104, -0.0288, -0.0018],
        [-0.0106,  0.0168, -0.0060,  ...,  0.0031, -0.0010, -0.0217]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.2656,  4.1680,  2.1641,  ...,  0.1807,  0.3652,  2.9766]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 22:49:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is including, it has included
After something is appointing, it has appointed
After something is relating, it has related
After something is remaining, it has remained
After something is sending, it has sent
After something is announcing, it has announced
After something is following, it has followed
After something is failing, it has
2024-07-17 22:49:59 root INFO     [order_1_approx] starting weight calculation for After something is failing, it has failed
After something is remaining, it has remained
After something is sending, it has sent
After something is appointing, it has appointed
After something is announcing, it has announced
After something is relating, it has related
After something is following, it has followed
After something is including, it has
2024-07-17 22:50:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 22:53:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5376, -0.0198,  1.0361,  ...,  0.7134,  0.6772,  0.8545],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4219,  0.7441,  0.4492,  ...,  1.7900, -2.6934,  0.1812],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0044, -0.0142,  0.0138,  ..., -0.0093,  0.0005, -0.0181],
        [-0.0008, -0.0104,  0.0010,  ..., -0.0011, -0.0158,  0.0006],
        [ 0.0012, -0.0237,  0.0058,  ..., -0.0115, -0.0012,  0.0115],
        ...,
        [-0.0079, -0.0036, -0.0017,  ...,  0.0008, -0.0122, -0.0010],
        [ 0.0030,  0.0178, -0.0009,  ..., -0.0115, -0.0091,  0.0073],
        [-0.0143,  0.0238, -0.0090,  ..., -0.0077, -0.0099,  0.0005]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0146,  0.3486,  0.1335,  ...,  2.4473, -2.4590,  1.0703]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 22:53:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is failing, it has failed
After something is remaining, it has remained
After something is sending, it has sent
After something is appointing, it has appointed
After something is announcing, it has announced
After something is relating, it has related
After something is following, it has followed
After something is including, it has
2024-07-17 22:53:34 root INFO     total operator prediction time: 1712.9894502162933 seconds
2024-07-17 22:53:34 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_inf - Ved
2024-07-17 22:53:34 root INFO     building operator verb_inf - Ved
2024-07-17 22:53:34 root INFO     [order_1_approx] starting weight calculation for If the present form is perform, the past form is performed
If the present form is become, the past form is became
If the present form is provide, the past form is provided
If the present form is allow, the past form is allowed
If the present form is improve, the past form is improved
If the present form is follow, the past form is followed
If the present form is discover, the past form is discovered
If the present form is spend, the past form is
2024-07-17 22:53:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 22:57:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9756, -0.5469,  1.3164,  ...,  0.9346,  0.2175, -0.6797],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7656,  2.5586,  2.7480,  ...,  0.7856, -1.8154,  2.5000],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0233,  0.0097,  0.0060,  ..., -0.0091, -0.0094, -0.0114],
        [ 0.0003, -0.0079, -0.0054,  ..., -0.0034, -0.0056, -0.0063],
        [-0.0114, -0.0026, -0.0174,  ..., -0.0085,  0.0006,  0.0078],
        ...,
        [ 0.0015, -0.0019, -0.0120,  ..., -0.0209,  0.0049, -0.0053],
        [-0.0007,  0.0065,  0.0024,  ..., -0.0042, -0.0209, -0.0036],
        [-0.0159,  0.0167, -0.0013,  ..., -0.0176, -0.0041, -0.0174]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.2227,  2.6680,  3.5137,  ..., -0.1851, -1.9795,  2.9668]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 22:57:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is perform, the past form is performed
If the present form is become, the past form is became
If the present form is provide, the past form is provided
If the present form is allow, the past form is allowed
If the present form is improve, the past form is improved
If the present form is follow, the past form is followed
If the present form is discover, the past form is discovered
If the present form is spend, the past form is
2024-07-17 22:57:07 root INFO     [order_1_approx] starting weight calculation for If the present form is become, the past form is became
If the present form is provide, the past form is provided
If the present form is discover, the past form is discovered
If the present form is follow, the past form is followed
If the present form is improve, the past form is improved
If the present form is spend, the past form is spent
If the present form is allow, the past form is allowed
If the present form is perform, the past form is
2024-07-17 22:57:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 23:00:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9297,  1.0674,  1.7422,  ..., -0.2363,  0.9287, -0.7988],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4316, -0.3059, -0.8477,  ...,  2.2773, -0.5117,  1.2588],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0247, -0.0093,  0.0038,  ..., -0.0111, -0.0026, -0.0252],
        [-0.0039, -0.0156, -0.0053,  ...,  0.0024,  0.0026,  0.0020],
        [-0.0006, -0.0050, -0.0144,  ..., -0.0087,  0.0030, -0.0055],
        ...,
        [-0.0144, -0.0011, -0.0080,  ..., -0.0209, -0.0041, -0.0038],
        [-0.0082,  0.0048,  0.0089,  ..., -0.0168, -0.0229,  0.0060],
        [-0.0140,  0.0067, -0.0080,  ..., -0.0013, -0.0022, -0.0115]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.0508, -0.3721, -0.3452,  ...,  2.4863, -0.5146,  1.4375]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 23:00:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is become, the past form is became
If the present form is provide, the past form is provided
If the present form is discover, the past form is discovered
If the present form is follow, the past form is followed
If the present form is improve, the past form is improved
If the present form is spend, the past form is spent
If the present form is allow, the past form is allowed
If the present form is perform, the past form is
2024-07-17 23:00:40 root INFO     [order_1_approx] starting weight calculation for If the present form is improve, the past form is improved
If the present form is become, the past form is became
If the present form is perform, the past form is performed
If the present form is follow, the past form is followed
If the present form is spend, the past form is spent
If the present form is allow, the past form is allowed
If the present form is discover, the past form is discovered
If the present form is provide, the past form is
2024-07-17 23:00:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 23:04:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3213,  0.0136, -0.4204,  ...,  0.4727,  1.1738,  0.1772],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4922,  0.8770, -1.0508,  ...,  0.2856, -1.0762, -0.1826],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0093,  0.0013, -0.0054,  ..., -0.0156, -0.0015, -0.0220],
        [-0.0007, -0.0018,  0.0001,  ...,  0.0056,  0.0020,  0.0082],
        [-0.0049,  0.0025, -0.0137,  ..., -0.0069, -0.0015, -0.0055],
        ...,
        [-0.0088, -0.0018, -0.0118,  ..., -0.0038, -0.0075, -0.0016],
        [ 0.0076, -0.0003,  0.0088,  ..., -0.0256, -0.0130,  0.0126],
        [-0.0149,  0.0116,  0.0041,  ..., -0.0047,  0.0056, -0.0282]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.4180,  1.1514, -1.3193,  ...,  0.1635, -2.1211, -0.3118]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 23:04:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is improve, the past form is improved
If the present form is become, the past form is became
If the present form is perform, the past form is performed
If the present form is follow, the past form is followed
If the present form is spend, the past form is spent
If the present form is allow, the past form is allowed
If the present form is discover, the past form is discovered
If the present form is provide, the past form is
2024-07-17 23:04:14 root INFO     [order_1_approx] starting weight calculation for If the present form is spend, the past form is spent
If the present form is follow, the past form is followed
If the present form is provide, the past form is provided
If the present form is perform, the past form is performed
If the present form is become, the past form is became
If the present form is improve, the past form is improved
If the present form is allow, the past form is allowed
If the present form is discover, the past form is
2024-07-17 23:04:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 23:07:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9204, -0.1748,  1.0000,  ...,  0.4360,  0.5186, -0.5151],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6406, -0.3271, -0.8457,  ..., -3.6094, -4.5938,  1.5742],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0101, -0.0096,  0.0038,  ..., -0.0054, -0.0100, -0.0060],
        [-0.0116, -0.0029, -0.0073,  ..., -0.0008,  0.0148,  0.0103],
        [-0.0017, -0.0105, -0.0226,  ..., -0.0051,  0.0160,  0.0022],
        ...,
        [-0.0086, -0.0043, -0.0076,  ..., -0.0154, -0.0006, -0.0073],
        [-0.0068,  0.0196, -0.0005,  ...,  0.0031, -0.0200, -0.0059],
        [-0.0164,  0.0067,  0.0089,  ..., -0.0064,  0.0090, -0.0251]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.9297, -0.0391, -0.9707,  ..., -3.3398, -4.5117,  1.8896]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 23:07:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is spend, the past form is spent
If the present form is follow, the past form is followed
If the present form is provide, the past form is provided
If the present form is perform, the past form is performed
If the present form is become, the past form is became
If the present form is improve, the past form is improved
If the present form is allow, the past form is allowed
If the present form is discover, the past form is
2024-07-17 23:07:48 root INFO     [order_1_approx] starting weight calculation for If the present form is improve, the past form is improved
If the present form is provide, the past form is provided
If the present form is allow, the past form is allowed
If the present form is spend, the past form is spent
If the present form is perform, the past form is performed
If the present form is become, the past form is became
If the present form is discover, the past form is discovered
If the present form is follow, the past form is
2024-07-17 23:07:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 23:11:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1260,  0.4502,  2.9336,  ...,  1.5762, -0.2573,  0.4014],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0469, -2.2500,  0.8486,  ..., -3.0742, -0.0701,  1.9512],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.3193e-02, -2.9335e-03,  6.7234e-04,  ..., -8.9722e-03,
         -1.0071e-02, -2.1408e-02],
        [-8.3694e-03, -4.9667e-03,  9.7275e-04,  ...,  1.1873e-03,
          2.9755e-03, -9.9754e-04],
        [-1.2802e-02, -1.3809e-03, -1.1475e-02,  ..., -7.5722e-04,
         -6.3629e-03,  1.3885e-03],
        ...,
        [-5.7487e-03, -7.5989e-03, -7.8888e-03,  ...,  2.7561e-03,
         -6.6996e-05, -7.3586e-03],
        [-3.4370e-03, -6.1035e-04,  1.1925e-02,  ..., -1.4877e-04,
         -2.0813e-02, -1.8692e-04],
        [-1.2436e-02,  6.6528e-03,  2.8706e-03,  ...,  3.4714e-04,
          3.8528e-04, -1.6251e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1816, -2.4453,  1.2461,  ..., -3.2969, -0.1220,  1.6514]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 23:11:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is improve, the past form is improved
If the present form is provide, the past form is provided
If the present form is allow, the past form is allowed
If the present form is spend, the past form is spent
If the present form is perform, the past form is performed
If the present form is become, the past form is became
If the present form is discover, the past form is discovered
If the present form is follow, the past form is
2024-07-17 23:11:21 root INFO     [order_1_approx] starting weight calculation for If the present form is provide, the past form is provided
If the present form is discover, the past form is discovered
If the present form is improve, the past form is improved
If the present form is follow, the past form is followed
If the present form is perform, the past form is performed
If the present form is become, the past form is became
If the present form is spend, the past form is spent
If the present form is allow, the past form is
2024-07-17 23:11:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 23:14:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5137,  0.3157,  1.7998,  ...,  0.5977, -0.5938, -0.3916],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5977, -0.5264, -0.2812,  ..., -0.0757, -1.6494,  2.1191],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.9488e-03,  1.0567e-03,  3.6545e-03,  ...,  4.4441e-03,
         -1.7719e-03, -1.9592e-02],
        [-4.4403e-03, -9.9716e-03, -3.3779e-03,  ...,  4.8409e-03,
         -1.6937e-03, -1.4519e-02],
        [-1.6098e-03, -7.7248e-05, -1.5884e-02,  ..., -4.7874e-03,
         -1.6251e-03, -1.1543e-02],
        ...,
        [-1.5732e-02, -6.8893e-03, -5.4741e-04,  ..., -1.2306e-02,
          2.7542e-03, -3.3283e-03],
        [-1.0777e-04,  8.0719e-03,  1.4496e-03,  ..., -1.0300e-02,
         -2.1759e-02, -3.8414e-03],
        [-6.2828e-03,  5.1842e-03, -6.2714e-03,  ...,  7.3128e-03,
         -1.2611e-02, -2.5101e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7559, -0.2673,  0.3252,  ..., -0.2041, -1.8887,  2.1914]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 23:14:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is provide, the past form is provided
If the present form is discover, the past form is discovered
If the present form is improve, the past form is improved
If the present form is follow, the past form is followed
If the present form is perform, the past form is performed
If the present form is become, the past form is became
If the present form is spend, the past form is spent
If the present form is allow, the past form is
2024-07-17 23:14:55 root INFO     [order_1_approx] starting weight calculation for If the present form is spend, the past form is spent
If the present form is discover, the past form is discovered
If the present form is allow, the past form is allowed
If the present form is follow, the past form is followed
If the present form is provide, the past form is provided
If the present form is become, the past form is became
If the present form is perform, the past form is performed
If the present form is improve, the past form is
2024-07-17 23:14:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 23:18:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9453, -0.0076,  1.6543,  ...,  0.3799,  0.6992, -1.2305],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 7.9102, -0.6719, -0.3691,  ...,  2.2344, -1.6367, -2.0078],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0169, -0.0138,  0.0021,  ..., -0.0099, -0.0145, -0.0086],
        [-0.0008, -0.0062, -0.0104,  ...,  0.0025,  0.0022, -0.0115],
        [-0.0077, -0.0104, -0.0160,  ...,  0.0012,  0.0121,  0.0010],
        ...,
        [ 0.0077,  0.0125,  0.0145,  ...,  0.0066,  0.0001,  0.0068],
        [-0.0079,  0.0017,  0.0088,  ..., -0.0294, -0.0247, -0.0015],
        [-0.0072,  0.0018,  0.0073,  ...,  0.0017,  0.0010, -0.0186]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 8.5859, -0.7783,  0.0815,  ...,  2.4707, -1.6396, -1.6738]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 23:18:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is spend, the past form is spent
If the present form is discover, the past form is discovered
If the present form is allow, the past form is allowed
If the present form is follow, the past form is followed
If the present form is provide, the past form is provided
If the present form is become, the past form is became
If the present form is perform, the past form is performed
If the present form is improve, the past form is
2024-07-17 23:18:28 root INFO     [order_1_approx] starting weight calculation for If the present form is discover, the past form is discovered
If the present form is perform, the past form is performed
If the present form is follow, the past form is followed
If the present form is provide, the past form is provided
If the present form is spend, the past form is spent
If the present form is improve, the past form is improved
If the present form is allow, the past form is allowed
If the present form is become, the past form is
2024-07-17 23:18:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 23:21:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9219,  0.4553,  1.7256,  ...,  0.7158, -0.3789,  0.5244],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.1484, -0.6387, -1.0371,  ..., -2.3242,  0.9355, -0.7832],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0258, -0.0164,  0.0084,  ..., -0.0051, -0.0041, -0.0134],
        [-0.0265, -0.0027, -0.0061,  ...,  0.0232,  0.0133, -0.0159],
        [ 0.0056,  0.0076, -0.0294,  ..., -0.0008,  0.0076,  0.0072],
        ...,
        [ 0.0007,  0.0076,  0.0059,  ..., -0.0204, -0.0208,  0.0036],
        [-0.0137, -0.0052,  0.0095,  ..., -0.0120, -0.0168, -0.0028],
        [-0.0083,  0.0118,  0.0036,  ..., -0.0021,  0.0018, -0.0205]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.4141, -0.2200, -0.6353,  ..., -2.6152,  0.8198, -0.7881]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 23:22:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is discover, the past form is discovered
If the present form is perform, the past form is performed
If the present form is follow, the past form is followed
If the present form is provide, the past form is provided
If the present form is spend, the past form is spent
If the present form is improve, the past form is improved
If the present form is allow, the past form is allowed
If the present form is become, the past form is
2024-07-17 23:22:00 root INFO     total operator prediction time: 1706.6067662239075 seconds
2024-07-17 23:22:00 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_inf - 3pSg
2024-07-17 23:22:00 root INFO     building operator verb_inf - 3pSg
2024-07-17 23:22:00 root INFO     [order_1_approx] starting weight calculation for I maintain, he maintains
I prevent, he prevents
I believe, he believes
I accept, he accepts
I describe, he describes
I achieve, he achieves
I enable, he enables
I develop, he
2024-07-17 23:22:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 23:25:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9453, -0.3979,  2.3301,  ...,  0.1206,  0.1038, -0.1433],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9941, -4.9023,  1.1924,  ...,  0.9590, -0.2803,  3.0742],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0115, -0.0062,  0.0051,  ..., -0.0192, -0.0143, -0.0034],
        [-0.0106, -0.0093, -0.0036,  ...,  0.0029, -0.0117,  0.0047],
        [-0.0087,  0.0020, -0.0160,  ..., -0.0017,  0.0025, -0.0023],
        ...,
        [ 0.0049, -0.0180, -0.0235,  ..., -0.0060, -0.0157, -0.0121],
        [-0.0047,  0.0211,  0.0173,  ..., -0.0135, -0.0306,  0.0197],
        [ 0.0025,  0.0264,  0.0167,  ...,  0.0066, -0.0017, -0.0140]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1973, -4.1406,  1.2559,  ...,  0.4617,  0.4170,  2.5547]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 23:25:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I maintain, he maintains
I prevent, he prevents
I believe, he believes
I accept, he accepts
I describe, he describes
I achieve, he achieves
I enable, he enables
I develop, he
2024-07-17 23:25:34 root INFO     [order_1_approx] starting weight calculation for I maintain, he maintains
I prevent, he prevents
I accept, he accepts
I achieve, he achieves
I develop, he develops
I believe, he believes
I enable, he enables
I describe, he
2024-07-17 23:25:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 23:29:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3574, -0.2783,  1.9824,  ..., -0.7578, -0.3752, -0.2563],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.6992, -4.3828, -0.3418,  ...,  2.2812, -0.8667,  2.8066],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0087, -0.0118,  0.0121,  ..., -0.0073, -0.0242, -0.0033],
        [ 0.0009, -0.0050, -0.0004,  ..., -0.0111, -0.0011,  0.0067],
        [-0.0169, -0.0007, -0.0143,  ...,  0.0065,  0.0150, -0.0149],
        ...,
        [-0.0190, -0.0175, -0.0073,  ..., -0.0057, -0.0145, -0.0023],
        [-0.0055,  0.0069,  0.0201,  ..., -0.0182, -0.0189, -0.0009],
        [-0.0123,  0.0159,  0.0110,  ..., -0.0115,  0.0016, -0.0242]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.4277, -3.9902, -0.7329,  ...,  1.7256, -0.4746,  2.2520]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 23:29:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I maintain, he maintains
I prevent, he prevents
I accept, he accepts
I achieve, he achieves
I develop, he develops
I believe, he believes
I enable, he enables
I describe, he
2024-07-17 23:29:07 root INFO     [order_1_approx] starting weight calculation for I maintain, he maintains
I develop, he develops
I believe, he believes
I achieve, he achieves
I accept, he accepts
I prevent, he prevents
I describe, he describes
I enable, he
2024-07-17 23:29:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 23:32:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3113,  0.0413,  1.8926,  ...,  0.2107,  0.1331,  0.2520],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5762, -2.4609,  4.2383,  ...,  1.5820, -1.6836,  5.3438],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.1780e-02, -9.6207e-03,  2.0828e-03,  ..., -7.5302e-03,
          1.6785e-03, -2.3285e-02],
        [ 6.4754e-04, -2.1439e-03, -9.7275e-04,  ...,  2.1648e-03,
         -1.7195e-03,  1.2970e-02],
        [-2.0828e-03, -1.5305e-02, -1.7670e-02,  ...,  1.2531e-03,
          2.6184e-02, -4.2076e-03],
        ...,
        [-1.0918e-02, -1.2230e-02, -3.1174e-02,  ..., -5.9509e-04,
         -7.6027e-03,  8.3447e-06],
        [-4.1580e-03,  3.0556e-03,  1.8845e-02,  ..., -1.9791e-02,
         -2.6062e-02,  8.4610e-03],
        [-1.2848e-02,  2.8458e-02,  1.4210e-03,  ..., -2.7771e-02,
         -3.2449e-04, -1.4542e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.1016, -2.4043,  3.7949,  ...,  1.0430, -1.2012,  4.9062]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 23:32:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I maintain, he maintains
I develop, he develops
I believe, he believes
I achieve, he achieves
I accept, he accepts
I prevent, he prevents
I describe, he describes
I enable, he
2024-07-17 23:32:40 root INFO     [order_1_approx] starting weight calculation for I enable, he enables
I believe, he believes
I describe, he describes
I develop, he develops
I maintain, he maintains
I accept, he accepts
I prevent, he prevents
I achieve, he
2024-07-17 23:32:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 23:36:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1620,  0.1829,  2.1523,  ..., -0.5029,  0.4492, -0.0706],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0518,  2.2285,  0.6099,  ..., -3.5977, -0.1058,  0.9531],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0049, -0.0089,  0.0042,  ..., -0.0019, -0.0027, -0.0209],
        [ 0.0023, -0.0052, -0.0047,  ...,  0.0030, -0.0069,  0.0007],
        [-0.0045, -0.0068, -0.0142,  ...,  0.0034,  0.0110,  0.0021],
        ...,
        [ 0.0038, -0.0029, -0.0114,  ..., -0.0023, -0.0133, -0.0003],
        [-0.0003,  0.0065,  0.0048,  ..., -0.0109, -0.0178,  0.0076],
        [-0.0045,  0.0208,  0.0117,  ..., -0.0084,  0.0101, -0.0108]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2578,  2.5840, -0.0840,  ..., -3.9121,  0.2198,  0.5303]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 23:36:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I enable, he enables
I believe, he believes
I describe, he describes
I develop, he develops
I maintain, he maintains
I accept, he accepts
I prevent, he prevents
I achieve, he
2024-07-17 23:36:14 root INFO     [order_1_approx] starting weight calculation for I maintain, he maintains
I enable, he enables
I believe, he believes
I achieve, he achieves
I describe, he describes
I develop, he develops
I prevent, he prevents
I accept, he
2024-07-17 23:36:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 23:39:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4468, -0.3433,  1.5869,  ..., -0.2637,  1.0146,  1.3916],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.7383, -3.8281,  4.0078,  ...,  2.0215, -2.3438,  3.5898],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0193,  0.0002,  0.0102,  ...,  0.0033, -0.0054, -0.0124],
        [ 0.0040,  0.0090,  0.0035,  ...,  0.0002, -0.0015,  0.0057],
        [-0.0080, -0.0162, -0.0200,  ...,  0.0040,  0.0153, -0.0070],
        ...,
        [ 0.0022, -0.0166, -0.0180,  ..., -0.0033,  0.0011, -0.0082],
        [-0.0010,  0.0055,  0.0177,  ..., -0.0166, -0.0264,  0.0045],
        [-0.0123,  0.0160,  0.0067,  ..., -0.0011,  0.0003, -0.0209]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.5742, -4.0391,  2.8262,  ...,  2.2656, -1.7305,  3.1289]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 23:39:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I maintain, he maintains
I enable, he enables
I believe, he believes
I achieve, he achieves
I describe, he describes
I develop, he develops
I prevent, he prevents
I accept, he
2024-07-17 23:39:48 root INFO     [order_1_approx] starting weight calculation for I develop, he develops
I achieve, he achieves
I accept, he accepts
I describe, he describes
I prevent, he prevents
I enable, he enables
I believe, he believes
I maintain, he
2024-07-17 23:39:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 23:43:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1613, -0.1508,  2.1309,  ..., -0.7881,  0.3042,  1.5674],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4238, -4.6562, -4.9492,  ...,  1.8223,  0.8662,  6.8555],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.1490e-02,  1.9073e-04,  1.2131e-02,  ...,  7.4844e-03,
          5.4092e-03, -1.2627e-02],
        [-4.5929e-03, -4.6005e-03,  1.0696e-02,  ...,  2.6894e-04,
          8.1863e-03,  2.9354e-03],
        [-1.8341e-02,  1.3800e-03,  1.0948e-03,  ...,  8.4381e-03,
          8.5907e-03, -9.0027e-03],
        ...,
        [-1.0452e-02, -2.2034e-02, -1.8860e-02,  ..., -2.6245e-02,
         -1.3222e-02,  6.3324e-03],
        [-9.6741e-03,  1.7151e-02,  6.3858e-03,  ..., -2.4933e-02,
         -3.4943e-02,  2.8687e-02],
        [ 4.2439e-05,  1.3748e-02, -7.6256e-03,  ...,  4.4632e-03,
         -3.7193e-03, -1.8753e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9766, -3.8711, -4.6328,  ...,  0.9429,  1.3184,  5.8984]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 23:43:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I develop, he develops
I achieve, he achieves
I accept, he accepts
I describe, he describes
I prevent, he prevents
I enable, he enables
I believe, he believes
I maintain, he
2024-07-17 23:43:23 root INFO     [order_1_approx] starting weight calculation for I develop, he develops
I accept, he accepts
I describe, he describes
I maintain, he maintains
I enable, he enables
I prevent, he prevents
I achieve, he achieves
I believe, he
2024-07-17 23:43:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 23:46:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3567, -0.7832,  0.7422,  ..., -0.3418,  1.7754,  0.4031],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2539, -1.1074, -0.8691,  ...,  2.7383, -2.5293,  3.9375],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0115, -0.0300,  0.0186,  ..., -0.0001, -0.0022, -0.0075],
        [-0.0095, -0.0051,  0.0022,  ...,  0.0030,  0.0040,  0.0015],
        [-0.0095,  0.0047, -0.0262,  ...,  0.0139,  0.0116,  0.0046],
        ...,
        [-0.0019, -0.0098, -0.0108,  ..., -0.0120,  0.0075, -0.0018],
        [-0.0010,  0.0027,  0.0165,  ..., -0.0149, -0.0340,  0.0035],
        [-0.0028,  0.0228, -0.0022,  ..., -0.0036, -0.0041, -0.0333]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0306, -0.8711, -0.9995,  ...,  2.3301, -2.2617,  3.4980]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 23:46:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I develop, he develops
I accept, he accepts
I describe, he describes
I maintain, he maintains
I enable, he enables
I prevent, he prevents
I achieve, he achieves
I believe, he
2024-07-17 23:46:56 root INFO     [order_1_approx] starting weight calculation for I accept, he accepts
I develop, he develops
I describe, he describes
I maintain, he maintains
I believe, he believes
I enable, he enables
I achieve, he achieves
I prevent, he
2024-07-17 23:46:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 23:50:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1746,  0.4067,  1.0010,  ...,  0.1714,  0.5352, -0.3997],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.5469, -5.3359,  1.1152,  ...,  1.5752,  1.3721,  5.5703],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0085, -0.0177,  0.0191,  ..., -0.0027, -0.0085, -0.0262],
        [-0.0004, -0.0100,  0.0059,  ..., -0.0071,  0.0003, -0.0010],
        [-0.0178, -0.0196, -0.0226,  ..., -0.0080,  0.0166, -0.0026],
        ...,
        [-0.0126, -0.0155, -0.0167,  ...,  0.0119, -0.0031, -0.0295],
        [ 0.0059,  0.0118, -0.0091,  ..., -0.0201, -0.0176,  0.0038],
        [-0.0123,  0.0264,  0.0159,  ..., -0.0229,  0.0036, -0.0322]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.6250, -4.6602,  0.3384,  ...,  0.4268,  1.3955,  5.3398]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 23:50:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I accept, he accepts
I develop, he develops
I describe, he describes
I maintain, he maintains
I believe, he believes
I enable, he enables
I achieve, he achieves
I prevent, he
2024-07-17 23:50:29 root INFO     total operator prediction time: 1708.956392288208 seconds
2024-07-17 23:50:29 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_Ving - 3pSg
2024-07-17 23:50:29 root INFO     building operator verb_Ving - 3pSg
2024-07-17 23:50:29 root INFO     [order_1_approx] starting weight calculation for When something is spending, it spends
When something is discovering, it discovers
When something is learning, it learns
When something is telling, it tells
When something is referring, it refers
When something is happening, it happens
When something is allowing, it allows
When something is involving, it
2024-07-17 23:50:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 23:54:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.7524, 0.3245, 0.1703,  ..., 0.8599, 1.4707, 0.7812], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([3.1016, 0.1584, 2.0039,  ..., 1.8506, 0.5479, 1.1562], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0049,  0.0040,  0.0016,  ...,  0.0071,  0.0016, -0.0108],
        [-0.0035,  0.0057, -0.0097,  ..., -0.0020, -0.0110, -0.0048],
        [ 0.0129,  0.0076, -0.0074,  ..., -0.0014, -0.0126,  0.0144],
        ...,
        [-0.0178,  0.0024, -0.0279,  ...,  0.0096, -0.0100, -0.0047],
        [ 0.0029,  0.0049, -0.0007,  ..., -0.0044, -0.0093, -0.0100],
        [ 0.0007,  0.0187, -0.0027,  ..., -0.0052,  0.0079, -0.0132]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[2.4473, 0.3982, 2.7246,  ..., 2.2227, 0.7241, 0.7427]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 23:54:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is spending, it spends
When something is discovering, it discovers
When something is learning, it learns
When something is telling, it tells
When something is referring, it refers
When something is happening, it happens
When something is allowing, it allows
When something is involving, it
2024-07-17 23:54:03 root INFO     [order_1_approx] starting weight calculation for When something is discovering, it discovers
When something is allowing, it allows
When something is learning, it learns
When something is telling, it tells
When something is spending, it spends
When something is happening, it happens
When something is involving, it involves
When something is referring, it
2024-07-17 23:54:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-17 23:57:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6528,  0.0443,  0.8857,  ...,  0.1156,  2.1875, -0.0068],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6406,  1.0996,  2.0176,  ...,  5.9922,  0.3381,  2.0664],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0165, -0.0191,  0.0125,  ...,  0.0063, -0.0078,  0.0060],
        [-0.0052, -0.0108, -0.0116,  ..., -0.0019, -0.0053, -0.0048],
        [-0.0054, -0.0037, -0.0252,  ...,  0.0031,  0.0041, -0.0024],
        ...,
        [-0.0148, -0.0216, -0.0127,  ..., -0.0104, -0.0164, -0.0102],
        [ 0.0071, -0.0007,  0.0246,  ..., -0.0168, -0.0297,  0.0068],
        [-0.0062,  0.0211,  0.0141,  ..., -0.0065, -0.0037, -0.0262]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.1445,  1.7656,  2.4746,  ...,  6.0039,  0.1058,  2.8652]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-17 23:57:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is discovering, it discovers
When something is allowing, it allows
When something is learning, it learns
When something is telling, it tells
When something is spending, it spends
When something is happening, it happens
When something is involving, it involves
When something is referring, it
2024-07-17 23:57:36 root INFO     [order_1_approx] starting weight calculation for When something is spending, it spends
When something is allowing, it allows
When something is referring, it refers
When something is learning, it learns
When something is discovering, it discovers
When something is happening, it happens
When something is involving, it involves
When something is telling, it
2024-07-17 23:57:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 00:01:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([1.7246, 0.3765, 0.7100,  ..., 0.8281, 1.2422, 0.4214], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6152,  0.8940,  1.9688,  ...,  3.0332,  0.0264,  3.2090],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0081, -0.0200,  0.0053,  ...,  0.0160, -0.0113, -0.0037],
        [-0.0086,  0.0087, -0.0020,  ..., -0.0055,  0.0023,  0.0224],
        [-0.0127,  0.0138,  0.0027,  ..., -0.0107,  0.0183,  0.0112],
        ...,
        [-0.0365,  0.0013, -0.0404,  ..., -0.0134,  0.0043, -0.0079],
        [ 0.0123, -0.0018,  0.0127,  ..., -0.0087, -0.0324, -0.0005],
        [-0.0098,  0.0288,  0.0066,  ..., -0.0043,  0.0106, -0.0206]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.9473,  1.9004,  2.1289,  ...,  3.9004, -0.0357,  2.3848]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 00:01:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is spending, it spends
When something is allowing, it allows
When something is referring, it refers
When something is learning, it learns
When something is discovering, it discovers
When something is happening, it happens
When something is involving, it involves
When something is telling, it
2024-07-18 00:01:09 root INFO     [order_1_approx] starting weight calculation for When something is telling, it tells
When something is discovering, it discovers
When something is learning, it learns
When something is spending, it spends
When something is involving, it involves
When something is referring, it refers
When something is allowing, it allows
When something is happening, it
2024-07-18 00:01:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 00:04:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3967,  0.0034,  2.0586,  ...,  0.2583,  1.8555, -0.6895],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0762, -0.5312, -1.4541,  ..., -0.4351,  2.0098,  5.5859],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0316, -0.0190,  0.0055,  ...,  0.0071, -0.0051, -0.0048],
        [-0.0006, -0.0108, -0.0246,  ...,  0.0054, -0.0023,  0.0026],
        [-0.0042,  0.0085, -0.0275,  ...,  0.0039,  0.0069,  0.0156],
        ...,
        [-0.0115, -0.0057, -0.0177,  ..., -0.0212, -0.0025, -0.0016],
        [ 0.0043,  0.0082,  0.0068,  ..., -0.0135, -0.0272,  0.0099],
        [-0.0194,  0.0076,  0.0079,  ...,  0.0081, -0.0018, -0.0095]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7739,  0.3438, -1.3604,  ..., -1.1914,  1.0508,  5.2500]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 00:04:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is telling, it tells
When something is discovering, it discovers
When something is learning, it learns
When something is spending, it spends
When something is involving, it involves
When something is referring, it refers
When something is allowing, it allows
When something is happening, it
2024-07-18 00:04:43 root INFO     [order_1_approx] starting weight calculation for When something is referring, it refers
When something is happening, it happens
When something is involving, it involves
When something is learning, it learns
When something is discovering, it discovers
When something is telling, it tells
When something is allowing, it allows
When something is spending, it
2024-07-18 00:04:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 00:08:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6680, -0.1030, -0.6455,  ..., -0.1370,  1.6914,  0.1548],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8838,  2.2051,  1.4062,  ...,  1.7480, -1.0293,  4.4062],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0215, -0.0208,  0.0119,  ..., -0.0043,  0.0059, -0.0100],
        [ 0.0025, -0.0043, -0.0103,  ..., -0.0055, -0.0168, -0.0044],
        [-0.0219,  0.0035,  0.0017,  ..., -0.0041,  0.0030, -0.0038],
        ...,
        [-0.0003, -0.0061, -0.0154,  ..., -0.0157, -0.0266, -0.0124],
        [-0.0086,  0.0068,  0.0067,  ...,  0.0043, -0.0109, -0.0007],
        [-0.0152, -0.0022,  0.0108,  ..., -0.0019, -0.0012, -0.0080]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3828,  2.2129,  1.6807,  ...,  1.3613, -1.3604,  4.7305]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 00:08:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is referring, it refers
When something is happening, it happens
When something is involving, it involves
When something is learning, it learns
When something is discovering, it discovers
When something is telling, it tells
When something is allowing, it allows
When something is spending, it
2024-07-18 00:08:16 root INFO     [order_1_approx] starting weight calculation for When something is happening, it happens
When something is discovering, it discovers
When something is referring, it refers
When something is allowing, it allows
When something is telling, it tells
When something is involving, it involves
When something is spending, it spends
When something is learning, it
2024-07-18 00:08:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 00:11:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.6934,  0.8491,  1.1055,  ..., -1.3018,  1.7705,  0.7246],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.3359,  3.5977,  0.8247,  ...,  1.5859, -3.2188,  2.3750],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0140, -0.0095,  0.0120,  ...,  0.0082, -0.0001, -0.0166],
        [-0.0132,  0.0067, -0.0091,  ..., -0.0080, -0.0063,  0.0059],
        [-0.0129,  0.0109, -0.0129,  ..., -0.0055,  0.0105,  0.0028],
        ...,
        [-0.0108, -0.0020, -0.0157,  ..., -0.0150,  0.0013, -0.0148],
        [ 0.0050,  0.0035,  0.0063,  ..., -0.0076, -0.0133, -0.0016],
        [-0.0220,  0.0124,  0.0102,  ..., -0.0143, -0.0046, -0.0231]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7080,  3.8711,  0.6460,  ...,  2.0039, -3.5449,  2.3555]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 00:11:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is happening, it happens
When something is discovering, it discovers
When something is referring, it refers
When something is allowing, it allows
When something is telling, it tells
When something is involving, it involves
When something is spending, it spends
When something is learning, it
2024-07-18 00:11:49 root INFO     [order_1_approx] starting weight calculation for When something is happening, it happens
When something is telling, it tells
When something is learning, it learns
When something is spending, it spends
When something is involving, it involves
When something is discovering, it discovers
When something is referring, it refers
When something is allowing, it
2024-07-18 00:11:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 00:15:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.8008, -0.0637,  0.2920,  ...,  0.6719,  1.2217, -0.0979],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1818, -1.7822,  2.2168,  ...,  2.0254, -1.9258,  2.9805],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0123, -0.0202,  0.0014,  ...,  0.0012,  0.0132, -0.0146],
        [ 0.0065, -0.0088, -0.0008,  ..., -0.0067, -0.0046, -0.0026],
        [-0.0084, -0.0015, -0.0119,  ...,  0.0041,  0.0011, -0.0010],
        ...,
        [-0.0087, -0.0149, -0.0153,  ..., -0.0064,  0.0020, -0.0057],
        [ 0.0045, -0.0023,  0.0118,  ..., -0.0128, -0.0194, -0.0041],
        [-0.0030,  0.0081, -0.0032,  ...,  0.0104, -0.0158, -0.0051]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5767, -1.1270,  2.8633,  ...,  2.0918, -2.2012,  2.3027]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 00:15:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is happening, it happens
When something is telling, it tells
When something is learning, it learns
When something is spending, it spends
When something is involving, it involves
When something is discovering, it discovers
When something is referring, it refers
When something is allowing, it
2024-07-18 00:15:21 root INFO     [order_1_approx] starting weight calculation for When something is involving, it involves
When something is telling, it tells
When something is allowing, it allows
When something is referring, it refers
When something is spending, it spends
When something is happening, it happens
When something is learning, it learns
When something is discovering, it
2024-07-18 00:15:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 00:18:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4219, -0.2593,  0.6533,  ..., -0.0693,  1.0000,  0.0144],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4756, -0.7954,  1.2930,  ..., -1.9229, -4.9688,  2.9395],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-3.3779e-03, -2.1622e-02,  1.0941e-02,  ..., -4.4155e-04,
         -9.8114e-03, -2.2507e-02],
        [ 1.1253e-04,  1.5411e-02, -5.9662e-03,  ..., -2.9945e-03,
         -1.0371e-05,  8.1482e-03],
        [ 5.5428e-03, -2.1286e-03, -6.0883e-03,  ..., -3.6621e-03,
          6.9008e-03,  9.9487e-03],
        ...,
        [-1.9379e-02,  4.7073e-03, -1.1398e-02,  ...,  3.4943e-03,
          2.4014e-03, -4.5891e-03],
        [ 5.7602e-03,  5.5237e-03,  1.7853e-02,  ..., -7.1106e-03,
         -9.5444e-03, -3.7575e-03],
        [-6.4354e-03,  3.6163e-03, -2.5768e-03,  ..., -1.8448e-02,
          5.3482e-03, -5.9471e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4319, -0.3901,  0.9019,  ..., -1.8604, -4.8203,  2.9980]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 00:18:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is involving, it involves
When something is telling, it tells
When something is allowing, it allows
When something is referring, it refers
When something is spending, it spends
When something is happening, it happens
When something is learning, it learns
When something is discovering, it
2024-07-18 00:18:53 root INFO     total operator prediction time: 1703.7690269947052 seconds
2024-07-18 00:18:53 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on noun - plural_reg
2024-07-18 00:18:53 root INFO     building operator noun - plural_reg
2024-07-18 00:18:53 root INFO     [order_1_approx] starting weight calculation for The plural form of solution is solutions
The plural form of director is directors
The plural form of album is albums
The plural form of customer is customers
The plural form of website is websites
The plural form of thing is things
The plural form of version is versions
The plural form of god is
2024-07-18 00:18:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 00:22:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.7471, -0.4307, -0.0762,  ..., -1.1689, -1.1328, -0.1299],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.7012, -1.0508, -0.1582,  ...,  0.9766, -0.9961,  0.8984],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0081, -0.0095,  0.0162,  ...,  0.0040, -0.0050, -0.0173],
        [-0.0014, -0.0125, -0.0018,  ...,  0.0153,  0.0007,  0.0083],
        [ 0.0133, -0.0131, -0.0060,  ..., -0.0126, -0.0054, -0.0098],
        ...,
        [ 0.0045, -0.0005, -0.0119,  ..., -0.0246,  0.0047, -0.0070],
        [-0.0035, -0.0046, -0.0012,  ..., -0.0162, -0.0308, -0.0043],
        [-0.0164,  0.0031, -0.0028,  ..., -0.0008, -0.0078, -0.0161]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.1445, -1.4814,  0.6104,  ...,  0.6406, -0.6816,  1.1221]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 00:22:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of solution is solutions
The plural form of director is directors
The plural form of album is albums
The plural form of customer is customers
The plural form of website is websites
The plural form of thing is things
The plural form of version is versions
The plural form of god is
2024-07-18 00:22:26 root INFO     [order_1_approx] starting weight calculation for The plural form of thing is things
The plural form of director is directors
The plural form of version is versions
The plural form of god is gods
The plural form of solution is solutions
The plural form of album is albums
The plural form of customer is customers
The plural form of website is
2024-07-18 00:22:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 00:25:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.7979e+00,  2.8906e-01, -7.0361e-01,  ..., -1.4648e-03,
         4.4800e-01,  1.4199e+00], device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6426, -5.1875,  2.2930,  ..., -2.2793,  0.9932,  4.6289],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0065, -0.0114,  0.0109,  ...,  0.0065, -0.0038, -0.0150],
        [-0.0036, -0.0025,  0.0001,  ...,  0.0100, -0.0015,  0.0047],
        [ 0.0021, -0.0199, -0.0248,  ..., -0.0088,  0.0136,  0.0183],
        ...,
        [-0.0099,  0.0002,  0.0022,  ..., -0.0194,  0.0046, -0.0019],
        [ 0.0001, -0.0299,  0.0143,  ..., -0.0107, -0.0365,  0.0077],
        [-0.0198,  0.0082, -0.0073,  ..., -0.0086, -0.0146, -0.0046]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0381, -5.5977,  2.2520,  ..., -2.1074,  1.3174,  4.8477]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 00:25:59 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of thing is things
The plural form of director is directors
The plural form of version is versions
The plural form of god is gods
The plural form of solution is solutions
The plural form of album is albums
The plural form of customer is customers
The plural form of website is
2024-07-18 00:26:00 root INFO     [order_1_approx] starting weight calculation for The plural form of website is websites
The plural form of version is versions
The plural form of album is albums
The plural form of thing is things
The plural form of director is directors
The plural form of god is gods
The plural form of customer is customers
The plural form of solution is
2024-07-18 00:26:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 00:29:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([1.6074, 0.9927, 0.7993,  ..., 0.1121, 0.6914, 0.3494], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.8555,  0.1731,  3.2559,  ...,  1.6787, -1.2725,  0.9453],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0085,  0.0034,  0.0028,  ..., -0.0085, -0.0108, -0.0166],
        [ 0.0036, -0.0068, -0.0025,  ...,  0.0049, -0.0110, -0.0106],
        [-0.0141, -0.0089, -0.0160,  ..., -0.0100,  0.0107, -0.0038],
        ...,
        [-0.0085, -0.0079, -0.0052,  ..., -0.0015,  0.0035, -0.0028],
        [-0.0048, -0.0041,  0.0055,  ..., -0.0040, -0.0132,  0.0015],
        [-0.0135,  0.0212, -0.0007,  ..., -0.0060, -0.0058, -0.0012]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.4961,  0.7363,  2.9453,  ...,  1.3477, -1.1299,  1.1914]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 00:29:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of website is websites
The plural form of version is versions
The plural form of album is albums
The plural form of thing is things
The plural form of director is directors
The plural form of god is gods
The plural form of customer is customers
The plural form of solution is
2024-07-18 00:29:28 root INFO     [order_1_approx] starting weight calculation for The plural form of website is websites
The plural form of god is gods
The plural form of version is versions
The plural form of solution is solutions
The plural form of customer is customers
The plural form of director is directors
The plural form of album is albums
The plural form of thing is
2024-07-18 00:29:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 00:32:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3809,  0.1249,  0.4226,  ..., -0.4067, -0.2469, -0.0825],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.4053, -0.6162,  0.3262,  ...,  0.5171,  1.0107, -0.6738],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0150, -0.0104,  0.0233,  ..., -0.0015, -0.0077, -0.0041],
        [-0.0121, -0.0192, -0.0111,  ...,  0.0139, -0.0027, -0.0117],
        [ 0.0118, -0.0020, -0.0234,  ..., -0.0104,  0.0154,  0.0092],
        ...,
        [ 0.0168, -0.0103,  0.0102,  ..., -0.0247,  0.0036,  0.0035],
        [-0.0157, -0.0199, -0.0021,  ..., -0.0081, -0.0263,  0.0017],
        [-0.0109,  0.0214,  0.0044,  ..., -0.0042, -0.0064, -0.0017]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9365, -0.1719,  0.4238,  ...,  0.3486,  0.9561, -0.0522]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 00:33:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of website is websites
The plural form of god is gods
The plural form of version is versions
The plural form of solution is solutions
The plural form of customer is customers
The plural form of director is directors
The plural form of album is albums
The plural form of thing is
2024-07-18 00:33:00 root INFO     [order_1_approx] starting weight calculation for The plural form of god is gods
The plural form of website is websites
The plural form of customer is customers
The plural form of album is albums
The plural form of solution is solutions
The plural form of thing is things
The plural form of director is directors
The plural form of version is
2024-07-18 00:33:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 00:36:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0527,  0.6914,  0.1482,  ...,  0.1836,  0.1879, -0.1750],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9238, -2.9082,  2.2617,  ...,  0.2686, -0.8955, -0.4941],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.2543e-02,  1.6479e-03,  1.4603e-02,  ...,  4.2534e-03,
         -3.4447e-03, -1.6891e-02],
        [-2.6302e-03, -3.0518e-05,  1.6098e-03,  ...,  1.7609e-02,
         -4.8904e-03,  8.6670e-03],
        [ 5.0430e-03, -1.1086e-02, -1.7914e-02,  ..., -9.6970e-03,
          2.1210e-02,  3.8528e-04],
        ...,
        [-6.3934e-03, -7.1297e-03, -8.1940e-03,  ..., -8.6365e-03,
          1.1299e-02, -1.5366e-02],
        [-8.0719e-03,  1.8921e-03,  1.9569e-03,  ..., -1.0956e-02,
         -2.2888e-02, -1.0300e-04],
        [ 4.8065e-04,  2.2537e-02,  8.6212e-03,  ..., -5.2338e-03,
          1.5717e-03, -4.8065e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.8633, -3.3516,  2.5117,  ..., -0.4746, -0.2764, -0.0254]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 00:36:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of god is gods
The plural form of website is websites
The plural form of customer is customers
The plural form of album is albums
The plural form of solution is solutions
The plural form of thing is things
The plural form of director is directors
The plural form of version is
2024-07-18 00:36:33 root INFO     [order_1_approx] starting weight calculation for The plural form of customer is customers
The plural form of solution is solutions
The plural form of thing is things
The plural form of director is directors
The plural form of god is gods
The plural form of website is websites
The plural form of version is versions
The plural form of album is
2024-07-18 00:36:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 00:40:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.3457, -0.3940, -0.7764,  ..., -0.0361, -1.0059, -0.2515],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7397, -1.0557, -0.6934,  ...,  2.1094, -0.2625,  4.8281],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0192, -0.0142,  0.0172,  ...,  0.0004,  0.0069, -0.0339],
        [ 0.0042, -0.0125, -0.0080,  ...,  0.0148, -0.0023,  0.0131],
        [-0.0015, -0.0099, -0.0041,  ..., -0.0139,  0.0187,  0.0001],
        ...,
        [ 0.0011, -0.0075, -0.0154,  ..., -0.0093,  0.0040,  0.0033],
        [-0.0020, -0.0051, -0.0038,  ..., -0.0169, -0.0150, -0.0223],
        [-0.0349,  0.0246, -0.0034,  ..., -0.0169,  0.0084,  0.0076]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3311, -0.8716, -0.4993,  ...,  1.7402,  0.0881,  5.9102]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 00:40:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of customer is customers
The plural form of solution is solutions
The plural form of thing is things
The plural form of director is directors
The plural form of god is gods
The plural form of website is websites
The plural form of version is versions
The plural form of album is
2024-07-18 00:40:06 root INFO     [order_1_approx] starting weight calculation for The plural form of god is gods
The plural form of version is versions
The plural form of thing is things
The plural form of director is directors
The plural form of album is albums
The plural form of website is websites
The plural form of solution is solutions
The plural form of customer is
2024-07-18 00:40:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 00:43:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1777,  0.0829, -0.0267,  ...,  1.4492, -0.6396,  0.9224],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.9727,  1.3594,  2.8418,  ..., -5.0430, -1.9961, -0.3770],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0054, -0.0027,  0.0198,  ...,  0.0005, -0.0019, -0.0097],
        [-0.0148, -0.0057, -0.0026,  ...,  0.0168, -0.0012,  0.0159],
        [ 0.0135, -0.0081,  0.0006,  ..., -0.0072,  0.0262,  0.0128],
        ...,
        [-0.0024, -0.0067, -0.0110,  ..., -0.0034,  0.0140, -0.0195],
        [-0.0074,  0.0048,  0.0067,  ..., -0.0140, -0.0191, -0.0059],
        [-0.0252,  0.0017,  0.0068,  ..., -0.0026, -0.0071,  0.0016]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.4258,  1.5469,  3.1895,  ..., -5.5469, -1.5488, -0.0913]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 00:43:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of god is gods
The plural form of version is versions
The plural form of thing is things
The plural form of director is directors
The plural form of album is albums
The plural form of website is websites
The plural form of solution is solutions
The plural form of customer is
2024-07-18 00:43:39 root INFO     [order_1_approx] starting weight calculation for The plural form of album is albums
The plural form of website is websites
The plural form of god is gods
The plural form of solution is solutions
The plural form of version is versions
The plural form of thing is things
The plural form of customer is customers
The plural form of director is
2024-07-18 00:43:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 00:47:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.5703, -0.5845,  0.4795,  ..., -0.0859, -0.4546, -0.2341],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.7266, -3.0977,  1.3750,  ..., -3.5312, -0.8301,  6.3438],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0035, -0.0071,  0.0090,  ..., -0.0165, -0.0134, -0.0061],
        [ 0.0032,  0.0033,  0.0005,  ..., -0.0037, -0.0097, -0.0080],
        [ 0.0077, -0.0102, -0.0087,  ...,  0.0028,  0.0074,  0.0030],
        ...,
        [-0.0091, -0.0072, -0.0030,  ...,  0.0003,  0.0090, -0.0154],
        [ 0.0092, -0.0128,  0.0109,  ..., -0.0204, -0.0178,  0.0014],
        [-0.0205,  0.0038, -0.0092,  ..., -0.0077, -0.0067,  0.0074]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.3242, -3.2402,  1.3564,  ..., -4.7617, -1.2158,  7.2109]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 00:47:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of album is albums
The plural form of website is websites
The plural form of god is gods
The plural form of solution is solutions
The plural form of version is versions
The plural form of thing is things
The plural form of customer is customers
The plural form of director is
2024-07-18 00:47:12 root INFO     total operator prediction time: 1698.639660835266 seconds
2024-07-18 00:47:12 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_3pSg - Ved
2024-07-18 00:47:12 root INFO     building operator verb_3pSg - Ved
2024-07-18 00:47:12 root INFO     [order_1_approx] starting weight calculation for When he requires something, something has been required
When he introduces something, something has been introduced
When he considers something, something has been considered
When he performs something, something has been performed
When he becomes something, something has been became
When he expects something, something has been expected
When he seems something, something has been seemed
When he publishes something, something has been
2024-07-18 00:47:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 00:50:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5264,  0.3159,  1.0078,  ...,  0.6768, -0.0684,  0.4673],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.8047, -2.6035, -0.3447,  ...,  2.4043, -2.5156, -0.7100],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0012, -0.0131,  0.0097,  ..., -0.0044, -0.0064, -0.0060],
        [ 0.0011,  0.0086, -0.0053,  ...,  0.0097, -0.0132, -0.0016],
        [ 0.0042, -0.0044,  0.0006,  ..., -0.0093, -0.0060,  0.0102],
        ...,
        [-0.0123, -0.0136, -0.0147,  ..., -0.0046, -0.0080, -0.0062],
        [-0.0070, -0.0042,  0.0186,  ..., -0.0139, -0.0146,  0.0042],
        [-0.0118,  0.0118,  0.0029,  ...,  0.0006,  0.0088, -0.0273]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.9336, -2.3594,  0.1499,  ...,  2.4668, -1.9648, -0.7349]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 00:50:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he requires something, something has been required
When he introduces something, something has been introduced
When he considers something, something has been considered
When he performs something, something has been performed
When he becomes something, something has been became
When he expects something, something has been expected
When he seems something, something has been seemed
When he publishes something, something has been
2024-07-18 00:50:46 root INFO     [order_1_approx] starting weight calculation for When he considers something, something has been considered
When he becomes something, something has been became
When he seems something, something has been seemed
When he expects something, something has been expected
When he publishes something, something has been published
When he requires something, something has been required
When he introduces something, something has been introduced
When he performs something, something has been
2024-07-18 00:50:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 00:54:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.5752,  1.0039, -0.1926,  ..., -0.6382,  0.3027, -1.0703],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8311, -0.2612,  1.8936,  ...,  0.8599,  1.4707, -0.6240],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-8.8272e-03, -8.5068e-03,  5.2452e-03,  ..., -4.5547e-03,
         -4.4785e-03, -7.2556e-03],
        [-6.6452e-03, -7.3242e-04,  6.7616e-04,  ..., -3.5515e-03,
         -5.4855e-03, -3.1013e-03],
        [ 1.3161e-02,  6.7635e-03, -1.2741e-02,  ..., -4.3945e-03,
          1.0300e-03,  2.8648e-03],
        ...,
        [-3.3264e-03, -5.5456e-04, -7.0877e-03,  ..., -5.2261e-03,
          1.5984e-03, -9.7580e-03],
        [ 1.6651e-03,  1.2851e-04,  7.1793e-03,  ..., -7.6866e-03,
         -1.8463e-02,  4.8161e-05],
        [-7.1182e-03,  1.8188e-02,  4.7989e-03,  ...,  4.2419e-03,
         -3.4161e-03, -1.0498e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5117,  0.1199,  2.1211,  ...,  1.0195,  1.1680, -0.1050]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 00:54:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he considers something, something has been considered
When he becomes something, something has been became
When he seems something, something has been seemed
When he expects something, something has been expected
When he publishes something, something has been published
When he requires something, something has been required
When he introduces something, something has been introduced
When he performs something, something has been
2024-07-18 00:54:21 root INFO     [order_1_approx] starting weight calculation for When he requires something, something has been required
When he seems something, something has been seemed
When he becomes something, something has been became
When he publishes something, something has been published
When he performs something, something has been performed
When he expects something, something has been expected
When he considers something, something has been considered
When he introduces something, something has been
2024-07-18 00:54:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 00:57:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2354,  1.0918,  0.4741,  ...,  0.2156, -0.5820, -0.6055],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3110, -1.1641,  0.8418,  ...,  3.2617,  0.4092,  4.1680],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-6.5613e-04, -1.6663e-02,  1.3351e-02,  ...,  3.7670e-03,
         -8.4839e-03, -4.5395e-03],
        [ 4.8218e-03,  6.0883e-03,  1.0490e-03,  ...,  6.6032e-03,
         -7.8659e-03,  7.3814e-04],
        [-4.2915e-03, -4.9820e-03,  1.5488e-03,  ..., -1.9894e-03,
         -4.0169e-03,  3.5343e-03],
        ...,
        [-1.2550e-02, -5.9319e-03, -8.0338e-03,  ...,  5.6534e-03,
         -9.4299e-03, -4.7207e-04],
        [ 1.1765e-02,  2.5883e-03, -6.7406e-03,  ..., -1.4389e-02,
         -1.9226e-02, -1.6785e-04],
        [-9.6893e-03,  1.5144e-02,  7.8125e-03,  ...,  9.9030e-03,
         -6.1512e-05, -1.3161e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1171, -1.2246,  1.6172,  ...,  2.7109,  0.6553,  4.2812]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 00:57:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he requires something, something has been required
When he seems something, something has been seemed
When he becomes something, something has been became
When he publishes something, something has been published
When he performs something, something has been performed
When he expects something, something has been expected
When he considers something, something has been considered
When he introduces something, something has been
2024-07-18 00:57:55 root INFO     [order_1_approx] starting weight calculation for When he introduces something, something has been introduced
When he publishes something, something has been published
When he considers something, something has been considered
When he requires something, something has been required
When he performs something, something has been performed
When he seems something, something has been seemed
When he expects something, something has been expected
When he becomes something, something has been
2024-07-18 00:57:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 01:01:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5078,  1.5098,  1.1211,  ...,  1.0801,  0.4128, -0.4194],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7461,  0.7383,  0.4043,  ..., -0.6089,  1.1963, -1.1973],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0138, -0.0161,  0.0097,  ..., -0.0035,  0.0012, -0.0191],
        [-0.0078,  0.0067, -0.0061,  ...,  0.0006,  0.0008,  0.0018],
        [-0.0059, -0.0061,  0.0014,  ..., -0.0107, -0.0127,  0.0114],
        ...,
        [-0.0022, -0.0165, -0.0095,  ..., -0.0065, -0.0015, -0.0027],
        [ 0.0043, -0.0157,  0.0014,  ..., -0.0065, -0.0071, -0.0085],
        [-0.0040,  0.0173,  0.0072,  ..., -0.0023, -0.0026, -0.0178]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.3574,  0.3909,  0.9600,  ..., -0.3608,  1.0537, -0.7793]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 01:01:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he introduces something, something has been introduced
When he publishes something, something has been published
When he considers something, something has been considered
When he requires something, something has been required
When he performs something, something has been performed
When he seems something, something has been seemed
When he expects something, something has been expected
When he becomes something, something has been
2024-07-18 01:01:28 root INFO     [order_1_approx] starting weight calculation for When he publishes something, something has been published
When he performs something, something has been performed
When he becomes something, something has been became
When he introduces something, something has been introduced
When he considers something, something has been considered
When he seems something, something has been seemed
When he requires something, something has been required
When he expects something, something has been
2024-07-18 01:01:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 01:05:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5264,  1.2246, -0.4189,  ...,  1.1650,  1.5273,  0.1936],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.3828,  0.3599, -0.0811,  ..., -0.3123, -1.3330, -0.3818],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0045, -0.0147,  0.0032,  ..., -0.0014, -0.0053, -0.0055],
        [-0.0071, -0.0048, -0.0114,  ...,  0.0012,  0.0035, -0.0007],
        [ 0.0057,  0.0028, -0.0190,  ...,  0.0055, -0.0034,  0.0060],
        ...,
        [-0.0031, -0.0019, -0.0108,  ..., -0.0048,  0.0029, -0.0084],
        [ 0.0099, -0.0057, -0.0007,  ..., -0.0199, -0.0263,  0.0143],
        [-0.0063,  0.0014,  0.0060,  ..., -0.0039,  0.0053, -0.0150]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.0625,  0.8467,  0.4434,  ..., -0.7910, -0.7427, -0.9805]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 01:05:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he publishes something, something has been published
When he performs something, something has been performed
When he becomes something, something has been became
When he introduces something, something has been introduced
When he considers something, something has been considered
When he seems something, something has been seemed
When he requires something, something has been required
When he expects something, something has been
2024-07-18 01:05:02 root INFO     [order_1_approx] starting weight calculation for When he requires something, something has been required
When he publishes something, something has been published
When he seems something, something has been seemed
When he performs something, something has been performed
When he expects something, something has been expected
When he becomes something, something has been became
When he introduces something, something has been introduced
When he considers something, something has been
2024-07-18 01:05:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 01:08:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7100,  1.7598,  0.4409,  ...,  0.2930,  0.6172,  0.1309],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1686,  2.3320,  0.0928,  ...,  0.7407, -4.0195, -3.2539],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0111,  0.0065,  0.0137,  ...,  0.0033, -0.0175,  0.0008],
        [-0.0103,  0.0065, -0.0042,  ..., -0.0034, -0.0177, -0.0108],
        [ 0.0071, -0.0017,  0.0011,  ...,  0.0133, -0.0020, -0.0049],
        ...,
        [-0.0135, -0.0073,  0.0008,  ...,  0.0078, -0.0057, -0.0068],
        [ 0.0191,  0.0089, -0.0035,  ..., -0.0220, -0.0221,  0.0056],
        [-0.0100,  0.0258,  0.0049,  ...,  0.0086,  0.0181, -0.0123]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2986,  2.4102,  0.6606,  ...,  0.8413, -4.4648, -3.4258]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 01:08:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he requires something, something has been required
When he publishes something, something has been published
When he seems something, something has been seemed
When he performs something, something has been performed
When he expects something, something has been expected
When he becomes something, something has been became
When he introduces something, something has been introduced
When he considers something, something has been
2024-07-18 01:08:36 root INFO     [order_1_approx] starting weight calculation for When he performs something, something has been performed
When he expects something, something has been expected
When he seems something, something has been seemed
When he becomes something, something has been became
When he introduces something, something has been introduced
When he publishes something, something has been published
When he considers something, something has been considered
When he requires something, something has been
2024-07-18 01:08:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 01:12:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2808,  1.2188,  0.0283,  ...,  0.8818,  0.7900, -0.1335],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.4883, -0.6670, -1.1094,  ..., -0.5518, -0.7842,  2.3984],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0063, -0.0054,  0.0125,  ..., -0.0179, -0.0055, -0.0099],
        [ 0.0023,  0.0228, -0.0017,  ...,  0.0003, -0.0021,  0.0095],
        [ 0.0080, -0.0007,  0.0019,  ...,  0.0057, -0.0005,  0.0101],
        ...,
        [-0.0206, -0.0101, -0.0178,  ..., -0.0105,  0.0086, -0.0056],
        [ 0.0044,  0.0048,  0.0152,  ..., -0.0274, -0.0079,  0.0117],
        [-0.0181,  0.0132, -0.0017,  ..., -0.0027, -0.0024,  0.0023]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.5312, -0.3726, -0.4785,  ..., -0.5024, -0.7031,  2.1133]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 01:12:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he performs something, something has been performed
When he expects something, something has been expected
When he seems something, something has been seemed
When he becomes something, something has been became
When he introduces something, something has been introduced
When he publishes something, something has been published
When he considers something, something has been considered
When he requires something, something has been
2024-07-18 01:12:09 root INFO     [order_1_approx] starting weight calculation for When he publishes something, something has been published
When he considers something, something has been considered
When he becomes something, something has been became
When he introduces something, something has been introduced
When he performs something, something has been performed
When he requires something, something has been required
When he expects something, something has been expected
When he seems something, something has been
2024-07-18 01:12:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 01:15:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8003,  1.2305,  0.3513,  ...,  0.6538,  0.3757, -0.9727],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5522,  3.0371,  1.6943,  ...,  0.4736, -4.9688,  0.4414],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0033, -0.0154,  0.0087,  ...,  0.0065,  0.0019,  0.0024],
        [-0.0254, -0.0070, -0.0060,  ..., -0.0014, -0.0069,  0.0088],
        [ 0.0028, -0.0138, -0.0110,  ..., -0.0024,  0.0155, -0.0040],
        ...,
        [-0.0021, -0.0047,  0.0015,  ...,  0.0019, -0.0052, -0.0188],
        [ 0.0047,  0.0101,  0.0047,  ..., -0.0180,  0.0001,  0.0032],
        [-0.0002,  0.0102,  0.0045,  ..., -0.0054, -0.0016, -0.0034]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3271,  3.4785,  1.4893,  ..., -0.0938, -4.8125,  0.1619]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 01:15:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he publishes something, something has been published
When he considers something, something has been considered
When he becomes something, something has been became
When he introduces something, something has been introduced
When he performs something, something has been performed
When he requires something, something has been required
When he expects something, something has been expected
When he seems something, something has been
2024-07-18 01:15:43 root INFO     total operator prediction time: 1711.3129637241364 seconds
2024-07-18 01:15:43 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on adj - superlative
2024-07-18 01:15:43 root INFO     building operator adj - superlative
2024-07-18 01:15:43 root INFO     [order_1_approx] starting weight calculation for If something is the most nasty, it is nastiest
If something is the most lucky, it is luckiest
If something is the most clever, it is cleverest
If something is the most strict, it is strictest
If something is the most cheap, it is cheapest
If something is the most lazy, it is laziest
If something is the most tasty, it is tastiest
If something is the most hot, it is
2024-07-18 01:15:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 01:19:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5728,  0.6440,  0.7773,  ...,  0.1001,  0.6362, -0.1954],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0908, -1.0547, -1.0000,  ..., -0.9473,  3.9102,  5.0508],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-5.7793e-03, -1.4267e-02,  3.6793e-03,  ..., -7.0496e-03,
         -5.1651e-03, -8.8882e-03],
        [-2.9297e-03,  1.4610e-03, -3.4752e-03,  ...,  3.7098e-03,
         -2.7161e-03, -8.5831e-05],
        [-1.0056e-02,  8.5754e-03,  6.1951e-03,  ...,  1.2646e-03,
          1.1185e-02, -1.0719e-03],
        ...,
        [-4.3869e-03,  5.4512e-03,  9.0332e-03,  ...,  2.6207e-03,
         -1.9398e-03,  1.2665e-02],
        [-6.1188e-03,  2.9678e-03,  3.6697e-03,  ..., -3.8452e-03,
         -1.6037e-02, -1.1432e-04],
        [-2.0782e-02,  3.3436e-03, -3.1242e-03,  ..., -6.7711e-03,
         -4.0102e-04, -6.0997e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1028, -0.9814, -0.9341,  ..., -1.0928,  3.4668,  5.7070]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 01:19:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most nasty, it is nastiest
If something is the most lucky, it is luckiest
If something is the most clever, it is cleverest
If something is the most strict, it is strictest
If something is the most cheap, it is cheapest
If something is the most lazy, it is laziest
If something is the most tasty, it is tastiest
If something is the most hot, it is
2024-07-18 01:19:17 root INFO     [order_1_approx] starting weight calculation for If something is the most tasty, it is tastiest
If something is the most lazy, it is laziest
If something is the most strict, it is strictest
If something is the most nasty, it is nastiest
If something is the most hot, it is hottest
If something is the most cheap, it is cheapest
If something is the most lucky, it is luckiest
If something is the most clever, it is
2024-07-18 01:19:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 01:22:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1846, -1.4502,  0.6289,  ...,  0.1611,  0.7007, -0.6133],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0645, -1.0166,  0.2920,  ...,  1.4844, -2.1289,  1.3271],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0053, -0.0200,  0.0050,  ..., -0.0063, -0.0057, -0.0094],
        [-0.0039, -0.0040,  0.0084,  ...,  0.0118, -0.0070, -0.0093],
        [-0.0082,  0.0013, -0.0031,  ..., -0.0090,  0.0097, -0.0014],
        ...,
        [-0.0043, -0.0126,  0.0069,  ..., -0.0073,  0.0012, -0.0086],
        [ 0.0029,  0.0028,  0.0050,  ..., -0.0057, -0.0162,  0.0095],
        [-0.0184,  0.0021,  0.0030,  ..., -0.0088, -0.0066, -0.0126]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.9277, -1.2520,  0.6655,  ...,  1.2715, -1.7881,  1.7725]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 01:22:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most tasty, it is tastiest
If something is the most lazy, it is laziest
If something is the most strict, it is strictest
If something is the most nasty, it is nastiest
If something is the most hot, it is hottest
If something is the most cheap, it is cheapest
If something is the most lucky, it is luckiest
If something is the most clever, it is
2024-07-18 01:22:50 root INFO     [order_1_approx] starting weight calculation for If something is the most lazy, it is laziest
If something is the most clever, it is cleverest
If something is the most lucky, it is luckiest
If something is the most cheap, it is cheapest
If something is the most strict, it is strictest
If something is the most nasty, it is nastiest
If something is the most hot, it is hottest
If something is the most tasty, it is
2024-07-18 01:22:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 01:26:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1334,  1.0215, -0.0820,  ..., -0.7241,  0.8999, -0.8291],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.5918,  0.5479,  2.1914,  ...,  2.3047,  0.9238,  2.9531],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0094, -0.0110,  0.0127,  ..., -0.0081,  0.0117, -0.0056],
        [-0.0009, -0.0055, -0.0036,  ...,  0.0025, -0.0142,  0.0045],
        [-0.0088,  0.0023, -0.0028,  ..., -0.0052, -0.0038,  0.0019],
        ...,
        [-0.0141,  0.0004, -0.0002,  ..., -0.0132, -0.0028, -0.0103],
        [ 0.0052,  0.0070,  0.0112,  ...,  0.0004, -0.0079, -0.0070],
        [-0.0366, -0.0024,  0.0035,  ..., -0.0012, -0.0090, -0.0098]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.9453,  0.6509,  1.8359,  ...,  1.7324,  0.6826,  3.3418]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 01:26:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most lazy, it is laziest
If something is the most clever, it is cleverest
If something is the most lucky, it is luckiest
If something is the most cheap, it is cheapest
If something is the most strict, it is strictest
If something is the most nasty, it is nastiest
If something is the most hot, it is hottest
If something is the most tasty, it is
2024-07-18 01:26:23 root INFO     [order_1_approx] starting weight calculation for If something is the most hot, it is hottest
If something is the most lucky, it is luckiest
If something is the most tasty, it is tastiest
If something is the most clever, it is cleverest
If something is the most strict, it is strictest
If something is the most lazy, it is laziest
If something is the most cheap, it is cheapest
If something is the most nasty, it is
2024-07-18 01:26:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 01:29:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1152, -0.8740,  0.0078,  ..., -1.1836,  1.1055, -0.8320],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.0273, -2.3066, -1.6191,  ...,  3.7969, -0.4951,  8.5391],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0064, -0.0117,  0.0115,  ...,  0.0013, -0.0061, -0.0158],
        [-0.0031, -0.0100,  0.0078,  ...,  0.0011, -0.0005,  0.0051],
        [-0.0082,  0.0005, -0.0101,  ...,  0.0016,  0.0155,  0.0147],
        ...,
        [ 0.0036, -0.0077,  0.0107,  ..., -0.0289, -0.0257,  0.0045],
        [-0.0048,  0.0002,  0.0141,  ..., -0.0034, -0.0284,  0.0123],
        [-0.0226,  0.0023, -0.0063,  ..., -0.0010, -0.0015, -0.0517]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.9414, -2.3555, -1.5039,  ...,  3.4062,  0.4229,  8.2422]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 01:29:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most hot, it is hottest
If something is the most lucky, it is luckiest
If something is the most tasty, it is tastiest
If something is the most clever, it is cleverest
If something is the most strict, it is strictest
If something is the most lazy, it is laziest
If something is the most cheap, it is cheapest
If something is the most nasty, it is
2024-07-18 01:29:55 root INFO     [order_1_approx] starting weight calculation for If something is the most strict, it is strictest
If something is the most nasty, it is nastiest
If something is the most hot, it is hottest
If something is the most lazy, it is laziest
If something is the most tasty, it is tastiest
If something is the most clever, it is cleverest
If something is the most lucky, it is luckiest
If something is the most cheap, it is
2024-07-18 01:29:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 01:33:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0947, -1.8965, -0.2542,  ..., -0.0955,  1.0303, -0.2155],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.2773, -2.7402, -3.8320,  ...,  1.5645, -0.1050,  1.6084],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.3994e-03, -1.2268e-02,  1.7548e-02,  ..., -3.9940e-03,
         -8.8806e-03, -3.8662e-03],
        [-4.7951e-03,  6.1302e-03,  5.2795e-03,  ...,  3.8910e-04,
         -1.0117e-02, -4.2419e-03],
        [-1.1734e-02,  1.1551e-02, -5.3406e-05,  ..., -4.1313e-03,
          1.3664e-02,  8.7357e-03],
        ...,
        [-6.2370e-04, -3.3398e-03, -4.7989e-03,  ..., -9.6741e-03,
          1.0895e-02, -3.2654e-03],
        [-2.4834e-03, -2.1973e-03,  7.8583e-03,  ..., -6.6223e-03,
         -1.0666e-02,  1.0918e-02],
        [-2.5955e-02, -1.2188e-03, -1.9073e-03,  ...,  6.0730e-03,
         -1.0366e-03, -1.9104e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.8564, -2.6113, -4.7188,  ...,  1.5078, -0.1989,  1.5791]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 01:33:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most strict, it is strictest
If something is the most nasty, it is nastiest
If something is the most hot, it is hottest
If something is the most lazy, it is laziest
If something is the most tasty, it is tastiest
If something is the most clever, it is cleverest
If something is the most lucky, it is luckiest
If something is the most cheap, it is
2024-07-18 01:33:29 root INFO     [order_1_approx] starting weight calculation for If something is the most hot, it is hottest
If something is the most lucky, it is luckiest
If something is the most clever, it is cleverest
If something is the most tasty, it is tastiest
If something is the most cheap, it is cheapest
If something is the most lazy, it is laziest
If something is the most nasty, it is nastiest
If something is the most strict, it is
2024-07-18 01:33:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 01:37:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0859, -1.1367,  0.3604,  ...,  0.2847,  1.5303,  0.1929],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.4883, -1.0332,  0.4604,  ...,  1.3838,  0.6699,  4.2148],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-6.7291e-03, -2.0554e-02,  1.0124e-02,  ..., -1.5701e-02,
         -1.2772e-02, -9.4604e-03],
        [-2.8572e-03, -8.3160e-03, -4.2725e-03,  ..., -5.7449e-03,
         -7.7591e-03, -3.0479e-03],
        [ 5.2910e-03, -1.0681e-02, -7.3586e-03,  ...,  1.6632e-02,
          5.8365e-03,  4.3564e-03],
        ...,
        [ 8.5831e-05, -8.2703e-03, -1.3771e-03,  ..., -2.8038e-03,
         -6.2065e-03, -4.6082e-03],
        [-5.5542e-03, -4.6768e-03,  3.1738e-02,  ..., -4.7989e-03,
         -2.3453e-02,  1.9958e-02],
        [-3.1097e-02,  2.7039e-02, -6.9809e-03,  ...,  2.3422e-03,
         -1.8444e-03, -1.7303e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.5605, -1.4395,  0.0854,  ...,  1.3818,  1.0566,  5.0586]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 01:37:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most hot, it is hottest
If something is the most lucky, it is luckiest
If something is the most clever, it is cleverest
If something is the most tasty, it is tastiest
If something is the most cheap, it is cheapest
If something is the most lazy, it is laziest
If something is the most nasty, it is nastiest
If something is the most strict, it is
2024-07-18 01:37:01 root INFO     [order_1_approx] starting weight calculation for If something is the most tasty, it is tastiest
If something is the most nasty, it is nastiest
If something is the most hot, it is hottest
If something is the most cheap, it is cheapest
If something is the most clever, it is cleverest
If something is the most lazy, it is laziest
If something is the most strict, it is strictest
If something is the most lucky, it is
2024-07-18 01:37:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 01:40:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7598,  1.1113,  0.2047,  ...,  0.5420,  0.4951, -1.0625],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.8555,  0.6807, -3.9375,  ..., -1.6895,  4.0859,  3.9902],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.5640e-04, -1.8951e-02,  1.1185e-02,  ...,  1.0651e-02,
         -6.4545e-03, -1.0986e-02],
        [-1.9836e-03, -5.6458e-04,  2.3422e-03,  ...,  9.4452e-03,
          1.7929e-04,  2.7695e-03],
        [-1.0109e-04,  8.4839e-03,  4.4174e-03,  ...,  6.1874e-03,
          1.0880e-02,  2.5272e-03],
        ...,
        [ 1.9073e-05, -3.0899e-04, -1.1444e-03,  ..., -1.5793e-02,
          1.2436e-03, -5.2643e-03],
        [ 6.9580e-03, -1.2573e-02,  4.5395e-03,  ..., -1.0468e-02,
         -2.2949e-02,  5.6038e-03],
        [-1.5930e-02,  6.9122e-03,  8.6517e-03,  ...,  1.1154e-02,
         -8.5373e-03, -1.8341e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7197,  1.1582, -3.8477,  ..., -1.7051,  4.2344,  3.8652]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 01:40:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most tasty, it is tastiest
If something is the most nasty, it is nastiest
If something is the most hot, it is hottest
If something is the most cheap, it is cheapest
If something is the most clever, it is cleverest
If something is the most lazy, it is laziest
If something is the most strict, it is strictest
If something is the most lucky, it is
2024-07-18 01:40:35 root INFO     [order_1_approx] starting weight calculation for If something is the most clever, it is cleverest
If something is the most hot, it is hottest
If something is the most tasty, it is tastiest
If something is the most strict, it is strictest
If something is the most lucky, it is luckiest
If something is the most nasty, it is nastiest
If something is the most cheap, it is cheapest
If something is the most lazy, it is
2024-07-18 01:40:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 01:44:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0098, -0.2412, -0.2603,  ..., -0.1201,  0.4465, -0.3818],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.0078, -1.9941, -3.5293,  ..., -1.4180,  3.3008,  1.2637],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0003, -0.0118,  0.0139,  ..., -0.0031,  0.0048, -0.0165],
        [ 0.0048,  0.0021,  0.0051,  ...,  0.0022, -0.0042,  0.0017],
        [-0.0072,  0.0120,  0.0012,  ...,  0.0045,  0.0020,  0.0066],
        ...,
        [-0.0036, -0.0086, -0.0006,  ..., -0.0026, -0.0106, -0.0024],
        [-0.0013, -0.0058,  0.0052,  ...,  0.0007, -0.0110,  0.0070],
        [-0.0163,  0.0024, -0.0004,  ...,  0.0050, -0.0090, -0.0100]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.8984, -1.7227, -3.5039,  ..., -1.7549,  3.6855,  0.9775]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 01:44:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most clever, it is cleverest
If something is the most hot, it is hottest
If something is the most tasty, it is tastiest
If something is the most strict, it is strictest
If something is the most lucky, it is luckiest
If something is the most nasty, it is nastiest
If something is the most cheap, it is cheapest
If something is the most lazy, it is
2024-07-18 01:44:08 root INFO     total operator prediction time: 1705.0863485336304 seconds
2024-07-18 01:44:08 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+er_irreg
2024-07-18 01:44:08 root INFO     building operator verb+er_irreg
2024-07-18 01:44:08 root INFO     [order_1_approx] starting weight calculation for If you interpret something, you are a interpreter
If you send something, you are a sender
If you organise something, you are a organiser
If you subscribe something, you are a subscriber
If you promote something, you are a promoter
If you organize something, you are a organizer
If you preach something, you are a preacher
If you speak something, you are a
2024-07-18 01:44:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 01:47:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4487, -0.2004,  1.2861,  ...,  0.0942, -0.0774,  0.0032],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.1602,  1.8574, -2.4219,  ..., -3.6836,  1.1152,  2.3770],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0004,  0.0121,  0.0055,  ..., -0.0100, -0.0006, -0.0005],
        [ 0.0013,  0.0197,  0.0081,  ...,  0.0119, -0.0037, -0.0067],
        [ 0.0079, -0.0003,  0.0086,  ..., -0.0034,  0.0034, -0.0114],
        ...,
        [ 0.0025,  0.0004, -0.0050,  ...,  0.0096,  0.0055, -0.0010],
        [ 0.0003, -0.0097,  0.0133,  ...,  0.0053, -0.0061, -0.0020],
        [-0.0113,  0.0143,  0.0045,  ..., -0.0203, -0.0056,  0.0017]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1191,  2.0098, -2.9746,  ..., -3.6875,  0.8311,  2.3926]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 01:47:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you interpret something, you are a interpreter
If you send something, you are a sender
If you organise something, you are a organiser
If you subscribe something, you are a subscriber
If you promote something, you are a promoter
If you organize something, you are a organizer
If you preach something, you are a preacher
If you speak something, you are a
2024-07-18 01:47:43 root INFO     [order_1_approx] starting weight calculation for If you speak something, you are a speaker
If you organize something, you are a organizer
If you send something, you are a sender
If you organise something, you are a organiser
If you interpret something, you are a interpreter
If you promote something, you are a promoter
If you preach something, you are a preacher
If you subscribe something, you are a
2024-07-18 01:47:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 01:51:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.9189, -0.6011,  0.0730,  ...,  2.5352, -0.3081,  1.0889],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3789,  1.5000, -2.6680,  ..., -1.5088, -1.0605,  1.8467],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-3.0231e-03, -1.0437e-02,  2.3899e-03,  ...,  5.4245e-03,
         -6.2141e-03, -4.9706e-03],
        [-2.1172e-03,  7.0724e-03, -5.7220e-05,  ...,  5.0888e-03,
          3.0708e-03, -2.1076e-03],
        [-2.1667e-03,  2.7180e-03,  4.1962e-04,  ...,  4.0474e-03,
         -1.9035e-03,  3.8147e-03],
        ...,
        [-1.0910e-02, -1.2894e-02, -8.2703e-03,  ..., -3.4943e-03,
          2.2964e-03, -6.0272e-03],
        [-1.0757e-03, -1.8265e-02, -4.3335e-03,  ..., -8.4076e-03,
         -1.2123e-02,  6.6643e-03],
        [-1.2951e-03,  1.0269e-02, -6.4316e-03,  ..., -8.7433e-03,
          2.0943e-03, -2.8973e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4668,  1.2217, -2.4824,  ..., -1.6367, -2.1406,  1.3135]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 01:51:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you speak something, you are a speaker
If you organize something, you are a organizer
If you send something, you are a sender
If you organise something, you are a organiser
If you interpret something, you are a interpreter
If you promote something, you are a promoter
If you preach something, you are a preacher
If you subscribe something, you are a
2024-07-18 01:51:18 root INFO     [order_1_approx] starting weight calculation for If you promote something, you are a promoter
If you preach something, you are a preacher
If you subscribe something, you are a subscriber
If you organize something, you are a organizer
If you interpret something, you are a interpreter
If you organise something, you are a organiser
If you speak something, you are a speaker
If you send something, you are a
2024-07-18 01:51:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 01:54:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2642,  0.2573,  0.4512,  ...,  0.7842,  0.0244,  1.2549],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6743,  1.3457, -0.1689,  ...,  0.6387, -1.8066,  1.1396],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0073,  0.0089, -0.0101,  ..., -0.0016,  0.0108, -0.0064],
        [ 0.0050, -0.0014,  0.0074,  ...,  0.0020, -0.0024, -0.0031],
        [ 0.0078, -0.0004, -0.0011,  ...,  0.0039,  0.0028, -0.0038],
        ...,
        [-0.0091, -0.0053, -0.0055,  ...,  0.0061,  0.0021,  0.0034],
        [ 0.0031, -0.0042, -0.0018,  ..., -0.0067, -0.0072, -0.0016],
        [-0.0012,  0.0002,  0.0065,  ..., -0.0031, -0.0004,  0.0030]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.1436,  0.9585, -0.7788,  ...,  0.9282, -2.6797,  0.8252]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 01:54:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you promote something, you are a promoter
If you preach something, you are a preacher
If you subscribe something, you are a subscriber
If you organize something, you are a organizer
If you interpret something, you are a interpreter
If you organise something, you are a organiser
If you speak something, you are a speaker
If you send something, you are a
2024-07-18 01:54:51 root INFO     [order_1_approx] starting weight calculation for If you promote something, you are a promoter
If you speak something, you are a speaker
If you organize something, you are a organizer
If you preach something, you are a preacher
If you subscribe something, you are a subscriber
If you send something, you are a sender
If you organise something, you are a organiser
If you interpret something, you are a
2024-07-18 01:54:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 01:58:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4861,  0.4021, -0.8516,  ..., -0.0637,  0.7129,  0.7295],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6240,  1.2080, -2.1973,  ..., -2.0215,  1.5088,  4.0391],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0101,  0.0062, -0.0071,  ...,  0.0009,  0.0031, -0.0112],
        [ 0.0042,  0.0018,  0.0085,  ...,  0.0037, -0.0052, -0.0022],
        [-0.0025, -0.0018,  0.0050,  ..., -0.0053,  0.0019, -0.0041],
        ...,
        [-0.0105, -0.0013, -0.0033,  ...,  0.0211,  0.0108, -0.0056],
        [-0.0131, -0.0004,  0.0057,  ...,  0.0124,  0.0015,  0.0072],
        [-0.0053, -0.0056, -0.0036,  ..., -0.0166, -0.0033,  0.0010]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3210,  1.0967, -2.6543,  ..., -1.9492,  1.9746,  3.5391]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 01:58:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you promote something, you are a promoter
If you speak something, you are a speaker
If you organize something, you are a organizer
If you preach something, you are a preacher
If you subscribe something, you are a subscriber
If you send something, you are a sender
If you organise something, you are a organiser
If you interpret something, you are a
2024-07-18 01:58:25 root INFO     [order_1_approx] starting weight calculation for If you send something, you are a sender
If you organize something, you are a organizer
If you subscribe something, you are a subscriber
If you preach something, you are a preacher
If you promote something, you are a promoter
If you speak something, you are a speaker
If you interpret something, you are a interpreter
If you organise something, you are a
2024-07-18 01:58:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 02:01:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0049,  0.1553,  1.2207,  ..., -0.3125,  0.2578,  1.2715],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1008,  1.0059,  0.2383,  ..., -0.9258,  0.3838,  4.0156],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.8646e-02, -4.9973e-03, -4.6349e-03,  ..., -7.2479e-05,
          8.3466e-03, -3.6774e-03],
        [ 3.5629e-03,  9.8267e-03,  1.0357e-03,  ...,  5.5466e-03,
          9.7046e-03,  5.5161e-03],
        [-3.3054e-03, -3.4142e-03,  5.3329e-03,  ..., -3.5305e-03,
          3.8910e-03, -3.1414e-03],
        ...,
        [-1.0735e-02, -4.5166e-03,  2.9907e-03,  ...,  1.3458e-02,
         -4.8561e-03,  1.0345e-02],
        [ 9.4175e-04, -3.1891e-03, -4.1580e-03,  ..., -9.2926e-03,
          6.1226e-03,  4.9286e-03],
        [-9.4299e-03,  3.5000e-03,  1.1070e-02,  ..., -2.0157e-02,
          3.0708e-03,  1.0399e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1726,  1.1836,  0.6221,  ..., -0.9312,  0.4775,  4.4141]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 02:01:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you send something, you are a sender
If you organize something, you are a organizer
If you subscribe something, you are a subscriber
If you preach something, you are a preacher
If you promote something, you are a promoter
If you speak something, you are a speaker
If you interpret something, you are a interpreter
If you organise something, you are a
2024-07-18 02:01:59 root INFO     [order_1_approx] starting weight calculation for If you preach something, you are a preacher
If you speak something, you are a speaker
If you send something, you are a sender
If you interpret something, you are a interpreter
If you promote something, you are a promoter
If you organise something, you are a organiser
If you subscribe something, you are a subscriber
If you organize something, you are a
2024-07-18 02:01:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 02:05:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4702, -0.1016,  1.0137,  ..., -1.1094,  1.2441,  0.3052],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9170, -0.0297,  0.1040,  ..., -1.0205,  0.6650,  3.6934],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0087, -0.0090, -0.0017,  ...,  0.0035,  0.0086, -0.0140],
        [ 0.0014,  0.0019, -0.0057,  ...,  0.0079,  0.0118, -0.0039],
        [-0.0065, -0.0039,  0.0008,  ...,  0.0026,  0.0038, -0.0069],
        ...,
        [-0.0130, -0.0007,  0.0007,  ...,  0.0057,  0.0018,  0.0073],
        [-0.0002,  0.0010,  0.0007,  ..., -0.0016, -0.0022,  0.0090],
        [-0.0060,  0.0025,  0.0108,  ..., -0.0148, -0.0070,  0.0079]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.8403,  0.2156,  0.7173,  ..., -0.8892,  0.5557,  4.0820]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 02:05:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you preach something, you are a preacher
If you speak something, you are a speaker
If you send something, you are a sender
If you interpret something, you are a interpreter
If you promote something, you are a promoter
If you organise something, you are a organiser
If you subscribe something, you are a subscriber
If you organize something, you are a
2024-07-18 02:05:32 root INFO     [order_1_approx] starting weight calculation for If you preach something, you are a preacher
If you organize something, you are a organizer
If you speak something, you are a speaker
If you organise something, you are a organiser
If you subscribe something, you are a subscriber
If you interpret something, you are a interpreter
If you send something, you are a sender
If you promote something, you are a
2024-07-18 02:05:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 02:09:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5474,  0.4490,  0.2852,  ...,  0.5073,  0.0769,  0.3250],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3789, -0.8467, -4.5430,  ...,  0.3252,  0.0050,  0.8228],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0040, -0.0053,  0.0058,  ...,  0.0018,  0.0014, -0.0014],
        [-0.0072,  0.0054,  0.0030,  ...,  0.0031,  0.0019,  0.0049],
        [-0.0082,  0.0186,  0.0094,  ...,  0.0042,  0.0071,  0.0039],
        ...,
        [-0.0057, -0.0156, -0.0017,  ...,  0.0120, -0.0061,  0.0029],
        [ 0.0010, -0.0093,  0.0025,  ..., -0.0078, -0.0079,  0.0163],
        [-0.0005, -0.0037,  0.0098,  ..., -0.0014,  0.0033,  0.0132]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.2422, -0.8735, -4.2344,  ..., -0.2300, -0.2499,  1.0059]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 02:09:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you preach something, you are a preacher
If you organize something, you are a organizer
If you speak something, you are a speaker
If you organise something, you are a organiser
If you subscribe something, you are a subscriber
If you interpret something, you are a interpreter
If you send something, you are a sender
If you promote something, you are a
2024-07-18 02:09:05 root INFO     [order_1_approx] starting weight calculation for If you speak something, you are a speaker
If you organize something, you are a organizer
If you subscribe something, you are a subscriber
If you send something, you are a sender
If you interpret something, you are a interpreter
If you organise something, you are a organiser
If you promote something, you are a promoter
If you preach something, you are a
2024-07-18 02:09:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 02:12:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5107, -0.1039,  0.4785,  ...,  1.1953,  0.0042, -0.4243],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0312,  0.1431, -3.9121,  ...,  1.0107,  2.0469,  4.2812],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-4.5776e-05,  1.2522e-03,  3.6507e-03,  ...,  5.3883e-04,
         -6.1150e-03, -7.8583e-04],
        [ 3.6087e-03,  8.2932e-03,  4.5776e-05,  ...,  1.1871e-02,
         -8.7662e-03, -3.7842e-03],
        [ 9.7504e-03,  4.9553e-03, -6.5155e-03,  ...,  1.2360e-02,
          1.0765e-02,  9.0179e-03],
        ...,
        [-1.0956e-02, -6.4087e-03, -1.0139e-02,  ...,  5.9319e-04,
          2.3975e-03, -9.9106e-03],
        [ 7.2517e-03,  9.6130e-03,  1.1024e-02,  ..., -1.1421e-02,
         -9.6283e-03,  9.8267e-03],
        [-1.7029e-02,  1.5774e-03,  9.5940e-04,  ..., -7.9803e-03,
         -2.2659e-03, -1.2100e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1582,  0.0591, -4.7656,  ...,  1.0469,  2.1367,  4.3242]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 02:12:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you speak something, you are a speaker
If you organize something, you are a organizer
If you subscribe something, you are a subscriber
If you send something, you are a sender
If you interpret something, you are a interpreter
If you organise something, you are a organiser
If you promote something, you are a promoter
If you preach something, you are a
2024-07-18 02:12:40 root INFO     total operator prediction time: 1712.2165775299072 seconds
2024-07-18 02:12:40 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on over+adj_reg
2024-07-18 02:12:40 root INFO     building operator over+adj_reg
2024-07-18 02:12:41 root INFO     [order_1_approx] starting weight calculation for If something is too painted, it is overpainted
If something is too optimistic, it is overoptimistic
If something is too populated, it is overpopulated
If something is too simplified, it is oversimplified
If something is too exposed, it is overexposed
If something is too protective, it is overprotective
If something is too developed, it is overdeveloped
If something is too done, it is
2024-07-18 02:12:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 02:16:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4202,  0.5967,  2.0410,  ...,  0.5557,  1.6172, -0.8755],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0205e+00, -1.2812e+00,  1.9531e-03,  ..., -1.2051e+00,
         4.7930e+00,  3.6211e+00], device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0288, -0.0361,  0.0171,  ...,  0.0019, -0.0237, -0.0279],
        [-0.0037, -0.0004,  0.0003,  ...,  0.0032,  0.0042,  0.0094],
        [ 0.0049,  0.0162, -0.0336,  ..., -0.0222,  0.0134, -0.0013],
        ...,
        [ 0.0032, -0.0109, -0.0025,  ..., -0.0064,  0.0064, -0.0207],
        [-0.0230,  0.0160,  0.0052,  ..., -0.0074, -0.0225, -0.0041],
        [ 0.0083,  0.0084,  0.0082,  ...,  0.0045, -0.0029,  0.0069]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6123, -1.2236, -0.2205,  ..., -2.6836,  4.9883,  4.9648]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 02:16:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too painted, it is overpainted
If something is too optimistic, it is overoptimistic
If something is too populated, it is overpopulated
If something is too simplified, it is oversimplified
If something is too exposed, it is overexposed
If something is too protective, it is overprotective
If something is too developed, it is overdeveloped
If something is too done, it is
2024-07-18 02:16:15 root INFO     [order_1_approx] starting weight calculation for If something is too developed, it is overdeveloped
If something is too populated, it is overpopulated
If something is too optimistic, it is overoptimistic
If something is too painted, it is overpainted
If something is too simplified, it is oversimplified
If something is too done, it is overdone
If something is too protective, it is overprotective
If something is too exposed, it is
2024-07-18 02:16:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 02:19:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.6104, -0.1382,  1.3320,  ..., -0.5176,  1.9678,  0.4895],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3008,  0.0938, -0.4399,  ..., -1.9961, -0.9141,  0.3081],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0083, -0.0142, -0.0023,  ..., -0.0090, -0.0158, -0.0095],
        [-0.0020, -0.0001,  0.0058,  ...,  0.0036,  0.0031, -0.0061],
        [ 0.0032,  0.0146, -0.0083,  ..., -0.0003,  0.0134, -0.0034],
        ...,
        [-0.0015,  0.0048,  0.0018,  ...,  0.0011,  0.0085, -0.0128],
        [ 0.0012, -0.0070,  0.0038,  ...,  0.0044, -0.0134,  0.0063],
        [-0.0058,  0.0136,  0.0091,  ..., -0.0156,  0.0012, -0.0112]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1582,  0.2896, -0.7158,  ..., -2.0859, -0.9985,  0.2905]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 02:19:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too developed, it is overdeveloped
If something is too populated, it is overpopulated
If something is too optimistic, it is overoptimistic
If something is too painted, it is overpainted
If something is too simplified, it is oversimplified
If something is too done, it is overdone
If something is too protective, it is overprotective
If something is too exposed, it is
2024-07-18 02:19:48 root INFO     [order_1_approx] starting weight calculation for If something is too painted, it is overpainted
If something is too simplified, it is oversimplified
If something is too developed, it is overdeveloped
If something is too protective, it is overprotective
If something is too populated, it is overpopulated
If something is too done, it is overdone
If something is too exposed, it is overexposed
If something is too optimistic, it is
2024-07-18 02:19:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 02:23:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9302, -0.8936, -0.7842,  ..., -1.1426,  1.8232, -0.2803],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6758, -0.4624, -0.5205,  ...,  0.1504,  0.3013,  3.8711],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0040, -0.0210,  0.0109,  ..., -0.0046, -0.0153, -0.0169],
        [-0.0088,  0.0016,  0.0138,  ...,  0.0098, -0.0031,  0.0012],
        [-0.0042,  0.0030, -0.0057,  ...,  0.0071,  0.0130,  0.0168],
        ...,
        [ 0.0032, -0.0236, -0.0042,  ...,  0.0018, -0.0041, -0.0130],
        [-0.0041,  0.0006, -0.0093,  ..., -0.0125, -0.0123,  0.0074],
        [-0.0051,  0.0112,  0.0203,  ...,  0.0021, -0.0040, -0.0267]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6211, -0.1724, -0.2473,  ...,  0.2300, -0.1057,  3.9219]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 02:23:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too painted, it is overpainted
If something is too simplified, it is oversimplified
If something is too developed, it is overdeveloped
If something is too protective, it is overprotective
If something is too populated, it is overpopulated
If something is too done, it is overdone
If something is too exposed, it is overexposed
If something is too optimistic, it is
2024-07-18 02:23:23 root INFO     [order_1_approx] starting weight calculation for If something is too protective, it is overprotective
If something is too optimistic, it is overoptimistic
If something is too developed, it is overdeveloped
If something is too populated, it is overpopulated
If something is too simplified, it is oversimplified
If something is too done, it is overdone
If something is too exposed, it is overexposed
If something is too painted, it is
2024-07-18 02:23:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 02:26:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3757,  1.3662,  0.4338,  ...,  0.0211,  1.7471,  0.4290],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6406, -2.5645,  1.6016,  ..., -0.3872, -0.3108,  3.4414],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0166, -0.0163,  0.0060,  ..., -0.0058, -0.0090, -0.0177],
        [ 0.0005,  0.0119,  0.0077,  ...,  0.0003,  0.0076,  0.0043],
        [-0.0089,  0.0017, -0.0071,  ...,  0.0066, -0.0011,  0.0078],
        ...,
        [-0.0063, -0.0094, -0.0203,  ...,  0.0115, -0.0055,  0.0030],
        [-0.0024, -0.0022, -0.0013,  ..., -0.0025, -0.0158,  0.0022],
        [-0.0154,  0.0040,  0.0065,  ..., -0.0056, -0.0096, -0.0118]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4648, -3.0078,  1.4092,  ..., -0.3379,  0.2786,  4.7891]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 02:26:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too protective, it is overprotective
If something is too optimistic, it is overoptimistic
If something is too developed, it is overdeveloped
If something is too populated, it is overpopulated
If something is too simplified, it is oversimplified
If something is too done, it is overdone
If something is too exposed, it is overexposed
If something is too painted, it is
2024-07-18 02:26:57 root INFO     [order_1_approx] starting weight calculation for If something is too optimistic, it is overoptimistic
If something is too done, it is overdone
If something is too developed, it is overdeveloped
If something is too exposed, it is overexposed
If something is too simplified, it is oversimplified
If something is too painted, it is overpainted
If something is too protective, it is overprotective
If something is too populated, it is
2024-07-18 02:26:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 02:30:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5332,  0.4827,  1.0645,  ...,  0.2021,  2.8789, -0.6016],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.1719, -0.9336, -0.1816,  ..., -0.1270,  0.8359,  1.6660],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0094, -0.0147,  0.0005,  ..., -0.0050, -0.0129, -0.0184],
        [-0.0097,  0.0099, -0.0017,  ...,  0.0089,  0.0076, -0.0015],
        [ 0.0065,  0.0007, -0.0124,  ...,  0.0053, -0.0014,  0.0042],
        ...,
        [-0.0083, -0.0009, -0.0035,  ...,  0.0120, -0.0006,  0.0009],
        [-0.0020,  0.0017, -0.0056,  ..., -0.0069, -0.0161, -0.0042],
        [-0.0082, -0.0042,  0.0015,  ..., -0.0243,  0.0030, -0.0190]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.0234, -0.3018, -0.3057,  ..., -0.9663,  0.9902,  2.6406]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 02:30:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too optimistic, it is overoptimistic
If something is too done, it is overdone
If something is too developed, it is overdeveloped
If something is too exposed, it is overexposed
If something is too simplified, it is oversimplified
If something is too painted, it is overpainted
If something is too protective, it is overprotective
If something is too populated, it is
2024-07-18 02:30:31 root INFO     [order_1_approx] starting weight calculation for If something is too done, it is overdone
If something is too populated, it is overpopulated
If something is too optimistic, it is overoptimistic
If something is too exposed, it is overexposed
If something is too protective, it is overprotective
If something is too painted, it is overpainted
If something is too developed, it is overdeveloped
If something is too simplified, it is
2024-07-18 02:30:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 02:34:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1904,  0.3198,  0.3208,  ...,  0.2559,  1.7207,  0.3677],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3242, -0.2935,  0.8271,  ..., -0.8716,  0.5400,  1.7842],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0167, -0.0250,  0.0154,  ...,  0.0025, -0.0128, -0.0056],
        [-0.0020,  0.0159,  0.0114,  ..., -0.0055, -0.0161, -0.0034],
        [ 0.0044, -0.0003, -0.0104,  ..., -0.0085,  0.0000,  0.0037],
        ...,
        [-0.0023, -0.0003,  0.0003,  ...,  0.0089, -0.0066, -0.0111],
        [-0.0064,  0.0031,  0.0078,  ..., -0.0082, -0.0157,  0.0062],
        [-0.0036,  0.0083,  0.0052,  ..., -0.0177, -0.0050, -0.0114]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1937,  0.3213,  1.0703,  ..., -1.5127,  0.3574,  1.8721]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 02:34:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too done, it is overdone
If something is too populated, it is overpopulated
If something is too optimistic, it is overoptimistic
If something is too exposed, it is overexposed
If something is too protective, it is overprotective
If something is too painted, it is overpainted
If something is too developed, it is overdeveloped
If something is too simplified, it is
2024-07-18 02:34:02 root INFO     [order_1_approx] starting weight calculation for If something is too populated, it is overpopulated
If something is too exposed, it is overexposed
If something is too optimistic, it is overoptimistic
If something is too done, it is overdone
If something is too painted, it is overpainted
If something is too simplified, it is oversimplified
If something is too protective, it is overprotective
If something is too developed, it is
2024-07-18 02:34:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 02:37:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1680,  0.2056,  2.5898,  ...,  0.1375,  1.6826, -0.9287],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8457, -1.8438,  1.2617,  ..., -3.3633,  2.0977,  1.0264],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0024, -0.0294,  0.0026,  ..., -0.0091, -0.0100, -0.0133],
        [-0.0073,  0.0118,  0.0004,  ..., -0.0098,  0.0167,  0.0038],
        [-0.0007,  0.0003, -0.0249,  ..., -0.0163,  0.0115, -0.0023],
        ...,
        [-0.0076, -0.0041,  0.0110,  ...,  0.0109, -0.0071, -0.0141],
        [-0.0099, -0.0057,  0.0037,  ...,  0.0007, -0.0090,  0.0153],
        [-0.0213,  0.0083,  0.0144,  ..., -0.0033, -0.0056, -0.0033]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4102, -1.8643,  1.7158,  ..., -2.9355,  1.4639,  0.7930]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 02:37:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too populated, it is overpopulated
If something is too exposed, it is overexposed
If something is too optimistic, it is overoptimistic
If something is too done, it is overdone
If something is too painted, it is overpainted
If something is too simplified, it is oversimplified
If something is too protective, it is overprotective
If something is too developed, it is
2024-07-18 02:37:38 root INFO     [order_1_approx] starting weight calculation for If something is too exposed, it is overexposed
If something is too populated, it is overpopulated
If something is too done, it is overdone
If something is too developed, it is overdeveloped
If something is too simplified, it is oversimplified
If something is too optimistic, it is overoptimistic
If something is too painted, it is overpainted
If something is too protective, it is
2024-07-18 02:37:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 02:41:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([1.0674, 0.0962, 1.2178,  ..., 0.5640, 1.2051, 0.3857], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9316, -1.3135, -0.5205,  ...,  0.3086,  0.8057,  1.2539],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0080, -0.0156,  0.0074,  ..., -0.0111, -0.0084, -0.0179],
        [ 0.0013,  0.0068,  0.0070,  ...,  0.0040,  0.0027, -0.0068],
        [ 0.0034,  0.0017, -0.0007,  ...,  0.0026,  0.0032,  0.0068],
        ...,
        [-0.0058, -0.0072, -0.0152,  ...,  0.0078, -0.0080, -0.0046],
        [-0.0062,  0.0020,  0.0035,  ...,  0.0013, -0.0016,  0.0131],
        [-0.0034,  0.0099,  0.0130,  ..., -0.0091, -0.0002, -0.0025]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5391, -0.8994, -0.2175,  ..., -0.2734, -0.0557,  1.2393]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 02:41:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too exposed, it is overexposed
If something is too populated, it is overpopulated
If something is too done, it is overdone
If something is too developed, it is overdeveloped
If something is too simplified, it is oversimplified
If something is too optimistic, it is overoptimistic
If something is too painted, it is overpainted
If something is too protective, it is
2024-07-18 02:41:12 root INFO     total operator prediction time: 1711.5516095161438 seconds
2024-07-18 02:41:12 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on adj+ly_reg
2024-07-18 02:41:12 root INFO     building operator adj+ly_reg
2024-07-18 02:41:12 root INFO     [order_1_approx] starting weight calculation for The adjective form of strong is strongly
The adjective form of significant is significantly
The adjective form of nice is nicely
The adjective form of huge is hugely
The adjective form of global is globally
The adjective form of cultural is culturally
The adjective form of similar is similarly
The adjective form of digital is
2024-07-18 02:41:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 02:44:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6299, -0.4331,  0.0769,  ...,  1.0557, -0.4187, -0.1255],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2114,  2.1406,  0.3789,  ..., -1.5312,  1.0098,  2.1055],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0117, -0.0154, -0.0037,  ..., -0.0159,  0.0003, -0.0045],
        [-0.0124, -0.0061,  0.0090,  ...,  0.0071, -0.0107, -0.0014],
        [ 0.0012, -0.0017, -0.0090,  ..., -0.0057,  0.0147,  0.0017],
        ...,
        [-0.0106, -0.0006, -0.0039,  ...,  0.0088,  0.0011, -0.0015],
        [ 0.0005, -0.0090,  0.0052,  ..., -0.0095, -0.0053, -0.0066],
        [-0.0074,  0.0019, -0.0007,  ..., -0.0044,  0.0050, -0.0040]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3945,  2.0781,  0.4214,  ..., -1.9893,  1.5020,  2.2559]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 02:44:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of strong is strongly
The adjective form of significant is significantly
The adjective form of nice is nicely
The adjective form of huge is hugely
The adjective form of global is globally
The adjective form of cultural is culturally
The adjective form of similar is similarly
The adjective form of digital is
2024-07-18 02:44:45 root INFO     [order_1_approx] starting weight calculation for The adjective form of cultural is culturally
The adjective form of strong is strongly
The adjective form of nice is nicely
The adjective form of digital is digitally
The adjective form of significant is significantly
The adjective form of global is globally
The adjective form of huge is hugely
The adjective form of similar is
2024-07-18 02:44:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 02:48:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0638,  0.4414,  0.6865,  ..., -0.5171,  0.5630, -0.2137],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8398,  2.9180,  1.7930,  ...,  2.6094, -2.0352,  0.9316],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.5436e-02, -2.0966e-02,  6.7520e-03,  ..., -6.8550e-03,
          2.8496e-03, -1.5701e-02],
        [-6.1417e-03,  7.3853e-03,  3.4294e-03,  ...,  1.0460e-02,
         -1.2924e-02,  9.1324e-03],
        [ 4.9095e-03, -1.0300e-02, -2.0248e-02,  ...,  7.1526e-03,
          2.2583e-02,  4.7913e-03],
        ...,
        [-2.3987e-02, -1.3000e-02, -5.7220e-05,  ..., -1.8272e-03,
         -5.7297e-03, -2.2903e-02],
        [ 2.1057e-03, -1.7517e-02,  4.6349e-03,  ..., -2.2217e-02,
         -1.5747e-02, -1.7578e-02],
        [-1.2444e-02,  2.0203e-02, -2.9564e-05,  ..., -1.1612e-02,
         -2.6703e-05, -2.7832e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1250,  2.4766,  2.3438,  ...,  1.1846, -1.3799,  0.5142]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 02:48:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of cultural is culturally
The adjective form of strong is strongly
The adjective form of nice is nicely
The adjective form of digital is digitally
The adjective form of significant is significantly
The adjective form of global is globally
The adjective form of huge is hugely
The adjective form of similar is
2024-07-18 02:48:19 root INFO     [order_1_approx] starting weight calculation for The adjective form of huge is hugely
The adjective form of nice is nicely
The adjective form of similar is similarly
The adjective form of cultural is culturally
The adjective form of digital is digitally
The adjective form of strong is strongly
The adjective form of global is globally
The adjective form of significant is
2024-07-18 02:48:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 02:51:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1233,  1.1553,  0.9146,  ..., -0.1787,  0.5088,  0.0381],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.1055,  3.3203,  5.8281,  ...,  1.4062, -3.3320, -2.3145],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0246, -0.0181, -0.0004,  ...,  0.0032, -0.0067, -0.0141],
        [-0.0157,  0.0038, -0.0036,  ...,  0.0219, -0.0341,  0.0136],
        [ 0.0047, -0.0083,  0.0022,  ...,  0.0023,  0.0225,  0.0041],
        ...,
        [-0.0140, -0.0166, -0.0016,  ...,  0.0023,  0.0121, -0.0270],
        [-0.0163, -0.0030,  0.0235,  ..., -0.0201,  0.0030, -0.0083],
        [-0.0067,  0.0280,  0.0145,  ..., -0.0268,  0.0200, -0.0349]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.9922,  4.3477,  6.1953,  ...,  1.5938, -3.0977, -2.4570]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 02:51:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of huge is hugely
The adjective form of nice is nicely
The adjective form of similar is similarly
The adjective form of cultural is culturally
The adjective form of digital is digitally
The adjective form of strong is strongly
The adjective form of global is globally
The adjective form of significant is
2024-07-18 02:51:51 root INFO     [order_1_approx] starting weight calculation for The adjective form of strong is strongly
The adjective form of huge is hugely
The adjective form of significant is significantly
The adjective form of similar is similarly
The adjective form of digital is digitally
The adjective form of cultural is culturally
The adjective form of nice is nicely
The adjective form of global is
2024-07-18 02:51:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 02:55:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.6875, -0.0469,  0.4421,  ..., -0.6494,  0.2832,  0.1631],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.3965,  2.6621, -0.1611,  ...,  1.8340, -0.6182, -1.1641],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0060, -0.0052,  0.0028,  ..., -0.0077,  0.0048, -0.0159],
        [-0.0052,  0.0054,  0.0101,  ...,  0.0166, -0.0174,  0.0067],
        [ 0.0109, -0.0061, -0.0079,  ..., -0.0067,  0.0061,  0.0088],
        ...,
        [-0.0188, -0.0138,  0.0018,  ...,  0.0118,  0.0077, -0.0060],
        [-0.0041, -0.0101, -0.0053,  ..., -0.0216, -0.0171,  0.0023],
        [-0.0075,  0.0135, -0.0063,  ..., -0.0057,  0.0116, -0.0226]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4058,  2.3379, -0.2490,  ...,  1.4277,  0.2568, -0.2573]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 02:55:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of strong is strongly
The adjective form of huge is hugely
The adjective form of significant is significantly
The adjective form of similar is similarly
The adjective form of digital is digitally
The adjective form of cultural is culturally
The adjective form of nice is nicely
The adjective form of global is
2024-07-18 02:55:25 root INFO     [order_1_approx] starting weight calculation for The adjective form of global is globally
The adjective form of similar is similarly
The adjective form of huge is hugely
The adjective form of nice is nicely
The adjective form of digital is digitally
The adjective form of significant is significantly
The adjective form of strong is strongly
The adjective form of cultural is
2024-07-18 02:55:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 02:58:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5264,  0.7480, -0.1919,  ...,  1.1816,  0.7051,  0.3765],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.4824,  1.0312, -1.0703,  ..., -0.6689, -0.5264, -0.6787],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0027, -0.0189,  0.0048,  ..., -0.0146,  0.0091, -0.0328],
        [-0.0179,  0.0029,  0.0187,  ...,  0.0196, -0.0289,  0.0024],
        [ 0.0148, -0.0068,  0.0023,  ..., -0.0019,  0.0175,  0.0025],
        ...,
        [-0.0045, -0.0234, -0.0134,  ...,  0.0023,  0.0183, -0.0248],
        [-0.0055, -0.0085,  0.0096,  ..., -0.0115, -0.0051,  0.0006],
        [-0.0099, -0.0010,  0.0044,  ..., -0.0061,  0.0020, -0.0248]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.8867,  0.0977, -1.3633,  ..., -1.1250, -0.9849,  0.3340]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 02:58:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of global is globally
The adjective form of similar is similarly
The adjective form of huge is hugely
The adjective form of nice is nicely
The adjective form of digital is digitally
The adjective form of significant is significantly
The adjective form of strong is strongly
The adjective form of cultural is
2024-07-18 02:58:58 root INFO     [order_1_approx] starting weight calculation for The adjective form of cultural is culturally
The adjective form of significant is significantly
The adjective form of global is globally
The adjective form of similar is similarly
The adjective form of huge is hugely
The adjective form of digital is digitally
The adjective form of strong is strongly
The adjective form of nice is
2024-07-18 02:58:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 03:02:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2349, -0.7866, -0.7773,  ..., -1.0098, -0.2578,  0.8877],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3169,  0.4001, -0.6426,  ...,  3.1543,  1.2402,  0.4941],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0024, -0.0192,  0.0154,  ...,  0.0041, -0.0031, -0.0041],
        [-0.0039, -0.0147,  0.0002,  ...,  0.0185, -0.0228,  0.0053],
        [ 0.0196, -0.0113,  0.0107,  ...,  0.0029,  0.0314,  0.0009],
        ...,
        [-0.0041, -0.0054,  0.0023,  ...,  0.0060, -0.0062,  0.0006],
        [-0.0110, -0.0083,  0.0034,  ..., -0.0151, -0.0101, -0.0144],
        [-0.0029,  0.0120,  0.0064,  ..., -0.0133, -0.0119, -0.0191]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[0.4526, 0.3933, 0.3701,  ..., 2.5586, 1.5215, 0.6436]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 03:02:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of cultural is culturally
The adjective form of significant is significantly
The adjective form of global is globally
The adjective form of similar is similarly
The adjective form of huge is hugely
The adjective form of digital is digitally
The adjective form of strong is strongly
The adjective form of nice is
2024-07-18 03:02:32 root INFO     [order_1_approx] starting weight calculation for The adjective form of global is globally
The adjective form of nice is nicely
The adjective form of significant is significantly
The adjective form of similar is similarly
The adjective form of cultural is culturally
The adjective form of digital is digitally
The adjective form of strong is strongly
The adjective form of huge is
2024-07-18 03:02:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 03:06:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4404,  0.1609,  0.0415,  ..., -0.3828,  0.6143,  1.4639],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7070, -0.2412,  0.3213,  ..., -0.8979,  3.1816,  3.5547],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0247, -0.0057,  0.0120,  ..., -0.0012,  0.0022, -0.0091],
        [-0.0072, -0.0136,  0.0057,  ...,  0.0208, -0.0106,  0.0016],
        [ 0.0077,  0.0261, -0.0296,  ...,  0.0064,  0.0232,  0.0200],
        ...,
        [ 0.0068, -0.0202,  0.0147,  ..., -0.0085, -0.0127, -0.0069],
        [-0.0327, -0.0197,  0.0089,  ..., -0.0286, -0.0355, -0.0133],
        [-0.0202,  0.0174,  0.0071,  ..., -0.0145, -0.0139, -0.0374]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0234, -0.5903,  0.2036,  ..., -1.0586,  4.4531,  4.3047]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 03:06:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of global is globally
The adjective form of nice is nicely
The adjective form of significant is significantly
The adjective form of similar is similarly
The adjective form of cultural is culturally
The adjective form of digital is digitally
The adjective form of strong is strongly
The adjective form of huge is
2024-07-18 03:06:05 root INFO     [order_1_approx] starting weight calculation for The adjective form of cultural is culturally
The adjective form of global is globally
The adjective form of significant is significantly
The adjective form of digital is digitally
The adjective form of huge is hugely
The adjective form of nice is nicely
The adjective form of similar is similarly
The adjective form of strong is
2024-07-18 03:06:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 03:09:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7402, -0.5859,  0.7617,  ...,  0.1382,  0.1045, -0.5967],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3203, -1.2334,  0.9463,  ...,  1.1719, -1.3320, -0.7559],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0090, -0.0064,  0.0047,  ..., -0.0151, -0.0013, -0.0092],
        [-0.0041,  0.0027,  0.0005,  ...,  0.0173, -0.0167,  0.0047],
        [ 0.0022, -0.0005, -0.0029,  ...,  0.0045,  0.0137,  0.0001],
        ...,
        [-0.0118, -0.0128,  0.0007,  ..., -0.0006, -0.0126, -0.0094],
        [-0.0150, -0.0162,  0.0032,  ..., -0.0164,  0.0075,  0.0038],
        [-0.0106,  0.0092,  0.0112,  ..., -0.0148,  0.0187, -0.0331]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3486, -1.7021,  1.2480,  ...,  0.9253, -0.9302, -0.1904]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 03:09:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of cultural is culturally
The adjective form of global is globally
The adjective form of significant is significantly
The adjective form of digital is digitally
The adjective form of huge is hugely
The adjective form of nice is nicely
The adjective form of similar is similarly
The adjective form of strong is
2024-07-18 03:09:38 root INFO     total operator prediction time: 1705.9692468643188 seconds
2024-07-18 03:09:38 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+tion_irreg
2024-07-18 03:09:38 root INFO     building operator verb+tion_irreg
2024-07-18 03:09:38 root INFO     [order_1_approx] starting weight calculation for To explore results in exploration
To utilize results in utilization
To privatize results in privatization
To imagine results in imagination
To compile results in compilation
To consult results in consulation
To characterize results in characterization
To expire results in
2024-07-18 03:09:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 03:13:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.8438, -0.9053,  0.0610,  ...,  0.5537,  1.2266,  0.9497],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7520,  1.0547, -1.0449,  ..., -1.0859, -0.4011,  0.2070],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.4567e-03, -4.5776e-05,  6.1836e-03,  ..., -6.4621e-03,
         -1.6586e-02, -1.5488e-02],
        [-9.9335e-03, -5.5389e-03,  5.0430e-03,  ...,  5.9586e-03,
         -1.8250e-02, -8.2092e-03],
        [-2.3918e-03, -7.1487e-03,  1.4420e-03,  ..., -3.4637e-03,
          9.3994e-03, -6.6910e-03],
        ...,
        [ 1.0147e-03, -1.0368e-02, -4.0131e-03,  ...,  2.1458e-03,
         -7.9651e-03, -7.4883e-03],
        [-1.5116e-03,  1.7090e-03,  1.1871e-02,  ...,  7.2784e-03,
          5.4026e-04,  5.9509e-04],
        [-7.2212e-03,  1.8082e-02,  1.0284e-02,  ..., -5.1308e-03,
         -2.8419e-03,  2.7847e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5859,  0.8506, -1.4297,  ..., -1.5986, -0.3901,  0.1942]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 03:13:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To explore results in exploration
To utilize results in utilization
To privatize results in privatization
To imagine results in imagination
To compile results in compilation
To consult results in consulation
To characterize results in characterization
To expire results in
2024-07-18 03:13:12 root INFO     [order_1_approx] starting weight calculation for To expire results in expiration
To imagine results in imagination
To characterize results in characterization
To compile results in compilation
To privatize results in privatization
To utilize results in utilization
To explore results in exploration
To consult results in
2024-07-18 03:13:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 03:16:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0117,  0.1101, -0.0327,  ...,  0.8770,  0.5718, -0.2358],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2113,  2.0352, -5.9883,  ..., -1.5215,  0.0479,  3.5391],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0055, -0.0116,  0.0111,  ..., -0.0229,  0.0034, -0.0065],
        [-0.0142,  0.0045,  0.0114,  ...,  0.0005, -0.0095,  0.0086],
        [-0.0021, -0.0171,  0.0109,  ..., -0.0005,  0.0157, -0.0083],
        ...,
        [-0.0176, -0.0258, -0.0027,  ...,  0.0134, -0.0108, -0.0126],
        [-0.0019, -0.0002,  0.0094,  ..., -0.0102, -0.0065,  0.0125],
        [-0.0256,  0.0107,  0.0115,  ...,  0.0047, -0.0066,  0.0090]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6167,  2.1094, -6.7930,  ..., -2.6816,  0.0515,  3.4531]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 03:16:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To expire results in expiration
To imagine results in imagination
To characterize results in characterization
To compile results in compilation
To privatize results in privatization
To utilize results in utilization
To explore results in exploration
To consult results in
2024-07-18 03:16:44 root INFO     [order_1_approx] starting weight calculation for To characterize results in characterization
To utilize results in utilization
To consult results in consulation
To privatize results in privatization
To imagine results in imagination
To compile results in compilation
To expire results in expiration
To explore results in
2024-07-18 03:16:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 03:20:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([1.3301, 0.0333, 0.8389,  ..., 0.1143, 0.4885, 0.0806], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5552,  3.5352,  1.0498,  ..., -2.7266, -2.8945,  2.7559],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0068, -0.0150, -0.0118,  ..., -0.0003,  0.0004, -0.0064],
        [-0.0152, -0.0056,  0.0036,  ...,  0.0083, -0.0196,  0.0090],
        [ 0.0082,  0.0049, -0.0119,  ..., -0.0064,  0.0082, -0.0151],
        ...,
        [-0.0181, -0.0180, -0.0158,  ..., -0.0023, -0.0151, -0.0219],
        [ 0.0230,  0.0089,  0.0088,  ...,  0.0055, -0.0119,  0.0145],
        [ 0.0072,  0.0163,  0.0077,  ...,  0.0008,  0.0050, -0.0083]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0767,  3.8145,  0.1729,  ..., -3.0391, -2.5840,  2.2871]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 03:20:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To characterize results in characterization
To utilize results in utilization
To consult results in consulation
To privatize results in privatization
To imagine results in imagination
To compile results in compilation
To expire results in expiration
To explore results in
2024-07-18 03:20:16 root INFO     [order_1_approx] starting weight calculation for To imagine results in imagination
To utilize results in utilization
To explore results in exploration
To privatize results in privatization
To compile results in compilation
To consult results in consulation
To expire results in expiration
To characterize results in
2024-07-18 03:20:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 03:23:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.7139, -0.1766,  0.5869,  ..., -0.4141, -0.7505,  0.2222],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7979, -0.6357, -3.0664,  ..., -0.9385, -1.6826,  3.1348],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.3680e-02, -6.0501e-03, -4.5204e-03,  ..., -3.9330e-03,
         -8.6670e-03, -1.4130e-02],
        [-9.3994e-03,  2.4986e-03,  6.6605e-03,  ..., -1.7242e-02,
         -4.9133e-03,  9.3994e-03],
        [-5.3940e-03, -4.5509e-03, -5.8136e-03,  ..., -1.0956e-02,
          1.7670e-02,  1.4648e-03],
        ...,
        [-1.5327e-02, -1.2268e-02, -9.4757e-03,  ...,  2.1362e-03,
         -1.5358e-02, -1.4511e-02],
        [ 8.9417e-03,  5.3024e-03,  1.9131e-03,  ..., -1.1017e-02,
          6.3553e-03,  5.4817e-03],
        [-6.1302e-03,  1.3618e-02,  8.9798e-03,  ..., -5.1498e-05,
          1.4542e-02,  1.6388e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2529, -1.0029, -3.8770,  ..., -0.9272, -2.2012,  2.7324]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 03:23:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To imagine results in imagination
To utilize results in utilization
To explore results in exploration
To privatize results in privatization
To compile results in compilation
To consult results in consulation
To expire results in expiration
To characterize results in
2024-07-18 03:23:49 root INFO     [order_1_approx] starting weight calculation for To compile results in compilation
To utilize results in utilization
To explore results in exploration
To expire results in expiration
To consult results in consulation
To characterize results in characterization
To privatize results in privatization
To imagine results in
2024-07-18 03:23:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 03:27:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2231,  0.3411, -0.2218,  ...,  0.6162,  0.9800, -0.0078],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0684,  1.7236, -3.6953,  ...,  1.6504, -3.4219, -0.4473],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0140, -0.0083, -0.0092,  ..., -0.0052,  0.0095,  0.0068],
        [-0.0015,  0.0125,  0.0059,  ..., -0.0012, -0.0158,  0.0043],
        [-0.0074,  0.0087, -0.0077,  ...,  0.0040,  0.0191,  0.0053],
        ...,
        [-0.0065,  0.0020,  0.0014,  ...,  0.0057, -0.0081, -0.0117],
        [ 0.0091,  0.0137, -0.0123,  ..., -0.0094, -0.0004, -0.0058],
        [-0.0076,  0.0111, -0.0024,  ...,  0.0008,  0.0077,  0.0128]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0103,  1.3457, -2.7422,  ...,  1.2148, -3.4316, -0.8438]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 03:27:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To compile results in compilation
To utilize results in utilization
To explore results in exploration
To expire results in expiration
To consult results in consulation
To characterize results in characterization
To privatize results in privatization
To imagine results in
2024-07-18 03:27:22 root INFO     [order_1_approx] starting weight calculation for To characterize results in characterization
To utilize results in utilization
To consult results in consulation
To compile results in compilation
To expire results in expiration
To imagine results in imagination
To explore results in exploration
To privatize results in
2024-07-18 03:27:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 03:30:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1567, -0.7256, -0.0938,  ...,  0.7998,  0.3152,  0.4016],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2344,  1.0869, -0.4736,  ...,  1.6543, -3.3066, -0.6626],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0041, -0.0129,  0.0051,  ...,  0.0044,  0.0056, -0.0097],
        [ 0.0055,  0.0063,  0.0070,  ...,  0.0039, -0.0059,  0.0017],
        [ 0.0042,  0.0075, -0.0010,  ..., -0.0087, -0.0008, -0.0028],
        ...,
        [-0.0017, -0.0135, -0.0072,  ...,  0.0175, -0.0120,  0.0053],
        [-0.0006,  0.0053,  0.0105,  ..., -0.0095,  0.0007, -0.0048],
        [-0.0041,  0.0081,  0.0065,  ..., -0.0070,  0.0120, -0.0042]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0901,  0.9692, -0.9688,  ...,  1.2793, -3.6484, -0.7095]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 03:30:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To characterize results in characterization
To utilize results in utilization
To consult results in consulation
To compile results in compilation
To expire results in expiration
To imagine results in imagination
To explore results in exploration
To privatize results in
2024-07-18 03:30:56 root INFO     [order_1_approx] starting weight calculation for To explore results in exploration
To privatize results in privatization
To consult results in consulation
To characterize results in characterization
To expire results in expiration
To imagine results in imagination
To utilize results in utilization
To compile results in
2024-07-18 03:30:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 03:34:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4229,  0.2144, -0.8667,  ...,  0.4758, -0.2505,  0.3984],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1082, -1.0938, -4.4062,  ...,  0.7915,  0.4509,  3.6641],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0052, -0.0120,  0.0024,  ..., -0.0014,  0.0095, -0.0115],
        [-0.0062,  0.0167, -0.0025,  ...,  0.0062, -0.0110,  0.0041],
        [ 0.0083, -0.0140, -0.0012,  ..., -0.0134, -0.0059, -0.0079],
        ...,
        [-0.0147, -0.0173, -0.0069,  ...,  0.0133, -0.0004, -0.0128],
        [ 0.0069,  0.0002,  0.0069,  ..., -0.0135,  0.0042,  0.0059],
        [-0.0002,  0.0058,  0.0108,  ..., -0.0061, -0.0053,  0.0114]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0150, -1.1025, -5.2031,  ...,  0.9028,  0.2983,  3.6816]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 03:34:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To explore results in exploration
To privatize results in privatization
To consult results in consulation
To characterize results in characterization
To expire results in expiration
To imagine results in imagination
To utilize results in utilization
To compile results in
2024-07-18 03:34:30 root INFO     [order_1_approx] starting weight calculation for To explore results in exploration
To compile results in compilation
To consult results in consulation
To characterize results in characterization
To privatize results in privatization
To imagine results in imagination
To expire results in expiration
To utilize results in
2024-07-18 03:34:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 03:38:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9302,  0.7607,  0.9023,  ...,  1.0439, -0.1287,  0.0948],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6670,  4.4922,  1.0352,  ..., -0.9482,  0.3149,  5.8906],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 6.7215e-03, -7.8125e-03, -2.9373e-03,  ..., -1.2283e-02,
         -8.4457e-03, -7.0000e-03],
        [ 4.2114e-03, -2.8744e-03,  1.9646e-03,  ...,  4.7150e-03,
         -3.0518e-03,  1.2039e-02],
        [ 4.3030e-03, -1.1806e-03,  7.2021e-03,  ...,  1.7776e-03,
          5.1498e-05, -2.2583e-03],
        ...,
        [-5.0583e-03, -1.7212e-02, -3.2425e-03,  ...,  1.1139e-03,
         -5.6343e-03, -1.0460e-02],
        [-3.7651e-03,  8.7738e-04,  2.1572e-03,  ..., -1.3153e-02,
         -1.4095e-03,  1.4847e-02],
        [-5.6839e-03,  1.1063e-02,  1.1887e-02,  ..., -1.5839e-02,
         -7.5073e-03,  1.0185e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1826,  4.2422,  0.9292,  ..., -1.1318,  0.6816,  5.8906]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 03:38:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To explore results in exploration
To compile results in compilation
To consult results in consulation
To characterize results in characterization
To privatize results in privatization
To imagine results in imagination
To expire results in expiration
To utilize results in
2024-07-18 03:38:02 root INFO     total operator prediction time: 1703.754075050354 seconds
2024-07-18 03:38:02 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+able_reg
2024-07-18 03:38:02 root INFO     building operator verb+able_reg
2024-07-18 03:38:02 root INFO     [order_1_approx] starting weight calculation for If you can identify something, that thing is identifiable
If you can protect something, that thing is protectable
If you can adore something, that thing is adorable
If you can consider something, that thing is considerable
If you can extend something, that thing is extendable
If you can enjoy something, that thing is enjoyable
If you can avoid something, that thing is avoidable
If you can observe something, that thing is
2024-07-18 03:38:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 03:41:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5068,  0.8838,  0.8735,  ..., -1.4199,  0.2395,  1.7285],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.8301,  3.6426, -5.5859,  ..., -1.2559, -1.4082, -1.3174],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.1948e-03, -6.9618e-03, -6.6109e-03,  ..., -1.0262e-02,
         -4.1656e-03, -3.9673e-04],
        [ 1.1101e-03,  3.0346e-03,  8.1024e-03,  ...,  1.0139e-02,
          1.6518e-03,  6.7673e-03],
        [-5.4665e-03,  2.8305e-03, -9.3002e-03,  ...,  4.2725e-03,
         -9.6703e-04, -1.1482e-03],
        ...,
        [-9.0332e-03, -1.6184e-03, -4.8637e-03,  ...,  5.7487e-03,
         -1.0300e-03, -8.0414e-03],
        [-5.1880e-03, -3.2539e-03, -6.5079e-03,  ..., -1.6052e-02,
         -2.1027e-02,  2.7351e-03],
        [-5.7831e-03,  1.0040e-02, -2.9049e-03,  ..., -7.0305e-03,
          3.0518e-05, -4.8065e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.6875,  3.8320, -5.2148,  ..., -1.1221, -1.6074, -1.5205]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 03:41:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can identify something, that thing is identifiable
If you can protect something, that thing is protectable
If you can adore something, that thing is adorable
If you can consider something, that thing is considerable
If you can extend something, that thing is extendable
If you can enjoy something, that thing is enjoyable
If you can avoid something, that thing is avoidable
If you can observe something, that thing is
2024-07-18 03:41:36 root INFO     [order_1_approx] starting weight calculation for If you can avoid something, that thing is avoidable
If you can identify something, that thing is identifiable
If you can extend something, that thing is extendable
If you can enjoy something, that thing is enjoyable
If you can observe something, that thing is observable
If you can protect something, that thing is protectable
If you can consider something, that thing is considerable
If you can adore something, that thing is
2024-07-18 03:41:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 03:45:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5166,  0.4185,  1.7129,  ..., -0.6807,  1.6514, -0.1086],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.9062,  2.4980, -2.6133,  ..., -6.3047,  1.9365,  3.7227],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-3.5324e-03,  1.3990e-03, -7.7629e-04,  ...,  6.9275e-03,
          8.6403e-04, -9.0103e-03],
        [-9.7351e-03, -5.3864e-03,  7.5302e-03,  ...,  4.8904e-03,
         -1.4248e-03, -1.5764e-03],
        [ 1.8120e-03,  8.5602e-03, -1.2001e-02,  ...,  1.0910e-02,
          7.8354e-03, -7.7286e-03],
        ...,
        [-5.9700e-03, -1.4057e-03, -9.1705e-03,  ..., -7.6675e-03,
          1.0315e-02, -6.0158e-03],
        [-3.2158e-03,  4.1809e-03, -6.2256e-03,  ..., -1.2131e-02,
         -2.0386e-02,  1.5915e-02],
        [-1.1223e-02,  8.4305e-03, -4.2801e-03,  ..., -5.3406e-05,
          7.4959e-03, -1.4893e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.8477,  2.3633, -2.3574,  ..., -6.1484,  0.9448,  2.8340]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 03:45:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can avoid something, that thing is avoidable
If you can identify something, that thing is identifiable
If you can extend something, that thing is extendable
If you can enjoy something, that thing is enjoyable
If you can observe something, that thing is observable
If you can protect something, that thing is protectable
If you can consider something, that thing is considerable
If you can adore something, that thing is
2024-07-18 03:45:10 root INFO     [order_1_approx] starting weight calculation for If you can extend something, that thing is extendable
If you can observe something, that thing is observable
If you can identify something, that thing is identifiable
If you can consider something, that thing is considerable
If you can enjoy something, that thing is enjoyable
If you can protect something, that thing is protectable
If you can adore something, that thing is adorable
If you can avoid something, that thing is
2024-07-18 03:45:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 03:48:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4014,  0.1987, -0.9629,  ..., -0.1171, -0.1145, -0.4294],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.3672,  2.9844, -4.1406,  ...,  1.6172,  5.6875, -0.5337],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0006, -0.0091,  0.0123,  ...,  0.0075, -0.0077, -0.0005],
        [-0.0013,  0.0065,  0.0062,  ..., -0.0072, -0.0039,  0.0047],
        [ 0.0013, -0.0099, -0.0007,  ..., -0.0023, -0.0019, -0.0005],
        ...,
        [-0.0084, -0.0002, -0.0041,  ...,  0.0040, -0.0013, -0.0189],
        [-0.0002,  0.0050, -0.0122,  ..., -0.0142, -0.0103, -0.0066],
        [-0.0077,  0.0050, -0.0012,  ...,  0.0031,  0.0029, -0.0139]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7461,  2.8711, -4.1797,  ...,  1.2773,  5.2148, -0.6680]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 03:48:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can extend something, that thing is extendable
If you can observe something, that thing is observable
If you can identify something, that thing is identifiable
If you can consider something, that thing is considerable
If you can enjoy something, that thing is enjoyable
If you can protect something, that thing is protectable
If you can adore something, that thing is adorable
If you can avoid something, that thing is
2024-07-18 03:48:44 root INFO     [order_1_approx] starting weight calculation for If you can enjoy something, that thing is enjoyable
If you can adore something, that thing is adorable
If you can extend something, that thing is extendable
If you can identify something, that thing is identifiable
If you can observe something, that thing is observable
If you can protect something, that thing is protectable
If you can avoid something, that thing is avoidable
If you can consider something, that thing is
2024-07-18 03:48:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 03:52:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3152,  0.8457, -0.0366,  ...,  0.1389,  1.2646,  0.5439],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6714,  3.7812, -4.4062,  ..., -0.3936,  2.1816, -3.2676],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0072,  0.0003, -0.0010,  ...,  0.0019,  0.0002,  0.0026],
        [-0.0099,  0.0079, -0.0028,  ...,  0.0096, -0.0051,  0.0014],
        [-0.0106,  0.0040, -0.0161,  ..., -0.0054,  0.0043, -0.0047],
        ...,
        [-0.0051, -0.0203, -0.0034,  ...,  0.0016,  0.0061, -0.0066],
        [ 0.0009, -0.0124, -0.0038,  ..., -0.0081, -0.0087,  0.0081],
        [ 0.0003,  0.0180, -0.0028,  ..., -0.0015,  0.0066, -0.0119]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5845,  2.8828, -5.0938,  ...,  0.4526,  2.9219, -3.2773]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 03:52:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can enjoy something, that thing is enjoyable
If you can adore something, that thing is adorable
If you can extend something, that thing is extendable
If you can identify something, that thing is identifiable
If you can observe something, that thing is observable
If you can protect something, that thing is protectable
If you can avoid something, that thing is avoidable
If you can consider something, that thing is
2024-07-18 03:52:18 root INFO     [order_1_approx] starting weight calculation for If you can adore something, that thing is adorable
If you can enjoy something, that thing is enjoyable
If you can avoid something, that thing is avoidable
If you can extend something, that thing is extendable
If you can observe something, that thing is observable
If you can consider something, that thing is considerable
If you can identify something, that thing is identifiable
If you can protect something, that thing is
2024-07-18 03:52:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 03:55:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.5547, 1.0605, 0.5005,  ..., 1.3691, 1.3535, 0.6494], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.2168,  2.5742, -3.1680,  ..., -1.8145,  1.1973,  1.9600],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0073, -0.0121, -0.0022,  ...,  0.0040, -0.0001, -0.0007],
        [ 0.0009, -0.0041,  0.0007,  ...,  0.0049, -0.0017,  0.0004],
        [ 0.0025,  0.0050, -0.0024,  ..., -0.0055,  0.0086, -0.0101],
        ...,
        [-0.0161, -0.0003, -0.0149,  ..., -0.0005, -0.0028, -0.0111],
        [-0.0113, -0.0074, -0.0090,  ..., -0.0125, -0.0149,  0.0070],
        [ 0.0024,  0.0090, -0.0086,  ..., -0.0034, -0.0051, -0.0101]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.9873,  2.6777, -3.2012,  ..., -1.8418,  0.2905,  1.8311]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 03:55:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can adore something, that thing is adorable
If you can enjoy something, that thing is enjoyable
If you can avoid something, that thing is avoidable
If you can extend something, that thing is extendable
If you can observe something, that thing is observable
If you can consider something, that thing is considerable
If you can identify something, that thing is identifiable
If you can protect something, that thing is
2024-07-18 03:55:53 root INFO     [order_1_approx] starting weight calculation for If you can enjoy something, that thing is enjoyable
If you can adore something, that thing is adorable
If you can observe something, that thing is observable
If you can protect something, that thing is protectable
If you can avoid something, that thing is avoidable
If you can identify something, that thing is identifiable
If you can consider something, that thing is considerable
If you can extend something, that thing is
2024-07-18 03:55:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 03:59:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1897,  0.3345,  0.4121,  ...,  0.1663,  0.6055,  0.5288],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-5.2422, -0.7876, -4.3594,  ..., -0.2290, -3.1855, -1.1914],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0070, -0.0098,  0.0031,  ..., -0.0033, -0.0043,  0.0047],
        [ 0.0070, -0.0037,  0.0033,  ...,  0.0027,  0.0056,  0.0054],
        [ 0.0074,  0.0099, -0.0048,  ..., -0.0017,  0.0007, -0.0070],
        ...,
        [ 0.0002,  0.0114, -0.0138,  ..., -0.0017,  0.0051, -0.0077],
        [-0.0131,  0.0077,  0.0012,  ..., -0.0008,  0.0007,  0.0051],
        [ 0.0010,  0.0039,  0.0013,  ..., -0.0060, -0.0016,  0.0004]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.0508, -1.0430, -4.3945,  ..., -0.3911, -3.3125, -1.0166]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 03:59:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can enjoy something, that thing is enjoyable
If you can adore something, that thing is adorable
If you can observe something, that thing is observable
If you can protect something, that thing is protectable
If you can avoid something, that thing is avoidable
If you can identify something, that thing is identifiable
If you can consider something, that thing is considerable
If you can extend something, that thing is
2024-07-18 03:59:26 root INFO     [order_1_approx] starting weight calculation for If you can observe something, that thing is observable
If you can enjoy something, that thing is enjoyable
If you can adore something, that thing is adorable
If you can extend something, that thing is extendable
If you can consider something, that thing is considerable
If you can protect something, that thing is protectable
If you can avoid something, that thing is avoidable
If you can identify something, that thing is
2024-07-18 03:59:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 04:02:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1970,  1.5029, -0.9048,  ..., -0.1448,  0.7891,  0.0530],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5078,  1.6826, -5.6562,  ..., -0.4624,  2.0234, -1.1260],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0021, -0.0211, -0.0099,  ..., -0.0026, -0.0107, -0.0052],
        [-0.0013,  0.0101,  0.0117,  ...,  0.0081,  0.0076,  0.0137],
        [-0.0015, -0.0065, -0.0042,  ..., -0.0049, -0.0072, -0.0065],
        ...,
        [-0.0068, -0.0207, -0.0113,  ...,  0.0068, -0.0007, -0.0100],
        [-0.0076, -0.0101, -0.0080,  ..., -0.0112, -0.0137,  0.0011],
        [ 0.0047,  0.0063,  0.0020,  ..., -0.0093,  0.0089, -0.0128]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2959,  2.0234, -5.8750,  ..., -0.6567,  1.8350, -1.3809]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 04:03:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can observe something, that thing is observable
If you can enjoy something, that thing is enjoyable
If you can adore something, that thing is adorable
If you can extend something, that thing is extendable
If you can consider something, that thing is considerable
If you can protect something, that thing is protectable
If you can avoid something, that thing is avoidable
If you can identify something, that thing is
2024-07-18 04:03:00 root INFO     [order_1_approx] starting weight calculation for If you can protect something, that thing is protectable
If you can identify something, that thing is identifiable
If you can adore something, that thing is adorable
If you can consider something, that thing is considerable
If you can observe something, that thing is observable
If you can avoid something, that thing is avoidable
If you can extend something, that thing is extendable
If you can enjoy something, that thing is
2024-07-18 04:03:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 04:06:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6821,  1.5596,  1.1357,  ..., -0.2981,  1.2939,  0.0371],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.3672,  0.1738,  0.9092,  ..., -3.1016,  3.6191,  3.2500],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0015, -0.0040,  0.0010,  ..., -0.0008,  0.0019,  0.0005],
        [-0.0051, -0.0097, -0.0051,  ...,  0.0080, -0.0010,  0.0061],
        [ 0.0052, -0.0015, -0.0118,  ..., -0.0062,  0.0053, -0.0189],
        ...,
        [-0.0022,  0.0031, -0.0108,  ..., -0.0081,  0.0107, -0.0064],
        [ 0.0002,  0.0004, -0.0123,  ..., -0.0117, -0.0193,  0.0151],
        [ 0.0010,  0.0073, -0.0015,  ..., -0.0073,  0.0040, -0.0092]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.4219,  0.1208,  1.1680,  ..., -3.5352,  3.1934,  3.0488]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 04:06:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can protect something, that thing is protectable
If you can identify something, that thing is identifiable
If you can adore something, that thing is adorable
If you can consider something, that thing is considerable
If you can observe something, that thing is observable
If you can avoid something, that thing is avoidable
If you can extend something, that thing is extendable
If you can enjoy something, that thing is
2024-07-18 04:06:34 root INFO     total operator prediction time: 1711.9508764743805 seconds
2024-07-18 04:06:34 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on un+adj_reg
2024-07-18 04:06:34 root INFO     building operator un+adj_reg
2024-07-18 04:06:34 root INFO     [order_1_approx] starting weight calculation for The opposite of healthy is unhealthy
The opposite of finished is unfinished
The opposite of veiled is unveiled
The opposite of intended is unintended
The opposite of reasonable is unreasonable
The opposite of certain is uncertain
The opposite of believable is unbelievable
The opposite of conditional is
2024-07-18 04:06:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 04:10:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.5527, -0.5557,  0.8525,  ..., -0.0156,  2.0625,  0.0640],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3975, -4.2031, -3.7734,  ...,  1.3096,  0.2515, -3.7266],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0089, -0.0123, -0.0015,  ..., -0.0155, -0.0081, -0.0126],
        [-0.0057, -0.0220,  0.0008,  ...,  0.0015, -0.0085,  0.0197],
        [-0.0091, -0.0006, -0.0404,  ..., -0.0103,  0.0059, -0.0176],
        ...,
        [-0.0047, -0.0095, -0.0005,  ..., -0.0018,  0.0075,  0.0037],
        [-0.0033, -0.0054,  0.0118,  ..., -0.0023, -0.0020, -0.0063],
        [ 0.0198, -0.0231, -0.0053,  ...,  0.0063,  0.0039,  0.0052]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5488, -3.6406, -4.9883,  ...,  0.6216,  0.8994, -2.8965]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 04:10:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of healthy is unhealthy
The opposite of finished is unfinished
The opposite of veiled is unveiled
The opposite of intended is unintended
The opposite of reasonable is unreasonable
The opposite of certain is uncertain
The opposite of believable is unbelievable
The opposite of conditional is
2024-07-18 04:10:07 root INFO     [order_1_approx] starting weight calculation for The opposite of certain is uncertain
The opposite of healthy is unhealthy
The opposite of intended is unintended
The opposite of veiled is unveiled
The opposite of reasonable is unreasonable
The opposite of conditional is unconditional
The opposite of finished is unfinished
The opposite of believable is
2024-07-18 04:10:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 04:13:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1685,  0.1411,  1.8799,  ...,  0.1611,  1.6543, -0.2073],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.4297, -0.1431,  1.5977,  ...,  3.5410,  2.3730,  1.0762],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0122, -0.0106, -0.0020,  ..., -0.0043, -0.0060, -0.0208],
        [ 0.0029, -0.0134, -0.0235,  ..., -0.0179, -0.0208, -0.0106],
        [-0.0188, -0.0070, -0.0325,  ..., -0.0165, -0.0041,  0.0227],
        ...,
        [ 0.0111, -0.0166, -0.0150,  ..., -0.0090, -0.0312,  0.0049],
        [-0.0047, -0.0213,  0.0157,  ...,  0.0127, -0.0037,  0.0017],
        [-0.0007, -0.0065, -0.0096,  ..., -0.0276,  0.0299, -0.0069]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.7305,  0.3926,  1.1895,  ...,  3.3633,  3.0176, -0.4209]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 04:13:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of certain is uncertain
The opposite of healthy is unhealthy
The opposite of intended is unintended
The opposite of veiled is unveiled
The opposite of reasonable is unreasonable
The opposite of conditional is unconditional
The opposite of finished is unfinished
The opposite of believable is
2024-07-18 04:13:41 root INFO     [order_1_approx] starting weight calculation for The opposite of reasonable is unreasonable
The opposite of intended is unintended
The opposite of certain is uncertain
The opposite of healthy is unhealthy
The opposite of believable is unbelievable
The opposite of conditional is unconditional
The opposite of finished is unfinished
The opposite of veiled is
2024-07-18 04:13:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 04:17:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0391, -1.4834, -0.3284,  ...,  1.1270,  1.7881,  0.8271],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2549,  1.7549, -4.4727,  ...,  0.6060, -4.2734, -2.0859],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.6937e-02, -5.3024e-03, -5.6381e-03,  ..., -6.1111e-03,
         -5.1003e-03, -2.3911e-02],
        [-1.3184e-02, -1.8478e-02, -6.3019e-03,  ..., -1.8341e-02,
         -9.4757e-03, -4.6082e-03],
        [-2.1133e-03, -1.2772e-02, -2.0813e-02,  ..., -7.9346e-03,
          1.2054e-03,  1.0727e-02],
        ...,
        [ 1.2646e-03, -5.4932e-03,  7.9193e-03,  ...,  3.3817e-03,
         -2.0485e-03,  1.7090e-03],
        [-1.6281e-02,  9.0599e-06,  1.0193e-02,  ...,  1.6663e-02,
         -6.6261e-03, -1.9547e-02],
        [ 9.4604e-03, -1.8768e-02,  1.2009e-02,  ..., -1.2009e-02,
          1.0033e-02,  4.0207e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6816,  2.7363, -4.2930,  ...,  0.7373, -3.4277, -2.1348]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 04:17:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of reasonable is unreasonable
The opposite of intended is unintended
The opposite of certain is uncertain
The opposite of healthy is unhealthy
The opposite of believable is unbelievable
The opposite of conditional is unconditional
The opposite of finished is unfinished
The opposite of veiled is
2024-07-18 04:17:17 root INFO     [order_1_approx] starting weight calculation for The opposite of conditional is unconditional
The opposite of intended is unintended
The opposite of veiled is unveiled
The opposite of finished is unfinished
The opposite of certain is uncertain
The opposite of reasonable is unreasonable
The opposite of believable is unbelievable
The opposite of healthy is
2024-07-18 04:17:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 04:20:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4072,  0.1035, -0.8794,  ..., -0.1929,  1.7998,  0.9478],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1204, -0.5796,  2.1914,  ..., -5.6797,  3.7148, -2.2695],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0017, -0.0174,  0.0108,  ..., -0.0076,  0.0106, -0.0047],
        [ 0.0025, -0.0088, -0.0103,  ..., -0.0134, -0.0069, -0.0021],
        [ 0.0004, -0.0135, -0.0020,  ...,  0.0055,  0.0122,  0.0161],
        ...,
        [ 0.0043, -0.0022,  0.0123,  ...,  0.0225,  0.0084, -0.0105],
        [-0.0051, -0.0049,  0.0009,  ...,  0.0008, -0.0101, -0.0072],
        [-0.0048, -0.0055, -0.0068,  ..., -0.0121, -0.0018, -0.0189]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4871,  0.1201,  1.5000,  ..., -5.6602,  3.8477, -1.5059]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 04:20:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of conditional is unconditional
The opposite of intended is unintended
The opposite of veiled is unveiled
The opposite of finished is unfinished
The opposite of certain is uncertain
The opposite of reasonable is unreasonable
The opposite of believable is unbelievable
The opposite of healthy is
2024-07-18 04:20:51 root INFO     [order_1_approx] starting weight calculation for The opposite of healthy is unhealthy
The opposite of certain is uncertain
The opposite of veiled is unveiled
The opposite of finished is unfinished
The opposite of conditional is unconditional
The opposite of believable is unbelievable
The opposite of intended is unintended
The opposite of reasonable is
2024-07-18 04:20:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 04:24:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7988, -0.6577,  0.9746,  ..., -0.5894,  2.2129,  0.1650],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2637, -2.7695, -0.5869,  ..., -0.3015,  5.1719,  1.4463],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0091, -0.0101, -0.0016,  ..., -0.0176, -0.0155, -0.0047],
        [ 0.0041, -0.0096, -0.0098,  ...,  0.0009, -0.0051, -0.0052],
        [ 0.0096, -0.0209, -0.0062,  ..., -0.0125,  0.0048,  0.0105],
        ...,
        [-0.0025, -0.0035, -0.0175,  ...,  0.0038,  0.0052, -0.0186],
        [-0.0147,  0.0033,  0.0132,  ...,  0.0131, -0.0204, -0.0118],
        [-0.0032, -0.0071, -0.0151,  ...,  0.0026,  0.0198,  0.0056]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3438, -2.9688, -1.1641,  ..., -0.3569,  5.9766,  0.9961]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 04:24:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of healthy is unhealthy
The opposite of certain is uncertain
The opposite of veiled is unveiled
The opposite of finished is unfinished
The opposite of conditional is unconditional
The opposite of believable is unbelievable
The opposite of intended is unintended
The opposite of reasonable is
2024-07-18 04:24:23 root INFO     [order_1_approx] starting weight calculation for The opposite of certain is uncertain
The opposite of veiled is unveiled
The opposite of healthy is unhealthy
The opposite of reasonable is unreasonable
The opposite of believable is unbelievable
The opposite of intended is unintended
The opposite of conditional is unconditional
The opposite of finished is
2024-07-18 04:24:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 04:27:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4695,  0.6382,  0.7021,  ...,  0.6948,  1.3867,  0.8877],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0820, -0.3149, -5.5352,  ..., -0.1484,  0.0530, -0.5391],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0182, -0.0058, -0.0006,  ...,  0.0073, -0.0191, -0.0257],
        [-0.0184, -0.0124, -0.0110,  ..., -0.0106, -0.0023, -0.0144],
        [-0.0002,  0.0057, -0.0247,  ..., -0.0149,  0.0115, -0.0085],
        ...,
        [ 0.0090, -0.0035,  0.0003,  ...,  0.0092, -0.0021, -0.0046],
        [-0.0173,  0.0060,  0.0060,  ...,  0.0089, -0.0142, -0.0180],
        [-0.0071, -0.0009, -0.0156,  ..., -0.0146,  0.0107, -0.0272]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2500, -0.2347, -5.6055,  ...,  0.0812, -0.6689, -1.9688]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 04:27:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of certain is uncertain
The opposite of veiled is unveiled
The opposite of healthy is unhealthy
The opposite of reasonable is unreasonable
The opposite of believable is unbelievable
The opposite of intended is unintended
The opposite of conditional is unconditional
The opposite of finished is
2024-07-18 04:27:58 root INFO     [order_1_approx] starting weight calculation for The opposite of finished is unfinished
The opposite of veiled is unveiled
The opposite of conditional is unconditional
The opposite of reasonable is unreasonable
The opposite of certain is uncertain
The opposite of healthy is unhealthy
The opposite of believable is unbelievable
The opposite of intended is
2024-07-18 04:27:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 04:31:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4404,  1.2627, -0.9131,  ..., -0.1702,  1.6475, -0.3708],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5244,  1.6094, -0.9111,  ...,  2.0293, -1.2539,  0.8867],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0082, -0.0012,  0.0018,  ..., -0.0132, -0.0110, -0.0039],
        [-0.0155, -0.0045, -0.0092,  ...,  0.0033, -0.0108, -0.0031],
        [-0.0064, -0.0078, -0.0321,  ..., -0.0116, -0.0142, -0.0082],
        ...,
        [-0.0025, -0.0144,  0.0024,  ..., -0.0055, -0.0012,  0.0014],
        [-0.0135, -0.0071,  0.0164,  ...,  0.0184, -0.0033, -0.0030],
        [-0.0104,  0.0029, -0.0060,  ..., -0.0091, -0.0030, -0.0122]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1890,  1.5645, -1.5645,  ...,  1.8535, -0.8027,  0.8511]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 04:31:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of finished is unfinished
The opposite of veiled is unveiled
The opposite of conditional is unconditional
The opposite of reasonable is unreasonable
The opposite of certain is uncertain
The opposite of healthy is unhealthy
The opposite of believable is unbelievable
The opposite of intended is
2024-07-18 04:31:33 root INFO     [order_1_approx] starting weight calculation for The opposite of intended is unintended
The opposite of reasonable is unreasonable
The opposite of finished is unfinished
The opposite of healthy is unhealthy
The opposite of veiled is unveiled
The opposite of conditional is unconditional
The opposite of believable is unbelievable
The opposite of certain is
2024-07-18 04:31:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 04:35:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6675, -0.0852, -0.2452,  ..., -0.2133,  1.3008,  0.7993],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.7266, -1.7881, -0.5742,  ...,  0.8877,  5.0391, -4.9062],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0189, -0.0217,  0.0062,  ..., -0.0204, -0.0084, -0.0072],
        [ 0.0015, -0.0182, -0.0133,  ..., -0.0120, -0.0160,  0.0141],
        [ 0.0191,  0.0032, -0.0230,  ..., -0.0061, -0.0004,  0.0136],
        ...,
        [ 0.0034, -0.0064, -0.0054,  ..., -0.0124,  0.0021,  0.0004],
        [-0.0157,  0.0025, -0.0103,  ...,  0.0028, -0.0066, -0.0244],
        [-0.0114,  0.0073,  0.0026,  ...,  0.0031,  0.0131, -0.0329]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.1836, -1.5449, -0.9951,  ..., -0.0176,  4.8359, -3.8984]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 04:35:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of intended is unintended
The opposite of reasonable is unreasonable
The opposite of finished is unfinished
The opposite of healthy is unhealthy
The opposite of veiled is unveiled
The opposite of conditional is unconditional
The opposite of believable is unbelievable
The opposite of certain is
2024-07-18 04:35:08 root INFO     total operator prediction time: 1714.520664691925 seconds
2024-07-18 04:35:08 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on re+verb_reg
2024-07-18 04:35:08 root INFO     building operator re+verb_reg
2024-07-18 04:35:08 root INFO     [order_1_approx] starting weight calculation for To examine again is to reexamine
To occur again is to reoccur
To publish again is to republish
To investigate again is to reinvestigate
To cognize again is to recognize
To solve again is to resolve
To commend again is to recommend
To appoint again is to
2024-07-18 04:35:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 04:38:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1875, -0.8203,  0.0813,  ...,  0.5234, -0.2605,  0.9961],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.2324,  1.2295, -5.5898,  ...,  3.7773, -3.0625, -0.3838],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0179, -0.0206, -0.0023,  ..., -0.0193,  0.0119, -0.0184],
        [-0.0009,  0.0082,  0.0042,  ...,  0.0059, -0.0027,  0.0187],
        [ 0.0044, -0.0038, -0.0065,  ..., -0.0162, -0.0059, -0.0136],
        ...,
        [-0.0026,  0.0132, -0.0090,  ...,  0.0098, -0.0052, -0.0030],
        [ 0.0046,  0.0016, -0.0079,  ..., -0.0143, -0.0002,  0.0022],
        [ 0.0128,  0.0040,  0.0221,  ...,  0.0009,  0.0300, -0.0027]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3877,  1.0010, -4.9727,  ...,  4.2344, -3.4980, -0.3423]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 04:38:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To examine again is to reexamine
To occur again is to reoccur
To publish again is to republish
To investigate again is to reinvestigate
To cognize again is to recognize
To solve again is to resolve
To commend again is to recommend
To appoint again is to
2024-07-18 04:38:43 root INFO     [order_1_approx] starting weight calculation for To investigate again is to reinvestigate
To examine again is to reexamine
To solve again is to resolve
To cognize again is to recognize
To appoint again is to reappoint
To commend again is to recommend
To occur again is to reoccur
To publish again is to
2024-07-18 04:38:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 04:42:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.9980, 0.0156, 2.0195,  ..., 0.9521, 0.1958, 1.3721], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.8516,  0.2075, -7.0391,  ...,  2.1133, -2.1484, -2.0156],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0070,  0.0009, -0.0054,  ..., -0.0062, -0.0021, -0.0061],
        [-0.0017,  0.0114,  0.0125,  ...,  0.0119, -0.0001, -0.0031],
        [-0.0053, -0.0121, -0.0033,  ..., -0.0120, -0.0078, -0.0032],
        ...,
        [-0.0104,  0.0132, -0.0029,  ..., -0.0029, -0.0167,  0.0020],
        [-0.0017, -0.0092,  0.0005,  ..., -0.0070, -0.0006, -0.0077],
        [-0.0111,  0.0057,  0.0009,  ..., -0.0047, -0.0024,  0.0052]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.3047, -0.5796, -7.4375,  ...,  1.6943, -2.2031, -2.0352]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 04:42:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To investigate again is to reinvestigate
To examine again is to reexamine
To solve again is to resolve
To cognize again is to recognize
To appoint again is to reappoint
To commend again is to recommend
To occur again is to reoccur
To publish again is to
2024-07-18 04:42:17 root INFO     [order_1_approx] starting weight calculation for To publish again is to republish
To appoint again is to reappoint
To cognize again is to recognize
To commend again is to recommend
To occur again is to reoccur
To solve again is to resolve
To investigate again is to reinvestigate
To examine again is to
2024-07-18 04:42:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 04:45:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3408, -0.0657,  0.6670,  ..., -0.9033,  0.9619,  1.4131],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6641,  2.3945, -6.5391,  ...,  2.1523, -3.9258,  0.4531],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0160,  0.0065, -0.0036,  ..., -0.0111, -0.0082, -0.0121],
        [ 0.0005, -0.0006,  0.0142,  ..., -0.0025,  0.0112,  0.0070],
        [ 0.0082,  0.0076,  0.0029,  ...,  0.0048,  0.0115, -0.0042],
        ...,
        [-0.0218, -0.0070, -0.0103,  ...,  0.0078, -0.0117, -0.0003],
        [ 0.0013, -0.0056, -0.0113,  ..., -0.0082, -0.0009,  0.0024],
        [-0.0002,  0.0079,  0.0063,  ..., -0.0007,  0.0066,  0.0034]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0977,  3.1348, -7.5703,  ...,  1.6924, -4.3711,  0.3750]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 04:45:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To publish again is to republish
To appoint again is to reappoint
To cognize again is to recognize
To commend again is to recommend
To occur again is to reoccur
To solve again is to resolve
To investigate again is to reinvestigate
To examine again is to
2024-07-18 04:45:50 root INFO     [order_1_approx] starting weight calculation for To investigate again is to reinvestigate
To publish again is to republish
To cognize again is to recognize
To appoint again is to reappoint
To examine again is to reexamine
To occur again is to reoccur
To solve again is to resolve
To commend again is to
2024-07-18 04:45:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 04:49:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5615,  0.1033,  1.0332,  ..., -0.1316,  0.6519,  0.8140],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.8086, -0.6758, -3.2188,  ..., -0.4502, -2.6855, -0.2686],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.0065e-03, -6.0921e-03,  1.2039e-02,  ..., -3.5858e-04,
          4.5776e-03, -1.2207e-02],
        [ 3.2692e-03, -6.6490e-03,  8.5754e-03,  ...,  5.7220e-05,
          1.3885e-02,  9.4910e-03],
        [-7.8583e-03, -4.8218e-03,  6.1264e-03,  ..., -1.3092e-02,
          8.8043e-03,  1.0567e-02],
        ...,
        [ 3.5267e-03, -8.4686e-03,  3.1643e-03,  ...,  2.7390e-03,
          1.1566e-02,  9.5596e-03],
        [ 7.2174e-03,  1.0376e-02,  8.3237e-03,  ..., -2.5196e-03,
         -3.1624e-03, -4.2572e-03],
        [-3.3569e-03, -5.3902e-03,  4.0131e-03,  ...,  2.4719e-03,
         -1.2493e-03,  6.3591e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.8594, -0.4219, -4.0742,  ..., -0.4978, -2.2031,  0.2832]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 04:49:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To investigate again is to reinvestigate
To publish again is to republish
To cognize again is to recognize
To appoint again is to reappoint
To examine again is to reexamine
To occur again is to reoccur
To solve again is to resolve
To commend again is to
2024-07-18 04:49:23 root INFO     [order_1_approx] starting weight calculation for To cognize again is to recognize
To appoint again is to reappoint
To occur again is to reoccur
To examine again is to reexamine
To solve again is to resolve
To publish again is to republish
To commend again is to recommend
To investigate again is to
2024-07-18 04:49:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 04:52:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0928,  0.0697,  1.4990,  ..., -0.2559,  0.6904,  1.2949],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7598,  2.0117, -8.6406,  ...,  0.4707, -0.7754, -1.4512],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.1513e-02,  1.1826e-03,  4.5090e-03,  ..., -1.0040e-02,
          6.4507e-03, -4.1223e-04],
        [ 6.3782e-03,  2.2564e-03,  6.1226e-03,  ...,  7.4387e-03,
         -5.4264e-04,  1.8616e-02],
        [ 9.1400e-03,  8.7891e-03,  3.2425e-03,  ..., -7.8821e-04,
         -6.6299e-03, -1.7624e-03],
        ...,
        [-1.2764e-02, -1.8120e-05, -5.0125e-03,  ...,  5.3024e-03,
         -9.7046e-03, -2.3193e-03],
        [ 1.2314e-02, -3.8910e-04, -6.8817e-03,  ...,  9.6588e-03,
          5.8250e-03, -5.5504e-03],
        [-5.3177e-03,  1.9035e-03,  1.3519e-02,  ..., -2.4853e-03,
          4.4212e-03,  8.2016e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.8213,  2.7949, -9.5625,  ...,  0.4766, -1.3408, -2.0566]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 04:52:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To cognize again is to recognize
To appoint again is to reappoint
To occur again is to reoccur
To examine again is to reexamine
To solve again is to resolve
To publish again is to republish
To commend again is to recommend
To investigate again is to
2024-07-18 04:52:57 root INFO     [order_1_approx] starting weight calculation for To publish again is to republish
To occur again is to reoccur
To investigate again is to reinvestigate
To appoint again is to reappoint
To cognize again is to recognize
To commend again is to recommend
To examine again is to reexamine
To solve again is to
2024-07-18 04:52:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 04:56:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4644,  1.3027,  1.6973,  ..., -1.0996,  1.5947,  0.6240],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4688,  2.4824, -6.8906,  ..., -2.5957,  1.0635, -0.9443],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-3.7842e-03,  7.3242e-03, -4.4136e-03,  ..., -7.9651e-03,
         -1.1116e-02, -8.4076e-03],
        [ 2.0477e-02,  4.5776e-03,  1.1978e-02,  ...,  1.0376e-03,
         -4.9744e-03,  3.7003e-03],
        [ 9.2602e-04, -1.9165e-02, -5.3177e-03,  ..., -2.4216e-02,
          1.6937e-02, -1.2100e-02],
        ...,
        [-9.4910e-03, -6.1417e-03, -4.2000e-03,  ..., -1.0271e-03,
          6.1035e-05,  1.2970e-03],
        [-3.6449e-03,  8.8348e-03, -3.5172e-03,  ...,  1.8417e-02,
         -1.1696e-02, -9.4414e-04],
        [-1.3168e-02,  1.4877e-04,  1.9150e-03,  ..., -6.3858e-03,
          1.6846e-02,  5.8746e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5303,  3.1270, -8.1875,  ..., -2.6348,  0.4009, -0.4944]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 04:56:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To publish again is to republish
To occur again is to reoccur
To investigate again is to reinvestigate
To appoint again is to reappoint
To cognize again is to recognize
To commend again is to recommend
To examine again is to reexamine
To solve again is to
2024-07-18 04:56:31 root INFO     [order_1_approx] starting weight calculation for To appoint again is to reappoint
To examine again is to reexamine
To solve again is to resolve
To publish again is to republish
To investigate again is to reinvestigate
To commend again is to recommend
To cognize again is to recognize
To occur again is to
2024-07-18 04:56:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 05:00:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1240,  0.6812,  1.5332,  ..., -0.4153,  1.3330,  0.4751],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9180, -2.2539, -7.1250,  ...,  1.5996,  0.8032,  0.2031],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0010, -0.0009,  0.0055,  ..., -0.0047,  0.0021, -0.0048],
        [ 0.0023, -0.0007,  0.0058,  ..., -0.0019,  0.0104,  0.0170],
        [-0.0048,  0.0034, -0.0152,  ..., -0.0102,  0.0021, -0.0036],
        ...,
        [-0.0005, -0.0076, -0.0075,  ..., -0.0121, -0.0053,  0.0054],
        [-0.0075,  0.0144, -0.0067,  ..., -0.0049, -0.0091, -0.0121],
        [-0.0091,  0.0193,  0.0097,  ...,  0.0029,  0.0046, -0.0128]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5781, -2.6738, -7.4258,  ...,  1.4541, -0.4214, -0.2612]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 05:00:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To appoint again is to reappoint
To examine again is to reexamine
To solve again is to resolve
To publish again is to republish
To investigate again is to reinvestigate
To commend again is to recommend
To cognize again is to recognize
To occur again is to
2024-07-18 05:00:05 root INFO     [order_1_approx] starting weight calculation for To occur again is to reoccur
To commend again is to recommend
To publish again is to republish
To solve again is to resolve
To investigate again is to reinvestigate
To examine again is to reexamine
To appoint again is to reappoint
To cognize again is to
2024-07-18 05:00:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 05:03:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([1.0811, 0.0952, 0.3811,  ..., 0.0951, 0.0593, 1.0234], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.4082,  0.6221, -9.2031,  ..., -0.9048, -4.0781, -6.0156],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 7.9155e-05, -4.7455e-03, -9.5463e-04,  ..., -7.1640e-03,
         -7.3318e-03, -1.6251e-02],
        [ 6.7215e-03, -9.5558e-04,  3.5820e-03,  ...,  2.4452e-03,
         -4.6730e-05,  4.6883e-03],
        [-4.0817e-04,  5.5542e-03, -9.6893e-04,  ...,  7.5531e-04,
          2.6455e-03, -6.3019e-03],
        ...,
        [-1.0414e-02, -7.3776e-03, -6.8283e-03,  ..., -2.7485e-03,
         -2.7504e-03, -9.4032e-04],
        [-5.8365e-03, -8.0261e-03,  4.2267e-03,  ...,  5.6381e-03,
         -9.7427e-03,  4.7455e-03],
        [-1.0315e-02,  9.3689e-03, -2.4986e-04,  ..., -3.4237e-03,
          8.4152e-03,  2.4490e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5195,  0.2546, -9.7500,  ..., -0.8145, -4.3203, -6.3320]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 05:03:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To occur again is to reoccur
To commend again is to recommend
To publish again is to republish
To solve again is to resolve
To investigate again is to reinvestigate
To examine again is to reexamine
To appoint again is to reappoint
To cognize again is to
2024-07-18 05:03:38 root INFO     total operator prediction time: 1710.2476027011871 seconds
2024-07-18 05:03:38 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on adj+ness_reg
2024-07-18 05:03:38 root INFO     building operator adj+ness_reg
2024-07-18 05:03:39 root INFO     [order_1_approx] starting weight calculation for The state of being righteous is righteousness
The state of being connected is connectedness
The state of being competitive is competitiveness
The state of being obvious is obviousness
The state of being devoted is devotedness
The state of being helpful is helpfulness
The state of being amazing is amazingness
The state of being same is
2024-07-18 05:03:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 05:07:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1416,  0.9541,  1.5674,  ..., -1.2910,  0.1520, -1.0176],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3672,  3.5117, -1.7354,  ...,  0.1089, -0.3394,  4.0781],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0184, -0.0025,  0.0050,  ..., -0.0072, -0.0138, -0.0205],
        [ 0.0105, -0.0232, -0.0039,  ...,  0.0073, -0.0142, -0.0095],
        [ 0.0004, -0.0076, -0.0073,  ..., -0.0154,  0.0111,  0.0067],
        ...,
        [-0.0074, -0.0022,  0.0026,  ...,  0.0039,  0.0054, -0.0154],
        [-0.0146,  0.0134,  0.0145,  ...,  0.0077, -0.0144, -0.0018],
        [-0.0148,  0.0032, -0.0069,  ..., -0.0191, -0.0050, -0.0285]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1047,  4.4922, -1.8750,  ..., -0.1597, -0.6060,  4.0273]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 05:07:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being righteous is righteousness
The state of being connected is connectedness
The state of being competitive is competitiveness
The state of being obvious is obviousness
The state of being devoted is devotedness
The state of being helpful is helpfulness
The state of being amazing is amazingness
The state of being same is
2024-07-18 05:07:14 root INFO     [order_1_approx] starting weight calculation for The state of being helpful is helpfulness
The state of being connected is connectedness
The state of being devoted is devotedness
The state of being righteous is righteousness
The state of being competitive is competitiveness
The state of being same is sameness
The state of being obvious is obviousness
The state of being amazing is
2024-07-18 05:07:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 05:10:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0732, -0.0312,  0.8560,  ..., -0.2515,  1.7773, -0.3760],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.8516,  1.5361, -1.8359,  ..., -1.6514,  0.7314,  9.2812],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0266, -0.0179,  0.0156,  ...,  0.0114, -0.0132, -0.0229],
        [ 0.0003, -0.0026, -0.0042,  ...,  0.0054,  0.0009,  0.0026],
        [-0.0011,  0.0081, -0.0490,  ..., -0.0060,  0.0121, -0.0114],
        ...,
        [ 0.0085, -0.0050, -0.0097,  ..., -0.0093, -0.0156, -0.0125],
        [ 0.0014,  0.0073,  0.0004,  ..., -0.0159, -0.0227, -0.0048],
        [-0.0179,  0.0047,  0.0054,  ..., -0.0008, -0.0263, -0.0291]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.9141,  1.6230, -1.0000,  ..., -2.1191,  1.0645,  9.6250]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 05:10:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being helpful is helpfulness
The state of being connected is connectedness
The state of being devoted is devotedness
The state of being righteous is righteousness
The state of being competitive is competitiveness
The state of being same is sameness
The state of being obvious is obviousness
The state of being amazing is
2024-07-18 05:10:43 root INFO     [order_1_approx] starting weight calculation for The state of being same is sameness
The state of being amazing is amazingness
The state of being righteous is righteousness
The state of being competitive is competitiveness
The state of being helpful is helpfulness
The state of being obvious is obviousness
The state of being connected is connectedness
The state of being devoted is
2024-07-18 05:10:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 05:14:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1797, -0.3354, -0.2305,  ...,  0.2280,  1.0439, -0.6084],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5566,  3.7070,  3.8809,  ..., -5.1953, -1.9688,  5.9336],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0003, -0.0150,  0.0167,  ..., -0.0074, -0.0094, -0.0319],
        [-0.0204, -0.0226, -0.0154,  ...,  0.0072,  0.0196,  0.0119],
        [ 0.0006,  0.0013, -0.0335,  ..., -0.0128,  0.0050,  0.0077],
        ...,
        [-0.0302, -0.0177,  0.0067,  ..., -0.0382, -0.0018,  0.0099],
        [-0.0067,  0.0071,  0.0160,  ..., -0.0035, -0.0122, -0.0093],
        [-0.0250,  0.0016,  0.0005,  ..., -0.0096, -0.0162, -0.0362]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6895,  4.0312,  4.3672,  ..., -4.8047, -2.0098,  6.2031]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 05:14:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being same is sameness
The state of being amazing is amazingness
The state of being righteous is righteousness
The state of being competitive is competitiveness
The state of being helpful is helpfulness
The state of being obvious is obviousness
The state of being connected is connectedness
The state of being devoted is
2024-07-18 05:14:17 root INFO     [order_1_approx] starting weight calculation for The state of being same is sameness
The state of being obvious is obviousness
The state of being helpful is helpfulness
The state of being righteous is righteousness
The state of being competitive is competitiveness
The state of being devoted is devotedness
The state of being amazing is amazingness
The state of being connected is
2024-07-18 05:14:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 05:17:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8232, -0.1301,  0.8564,  ...,  0.1511,  1.1338,  0.5342],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.4941,  0.7080,  0.0215,  ..., -2.8477, -3.2930,  4.3594],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0019, -0.0027,  0.0152,  ..., -0.0044,  0.0100, -0.0202],
        [-0.0003, -0.0132, -0.0028,  ...,  0.0023, -0.0024, -0.0033],
        [ 0.0052, -0.0104, -0.0148,  ..., -0.0049, -0.0044, -0.0009],
        ...,
        [-0.0079, -0.0027, -0.0068,  ..., -0.0055,  0.0053, -0.0037],
        [ 0.0114,  0.0039,  0.0052,  ..., -0.0159, -0.0004,  0.0030],
        [-0.0147,  0.0160,  0.0081,  ..., -0.0183, -0.0063, -0.0203]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.5469,  0.8359,  0.2357,  ..., -3.0547, -3.7676,  4.9805]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 05:17:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being same is sameness
The state of being obvious is obviousness
The state of being helpful is helpfulness
The state of being righteous is righteousness
The state of being competitive is competitiveness
The state of being devoted is devotedness
The state of being amazing is amazingness
The state of being connected is
2024-07-18 05:17:51 root INFO     [order_1_approx] starting weight calculation for The state of being same is sameness
The state of being obvious is obviousness
The state of being righteous is righteousness
The state of being connected is connectedness
The state of being devoted is devotedness
The state of being helpful is helpfulness
The state of being amazing is amazingness
The state of being competitive is
2024-07-18 05:17:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 05:21:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0852, -0.7319,  1.8789,  ..., -0.4490,  2.4746,  0.5415],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-5.0664, -0.4182, -2.4023,  ..., -0.2739,  2.0605,  3.7090],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0017, -0.0156,  0.0199,  ..., -0.0164,  0.0032, -0.0198],
        [-0.0163, -0.0025,  0.0104,  ...,  0.0141,  0.0014, -0.0002],
        [ 0.0024, -0.0009, -0.0136,  ..., -0.0131,  0.0108,  0.0123],
        ...,
        [-0.0133, -0.0060,  0.0077,  ..., -0.0039, -0.0057, -0.0141],
        [-0.0069, -0.0035,  0.0087,  ..., -0.0151, -0.0118,  0.0119],
        [-0.0228,  0.0047,  0.0193,  ..., -0.0061, -0.0034, -0.0219]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.0898,  0.3250, -2.2891,  ..., -0.2673,  2.0859,  4.0898]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 05:21:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being same is sameness
The state of being obvious is obviousness
The state of being righteous is righteousness
The state of being connected is connectedness
The state of being devoted is devotedness
The state of being helpful is helpfulness
The state of being amazing is amazingness
The state of being competitive is
2024-07-18 05:21:25 root INFO     [order_1_approx] starting weight calculation for The state of being amazing is amazingness
The state of being connected is connectedness
The state of being competitive is competitiveness
The state of being obvious is obviousness
The state of being righteous is righteousness
The state of being same is sameness
The state of being devoted is devotedness
The state of being helpful is
2024-07-18 05:21:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 05:24:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7163,  0.7188, -0.2607,  ..., -0.0662,  1.3584,  0.6255],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.4766,  1.8125,  0.4175,  ...,  1.4814,  0.7695,  7.1680],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0117, -0.0165,  0.0097,  ..., -0.0047,  0.0087, -0.0257],
        [-0.0009, -0.0130,  0.0024,  ...,  0.0013,  0.0022,  0.0098],
        [-0.0039, -0.0033, -0.0170,  ..., -0.0063,  0.0100,  0.0057],
        ...,
        [-0.0094, -0.0035, -0.0040,  ..., -0.0102, -0.0115, -0.0079],
        [-0.0013,  0.0039,  0.0097,  ..., -0.0097, -0.0253, -0.0067],
        [-0.0307,  0.0048,  0.0274,  ..., -0.0122, -0.0277, -0.0262]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.7285,  2.2031,  0.9521,  ...,  1.7148,  0.8218,  7.6758]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 05:25:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being amazing is amazingness
The state of being connected is connectedness
The state of being competitive is competitiveness
The state of being obvious is obviousness
The state of being righteous is righteousness
The state of being same is sameness
The state of being devoted is devotedness
The state of being helpful is
2024-07-18 05:25:00 root INFO     [order_1_approx] starting weight calculation for The state of being connected is connectedness
The state of being amazing is amazingness
The state of being devoted is devotedness
The state of being same is sameness
The state of being competitive is competitiveness
The state of being righteous is righteousness
The state of being helpful is helpfulness
The state of being obvious is
2024-07-18 05:25:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 05:28:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8652,  0.0692,  0.3254,  ..., -0.9595,  0.8066,  0.1260],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.6797,  3.9590, -3.5059,  ..., -0.3181, -2.3770,  5.0469],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-5.6763e-03, -1.5129e-02,  4.1122e-03,  ..., -7.3776e-03,
          4.9210e-03, -2.2766e-02],
        [-9.6588e-03, -1.5015e-02,  4.4632e-03,  ...,  1.1467e-02,
         -2.7580e-03,  2.3994e-03],
        [-2.9373e-03, -4.5319e-03, -2.9510e-02,  ..., -1.3466e-02,
          1.0780e-02, -4.5776e-03],
        ...,
        [-2.6112e-03, -2.3483e-02, -6.9008e-03,  ..., -1.6068e-02,
         -1.0544e-02, -1.9569e-03],
        [-1.2932e-02, -4.5471e-03,  3.0518e-05,  ..., -1.3016e-02,
         -2.3270e-02, -4.8447e-03],
        [-1.5686e-02,  6.3782e-03,  1.4359e-02,  ...,  3.5667e-03,
         -5.0964e-03, -2.4170e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5605,  4.1953, -2.7676,  ..., -0.1631, -2.6055,  5.8672]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 05:28:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being connected is connectedness
The state of being amazing is amazingness
The state of being devoted is devotedness
The state of being same is sameness
The state of being competitive is competitiveness
The state of being righteous is righteousness
The state of being helpful is helpfulness
The state of being obvious is
2024-07-18 05:28:35 root INFO     [order_1_approx] starting weight calculation for The state of being amazing is amazingness
The state of being same is sameness
The state of being devoted is devotedness
The state of being connected is connectedness
The state of being obvious is obviousness
The state of being competitive is competitiveness
The state of being helpful is helpfulness
The state of being righteous is
2024-07-18 05:28:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 05:32:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5098e+00, -4.4287e-01,  1.2207e-03,  ..., -4.8340e-02,
         1.3877e+00, -7.8125e-01], device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9639,  3.0000, -4.0156,  ...,  0.8477, -0.4346,  6.6797],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0168, -0.0281,  0.0157,  ..., -0.0195, -0.0116, -0.0240],
        [-0.0016, -0.0046,  0.0003,  ...,  0.0007,  0.0113, -0.0038],
        [ 0.0073, -0.0078, -0.0294,  ...,  0.0079,  0.0138,  0.0024],
        ...,
        [-0.0153, -0.0160, -0.0029,  ..., -0.0154, -0.0019, -0.0021],
        [-0.0076,  0.0051,  0.0004,  ..., -0.0056, -0.0196, -0.0110],
        [-0.0280,  0.0145,  0.0183,  ..., -0.0098, -0.0114, -0.0360]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8833,  3.4766, -4.3047,  ...,  1.5312, -0.0195,  6.6641]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 05:32:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being amazing is amazingness
The state of being same is sameness
The state of being devoted is devotedness
The state of being connected is connectedness
The state of being obvious is obviousness
The state of being competitive is competitiveness
The state of being helpful is helpfulness
The state of being righteous is
2024-07-18 05:32:09 root INFO     total operator prediction time: 1710.9844288825989 seconds
2024-07-18 05:32:09 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on noun+less_reg
2024-07-18 05:32:09 root INFO     building operator noun+less_reg
2024-07-18 05:32:10 root INFO     [order_1_approx] starting weight calculation for Something without mirth is mirthless
Something without guilt is guiltless
Something without defence is defenceless
Something without arm is armless
Something without goal is goalless
Something without luck is luckless
Something without leg is legless
Something without odor is
2024-07-18 05:32:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 05:35:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2900,  0.7900, -0.6108,  ..., -2.3887,  1.1660,  1.5068],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6094,  1.1914,  1.4727,  ...,  0.2896, -0.9487,  0.3066],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-9.6512e-03, -1.9073e-02,  8.1711e-03,  ...,  8.6212e-04,
         -4.5242e-03, -2.9755e-04],
        [-8.1177e-03, -1.9302e-02, -6.5994e-03,  ...,  6.4163e-03,
          2.4643e-03,  3.7193e-05],
        [-1.2962e-02, -9.4910e-03, -2.3361e-02,  ..., -5.7220e-04,
         -1.1139e-02,  1.2646e-03],
        ...,
        [-9.5825e-03, -1.1635e-03,  1.0468e-02,  ..., -3.0766e-03,
         -2.4433e-03,  5.7259e-03],
        [ 1.4168e-02, -1.0597e-02,  1.1322e-02,  ..., -1.8616e-03,
         -2.4704e-02, -6.9809e-03],
        [-2.1759e-02,  7.3547e-03,  7.5531e-04,  ..., -1.2589e-02,
         -6.9809e-04, -2.4918e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6768,  1.8418,  0.4404,  ..., -0.3843, -1.7471,  0.4404]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 05:35:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without mirth is mirthless
Something without guilt is guiltless
Something without defence is defenceless
Something without arm is armless
Something without goal is goalless
Something without luck is luckless
Something without leg is legless
Something without odor is
2024-07-18 05:35:42 root INFO     [order_1_approx] starting weight calculation for Something without leg is legless
Something without luck is luckless
Something without goal is goalless
Something without odor is odorless
Something without defence is defenceless
Something without mirth is mirthless
Something without arm is armless
Something without guilt is
2024-07-18 05:35:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 05:39:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.7188,  0.4062,  0.7197,  ..., -0.4126,  0.7910,  0.3101],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.2383, -0.1494, -4.8359,  ...,  2.4531,  1.5400, -1.3330],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0027, -0.0172,  0.0120,  ...,  0.0031, -0.0051, -0.0170],
        [-0.0060, -0.0074, -0.0074,  ..., -0.0038,  0.0008, -0.0080],
        [-0.0114, -0.0068, -0.0052,  ..., -0.0022, -0.0034,  0.0150],
        ...,
        [-0.0087, -0.0245, -0.0127,  ..., -0.0046,  0.0091,  0.0046],
        [-0.0089,  0.0016,  0.0123,  ..., -0.0025, -0.0110,  0.0010],
        [-0.0235,  0.0051,  0.0060,  ...,  0.0048,  0.0068, -0.0246]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.8477, -0.5254, -5.3438,  ...,  1.5781,  0.6406, -1.6133]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 05:39:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without leg is legless
Something without luck is luckless
Something without goal is goalless
Something without odor is odorless
Something without defence is defenceless
Something without mirth is mirthless
Something without arm is armless
Something without guilt is
2024-07-18 05:39:15 root INFO     [order_1_approx] starting weight calculation for Something without leg is legless
Something without guilt is guiltless
Something without goal is goalless
Something without arm is armless
Something without defence is defenceless
Something without odor is odorless
Something without luck is luckless
Something without mirth is
2024-07-18 05:39:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 05:42:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.6670, -0.3633,  0.3804,  ...,  1.6094,  1.7158,  1.2002],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5254,  1.0273, -3.7695,  ...,  2.3770,  2.1250,  2.9590],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0084, -0.0075,  0.0140,  ..., -0.0062, -0.0020, -0.0078],
        [-0.0093, -0.0061, -0.0053,  ...,  0.0079,  0.0102,  0.0033],
        [ 0.0109,  0.0017, -0.0156,  ..., -0.0058,  0.0066, -0.0038],
        ...,
        [-0.0025, -0.0095,  0.0042,  ...,  0.0025, -0.0113,  0.0018],
        [-0.0013, -0.0006,  0.0056,  ..., -0.0038, -0.0084, -0.0008],
        [-0.0147, -0.0050,  0.0078,  ...,  0.0023, -0.0023, -0.0127]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6289,  0.7983, -3.4023,  ...,  1.8359,  1.6895,  3.2930]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 05:42:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without leg is legless
Something without guilt is guiltless
Something without goal is goalless
Something without arm is armless
Something without defence is defenceless
Something without odor is odorless
Something without luck is luckless
Something without mirth is
2024-07-18 05:42:48 root INFO     [order_1_approx] starting weight calculation for Something without defence is defenceless
Something without leg is legless
Something without mirth is mirthless
Something without odor is odorless
Something without arm is armless
Something without guilt is guiltless
Something without goal is goalless
Something without luck is
2024-07-18 05:42:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 05:46:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7188,  0.8760, -1.2871,  ...,  1.6465,  0.3833,  0.2139],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.9053,  1.8037, -2.1484,  ...,  0.2544,  3.1953,  4.0859],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.9424e-02, -2.2018e-02,  1.8478e-02,  ..., -3.9902e-03,
         -8.8501e-03, -2.0569e-02],
        [-1.5106e-02, -6.8245e-03,  7.7057e-03,  ...,  9.2163e-03,
         -6.7635e-03, -7.1812e-04],
        [-1.8063e-03,  2.3193e-02, -9.1248e-03,  ..., -8.4686e-03,
          1.5106e-02, -4.2725e-04],
        ...,
        [ 9.0256e-03, -5.0201e-03,  1.1154e-02,  ..., -1.6327e-02,
         -1.9855e-03, -5.6458e-03],
        [-4.6425e-03, -3.1860e-02,  1.3672e-02,  ..., -1.2840e-02,
         -7.1831e-03, -1.2024e-02],
        [-1.7090e-02,  6.1035e-05,  1.3199e-02,  ..., -1.1282e-03,
          1.8063e-03, -6.0539e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.4375,  2.3027, -3.0234,  ..., -0.1562,  2.6172,  4.5391]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 05:46:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without defence is defenceless
Something without leg is legless
Something without mirth is mirthless
Something without odor is odorless
Something without arm is armless
Something without guilt is guiltless
Something without goal is goalless
Something without luck is
2024-07-18 05:46:22 root INFO     [order_1_approx] starting weight calculation for Something without arm is armless
Something without goal is goalless
Something without guilt is guiltless
Something without mirth is mirthless
Something without odor is odorless
Something without leg is legless
Something without luck is luckless
Something without defence is
2024-07-18 05:46:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 05:49:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([1.4531, 1.4980, 0.1045,  ..., 0.5459, 0.9902, 1.8848], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6387,  1.9912, -3.2109,  ..., -2.2969,  1.5986, -2.1172],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.4142e-03, -3.1281e-02,  5.2223e-03,  ...,  3.2864e-03,
         -6.8741e-03, -1.5778e-02],
        [-5.2872e-03, -2.3785e-03,  8.9951e-03,  ...,  2.8400e-03,
          1.2901e-02, -4.9324e-03],
        [ 1.0559e-02, -4.5967e-03, -1.0010e-02,  ...,  4.3869e-05,
         -7.1716e-04,  1.0239e-02],
        ...,
        [ 2.5826e-03,  3.6697e-03, -3.3989e-03,  ..., -3.0106e-02,
         -1.0880e-02, -5.8289e-03],
        [ 3.3913e-03, -1.3611e-02,  1.4633e-02,  ..., -3.2997e-03,
         -3.2257e-02,  1.1292e-02],
        [ 3.2616e-03, -3.4332e-03,  4.1199e-03,  ..., -2.8000e-03,
         -9.2773e-03, -1.2970e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4453,  1.9268, -3.7461,  ..., -2.2812,  1.6045, -1.7734]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 05:49:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without arm is armless
Something without goal is goalless
Something without guilt is guiltless
Something without mirth is mirthless
Something without odor is odorless
Something without leg is legless
Something without luck is luckless
Something without defence is
2024-07-18 05:49:57 root INFO     [order_1_approx] starting weight calculation for Something without goal is goalless
Something without luck is luckless
Something without odor is odorless
Something without arm is armless
Something without defence is defenceless
Something without mirth is mirthless
Something without guilt is guiltless
Something without leg is
2024-07-18 05:49:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 05:53:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2012,  1.2432, -0.1388,  ...,  0.4810,  0.0883, -0.0283],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.4805, -0.8218, -0.5269,  ..., -3.9766,  1.1553,  4.1797],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0031, -0.0110,  0.0233,  ..., -0.0015, -0.0048,  0.0078],
        [-0.0120,  0.0027, -0.0086,  ..., -0.0051, -0.0009, -0.0080],
        [ 0.0049,  0.0016, -0.0062,  ..., -0.0091, -0.0019,  0.0049],
        ...,
        [ 0.0009,  0.0120,  0.0115,  ..., -0.0035, -0.0079,  0.0091],
        [-0.0060,  0.0130,  0.0127,  ..., -0.0039, -0.0178, -0.0060],
        [-0.0303, -0.0009, -0.0041,  ...,  0.0090,  0.0045, -0.0278]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.9980, -0.5620, -0.7256,  ..., -4.0273,  0.9922,  3.6758]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 05:53:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without goal is goalless
Something without luck is luckless
Something without odor is odorless
Something without arm is armless
Something without defence is defenceless
Something without mirth is mirthless
Something without guilt is guiltless
Something without leg is
2024-07-18 05:53:32 root INFO     [order_1_approx] starting weight calculation for Something without mirth is mirthless
Something without arm is armless
Something without luck is luckless
Something without defence is defenceless
Something without guilt is guiltless
Something without odor is odorless
Something without leg is legless
Something without goal is
2024-07-18 05:53:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 05:57:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4829,  1.3848,  1.4717,  ..., -0.0791,  3.1328,  1.0967],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3047,  1.1289, -2.9414,  ...,  0.6440,  2.3301,  2.6562],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0129, -0.0172,  0.0193,  ..., -0.0071, -0.0078, -0.0247],
        [-0.0149, -0.0142, -0.0083,  ...,  0.0045, -0.0182, -0.0140],
        [ 0.0058, -0.0086, -0.0112,  ..., -0.0065, -0.0057,  0.0028],
        ...,
        [ 0.0015, -0.0146,  0.0009,  ..., -0.0286, -0.0031,  0.0108],
        [-0.0062,  0.0038,  0.0042,  ...,  0.0001, -0.0208,  0.0048],
        [-0.0125,  0.0092,  0.0138,  ...,  0.0347, -0.0003, -0.0051]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1094,  1.6172, -3.0762,  ...,  0.5825,  2.8730,  2.8535]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 05:57:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without mirth is mirthless
Something without arm is armless
Something without luck is luckless
Something without defence is defenceless
Something without guilt is guiltless
Something without odor is odorless
Something without leg is legless
Something without goal is
2024-07-18 05:57:05 root INFO     [order_1_approx] starting weight calculation for Something without mirth is mirthless
Something without guilt is guiltless
Something without defence is defenceless
Something without leg is legless
Something without luck is luckless
Something without odor is odorless
Something without goal is goalless
Something without arm is
2024-07-18 05:57:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 06:00:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9380,  1.0654,  0.4609,  ...,  0.8135, -0.0346,  1.0537],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7529,  1.7686,  1.0625,  ..., -3.0020,  2.5039,  4.0625],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0084,  0.0142,  0.0308,  ...,  0.0146, -0.0179, -0.0212],
        [-0.0121, -0.0139, -0.0217,  ..., -0.0066,  0.0029,  0.0074],
        [ 0.0199, -0.0071, -0.0226,  ..., -0.0104, -0.0015,  0.0005],
        ...,
        [ 0.0119,  0.0147,  0.0081,  ..., -0.0058, -0.0206,  0.0053],
        [-0.0014, -0.0092,  0.0012,  ..., -0.0024, -0.0112,  0.0034],
        [-0.0186,  0.0058,  0.0014,  ..., -0.0038, -0.0143, -0.0239]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.5703,  1.8896,  1.0166,  ..., -3.3926,  3.0840,  4.1211]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 06:00:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without mirth is mirthless
Something without guilt is guiltless
Something without defence is defenceless
Something without leg is legless
Something without luck is luckless
Something without odor is odorless
Something without goal is goalless
Something without arm is
2024-07-18 06:00:40 root INFO     total operator prediction time: 1710.870563030243 seconds
2024-07-18 06:00:40 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+ment_irreg
2024-07-18 06:00:40 root INFO     building operator verb+ment_irreg
2024-07-18 06:00:40 root INFO     [order_1_approx] starting weight calculation for To arrange results in a arrangement
To enroll results in a enrollment
To enhance results in a enhancement
To entertain results in a entertainment
To require results in a requirement
To amuse results in a amusement
To commit results in a commitment
To resent results in a
2024-07-18 06:00:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 06:04:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1973, -0.2179,  0.9727,  ...,  0.1979,  0.0474, -0.7759],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.2695,  2.8887, -1.0732,  ...,  5.2930, -0.5518,  4.8555],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0024,  0.0018, -0.0034,  ...,  0.0008,  0.0031, -0.0008],
        [ 0.0016, -0.0165,  0.0083,  ..., -0.0059, -0.0024,  0.0025],
        [-0.0008, -0.0079, -0.0044,  ...,  0.0002,  0.0056, -0.0024],
        ...,
        [ 0.0043, -0.0164, -0.0152,  ..., -0.0031,  0.0022, -0.0033],
        [-0.0146,  0.0138,  0.0218,  ..., -0.0096, -0.0143, -0.0024],
        [-0.0094,  0.0110,  0.0019,  ..., -0.0047, -0.0161,  0.0019]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.5527,  2.6055, -1.6621,  ...,  4.2383,  0.1929,  4.9414]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 06:04:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To arrange results in a arrangement
To enroll results in a enrollment
To enhance results in a enhancement
To entertain results in a entertainment
To require results in a requirement
To amuse results in a amusement
To commit results in a commitment
To resent results in a
2024-07-18 06:04:14 root INFO     [order_1_approx] starting weight calculation for To entertain results in a entertainment
To enhance results in a enhancement
To resent results in a resentment
To require results in a requirement
To enroll results in a enrollment
To commit results in a commitment
To amuse results in a amusement
To arrange results in a
2024-07-18 06:04:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 06:07:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0566,  0.5488,  0.1484,  ..., -0.6611, -0.1802,  0.5381],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.3574,  1.9111, -1.3613,  ...,  3.4121,  0.1913,  5.2383],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0071, -0.0043,  0.0047,  ..., -0.0104,  0.0151, -0.0032],
        [ 0.0043, -0.0031, -0.0005,  ..., -0.0130,  0.0065, -0.0059],
        [-0.0201,  0.0095, -0.0071,  ...,  0.0026,  0.0034, -0.0137],
        ...,
        [-0.0075, -0.0204, -0.0033,  ..., -0.0183,  0.0014,  0.0056],
        [ 0.0047,  0.0070,  0.0090,  ..., -0.0280, -0.0067,  0.0039],
        [-0.0207,  0.0228,  0.0100,  ..., -0.0079, -0.0089, -0.0254]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9707,  2.1523, -2.2793,  ...,  4.0469,  0.9434,  5.3672]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 06:07:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To entertain results in a entertainment
To enhance results in a enhancement
To resent results in a resentment
To require results in a requirement
To enroll results in a enrollment
To commit results in a commitment
To amuse results in a amusement
To arrange results in a
2024-07-18 06:07:49 root INFO     [order_1_approx] starting weight calculation for To enhance results in a enhancement
To commit results in a commitment
To entertain results in a entertainment
To enroll results in a enrollment
To resent results in a resentment
To arrange results in a arrangement
To amuse results in a amusement
To require results in a
2024-07-18 06:07:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 06:11:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.4333, 0.1438, 0.3621,  ..., 1.1631, 0.3789, 1.0762], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7832, -0.0503, -3.4727,  ...,  0.0093, -0.2185,  4.5078],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.1841e-02,  2.7237e-03, -8.9493e-03,  ...,  4.5090e-03,
          2.7847e-04, -1.0910e-02],
        [ 9.3079e-03,  5.9204e-03, -1.2245e-03,  ..., -4.6539e-04,
         -5.8517e-03,  4.4785e-03],
        [ 3.4180e-03, -1.2482e-02,  6.0844e-03,  ..., -1.9760e-03,
         -1.4053e-02, -3.1948e-03],
        ...,
        [-1.1169e-02, -1.4702e-02, -6.2408e-03,  ..., -3.2501e-03,
         -6.8512e-03,  2.4796e-05],
        [-1.3992e-02,  6.4697e-03, -3.0975e-03,  ..., -1.4496e-02,
         -7.9956e-03,  6.4163e-03],
        [-1.9180e-02,  1.3916e-02,  1.3596e-02,  ..., -4.3335e-03,
         -1.7929e-03,  2.0233e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4824,  0.6763, -4.1445,  ..., -0.0658, -0.1394,  4.1719]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 06:11:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To enhance results in a enhancement
To commit results in a commitment
To entertain results in a entertainment
To enroll results in a enrollment
To resent results in a resentment
To arrange results in a arrangement
To amuse results in a amusement
To require results in a
2024-07-18 06:11:23 root INFO     [order_1_approx] starting weight calculation for To enhance results in a enhancement
To amuse results in a amusement
To commit results in a commitment
To require results in a requirement
To resent results in a resentment
To arrange results in a arrangement
To entertain results in a entertainment
To enroll results in a
2024-07-18 06:11:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 06:14:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.5610, 0.7246, 0.1572,  ..., 1.1660, 0.5103, 0.6367], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.8906,  1.5557, -0.6172,  ...,  0.8223,  0.3796,  7.1289],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0149, -0.0039,  0.0008,  ..., -0.0023,  0.0074, -0.0065],
        [-0.0064,  0.0051,  0.0028,  ...,  0.0102, -0.0045,  0.0088],
        [ 0.0098,  0.0016,  0.0101,  ..., -0.0063, -0.0073, -0.0190],
        ...,
        [-0.0084, -0.0169, -0.0048,  ...,  0.0068, -0.0032, -0.0035],
        [ 0.0006,  0.0037, -0.0091,  ..., -0.0062, -0.0052,  0.0144],
        [-0.0141,  0.0156,  0.0166,  ..., -0.0054, -0.0031,  0.0056]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3223,  1.9365, -0.9258,  ...,  0.7104,  0.0808,  6.8984]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 06:14:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To enhance results in a enhancement
To amuse results in a amusement
To commit results in a commitment
To require results in a requirement
To resent results in a resentment
To arrange results in a arrangement
To entertain results in a entertainment
To enroll results in a
2024-07-18 06:14:56 root INFO     [order_1_approx] starting weight calculation for To commit results in a commitment
To arrange results in a arrangement
To require results in a requirement
To entertain results in a entertainment
To enhance results in a enhancement
To resent results in a resentment
To enroll results in a enrollment
To amuse results in a
2024-07-18 06:14:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 06:18:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.7695, 1.0264, 0.7441,  ..., 0.0925, 0.8799, 0.3477], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.2695,  3.2949, -0.6899,  ...,  0.7480, -0.3750,  7.6602],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0020, -0.0112,  0.0036,  ..., -0.0038,  0.0109, -0.0101],
        [-0.0076, -0.0072,  0.0037,  ...,  0.0050, -0.0117,  0.0092],
        [-0.0052,  0.0077, -0.0191,  ...,  0.0045,  0.0065, -0.0071],
        ...,
        [-0.0115, -0.0173, -0.0078,  ..., -0.0073, -0.0039,  0.0041],
        [ 0.0092,  0.0203, -0.0021,  ..., -0.0006, -0.0272, -0.0006],
        [-0.0055,  0.0189, -0.0067,  ..., -0.0017, -0.0018, -0.0149]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5029,  3.6719, -1.5996,  ...,  1.1162, -0.3196,  7.8281]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 06:18:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To commit results in a commitment
To arrange results in a arrangement
To require results in a requirement
To entertain results in a entertainment
To enhance results in a enhancement
To resent results in a resentment
To enroll results in a enrollment
To amuse results in a
2024-07-18 06:18:28 root INFO     [order_1_approx] starting weight calculation for To commit results in a commitment
To amuse results in a amusement
To arrange results in a arrangement
To entertain results in a entertainment
To resent results in a resentment
To enroll results in a enrollment
To require results in a requirement
To enhance results in a
2024-07-18 06:18:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 06:21:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1877,  0.8408,  1.7891,  ..., -0.2236,  0.1962, -0.5039],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.9570,  1.0859,  1.7832,  ...,  0.2661, -1.6816,  7.3555],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.6159e-02, -2.0599e-02,  8.5831e-04,  ..., -1.3000e-02,
          5.6305e-03, -1.4221e-02],
        [-6.0196e-03, -6.3362e-03, -2.6627e-03,  ..., -5.1880e-04,
         -1.6693e-02,  7.8659e-03],
        [-1.1665e-02,  9.0408e-04, -3.9482e-03,  ..., -1.8921e-03,
          7.5531e-03, -1.2939e-02],
        ...,
        [-1.6174e-02, -1.1177e-02, -5.5504e-03,  ...,  1.2665e-03,
         -4.2915e-05,  8.0109e-04],
        [-6.1874e-03,  6.7215e-03, -1.1787e-03,  ..., -3.9635e-03,
         -9.1476e-03,  1.8387e-02],
        [-8.7814e-03,  1.5427e-02,  7.0686e-03,  ...,  7.0000e-03,
          5.8632e-03,  1.3138e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1055,  0.9863,  1.5410,  ...,  0.2410, -2.2969,  7.3945]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 06:22:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To commit results in a commitment
To amuse results in a amusement
To arrange results in a arrangement
To entertain results in a entertainment
To resent results in a resentment
To enroll results in a enrollment
To require results in a requirement
To enhance results in a
2024-07-18 06:22:01 root INFO     [order_1_approx] starting weight calculation for To enroll results in a enrollment
To arrange results in a arrangement
To commit results in a commitment
To require results in a requirement
To resent results in a resentment
To amuse results in a amusement
To enhance results in a enhancement
To entertain results in a
2024-07-18 06:22:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 06:25:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6450,  0.5415,  1.3887,  ...,  0.0630,  1.9316, -0.5586],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6865,  0.7529,  3.5898,  ...,  3.1445, -0.0508,  7.8125],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0083, -0.0165,  0.0106,  ..., -0.0058, -0.0020, -0.0145],
        [-0.0142, -0.0036,  0.0068,  ...,  0.0045, -0.0116,  0.0007],
        [ 0.0019, -0.0084, -0.0069,  ..., -0.0066, -0.0008,  0.0042],
        ...,
        [-0.0206, -0.0342,  0.0025,  ...,  0.0130, -0.0167, -0.0046],
        [-0.0017,  0.0085, -0.0042,  ..., -0.0014, -0.0137,  0.0062],
        [-0.0141,  0.0283, -0.0018,  ...,  0.0034, -0.0110, -0.0210]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5972,  1.5596,  3.6484,  ...,  3.4238, -0.3877,  7.2344]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 06:25:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To enroll results in a enrollment
To arrange results in a arrangement
To commit results in a commitment
To require results in a requirement
To resent results in a resentment
To amuse results in a amusement
To enhance results in a enhancement
To entertain results in a
2024-07-18 06:25:34 root INFO     [order_1_approx] starting weight calculation for To amuse results in a amusement
To entertain results in a entertainment
To require results in a requirement
To arrange results in a arrangement
To resent results in a resentment
To enroll results in a enrollment
To enhance results in a enhancement
To commit results in a
2024-07-18 06:25:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 06:29:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5518,  1.6318,  0.7773,  ...,  1.7803,  0.2607, -0.8618],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7280, -2.4023, -2.9883,  ...,  0.9302,  0.5737,  6.1719],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0016,  0.0050,  0.0004,  ..., -0.0190,  0.0160, -0.0072],
        [-0.0051,  0.0045,  0.0007,  ...,  0.0039, -0.0069, -0.0091],
        [-0.0152,  0.0067, -0.0014,  ...,  0.0043,  0.0024, -0.0105],
        ...,
        [-0.0133, -0.0303, -0.0054,  ...,  0.0078, -0.0089,  0.0033],
        [-0.0142,  0.0030,  0.0030,  ..., -0.0173, -0.0017,  0.0008],
        [-0.0150,  0.0128,  0.0137,  ..., -0.0089, -0.0067,  0.0092]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3298, -2.3164, -3.3965,  ...,  1.0518,  0.2336,  6.2344]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 06:29:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To amuse results in a amusement
To entertain results in a entertainment
To require results in a requirement
To arrange results in a arrangement
To resent results in a resentment
To enroll results in a enrollment
To enhance results in a enhancement
To commit results in a
2024-07-18 06:29:07 root INFO     total operator prediction time: 1707.2723195552826 seconds
2024-07-18 06:29:07 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on name - nationality
2024-07-18 06:29:07 root INFO     building operator name - nationality
2024-07-18 06:29:08 root INFO     [order_1_approx] starting weight calculation for depp was american
tchaikovsky was russian
mencius was chinese
lennon was english
caesar was roman
stalin was soviet
machiavelli was italian
spinoza was
2024-07-18 06:29:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 06:32:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6396, -1.6172, -1.4648,  ..., -0.4963, -0.2415,  0.5591],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7773, -7.4297, -6.3242,  ..., -1.2002, -7.1797,  1.2188],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0170, -0.0073, -0.0053,  ..., -0.0221,  0.0043,  0.0025],
        [-0.0020,  0.0170, -0.0069,  ..., -0.0012,  0.0020,  0.0011],
        [-0.0081,  0.0049,  0.0007,  ..., -0.0097, -0.0181, -0.0028],
        ...,
        [-0.0071, -0.0110, -0.0007,  ...,  0.0236, -0.0230,  0.0027],
        [-0.0102, -0.0026, -0.0042,  ..., -0.0083,  0.0084,  0.0139],
        [-0.0123,  0.0093,  0.0097,  ..., -0.0143,  0.0014,  0.0039]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.0039, -7.1641, -5.4492,  ..., -0.5317, -6.5625,  0.6787]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 06:32:40 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for depp was american
tchaikovsky was russian
mencius was chinese
lennon was english
caesar was roman
stalin was soviet
machiavelli was italian
spinoza was
2024-07-18 06:32:40 root INFO     [order_1_approx] starting weight calculation for mencius was chinese
tchaikovsky was russian
lennon was english
spinoza was dutch
stalin was soviet
depp was american
caesar was roman
machiavelli was
2024-07-18 06:32:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 06:36:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7651, -0.7500, -0.4067,  ...,  0.3872,  0.1658,  1.0195],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4453, -2.9160, -8.1172,  ..., -2.3164, -2.7930, -2.3535],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0165, -0.0129,  0.0026,  ..., -0.0069,  0.0044,  0.0063],
        [-0.0052,  0.0194, -0.0001,  ...,  0.0075, -0.0059,  0.0016],
        [-0.0072, -0.0043,  0.0145,  ..., -0.0211, -0.0182,  0.0019],
        ...,
        [-0.0081, -0.0006,  0.0053,  ...,  0.0236, -0.0062, -0.0009],
        [-0.0036,  0.0007, -0.0047,  ..., -0.0057,  0.0234,  0.0053],
        [-0.0087,  0.0062,  0.0096,  ..., -0.0151,  0.0020,  0.0070]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1992, -2.9453, -7.5352,  ..., -2.7969, -2.4746, -1.7861]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 06:36:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for mencius was chinese
tchaikovsky was russian
lennon was english
spinoza was dutch
stalin was soviet
depp was american
caesar was roman
machiavelli was
2024-07-18 06:36:14 root INFO     [order_1_approx] starting weight calculation for depp was american
spinoza was dutch
stalin was soviet
machiavelli was italian
lennon was english
caesar was roman
tchaikovsky was russian
mencius was
2024-07-18 06:36:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 06:39:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.7490, -0.2449, -1.4014,  ...,  0.6455, -0.5225,  3.0391],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4570, -4.4922, -6.3125,  ..., -2.7305, -1.5303, -2.6016],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0306, -0.0032, -0.0098,  ..., -0.0206,  0.0019, -0.0098],
        [ 0.0015,  0.0138,  0.0052,  ...,  0.0049, -0.0097,  0.0098],
        [ 0.0013, -0.0056,  0.0190,  ..., -0.0115, -0.0130, -0.0141],
        ...,
        [-0.0137,  0.0039, -0.0010,  ...,  0.0306, -0.0083, -0.0065],
        [-0.0015, -0.0105, -0.0025,  ..., -0.0044,  0.0211,  0.0019],
        [-0.0075, -0.0018, -0.0068,  ..., -0.0121,  0.0060,  0.0120]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3887, -5.1055, -6.3281,  ..., -3.0176, -1.0820, -3.0977]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 06:39:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for depp was american
spinoza was dutch
stalin was soviet
machiavelli was italian
lennon was english
caesar was roman
tchaikovsky was russian
mencius was
2024-07-18 06:39:47 root INFO     [order_1_approx] starting weight calculation for tchaikovsky was russian
caesar was roman
spinoza was dutch
stalin was soviet
machiavelli was italian
lennon was english
mencius was chinese
depp was
2024-07-18 06:39:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 06:43:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.9668,  1.3115, -0.4995,  ...,  1.0029, -0.5488, -0.7646],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1074, -1.6113, -3.7773,  ..., -4.0039, -2.7910, -3.1641],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0062, -0.0031, -0.0083,  ..., -0.0040,  0.0089,  0.0034],
        [-0.0024,  0.0154,  0.0045,  ..., -0.0058, -0.0060,  0.0114],
        [ 0.0041, -0.0238,  0.0081,  ...,  0.0014,  0.0102,  0.0054],
        ...,
        [-0.0187, -0.0095, -0.0038,  ...,  0.0093, -0.0028, -0.0134],
        [ 0.0010, -0.0037,  0.0087,  ...,  0.0094,  0.0225, -0.0070],
        [-0.0011, -0.0071,  0.0006,  ..., -0.0007, -0.0015,  0.0229]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4307, -1.9717, -3.8555,  ..., -3.7949, -2.6250, -3.3594]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 06:43:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for tchaikovsky was russian
caesar was roman
spinoza was dutch
stalin was soviet
machiavelli was italian
lennon was english
mencius was chinese
depp was
2024-07-18 06:43:21 root INFO     [order_1_approx] starting weight calculation for machiavelli was italian
caesar was roman
spinoza was dutch
tchaikovsky was russian
mencius was chinese
depp was american
lennon was english
stalin was
2024-07-18 06:43:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 06:46:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2153,  1.2285,  0.0100,  ..., -0.1438,  0.3584,  0.3569],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7383,  0.7881, -3.9531,  ..., -7.0859, -3.8945,  3.3867],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0083, -0.0172, -0.0030,  ..., -0.0131,  0.0154, -0.0046],
        [ 0.0028,  0.0262,  0.0130,  ...,  0.0163, -0.0022,  0.0025],
        [ 0.0027, -0.0032, -0.0017,  ..., -0.0130, -0.0167, -0.0044],
        ...,
        [-0.0082,  0.0044, -0.0124,  ...,  0.0140, -0.0142, -0.0072],
        [ 0.0071, -0.0127,  0.0011,  ..., -0.0013, -0.0017, -0.0039],
        [-0.0068, -0.0103,  0.0171,  ..., -0.0234,  0.0123,  0.0104]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6191,  1.0361, -4.2539,  ..., -7.0352, -3.5234,  3.8281]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 06:46:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for machiavelli was italian
caesar was roman
spinoza was dutch
tchaikovsky was russian
mencius was chinese
depp was american
lennon was english
stalin was
2024-07-18 06:46:55 root INFO     [order_1_approx] starting weight calculation for machiavelli was italian
caesar was roman
spinoza was dutch
depp was american
stalin was soviet
mencius was chinese
tchaikovsky was russian
lennon was
2024-07-18 06:46:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 06:50:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9507,  2.2578, -0.1716,  ...,  1.3135,  0.8071,  0.3516],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3438, -0.2612, -7.8984,  ...,  0.3047,  1.2480, -5.4453],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.2583e-02, -9.7656e-03,  4.5128e-03,  ..., -7.9193e-03,
         -6.7520e-03, -2.9907e-03],
        [-7.7095e-03,  2.2369e-02,  3.0518e-05,  ...,  1.0216e-02,
          1.9855e-03,  2.3842e-03],
        [ 1.1520e-03, -6.3629e-03, -1.7242e-03,  ..., -1.5717e-02,
         -2.9488e-03,  2.4509e-03],
        ...,
        [-2.8458e-03, -2.0355e-02,  7.8735e-03,  ...,  2.1729e-02,
         -4.0894e-03, -1.7303e-02],
        [ 5.5885e-04, -2.7199e-03,  4.7112e-04,  ...,  4.4785e-03,
          2.0813e-02,  3.0994e-03],
        [-1.5488e-03, -3.5477e-03, -5.5389e-03,  ..., -1.3481e-02,
          1.1322e-02, -1.5163e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4072, -0.2830, -7.6328,  ...,  0.6333,  1.2734, -5.0195]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 06:50:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for machiavelli was italian
caesar was roman
spinoza was dutch
depp was american
stalin was soviet
mencius was chinese
tchaikovsky was russian
lennon was
2024-07-18 06:50:29 root INFO     [order_1_approx] starting weight calculation for mencius was chinese
depp was american
spinoza was dutch
lennon was english
stalin was soviet
machiavelli was italian
caesar was roman
tchaikovsky was
2024-07-18 06:50:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 06:54:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2603, -0.2617,  1.6289,  ...,  0.3040,  1.2070,  1.3418],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7188, -3.3164, -4.3516,  ..., -4.3984,  0.5449, -3.8223],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0245, -0.0129, -0.0031,  ..., -0.0054, -0.0021, -0.0047],
        [ 0.0145,  0.0232, -0.0141,  ...,  0.0013, -0.0057,  0.0135],
        [ 0.0024,  0.0056,  0.0112,  ..., -0.0186, -0.0139,  0.0017],
        ...,
        [-0.0119,  0.0066, -0.0041,  ...,  0.0293, -0.0248,  0.0042],
        [-0.0091, -0.0026, -0.0026,  ..., -0.0004,  0.0256,  0.0213],
        [-0.0133,  0.0095,  0.0066,  ..., -0.0275, -0.0106,  0.0028]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0488, -3.3086, -4.2773,  ..., -4.6641,  0.9580, -4.3086]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 06:54:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for mencius was chinese
depp was american
spinoza was dutch
lennon was english
stalin was soviet
machiavelli was italian
caesar was roman
tchaikovsky was
2024-07-18 06:54:03 root INFO     [order_1_approx] starting weight calculation for depp was american
machiavelli was italian
spinoza was dutch
stalin was soviet
lennon was english
tchaikovsky was russian
mencius was chinese
caesar was
2024-07-18 06:54:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 06:57:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0000, -1.5186, -0.8989,  ...,  1.0820, -0.4268,  1.2119],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3164, -0.2495, -8.0703,  ..., -0.7861, -2.7109, -4.8359],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0256, -0.0116, -0.0020,  ..., -0.0111,  0.0060,  0.0013],
        [-0.0005,  0.0148,  0.0143,  ...,  0.0110, -0.0002,  0.0109],
        [ 0.0059,  0.0033,  0.0038,  ..., -0.0100, -0.0093,  0.0025],
        ...,
        [-0.0067, -0.0011, -0.0041,  ...,  0.0152, -0.0078,  0.0015],
        [-0.0020, -0.0027, -0.0090,  ..., -0.0094,  0.0064,  0.0059],
        [-0.0079,  0.0009, -0.0192,  ..., -0.0118,  0.0073,  0.0072]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0823,  0.3711, -7.7695,  ..., -0.9600, -2.8203, -4.9062]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 06:57:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for depp was american
machiavelli was italian
spinoza was dutch
stalin was soviet
lennon was english
tchaikovsky was russian
mencius was chinese
caesar was
2024-07-18 06:57:35 root INFO     total operator prediction time: 1707.9303600788116 seconds
2024-07-18 06:57:35 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on country - language
2024-07-18 06:57:35 root INFO     building operator country - language
2024-07-18 06:57:36 root INFO     [order_1_approx] starting weight calculation for The country of taiwan primarily speaks the language of chinese
The country of mexico primarily speaks the language of spanish
The country of denmark primarily speaks the language of danish
The country of guatemala primarily speaks the language of spanish
The country of bangladesh primarily speaks the language of bengali
The country of belize primarily speaks the language of english
The country of ethiopia primarily speaks the language of amharic
The country of chile primarily speaks the language of
2024-07-18 06:57:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 07:01:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4666, -0.9868, -0.6934,  ..., -0.5874,  2.7363, -0.6816],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3203, -1.5273, -3.5000,  ..., -2.2852,  1.3203, -3.3945],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 5.7220e-04, -2.6398e-03,  3.7117e-03,  ..., -3.4790e-03,
         -2.9163e-03,  7.9727e-04],
        [ 4.4250e-03,  1.9836e-03,  2.0721e-02,  ...,  2.3499e-02,
          6.3057e-03, -5.8746e-04],
        [-4.2915e-03,  2.5654e-03, -1.8585e-02,  ..., -4.0588e-03,
          1.0269e-02,  3.2883e-03],
        ...,
        [-2.3918e-03, -1.4435e-02,  5.3024e-03,  ...,  1.6068e-02,
          4.3373e-03,  9.3842e-04],
        [ 5.3406e-05,  9.3460e-05, -1.1387e-03,  ...,  4.6234e-03,
          3.4142e-04, -5.9700e-03],
        [-1.6144e-02, -1.1082e-03,  2.0065e-03,  ...,  6.6071e-03,
          1.2207e-02, -9.0790e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3066, -0.9805, -3.9414,  ..., -1.9346,  1.0791, -3.0332]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 07:01:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of taiwan primarily speaks the language of chinese
The country of mexico primarily speaks the language of spanish
The country of denmark primarily speaks the language of danish
The country of guatemala primarily speaks the language of spanish
The country of bangladesh primarily speaks the language of bengali
The country of belize primarily speaks the language of english
The country of ethiopia primarily speaks the language of amharic
The country of chile primarily speaks the language of
2024-07-18 07:01:08 root INFO     [order_1_approx] starting weight calculation for The country of denmark primarily speaks the language of danish
The country of belize primarily speaks the language of english
The country of bangladesh primarily speaks the language of bengali
The country of mexico primarily speaks the language of spanish
The country of chile primarily speaks the language of spanish
The country of taiwan primarily speaks the language of chinese
The country of guatemala primarily speaks the language of spanish
The country of ethiopia primarily speaks the language of
2024-07-18 07:01:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 07:04:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3530, -1.2451,  0.6665,  ..., -0.0616,  0.0962, -0.2488],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 7.8828, -0.8306, -6.5391,  ...,  0.7754, -1.6982, -3.5527],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 9.8419e-04, -7.4310e-03,  1.0414e-03,  ...,  9.0218e-04,
          6.8283e-04, -9.0218e-04],
        [-2.0409e-04,  3.7003e-04,  1.0727e-02,  ...,  1.3794e-02,
          6.2981e-03, -1.4496e-03],
        [ 3.2082e-03, -7.2575e-04, -1.8570e-02,  ..., -7.7362e-03,
         -1.3771e-02,  4.9858e-03],
        ...,
        [-1.1971e-02, -8.8196e-03,  3.0060e-03,  ...,  3.9978e-03,
         -7.1564e-03,  1.2283e-03],
        [-1.0443e-03, -6.5651e-03,  3.5515e-03,  ..., -5.9357e-03,
          8.4991e-03,  3.6850e-03],
        [-3.2425e-05, -3.8452e-03, -1.3458e-02,  ..., -1.5717e-02,
         -7.4081e-03,  8.3694e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 8.3359,  0.4058, -7.5117,  ...,  0.8403, -1.7705, -4.9375]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 07:04:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of denmark primarily speaks the language of danish
The country of belize primarily speaks the language of english
The country of bangladesh primarily speaks the language of bengali
The country of mexico primarily speaks the language of spanish
The country of chile primarily speaks the language of spanish
The country of taiwan primarily speaks the language of chinese
The country of guatemala primarily speaks the language of spanish
The country of ethiopia primarily speaks the language of
2024-07-18 07:04:42 root INFO     [order_1_approx] starting weight calculation for The country of denmark primarily speaks the language of danish
The country of mexico primarily speaks the language of spanish
The country of bangladesh primarily speaks the language of bengali
The country of ethiopia primarily speaks the language of amharic
The country of chile primarily speaks the language of spanish
The country of guatemala primarily speaks the language of spanish
The country of taiwan primarily speaks the language of chinese
The country of belize primarily speaks the language of
2024-07-18 07:04:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 07:08:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4995, -0.4131, -1.2070,  ...,  0.2218,  0.3237,  0.2085],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7139, -3.9082, -7.3828,  ...,  0.4834,  2.8418, -1.6953],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0025, -0.0085,  0.0018,  ..., -0.0037, -0.0007, -0.0039],
        [-0.0032,  0.0032,  0.0052,  ...,  0.0045,  0.0091,  0.0033],
        [ 0.0005,  0.0103, -0.0037,  ..., -0.0039,  0.0124, -0.0024],
        ...,
        [-0.0054, -0.0068,  0.0017,  ...,  0.0080, -0.0068, -0.0034],
        [-0.0013, -0.0049, -0.0042,  ...,  0.0038,  0.0052, -0.0026],
        [-0.0075, -0.0018,  0.0041,  ..., -0.0022,  0.0102,  0.0086]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9277, -3.5781, -6.8359,  ...,  0.8779,  2.8848, -1.4922]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 07:08:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of denmark primarily speaks the language of danish
The country of mexico primarily speaks the language of spanish
The country of bangladesh primarily speaks the language of bengali
The country of ethiopia primarily speaks the language of amharic
The country of chile primarily speaks the language of spanish
The country of guatemala primarily speaks the language of spanish
The country of taiwan primarily speaks the language of chinese
The country of belize primarily speaks the language of
2024-07-18 07:08:16 root INFO     [order_1_approx] starting weight calculation for The country of taiwan primarily speaks the language of chinese
The country of bangladesh primarily speaks the language of bengali
The country of chile primarily speaks the language of spanish
The country of guatemala primarily speaks the language of spanish
The country of denmark primarily speaks the language of danish
The country of belize primarily speaks the language of english
The country of ethiopia primarily speaks the language of amharic
The country of mexico primarily speaks the language of
2024-07-18 07:08:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 07:11:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4963, -0.0806,  0.0181,  ..., -1.1768,  1.2871,  0.1221],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1934, -1.2061, -3.5742,  ..., -4.2461, -0.9141, -1.9453],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.5700e-03,  1.8167e-03, -1.9398e-03,  ...,  1.3084e-03,
         -9.5291e-03,  2.1210e-03],
        [ 3.2120e-03,  1.8444e-03,  1.9760e-03,  ...,  1.1749e-02,
          1.8034e-03,  5.5199e-03],
        [-1.1185e-02,  2.9888e-03, -7.1030e-03,  ..., -8.5449e-03,
          1.3351e-05,  6.2180e-03],
        ...,
        [-4.2572e-03, -2.5806e-03, -4.3793e-03,  ...,  5.6915e-03,
         -6.9504e-03,  3.9520e-03],
        [-1.8406e-03,  1.7509e-03, -4.3297e-04,  ..., -5.5008e-03,
         -5.9776e-03,  5.8699e-04],
        [-4.5729e-04, -7.6752e-03,  7.1144e-04,  ..., -4.5471e-03,
         -1.9503e-04, -4.6158e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3726, -0.9590, -3.1445,  ..., -4.2578, -0.7681, -1.7764]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 07:11:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of taiwan primarily speaks the language of chinese
The country of bangladesh primarily speaks the language of bengali
The country of chile primarily speaks the language of spanish
The country of guatemala primarily speaks the language of spanish
The country of denmark primarily speaks the language of danish
The country of belize primarily speaks the language of english
The country of ethiopia primarily speaks the language of amharic
The country of mexico primarily speaks the language of
2024-07-18 07:11:51 root INFO     [order_1_approx] starting weight calculation for The country of taiwan primarily speaks the language of chinese
The country of ethiopia primarily speaks the language of amharic
The country of denmark primarily speaks the language of danish
The country of belize primarily speaks the language of english
The country of bangladesh primarily speaks the language of bengali
The country of chile primarily speaks the language of spanish
The country of mexico primarily speaks the language of spanish
The country of guatemala primarily speaks the language of
2024-07-18 07:11:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 07:15:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2186, -0.6997, -1.5693,  ...,  1.2949,  0.5088,  0.5918],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5840, -0.5869, -4.8867,  ...,  0.5923,  0.7754, -2.4609],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0142, -0.0084,  0.0053,  ...,  0.0004, -0.0010,  0.0009],
        [ 0.0072, -0.0035,  0.0014,  ...,  0.0156,  0.0119,  0.0023],
        [-0.0012,  0.0126, -0.0183,  ..., -0.0294, -0.0073,  0.0077],
        ...,
        [-0.0125,  0.0002, -0.0050,  ...,  0.0074, -0.0028,  0.0010],
        [-0.0004, -0.0024, -0.0038,  ...,  0.0063,  0.0002, -0.0004],
        [-0.0100, -0.0017,  0.0034,  ...,  0.0002,  0.0011,  0.0078]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4727,  0.2700, -6.0703,  ...,  0.1462,  0.5908, -2.3770]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 07:15:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of taiwan primarily speaks the language of chinese
The country of ethiopia primarily speaks the language of amharic
The country of denmark primarily speaks the language of danish
The country of belize primarily speaks the language of english
The country of bangladesh primarily speaks the language of bengali
The country of chile primarily speaks the language of spanish
The country of mexico primarily speaks the language of spanish
The country of guatemala primarily speaks the language of
2024-07-18 07:15:26 root INFO     [order_1_approx] starting weight calculation for The country of taiwan primarily speaks the language of chinese
The country of mexico primarily speaks the language of spanish
The country of belize primarily speaks the language of english
The country of chile primarily speaks the language of spanish
The country of ethiopia primarily speaks the language of amharic
The country of bangladesh primarily speaks the language of bengali
The country of guatemala primarily speaks the language of spanish
The country of denmark primarily speaks the language of
2024-07-18 07:15:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 07:18:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5664,  0.3721, -0.1251,  ..., -0.3794, -1.2285,  1.0518],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.1133, -4.5195, -2.2246,  ..., -6.0508, -1.3398,  1.4219],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0007, -0.0023,  0.0036,  ..., -0.0025, -0.0042, -0.0043],
        [ 0.0079,  0.0074, -0.0068,  ...,  0.0086,  0.0106, -0.0022],
        [-0.0022,  0.0019, -0.0034,  ..., -0.0018,  0.0012,  0.0068],
        ...,
        [ 0.0061, -0.0070, -0.0048,  ...,  0.0013, -0.0126, -0.0005],
        [-0.0035, -0.0076, -0.0023,  ...,  0.0077, -0.0085,  0.0117],
        [-0.0114, -0.0025,  0.0067,  ..., -0.0053,  0.0077,  0.0060]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.5840, -4.5000, -2.1250,  ..., -6.6875, -1.3828,  1.9355]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 07:19:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of taiwan primarily speaks the language of chinese
The country of mexico primarily speaks the language of spanish
The country of belize primarily speaks the language of english
The country of chile primarily speaks the language of spanish
The country of ethiopia primarily speaks the language of amharic
The country of bangladesh primarily speaks the language of bengali
The country of guatemala primarily speaks the language of spanish
The country of denmark primarily speaks the language of
2024-07-18 07:19:00 root INFO     [order_1_approx] starting weight calculation for The country of ethiopia primarily speaks the language of amharic
The country of guatemala primarily speaks the language of spanish
The country of mexico primarily speaks the language of spanish
The country of denmark primarily speaks the language of danish
The country of chile primarily speaks the language of spanish
The country of bangladesh primarily speaks the language of bengali
The country of belize primarily speaks the language of english
The country of taiwan primarily speaks the language of
2024-07-18 07:19:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 07:22:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.5723,  0.3723, -0.9404,  ..., -0.0785,  0.2874, -0.9629],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.1367,  0.0981, -3.9082,  ..., -5.3047,  3.1738, -1.5586],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-4.1656e-03, -1.0605e-02,  7.2632e-03,  ...,  9.6817e-03,
          1.0452e-03, -1.7426e-02],
        [ 2.3060e-03, -6.5460e-03,  1.1581e-02,  ...,  1.3878e-02,
          6.1798e-03, -6.3324e-04],
        [ 3.5706e-03,  8.2245e-03, -1.3618e-02,  ..., -1.1963e-02,
          6.9351e-03, -1.3733e-03],
        ...,
        [ 4.7874e-03,  1.0157e-03, -4.8447e-03,  ..., -1.1887e-02,
         -1.4832e-02, -4.5776e-05],
        [-9.0179e-03, -3.6297e-03, -4.3716e-03,  ...,  3.8567e-03,
         -4.0359e-03,  8.6670e-03],
        [ 7.5531e-04, -6.8932e-03,  4.2343e-03,  ..., -2.0123e-03,
          8.3618e-03, -4.6768e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.8223, -0.3323, -3.4746,  ..., -5.3594,  2.3535, -1.2783]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 07:22:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of ethiopia primarily speaks the language of amharic
The country of guatemala primarily speaks the language of spanish
The country of mexico primarily speaks the language of spanish
The country of denmark primarily speaks the language of danish
The country of chile primarily speaks the language of spanish
The country of bangladesh primarily speaks the language of bengali
The country of belize primarily speaks the language of english
The country of taiwan primarily speaks the language of
2024-07-18 07:22:33 root INFO     [order_1_approx] starting weight calculation for The country of taiwan primarily speaks the language of chinese
The country of guatemala primarily speaks the language of spanish
The country of chile primarily speaks the language of spanish
The country of denmark primarily speaks the language of danish
The country of mexico primarily speaks the language of spanish
The country of belize primarily speaks the language of english
The country of ethiopia primarily speaks the language of amharic
The country of bangladesh primarily speaks the language of
2024-07-18 07:22:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 07:26:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6538, -0.6553,  0.2290,  ...,  0.8896,  0.8164, -0.5352],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.5371, -2.1699, -6.0000,  ..., -1.4414,  2.9961, -0.0479],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0027,  0.0028,  0.0006,  ...,  0.0060, -0.0057, -0.0074],
        [ 0.0108,  0.0038,  0.0019,  ...,  0.0114,  0.0160,  0.0041],
        [-0.0087,  0.0013,  0.0022,  ..., -0.0162, -0.0163, -0.0070],
        ...,
        [-0.0019, -0.0076, -0.0026,  ...,  0.0057, -0.0093,  0.0127],
        [-0.0044, -0.0055, -0.0026,  ...,  0.0019, -0.0052,  0.0047],
        [-0.0114,  0.0091, -0.0037,  ..., -0.0159,  0.0017,  0.0008]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4619, -1.9805, -5.2070,  ..., -1.3750,  2.8594,  0.3533]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 07:26:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of taiwan primarily speaks the language of chinese
The country of guatemala primarily speaks the language of spanish
The country of chile primarily speaks the language of spanish
The country of denmark primarily speaks the language of danish
The country of mexico primarily speaks the language of spanish
The country of belize primarily speaks the language of english
The country of ethiopia primarily speaks the language of amharic
The country of bangladesh primarily speaks the language of
2024-07-18 07:26:06 root INFO     total operator prediction time: 1710.5522646903992 seconds
2024-07-18 07:26:06 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on animal - shelter
2024-07-18 07:26:06 root INFO     building operator animal - shelter
2024-07-18 07:26:06 root INFO     [order_1_approx] starting weight calculation for The place termite lives in is called hill
The place fox lives in is called den
The place snake lives in is called nest
The place raven lives in is called nest
The place pig lives in is called sty
The place horse lives in is called stable
The place ape lives in is called grove
The place herring lives in is called
2024-07-18 07:26:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 07:29:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0193,  1.2871,  0.5557,  ...,  0.3591, -0.2998,  2.0312],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.9082,  0.5039, -1.1758,  ...,  0.4395,  1.1377, -1.6934],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0077, -0.0079,  0.0033,  ...,  0.0019, -0.0048, -0.0015],
        [ 0.0011,  0.0114,  0.0028,  ...,  0.0157,  0.0041, -0.0009],
        [-0.0014, -0.0021, -0.0033,  ..., -0.0102,  0.0031,  0.0078],
        ...,
        [-0.0042, -0.0109, -0.0150,  ...,  0.0009, -0.0042,  0.0063],
        [-0.0010, -0.0088,  0.0051,  ..., -0.0039,  0.0034,  0.0042],
        [-0.0056, -0.0048, -0.0120,  ..., -0.0056,  0.0032, -0.0023]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.0332,  0.2886, -1.0098,  ...,  0.6123,  1.0068, -1.4434]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 07:29:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place termite lives in is called hill
The place fox lives in is called den
The place snake lives in is called nest
The place raven lives in is called nest
The place pig lives in is called sty
The place horse lives in is called stable
The place ape lives in is called grove
The place herring lives in is called
2024-07-18 07:29:42 root INFO     [order_1_approx] starting weight calculation for The place horse lives in is called stable
The place raven lives in is called nest
The place herring lives in is called sea
The place termite lives in is called hill
The place pig lives in is called sty
The place snake lives in is called nest
The place fox lives in is called den
The place ape lives in is called
2024-07-18 07:29:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 07:33:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0151, -1.0615, -1.1025,  ...,  0.3291, -0.7842,  1.0098],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.0371, -3.0000, -1.2178,  ..., -4.5547,  0.8916,  1.5117],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0018, -0.0223,  0.0066,  ...,  0.0202, -0.0189,  0.0092],
        [ 0.0008,  0.0156,  0.0022,  ...,  0.0176, -0.0003,  0.0152],
        [-0.0025, -0.0044,  0.0005,  ...,  0.0017,  0.0136,  0.0040],
        ...,
        [ 0.0049, -0.0135, -0.0020,  ...,  0.0118, -0.0069, -0.0010],
        [-0.0069, -0.0018,  0.0003,  ...,  0.0036,  0.0024, -0.0015],
        [-0.0080, -0.0017,  0.0063,  ..., -0.0060,  0.0051,  0.0126]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.0039, -3.2188, -1.1865,  ..., -3.9375,  0.9082,  1.4287]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 07:33:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place horse lives in is called stable
The place raven lives in is called nest
The place herring lives in is called sea
The place termite lives in is called hill
The place pig lives in is called sty
The place snake lives in is called nest
The place fox lives in is called den
The place ape lives in is called
2024-07-18 07:33:14 root INFO     [order_1_approx] starting weight calculation for The place horse lives in is called stable
The place herring lives in is called sea
The place pig lives in is called sty
The place raven lives in is called nest
The place ape lives in is called grove
The place fox lives in is called den
The place termite lives in is called hill
The place snake lives in is called
2024-07-18 07:33:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 07:36:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5986,  0.8843, -0.4121,  ...,  0.6211, -0.4778,  0.8101],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.1328, -2.2285, -0.1045,  ..., -1.8184,  2.6816, -2.2812],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0307, -0.0076, -0.0060,  ..., -0.0017,  0.0077,  0.0014],
        [ 0.0052,  0.0189,  0.0011,  ...,  0.0053,  0.0026, -0.0080],
        [ 0.0097, -0.0096,  0.0137,  ..., -0.0033,  0.0235,  0.0131],
        ...,
        [-0.0012, -0.0022,  0.0019,  ...,  0.0291,  0.0017,  0.0062],
        [-0.0140, -0.0040,  0.0169,  ...,  0.0101, -0.0014,  0.0126],
        [-0.0055, -0.0026, -0.0034,  ..., -0.0081,  0.0035,  0.0202]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.9316, -1.9697, -0.4336,  ..., -2.2812,  2.6855, -2.4590]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 07:36:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place horse lives in is called stable
The place herring lives in is called sea
The place pig lives in is called sty
The place raven lives in is called nest
The place ape lives in is called grove
The place fox lives in is called den
The place termite lives in is called hill
The place snake lives in is called
2024-07-18 07:36:49 root INFO     [order_1_approx] starting weight calculation for The place horse lives in is called stable
The place raven lives in is called nest
The place ape lives in is called grove
The place herring lives in is called sea
The place snake lives in is called nest
The place pig lives in is called sty
The place termite lives in is called hill
The place fox lives in is called
2024-07-18 07:36:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 07:40:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7236, -0.1384, -0.1450,  ...,  0.9912, -0.7139,  1.2334],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9561, -3.6445, -3.4492,  ..., -4.9297,  0.7861,  1.8936],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.0132e-02, -1.9485e-02, -6.0310e-03,  ..., -1.6937e-03,
          1.8578e-03,  5.6305e-03],
        [-2.7847e-03,  1.9318e-02, -9.8228e-04,  ...,  3.9101e-03,
          6.4240e-03, -2.6703e-05],
        [-8.0109e-03,  1.3161e-04,  9.7580e-03,  ..., -1.0376e-02,
          2.1713e-02, -4.1695e-03],
        ...,
        [-7.4425e-03, -1.3245e-02,  1.8454e-03,  ...,  1.0979e-02,
         -6.5994e-03,  1.6117e-03],
        [-1.3504e-02,  2.7370e-03,  3.1986e-03,  ...,  9.3460e-04,
          2.0733e-03,  4.1962e-03],
        [-6.4316e-03,  3.6926e-03,  1.8349e-03,  ..., -8.1406e-03,
          4.6234e-03,  1.0101e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7559, -3.7773, -2.8398,  ..., -5.2422,  0.4717,  1.5654]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 07:40:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place horse lives in is called stable
The place raven lives in is called nest
The place ape lives in is called grove
The place herring lives in is called sea
The place snake lives in is called nest
The place pig lives in is called sty
The place termite lives in is called hill
The place fox lives in is called
2024-07-18 07:40:23 root INFO     [order_1_approx] starting weight calculation for The place herring lives in is called sea
The place horse lives in is called stable
The place pig lives in is called sty
The place termite lives in is called hill
The place snake lives in is called nest
The place ape lives in is called grove
The place fox lives in is called den
The place raven lives in is called
2024-07-18 07:40:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 07:43:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7129, -0.7417, -0.3662,  ...,  0.4893, -0.3281, -0.2915],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.8379, -0.7612, -0.3091,  ..., -1.8262, -0.8979, -1.6357],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.4503e-02, -2.3514e-02, -1.8539e-03,  ...,  8.3771e-03,
         -4.0970e-03,  3.8147e-06],
        [ 1.2255e-03,  2.6428e-02,  1.3618e-02,  ...,  5.1537e-03,
          6.0349e-03,  3.3493e-03],
        [-5.5923e-03, -2.4681e-03, -8.6975e-04,  ...,  4.1351e-03,
          1.0963e-02, -8.9874e-03],
        ...,
        [-1.1406e-03, -9.3079e-04,  7.6332e-03,  ...,  2.6581e-02,
         -4.6730e-04,  2.1027e-02],
        [-1.0941e-02, -1.1719e-02, -2.4929e-03,  ...,  5.2376e-03,
          4.6959e-03,  2.5082e-03],
        [-1.0963e-02, -9.2163e-03, -3.2539e-03,  ..., -6.7635e-03,
          6.3972e-03,  1.4069e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7988, -0.9453,  0.4312,  ..., -1.4023, -0.7998, -1.5918]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 07:43:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place herring lives in is called sea
The place horse lives in is called stable
The place pig lives in is called sty
The place termite lives in is called hill
The place snake lives in is called nest
The place ape lives in is called grove
The place fox lives in is called den
The place raven lives in is called
2024-07-18 07:43:55 root INFO     [order_1_approx] starting weight calculation for The place termite lives in is called hill
The place herring lives in is called sea
The place raven lives in is called nest
The place ape lives in is called grove
The place snake lives in is called nest
The place pig lives in is called sty
The place fox lives in is called den
The place horse lives in is called
2024-07-18 07:43:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 07:47:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1609, -0.2328, -0.0933,  ...,  0.4412, -0.9692,  1.0205],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6089, -6.1367, -2.8652,  ..., -5.7617, -1.0488,  0.8701],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.3817e-02, -2.0798e-02,  3.1357e-03,  ..., -3.1357e-03,
         -1.0231e-02, -7.0915e-03],
        [-3.0899e-03,  1.8738e-02, -2.0862e-04,  ...,  6.6566e-03,
          1.5411e-02, -1.0300e-02],
        [-4.6158e-03, -6.5613e-03,  1.3145e-02,  ..., -9.6970e-03,
          9.7961e-03, -1.7242e-03],
        ...,
        [-3.4924e-03, -2.8496e-03, -7.4310e-03,  ...,  2.6131e-03,
         -5.5790e-05,  6.7978e-03],
        [-5.7602e-03, -4.1275e-03,  3.1414e-03,  ...,  4.9591e-04,
         -1.8291e-03,  1.4282e-02],
        [-4.2801e-03, -5.4474e-03, -4.1428e-03,  ..., -5.1880e-03,
         -3.9139e-03,  7.8125e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9131, -5.9414, -3.6836,  ..., -5.5273, -1.6523,  1.3633]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 07:47:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place termite lives in is called hill
The place herring lives in is called sea
The place raven lives in is called nest
The place ape lives in is called grove
The place snake lives in is called nest
The place pig lives in is called sty
The place fox lives in is called den
The place horse lives in is called
2024-07-18 07:47:28 root INFO     [order_1_approx] starting weight calculation for The place raven lives in is called nest
The place fox lives in is called den
The place horse lives in is called stable
The place ape lives in is called grove
The place snake lives in is called nest
The place herring lives in is called sea
The place pig lives in is called sty
The place termite lives in is called
2024-07-18 07:47:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 07:51:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4980, -0.6821,  1.3965,  ...,  0.0836, -0.1045,  0.2939],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.5400e-03, -3.7578e+00,  3.5742e-01,  ..., -5.0508e+00,
         2.3984e+00,  8.4473e-01], device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0003, -0.0008, -0.0003,  ...,  0.0010, -0.0028,  0.0048],
        [ 0.0022,  0.0059,  0.0016,  ...,  0.0026,  0.0017,  0.0052],
        [-0.0063,  0.0003,  0.0040,  ...,  0.0010,  0.0074,  0.0083],
        ...,
        [-0.0073, -0.0110, -0.0013,  ...,  0.0102,  0.0042,  0.0103],
        [-0.0026,  0.0028,  0.0053,  ...,  0.0037,  0.0019,  0.0029],
        [-0.0019, -0.0038,  0.0018,  ..., -0.0002,  0.0051,  0.0119]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0156, -3.9688, -0.1792,  ..., -5.0234,  2.2520,  0.8457]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 07:51:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place raven lives in is called nest
The place fox lives in is called den
The place horse lives in is called stable
The place ape lives in is called grove
The place snake lives in is called nest
The place herring lives in is called sea
The place pig lives in is called sty
The place termite lives in is called
2024-07-18 07:51:03 root INFO     [order_1_approx] starting weight calculation for The place ape lives in is called grove
The place horse lives in is called stable
The place fox lives in is called den
The place snake lives in is called nest
The place raven lives in is called nest
The place herring lives in is called sea
The place termite lives in is called hill
The place pig lives in is called
2024-07-18 07:51:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 07:54:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.4004,  0.7109, -0.3662,  ..., -0.0605, -1.0732,  1.8184],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.2812, -4.9492, -2.5391,  ..., -3.8145,  0.3694,  0.5195],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0012, -0.0165, -0.0029,  ...,  0.0058,  0.0066,  0.0106],
        [-0.0075,  0.0207,  0.0103,  ..., -0.0057,  0.0092, -0.0013],
        [-0.0024,  0.0090,  0.0017,  ..., -0.0115,  0.0179, -0.0053],
        ...,
        [-0.0008,  0.0003, -0.0088,  ..., -0.0013,  0.0007,  0.0037],
        [-0.0158, -0.0034,  0.0112,  ...,  0.0021, -0.0045, -0.0031],
        [-0.0058, -0.0045,  0.0047,  ..., -0.0081, -0.0008,  0.0050]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5693, -3.7656, -2.4277,  ..., -3.1055,  0.1576, -0.4023]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 07:54:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place ape lives in is called grove
The place horse lives in is called stable
The place fox lives in is called den
The place snake lives in is called nest
The place raven lives in is called nest
The place herring lives in is called sea
The place termite lives in is called hill
The place pig lives in is called
2024-07-18 07:54:37 root INFO     total operator prediction time: 1711.3164513111115 seconds
2024-07-18 07:54:37 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on male - female
2024-07-18 07:54:37 root INFO     building operator male - female
2024-07-18 07:54:38 root INFO     [order_1_approx] starting weight calculation for A female stepfather is known as a stepmother
A female fisherman is known as a fisherwoman
A female buck is known as a doe
A female grandfather is known as a grandmother
A female brother is known as a sister
A female grandson is known as a granddaughter
A female daddy is known as a mommy
A female sir is known as a
2024-07-18 07:54:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 07:58:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2480,  0.4121, -0.0809,  ...,  0.5742,  0.9736,  0.5615],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.9062, -3.0039, -2.9297,  ..., -4.4141,  1.7305, -5.2539],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.1086e-02, -2.9984e-03, -1.0033e-02,  ..., -2.7390e-03,
          2.3499e-03, -9.5062e-03],
        [-1.8291e-03, -9.1476e-03, -1.0109e-02,  ..., -1.6846e-02,
         -1.3885e-03,  8.1329e-03],
        [ 9.0790e-03, -5.3406e-05, -1.5335e-03,  ...,  1.2100e-02,
         -1.9484e-03, -1.4824e-02],
        ...,
        [-1.2871e-02, -1.9867e-02, -7.1144e-04,  ...,  1.1902e-03,
         -1.1047e-02,  1.9646e-03],
        [ 4.0665e-03,  1.9951e-03,  1.3382e-02,  ...,  7.1716e-03,
         -5.8632e-03, -1.4977e-02],
        [-1.0056e-02, -1.3680e-02,  5.8250e-03,  ..., -6.2027e-03,
          6.4316e-03,  1.1642e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.9727, -3.2285, -2.8164,  ..., -4.2578,  1.8193, -5.5742]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 07:58:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female stepfather is known as a stepmother
A female fisherman is known as a fisherwoman
A female buck is known as a doe
A female grandfather is known as a grandmother
A female brother is known as a sister
A female grandson is known as a granddaughter
A female daddy is known as a mommy
A female sir is known as a
2024-07-18 07:58:11 root INFO     [order_1_approx] starting weight calculation for A female fisherman is known as a fisherwoman
A female brother is known as a sister
A female grandson is known as a granddaughter
A female stepfather is known as a stepmother
A female grandfather is known as a grandmother
A female buck is known as a doe
A female sir is known as a madam
A female daddy is known as a
2024-07-18 07:58:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 08:01:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0813, -0.2778,  0.5718,  ...,  0.2832, -0.1537,  0.5957],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.7656, -6.0586, -3.0469,  ..., -0.8345,  5.6953, -1.2383],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0242,  0.0275, -0.0059,  ..., -0.0011, -0.0051, -0.0096],
        [-0.0088, -0.0054,  0.0107,  ..., -0.0031,  0.0014,  0.0014],
        [-0.0044, -0.0051, -0.0286,  ..., -0.0248,  0.0053, -0.0042],
        ...,
        [-0.0168, -0.0091,  0.0093,  ..., -0.0013, -0.0151,  0.0097],
        [ 0.0131,  0.0011,  0.0164,  ..., -0.0062, -0.0294, -0.0007],
        [ 0.0002,  0.0104, -0.0013,  ...,  0.0143,  0.0101, -0.0087]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.7070, -6.3516, -3.2676,  ..., -1.4980,  5.5938, -2.0000]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 08:01:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female fisherman is known as a fisherwoman
A female brother is known as a sister
A female grandson is known as a granddaughter
A female stepfather is known as a stepmother
A female grandfather is known as a grandmother
A female buck is known as a doe
A female sir is known as a madam
A female daddy is known as a
2024-07-18 08:01:45 root INFO     [order_1_approx] starting weight calculation for A female fisherman is known as a fisherwoman
A female grandson is known as a granddaughter
A female stepfather is known as a stepmother
A female sir is known as a madam
A female daddy is known as a mommy
A female brother is known as a sister
A female buck is known as a doe
A female grandfather is known as a
2024-07-18 08:01:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 08:05:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1414, -1.6768,  0.3867,  ..., -0.4907,  1.0166, -0.8760],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 8.3672, -5.4258, -0.6035,  ..., -2.0156,  3.5781, -3.8750],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0083,  0.0021, -0.0052,  ...,  0.0081,  0.0020, -0.0091],
        [ 0.0014, -0.0060,  0.0155,  ...,  0.0025, -0.0016, -0.0041],
        [ 0.0145, -0.0066,  0.0076,  ...,  0.0016, -0.0019, -0.0026],
        ...,
        [-0.0077, -0.0007,  0.0102,  ...,  0.0025, -0.0300, -0.0043],
        [ 0.0101,  0.0099,  0.0051,  ...,  0.0088, -0.0087, -0.0118],
        [-0.0014, -0.0094,  0.0178,  ...,  0.0066,  0.0117,  0.0007]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 8.5312, -5.8008,  0.4463,  ..., -1.4170,  4.3242, -5.1445]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 08:05:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female fisherman is known as a fisherwoman
A female grandson is known as a granddaughter
A female stepfather is known as a stepmother
A female sir is known as a madam
A female daddy is known as a mommy
A female brother is known as a sister
A female buck is known as a doe
A female grandfather is known as a
2024-07-18 08:05:17 root INFO     [order_1_approx] starting weight calculation for A female grandson is known as a granddaughter
A female sir is known as a madam
A female grandfather is known as a grandmother
A female buck is known as a doe
A female daddy is known as a mommy
A female fisherman is known as a fisherwoman
A female stepfather is known as a stepmother
A female brother is known as a
2024-07-18 08:05:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 08:08:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2344, -0.3069,  0.2323,  ...,  0.2258,  0.5220, -0.5464],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.7227, -3.0996, -0.1934,  ..., -1.8203,  2.1914, -1.7285],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0054,  0.0103,  0.0024,  ...,  0.0124, -0.0022, -0.0096],
        [-0.0079, -0.0024, -0.0075,  ...,  0.0014,  0.0061, -0.0007],
        [-0.0025,  0.0069, -0.0075,  ...,  0.0005, -0.0099,  0.0056],
        ...,
        [-0.0085,  0.0042,  0.0003,  ..., -0.0046, -0.0053,  0.0069],
        [ 0.0018,  0.0116,  0.0044,  ...,  0.0061, -0.0005, -0.0053],
        [ 0.0103, -0.0140,  0.0117,  ...,  0.0044,  0.0063,  0.0044]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.3477, -2.9219,  0.2239,  ..., -2.0469,  2.6250, -2.0039]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 08:08:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female grandson is known as a granddaughter
A female sir is known as a madam
A female grandfather is known as a grandmother
A female buck is known as a doe
A female daddy is known as a mommy
A female fisherman is known as a fisherwoman
A female stepfather is known as a stepmother
A female brother is known as a
2024-07-18 08:08:50 root INFO     [order_1_approx] starting weight calculation for A female stepfather is known as a stepmother
A female grandson is known as a granddaughter
A female brother is known as a sister
A female fisherman is known as a fisherwoman
A female daddy is known as a mommy
A female grandfather is known as a grandmother
A female sir is known as a madam
A female buck is known as a
2024-07-18 08:08:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 08:12:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2344, -1.2969, -0.0137,  ...,  0.9683, -1.4316,  0.4675],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.6875, -0.6104, -4.6406,  ..., -2.3281,  2.9336, -1.0762],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.0155e-03, -7.4768e-03, -3.9024e-03,  ..., -3.1414e-03,
          1.3962e-03,  8.9111e-03],
        [-1.4229e-02, -1.1063e-02, -2.4223e-03,  ...,  5.6267e-05,
          6.8359e-03,  4.9324e-03],
        [-8.1825e-04, -1.8555e-02,  2.3632e-03,  ..., -9.7122e-03,
         -2.8362e-03, -2.6337e-02],
        ...,
        [-1.6754e-02, -3.4393e-02,  4.7722e-03,  ..., -1.2169e-03,
         -7.4577e-03, -6.6910e-03],
        [-5.0354e-04, -2.5177e-03,  5.1689e-03,  ...,  5.6229e-03,
          2.7523e-03,  1.9875e-03],
        [ 4.1199e-04,  5.1956e-03,  1.7033e-03,  ...,  2.7809e-03,
          1.0849e-02,  1.3687e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.3555, -1.2500, -5.7266,  ..., -2.1211,  3.5312, -2.3340]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 08:12:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female stepfather is known as a stepmother
A female grandson is known as a granddaughter
A female brother is known as a sister
A female fisherman is known as a fisherwoman
A female daddy is known as a mommy
A female grandfather is known as a grandmother
A female sir is known as a madam
A female buck is known as a
2024-07-18 08:12:26 root INFO     [order_1_approx] starting weight calculation for A female grandson is known as a granddaughter
A female fisherman is known as a fisherwoman
A female grandfather is known as a grandmother
A female daddy is known as a mommy
A female sir is known as a madam
A female buck is known as a doe
A female brother is known as a sister
A female stepfather is known as a
2024-07-18 08:12:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 08:16:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1548, -0.5825,  0.0149,  ..., -0.0651, -0.1143, -0.8647],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.6953, -6.7148,  1.6562,  ..., -2.7812,  3.3848, -2.1016],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-7.1182e-03, -2.1706e-03, -2.0676e-03,  ..., -1.6680e-03,
         -3.3569e-04, -8.1787e-03],
        [ 4.6310e-03,  9.5673e-03,  1.5060e-02,  ..., -1.1513e-02,
          3.4618e-03,  7.4005e-03],
        [ 8.0643e-03, -1.8120e-03, -6.3095e-03,  ..., -8.4381e-03,
         -3.1319e-03,  4.9744e-03],
        ...,
        [ 6.8665e-04, -7.1335e-03,  4.3869e-04,  ..., -4.7951e-03,
         -1.6434e-02,  6.7329e-03],
        [ 6.8016e-03,  6.8283e-03, -3.7003e-04,  ...,  1.0231e-02,
          4.6997e-03,  1.5236e-02],
        [ 5.7373e-03, -5.7220e-06, -2.8305e-03,  ..., -1.8654e-03,
          1.4435e-02, -3.9864e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.9844, -6.3242,  1.0527,  ..., -2.7422,  3.3320, -2.2930]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 08:16:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female grandson is known as a granddaughter
A female fisherman is known as a fisherwoman
A female grandfather is known as a grandmother
A female daddy is known as a mommy
A female sir is known as a madam
A female buck is known as a doe
A female brother is known as a sister
A female stepfather is known as a
2024-07-18 08:16:01 root INFO     [order_1_approx] starting weight calculation for A female brother is known as a sister
A female grandson is known as a granddaughter
A female daddy is known as a mommy
A female sir is known as a madam
A female buck is known as a doe
A female grandfather is known as a grandmother
A female stepfather is known as a stepmother
A female fisherman is known as a
2024-07-18 08:16:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 08:19:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6250, -0.2468,  0.3130,  ..., -0.2588, -0.1508,  0.7417],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.7656,  1.4453, -6.2227,  ...,  1.4658,  0.8848,  3.2773],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0032, -0.0239, -0.0148,  ...,  0.0046, -0.0119, -0.0016],
        [ 0.0006, -0.0172, -0.0122,  ...,  0.0205,  0.0088,  0.0080],
        [ 0.0034,  0.0027,  0.0122,  ..., -0.0263,  0.0037,  0.0110],
        ...,
        [-0.0172, -0.0109, -0.0112,  ...,  0.0064, -0.0052,  0.0069],
        [ 0.0017, -0.0008,  0.0115,  ..., -0.0048, -0.0170, -0.0025],
        [ 0.0012,  0.0133,  0.0078,  ...,  0.0213,  0.0060,  0.0096]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.8555,  0.7241, -5.3281,  ...,  2.0273,  2.0742,  2.6016]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 08:19:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female brother is known as a sister
A female grandson is known as a granddaughter
A female daddy is known as a mommy
A female sir is known as a madam
A female buck is known as a doe
A female grandfather is known as a grandmother
A female stepfather is known as a stepmother
A female fisherman is known as a
2024-07-18 08:19:35 root INFO     [order_1_approx] starting weight calculation for A female fisherman is known as a fisherwoman
A female brother is known as a sister
A female daddy is known as a mommy
A female buck is known as a doe
A female grandfather is known as a grandmother
A female sir is known as a madam
A female stepfather is known as a stepmother
A female grandson is known as a
2024-07-18 08:19:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 08:23:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5601, -1.2793, -0.8604,  ...,  0.0795, -0.5469,  0.0115],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.2734, -4.2891, -2.2930,  ...,  2.7012,  3.3555, -2.3555],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-9.5215e-03, -2.7485e-03,  1.6403e-04,  ..., -6.1607e-03,
          1.4938e-02, -1.1543e-02],
        [ 7.0381e-03,  1.4610e-03, -2.1172e-03,  ..., -9.1505e-04,
          9.1934e-04, -6.4507e-03],
        [ 2.0370e-03,  7.9041e-03, -2.3003e-03,  ..., -4.5280e-03,
          2.9831e-03,  1.4977e-02],
        ...,
        [-2.5902e-03,  4.3488e-04,  5.3024e-03,  ...,  2.6875e-03,
         -1.6022e-02, -1.4198e-02],
        [-1.6174e-03,  4.8637e-03,  4.9286e-03,  ...,  4.7951e-03,
         -9.2621e-03, -2.3499e-03],
        [-4.1008e-03, -4.5013e-04,  1.8806e-03,  ...,  4.7684e-06,
          3.7804e-03, -8.6823e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.1289, -4.4453, -2.5820,  ...,  2.9980,  4.1875, -2.6426]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 08:23:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female fisherman is known as a fisherwoman
A female brother is known as a sister
A female daddy is known as a mommy
A female buck is known as a doe
A female grandfather is known as a grandmother
A female sir is known as a madam
A female stepfather is known as a stepmother
A female grandson is known as a
2024-07-18 08:23:08 root INFO     total operator prediction time: 1710.3023416996002 seconds
2024-07-18 08:23:08 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on name - occupation
2024-07-18 08:23:08 root INFO     building operator name - occupation
2024-07-18 08:23:08 root INFO     [order_1_approx] starting weight calculation for truman was known for their work as a  president
marx was known for their work as a  philosopher
kant was known for their work as a  philosopher
tolstoi was known for their work as a  novelist
mozart was known for their work as a  composer
hegel was known for their work as a  philosopher
spinoza was known for their work as a  philosopher
rousseau was known for their work as a 
2024-07-18 08:23:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 08:26:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5269, -0.6211, -0.9907,  ...,  0.6851, -0.7026,  0.2676],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3125, -0.5020, -0.7031,  ..., -5.6914, -0.3022,  0.0072],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0130,  0.0063,  0.0061,  ...,  0.0007,  0.0080, -0.0038],
        [ 0.0173,  0.0039,  0.0129,  ...,  0.0175, -0.0058, -0.0033],
        [-0.0119, -0.0023, -0.0014,  ..., -0.0047,  0.0050, -0.0141],
        ...,
        [-0.0089,  0.0062, -0.0005,  ..., -0.0016, -0.0230,  0.0064],
        [-0.0092, -0.0173,  0.0109,  ...,  0.0022,  0.0100, -0.0027],
        [-0.0136,  0.0121, -0.0063,  ..., -0.0115,  0.0112,  0.0247]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7627, -0.2686,  0.1060,  ..., -5.4922, -0.2076, -0.0879]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 08:26:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for truman was known for their work as a  president
marx was known for their work as a  philosopher
kant was known for their work as a  philosopher
tolstoi was known for their work as a  novelist
mozart was known for their work as a  composer
hegel was known for their work as a  philosopher
spinoza was known for their work as a  philosopher
rousseau was known for their work as a 
2024-07-18 08:26:41 root INFO     [order_1_approx] starting weight calculation for mozart was known for their work as a  composer
hegel was known for their work as a  philosopher
rousseau was known for their work as a  writer
marx was known for their work as a  philosopher
kant was known for their work as a  philosopher
tolstoi was known for their work as a  novelist
spinoza was known for their work as a  philosopher
truman was known for their work as a 
2024-07-18 08:26:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 08:30:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.8477,  1.1348, -0.9072,  ...,  0.3511, -0.2231, -0.6724],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0312, -0.0100,  1.0586,  ..., -2.4824,  0.5186, -1.3613],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.6894e-04,  3.0994e-06,  9.6416e-04,  ..., -1.0361e-02,
          8.5907e-03, -1.2589e-02],
        [-9.6436e-03, -4.4785e-03, -3.7937e-03,  ...,  8.0414e-03,
          2.9945e-03, -1.0994e-02],
        [ 1.6975e-03,  9.4891e-04, -4.1504e-03,  ..., -2.8491e-05,
          7.0992e-03, -1.8349e-03],
        ...,
        [-1.3763e-02, -6.9771e-03,  4.1313e-03,  ...,  6.9141e-04,
         -1.4999e-02,  8.9645e-03],
        [ 2.1782e-03, -9.8572e-03, -1.6632e-02,  ..., -7.6294e-04,
          2.0859e-02, -3.5439e-03],
        [-4.1924e-03, -2.6093e-03, -5.2338e-03,  ..., -4.3411e-03,
          2.0081e-02, -1.1520e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9766, -0.0916,  0.5635,  ..., -1.8584,  1.2363, -1.1689]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 08:30:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for mozart was known for their work as a  composer
hegel was known for their work as a  philosopher
rousseau was known for their work as a  writer
marx was known for their work as a  philosopher
kant was known for their work as a  philosopher
tolstoi was known for their work as a  novelist
spinoza was known for their work as a  philosopher
truman was known for their work as a 
2024-07-18 08:30:15 root INFO     [order_1_approx] starting weight calculation for truman was known for their work as a  president
tolstoi was known for their work as a  novelist
marx was known for their work as a  philosopher
rousseau was known for their work as a  writer
mozart was known for their work as a  composer
spinoza was known for their work as a  philosopher
hegel was known for their work as a  philosopher
kant was known for their work as a 
2024-07-18 08:30:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 08:33:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6274,  0.0452,  1.1250,  ..., -1.1016, -0.4922,  0.7549],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1658, -5.3242, -4.0898,  ..., -3.7930,  2.3477, -1.4717],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0110,  0.0074, -0.0011,  ..., -0.0113,  0.0077,  0.0030],
        [ 0.0116,  0.0121,  0.0148,  ..., -0.0020, -0.0010,  0.0080],
        [-0.0037, -0.0073,  0.0014,  ..., -0.0064, -0.0004,  0.0020],
        ...,
        [-0.0117, -0.0120, -0.0004,  ...,  0.0044, -0.0071, -0.0058],
        [ 0.0087, -0.0098,  0.0047,  ...,  0.0167, -0.0014,  0.0025],
        [-0.0059, -0.0050,  0.0075,  ...,  0.0045, -0.0015,  0.0117]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0311, -4.9609, -3.5117,  ..., -3.0977,  2.1152, -0.8467]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 08:33:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for truman was known for their work as a  president
tolstoi was known for their work as a  novelist
marx was known for their work as a  philosopher
rousseau was known for their work as a  writer
mozart was known for their work as a  composer
spinoza was known for their work as a  philosopher
hegel was known for their work as a  philosopher
kant was known for their work as a 
2024-07-18 08:33:49 root INFO     [order_1_approx] starting weight calculation for marx was known for their work as a  philosopher
tolstoi was known for their work as a  novelist
rousseau was known for their work as a  writer
hegel was known for their work as a  philosopher
truman was known for their work as a  president
kant was known for their work as a  philosopher
spinoza was known for their work as a  philosopher
mozart was known for their work as a 
2024-07-18 08:33:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 08:37:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4194,  0.5610, -1.0674,  ...,  0.8779,  0.5259, -0.4146],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6562, -0.4434, -2.3086,  ..., -0.0088,  0.5952, -1.0566],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-9.7351e-03, -5.9700e-03,  1.4191e-02,  ...,  1.6235e-02,
          6.0081e-03,  6.3400e-03],
        [ 1.8253e-03,  2.6054e-03, -2.7351e-03,  ...,  4.7340e-03,
         -1.8406e-03,  1.1139e-02],
        [-8.3771e-03, -2.9831e-03, -1.6052e-02,  ..., -9.2850e-03,
          3.5973e-03, -4.3411e-03],
        ...,
        [-1.5411e-02, -2.4166e-03, -2.2411e-05,  ...,  6.2065e-03,
         -1.8372e-02,  8.5640e-04],
        [-1.0422e-02, -1.1917e-02,  1.6422e-03,  ...,  1.6174e-03,
         -1.8555e-02,  1.1703e-02],
        [-3.4752e-03, -1.7868e-02, -6.5613e-04,  ...,  1.2268e-02,
          5.5542e-03,  1.1650e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.2852, -0.6436, -2.4902,  ..., -0.0083,  1.0059, -1.1094]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 08:37:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for marx was known for their work as a  philosopher
tolstoi was known for their work as a  novelist
rousseau was known for their work as a  writer
hegel was known for their work as a  philosopher
truman was known for their work as a  president
kant was known for their work as a  philosopher
spinoza was known for their work as a  philosopher
mozart was known for their work as a 
2024-07-18 08:37:22 root INFO     [order_1_approx] starting weight calculation for tolstoi was known for their work as a  novelist
truman was known for their work as a  president
kant was known for their work as a  philosopher
marx was known for their work as a  philosopher
mozart was known for their work as a  composer
spinoza was known for their work as a  philosopher
rousseau was known for their work as a  writer
hegel was known for their work as a 
2024-07-18 08:37:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 08:40:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.9072,  0.1980, -0.5400,  ...,  0.8501,  0.2007,  2.0273],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7920, -3.9297, -1.6016,  ..., -6.5273,  0.6514, -1.7510],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0060, -0.0051, -0.0010,  ..., -0.0131,  0.0145, -0.0141],
        [ 0.0076,  0.0094, -0.0009,  ...,  0.0137,  0.0002, -0.0139],
        [-0.0043, -0.0031, -0.0156,  ..., -0.0130,  0.0039,  0.0019],
        ...,
        [-0.0008, -0.0061, -0.0053,  ...,  0.0044, -0.0126, -0.0108],
        [ 0.0025, -0.0111,  0.0098,  ...,  0.0060,  0.0017,  0.0149],
        [-0.0098,  0.0058,  0.0044,  ..., -0.0036,  0.0074,  0.0201]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1426, -4.7461, -1.9668,  ..., -5.6406,  0.4492, -2.1621]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 08:40:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for tolstoi was known for their work as a  novelist
truman was known for their work as a  president
kant was known for their work as a  philosopher
marx was known for their work as a  philosopher
mozart was known for their work as a  composer
spinoza was known for their work as a  philosopher
rousseau was known for their work as a  writer
hegel was known for their work as a 
2024-07-18 08:40:53 root INFO     [order_1_approx] starting weight calculation for truman was known for their work as a  president
rousseau was known for their work as a  writer
mozart was known for their work as a  composer
kant was known for their work as a  philosopher
hegel was known for their work as a  philosopher
marx was known for their work as a  philosopher
spinoza was known for their work as a  philosopher
tolstoi was known for their work as a 
2024-07-18 08:40:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 08:44:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3521,  0.2610,  0.6812,  ...,  0.9707,  0.1062,  0.2407],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.9180,  1.7314, -3.4395,  ..., -4.5938,  1.3594, -2.1289],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0126, -0.0069,  0.0012,  ..., -0.0125,  0.0208, -0.0301],
        [ 0.0071,  0.0109,  0.0179,  ...,  0.0140, -0.0104, -0.0025],
        [-0.0052,  0.0027, -0.0070,  ..., -0.0293,  0.0008, -0.0073],
        ...,
        [-0.0131, -0.0132, -0.0014,  ...,  0.0129, -0.0211,  0.0021],
        [-0.0113, -0.0178,  0.0003,  ...,  0.0069,  0.0013,  0.0162],
        [-0.0005, -0.0009,  0.0040,  ..., -0.0079,  0.0098,  0.0073]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7871,  1.0957, -3.6387,  ..., -4.1133,  1.5547, -0.6094]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 08:44:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for truman was known for their work as a  president
rousseau was known for their work as a  writer
mozart was known for their work as a  composer
kant was known for their work as a  philosopher
hegel was known for their work as a  philosopher
marx was known for their work as a  philosopher
spinoza was known for their work as a  philosopher
tolstoi was known for their work as a 
2024-07-18 08:44:26 root INFO     [order_1_approx] starting weight calculation for rousseau was known for their work as a  writer
spinoza was known for their work as a  philosopher
hegel was known for their work as a  philosopher
mozart was known for their work as a  composer
kant was known for their work as a  philosopher
tolstoi was known for their work as a  novelist
truman was known for their work as a  president
marx was known for their work as a 
2024-07-18 08:44:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 08:47:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4771, -0.5049, -0.3538,  ..., -0.7812, -2.0469,  0.0920],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1758, -2.8828, -6.4844,  ..., -5.5352, -1.1885, -0.2529],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0041, -0.0093,  0.0034,  ..., -0.0073,  0.0095,  0.0086],
        [ 0.0075,  0.0060, -0.0056,  ...,  0.0094,  0.0052,  0.0065],
        [ 0.0019,  0.0137, -0.0068,  ..., -0.0178, -0.0134, -0.0035],
        ...,
        [-0.0120, -0.0078, -0.0059,  ..., -0.0103, -0.0254,  0.0033],
        [ 0.0122, -0.0087,  0.0059,  ...,  0.0011, -0.0021,  0.0064],
        [-0.0053, -0.0133,  0.0106,  ..., -0.0072,  0.0015, -0.0068]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9365, -3.3770, -5.8594,  ..., -4.3555, -0.9502,  0.1335]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 08:48:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for rousseau was known for their work as a  writer
spinoza was known for their work as a  philosopher
hegel was known for their work as a  philosopher
mozart was known for their work as a  composer
kant was known for their work as a  philosopher
tolstoi was known for their work as a  novelist
truman was known for their work as a  president
marx was known for their work as a 
2024-07-18 08:48:00 root INFO     [order_1_approx] starting weight calculation for tolstoi was known for their work as a  novelist
rousseau was known for their work as a  writer
mozart was known for their work as a  composer
hegel was known for their work as a  philosopher
kant was known for their work as a  philosopher
truman was known for their work as a  president
marx was known for their work as a  philosopher
spinoza was known for their work as a 
2024-07-18 08:48:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 08:51:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6377, -1.7734, -1.3057,  ..., -0.2607, -0.4070, -0.0488],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.0352, -6.3789, -4.3750,  ..., -5.7734, -1.8340,  0.4546],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0050, -0.0148, -0.0010,  ..., -0.0143,  0.0151, -0.0040],
        [-0.0115,  0.0262, -0.0018,  ...,  0.0031,  0.0044, -0.0042],
        [ 0.0020,  0.0052, -0.0003,  ..., -0.0110,  0.0003, -0.0090],
        ...,
        [-0.0091, -0.0206,  0.0037,  ...,  0.0058, -0.0137, -0.0083],
        [-0.0036, -0.0223, -0.0008,  ...,  0.0004, -0.0031,  0.0191],
        [-0.0126,  0.0008, -0.0061,  ..., -0.0023, -0.0026,  0.0122]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.0000, -6.4531, -5.7109,  ..., -5.8633, -1.3818, -0.5737]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 08:51:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for tolstoi was known for their work as a  novelist
rousseau was known for their work as a  writer
mozart was known for their work as a  composer
hegel was known for their work as a  philosopher
kant was known for their work as a  philosopher
truman was known for their work as a  president
marx was known for their work as a  philosopher
spinoza was known for their work as a 
2024-07-18 08:51:34 root INFO     total operator prediction time: 1706.6977026462555 seconds
2024-07-18 08:51:34 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on country - capital
2024-07-18 08:51:34 root INFO     building operator country - capital
2024-07-18 08:51:35 root INFO     [order_1_approx] starting weight calculation for The country with cairo as its capital is known as egypt
The country with baghdad as its capital is known as iraq
The country with ottawa as its capital is known as canada
The country with hanoi as its capital is known as vietnam
The country with tokyo as its capital is known as japan
The country with jakarta as its capital is known as indonesia
The country with moscow as its capital is known as russia
The country with bern as its capital is known as
2024-07-18 08:51:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
2024-07-18 08:55:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9375, -0.2715, -1.3076,  ...,  0.9854,  0.1239, -0.1069],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7656,  1.5195, -3.3301,  ..., -1.4580, -3.5742, -2.9805],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0005, -0.0027,  0.0035,  ..., -0.0006,  0.0008, -0.0064],
        [ 0.0014,  0.0015,  0.0006,  ...,  0.0082, -0.0017, -0.0032],
        [ 0.0037,  0.0008, -0.0020,  ..., -0.0024, -0.0007, -0.0016],
        ...,
        [-0.0011, -0.0063, -0.0065,  ...,  0.0139, -0.0081, -0.0066],
        [-0.0011, -0.0008, -0.0025,  ..., -0.0015, -0.0022,  0.0092],
        [-0.0020,  0.0007, -0.0033,  ...,  0.0039,  0.0009,  0.0013]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6035,  2.0469, -3.9922,  ..., -0.9863, -3.7793, -2.7832]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 08:55:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with cairo as its capital is known as egypt
The country with baghdad as its capital is known as iraq
The country with ottawa as its capital is known as canada
The country with hanoi as its capital is known as vietnam
The country with tokyo as its capital is known as japan
The country with jakarta as its capital is known as indonesia
The country with moscow as its capital is known as russia
The country with bern as its capital is known as
2024-07-18 08:55:08 root INFO     [order_1_approx] starting weight calculation for The country with bern as its capital is known as switzerland
The country with moscow as its capital is known as russia
The country with jakarta as its capital is known as indonesia
The country with tokyo as its capital is known as japan
The country with cairo as its capital is known as egypt
The country with hanoi as its capital is known as vietnam
The country with baghdad as its capital is known as iraq
The country with ottawa as its capital is known as
2024-07-18 08:55:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.8
