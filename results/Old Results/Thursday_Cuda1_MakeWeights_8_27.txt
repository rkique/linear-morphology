2024-07-18 19:01:59 root INFO     loading model + tokenizer
2024-07-18 19:02:17 root INFO     model + tokenizer loaded
2024-07-18 19:02:17 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - part
2024-07-18 19:02:17 root INFO     building operator meronyms - part
2024-07-18 19:02:18 root INFO     [order_1_approx] starting weight calculation for A part of a jail is a cell
A part of a sword is a blade
A part of a comb is a teeth
A part of a poem is a stanza
A part of a shilling is a pence
A part of a pub is a bar
A part of a bird is a feathers
A part of a orthography is a
2024-07-18 19:02:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 19:06:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4746,  0.7129,  0.1588,  ..., -1.1699, -0.2769,  0.7363],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.2773,  1.3496, -1.6934,  ..., -1.0215,  0.2258,  0.6963],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0020,  0.0182, -0.0087,  ..., -0.0040,  0.0012, -0.0027],
        [-0.0105, -0.0012, -0.0031,  ..., -0.0053, -0.0128,  0.0041],
        [ 0.0211,  0.0044, -0.0019,  ...,  0.0184,  0.0051,  0.0104],
        ...,
        [ 0.0111,  0.0078, -0.0028,  ...,  0.0032,  0.0039, -0.0048],
        [ 0.0060,  0.0077, -0.0096,  ..., -0.0044, -0.0019, -0.0013],
        [-0.0023, -0.0306,  0.0097,  ..., -0.0040, -0.0091,  0.0110]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.4375,  1.5303, -1.8926,  ..., -0.5234,  0.0508, -0.0586]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 19:06:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a jail is a cell
A part of a sword is a blade
A part of a comb is a teeth
A part of a poem is a stanza
A part of a shilling is a pence
A part of a pub is a bar
A part of a bird is a feathers
A part of a orthography is a
2024-07-18 19:06:38 root INFO     [order_1_approx] starting weight calculation for A part of a pub is a bar
A part of a sword is a blade
A part of a shilling is a pence
A part of a jail is a cell
A part of a orthography is a hyphenation
A part of a poem is a stanza
A part of a comb is a teeth
A part of a bird is a
2024-07-18 19:06:38 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 19:10:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2930, -0.2915,  0.3896,  ...,  1.0107, -0.5337, -0.7441],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2021,  0.8716, -3.2188,  ...,  0.8271, -0.5547, -4.5938],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0105, -0.0027,  0.0240,  ...,  0.0212, -0.0184, -0.0007],
        [-0.0124,  0.0043,  0.0199,  ...,  0.0119, -0.0078, -0.0007],
        [ 0.0148,  0.0104,  0.0077,  ..., -0.0022, -0.0123,  0.0166],
        ...,
        [ 0.0026, -0.0177, -0.0104,  ..., -0.0047, -0.0081,  0.0003],
        [ 0.0124, -0.0060,  0.0080,  ..., -0.0054,  0.0022,  0.0092],
        [-0.0004, -0.0065, -0.0004,  ...,  0.0183, -0.0104, -0.0038]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6040,  1.7246, -3.9609,  ...,  0.7100, -0.9966, -3.7109]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 19:11:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a pub is a bar
A part of a sword is a blade
A part of a shilling is a pence
A part of a jail is a cell
A part of a orthography is a hyphenation
A part of a poem is a stanza
A part of a comb is a teeth
A part of a bird is a
2024-07-18 19:11:00 root INFO     [order_1_approx] starting weight calculation for A part of a shilling is a pence
A part of a sword is a blade
A part of a orthography is a hyphenation
A part of a bird is a feathers
A part of a comb is a teeth
A part of a poem is a stanza
A part of a pub is a bar
A part of a jail is a
2024-07-18 19:11:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 19:15:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0731, -0.1219, -0.0590,  ..., -0.0848, -0.7285, -0.4888],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1270, -2.0312,  0.0996,  ..., -1.6416,  0.4507, -0.2114],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0055, -0.0182,  0.0098,  ...,  0.0132,  0.0025, -0.0154],
        [ 0.0011,  0.0115,  0.0024,  ...,  0.0117, -0.0176, -0.0079],
        [ 0.0110, -0.0058,  0.0088,  ..., -0.0048,  0.0088,  0.0173],
        ...,
        [-0.0085, -0.0078, -0.0146,  ..., -0.0060, -0.0024,  0.0013],
        [ 0.0047,  0.0033,  0.0021,  ...,  0.0133,  0.0036,  0.0053],
        [-0.0092, -0.0022, -0.0002,  ..., -0.0071, -0.0229,  0.0004]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2383, -1.4688, -0.4800,  ..., -1.0625,  0.6880, -0.4229]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 19:15:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a shilling is a pence
A part of a sword is a blade
A part of a orthography is a hyphenation
A part of a bird is a feathers
A part of a comb is a teeth
A part of a poem is a stanza
A part of a pub is a bar
A part of a jail is a
2024-07-18 19:15:22 root INFO     [order_1_approx] starting weight calculation for A part of a orthography is a hyphenation
A part of a bird is a feathers
A part of a comb is a teeth
A part of a pub is a bar
A part of a poem is a stanza
A part of a sword is a blade
A part of a jail is a cell
A part of a shilling is a
2024-07-18 19:15:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 19:19:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5459, -0.6250, -0.3135,  ...,  0.3252,  0.0630,  0.2515],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.1055, -0.5640, -3.5859,  ..., -2.0527, -3.7832, -2.6543],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-3.3325e-02, -1.7365e-02, -6.5536e-03,  ..., -1.6663e-02,
         -2.1866e-02, -2.3590e-02],
        [-1.4648e-03,  6.8512e-03,  2.2469e-03,  ..., -1.3809e-03,
          2.0790e-03, -1.2711e-02],
        [ 4.9133e-03, -4.9515e-03, -2.0294e-03,  ...,  3.1853e-03,
          1.7242e-02,  1.5152e-02],
        ...,
        [-1.2352e-02,  6.0177e-04,  1.9073e-05,  ..., -1.0052e-03,
          1.1642e-02, -6.8970e-03],
        [ 1.5160e-02,  6.4430e-03, -7.5150e-04,  ..., -8.0109e-03,
          2.2400e-02, -8.4839e-03],
        [-1.9943e-02,  6.2103e-03, -1.6441e-03,  ..., -1.4946e-02,
          3.8891e-03, -4.0314e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.0215,  0.0757, -3.7734,  ..., -2.2695, -2.6543, -2.5254]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 19:19:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a orthography is a hyphenation
A part of a bird is a feathers
A part of a comb is a teeth
A part of a pub is a bar
A part of a poem is a stanza
A part of a sword is a blade
A part of a jail is a cell
A part of a shilling is a
2024-07-18 19:19:44 root INFO     [order_1_approx] starting weight calculation for A part of a orthography is a hyphenation
A part of a shilling is a pence
A part of a bird is a feathers
A part of a jail is a cell
A part of a comb is a teeth
A part of a pub is a bar
A part of a sword is a blade
A part of a poem is a
2024-07-18 19:19:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 19:24:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0018,  0.2668,  0.0181,  ..., -0.5732, -0.0798, -0.0410],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7690,  1.4082, -4.4375,  ..., -2.8223, -1.0908, -7.1250],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0053, -0.0067, -0.0044,  ..., -0.0029,  0.0036, -0.0152],
        [-0.0032,  0.0127, -0.0126,  ...,  0.0375,  0.0064, -0.0051],
        [ 0.0096,  0.0052, -0.0124,  ..., -0.0094, -0.0069,  0.0072],
        ...,
        [ 0.0170,  0.0058, -0.0118,  ..., -0.0476, -0.0219,  0.0180],
        [-0.0020, -0.0022,  0.0070,  ..., -0.0095, -0.0287, -0.0049],
        [ 0.0093, -0.0264,  0.0170,  ..., -0.0692, -0.0325, -0.0308]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4817,  0.9902, -6.4062,  ..., -2.9062, -2.3242, -6.5703]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 19:24:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a orthography is a hyphenation
A part of a shilling is a pence
A part of a bird is a feathers
A part of a jail is a cell
A part of a comb is a teeth
A part of a pub is a bar
A part of a sword is a blade
A part of a poem is a
2024-07-18 19:24:04 root INFO     [order_1_approx] starting weight calculation for A part of a orthography is a hyphenation
A part of a pub is a bar
A part of a comb is a teeth
A part of a shilling is a pence
A part of a bird is a feathers
A part of a poem is a stanza
A part of a jail is a cell
A part of a sword is a
2024-07-18 19:24:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 19:28:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1169,  0.1990, -0.0144,  ...,  0.2793, -0.3145,  0.7168],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.2109, -1.0264, -1.8477,  ..., -3.3867, -0.0407, -1.9150],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0084, -0.0181, -0.0115,  ...,  0.0082,  0.0053, -0.0247],
        [ 0.0066,  0.0037,  0.0152,  ...,  0.0047, -0.0040,  0.0063],
        [ 0.0016,  0.0168, -0.0100,  ..., -0.0089, -0.0183,  0.0162],
        ...,
        [-0.0019,  0.0119, -0.0025,  ..., -0.0065, -0.0179,  0.0117],
        [ 0.0269,  0.0049,  0.0208,  ...,  0.0131,  0.0046, -0.0192],
        [ 0.0016,  0.0037, -0.0096,  ..., -0.0154,  0.0187,  0.0103]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.3047, -0.7178, -1.9727,  ..., -3.0332,  0.9458, -1.4736]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 19:28:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a orthography is a hyphenation
A part of a pub is a bar
A part of a comb is a teeth
A part of a shilling is a pence
A part of a bird is a feathers
A part of a poem is a stanza
A part of a jail is a cell
A part of a sword is a
2024-07-18 19:28:25 root INFO     [order_1_approx] starting weight calculation for A part of a shilling is a pence
A part of a pub is a bar
A part of a sword is a blade
A part of a orthography is a hyphenation
A part of a bird is a feathers
A part of a jail is a cell
A part of a poem is a stanza
A part of a comb is a
2024-07-18 19:28:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 19:32:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0641,  0.5415,  0.0338,  ..., -0.2383, -0.6206, -0.2688],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.2832, -4.0703, -3.7656,  ...,  1.8838,  0.7524,  2.3379],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0131,  0.0111,  0.0059,  ...,  0.0088,  0.0223, -0.0104],
        [-0.0263,  0.0117,  0.0086,  ..., -0.0055,  0.0178, -0.0046],
        [-0.0454,  0.0366,  0.0097,  ...,  0.0002,  0.0327, -0.0079],
        ...,
        [ 0.0176, -0.0103, -0.0058,  ...,  0.0103, -0.0357,  0.0083],
        [ 0.0129, -0.0035,  0.0171,  ...,  0.0018,  0.0383,  0.0011],
        [-0.0102, -0.0011,  0.0016,  ...,  0.0015,  0.0479, -0.0081]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.1309, -1.6523, -3.4746,  ...,  1.5586,  0.1274,  1.5156]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 19:32:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a shilling is a pence
A part of a pub is a bar
A part of a sword is a blade
A part of a orthography is a hyphenation
A part of a bird is a feathers
A part of a jail is a cell
A part of a poem is a stanza
A part of a comb is a
2024-07-18 19:32:45 root INFO     [order_1_approx] starting weight calculation for A part of a sword is a blade
A part of a bird is a feathers
A part of a shilling is a pence
A part of a jail is a cell
A part of a comb is a teeth
A part of a poem is a stanza
A part of a orthography is a hyphenation
A part of a pub is a
2024-07-18 19:32:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 19:37:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0139,  0.3086,  0.3940,  ...,  0.8320,  0.3848, -0.2881],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.0391, -0.6367, -0.9985,  ...,  1.0010, -0.2322, -4.5625],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0067, -0.0028, -0.0147,  ...,  0.0036, -0.0009, -0.0258],
        [ 0.0167,  0.0151,  0.0178,  ..., -0.0136,  0.0069, -0.0046],
        [ 0.0194,  0.0176, -0.0098,  ...,  0.0088,  0.0084,  0.0035],
        ...,
        [ 0.0050,  0.0013, -0.0095,  ..., -0.0131, -0.0057, -0.0003],
        [ 0.0106,  0.0132, -0.0149,  ...,  0.0154, -0.0011,  0.0018],
        [ 0.0232, -0.0249, -0.0090,  ..., -0.0042,  0.0159, -0.0063]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.2812, -1.1934, -0.8413,  ...,  2.6328,  0.0723, -4.9453]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 19:37:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A part of a sword is a blade
A part of a bird is a feathers
A part of a shilling is a pence
A part of a jail is a cell
A part of a comb is a teeth
A part of a poem is a stanza
A part of a orthography is a hyphenation
A part of a pub is a
2024-07-18 19:37:07 root INFO     total operator prediction time: 2090.161924600601 seconds
2024-07-18 19:37:07 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on synonyms - exact
2024-07-18 19:37:07 root INFO     building operator synonyms - exact
2024-07-18 19:37:07 root INFO     [order_1_approx] starting weight calculation for Another word for package is parcel
Another word for lady is madam
Another word for honest is sincere
Another word for identical is same
Another word for vocabulary is lexicon
Another word for phone is telephone
Another word for airplane is aeroplane
Another word for sweets is
2024-07-18 19:37:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 19:41:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0332,  0.6509, -0.3044,  ...,  0.6191,  0.1670, -0.6372],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.1055,  5.3867, -5.4766,  ..., -2.6758,  3.6621, -1.7754],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 6.5727e-03,  7.8812e-03, -3.9215e-03,  ..., -1.8250e-02,
          1.5991e-02, -1.4694e-02],
        [-9.7122e-03, -1.8768e-02, -1.1246e-02,  ...,  7.0877e-03,
         -1.5778e-02,  1.0738e-03],
        [ 1.2657e-02,  3.4241e-02, -7.2937e-03,  ..., -1.6117e-03,
          1.4946e-02,  2.3308e-03],
        ...,
        [-3.0518e-05,  5.4054e-03,  5.0354e-04,  ..., -2.7466e-04,
          1.0956e-02,  4.2839e-03],
        [ 1.1368e-03, -1.1688e-02,  7.2746e-03,  ..., -1.8311e-03,
         -1.0345e-02,  1.9730e-02],
        [-3.9124e-02, -9.6512e-03,  5.4321e-03,  ...,  9.8877e-03,
         -3.2616e-03,  6.5088e-04]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.4512,  3.8984, -4.5117,  ..., -2.4551,  3.1836, -1.6816]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 19:41:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for package is parcel
Another word for lady is madam
Another word for honest is sincere
Another word for identical is same
Another word for vocabulary is lexicon
Another word for phone is telephone
Another word for airplane is aeroplane
Another word for sweets is
2024-07-18 19:41:27 root INFO     [order_1_approx] starting weight calculation for Another word for phone is telephone
Another word for sweets is confectionery
Another word for airplane is aeroplane
Another word for honest is sincere
Another word for vocabulary is lexicon
Another word for identical is same
Another word for package is parcel
Another word for lady is
2024-07-18 19:41:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 19:45:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4412, -0.0949, -0.5430,  ...,  0.7617, -0.2578,  0.6099],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1943,  0.1577, -1.6426,  ..., -3.3086,  1.8252, -5.0703],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0320, -0.0007, -0.0069,  ..., -0.0036, -0.0029, -0.0185],
        [-0.0009,  0.0009, -0.0012,  ...,  0.0041, -0.0102,  0.0058],
        [ 0.0234, -0.0023, -0.0052,  ..., -0.0038, -0.0095, -0.0025],
        ...,
        [-0.0017,  0.0028,  0.0009,  ...,  0.0131, -0.0098, -0.0074],
        [ 0.0040, -0.0156, -0.0064,  ..., -0.0098, -0.0007, -0.0063],
        [-0.0385, -0.0098,  0.0052,  ...,  0.0047, -0.0100,  0.0034]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6699,  0.5376, -2.5430,  ..., -4.1875,  1.6846, -5.8984]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 19:45:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for phone is telephone
Another word for sweets is confectionery
Another word for airplane is aeroplane
Another word for honest is sincere
Another word for vocabulary is lexicon
Another word for identical is same
Another word for package is parcel
Another word for lady is
2024-07-18 19:45:47 root INFO     [order_1_approx] starting weight calculation for Another word for sweets is confectionery
Another word for vocabulary is lexicon
Another word for identical is same
Another word for phone is telephone
Another word for airplane is aeroplane
Another word for lady is madam
Another word for package is parcel
Another word for honest is
2024-07-18 19:45:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 19:50:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3118, -0.4099, -0.8799,  ...,  0.2773, -0.2930, -0.0679],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.2734,  2.7090, -4.4297,  ..., -0.2734,  2.3145, -0.0840],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0085,  0.0031,  0.0042,  ..., -0.0008, -0.0044,  0.0012],
        [ 0.0140,  0.0184, -0.0049,  ...,  0.0172,  0.0004,  0.0210],
        [ 0.0014, -0.0031,  0.0004,  ..., -0.0068, -0.0056,  0.0121],
        ...,
        [-0.0023,  0.0153, -0.0042,  ...,  0.0114,  0.0014, -0.0031],
        [-0.0032,  0.0045, -0.0110,  ...,  0.0054,  0.0025, -0.0030],
        [ 0.0062,  0.0037, -0.0025,  ...,  0.0063,  0.0067,  0.0051]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7930,  1.2070, -4.5703,  ..., -0.2202,  2.3105, -0.1974]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 19:50:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for sweets is confectionery
Another word for vocabulary is lexicon
Another word for identical is same
Another word for phone is telephone
Another word for airplane is aeroplane
Another word for lady is madam
Another word for package is parcel
Another word for honest is
2024-07-18 19:50:09 root INFO     [order_1_approx] starting weight calculation for Another word for vocabulary is lexicon
Another word for identical is same
Another word for honest is sincere
Another word for sweets is confectionery
Another word for phone is telephone
Another word for lady is madam
Another word for airplane is aeroplane
Another word for package is
2024-07-18 19:50:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 19:54:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1487,  0.0677, -0.4165,  ..., -0.4758, -0.3618,  0.5737],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.1699, -0.8394, -2.7051,  ...,  1.2227,  1.6387,  2.8301],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0091,  0.0134, -0.0052,  ..., -0.0062, -0.0037,  0.0015],
        [-0.0122, -0.0025,  0.0027,  ...,  0.0073, -0.0029, -0.0009],
        [-0.0131,  0.0015, -0.0055,  ..., -0.0131, -0.0116,  0.0096],
        ...,
        [ 0.0079,  0.0071,  0.0130,  ...,  0.0002,  0.0053, -0.0025],
        [ 0.0072, -0.0081, -0.0089,  ..., -0.0220, -0.0010,  0.0094],
        [-0.0004, -0.0112, -0.0063,  ..., -0.0123,  0.0078,  0.0101]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.9053,  0.0737, -2.7852,  ...,  0.6641,  2.0137,  1.9141]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 19:54:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for vocabulary is lexicon
Another word for identical is same
Another word for honest is sincere
Another word for sweets is confectionery
Another word for phone is telephone
Another word for lady is madam
Another word for airplane is aeroplane
Another word for package is
2024-07-18 19:54:27 root INFO     [order_1_approx] starting weight calculation for Another word for lady is madam
Another word for identical is same
Another word for package is parcel
Another word for honest is sincere
Another word for vocabulary is lexicon
Another word for sweets is confectionery
Another word for airplane is aeroplane
Another word for phone is
2024-07-18 19:54:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 19:58:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2334, -0.4541, -0.8286,  ...,  0.3774, -0.4326,  0.0985],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0625,  2.6621, -3.5234,  ..., -3.2227,  2.4922,  2.7676],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0107, -0.0167, -0.0027,  ..., -0.0014,  0.0024,  0.0040],
        [ 0.0032, -0.0024, -0.0078,  ...,  0.0154, -0.0151, -0.0028],
        [-0.0043, -0.0131, -0.0004,  ..., -0.0083,  0.0040,  0.0203],
        ...,
        [-0.0069, -0.0005,  0.0221,  ...,  0.0098,  0.0060,  0.0016],
        [-0.0080,  0.0002,  0.0048,  ..., -0.0046, -0.0072, -0.0011],
        [-0.0056, -0.0033,  0.0161,  ...,  0.0008,  0.0150, -0.0050]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4702,  2.8496, -3.3965,  ..., -1.8975,  2.4922,  2.2852]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 19:58:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for lady is madam
Another word for identical is same
Another word for package is parcel
Another word for honest is sincere
Another word for vocabulary is lexicon
Another word for sweets is confectionery
Another word for airplane is aeroplane
Another word for phone is
2024-07-18 19:58:47 root INFO     [order_1_approx] starting weight calculation for Another word for honest is sincere
Another word for package is parcel
Another word for lady is madam
Another word for vocabulary is lexicon
Another word for phone is telephone
Another word for airplane is aeroplane
Another word for sweets is confectionery
Another word for identical is
2024-07-18 19:58:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 20:03:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6016,  0.7100, -0.1859,  ..., -0.1934, -0.7266, -0.8369],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7524, -1.1123, -6.8438,  ...,  1.0469,  1.9590, -2.5273],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0146, -0.0023,  0.0062,  ...,  0.0104, -0.0137,  0.0042],
        [-0.0177,  0.0021,  0.0048,  ..., -0.0005,  0.0058,  0.0117],
        [ 0.0139, -0.0169, -0.0094,  ..., -0.0179, -0.0305, -0.0067],
        ...,
        [-0.0279, -0.0105,  0.0209,  ...,  0.0035,  0.0011,  0.0054],
        [ 0.0095,  0.0066, -0.0198,  ...,  0.0052, -0.0051,  0.0086],
        [-0.0056, -0.0004, -0.0147,  ..., -0.0027,  0.0071, -0.0170]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1572, -2.2188, -6.1641,  ...,  0.8564,  1.8418, -2.4590]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 20:03:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for honest is sincere
Another word for package is parcel
Another word for lady is madam
Another word for vocabulary is lexicon
Another word for phone is telephone
Another word for airplane is aeroplane
Another word for sweets is confectionery
Another word for identical is
2024-07-18 20:03:08 root INFO     [order_1_approx] starting weight calculation for Another word for honest is sincere
Another word for sweets is confectionery
Another word for phone is telephone
Another word for package is parcel
Another word for lady is madam
Another word for identical is same
Another word for vocabulary is lexicon
Another word for airplane is
2024-07-18 20:03:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 20:07:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4062,  0.3950, -0.3752,  ..., -0.4521, -0.9028,  0.3420],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.0840, -0.4482, -0.5098,  ..., -2.8555,  1.2861, -4.5938],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0048, -0.0088,  0.0244,  ..., -0.0019, -0.0153,  0.0072],
        [ 0.0099, -0.0246,  0.0008,  ...,  0.0240, -0.0118,  0.0141],
        [-0.0201,  0.0106, -0.0190,  ..., -0.0175, -0.0025,  0.0100],
        ...,
        [-0.0051,  0.0059,  0.0174,  ...,  0.0062, -0.0139,  0.0078],
        [ 0.0067, -0.0276, -0.0111,  ...,  0.0146, -0.0035,  0.0352],
        [ 0.0222, -0.0032,  0.0091,  ..., -0.0096, -0.0184,  0.0031]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.3516, -0.4075, -1.7168,  ..., -1.5439,  1.3301, -3.8633]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 20:07:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for honest is sincere
Another word for sweets is confectionery
Another word for phone is telephone
Another word for package is parcel
Another word for lady is madam
Another word for identical is same
Another word for vocabulary is lexicon
Another word for airplane is
2024-07-18 20:07:30 root INFO     [order_1_approx] starting weight calculation for Another word for phone is telephone
Another word for sweets is confectionery
Another word for lady is madam
Another word for honest is sincere
Another word for package is parcel
Another word for airplane is aeroplane
Another word for identical is same
Another word for vocabulary is
2024-07-18 20:07:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 20:11:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2791,  0.2778,  1.2070,  ..., -0.3359,  0.0708,  0.2827],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5781,  2.1445, -6.2422,  ...,  1.8545,  1.4033, -0.2036],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0041, -0.0334,  0.0183,  ...,  0.0002, -0.0138,  0.0155],
        [ 0.0031,  0.0057,  0.0135,  ...,  0.0188, -0.0104,  0.0127],
        [-0.0029, -0.0043, -0.0258,  ..., -0.0388, -0.0149,  0.0252],
        ...,
        [-0.0230,  0.0237, -0.0072,  ...,  0.0138,  0.0244,  0.0210],
        [ 0.0236, -0.0278, -0.0036,  ..., -0.0192, -0.0049, -0.0027],
        [-0.0075, -0.0249,  0.0222,  ...,  0.0012,  0.0048, -0.0126]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.0898,  1.0244, -6.8125,  ...,  1.8555,  1.0010,  0.1633]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 20:11:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Another word for phone is telephone
Another word for sweets is confectionery
Another word for lady is madam
Another word for honest is sincere
Another word for package is parcel
Another word for airplane is aeroplane
Another word for identical is same
Another word for vocabulary is
2024-07-18 20:11:51 root INFO     total operator prediction time: 2084.319908618927 seconds
2024-07-18 20:11:51 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on hypernyms - misc
2024-07-18 20:11:51 root INFO     building operator hypernyms - misc
2024-07-18 20:11:52 root INFO     [order_1_approx] starting weight calculation for The shirt falls into the category of clothes
The peach falls into the category of fruit
The grapefruit falls into the category of citrus
The lotion falls into the category of toiletry
The hairpin falls into the category of pin
The deodorant falls into the category of toiletry
The diary falls into the category of journal
The dress falls into the category of
2024-07-18 20:11:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 20:16:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1021, -0.3315,  0.4646,  ..., -0.1216, -0.2783, -0.5723],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.4980, -0.9551, -0.8730,  ..., -3.1133, -0.8057, -2.3164],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.1342e-02, -8.9798e-03, -4.8103e-03,  ...,  8.3923e-04,
          1.6998e-02, -1.3031e-02],
        [-3.1036e-02, -5.1117e-03,  1.3733e-02,  ...,  2.6550e-03,
         -1.2047e-02,  2.1255e-02],
        [ 1.1620e-02, -7.8125e-03, -1.5701e-02,  ..., -1.0582e-02,
         -1.2711e-02, -7.0190e-03],
        ...,
        [-9.5291e-03,  2.4490e-03, -3.7384e-04,  ..., -4.6921e-03,
          6.8512e-03,  1.7990e-02],
        [ 2.7435e-02, -1.5961e-02,  4.0207e-03,  ...,  3.2425e-05,
          2.3956e-02,  8.0719e-03],
        [-3.6278e-03, -6.2943e-03,  1.1795e-02,  ...,  2.1957e-02,
          1.2909e-02,  1.4969e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8521, -1.0020, -1.1211,  ..., -3.7012, -0.8618, -2.3379]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 20:16:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The shirt falls into the category of clothes
The peach falls into the category of fruit
The grapefruit falls into the category of citrus
The lotion falls into the category of toiletry
The hairpin falls into the category of pin
The deodorant falls into the category of toiletry
The diary falls into the category of journal
The dress falls into the category of
2024-07-18 20:16:14 root INFO     [order_1_approx] starting weight calculation for The diary falls into the category of journal
The dress falls into the category of clothes
The hairpin falls into the category of pin
The lotion falls into the category of toiletry
The shirt falls into the category of clothes
The deodorant falls into the category of toiletry
The grapefruit falls into the category of citrus
The peach falls into the category of
2024-07-18 20:16:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 20:20:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8247, -0.3213, -0.1476,  ..., -1.0430, -0.4885, -0.6309],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.1934, -3.9805, -2.3125,  ..., -2.4668, -1.3838,  0.3921],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0094, -0.0044, -0.0027,  ..., -0.0058,  0.0180,  0.0083],
        [ 0.0101,  0.0118, -0.0025,  ...,  0.0229, -0.0021,  0.0055],
        [ 0.0169, -0.0089,  0.0069,  ...,  0.0056, -0.0016,  0.0149],
        ...,
        [-0.0003, -0.0184, -0.0026,  ..., -0.0042, -0.0156,  0.0009],
        [-0.0094,  0.0059,  0.0034,  ...,  0.0048,  0.0043,  0.0007],
        [ 0.0007,  0.0014,  0.0037,  ..., -0.0131, -0.0022,  0.0128]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.6992, -4.2266, -2.6387,  ..., -2.6289, -2.6406,  0.1429]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 20:20:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The diary falls into the category of journal
The dress falls into the category of clothes
The hairpin falls into the category of pin
The lotion falls into the category of toiletry
The shirt falls into the category of clothes
The deodorant falls into the category of toiletry
The grapefruit falls into the category of citrus
The peach falls into the category of
2024-07-18 20:20:37 root INFO     [order_1_approx] starting weight calculation for The deodorant falls into the category of toiletry
The peach falls into the category of fruit
The lotion falls into the category of toiletry
The shirt falls into the category of clothes
The dress falls into the category of clothes
The grapefruit falls into the category of citrus
The hairpin falls into the category of pin
The diary falls into the category of
2024-07-18 20:20:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 20:24:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7539, -0.0554,  0.3267,  ..., -0.4985, -0.2212,  0.1030],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.0684, -1.0781, -1.0371,  ...,  1.7539,  0.5410,  0.9072],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0605, -0.0076,  0.0002,  ...,  0.0036, -0.0100,  0.0230],
        [-0.0198, -0.0036,  0.0189,  ...,  0.0126, -0.0042, -0.0017],
        [ 0.0093,  0.0053,  0.0037,  ...,  0.0051, -0.0121,  0.0029],
        ...,
        [-0.0306, -0.0323,  0.0235,  ..., -0.0127,  0.0236, -0.0091],
        [ 0.0367, -0.0130,  0.0221,  ..., -0.0117,  0.0145,  0.0021],
        [-0.0146, -0.0182, -0.0004,  ..., -0.0136,  0.0246,  0.0242]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.8281, -1.0908, -2.1016,  ...,  1.0840, -0.4297,  1.1338]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 20:25:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The deodorant falls into the category of toiletry
The peach falls into the category of fruit
The lotion falls into the category of toiletry
The shirt falls into the category of clothes
The dress falls into the category of clothes
The grapefruit falls into the category of citrus
The hairpin falls into the category of pin
The diary falls into the category of
2024-07-18 20:25:00 root INFO     [order_1_approx] starting weight calculation for The peach falls into the category of fruit
The diary falls into the category of journal
The shirt falls into the category of clothes
The grapefruit falls into the category of citrus
The lotion falls into the category of toiletry
The deodorant falls into the category of toiletry
The dress falls into the category of clothes
The hairpin falls into the category of
2024-07-18 20:25:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 20:29:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8110,  0.0315,  0.0231,  ...,  0.3696,  0.1108,  0.4500],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-5.0078, -1.3613, -3.9648,  ..., -0.3567, -2.3984,  3.7266],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0242, -0.0035,  0.0123,  ..., -0.0015, -0.0041,  0.0115],
        [-0.0124,  0.0066,  0.0005,  ...,  0.0088,  0.0074,  0.0084],
        [ 0.0125, -0.0085,  0.0018,  ...,  0.0061,  0.0026,  0.0042],
        ...,
        [ 0.0054, -0.0092,  0.0100,  ..., -0.0007, -0.0165,  0.0175],
        [ 0.0226, -0.0042,  0.0120,  ...,  0.0041, -0.0096,  0.0011],
        [-0.0035, -0.0056, -0.0060,  ..., -0.0022,  0.0077,  0.0074]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.2305, -1.3535, -4.0547,  ..., -1.5859, -2.3359,  3.7227]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 20:29:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The peach falls into the category of fruit
The diary falls into the category of journal
The shirt falls into the category of clothes
The grapefruit falls into the category of citrus
The lotion falls into the category of toiletry
The deodorant falls into the category of toiletry
The dress falls into the category of clothes
The hairpin falls into the category of
2024-07-18 20:29:22 root INFO     [order_1_approx] starting weight calculation for The dress falls into the category of clothes
The diary falls into the category of journal
The peach falls into the category of fruit
The hairpin falls into the category of pin
The deodorant falls into the category of toiletry
The grapefruit falls into the category of citrus
The shirt falls into the category of clothes
The lotion falls into the category of
2024-07-18 20:29:22 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 20:33:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4429, -0.0637,  0.0515,  ..., -0.4468, -0.1653, -0.6968],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7715, -2.4863,  0.2451,  ..., -1.5371, -1.8760, -0.7002],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0362, -0.0104, -0.0139,  ..., -0.0162,  0.0182,  0.0120],
        [-0.0122,  0.0070,  0.0079,  ..., -0.0017, -0.0039,  0.0128],
        [-0.0157,  0.0033,  0.0075,  ...,  0.0033, -0.0108, -0.0026],
        ...,
        [ 0.0032, -0.0059,  0.0105,  ..., -0.0038,  0.0103,  0.0110],
        [ 0.0322, -0.0160, -0.0195,  ..., -0.0160,  0.0223,  0.0060],
        [-0.0122,  0.0052,  0.0116,  ...,  0.0205,  0.0113,  0.0104]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2744, -1.9199, -0.1550,  ..., -2.1309, -1.5361, -1.7334]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 20:33:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The dress falls into the category of clothes
The diary falls into the category of journal
The peach falls into the category of fruit
The hairpin falls into the category of pin
The deodorant falls into the category of toiletry
The grapefruit falls into the category of citrus
The shirt falls into the category of clothes
The lotion falls into the category of
2024-07-18 20:33:43 root INFO     [order_1_approx] starting weight calculation for The deodorant falls into the category of toiletry
The shirt falls into the category of clothes
The peach falls into the category of fruit
The hairpin falls into the category of pin
The lotion falls into the category of toiletry
The diary falls into the category of journal
The dress falls into the category of clothes
The grapefruit falls into the category of
2024-07-18 20:33:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 20:38:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4194, -0.4775,  0.3494,  ...,  0.2837, -0.2041,  0.1066],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.0430, -1.2129, -0.2930,  ..., -0.5068, -3.1152, -1.5723],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0040, -0.0069,  0.0112,  ...,  0.0082,  0.0164,  0.0004],
        [-0.0144,  0.0073,  0.0085,  ...,  0.0030,  0.0011,  0.0073],
        [ 0.0069,  0.0009, -0.0107,  ..., -0.0039, -0.0089, -0.0085],
        ...,
        [-0.0008, -0.0050,  0.0222,  ...,  0.0035, -0.0047,  0.0006],
        [ 0.0176,  0.0083, -0.0003,  ..., -0.0217, -0.0248, -0.0003],
        [ 0.0014,  0.0010, -0.0030,  ..., -0.0138, -0.0181,  0.0054]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.4922, -1.0713, -0.1960,  ..., -0.3408, -3.2676, -1.6377]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 20:38:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The deodorant falls into the category of toiletry
The shirt falls into the category of clothes
The peach falls into the category of fruit
The hairpin falls into the category of pin
The lotion falls into the category of toiletry
The diary falls into the category of journal
The dress falls into the category of clothes
The grapefruit falls into the category of
2024-07-18 20:38:04 root INFO     [order_1_approx] starting weight calculation for The peach falls into the category of fruit
The grapefruit falls into the category of citrus
The deodorant falls into the category of toiletry
The hairpin falls into the category of pin
The lotion falls into the category of toiletry
The dress falls into the category of clothes
The diary falls into the category of journal
The shirt falls into the category of
2024-07-18 20:38:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 20:42:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0353, -0.4243,  0.3013,  ..., -0.3733,  0.1921,  0.3003],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5869, -1.1465, -0.9854,  ..., -6.0469,  0.4124, -1.9971],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0229,  0.0029,  0.0135,  ..., -0.0096, -0.0108,  0.0043],
        [-0.0141, -0.0093,  0.0129,  ...,  0.0108, -0.0069,  0.0072],
        [-0.0024,  0.0004, -0.0102,  ..., -0.0019,  0.0036,  0.0019],
        ...,
        [-0.0029,  0.0015,  0.0081,  ...,  0.0031,  0.0019,  0.0094],
        [ 0.0075, -0.0133, -0.0021,  ..., -0.0072, -0.0077,  0.0064],
        [-0.0010, -0.0013,  0.0018,  ..., -0.0156,  0.0210,  0.0167]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4629, -2.3555, -0.5620,  ..., -4.6641,  0.8970, -0.6611]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 20:42:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The peach falls into the category of fruit
The grapefruit falls into the category of citrus
The deodorant falls into the category of toiletry
The hairpin falls into the category of pin
The lotion falls into the category of toiletry
The dress falls into the category of clothes
The diary falls into the category of journal
The shirt falls into the category of
2024-07-18 20:42:24 root INFO     [order_1_approx] starting weight calculation for The grapefruit falls into the category of citrus
The peach falls into the category of fruit
The diary falls into the category of journal
The dress falls into the category of clothes
The lotion falls into the category of toiletry
The shirt falls into the category of clothes
The hairpin falls into the category of pin
The deodorant falls into the category of
2024-07-18 20:42:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 20:46:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0681, -0.8135,  0.2126,  ..., -1.2510,  0.9263,  0.4492],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.5000, -2.6973, -0.6455,  ..., -4.5820, -1.1914,  1.1699],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.6459e-02,  8.1158e-04, -8.7738e-03,  ..., -1.3802e-02,
         -7.4844e-03,  1.6083e-02],
        [ 2.6131e-04,  1.0025e-02, -6.0368e-04,  ...,  1.3275e-02,
          4.3869e-04,  3.6297e-03],
        [-1.1971e-02, -2.2221e-03,  2.1152e-03,  ...,  1.7242e-03,
         -2.3270e-04, -1.0681e-02],
        ...,
        [-5.6152e-03, -1.3657e-02, -5.0545e-05,  ..., -2.8000e-03,
          1.5251e-02,  7.3547e-03],
        [ 2.3941e-02, -1.2451e-02, -5.5084e-03,  ..., -1.1154e-02,
          1.4999e-02,  1.4648e-02],
        [ 1.4114e-03, -8.1635e-03, -1.0815e-03,  ..., -7.0572e-03,
          8.2550e-03,  1.5465e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.8262, -2.0078, -0.7671,  ..., -5.4141, -2.0605,  0.6558]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 20:46:38 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The grapefruit falls into the category of citrus
The peach falls into the category of fruit
The diary falls into the category of journal
The dress falls into the category of clothes
The lotion falls into the category of toiletry
The shirt falls into the category of clothes
The hairpin falls into the category of pin
The deodorant falls into the category of
2024-07-18 20:46:38 root INFO     total operator prediction time: 2087.0403327941895 seconds
2024-07-18 20:46:38 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - substance
2024-07-18 20:46:38 root INFO     building operator meronyms - substance
2024-07-18 20:46:39 root INFO     [order_1_approx] starting weight calculation for A cocktail is made up of alcohol
A glass is made up of silicone
A diamond is made up of carbon
A money is made up of paper
A bag is made up of leather
A concrete is made up of silicon
A wall is made up of cement
A bronze is made up of
2024-07-18 20:46:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 20:50:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.7363, -0.2495, -0.3242,  ...,  0.2590, -0.3623, -0.3093],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4985,  0.2866,  0.1768,  ..., -3.8887,  0.9375,  0.9561],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0175, -0.0097, -0.0093,  ...,  0.0107, -0.0151, -0.0173],
        [ 0.0062, -0.0086,  0.0017,  ...,  0.0102,  0.0229,  0.0049],
        [-0.0012,  0.0126,  0.0026,  ...,  0.0152, -0.0056,  0.0136],
        ...,
        [-0.0133, -0.0052, -0.0051,  ..., -0.0209, -0.0144, -0.0131],
        [ 0.0018, -0.0030, -0.0011,  ...,  0.0037, -0.0122, -0.0012],
        [-0.0035,  0.0139, -0.0037,  ...,  0.0060,  0.0050,  0.0004]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5679, -2.2930,  0.8555,  ..., -2.9023,  0.6123,  2.1738]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 20:50:57 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A cocktail is made up of alcohol
A glass is made up of silicone
A diamond is made up of carbon
A money is made up of paper
A bag is made up of leather
A concrete is made up of silicon
A wall is made up of cement
A bronze is made up of
2024-07-18 20:50:57 root INFO     [order_1_approx] starting weight calculation for A cocktail is made up of alcohol
A concrete is made up of silicon
A glass is made up of silicone
A money is made up of paper
A diamond is made up of carbon
A bag is made up of leather
A bronze is made up of copper
A wall is made up of
2024-07-18 20:50:57 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 20:55:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9736,  0.2896,  0.2976,  ..., -0.5352, -0.3523,  0.0846],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.3008,  0.3518, -2.6484,  ..., -0.1512, -3.9238, -0.3320],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0180, -0.0077,  0.0028,  ..., -0.0073, -0.0080, -0.0136],
        [ 0.0056,  0.0077,  0.0007,  ...,  0.0154,  0.0195,  0.0024],
        [ 0.0021,  0.0249, -0.0049,  ..., -0.0085, -0.0051,  0.0007],
        ...,
        [-0.0052, -0.0199, -0.0032,  ..., -0.0006, -0.0090,  0.0050],
        [-0.0124, -0.0080,  0.0167,  ..., -0.0198, -0.0077, -0.0044],
        [-0.0057, -0.0126, -0.0038,  ...,  0.0053,  0.0071, -0.0060]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.6680,  0.1555, -3.7188,  ...,  0.0626, -4.6094, -0.7515]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 20:55:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A cocktail is made up of alcohol
A concrete is made up of silicon
A glass is made up of silicone
A money is made up of paper
A diamond is made up of carbon
A bag is made up of leather
A bronze is made up of copper
A wall is made up of
2024-07-18 20:55:18 root INFO     [order_1_approx] starting weight calculation for A diamond is made up of carbon
A concrete is made up of silicon
A wall is made up of cement
A money is made up of paper
A bag is made up of leather
A cocktail is made up of alcohol
A bronze is made up of copper
A glass is made up of
2024-07-18 20:55:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 20:59:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8447, -1.0254,  0.2715,  ..., -0.2991,  0.0662,  0.2329],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3711,  4.1016,  0.9370,  ...,  0.2949, -1.8770,  2.0332],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0077, -0.0105, -0.0126,  ..., -0.0120,  0.0014, -0.0051],
        [-0.0121,  0.0178,  0.0115,  ...,  0.0050,  0.0090,  0.0019],
        [ 0.0050, -0.0038, -0.0099,  ..., -0.0008,  0.0126,  0.0242],
        ...,
        [ 0.0106, -0.0010,  0.0065,  ...,  0.0088, -0.0194,  0.0064],
        [ 0.0030,  0.0142, -0.0127,  ..., -0.0084,  0.0190, -0.0054],
        [-0.0197,  0.0198,  0.0017,  ..., -0.0014, -0.0035,  0.0100]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3694,  3.7773, -0.0278,  ...,  0.7852, -1.7168,  3.1113]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 20:59:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A diamond is made up of carbon
A concrete is made up of silicon
A wall is made up of cement
A money is made up of paper
A bag is made up of leather
A cocktail is made up of alcohol
A bronze is made up of copper
A glass is made up of
2024-07-18 20:59:39 root INFO     [order_1_approx] starting weight calculation for A concrete is made up of silicon
A bag is made up of leather
A bronze is made up of copper
A wall is made up of cement
A money is made up of paper
A cocktail is made up of alcohol
A glass is made up of silicone
A diamond is made up of
2024-07-18 20:59:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 21:03:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6963, -0.6987, -0.7749,  ..., -1.3281,  0.1722,  0.2556],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6211,  0.5156,  3.4414,  ..., -1.4609, -1.3584,  1.7764],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-6.3171e-03,  4.8218e-03, -2.0103e-03,  ..., -3.9825e-03,
         -2.8229e-03,  2.1133e-03],
        [ 6.8283e-04, -1.5129e-02,  1.1749e-03,  ...,  2.3060e-03,
          7.1449e-03, -1.5640e-03],
        [ 1.3199e-02, -7.3547e-03, -7.0381e-03,  ...,  2.6512e-04,
          6.1722e-03, -8.8501e-03],
        ...,
        [ 4.3182e-03,  8.3008e-03,  2.6855e-03,  ..., -1.2573e-02,
         -1.2680e-02,  4.1819e-04],
        [ 1.1032e-02, -8.1177e-03,  5.6610e-03,  ..., -2.2659e-03,
         -1.3557e-02, -1.3229e-02],
        [ 2.0885e-03, -8.1177e-03,  1.3351e-05,  ..., -4.5242e-03,
          4.1656e-03,  2.1400e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9048,  0.8740,  1.9287,  ..., -1.2051, -2.2734,  2.0547]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 21:03:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A concrete is made up of silicon
A bag is made up of leather
A bronze is made up of copper
A wall is made up of cement
A money is made up of paper
A cocktail is made up of alcohol
A glass is made up of silicone
A diamond is made up of
2024-07-18 21:03:56 root INFO     [order_1_approx] starting weight calculation for A concrete is made up of silicon
A bag is made up of leather
A bronze is made up of copper
A wall is made up of cement
A diamond is made up of carbon
A cocktail is made up of alcohol
A glass is made up of silicone
A money is made up of
2024-07-18 21:03:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 21:08:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8389,  0.0623,  0.1254,  ...,  0.3408,  0.3657,  0.1381],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2388,  0.5889,  0.1680,  ..., -1.9482,  1.3516, -1.1758],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0067,  0.0033,  0.0137,  ..., -0.0130, -0.0265, -0.0032],
        [ 0.0122, -0.0111,  0.0036,  ..., -0.0034,  0.0182, -0.0128],
        [ 0.0039, -0.0022, -0.0153,  ...,  0.0108,  0.0069, -0.0169],
        ...,
        [-0.0177,  0.0156,  0.0075,  ..., -0.0212, -0.0014, -0.0051],
        [ 0.0094,  0.0114, -0.0203,  ...,  0.0187, -0.0126,  0.0085],
        [-0.0002, -0.0044,  0.0127,  ..., -0.0087, -0.0022,  0.0030]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7095,  0.0210, -0.7192,  ..., -1.3477,  1.6309, -1.2041]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 21:08:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A concrete is made up of silicon
A bag is made up of leather
A bronze is made up of copper
A wall is made up of cement
A diamond is made up of carbon
A cocktail is made up of alcohol
A glass is made up of silicone
A money is made up of
2024-07-18 21:08:15 root INFO     [order_1_approx] starting weight calculation for A concrete is made up of silicon
A money is made up of paper
A bag is made up of leather
A wall is made up of cement
A glass is made up of silicone
A diamond is made up of carbon
A bronze is made up of copper
A cocktail is made up of
2024-07-18 21:08:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 21:12:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5830,  0.1621, -0.6230,  ..., -1.0264, -0.0325,  0.1948],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1504, -2.8301,  0.9443,  ...,  3.9688, -2.4219, -0.0347],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0008, -0.0031, -0.0083,  ..., -0.0068, -0.0069, -0.0044],
        [ 0.0056, -0.0018,  0.0096,  ...,  0.0044,  0.0153,  0.0074],
        [ 0.0033,  0.0124, -0.0109,  ...,  0.0087,  0.0012, -0.0166],
        ...,
        [-0.0106, -0.0110, -0.0025,  ..., -0.0087, -0.0126,  0.0003],
        [-0.0040, -0.0018,  0.0026,  ...,  0.0086, -0.0092,  0.0195],
        [-0.0036, -0.0015, -0.0143,  ..., -0.0026,  0.0130, -0.0005]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2148, -2.6387,  1.1191,  ...,  3.8926, -3.9668,  0.1711]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 21:12:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A concrete is made up of silicon
A money is made up of paper
A bag is made up of leather
A wall is made up of cement
A glass is made up of silicone
A diamond is made up of carbon
A bronze is made up of copper
A cocktail is made up of
2024-07-18 21:12:35 root INFO     [order_1_approx] starting weight calculation for A wall is made up of cement
A diamond is made up of carbon
A money is made up of paper
A glass is made up of silicone
A concrete is made up of silicon
A bronze is made up of copper
A cocktail is made up of alcohol
A bag is made up of
2024-07-18 21:12:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 21:16:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8467, -0.2612,  0.1294,  ..., -0.2761, -0.5996,  0.0667],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4106, -0.3499, -0.0449,  ...,  3.2988, -2.7227, -2.5625],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0002, -0.0047, -0.0053,  ..., -0.0158, -0.0132,  0.0156],
        [ 0.0061, -0.0095,  0.0045,  ...,  0.0093,  0.0081,  0.0074],
        [ 0.0093, -0.0093,  0.0134,  ...,  0.0023,  0.0019,  0.0145],
        ...,
        [ 0.0001,  0.0062, -0.0054,  ..., -0.0116,  0.0029,  0.0006],
        [ 0.0113, -0.0035,  0.0065,  ...,  0.0112,  0.0015,  0.0204],
        [-0.0101, -0.0210, -0.0020,  ...,  0.0177,  0.0063,  0.0064]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1582,  0.2659, -0.0509,  ...,  3.3496, -2.5957, -2.9766]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 21:16:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A wall is made up of cement
A diamond is made up of carbon
A money is made up of paper
A glass is made up of silicone
A concrete is made up of silicon
A bronze is made up of copper
A cocktail is made up of alcohol
A bag is made up of
2024-07-18 21:16:55 root INFO     [order_1_approx] starting weight calculation for A bronze is made up of copper
A money is made up of paper
A diamond is made up of carbon
A glass is made up of silicone
A wall is made up of cement
A cocktail is made up of alcohol
A bag is made up of leather
A concrete is made up of
2024-07-18 21:16:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 21:21:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2480,  0.0434, -0.8125,  ...,  0.0767,  0.3643, -1.1455],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.1445,  0.3679, -2.6523,  ...,  2.7949, -1.6895,  1.4404],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0072, -0.0120, -0.0179,  ...,  0.0197, -0.0121, -0.0134],
        [ 0.0130,  0.0020,  0.0019,  ...,  0.0020,  0.0023, -0.0094],
        [ 0.0223, -0.0008,  0.0048,  ..., -0.0211, -0.0139,  0.0199],
        ...,
        [ 0.0078, -0.0066,  0.0045,  ..., -0.0107,  0.0111,  0.0169],
        [-0.0004, -0.0093,  0.0047,  ...,  0.0016, -0.0258, -0.0025],
        [-0.0074, -0.0144, -0.0046,  ..., -0.0056, -0.0150,  0.0041]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4551,  0.9717, -1.4307,  ...,  4.0000, -1.6396,  1.3525]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 21:21:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A bronze is made up of copper
A money is made up of paper
A diamond is made up of carbon
A glass is made up of silicone
A wall is made up of cement
A cocktail is made up of alcohol
A bag is made up of leather
A concrete is made up of
2024-07-18 21:21:16 root INFO     total operator prediction time: 2077.6133255958557 seconds
2024-07-18 21:21:16 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on synonyms - intensity
2024-07-18 21:21:16 root INFO     building operator synonyms - intensity
2024-07-18 21:21:16 root INFO     [order_1_approx] starting weight calculation for A more intense word for doze is sleep
A more intense word for bad is awful
A more intense word for sea is ocean
A more intense word for unhappy is miserable
A more intense word for angry is furious
A more intense word for pain is torment
A more intense word for love is adore
A more intense word for sniffles is
2024-07-18 21:21:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 21:25:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4260, -0.2385, -0.5483,  ...,  1.0400,  0.0123, -0.0171],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.3652, -2.5840,  2.8125,  ..., -0.1855,  3.7715, -0.9282],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-7.9269e-03,  1.7731e-02,  2.4612e-02,  ...,  2.9587e-02,
          3.4065e-03, -4.5700e-03],
        [ 1.2230e-02, -1.5556e-02,  8.1863e-03,  ..., -3.7365e-03,
         -6.7520e-04, -6.1035e-05],
        [ 5.0659e-03,  2.0264e-02, -1.4877e-02,  ...,  1.7471e-02,
         -6.4926e-03, -5.6534e-03],
        ...,
        [-2.1713e-02, -3.6041e-02, -1.1292e-02,  ...,  8.2703e-03,
         -1.4786e-02,  1.3695e-02],
        [-2.4231e-02, -1.2131e-02, -2.1301e-02,  ...,  5.3787e-03,
          2.7756e-02, -1.3313e-02],
        [ 4.5891e-03,  1.8173e-02, -3.6659e-03,  ..., -1.1208e-02,
          9.2850e-03, -2.8610e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.2285, -2.7109,  3.5117,  ..., -0.8267,  3.3711, -1.3301]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 21:25:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for doze is sleep
A more intense word for bad is awful
A more intense word for sea is ocean
A more intense word for unhappy is miserable
A more intense word for angry is furious
A more intense word for pain is torment
A more intense word for love is adore
A more intense word for sniffles is
2024-07-18 21:25:37 root INFO     [order_1_approx] starting weight calculation for A more intense word for angry is furious
A more intense word for sniffles is pneumonia
A more intense word for unhappy is miserable
A more intense word for bad is awful
A more intense word for sea is ocean
A more intense word for doze is sleep
A more intense word for pain is torment
A more intense word for love is
2024-07-18 21:25:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 21:29:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9556,  0.4678,  0.0460,  ...,  0.1770, -0.2279, -0.3445],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5781, -0.6748, -1.0049,  ..., -3.8125,  2.8848, -0.9404],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0036,  0.0032, -0.0135,  ..., -0.0062,  0.0055, -0.0053],
        [ 0.0181, -0.0066,  0.0104,  ..., -0.0040, -0.0129, -0.0100],
        [-0.0104,  0.0055,  0.0010,  ..., -0.0048, -0.0013,  0.0065],
        ...,
        [-0.0103,  0.0156,  0.0133,  ...,  0.0003, -0.0105, -0.0142],
        [ 0.0078,  0.0028,  0.0213,  ...,  0.0158, -0.0077, -0.0106],
        [-0.0018,  0.0017,  0.0106,  ..., -0.0094,  0.0005, -0.0159]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.9697, -0.5107, -1.9844,  ..., -3.5508,  3.2266, -1.3701]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 21:29:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for angry is furious
A more intense word for sniffles is pneumonia
A more intense word for unhappy is miserable
A more intense word for bad is awful
A more intense word for sea is ocean
A more intense word for doze is sleep
A more intense word for pain is torment
A more intense word for love is
2024-07-18 21:29:58 root INFO     [order_1_approx] starting weight calculation for A more intense word for unhappy is miserable
A more intense word for sea is ocean
A more intense word for pain is torment
A more intense word for sniffles is pneumonia
A more intense word for angry is furious
A more intense word for love is adore
A more intense word for bad is awful
A more intense word for doze is
2024-07-18 21:29:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 21:34:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2249,  0.3904, -0.5991,  ...,  0.0273,  0.2217,  0.1504],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6934,  0.0646, -1.0332,  ..., -3.8340, -0.8574,  1.7285],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0090, -0.0152,  0.0064,  ..., -0.0061,  0.0077, -0.0097],
        [-0.0072, -0.0184,  0.0146,  ..., -0.0137,  0.0074, -0.0021],
        [-0.0072,  0.0017, -0.0079,  ...,  0.0019, -0.0140,  0.0111],
        ...,
        [-0.0258,  0.0010, -0.0134,  ...,  0.0075, -0.0061,  0.0165],
        [-0.0144, -0.0074, -0.0077,  ...,  0.0089,  0.0105, -0.0125],
        [-0.0041,  0.0254, -0.0047,  ..., -0.0099,  0.0104, -0.0010]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4785,  0.1016, -0.7510,  ..., -4.2266, -1.2627,  1.4238]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 21:34:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for unhappy is miserable
A more intense word for sea is ocean
A more intense word for pain is torment
A more intense word for sniffles is pneumonia
A more intense word for angry is furious
A more intense word for love is adore
A more intense word for bad is awful
A more intense word for doze is
2024-07-18 21:34:19 root INFO     [order_1_approx] starting weight calculation for A more intense word for angry is furious
A more intense word for bad is awful
A more intense word for pain is torment
A more intense word for love is adore
A more intense word for doze is sleep
A more intense word for unhappy is miserable
A more intense word for sniffles is pneumonia
A more intense word for sea is
2024-07-18 21:34:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 21:38:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0893,  0.4756, -0.0373,  ..., -0.3735, -0.9463,  0.2397],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4287,  2.9062, -3.0000,  ...,  1.9736,  2.2617, -1.0029],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0054, -0.0166, -0.0093,  ..., -0.0118,  0.0050,  0.0022],
        [ 0.0258, -0.0215,  0.0074,  ..., -0.0103, -0.0066, -0.0086],
        [ 0.0114,  0.0206, -0.0202,  ..., -0.0011, -0.0122,  0.0131],
        ...,
        [-0.0103,  0.0125,  0.0192,  ...,  0.0067, -0.0083,  0.0093],
        [-0.0067,  0.0194, -0.0074,  ..., -0.0020, -0.0005,  0.0035],
        [-0.0041, -0.0019,  0.0057,  ...,  0.0025, -0.0101, -0.0038]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1426,  3.1367, -3.4160,  ...,  1.6074,  2.7012, -1.2217]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 21:38:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for angry is furious
A more intense word for bad is awful
A more intense word for pain is torment
A more intense word for love is adore
A more intense word for doze is sleep
A more intense word for unhappy is miserable
A more intense word for sniffles is pneumonia
A more intense word for sea is
2024-07-18 21:38:42 root INFO     [order_1_approx] starting weight calculation for A more intense word for pain is torment
A more intense word for love is adore
A more intense word for unhappy is miserable
A more intense word for sniffles is pneumonia
A more intense word for doze is sleep
A more intense word for sea is ocean
A more intense word for angry is furious
A more intense word for bad is
2024-07-18 21:38:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 21:43:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3467, -0.3521,  0.0886,  ..., -0.6890, -0.5811,  0.4028],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.3535, -1.8574, -4.2031,  ...,  3.2148,  0.5742,  0.6387],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0147,  0.0124, -0.0024,  ..., -0.0166,  0.0061,  0.0038],
        [ 0.0072,  0.0109, -0.0068,  ...,  0.0027,  0.0008, -0.0219],
        [ 0.0067,  0.0034, -0.0182,  ..., -0.0123,  0.0044, -0.0015],
        ...,
        [-0.0169, -0.0241,  0.0097,  ..., -0.0023, -0.0307,  0.0130],
        [-0.0119, -0.0077, -0.0175,  ..., -0.0042, -0.0135,  0.0112],
        [ 0.0163, -0.0115,  0.0140,  ..., -0.0035,  0.0059,  0.0008]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6748, -2.1504, -4.4961,  ...,  3.9688, -0.2983,  0.0249]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 21:43:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for pain is torment
A more intense word for love is adore
A more intense word for unhappy is miserable
A more intense word for sniffles is pneumonia
A more intense word for doze is sleep
A more intense word for sea is ocean
A more intense word for angry is furious
A more intense word for bad is
2024-07-18 21:43:04 root INFO     [order_1_approx] starting weight calculation for A more intense word for love is adore
A more intense word for doze is sleep
A more intense word for sniffles is pneumonia
A more intense word for unhappy is miserable
A more intense word for bad is awful
A more intense word for angry is furious
A more intense word for sea is ocean
A more intense word for pain is
2024-07-18 21:43:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 21:47:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1501,  0.1987,  0.4639,  ..., -0.2140, -0.5410,  0.5430],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8184,  2.0391, -6.2773,  ...,  0.7695,  1.4717, -4.3711],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0102, -0.0069, -0.0038,  ..., -0.0055,  0.0064, -0.0063],
        [ 0.0184,  0.0080,  0.0225,  ...,  0.0104, -0.0165,  0.0022],
        [ 0.0027, -0.0049, -0.0071,  ..., -0.0035,  0.0291, -0.0141],
        ...,
        [-0.0154, -0.0006,  0.0124,  ...,  0.0229, -0.0231,  0.0097],
        [-0.0131, -0.0019, -0.0179,  ..., -0.0080,  0.0232, -0.0056],
        [-0.0100, -0.0134, -0.0033,  ...,  0.0093,  0.0074, -0.0302]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4707,  2.2441, -6.1602,  ...,  0.4912,  0.5864, -4.8438]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 21:47:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for love is adore
A more intense word for doze is sleep
A more intense word for sniffles is pneumonia
A more intense word for unhappy is miserable
A more intense word for bad is awful
A more intense word for angry is furious
A more intense word for sea is ocean
A more intense word for pain is
2024-07-18 21:47:25 root INFO     [order_1_approx] starting weight calculation for A more intense word for sea is ocean
A more intense word for unhappy is miserable
A more intense word for pain is torment
A more intense word for doze is sleep
A more intense word for sniffles is pneumonia
A more intense word for bad is awful
A more intense word for love is adore
A more intense word for angry is
2024-07-18 21:47:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 21:51:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1974,  0.2018,  0.5210,  ...,  0.0910, -0.4766,  0.3201],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0488, -4.8633, -1.6953,  ...,  6.1758,  8.0156, -0.9731],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0026, -0.0160,  0.0151,  ...,  0.0132,  0.0012, -0.0065],
        [ 0.0087,  0.0102, -0.0061,  ..., -0.0055, -0.0129, -0.0049],
        [-0.0122, -0.0005,  0.0076,  ..., -0.0050, -0.0046,  0.0029],
        ...,
        [-0.0052, -0.0271,  0.0072,  ...,  0.0188, -0.0259, -0.0017],
        [ 0.0016, -0.0275,  0.0041,  ...,  0.0033, -0.0045, -0.0105],
        [-0.0010,  0.0037, -0.0097,  ..., -0.0232,  0.0020,  0.0231]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7539, -4.8828, -1.7656,  ...,  6.7383,  9.4062, -2.4492]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 21:51:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for sea is ocean
A more intense word for unhappy is miserable
A more intense word for pain is torment
A more intense word for doze is sleep
A more intense word for sniffles is pneumonia
A more intense word for bad is awful
A more intense word for love is adore
A more intense word for angry is
2024-07-18 21:51:46 root INFO     [order_1_approx] starting weight calculation for A more intense word for love is adore
A more intense word for doze is sleep
A more intense word for pain is torment
A more intense word for sea is ocean
A more intense word for bad is awful
A more intense word for angry is furious
A more intense word for sniffles is pneumonia
A more intense word for unhappy is
2024-07-18 21:51:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 21:56:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-6.4941e-02,  2.1338e-01, -2.4414e-04,  ...,  2.9761e-01,
        -1.2830e-01,  4.4507e-01], device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1069, -1.3018, -5.0547,  ...,  4.0234,  0.2710,  0.1885],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0020, -0.0029, -0.0079,  ...,  0.0087, -0.0005, -0.0085],
        [-0.0034, -0.0062, -0.0065,  ...,  0.0227, -0.0096, -0.0179],
        [-0.0006,  0.0241, -0.0179,  ..., -0.0209,  0.0037,  0.0006],
        ...,
        [-0.0142, -0.0222,  0.0071,  ...,  0.0203, -0.0186,  0.0068],
        [ 0.0047,  0.0280,  0.0186,  ..., -0.0257,  0.0148, -0.0070],
        [-0.0112,  0.0255, -0.0056,  ..., -0.0100, -0.0152,  0.0022]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5059, -1.5186, -6.5625,  ...,  4.7578, -0.5044, -1.4424]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 21:56:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for love is adore
A more intense word for doze is sleep
A more intense word for pain is torment
A more intense word for sea is ocean
A more intense word for bad is awful
A more intense word for angry is furious
A more intense word for sniffles is pneumonia
A more intense word for unhappy is
2024-07-18 21:56:07 root INFO     total operator prediction time: 2090.9641196727753 seconds
2024-07-18 21:56:07 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on hypernyms - animals
2024-07-18 21:56:07 root INFO     building operator hypernyms - animals
2024-07-18 21:56:07 root INFO     [order_1_approx] starting weight calculation for The bee falls into the category of insect
The chimpanzee falls into the category of primate
The cockroach falls into the category of insect
The gibbon falls into the category of primate
The lion falls into the category of feline
The turkey falls into the category of fowl
The jaguar falls into the category of feline
The viper falls into the category of
2024-07-18 21:56:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 22:00:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7026, -0.1150, -0.5020,  ..., -0.3296, -0.0908, -0.2544],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.1992, -0.6138, -3.7539,  ..., -0.1782, -2.1055,  0.9727],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0132, -0.0115, -0.0011,  ...,  0.0048,  0.0082, -0.0016],
        [-0.0212,  0.0202,  0.0096,  ...,  0.0103, -0.0238, -0.0049],
        [ 0.0261, -0.0021, -0.0162,  ...,  0.0096,  0.0062,  0.0311],
        ...,
        [-0.0200, -0.0223,  0.0145,  ...,  0.0180, -0.0235,  0.0010],
        [ 0.0208, -0.0068,  0.0113,  ...,  0.0056,  0.0254,  0.0082],
        [-0.0276, -0.0091,  0.0102,  ..., -0.0120, -0.0133, -0.0154]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.9414, -0.2620, -3.6055,  ..., -0.8115, -2.8574,  1.3848]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 22:00:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The bee falls into the category of insect
The chimpanzee falls into the category of primate
The cockroach falls into the category of insect
The gibbon falls into the category of primate
The lion falls into the category of feline
The turkey falls into the category of fowl
The jaguar falls into the category of feline
The viper falls into the category of
2024-07-18 22:00:29 root INFO     [order_1_approx] starting weight calculation for The gibbon falls into the category of primate
The viper falls into the category of snake
The turkey falls into the category of fowl
The cockroach falls into the category of insect
The jaguar falls into the category of feline
The chimpanzee falls into the category of primate
The bee falls into the category of insect
The lion falls into the category of
2024-07-18 22:00:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 22:04:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3315, -0.6831, -0.2976,  ...,  0.5781,  0.1255,  0.1667],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.9727, -0.0668,  0.6006,  ..., -3.0898, -6.5859,  3.0254],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0083, -0.0130, -0.0071,  ...,  0.0154, -0.0023, -0.0028],
        [-0.0162,  0.0104,  0.0060,  ...,  0.0055,  0.0017, -0.0128],
        [-0.0041, -0.0036,  0.0032,  ...,  0.0068, -0.0032,  0.0037],
        ...,
        [-0.0073, -0.0063,  0.0077,  ...,  0.0044, -0.0050,  0.0018],
        [ 0.0290, -0.0141,  0.0052,  ..., -0.0019,  0.0003,  0.0211],
        [-0.0117,  0.0061,  0.0142,  ...,  0.0124, -0.0018,  0.0130]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.8906,  1.0029,  1.0459,  ..., -2.6953, -7.5000,  2.9941]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 22:04:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The gibbon falls into the category of primate
The viper falls into the category of snake
The turkey falls into the category of fowl
The cockroach falls into the category of insect
The jaguar falls into the category of feline
The chimpanzee falls into the category of primate
The bee falls into the category of insect
The lion falls into the category of
2024-07-18 22:04:50 root INFO     [order_1_approx] starting weight calculation for The turkey falls into the category of fowl
The bee falls into the category of insect
The jaguar falls into the category of feline
The gibbon falls into the category of primate
The chimpanzee falls into the category of primate
The lion falls into the category of feline
The viper falls into the category of snake
The cockroach falls into the category of
2024-07-18 22:04:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 22:09:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0625, -0.9482, -0.3892,  ..., -0.2917, -0.4580, -0.7432],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.0645, -1.8105,  1.0254,  ..., -1.9297, -3.8359,  1.7129],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0048, -0.0089, -0.0032,  ...,  0.0022,  0.0010,  0.0022],
        [-0.0221,  0.0061,  0.0107,  ...,  0.0068, -0.0107,  0.0257],
        [ 0.0081, -0.0046, -0.0017,  ...,  0.0048,  0.0077,  0.0055],
        ...,
        [-0.0059, -0.0048,  0.0039,  ...,  0.0149,  0.0083,  0.0019],
        [-0.0033, -0.0104,  0.0016,  ..., -0.0128,  0.0265,  0.0069],
        [-0.0386,  0.0132,  0.0099,  ..., -0.0045, -0.0019,  0.0138]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.3027, -2.3125,  0.9028,  ..., -2.3320, -4.3242,  1.8428]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 22:09:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The turkey falls into the category of fowl
The bee falls into the category of insect
The jaguar falls into the category of feline
The gibbon falls into the category of primate
The chimpanzee falls into the category of primate
The lion falls into the category of feline
The viper falls into the category of snake
The cockroach falls into the category of
2024-07-18 22:09:13 root INFO     [order_1_approx] starting weight calculation for The jaguar falls into the category of feline
The viper falls into the category of snake
The lion falls into the category of feline
The gibbon falls into the category of primate
The bee falls into the category of insect
The chimpanzee falls into the category of primate
The cockroach falls into the category of insect
The turkey falls into the category of
2024-07-18 22:09:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 22:13:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0299, -0.1985,  0.3003,  ..., -0.0242, -0.7583,  0.4568],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4844,  0.7402, -2.1680,  ..., -2.6172, -1.1689,  0.0732],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0002, -0.0037, -0.0057,  ...,  0.0066, -0.0036,  0.0075],
        [ 0.0033, -0.0005,  0.0041,  ...,  0.0069, -0.0047, -0.0056],
        [ 0.0035,  0.0079, -0.0102,  ..., -0.0026, -0.0062, -0.0123],
        ...,
        [-0.0188, -0.0173,  0.0056,  ...,  0.0062, -0.0014,  0.0106],
        [ 0.0042, -0.0061,  0.0005,  ..., -0.0032,  0.0012,  0.0051],
        [-0.0096,  0.0008,  0.0077,  ..., -0.0076, -0.0040, -0.0087]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4336,  1.2158, -2.2949,  ..., -3.2852, -1.5186,  0.2695]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 22:13:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The jaguar falls into the category of feline
The viper falls into the category of snake
The lion falls into the category of feline
The gibbon falls into the category of primate
The bee falls into the category of insect
The chimpanzee falls into the category of primate
The cockroach falls into the category of insect
The turkey falls into the category of
2024-07-18 22:13:34 root INFO     [order_1_approx] starting weight calculation for The turkey falls into the category of fowl
The bee falls into the category of insect
The chimpanzee falls into the category of primate
The lion falls into the category of feline
The gibbon falls into the category of primate
The viper falls into the category of snake
The cockroach falls into the category of insect
The jaguar falls into the category of
2024-07-18 22:13:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 22:17:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6660, -0.4614, -0.3755,  ..., -0.8691,  0.2351, -0.0649],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.9219, -1.4004, -0.0342,  ..., -1.8008, -5.0898,  2.7461],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 6.5613e-04, -1.1436e-02,  1.0025e-02,  ...,  1.6769e-02,
          2.6493e-03, -7.2479e-05],
        [-2.2995e-02,  7.2250e-03,  3.3020e-02,  ...,  1.4099e-02,
         -1.5125e-03, -9.0027e-03],
        [-8.9798e-03, -7.5111e-03,  6.4850e-04,  ...,  4.5700e-03,
         -7.9346e-03,  1.1658e-02],
        ...,
        [ 1.2207e-03, -1.4931e-02, -2.0294e-03,  ...,  9.6130e-04,
         -5.5275e-03,  1.1230e-02],
        [ 1.7273e-02, -1.8707e-02, -2.7008e-02,  ...,  3.4180e-03,
         -1.2207e-02,  1.3100e-02],
        [-6.2637e-03, -1.1398e-02,  2.5894e-02,  ...,  3.3798e-03,
          1.7212e-02, -4.0588e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1875, -1.7666, -0.0165,  ..., -1.7832, -5.5273,  3.3867]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 22:17:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The turkey falls into the category of fowl
The bee falls into the category of insect
The chimpanzee falls into the category of primate
The lion falls into the category of feline
The gibbon falls into the category of primate
The viper falls into the category of snake
The cockroach falls into the category of insect
The jaguar falls into the category of
2024-07-18 22:17:55 root INFO     [order_1_approx] starting weight calculation for The bee falls into the category of insect
The cockroach falls into the category of insect
The viper falls into the category of snake
The lion falls into the category of feline
The chimpanzee falls into the category of primate
The jaguar falls into the category of feline
The turkey falls into the category of fowl
The gibbon falls into the category of
2024-07-18 22:17:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 22:22:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2378, -1.1973, -0.6069,  ...,  0.5303, -0.1143, -0.4033],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 8.5547,  1.1621,  1.8145,  ..., -0.4902, -0.6309, -1.1143],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0065, -0.0060,  0.0014,  ...,  0.0195, -0.0027, -0.0119],
        [-0.0160,  0.0198,  0.0234,  ...,  0.0457, -0.0014, -0.0079],
        [ 0.0056, -0.0023, -0.0009,  ..., -0.0039,  0.0032,  0.0103],
        ...,
        [-0.0028, -0.0070,  0.0031,  ...,  0.0078,  0.0011, -0.0034],
        [ 0.0303, -0.0186,  0.0015,  ..., -0.0310,  0.0182,  0.0033],
        [ 0.0006,  0.0068,  0.0102,  ..., -0.0148, -0.0056,  0.0181]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 8.5078,  1.9082,  1.3555,  ..., -1.0400,  0.1099, -0.9751]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 22:22:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The bee falls into the category of insect
The cockroach falls into the category of insect
The viper falls into the category of snake
The lion falls into the category of feline
The chimpanzee falls into the category of primate
The jaguar falls into the category of feline
The turkey falls into the category of fowl
The gibbon falls into the category of
2024-07-18 22:22:14 root INFO     [order_1_approx] starting weight calculation for The chimpanzee falls into the category of primate
The viper falls into the category of snake
The jaguar falls into the category of feline
The gibbon falls into the category of primate
The cockroach falls into the category of insect
The turkey falls into the category of fowl
The lion falls into the category of feline
The bee falls into the category of
2024-07-18 22:22:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 22:26:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1780, -0.3755, -1.1865,  ...,  0.0753, -0.3403,  0.4590],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4590, -1.2871,  0.1865,  ..., -0.6709, -3.0000,  1.1504],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.5831e-04, -1.0925e-02, -1.2093e-03,  ...,  1.4549e-02,
          5.4398e-03, -5.8327e-03],
        [-5.3749e-03,  3.9101e-05,  1.6212e-03,  ...,  6.7940e-03,
          1.1383e-02,  1.7456e-02],
        [-4.4212e-03,  1.0223e-03,  1.7271e-03,  ..., -1.7029e-02,
          2.8763e-03, -1.0559e-02],
        ...,
        [-7.0381e-03, -8.5907e-03,  4.0627e-03,  ...,  1.2627e-03,
         -6.4278e-03, -6.1378e-03],
        [-2.1305e-03, -1.3634e-02,  1.2596e-02,  ..., -1.0834e-02,
         -1.5404e-02,  2.2430e-03],
        [-1.3519e-02, -3.7231e-03,  1.3779e-02,  ...,  1.0559e-02,
         -7.8430e-03,  1.2978e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3398, -0.6724, -0.3750,  ..., -1.7168, -4.0938,  1.8047]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 22:26:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The chimpanzee falls into the category of primate
The viper falls into the category of snake
The jaguar falls into the category of feline
The gibbon falls into the category of primate
The cockroach falls into the category of insect
The turkey falls into the category of fowl
The lion falls into the category of feline
The bee falls into the category of
2024-07-18 22:26:37 root INFO     [order_1_approx] starting weight calculation for The bee falls into the category of insect
The viper falls into the category of snake
The cockroach falls into the category of insect
The gibbon falls into the category of primate
The lion falls into the category of feline
The jaguar falls into the category of feline
The turkey falls into the category of fowl
The chimpanzee falls into the category of
2024-07-18 22:26:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 22:30:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1020, -0.4253, -0.6104,  ..., -0.6079, -0.4141,  0.9160],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.7734, -0.8384,  5.1797,  ..., -3.3555, -2.3340, -0.4736],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0224, -0.0131, -0.0054,  ..., -0.0057, -0.0056, -0.0031],
        [-0.0191,  0.0038,  0.0170,  ..., -0.0007,  0.0010,  0.0056],
        [-0.0214, -0.0056, -0.0036,  ..., -0.0170,  0.0015,  0.0055],
        ...,
        [ 0.0042, -0.0116, -0.0165,  ..., -0.0005, -0.0056,  0.0076],
        [ 0.0132,  0.0030,  0.0055,  ...,  0.0004,  0.0130,  0.0023],
        [-0.0134, -0.0055,  0.0058,  ...,  0.0030, -0.0094, -0.0006]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.6250, -0.9854,  5.0352,  ..., -2.9844, -1.7637, -1.7266]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 22:30:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The bee falls into the category of insect
The viper falls into the category of snake
The cockroach falls into the category of insect
The gibbon falls into the category of primate
The lion falls into the category of feline
The jaguar falls into the category of feline
The turkey falls into the category of fowl
The chimpanzee falls into the category of
2024-07-18 22:30:58 root INFO     total operator prediction time: 2091.2032017707825 seconds
2024-07-18 22:30:58 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on hyponyms - misc
2024-07-18 22:30:58 root INFO     building operator hyponyms - misc
2024-07-18 22:30:58 root INFO     [order_1_approx] starting weight calculation for A more specific term for a painting is watercolor
A more specific term for a season is spring
A more specific term for a poem is haiku
A more specific term for a railway is monorail
A more specific term for a car is limousine
A more specific term for a cutlery is knife
A more specific term for a computer is laptop
A more specific term for a cup is
2024-07-18 22:30:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 22:35:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1327,  0.0775,  0.0984,  ..., -0.2073, -0.5669,  0.9302],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6978,  0.4417, -1.3301,  ..., -1.6504,  1.5127,  3.0781],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-7.4158e-03, -3.7193e-03, -1.3710e-02,  ..., -3.7498e-03,
         -1.3075e-03, -1.3046e-02],
        [-1.6739e-02,  1.3908e-02,  3.0308e-03,  ...,  3.2532e-02,
          1.8890e-02,  1.1353e-02],
        [-3.7155e-03, -1.2604e-02,  2.0798e-02,  ...,  1.6754e-02,
         -2.4090e-03,  1.0849e-02],
        ...,
        [-6.4735e-03,  5.1231e-03, -5.7182e-03,  ...,  1.2856e-02,
         -2.4780e-02, -9.5444e-03],
        [ 1.8997e-03, -8.8348e-03,  7.6332e-03,  ..., -8.2397e-04,
          1.3828e-05,  8.3313e-03],
        [ 1.0056e-02, -4.3221e-03, -2.8503e-02,  ..., -1.4282e-02,
          9.0332e-03, -3.9711e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5742, -0.5693, -1.5205,  ..., -2.0195,  0.8911,  3.3418]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 22:35:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a painting is watercolor
A more specific term for a season is spring
A more specific term for a poem is haiku
A more specific term for a railway is monorail
A more specific term for a car is limousine
A more specific term for a cutlery is knife
A more specific term for a computer is laptop
A more specific term for a cup is
2024-07-18 22:35:20 root INFO     [order_1_approx] starting weight calculation for A more specific term for a cutlery is knife
A more specific term for a railway is monorail
A more specific term for a computer is laptop
A more specific term for a poem is haiku
A more specific term for a painting is watercolor
A more specific term for a cup is teacup
A more specific term for a car is limousine
A more specific term for a season is
2024-07-18 22:35:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 22:39:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3181,  0.3933,  0.0566,  ..., -0.7163,  0.7046, -0.4404],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3496, -3.3750, -8.2500,  ..., -1.6016,  4.0117, -1.8350],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0420,  0.0174, -0.0298,  ..., -0.0304, -0.0004, -0.0007],
        [ 0.0389, -0.0184, -0.0039,  ...,  0.0050, -0.0086,  0.0046],
        [ 0.0070, -0.0155,  0.0217,  ..., -0.0013, -0.0363, -0.0062],
        ...,
        [ 0.0100, -0.0154,  0.0005,  ...,  0.0197, -0.0071,  0.0274],
        [-0.0069,  0.0179, -0.0002,  ...,  0.0164,  0.0282,  0.0232],
        [ 0.0106, -0.0157,  0.0051,  ...,  0.0085,  0.0317,  0.0053]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7441, -3.1211, -9.3047,  ..., -2.0547,  4.3281, -1.8047]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 22:39:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a cutlery is knife
A more specific term for a railway is monorail
A more specific term for a computer is laptop
A more specific term for a poem is haiku
A more specific term for a painting is watercolor
A more specific term for a cup is teacup
A more specific term for a car is limousine
A more specific term for a season is
2024-07-18 22:39:41 root INFO     [order_1_approx] starting weight calculation for A more specific term for a railway is monorail
A more specific term for a computer is laptop
A more specific term for a season is spring
A more specific term for a painting is watercolor
A more specific term for a cup is teacup
A more specific term for a poem is haiku
A more specific term for a car is limousine
A more specific term for a cutlery is
2024-07-18 22:39:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 22:44:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1749,  0.0265,  0.2559,  ...,  0.3936, -0.7979,  0.8706],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-6.3711,  0.5898, -3.8555,  ...,  1.9082,  1.5400,  1.2471],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0019,  0.0077, -0.0166,  ..., -0.0104,  0.0040, -0.0135],
        [-0.0217,  0.0116,  0.0031,  ...,  0.0215,  0.0052, -0.0124],
        [-0.0017,  0.0042,  0.0150,  ...,  0.0073, -0.0165,  0.0210],
        ...,
        [-0.0091, -0.0057,  0.0009,  ...,  0.0250, -0.0171,  0.0155],
        [ 0.0114,  0.0365, -0.0015,  ...,  0.0045, -0.0019,  0.0062],
        [ 0.0149,  0.0070,  0.0111,  ..., -0.0102,  0.0099, -0.0103]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-6.2266,  0.1912, -2.8379,  ...,  0.7607,  1.6787,  1.6982]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 22:44:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a railway is monorail
A more specific term for a computer is laptop
A more specific term for a season is spring
A more specific term for a painting is watercolor
A more specific term for a cup is teacup
A more specific term for a poem is haiku
A more specific term for a car is limousine
A more specific term for a cutlery is
2024-07-18 22:44:02 root INFO     [order_1_approx] starting weight calculation for A more specific term for a painting is watercolor
A more specific term for a cup is teacup
A more specific term for a poem is haiku
A more specific term for a season is spring
A more specific term for a car is limousine
A more specific term for a computer is laptop
A more specific term for a cutlery is knife
A more specific term for a railway is
2024-07-18 22:44:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 22:48:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2019,  0.5171,  0.0032,  ..., -0.0850,  0.2042,  0.7197],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1875, -1.6885, -2.0859,  ..., -3.3359,  5.1328, -3.5391],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0377, -0.0009, -0.0109,  ..., -0.0249, -0.0322, -0.0007],
        [-0.0102,  0.0030,  0.0216,  ...,  0.0148,  0.0173,  0.0079],
        [-0.0053,  0.0114, -0.0132,  ..., -0.0234, -0.0057, -0.0054],
        ...,
        [-0.0025,  0.0004, -0.0104,  ...,  0.0205, -0.0050,  0.0169],
        [-0.0155, -0.0163,  0.0121,  ...,  0.0144,  0.0249,  0.0052],
        [ 0.0077, -0.0056, -0.0074,  ..., -0.0232, -0.0074, -0.0068]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0547, -2.1328, -3.3027,  ..., -2.8145,  4.4688, -2.3652]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 22:48:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a painting is watercolor
A more specific term for a cup is teacup
A more specific term for a poem is haiku
A more specific term for a season is spring
A more specific term for a car is limousine
A more specific term for a computer is laptop
A more specific term for a cutlery is knife
A more specific term for a railway is
2024-07-18 22:48:24 root INFO     [order_1_approx] starting weight calculation for A more specific term for a cup is teacup
A more specific term for a car is limousine
A more specific term for a painting is watercolor
A more specific term for a cutlery is knife
A more specific term for a computer is laptop
A more specific term for a season is spring
A more specific term for a railway is monorail
A more specific term for a poem is
2024-07-18 22:48:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 22:52:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2805,  0.2886,  0.4109,  ..., -0.4824,  0.0510,  0.0759],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8887,  1.2090, -4.4844,  ..., -7.5391,  4.0156, -8.6094],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0080, -0.0040, -0.0147,  ..., -0.0053, -0.0019,  0.0043],
        [-0.0005,  0.0103,  0.0130,  ...,  0.0222, -0.0088,  0.0203],
        [ 0.0263,  0.0090, -0.0218,  ..., -0.0082,  0.0125, -0.0014],
        ...,
        [ 0.0183,  0.0009, -0.0139,  ..., -0.0065, -0.0069,  0.0066],
        [ 0.0005,  0.0088,  0.0019,  ..., -0.0033, -0.0113, -0.0032],
        [ 0.0375, -0.0330, -0.0042,  ..., -0.0300,  0.0117, -0.0194]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7485,  2.6328, -6.0469,  ..., -8.9531,  4.0195, -8.7656]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 22:52:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a cup is teacup
A more specific term for a car is limousine
A more specific term for a painting is watercolor
A more specific term for a cutlery is knife
A more specific term for a computer is laptop
A more specific term for a season is spring
A more specific term for a railway is monorail
A more specific term for a poem is
2024-07-18 22:52:44 root INFO     [order_1_approx] starting weight calculation for A more specific term for a computer is laptop
A more specific term for a painting is watercolor
A more specific term for a cup is teacup
A more specific term for a cutlery is knife
A more specific term for a season is spring
A more specific term for a railway is monorail
A more specific term for a poem is haiku
A more specific term for a car is
2024-07-18 22:52:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 22:57:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2483,  0.2350, -0.5732,  ..., -0.0440, -0.0687,  0.4880],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-6.1016, -3.5332, -0.1572,  ..., -1.4863,  1.0244, -1.8857],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0349, -0.0039,  0.0098,  ..., -0.0233, -0.0152, -0.0065],
        [ 0.0102,  0.0029,  0.0140,  ...,  0.0177,  0.0028, -0.0025],
        [-0.0286, -0.0036, -0.0223,  ..., -0.0209, -0.0066, -0.0079],
        ...,
        [-0.0034,  0.0009, -0.0097,  ...,  0.0064,  0.0105, -0.0070],
        [ 0.0134, -0.0119, -0.0083,  ..., -0.0241, -0.0072, -0.0107],
        [ 0.0024, -0.0300, -0.0142,  ..., -0.0421, -0.0083,  0.0147]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-7.0234, -2.7832, -2.0723,  ..., -1.4365,  1.1836, -2.8301]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 22:57:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a computer is laptop
A more specific term for a painting is watercolor
A more specific term for a cup is teacup
A more specific term for a cutlery is knife
A more specific term for a season is spring
A more specific term for a railway is monorail
A more specific term for a poem is haiku
A more specific term for a car is
2024-07-18 22:57:04 root INFO     [order_1_approx] starting weight calculation for A more specific term for a cup is teacup
A more specific term for a poem is haiku
A more specific term for a railway is monorail
A more specific term for a car is limousine
A more specific term for a computer is laptop
A more specific term for a season is spring
A more specific term for a cutlery is knife
A more specific term for a painting is
2024-07-18 22:57:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 23:01:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2515,  0.5903,  0.0811,  ..., -0.3633,  0.4429,  0.4956],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.6270, -1.3691, -3.1562,  ..., -0.8911, -1.4082,  2.4199],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0289,  0.0117, -0.0137,  ..., -0.0229, -0.0117,  0.0069],
        [-0.0014,  0.0041, -0.0031,  ...,  0.0138, -0.0178, -0.0112],
        [ 0.0480,  0.0249,  0.0151,  ..., -0.0239, -0.0162,  0.0166],
        ...,
        [-0.0067,  0.0072,  0.0077,  ...,  0.0370,  0.0020,  0.0139],
        [ 0.0021,  0.0250,  0.0058,  ..., -0.0130,  0.0097,  0.0206],
        [-0.0027,  0.0132, -0.0019,  ..., -0.0337,  0.0082, -0.0022]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.3809, -0.0654, -3.2754,  ..., -2.2891, -2.3105,  2.6797]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 23:01:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a cup is teacup
A more specific term for a poem is haiku
A more specific term for a railway is monorail
A more specific term for a car is limousine
A more specific term for a computer is laptop
A more specific term for a season is spring
A more specific term for a cutlery is knife
A more specific term for a painting is
2024-07-18 23:01:24 root INFO     [order_1_approx] starting weight calculation for A more specific term for a cutlery is knife
A more specific term for a painting is watercolor
A more specific term for a car is limousine
A more specific term for a poem is haiku
A more specific term for a cup is teacup
A more specific term for a railway is monorail
A more specific term for a season is spring
A more specific term for a computer is
2024-07-18 23:01:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 23:05:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2573,  0.2361,  0.0048,  ...,  0.4729,  0.5645,  1.4609],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.8350, -1.2715, -1.0449,  ...,  0.3936,  4.2812,  3.2090],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0170,  0.0008,  0.0189,  ..., -0.0223,  0.0005,  0.0079],
        [ 0.0182,  0.0148,  0.0031,  ..., -0.0024, -0.0151,  0.0009],
        [-0.0056,  0.0153,  0.0062,  ..., -0.0108, -0.0093,  0.0044],
        ...,
        [-0.0191,  0.0031,  0.0035,  ...,  0.0256, -0.0208,  0.0138],
        [ 0.0040, -0.0026,  0.0085,  ...,  0.0092, -0.0018, -0.0004],
        [ 0.0039, -0.0071,  0.0278,  ..., -0.0333,  0.0160,  0.0225]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.9570,  0.0762, -1.6201,  ..., -0.5991,  4.8594,  3.4141]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 23:05:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a cutlery is knife
A more specific term for a painting is watercolor
A more specific term for a car is limousine
A more specific term for a poem is haiku
A more specific term for a cup is teacup
A more specific term for a railway is monorail
A more specific term for a season is spring
A more specific term for a computer is
2024-07-18 23:05:45 root INFO     total operator prediction time: 2087.3720359802246 seconds
2024-07-18 23:05:45 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on antonyms - binary
2024-07-18 23:05:45 root INFO     building operator antonyms - binary
2024-07-18 23:05:46 root INFO     [order_1_approx] starting weight calculation for The opposite of under is over
The opposite of up is down
The opposite of beginning is end
The opposite of in is out
The opposite of exit is entrance
The opposite of mortal is immortal
The opposite of before is after
The opposite of inbound is
2024-07-18 23:05:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 23:10:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6221,  0.1902,  0.5532,  ..., -0.4001, -0.8760, -0.7266],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0897, -5.3789, -1.7969,  ...,  2.1719,  6.1758,  0.0820],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.6062e-02, -1.1650e-02, -6.1417e-03,  ..., -2.7161e-02,
         -2.7466e-04,  8.4229e-03],
        [-2.0355e-02,  1.6678e-02, -8.4534e-03,  ..., -3.7060e-03,
          3.0518e-03, -1.5030e-02],
        [-8.1177e-03,  4.9591e-03, -4.7882e-02,  ..., -1.1108e-02,
          1.2436e-02, -4.0985e-02],
        ...,
        [-2.9602e-02, -3.1799e-02,  1.9012e-02,  ...,  7.8201e-03,
          9.3765e-03,  4.4800e-02],
        [-1.2283e-02,  4.1504e-02, -1.1833e-02,  ...,  2.3518e-03,
          7.2174e-03, -1.5259e-02],
        [-1.2169e-02, -2.1515e-02,  1.7990e-02,  ..., -4.0054e-05,
          8.0414e-03, -9.4604e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1311, -5.0234, -1.0859,  ...,  2.9180,  5.4531, -0.3784]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 23:10:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of under is over
The opposite of up is down
The opposite of beginning is end
The opposite of in is out
The opposite of exit is entrance
The opposite of mortal is immortal
The opposite of before is after
The opposite of inbound is
2024-07-18 23:10:07 root INFO     [order_1_approx] starting weight calculation for The opposite of under is over
The opposite of inbound is outbound
The opposite of before is after
The opposite of beginning is end
The opposite of mortal is immortal
The opposite of in is out
The opposite of up is down
The opposite of exit is
2024-07-18 23:10:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 23:14:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1345,  0.3450,  0.3652,  ..., -0.4141, -0.3735, -0.0753],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.9023, -0.6592,  0.9160,  ..., -1.6123,  4.5703,  2.7637],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0200,  0.0103, -0.0294,  ..., -0.0037, -0.0121,  0.0251],
        [-0.0166, -0.0153,  0.0068,  ...,  0.0172, -0.0167,  0.0120],
        [-0.0220, -0.0356, -0.0031,  ..., -0.0292, -0.0153,  0.0142],
        ...,
        [ 0.0010, -0.0255,  0.0087,  ..., -0.0005, -0.0064,  0.0148],
        [ 0.0046,  0.0152,  0.0437,  ..., -0.0047, -0.0302, -0.0063],
        [-0.0142,  0.0195,  0.0154,  ...,  0.0008,  0.0075, -0.0085]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.4746, -0.5352,  0.9614,  ..., -1.4570,  4.2578,  3.2402]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 23:14:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of under is over
The opposite of inbound is outbound
The opposite of before is after
The opposite of beginning is end
The opposite of mortal is immortal
The opposite of in is out
The opposite of up is down
The opposite of exit is
2024-07-18 23:14:27 root INFO     [order_1_approx] starting weight calculation for The opposite of under is over
The opposite of exit is entrance
The opposite of beginning is end
The opposite of up is down
The opposite of inbound is outbound
The opposite of in is out
The opposite of mortal is immortal
The opposite of before is
2024-07-18 23:14:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 23:18:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2297,  0.7627, -0.0325,  ..., -0.2229, -0.5381, -0.3062],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9102, -2.4727, -1.0664,  ...,  2.9199,  4.1328,  2.3555],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.1347e-02, -2.6627e-03, -1.2840e-02,  ...,  2.0538e-02,
          2.0538e-02,  1.1215e-02],
        [-1.1116e-02,  2.6703e-02,  1.1116e-02,  ...,  6.5460e-03,
         -3.1891e-02,  9.9258e-03],
        [ 2.9327e-02,  2.4948e-02, -6.1035e-05,  ..., -7.2556e-03,
         -2.5513e-02,  2.0767e-02],
        ...,
        [-2.7283e-02, -3.2288e-02,  1.8188e-02,  ...,  4.5776e-03,
         -1.8204e-02,  2.3224e-02],
        [-1.3138e-02,  3.8719e-03,  4.8218e-02,  ..., -1.7334e-02,
         -7.9117e-03, -1.4801e-03],
        [-1.1543e-02, -1.8097e-02,  5.7220e-03,  ...,  2.5940e-03,
          2.8183e-02, -4.8141e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1494, -2.0762, -1.3848,  ...,  2.3535,  3.3711,  2.2656]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 23:18:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of under is over
The opposite of exit is entrance
The opposite of beginning is end
The opposite of up is down
The opposite of inbound is outbound
The opposite of in is out
The opposite of mortal is immortal
The opposite of before is
2024-07-18 23:18:48 root INFO     [order_1_approx] starting weight calculation for The opposite of in is out
The opposite of inbound is outbound
The opposite of under is over
The opposite of exit is entrance
The opposite of before is after
The opposite of up is down
The opposite of mortal is immortal
The opposite of beginning is
2024-07-18 23:18:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 23:23:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2900, -0.1317, -0.3391,  ..., -0.5596, -0.1821,  0.3655],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4707, -3.3906, -0.4419,  ...,  3.8008,  1.0137, -0.2407],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0361, -0.0093,  0.0006,  ...,  0.0095, -0.0123,  0.0152],
        [ 0.0164,  0.0097,  0.0029,  ...,  0.0022,  0.0202,  0.0101],
        [-0.0261, -0.0080, -0.0004,  ..., -0.0090,  0.0074, -0.0049],
        ...,
        [ 0.0270, -0.0092, -0.0357,  ..., -0.0100, -0.0153,  0.0078],
        [ 0.0052, -0.0172,  0.0394,  ..., -0.0222, -0.0114, -0.0061],
        [ 0.0182,  0.0103,  0.0122,  ..., -0.0063,  0.0314, -0.0097]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5645, -3.7578, -0.7666,  ...,  2.7598,  0.8862,  0.9438]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 23:23:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of in is out
The opposite of inbound is outbound
The opposite of under is over
The opposite of exit is entrance
The opposite of before is after
The opposite of up is down
The opposite of mortal is immortal
The opposite of beginning is
2024-07-18 23:23:09 root INFO     [order_1_approx] starting weight calculation for The opposite of inbound is outbound
The opposite of in is out
The opposite of under is over
The opposite of mortal is immortal
The opposite of before is after
The opposite of exit is entrance
The opposite of beginning is end
The opposite of up is
2024-07-18 23:23:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 23:27:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1322,  0.1249,  0.0718,  ..., -0.2578, -1.2734, -0.0282],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3281, -4.7891,  2.6211,  ..., -0.4116,  5.3008,  1.0566],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0041, -0.0197,  0.0016,  ...,  0.0042,  0.0112,  0.0037],
        [ 0.0100, -0.0010, -0.0170,  ...,  0.0129, -0.0092,  0.0168],
        [-0.0018, -0.0032, -0.0061,  ..., -0.0175, -0.0119,  0.0047],
        ...,
        [-0.0092, -0.0136,  0.0052,  ...,  0.0155, -0.0020,  0.0072],
        [-0.0166,  0.0389,  0.0081,  ..., -0.0268, -0.0204,  0.0078],
        [-0.0121,  0.0034,  0.0075,  ...,  0.0032,  0.0016,  0.0090]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.8203, -4.5898,  2.9121,  ..., -0.1250,  5.0508,  0.6289]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 23:27:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of inbound is outbound
The opposite of in is out
The opposite of under is over
The opposite of mortal is immortal
The opposite of before is after
The opposite of exit is entrance
The opposite of beginning is end
The opposite of up is
2024-07-18 23:27:30 root INFO     [order_1_approx] starting weight calculation for The opposite of mortal is immortal
The opposite of inbound is outbound
The opposite of before is after
The opposite of up is down
The opposite of exit is entrance
The opposite of under is over
The opposite of beginning is end
The opposite of in is
2024-07-18 23:27:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 23:31:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2179,  0.5195, -0.6123,  ..., -0.8809, -0.5654, -0.2720],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.4062, -5.7969,  2.4570,  ...,  1.1475,  3.7773, -2.3418],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0128, -0.0128,  0.0003,  ...,  0.0003, -0.0154,  0.0076],
        [ 0.0199,  0.0047, -0.0080,  ...,  0.0112, -0.0001,  0.0285],
        [-0.0020,  0.0084, -0.0049,  ..., -0.0201, -0.0241,  0.0226],
        ...,
        [ 0.0042, -0.0041,  0.0186,  ...,  0.0076, -0.0124,  0.0391],
        [-0.0248,  0.0093, -0.0146,  ..., -0.0094,  0.0094,  0.0065],
        [-0.0081,  0.0105,  0.0384,  ...,  0.0162, -0.0125,  0.0210]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.7891, -6.5781,  2.9453,  ..., -0.0371,  2.2695, -1.2207]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 23:31:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of mortal is immortal
The opposite of inbound is outbound
The opposite of before is after
The opposite of up is down
The opposite of exit is entrance
The opposite of under is over
The opposite of beginning is end
The opposite of in is
2024-07-18 23:31:51 root INFO     [order_1_approx] starting weight calculation for The opposite of up is down
The opposite of beginning is end
The opposite of before is after
The opposite of in is out
The opposite of inbound is outbound
The opposite of exit is entrance
The opposite of mortal is immortal
The opposite of under is
2024-07-18 23:31:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 23:36:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4617,  0.2620, -0.8984,  ..., -0.4902, -0.5288,  0.2301],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4412, -7.3438, -0.0762,  ...,  0.5127,  3.5508,  1.4727],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0159, -0.0057, -0.0114,  ...,  0.0407,  0.0262,  0.0070],
        [ 0.0279,  0.0305, -0.0051,  ...,  0.0081, -0.0145,  0.0208],
        [ 0.0274,  0.0360, -0.0374,  ...,  0.0245,  0.0080,  0.0149],
        ...,
        [ 0.0050, -0.0299, -0.0157,  ...,  0.0072,  0.0041,  0.0102],
        [-0.0155,  0.0614,  0.0018,  ..., -0.0012, -0.0158,  0.0287],
        [ 0.0002,  0.0241, -0.0131,  ..., -0.0314, -0.0088, -0.0352]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0959, -9.4062, -1.4893,  ...,  0.7544,  2.7246,  1.4170]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 23:36:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of up is down
The opposite of beginning is end
The opposite of before is after
The opposite of in is out
The opposite of inbound is outbound
The opposite of exit is entrance
The opposite of mortal is immortal
The opposite of under is
2024-07-18 23:36:12 root INFO     [order_1_approx] starting weight calculation for The opposite of up is down
The opposite of in is out
The opposite of before is after
The opposite of under is over
The opposite of beginning is end
The opposite of inbound is outbound
The opposite of exit is entrance
The opposite of mortal is
2024-07-18 23:36:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 23:40:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5596,  0.5723, -0.6455,  ..., -0.6797, -1.2764, -0.5386],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3027,  2.8438,  0.0176,  ..., -2.6172,  1.4883,  5.5352],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0265, -0.0217, -0.0065,  ...,  0.0014, -0.0153, -0.0022],
        [ 0.0135, -0.0073, -0.0113,  ..., -0.0060,  0.0175, -0.0028],
        [-0.0356, -0.0090, -0.0524,  ...,  0.0052, -0.0174,  0.0099],
        ...,
        [-0.0069, -0.0240, -0.0072,  ..., -0.0002,  0.0075, -0.0155],
        [ 0.0101,  0.0149,  0.0177,  ...,  0.0136, -0.0300, -0.0185],
        [-0.0087, -0.0024, -0.0089,  ..., -0.0270,  0.0187, -0.0295]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.2344,  1.5488, -0.7612,  ..., -3.1641,  0.1924,  3.8828]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 23:40:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of up is down
The opposite of in is out
The opposite of before is after
The opposite of under is over
The opposite of beginning is end
The opposite of inbound is outbound
The opposite of exit is entrance
The opposite of mortal is
2024-07-18 23:40:34 root INFO     total operator prediction time: 2088.057998418808 seconds
2024-07-18 23:40:34 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on meronyms - member
2024-07-18 23:40:34 root INFO     building operator meronyms - member
2024-07-18 23:40:34 root INFO     [order_1_approx] starting weight calculation for A word is a member of a paragraph
A soldier is a member of a army
A wolf is a member of a pack
A elephant is a member of a herd
A bee is a member of a swarm
A acrobat is a member of a troupe
A juror is a member of a jury
A song is a member of a
2024-07-18 23:40:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 23:44:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0623,  0.2563, -0.2288,  ...,  0.1392,  0.3096, -0.8677],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6074,  3.4746,  0.7891,  ...,  3.1406,  4.5586, -4.5352],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0142, -0.0187, -0.0004,  ..., -0.0053,  0.0104, -0.0258],
        [-0.0196, -0.0244,  0.0077,  ...,  0.0219,  0.0104, -0.0076],
        [ 0.0191,  0.0143,  0.0003,  ..., -0.0064, -0.0080,  0.0047],
        ...,
        [-0.0141,  0.0207,  0.0079,  ...,  0.0138, -0.0037,  0.0380],
        [-0.0139,  0.0122, -0.0048,  ...,  0.0055,  0.0074,  0.0131],
        [-0.0012,  0.0241, -0.0022,  ..., -0.0077,  0.0246, -0.0211]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7617,  4.0195,  0.2104,  ...,  3.3770,  4.4336, -4.1016]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 23:44:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A word is a member of a paragraph
A soldier is a member of a army
A wolf is a member of a pack
A elephant is a member of a herd
A bee is a member of a swarm
A acrobat is a member of a troupe
A juror is a member of a jury
A song is a member of a
2024-07-18 23:44:56 root INFO     [order_1_approx] starting weight calculation for A word is a member of a paragraph
A elephant is a member of a herd
A bee is a member of a swarm
A wolf is a member of a pack
A juror is a member of a jury
A acrobat is a member of a troupe
A song is a member of a album
A soldier is a member of a
2024-07-18 23:44:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 23:49:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0682,  0.4771, -0.1652,  ...,  0.5483,  0.0309,  0.0671],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1131,  1.0566, -1.4922,  ..., -3.9980,  1.9385,  1.0078],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0064, -0.0047, -0.0023,  ...,  0.0011,  0.0116, -0.0192],
        [-0.0177, -0.0208, -0.0145,  ...,  0.0151, -0.0115,  0.0014],
        [-0.0072,  0.0249, -0.0012,  ..., -0.0013, -0.0090,  0.0080],
        ...,
        [ 0.0048,  0.0130, -0.0046,  ..., -0.0043, -0.0037,  0.0052],
        [-0.0059, -0.0113, -0.0032,  ...,  0.0202, -0.0179,  0.0015],
        [ 0.0023, -0.0107, -0.0016,  ...,  0.0080,  0.0107, -0.0050]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0125,  0.8169, -1.8291,  ..., -3.2227,  2.6992,  0.9146]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 23:49:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A word is a member of a paragraph
A elephant is a member of a herd
A bee is a member of a swarm
A wolf is a member of a pack
A juror is a member of a jury
A acrobat is a member of a troupe
A song is a member of a album
A soldier is a member of a
2024-07-18 23:49:18 root INFO     [order_1_approx] starting weight calculation for A wolf is a member of a pack
A soldier is a member of a army
A acrobat is a member of a troupe
A juror is a member of a jury
A elephant is a member of a herd
A song is a member of a album
A word is a member of a paragraph
A bee is a member of a
2024-07-18 23:49:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 23:53:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1106, -0.0808, -0.8262,  ...,  0.3521, -0.4639,  0.8037],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.6191,  2.0488, -1.3359,  ...,  1.9473,  1.1465,  3.5527],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0012,  0.0068, -0.0075,  ...,  0.0220, -0.0125, -0.0098],
        [-0.0091, -0.0061,  0.0186,  ..., -0.0116,  0.0194,  0.0072],
        [ 0.0021,  0.0124, -0.0048,  ...,  0.0128,  0.0064, -0.0135],
        ...,
        [-0.0029,  0.0016, -0.0082,  ..., -0.0135, -0.0039,  0.0008],
        [ 0.0271, -0.0163, -0.0131,  ..., -0.0008, -0.0220,  0.0229],
        [-0.0100, -0.0106,  0.0137,  ...,  0.0028,  0.0120,  0.0041]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.7070,  2.2793, -0.6147,  ...,  1.4941,  2.2871,  3.4707]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 23:53:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A wolf is a member of a pack
A soldier is a member of a army
A acrobat is a member of a troupe
A juror is a member of a jury
A elephant is a member of a herd
A song is a member of a album
A word is a member of a paragraph
A bee is a member of a
2024-07-18 23:53:41 root INFO     [order_1_approx] starting weight calculation for A wolf is a member of a pack
A soldier is a member of a army
A juror is a member of a jury
A acrobat is a member of a troupe
A bee is a member of a swarm
A elephant is a member of a herd
A song is a member of a album
A word is a member of a
2024-07-18 23:53:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-18 23:58:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6196, -0.6919,  0.0264,  ..., -0.4368,  0.0009,  0.6274],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.1465,  0.7275, -0.8037,  ...,  0.2109,  0.6787,  5.4062],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0046, -0.0068, -0.0175,  ...,  0.0233, -0.0088,  0.0034],
        [-0.0363,  0.0150,  0.0227,  ...,  0.0036, -0.0064,  0.0257],
        [ 0.0214,  0.0038, -0.0127,  ...,  0.0008, -0.0076, -0.0060],
        ...,
        [-0.0010, -0.0073,  0.0004,  ..., -0.0138, -0.0160,  0.0014],
        [ 0.0316, -0.0043, -0.0154,  ...,  0.0042,  0.0061, -0.0203],
        [-0.0280,  0.0045,  0.0288,  ...,  0.0118, -0.0107, -0.0006]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.5879, -1.4873, -0.1621,  ...,  0.7354,  0.6968,  2.6738]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-18 23:58:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A wolf is a member of a pack
A soldier is a member of a army
A juror is a member of a jury
A acrobat is a member of a troupe
A bee is a member of a swarm
A elephant is a member of a herd
A song is a member of a album
A word is a member of a
2024-07-18 23:58:04 root INFO     [order_1_approx] starting weight calculation for A bee is a member of a swarm
A elephant is a member of a herd
A song is a member of a album
A acrobat is a member of a troupe
A soldier is a member of a army
A word is a member of a paragraph
A wolf is a member of a pack
A juror is a member of a
2024-07-18 23:58:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 00:02:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2039,  0.7520,  0.0714,  ..., -0.1287,  0.3408,  0.3799],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([8.4844, 1.2529, 0.8984,  ..., 2.8242, 1.2539, 2.2734], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0006,  0.0086,  0.0106,  ...,  0.0105, -0.0128, -0.0004],
        [-0.0123, -0.0078,  0.0220,  ...,  0.0060,  0.0316,  0.0091],
        [-0.0001,  0.0107, -0.0185,  ..., -0.0068, -0.0234, -0.0118],
        ...,
        [-0.0124, -0.0228, -0.0037,  ...,  0.0007,  0.0101,  0.0023],
        [ 0.0130, -0.0095,  0.0051,  ...,  0.0094,  0.0093, -0.0064],
        [ 0.0155, -0.0120, -0.0027,  ...,  0.0170,  0.0048, -0.0080]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[7.7109, 1.7021, 1.3301,  ..., 3.0586, 0.6313, 1.8789]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 00:02:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A bee is a member of a swarm
A elephant is a member of a herd
A song is a member of a album
A acrobat is a member of a troupe
A soldier is a member of a army
A word is a member of a paragraph
A wolf is a member of a pack
A juror is a member of a
2024-07-19 00:02:24 root INFO     [order_1_approx] starting weight calculation for A song is a member of a album
A bee is a member of a swarm
A soldier is a member of a army
A juror is a member of a jury
A word is a member of a paragraph
A elephant is a member of a herd
A acrobat is a member of a troupe
A wolf is a member of a
2024-07-19 00:02:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 00:06:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2764, -0.8682, -0.8228,  ...,  0.1206, -0.3391, -0.1334],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.1133, -1.3281, -3.7695,  ...,  3.4082, -0.7104,  2.6641],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0020, -0.0218,  0.0042,  ...,  0.0159, -0.0296, -0.0065],
        [ 0.0013,  0.0111,  0.0052,  ...,  0.0311,  0.0047, -0.0044],
        [ 0.0171,  0.0053, -0.0049,  ..., -0.0011,  0.0059, -0.0103],
        ...,
        [ 0.0137,  0.0026, -0.0023,  ...,  0.0105, -0.0089,  0.0234],
        [ 0.0165, -0.0096,  0.0008,  ...,  0.0016, -0.0171,  0.0038],
        [ 0.0060,  0.0009, -0.0127,  ..., -0.0078,  0.0107, -0.0065]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5781, -1.0488, -3.3340,  ...,  4.2188, -1.1543,  2.1816]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 00:06:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A song is a member of a album
A bee is a member of a swarm
A soldier is a member of a army
A juror is a member of a jury
A word is a member of a paragraph
A elephant is a member of a herd
A acrobat is a member of a troupe
A wolf is a member of a
2024-07-19 00:06:43 root INFO     [order_1_approx] starting weight calculation for A elephant is a member of a herd
A word is a member of a paragraph
A juror is a member of a jury
A soldier is a member of a army
A song is a member of a album
A wolf is a member of a pack
A bee is a member of a swarm
A acrobat is a member of a
2024-07-19 00:06:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 00:11:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4414, -1.3281, -0.0620,  ..., -1.3555, -0.1250, -0.5649],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1084,  1.1895, -0.1606,  ...,  0.1523, -1.0098,  1.7285],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0126, -0.0033, -0.0027,  ...,  0.0051,  0.0014, -0.0017],
        [-0.0231, -0.0009,  0.0169,  ...,  0.0064,  0.0087,  0.0032],
        [ 0.0129,  0.0047, -0.0059,  ...,  0.0170, -0.0081,  0.0090],
        ...,
        [-0.0121, -0.0058, -0.0067,  ...,  0.0079, -0.0135,  0.0045],
        [ 0.0295,  0.0080, -0.0191,  ...,  0.0086, -0.0032,  0.0142],
        [-0.0088,  0.0083, -0.0092,  ...,  0.0136,  0.0065,  0.0145]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1726,  1.3789,  0.0880,  ...,  0.9653, -0.5938,  1.6611]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 00:11:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A elephant is a member of a herd
A word is a member of a paragraph
A juror is a member of a jury
A soldier is a member of a army
A song is a member of a album
A wolf is a member of a pack
A bee is a member of a swarm
A acrobat is a member of a
2024-07-19 00:11:05 root INFO     [order_1_approx] starting weight calculation for A word is a member of a paragraph
A wolf is a member of a pack
A soldier is a member of a army
A acrobat is a member of a troupe
A juror is a member of a jury
A bee is a member of a swarm
A song is a member of a album
A elephant is a member of a
2024-07-19 00:11:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 00:15:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6479, -0.8408, -0.1313,  ...,  0.4749, -0.1257,  0.1780],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.5137,  2.6602,  0.5850,  ..., -1.1260, -2.8457,  2.7031],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0033, -0.0142, -0.0092,  ...,  0.0024, -0.0265, -0.0096],
        [-0.0161, -0.0050, -0.0130,  ...,  0.0261,  0.0137, -0.0195],
        [ 0.0173,  0.0008,  0.0043,  ..., -0.0208, -0.0100, -0.0015],
        ...,
        [-0.0058, -0.0127,  0.0015,  ...,  0.0016,  0.0059, -0.0128],
        [ 0.0042,  0.0097,  0.0077,  ..., -0.0029, -0.0291,  0.0043],
        [-0.0046,  0.0181,  0.0045,  ..., -0.0033, -0.0034, -0.0110]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7031,  2.9414,  0.8428,  ..., -0.7939, -3.3418,  3.6855]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 00:15:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A word is a member of a paragraph
A wolf is a member of a pack
A soldier is a member of a army
A acrobat is a member of a troupe
A juror is a member of a jury
A bee is a member of a swarm
A song is a member of a album
A elephant is a member of a
2024-07-19 00:15:27 root INFO     total operator prediction time: 2093.5828988552094 seconds
2024-07-19 00:15:27 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on noun - plural_irreg
2024-07-19 00:15:27 root INFO     building operator noun - plural_irreg
2024-07-19 00:15:27 root INFO     [order_1_approx] starting weight calculation for The plural form of duty is duties
The plural form of child is children
The plural form of industry is industries
The plural form of safety is safeties
The plural form of county is counties
The plural form of formula is formulae
The plural form of security is securities
The plural form of energy is
2024-07-19 00:15:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 00:19:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7290,  0.2761, -0.3816,  ..., -0.3599,  0.4795,  0.2322],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.5176,  0.3750,  0.1123,  ..., -0.7588, -0.7422,  4.6758],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0100, -0.0241,  0.0204,  ..., -0.0127, -0.0048,  0.0024],
        [-0.0223, -0.0157, -0.0098,  ...,  0.0082, -0.0010,  0.0045],
        [-0.0071,  0.0142, -0.0239,  ..., -0.0168,  0.0059, -0.0035],
        ...,
        [-0.0232, -0.0089, -0.0023,  ..., -0.0035,  0.0178,  0.0163],
        [ 0.0333, -0.0107, -0.0241,  ...,  0.0098, -0.0343, -0.0101],
        [-0.0454, -0.0191,  0.0286,  ..., -0.0234,  0.0139, -0.0112]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.3535,  0.3494, -0.6260,  ..., -0.8369, -0.5322,  4.1367]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 00:19:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of duty is duties
The plural form of child is children
The plural form of industry is industries
The plural form of safety is safeties
The plural form of county is counties
The plural form of formula is formulae
The plural form of security is securities
The plural form of energy is
2024-07-19 00:19:49 root INFO     [order_1_approx] starting weight calculation for The plural form of county is counties
The plural form of formula is formulae
The plural form of industry is industries
The plural form of child is children
The plural form of duty is duties
The plural form of energy is energies
The plural form of safety is safeties
The plural form of security is
2024-07-19 00:19:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 00:24:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0033, -0.6855,  0.1416,  ..., -0.4473, -0.3120,  1.2490],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.2227,  6.6953,  3.4883,  ..., -3.8906, -1.8926, -1.9570],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0045, -0.0153,  0.0369,  ..., -0.0085, -0.0032, -0.0148],
        [-0.0366, -0.0204,  0.0304,  ...,  0.0419, -0.0047,  0.0106],
        [ 0.0013,  0.0115, -0.0156,  ..., -0.0030, -0.0023,  0.0003],
        ...,
        [-0.0330, -0.0139, -0.0158,  ..., -0.0071,  0.0178, -0.0205],
        [-0.0262, -0.0276,  0.0060,  ..., -0.0204, -0.0211,  0.0161],
        [ 0.0068, -0.0006, -0.0095,  ..., -0.0050,  0.0026, -0.0377]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.5996,  3.2910,  2.8691,  ..., -3.2891, -2.0059, -2.0586]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 00:24:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of county is counties
The plural form of formula is formulae
The plural form of industry is industries
The plural form of child is children
The plural form of duty is duties
The plural form of energy is energies
The plural form of safety is safeties
The plural form of security is
2024-07-19 00:24:10 root INFO     [order_1_approx] starting weight calculation for The plural form of child is children
The plural form of energy is energies
The plural form of duty is duties
The plural form of security is securities
The plural form of county is counties
The plural form of safety is safeties
The plural form of formula is formulae
The plural form of industry is
2024-07-19 00:24:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 00:28:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2893,  0.3909,  0.1606,  ..., -0.0642, -0.5083,  0.1569],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7803, -2.4180,  0.0068,  ...,  0.0527, -2.2871,  1.8916],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0100, -0.0170,  0.0179,  ...,  0.0034, -0.0061,  0.0034],
        [-0.0148, -0.0038, -0.0006,  ...,  0.0359,  0.0006, -0.0171],
        [ 0.0311,  0.0014, -0.0286,  ..., -0.0085,  0.0255,  0.0013],
        ...,
        [-0.0012, -0.0157,  0.0041,  ..., -0.0164,  0.0247,  0.0045],
        [ 0.0072, -0.0229,  0.0187,  ..., -0.0414, -0.0511,  0.0214],
        [-0.0020,  0.0239, -0.0090,  ..., -0.0002, -0.0026, -0.0441]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.3594, -2.7754, -0.1207,  ..., -0.4937, -2.9453,  0.9683]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 00:28:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of child is children
The plural form of energy is energies
The plural form of duty is duties
The plural form of security is securities
The plural form of county is counties
The plural form of safety is safeties
The plural form of formula is formulae
The plural form of industry is
2024-07-19 00:28:32 root INFO     [order_1_approx] starting weight calculation for The plural form of child is children
The plural form of security is securities
The plural form of industry is industries
The plural form of county is counties
The plural form of energy is energies
The plural form of duty is duties
The plural form of safety is safeties
The plural form of formula is
2024-07-19 00:28:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 00:32:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2607,  0.1565, -0.3123,  ..., -0.5459,  0.0964, -0.4841],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.8945,  1.1562, -0.5293,  ...,  1.5410, -1.3906,  1.8184],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0225, -0.0357,  0.0014,  ..., -0.0023,  0.0120,  0.0018],
        [-0.0184,  0.0050, -0.0094,  ...,  0.0017,  0.0236,  0.0039],
        [ 0.0102,  0.0105, -0.0451,  ...,  0.0009, -0.0018, -0.0071],
        ...,
        [-0.0010, -0.0088, -0.0013,  ..., -0.0033, -0.0031, -0.0037],
        [-0.0046, -0.0003, -0.0066,  ..., -0.0037, -0.0399, -0.0126],
        [ 0.0051,  0.0168, -0.0018,  ..., -0.0176,  0.0083, -0.0402]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.0859,  2.0977, -0.7803,  ...,  0.9565, -0.9858,  1.5420]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 00:32:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of child is children
The plural form of security is securities
The plural form of industry is industries
The plural form of county is counties
The plural form of energy is energies
The plural form of duty is duties
The plural form of safety is safeties
The plural form of formula is
2024-07-19 00:32:52 root INFO     [order_1_approx] starting weight calculation for The plural form of duty is duties
The plural form of formula is formulae
The plural form of county is counties
The plural form of security is securities
The plural form of safety is safeties
The plural form of energy is energies
The plural form of industry is industries
The plural form of child is
2024-07-19 00:32:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 00:37:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3635, -0.2603, -0.2222,  ...,  0.1790, -0.4326,  0.4634],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6768, -2.7812, -3.7109,  ...,  0.4023,  3.5352,  1.6738],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0215,  0.0054,  0.0140,  ..., -0.0012, -0.0108, -0.0223],
        [-0.0116, -0.0129, -0.0015,  ...,  0.0126,  0.0162,  0.0092],
        [ 0.0411,  0.0242, -0.0322,  ..., -0.0011,  0.0061, -0.0041],
        ...,
        [-0.0157, -0.0024,  0.0031,  ..., -0.0193,  0.0039,  0.0028],
        [-0.0014,  0.0165,  0.0109,  ..., -0.0159, -0.0376, -0.0065],
        [-0.0113,  0.0031, -0.0021,  ..., -0.0281,  0.0041, -0.0231]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.4531, -2.4785, -4.1016,  ..., -0.3228,  3.0000,  2.2207]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 00:37:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of duty is duties
The plural form of formula is formulae
The plural form of county is counties
The plural form of security is securities
The plural form of safety is safeties
The plural form of energy is energies
The plural form of industry is industries
The plural form of child is
2024-07-19 00:37:14 root INFO     [order_1_approx] starting weight calculation for The plural form of energy is energies
The plural form of safety is safeties
The plural form of industry is industries
The plural form of security is securities
The plural form of duty is duties
The plural form of child is children
The plural form of formula is formulae
The plural form of county is
2024-07-19 00:37:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 00:41:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3525,  0.0911, -1.4668,  ...,  0.1309, -0.8975,  0.5581],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.4404,  0.8037, -2.7383,  ..., -3.8594, -1.0518, -1.0283],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0161, -0.0064,  0.0212,  ..., -0.0215, -0.0028, -0.0080],
        [-0.0339, -0.0144,  0.0100,  ...,  0.0121,  0.0085,  0.0090],
        [ 0.0245,  0.0102, -0.0110,  ..., -0.0134,  0.0005, -0.0118],
        ...,
        [ 0.0018, -0.0228, -0.0228,  ..., -0.0186,  0.0100,  0.0024],
        [ 0.0312,  0.0039, -0.0107,  ..., -0.0206, -0.0110,  0.0149],
        [-0.0213, -0.0092,  0.0210,  ...,  0.0035,  0.0134, -0.0181]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2480,  1.2588, -3.7617,  ..., -2.9961, -1.1338, -1.3193]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 00:41:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of energy is energies
The plural form of safety is safeties
The plural form of industry is industries
The plural form of security is securities
The plural form of duty is duties
The plural form of child is children
The plural form of formula is formulae
The plural form of county is
2024-07-19 00:41:34 root INFO     [order_1_approx] starting weight calculation for The plural form of duty is duties
The plural form of security is securities
The plural form of formula is formulae
The plural form of energy is energies
The plural form of industry is industries
The plural form of child is children
The plural form of county is counties
The plural form of safety is
2024-07-19 00:41:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 00:45:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6733,  1.0117, -0.2257,  ...,  0.0502, -0.0996, -0.1943],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.4668,  7.6523,  4.2383,  ..., -3.1016,  0.5293,  2.2656],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0140, -0.0042,  0.0302,  ..., -0.0095, -0.0134, -0.0206],
        [-0.0243, -0.0218, -0.0037,  ...,  0.0397, -0.0012, -0.0124],
        [-0.0128, -0.0047, -0.0070,  ..., -0.0224,  0.0019, -0.0121],
        ...,
        [-0.0366, -0.0271,  0.0095,  ..., -0.0152,  0.0085,  0.0232],
        [ 0.0326,  0.0073, -0.0111,  ..., -0.0240, -0.0259,  0.0031],
        [-0.0231, -0.0076,  0.0097,  ..., -0.0042, -0.0014, -0.0196]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.5391,  7.7461,  3.6152,  ..., -3.4746,  0.9204,  2.5234]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 00:45:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of duty is duties
The plural form of security is securities
The plural form of formula is formulae
The plural form of energy is energies
The plural form of industry is industries
The plural form of child is children
The plural form of county is counties
The plural form of safety is
2024-07-19 00:45:53 root INFO     [order_1_approx] starting weight calculation for The plural form of energy is energies
The plural form of child is children
The plural form of industry is industries
The plural form of safety is safeties
The plural form of security is securities
The plural form of county is counties
The plural form of formula is formulae
The plural form of duty is
2024-07-19 00:45:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 00:50:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2854,  1.1699,  0.3369,  ..., -0.5112, -1.3994,  0.9160],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.1602,  0.3794, -1.0840,  ..., -4.3555,  1.6562,  1.3223],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0068, -0.0197,  0.0172,  ..., -0.0079, -0.0184, -0.0486],
        [ 0.0029, -0.0381,  0.0099,  ..., -0.0024,  0.0184,  0.0205],
        [ 0.0352,  0.0367, -0.0634,  ...,  0.0161, -0.0195, -0.0064],
        ...,
        [-0.0091, -0.0293, -0.0070,  ..., -0.0287,  0.0229,  0.0167],
        [ 0.0017,  0.0068, -0.0215,  ..., -0.0242, -0.0508, -0.0191],
        [ 0.0145,  0.0278, -0.0016,  ...,  0.0171, -0.0019, -0.0167]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.8770,  2.0371, -2.0664,  ..., -3.1777,  1.0957,  2.5684]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 00:50:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of energy is energies
The plural form of child is children
The plural form of industry is industries
The plural form of safety is safeties
The plural form of security is securities
The plural form of county is counties
The plural form of formula is formulae
The plural form of duty is
2024-07-19 00:50:14 root INFO     total operator prediction time: 2087.1962463855743 seconds
2024-07-19 00:50:14 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on Ving - verb_inf
2024-07-19 00:50:14 root INFO     building operator Ving - verb_inf
2024-07-19 00:50:15 root INFO     [order_1_approx] starting weight calculation for adding is the active form of add
ensuring is the active form of ensure
remembering is the active form of remember
spending is the active form of spend
creating is the active form of create
providing is the active form of provide
identifying is the active form of identify
considering is the active form of
2024-07-19 00:50:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 00:54:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2443,  0.2593, -0.2335,  ..., -0.6021,  0.4729,  0.2886],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.2812,  0.8208, -0.1641,  ..., -1.0820, -1.6113, -2.9590],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.9602e-02, -2.1942e-02,  4.7302e-02,  ...,  3.2043e-02,
          2.0935e-02,  6.5460e-03],
        [ 2.3956e-02, -2.4166e-03,  8.5297e-03,  ...,  9.5825e-03,
          2.6794e-02, -2.4841e-02],
        [ 4.6234e-03,  2.8107e-02, -1.4183e-02,  ..., -6.5155e-03,
          1.7729e-03, -3.2410e-02],
        ...,
        [-2.8229e-02, -7.3509e-03, -1.7071e-03,  ..., -6.8321e-03,
          2.3895e-02, -1.1978e-03],
        [ 6.7993e-02, -2.7679e-02,  2.4353e-02,  ..., -4.8218e-03,
         -4.5776e-04,  6.6833e-03],
        [ 2.0554e-02,  7.2327e-03,  2.3758e-02,  ...,  4.5776e-05,
          1.3313e-03, -5.1056e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.8438,  3.4199,  0.3486,  ..., -0.8945,  1.5195, -1.1641]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 00:54:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for adding is the active form of add
ensuring is the active form of ensure
remembering is the active form of remember
spending is the active form of spend
creating is the active form of create
providing is the active form of provide
identifying is the active form of identify
considering is the active form of
2024-07-19 00:54:36 root INFO     [order_1_approx] starting weight calculation for ensuring is the active form of ensure
remembering is the active form of remember
considering is the active form of consider
adding is the active form of add
spending is the active form of spend
providing is the active form of provide
creating is the active form of create
identifying is the active form of
2024-07-19 00:54:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 00:58:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4458,  0.3164, -0.4985,  ..., -0.1667,  0.2087, -0.2949],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3867,  0.0059, -4.1562,  ..., -0.2563,  0.8784,  1.1084],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0220, -0.0260,  0.0239,  ...,  0.0232,  0.0024, -0.0014],
        [ 0.0034,  0.0016,  0.0082,  ...,  0.0017,  0.0294,  0.0082],
        [ 0.0105,  0.0028, -0.0182,  ...,  0.0119, -0.0028, -0.0171],
        ...,
        [-0.0217, -0.0027,  0.0015,  ..., -0.0315,  0.0284,  0.0071],
        [-0.0201,  0.0134,  0.0060,  ..., -0.0137, -0.0344,  0.0284],
        [ 0.0015,  0.0192,  0.0075,  ..., -0.0043,  0.0062, -0.0133]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7422,  0.6548, -3.2422,  ..., -1.0537,  0.1675,  1.0068]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 00:58:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for ensuring is the active form of ensure
remembering is the active form of remember
considering is the active form of consider
adding is the active form of add
spending is the active form of spend
providing is the active form of provide
creating is the active form of create
identifying is the active form of
2024-07-19 00:58:58 root INFO     [order_1_approx] starting weight calculation for considering is the active form of consider
creating is the active form of create
identifying is the active form of identify
providing is the active form of provide
spending is the active form of spend
remembering is the active form of remember
ensuring is the active form of ensure
adding is the active form of
2024-07-19 00:58:58 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 01:03:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1194,  0.2585,  0.3223,  ..., -0.7627, -0.2788, -0.2245],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8486,  0.3599,  4.4336,  ..., -3.6621, -3.6621,  1.5039],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0098,  0.0120,  0.0184,  ...,  0.0063,  0.0013,  0.0024],
        [ 0.0035,  0.0001,  0.0079,  ..., -0.0131,  0.0005,  0.0365],
        [ 0.0005, -0.0091, -0.0117,  ..., -0.0106, -0.0121,  0.0195],
        ...,
        [-0.0388, -0.0070, -0.0030,  ..., -0.0016, -0.0024,  0.0112],
        [-0.0239,  0.0007,  0.0040,  ..., -0.0131, -0.0095, -0.0152],
        [-0.0064, -0.0192,  0.0066,  ..., -0.0175,  0.0069,  0.0276]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.2598,  0.1974,  3.1367,  ..., -4.4883, -4.2656,  1.1572]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 01:03:19 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for considering is the active form of consider
creating is the active form of create
identifying is the active form of identify
providing is the active form of provide
spending is the active form of spend
remembering is the active form of remember
ensuring is the active form of ensure
adding is the active form of
2024-07-19 01:03:19 root INFO     [order_1_approx] starting weight calculation for providing is the active form of provide
adding is the active form of add
identifying is the active form of identify
creating is the active form of create
considering is the active form of consider
spending is the active form of spend
remembering is the active form of remember
ensuring is the active form of
2024-07-19 01:03:19 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 01:07:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3416,  0.3840,  0.2942,  ..., -0.1938, -0.3662,  0.3052],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.3418,  1.8232,  2.1211,  ..., -0.0209, -0.4175,  2.2285],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0074, -0.0118,  0.0403,  ..., -0.0054,  0.0145,  0.0128],
        [-0.0003,  0.0037, -0.0040,  ...,  0.0248,  0.0095, -0.0066],
        [ 0.0143, -0.0017,  0.0009,  ..., -0.0031,  0.0061, -0.0127],
        ...,
        [ 0.0027, -0.0182, -0.0310,  ..., -0.0138, -0.0273,  0.0025],
        [ 0.0052,  0.0076,  0.0134,  ..., -0.0439, -0.0319,  0.0008],
        [ 0.0272, -0.0034,  0.0199,  ...,  0.0060,  0.0215, -0.0131]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.8750,  2.4492,  3.5469,  ...,  1.2793, -1.3496,  1.2100]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 01:07:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for providing is the active form of provide
adding is the active form of add
identifying is the active form of identify
creating is the active form of create
considering is the active form of consider
spending is the active form of spend
remembering is the active form of remember
ensuring is the active form of
2024-07-19 01:07:41 root INFO     [order_1_approx] starting weight calculation for ensuring is the active form of ensure
considering is the active form of consider
identifying is the active form of identify
creating is the active form of create
remembering is the active form of remember
adding is the active form of add
spending is the active form of spend
providing is the active form of
2024-07-19 01:07:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 01:12:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3977,  0.2250,  0.0012,  ..., -0.1606,  0.3252, -0.0129],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.1211,  1.7334,  0.9888,  ..., -1.2441, -3.7520, -2.0938],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0132, -0.0127,  0.0485,  ...,  0.0137, -0.0106, -0.0074],
        [ 0.0111, -0.0101, -0.0084,  ...,  0.0043,  0.0123,  0.0098],
        [ 0.0047, -0.0230,  0.0046,  ...,  0.0102, -0.0092, -0.0096],
        ...,
        [-0.0227, -0.0035,  0.0055,  ..., -0.0239,  0.0056,  0.0022],
        [-0.0127, -0.0166,  0.0040,  ...,  0.0087, -0.0302,  0.0281],
        [ 0.0050,  0.0168,  0.0026,  ...,  0.0013, -0.0129, -0.0240]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.7031,  1.5469,  0.1001,  ..., -0.2471, -5.1523, -3.1914]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 01:12:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for ensuring is the active form of ensure
considering is the active form of consider
identifying is the active form of identify
creating is the active form of create
remembering is the active form of remember
adding is the active form of add
spending is the active form of spend
providing is the active form of
2024-07-19 01:12:03 root INFO     [order_1_approx] starting weight calculation for providing is the active form of provide
creating is the active form of create
ensuring is the active form of ensure
considering is the active form of consider
identifying is the active form of identify
remembering is the active form of remember
adding is the active form of add
spending is the active form of
2024-07-19 01:12:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 01:16:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2205,  0.7661, -0.1628,  ..., -0.8501,  0.7979, -0.2693],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 6.0508,  0.8242,  2.3887,  ..., -1.4150, -1.4082,  0.9453],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0025, -0.0224,  0.0158,  ..., -0.0083, -0.0081,  0.0325],
        [-0.0110,  0.0060, -0.0089,  ...,  0.0163,  0.0030,  0.0079],
        [ 0.0027, -0.0179,  0.0063,  ..., -0.0070, -0.0047,  0.0186],
        ...,
        [ 0.0030, -0.0200, -0.0152,  ..., -0.0381, -0.0167,  0.0356],
        [-0.0118,  0.0203,  0.0251,  ..., -0.0036, -0.0327, -0.0182],
        [ 0.0182, -0.0065, -0.0025,  ..., -0.0048,  0.0037, -0.0082]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.1250,  0.7529,  0.8066,  ..., -2.5430, -0.5449,  1.1504]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 01:16:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for providing is the active form of provide
creating is the active form of create
ensuring is the active form of ensure
considering is the active form of consider
identifying is the active form of identify
remembering is the active form of remember
adding is the active form of add
spending is the active form of
2024-07-19 01:16:25 root INFO     [order_1_approx] starting weight calculation for spending is the active form of spend
identifying is the active form of identify
adding is the active form of add
creating is the active form of create
considering is the active form of consider
providing is the active form of provide
ensuring is the active form of ensure
remembering is the active form of
2024-07-19 01:16:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 01:20:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9971,  0.3726, -0.1066,  ...,  0.2412,  0.2351,  0.7451],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7622,  2.7246, -2.3711,  ..., -2.1172, -3.1914,  3.3145],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0465, -0.0278, -0.0164,  ..., -0.0077,  0.0417, -0.0586],
        [-0.0364,  0.0194,  0.0009,  ...,  0.0242, -0.0311,  0.0392],
        [ 0.0085,  0.0216, -0.0067,  ...,  0.0119,  0.0005, -0.0139],
        ...,
        [-0.0447, -0.0184,  0.0189,  ...,  0.0112, -0.0073,  0.0052],
        [ 0.0350, -0.0143, -0.0136,  ..., -0.0278,  0.0311, -0.0428],
        [ 0.0298, -0.0072, -0.0099,  ..., -0.0209,  0.0166, -0.0110]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.8809,  2.4336, -2.6055,  ..., -1.9336, -1.8984,  4.3828]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 01:20:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for spending is the active form of spend
identifying is the active form of identify
adding is the active form of add
creating is the active form of create
considering is the active form of consider
providing is the active form of provide
ensuring is the active form of ensure
remembering is the active form of
2024-07-19 01:20:46 root INFO     [order_1_approx] starting weight calculation for considering is the active form of consider
adding is the active form of add
spending is the active form of spend
remembering is the active form of remember
identifying is the active form of identify
ensuring is the active form of ensure
providing is the active form of provide
creating is the active form of
2024-07-19 01:20:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 01:25:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5527,  0.0312, -0.2295,  ...,  0.3696,  0.5435, -0.6406],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4258, -1.4502, -0.5596,  ..., -1.7402, -1.6094, -2.3535],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0217, -0.0125,  0.0280,  ...,  0.0007, -0.0067, -0.0121],
        [-0.0021, -0.0013,  0.0008,  ...,  0.0121, -0.0053,  0.0185],
        [ 0.0040,  0.0126, -0.0133,  ...,  0.0145, -0.0005,  0.0139],
        ...,
        [-0.0098, -0.0127,  0.0053,  ..., -0.0038, -0.0011,  0.0145],
        [ 0.0005, -0.0072,  0.0223,  ...,  0.0004,  0.0074,  0.0081],
        [-0.0035,  0.0002,  0.0017,  ...,  0.0103,  0.0150,  0.0042]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7373, -0.3760, -0.2407,  ..., -1.7275, -1.9150, -2.6992]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 01:25:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for considering is the active form of consider
adding is the active form of add
spending is the active form of spend
remembering is the active form of remember
identifying is the active form of identify
ensuring is the active form of ensure
providing is the active form of provide
creating is the active form of
2024-07-19 01:25:07 root INFO     total operator prediction time: 2092.6947457790375 seconds
2024-07-19 01:25:07 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_Ving - Ved
2024-07-19 01:25:07 root INFO     building operator verb_Ving - Ved
2024-07-19 01:25:07 root INFO     [order_1_approx] starting weight calculation for After something is expecting, it has expected
After something is asking, it has asked
After something is improving, it has improved
After something is introducing, it has introduced
After something is losing, it has lost
After something is telling, it has told
After something is considering, it has considered
After something is representing, it has
2024-07-19 01:25:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 01:29:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7109, -0.0022, -0.0146,  ..., -0.6309,  0.1904,  0.6372],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.4492,  0.2920,  1.8682,  ...,  2.3613, -0.3733,  5.0664],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0063, -0.0030,  0.0242,  ...,  0.0131, -0.0302,  0.0058],
        [ 0.0019, -0.0246, -0.0168,  ..., -0.0012, -0.0044,  0.0112],
        [ 0.0118,  0.0242, -0.0048,  ...,  0.0033,  0.0066, -0.0145],
        ...,
        [-0.0253,  0.0168,  0.0040,  ..., -0.0326,  0.0274, -0.0072],
        [ 0.0177, -0.0048, -0.0142,  ..., -0.0387, -0.0245, -0.0104],
        [ 0.0253, -0.0100,  0.0038,  ..., -0.0122, -0.0039, -0.0304]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.5078, -0.5508,  1.2002,  ...,  2.3496, -0.0925,  5.0391]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 01:29:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is expecting, it has expected
After something is asking, it has asked
After something is improving, it has improved
After something is introducing, it has introduced
After something is losing, it has lost
After something is telling, it has told
After something is considering, it has considered
After something is representing, it has
2024-07-19 01:29:30 root INFO     [order_1_approx] starting weight calculation for After something is expecting, it has expected
After something is telling, it has told
After something is introducing, it has introduced
After something is improving, it has improved
After something is considering, it has considered
After something is losing, it has lost
After something is representing, it has represented
After something is asking, it has
2024-07-19 01:29:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 01:33:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2788,  0.1443,  0.0574,  ..., -0.7749,  0.5615,  0.2810],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7944,  2.4453,  2.6367,  ...,  3.7402, -0.0294,  3.8047],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0389, -0.0169,  0.0132,  ...,  0.0065, -0.0324,  0.0166],
        [-0.0331, -0.0448,  0.0476,  ...,  0.0397, -0.0432,  0.0151],
        [-0.0370, -0.0105,  0.0063,  ...,  0.0300, -0.0248,  0.0333],
        ...,
        [-0.0793, -0.0269,  0.0731,  ...,  0.0457, -0.0859,  0.0443],
        [ 0.0169,  0.0069,  0.0100,  ..., -0.0248, -0.0176, -0.0193],
        [-0.0009,  0.0235,  0.0134,  ...,  0.0064,  0.0082, -0.0332]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[0.3413, 0.8730, 1.1250,  ..., 1.0352, 0.2347, 4.6172]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 01:33:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is expecting, it has expected
After something is telling, it has told
After something is introducing, it has introduced
After something is improving, it has improved
After something is considering, it has considered
After something is losing, it has lost
After something is representing, it has represented
After something is asking, it has
2024-07-19 01:33:51 root INFO     [order_1_approx] starting weight calculation for After something is expecting, it has expected
After something is losing, it has lost
After something is introducing, it has introduced
After something is telling, it has told
After something is asking, it has asked
After something is representing, it has represented
After something is considering, it has considered
After something is improving, it has
2024-07-19 01:33:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 01:38:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0164,  0.1699,  0.5186,  ..., -0.2568,  0.5498, -0.0517],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 6.5938,  1.2734,  2.0820,  ...,  5.0391, -1.4980,  1.8486],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0300, -0.0264, -0.0119,  ..., -0.0177, -0.0125, -0.0301],
        [-0.0078, -0.0148, -0.0026,  ..., -0.0100, -0.0057, -0.0073],
        [ 0.0081, -0.0101, -0.0226,  ..., -0.0014,  0.0065, -0.0060],
        ...,
        [ 0.0069, -0.0037, -0.0212,  ..., -0.0226,  0.0048,  0.0033],
        [-0.0167,  0.0061, -0.0032,  ..., -0.0181, -0.0529, -0.0094],
        [-0.0076,  0.0079,  0.0215,  ...,  0.0166, -0.0184, -0.0273]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.2266,  1.4258,  2.3496,  ...,  4.6289, -2.0039,  1.7578]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 01:38:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is expecting, it has expected
After something is losing, it has lost
After something is introducing, it has introduced
After something is telling, it has told
After something is asking, it has asked
After something is representing, it has represented
After something is considering, it has considered
After something is improving, it has
2024-07-19 01:38:13 root INFO     [order_1_approx] starting weight calculation for After something is representing, it has represented
After something is considering, it has considered
After something is improving, it has improved
After something is asking, it has asked
After something is losing, it has lost
After something is telling, it has told
After something is introducing, it has introduced
After something is expecting, it has
2024-07-19 01:38:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 01:42:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4561,  0.3936, -0.2822,  ...,  0.1553,  0.7046,  0.4910],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4883,  1.6836,  0.9712,  ...,  1.7549, -2.3613,  4.2109],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0351, -0.0211,  0.0131,  ...,  0.0108, -0.0224,  0.0090],
        [-0.0065, -0.0254, -0.0016,  ..., -0.0124, -0.0046, -0.0022],
        [ 0.0205,  0.0263, -0.0208,  ...,  0.0246,  0.0021, -0.0023],
        ...,
        [-0.0111, -0.0020, -0.0034,  ..., -0.0082, -0.0264, -0.0175],
        [ 0.0249,  0.0187,  0.0016,  ..., -0.0173, -0.0096, -0.0133],
        [ 0.0067,  0.0009, -0.0017,  ..., -0.0020, -0.0050, -0.0466]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7500,  1.8184,  1.9004,  ...,  1.5391, -1.2451,  4.4414]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 01:42:34 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is representing, it has represented
After something is considering, it has considered
After something is improving, it has improved
After something is asking, it has asked
After something is losing, it has lost
After something is telling, it has told
After something is introducing, it has introduced
After something is expecting, it has
2024-07-19 01:42:34 root INFO     [order_1_approx] starting weight calculation for After something is representing, it has represented
After something is asking, it has asked
After something is expecting, it has expected
After something is improving, it has improved
After something is introducing, it has introduced
After something is considering, it has considered
After something is losing, it has lost
After something is telling, it has
2024-07-19 01:42:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 01:46:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0511,  0.0031,  0.3379,  ...,  0.2358, -0.1991,  0.4580],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([1.9785, 2.3926, 2.8984,  ..., 3.5059, 2.2969, 2.4668], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0064, -0.0370,  0.0084,  ..., -0.0092,  0.0226, -0.0105],
        [ 0.0026, -0.0356, -0.0011,  ..., -0.0117,  0.0264, -0.0084],
        [-0.0175,  0.0385, -0.0043,  ...,  0.0005,  0.0135, -0.0181],
        ...,
        [-0.0284, -0.0426, -0.0047,  ...,  0.0014, -0.0184,  0.0352],
        [-0.0090, -0.0052,  0.0213,  ..., -0.0122, -0.0393, -0.0171],
        [-0.0093,  0.0381,  0.0178,  ...,  0.0084, -0.0095, -0.0394]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[1.6719, 3.0391, 1.7744,  ..., 3.6484, 3.0938, 3.1680]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 01:46:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is representing, it has represented
After something is asking, it has asked
After something is expecting, it has expected
After something is improving, it has improved
After something is introducing, it has introduced
After something is considering, it has considered
After something is losing, it has lost
After something is telling, it has
2024-07-19 01:46:55 root INFO     [order_1_approx] starting weight calculation for After something is improving, it has improved
After something is asking, it has asked
After something is considering, it has considered
After something is introducing, it has introduced
After something is telling, it has told
After something is expecting, it has expected
After something is representing, it has represented
After something is losing, it has
2024-07-19 01:46:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 01:51:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0918, -0.2262,  0.3706,  ...,  0.0022, -0.5283,  0.5308],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.2344, -0.1555,  1.0645,  ...,  1.8965, -0.2837,  0.8203],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.9287e-02, -2.1912e-02, -2.6798e-03,  ...,  1.2085e-02,
         -1.8478e-02, -3.0243e-02],
        [ 2.4841e-02, -1.4999e-02,  7.1220e-03,  ..., -1.7273e-02,
          3.3417e-03,  4.1313e-03],
        [-7.3624e-03,  1.0700e-03,  3.2692e-03,  ..., -4.7684e-05,
          8.5220e-03,  1.0025e-02],
        ...,
        [ 7.7820e-04,  1.3733e-03,  5.3864e-03,  ..., -2.4445e-02,
         -2.2339e-02, -3.2959e-03],
        [ 2.7008e-03, -1.4091e-02, -3.6682e-02,  ..., -3.1738e-02,
         -1.3443e-02,  1.6235e-02],
        [-6.1264e-03,  2.5101e-03,  4.8401e-02,  ..., -8.1787e-03,
          1.5244e-02, -3.2959e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.7734, -1.0674,  0.8232,  ...,  1.0176,  0.5400,  0.7715]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 01:51:17 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is improving, it has improved
After something is asking, it has asked
After something is considering, it has considered
After something is introducing, it has introduced
After something is telling, it has told
After something is expecting, it has expected
After something is representing, it has represented
After something is losing, it has
2024-07-19 01:51:17 root INFO     [order_1_approx] starting weight calculation for After something is representing, it has represented
After something is asking, it has asked
After something is considering, it has considered
After something is expecting, it has expected
After something is losing, it has lost
After something is telling, it has told
After something is improving, it has improved
After something is introducing, it has
2024-07-19 01:51:17 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 01:55:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3154,  0.2812, -0.1774,  ..., -0.1273, -0.2954, -0.0760],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.7451, -1.0312,  1.7656,  ...,  5.5352, -0.8950,  5.3281],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0077,  0.0190, -0.0300,  ..., -0.0001,  0.0118, -0.0072],
        [ 0.0034, -0.0317, -0.0127,  ..., -0.0051,  0.0019, -0.0047],
        [ 0.0206,  0.0174, -0.0218,  ..., -0.0486, -0.0059,  0.0049],
        ...,
        [ 0.0129, -0.0119, -0.0174,  ...,  0.0120, -0.0425, -0.0213],
        [ 0.0337,  0.0140, -0.0194,  ..., -0.0460, -0.0509, -0.0180],
        [-0.0199, -0.0298,  0.0479,  ...,  0.0161, -0.0200, -0.0162]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1396, -1.7520,  1.9375,  ...,  3.1953, -0.2808,  4.0859]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 01:55:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is representing, it has represented
After something is asking, it has asked
After something is considering, it has considered
After something is expecting, it has expected
After something is losing, it has lost
After something is telling, it has told
After something is improving, it has improved
After something is introducing, it has
2024-07-19 01:55:39 root INFO     [order_1_approx] starting weight calculation for After something is representing, it has represented
After something is telling, it has told
After something is improving, it has improved
After something is losing, it has lost
After something is expecting, it has expected
After something is asking, it has asked
After something is introducing, it has introduced
After something is considering, it has
2024-07-19 01:55:39 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 01:59:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4041,  0.2283, -0.3564,  ..., -0.5215,  0.0409,  1.0586],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1926,  2.7168,  0.4062,  ...,  1.1973, -4.2266, -0.4551],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0310,  0.0263, -0.0153,  ..., -0.0078,  0.0201,  0.0144],
        [-0.0264, -0.0064,  0.0106,  ...,  0.0033, -0.0024, -0.0108],
        [ 0.0175,  0.0051, -0.0344,  ...,  0.0177,  0.0179, -0.0241],
        ...,
        [-0.0141,  0.0013, -0.0022,  ..., -0.0180, -0.0091,  0.0060],
        [ 0.0161,  0.0144,  0.0035,  ..., -0.0286, -0.0402,  0.0150],
        [-0.0088,  0.0263,  0.0143,  ...,  0.0320, -0.0145, -0.0773]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0737,  2.0879,  0.5923,  ...,  0.4355, -3.3398, -0.1482]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 02:00:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for After something is representing, it has represented
After something is telling, it has told
After something is improving, it has improved
After something is losing, it has lost
After something is expecting, it has expected
After something is asking, it has asked
After something is introducing, it has introduced
After something is considering, it has
2024-07-19 02:00:01 root INFO     total operator prediction time: 2093.4888043403625 seconds
2024-07-19 02:00:01 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_inf - Ved
2024-07-19 02:00:01 root INFO     building operator verb_inf - Ved
2024-07-19 02:00:01 root INFO     [order_1_approx] starting weight calculation for If the present form is send, the past form is sent
If the present form is relate, the past form is related
If the present form is reduce, the past form is reduced
If the present form is perform, the past form is performed
If the present form is create, the past form is created
If the present form is discover, the past form is discovered
If the present form is enjoy, the past form is enjoyed
If the present form is remain, the past form is
2024-07-19 02:00:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 02:04:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-6.8164e-01,  1.1592e+00,  3.1348e-01,  ...,  2.7271e-01,
        -1.6333e-01, -2.4414e-04], device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.4648,  2.0840, -1.4883,  ...,  1.9375,  1.3496,  0.8481],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0091, -0.0043, -0.0109,  ...,  0.0226, -0.0094, -0.0051],
        [-0.0159, -0.0240, -0.0097,  ..., -0.0014,  0.0205,  0.0249],
        [-0.0066,  0.0086, -0.0361,  ...,  0.0096,  0.0080,  0.0029],
        ...,
        [-0.0054,  0.0061, -0.0237,  ..., -0.0199, -0.0064, -0.0125],
        [ 0.0262,  0.0104, -0.0113,  ..., -0.0260, -0.0219, -0.0298],
        [-0.0294,  0.0042,  0.0203,  ...,  0.0055, -0.0142, -0.0393]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.9297,  1.3555, -1.4854,  ...,  1.5283,  1.3662,  0.7974]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 02:04:20 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is send, the past form is sent
If the present form is relate, the past form is related
If the present form is reduce, the past form is reduced
If the present form is perform, the past form is performed
If the present form is create, the past form is created
If the present form is discover, the past form is discovered
If the present form is enjoy, the past form is enjoyed
If the present form is remain, the past form is
2024-07-19 02:04:20 root INFO     [order_1_approx] starting weight calculation for If the present form is create, the past form is created
If the present form is relate, the past form is related
If the present form is send, the past form is sent
If the present form is remain, the past form is remained
If the present form is enjoy, the past form is enjoyed
If the present form is reduce, the past form is reduced
If the present form is discover, the past form is discovered
If the present form is perform, the past form is
2024-07-19 02:04:20 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 02:08:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-3.3350e-01,  6.4941e-01, -1.1853e-01,  ..., -3.5254e-01,
         9.3262e-02, -5.4932e-04], device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1436, -0.1099, -0.9443,  ...,  3.7910,  1.2773,  0.8779],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0285, -0.0150, -0.0032,  ...,  0.0002, -0.0201, -0.0197],
        [-0.0043, -0.0147, -0.0028,  ..., -0.0040, -0.0026,  0.0085],
        [ 0.0242,  0.0053, -0.0162,  ...,  0.0004,  0.0083, -0.0151],
        ...,
        [-0.0279, -0.0251, -0.0094,  ..., -0.0322, -0.0018, -0.0006],
        [ 0.0122,  0.0077, -0.0056,  ..., -0.0311, -0.0287,  0.0162],
        [-0.0272,  0.0127,  0.0027,  ...,  0.0103, -0.0084, -0.0493]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1519,  0.6890, -1.8359,  ...,  3.5664,  1.4727,  1.0684]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 02:08:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is create, the past form is created
If the present form is relate, the past form is related
If the present form is send, the past form is sent
If the present form is remain, the past form is remained
If the present form is enjoy, the past form is enjoyed
If the present form is reduce, the past form is reduced
If the present form is discover, the past form is discovered
If the present form is perform, the past form is
2024-07-19 02:08:39 root INFO     [order_1_approx] starting weight calculation for If the present form is reduce, the past form is reduced
If the present form is perform, the past form is performed
If the present form is enjoy, the past form is enjoyed
If the present form is remain, the past form is remained
If the present form is create, the past form is created
If the present form is relate, the past form is related
If the present form is discover, the past form is discovered
If the present form is send, the past form is
2024-07-19 02:08:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 02:12:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3232,  0.5264,  0.3633,  ...,  0.0981, -0.1213,  0.1562],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.7461,  1.7139,  2.3379,  ...,  3.0703, -1.6260,  1.6016],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0253,  0.0102,  0.0012,  ...,  0.0113,  0.0018, -0.0183],
        [-0.0132, -0.0312,  0.0034,  ..., -0.0039, -0.0141,  0.0102],
        [ 0.0251,  0.0049, -0.0158,  ..., -0.0073, -0.0104,  0.0039],
        ...,
        [-0.0191, -0.0144, -0.0206,  ..., -0.0352,  0.0031, -0.0067],
        [ 0.0168,  0.0020,  0.0086,  ..., -0.0220, -0.0410, -0.0165],
        [-0.0155,  0.0075,  0.0240,  ..., -0.0191, -0.0066, -0.0369]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.3008,  1.8984,  1.5586,  ...,  3.8809, -2.8242,  1.8164]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 02:12:58 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is reduce, the past form is reduced
If the present form is perform, the past form is performed
If the present form is enjoy, the past form is enjoyed
If the present form is remain, the past form is remained
If the present form is create, the past form is created
If the present form is relate, the past form is related
If the present form is discover, the past form is discovered
If the present form is send, the past form is
2024-07-19 02:12:58 root INFO     [order_1_approx] starting weight calculation for If the present form is perform, the past form is performed
If the present form is discover, the past form is discovered
If the present form is relate, the past form is related
If the present form is enjoy, the past form is enjoyed
If the present form is reduce, the past form is reduced
If the present form is send, the past form is sent
If the present form is remain, the past form is remained
If the present form is create, the past form is
2024-07-19 02:12:59 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 02:17:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2512,  0.0845, -0.3726,  ...,  0.8745,  0.1052, -0.6982],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4683, -0.0083, -0.2705,  ..., -1.4414, -1.7627, -0.9331],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0343,  0.0006,  0.0038,  ...,  0.0131, -0.0100, -0.0175],
        [-0.0068, -0.0140, -0.0053,  ...,  0.0051,  0.0143,  0.0156],
        [-0.0030, -0.0033, -0.0249,  ..., -0.0029,  0.0080, -0.0115],
        ...,
        [-0.0013,  0.0113, -0.0182,  ..., -0.0235,  0.0175, -0.0293],
        [ 0.0050,  0.0285,  0.0076,  ..., -0.0275, -0.0347,  0.0052],
        [-0.0290, -0.0143,  0.0029,  ...,  0.0031,  0.0006, -0.0419]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5703,  0.0347, -0.5962,  ..., -3.0176, -1.8018, -1.6865]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 02:17:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is perform, the past form is performed
If the present form is discover, the past form is discovered
If the present form is relate, the past form is related
If the present form is enjoy, the past form is enjoyed
If the present form is reduce, the past form is reduced
If the present form is send, the past form is sent
If the present form is remain, the past form is remained
If the present form is create, the past form is
2024-07-19 02:17:18 root INFO     [order_1_approx] starting weight calculation for If the present form is relate, the past form is related
If the present form is enjoy, the past form is enjoyed
If the present form is discover, the past form is discovered
If the present form is remain, the past form is remained
If the present form is send, the past form is sent
If the present form is perform, the past form is performed
If the present form is create, the past form is created
If the present form is reduce, the past form is
2024-07-19 02:17:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 02:21:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2134,  0.4331, -0.5137,  ..., -0.4624, -0.6113,  0.3264],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1250, -1.0508,  0.6475,  ..., -0.6602, -0.2822, -1.5889],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0103,  0.0104,  0.0057,  ...,  0.0267, -0.0066, -0.0103],
        [-0.0169, -0.0335,  0.0012,  ..., -0.0083, -0.0066,  0.0045],
        [ 0.0176,  0.0095, -0.0258,  ..., -0.0136,  0.0083, -0.0018],
        ...,
        [-0.0151, -0.0028,  0.0045,  ..., -0.0500,  0.0056, -0.0177],
        [ 0.0021,  0.0040,  0.0051,  ..., -0.0212, -0.0198, -0.0041],
        [-0.0019,  0.0151, -0.0171,  ..., -0.0047,  0.0024, -0.0318]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7324, -1.4385, -0.0635,  ..., -2.0664, -0.1224, -1.1797]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 02:21:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is relate, the past form is related
If the present form is enjoy, the past form is enjoyed
If the present form is discover, the past form is discovered
If the present form is remain, the past form is remained
If the present form is send, the past form is sent
If the present form is perform, the past form is performed
If the present form is create, the past form is created
If the present form is reduce, the past form is
2024-07-19 02:21:37 root INFO     [order_1_approx] starting weight calculation for If the present form is send, the past form is sent
If the present form is discover, the past form is discovered
If the present form is create, the past form is created
If the present form is reduce, the past form is reduced
If the present form is relate, the past form is related
If the present form is remain, the past form is remained
If the present form is perform, the past form is performed
If the present form is enjoy, the past form is
2024-07-19 02:21:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 02:25:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4775,  0.6133, -0.5635,  ..., -0.0060, -0.2290,  0.1814],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5820e+00,  1.4844e+00,  2.2578e+00,  ..., -1.9531e-03,
        -2.0957e+00,  3.0000e+00], device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0321,  0.0130,  0.0026,  ..., -0.0060, -0.0101, -0.0262],
        [-0.0087, -0.0236,  0.0130,  ..., -0.0056,  0.0066,  0.0122],
        [ 0.0119, -0.0106, -0.0399,  ...,  0.0161,  0.0068, -0.0246],
        ...,
        [-0.0183, -0.0145,  0.0094,  ..., -0.0494,  0.0281,  0.0077],
        [-0.0003,  0.0392, -0.0111,  ..., -0.0028, -0.0267,  0.0145],
        [-0.0064,  0.0049, -0.0075,  ...,  0.0093,  0.0273, -0.0473]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.8887,  2.5234,  3.1855,  ..., -0.1394, -2.1992,  3.8125]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 02:25:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is send, the past form is sent
If the present form is discover, the past form is discovered
If the present form is create, the past form is created
If the present form is reduce, the past form is reduced
If the present form is relate, the past form is related
If the present form is remain, the past form is remained
If the present form is perform, the past form is performed
If the present form is enjoy, the past form is
2024-07-19 02:25:56 root INFO     [order_1_approx] starting weight calculation for If the present form is create, the past form is created
If the present form is discover, the past form is discovered
If the present form is reduce, the past form is reduced
If the present form is send, the past form is sent
If the present form is perform, the past form is performed
If the present form is enjoy, the past form is enjoyed
If the present form is remain, the past form is remained
If the present form is relate, the past form is
2024-07-19 02:25:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 02:30:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0817,  0.1328, -0.3618,  ..., -0.0347, -0.4712, -0.3406],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5654,  0.2725, -0.6475,  ..., -0.4756,  1.9824,  0.6934],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0083, -0.0012,  0.0098,  ...,  0.0145, -0.0188, -0.0159],
        [-0.0358, -0.0118, -0.0054,  ...,  0.0016,  0.0129,  0.0065],
        [-0.0009,  0.0024, -0.0237,  ...,  0.0039, -0.0053, -0.0114],
        ...,
        [-0.0414, -0.0120,  0.0021,  ..., -0.0336,  0.0194, -0.0052],
        [ 0.0081,  0.0083,  0.0042,  ..., -0.0353, -0.0227, -0.0174],
        [-0.0061,  0.0053,  0.0119,  ..., -0.0031, -0.0004, -0.0264]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3154, -0.0085, -0.7612,  ..., -1.2539,  2.0039,  0.9370]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 02:30:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is create, the past form is created
If the present form is discover, the past form is discovered
If the present form is reduce, the past form is reduced
If the present form is send, the past form is sent
If the present form is perform, the past form is performed
If the present form is enjoy, the past form is enjoyed
If the present form is remain, the past form is remained
If the present form is relate, the past form is
2024-07-19 02:30:13 root INFO     [order_1_approx] starting weight calculation for If the present form is remain, the past form is remained
If the present form is enjoy, the past form is enjoyed
If the present form is reduce, the past form is reduced
If the present form is send, the past form is sent
If the present form is perform, the past form is performed
If the present form is relate, the past form is related
If the present form is create, the past form is created
If the present form is discover, the past form is
2024-07-19 02:30:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 02:34:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2876,  0.6621, -0.3884,  ...,  0.4373, -0.3101, -0.4690],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6523,  0.3857, -0.0293,  ..., -2.6875, -4.5781,  0.9102],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0006, -0.0150, -0.0018,  ...,  0.0266, -0.0200, -0.0022],
        [-0.0132, -0.0171, -0.0049,  ...,  0.0180,  0.0341,  0.0114],
        [ 0.0075,  0.0033, -0.0456,  ..., -0.0106,  0.0045, -0.0088],
        ...,
        [-0.0193,  0.0069, -0.0080,  ..., -0.0157,  0.0122,  0.0090],
        [ 0.0143,  0.0223,  0.0064,  ...,  0.0088, -0.0413, -0.0087],
        [-0.0276, -0.0053, -0.0118,  ..., -0.0144,  0.0223, -0.0291]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6255, -0.4717,  0.2842,  ..., -2.5879, -4.5586,  1.6680]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 02:34:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is remain, the past form is remained
If the present form is enjoy, the past form is enjoyed
If the present form is reduce, the past form is reduced
If the present form is send, the past form is sent
If the present form is perform, the past form is performed
If the present form is relate, the past form is related
If the present form is create, the past form is created
If the present form is discover, the past form is
2024-07-19 02:34:30 root INFO     total operator prediction time: 2069.769946575165 seconds
2024-07-19 02:34:30 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_inf - 3pSg
2024-07-19 02:34:30 root INFO     building operator verb_inf - 3pSg
2024-07-19 02:34:31 root INFO     [order_1_approx] starting weight calculation for I suggest, he suggests
I exist, he exists
I agree, he agrees
I operate, he operates
I occur, he occurs
I avoid, he avoids
I hear, he hears
I describe, he
2024-07-19 02:34:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 02:38:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2393, -0.1940,  0.3596,  ..., -0.6948, -0.2468, -0.1365],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.4795, -3.2148, -1.1162,  ...,  2.3926, -0.2139,  2.1113],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0215, -0.0170,  0.0346,  ...,  0.0174, -0.0024, -0.0206],
        [ 0.0087, -0.0235, -0.0219,  ...,  0.0026, -0.0054, -0.0206],
        [-0.0029, -0.0142, -0.0468,  ...,  0.0096,  0.0095, -0.0208],
        ...,
        [-0.0025, -0.0169,  0.0172,  ..., -0.0170, -0.0430,  0.0319],
        [ 0.0208,  0.0300,  0.0392,  ..., -0.0003, -0.0110,  0.0254],
        [-0.0270,  0.0055,  0.0106,  ..., -0.0066,  0.0164,  0.0065]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.9727, -3.6660, -2.1641,  ...,  3.2383,  1.1455,  1.8828]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 02:38:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I suggest, he suggests
I exist, he exists
I agree, he agrees
I operate, he operates
I occur, he occurs
I avoid, he avoids
I hear, he hears
I describe, he
2024-07-19 02:38:51 root INFO     [order_1_approx] starting weight calculation for I hear, he hears
I suggest, he suggests
I agree, he agrees
I operate, he operates
I describe, he describes
I avoid, he avoids
I occur, he occurs
I exist, he
2024-07-19 02:38:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 02:43:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4248,  0.1250,  1.1602,  ..., -0.2993,  0.4661,  0.1088],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5693, -2.4004,  1.9199,  ..., -0.5898, -1.9775,  3.6621],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0386, -0.0202,  0.0345,  ...,  0.0073, -0.0122, -0.0035],
        [ 0.0052, -0.0328, -0.0133,  ...,  0.0004,  0.0272, -0.0010],
        [ 0.0061,  0.0148, -0.0411,  ...,  0.0019, -0.0109, -0.0101],
        ...,
        [ 0.0003, -0.0081, -0.0104,  ..., -0.0208,  0.0145,  0.0062],
        [-0.0010,  0.0063,  0.0426,  ..., -0.0289, -0.0419,  0.0129],
        [ 0.0134,  0.0293, -0.0148,  ..., -0.0159,  0.0243, -0.0407]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3813, -1.7842,  2.0430,  ..., -1.0693, -1.8047,  4.3438]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 02:43:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I hear, he hears
I suggest, he suggests
I agree, he agrees
I operate, he operates
I describe, he describes
I avoid, he avoids
I occur, he occurs
I exist, he
2024-07-19 02:43:12 root INFO     [order_1_approx] starting weight calculation for I agree, he agrees
I avoid, he avoids
I exist, he exists
I operate, he operates
I suggest, he suggests
I occur, he occurs
I describe, he describes
I hear, he
2024-07-19 02:43:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 02:47:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0967, -0.0798,  0.8447,  ..., -0.0847,  0.5117,  0.2922],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5605,  0.5195, -0.4805,  ...,  0.3118,  1.0371,  5.3203],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0162, -0.0068,  0.0192,  ...,  0.0080,  0.0054, -0.0061],
        [ 0.0005, -0.0465,  0.0430,  ..., -0.0252,  0.0141,  0.0134],
        [-0.0252,  0.0015, -0.0460,  ...,  0.0351,  0.0072, -0.0131],
        ...,
        [ 0.0120, -0.0153, -0.0237,  ..., -0.0141,  0.0096, -0.0031],
        [ 0.0020,  0.0143,  0.0066,  ..., -0.0258, -0.0476, -0.0120],
        [-0.0191, -0.0056, -0.0015,  ..., -0.0278,  0.0150, -0.0209]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6675,  0.8623, -1.3262,  ..., -0.0325,  0.8926,  5.5898]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 02:47:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I agree, he agrees
I avoid, he avoids
I exist, he exists
I operate, he operates
I suggest, he suggests
I occur, he occurs
I describe, he describes
I hear, he
2024-07-19 02:47:32 root INFO     [order_1_approx] starting weight calculation for I avoid, he avoids
I operate, he operates
I suggest, he suggests
I agree, he agrees
I hear, he hears
I describe, he describes
I exist, he exists
I occur, he
2024-07-19 02:47:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 02:51:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7676,  0.0127,  1.4697,  ..., -0.2065,  0.7490, -0.4160],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3008, -2.9648, -2.0312,  ..., -1.0127, -1.2129,  3.1758],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.2152e-03, -3.2135e-02,  1.5060e-02,  ...,  1.6754e-02,
         -2.8210e-03, -2.3392e-02],
        [ 1.8906e-02, -1.6693e-02,  1.6724e-02,  ...,  5.0659e-03,
          8.3771e-03,  1.3947e-02],
        [-2.1088e-02,  2.0142e-02, -5.2429e-02,  ..., -3.6888e-03,
          3.4515e-02, -6.9199e-03],
        ...,
        [ 3.1815e-03, -3.0670e-02, -1.5564e-02,  ..., -2.6611e-02,
         -2.3849e-02,  2.2354e-03],
        [ 1.5869e-02,  2.2827e-02, -7.3090e-03,  ..., -4.0779e-03,
         -3.2654e-02,  2.2903e-02],
        [-3.6377e-02, -1.9073e-06,  4.6921e-03,  ..., -2.2720e-02,
          1.5274e-02, -3.0136e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2451, -2.2539, -3.0449,  ..., -1.4404, -0.8584,  2.2324]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 02:51:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I avoid, he avoids
I operate, he operates
I suggest, he suggests
I agree, he agrees
I hear, he hears
I describe, he describes
I exist, he exists
I occur, he
2024-07-19 02:51:52 root INFO     [order_1_approx] starting weight calculation for I describe, he describes
I exist, he exists
I occur, he occurs
I suggest, he suggests
I hear, he hears
I avoid, he avoids
I agree, he agrees
I operate, he
2024-07-19 02:51:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 02:56:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5381, -0.1621,  0.6396,  ..., -0.3589, -0.0332,  0.8379],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3875, -4.0195, -0.4727,  ...,  2.0430, -1.9375,  2.1055],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0065, -0.0046,  0.0154,  ...,  0.0062, -0.0257, -0.0124],
        [-0.0006, -0.0300, -0.0043,  ..., -0.0116,  0.0084, -0.0106],
        [-0.0190, -0.0196, -0.0352,  ...,  0.0097,  0.0159,  0.0062],
        ...,
        [-0.0019, -0.0071, -0.0154,  ..., -0.0340, -0.0197,  0.0055],
        [-0.0081,  0.0163,  0.0120,  ..., -0.0230, -0.0331, -0.0014],
        [-0.0116,  0.0179, -0.0124,  ..., -0.0130,  0.0159, -0.0256]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6191, -3.8828, -0.8711,  ...,  1.2812, -1.4609,  2.1426]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 02:56:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I describe, he describes
I exist, he exists
I occur, he occurs
I suggest, he suggests
I hear, he hears
I avoid, he avoids
I agree, he agrees
I operate, he
2024-07-19 02:56:12 root INFO     [order_1_approx] starting weight calculation for I occur, he occurs
I operate, he operates
I hear, he hears
I describe, he describes
I agree, he agrees
I exist, he exists
I suggest, he suggests
I avoid, he
2024-07-19 02:56:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 03:00:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0596, -0.3232,  0.1321,  ..., -1.0469, -0.6924,  0.2683],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4902, -0.7085,  0.2627,  ...,  1.3477,  0.1494,  4.5078],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0095,  0.0182,  0.0041,  ...,  0.0058, -0.0093, -0.0042],
        [-0.0047, -0.0176,  0.0135,  ...,  0.0138,  0.0048,  0.0163],
        [-0.0085, -0.0022, -0.0281,  ...,  0.0301,  0.0099,  0.0070],
        ...,
        [-0.0135,  0.0041,  0.0012,  ..., -0.0267, -0.0133,  0.0231],
        [ 0.0079,  0.0121, -0.0155,  ..., -0.0155, -0.0580,  0.0009],
        [-0.0019,  0.0093,  0.0106,  ..., -0.0181,  0.0050, -0.0458]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9775,  0.0200,  0.7236,  ...,  1.4893, -0.1621,  4.7031]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 03:00:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I occur, he occurs
I operate, he operates
I hear, he hears
I describe, he describes
I agree, he agrees
I exist, he exists
I suggest, he suggests
I avoid, he
2024-07-19 03:00:32 root INFO     [order_1_approx] starting weight calculation for I occur, he occurs
I avoid, he avoids
I exist, he exists
I hear, he hears
I describe, he describes
I operate, he operates
I agree, he agrees
I suggest, he
2024-07-19 03:00:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 03:04:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1219, -0.1292,  0.0676,  ...,  0.3533, -0.3320,  0.1562],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.0391, -2.1035,  5.5195,  ...,  2.8789, -5.1484,  3.7070],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0109, -0.0060,  0.0081,  ..., -0.0021, -0.0038, -0.0101],
        [-0.0005, -0.0287, -0.0017,  ...,  0.0057,  0.0130, -0.0014],
        [ 0.0039, -0.0084, -0.0432,  ...,  0.0224,  0.0179,  0.0070],
        ...,
        [-0.0133, -0.0371,  0.0119,  ..., -0.0353, -0.0109,  0.0002],
        [ 0.0158, -0.0080,  0.0162,  ..., -0.0029, -0.0629, -0.0047],
        [ 0.0155,  0.0126, -0.0036,  ..., -0.0108,  0.0071, -0.0127]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.2051, -2.2559,  4.9961,  ...,  3.3438, -4.3203,  4.1875]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 03:04:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I occur, he occurs
I avoid, he avoids
I exist, he exists
I hear, he hears
I describe, he describes
I operate, he operates
I agree, he agrees
I suggest, he
2024-07-19 03:04:51 root INFO     [order_1_approx] starting weight calculation for I hear, he hears
I describe, he describes
I occur, he occurs
I avoid, he avoids
I operate, he operates
I suggest, he suggests
I exist, he exists
I agree, he
2024-07-19 03:04:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 03:09:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7959, -0.0029,  0.7358,  ..., -0.3081, -0.4260,  0.0952],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2500, -1.2188,  3.1289,  ...,  0.2168, -2.3418,  0.5889],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0527, -0.0035,  0.0328,  ...,  0.0116, -0.0209, -0.0116],
        [ 0.0236, -0.0385,  0.0120,  ..., -0.0016,  0.0063,  0.0048],
        [-0.0116,  0.0007, -0.0406,  ...,  0.0156,  0.0041, -0.0014],
        ...,
        [-0.0032, -0.0050, -0.0031,  ..., -0.0289, -0.0104,  0.0260],
        [ 0.0069,  0.0014,  0.0316,  ..., -0.0339, -0.0392,  0.0277],
        [ 0.0082,  0.0154, -0.0087,  ..., -0.0186,  0.0204, -0.0523]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5293, -0.1104,  2.5547,  ...,  0.1952, -1.5742,  2.3320]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 03:09:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for I hear, he hears
I describe, he describes
I occur, he occurs
I avoid, he avoids
I operate, he operates
I suggest, he suggests
I exist, he exists
I agree, he
2024-07-19 03:09:12 root INFO     total operator prediction time: 2081.582508325577 seconds
2024-07-19 03:09:12 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_Ving - 3pSg
2024-07-19 03:09:12 root INFO     building operator verb_Ving - 3pSg
2024-07-19 03:09:12 root INFO     [order_1_approx] starting weight calculation for When something is requiring, it requires
When something is believing, it believes
When something is following, it follows
When something is continuing, it continues
When something is involving, it involves
When something is operating, it operates
When something is creating, it creates
When something is losing, it
2024-07-19 03:09:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 03:13:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0295, -0.2751,  0.4126,  ..., -0.1829, -0.4800,  0.5459],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.0859, -0.4585,  0.7549,  ..., -0.5347, -0.0316,  3.3887],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0305, -0.0166, -0.0045,  ...,  0.0279,  0.0001, -0.0473],
        [ 0.0115, -0.0031, -0.0084,  ...,  0.0076,  0.0007,  0.0081],
        [-0.0322, -0.0209, -0.0099,  ...,  0.0232,  0.0070,  0.0046],
        ...,
        [-0.0058,  0.0097, -0.0159,  ..., -0.0321,  0.0374, -0.0101],
        [ 0.0025, -0.0112,  0.0018,  ..., -0.0183, -0.0282,  0.0041],
        [-0.0169,  0.0162,  0.0086,  ...,  0.0098,  0.0301, -0.0157]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.3711,  0.1318, -0.4277,  ..., -1.2246, -1.2686,  5.0391]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 03:13:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is requiring, it requires
When something is believing, it believes
When something is following, it follows
When something is continuing, it continues
When something is involving, it involves
When something is operating, it operates
When something is creating, it creates
When something is losing, it
2024-07-19 03:13:32 root INFO     [order_1_approx] starting weight calculation for When something is operating, it operates
When something is following, it follows
When something is requiring, it requires
When something is believing, it believes
When something is creating, it creates
When something is continuing, it continues
When something is losing, it loses
When something is involving, it
2024-07-19 03:13:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 03:17:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.3911, 0.1652, 0.0543,  ..., 0.0151, 0.2136, 0.7998], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([3.1562, 0.1421, 1.4092,  ..., 2.3320, 1.3105, 1.4453], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0187, -0.0108,  0.0224,  ...,  0.0159, -0.0134, -0.0248],
        [-0.0077, -0.0106, -0.0023,  ..., -0.0070,  0.0069, -0.0047],
        [ 0.0256, -0.0003, -0.0147,  ...,  0.0112, -0.0140,  0.0197],
        ...,
        [-0.0267,  0.0014, -0.0317,  ..., -0.0031, -0.0152,  0.0093],
        [-0.0041,  0.0182,  0.0397,  ...,  0.0085, -0.0359, -0.0027],
        [-0.0048,  0.0099, -0.0120,  ...,  0.0113,  0.0217, -0.0258]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[2.2949, 0.4204, 2.0449,  ..., 2.9395, 1.9414, 2.2227]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 03:17:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is operating, it operates
When something is following, it follows
When something is requiring, it requires
When something is believing, it believes
When something is creating, it creates
When something is continuing, it continues
When something is losing, it loses
When something is involving, it
2024-07-19 03:17:52 root INFO     [order_1_approx] starting weight calculation for When something is creating, it creates
When something is continuing, it continues
When something is believing, it believes
When something is involving, it involves
When something is requiring, it requires
When something is losing, it loses
When something is operating, it operates
When something is following, it
2024-07-19 03:17:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 03:22:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1487,  0.2181,  1.1934,  ..., -0.5146, -0.2130,  0.9092],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7305, -1.0723,  1.0840,  ..., -0.3853,  0.4849,  4.6875],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0244, -0.0274,  0.0141,  ...,  0.0156, -0.0074, -0.0195],
        [ 0.0052, -0.0215,  0.0185,  ..., -0.0037, -0.0118, -0.0126],
        [-0.0035, -0.0026,  0.0126,  ...,  0.0097, -0.0056,  0.0096],
        ...,
        [-0.0042, -0.0076, -0.0113,  ..., -0.0228, -0.0030, -0.0072],
        [ 0.0072,  0.0093,  0.0150,  ...,  0.0010, -0.0247,  0.0080],
        [-0.0068,  0.0013,  0.0000,  ...,  0.0029,  0.0151, -0.0420]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.8477, -1.7734,  0.3296,  ..., -0.9268,  1.2969,  5.5312]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 03:22:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is creating, it creates
When something is continuing, it continues
When something is believing, it believes
When something is involving, it involves
When something is requiring, it requires
When something is losing, it loses
When something is operating, it operates
When something is following, it
2024-07-19 03:22:12 root INFO     [order_1_approx] starting weight calculation for When something is continuing, it continues
When something is involving, it involves
When something is following, it follows
When something is losing, it loses
When something is requiring, it requires
When something is believing, it believes
When something is operating, it operates
When something is creating, it
2024-07-19 03:22:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 03:26:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4729,  0.1108, -0.0869,  ...,  0.1743,  0.2478, -0.5830],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1533, -0.9600,  1.0205,  ...,  1.4883, -0.5615,  0.1172],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0206, -0.0165,  0.0177,  ...,  0.0192, -0.0226, -0.0224],
        [-0.0040, -0.0201,  0.0122,  ...,  0.0040, -0.0113, -0.0104],
        [-0.0172, -0.0050, -0.0089,  ...,  0.0124,  0.0052,  0.0112],
        ...,
        [-0.0058,  0.0032, -0.0109,  ..., -0.0248, -0.0207,  0.0019],
        [-0.0188,  0.0164,  0.0033,  ..., -0.0320, -0.0504,  0.0088],
        [ 0.0065, -0.0173,  0.0135,  ...,  0.0164,  0.0164, -0.0294]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6172, -1.1836,  0.2622,  ...,  1.7207, -0.8506,  1.1094]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 03:26:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is continuing, it continues
When something is involving, it involves
When something is following, it follows
When something is losing, it loses
When something is requiring, it requires
When something is believing, it believes
When something is operating, it operates
When something is creating, it
2024-07-19 03:26:31 root INFO     [order_1_approx] starting weight calculation for When something is creating, it creates
When something is involving, it involves
When something is losing, it loses
When something is continuing, it continues
When something is believing, it believes
When something is following, it follows
When something is requiring, it requires
When something is operating, it
2024-07-19 03:26:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 03:30:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4375,  0.1829,  0.5454,  ..., -0.5581,  0.2502,  0.4185],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2031, -1.0088,  1.2803,  ...,  0.6743,  0.7734,  1.8008],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.5579e-02, -3.0022e-03, -9.8648e-03,  ...,  1.7120e-02,
         -1.7242e-02, -3.2425e-03],
        [-2.2888e-02, -9.9106e-03,  1.9012e-02,  ..., -4.3488e-04,
          2.7790e-03, -2.5425e-03],
        [ 1.5182e-03, -6.2180e-04, -6.5689e-03,  ...,  9.0179e-03,
          4.7531e-03,  1.7181e-02],
        ...,
        [-1.3748e-02,  4.1199e-04, -3.3569e-02,  ..., -3.1708e-02,
         -2.2469e-03, -7.2479e-05],
        [ 1.3397e-02, -5.7907e-03,  7.1106e-03,  ..., -4.6806e-03,
         -2.3132e-02,  1.1702e-03],
        [ 9.8190e-03, -1.0185e-03, -4.3602e-03,  ...,  2.1172e-04,
          4.3983e-03, -2.3880e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9053, -1.1865,  1.9424,  ..., -0.0132, -0.7236,  2.1094]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 03:30:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is creating, it creates
When something is involving, it involves
When something is losing, it loses
When something is continuing, it continues
When something is believing, it believes
When something is following, it follows
When something is requiring, it requires
When something is operating, it
2024-07-19 03:30:49 root INFO     [order_1_approx] starting weight calculation for When something is involving, it involves
When something is believing, it believes
When something is losing, it loses
When something is creating, it creates
When something is operating, it operates
When something is requiring, it requires
When something is following, it follows
When something is continuing, it
2024-07-19 03:30:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 03:35:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1989,  0.3452,  0.4543,  ..., -0.2700,  0.7676,  0.2480],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1130, -0.5796, -0.5605,  ...,  0.8760, -0.7305,  2.7930],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0376, -0.0093,  0.0194,  ...,  0.0018, -0.0306, -0.0317],
        [-0.0169, -0.0161, -0.0091,  ..., -0.0156,  0.0061, -0.0113],
        [-0.0108,  0.0066, -0.0249,  ...,  0.0069,  0.0116,  0.0210],
        ...,
        [-0.0089, -0.0205, -0.0353,  ..., -0.0334,  0.0036, -0.0222],
        [-0.0015,  0.0193,  0.0032,  ..., -0.0135, -0.0377,  0.0095],
        [ 0.0140, -0.0071,  0.0117,  ...,  0.0190,  0.0190, -0.0333]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9062, -1.5781, -1.0215,  ...,  1.6758, -0.7734,  3.5312]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 03:35:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is involving, it involves
When something is believing, it believes
When something is losing, it loses
When something is creating, it creates
When something is operating, it operates
When something is requiring, it requires
When something is following, it follows
When something is continuing, it
2024-07-19 03:35:09 root INFO     [order_1_approx] starting weight calculation for When something is following, it follows
When something is involving, it involves
When something is operating, it operates
When something is creating, it creates
When something is losing, it loses
When something is continuing, it continues
When something is believing, it believes
When something is requiring, it
2024-07-19 03:35:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 03:39:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6802,  0.3462,  0.0144,  ..., -0.4543,  0.3159,  0.4868],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.4434,  0.0493,  0.2432,  ...,  1.1172, -1.5781,  4.2734],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-3.2471e-02, -1.3748e-02,  3.2776e-02,  ..., -1.4656e-02,
         -2.5162e-02, -2.0782e-02],
        [-1.3447e-04, -1.8005e-03, -2.4597e-02,  ..., -1.6241e-03,
         -1.4496e-02, -2.5391e-02],
        [ 2.4261e-02,  2.3056e-02, -2.8107e-02,  ...,  1.0933e-02,
         -1.6052e-02, -6.6414e-03],
        ...,
        [-2.7496e-02, -2.0721e-02, -1.0353e-02,  ..., -1.0300e-02,
          8.5144e-03,  7.2517e-03],
        [-6.1264e-03,  6.1035e-05,  5.5054e-02,  ..., -1.3329e-02,
          8.0872e-04,  1.0147e-02],
        [-1.3161e-02,  6.8436e-03,  1.2978e-02,  ...,  2.5131e-02,
         -1.1444e-03, -3.1372e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.7490,  0.3401, -0.0347,  ...,  0.8877, -2.1094,  4.8125]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 03:39:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is following, it follows
When something is involving, it involves
When something is operating, it operates
When something is creating, it creates
When something is losing, it loses
When something is continuing, it continues
When something is believing, it believes
When something is requiring, it
2024-07-19 03:39:29 root INFO     [order_1_approx] starting weight calculation for When something is creating, it creates
When something is following, it follows
When something is involving, it involves
When something is losing, it loses
When something is requiring, it requires
When something is operating, it operates
When something is continuing, it continues
When something is believing, it
2024-07-19 03:39:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 03:43:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3892,  0.6494, -0.1196,  ...,  0.3542,  0.1743,  0.1575],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5166,  1.4053,  1.2266,  ...,  1.3516, -1.5684,  3.5234],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0398, -0.0346,  0.0208,  ...,  0.0174,  0.0002, -0.0260],
        [ 0.0181, -0.0023,  0.0172,  ..., -0.0202, -0.0113, -0.0042],
        [ 0.0086,  0.0061,  0.0024,  ...,  0.0311, -0.0115,  0.0120],
        ...,
        [ 0.0003, -0.0158, -0.0160,  ..., -0.0239,  0.0181, -0.0115],
        [-0.0349, -0.0117,  0.0207,  ..., -0.0201, -0.0357,  0.0136],
        [-0.0016,  0.0184,  0.0022,  ...,  0.0087, -0.0067, -0.0334]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8027,  1.6455,  1.7812,  ...,  2.0449, -2.4707,  4.1523]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 03:43:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When something is creating, it creates
When something is following, it follows
When something is involving, it involves
When something is losing, it loses
When something is requiring, it requires
When something is operating, it operates
When something is continuing, it continues
When something is believing, it
2024-07-19 03:43:48 root INFO     total operator prediction time: 2075.966250181198 seconds
2024-07-19 03:43:48 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on noun - plural_reg
2024-07-19 03:43:48 root INFO     building operator noun - plural_reg
2024-07-19 03:43:48 root INFO     [order_1_approx] starting weight calculation for The plural form of period is periods
The plural form of college is colleges
The plural form of area is areas
The plural form of role is roles
The plural form of song is songs
The plural form of office is offices
The plural form of application is applications
The plural form of system is
2024-07-19 03:43:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 03:48:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4854, -0.1838, -0.4880,  ..., -0.1915, -0.1425, -0.3452],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.7695, -0.3459,  5.0469,  ...,  1.2998, -3.3809,  3.3574],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0183, -0.0009,  0.0039,  ..., -0.0097,  0.0060, -0.0313],
        [-0.0246, -0.0089,  0.0049,  ...,  0.0074, -0.0241,  0.0240],
        [ 0.0088, -0.0074, -0.0132,  ..., -0.0026, -0.0016, -0.0027],
        ...,
        [-0.0196, -0.0158,  0.0014,  ..., -0.0136,  0.0136,  0.0180],
        [ 0.0183, -0.0025,  0.0030,  ..., -0.0147, -0.0109,  0.0118],
        [-0.0102,  0.0005,  0.0057,  ..., -0.0156,  0.0076, -0.0079]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.7852, -0.3481,  4.2812,  ...,  1.2510, -3.2637,  3.5352]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 03:48:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of period is periods
The plural form of college is colleges
The plural form of area is areas
The plural form of role is roles
The plural form of song is songs
The plural form of office is offices
The plural form of application is applications
The plural form of system is
2024-07-19 03:48:05 root INFO     [order_1_approx] starting weight calculation for The plural form of college is colleges
The plural form of role is roles
The plural form of application is applications
The plural form of song is songs
The plural form of period is periods
The plural form of office is offices
The plural form of system is systems
The plural form of area is
2024-07-19 03:48:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 03:52:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5020,  0.3828, -0.5308,  ..., -0.5967, -0.2812, -0.2998],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.8770, -0.8525,  2.3438,  ..., -2.6562,  3.6953,  3.0723],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0113, -0.0050,  0.0048,  ...,  0.0077, -0.0079, -0.0125],
        [-0.0153,  0.0012,  0.0189,  ..., -0.0021, -0.0015, -0.0048],
        [ 0.0070,  0.0126, -0.0107,  ..., -0.0004, -0.0022,  0.0021],
        ...,
        [ 0.0034, -0.0019, -0.0094,  ...,  0.0013,  0.0192,  0.0059],
        [ 0.0070,  0.0077, -0.0072,  ..., -0.0122, -0.0105,  0.0056],
        [-0.0043, -0.0271,  0.0206,  ...,  0.0006,  0.0065, -0.0198]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2832, -3.0820,  2.3496,  ..., -2.5371,  4.1445,  2.6855]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 03:52:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of college is colleges
The plural form of role is roles
The plural form of application is applications
The plural form of song is songs
The plural form of period is periods
The plural form of office is offices
The plural form of system is systems
The plural form of area is
2024-07-19 03:52:24 root INFO     [order_1_approx] starting weight calculation for The plural form of area is areas
The plural form of system is systems
The plural form of song is songs
The plural form of office is offices
The plural form of period is periods
The plural form of college is colleges
The plural form of role is roles
The plural form of application is
2024-07-19 03:52:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 03:56:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0979, -0.2544,  0.3892,  ..., -0.6187, -0.1001, -0.3179],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1787,  0.7700,  2.6992,  ...,  0.2588,  0.9893,  4.0078],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0107, -0.0053,  0.0303,  ...,  0.0074,  0.0048, -0.0166],
        [-0.0068,  0.0045, -0.0065,  ...,  0.0012,  0.0084,  0.0093],
        [ 0.0229,  0.0237, -0.0156,  ...,  0.0047,  0.0186,  0.0101],
        ...,
        [-0.0072, -0.0182,  0.0029,  ..., -0.0036,  0.0068,  0.0031],
        [ 0.0186, -0.0111,  0.0215,  ..., -0.0295, -0.0192, -0.0080],
        [ 0.0228,  0.0165, -0.0151,  ..., -0.0093, -0.0076, -0.0377]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.4902,  0.0972,  2.5840,  ..., -0.2856,  0.6548,  3.5977]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 03:56:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of area is areas
The plural form of system is systems
The plural form of song is songs
The plural form of office is offices
The plural form of period is periods
The plural form of college is colleges
The plural form of role is roles
The plural form of application is
2024-07-19 03:56:44 root INFO     [order_1_approx] starting weight calculation for The plural form of period is periods
The plural form of office is offices
The plural form of role is roles
The plural form of application is applications
The plural form of system is systems
The plural form of area is areas
The plural form of college is colleges
The plural form of song is
2024-07-19 03:56:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 04:01:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5854,  0.2437, -0.3538,  ...,  0.1967,  0.4639, -1.1699],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5327, -0.9746,  0.5469,  ...,  3.0156, -2.2070, -1.4756],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0186, -0.0032, -0.0018,  ..., -0.0047, -0.0022,  0.0015],
        [-0.0098,  0.0027,  0.0043,  ...,  0.0089,  0.0068,  0.0175],
        [ 0.0073,  0.0068, -0.0191,  ..., -0.0008,  0.0109,  0.0081],
        ...,
        [-0.0166, -0.0096,  0.0097,  ..., -0.0199,  0.0081,  0.0064],
        [ 0.0139,  0.0101,  0.0230,  ..., -0.0041, -0.0302,  0.0109],
        [-0.0023, -0.0145, -0.0154,  ..., -0.0075, -0.0110, -0.0290]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.3965,  0.2979, -0.2959,  ...,  2.8711, -0.7920, -3.0723]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 04:01:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of period is periods
The plural form of office is offices
The plural form of role is roles
The plural form of application is applications
The plural form of system is systems
The plural form of area is areas
The plural form of college is colleges
The plural form of song is
2024-07-19 04:01:04 root INFO     [order_1_approx] starting weight calculation for The plural form of college is colleges
The plural form of area is areas
The plural form of song is songs
The plural form of period is periods
The plural form of application is applications
The plural form of role is roles
The plural form of system is systems
The plural form of office is
2024-07-19 04:01:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 04:05:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2428,  0.0486, -0.2104,  ...,  0.1262, -0.6055, -0.0017],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6523,  1.0410, -2.0176,  ...,  0.2695,  0.5811,  1.6533],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0019, -0.0192,  0.0155,  ...,  0.0118,  0.0090, -0.0257],
        [-0.0252, -0.0007, -0.0193,  ...,  0.0039, -0.0006, -0.0047],
        [ 0.0065, -0.0083, -0.0079,  ...,  0.0068,  0.0097,  0.0077],
        ...,
        [-0.0181, -0.0232,  0.0025,  ...,  0.0044, -0.0142, -0.0010],
        [ 0.0121,  0.0241, -0.0104,  ..., -0.0194, -0.0071, -0.0019],
        [ 0.0062, -0.0346,  0.0077,  ...,  0.0089,  0.0026, -0.0221]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7964,  0.5547, -2.4727,  ...,  0.0768,  0.5298,  0.0879]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 04:05:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of college is colleges
The plural form of area is areas
The plural form of song is songs
The plural form of period is periods
The plural form of application is applications
The plural form of role is roles
The plural form of system is systems
The plural form of office is
2024-07-19 04:05:25 root INFO     [order_1_approx] starting weight calculation for The plural form of period is periods
The plural form of area is areas
The plural form of application is applications
The plural form of office is offices
The plural form of role is roles
The plural form of song is songs
The plural form of system is systems
The plural form of college is
2024-07-19 04:05:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 04:09:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2031,  0.0950, -0.0613,  ...,  0.2793,  0.5732, -0.1243],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.7734, -1.0293, -1.0771,  ..., -2.7520,  1.1123,  5.1289],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0120,  0.0032,  0.0347,  ..., -0.0177,  0.0053, -0.0114],
        [-0.0068, -0.0035, -0.0020,  ...,  0.0102,  0.0015,  0.0019],
        [ 0.0179,  0.0060, -0.0304,  ...,  0.0090,  0.0093,  0.0035],
        ...,
        [-0.0185, -0.0117, -0.0097,  ..., -0.0191, -0.0149,  0.0130],
        [ 0.0154, -0.0107,  0.0049,  ..., -0.0233, -0.0354,  0.0084],
        [-0.0226, -0.0197, -0.0026,  ..., -0.0070,  0.0245, -0.0526]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.6758,  0.0420, -0.0752,  ..., -1.2832,  1.6133,  4.2305]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 04:09:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of period is periods
The plural form of area is areas
The plural form of application is applications
The plural form of office is offices
The plural form of role is roles
The plural form of song is songs
The plural form of system is systems
The plural form of college is
2024-07-19 04:09:45 root INFO     [order_1_approx] starting weight calculation for The plural form of college is colleges
The plural form of application is applications
The plural form of role is roles
The plural form of area is areas
The plural form of system is systems
The plural form of song is songs
The plural form of office is offices
The plural form of period is
2024-07-19 04:09:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 04:14:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5708,  0.2087, -0.3345,  ..., -0.9683, -0.7603, -0.6113],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0508, -0.4365, -1.8506,  ...,  3.0938,  1.7246,  2.3887],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0152, -0.0062,  0.0289,  ...,  0.0051,  0.0175, -0.0303],
        [-0.0063, -0.0041, -0.0097,  ...,  0.0117, -0.0141,  0.0026],
        [-0.0030,  0.0105, -0.0218,  ..., -0.0195, -0.0006, -0.0100],
        ...,
        [ 0.0112, -0.0084,  0.0077,  ...,  0.0159,  0.0215,  0.0206],
        [ 0.0144,  0.0070, -0.0153,  ..., -0.0025, -0.0210, -0.0086],
        [ 0.0065, -0.0079,  0.0089,  ...,  0.0097,  0.0175,  0.0003]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6748, -0.3330, -2.7852,  ...,  1.9590,  2.0039,  1.8457]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 04:14:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of college is colleges
The plural form of application is applications
The plural form of role is roles
The plural form of area is areas
The plural form of system is systems
The plural form of song is songs
The plural form of office is offices
The plural form of period is
2024-07-19 04:14:05 root INFO     [order_1_approx] starting weight calculation for The plural form of application is applications
The plural form of song is songs
The plural form of college is colleges
The plural form of period is periods
The plural form of system is systems
The plural form of area is areas
The plural form of office is offices
The plural form of role is
2024-07-19 04:14:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 04:18:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1266,  0.1123,  0.5728,  ...,  0.1862, -0.4814, -0.2827],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.7227, -1.2949,  2.7969,  ...,  2.7363,  1.9482,  3.2773],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0017, -0.0035,  0.0195,  ...,  0.0082, -0.0088, -0.0387],
        [-0.0073,  0.0052, -0.0013,  ...,  0.0047,  0.0191,  0.0074],
        [ 0.0066,  0.0069, -0.0122,  ..., -0.0003,  0.0068, -0.0058],
        ...,
        [-0.0101,  0.0119, -0.0132,  ..., -0.0191,  0.0279, -0.0068],
        [-0.0123,  0.0272,  0.0106,  ..., -0.0345, -0.0229,  0.0118],
        [ 0.0042,  0.0120, -0.0084,  ...,  0.0038,  0.0091, -0.0103]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.8477, -1.8965,  2.9277,  ...,  1.4863,  1.9131,  2.8320]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 04:18:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of application is applications
The plural form of song is songs
The plural form of college is colleges
The plural form of period is periods
The plural form of system is systems
The plural form of area is areas
The plural form of office is offices
The plural form of role is
2024-07-19 04:18:26 root INFO     total operator prediction time: 2077.6916887760162 seconds
2024-07-19 04:18:26 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb_3pSg - Ved
2024-07-19 04:18:26 root INFO     building operator verb_3pSg - Ved
2024-07-19 04:18:26 root INFO     [order_1_approx] starting weight calculation for When he receives something, something has been received
When he develops something, something has been developed
When he locates something, something has been located
When he appears something, something has been appeared
When he applies something, something has been applied
When he replaces something, something has been replaced
When he establishes something, something has been established
When he involves something, something has been
2024-07-19 04:18:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 04:22:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0017,  0.3635,  0.1200,  ..., -0.3110,  0.0026,  0.7725],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.5703,  1.8008, -1.4336,  ...,  2.0742,  1.1709,  1.1309],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0199, -0.0089,  0.0092,  ...,  0.0441, -0.0307, -0.0106],
        [-0.0064, -0.0112, -0.0005,  ..., -0.0030,  0.0109, -0.0086],
        [ 0.0422,  0.0023, -0.0309,  ..., -0.0228,  0.0123, -0.0106],
        ...,
        [-0.0083, -0.0126, -0.0159,  ..., -0.0059,  0.0047,  0.0187],
        [ 0.0197,  0.0176,  0.0127,  ..., -0.0055, -0.0108, -0.0064],
        [-0.0052, -0.0178,  0.0351,  ..., -0.0088,  0.0114, -0.0425]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5156,  1.9785, -0.7114,  ...,  2.7480,  1.5234,  1.4414]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 04:22:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he receives something, something has been received
When he develops something, something has been developed
When he locates something, something has been located
When he appears something, something has been appeared
When he applies something, something has been applied
When he replaces something, something has been replaced
When he establishes something, something has been established
When he involves something, something has been
2024-07-19 04:22:47 root INFO     [order_1_approx] starting weight calculation for When he develops something, something has been developed
When he locates something, something has been located
When he involves something, something has been involved
When he establishes something, something has been established
When he appears something, something has been appeared
When he replaces something, something has been replaced
When he receives something, something has been received
When he applies something, something has been
2024-07-19 04:22:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 04:27:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5273,  0.2764,  0.5454,  ..., -0.4321,  0.0167,  0.4824],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.7520,  1.4521,  0.8779,  ...,  1.9395,  0.1670,  0.4629],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-8.0795e-03,  1.2009e-02, -1.0147e-03,  ...,  1.8143e-02,
         -2.4338e-02,  5.4398e-03],
        [ 7.8583e-03,  1.0849e-02, -5.7793e-03,  ..., -1.1978e-03,
         -4.4632e-04,  1.0017e-02],
        [ 1.4099e-02,  2.5146e-02, -1.5457e-02,  ...,  1.1238e-02,
          6.3944e-04, -1.0994e-02],
        ...,
        [-1.9073e-04,  4.2191e-03,  1.0323e-02,  ..., -7.6294e-05,
          6.9466e-03, -5.2185e-03],
        [ 6.2180e-03,  3.7651e-03,  1.7044e-02,  ..., -1.1436e-02,
         -2.7359e-02,  2.8992e-04],
        [-1.8730e-03,  7.0686e-03,  6.7253e-03,  ...,  1.7273e-02,
          5.2147e-03, -2.3041e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.0703,  1.4512,  2.0195,  ...,  1.7734,  0.0094,  0.6323]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 04:27:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he develops something, something has been developed
When he locates something, something has been located
When he involves something, something has been involved
When he establishes something, something has been established
When he appears something, something has been appeared
When he replaces something, something has been replaced
When he receives something, something has been received
When he applies something, something has been
2024-07-19 04:27:08 root INFO     [order_1_approx] starting weight calculation for When he establishes something, something has been established
When he replaces something, something has been replaced
When he involves something, something has been involved
When he appears something, something has been appeared
When he locates something, something has been located
When he applies something, something has been applied
When he develops something, something has been developed
When he receives something, something has been
2024-07-19 04:27:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 04:31:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5825,  0.1440,  0.0104,  ..., -0.4307, -0.1105,  0.4983],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4741,  1.6553, -0.9248,  ...,  1.0928,  3.2930,  2.8262],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0119, -0.0041, -0.0067,  ..., -0.0010,  0.0016, -0.0172],
        [-0.0052,  0.0008,  0.0140,  ..., -0.0103,  0.0063,  0.0028],
        [-0.0026,  0.0059, -0.0067,  ..., -0.0015, -0.0159,  0.0114],
        ...,
        [-0.0162, -0.0226, -0.0049,  ..., -0.0121, -0.0232,  0.0004],
        [ 0.0132,  0.0042, -0.0016,  ..., -0.0101, -0.0060,  0.0153],
        [-0.0341, -0.0136,  0.0099,  ..., -0.0053,  0.0024, -0.0234]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.4236,  2.0098, -0.1587,  ...,  0.6660,  3.2852,  3.3125]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 04:31:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he establishes something, something has been established
When he replaces something, something has been replaced
When he involves something, something has been involved
When he appears something, something has been appeared
When he locates something, something has been located
When he applies something, something has been applied
When he develops something, something has been developed
When he receives something, something has been
2024-07-19 04:31:29 root INFO     [order_1_approx] starting weight calculation for When he receives something, something has been received
When he applies something, something has been applied
When he develops something, something has been developed
When he locates something, something has been located
When he establishes something, something has been established
When he replaces something, something has been replaced
When he involves something, something has been involved
When he appears something, something has been
2024-07-19 04:31:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 04:35:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7051,  0.1747,  0.5771,  ..., -0.1587,  0.3706, -0.0529],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8882,  1.3252,  0.3242,  ...,  1.6104, -3.9102, -3.5703],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0278, -0.0039, -0.0024,  ..., -0.0005, -0.0043,  0.0078],
        [-0.0245, -0.0134,  0.0224,  ...,  0.0060, -0.0023, -0.0046],
        [-0.0022,  0.0071, -0.0030,  ...,  0.0280, -0.0009,  0.0056],
        ...,
        [-0.0068,  0.0268, -0.0021,  ..., -0.0069, -0.0071, -0.0066],
        [ 0.0436,  0.0004, -0.0085,  ..., -0.0487, -0.0216, -0.0191],
        [-0.0131, -0.0082, -0.0115,  ...,  0.0123,  0.0051, -0.0281]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6250,  2.1934, -0.0479,  ...,  1.9902, -4.3125, -3.7031]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 04:35:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he receives something, something has been received
When he applies something, something has been applied
When he develops something, something has been developed
When he locates something, something has been located
When he establishes something, something has been established
When he replaces something, something has been replaced
When he involves something, something has been involved
When he appears something, something has been
2024-07-19 04:35:46 root INFO     [order_1_approx] starting weight calculation for When he establishes something, something has been established
When he involves something, something has been involved
When he applies something, something has been applied
When he receives something, something has been received
When he appears something, something has been appeared
When he locates something, something has been located
When he develops something, something has been developed
When he replaces something, something has been
2024-07-19 04:35:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 04:40:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4075,  0.2324,  0.1086,  ...,  0.0684, -0.2021, -0.0222],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4160,  2.1016, -0.8745,  ...,  2.6855,  3.0234,  4.4062],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-7.0801e-03,  1.8845e-03,  1.0162e-02,  ..., -1.5991e-02,
         -2.1530e-02, -2.7733e-03],
        [ 1.5259e-05, -8.2703e-03,  1.4587e-02,  ...,  5.0659e-03,
          2.0462e-02,  6.2027e-03],
        [ 1.7761e-02,  1.1841e-02, -2.2461e-02,  ...,  1.0025e-02,
         -4.5967e-03,  3.8986e-03],
        ...,
        [-2.9800e-02, -1.7395e-02, -3.6697e-03,  ..., -2.1820e-03,
          7.7362e-03, -3.2616e-03],
        [-7.1182e-03,  3.1738e-03, -2.8172e-03,  ..., -2.7603e-02,
         -1.3535e-02, -1.8188e-02],
        [-2.4872e-02, -1.5015e-02, -9.1324e-03,  ...,  2.9144e-03,
          5.1117e-03, -3.1616e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2305,  2.1035, -0.1235,  ...,  2.5117,  2.6016,  4.4414]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 04:40:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he establishes something, something has been established
When he involves something, something has been involved
When he applies something, something has been applied
When he receives something, something has been received
When he appears something, something has been appeared
When he locates something, something has been located
When he develops something, something has been developed
When he replaces something, something has been
2024-07-19 04:40:07 root INFO     [order_1_approx] starting weight calculation for When he replaces something, something has been replaced
When he receives something, something has been received
When he applies something, something has been applied
When he locates something, something has been located
When he involves something, something has been involved
When he appears something, something has been appeared
When he establishes something, something has been established
When he develops something, something has been
2024-07-19 04:40:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 04:44:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3181,  0.1019,  0.6445,  ..., -0.3254, -0.4678,  0.4609],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4922, -0.6362,  0.5342,  ..., -2.2109,  2.2656, -2.3633],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0637,  0.0079, -0.0191,  ..., -0.0029,  0.0139, -0.0235],
        [ 0.0256, -0.0123,  0.0078,  ..., -0.0018,  0.0240,  0.0059],
        [ 0.0280, -0.0050, -0.0304,  ..., -0.0305,  0.0249,  0.0033],
        ...,
        [-0.0046, -0.0038, -0.0091,  ..., -0.0144, -0.0002,  0.0060],
        [-0.0039,  0.0150,  0.0099,  ..., -0.0410, -0.0281,  0.0072],
        [-0.0230, -0.0027, -0.0002,  ...,  0.0221,  0.0101, -0.0365]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3706, -0.4417,  0.9697,  ..., -2.5840,  0.9160, -2.5234]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 04:44:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he replaces something, something has been replaced
When he receives something, something has been received
When he applies something, something has been applied
When he locates something, something has been located
When he involves something, something has been involved
When he appears something, something has been appeared
When he establishes something, something has been established
When he develops something, something has been
2024-07-19 04:44:29 root INFO     [order_1_approx] starting weight calculation for When he involves something, something has been involved
When he establishes something, something has been established
When he replaces something, something has been replaced
When he appears something, something has been appeared
When he applies something, something has been applied
When he develops something, something has been developed
When he receives something, something has been received
When he locates something, something has been
2024-07-19 04:44:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 04:48:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5029,  0.2347, -0.0978,  ..., -0.0742, -0.0297, -0.0309],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.9512,  1.6719, -2.4688,  ..., -4.3438,  0.8857,  0.9673],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0204, -0.0089,  0.0171,  ...,  0.0138, -0.0213, -0.0030],
        [-0.0290,  0.0011,  0.0067,  ...,  0.0048,  0.0130, -0.0021],
        [ 0.0280,  0.0146, -0.0194,  ..., -0.0007,  0.0191, -0.0088],
        ...,
        [-0.0211,  0.0077,  0.0106,  ..., -0.0126, -0.0070, -0.0135],
        [ 0.0213,  0.0104, -0.0029,  ..., -0.0176,  0.0075, -0.0138],
        [-0.0082, -0.0114,  0.0172,  ...,  0.0154,  0.0116, -0.0149]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6777,  0.9263, -2.1992,  ..., -4.7930,  0.4141,  1.3027]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 04:48:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he involves something, something has been involved
When he establishes something, something has been established
When he replaces something, something has been replaced
When he appears something, something has been appeared
When he applies something, something has been applied
When he develops something, something has been developed
When he receives something, something has been received
When he locates something, something has been
2024-07-19 04:48:50 root INFO     [order_1_approx] starting weight calculation for When he involves something, something has been involved
When he appears something, something has been appeared
When he develops something, something has been developed
When he locates something, something has been located
When he replaces something, something has been replaced
When he receives something, something has been received
When he applies something, something has been applied
When he establishes something, something has been
2024-07-19 04:48:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 04:53:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3516,  0.6025,  0.0737,  ...,  0.4026, -0.2617,  0.3237],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2627, -0.3027,  0.0625,  ...,  0.9487,  0.0713,  1.5342],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0115, -0.0025,  0.0047,  ...,  0.0056, -0.0223, -0.0114],
        [-0.0091, -0.0051,  0.0023,  ...,  0.0104,  0.0031,  0.0043],
        [ 0.0230,  0.0026, -0.0235,  ..., -0.0059,  0.0223,  0.0151],
        ...,
        [-0.0232,  0.0020, -0.0047,  ..., -0.0180, -0.0035, -0.0098],
        [-0.0159, -0.0049,  0.0024,  ..., -0.0246, -0.0317, -0.0101],
        [-0.0131, -0.0205,  0.0136,  ..., -0.0112,  0.0137, -0.0240]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1472,  0.1287,  1.6270,  ..., -0.0903, -0.7861,  2.7734]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 04:53:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for When he involves something, something has been involved
When he appears something, something has been appeared
When he develops something, something has been developed
When he locates something, something has been located
When he replaces something, something has been replaced
When he receives something, something has been received
When he applies something, something has been applied
When he establishes something, something has been
2024-07-19 04:53:12 root INFO     total operator prediction time: 2086.356331348419 seconds
2024-07-19 04:53:12 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on adj - superlative
2024-07-19 04:53:12 root INFO     building operator adj - superlative
2024-07-19 04:53:12 root INFO     [order_1_approx] starting weight calculation for If something is the most fierce, it is fiercest
If something is the most mild, it is mildest
If something is the most scary, it is scariest
If something is the most hot, it is hottest
If something is the most cheap, it is cheapest
If something is the most wealthy, it is wealthiest
If something is the most noisy, it is noisiest
If something is the most clever, it is
2024-07-19 04:53:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 04:57:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2460, -0.2737,  0.4907,  ...,  0.0227, -0.4805,  0.6924],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5918, -1.0977, -0.2075,  ...,  1.4844, -2.9102,  0.7949],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.0704e-02, -2.7420e-02, -5.2567e-03,  ..., -9.9945e-03,
         -1.7914e-02, -6.3477e-03],
        [-3.0518e-05, -2.0142e-02,  4.4174e-03,  ..., -6.8588e-03,
         -1.0132e-02, -1.0643e-03],
        [ 5.4817e-03, -1.0624e-03,  1.5625e-02,  ...,  2.9087e-03,
          7.5302e-03, -1.1581e-02],
        ...,
        [-2.0752e-03, -2.5360e-02,  2.8229e-04,  ..., -2.4506e-02,
          2.3117e-03, -1.4519e-02],
        [-5.1346e-03, -1.6830e-02, -2.5650e-02,  ..., -2.1439e-03,
         -2.8107e-02,  1.9348e-02],
        [-2.4200e-02,  8.4000e-03,  6.9046e-04,  ..., -1.3046e-02,
          6.4812e-03, -2.7054e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.9141, -1.6777,  0.4126,  ...,  0.8618, -3.1992,  0.2676]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 04:57:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most fierce, it is fiercest
If something is the most mild, it is mildest
If something is the most scary, it is scariest
If something is the most hot, it is hottest
If something is the most cheap, it is cheapest
If something is the most wealthy, it is wealthiest
If something is the most noisy, it is noisiest
If something is the most clever, it is
2024-07-19 04:57:32 root INFO     [order_1_approx] starting weight calculation for If something is the most clever, it is cleverest
If something is the most wealthy, it is wealthiest
If something is the most fierce, it is fiercest
If something is the most scary, it is scariest
If something is the most noisy, it is noisiest
If something is the most mild, it is mildest
If something is the most hot, it is hottest
If something is the most cheap, it is
2024-07-19 04:57:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 05:01:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0131, -0.1824, -0.0522,  ..., -0.1665, -0.5498,  0.5054],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6523, -2.5684, -4.4648,  ...,  0.5186, -1.5049,  0.9609],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-7.9651e-03, -8.9569e-03,  1.1444e-05,  ...,  7.0305e-03,
         -2.0630e-02, -5.8556e-03],
        [ 1.2207e-03, -2.0962e-03,  7.4539e-03,  ...,  5.9204e-03,
         -1.1566e-02,  4.1885e-03],
        [ 2.3193e-03, -3.2673e-03, -1.8463e-03,  ...,  5.8899e-03,
          1.3115e-02,  1.6159e-02],
        ...,
        [-8.1024e-03, -1.7609e-02,  1.1642e-02,  ..., -1.3367e-02,
          4.6234e-03,  2.1782e-03],
        [-9.8419e-04, -1.5533e-02,  5.0468e-03,  ..., -1.4214e-02,
         -3.2623e-02,  8.3313e-03],
        [-3.1860e-02,  1.0956e-02,  1.5457e-02,  ..., -1.3260e-02,
         -5.9814e-03, -1.9684e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7900, -3.0820, -4.3555,  ...,  0.0242, -1.8848,  0.1304]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 05:01:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most clever, it is cleverest
If something is the most wealthy, it is wealthiest
If something is the most fierce, it is fiercest
If something is the most scary, it is scariest
If something is the most noisy, it is noisiest
If something is the most mild, it is mildest
If something is the most hot, it is hottest
If something is the most cheap, it is
2024-07-19 05:01:50 root INFO     [order_1_approx] starting weight calculation for If something is the most noisy, it is noisiest
If something is the most wealthy, it is wealthiest
If something is the most cheap, it is cheapest
If something is the most hot, it is hottest
If something is the most clever, it is cleverest
If something is the most mild, it is mildest
If something is the most fierce, it is fiercest
If something is the most scary, it is
2024-07-19 05:01:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 05:06:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6753,  0.6309,  0.4741,  ..., -0.2505, -0.4727,  0.5342],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.3711, -0.6807, -0.0992,  ...,  0.5889, -0.9546, -0.5781],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0194, -0.0092,  0.0104,  ...,  0.0170,  0.0144, -0.0182],
        [-0.0028, -0.0189,  0.0127,  ..., -0.0067, -0.0177, -0.0011],
        [-0.0016, -0.0180, -0.0128,  ...,  0.0215, -0.0010,  0.0046],
        ...,
        [-0.0144, -0.0233,  0.0149,  ..., -0.0410, -0.0112, -0.0508],
        [-0.0105, -0.0057,  0.0142,  ..., -0.0194, -0.0189,  0.0239],
        [-0.0170,  0.0039,  0.0200,  ..., -0.0433, -0.0176, -0.0492]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.2969, -0.8271,  0.7085,  ...,  0.4700, -1.4492, -0.5273]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 05:06:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most noisy, it is noisiest
If something is the most wealthy, it is wealthiest
If something is the most cheap, it is cheapest
If something is the most hot, it is hottest
If something is the most clever, it is cleverest
If something is the most mild, it is mildest
If something is the most fierce, it is fiercest
If something is the most scary, it is
2024-07-19 05:06:08 root INFO     [order_1_approx] starting weight calculation for If something is the most scary, it is scariest
If something is the most clever, it is cleverest
If something is the most wealthy, it is wealthiest
If something is the most hot, it is hottest
If something is the most noisy, it is noisiest
If something is the most mild, it is mildest
If something is the most cheap, it is cheapest
If something is the most fierce, it is
2024-07-19 05:06:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 05:10:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3525,  0.2554,  0.0229,  ..., -0.3379, -0.4287,  1.2812],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.9570,  0.2305, -4.2070,  ..., -2.2891, -1.7090,  0.0537],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.1027e-02, -1.6724e-02,  1.4023e-02,  ..., -1.8951e-02,
         -1.0086e-02,  1.9058e-02],
        [-1.6022e-02, -7.7591e-03,  9.0637e-03,  ...,  3.9139e-03,
         -1.6769e-02, -1.5335e-02],
        [-5.2681e-03, -7.4234e-03, -1.0223e-02,  ...,  1.7334e-02,
          1.1230e-02,  2.3422e-03],
        ...,
        [ 2.8744e-03, -2.9800e-02, -3.5286e-05,  ..., -2.4658e-02,
         -6.8130e-03, -1.9531e-03],
        [-6.3820e-03, -3.4103e-03,  1.1703e-02,  ..., -4.9019e-03,
         -1.6998e-02,  9.1476e-03],
        [-5.5695e-03,  2.4033e-02,  1.4038e-03,  ..., -2.4124e-02,
         -2.1801e-03, -3.3539e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.8770, -0.2180, -3.7969,  ..., -2.6875, -1.8652,  0.3640]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 05:10:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most scary, it is scariest
If something is the most clever, it is cleverest
If something is the most wealthy, it is wealthiest
If something is the most hot, it is hottest
If something is the most noisy, it is noisiest
If something is the most mild, it is mildest
If something is the most cheap, it is cheapest
If something is the most fierce, it is
2024-07-19 05:10:26 root INFO     [order_1_approx] starting weight calculation for If something is the most mild, it is mildest
If something is the most fierce, it is fiercest
If something is the most wealthy, it is wealthiest
If something is the most hot, it is hottest
If something is the most clever, it is cleverest
If something is the most cheap, it is cheapest
If something is the most scary, it is scariest
If something is the most noisy, it is
2024-07-19 05:10:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 05:14:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7295,  0.8940,  0.1921,  ..., -0.5981, -0.9932,  0.9512],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4031, -1.5449, -2.5742,  ...,  3.2227,  2.3047,  3.8945],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.1368e-02, -2.5818e-02,  4.3869e-03,  ...,  1.9928e-02,
         -1.5022e-02, -1.3565e-02],
        [-1.1787e-02, -1.8265e-02, -1.4744e-03,  ...,  1.9104e-02,
          6.6452e-03, -4.5776e-05],
        [ 3.8338e-03, -1.0109e-02, -1.9928e-02,  ..., -5.7335e-03,
          2.0157e-02,  1.7212e-02],
        ...,
        [-4.1847e-03, -1.2222e-02,  2.4891e-03,  ..., -1.6449e-02,
         -4.3411e-03, -3.0231e-03],
        [ 1.0269e-02, -9.4604e-03,  1.5289e-02,  ..., -6.9389e-03,
         -1.6434e-02,  2.6722e-03],
        [-7.3090e-03,  7.8125e-03,  9.7961e-03,  ..., -9.0179e-03,
         -1.4023e-02, -3.4668e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3975, -2.8789, -1.3184,  ...,  3.4492,  2.0742,  2.8164]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 05:14:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most mild, it is mildest
If something is the most fierce, it is fiercest
If something is the most wealthy, it is wealthiest
If something is the most hot, it is hottest
If something is the most clever, it is cleverest
If something is the most cheap, it is cheapest
If something is the most scary, it is scariest
If something is the most noisy, it is
2024-07-19 05:14:44 root INFO     [order_1_approx] starting weight calculation for If something is the most scary, it is scariest
If something is the most hot, it is hottest
If something is the most noisy, it is noisiest
If something is the most wealthy, it is wealthiest
If something is the most cheap, it is cheapest
If something is the most clever, it is cleverest
If something is the most fierce, it is fiercest
If something is the most mild, it is
2024-07-19 05:14:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 05:19:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3416,  0.2322,  0.7163,  ...,  0.3521, -1.0684,  0.4417],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.8555, -0.6343, -5.8516,  ..., -0.8550, -0.8564,  3.8184],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0079, -0.0268, -0.0048,  ...,  0.0027, -0.0042,  0.0021],
        [-0.0052, -0.0265, -0.0094,  ...,  0.0053,  0.0032, -0.0028],
        [ 0.0083,  0.0073, -0.0170,  ...,  0.0165,  0.0054,  0.0061],
        ...,
        [-0.0029, -0.0343,  0.0013,  ..., -0.0048, -0.0035, -0.0010],
        [-0.0168, -0.0021,  0.0036,  ..., -0.0010, -0.0054,  0.0100],
        [ 0.0015, -0.0123,  0.0136,  ..., -0.0193, -0.0042, -0.0142]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.8633, -1.0107, -6.1172,  ..., -1.5928, -0.7021,  2.8418]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 05:19:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most scary, it is scariest
If something is the most hot, it is hottest
If something is the most noisy, it is noisiest
If something is the most wealthy, it is wealthiest
If something is the most cheap, it is cheapest
If something is the most clever, it is cleverest
If something is the most fierce, it is fiercest
If something is the most mild, it is
2024-07-19 05:19:04 root INFO     [order_1_approx] starting weight calculation for If something is the most fierce, it is fiercest
If something is the most clever, it is cleverest
If something is the most mild, it is mildest
If something is the most cheap, it is cheapest
If something is the most scary, it is scariest
If something is the most noisy, it is noisiest
If something is the most wealthy, it is wealthiest
If something is the most hot, it is
2024-07-19 05:19:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 05:23:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0305,  0.6943,  0.7173,  ...,  0.4165, -0.2917,  0.6421],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0312,  0.1868, -0.4629,  ..., -1.0723,  1.5439,  4.7891],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.5671e-02, -8.0490e-03,  3.4676e-03,  ..., -1.2222e-02,
          5.4016e-03, -1.5564e-02],
        [-9.9106e-03, -1.2779e-02, -7.1716e-03,  ...,  8.1024e-03,
         -4.4899e-03, -1.8677e-02],
        [-2.8076e-03, -6.2065e-03,  2.0721e-02,  ..., -7.7896e-03,
          2.3331e-02,  1.4030e-02],
        ...,
        [-1.0633e-03, -1.6098e-02, -9.9182e-05,  ...,  8.3847e-03,
          1.5259e-05,  1.5533e-02],
        [-1.0109e-02, -2.8286e-03,  1.0712e-02,  ..., -1.4420e-02,
          3.9520e-03,  3.3855e-03],
        [-2.7298e-02,  1.3657e-03,  4.1275e-03,  ..., -1.6327e-02,
          1.1993e-02, -3.3203e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7671, -0.3684, -1.1035,  ..., -0.9888,  0.9561,  5.4023]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 05:23:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most fierce, it is fiercest
If something is the most clever, it is cleverest
If something is the most mild, it is mildest
If something is the most cheap, it is cheapest
If something is the most scary, it is scariest
If something is the most noisy, it is noisiest
If something is the most wealthy, it is wealthiest
If something is the most hot, it is
2024-07-19 05:23:23 root INFO     [order_1_approx] starting weight calculation for If something is the most hot, it is hottest
If something is the most cheap, it is cheapest
If something is the most fierce, it is fiercest
If something is the most mild, it is mildest
If something is the most scary, it is scariest
If something is the most noisy, it is noisiest
If something is the most clever, it is cleverest
If something is the most wealthy, it is
2024-07-19 05:23:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 05:27:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2180,  0.6646,  0.4839,  ...,  0.2866,  0.0155,  0.3977],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.3750, -1.6924, -0.0244,  ..., -3.0508, -0.1172,  2.7344],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0088, -0.0222,  0.0114,  ..., -0.0084, -0.0161,  0.0031],
        [-0.0057,  0.0003, -0.0005,  ...,  0.0277,  0.0040, -0.0197],
        [ 0.0052,  0.0122, -0.0098,  ...,  0.0266,  0.0054,  0.0125],
        ...,
        [-0.0223, -0.0286,  0.0025,  ..., -0.0209,  0.0011, -0.0002],
        [ 0.0114, -0.0112, -0.0060,  ..., -0.0114, -0.0384,  0.0285],
        [-0.0240,  0.0146, -0.0026,  ...,  0.0223, -0.0022, -0.0593]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0137, -1.7861,  0.2969,  ..., -3.9336, -1.3467,  1.6709]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 05:27:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most hot, it is hottest
If something is the most cheap, it is cheapest
If something is the most fierce, it is fiercest
If something is the most mild, it is mildest
If something is the most scary, it is scariest
If something is the most noisy, it is noisiest
If something is the most clever, it is cleverest
If something is the most wealthy, it is
2024-07-19 05:27:42 root INFO     total operator prediction time: 2070.2824103832245 seconds
2024-07-19 05:27:42 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+er_irreg
2024-07-19 05:27:42 root INFO     building operator verb+er_irreg
2024-07-19 05:27:42 root INFO     [order_1_approx] starting weight calculation for If you tell something, you are a teller
If you speak something, you are a speaker
If you defend something, you are a defender
If you bake something, you are a baker
If you intrude something, you are a intruder
If you subscribe something, you are a subscriber
If you recommend something, you are a recommender
If you choreograph something, you are a
2024-07-19 05:27:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 05:32:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0354, -0.1219,  0.6064,  ...,  0.3066, -1.1406,  0.4509],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.8828, -0.8232, -0.7705,  ..., -1.5381, -0.2305,  3.0293],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0320,  0.0160, -0.0243,  ..., -0.0089, -0.0036, -0.0109],
        [-0.0170, -0.0074,  0.0072,  ..., -0.0013,  0.0002, -0.0002],
        [ 0.0086, -0.0080, -0.0024,  ...,  0.0201,  0.0114,  0.0280],
        ...,
        [-0.0114,  0.0010,  0.0072,  ..., -0.0022,  0.0056, -0.0086],
        [ 0.0197,  0.0222, -0.0177,  ..., -0.0188, -0.0188, -0.0056],
        [-0.0134,  0.0008,  0.0080,  ..., -0.0006,  0.0130, -0.0117]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.6621, -1.4414, -0.7407,  ..., -2.0000, -0.1804,  2.9121]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 05:32:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you tell something, you are a teller
If you speak something, you are a speaker
If you defend something, you are a defender
If you bake something, you are a baker
If you intrude something, you are a intruder
If you subscribe something, you are a subscriber
If you recommend something, you are a recommender
If you choreograph something, you are a
2024-07-19 05:32:05 root INFO     [order_1_approx] starting weight calculation for If you bake something, you are a baker
If you choreograph something, you are a choreographer
If you recommend something, you are a recommender
If you speak something, you are a speaker
If you subscribe something, you are a subscriber
If you intrude something, you are a intruder
If you defend something, you are a defender
If you tell something, you are a
2024-07-19 05:32:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 05:36:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4299, -0.0186,  0.5981,  ...,  0.6748, -0.6812,  0.7412],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.8828,  1.5215, -1.7305,  ..., -1.2676,  1.9023,  3.6133],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0100, -0.0038,  0.0312,  ..., -0.0124, -0.0106,  0.0212],
        [-0.0200, -0.0117,  0.0504,  ..., -0.0156, -0.0042, -0.0090],
        [ 0.0079,  0.0009, -0.0113,  ...,  0.0002,  0.0248, -0.0036],
        ...,
        [-0.0161, -0.0154,  0.0124,  ..., -0.0200,  0.0092,  0.0134],
        [ 0.0102, -0.0156, -0.0081,  ..., -0.0013,  0.0027, -0.0075],
        [-0.0019, -0.0137,  0.0219,  ..., -0.0145,  0.0134,  0.0029]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.8711,  0.2988, -1.2646,  ..., -2.4395,  2.6641,  3.8555]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 05:36:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you bake something, you are a baker
If you choreograph something, you are a choreographer
If you recommend something, you are a recommender
If you speak something, you are a speaker
If you subscribe something, you are a subscriber
If you intrude something, you are a intruder
If you defend something, you are a defender
If you tell something, you are a
2024-07-19 05:36:27 root INFO     [order_1_approx] starting weight calculation for If you bake something, you are a baker
If you speak something, you are a speaker
If you recommend something, you are a recommender
If you choreograph something, you are a choreographer
If you subscribe something, you are a subscriber
If you intrude something, you are a intruder
If you tell something, you are a teller
If you defend something, you are a
2024-07-19 05:36:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 05:40:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1667, -0.0022,  0.4546,  ..., -0.1586, -1.0488,  0.7285],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.8047,  3.0176, -2.0449,  ..., -6.7812,  6.7422,  7.5859],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0261, -0.0002,  0.0264,  ...,  0.0099, -0.0057, -0.0088],
        [ 0.0147, -0.0012,  0.0027,  ..., -0.0127,  0.0067,  0.0188],
        [ 0.0118,  0.0159, -0.0233,  ...,  0.0090,  0.0159,  0.0232],
        ...,
        [ 0.0025, -0.0076, -0.0139,  ..., -0.0027, -0.0105,  0.0014],
        [ 0.0031, -0.0227,  0.0055,  ..., -0.0107, -0.0130, -0.0098],
        [-0.0155,  0.0006,  0.0262,  ..., -0.0080,  0.0080, -0.0068]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3750,  3.6289, -0.9834,  ..., -6.0234,  5.4688,  7.1719]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 05:40:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you bake something, you are a baker
If you speak something, you are a speaker
If you recommend something, you are a recommender
If you choreograph something, you are a choreographer
If you subscribe something, you are a subscriber
If you intrude something, you are a intruder
If you tell something, you are a teller
If you defend something, you are a
2024-07-19 05:40:48 root INFO     [order_1_approx] starting weight calculation for If you tell something, you are a teller
If you choreograph something, you are a choreographer
If you speak something, you are a speaker
If you defend something, you are a defender
If you recommend something, you are a recommender
If you intrude something, you are a intruder
If you subscribe something, you are a subscriber
If you bake something, you are a
2024-07-19 05:40:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 05:45:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6880,  0.1743, -0.6528,  ..., -0.6582, -0.6606,  0.5430],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.2656,  7.1016, -3.1113,  ..., -3.3008,  0.1152,  4.3281],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0100,  0.0021,  0.0075,  ..., -0.0030, -0.0010, -0.0077],
        [-0.0220, -0.0051,  0.0021,  ..., -0.0049,  0.0170,  0.0071],
        [ 0.0118, -0.0063,  0.0230,  ..., -0.0003, -0.0094,  0.0179],
        ...,
        [ 0.0007,  0.0072,  0.0065,  ..., -0.0077,  0.0035,  0.0037],
        [ 0.0071, -0.0089,  0.0077,  ...,  0.0034, -0.0197,  0.0157],
        [-0.0125, -0.0134,  0.0069,  ..., -0.0085,  0.0122, -0.0163]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.6953,  7.1367, -3.3418,  ..., -3.4316,  0.2031,  4.4727]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 05:45:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you tell something, you are a teller
If you choreograph something, you are a choreographer
If you speak something, you are a speaker
If you defend something, you are a defender
If you recommend something, you are a recommender
If you intrude something, you are a intruder
If you subscribe something, you are a subscriber
If you bake something, you are a
2024-07-19 05:45:06 root INFO     [order_1_approx] starting weight calculation for If you tell something, you are a teller
If you subscribe something, you are a subscriber
If you choreograph something, you are a choreographer
If you recommend something, you are a recommender
If you bake something, you are a baker
If you defend something, you are a defender
If you intrude something, you are a intruder
If you speak something, you are a
2024-07-19 05:45:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 05:49:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0441, -0.5322,  0.8813,  ..., -0.1013, -0.4160,  0.4673],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.0391,  1.8105, -1.3721,  ..., -2.6172,  0.6909,  1.5332],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0096,  0.0048,  0.0066,  ..., -0.0109,  0.0010,  0.0136],
        [-0.0161,  0.0051,  0.0109,  ..., -0.0066, -0.0065,  0.0074],
        [ 0.0243,  0.0049, -0.0065,  ...,  0.0128,  0.0287, -0.0155],
        ...,
        [ 0.0076,  0.0053, -0.0032,  ..., -0.0060,  0.0023, -0.0180],
        [ 0.0419, -0.0111,  0.0053,  ...,  0.0076, -0.0034, -0.0240],
        [-0.0060,  0.0060, -0.0023,  ..., -0.0062,  0.0015, -0.0147]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.4082,  1.3086, -0.6738,  ..., -2.6816,  1.7090,  1.3848]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 05:49:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you tell something, you are a teller
If you subscribe something, you are a subscriber
If you choreograph something, you are a choreographer
If you recommend something, you are a recommender
If you bake something, you are a baker
If you defend something, you are a defender
If you intrude something, you are a intruder
If you speak something, you are a
2024-07-19 05:49:26 root INFO     [order_1_approx] starting weight calculation for If you recommend something, you are a recommender
If you bake something, you are a baker
If you defend something, you are a defender
If you speak something, you are a speaker
If you choreograph something, you are a choreographer
If you subscribe something, you are a subscriber
If you tell something, you are a teller
If you intrude something, you are a
2024-07-19 05:49:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 05:53:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5693, -0.9287,  0.2266,  ...,  0.0779, -1.3262,  1.3164],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 6.6523,  0.2402, -4.7539,  ...,  0.0562,  0.1304,  3.4141],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.9424e-02, -1.0239e-02,  1.2321e-02,  ...,  1.0429e-02,
         -2.1942e-02, -4.2877e-03],
        [-2.6947e-02, -4.5509e-03,  1.3664e-02,  ..., -5.0812e-03,
         -7.0095e-05, -8.0566e-03],
        [ 3.9734e-02,  2.7527e-02, -1.6418e-02,  ..., -2.0828e-03,
          1.1353e-02,  6.2561e-03],
        ...,
        [-1.4534e-02, -1.7975e-02, -1.7548e-02,  ..., -1.5106e-02,
          1.3290e-02, -1.4130e-02],
        [ 2.0660e-02, -1.1818e-02,  1.3107e-02,  ..., -3.8528e-04,
         -2.8580e-02, -3.3360e-03],
        [-7.8583e-03,  8.6517e-03,  1.0857e-02,  ...,  7.0724e-03,
         -2.1992e-03, -6.4659e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.1484, -0.2590, -3.5625,  ..., -0.4006,  0.1821,  2.9863]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 05:53:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you recommend something, you are a recommender
If you bake something, you are a baker
If you defend something, you are a defender
If you speak something, you are a speaker
If you choreograph something, you are a choreographer
If you subscribe something, you are a subscriber
If you tell something, you are a teller
If you intrude something, you are a
2024-07-19 05:53:47 root INFO     [order_1_approx] starting weight calculation for If you speak something, you are a speaker
If you recommend something, you are a recommender
If you intrude something, you are a intruder
If you bake something, you are a baker
If you defend something, you are a defender
If you choreograph something, you are a choreographer
If you tell something, you are a teller
If you subscribe something, you are a
2024-07-19 05:53:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 05:58:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8594, -0.2109, -0.2881,  ...,  0.7852, -0.6426,  0.8086],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.5889,  2.9492, -0.9248,  ..., -3.2852, -0.1172,  2.5059],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0141, -0.0021,  0.0017,  ...,  0.0188,  0.0049, -0.0021],
        [-0.0126,  0.0153, -0.0031,  ...,  0.0050, -0.0121, -0.0055],
        [ 0.0083,  0.0031, -0.0081,  ...,  0.0032, -0.0074,  0.0125],
        ...,
        [-0.0318, -0.0193, -0.0039,  ..., -0.0084, -0.0006, -0.0064],
        [ 0.0055, -0.0075, -0.0144,  ..., -0.0121, -0.0102,  0.0182],
        [-0.0065, -0.0027,  0.0069,  ..., -0.0078, -0.0007, -0.0036]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5420,  3.3438, -1.2578,  ..., -3.5137, -1.9023,  1.7695]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 05:58:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you speak something, you are a speaker
If you recommend something, you are a recommender
If you intrude something, you are a intruder
If you bake something, you are a baker
If you defend something, you are a defender
If you choreograph something, you are a choreographer
If you tell something, you are a teller
If you subscribe something, you are a
2024-07-19 05:58:08 root INFO     [order_1_approx] starting weight calculation for If you defend something, you are a defender
If you bake something, you are a baker
If you choreograph something, you are a choreographer
If you speak something, you are a speaker
If you tell something, you are a teller
If you subscribe something, you are a subscriber
If you intrude something, you are a intruder
If you recommend something, you are a
2024-07-19 05:58:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 06:02:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4917,  0.3984, -0.6797,  ...,  0.2847, -0.9932,  0.5405],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5942, -1.7334, -2.8906,  ..., -2.0742,  2.3867,  3.0176],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0365,  0.0009,  0.0264,  ..., -0.0046,  0.0141, -0.0088],
        [ 0.0143,  0.0176,  0.0227,  ..., -0.0045,  0.0118,  0.0084],
        [ 0.0235,  0.0135, -0.0204,  ..., -0.0069,  0.0323,  0.0298],
        ...,
        [-0.0286, -0.0084,  0.0188,  ..., -0.0103, -0.0038, -0.0012],
        [ 0.0349,  0.0094, -0.0042,  ..., -0.0011, -0.0217,  0.0213],
        [-0.0124, -0.0085,  0.0136,  ...,  0.0154, -0.0002, -0.0200]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6455, -0.1787, -0.3574,  ..., -3.3438,  1.6250,  2.7559]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 06:02:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you defend something, you are a defender
If you bake something, you are a baker
If you choreograph something, you are a choreographer
If you speak something, you are a speaker
If you tell something, you are a teller
If you subscribe something, you are a subscriber
If you intrude something, you are a intruder
If you recommend something, you are a
2024-07-19 06:02:30 root INFO     total operator prediction time: 2087.3966784477234 seconds
2024-07-19 06:02:30 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on over+adj_reg
2024-07-19 06:02:30 root INFO     building operator over+adj_reg
2024-07-19 06:02:30 root INFO     [order_1_approx] starting weight calculation for If something is too stated, it is overstated
If something is too sized, it is oversized
If something is too used, it is overused
If something is too sold, it is oversold
If something is too stretched, it is overstretched
If something is too cooked, it is overcooked
If something is too powered, it is overpowered
If something is too protected, it is
2024-07-19 06:02:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 06:06:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4651,  0.5322,  0.3875,  ...,  0.3733, -0.5063,  0.2174],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8613, -0.0596, -1.4688,  ...,  1.1787, -0.0503,  1.3291],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0014, -0.0075,  0.0037,  ..., -0.0137, -0.0064, -0.0011],
        [-0.0035, -0.0041, -0.0027,  ...,  0.0190, -0.0105, -0.0130],
        [ 0.0027,  0.0230,  0.0068,  ..., -0.0201,  0.0059,  0.0044],
        ...,
        [-0.0113, -0.0014, -0.0010,  ..., -0.0049, -0.0172, -0.0066],
        [-0.0137, -0.0032, -0.0106,  ..., -0.0286, -0.0273,  0.0187],
        [ 0.0024,  0.0073, -0.0076,  ..., -0.0008,  0.0034,  0.0143]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.6406, -0.5234, -0.8652,  ...,  0.7578,  0.1598,  1.4805]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 06:06:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too stated, it is overstated
If something is too sized, it is oversized
If something is too used, it is overused
If something is too sold, it is oversold
If something is too stretched, it is overstretched
If something is too cooked, it is overcooked
If something is too powered, it is overpowered
If something is too protected, it is
2024-07-19 06:06:48 root INFO     [order_1_approx] starting weight calculation for If something is too stretched, it is overstretched
If something is too used, it is overused
If something is too powered, it is overpowered
If something is too stated, it is overstated
If something is too cooked, it is overcooked
If something is too protected, it is overprotected
If something is too sized, it is oversized
If something is too sold, it is
2024-07-19 06:06:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 06:11:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6934,  0.4189,  0.4331,  ..., -0.0632, -0.4932, -0.2332],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.1035,  0.5317, -1.9375,  ..., -0.8179,  0.9634,  3.3105],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0002, -0.0396,  0.0132,  ...,  0.0148, -0.0003, -0.0388],
        [ 0.0005,  0.0252,  0.0186,  ...,  0.0064, -0.0189, -0.0010],
        [-0.0021, -0.0040, -0.0105,  ...,  0.0022,  0.0031, -0.0167],
        ...,
        [ 0.0156, -0.0206, -0.0285,  ..., -0.0004, -0.0022, -0.0029],
        [-0.0081, -0.0039, -0.0016,  ...,  0.0021,  0.0099,  0.0114],
        [-0.0092, -0.0093,  0.0151,  ..., -0.0027, -0.0033,  0.0053]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.0176,  0.6396, -1.2793,  ..., -0.7378,  0.5249,  2.8574]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 06:11:08 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too stretched, it is overstretched
If something is too used, it is overused
If something is too powered, it is overpowered
If something is too stated, it is overstated
If something is too cooked, it is overcooked
If something is too protected, it is overprotected
If something is too sized, it is oversized
If something is too sold, it is
2024-07-19 06:11:08 root INFO     [order_1_approx] starting weight calculation for If something is too cooked, it is overcooked
If something is too stretched, it is overstretched
If something is too sold, it is oversold
If something is too protected, it is overprotected
If something is too powered, it is overpowered
If something is too used, it is overused
If something is too stated, it is overstated
If something is too sized, it is
2024-07-19 06:11:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 06:15:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2410,  0.7114, -0.5771,  ..., -0.2028, -0.0234, -0.4722],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.5781, -2.0723,  3.2266,  ..., -2.4043,  2.0449,  1.6133],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0101, -0.0158, -0.0152,  ..., -0.0009, -0.0155,  0.0048],
        [-0.0047,  0.0035,  0.0056,  ..., -0.0076,  0.0059, -0.0205],
        [-0.0056,  0.0133,  0.0043,  ..., -0.0070, -0.0055,  0.0020],
        ...,
        [-0.0076, -0.0097, -0.0121,  ..., -0.0128,  0.0059, -0.0079],
        [-0.0268, -0.0214,  0.0109,  ..., -0.0071, -0.0502, -0.0236],
        [ 0.0024,  0.0037, -0.0036,  ..., -0.0320,  0.0124,  0.0061]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1201, -1.6455,  3.7344,  ..., -1.7344,  1.5850,  0.9839]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 06:15:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too cooked, it is overcooked
If something is too stretched, it is overstretched
If something is too sold, it is oversold
If something is too protected, it is overprotected
If something is too powered, it is overpowered
If something is too used, it is overused
If something is too stated, it is overstated
If something is too sized, it is
2024-07-19 06:15:28 root INFO     [order_1_approx] starting weight calculation for If something is too protected, it is overprotected
If something is too cooked, it is overcooked
If something is too used, it is overused
If something is too sized, it is oversized
If something is too sold, it is oversold
If something is too powered, it is overpowered
If something is too stated, it is overstated
If something is too stretched, it is
2024-07-19 06:15:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 06:19:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2522,  0.3596,  0.6621,  ...,  0.4663, -0.2358, -1.0322],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4473,  0.3162, -1.6318,  ...,  1.0039, -0.3564,  2.6367],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0176, -0.0031, -0.0117,  ..., -0.0001,  0.0032, -0.0101],
        [-0.0105, -0.0146,  0.0087,  ..., -0.0038, -0.0089, -0.0215],
        [-0.0066,  0.0135, -0.0165,  ..., -0.0013,  0.0244,  0.0202],
        ...,
        [ 0.0016, -0.0171, -0.0101,  ..., -0.0039, -0.0328, -0.0106],
        [ 0.0090, -0.0184, -0.0177,  ..., -0.0148, -0.0138,  0.0201],
        [-0.0135,  0.0139, -0.0046,  ..., -0.0195,  0.0020, -0.0108]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9619, -0.5322, -1.6406,  ...,  1.2148, -0.4260,  2.5801]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 06:19:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too protected, it is overprotected
If something is too cooked, it is overcooked
If something is too used, it is overused
If something is too sized, it is oversized
If something is too sold, it is oversold
If something is too powered, it is overpowered
If something is too stated, it is overstated
If something is too stretched, it is
2024-07-19 06:19:48 root INFO     [order_1_approx] starting weight calculation for If something is too used, it is overused
If something is too cooked, it is overcooked
If something is too sized, it is oversized
If something is too stated, it is overstated
If something is too protected, it is overprotected
If something is too sold, it is oversold
If something is too stretched, it is overstretched
If something is too powered, it is
2024-07-19 06:19:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 06:24:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0051,  0.6782,  0.4111,  ...,  0.2720, -0.0151,  0.1479],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.0059, -2.4629, -0.2314,  ..., -0.5205,  0.3149,  2.9727],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.5520e-03, -1.3985e-02,  1.8606e-03,  ...,  8.4305e-03,
         -2.3361e-02, -1.7380e-02],
        [-2.4384e-02,  1.1322e-02,  9.4070e-03,  ...,  4.3793e-03,
          9.4452e-03, -1.2657e-02],
        [-1.0529e-02,  3.8147e-05, -3.9749e-03,  ...,  6.1340e-03,
          4.0665e-03, -3.7060e-03],
        ...,
        [-4.7264e-03, -3.4027e-02, -1.4801e-03,  ...,  2.1172e-03,
         -8.4076e-03,  2.1072e-02],
        [-1.2985e-02, -2.2369e-02,  4.8676e-03,  ..., -8.8043e-03,
          2.3117e-03,  1.3390e-03],
        [ 8.2350e-04,  1.8661e-02,  1.3390e-03,  ..., -7.0457e-03,
          1.9398e-03, -1.0101e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.0156, -2.4551, -0.2571,  ..., -1.3125, -0.6562,  2.8242]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 06:24:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too used, it is overused
If something is too cooked, it is overcooked
If something is too sized, it is oversized
If something is too stated, it is overstated
If something is too protected, it is overprotected
If something is too sold, it is oversold
If something is too stretched, it is overstretched
If something is too powered, it is
2024-07-19 06:24:07 root INFO     [order_1_approx] starting weight calculation for If something is too used, it is overused
If something is too stretched, it is overstretched
If something is too powered, it is overpowered
If something is too stated, it is overstated
If something is too protected, it is overprotected
If something is too sized, it is oversized
If something is too sold, it is oversold
If something is too cooked, it is
2024-07-19 06:24:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 06:28:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6045,  0.8291,  0.8506,  ...,  0.0303, -0.7686, -0.2378],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9336, -2.1484, -1.7705,  ..., -2.6602,  4.4180,  0.8408],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.3956e-02,  1.6518e-03, -1.6586e-02,  ...,  9.8877e-03,
         -1.9283e-03, -2.9373e-03],
        [ 7.6141e-03,  4.1962e-03,  4.3335e-03,  ..., -2.0027e-05,
          3.1719e-03, -1.3809e-02],
        [-1.4946e-02,  1.1131e-02,  8.4305e-03,  ...,  6.4621e-03,
          1.2970e-02,  9.1400e-03],
        ...,
        [-3.2082e-03, -4.9667e-03,  3.0594e-03,  ..., -9.2010e-03,
         -4.5166e-03, -1.4503e-02],
        [-6.7329e-03,  1.6327e-02, -9.1400e-03,  ..., -1.1604e-02,
         -1.7349e-02,  1.9684e-03],
        [-4.0741e-03, -1.6724e-02,  8.1024e-03,  ..., -4.0741e-03,
          1.0483e-02,  1.0803e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9790, -2.6191, -1.2090,  ..., -2.6777,  4.6445,  1.6016]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 06:28:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too used, it is overused
If something is too stretched, it is overstretched
If something is too powered, it is overpowered
If something is too stated, it is overstated
If something is too protected, it is overprotected
If something is too sized, it is oversized
If something is too sold, it is oversold
If something is too cooked, it is
2024-07-19 06:28:28 root INFO     [order_1_approx] starting weight calculation for If something is too used, it is overused
If something is too powered, it is overpowered
If something is too sized, it is oversized
If something is too protected, it is overprotected
If something is too cooked, it is overcooked
If something is too sold, it is oversold
If something is too stretched, it is overstretched
If something is too stated, it is
2024-07-19 06:28:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 06:32:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7769,  0.3213,  0.0922,  ...,  0.1616, -0.3865, -0.4541],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3398, -1.8555,  0.8867,  ..., -0.2910,  1.8701,  5.2500],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0174, -0.0038,  0.0171,  ...,  0.0269, -0.0261, -0.0117],
        [-0.0010, -0.0140, -0.0022,  ...,  0.0106, -0.0016, -0.0070],
        [-0.0095,  0.0181,  0.0054,  ...,  0.0024, -0.0117, -0.0058],
        ...,
        [ 0.0105, -0.0083,  0.0052,  ..., -0.0139, -0.0101,  0.0025],
        [-0.0123,  0.0242, -0.0222,  ..., -0.0462, -0.0332,  0.0024],
        [-0.0043, -0.0133,  0.0157,  ..., -0.0010, -0.0068, -0.0136]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3643,  0.0059,  0.8970,  ..., -0.5283,  1.1270,  4.8281]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 06:32:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too used, it is overused
If something is too powered, it is overpowered
If something is too sized, it is oversized
If something is too protected, it is overprotected
If something is too cooked, it is overcooked
If something is too sold, it is oversold
If something is too stretched, it is overstretched
If something is too stated, it is
2024-07-19 06:32:46 root INFO     [order_1_approx] starting weight calculation for If something is too stretched, it is overstretched
If something is too sold, it is oversold
If something is too powered, it is overpowered
If something is too protected, it is overprotected
If something is too cooked, it is overcooked
If something is too sized, it is oversized
If something is too stated, it is overstated
If something is too used, it is
2024-07-19 06:32:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 06:37:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5439,  0.5459,  1.3076,  ..., -0.3140, -0.4934, -0.3809],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.1367, -1.5742, -0.9746,  ..., -1.5430,  0.3677,  3.0078],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0122, -0.0247, -0.0135,  ...,  0.0212,  0.0383, -0.0114],
        [-0.0080, -0.0125, -0.0013,  ..., -0.0125,  0.0170, -0.0017],
        [-0.0173,  0.0116,  0.0300,  ..., -0.0021, -0.0253,  0.0252],
        ...,
        [ 0.0054, -0.0047, -0.0059,  ..., -0.0223,  0.0150,  0.0046],
        [-0.0076, -0.0351, -0.0177,  ..., -0.0052, -0.0036, -0.0095],
        [ 0.0060, -0.0067,  0.0050,  ..., -0.0047,  0.0178,  0.0103]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.4004, -1.4062, -1.5859,  ..., -1.3164,  0.6997,  2.8066]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 06:37:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If something is too stretched, it is overstretched
If something is too sold, it is oversold
If something is too powered, it is overpowered
If something is too protected, it is overprotected
If something is too cooked, it is overcooked
If something is too sized, it is oversized
If something is too stated, it is overstated
If something is too used, it is
2024-07-19 06:37:07 root INFO     total operator prediction time: 2077.004764318466 seconds
2024-07-19 06:37:07 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on adj+ly_reg
2024-07-19 06:37:07 root INFO     building operator adj+ly_reg
2024-07-19 06:37:07 root INFO     [order_1_approx] starting weight calculation for The adjective form of historical is historically
The adjective form of serious is seriously
The adjective form of unique is uniquely
The adjective form of interesting is interestingly
The adjective form of political is politically
The adjective form of similar is similarly
The adjective form of according is accordingly
The adjective form of significant is
2024-07-19 06:37:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 06:41:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2961,  0.6968,  0.0598,  ..., -0.2766, -0.1462, -0.3936],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.5586,  2.6211,  4.1602,  ..., -0.1537, -2.2422, -2.1406],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0505, -0.0161, -0.0082,  ...,  0.0029, -0.0058, -0.0326],
        [-0.0287, -0.0246,  0.0111,  ...,  0.0045, -0.0102,  0.0001],
        [ 0.0192, -0.0410, -0.0131,  ...,  0.0196,  0.0326, -0.0010],
        ...,
        [ 0.0023, -0.0284,  0.0076,  ..., -0.0179, -0.0235,  0.0080],
        [-0.0016,  0.0175,  0.0027,  ..., -0.0054, -0.0299,  0.0008],
        [ 0.0304,  0.0103,  0.0057,  ...,  0.0019,  0.0208, -0.0102]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.7891,  3.0273,  3.4844,  ..., -0.9849, -3.5234, -0.5977]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 06:41:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of historical is historically
The adjective form of serious is seriously
The adjective form of unique is uniquely
The adjective form of interesting is interestingly
The adjective form of political is politically
The adjective form of similar is similarly
The adjective form of according is accordingly
The adjective form of significant is
2024-07-19 06:41:28 root INFO     [order_1_approx] starting weight calculation for The adjective form of serious is seriously
The adjective form of interesting is interestingly
The adjective form of similar is similarly
The adjective form of historical is historically
The adjective form of significant is significantly
The adjective form of unique is uniquely
The adjective form of according is accordingly
The adjective form of political is
2024-07-19 06:41:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 06:45:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5884,  0.4006, -0.3232,  ..., -0.4355, -0.2061, -0.2126],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.1328, -1.4512,  0.6104,  ..., -2.8711, -0.8857, -1.3379],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0054, -0.0181,  0.0267,  ..., -0.0126, -0.0222,  0.0057],
        [-0.0003,  0.0044,  0.0076,  ...,  0.0062,  0.0148, -0.0060],
        [-0.0059, -0.0541, -0.0200,  ...,  0.0049, -0.0016,  0.0202],
        ...,
        [-0.0209,  0.0132, -0.0089,  ..., -0.0087,  0.0263, -0.0070],
        [ 0.0037,  0.0030, -0.0018,  ..., -0.0110, -0.0132, -0.0007],
        [ 0.0075, -0.0072,  0.0326,  ..., -0.0208, -0.0330, -0.0100]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.3945, -2.1289,  0.3120,  ..., -3.1035, -1.3691, -1.6729]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 06:45:48 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of serious is seriously
The adjective form of interesting is interestingly
The adjective form of similar is similarly
The adjective form of historical is historically
The adjective form of significant is significantly
The adjective form of unique is uniquely
The adjective form of according is accordingly
The adjective form of political is
2024-07-19 06:45:48 root INFO     [order_1_approx] starting weight calculation for The adjective form of serious is seriously
The adjective form of significant is significantly
The adjective form of interesting is interestingly
The adjective form of historical is historically
The adjective form of according is accordingly
The adjective form of similar is similarly
The adjective form of political is politically
The adjective form of unique is
2024-07-19 06:45:48 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 06:50:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1512,  0.2341, -0.5532,  ..., -0.0901,  1.3154, -0.4426],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7051,  1.2500,  0.2676,  ..., -0.4692,  3.4531,  2.2891],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0268,  0.0057, -0.0193,  ..., -0.0101, -0.0082,  0.0018],
        [ 0.0068, -0.0182,  0.0157,  ...,  0.0140,  0.0117, -0.0006],
        [-0.0124, -0.0099, -0.0254,  ..., -0.0048, -0.0017,  0.0028],
        ...,
        [ 0.0060,  0.0153, -0.0185,  ..., -0.0066, -0.0028, -0.0078],
        [-0.0114,  0.0133, -0.0096,  ..., -0.0125,  0.0021, -0.0210],
        [ 0.0018,  0.0101,  0.0058,  ...,  0.0066, -0.0069, -0.0107]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.2520,  1.2412,  0.9321,  ..., -0.8340,  2.8457,  2.6836]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 06:50:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of serious is seriously
The adjective form of significant is significantly
The adjective form of interesting is interestingly
The adjective form of historical is historically
The adjective form of according is accordingly
The adjective form of similar is similarly
The adjective form of political is politically
The adjective form of unique is
2024-07-19 06:50:09 root INFO     [order_1_approx] starting weight calculation for The adjective form of interesting is interestingly
The adjective form of serious is seriously
The adjective form of similar is similarly
The adjective form of unique is uniquely
The adjective form of significant is significantly
The adjective form of according is accordingly
The adjective form of political is politically
The adjective form of historical is
2024-07-19 06:50:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 06:54:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3271,  0.7622, -0.0195,  ..., -0.9756, -0.6558, -0.3279],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.0195,  0.0933, -0.5210,  ..., -1.6973,  0.6885,  5.2812],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.7593e-02, -5.2948e-03, -3.6354e-03,  ..., -7.5378e-03,
         -3.8452e-03, -7.8888e-03],
        [ 7.2136e-03, -8.4305e-04,  6.1646e-03,  ..., -1.4297e-02,
         -6.4087e-03, -6.2943e-05],
        [-4.7150e-03, -7.1259e-03, -6.5079e-03,  ..., -1.3542e-04,
          1.1406e-03, -6.8893e-03],
        ...,
        [-3.3813e-02,  8.3008e-03, -1.4908e-02,  ..., -4.9667e-03,
          1.0574e-02, -9.1553e-04],
        [ 3.6469e-03, -1.3474e-02, -4.5700e-03,  ..., -7.2861e-03,
         -5.1537e-03,  2.8572e-03],
        [ 3.1586e-03, -1.2787e-02,  2.5177e-02,  ..., -8.8501e-03,
         -2.2545e-03, -9.9792e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5137, -0.3091, -0.6470,  ..., -1.5801, -1.0479,  4.2227]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 06:54:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of interesting is interestingly
The adjective form of serious is seriously
The adjective form of similar is similarly
The adjective form of unique is uniquely
The adjective form of significant is significantly
The adjective form of according is accordingly
The adjective form of political is politically
The adjective form of historical is
2024-07-19 06:54:30 root INFO     [order_1_approx] starting weight calculation for The adjective form of similar is similarly
The adjective form of according is accordingly
The adjective form of interesting is interestingly
The adjective form of historical is historically
The adjective form of unique is uniquely
The adjective form of political is politically
The adjective form of significant is significantly
The adjective form of serious is
2024-07-19 06:54:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 06:58:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0889,  0.4658,  0.3462,  ..., -0.3904,  0.7710,  0.0635],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.2930,  3.0918,  4.1602,  ...,  0.4966, -1.9404, -2.1719],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0239, -0.0122, -0.0100,  ..., -0.0084, -0.0085, -0.0022],
        [ 0.0213, -0.0246,  0.0131,  ...,  0.0168,  0.0019,  0.0071],
        [ 0.0159,  0.0003,  0.0116,  ..., -0.0021,  0.0119, -0.0121],
        ...,
        [ 0.0106, -0.0223,  0.0014,  ..., -0.0153,  0.0123, -0.0080],
        [-0.0296,  0.0072, -0.0093,  ..., -0.0051, -0.0154,  0.0006],
        [-0.0108,  0.0250,  0.0162,  ..., -0.0360,  0.0181, -0.0399]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.2188,  1.5674,  4.5664,  ...,  0.6201, -1.3262,  0.7520]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 06:58:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of similar is similarly
The adjective form of according is accordingly
The adjective form of interesting is interestingly
The adjective form of historical is historically
The adjective form of unique is uniquely
The adjective form of political is politically
The adjective form of significant is significantly
The adjective form of serious is
2024-07-19 06:58:50 root INFO     [order_1_approx] starting weight calculation for The adjective form of interesting is interestingly
The adjective form of historical is historically
The adjective form of political is politically
The adjective form of according is accordingly
The adjective form of unique is uniquely
The adjective form of serious is seriously
The adjective form of significant is significantly
The adjective form of similar is
2024-07-19 06:58:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 07:03:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4102,  0.6108,  0.3940,  ..., -0.0096,  0.0254, -0.9844],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2932,  1.5127, -0.2783,  ...,  2.5234, -2.7188, -1.3320],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0233, -0.0190, -0.0073,  ..., -0.0233, -0.0074, -0.0348],
        [-0.0047, -0.0171,  0.0019,  ...,  0.0011, -0.0104, -0.0007],
        [ 0.0038, -0.0222, -0.0422,  ..., -0.0050,  0.0213, -0.0007],
        ...,
        [-0.0078, -0.0075,  0.0032,  ..., -0.0135, -0.0063,  0.0082],
        [ 0.0215, -0.0134,  0.0073,  ..., -0.0041, -0.0466,  0.0172],
        [ 0.0260, -0.0035,  0.0070,  ..., -0.0005,  0.0176, -0.0369]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3967,  1.6699,  0.3433,  ...,  1.9707, -3.5801, -0.6255]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 07:03:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of interesting is interestingly
The adjective form of historical is historically
The adjective form of political is politically
The adjective form of according is accordingly
The adjective form of unique is uniquely
The adjective form of serious is seriously
The adjective form of significant is significantly
The adjective form of similar is
2024-07-19 07:03:11 root INFO     [order_1_approx] starting weight calculation for The adjective form of similar is similarly
The adjective form of significant is significantly
The adjective form of political is politically
The adjective form of historical is historically
The adjective form of interesting is interestingly
The adjective form of unique is uniquely
The adjective form of serious is seriously
The adjective form of according is
2024-07-19 07:03:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 07:07:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1312,  0.3950, -0.4985,  ..., -0.1624,  0.0135,  0.4758],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.4414, -0.0956,  1.5625,  ..., -2.0371, -0.7446, -3.1602],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0338, -0.0163,  0.0575,  ..., -0.0164, -0.0241,  0.0091],
        [ 0.0087,  0.0021,  0.0092,  ...,  0.0042,  0.0153, -0.0104],
        [-0.0491, -0.0387,  0.0513,  ...,  0.0005, -0.0570, -0.0383],
        ...,
        [-0.0297, -0.0020, -0.0079,  ...,  0.0005, -0.0264,  0.0125],
        [ 0.0482,  0.0391, -0.0035,  ...,  0.0107, -0.0255,  0.0184],
        [ 0.0020, -0.0125,  0.0107,  ...,  0.0154, -0.0598, -0.0358]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.4727, -1.5361,  1.7002,  ..., -0.4727, -1.4395, -1.5303]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 07:07:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of similar is similarly
The adjective form of significant is significantly
The adjective form of political is politically
The adjective form of historical is historically
The adjective form of interesting is interestingly
The adjective form of unique is uniquely
The adjective form of serious is seriously
The adjective form of according is
2024-07-19 07:07:33 root INFO     [order_1_approx] starting weight calculation for The adjective form of serious is seriously
The adjective form of similar is similarly
The adjective form of historical is historically
The adjective form of unique is uniquely
The adjective form of political is politically
The adjective form of significant is significantly
The adjective form of according is accordingly
The adjective form of interesting is
2024-07-19 07:07:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 07:11:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5239,  0.0650,  0.0177,  ...,  0.2659,  0.4895, -0.0436],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([1.1123, 2.9863, 0.6504,  ..., 1.8135, 1.5371, 1.4414], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.2654e-03, -2.2888e-05,  1.2329e-02,  ..., -1.1566e-02,
          1.2024e-02, -3.0060e-02],
        [-2.0065e-02, -1.8707e-02,  9.0790e-03,  ...,  2.3804e-02,
          8.0185e-03,  1.2787e-02],
        [ 2.7969e-02, -7.3624e-03, -2.0264e-02,  ..., -5.8975e-03,
         -2.9793e-03,  8.0872e-04],
        ...,
        [-1.9653e-02,  4.5967e-03, -8.9722e-03,  ..., -2.9938e-02,
          1.3428e-03, -5.3596e-04],
        [-4.5929e-03,  4.8409e-03,  8.4381e-03,  ..., -2.6340e-03,
         -4.2236e-02,  1.3474e-02],
        [ 2.5070e-02,  1.5915e-02, -3.0079e-03,  ..., -2.6276e-02,
         -6.9427e-03, -3.1952e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.8735,  2.3496, -0.2109,  ...,  1.8164,  1.5332,  1.9551]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 07:11:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of serious is seriously
The adjective form of similar is similarly
The adjective form of historical is historically
The adjective form of unique is uniquely
The adjective form of political is politically
The adjective form of significant is significantly
The adjective form of according is accordingly
The adjective form of interesting is
2024-07-19 07:11:54 root INFO     total operator prediction time: 2086.9148645401 seconds
2024-07-19 07:11:54 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+tion_irreg
2024-07-19 07:11:54 root INFO     building operator verb+tion_irreg
2024-07-19 07:11:54 root INFO     [order_1_approx] starting weight calculation for To modernize results in modernization
To install results in installation
To privatize results in privatization
To observe results in observation
To occupy results in occupation
To organize results in organization
To customize results in customization
To imagine results in
2024-07-19 07:11:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 07:16:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0728, -0.3569, -0.6680,  ...,  0.5513, -0.1787, -0.3027],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9287,  1.9375, -4.9766,  ...,  0.1655, -3.8301,  1.4609],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0066, -0.0327, -0.0289,  ...,  0.0018,  0.0234,  0.0033],
        [ 0.0049,  0.0088,  0.0011,  ..., -0.0107, -0.0009,  0.0013],
        [ 0.0169,  0.0072, -0.0040,  ...,  0.0051,  0.0063, -0.0002],
        ...,
        [-0.0275, -0.0132,  0.0004,  ..., -0.0152,  0.0017, -0.0081],
        [ 0.0102,  0.0024,  0.0082,  ..., -0.0150, -0.0314,  0.0107],
        [-0.0120,  0.0044, -0.0049,  ..., -0.0027,  0.0184, -0.0086]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.2695,  1.8203, -4.6992,  ..., -0.8569, -4.2070,  1.6348]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 07:16:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To modernize results in modernization
To install results in installation
To privatize results in privatization
To observe results in observation
To occupy results in occupation
To organize results in organization
To customize results in customization
To imagine results in
2024-07-19 07:16:14 root INFO     [order_1_approx] starting weight calculation for To privatize results in privatization
To install results in installation
To modernize results in modernization
To imagine results in imagination
To occupy results in occupation
To customize results in customization
To observe results in observation
To organize results in
2024-07-19 07:16:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 07:20:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2744, -0.0192, -0.3972,  ..., -0.2130,  0.3843,  0.0032],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8223,  0.0718, -0.1924,  ..., -0.2192, -1.3438,  4.3750],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0021, -0.0315,  0.0075,  ..., -0.0095, -0.0024, -0.0111],
        [ 0.0061, -0.0012, -0.0052,  ...,  0.0228,  0.0033,  0.0055],
        [ 0.0158, -0.0038, -0.0076,  ..., -0.0101,  0.0118, -0.0018],
        ...,
        [ 0.0146,  0.0005, -0.0197,  ..., -0.0127, -0.0146, -0.0110],
        [ 0.0193,  0.0018,  0.0015,  ..., -0.0215, -0.0104, -0.0111],
        [ 0.0151,  0.0088, -0.0185,  ...,  0.0047,  0.0105,  0.0045]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9106,  0.7192,  0.6240,  ...,  0.0632, -1.8223,  5.2734]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 07:20:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To privatize results in privatization
To install results in installation
To modernize results in modernization
To imagine results in imagination
To occupy results in occupation
To customize results in customization
To observe results in observation
To organize results in
2024-07-19 07:20:33 root INFO     [order_1_approx] starting weight calculation for To modernize results in modernization
To install results in installation
To privatize results in privatization
To organize results in organization
To imagine results in imagination
To occupy results in occupation
To observe results in observation
To customize results in
2024-07-19 07:20:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 07:24:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2061, -0.4292, -0.5986,  ...,  0.5708, -0.3809, -0.2048],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1426, -0.1790, -3.8477,  ..., -3.3281, -0.6875,  1.7637],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0011, -0.0105, -0.0115,  ...,  0.0047, -0.0124,  0.0020],
        [-0.0111, -0.0013, -0.0133,  ...,  0.0199, -0.0182,  0.0020],
        [ 0.0047, -0.0040, -0.0031,  ..., -0.0060,  0.0002, -0.0086],
        ...,
        [-0.0048, -0.0058, -0.0136,  ..., -0.0016, -0.0007,  0.0078],
        [ 0.0098,  0.0264, -0.0105,  ..., -0.0117,  0.0125, -0.0214],
        [-0.0026,  0.0070,  0.0052,  ..., -0.0094, -0.0009,  0.0022]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.1602, -0.7451, -3.9219,  ..., -3.4883, -1.6426,  1.5537]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 07:24:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To modernize results in modernization
To install results in installation
To privatize results in privatization
To organize results in organization
To imagine results in imagination
To occupy results in occupation
To observe results in observation
To customize results in
2024-07-19 07:24:53 root INFO     [order_1_approx] starting weight calculation for To imagine results in imagination
To customize results in customization
To modernize results in modernization
To organize results in organization
To observe results in observation
To privatize results in privatization
To occupy results in occupation
To install results in
2024-07-19 07:24:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 07:29:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0997,  0.0833, -0.0692,  ...,  0.2886,  0.0364, -0.2578],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.6455,  1.6426,  0.1631,  ..., -2.5625,  1.3184,  8.0938],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0013, -0.0159, -0.0030,  ...,  0.0057, -0.0091, -0.0093],
        [ 0.0032,  0.0150, -0.0055,  ...,  0.0102, -0.0036,  0.0068],
        [-0.0057,  0.0062,  0.0063,  ..., -0.0017, -0.0106, -0.0067],
        ...,
        [-0.0158,  0.0025, -0.0036,  ..., -0.0069,  0.0144,  0.0157],
        [ 0.0017,  0.0041,  0.0174,  ..., -0.0203, -0.0035, -0.0049],
        [ 0.0028,  0.0162,  0.0073,  ...,  0.0125, -0.0026, -0.0070]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1753,  0.6650,  0.0443,  ..., -3.2109,  1.5303,  7.2148]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 07:29:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To imagine results in imagination
To customize results in customization
To modernize results in modernization
To organize results in organization
To observe results in observation
To privatize results in privatization
To occupy results in occupation
To install results in
2024-07-19 07:29:13 root INFO     [order_1_approx] starting weight calculation for To occupy results in occupation
To install results in installation
To customize results in customization
To privatize results in privatization
To organize results in organization
To imagine results in imagination
To observe results in observation
To modernize results in
2024-07-19 07:29:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 07:33:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0352, -0.0713,  1.1602,  ..., -0.4231,  0.2351,  0.1597],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3125,  1.0967, -5.1836,  ...,  0.8711, -1.6270,  4.1641],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0142, -0.0148,  0.0084,  ...,  0.0151, -0.0003,  0.0133],
        [ 0.0037,  0.0042,  0.0003,  ..., -0.0017,  0.0020,  0.0032],
        [ 0.0088,  0.0112, -0.0153,  ..., -0.0093,  0.0100,  0.0004],
        ...,
        [-0.0247, -0.0056,  0.0053,  ...,  0.0058, -0.0107,  0.0135],
        [ 0.0067,  0.0079, -0.0056,  ..., -0.0105,  0.0063, -0.0066],
        [-0.0200, -0.0160,  0.0103,  ..., -0.0023, -0.0009,  0.0042]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.5723,  1.4014, -5.0312,  ...,  0.4099, -1.9766,  3.2617]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 07:33:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To occupy results in occupation
To install results in installation
To customize results in customization
To privatize results in privatization
To organize results in organization
To imagine results in imagination
To observe results in observation
To modernize results in
2024-07-19 07:33:31 root INFO     [order_1_approx] starting weight calculation for To privatize results in privatization
To occupy results in occupation
To customize results in customization
To organize results in organization
To imagine results in imagination
To install results in installation
To modernize results in modernization
To observe results in
2024-07-19 07:33:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 07:37:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2866, -0.0314, -0.4031,  ..., -0.8843, -0.2686,  0.3147],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.9727,  2.9668, -4.6758,  ..., -1.4990, -2.4551,  1.7891],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0056, -0.0030, -0.0044,  ...,  0.0132, -0.0157, -0.0033],
        [-0.0091,  0.0036, -0.0033,  ...,  0.0113,  0.0022,  0.0049],
        [ 0.0002,  0.0007, -0.0209,  ...,  0.0015,  0.0182, -0.0014],
        ...,
        [-0.0126, -0.0119, -0.0207,  ..., -0.0155,  0.0094, -0.0054],
        [ 0.0314,  0.0154,  0.0050,  ...,  0.0019, -0.0269,  0.0309],
        [-0.0244,  0.0192,  0.0037,  ...,  0.0101,  0.0099, -0.0120]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.8730,  2.9336, -4.5078,  ..., -1.3516, -0.9893,  2.3477]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 07:37:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To privatize results in privatization
To occupy results in occupation
To customize results in customization
To organize results in organization
To imagine results in imagination
To install results in installation
To modernize results in modernization
To observe results in
2024-07-19 07:37:50 root INFO     [order_1_approx] starting weight calculation for To modernize results in modernization
To organize results in organization
To observe results in observation
To customize results in customization
To imagine results in imagination
To install results in installation
To privatize results in privatization
To occupy results in
2024-07-19 07:37:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 07:42:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2402, -0.0836,  0.3723,  ..., -0.4282, -0.1428, -0.3511],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.3203,  4.3789, -1.3125,  ..., -3.7031,  1.3320,  3.5820],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0073, -0.0111, -0.0029,  ...,  0.0143, -0.0076, -0.0008],
        [ 0.0066, -0.0132,  0.0089,  ...,  0.0142, -0.0087,  0.0120],
        [-0.0060, -0.0001, -0.0026,  ..., -0.0021,  0.0032, -0.0085],
        ...,
        [-0.0205, -0.0073, -0.0113,  ..., -0.0018, -0.0029,  0.0019],
        [ 0.0026,  0.0165, -0.0111,  ...,  0.0018, -0.0151, -0.0051],
        [-0.0086,  0.0149, -0.0083,  ..., -0.0064,  0.0093, -0.0114]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4785,  4.5781, -1.5488,  ..., -4.5898,  1.4746,  3.4824]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 07:42:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To modernize results in modernization
To organize results in organization
To observe results in observation
To customize results in customization
To imagine results in imagination
To install results in installation
To privatize results in privatization
To occupy results in
2024-07-19 07:42:07 root INFO     [order_1_approx] starting weight calculation for To install results in installation
To modernize results in modernization
To customize results in customization
To occupy results in occupation
To organize results in organization
To imagine results in imagination
To observe results in observation
To privatize results in
2024-07-19 07:42:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 07:46:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0247, -0.0561, -0.2274,  ...,  0.2666, -0.1487,  0.1956],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6768,  0.6328, -0.9160,  ...,  0.1001, -3.8379,  0.2900],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0075, -0.0106,  0.0008,  ...,  0.0070, -0.0143,  0.0094],
        [ 0.0036,  0.0046,  0.0127,  ...,  0.0160, -0.0028,  0.0133],
        [ 0.0047,  0.0063, -0.0218,  ..., -0.0090,  0.0104,  0.0054],
        ...,
        [-0.0243,  0.0000, -0.0179,  ...,  0.0070, -0.0066, -0.0046],
        [ 0.0200,  0.0020, -0.0007,  ..., -0.0041,  0.0007,  0.0025],
        [ 0.0055, -0.0083,  0.0174,  ..., -0.0132,  0.0050, -0.0033]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0869,  1.3633, -0.2339,  ..., -0.6133, -4.0000, -0.1404]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 07:46:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To install results in installation
To modernize results in modernization
To customize results in customization
To occupy results in occupation
To organize results in organization
To imagine results in imagination
To observe results in observation
To privatize results in
2024-07-19 07:46:26 root INFO     total operator prediction time: 2072.5726737976074 seconds
2024-07-19 07:46:26 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+able_reg
2024-07-19 07:46:26 root INFO     building operator verb+able_reg
2024-07-19 07:46:26 root INFO     [order_1_approx] starting weight calculation for If you can adore something, that thing is adorable
If you can discover something, that thing is discoverable
If you can renew something, that thing is renewable
If you can download something, that thing is downloadable
If you can expand something, that thing is expandable
If you can inflate something, that thing is inflatable
If you can adjust something, that thing is adjustable
If you can explain something, that thing is
2024-07-19 07:46:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 07:50:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1345,  0.7334,  0.5742,  ..., -0.1173, -0.2783, -0.2366],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.9141,  1.4180, -4.9219,  ...,  0.3286,  4.0898,  1.0488],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0107, -0.0065,  0.0202,  ...,  0.0171, -0.0215, -0.0016],
        [-0.0138,  0.0012, -0.0011,  ..., -0.0033, -0.0038, -0.0112],
        [ 0.0110,  0.0003, -0.0143,  ..., -0.0129,  0.0092, -0.0055],
        ...,
        [ 0.0193,  0.0030,  0.0072,  ...,  0.0079,  0.0036,  0.0101],
        [ 0.0028,  0.0184, -0.0255,  ..., -0.0026, -0.0172,  0.0125],
        [-0.0029, -0.0045,  0.0071,  ..., -0.0127,  0.0085, -0.0153]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.2266,  2.3301, -3.8770,  ..., -0.1646,  4.0117,  0.4312]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 07:50:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can adore something, that thing is adorable
If you can discover something, that thing is discoverable
If you can renew something, that thing is renewable
If you can download something, that thing is downloadable
If you can expand something, that thing is expandable
If you can inflate something, that thing is inflatable
If you can adjust something, that thing is adjustable
If you can explain something, that thing is
2024-07-19 07:50:47 root INFO     [order_1_approx] starting weight calculation for If you can adore something, that thing is adorable
If you can inflate something, that thing is inflatable
If you can explain something, that thing is explainable
If you can adjust something, that thing is adjustable
If you can download something, that thing is downloadable
If you can expand something, that thing is expandable
If you can renew something, that thing is renewable
If you can discover something, that thing is
2024-07-19 07:50:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 07:55:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3696,  0.6899, -0.6621,  ...,  0.0846, -0.1724, -0.1816],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.8555,  1.3066, -2.8984,  ..., -1.5732,  0.9897,  2.5410],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0019, -0.0133,  0.0056,  ...,  0.0029, -0.0241,  0.0015],
        [-0.0293, -0.0181,  0.0085,  ..., -0.0054,  0.0235,  0.0085],
        [ 0.0130,  0.0046, -0.0170,  ..., -0.0043,  0.0165, -0.0145],
        ...,
        [ 0.0045,  0.0126,  0.0063,  ...,  0.0034, -0.0127, -0.0062],
        [ 0.0107,  0.0202,  0.0059,  ...,  0.0019,  0.0074,  0.0085],
        [-0.0179, -0.0141, -0.0005,  ..., -0.0109,  0.0255, -0.0120]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.4102,  2.2520, -2.4258,  ..., -2.1270,  0.8564,  2.3730]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 07:55:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can adore something, that thing is adorable
If you can inflate something, that thing is inflatable
If you can explain something, that thing is explainable
If you can adjust something, that thing is adjustable
If you can download something, that thing is downloadable
If you can expand something, that thing is expandable
If you can renew something, that thing is renewable
If you can discover something, that thing is
2024-07-19 07:55:06 root INFO     [order_1_approx] starting weight calculation for If you can download something, that thing is downloadable
If you can adjust something, that thing is adjustable
If you can renew something, that thing is renewable
If you can explain something, that thing is explainable
If you can expand something, that thing is expandable
If you can discover something, that thing is discoverable
If you can adore something, that thing is adorable
If you can inflate something, that thing is
2024-07-19 07:55:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 07:59:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4937,  0.6465,  0.9702,  ..., -0.0273,  0.2493,  0.2263],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7236,  1.7617, -5.3594,  ..., -0.1635,  3.0586, -0.0811],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.9945e-04, -4.5013e-04, -7.2746e-03,  ...,  1.4267e-03,
         -1.3359e-02, -6.1264e-03],
        [-1.8616e-02,  1.0002e-02,  7.0572e-04,  ...,  6.2981e-03,
          3.5858e-04, -2.3041e-02],
        [ 2.8778e-02, -4.9858e-03,  5.2414e-03,  ..., -8.7433e-03,
         -1.9302e-03,  3.7804e-03],
        ...,
        [-1.0567e-02, -3.9825e-03,  1.5259e-05,  ..., -1.6098e-03,
         -1.4328e-02,  1.4725e-02],
        [-1.8463e-02,  7.3853e-03,  1.4709e-02,  ...,  2.7809e-03,
         -6.4316e-03, -1.7090e-02],
        [-9.4833e-03, -3.2349e-03, -1.0040e-02,  ..., -1.2177e-02,
         -3.0079e-03, -6.9847e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4600,  1.4688, -5.2461,  ..., -0.8530,  2.9297,  0.1809]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 07:59:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can download something, that thing is downloadable
If you can adjust something, that thing is adjustable
If you can renew something, that thing is renewable
If you can explain something, that thing is explainable
If you can expand something, that thing is expandable
If you can discover something, that thing is discoverable
If you can adore something, that thing is adorable
If you can inflate something, that thing is
2024-07-19 07:59:25 root INFO     [order_1_approx] starting weight calculation for If you can download something, that thing is downloadable
If you can discover something, that thing is discoverable
If you can explain something, that thing is explainable
If you can adore something, that thing is adorable
If you can renew something, that thing is renewable
If you can adjust something, that thing is adjustable
If you can inflate something, that thing is inflatable
If you can expand something, that thing is
2024-07-19 07:59:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 08:03:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0814,  0.9717,  0.0708,  ..., -0.0088, -0.1780, -0.3538],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6680, -0.3616, -3.9648,  ..., -1.1611,  1.3633,  0.0225],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0073, -0.0088,  0.0025,  ...,  0.0038,  0.0017, -0.0031],
        [-0.0022, -0.0071,  0.0086,  ...,  0.0055, -0.0155, -0.0103],
        [ 0.0192,  0.0166, -0.0113,  ..., -0.0145,  0.0020,  0.0148],
        ...,
        [-0.0121, -0.0004, -0.0025,  ..., -0.0079,  0.0060, -0.0072],
        [ 0.0124, -0.0047, -0.0150,  ..., -0.0126,  0.0122,  0.0019],
        [ 0.0028, -0.0371,  0.0038,  ..., -0.0013,  0.0065, -0.0128]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.3198, -0.7812, -4.2656,  ..., -2.5957,  1.3828, -0.4382]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 08:03:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can download something, that thing is downloadable
If you can discover something, that thing is discoverable
If you can explain something, that thing is explainable
If you can adore something, that thing is adorable
If you can renew something, that thing is renewable
If you can adjust something, that thing is adjustable
If you can inflate something, that thing is inflatable
If you can expand something, that thing is
2024-07-19 08:03:45 root INFO     [order_1_approx] starting weight calculation for If you can download something, that thing is downloadable
If you can discover something, that thing is discoverable
If you can expand something, that thing is expandable
If you can renew something, that thing is renewable
If you can adore something, that thing is adorable
If you can explain something, that thing is explainable
If you can inflate something, that thing is inflatable
If you can adjust something, that thing is
2024-07-19 08:03:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 08:08:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3335,  0.8706,  0.4414,  ...,  0.1528, -0.3125, -0.1707],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7705,  5.2617, -3.3145,  ..., -2.1543,  4.1055,  2.0293],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0054, -0.0015,  0.0111,  ..., -0.0043, -0.0140, -0.0058],
        [-0.0025, -0.0141,  0.0010,  ...,  0.0016, -0.0185, -0.0166],
        [ 0.0025,  0.0269, -0.0034,  ...,  0.0340, -0.0087, -0.0011],
        ...,
        [-0.0208, -0.0084,  0.0030,  ..., -0.0184,  0.0060,  0.0090],
        [-0.0113,  0.0058,  0.0006,  ..., -0.0339, -0.0145, -0.0022],
        [ 0.0048,  0.0012, -0.0190,  ...,  0.0050,  0.0019, -0.0229]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0566,  4.7734, -2.2012,  ..., -1.9248,  3.5098,  1.3398]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 08:08:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can download something, that thing is downloadable
If you can discover something, that thing is discoverable
If you can expand something, that thing is expandable
If you can renew something, that thing is renewable
If you can adore something, that thing is adorable
If you can explain something, that thing is explainable
If you can inflate something, that thing is inflatable
If you can adjust something, that thing is
2024-07-19 08:08:04 root INFO     [order_1_approx] starting weight calculation for If you can inflate something, that thing is inflatable
If you can adore something, that thing is adorable
If you can renew something, that thing is renewable
If you can adjust something, that thing is adjustable
If you can discover something, that thing is discoverable
If you can explain something, that thing is explainable
If you can expand something, that thing is expandable
If you can download something, that thing is
2024-07-19 08:08:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 08:12:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4033,  0.7163, -0.4314,  ...,  0.6982,  0.1752,  0.4307],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.4824,  1.6289, -3.4492,  ..., -3.4199,  3.8125,  1.1523],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0317, -0.0005,  0.0031,  ...,  0.0018, -0.0076, -0.0032],
        [-0.0073, -0.0011,  0.0186,  ...,  0.0042, -0.0073,  0.0260],
        [ 0.0156,  0.0080, -0.0018,  ..., -0.0003, -0.0131, -0.0018],
        ...,
        [ 0.0001,  0.0075,  0.0083,  ..., -0.0214, -0.0081,  0.0376],
        [ 0.0124,  0.0106,  0.0071,  ..., -0.0049, -0.0291, -0.0054],
        [ 0.0003, -0.0036, -0.0021,  ..., -0.0077,  0.0048, -0.0225]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.5391,  1.0312, -3.5898,  ..., -2.8164,  2.8438,  0.2100]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 08:12:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can inflate something, that thing is inflatable
If you can adore something, that thing is adorable
If you can renew something, that thing is renewable
If you can adjust something, that thing is adjustable
If you can discover something, that thing is discoverable
If you can explain something, that thing is explainable
If you can expand something, that thing is expandable
If you can download something, that thing is
2024-07-19 08:12:23 root INFO     [order_1_approx] starting weight calculation for If you can discover something, that thing is discoverable
If you can inflate something, that thing is inflatable
If you can expand something, that thing is expandable
If you can adjust something, that thing is adjustable
If you can explain something, that thing is explainable
If you can download something, that thing is downloadable
If you can renew something, that thing is renewable
If you can adore something, that thing is
2024-07-19 08:12:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 08:16:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2075, -0.1724, -0.0229,  ..., -0.4614, -0.2233, -0.2285],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.5020,  3.3008, -0.6157,  ..., -3.0742,  5.4375,  6.3047],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0244,  0.0089,  0.0245,  ...,  0.0130,  0.0010, -0.0094],
        [-0.0251, -0.0312, -0.0023,  ..., -0.0174,  0.0320,  0.0038],
        [ 0.0160,  0.0161, -0.0089,  ...,  0.0128,  0.0127,  0.0084],
        ...,
        [-0.0061,  0.0036, -0.0187,  ..., -0.0331,  0.0006, -0.0060],
        [-0.0283, -0.0041, -0.0042,  ..., -0.0092, -0.0214,  0.0196],
        [-0.0158,  0.0029, -0.0082,  ..., -0.0049,  0.0383, -0.0199]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.7812,  3.6660,  0.2471,  ..., -2.9980,  4.6406,  5.5859]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 08:16:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can discover something, that thing is discoverable
If you can inflate something, that thing is inflatable
If you can expand something, that thing is expandable
If you can adjust something, that thing is adjustable
If you can explain something, that thing is explainable
If you can download something, that thing is downloadable
If you can renew something, that thing is renewable
If you can adore something, that thing is
2024-07-19 08:16:41 root INFO     [order_1_approx] starting weight calculation for If you can discover something, that thing is discoverable
If you can inflate something, that thing is inflatable
If you can expand something, that thing is expandable
If you can adore something, that thing is adorable
If you can download something, that thing is downloadable
If you can explain something, that thing is explainable
If you can adjust something, that thing is adjustable
If you can renew something, that thing is
2024-07-19 08:16:41 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 08:20:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3374,  0.4644,  0.4563,  ..., -0.3418,  0.0895,  0.2483],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5312, -0.5269, -8.2812,  ...,  1.5918,  1.5664,  2.2422],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0179, -0.0074,  0.0013,  ...,  0.0072,  0.0027,  0.0106],
        [ 0.0011, -0.0175,  0.0088,  ...,  0.0062,  0.0082,  0.0111],
        [ 0.0074,  0.0030, -0.0094,  ..., -0.0103,  0.0128, -0.0116],
        ...,
        [-0.0152,  0.0157,  0.0081,  ..., -0.0077, -0.0093, -0.0014],
        [-0.0086, -0.0071,  0.0025,  ..., -0.0076, -0.0037,  0.0236],
        [ 0.0025, -0.0025,  0.0117,  ..., -0.0202, -0.0049, -0.0048]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1094,  0.0391, -7.9648,  ...,  1.4521,  0.6787,  1.7812]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 08:21:00 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for If you can discover something, that thing is discoverable
If you can inflate something, that thing is inflatable
If you can expand something, that thing is expandable
If you can adore something, that thing is adorable
If you can download something, that thing is downloadable
If you can explain something, that thing is explainable
If you can adjust something, that thing is adjustable
If you can renew something, that thing is
2024-07-19 08:21:00 root INFO     total operator prediction time: 2073.918764591217 seconds
2024-07-19 08:21:00 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on un+adj_reg
2024-07-19 08:21:00 root INFO     building operator un+adj_reg
2024-07-19 08:21:00 root INFO     [order_1_approx] starting weight calculation for The opposite of lawful is unlawful
The opposite of pleasant is unpleasant
The opposite of controlled is uncontrolled
The opposite of identified is unidentified
The opposite of available is unavailable
The opposite of intended is unintended
The opposite of related is unrelated
The opposite of comfortable is
2024-07-19 08:21:00 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 08:25:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0662,  0.5000, -0.9307,  ..., -0.3455,  0.1729,  0.2952],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.9570,  0.4033, -2.9844,  ..., -0.3752,  3.8613,  3.1387],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0022,  0.0044, -0.0103,  ...,  0.0022,  0.0019, -0.0182],
        [-0.0197, -0.0103,  0.0027,  ...,  0.0017, -0.0053,  0.0089],
        [-0.0013,  0.0085, -0.0073,  ...,  0.0115, -0.0125,  0.0028],
        ...,
        [-0.0163, -0.0131,  0.0044,  ...,  0.0035, -0.0251,  0.0068],
        [-0.0114,  0.0243, -0.0033,  ..., -0.0107,  0.0006,  0.0005],
        [-0.0095, -0.0034,  0.0224,  ..., -0.0034, -0.0154, -0.0086]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.8711, -0.3853, -2.6855,  ..., -0.8271,  2.4941,  0.6523]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 08:25:21 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of lawful is unlawful
The opposite of pleasant is unpleasant
The opposite of controlled is uncontrolled
The opposite of identified is unidentified
The opposite of available is unavailable
The opposite of intended is unintended
The opposite of related is unrelated
The opposite of comfortable is
2024-07-19 08:25:21 root INFO     [order_1_approx] starting weight calculation for The opposite of comfortable is uncomfortable
The opposite of controlled is uncontrolled
The opposite of identified is unidentified
The opposite of related is unrelated
The opposite of intended is unintended
The opposite of available is unavailable
The opposite of lawful is unlawful
The opposite of pleasant is
2024-07-19 08:25:21 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 08:29:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3530, -0.3105, -0.3315,  ..., -1.3018, -0.1346,  0.0764],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.6953,  0.5410, -1.2539,  ...,  3.1523, -0.1841, -0.0117],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0256, -0.0032, -0.0179,  ..., -0.0047, -0.0110, -0.0206],
        [-0.0334, -0.0204,  0.0092,  ..., -0.0045,  0.0033, -0.0042],
        [ 0.0162,  0.0018, -0.0131,  ..., -0.0021,  0.0021,  0.0164],
        ...,
        [-0.0186, -0.0019,  0.0050,  ...,  0.0042, -0.0049,  0.0133],
        [-0.0114, -0.0141, -0.0091,  ..., -0.0089, -0.0152,  0.0025],
        [ 0.0019,  0.0220,  0.0217,  ...,  0.0036, -0.0082, -0.0175]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.0547, -0.8086, -1.0840,  ...,  2.5996, -0.5811, -1.2979]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 08:29:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of comfortable is uncomfortable
The opposite of controlled is uncontrolled
The opposite of identified is unidentified
The opposite of related is unrelated
The opposite of intended is unintended
The opposite of available is unavailable
The opposite of lawful is unlawful
The opposite of pleasant is
2024-07-19 08:29:42 root INFO     [order_1_approx] starting weight calculation for The opposite of identified is unidentified
The opposite of controlled is uncontrolled
The opposite of comfortable is uncomfortable
The opposite of lawful is unlawful
The opposite of available is unavailable
The opposite of pleasant is unpleasant
The opposite of related is unrelated
The opposite of intended is
2024-07-19 08:29:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 08:34:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3176,  1.0381, -0.6455,  ..., -0.3809,  0.2427, -0.5010],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1191,  0.4292, -0.8340,  ...,  3.9727, -1.1182, -0.5278],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0084,  0.0017,  0.0006,  ..., -0.0163, -0.0056, -0.0102],
        [-0.0315, -0.0129, -0.0051,  ..., -0.0043, -0.0235,  0.0094],
        [ 0.0061,  0.0141, -0.0183,  ..., -0.0063, -0.0221,  0.0177],
        ...,
        [-0.0184, -0.0102,  0.0040,  ..., -0.0202,  0.0244, -0.0074],
        [ 0.0137,  0.0042,  0.0220,  ...,  0.0043, -0.0114, -0.0184],
        [-0.0011, -0.0021,  0.0064,  ..., -0.0024,  0.0236, -0.0119]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9775,  0.8057, -1.3906,  ...,  3.1992, -1.2236, -1.1973]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 08:34:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of identified is unidentified
The opposite of controlled is uncontrolled
The opposite of comfortable is uncomfortable
The opposite of lawful is unlawful
The opposite of available is unavailable
The opposite of pleasant is unpleasant
The opposite of related is unrelated
The opposite of intended is
2024-07-19 08:34:02 root INFO     [order_1_approx] starting weight calculation for The opposite of pleasant is unpleasant
The opposite of comfortable is uncomfortable
The opposite of available is unavailable
The opposite of identified is unidentified
The opposite of intended is unintended
The opposite of controlled is uncontrolled
The opposite of related is unrelated
The opposite of lawful is
2024-07-19 08:34:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 08:38:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7939,  0.5718,  0.4922,  ..., -0.5391, -1.1406,  0.3838],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3564, -6.2930, -2.0234,  ..., -0.1702,  4.9766, -2.3359],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0076,  0.0079,  0.0042,  ..., -0.0051, -0.0166, -0.0068],
        [ 0.0043, -0.0186, -0.0117,  ..., -0.0008,  0.0106, -0.0111],
        [-0.0078,  0.0147, -0.0160,  ...,  0.0102, -0.0209, -0.0195],
        ...,
        [-0.0173,  0.0108, -0.0120,  ...,  0.0075, -0.0032, -0.0191],
        [-0.0092,  0.0022,  0.0076,  ...,  0.0009, -0.0103, -0.0153],
        [-0.0016, -0.0094,  0.0015,  ...,  0.0009, -0.0014,  0.0042]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8184, -6.3086, -1.9941,  ...,  0.2856,  3.5977, -2.4844]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 08:38:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of pleasant is unpleasant
The opposite of comfortable is uncomfortable
The opposite of available is unavailable
The opposite of identified is unidentified
The opposite of intended is unintended
The opposite of controlled is uncontrolled
The opposite of related is unrelated
The opposite of lawful is
2024-07-19 08:38:23 root INFO     [order_1_approx] starting weight calculation for The opposite of pleasant is unpleasant
The opposite of identified is unidentified
The opposite of comfortable is uncomfortable
The opposite of available is unavailable
The opposite of lawful is unlawful
The opposite of intended is unintended
The opposite of related is unrelated
The opposite of controlled is
2024-07-19 08:38:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 08:42:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6694,  1.4092, -0.6143,  ..., -1.0859, -0.1904, -0.2922],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.0898, -4.7344, -1.6133,  ..., -1.4609,  1.1260, -2.2500],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0004, -0.0113,  0.0050,  ..., -0.0120, -0.0164, -0.0111],
        [-0.0320,  0.0077, -0.0017,  ...,  0.0124, -0.0045, -0.0038],
        [ 0.0236,  0.0024, -0.0532,  ...,  0.0205, -0.0271, -0.0195],
        ...,
        [-0.0119,  0.0184, -0.0098,  ..., -0.0055,  0.0119,  0.0007],
        [ 0.0141, -0.0133,  0.0152,  ...,  0.0025, -0.0201,  0.0003],
        [-0.0032, -0.0046,  0.0046,  ...,  0.0101,  0.0035, -0.0003]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6211, -4.7266, -0.8042,  ..., -1.4932,  1.4092, -3.4785]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 08:42:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of pleasant is unpleasant
The opposite of identified is unidentified
The opposite of comfortable is uncomfortable
The opposite of available is unavailable
The opposite of lawful is unlawful
The opposite of intended is unintended
The opposite of related is unrelated
The opposite of controlled is
2024-07-19 08:42:43 root INFO     [order_1_approx] starting weight calculation for The opposite of intended is unintended
The opposite of pleasant is unpleasant
The opposite of available is unavailable
The opposite of controlled is uncontrolled
The opposite of comfortable is uncomfortable
The opposite of identified is unidentified
The opposite of lawful is unlawful
The opposite of related is
2024-07-19 08:42:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 08:47:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3467,  0.8262,  0.3354,  ..., -0.4841, -0.5225, -0.4482],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.0469, -2.5391,  0.0400,  ...,  0.0410,  3.6777, -1.2773],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0028,  0.0004, -0.0114,  ..., -0.0072, -0.0111, -0.0113],
        [ 0.0116,  0.0060, -0.0276,  ...,  0.0386, -0.0060, -0.0064],
        [ 0.0280,  0.0124, -0.0115,  ..., -0.0241, -0.0015,  0.0022],
        ...,
        [-0.0227, -0.0024, -0.0072,  ..., -0.0005,  0.0002,  0.0072],
        [-0.0184, -0.0017,  0.0270,  ...,  0.0093, -0.0136,  0.0042],
        [ 0.0075, -0.0044,  0.0051,  ..., -0.0174,  0.0197, -0.0172]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.4688, -2.6758,  0.8359,  ..., -1.8564,  2.7949, -2.3145]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 08:47:04 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of intended is unintended
The opposite of pleasant is unpleasant
The opposite of available is unavailable
The opposite of controlled is uncontrolled
The opposite of comfortable is uncomfortable
The opposite of identified is unidentified
The opposite of lawful is unlawful
The opposite of related is
2024-07-19 08:47:04 root INFO     [order_1_approx] starting weight calculation for The opposite of intended is unintended
The opposite of identified is unidentified
The opposite of comfortable is uncomfortable
The opposite of pleasant is unpleasant
The opposite of controlled is uncontrolled
The opposite of related is unrelated
The opposite of lawful is unlawful
The opposite of available is
2024-07-19 08:47:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 08:51:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-4.4922e-02,  3.1030e-01,  9.7266e-01,  ...,  7.0190e-04,
        -3.2715e-01, -3.6890e-01], device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.6914,  0.1401,  3.0293,  ...,  0.9941,  4.0039, -0.9111],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0125, -0.0108,  0.0042,  ...,  0.0128, -0.0267,  0.0043],
        [-0.0216,  0.0089, -0.0029,  ..., -0.0201,  0.0180, -0.0241],
        [ 0.0060,  0.0105,  0.0068,  ..., -0.0194, -0.0128,  0.0122],
        ...,
        [ 0.0024,  0.0036, -0.0199,  ..., -0.0216,  0.0228, -0.0158],
        [ 0.0320, -0.0205, -0.0156,  ..., -0.0063, -0.0154, -0.0017],
        [-0.0081,  0.0288, -0.0002,  ..., -0.0343,  0.0320, -0.0091]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5547, -0.6729,  2.3711,  ..., -0.1064,  4.0391, -2.8984]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 08:51:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of intended is unintended
The opposite of identified is unidentified
The opposite of comfortable is uncomfortable
The opposite of pleasant is unpleasant
The opposite of controlled is uncontrolled
The opposite of related is unrelated
The opposite of lawful is unlawful
The opposite of available is
2024-07-19 08:51:24 root INFO     [order_1_approx] starting weight calculation for The opposite of pleasant is unpleasant
The opposite of controlled is uncontrolled
The opposite of lawful is unlawful
The opposite of comfortable is uncomfortable
The opposite of related is unrelated
The opposite of available is unavailable
The opposite of intended is unintended
The opposite of identified is
2024-07-19 08:51:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 08:55:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2272,  1.1445, -0.9478,  ...,  0.1045, -0.7744, -0.6895],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.8535, -2.2910, -1.4053,  ...,  4.1055,  2.4297, -0.4912],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.2202e-02, -3.0884e-02, -1.7349e-02,  ...,  2.9984e-03,
          9.1095e-03, -1.0773e-02],
        [-3.3752e-02, -2.6001e-02,  2.5921e-03,  ..., -2.4910e-03,
          1.5244e-02, -1.3382e-02],
        [ 1.2115e-02,  2.0645e-02, -2.3941e-02,  ..., -1.0929e-03,
         -1.3802e-02,  1.0880e-02],
        ...,
        [-1.3031e-02, -2.0966e-02,  1.6205e-02,  ..., -2.6894e-03,
          6.8665e-05,  2.1255e-02],
        [ 2.2537e-02, -1.4610e-02,  1.2016e-02,  ...,  4.4975e-03,
         -1.5808e-02,  4.6043e-03],
        [ 2.6138e-02, -1.3939e-02,  2.7771e-03,  ..., -3.4973e-02,
          2.9602e-02, -4.5471e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.5762, -2.6934, -0.8862,  ...,  3.9980,  1.9668, -1.3223]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 08:55:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of pleasant is unpleasant
The opposite of controlled is uncontrolled
The opposite of lawful is unlawful
The opposite of comfortable is uncomfortable
The opposite of related is unrelated
The opposite of available is unavailable
The opposite of intended is unintended
The opposite of identified is
2024-07-19 08:55:44 root INFO     total operator prediction time: 2084.154314994812 seconds
2024-07-19 08:55:44 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on re+verb_reg
2024-07-19 08:55:44 root INFO     building operator re+verb_reg
2024-07-19 08:55:45 root INFO     [order_1_approx] starting weight calculation for To deem again is to redeem
To publish again is to republish
To learn again is to relearn
To assess again is to reassess
To introduce again is to reintroduce
To locate again is to relocate
To interpret again is to reinterpret
To generate again is to
2024-07-19 08:55:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 09:00:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2339,  0.1207, -0.5205,  ..., -0.1282, -0.2089, -0.0954],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.0391, -0.5474, -3.9297,  ...,  4.2539,  2.0605, -1.5586],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0127, -0.0033,  0.0045,  ...,  0.0169,  0.0172, -0.0107],
        [ 0.0099, -0.0047,  0.0046,  ...,  0.0185,  0.0133, -0.0060],
        [ 0.0220, -0.0162, -0.0018,  ..., -0.0001,  0.0025, -0.0005],
        ...,
        [-0.0116, -0.0393, -0.0145,  ..., -0.0260, -0.0099,  0.0323],
        [-0.0113, -0.0063, -0.0037,  ..., -0.0156, -0.0100,  0.0095],
        [-0.0261,  0.0179, -0.0067,  ...,  0.0050,  0.0090,  0.0061]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.6387, -0.2712, -4.5234,  ...,  3.5273,  2.6582, -1.8291]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 09:00:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To deem again is to redeem
To publish again is to republish
To learn again is to relearn
To assess again is to reassess
To introduce again is to reintroduce
To locate again is to relocate
To interpret again is to reinterpret
To generate again is to
2024-07-19 09:00:06 root INFO     [order_1_approx] starting weight calculation for To learn again is to relearn
To deem again is to redeem
To publish again is to republish
To generate again is to regenerate
To introduce again is to reintroduce
To interpret again is to reinterpret
To assess again is to reassess
To locate again is to
2024-07-19 09:00:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 09:04:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1820,  0.7393, -0.1974,  ..., -0.0825,  0.0125, -0.5820],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6953,  0.8857, -7.3555,  ..., -3.3066,  3.5293,  3.1543],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0140, -0.0377,  0.0031,  ...,  0.0213, -0.0052, -0.0066],
        [-0.0062, -0.0056,  0.0027,  ...,  0.0030,  0.0213, -0.0029],
        [ 0.0152,  0.0273, -0.0032,  ..., -0.0064,  0.0045,  0.0129],
        ...,
        [-0.0007,  0.0008,  0.0137,  ..., -0.0295,  0.0110,  0.0028],
        [ 0.0188,  0.0046, -0.0039,  ..., -0.0190, -0.0037,  0.0162],
        [-0.0031,  0.0008, -0.0072,  ..., -0.0145,  0.0162, -0.0124]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.0117,  0.6562, -6.6953,  ..., -2.5137,  4.1562,  2.7637]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 09:04:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To learn again is to relearn
To deem again is to redeem
To publish again is to republish
To generate again is to regenerate
To introduce again is to reintroduce
To interpret again is to reinterpret
To assess again is to reassess
To locate again is to
2024-07-19 09:04:24 root INFO     [order_1_approx] starting weight calculation for To publish again is to republish
To introduce again is to reintroduce
To generate again is to regenerate
To learn again is to relearn
To deem again is to redeem
To assess again is to reassess
To locate again is to relocate
To interpret again is to
2024-07-19 09:04:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 09:08:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1909,  0.0693, -0.3271,  ..., -0.3032, -0.1985,  0.3262],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.9951,  0.3770, -5.9453,  ..., -0.4307, -1.3848,  2.0762],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0055, -0.0115, -0.0080,  ...,  0.0151,  0.0023, -0.0180],
        [-0.0101, -0.0170,  0.0007,  ..., -0.0068, -0.0104,  0.0091],
        [ 0.0172,  0.0194, -0.0005,  ...,  0.0197,  0.0210, -0.0219],
        ...,
        [ 0.0135, -0.0210,  0.0027,  ..., -0.0151,  0.0105, -0.0378],
        [-0.0022, -0.0098, -0.0033,  ..., -0.0198,  0.0028, -0.0025],
        [ 0.0080,  0.0034,  0.0017,  ..., -0.0075, -0.0025, -0.0101]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.2891, -0.7432, -4.7812,  ..., -0.2747, -1.2402,  2.6543]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 09:08:41 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To publish again is to republish
To introduce again is to reintroduce
To generate again is to regenerate
To learn again is to relearn
To deem again is to redeem
To assess again is to reassess
To locate again is to relocate
To interpret again is to
2024-07-19 09:08:42 root INFO     [order_1_approx] starting weight calculation for To deem again is to redeem
To generate again is to regenerate
To interpret again is to reinterpret
To introduce again is to reintroduce
To locate again is to relocate
To assess again is to reassess
To publish again is to republish
To learn again is to
2024-07-19 09:08:42 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 09:13:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0516,  0.2334,  0.3892,  ...,  0.0090,  0.2429,  0.0878],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4543,  0.8984, -7.8164,  ..., -1.0137, -3.0098,  0.6846],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0020, -0.0061, -0.0078,  ...,  0.0014,  0.0019,  0.0002],
        [-0.0095, -0.0062,  0.0017,  ...,  0.0116,  0.0098,  0.0012],
        [ 0.0085, -0.0029, -0.0109,  ...,  0.0063,  0.0172,  0.0001],
        ...,
        [-0.0054, -0.0326, -0.0021,  ..., -0.0176, -0.0340,  0.0105],
        [ 0.0232, -0.0087, -0.0001,  ..., -0.0196, -0.0180,  0.0389],
        [-0.0091, -0.0008, -0.0020,  ...,  0.0027, -0.0012, -0.0067]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0276,  0.3086, -7.7656,  ..., -1.3008, -2.7168, -0.0410]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 09:13:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To deem again is to redeem
To generate again is to regenerate
To interpret again is to reinterpret
To introduce again is to reintroduce
To locate again is to relocate
To assess again is to reassess
To publish again is to republish
To learn again is to
2024-07-19 09:13:03 root INFO     [order_1_approx] starting weight calculation for To generate again is to regenerate
To publish again is to republish
To introduce again is to reintroduce
To deem again is to redeem
To learn again is to relearn
To interpret again is to reinterpret
To locate again is to relocate
To assess again is to
2024-07-19 09:13:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 09:17:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1758, -0.5713, -0.5942,  ..., -0.6284, -0.6006,  0.3057],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.0352,  4.1406, -6.4844,  ..., -1.9648, -2.7871,  1.5996],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0087, -0.0171,  0.0179,  ...,  0.0182, -0.0132,  0.0055],
        [-0.0053, -0.0079,  0.0043,  ...,  0.0045,  0.0112,  0.0301],
        [ 0.0037,  0.0285, -0.0172,  ...,  0.0265,  0.0090, -0.0394],
        ...,
        [-0.0202, -0.0027, -0.0058,  ..., -0.0068, -0.0036, -0.0079],
        [-0.0004,  0.0085,  0.0121,  ..., -0.0265, -0.0161, -0.0361],
        [-0.0107,  0.0037, -0.0131,  ..., -0.0066,  0.0061, -0.0152]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.2305,  2.9941, -6.1250,  ..., -1.9336, -1.2041,  2.0879]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 09:17:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To generate again is to regenerate
To publish again is to republish
To introduce again is to reintroduce
To deem again is to redeem
To learn again is to relearn
To interpret again is to reinterpret
To locate again is to relocate
To assess again is to
2024-07-19 09:17:23 root INFO     [order_1_approx] starting weight calculation for To locate again is to relocate
To assess again is to reassess
To interpret again is to reinterpret
To introduce again is to reintroduce
To learn again is to relearn
To publish again is to republish
To generate again is to regenerate
To deem again is to
2024-07-19 09:17:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 09:21:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0674,  0.2405, -0.2489,  ..., -0.5654, -1.0859,  0.1470],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.2070, -0.1819, -1.7383,  ..., -1.2500, -1.7529,  2.7344],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.2308e-02, -2.5970e-02,  2.1667e-03,  ...,  8.4076e-03,
          8.8501e-04,  1.7605e-03],
        [-2.6352e-02, -7.1220e-03,  9.7351e-03,  ..., -3.1891e-03,
          2.0767e-02,  1.5358e-02],
        [ 1.3474e-02, -1.5137e-02, -2.5116e-02,  ...,  2.0782e-02,
          2.1881e-02, -8.1787e-03],
        ...,
        [-4.0359e-03, -1.9760e-02, -1.3733e-03,  ..., -1.1734e-02,
         -1.1421e-02,  1.8372e-02],
        [ 2.1713e-02,  7.6294e-05,  1.0979e-02,  ..., -7.0724e-03,
         -1.6876e-02, -5.1498e-03],
        [ 1.1673e-02, -2.0237e-03,  7.4654e-03,  ...,  2.8820e-03,
          4.1504e-03,  3.7460e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1816,  0.4397, -1.7900,  ..., -1.8379, -2.3125,  2.3574]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 09:21:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To locate again is to relocate
To assess again is to reassess
To interpret again is to reinterpret
To introduce again is to reintroduce
To learn again is to relearn
To publish again is to republish
To generate again is to regenerate
To deem again is to
2024-07-19 09:21:45 root INFO     [order_1_approx] starting weight calculation for To generate again is to regenerate
To introduce again is to reintroduce
To learn again is to relearn
To deem again is to redeem
To locate again is to relocate
To interpret again is to reinterpret
To assess again is to reassess
To publish again is to
2024-07-19 09:21:45 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 09:26:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2507, -0.0164,  0.7563,  ...,  0.7666,  0.1326,  0.4143],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.3125, -0.8657, -6.2656,  ...,  3.4727, -1.3330, -1.1621],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0150, -0.0065,  0.0182,  ...,  0.0019,  0.0030,  0.0029],
        [ 0.0070, -0.0020,  0.0071,  ...,  0.0076, -0.0015,  0.0136],
        [ 0.0161, -0.0084, -0.0053,  ..., -0.0019, -0.0009, -0.0047],
        ...,
        [ 0.0021, -0.0133,  0.0129,  ..., -0.0116, -0.0024,  0.0069],
        [ 0.0183,  0.0154,  0.0087,  ..., -0.0185,  0.0224, -0.0090],
        [-0.0018, -0.0052, -0.0089,  ...,  0.0011,  0.0121, -0.0033]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.6562, -1.6777, -5.2734,  ...,  3.7148, -0.9727, -0.9043]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 09:26:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To generate again is to regenerate
To introduce again is to reintroduce
To learn again is to relearn
To deem again is to redeem
To locate again is to relocate
To interpret again is to reinterpret
To assess again is to reassess
To publish again is to
2024-07-19 09:26:07 root INFO     [order_1_approx] starting weight calculation for To publish again is to republish
To deem again is to redeem
To generate again is to regenerate
To interpret again is to reinterpret
To learn again is to relearn
To locate again is to relocate
To assess again is to reassess
To introduce again is to
2024-07-19 09:26:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 09:30:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3547,  0.1236, -0.0569,  ...,  0.0615, -0.6748, -0.1842],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.6992, -1.9824, -6.0547,  ...,  2.3008, -1.6582,  3.9941],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.0254e-02, -1.6830e-02,  5.0583e-03,  ...,  2.9755e-02,
          8.5602e-03,  4.5471e-03],
        [-1.7166e-04, -3.4714e-03, -1.2451e-02,  ...,  1.3641e-02,
          8.1100e-03,  1.0017e-02],
        [ 2.3163e-02,  1.4099e-02,  5.9357e-03,  ..., -2.8473e-02,
          1.7662e-03,  2.1210e-03],
        ...,
        [ 2.1439e-03, -7.3166e-03,  1.3046e-02,  ..., -2.3529e-02,
          5.3253e-03,  2.1149e-02],
        [ 1.7471e-02,  1.5656e-02,  4.6768e-03,  ..., -2.4170e-02,
         -2.6917e-02, -2.7008e-03],
        [ 1.5259e-05,  1.6235e-02, -7.5912e-04,  ..., -1.0651e-02,
          2.7206e-02, -5.3635e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2600, -2.6797, -5.2109,  ...,  2.2969, -1.1436,  3.2578]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 09:30:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To publish again is to republish
To deem again is to redeem
To generate again is to regenerate
To interpret again is to reinterpret
To learn again is to relearn
To locate again is to relocate
To assess again is to reassess
To introduce again is to
2024-07-19 09:30:28 root INFO     total operator prediction time: 2083.528214931488 seconds
2024-07-19 09:30:28 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on adj+ness_reg
2024-07-19 09:30:28 root INFO     building operator adj+ness_reg
2024-07-19 09:30:28 root INFO     [order_1_approx] starting weight calculation for The state of being competitive is competitiveness
The state of being same is sameness
The state of being innovative is innovativeness
The state of being odd is oddness
The state of being fixed is fixedness
The state of being obvious is obviousness
The state of being creative is creativeness
The state of being prepared is
2024-07-19 09:30:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 09:34:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0408, -0.1482, -0.9648,  ...,  0.0020,  1.1484, -0.1982],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1221,  1.3438, -2.8477,  ..., -3.0312, -1.4219,  2.8281],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 9.9640e-03, -8.5754e-03, -8.2970e-05,  ..., -4.5700e-03,
          2.9907e-03, -7.2174e-03],
        [-1.6617e-02, -5.0468e-03,  1.3409e-03,  ...,  1.0345e-02,
         -5.1117e-04, -5.2795e-03],
        [-3.4828e-03,  1.7685e-02,  5.4169e-03,  ...,  3.6850e-03,
         -1.3123e-02,  1.1055e-02],
        ...,
        [-1.5839e-02, -1.3000e-02, -6.8665e-03,  ..., -1.8997e-02,
          5.4436e-03, -2.4128e-03],
        [-5.1079e-03,  1.8875e-02,  7.5531e-04,  ..., -8.5754e-03,
         -1.9089e-02,  2.3132e-02],
        [-1.1948e-02,  2.3899e-03,  6.8207e-03,  ...,  1.2421e-02,
         -1.3794e-02, -3.2349e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0420,  1.5059, -2.6016,  ..., -3.0703, -0.7847,  2.7402]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 09:34:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being competitive is competitiveness
The state of being same is sameness
The state of being innovative is innovativeness
The state of being odd is oddness
The state of being fixed is fixedness
The state of being obvious is obviousness
The state of being creative is creativeness
The state of being prepared is
2024-07-19 09:34:50 root INFO     [order_1_approx] starting weight calculation for The state of being obvious is obviousness
The state of being competitive is competitiveness
The state of being odd is oddness
The state of being prepared is preparedness
The state of being innovative is innovativeness
The state of being same is sameness
The state of being creative is creativeness
The state of being fixed is
2024-07-19 09:34:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 09:39:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2048, -0.0771, -0.0659,  ..., -0.2686,  0.1755, -0.7388],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.5654,  1.1865, -0.9619,  ..., -3.1875,  2.6406,  6.3203],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0061, -0.0101, -0.0067,  ...,  0.0195,  0.0004, -0.0139],
        [-0.0128, -0.0170,  0.0055,  ..., -0.0150, -0.0095, -0.0163],
        [ 0.0022,  0.0040, -0.0129,  ...,  0.0127,  0.0038, -0.0095],
        ...,
        [-0.0336, -0.0071,  0.0006,  ..., -0.0083,  0.0268, -0.0282],
        [-0.0010, -0.0069,  0.0029,  ..., -0.0145, -0.0175,  0.0011],
        [-0.0108,  0.0352,  0.0161,  ..., -0.0041, -0.0172, -0.0237]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.1611,  0.4849, -0.9541,  ..., -3.9551,  3.2930,  6.2344]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 09:39:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being obvious is obviousness
The state of being competitive is competitiveness
The state of being odd is oddness
The state of being prepared is preparedness
The state of being innovative is innovativeness
The state of being same is sameness
The state of being creative is creativeness
The state of being fixed is
2024-07-19 09:39:12 root INFO     [order_1_approx] starting weight calculation for The state of being innovative is innovativeness
The state of being prepared is preparedness
The state of being same is sameness
The state of being obvious is obviousness
The state of being odd is oddness
The state of being creative is creativeness
The state of being fixed is fixedness
The state of being competitive is
2024-07-19 09:39:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 09:43:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3252,  0.7227,  1.3750,  ..., -1.1953,  1.1855,  0.0446],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.3438,  0.1282, -3.7383,  ...,  1.0918,  2.0605,  2.1758],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0081, -0.0114,  0.0094,  ..., -0.0183, -0.0090,  0.0017],
        [-0.0014,  0.0053,  0.0083,  ...,  0.0029,  0.0119, -0.0122],
        [ 0.0098, -0.0065, -0.0174,  ..., -0.0046,  0.0087,  0.0056],
        ...,
        [-0.0237, -0.0228, -0.0022,  ..., -0.0164,  0.0082,  0.0018],
        [ 0.0150,  0.0043,  0.0061,  ..., -0.0296, -0.0052,  0.0175],
        [-0.0096,  0.0024,  0.0064,  ...,  0.0051, -0.0048, -0.0257]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.3984, -0.4573, -4.0859,  ...,  0.5044,  1.6875,  1.8936]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 09:43:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being innovative is innovativeness
The state of being prepared is preparedness
The state of being same is sameness
The state of being obvious is obviousness
The state of being odd is oddness
The state of being creative is creativeness
The state of being fixed is fixedness
The state of being competitive is
2024-07-19 09:43:33 root INFO     [order_1_approx] starting weight calculation for The state of being obvious is obviousness
The state of being competitive is competitiveness
The state of being same is sameness
The state of being prepared is preparedness
The state of being fixed is fixedness
The state of being innovative is innovativeness
The state of being odd is oddness
The state of being creative is
2024-07-19 09:43:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 09:47:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0958,  0.0907,  0.3479,  ..., -0.0791,  0.6387,  0.2258],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.0293, -0.3257, -1.6602,  ..., -0.1982,  0.8101,  0.2832],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0028, -0.0142,  0.0110,  ..., -0.0012, -0.0255, -0.0228],
        [-0.0006, -0.0058,  0.0106,  ..., -0.0080,  0.0207, -0.0036],
        [ 0.0075, -0.0072, -0.0253,  ..., -0.0011,  0.0028,  0.0084],
        ...,
        [-0.0049,  0.0028,  0.0005,  ..., -0.0223,  0.0066, -0.0127],
        [-0.0010, -0.0111,  0.0051,  ..., -0.0170, -0.0282,  0.0028],
        [-0.0059, -0.0065, -0.0062,  ..., -0.0083,  0.0088, -0.0442]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3945, -1.2031, -1.7871,  ..., -1.0166,  0.8296, -0.6514]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 09:47:52 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being obvious is obviousness
The state of being competitive is competitiveness
The state of being same is sameness
The state of being prepared is preparedness
The state of being fixed is fixedness
The state of being innovative is innovativeness
The state of being odd is oddness
The state of being creative is
2024-07-19 09:47:52 root INFO     [order_1_approx] starting weight calculation for The state of being obvious is obviousness
The state of being odd is oddness
The state of being competitive is competitiveness
The state of being prepared is preparedness
The state of being innovative is innovativeness
The state of being fixed is fixedness
The state of being creative is creativeness
The state of being same is
2024-07-19 09:47:52 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 09:52:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7739,  0.8115,  0.5083,  ..., -0.4683,  0.4087, -0.6025],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2549,  4.4414, -3.7617,  ...,  1.4424,  2.1484,  1.6113],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0048, -0.0184, -0.0017,  ...,  0.0129, -0.0263, -0.0056],
        [-0.0473,  0.0258,  0.0226,  ..., -0.0398,  0.0021,  0.0087],
        [ 0.0411, -0.0357, -0.0318,  ...,  0.0281,  0.0032, -0.0046],
        ...,
        [-0.0109,  0.0186,  0.0202,  ..., -0.0115,  0.0010, -0.0006],
        [ 0.0251,  0.0028, -0.0172,  ..., -0.0002, -0.0139,  0.0006],
        [-0.0087,  0.0177,  0.0154,  ..., -0.0107,  0.0035, -0.0064]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7686,  5.1641, -3.2656,  ..., -0.0557,  1.7529,  2.5684]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 09:52:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being obvious is obviousness
The state of being odd is oddness
The state of being competitive is competitiveness
The state of being prepared is preparedness
The state of being innovative is innovativeness
The state of being fixed is fixedness
The state of being creative is creativeness
The state of being same is
2024-07-19 09:52:13 root INFO     [order_1_approx] starting weight calculation for The state of being creative is creativeness
The state of being competitive is competitiveness
The state of being innovative is innovativeness
The state of being same is sameness
The state of being prepared is preparedness
The state of being obvious is obviousness
The state of being fixed is fixedness
The state of being odd is
2024-07-19 09:52:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 09:56:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2107, -0.9380, -0.1277,  ..., -0.7002,  0.5918,  0.1926],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.3906,  1.5830, -0.4287,  ..., -3.2031,  0.7622,  6.0547],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.8082e-03, -2.9602e-02, -1.3206e-02,  ...,  3.6560e-02,
         -3.7720e-02, -1.7044e-02],
        [-4.4403e-03, -1.7105e-02,  6.1417e-04,  ..., -1.9562e-02,
          2.5024e-02,  2.3842e-05],
        [-8.8501e-04,  8.4076e-03, -1.5450e-02,  ...,  1.5900e-02,
         -7.8888e-03, -1.2306e-02],
        ...,
        [-2.4414e-04,  2.3102e-02, -1.0658e-02,  ..., -3.3020e-02,
         -4.5471e-03, -1.4343e-03],
        [-1.0956e-02, -7.9956e-03,  4.3869e-04,  ..., -8.5449e-04,
         -3.8635e-02,  5.5771e-03],
        [-2.8076e-02, -6.7863e-03, -8.0414e-03,  ..., -2.3529e-02,
          2.1210e-02, -2.4139e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.5664,  1.1797, -0.3855,  ..., -3.1660,  0.9697,  5.8086]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 09:56:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being creative is creativeness
The state of being competitive is competitiveness
The state of being innovative is innovativeness
The state of being same is sameness
The state of being prepared is preparedness
The state of being obvious is obviousness
The state of being fixed is fixedness
The state of being odd is
2024-07-19 09:56:32 root INFO     [order_1_approx] starting weight calculation for The state of being prepared is preparedness
The state of being odd is oddness
The state of being competitive is competitiveness
The state of being obvious is obviousness
The state of being same is sameness
The state of being fixed is fixedness
The state of being creative is creativeness
The state of being innovative is
2024-07-19 09:56:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 10:00:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4116, -0.1365,  0.6436,  ..., -0.3594,  0.6074,  0.1890],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.9531, -0.8857, -1.9492,  ...,  0.9365, -0.4717,  2.6484],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0177, -0.0175,  0.0327,  ...,  0.0110, -0.0187, -0.0003],
        [-0.0077, -0.0097,  0.0075,  ...,  0.0237,  0.0191, -0.0113],
        [-0.0140, -0.0118, -0.0253,  ..., -0.0042,  0.0076, -0.0043],
        ...,
        [-0.0158, -0.0166, -0.0210,  ..., -0.0363, -0.0083, -0.0112],
        [ 0.0143, -0.0003,  0.0149,  ..., -0.0332, -0.0257,  0.0097],
        [ 0.0303,  0.0034, -0.0065,  ..., -0.0352, -0.0144, -0.0272]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.1836, -1.7832, -1.5098,  ..., -0.3975, -0.2450,  1.9414]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 10:00:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being prepared is preparedness
The state of being odd is oddness
The state of being competitive is competitiveness
The state of being obvious is obviousness
The state of being same is sameness
The state of being fixed is fixedness
The state of being creative is creativeness
The state of being innovative is
2024-07-19 10:00:53 root INFO     [order_1_approx] starting weight calculation for The state of being creative is creativeness
The state of being prepared is preparedness
The state of being same is sameness
The state of being competitive is competitiveness
The state of being odd is oddness
The state of being innovative is innovativeness
The state of being fixed is fixedness
The state of being obvious is
2024-07-19 10:00:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 10:05:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6768, -0.0991,  0.4067,  ..., -0.3982, -0.4072,  0.1959],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.8506,  5.4375, -4.7656,  ..., -1.2266, -2.7891,  4.5859],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0156, -0.0080,  0.0055,  ...,  0.0143, -0.0143, -0.0177],
        [ 0.0123, -0.0161,  0.0043,  ..., -0.0125, -0.0015,  0.0027],
        [-0.0328, -0.0042, -0.0339,  ..., -0.0178,  0.0088, -0.0052],
        ...,
        [ 0.0164,  0.0053,  0.0019,  ..., -0.0392,  0.0106,  0.0170],
        [ 0.0008,  0.0063, -0.0165,  ..., -0.0053, -0.0331,  0.0243],
        [-0.0086,  0.0143, -0.0215,  ...,  0.0096, -0.0257, -0.0454]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4395,  4.9141, -5.0938,  ..., -1.4375, -2.0664,  5.2969]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 10:05:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The state of being creative is creativeness
The state of being prepared is preparedness
The state of being same is sameness
The state of being competitive is competitiveness
The state of being odd is oddness
The state of being innovative is innovativeness
The state of being fixed is fixedness
The state of being obvious is
2024-07-19 10:05:15 root INFO     total operator prediction time: 2087.1972527503967 seconds
2024-07-19 10:05:15 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on noun+less_reg
2024-07-19 10:05:15 root INFO     building operator noun+less_reg
2024-07-19 10:05:15 root INFO     [order_1_approx] starting weight calculation for Something without breath is breathless
Something without ruth is ruthless
Something without gender is genderless
Something without defence is defenceless
Something without luck is luckless
Something without law is lawless
Something without friend is friendless
Something without faith is
2024-07-19 10:05:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 10:09:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 3.4644e-01, -3.8354e-01,  2.4414e-04,  ...,  7.3242e-02,
        -1.0083e-01, -3.7207e-01], device='cuda:1', dtype=torch.float16,
       grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.1953,  3.4375, -2.9805,  ..., -0.5854,  2.6836,  1.0586],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0246, -0.0057,  0.0175,  ...,  0.0163, -0.0110, -0.0017],
        [-0.0031, -0.0133,  0.0107,  ...,  0.0017, -0.0177, -0.0104],
        [ 0.0112,  0.0098, -0.0081,  ...,  0.0126, -0.0032,  0.0088],
        ...,
        [ 0.0134,  0.0058, -0.0096,  ..., -0.0187,  0.0212,  0.0056],
        [-0.0111, -0.0084,  0.0047,  ..., -0.0054, -0.0475,  0.0060],
        [ 0.0137,  0.0113,  0.0193,  ...,  0.0115, -0.0250, -0.0338]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8916,  2.8242, -2.8164,  ..., -1.4121,  2.8203,  0.6338]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 10:09:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without breath is breathless
Something without ruth is ruthless
Something without gender is genderless
Something without defence is defenceless
Something without luck is luckless
Something without law is lawless
Something without friend is friendless
Something without faith is
2024-07-19 10:09:36 root INFO     [order_1_approx] starting weight calculation for Something without gender is genderless
Something without faith is faithless
Something without defence is defenceless
Something without law is lawless
Something without luck is luckless
Something without breath is breathless
Something without friend is friendless
Something without ruth is
2024-07-19 10:09:36 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 10:13:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5767, -0.1848, -0.6177,  ...,  0.3564, -0.8984,  0.1700],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-4.3594, -1.5635, -7.0859,  ...,  2.5137, -2.9043,  2.0859],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-9.5825e-03,  1.1234e-03,  1.9806e-02,  ...,  5.1575e-03,
         -2.1927e-02,  1.6251e-02],
        [-9.3231e-03, -1.2466e-02, -5.1384e-03,  ...,  1.4114e-03,
          7.8659e-03,  2.2522e-02],
        [-5.7602e-04,  9.1629e-03, -3.5004e-02,  ...,  1.1780e-02,
         -1.5823e-02,  3.7327e-03],
        ...,
        [-8.3008e-03, -1.6968e-02, -7.3242e-03,  ..., -2.2751e-02,
          2.6665e-03,  1.7761e-02],
        [ 4.3755e-03,  4.5242e-03,  4.4823e-05,  ..., -1.3599e-03,
         -2.1713e-02,  9.5139e-03],
        [-9.4299e-03,  3.1891e-02,  2.4597e-02,  ...,  3.6163e-03,
         -1.2878e-02, -2.1042e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.7266, -1.4492, -7.5547,  ...,  1.8027, -3.6133,  0.0664]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 10:13:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without gender is genderless
Something without faith is faithless
Something without defence is defenceless
Something without law is lawless
Something without luck is luckless
Something without breath is breathless
Something without friend is friendless
Something without ruth is
2024-07-19 10:13:56 root INFO     [order_1_approx] starting weight calculation for Something without ruth is ruthless
Something without gender is genderless
Something without luck is luckless
Something without faith is faithless
Something without law is lawless
Something without breath is breathless
Something without friend is friendless
Something without defence is
2024-07-19 10:13:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 10:18:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6572,  0.2803, -0.4209,  ..., -0.1216, -1.0283,  0.3674],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.2422,  0.5010, -2.9277,  ..., -0.9668,  1.8281, -2.1973],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0180, -0.0032, -0.0012,  ...,  0.0169, -0.0203,  0.0084],
        [ 0.0197,  0.0075,  0.0045,  ...,  0.0048, -0.0033,  0.0052],
        [ 0.0135,  0.0022, -0.0185,  ...,  0.0088, -0.0152, -0.0014],
        ...,
        [ 0.0169, -0.0152, -0.0064,  ..., -0.0285, -0.0193,  0.0142],
        [-0.0131, -0.0113,  0.0046,  ..., -0.0322, -0.0113, -0.0159],
        [ 0.0041,  0.0013,  0.0043,  ...,  0.0035, -0.0051, -0.0411]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7080,  0.6260, -3.7129,  ..., -0.9160,  1.6865, -0.9355]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 10:18:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without ruth is ruthless
Something without gender is genderless
Something without luck is luckless
Something without faith is faithless
Something without law is lawless
Something without breath is breathless
Something without friend is friendless
Something without defence is
2024-07-19 10:18:16 root INFO     [order_1_approx] starting weight calculation for Something without ruth is ruthless
Something without breath is breathless
Something without friend is friendless
Something without defence is defenceless
Something without luck is luckless
Something without gender is genderless
Something without faith is faithless
Something without law is
2024-07-19 10:18:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 10:22:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1859, -0.2460,  0.2954,  ..., -0.7383, -0.1818,  0.2241],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.7461, -1.1133, -4.3359,  ...,  1.6162,  3.5430,  0.8867],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0022,  0.0080,  0.0157,  ...,  0.0028,  0.0017,  0.0032],
        [-0.0011, -0.0210,  0.0041,  ..., -0.0048,  0.0032, -0.0248],
        [ 0.0267,  0.0043, -0.0325,  ..., -0.0142,  0.0155, -0.0083],
        ...,
        [-0.0182,  0.0063, -0.0062,  ...,  0.0132, -0.0010,  0.0213],
        [-0.0100, -0.0225, -0.0076,  ..., -0.0234, -0.0340, -0.0078],
        [ 0.0061, -0.0074,  0.0134,  ..., -0.0149,  0.0022, -0.0526]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.1514, -0.9370, -3.7188,  ...,  0.3330,  3.3594,  1.7598]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 10:22:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without ruth is ruthless
Something without breath is breathless
Something without friend is friendless
Something without defence is defenceless
Something without luck is luckless
Something without gender is genderless
Something without faith is faithless
Something without law is
2024-07-19 10:22:35 root INFO     [order_1_approx] starting weight calculation for Something without ruth is ruthless
Something without law is lawless
Something without faith is faithless
Something without breath is breathless
Something without luck is luckless
Something without defence is defenceless
Something without gender is genderless
Something without friend is
2024-07-19 10:22:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 10:26:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2188,  0.3398, -0.5444,  ..., -0.2590,  0.1750,  0.4995],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.9180,  0.6846, -2.4473,  ...,  0.7959,  4.0430,  6.2383],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0214, -0.0020,  0.0023,  ...,  0.0146, -0.0206,  0.0126],
        [-0.0158, -0.0085,  0.0237,  ...,  0.0006, -0.0050, -0.0084],
        [-0.0062,  0.0164, -0.0140,  ...,  0.0082, -0.0038,  0.0030],
        ...,
        [-0.0163, -0.0132,  0.0201,  ..., -0.0414,  0.0110, -0.0130],
        [ 0.0056,  0.0258, -0.0105,  ..., -0.0076, -0.0186, -0.0027],
        [-0.0211,  0.0331,  0.0106,  ..., -0.0264, -0.0026, -0.0587]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.4238,  1.0537, -2.3262,  ..., -0.5186,  3.6270,  4.6719]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 10:26:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without ruth is ruthless
Something without law is lawless
Something without faith is faithless
Something without breath is breathless
Something without luck is luckless
Something without defence is defenceless
Something without gender is genderless
Something without friend is
2024-07-19 10:26:54 root INFO     [order_1_approx] starting weight calculation for Something without ruth is ruthless
Something without breath is breathless
Something without friend is friendless
Something without luck is luckless
Something without defence is defenceless
Something without law is lawless
Something without faith is faithless
Something without gender is
2024-07-19 10:26:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 10:31:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5127, -0.0754,  0.1182,  ..., -1.4092,  0.1932,  0.5820],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.9102,  1.4121, -5.1445,  ...,  0.2111,  4.9766,  4.4062],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0040, -0.0088,  0.0090,  ..., -0.0005, -0.0034,  0.0014],
        [-0.0082, -0.0186,  0.0142,  ...,  0.0011,  0.0131, -0.0039],
        [ 0.0238,  0.0167, -0.0540,  ...,  0.0111,  0.0107,  0.0193],
        ...,
        [-0.0041,  0.0059,  0.0153,  ..., -0.0209,  0.0058,  0.0187],
        [-0.0217,  0.0025,  0.0069,  ..., -0.0022, -0.0324,  0.0016],
        [-0.0012,  0.0142,  0.0028,  ..., -0.0145, -0.0009, -0.0229]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3691,  1.7334, -4.7383,  ..., -0.4341,  4.2852,  5.8398]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 10:31:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without ruth is ruthless
Something without breath is breathless
Something without friend is friendless
Something without luck is luckless
Something without defence is defenceless
Something without law is lawless
Something without faith is faithless
Something without gender is
2024-07-19 10:31:13 root INFO     [order_1_approx] starting weight calculation for Something without friend is friendless
Something without luck is luckless
Something without ruth is ruthless
Something without gender is genderless
Something without faith is faithless
Something without law is lawless
Something without defence is defenceless
Something without breath is
2024-07-19 10:31:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 10:35:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7861, -0.0978, -0.3408,  ..., -0.5732,  0.2803,  0.4429],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.1914, -1.2773, -1.5898,  ..., -6.0625,  3.4980, -0.8389],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0041, -0.0188,  0.0125,  ...,  0.0017, -0.0070, -0.0004],
        [-0.0147,  0.0121, -0.0084,  ...,  0.0137,  0.0093,  0.0129],
        [-0.0141,  0.0387, -0.0436,  ...,  0.0218,  0.0091,  0.0270],
        ...,
        [ 0.0037, -0.0169,  0.0086,  ..., -0.0391, -0.0087,  0.0146],
        [-0.0059,  0.0197, -0.0005,  ..., -0.0022,  0.0007, -0.0096],
        [ 0.0091, -0.0059,  0.0024,  ...,  0.0025, -0.0100, -0.0168]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5996, -1.9609, -2.0625,  ..., -6.0938,  3.3105, -1.3076]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 10:35:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without friend is friendless
Something without luck is luckless
Something without ruth is ruthless
Something without gender is genderless
Something without faith is faithless
Something without law is lawless
Something without defence is defenceless
Something without breath is
2024-07-19 10:35:30 root INFO     [order_1_approx] starting weight calculation for Something without breath is breathless
Something without friend is friendless
Something without law is lawless
Something without defence is defenceless
Something without ruth is ruthless
Something without gender is genderless
Something without faith is faithless
Something without luck is
2024-07-19 10:35:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 10:39:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1671,  0.6895, -1.1318,  ...,  0.1199, -0.1759, -0.0431],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.1445,  2.3633, -3.7988,  ...,  1.6357,  1.2012,  3.5176],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0029,  0.0177,  0.0419,  ...,  0.0314,  0.0014, -0.0007],
        [-0.0019, -0.0025, -0.0106,  ...,  0.0113, -0.0106, -0.0231],
        [ 0.0060,  0.0195, -0.0107,  ...,  0.0007,  0.0053,  0.0125],
        ...,
        [ 0.0277, -0.0304, -0.0106,  ..., -0.0301,  0.0077,  0.0073],
        [-0.0086, -0.0173, -0.0248,  ..., -0.0240, -0.0279,  0.0172],
        [ 0.0071, -0.0174,  0.0122,  ...,  0.0061, -0.0120, -0.0303]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2810,  1.6035, -3.5254,  ...,  0.7603,  1.0156,  2.6895]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 10:39:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for Something without breath is breathless
Something without friend is friendless
Something without law is lawless
Something without defence is defenceless
Something without ruth is ruthless
Something without gender is genderless
Something without faith is faithless
Something without luck is
2024-07-19 10:39:50 root INFO     total operator prediction time: 2074.621563911438 seconds
2024-07-19 10:39:50 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on verb+ment_irreg
2024-07-19 10:39:50 root INFO     building operator verb+ment_irreg
2024-07-19 10:39:50 root INFO     [order_1_approx] starting weight calculation for To align results in a alignment
To enroll results in a enrollment
To adjust results in a adjustment
To achieve results in a achievement
To accomplish results in a accomplishment
To invest results in a investment
To disappoint results in a disappointment
To advertise results in a
2024-07-19 10:39:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 10:44:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3911,  0.3484, -0.4429,  ...,  0.0515, -0.1809,  0.3047],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.0792,  0.8096, -5.1172,  ..., -1.5371, -0.1071,  3.1270],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0185, -0.0078,  0.0142,  ...,  0.0096,  0.0030, -0.0111],
        [ 0.0053, -0.0096,  0.0017,  ..., -0.0001, -0.0223,  0.0195],
        [-0.0159,  0.0227, -0.0220,  ...,  0.0090,  0.0105,  0.0134],
        ...,
        [-0.0304, -0.0170, -0.0211,  ..., -0.0043,  0.0088,  0.0003],
        [-0.0180,  0.0188,  0.0142,  ..., -0.0147, -0.0141, -0.0042],
        [-0.0219,  0.0199, -0.0002,  ...,  0.0158,  0.0116, -0.0320]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1960,  0.5518, -4.9961,  ..., -0.9712, -1.7695,  3.5508]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 10:44:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To align results in a alignment
To enroll results in a enrollment
To adjust results in a adjustment
To achieve results in a achievement
To accomplish results in a accomplishment
To invest results in a investment
To disappoint results in a disappointment
To advertise results in a
2024-07-19 10:44:11 root INFO     [order_1_approx] starting weight calculation for To achieve results in a achievement
To advertise results in a advertisement
To accomplish results in a accomplishment
To disappoint results in a disappointment
To enroll results in a enrollment
To adjust results in a adjustment
To invest results in a investment
To align results in a
2024-07-19 10:44:11 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 10:48:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5010, -0.8413,  0.3469,  ..., -0.5078, -1.1592, -0.5586],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.6973,  1.8525, -3.6328,  ...,  0.7041,  1.1787,  8.8281],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0061, -0.0109,  0.0111,  ...,  0.0112, -0.0080, -0.0089],
        [-0.0061, -0.0016, -0.0006,  ...,  0.0108, -0.0204, -0.0150],
        [ 0.0065, -0.0044, -0.0082,  ..., -0.0087,  0.0016, -0.0020],
        ...,
        [ 0.0013,  0.0068, -0.0099,  ..., -0.0036, -0.0025, -0.0037],
        [ 0.0098,  0.0035, -0.0030,  ..., -0.0042,  0.0018,  0.0090],
        [-0.0045,  0.0188, -0.0049,  ...,  0.0155,  0.0040, -0.0038]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.2754,  1.3330, -4.1172,  ...,  1.7764,  2.2539,  8.7734]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 10:48:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To achieve results in a achievement
To advertise results in a advertisement
To accomplish results in a accomplishment
To disappoint results in a disappointment
To enroll results in a enrollment
To adjust results in a adjustment
To invest results in a investment
To align results in a
2024-07-19 10:48:30 root INFO     [order_1_approx] starting weight calculation for To align results in a alignment
To invest results in a investment
To advertise results in a advertisement
To achieve results in a achievement
To adjust results in a adjustment
To enroll results in a enrollment
To accomplish results in a accomplishment
To disappoint results in a
2024-07-19 10:48:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 10:52:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3096, -0.5889,  0.9590,  ...,  0.8477, -0.1472, -0.7378],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.2070, -0.0322, -3.2773,  ...,  1.3789,  2.2969,  2.6855],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0002, -0.0218,  0.0098,  ..., -0.0068,  0.0084, -0.0241],
        [-0.0096, -0.0042,  0.0052,  ..., -0.0069, -0.0096, -0.0047],
        [ 0.0098,  0.0103, -0.0028,  ..., -0.0008,  0.0251, -0.0038],
        ...,
        [-0.0234, -0.0351, -0.0004,  ..., -0.0145,  0.0235, -0.0205],
        [ 0.0066,  0.0181,  0.0363,  ..., -0.0144,  0.0110,  0.0046],
        [-0.0129,  0.0158, -0.0026,  ...,  0.0002,  0.0061, -0.0107]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.6074,  0.3533, -3.0977,  ...,  0.9775,  1.4707,  2.3711]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 10:52:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To align results in a alignment
To invest results in a investment
To advertise results in a advertisement
To achieve results in a achievement
To adjust results in a adjustment
To enroll results in a enrollment
To accomplish results in a accomplishment
To disappoint results in a
2024-07-19 10:52:51 root INFO     [order_1_approx] starting weight calculation for To align results in a alignment
To achieve results in a achievement
To accomplish results in a accomplishment
To adjust results in a adjustment
To disappoint results in a disappointment
To invest results in a investment
To advertise results in a advertisement
To enroll results in a
2024-07-19 10:52:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 10:57:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4875,  0.5244, -0.7646,  ...,  0.9561, -0.5220,  0.0864],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3369,  0.6895, -2.1914,  ...,  0.8418,  1.6914,  8.0781],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-8.0414e-03, -3.6469e-03,  2.6703e-05,  ...,  7.9880e-03,
          5.8708e-03,  1.0857e-02],
        [ 2.0618e-03, -1.6251e-02,  1.1475e-02,  ...,  4.5586e-03,
         -2.8442e-02,  9.9030e-03],
        [ 4.0054e-03,  1.6769e-02, -2.0237e-03,  ...,  4.7112e-04,
          5.4779e-03,  1.7052e-03],
        ...,
        [-2.0630e-02, -1.0063e-02, -1.0307e-02,  ..., -6.7177e-03,
          1.1148e-03,  1.0300e-02],
        [-3.6106e-03, -4.8141e-03,  5.9242e-03,  ..., -3.4599e-03,
         -3.4409e-03,  1.1009e-02],
        [-1.2884e-03,  9.7198e-03,  2.6093e-03,  ..., -6.2332e-03,
          5.3024e-03, -2.1332e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5913,  1.0312, -2.4258,  ...,  1.4297,  1.5977,  7.5273]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 10:57:11 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To align results in a alignment
To achieve results in a achievement
To accomplish results in a accomplishment
To adjust results in a adjustment
To disappoint results in a disappointment
To invest results in a investment
To advertise results in a advertisement
To enroll results in a
2024-07-19 10:57:12 root INFO     [order_1_approx] starting weight calculation for To invest results in a investment
To achieve results in a achievement
To enroll results in a enrollment
To align results in a alignment
To disappoint results in a disappointment
To adjust results in a adjustment
To advertise results in a advertisement
To accomplish results in a
2024-07-19 10:57:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 11:01:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1725,  0.3672,  0.0557,  ..., -0.6582, -0.5459, -1.3994],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8037, -0.0151, -6.6172,  ..., -2.7441,  1.5869,  3.0586],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0278, -0.0149,  0.0041,  ...,  0.0219,  0.0267, -0.0097],
        [-0.0040, -0.0191,  0.0113,  ...,  0.0039, -0.0168, -0.0151],
        [ 0.0008, -0.0134, -0.0193,  ..., -0.0080,  0.0319,  0.0131],
        ...,
        [-0.0104,  0.0093,  0.0003,  ..., -0.0088, -0.0065, -0.0049],
        [ 0.0102,  0.0036,  0.0059,  ..., -0.0203, -0.0238, -0.0109],
        [-0.0132,  0.0010,  0.0027,  ...,  0.0151,  0.0232,  0.0042]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1094,  0.5698, -6.0547,  ..., -3.2930,  1.4707,  2.7617]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 11:01:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To invest results in a investment
To achieve results in a achievement
To enroll results in a enrollment
To align results in a alignment
To disappoint results in a disappointment
To adjust results in a adjustment
To advertise results in a advertisement
To accomplish results in a
2024-07-19 11:01:32 root INFO     [order_1_approx] starting weight calculation for To enroll results in a enrollment
To advertise results in a advertisement
To disappoint results in a disappointment
To invest results in a investment
To accomplish results in a accomplishment
To achieve results in a achievement
To align results in a alignment
To adjust results in a
2024-07-19 11:01:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 11:05:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1279,  0.1021,  0.1555,  ...,  0.1458, -0.0526, -0.9658],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4075,  2.4531, -5.5859,  ...,  0.2861,  1.9268,  6.6719],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0075, -0.0095,  0.0059,  ...,  0.0107, -0.0009, -0.0134],
        [-0.0042, -0.0005, -0.0091,  ...,  0.0011, -0.0011,  0.0090],
        [ 0.0079,  0.0039, -0.0068,  ...,  0.0432,  0.0265, -0.0071],
        ...,
        [-0.0287, -0.0184, -0.0018,  ..., -0.0130,  0.0222, -0.0178],
        [-0.0160, -0.0016,  0.0185,  ..., -0.0064,  0.0024, -0.0169],
        [-0.0003,  0.0320,  0.0063,  ...,  0.0256,  0.0366,  0.0109]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5972,  3.1250, -4.3750,  ...,  0.6504,  1.8184,  7.8750]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 11:05:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To enroll results in a enrollment
To advertise results in a advertisement
To disappoint results in a disappointment
To invest results in a investment
To accomplish results in a accomplishment
To achieve results in a achievement
To align results in a alignment
To adjust results in a
2024-07-19 11:05:53 root INFO     [order_1_approx] starting weight calculation for To adjust results in a adjustment
To achieve results in a achievement
To enroll results in a enrollment
To accomplish results in a accomplishment
To advertise results in a advertisement
To align results in a alignment
To disappoint results in a disappointment
To invest results in a
2024-07-19 11:05:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 11:10:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6157,  0.9951,  0.5850,  ..., -0.2333, -0.7515, -0.2705],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.7969,  2.0293, -2.8867,  ..., -0.4204,  0.3828,  5.3008],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0048, -0.0099,  0.0019,  ..., -0.0056,  0.0123,  0.0091],
        [-0.0362, -0.0125,  0.0006,  ...,  0.0337, -0.0536,  0.0154],
        [ 0.0079,  0.0180, -0.0176,  ..., -0.0166,  0.0139, -0.0070],
        ...,
        [-0.0009, -0.0037, -0.0218,  ..., -0.0080,  0.0059, -0.0018],
        [ 0.0091, -0.0003,  0.0101,  ..., -0.0073, -0.0043, -0.0045],
        [-0.0031,  0.0219,  0.0056,  ..., -0.0019,  0.0061, -0.0246]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.9375,  1.1191, -1.7148,  ..., -1.0156,  0.6338,  4.3750]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 11:10:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To adjust results in a adjustment
To achieve results in a achievement
To enroll results in a enrollment
To accomplish results in a accomplishment
To advertise results in a advertisement
To align results in a alignment
To disappoint results in a disappointment
To invest results in a
2024-07-19 11:10:14 root INFO     [order_1_approx] starting weight calculation for To enroll results in a enrollment
To align results in a alignment
To invest results in a investment
To accomplish results in a accomplishment
To adjust results in a adjustment
To disappoint results in a disappointment
To advertise results in a advertisement
To achieve results in a
2024-07-19 11:10:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 11:14:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7056,  0.1956,  0.0688,  ..., -0.1979, -0.8247, -0.9209],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.3682,  2.8008, -2.7500,  ..., -2.6953,  5.4844,  0.6792],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0162, -0.0145,  0.0016,  ...,  0.0259,  0.0104, -0.0072],
        [ 0.0009, -0.0206, -0.0024,  ...,  0.0223, -0.0127, -0.0284],
        [ 0.0015, -0.0202, -0.0298,  ..., -0.0026,  0.0173, -0.0024],
        ...,
        [-0.0120,  0.0019, -0.0044,  ..., -0.0267, -0.0152,  0.0146],
        [-0.0043,  0.0110,  0.0143,  ..., -0.0161, -0.0018,  0.0009],
        [ 0.0048,  0.0010,  0.0008,  ...,  0.0074,  0.0141,  0.0009]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7588,  2.6816, -1.4678,  ..., -3.1250,  4.4023,  2.0098]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 11:14:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for To enroll results in a enrollment
To align results in a alignment
To invest results in a investment
To accomplish results in a accomplishment
To adjust results in a adjustment
To disappoint results in a disappointment
To advertise results in a advertisement
To achieve results in a
2024-07-19 11:14:35 root INFO     total operator prediction time: 2085.054785013199 seconds
2024-07-19 11:14:35 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on name - nationality
2024-07-19 11:14:35 root INFO     building operator name - nationality
2024-07-19 11:14:35 root INFO     [order_1_approx] starting weight calculation for lennon was english
jolie was american
raphael was italian
hitler was german
machiavelli was italian
kepler was german
hawking was english
newton was
2024-07-19 11:14:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 11:18:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0518,  0.7793,  0.4102,  ...,  0.0751, -0.6514,  0.6543],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.1436, -1.7393, -4.7344,  ...,  0.3752, -1.5840, -0.6997],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 1.2833e-02, -2.6283e-03,  9.3689e-03,  ...,  3.8567e-03,
         -1.3107e-02,  2.3003e-03],
        [-5.2109e-03,  1.3695e-02, -3.5763e-03,  ...,  3.8795e-03,
         -9.4032e-04, -6.1226e-03],
        [ 2.6260e-02, -1.7548e-02, -2.2469e-03,  ...,  4.5700e-03,
         -4.4975e-03,  2.2507e-03],
        ...,
        [-1.0803e-02,  1.5884e-02, -3.3855e-03,  ...,  1.1627e-02,
          5.7220e-03,  6.6376e-04],
        [-1.0941e-02, -1.3599e-03,  5.1498e-03,  ..., -6.6376e-04,
         -6.6757e-03,  1.1642e-02],
        [-1.5900e-02,  1.0193e-02, -1.2856e-02,  ..., -6.7596e-03,
          1.7532e-02, -7.6294e-05]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5200, -2.6406, -3.9297,  ...,  0.4692, -3.0703, -0.3743]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 11:18:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for lennon was english
jolie was american
raphael was italian
hitler was german
machiavelli was italian
kepler was german
hawking was english
newton was
2024-07-19 11:18:54 root INFO     [order_1_approx] starting weight calculation for newton was english
machiavelli was italian
hawking was english
jolie was american
lennon was english
kepler was german
raphael was italian
hitler was
2024-07-19 11:18:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 11:23:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1176,  0.7344, -0.4995,  ...,  0.2834, -0.2322,  0.0113],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2070, -1.7852, -9.9922,  ..., -2.9570, -4.8594, -4.7148],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0385, -0.0165, -0.0163,  ..., -0.0233, -0.0005,  0.0047],
        [ 0.0099,  0.0287, -0.0060,  ...,  0.0032, -0.0014, -0.0088],
        [ 0.0190, -0.0004, -0.0127,  ..., -0.0263, -0.0057, -0.0021],
        ...,
        [-0.0136, -0.0002, -0.0068,  ...,  0.0102, -0.0355,  0.0006],
        [-0.0044,  0.0058,  0.0154,  ...,  0.0005,  0.0026, -0.0060],
        [ 0.0252, -0.0264, -0.0042,  ..., -0.0152, -0.0089,  0.0053]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9668, -0.1611, -8.7578,  ..., -4.0156, -6.0078, -4.7539]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 11:23:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for newton was english
machiavelli was italian
hawking was english
jolie was american
lennon was english
kepler was german
raphael was italian
hitler was
2024-07-19 11:23:15 root INFO     [order_1_approx] starting weight calculation for newton was english
raphael was italian
hawking was english
hitler was german
kepler was german
jolie was american
machiavelli was italian
lennon was
2024-07-19 11:23:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 11:27:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9839,  1.4424,  0.0857,  ...,  0.3040,  0.3667,  0.4746],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.3711, -0.4121, -6.8281,  ..., -0.6763,  0.5557, -5.8594],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0217, -0.0174, -0.0055,  ..., -0.0050, -0.0034,  0.0020],
        [-0.0153,  0.0308, -0.0119,  ..., -0.0031,  0.0020, -0.0050],
        [ 0.0110,  0.0010, -0.0072,  ..., -0.0162, -0.0041,  0.0006],
        ...,
        [-0.0107, -0.0044,  0.0069,  ..., -0.0006, -0.0031,  0.0003],
        [ 0.0586,  0.0023, -0.0033,  ...,  0.0004, -0.0076,  0.0186],
        [-0.0304,  0.0104, -0.0028,  ...,  0.0012, -0.0024,  0.0052]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1016, -0.7988, -6.8281,  ..., -0.1775,  0.5703, -5.7422]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 11:27:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for newton was english
raphael was italian
hawking was english
hitler was german
kepler was german
jolie was american
machiavelli was italian
lennon was
2024-07-19 11:27:33 root INFO     [order_1_approx] starting weight calculation for hawking was english
kepler was german
jolie was american
lennon was english
newton was english
hitler was german
raphael was italian
machiavelli was
2024-07-19 11:27:34 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 11:31:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6611,  0.6221, -0.4937,  ..., -0.2456, -0.5049, -0.4678],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.0938, -3.4727, -7.6367,  ..., -1.9053, -4.1172, -3.4785],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0416,  0.0155,  0.0247,  ..., -0.0037, -0.0230, -0.0091],
        [-0.0067,  0.0160, -0.0259,  ...,  0.0063,  0.0062,  0.0051],
        [ 0.0081,  0.0127, -0.0140,  ..., -0.0067,  0.0163,  0.0240],
        ...,
        [-0.0325, -0.0163,  0.0035,  ...,  0.0236,  0.0073,  0.0380],
        [ 0.0063, -0.0075, -0.0030,  ..., -0.0245, -0.0001, -0.0187],
        [-0.0049, -0.0338, -0.0134,  ..., -0.0193,  0.0015,  0.0028]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9258, -2.8750, -7.7031,  ..., -1.3477, -3.6270, -3.4414]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 11:31:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for hawking was english
kepler was german
jolie was american
lennon was english
newton was english
hitler was german
raphael was italian
machiavelli was
2024-07-19 11:31:53 root INFO     [order_1_approx] starting weight calculation for machiavelli was italian
newton was english
lennon was english
hitler was german
kepler was german
jolie was american
hawking was english
raphael was
2024-07-19 11:31:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 11:36:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0234, -0.5400, -0.5215,  ...,  0.5884,  0.9297,  0.3784],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9434, -2.5977, -5.9062,  ..., -0.5425, -6.4141, -0.5693],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0274, -0.0063, -0.0010,  ..., -0.0175, -0.0146,  0.0109],
        [ 0.0171, -0.0101,  0.0061,  ..., -0.0036, -0.0050, -0.0058],
        [ 0.0009, -0.0080, -0.0040,  ..., -0.0170, -0.0108, -0.0069],
        ...,
        [-0.0004,  0.0124, -0.0171,  ..., -0.0063, -0.0001,  0.0003],
        [ 0.0165, -0.0133, -0.0112,  ..., -0.0195, -0.0003,  0.0057],
        [-0.0150, -0.0137, -0.0067,  ...,  0.0048, -0.0191,  0.0226]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9395, -2.9219, -5.5078,  ...,  0.1479, -6.3203,  0.7314]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 11:36:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for machiavelli was italian
newton was english
lennon was english
hitler was german
kepler was german
jolie was american
hawking was english
raphael was
2024-07-19 11:36:14 root INFO     [order_1_approx] starting weight calculation for hitler was german
kepler was german
machiavelli was italian
hawking was english
newton was english
lennon was english
raphael was italian
jolie was
2024-07-19 11:36:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 11:40:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0933, -0.4365,  0.4243,  ...,  0.3000, -0.0442,  0.4785],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.8340,  1.2891, -3.9688,  ..., -2.9570,  1.1328, -2.7891],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0235,  0.0099, -0.0029,  ..., -0.0034,  0.0002, -0.0025],
        [ 0.0089,  0.0121, -0.0069,  ...,  0.0159, -0.0071,  0.0045],
        [-0.0084, -0.0018,  0.0098,  ..., -0.0001, -0.0180,  0.0182],
        ...,
        [ 0.0171,  0.0032, -0.0203,  ...,  0.0339,  0.0199,  0.0020],
        [-0.0144, -0.0033,  0.0013,  ...,  0.0211, -0.0143, -0.0224],
        [ 0.0014, -0.0216,  0.0115,  ..., -0.0052,  0.0082,  0.0222]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6494,  1.0273, -3.0918,  ..., -3.0371,  0.3745, -2.1055]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 11:40:33 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for hitler was german
kepler was german
machiavelli was italian
hawking was english
newton was english
lennon was english
raphael was italian
jolie was
2024-07-19 11:40:33 root INFO     [order_1_approx] starting weight calculation for machiavelli was italian
jolie was american
hawking was english
raphael was italian
newton was english
hitler was german
lennon was english
kepler was
2024-07-19 11:40:33 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 11:44:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4990, -0.0417,  0.1742,  ...,  0.8779, -0.3042, -0.1080],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.2656, -1.2344, -5.1328,  ..., -3.7676, -1.4678, -0.7451],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0105,  0.0069, -0.0035,  ..., -0.0029, -0.0023,  0.0008],
        [ 0.0176, -0.0003, -0.0007,  ..., -0.0124, -0.0032, -0.0223],
        [ 0.0376, -0.0322, -0.0482,  ..., -0.0100,  0.0093, -0.0313],
        ...,
        [ 0.0066, -0.0153, -0.0467,  ...,  0.0192,  0.0052, -0.0061],
        [ 0.0413, -0.0250, -0.0030,  ..., -0.0125,  0.0122, -0.0075],
        [-0.0159,  0.0136, -0.0042,  ..., -0.0128,  0.0217,  0.0033]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.5566, -0.7964, -3.3281,  ..., -1.4531, -1.6475,  0.7930]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 11:44:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for machiavelli was italian
jolie was american
hawking was english
raphael was italian
newton was english
hitler was german
lennon was english
kepler was
2024-07-19 11:44:54 root INFO     [order_1_approx] starting weight calculation for hitler was german
kepler was german
lennon was english
newton was english
raphael was italian
machiavelli was italian
jolie was american
hawking was
2024-07-19 11:44:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 11:49:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5039,  0.6924, -0.6934,  ...,  0.4507,  0.3997,  0.9780],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.6152,  0.2319, -4.1641,  ..., -1.3613,  0.6973, -1.2383],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0357, -0.0212, -0.0113,  ..., -0.0171,  0.0159, -0.0114],
        [-0.0043,  0.0044, -0.0028,  ...,  0.0291, -0.0228,  0.0020],
        [-0.0048,  0.0075, -0.0067,  ..., -0.0217,  0.0132,  0.0097],
        ...,
        [-0.0287, -0.0062,  0.0054,  ..., -0.0060, -0.0090,  0.0175],
        [-0.0105, -0.0135,  0.0194,  ...,  0.0275, -0.0127,  0.0140],
        [-0.0110,  0.0188, -0.0083,  ..., -0.0368, -0.0108,  0.0037]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.8496,  0.2852, -4.0742,  ..., -1.7803,  0.6626, -0.8452]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 11:49:13 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for hitler was german
kepler was german
lennon was english
newton was english
raphael was italian
machiavelli was italian
jolie was american
hawking was
2024-07-19 11:49:13 root INFO     total operator prediction time: 2078.208801984787 seconds
2024-07-19 11:49:13 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on country - language
2024-07-19 11:49:13 root INFO     building operator country - language
2024-07-19 11:49:13 root INFO     [order_1_approx] starting weight calculation for The country of peru primarily speaks the language of spanish
The country of denmark primarily speaks the language of danish
The country of haiti primarily speaks the language of creole
The country of kuwait primarily speaks the language of arabic
The country of iraq primarily speaks the language of arabic
The country of jordan primarily speaks the language of arabic
The country of colombia primarily speaks the language of spanish
The country of austria primarily speaks the language of
2024-07-19 11:49:13 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 11:53:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0090, -0.3464, -0.2976,  ..., -1.2637,  0.6841,  0.3506],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7402, -3.3750, -3.2637,  ..., -5.3359, -0.5552, -4.9609],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-3.7727e-03, -7.8506e-03, -5.9814e-03,  ..., -1.8341e-02,
         -6.9466e-03, -5.7869e-03],
        [-7.3204e-03, -1.8402e-02,  2.9434e-02,  ...,  1.2062e-02,
         -8.0490e-03, -1.5993e-03],
        [ 2.7664e-02,  3.6499e-02, -3.3020e-02,  ..., -2.0203e-02,
          1.8982e-02, -4.4594e-03],
        ...,
        [ 1.2741e-02,  8.1787e-03, -1.6663e-02,  ..., -6.2866e-03,
          8.6594e-03,  1.4427e-02],
        [ 1.2350e-03,  6.1378e-03, -1.7761e-02,  ..., -7.2136e-03,
         -9.2621e-03,  6.4240e-03],
        [ 2.0981e-05,  7.1983e-03,  4.5166e-03,  ..., -5.5389e-03,
          6.6376e-03, -4.2496e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0703, -2.1426, -3.3457,  ..., -5.4688, -1.1270, -4.7500]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 11:53:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of peru primarily speaks the language of spanish
The country of denmark primarily speaks the language of danish
The country of haiti primarily speaks the language of creole
The country of kuwait primarily speaks the language of arabic
The country of iraq primarily speaks the language of arabic
The country of jordan primarily speaks the language of arabic
The country of colombia primarily speaks the language of spanish
The country of austria primarily speaks the language of
2024-07-19 11:53:32 root INFO     [order_1_approx] starting weight calculation for The country of peru primarily speaks the language of spanish
The country of austria primarily speaks the language of german
The country of denmark primarily speaks the language of danish
The country of kuwait primarily speaks the language of arabic
The country of iraq primarily speaks the language of arabic
The country of haiti primarily speaks the language of creole
The country of colombia primarily speaks the language of spanish
The country of jordan primarily speaks the language of
2024-07-19 11:53:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 11:57:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1309, -0.3679, -0.5205,  ..., -1.5117,  0.6021, -0.6577],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.3242, -5.5547, -4.8359,  ..., -5.8672, -1.7598, -1.4766],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0092, -0.0128,  0.0106,  ..., -0.0025, -0.0030, -0.0059],
        [ 0.0005,  0.0136,  0.0142,  ...,  0.0161,  0.0085,  0.0019],
        [-0.0049, -0.0048, -0.0036,  ...,  0.0110,  0.0092, -0.0095],
        ...,
        [ 0.0012, -0.0215, -0.0108,  ..., -0.0178, -0.0031, -0.0040],
        [ 0.0047, -0.0019, -0.0098,  ..., -0.0261,  0.0012, -0.0062],
        [-0.0067,  0.0105,  0.0040,  ..., -0.0055,  0.0092,  0.0099]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.7969, -5.9375, -4.4492,  ..., -5.5234, -0.9731, -0.9688]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 11:57:51 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of peru primarily speaks the language of spanish
The country of austria primarily speaks the language of german
The country of denmark primarily speaks the language of danish
The country of kuwait primarily speaks the language of arabic
The country of iraq primarily speaks the language of arabic
The country of haiti primarily speaks the language of creole
The country of colombia primarily speaks the language of spanish
The country of jordan primarily speaks the language of
2024-07-19 11:57:51 root INFO     [order_1_approx] starting weight calculation for The country of kuwait primarily speaks the language of arabic
The country of iraq primarily speaks the language of arabic
The country of haiti primarily speaks the language of creole
The country of jordan primarily speaks the language of arabic
The country of colombia primarily speaks the language of spanish
The country of austria primarily speaks the language of german
The country of peru primarily speaks the language of spanish
The country of denmark primarily speaks the language of
2024-07-19 11:57:51 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 12:02:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5137,  0.1255,  0.0408,  ..., -0.6558, -0.2495, -0.1562],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.2383, -4.4883, -3.0273,  ..., -4.9297, -1.9795,  0.8936],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0067, -0.0002, -0.0118,  ...,  0.0072, -0.0010, -0.0142],
        [-0.0174, -0.0029,  0.0022,  ...,  0.0089,  0.0134, -0.0168],
        [ 0.0091,  0.0217, -0.0114,  ..., -0.0145,  0.0017,  0.0146],
        ...,
        [-0.0005, -0.0037, -0.0117,  ...,  0.0073, -0.0103, -0.0282],
        [ 0.0080,  0.0066, -0.0148,  ...,  0.0111, -0.0117,  0.0173],
        [-0.0128, -0.0055,  0.0072,  ...,  0.0105, -0.0011,  0.0182]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.9072, -4.1289, -3.1621,  ..., -4.7734, -2.3086,  0.7510]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 12:02:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of kuwait primarily speaks the language of arabic
The country of iraq primarily speaks the language of arabic
The country of haiti primarily speaks the language of creole
The country of jordan primarily speaks the language of arabic
The country of colombia primarily speaks the language of spanish
The country of austria primarily speaks the language of german
The country of peru primarily speaks the language of spanish
The country of denmark primarily speaks the language of
2024-07-19 12:02:09 root INFO     [order_1_approx] starting weight calculation for The country of kuwait primarily speaks the language of arabic
The country of jordan primarily speaks the language of arabic
The country of denmark primarily speaks the language of danish
The country of austria primarily speaks the language of german
The country of colombia primarily speaks the language of spanish
The country of iraq primarily speaks the language of arabic
The country of peru primarily speaks the language of spanish
The country of haiti primarily speaks the language of
2024-07-19 12:02:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 12:06:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5093,  0.2563,  0.0981,  ..., -0.2935, -0.4590,  0.3547],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 5.4414, -2.2148, -7.4961,  ..., -0.7148, -3.1016, -0.7705],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0079, -0.0092, -0.0008,  ...,  0.0015,  0.0143,  0.0014],
        [-0.0048,  0.0043, -0.0037,  ..., -0.0049,  0.0059, -0.0075],
        [ 0.0152,  0.0014,  0.0038,  ...,  0.0119, -0.0110,  0.0033],
        ...,
        [-0.0047, -0.0201, -0.0057,  ...,  0.0165, -0.0176,  0.0112],
        [-0.0095,  0.0005,  0.0060,  ...,  0.0214, -0.0090,  0.0130],
        [ 0.0054, -0.0043,  0.0024,  ..., -0.0040,  0.0029, -0.0066]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.0703, -2.5000, -8.2422,  ..., -0.9072, -3.7480, -0.8120]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 12:06:27 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of kuwait primarily speaks the language of arabic
The country of jordan primarily speaks the language of arabic
The country of denmark primarily speaks the language of danish
The country of austria primarily speaks the language of german
The country of colombia primarily speaks the language of spanish
The country of iraq primarily speaks the language of arabic
The country of peru primarily speaks the language of spanish
The country of haiti primarily speaks the language of
2024-07-19 12:06:27 root INFO     [order_1_approx] starting weight calculation for The country of austria primarily speaks the language of german
The country of haiti primarily speaks the language of creole
The country of denmark primarily speaks the language of danish
The country of iraq primarily speaks the language of arabic
The country of jordan primarily speaks the language of arabic
The country of peru primarily speaks the language of spanish
The country of colombia primarily speaks the language of spanish
The country of kuwait primarily speaks the language of
2024-07-19 12:06:27 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 12:10:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0221,  0.4937, -0.0610,  ...,  0.5781,  1.0547, -0.2512],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.6133, -3.3926, -6.0039,  ..., -3.2324, -1.7812, -0.3843],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 6.1607e-04,  6.0577e-03, -8.0719e-03,  ...,  3.0861e-03,
         -7.4863e-05, -5.5466e-03],
        [-1.0941e-02,  6.5117e-03, -9.0790e-03,  ...,  8.6975e-03,
          1.0956e-02, -1.8749e-03],
        [ 3.3913e-03,  1.2665e-02, -1.7059e-02,  ...,  3.1853e-04,
         -8.7738e-05,  1.1368e-02],
        ...,
        [ 4.7722e-03, -4.2572e-03,  9.0790e-04,  ...,  6.3858e-03,
         -4.4479e-03,  3.4599e-03],
        [-3.6945e-03,  8.1329e-03, -6.1989e-05,  ..., -4.6682e-04,
         -1.6830e-02,  1.1414e-02],
        [-8.2932e-03, -2.1118e-02,  3.6621e-03,  ..., -2.5063e-03,
         -5.2795e-03,  3.8261e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.5371, -3.3555, -6.0742,  ..., -4.0625, -2.8652, -0.3130]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 12:10:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of austria primarily speaks the language of german
The country of haiti primarily speaks the language of creole
The country of denmark primarily speaks the language of danish
The country of iraq primarily speaks the language of arabic
The country of jordan primarily speaks the language of arabic
The country of peru primarily speaks the language of spanish
The country of colombia primarily speaks the language of spanish
The country of kuwait primarily speaks the language of
2024-07-19 12:10:46 root INFO     [order_1_approx] starting weight calculation for The country of peru primarily speaks the language of spanish
The country of denmark primarily speaks the language of danish
The country of colombia primarily speaks the language of spanish
The country of austria primarily speaks the language of german
The country of kuwait primarily speaks the language of arabic
The country of haiti primarily speaks the language of creole
The country of jordan primarily speaks the language of arabic
The country of iraq primarily speaks the language of
2024-07-19 12:10:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 12:15:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1133, -0.2759, -0.6958,  ...,  0.5957,  0.4163,  0.3604],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.5078, -4.4961, -5.7148,  ..., -6.3516, -5.1250, -1.3281],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-8.6594e-03, -6.8970e-03, -1.0605e-02,  ...,  7.0267e-03,
         -4.1428e-03,  5.7983e-04],
        [-1.0712e-02, -1.3710e-02,  2.7637e-03,  ..., -3.5954e-03,
         -6.2904e-03, -6.5994e-03],
        [-9.7198e-03, -3.3760e-04,  9.1171e-03,  ...,  1.7242e-02,
         -1.1810e-02, -1.4763e-03],
        ...,
        [-2.2068e-03,  2.4242e-03,  4.1351e-03,  ..., -4.5280e-03,
         -4.6883e-03, -1.1864e-03],
        [ 3.8605e-03,  2.0523e-03, -5.2261e-03,  ..., -1.1993e-02,
          5.5161e-03,  1.7300e-03],
        [ 4.0207e-03, -7.3280e-03,  2.2888e-05,  ..., -4.6806e-03,
          3.8376e-03,  8.5907e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.7812, -3.9922, -6.4922,  ..., -7.1211, -5.2891, -0.7627]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 12:15:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of peru primarily speaks the language of spanish
The country of denmark primarily speaks the language of danish
The country of colombia primarily speaks the language of spanish
The country of austria primarily speaks the language of german
The country of kuwait primarily speaks the language of arabic
The country of haiti primarily speaks the language of creole
The country of jordan primarily speaks the language of arabic
The country of iraq primarily speaks the language of
2024-07-19 12:15:06 root INFO     [order_1_approx] starting weight calculation for The country of denmark primarily speaks the language of danish
The country of austria primarily speaks the language of german
The country of iraq primarily speaks the language of arabic
The country of kuwait primarily speaks the language of arabic
The country of haiti primarily speaks the language of creole
The country of peru primarily speaks the language of spanish
The country of jordan primarily speaks the language of arabic
The country of colombia primarily speaks the language of
2024-07-19 12:15:07 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 12:19:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6777,  0.1788, -0.5806,  ...,  0.3193,  1.1357, -0.3423],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.8291, -3.5781, -4.0703,  ..., -5.2695, -0.4839, -2.8301],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0035, -0.0232, -0.0043,  ..., -0.0036,  0.0070,  0.0036],
        [-0.0075,  0.0056,  0.0106,  ..., -0.0060, -0.0020, -0.0078],
        [ 0.0323,  0.0026, -0.0043,  ..., -0.0180, -0.0057, -0.0128],
        ...,
        [ 0.0230, -0.0019, -0.0062,  ..., -0.0086,  0.0086, -0.0289],
        [ 0.0120, -0.0033, -0.0123,  ..., -0.0101,  0.0045, -0.0244],
        [ 0.0126, -0.0217, -0.0064,  ..., -0.0219,  0.0124, -0.0221]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7920, -2.7969, -3.3242,  ..., -3.7773,  0.0557, -2.0527]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 12:19:25 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of denmark primarily speaks the language of danish
The country of austria primarily speaks the language of german
The country of iraq primarily speaks the language of arabic
The country of kuwait primarily speaks the language of arabic
The country of haiti primarily speaks the language of creole
The country of peru primarily speaks the language of spanish
The country of jordan primarily speaks the language of arabic
The country of colombia primarily speaks the language of
2024-07-19 12:19:25 root INFO     [order_1_approx] starting weight calculation for The country of denmark primarily speaks the language of danish
The country of kuwait primarily speaks the language of arabic
The country of iraq primarily speaks the language of arabic
The country of austria primarily speaks the language of german
The country of haiti primarily speaks the language of creole
The country of jordan primarily speaks the language of arabic
The country of colombia primarily speaks the language of spanish
The country of peru primarily speaks the language of
2024-07-19 12:19:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 12:23:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1565, -0.9585, -2.1777,  ..., -0.4653,  0.7949, -0.0236],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.2266, -0.5298, -8.5859,  ..., -1.9053,  2.9199, -2.2754],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0272, -0.0219,  0.0047,  ...,  0.0310, -0.0149,  0.0139],
        [-0.0306,  0.0256,  0.0150,  ...,  0.0092,  0.0035, -0.0086],
        [ 0.0741, -0.0442, -0.0479,  ..., -0.0514,  0.0303, -0.0002],
        ...,
        [ 0.0062, -0.0189, -0.0011,  ...,  0.0026,  0.0114, -0.0085],
        [ 0.0005, -0.0374, -0.0066,  ..., -0.0084, -0.0006, -0.0096],
        [-0.0070,  0.0003, -0.0051,  ..., -0.0128,  0.0142, -0.0091]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.9141, -0.0469, -7.1562,  ..., -0.8281,  3.6016, -1.2012]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 12:23:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country of denmark primarily speaks the language of danish
The country of kuwait primarily speaks the language of arabic
The country of iraq primarily speaks the language of arabic
The country of austria primarily speaks the language of german
The country of haiti primarily speaks the language of creole
The country of jordan primarily speaks the language of arabic
The country of colombia primarily speaks the language of spanish
The country of peru primarily speaks the language of
2024-07-19 12:23:43 root INFO     total operator prediction time: 2070.4921753406525 seconds
2024-07-19 12:23:43 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on animal - shelter
2024-07-19 12:23:43 root INFO     building operator animal - shelter
2024-07-19 12:23:44 root INFO     [order_1_approx] starting weight calculation for The place monkey lives in is called tree
The place ape lives in is called grove
The place ant lives in is called anthill
The place hamster lives in is called nest
The place mallard lives in is called nest
The place hornet lives in is called nest
The place gorilla lives in is called grove
The place termite lives in is called
2024-07-19 12:23:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 12:28:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8789, -1.1787, -0.1931,  ...,  0.1877, -0.5371, -0.7783],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7261, -3.4727, -1.2529,  ..., -2.9297,  3.7754,  2.2773],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0046, -0.0012, -0.0129,  ...,  0.0105, -0.0039,  0.0040],
        [-0.0088, -0.0125,  0.0089,  ...,  0.0110,  0.0142,  0.0155],
        [ 0.0030,  0.0207, -0.0046,  ...,  0.0072, -0.0084,  0.0040],
        ...,
        [-0.0084, -0.0003, -0.0161,  ...,  0.0194,  0.0014,  0.0190],
        [-0.0060, -0.0076,  0.0185,  ..., -0.0021, -0.0046,  0.0147],
        [-0.0042,  0.0025,  0.0023,  ...,  0.0004,  0.0105,  0.0048]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.2898, -3.5078, -2.3613,  ..., -3.3652,  4.1992,  2.2812]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 12:28:06 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place monkey lives in is called tree
The place ape lives in is called grove
The place ant lives in is called anthill
The place hamster lives in is called nest
The place mallard lives in is called nest
The place hornet lives in is called nest
The place gorilla lives in is called grove
The place termite lives in is called
2024-07-19 12:28:06 root INFO     [order_1_approx] starting weight calculation for The place ape lives in is called grove
The place termite lives in is called hill
The place hamster lives in is called nest
The place mallard lives in is called nest
The place monkey lives in is called tree
The place ant lives in is called anthill
The place hornet lives in is called nest
The place gorilla lives in is called
2024-07-19 12:28:06 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 12:32:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4915, -0.6211, -1.2705,  ..., -0.1371, -0.6133,  0.7261],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.9180, -2.3906, -3.6973,  ..., -0.7915,  1.9824, -0.8853],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-5.0926e-03, -1.3519e-02, -1.2512e-03,  ..., -2.0180e-03,
          1.3107e-02, -2.2240e-03],
        [-3.9215e-03,  5.3864e-03,  3.2135e-02,  ...,  1.3344e-02,
         -2.2354e-02,  1.2512e-03],
        [ 4.0817e-03,  9.1705e-03,  3.0365e-03,  ..., -2.3766e-03,
          5.6610e-03,  1.2894e-02],
        ...,
        [-1.6983e-02, -4.1962e-04,  5.5237e-03,  ...,  9.5825e-03,
         -1.0704e-02,  6.2103e-03],
        [-3.8204e-03,  1.6193e-03,  1.2337e-02,  ..., -5.3329e-03,
         -1.5144e-03,  1.0292e-02],
        [ 1.2558e-02,  3.2120e-03,  7.2479e-05,  ..., -8.3694e-03,
          9.4681e-03,  3.5782e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.5586, -2.2988, -4.4336,  ..., -2.0977,  1.9463, -0.5830]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 12:32:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place ape lives in is called grove
The place termite lives in is called hill
The place hamster lives in is called nest
The place mallard lives in is called nest
The place monkey lives in is called tree
The place ant lives in is called anthill
The place hornet lives in is called nest
The place gorilla lives in is called
2024-07-19 12:32:28 root INFO     [order_1_approx] starting weight calculation for The place monkey lives in is called tree
The place ant lives in is called anthill
The place ape lives in is called grove
The place termite lives in is called hill
The place hornet lives in is called nest
The place mallard lives in is called nest
The place gorilla lives in is called grove
The place hamster lives in is called
2024-07-19 12:32:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 12:36:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0706,  0.2917, -0.6162,  ..., -0.7783, -0.8677,  0.7090],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-3.2383, -2.3438, -1.8398,  ..., -6.6953, -0.6138,  1.7344],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0289,  0.0004, -0.0208,  ...,  0.0107, -0.0028, -0.0045],
        [-0.0117, -0.0016,  0.0045,  ...,  0.0371, -0.0103,  0.0059],
        [ 0.0220,  0.0165, -0.0002,  ..., -0.0058,  0.0048, -0.0022],
        ...,
        [-0.0026,  0.0001,  0.0049,  ...,  0.0004, -0.0166,  0.0167],
        [ 0.0057, -0.0018,  0.0021,  ..., -0.0273, -0.0010,  0.0144],
        [-0.0008, -0.0144,  0.0147,  ..., -0.0043,  0.0039,  0.0062]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.8711, -2.6836, -1.6230,  ..., -6.8867, -0.3445,  1.3271]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 12:36:47 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place monkey lives in is called tree
The place ant lives in is called anthill
The place ape lives in is called grove
The place termite lives in is called hill
The place hornet lives in is called nest
The place mallard lives in is called nest
The place gorilla lives in is called grove
The place hamster lives in is called
2024-07-19 12:36:47 root INFO     [order_1_approx] starting weight calculation for The place ape lives in is called grove
The place ant lives in is called anthill
The place gorilla lives in is called grove
The place hamster lives in is called nest
The place hornet lives in is called nest
The place termite lives in is called hill
The place mallard lives in is called nest
The place monkey lives in is called
2024-07-19 12:36:47 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 12:41:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1033, -0.8076, -1.0273,  ...,  0.3586, -0.7637, -0.2148],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.7969, -2.6289, -2.2676,  ..., -0.2825,  0.7148, -0.5415],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0052, -0.0053, -0.0090,  ...,  0.0190, -0.0011, -0.0025],
        [-0.0173,  0.0064,  0.0169,  ...,  0.0164, -0.0079,  0.0025],
        [ 0.0093,  0.0093, -0.0143,  ..., -0.0019, -0.0068, -0.0016],
        ...,
        [-0.0143, -0.0043,  0.0088,  ...,  0.0245, -0.0237,  0.0073],
        [ 0.0011,  0.0146, -0.0043,  ..., -0.0083, -0.0086,  0.0192],
        [-0.0052, -0.0026,  0.0037,  ..., -0.0113,  0.0083, -0.0045]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.0859, -3.0898, -2.4863,  ..., -1.7705,  0.5195,  0.1846]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 12:41:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place ape lives in is called grove
The place ant lives in is called anthill
The place gorilla lives in is called grove
The place hamster lives in is called nest
The place hornet lives in is called nest
The place termite lives in is called hill
The place mallard lives in is called nest
The place monkey lives in is called
2024-07-19 12:41:09 root INFO     [order_1_approx] starting weight calculation for The place monkey lives in is called tree
The place hamster lives in is called nest
The place termite lives in is called hill
The place hornet lives in is called nest
The place ape lives in is called grove
The place gorilla lives in is called grove
The place mallard lives in is called nest
The place ant lives in is called
2024-07-19 12:41:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 12:45:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2158,  0.4626, -1.0059,  ..., -0.6953, -0.8086,  0.1162],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4722, -3.6934, -1.1094,  ..., -5.8945,  5.4648,  2.5781],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0010, -0.0055,  0.0023,  ..., -0.0002,  0.0019,  0.0127],
        [-0.0179,  0.0007,  0.0166,  ...,  0.0082, -0.0095,  0.0012],
        [-0.0041,  0.0014,  0.0068,  ...,  0.0108,  0.0032,  0.0181],
        ...,
        [-0.0218, -0.0044,  0.0059,  ...,  0.0096, -0.0068, -0.0046],
        [-0.0053,  0.0026,  0.0010,  ...,  0.0032, -0.0065,  0.0171],
        [-0.0032,  0.0058,  0.0009,  ...,  0.0026,  0.0047,  0.0001]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.5352, -4.3711, -0.9463,  ..., -6.1406,  4.5117,  1.9922]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 12:45:31 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place monkey lives in is called tree
The place hamster lives in is called nest
The place termite lives in is called hill
The place hornet lives in is called nest
The place ape lives in is called grove
The place gorilla lives in is called grove
The place mallard lives in is called nest
The place ant lives in is called
2024-07-19 12:45:31 root INFO     [order_1_approx] starting weight calculation for The place hornet lives in is called nest
The place ant lives in is called anthill
The place termite lives in is called hill
The place mallard lives in is called nest
The place monkey lives in is called tree
The place hamster lives in is called nest
The place gorilla lives in is called grove
The place ape lives in is called
2024-07-19 12:45:31 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 12:49:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5503, -0.9058, -1.3359,  ..., -0.2529, -0.1967, -0.0585],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.3086, -1.9062, -2.3945,  ..., -2.4062,  2.4805,  1.5186],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-2.6932e-03, -1.5091e-02, -2.3804e-03,  ..., -3.3035e-03,
          5.1079e-03,  7.9193e-03],
        [-1.2894e-02,  8.2779e-03,  2.0340e-02,  ...,  1.6296e-02,
         -2.7740e-02, -7.6447e-03],
        [ 2.1729e-02, -3.9978e-03,  1.7136e-02,  ..., -1.9073e-05,
          5.3101e-03,  1.6907e-02],
        ...,
        [-8.9569e-03, -1.0979e-02,  6.2790e-03,  ...,  1.2344e-02,
         -1.2497e-02,  1.9012e-02],
        [ 5.7220e-05,  3.2806e-04,  1.2161e-02,  ..., -4.1733e-03,
         -1.7395e-03,  2.2324e-02],
        [ 1.5965e-03,  5.8899e-03,  7.5188e-03,  ...,  4.4708e-03,
          2.4414e-03,  1.6022e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.0547, -2.7305, -1.6299,  ..., -2.6543,  2.3965,  2.1348]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 12:49:53 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place hornet lives in is called nest
The place ant lives in is called anthill
The place termite lives in is called hill
The place mallard lives in is called nest
The place monkey lives in is called tree
The place hamster lives in is called nest
The place gorilla lives in is called grove
The place ape lives in is called
2024-07-19 12:49:53 root INFO     [order_1_approx] starting weight calculation for The place hamster lives in is called nest
The place ape lives in is called grove
The place monkey lives in is called tree
The place termite lives in is called hill
The place mallard lives in is called nest
The place ant lives in is called anthill
The place gorilla lives in is called grove
The place hornet lives in is called
2024-07-19 12:49:53 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 12:54:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3176, -0.4189, -0.1073,  ...,  0.5317,  0.1628,  0.9927],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.2002, -2.4238,  0.0215,  ..., -2.8047,  4.3594,  2.8770],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-5.6458e-03, -1.1703e-02, -1.5625e-02,  ...,  3.3207e-03,
          5.8670e-03, -7.0038e-03],
        [-4.3488e-03,  1.8120e-03,  5.3749e-03,  ...,  5.8594e-03,
         -8.0948e-03,  5.3406e-05],
        [-1.0437e-02,  3.8319e-03, -5.4550e-03,  ...,  1.2230e-02,
          4.4441e-03, -2.1629e-03],
        ...,
        [-2.7084e-03, -2.4605e-03, -1.4435e-02,  ...,  1.1703e-02,
         -1.3596e-02,  7.9498e-03],
        [-2.1011e-02, -1.0231e-02,  9.0942e-03,  ..., -4.6463e-03,
         -4.9095e-03,  1.7242e-02],
        [-5.2834e-03,  1.3094e-03,  8.8882e-04,  ..., -3.3283e-03,
          1.2642e-02,  5.4741e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.1989, -2.7520, -0.0763,  ..., -3.4922,  4.2969,  2.6426]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 12:54:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place hamster lives in is called nest
The place ape lives in is called grove
The place monkey lives in is called tree
The place termite lives in is called hill
The place mallard lives in is called nest
The place ant lives in is called anthill
The place gorilla lives in is called grove
The place hornet lives in is called
2024-07-19 12:54:15 root INFO     [order_1_approx] starting weight calculation for The place hamster lives in is called nest
The place monkey lives in is called tree
The place ant lives in is called anthill
The place ape lives in is called grove
The place gorilla lives in is called grove
The place termite lives in is called hill
The place hornet lives in is called nest
The place mallard lives in is called
2024-07-19 12:54:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 12:58:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2959,  0.1653, -0.5464,  ...,  0.2544, -1.0898,  0.1978],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.3711, -0.6606, -5.4766,  ..., -1.2637, -0.0291, -1.5820],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0040, -0.0003, -0.0027,  ...,  0.0017, -0.0005,  0.0052],
        [-0.0093, -0.0036,  0.0144,  ...,  0.0004,  0.0105, -0.0048],
        [ 0.0073,  0.0103, -0.0011,  ...,  0.0007,  0.0041,  0.0021],
        ...,
        [ 0.0029, -0.0027,  0.0066,  ...,  0.0049, -0.0129,  0.0047],
        [-0.0072, -0.0035,  0.0068,  ..., -0.0069,  0.0025,  0.0008],
        [-0.0191, -0.0046,  0.0059,  ..., -0.0018,  0.0029, -0.0042]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.4902, -0.6934, -6.1133,  ..., -1.1875,  0.0150, -1.6826]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 12:58:37 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The place hamster lives in is called nest
The place monkey lives in is called tree
The place ant lives in is called anthill
The place ape lives in is called grove
The place gorilla lives in is called grove
The place termite lives in is called hill
The place hornet lives in is called nest
The place mallard lives in is called
2024-07-19 12:58:37 root INFO     total operator prediction time: 2093.5701117515564 seconds
2024-07-19 12:58:37 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on male - female
2024-07-19 12:58:37 root INFO     building operator male - female
2024-07-19 12:58:37 root INFO     [order_1_approx] starting weight calculation for A female webmaster is known as a webmistress
A female god is known as a goddess
A female murderer is known as a murderess
A female grandson is known as a granddaughter
A female sculptor is known as a sculptress
A female groom is known as a bride
A female valet is known as a maid
A female chairman is known as a
2024-07-19 12:58:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 13:02:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0845, -0.2788,  0.4358,  ..., -0.4119, -0.5327, -0.0214],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.5312, -1.2373,  1.7402,  ..., -1.7715,  1.1543, -0.7925],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0123, -0.0256,  0.0231,  ...,  0.0146,  0.0003, -0.0227],
        [-0.0010, -0.0247,  0.0032,  ..., -0.0043,  0.0142,  0.0066],
        [ 0.0258, -0.0084, -0.0330,  ...,  0.0071,  0.0055, -0.0177],
        ...,
        [-0.0308,  0.0035, -0.0136,  ..., -0.0121, -0.0150, -0.0221],
        [ 0.0251,  0.0173,  0.0140,  ..., -0.0094, -0.0255,  0.0178],
        [-0.0112,  0.0054, -0.0048,  ...,  0.0029,  0.0143, -0.0127]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.1562, -1.6680,  0.6738,  ..., -1.5059,  1.5156, -0.9561]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 13:02:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female webmaster is known as a webmistress
A female god is known as a goddess
A female murderer is known as a murderess
A female grandson is known as a granddaughter
A female sculptor is known as a sculptress
A female groom is known as a bride
A female valet is known as a maid
A female chairman is known as a
2024-07-19 13:02:56 root INFO     [order_1_approx] starting weight calculation for A female groom is known as a bride
A female chairman is known as a chairwoman
A female murderer is known as a murderess
A female grandson is known as a granddaughter
A female god is known as a goddess
A female sculptor is known as a sculptress
A female valet is known as a maid
A female webmaster is known as a
2024-07-19 13:02:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 13:07:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1736,  1.4746,  1.6035,  ..., -0.0978,  0.3311,  1.2324],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.8262, -3.2266, -2.1367,  ..., -1.2598,  0.1724, -0.1082],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0295,  0.0211, -0.0180,  ...,  0.0181, -0.0269, -0.0229],
        [-0.0007, -0.0156, -0.0109,  ..., -0.0004, -0.0091, -0.0110],
        [ 0.0563,  0.0284, -0.0396,  ..., -0.0209, -0.0192,  0.0035],
        ...,
        [-0.0060, -0.0003, -0.0072,  ..., -0.0178, -0.0028, -0.0200],
        [ 0.0125, -0.0008,  0.0117,  ...,  0.0104, -0.0166,  0.0015],
        [ 0.0311,  0.0293,  0.0028,  ...,  0.0094, -0.0180,  0.0005]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.2598, -3.3379, -1.9326,  ..., -1.2588,  0.8945,  0.0688]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 13:07:18 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female groom is known as a bride
A female chairman is known as a chairwoman
A female murderer is known as a murderess
A female grandson is known as a granddaughter
A female god is known as a goddess
A female sculptor is known as a sculptress
A female valet is known as a maid
A female webmaster is known as a
2024-07-19 13:07:18 root INFO     [order_1_approx] starting weight calculation for A female groom is known as a bride
A female chairman is known as a chairwoman
A female valet is known as a maid
A female webmaster is known as a webmistress
A female sculptor is known as a sculptress
A female grandson is known as a granddaughter
A female murderer is known as a murderess
A female god is known as a
2024-07-19 13:07:18 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 13:11:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.2432, 0.2834, 0.6104,  ..., 0.4690, 0.2334, 0.1392], device='cuda:1',
       dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.5430, -0.8262,  0.1914,  ...,  0.9932,  0.8945, -0.8525],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0205, -0.0158,  0.0236,  ..., -0.0056, -0.0090, -0.0114],
        [ 0.0037, -0.0057, -0.0111,  ...,  0.0104, -0.0007,  0.0018],
        [ 0.0207, -0.0154, -0.0114,  ...,  0.0011, -0.0120, -0.0096],
        ...,
        [-0.0150, -0.0247,  0.0229,  ..., -0.0217, -0.0002,  0.0084],
        [ 0.0003, -0.0066,  0.0077,  ..., -0.0048, -0.0129,  0.0080],
        [-0.0010,  0.0036,  0.0050,  ...,  0.0062, -0.0057, -0.0116]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.8438, -1.4082, -0.3384,  ...,  0.2358,  0.8179, -0.2339]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 13:11:39 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female groom is known as a bride
A female chairman is known as a chairwoman
A female valet is known as a maid
A female webmaster is known as a webmistress
A female sculptor is known as a sculptress
A female grandson is known as a granddaughter
A female murderer is known as a murderess
A female god is known as a
2024-07-19 13:11:40 root INFO     [order_1_approx] starting weight calculation for A female webmaster is known as a webmistress
A female valet is known as a maid
A female murderer is known as a murderess
A female sculptor is known as a sculptress
A female grandson is known as a granddaughter
A female god is known as a goddess
A female chairman is known as a chairwoman
A female groom is known as a
2024-07-19 13:11:40 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 13:16:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5234, -0.4575,  0.3853,  ...,  0.2100, -0.3855,  0.9019],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.6475, -2.5449, -0.1328,  ..., -3.3730, -1.5234,  0.1904],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0138, -0.0374,  0.0390,  ...,  0.0151, -0.0141, -0.0335],
        [ 0.0019, -0.0100, -0.0146,  ...,  0.0093,  0.0346,  0.0081],
        [ 0.0069, -0.0253,  0.0287,  ...,  0.0039, -0.0037, -0.0266],
        ...,
        [ 0.0069, -0.0090, -0.0469,  ...,  0.0076, -0.0083, -0.0033],
        [ 0.0302, -0.0400,  0.0078,  ..., -0.0251, -0.0099,  0.0277],
        [-0.0089,  0.0021, -0.0021,  ..., -0.0004,  0.0121,  0.0123]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.2891, -2.6094, -0.4163,  ..., -3.1094, -2.9141,  0.4077]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 13:16:01 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female webmaster is known as a webmistress
A female valet is known as a maid
A female murderer is known as a murderess
A female sculptor is known as a sculptress
A female grandson is known as a granddaughter
A female god is known as a goddess
A female chairman is known as a chairwoman
A female groom is known as a
2024-07-19 13:16:01 root INFO     [order_1_approx] starting weight calculation for A female webmaster is known as a webmistress
A female murderer is known as a murderess
A female valet is known as a maid
A female groom is known as a bride
A female god is known as a goddess
A female grandson is known as a granddaughter
A female chairman is known as a chairwoman
A female sculptor is known as a
2024-07-19 13:16:01 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 13:20:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2231, -0.6187,  0.3989,  ..., -0.6978,  0.7979,  0.4451],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.8555,  2.2969, -5.0156,  ..., -1.9160, -2.7676, -2.2891],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.1008e-05, -3.8185e-03, -5.4588e-03,  ...,  4.9248e-03,
         -1.5472e-02, -1.7532e-02],
        [-2.3392e-02, -1.8280e-02,  3.2898e-02,  ...,  3.7750e-02,
          9.3689e-03,  1.1574e-02],
        [ 6.1890e-02,  4.2953e-03, -3.0060e-02,  ..., -3.0273e-02,
          8.1940e-03, -1.3733e-02],
        ...,
        [-1.0735e-02, -2.9068e-03,  2.7599e-03,  ..., -1.4465e-02,
          5.6648e-04, -3.9978e-03],
        [ 2.5757e-02, -2.6073e-03, -1.3618e-03,  ..., -1.1261e-02,
         -1.7700e-02, -4.2725e-03],
        [ 1.6365e-03,  1.0216e-02, -3.5744e-03,  ...,  6.9427e-03,
          1.8585e-02, -1.5640e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.8828,  0.2461, -4.0938,  ..., -1.5654, -1.5625, -1.4854]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 13:20:22 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female webmaster is known as a webmistress
A female murderer is known as a murderess
A female valet is known as a maid
A female groom is known as a bride
A female god is known as a goddess
A female grandson is known as a granddaughter
A female chairman is known as a chairwoman
A female sculptor is known as a
2024-07-19 13:20:23 root INFO     [order_1_approx] starting weight calculation for A female groom is known as a bride
A female god is known as a goddess
A female grandson is known as a granddaughter
A female webmaster is known as a webmistress
A female sculptor is known as a sculptress
A female chairman is known as a chairwoman
A female murderer is known as a murderess
A female valet is known as a
2024-07-19 13:20:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 13:24:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2277, -1.0762,  0.5898,  ...,  0.7173, -0.3809, -0.3091],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.6914, -0.8726,  2.3066,  ...,  0.4141,  2.2266,  2.6523],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 6.0120e-03,  8.2855e-03, -5.8365e-04,  ..., -3.4424e-02,
          8.2245e-03, -4.1504e-03],
        [-3.9948e-02, -4.1275e-03,  9.2468e-03,  ...,  2.2598e-02,
          2.2995e-02, -4.1046e-03],
        [ 9.6207e-03,  3.1528e-03,  1.2360e-03,  ...,  1.6815e-02,
          1.6235e-02, -8.1787e-03],
        ...,
        [-3.5645e-02, -1.4572e-02,  2.6894e-03,  ...,  5.4474e-03,
          9.0485e-03,  6.1035e-05],
        [ 7.0114e-03, -2.4994e-02, -1.0071e-03,  ...,  3.6736e-03,
         -1.9684e-03,  1.8219e-02],
        [ 4.2419e-03,  3.2959e-03,  1.2169e-02,  ...,  6.8665e-05,
          1.6880e-03,  1.5841e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.2012, -0.3782,  2.0840,  ...,  1.1426,  1.6523,  2.9316]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 13:24:43 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female groom is known as a bride
A female god is known as a goddess
A female grandson is known as a granddaughter
A female webmaster is known as a webmistress
A female sculptor is known as a sculptress
A female chairman is known as a chairwoman
A female murderer is known as a murderess
A female valet is known as a
2024-07-19 13:24:44 root INFO     [order_1_approx] starting weight calculation for A female groom is known as a bride
A female god is known as a goddess
A female chairman is known as a chairwoman
A female valet is known as a maid
A female murderer is known as a murderess
A female webmaster is known as a webmistress
A female sculptor is known as a sculptress
A female grandson is known as a
2024-07-19 13:24:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 13:29:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4395,  0.6260,  0.8770,  ..., -0.2302, -0.2603,  0.3638],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4922, -2.4492, -1.0439,  ..., -0.4641,  2.2207, -4.3438],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0161, -0.0014,  0.0027,  ...,  0.0196,  0.0096, -0.0330],
        [-0.0158,  0.0041,  0.0147,  ...,  0.0006, -0.0006,  0.0145],
        [ 0.0040, -0.0128, -0.0368,  ..., -0.0021, -0.0061,  0.0102],
        ...,
        [-0.0254, -0.0047, -0.0150,  ..., -0.0262, -0.0042, -0.0156],
        [-0.0009, -0.0028,  0.0089,  ..., -0.0154, -0.0210,  0.0012],
        [ 0.0105, -0.0065, -0.0075,  ...,  0.0122, -0.0052, -0.0096]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.5801, -2.3125, -1.2627,  ..., -0.4023,  2.1074, -4.0469]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 13:29:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female groom is known as a bride
A female god is known as a goddess
A female chairman is known as a chairwoman
A female valet is known as a maid
A female murderer is known as a murderess
A female webmaster is known as a webmistress
A female sculptor is known as a sculptress
A female grandson is known as a
2024-07-19 13:29:04 root INFO     [order_1_approx] starting weight calculation for A female groom is known as a bride
A female god is known as a goddess
A female valet is known as a maid
A female chairman is known as a chairwoman
A female grandson is known as a granddaughter
A female webmaster is known as a webmistress
A female sculptor is known as a sculptress
A female murderer is known as a
2024-07-19 13:29:04 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 13:33:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7368,  0.8623,  0.8335,  ...,  0.0034, -0.7803,  0.1157],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.6211, -3.5078,  0.4033,  ...,  0.4609,  4.0625, -1.5898],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0186, -0.0191,  0.0043,  ...,  0.0061, -0.0156, -0.0119],
        [ 0.0093, -0.0283,  0.0050,  ..., -0.0042, -0.0086,  0.0034],
        [-0.0042,  0.0024, -0.0236,  ...,  0.0105, -0.0102,  0.0051],
        ...,
        [-0.0304, -0.0155, -0.0013,  ..., -0.0094, -0.0174, -0.0105],
        [ 0.0077,  0.0088,  0.0327,  ..., -0.0087, -0.0224,  0.0340],
        [-0.0100,  0.0020,  0.0034,  ..., -0.0079, -0.0003, -0.0279]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.7109, -3.3066, -0.2603,  ..., -0.6094,  4.0625, -1.7061]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 13:33:26 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for A female groom is known as a bride
A female god is known as a goddess
A female valet is known as a maid
A female chairman is known as a chairwoman
A female grandson is known as a granddaughter
A female webmaster is known as a webmistress
A female sculptor is known as a sculptress
A female murderer is known as a
2024-07-19 13:33:26 root INFO     total operator prediction time: 2089.0442633628845 seconds
2024-07-19 13:33:26 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on name - occupation
2024-07-19 13:33:26 root INFO     building operator name - occupation
2024-07-19 13:33:26 root INFO     [order_1_approx] starting weight calculation for pascal was known for their work as a  mathematician
dante was known for their work as a  poet
caesar was known for their work as a  emperor
balzac was known for their work as a  novelist
raphael was known for their work as a  painter
marx was known for their work as a  philosopher
mozart was known for their work as a  composer
wittgenstein was known for their work as a 
2024-07-19 13:33:26 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 13:37:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.3887, -0.1279, -0.2198,  ..., -1.5215,  0.2871,  0.1616],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2939, -3.3164, -4.3750,  ..., -1.3828,  1.8398, -2.9141],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-7.2594e-03, -7.8583e-03, -1.3285e-03,  ..., -5.2452e-03,
          1.2268e-02,  1.3008e-02],
        [-1.6510e-02,  5.3940e-03,  1.5488e-02,  ...,  2.9068e-03,
          6.6147e-03, -4.3678e-04],
        [-9.1553e-05, -6.3705e-03,  7.3395e-03,  ...,  1.8311e-04,
          9.2468e-03, -1.0315e-02],
        ...,
        [ 1.5038e-02, -1.2756e-02, -1.0750e-02,  ..., -4.5776e-04,
         -2.6093e-03, -8.3542e-03],
        [ 5.0232e-02, -1.0567e-03, -2.3224e-02,  ...,  8.5602e-03,
         -5.5771e-03, -9.6512e-04],
        [ 3.6896e-02, -1.9341e-03, -2.6886e-02,  ..., -5.4169e-04,
          1.0956e-02, -1.3351e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0710, -3.3965, -5.4297,  ..., -0.7388,  3.1094, -2.1875]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 13:37:46 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for pascal was known for their work as a  mathematician
dante was known for their work as a  poet
caesar was known for their work as a  emperor
balzac was known for their work as a  novelist
raphael was known for their work as a  painter
marx was known for their work as a  philosopher
mozart was known for their work as a  composer
wittgenstein was known for their work as a 
2024-07-19 13:37:46 root INFO     [order_1_approx] starting weight calculation for mozart was known for their work as a  composer
wittgenstein was known for their work as a  philosopher
raphael was known for their work as a  painter
marx was known for their work as a  philosopher
pascal was known for their work as a  mathematician
caesar was known for their work as a  emperor
dante was known for their work as a  poet
balzac was known for their work as a 
2024-07-19 13:37:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 13:42:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8906,  0.3228, -1.2061,  ..., -0.8540, -0.2493,  0.9517],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4619, -1.7471, -2.5566,  ..., -0.7598, -4.0898, -2.2344],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0111, -0.0061, -0.0096,  ..., -0.0125, -0.0058, -0.0035],
        [-0.0251,  0.0228,  0.0172,  ..., -0.0093, -0.0138,  0.0037],
        [ 0.0118,  0.0139,  0.0072,  ..., -0.0003, -0.0035, -0.0094],
        ...,
        [ 0.0334, -0.0113,  0.0102,  ...,  0.0069, -0.0230, -0.0017],
        [-0.0071, -0.0175,  0.0113,  ..., -0.0078, -0.0077, -0.0081],
        [-0.0312, -0.0150, -0.0228,  ...,  0.0007,  0.0184,  0.0359]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.4941, -0.9072, -3.5156,  ..., -1.0967, -4.0430, -2.0059]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 13:42:05 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for mozart was known for their work as a  composer
wittgenstein was known for their work as a  philosopher
raphael was known for their work as a  painter
marx was known for their work as a  philosopher
pascal was known for their work as a  mathematician
caesar was known for their work as a  emperor
dante was known for their work as a  poet
balzac was known for their work as a 
2024-07-19 13:42:05 root INFO     [order_1_approx] starting weight calculation for wittgenstein was known for their work as a  philosopher
caesar was known for their work as a  emperor
marx was known for their work as a  philosopher
raphael was known for their work as a  painter
dante was known for their work as a  poet
balzac was known for their work as a  novelist
mozart was known for their work as a  composer
pascal was known for their work as a 
2024-07-19 13:42:05 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 13:46:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5317, -0.2974, -0.6621,  ..., -0.7539,  0.1212,  0.8530],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.1055, -2.8242, -3.7285,  ..., -3.3828, -0.8975, -0.7637],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.3535e-02, -2.3155e-03,  1.0139e-02,  ..., -5.5885e-04,
          8.9569e-03,  1.8158e-02],
        [-1.0735e-02,  1.1780e-02,  9.5367e-05,  ..., -3.7594e-03,
         -9.8114e-03, -4.9820e-03],
        [ 1.4999e-02, -5.4474e-03, -1.1887e-02,  ...,  8.5449e-03,
          6.5327e-04, -8.0261e-03],
        ...,
        [-3.1891e-03, -2.3911e-02, -1.0941e-02,  ..., -4.8752e-03,
          2.9068e-03,  1.7212e-02],
        [ 4.1565e-02, -1.6983e-02,  4.4441e-03,  ...,  1.7700e-02,
          1.5961e-02, -1.2436e-02],
        [ 4.5166e-03, -8.3389e-03, -9.3460e-03,  ...,  8.6288e-03,
          1.3916e-02,  7.1182e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.9004, -3.4512, -4.1328,  ..., -2.8027, -0.5742, -0.4851]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 13:46:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for wittgenstein was known for their work as a  philosopher
caesar was known for their work as a  emperor
marx was known for their work as a  philosopher
raphael was known for their work as a  painter
dante was known for their work as a  poet
balzac was known for their work as a  novelist
mozart was known for their work as a  composer
pascal was known for their work as a 
2024-07-19 13:46:25 root INFO     [order_1_approx] starting weight calculation for caesar was known for their work as a  emperor
balzac was known for their work as a  novelist
mozart was known for their work as a  composer
raphael was known for their work as a  painter
wittgenstein was known for their work as a  philosopher
dante was known for their work as a  poet
pascal was known for their work as a  mathematician
marx was known for their work as a 
2024-07-19 13:46:25 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 13:50:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1694,  0.3054, -0.3813,  ..., -0.8423, -0.2231,  0.2756],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.4883, -3.9727, -6.7188,  ..., -5.7930, -1.7129, -1.7109],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0068, -0.0049,  0.0088,  ...,  0.0098,  0.0075, -0.0003],
        [-0.0010,  0.0104, -0.0131,  ...,  0.0120,  0.0029,  0.0072],
        [ 0.0041,  0.0043, -0.0028,  ...,  0.0011, -0.0023,  0.0048],
        ...,
        [ 0.0007, -0.0051,  0.0008,  ...,  0.0049, -0.0158,  0.0063],
        [ 0.0226, -0.0070,  0.0112,  ...,  0.0052, -0.0024, -0.0027],
        [-0.0188, -0.0057, -0.0016,  ...,  0.0024, -0.0006,  0.0108]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6958, -4.0781, -6.4844,  ..., -5.4844, -1.9697, -1.4893]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 13:50:44 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for caesar was known for their work as a  emperor
balzac was known for their work as a  novelist
mozart was known for their work as a  composer
raphael was known for their work as a  painter
wittgenstein was known for their work as a  philosopher
dante was known for their work as a  poet
pascal was known for their work as a  mathematician
marx was known for their work as a 
2024-07-19 13:50:44 root INFO     [order_1_approx] starting weight calculation for caesar was known for their work as a  emperor
wittgenstein was known for their work as a  philosopher
balzac was known for their work as a  novelist
marx was known for their work as a  philosopher
mozart was known for their work as a  composer
pascal was known for their work as a  mathematician
raphael was known for their work as a  painter
dante was known for their work as a 
2024-07-19 13:50:44 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 13:55:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0105, -0.1219,  0.4590,  ...,  0.1616, -0.0499, -0.2200],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0750,  2.8379, -6.2969,  ..., -4.8047,  3.4824, -0.6260],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0040, -0.0090, -0.0176,  ...,  0.0017,  0.0117, -0.0047],
        [-0.0099,  0.0191,  0.0393,  ..., -0.0030, -0.0040, -0.0125],
        [ 0.0291, -0.0184, -0.0384,  ...,  0.0183,  0.0090, -0.0077],
        ...,
        [-0.0096,  0.0041,  0.0183,  ..., -0.0029,  0.0079,  0.0167],
        [-0.0003,  0.0139,  0.0234,  ..., -0.0044, -0.0074, -0.0019],
        [-0.0044, -0.0150, -0.0136,  ...,  0.0016,  0.0237, -0.0112]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2117,  2.8203, -6.0703,  ..., -4.9336,  3.2070, -0.2483]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 13:55:03 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for caesar was known for their work as a  emperor
wittgenstein was known for their work as a  philosopher
balzac was known for their work as a  novelist
marx was known for their work as a  philosopher
mozart was known for their work as a  composer
pascal was known for their work as a  mathematician
raphael was known for their work as a  painter
dante was known for their work as a 
2024-07-19 13:55:03 root INFO     [order_1_approx] starting weight calculation for raphael was known for their work as a  painter
balzac was known for their work as a  novelist
wittgenstein was known for their work as a  philosopher
pascal was known for their work as a  mathematician
marx was known for their work as a  philosopher
mozart was known for their work as a  composer
dante was known for their work as a  poet
caesar was known for their work as a 
2024-07-19 13:55:03 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 13:59:22 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3704, -0.3843, -0.4177,  ..., -0.3967,  0.3342,  0.2029],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2321, -0.1360, -5.1172,  ..., -2.6309,  0.3264, -1.3164],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0049, -0.0039, -0.0038,  ..., -0.0050,  0.0047,  0.0138],
        [-0.0352, -0.0030, -0.0074,  ..., -0.0037, -0.0164, -0.0105],
        [ 0.0043,  0.0065, -0.0021,  ..., -0.0064,  0.0099, -0.0087],
        ...,
        [-0.0047, -0.0037, -0.0044,  ..., -0.0090, -0.0033,  0.0192],
        [-0.0170, -0.0103,  0.0037,  ..., -0.0108,  0.0100, -0.0094],
        [-0.0119, -0.0172,  0.0070,  ..., -0.0111, -0.0019,  0.0081]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.7349,  0.8301, -5.1484,  ..., -2.2422, -0.3479, -1.5596]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 13:59:23 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for raphael was known for their work as a  painter
balzac was known for their work as a  novelist
wittgenstein was known for their work as a  philosopher
pascal was known for their work as a  mathematician
marx was known for their work as a  philosopher
mozart was known for their work as a  composer
dante was known for their work as a  poet
caesar was known for their work as a 
2024-07-19 13:59:23 root INFO     [order_1_approx] starting weight calculation for wittgenstein was known for their work as a  philosopher
balzac was known for their work as a  novelist
caesar was known for their work as a  emperor
raphael was known for their work as a  painter
dante was known for their work as a  poet
marx was known for their work as a  philosopher
pascal was known for their work as a  mathematician
mozart was known for their work as a 
2024-07-19 13:59:23 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 14:03:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6538, -0.5786, -0.2065,  ..., -0.8330,  0.2263,  0.0181],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.9980,  0.6387, -1.3701,  ...,  1.4062,  0.8135, -1.8643],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-9.3307e-03, -8.7585e-03,  1.3954e-02,  ...,  6.1035e-05,
          1.9045e-03,  1.1238e-02],
        [-4.8447e-03,  1.2444e-02, -1.9073e-04,  ..., -1.2558e-02,
         -3.6957e-02, -2.7103e-03],
        [ 2.2339e-02,  3.4088e-02, -2.2640e-03,  ..., -6.9466e-03,
         -2.0584e-02, -2.0294e-02],
        ...,
        [-1.9569e-03, -1.1802e-04,  3.6201e-03,  ..., -8.3771e-03,
         -6.1035e-04,  2.0355e-02],
        [ 8.3923e-04,  1.1383e-02, -8.8272e-03,  ...,  2.6608e-04,
          1.1955e-02, -2.4918e-02],
        [-8.5983e-03, -7.8125e-03,  5.1918e-03,  ...,  1.4626e-02,
          3.4546e-02, -2.7924e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.3926,  0.4126, -1.5967,  ...,  0.8599,  1.2637, -1.1992]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 14:03:42 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for wittgenstein was known for their work as a  philosopher
balzac was known for their work as a  novelist
caesar was known for their work as a  emperor
raphael was known for their work as a  painter
dante was known for their work as a  poet
marx was known for their work as a  philosopher
pascal was known for their work as a  mathematician
mozart was known for their work as a 
2024-07-19 14:03:42 root INFO     [order_1_approx] starting weight calculation for caesar was known for their work as a  emperor
pascal was known for their work as a  mathematician
wittgenstein was known for their work as a  philosopher
mozart was known for their work as a  composer
marx was known for their work as a  philosopher
dante was known for their work as a  poet
balzac was known for their work as a  novelist
raphael was known for their work as a 
2024-07-19 14:03:43 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 14:08:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1074, -0.6733, -0.2471,  ...,  0.7969,  1.1602,  0.1394],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3662,  0.1021, -3.9863,  ..., -2.0605, -1.9727,  3.2461],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0072, -0.0055,  0.0008,  ..., -0.0080,  0.0017,  0.0037],
        [-0.0105,  0.0053,  0.0063,  ...,  0.0128, -0.0096, -0.0007],
        [ 0.0126, -0.0031,  0.0096,  ..., -0.0060,  0.0206, -0.0090],
        ...,
        [ 0.0092,  0.0058, -0.0019,  ...,  0.0165, -0.0040,  0.0205],
        [-0.0008,  0.0017, -0.0002,  ...,  0.0063,  0.0061,  0.0107],
        [-0.0235, -0.0255, -0.0045,  ...,  0.0125,  0.0070,  0.0178]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3643,  0.9204, -3.2344,  ..., -2.0234, -1.1445,  4.1133]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 14:08:02 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for caesar was known for their work as a  emperor
pascal was known for their work as a  mathematician
wittgenstein was known for their work as a  philosopher
mozart was known for their work as a  composer
marx was known for their work as a  philosopher
dante was known for their work as a  poet
balzac was known for their work as a  novelist
raphael was known for their work as a 
2024-07-19 14:08:02 root INFO     total operator prediction time: 2075.923030614853 seconds
2024-07-19 14:08:02 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on country - capital
2024-07-19 14:08:02 root INFO     building operator country - capital
2024-07-19 14:08:02 root INFO     [order_1_approx] starting weight calculation for The country with nairobi as its capital is known as kenya
The country with bucharest as its capital is known as romania
The country with vienna as its capital is known as austria
The country with dhaka as its capital is known as bangladesh
The country with budapest as its capital is known as hungary
The country with amman as its capital is known as jordan
The country with lisbon as its capital is known as portugal
The country with berlin as its capital is known as
2024-07-19 14:08:02 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 14:12:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8413,  0.2458, -0.2325,  ..., -0.3857, -0.1123, -0.6455],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.4238, -1.9912,  0.2607,  ..., -2.4414,  2.0527, -2.9316],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0095,  0.0091, -0.0009,  ...,  0.0071,  0.0116, -0.0094],
        [-0.0126, -0.0020,  0.0104,  ...,  0.0064,  0.0044,  0.0052],
        [-0.0053, -0.0096, -0.0039,  ..., -0.0088, -0.0088,  0.0014],
        ...,
        [-0.0158, -0.0025,  0.0002,  ...,  0.0059, -0.0052,  0.0079],
        [-0.0114, -0.0031, -0.0007,  ..., -0.0010,  0.0090,  0.0058],
        [-0.0047,  0.0032, -0.0062,  ..., -0.0072,  0.0042, -0.0084]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.6680, -2.4844,  0.1965,  ..., -3.1855,  2.2812, -2.7305]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 14:12:24 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with nairobi as its capital is known as kenya
The country with bucharest as its capital is known as romania
The country with vienna as its capital is known as austria
The country with dhaka as its capital is known as bangladesh
The country with budapest as its capital is known as hungary
The country with amman as its capital is known as jordan
The country with lisbon as its capital is known as portugal
The country with berlin as its capital is known as
2024-07-19 14:12:24 root INFO     [order_1_approx] starting weight calculation for The country with berlin as its capital is known as germany
The country with amman as its capital is known as jordan
The country with budapest as its capital is known as hungary
The country with vienna as its capital is known as austria
The country with lisbon as its capital is known as portugal
The country with nairobi as its capital is known as kenya
The country with bucharest as its capital is known as romania
The country with dhaka as its capital is known as
2024-07-19 14:12:24 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 14:16:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2412,  0.5386,  0.5156,  ..., -0.2384, -0.0815,  0.4905],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.8633, -1.6396, -1.4209,  ..., -0.6367,  3.0977,  2.4141],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0037, -0.0019, -0.0021,  ...,  0.0014,  0.0179, -0.0039],
        [-0.0305,  0.0146, -0.0114,  ...,  0.0483,  0.0083,  0.0072],
        [ 0.0086,  0.0036, -0.0282,  ..., -0.0043,  0.0068, -0.0022],
        ...,
        [-0.0208, -0.0091,  0.0182,  ...,  0.0394,  0.0041,  0.0045],
        [ 0.0179, -0.0182,  0.0187,  ..., -0.0239, -0.0108, -0.0024],
        [-0.0049, -0.0391,  0.0083,  ...,  0.0001, -0.0210,  0.0011]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.8125, -1.9375, -1.3662,  ..., -0.8696,  4.1641,  2.8359]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 14:16:45 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with berlin as its capital is known as germany
The country with amman as its capital is known as jordan
The country with budapest as its capital is known as hungary
The country with vienna as its capital is known as austria
The country with lisbon as its capital is known as portugal
The country with nairobi as its capital is known as kenya
The country with bucharest as its capital is known as romania
The country with dhaka as its capital is known as
2024-07-19 14:16:46 root INFO     [order_1_approx] starting weight calculation for The country with amman as its capital is known as jordan
The country with lisbon as its capital is known as portugal
The country with nairobi as its capital is known as kenya
The country with vienna as its capital is known as austria
The country with berlin as its capital is known as germany
The country with bucharest as its capital is known as romania
The country with dhaka as its capital is known as bangladesh
The country with budapest as its capital is known as
2024-07-19 14:16:46 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 14:21:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2114, -0.5518, -0.5918,  ...,  0.2834,  1.1250, -0.1853],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.8184,  0.7256,  3.6777,  ...,  4.2969, -4.5898,  0.1934],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 8.9188e-03,  7.1602e-03,  7.6294e-05,  ..., -6.4373e-04,
          5.7259e-03,  7.2556e-03],
        [-1.5808e-02,  2.8000e-03,  3.0273e-02,  ...,  1.1353e-02,
         -1.6418e-02,  1.2665e-02],
        [-1.3153e-02, -1.4954e-02,  4.2542e-02,  ...,  6.9008e-03,
         -2.2385e-02,  1.2344e-02],
        ...,
        [-1.0933e-02, -3.4637e-02,  2.2659e-02,  ..., -7.1449e-03,
         -2.6978e-02,  2.0676e-03],
        [ 2.1820e-03,  9.7351e-03, -3.3264e-02,  ..., -6.8130e-03,
          1.5747e-02, -7.7324e-03],
        [-3.1052e-03, -1.9363e-02, -4.4518e-03,  ...,  6.8665e-04,
          1.2642e-02, -5.6915e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3965, -1.3096,  0.9805,  ...,  3.8105, -2.2969,  1.6201]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 14:21:07 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with amman as its capital is known as jordan
The country with lisbon as its capital is known as portugal
The country with nairobi as its capital is known as kenya
The country with vienna as its capital is known as austria
The country with berlin as its capital is known as germany
The country with bucharest as its capital is known as romania
The country with dhaka as its capital is known as bangladesh
The country with budapest as its capital is known as
2024-07-19 14:21:08 root INFO     [order_1_approx] starting weight calculation for The country with nairobi as its capital is known as kenya
The country with budapest as its capital is known as hungary
The country with lisbon as its capital is known as portugal
The country with dhaka as its capital is known as bangladesh
The country with vienna as its capital is known as austria
The country with bucharest as its capital is known as romania
The country with berlin as its capital is known as germany
The country with amman as its capital is known as
2024-07-19 14:21:08 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 14:25:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5820, -0.2170, -0.2299,  ..., -1.0557, -1.2529,  0.1318],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.7393,  1.3848, -1.5117,  ..., -1.9609,  0.5068, -2.9062],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0126, -0.0484, -0.0078,  ...,  0.0033,  0.0225, -0.0162],
        [-0.0210,  0.0266,  0.0039,  ...,  0.0142, -0.0190, -0.0044],
        [-0.0092,  0.0117, -0.0199,  ..., -0.0108,  0.0032,  0.0191],
        ...,
        [-0.0105, -0.0250, -0.0187,  ...,  0.0296,  0.0063, -0.0129],
        [-0.0074,  0.0319,  0.0070,  ..., -0.0023, -0.0040,  0.0302],
        [-0.0140, -0.0104, -0.0196,  ..., -0.0116,  0.0187, -0.0166]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.8530,  2.2109, -1.9365,  ..., -1.9219,  1.3301, -2.0859]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 14:25:28 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with nairobi as its capital is known as kenya
The country with budapest as its capital is known as hungary
The country with lisbon as its capital is known as portugal
The country with dhaka as its capital is known as bangladesh
The country with vienna as its capital is known as austria
The country with bucharest as its capital is known as romania
The country with berlin as its capital is known as germany
The country with amman as its capital is known as
2024-07-19 14:25:28 root INFO     [order_1_approx] starting weight calculation for The country with budapest as its capital is known as hungary
The country with amman as its capital is known as jordan
The country with berlin as its capital is known as germany
The country with lisbon as its capital is known as portugal
The country with vienna as its capital is known as austria
The country with bucharest as its capital is known as romania
The country with dhaka as its capital is known as bangladesh
The country with nairobi as its capital is known as
2024-07-19 14:25:28 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 14:29:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4304, -0.3813, -0.9907,  ..., -0.2009,  1.1270,  0.2499],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.3123,  5.7422,  2.4121,  ...,  1.8008,  1.0078, -2.4102],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.4261e-02, -2.2247e-02, -1.3916e-02,  ...,  1.7290e-03,
          1.3733e-02, -1.0895e-02],
        [-4.2786e-02,  1.9913e-02,  1.1047e-02,  ...,  2.3468e-02,
         -2.5391e-02,  5.7755e-03],
        [-1.1383e-02,  3.9330e-03, -8.7128e-03,  ...,  5.1994e-03,
         -8.5754e-03, -1.9321e-03],
        ...,
        [-8.2855e-03, -5.0468e-03,  9.3460e-04,  ..., -8.7261e-04,
          9.5367e-07,  7.0419e-03],
        [ 2.3804e-02, -2.3865e-02, -1.1421e-02,  ..., -3.6713e-02,
          1.8845e-02, -8.8654e-03],
        [ 5.1880e-03, -1.9440e-02, -4.8161e-04,  ..., -7.1487e-03,
          1.9638e-02,  1.0353e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0620,  4.3984,  2.4512,  ...,  1.6621,  1.7305, -2.6367]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 14:29:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with budapest as its capital is known as hungary
The country with amman as its capital is known as jordan
The country with berlin as its capital is known as germany
The country with lisbon as its capital is known as portugal
The country with vienna as its capital is known as austria
The country with bucharest as its capital is known as romania
The country with dhaka as its capital is known as bangladesh
The country with nairobi as its capital is known as
2024-07-19 14:29:50 root INFO     [order_1_approx] starting weight calculation for The country with lisbon as its capital is known as portugal
The country with amman as its capital is known as jordan
The country with bucharest as its capital is known as romania
The country with dhaka as its capital is known as bangladesh
The country with budapest as its capital is known as hungary
The country with nairobi as its capital is known as kenya
The country with berlin as its capital is known as germany
The country with vienna as its capital is known as
2024-07-19 14:29:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 14:34:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4336,  0.2290, -0.1368,  ..., -1.0293, -1.1914,  0.9038],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.7344, -1.7246, -1.8193,  ..., -1.6074, -0.5142, -4.2031],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0044, -0.0005, -0.0063,  ..., -0.0014,  0.0160, -0.0156],
        [-0.0380, -0.0307,  0.0221,  ..., -0.0010, -0.0107, -0.0184],
        [ 0.0089,  0.0280, -0.0212,  ..., -0.0065,  0.0048,  0.0138],
        ...,
        [-0.0146, -0.0106, -0.0102,  ...,  0.0037, -0.0101, -0.0017],
        [-0.0020,  0.0361, -0.0555,  ..., -0.0113,  0.0184,  0.0194],
        [ 0.0267, -0.0017, -0.0380,  ..., -0.0152,  0.0276,  0.0067]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.8877, -3.5664, -0.9224,  ..., -1.8301,  2.3516, -1.8086]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 14:34:12 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with lisbon as its capital is known as portugal
The country with amman as its capital is known as jordan
The country with bucharest as its capital is known as romania
The country with dhaka as its capital is known as bangladesh
The country with budapest as its capital is known as hungary
The country with nairobi as its capital is known as kenya
The country with berlin as its capital is known as germany
The country with vienna as its capital is known as
2024-07-19 14:34:12 root INFO     [order_1_approx] starting weight calculation for The country with nairobi as its capital is known as kenya
The country with dhaka as its capital is known as bangladesh
The country with berlin as its capital is known as germany
The country with vienna as its capital is known as austria
The country with amman as its capital is known as jordan
The country with lisbon as its capital is known as portugal
The country with budapest as its capital is known as hungary
The country with bucharest as its capital is known as
2024-07-19 14:34:12 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 14:38:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2727, -0.1991, -0.6509,  ..., -0.4014,  0.7812, -0.6416],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.2383, -1.6377,  2.9609,  ...,  3.8379, -4.7617, -0.7100],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0138,  0.0246, -0.0170,  ...,  0.0029,  0.0323, -0.0115],
        [ 0.0156,  0.0160,  0.0058,  ...,  0.0089, -0.0094, -0.0035],
        [ 0.0038, -0.0059,  0.0139,  ..., -0.0023, -0.0393, -0.0210],
        ...,
        [-0.0054, -0.0120,  0.0175,  ...,  0.0031, -0.0246,  0.0181],
        [-0.0160,  0.0159, -0.0349,  ..., -0.0086,  0.0310,  0.0089],
        [-0.0073, -0.0089, -0.0016,  ..., -0.0069, -0.0154,  0.0038]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3828, -1.8115,  1.4150,  ...,  4.0820, -3.8008, -1.2197]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 14:38:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with nairobi as its capital is known as kenya
The country with dhaka as its capital is known as bangladesh
The country with berlin as its capital is known as germany
The country with vienna as its capital is known as austria
The country with amman as its capital is known as jordan
The country with lisbon as its capital is known as portugal
The country with budapest as its capital is known as hungary
The country with bucharest as its capital is known as
2024-07-19 14:38:32 root INFO     [order_1_approx] starting weight calculation for The country with dhaka as its capital is known as bangladesh
The country with vienna as its capital is known as austria
The country with budapest as its capital is known as hungary
The country with amman as its capital is known as jordan
The country with bucharest as its capital is known as romania
The country with berlin as its capital is known as germany
The country with nairobi as its capital is known as kenya
The country with lisbon as its capital is known as
2024-07-19 14:38:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 14:42:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6572, -0.1799,  0.0115,  ...,  0.3076, -0.1240,  0.8374],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.5879, -0.6128,  5.9375,  ...,  3.1992, -3.0508, -1.9512],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0038,  0.0056,  0.0046,  ...,  0.0047,  0.0218,  0.0076],
        [-0.0062,  0.0092,  0.0054,  ...,  0.0173, -0.0146,  0.0167],
        [-0.0165,  0.0109,  0.0003,  ...,  0.0127, -0.0370, -0.0073],
        ...,
        [-0.0239,  0.0152,  0.0072,  ...,  0.0076, -0.0269,  0.0058],
        [ 0.0153,  0.0001, -0.0179,  ..., -0.0160,  0.0170, -0.0070],
        [-0.0206, -0.0183, -0.0043,  ...,  0.0076,  0.0355, -0.0009]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1797, -0.6753,  6.5117,  ...,  2.2422, -1.9150, -1.8389]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 14:42:54 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The country with dhaka as its capital is known as bangladesh
The country with vienna as its capital is known as austria
The country with budapest as its capital is known as hungary
The country with amman as its capital is known as jordan
The country with bucharest as its capital is known as romania
The country with berlin as its capital is known as germany
The country with nairobi as its capital is known as kenya
The country with lisbon as its capital is known as
2024-07-19 14:42:54 root INFO     total operator prediction time: 2091.492061138153 seconds
2024-07-19 14:42:54 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on things - color
2024-07-19 14:42:54 root INFO     building operator things - color
2024-07-19 14:42:54 root INFO     [order_1_approx] starting weight calculation for The sugar is colored white
The leaves is colored green
The chocolate is colored white
The cream is colored white
The parsley is colored green
The rose is colored red
The grapes is colored black
The tomato is colored
2024-07-19 14:42:54 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 14:47:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3091,  0.2603,  1.4443,  ...,  0.1056, -0.3633,  0.4653],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.2881,  4.3281,  1.4043,  ..., -0.6362,  6.9766, -3.7852],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 2.3193e-03,  7.9536e-04, -7.7057e-03,  ...,  3.5648e-03,
          2.5864e-03, -2.4700e-03],
        [-1.5011e-03,  1.7834e-03,  1.2798e-03,  ..., -1.3458e-02,
          5.1880e-03,  1.4496e-04],
        [-8.4610e-03,  5.1117e-03, -5.5647e-04,  ..., -9.5825e-03,
         -4.0703e-03,  1.0017e-02],
        ...,
        [ 4.0054e-04,  1.5869e-02, -9.1248e-03,  ...,  6.8512e-03,
         -1.5793e-02,  1.9165e-02],
        [ 1.2236e-03,  2.0180e-03,  1.7059e-02,  ..., -5.4474e-03,
          3.6359e-05,  8.4305e-03],
        [ 1.5289e-02, -3.4256e-03, -2.3193e-03,  ...,  4.9629e-03,
         -5.8670e-03,  1.1971e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.2871,  4.6758,  0.1758,  ..., -0.3623,  6.8555, -3.1895]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 14:47:15 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sugar is colored white
The leaves is colored green
The chocolate is colored white
The cream is colored white
The parsley is colored green
The rose is colored red
The grapes is colored black
The tomato is colored
2024-07-19 14:47:15 root INFO     [order_1_approx] starting weight calculation for The tomato is colored red
The cream is colored white
The rose is colored red
The leaves is colored green
The parsley is colored green
The grapes is colored black
The sugar is colored white
The chocolate is colored
2024-07-19 14:47:15 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 14:51:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3953,  0.5361,  0.9917,  ..., -0.2158,  0.5962,  0.4495],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.3516,  7.2305, -0.1973,  ..., -0.4150,  1.4189, -1.6738],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.3411e-03,  1.0948e-03, -7.5150e-03,  ..., -3.5343e-03,
         -2.4948e-03, -8.3923e-03],
        [-4.8599e-03,  1.1475e-02, -2.4605e-03,  ..., -7.6370e-03,
          4.5509e-03, -6.3171e-03],
        [-3.8185e-03,  4.1466e-03, -4.7112e-04,  ...,  1.1063e-02,
         -2.1706e-03,  1.9569e-03],
        ...,
        [-9.6283e-03, -8.7357e-03,  8.1329e-03,  ...,  2.2316e-03,
         -6.4888e-03,  1.5533e-02],
        [ 2.0920e-02,  1.0843e-03,  8.2932e-03,  ...,  1.2299e-02,
         -1.2436e-02,  1.4000e-02],
        [ 7.8735e-03, -3.8147e-05, -4.0398e-03,  ...,  2.6226e-03,
         -2.3022e-03, -9.5978e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.2695,  7.5430, -0.4316,  ..., -1.0166,  0.8887, -0.9102]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 14:51:35 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The tomato is colored red
The cream is colored white
The rose is colored red
The leaves is colored green
The parsley is colored green
The grapes is colored black
The sugar is colored white
The chocolate is colored
2024-07-19 14:51:35 root INFO     [order_1_approx] starting weight calculation for The sugar is colored white
The parsley is colored green
The grapes is colored black
The leaves is colored green
The rose is colored red
The chocolate is colored white
The tomato is colored red
The cream is colored
2024-07-19 14:51:35 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 14:55:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0439, -0.0125, -0.6040,  ..., -0.8921,  0.1055, -0.1577],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-1.1045,  6.8125,  1.7891,  ...,  2.6016,  1.1172, -3.2051],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 3.9291e-03, -5.9509e-03, -1.4526e-02,  ...,  1.5793e-03,
          6.0577e-03, -2.7130e-02],
        [ 3.3150e-03,  3.8147e-06, -5.1384e-03,  ..., -6.6452e-03,
          2.2602e-03,  1.4496e-02],
        [-8.9264e-04,  1.0399e-02, -3.7804e-03,  ...,  1.4259e-02,
         -4.6730e-03,  1.5076e-02],
        ...,
        [-1.5442e-02, -4.7760e-03, -1.2550e-03,  ...,  2.5063e-03,
         -1.0284e-02,  1.4015e-02],
        [ 2.1912e-02, -6.6605e-03,  1.2039e-02,  ...,  2.2766e-02,
         -5.3635e-03, -1.9745e-02],
        [-1.6403e-03, -5.2452e-03, -4.8676e-03,  ...,  1.4610e-03,
         -2.7275e-03,  9.2773e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.2759,  7.2305,  1.0801,  ...,  3.0859,  1.0557, -3.7656]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 14:55:56 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sugar is colored white
The parsley is colored green
The grapes is colored black
The leaves is colored green
The rose is colored red
The chocolate is colored white
The tomato is colored red
The cream is colored
2024-07-19 14:55:56 root INFO     [order_1_approx] starting weight calculation for The leaves is colored green
The rose is colored red
The parsley is colored green
The grapes is colored black
The chocolate is colored white
The cream is colored white
The tomato is colored red
The sugar is colored
2024-07-19 14:55:56 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 15:00:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4951, -0.3911,  0.8091,  ..., -0.6797,  0.6948, -0.1006],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.5986,  6.6172, -1.7432,  ..., -1.5684,  4.4688, -3.2383],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0038,  0.0009, -0.0076,  ...,  0.0012,  0.0098, -0.0092],
        [-0.0125, -0.0043,  0.0134,  ..., -0.0042,  0.0007,  0.0072],
        [-0.0054,  0.0044, -0.0174,  ...,  0.0265, -0.0043, -0.0193],
        ...,
        [-0.0108,  0.0029, -0.0084,  ...,  0.0046, -0.0145, -0.0013],
        [ 0.0113, -0.0093,  0.0162,  ..., -0.0054,  0.0132, -0.0090],
        [ 0.0134,  0.0013,  0.0028,  ..., -0.0042, -0.0076, -0.0023]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.1931,  7.7734, -2.5605,  ..., -1.7393,  4.4375, -3.2285]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 15:00:16 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The leaves is colored green
The rose is colored red
The parsley is colored green
The grapes is colored black
The chocolate is colored white
The cream is colored white
The tomato is colored red
The sugar is colored
2024-07-19 15:00:16 root INFO     [order_1_approx] starting weight calculation for The parsley is colored green
The sugar is colored white
The tomato is colored red
The grapes is colored black
The cream is colored white
The chocolate is colored white
The rose is colored red
The leaves is colored
2024-07-19 15:00:16 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 15:04:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6016,  0.7778,  0.7764,  ...,  0.8545, -0.3140, -0.2432],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.4121,  2.9766,  1.8301,  ...,  1.1504,  5.3281, -6.8438],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-8.8043e-03, -1.0544e-02,  2.5826e-03,  ...,  1.1955e-02,
          9.2468e-03,  1.0559e-02],
        [-1.9531e-02,  1.3046e-02, -8.0414e-03,  ..., -1.2650e-02,
          2.2217e-02, -1.9913e-02],
        [ 1.0277e-02,  1.8677e-02,  5.4398e-03,  ...,  8.5754e-03,
         -1.5190e-02,  1.5488e-02],
        ...,
        [ 5.2986e-03, -1.2589e-02, -2.8629e-03,  ...,  1.7303e-02,
         -2.5391e-02,  1.0548e-03],
        [-3.6926e-03,  3.4866e-03,  1.6174e-02,  ..., -1.1955e-02,
          9.3002e-03, -4.9820e-03],
        [ 3.7918e-03,  7.6981e-03,  3.8147e-05,  ...,  1.3641e-02,
         -1.0681e-02,  6.6376e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3477,  1.8359,  2.6797,  ...,  0.8379,  5.8867, -6.8633]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 15:04:36 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The parsley is colored green
The sugar is colored white
The tomato is colored red
The grapes is colored black
The cream is colored white
The chocolate is colored white
The rose is colored red
The leaves is colored
2024-07-19 15:04:36 root INFO     [order_1_approx] starting weight calculation for The chocolate is colored white
The cream is colored white
The tomato is colored red
The sugar is colored white
The rose is colored red
The leaves is colored green
The parsley is colored green
The grapes is colored
2024-07-19 15:04:37 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 15:08:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3843, -0.2224,  1.3408,  ..., -0.9785, -0.7256,  0.8125],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-2.5508,  3.7578,  1.1270,  ..., -2.0254,  4.6406, -2.4258],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 9.7122e-03, -9.7275e-04, -1.2634e-02,  ...,  1.1002e-02,
          1.0910e-02, -2.0046e-03],
        [-1.3252e-02,  1.6205e-02,  5.4054e-03,  ..., -1.6594e-03,
         -1.3458e-02, -3.7270e-03],
        [ 1.6365e-03,  4.3488e-03, -5.4855e-03,  ...,  5.8594e-03,
          4.1275e-03, -8.5220e-03],
        ...,
        [ 4.3259e-03, -9.2010e-03, -9.9869e-03,  ...,  2.5238e-02,
          8.0109e-03,  3.6469e-03],
        [-9.0256e-03,  4.7569e-03,  1.2077e-02,  ...,  6.5231e-03,
          3.3951e-04,  6.5207e-05],
        [ 1.9646e-03,  1.2970e-04, -4.0245e-03,  ..., -5.1765e-03,
          6.0577e-03, -2.3193e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.2812,  3.8340,  1.3516,  ..., -1.2520,  3.4453, -2.7852]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 15:08:55 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The chocolate is colored white
The cream is colored white
The tomato is colored red
The sugar is colored white
The rose is colored red
The leaves is colored green
The parsley is colored green
The grapes is colored
2024-07-19 15:08:55 root INFO     [order_1_approx] starting weight calculation for The sugar is colored white
The tomato is colored red
The rose is colored red
The leaves is colored green
The cream is colored white
The grapes is colored black
The chocolate is colored white
The parsley is colored
2024-07-19 15:08:55 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 15:13:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7993, -0.5693, -0.1584,  ...,  0.8052, -1.2324,  0.5664],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4258,  4.2188,  1.5801,  ..., -0.7109,  4.8672, -4.6914],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0045,  0.0056, -0.0025,  ..., -0.0014,  0.0042, -0.0020],
        [-0.0256, -0.0034,  0.0263,  ...,  0.0078, -0.0136, -0.0053],
        [-0.0208, -0.0045,  0.0084,  ..., -0.0021, -0.0095,  0.0031],
        ...,
        [-0.0067,  0.0027, -0.0025,  ...,  0.0041, -0.0084,  0.0120],
        [-0.0017,  0.0040,  0.0034,  ...,  0.0058, -0.0064,  0.0100],
        [ 0.0017,  0.0055,  0.0016,  ..., -0.0027, -0.0014,  0.0060]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3936,  3.5000,  1.0547,  ..., -1.0879,  5.4023, -4.8945]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 15:13:14 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sugar is colored white
The tomato is colored red
The rose is colored red
The leaves is colored green
The cream is colored white
The grapes is colored black
The chocolate is colored white
The parsley is colored
2024-07-19 15:13:14 root INFO     [order_1_approx] starting weight calculation for The parsley is colored green
The tomato is colored red
The grapes is colored black
The cream is colored white
The chocolate is colored white
The leaves is colored green
The sugar is colored white
The rose is colored
2024-07-19 15:13:14 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 15:17:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8203, -0.7158,  0.9648,  ..., -0.1152, -0.6763,  0.2764],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([-0.9863,  4.7852,  0.9395,  ..., -1.7275,  3.7949, -3.4512],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0074,  0.0023,  0.0026,  ...,  0.0056, -0.0040, -0.0024],
        [-0.0155,  0.0034,  0.0022,  ..., -0.0198,  0.0098, -0.0185],
        [-0.0212, -0.0007,  0.0119,  ..., -0.0141, -0.0031,  0.0003],
        ...,
        [-0.0094, -0.0073,  0.0027,  ..., -0.0038, -0.0054, -0.0006],
        [ 0.0025, -0.0012,  0.0062,  ...,  0.0091, -0.0126,  0.0206],
        [-0.0056, -0.0002, -0.0068,  ...,  0.0025, -0.0012, -0.0018]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.0127,  5.1797,  0.9604,  ..., -2.4121,  3.9062, -3.8027]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 15:17:32 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The parsley is colored green
The tomato is colored red
The grapes is colored black
The cream is colored white
The chocolate is colored white
The leaves is colored green
The sugar is colored white
The rose is colored
2024-07-19 15:17:32 root INFO     total operator prediction time: 2078.286596059799 seconds
2024-07-19 15:17:32 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on animal - sound
2024-07-19 15:17:32 root INFO     building operator animal - sound
2024-07-19 15:17:32 root INFO     [order_1_approx] starting weight calculation for The sound that a goat makes is called a bleat
The sound that a mouse makes is called a squeak
The sound that a mallard makes is called a quack
The sound that a hound makes is called a bark
The sound that a bear makes is called a growl
The sound that a turkey makes is called a gobble
The sound that a fly makes is called a buzz
The sound that a leopard makes is called a
2024-07-19 15:17:32 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 15:21:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3574, -0.3599, -0.7993,  ...,  0.5005, -0.7461,  0.3406],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.9570,  1.3594,  1.9033,  ..., -3.4277, -7.5898, -1.4082],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0082,  0.0106,  0.0008,  ..., -0.0087,  0.0003, -0.0044],
        [-0.0089, -0.0014, -0.0040,  ...,  0.0107, -0.0013,  0.0045],
        [-0.0075,  0.0133,  0.0044,  ...,  0.0028, -0.0012,  0.0122],
        ...,
        [ 0.0024,  0.0029,  0.0085,  ..., -0.0043,  0.0180,  0.0217],
        [ 0.0017, -0.0035,  0.0046,  ..., -0.0295,  0.0147,  0.0156],
        [-0.0143, -0.0044, -0.0144,  ...,  0.0174,  0.0181,  0.0025]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3242,  0.3398,  1.7803,  ..., -1.8193, -6.1055, -1.1074]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 15:21:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a goat makes is called a bleat
The sound that a mouse makes is called a squeak
The sound that a mallard makes is called a quack
The sound that a hound makes is called a bark
The sound that a bear makes is called a growl
The sound that a turkey makes is called a gobble
The sound that a fly makes is called a buzz
The sound that a leopard makes is called a
2024-07-19 15:21:50 root INFO     [order_1_approx] starting weight calculation for The sound that a bear makes is called a growl
The sound that a fly makes is called a buzz
The sound that a leopard makes is called a growl
The sound that a goat makes is called a bleat
The sound that a mouse makes is called a squeak
The sound that a turkey makes is called a gobble
The sound that a hound makes is called a bark
The sound that a mallard makes is called a
2024-07-19 15:21:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 15:26:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6953,  0.1588, -0.5679,  ...,  0.3765, -0.9897,  0.1660],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4883,  4.2422,  2.8945,  ...,  4.6094, -1.6660, -5.9062],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0086, -0.0095, -0.0065,  ..., -0.0080, -0.0102,  0.0181],
        [ 0.0137, -0.0157,  0.0195,  ..., -0.0086, -0.0085, -0.0182],
        [ 0.0004, -0.0233, -0.0049,  ...,  0.0101, -0.0124, -0.0006],
        ...,
        [ 0.0255, -0.0024,  0.0038,  ...,  0.0124, -0.0128, -0.0033],
        [-0.0044,  0.0031, -0.0110,  ...,  0.0009,  0.0078,  0.0111],
        [-0.0170,  0.0102, -0.0011,  ..., -0.0055,  0.0066, -0.0010]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4512,  4.9688,  3.2227,  ...,  5.7188, -2.7207, -6.3711]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 15:26:10 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a bear makes is called a growl
The sound that a fly makes is called a buzz
The sound that a leopard makes is called a growl
The sound that a goat makes is called a bleat
The sound that a mouse makes is called a squeak
The sound that a turkey makes is called a gobble
The sound that a hound makes is called a bark
The sound that a mallard makes is called a
2024-07-19 15:26:10 root INFO     [order_1_approx] starting weight calculation for The sound that a goat makes is called a bleat
The sound that a mouse makes is called a squeak
The sound that a hound makes is called a bark
The sound that a bear makes is called a growl
The sound that a mallard makes is called a quack
The sound that a leopard makes is called a growl
The sound that a fly makes is called a buzz
The sound that a turkey makes is called a
2024-07-19 15:26:10 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 15:30:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5420,  0.0120,  0.2627,  ...,  0.3848, -1.0195,  0.7944],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 3.5977,  3.7754,  4.1562,  ..., -2.1270, -1.7461, -4.9453],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-1.1429e-02, -1.4313e-02, -1.4778e-02,  ...,  6.0844e-03,
         -9.9869e-03,  1.5068e-03],
        [ 2.7847e-03, -3.6240e-04,  3.2692e-03,  ..., -1.4168e-02,
         -3.0518e-05, -1.0941e-02],
        [-1.2970e-02, -2.1210e-02, -1.2726e-02,  ..., -1.7395e-02,
         -7.9117e-03, -3.8696e-02],
        ...,
        [ 3.0960e-02,  1.1230e-02, -2.4681e-03,  ...,  7.4921e-03,
          1.8806e-03,  1.5869e-02],
        [-1.2817e-03, -8.1635e-04,  2.0542e-03,  ...,  3.7781e-02,
          9.1248e-03,  4.0527e-02],
        [ 1.1887e-02, -2.6588e-03,  6.5231e-03,  ...,  8.9874e-03,
          1.7654e-02,  2.1301e-02]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.9004,  3.5293,  1.0234,  ..., -1.6270, -0.6348, -2.5508]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 15:30:30 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a goat makes is called a bleat
The sound that a mouse makes is called a squeak
The sound that a hound makes is called a bark
The sound that a bear makes is called a growl
The sound that a mallard makes is called a quack
The sound that a leopard makes is called a growl
The sound that a fly makes is called a buzz
The sound that a turkey makes is called a
2024-07-19 15:30:30 root INFO     [order_1_approx] starting weight calculation for The sound that a goat makes is called a bleat
The sound that a mallard makes is called a quack
The sound that a fly makes is called a buzz
The sound that a turkey makes is called a gobble
The sound that a mouse makes is called a squeak
The sound that a leopard makes is called a growl
The sound that a bear makes is called a growl
The sound that a hound makes is called a
2024-07-19 15:30:30 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 15:34:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0496, -0.5508,  0.0044,  ...,  0.7720, -0.6963,  0.0972],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.4216,  0.3555, -3.8086,  ..., -0.5615,  1.6348,  0.9126],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 4.9057e-03,  1.0519e-03,  1.5459e-03,  ..., -1.8616e-03,
          1.3458e-02, -7.0496e-03],
        [ 5.4054e-03, -1.0117e-02,  8.4686e-03,  ..., -5.3673e-03,
         -6.6299e-03,  5.5313e-05],
        [ 4.8676e-03, -1.0862e-03,  4.3335e-03,  ...,  6.3782e-03,
          4.4632e-03, -4.8370e-03],
        ...,
        [ 1.1589e-02, -1.4221e-02,  1.0460e-02,  ..., -1.0557e-03,
          5.7449e-03, -6.1417e-03],
        [-2.7161e-02,  1.3130e-02, -1.4343e-02,  ...,  6.4316e-03,
          4.4174e-03,  1.8326e-02],
        [-1.7822e-02, -6.7902e-04, -1.5900e-02,  ...,  2.2446e-02,
          9.1171e-04, -1.6251e-03]], device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7065,  0.5566, -3.8926,  ..., -0.1021,  0.6270,  0.1523]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 15:34:50 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a goat makes is called a bleat
The sound that a mallard makes is called a quack
The sound that a fly makes is called a buzz
The sound that a turkey makes is called a gobble
The sound that a mouse makes is called a squeak
The sound that a leopard makes is called a growl
The sound that a bear makes is called a growl
The sound that a hound makes is called a
2024-07-19 15:34:50 root INFO     [order_1_approx] starting weight calculation for The sound that a hound makes is called a bark
The sound that a leopard makes is called a growl
The sound that a mouse makes is called a squeak
The sound that a mallard makes is called a quack
The sound that a bear makes is called a growl
The sound that a turkey makes is called a gobble
The sound that a goat makes is called a bleat
The sound that a fly makes is called a
2024-07-19 15:34:50 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 15:39:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9805,  0.0990,  0.5835,  ..., -0.4873, -0.5312,  0.7930],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 7.3242,  1.6562,  5.4141,  ...,  2.7793, -2.1816, -6.0938],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[-0.0183, -0.0051, -0.0137,  ...,  0.0294, -0.0089, -0.0111],
        [ 0.0070, -0.0080,  0.0236,  ..., -0.0162, -0.0012, -0.0006],
        [-0.0018, -0.0117, -0.0147,  ...,  0.0139, -0.0273, -0.0036],
        ...,
        [ 0.0115, -0.0078,  0.0028,  ...,  0.0238,  0.0035,  0.0071],
        [ 0.0127, -0.0129,  0.0089,  ..., -0.0117,  0.0115,  0.0127],
        [-0.0128,  0.0146,  0.0264,  ..., -0.0077,  0.0042,  0.0085]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.3164,  1.8613,  4.9648,  ...,  3.2891, -1.4570, -6.0508]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 15:39:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a hound makes is called a bark
The sound that a leopard makes is called a growl
The sound that a mouse makes is called a squeak
The sound that a mallard makes is called a quack
The sound that a bear makes is called a growl
The sound that a turkey makes is called a gobble
The sound that a goat makes is called a bleat
The sound that a fly makes is called a
2024-07-19 15:39:09 root INFO     [order_1_approx] starting weight calculation for The sound that a leopard makes is called a growl
The sound that a mallard makes is called a quack
The sound that a turkey makes is called a gobble
The sound that a bear makes is called a growl
The sound that a hound makes is called a bark
The sound that a goat makes is called a bleat
The sound that a fly makes is called a buzz
The sound that a mouse makes is called a
2024-07-19 15:39:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 15:43:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4014,  0.7100, -0.1108,  ...,  0.1787, -0.4712,  0.7822],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 4.0625,  1.4551,  1.5684,  ..., -1.6143, -2.6035, -6.8516],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0029, -0.0030, -0.0155,  ..., -0.0046,  0.0072,  0.0144],
        [-0.0092, -0.0204,  0.0036,  ...,  0.0106, -0.0142, -0.0180],
        [ 0.0011,  0.0022, -0.0012,  ...,  0.0089, -0.0215,  0.0249],
        ...,
        [ 0.0043, -0.0133,  0.0063,  ..., -0.0061, -0.0194,  0.0137],
        [ 0.0129, -0.0059,  0.0198,  ..., -0.0272, -0.0075, -0.0032],
        [-0.0004, -0.0172,  0.0124,  ..., -0.0184,  0.0158, -0.0310]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.1289,  1.3887,  1.1748,  ..., -0.9233, -2.8008, -5.4727]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 15:43:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a leopard makes is called a growl
The sound that a mallard makes is called a quack
The sound that a turkey makes is called a gobble
The sound that a bear makes is called a growl
The sound that a hound makes is called a bark
The sound that a goat makes is called a bleat
The sound that a fly makes is called a buzz
The sound that a mouse makes is called a
2024-07-19 15:43:29 root INFO     [order_1_approx] starting weight calculation for The sound that a mallard makes is called a quack
The sound that a leopard makes is called a growl
The sound that a turkey makes is called a gobble
The sound that a mouse makes is called a squeak
The sound that a bear makes is called a growl
The sound that a hound makes is called a bark
The sound that a fly makes is called a buzz
The sound that a goat makes is called a
2024-07-19 15:43:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 15:47:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0475, -0.6836, -1.1172,  ...,  1.1367, -0.9795,  0.8906],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 1.4805,  3.4785, -3.4102,  ...,  0.7720, -1.1270, -1.1729],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0030, -0.0187, -0.0037,  ..., -0.0052,  0.0032,  0.0008],
        [-0.0082,  0.0118, -0.0047,  ..., -0.0157,  0.0118, -0.0022],
        [-0.0108, -0.0101,  0.0057,  ...,  0.0046, -0.0103,  0.0286],
        ...,
        [-0.0168,  0.0051, -0.0175,  ...,  0.0210,  0.0278,  0.0115],
        [-0.0042,  0.0003, -0.0010,  ..., -0.0010, -0.0029, -0.0032],
        [-0.0231,  0.0042, -0.0035,  ..., -0.0089,  0.0019, -0.0148]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.1406,  2.3164, -3.7715,  ...,  0.1025, -0.2988, -2.1250]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 15:47:49 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a mallard makes is called a quack
The sound that a leopard makes is called a growl
The sound that a turkey makes is called a gobble
The sound that a mouse makes is called a squeak
The sound that a bear makes is called a growl
The sound that a hound makes is called a bark
The sound that a fly makes is called a buzz
The sound that a goat makes is called a
2024-07-19 15:47:49 root INFO     [order_1_approx] starting weight calculation for The sound that a mouse makes is called a squeak
The sound that a mallard makes is called a quack
The sound that a goat makes is called a bleat
The sound that a turkey makes is called a gobble
The sound that a fly makes is called a buzz
The sound that a leopard makes is called a growl
The sound that a hound makes is called a bark
The sound that a bear makes is called a
2024-07-19 15:47:49 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 15:52:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3579, -0.0521, -0.4656,  ..., -0.1704, -0.2764,  0.3416],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 2.8477,  1.6748, -4.0312,  ..., -2.7676, -2.8203, -0.8511],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0070, -0.0029,  0.0017,  ...,  0.0057,  0.0090,  0.0105],
        [-0.0039,  0.0212,  0.0109,  ...,  0.0263, -0.0102, -0.0005],
        [ 0.0039, -0.0057,  0.0053,  ..., -0.0244, -0.0079,  0.0054],
        ...,
        [-0.0032, -0.0079, -0.0063,  ...,  0.0039, -0.0011,  0.0006],
        [-0.0057, -0.0188, -0.0116,  ..., -0.0281,  0.0121,  0.0074],
        [ 0.0033,  0.0029,  0.0002,  ..., -0.0045, -0.0109,  0.0044]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.6719,  1.1543, -4.7578,  ..., -3.4277, -2.0410, -0.2808]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 15:52:09 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a mouse makes is called a squeak
The sound that a mallard makes is called a quack
The sound that a goat makes is called a bleat
The sound that a turkey makes is called a gobble
The sound that a fly makes is called a buzz
The sound that a leopard makes is called a growl
The sound that a hound makes is called a bark
The sound that a bear makes is called a
2024-07-19 15:52:09 root INFO     total operator prediction time: 2077.234572649002 seconds
2024-07-19 15:52:09 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on animal - youth
2024-07-19 15:52:09 root INFO     building operator animal - youth
2024-07-19 15:52:09 root INFO     [order_1_approx] starting weight calculation for The offspring of a herring is referred to as a fingerling
The offspring of a insect is referred to as a larva
The offspring of a gorilla is referred to as a infant
The offspring of a panda is referred to as a cub
The offspring of a cicada is referred to as a nymph
The offspring of a muskrat is referred to as a kit
The offspring of a butterfly is referred to as a larva
The offspring of a horse is referred to as a
2024-07-19 15:52:09 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
2024-07-19 15:56:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4492, -0.0834, -0.0548,  ...,  0.4285, -0.2446,  0.1781],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        o_j1=tensor([ 0.0649, -2.3477, -5.5820,  ..., -0.8232, -0.4604,  4.2539],
       device='cuda:1', dtype=torch.float16, grad_fn=<SelectBackward0>) 

                        s_o_weight: tensor([[ 0.0020, -0.0118,  0.0020,  ..., -0.0056, -0.0319,  0.0012],
        [-0.0170,  0.0044,  0.0006,  ...,  0.0045,  0.0002, -0.0255],
        [ 0.0089, -0.0018,  0.0008,  ..., -0.0091, -0.0114,  0.0040],
        ...,
        [ 0.0007, -0.0064,  0.0083,  ..., -0.0072, -0.0097, -0.0130],
        [ 0.0177,  0.0052,  0.0029,  ..., -0.0182, -0.0083,  0.0101],
        [-0.0234, -0.0094,  0.0053,  ...,  0.0158, -0.0015, -0.0132]],
       device='cuda:1', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0544, -2.6543, -4.5703,  ..., -1.3262,  0.2251,  2.8496]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-19 15:56:29 lre.operators INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a herring is referred to as a fingerling
The offspring of a insect is referred to as a larva
The offspring of a gorilla is referred to as a infant
The offspring of a panda is referred to as a cub
The offspring of a cicada is referred to as a nymph
The offspring of a muskrat is referred to as a kit
The offspring of a butterfly is referred to as a larva
The offspring of a horse is referred to as a
2024-07-19 15:56:29 root INFO     [order_1_approx] starting weight calculation for The offspring of a herring is referred to as a fingerling
The offspring of a cicada is referred to as a nymph
The offspring of a horse is referred to as a foal
The offspring of a panda is referred to as a cub
The offspring of a muskrat is referred to as a kit
The offspring of a butterfly is referred to as a larva
The offspring of a gorilla is referred to as a infant
The offspring of a insect is referred to as a
2024-07-19 15:56:29 lre.functional WARNING  [insert_s_j] layer transformer.h.27 does not match transformer.h.3
