2024-07-24 17:49:27 root INFO     loading model + tokenizer
2024-07-24 17:50:28 root INFO     loading model + tokenizer
2024-07-24 17:50:34 root INFO     loading model + tokenizer
2024-07-24 17:50:39 root INFO     model + tokenizer loaded
2024-07-24 17:50:39 __main__ INFO     storing weights: <class 'lre.operators.JacobianIclMeanEstimator'> on animal - youth
2024-07-24 17:50:39 root INFO     building operator animal - youth
2024-07-24 17:51:42 root INFO     loading model + tokenizer
2024-07-24 17:51:47 root INFO     model + tokenizer loaded
2024-07-24 17:51:47 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on animal - youth
2024-07-24 17:51:47 root INFO     building operator animal - youth
2024-07-24 17:54:43 root INFO     loading model + tokenizer
2024-07-24 17:54:48 root INFO     model + tokenizer loaded
2024-07-24 17:54:48 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on animal - youth
2024-07-24 17:54:48 root INFO     building operator animal - youth
2024-07-24 17:54:51 root INFO     [order_1_approx] starting weight calculation for The offspring of a monkey is referred to as a infant
The offspring of a deer is referred to as a fawn
The offspring of a snake is referred to as a hatchling
The offspring of a pig is referred to as a piglet
The offspring of a fish is referred to as a fingerling
The offspring of a bear is referred to as a cub
The offspring of a gorilla is referred to as a infant
The offspring of a trout is referred to as a
2024-07-24 17:54:51 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 18:00:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.4062, -0.4258, -0.5586,  ...,  2.0938, -0.4297,  2.4219],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([14.8125, -7.7812, -6.9375,  ..., -2.8281,  1.2031, -8.9375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0045,  0.0026, -0.0005,  ..., -0.0095,  0.0058, -0.0045],
        [-0.0025,  0.0070, -0.0303,  ..., -0.0055, -0.0040,  0.0099],
        [ 0.0024, -0.0046,  0.0564,  ..., -0.0044, -0.0021, -0.0057],
        ...,
        [-0.0013,  0.0116,  0.0208,  ..., -0.0034, -0.0046,  0.0044],
        [-0.0006, -0.0012,  0.0025,  ..., -0.0017,  0.0231,  0.0156],
        [-0.0044,  0.0029,  0.0182,  ...,  0.0012, -0.0048,  0.0133]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[14.7266, -8.7266, -6.9453,  ..., -1.1074,  0.0811, -9.2578]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 18:02:27 root INFO     loading model + tokenizer
2024-07-24 18:02:32 root INFO     model + tokenizer loaded
2024-07-24 18:02:32 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on animal - youth
2024-07-24 18:02:32 root INFO     building operator animal - youth
2024-07-24 18:02:33 root INFO     [order_1_approx] starting weight calculation for The offspring of a lion is referred to as a cub
The offspring of a fish is referred to as a fingerling
The offspring of a elephant is referred to as a calf
The offspring of a trout is referred to as a fingerling
The offspring of a beetle is referred to as a larva
The offspring of a cricket is referred to as a larva
The offspring of a bear is referred to as a cub
The offspring of a woodchuck is referred to as a
2024-07-24 18:02:33 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 18:03:25 root INFO     loading model + tokenizer
2024-07-24 18:03:30 root INFO     model + tokenizer loaded
2024-07-24 18:03:30 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on animal - youth
2024-07-24 18:03:30 root INFO     building operator animal - youth
2024-07-24 18:03:31 root INFO     [order_1_approx] starting weight calculation for The offspring of a ape is referred to as a baby
The offspring of a cricket is referred to as a larva
The offspring of a monkey is referred to as a infant
The offspring of a chimpanzee is referred to as a baby
The offspring of a mink is referred to as a kit
The offspring of a fly is referred to as a grub
The offspring of a dog is referred to as a puppy
The offspring of a horse is referred to as a
2024-07-24 18:03:31 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 18:04:55 root INFO     loading model + tokenizer
2024-07-24 18:05:00 root INFO     model + tokenizer loaded
2024-07-24 18:05:00 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on meronyms - part
2024-07-24 18:05:00 root INFO     building operator meronyms - part
2024-07-24 18:05:01 root INFO     [order_1_approx] starting weight calculation for A part of a window is a pane
A part of a table is a tabletop
A part of a church is a altar
A part of a telephone is a receiver
A part of a castle is a donjon
A part of a guitar is a string
A part of a pub is a bar
A part of a cat is a
2024-07-24 18:05:02 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 18:08:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8828, -2.2812,  7.7188,  ..., -1.7031, -3.0000, -1.8438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 13.7500,  -1.4531,   4.6875,  ..., -14.8125,  -3.1250, -18.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0111, -0.0040, -0.0052,  ...,  0.0049, -0.0143, -0.0087],
        [ 0.0159,  0.0187, -0.0549,  ...,  0.0008,  0.0140,  0.0079],
        [ 0.0052,  0.0124,  0.0967,  ...,  0.0008, -0.0270,  0.0063],
        ...,
        [-0.0049,  0.0048, -0.0143,  ...,  0.0264, -0.0269,  0.0046],
        [ 0.0199, -0.0032,  0.0369,  ...,  0.0022,  0.0322,  0.0029],
        [ 0.0044, -0.0014, -0.0518,  ..., -0.0025, -0.0222,  0.0369]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 16.3594,   0.3604,   3.3281,  ...,  -8.4453,  -3.4648, -15.8438]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 18:08:36 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a lion is referred to as a cub
The offspring of a fish is referred to as a fingerling
The offspring of a elephant is referred to as a calf
The offspring of a trout is referred to as a fingerling
The offspring of a beetle is referred to as a larva
The offspring of a cricket is referred to as a larva
The offspring of a bear is referred to as a cub
The offspring of a woodchuck is referred to as a
2024-07-24 18:08:36 root INFO     [order_1_approx] starting weight calculation for The offspring of a trout is referred to as a fingerling
The offspring of a cricket is referred to as a larva
The offspring of a elephant is referred to as a calf
The offspring of a bear is referred to as a cub
The offspring of a woodchuck is referred to as a kit
The offspring of a beetle is referred to as a larva
The offspring of a lion is referred to as a cub
The offspring of a fish is referred to as a
2024-07-24 18:08:36 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 18:10:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8125, -0.7891, -1.2188,  ...,  2.4375, -0.5273, -1.0781],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  5.7812,  18.5000,   3.1406,  ...,   3.8438,   9.7500, -12.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0069, -0.0002,  0.0036,  ...,  0.0027,  0.0013,  0.0070],
        [ 0.0011,  0.0007, -0.0050,  ..., -0.0006,  0.0041,  0.0058],
        [-0.0079, -0.0058,  0.0732,  ...,  0.0023,  0.0006, -0.0079],
        ...,
        [ 0.0018, -0.0058, -0.0051,  ...,  0.0027, -0.0009, -0.0045],
        [ 0.0087,  0.0085, -0.0020,  ..., -0.0025,  0.0190,  0.0027],
        [ 0.0022,  0.0044, -0.0059,  ...,  0.0049,  0.0019,  0.0101]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  5.7656,  17.7969,   3.0723,  ...,   3.4980,  10.9766, -12.6172]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 18:10:39 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A part of a window is a pane
A part of a table is a tabletop
A part of a church is a altar
A part of a telephone is a receiver
A part of a castle is a donjon
A part of a guitar is a string
A part of a pub is a bar
A part of a cat is a
2024-07-24 18:10:39 root INFO     [order_1_approx] starting weight calculation for A part of a castle is a donjon
A part of a window is a pane
A part of a cat is a whiskers
A part of a table is a tabletop
A part of a guitar is a string
A part of a church is a altar
A part of a telephone is a receiver
A part of a pub is a
2024-07-24 18:10:39 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 18:14:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.9062, -1.1719, -2.1719,  ...,  1.7812, -0.7969,  1.7188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 11.7500,   0.2344,  -7.1250,  ...,  -0.8516,   1.5469, -12.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0018,  0.0121,  0.0181,  ..., -0.0007,  0.0063, -0.0028],
        [ 0.0006,  0.0035, -0.0121,  ..., -0.0038, -0.0038,  0.0005],
        [ 0.0010,  0.0019,  0.0471,  ..., -0.0059,  0.0032, -0.0035],
        ...,
        [ 0.0014,  0.0037,  0.0149,  ..., -0.0028,  0.0019,  0.0025],
        [ 0.0030, -0.0012, -0.0015,  ...,  0.0004,  0.0096,  0.0020],
        [-0.0041,  0.0003,  0.0008,  ..., -0.0013, -0.0020,  0.0090]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 11.9688,  -0.1819,  -7.3477,  ...,  -1.2383,   1.3252, -13.3203]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 18:14:27 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a trout is referred to as a fingerling
The offspring of a cricket is referred to as a larva
The offspring of a elephant is referred to as a calf
The offspring of a bear is referred to as a cub
The offspring of a woodchuck is referred to as a kit
The offspring of a beetle is referred to as a larva
The offspring of a lion is referred to as a cub
The offspring of a fish is referred to as a
2024-07-24 18:14:27 root INFO     [order_1_approx] starting weight calculation for The offspring of a fish is referred to as a fingerling
The offspring of a woodchuck is referred to as a kit
The offspring of a beetle is referred to as a larva
The offspring of a bear is referred to as a cub
The offspring of a trout is referred to as a fingerling
The offspring of a elephant is referred to as a calf
The offspring of a lion is referred to as a cub
The offspring of a cricket is referred to as a
2024-07-24 18:14:28 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 18:16:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1562,  1.4531, -1.1250,  ..., -1.5156, -4.3438,  1.4062],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-1.8047,  6.6875, -5.1875,  ..., -1.7500,  8.5000, -4.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 7.0190e-03,  5.0354e-03,  1.1719e-02,  ..., -2.8992e-03,
          1.7548e-03,  4.6997e-03],
        [ 7.5531e-04, -4.1962e-04, -1.6602e-02,  ...,  3.1738e-03,
          2.6245e-03,  1.5991e-02],
        [-9.8877e-03,  4.9744e-03,  1.0010e-01,  ..., -5.5237e-03,
          6.5613e-03, -1.0803e-02],
        ...,
        [-5.0354e-04,  1.4114e-03,  1.7212e-02,  ...,  2.7924e-03,
          9.6436e-03, -6.4087e-03],
        [-9.3079e-04,  3.8300e-03,  2.1057e-03,  ...,  2.8381e-03,
          1.1780e-02, -3.0823e-03],
        [ 4.9591e-05,  1.0757e-03, -1.3733e-02,  ..., -7.0801e-03,
          5.8289e-03,  3.3936e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.7637,  7.6406, -4.9375,  ..., -1.7178,  9.1562, -4.5234]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 18:16:15 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A part of a castle is a donjon
A part of a window is a pane
A part of a cat is a whiskers
A part of a table is a tabletop
A part of a guitar is a string
A part of a church is a altar
A part of a telephone is a receiver
A part of a pub is a
2024-07-24 18:16:15 root INFO     [order_1_approx] starting weight calculation for A part of a pub is a bar
A part of a window is a pane
A part of a church is a altar
A part of a table is a tabletop
A part of a guitar is a string
A part of a cat is a whiskers
A part of a telephone is a receiver
A part of a castle is a
2024-07-24 18:16:15 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 18:20:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0469, -3.2031,  3.8750,  ...,  0.4941,  0.1973, -2.9531],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -0.8750,  -8.7500,  -2.8438,  ...,  -2.2031, -16.1250,  -1.3828],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-0.0027,  0.0045, -0.0036,  ..., -0.0049,  0.0049, -0.0020],
        [ 0.0092, -0.0029, -0.0188,  ..., -0.0029, -0.0006,  0.0052],
        [-0.0020,  0.0020,  0.0762,  ...,  0.0015, -0.0034,  0.0077],
        ...,
        [ 0.0017, -0.0015,  0.0062,  ...,  0.0121,  0.0008,  0.0035],
        [-0.0030, -0.0049,  0.0114,  ...,  0.0015,  0.0283,  0.0019],
        [-0.0062,  0.0096, -0.0088,  ..., -0.0015, -0.0015,  0.0193]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -1.9775,  -8.1328,  -2.9629,  ...,  -1.6855, -15.4062,  -1.8447]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 18:20:22 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a fish is referred to as a fingerling
The offspring of a woodchuck is referred to as a kit
The offspring of a beetle is referred to as a larva
The offspring of a bear is referred to as a cub
The offspring of a trout is referred to as a fingerling
The offspring of a elephant is referred to as a calf
The offspring of a lion is referred to as a cub
The offspring of a cricket is referred to as a
2024-07-24 18:20:22 root INFO     [order_1_approx] starting weight calculation for The offspring of a fish is referred to as a fingerling
The offspring of a beetle is referred to as a larva
The offspring of a cricket is referred to as a larva
The offspring of a elephant is referred to as a calf
The offspring of a lion is referred to as a cub
The offspring of a trout is referred to as a fingerling
The offspring of a woodchuck is referred to as a kit
The offspring of a bear is referred to as a
2024-07-24 18:20:22 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 18:21:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.6797, -2.4375, -4.0625,  ..., -1.2109, -1.6406,  2.7344],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-0.0625, 10.3125, -9.3750,  ...,  1.2188,  5.1562,  3.3594],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0188,  0.0034,  0.0105,  ...,  0.0007, -0.0109, -0.0128],
        [-0.0018, -0.0015, -0.0007,  ..., -0.0037, -0.0040,  0.0065],
        [-0.0116, -0.0011,  0.0933,  ..., -0.0077,  0.0078, -0.0025],
        ...,
        [ 0.0002, -0.0012, -0.0110,  ..., -0.0004, -0.0018,  0.0057],
        [-0.0001, -0.0075,  0.0310,  ..., -0.0094,  0.0315,  0.0008],
        [-0.0014, -0.0052, -0.0194,  ...,  0.0035, -0.0064,  0.0295]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -0.1555,   9.7031, -10.7500,  ...,   1.2676,   5.0547,   4.0391]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 18:21:56 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A part of a pub is a bar
A part of a window is a pane
A part of a church is a altar
A part of a table is a tabletop
A part of a guitar is a string
A part of a cat is a whiskers
A part of a telephone is a receiver
A part of a castle is a
2024-07-24 18:21:56 root INFO     [order_1_approx] starting weight calculation for A part of a church is a altar
A part of a guitar is a string
A part of a castle is a donjon
A part of a cat is a whiskers
A part of a telephone is a receiver
A part of a window is a pane
A part of a pub is a bar
A part of a table is a
2024-07-24 18:21:56 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 18:26:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7344, -0.4961,  2.9844,  ..., -0.1484,  0.3086, -1.7891],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 6.9688,  1.5938,  1.2344,  ..., -6.3750,  4.2500, -7.4375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0001, -0.0034,  0.0016,  ..., -0.0079,  0.0020, -0.0031],
        [ 0.0039,  0.0019, -0.0143,  ...,  0.0045, -0.0014,  0.0018],
        [ 0.0018,  0.0004,  0.0669,  ..., -0.0058,  0.0063, -0.0059],
        ...,
        [ 0.0049, -0.0003,  0.0012,  ...,  0.0037,  0.0006, -0.0087],
        [ 0.0014,  0.0052,  0.0310,  ...,  0.0012,  0.0106,  0.0011],
        [ 0.0117, -0.0038,  0.0097,  ...,  0.0006,  0.0050,  0.0225]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 7.8867,  1.1104,  0.7480,  ..., -5.1953,  5.9648, -9.9375]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 18:26:15 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a fish is referred to as a fingerling
The offspring of a beetle is referred to as a larva
The offspring of a cricket is referred to as a larva
The offspring of a elephant is referred to as a calf
The offspring of a lion is referred to as a cub
The offspring of a trout is referred to as a fingerling
The offspring of a woodchuck is referred to as a kit
The offspring of a bear is referred to as a
2024-07-24 18:26:15 root INFO     [order_1_approx] starting weight calculation for The offspring of a bear is referred to as a cub
The offspring of a beetle is referred to as a larva
The offspring of a woodchuck is referred to as a kit
The offspring of a fish is referred to as a fingerling
The offspring of a trout is referred to as a fingerling
The offspring of a cricket is referred to as a larva
The offspring of a elephant is referred to as a calf
The offspring of a lion is referred to as a
2024-07-24 18:26:15 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 18:27:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1250, -1.3984, -0.3164,  ...,  0.4414,  0.6875,  0.0835],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-4.5625, 16.0000, -0.6094,  ...,  7.9375,  4.5312, -2.5312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0110,  0.0102,  0.0222,  ...,  0.0008,  0.0054,  0.0118],
        [ 0.0037,  0.0045, -0.0140,  ..., -0.0016,  0.0027,  0.0132],
        [-0.0029,  0.0061,  0.0869,  ..., -0.0076, -0.0015, -0.0183],
        ...,
        [ 0.0062,  0.0091,  0.0188,  ...,  0.0005, -0.0238,  0.0033],
        [ 0.0025, -0.0076, -0.0079,  ...,  0.0048,  0.0525,  0.0051],
        [-0.0039, -0.0019, -0.0060,  ..., -0.0007,  0.0156,  0.0219]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.8262, 15.3906,  0.0820,  ...,  8.0000,  5.3867, -2.7793]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 18:27:43 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A part of a church is a altar
A part of a guitar is a string
A part of a castle is a donjon
A part of a cat is a whiskers
A part of a telephone is a receiver
A part of a window is a pane
A part of a pub is a bar
A part of a table is a
2024-07-24 18:27:43 root INFO     [order_1_approx] starting weight calculation for A part of a castle is a donjon
A part of a table is a tabletop
A part of a cat is a whiskers
A part of a pub is a bar
A part of a church is a altar
A part of a window is a pane
A part of a telephone is a receiver
A part of a guitar is a
2024-07-24 18:27:44 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 18:32:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3438, -0.3359, -1.6406,  ..., -0.6055, -0.4297,  1.9219],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 5.9375, -5.5625,  6.8750,  ..., -8.6250,  0.5938, -7.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0055,  0.0048,  0.0026,  ..., -0.0036,  0.0064,  0.0033],
        [ 0.0143,  0.0018, -0.0072,  ..., -0.0058, -0.0005,  0.0063],
        [-0.0009,  0.0016,  0.0552,  ...,  0.0011,  0.0009, -0.0071],
        ...,
        [ 0.0109, -0.0001,  0.0072,  ..., -0.0043,  0.0041, -0.0035],
        [-0.0030,  0.0038,  0.0177,  ..., -0.0042,  0.0142, -0.0031],
        [-0.0027, -0.0043, -0.0037,  ...,  0.0032,  0.0012,  0.0128]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.8555, -5.7969,  6.3711,  ..., -8.1562,  1.2686, -8.4297]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 18:32:03 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a bear is referred to as a cub
The offspring of a beetle is referred to as a larva
The offspring of a woodchuck is referred to as a kit
The offspring of a fish is referred to as a fingerling
The offspring of a trout is referred to as a fingerling
The offspring of a cricket is referred to as a larva
The offspring of a elephant is referred to as a calf
The offspring of a lion is referred to as a
2024-07-24 18:32:04 root INFO     [order_1_approx] starting weight calculation for The offspring of a elephant is referred to as a calf
The offspring of a cricket is referred to as a larva
The offspring of a fish is referred to as a fingerling
The offspring of a woodchuck is referred to as a kit
The offspring of a bear is referred to as a cub
The offspring of a lion is referred to as a cub
The offspring of a beetle is referred to as a larva
The offspring of a trout is referred to as a
2024-07-24 18:32:04 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 18:33:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5078, -1.1172, -1.9844,  ..., -0.5820, -2.5469,  4.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 6.2188,  1.4844, 10.5000,  ..., -6.5938, -0.6562, -1.0156],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.0681e-02,  3.9368e-03, -5.4321e-03,  ..., -3.0365e-03,
         -5.1270e-03,  1.4572e-03],
        [-3.8147e-06, -1.2665e-03,  1.3733e-03,  ...,  9.5749e-04,
          1.1292e-03, -1.6632e-03],
        [ 8.3160e-04,  1.7700e-03,  7.7148e-02,  ..., -1.8921e-03,
         -4.5166e-03,  1.3351e-03],
        ...,
        [-3.1281e-03, -4.9210e-04,  3.9978e-03,  ...,  1.1902e-02,
         -3.9673e-03, -5.9509e-03],
        [ 3.7079e-03,  2.4719e-03,  6.9885e-03,  ...,  8.9645e-04,
          2.2461e-02,  7.2021e-03],
        [ 5.2185e-03,  4.6539e-04,  5.6152e-03,  ..., -8.7891e-03,
          1.8311e-02,  2.1362e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.8828,  1.0898,  9.9609,  ..., -6.0000, -0.0640, -1.5596]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 18:33:25 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A part of a castle is a donjon
A part of a table is a tabletop
A part of a cat is a whiskers
A part of a pub is a bar
A part of a church is a altar
A part of a window is a pane
A part of a telephone is a receiver
A part of a guitar is a
2024-07-24 18:33:26 root INFO     [order_1_approx] starting weight calculation for A part of a pub is a bar
A part of a cat is a whiskers
A part of a guitar is a string
A part of a telephone is a receiver
A part of a table is a tabletop
A part of a castle is a donjon
A part of a window is a pane
A part of a church is a
2024-07-24 18:33:26 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 18:37:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.5938,  0.0781, -0.2773,  ...,  1.6016,  0.1553,  2.3906],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([13.0625, -4.4688, -6.6250,  ..., -5.1250, -1.2500, -9.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 5.4932e-03, -1.2512e-03,  3.7384e-03,  ..., -5.7068e-03,
          3.7842e-03,  3.3875e-03],
        [-1.6479e-03,  5.5542e-03, -2.7100e-02,  ..., -3.8757e-03,
         -3.7537e-03,  5.7983e-03],
        [ 2.0885e-04, -4.3030e-03,  5.5176e-02,  ..., -6.3477e-03,
         -6.2943e-05, -3.2349e-03],
        ...,
        [-5.8746e-04,  9.4604e-03,  1.4343e-02,  ...,  1.8234e-03,
         -7.1411e-03,  1.9073e-03],
        [ 7.3242e-03, -2.8839e-03,  2.4109e-03,  ...,  1.3733e-03,
          2.1729e-02,  1.2390e-02],
        [-4.3335e-03, -2.2888e-04,  1.4954e-02,  ...,  2.9144e-03,
         -4.7607e-03,  1.0132e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[11.6953, -4.0195, -6.5000,  ..., -4.2266, -2.7422, -9.7812]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 18:37:52 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a elephant is referred to as a calf
The offspring of a cricket is referred to as a larva
The offspring of a fish is referred to as a fingerling
The offspring of a woodchuck is referred to as a kit
The offspring of a bear is referred to as a cub
The offspring of a lion is referred to as a cub
The offspring of a beetle is referred to as a larva
The offspring of a trout is referred to as a
2024-07-24 18:37:52 root INFO     [order_1_approx] starting weight calculation for The offspring of a lion is referred to as a cub
The offspring of a bear is referred to as a cub
The offspring of a trout is referred to as a fingerling
The offspring of a fish is referred to as a fingerling
The offspring of a elephant is referred to as a calf
The offspring of a cricket is referred to as a larva
The offspring of a woodchuck is referred to as a kit
The offspring of a beetle is referred to as a
2024-07-24 18:37:52 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 18:39:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.6406, -1.3672, -0.8750,  ...,  0.5430, -3.6562,  0.3008],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 7.0000, 17.8750,  1.9375,  ...,  2.2969, -2.6875, -5.2812],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.1597e-02,  1.8158e-03, -4.8065e-04,  ...,  2.6855e-03,
          7.5684e-03, -1.7166e-03],
        [ 1.2589e-03, -5.8594e-03, -1.4526e-02,  ...,  1.1139e-03,
         -6.6223e-03,  8.6060e-03],
        [-1.4038e-02, -3.4943e-03,  5.3711e-02,  ..., -3.2043e-03,
         -6.8665e-03,  5.7220e-05],
        ...,
        [ 6.3324e-04, -3.7231e-03, -7.9346e-03,  ...,  2.1667e-03,
          3.4637e-03,  2.9602e-03],
        [-3.7575e-04,  1.7242e-03,  8.4839e-03,  ...,  2.4719e-03,
          2.5024e-02, -2.7008e-03],
        [-4.2915e-04, -3.2425e-05,  6.3477e-03,  ..., -2.6550e-03,
          3.7689e-03,  1.3306e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.0664, 16.0938,  1.1270,  ...,  2.5469, -2.1484, -5.2773]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 18:39:11 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A part of a pub is a bar
A part of a cat is a whiskers
A part of a guitar is a string
A part of a telephone is a receiver
A part of a table is a tabletop
A part of a castle is a donjon
A part of a window is a pane
A part of a church is a
2024-07-24 18:39:11 root INFO     [order_1_approx] starting weight calculation for A part of a window is a pane
A part of a castle is a donjon
A part of a guitar is a string
A part of a church is a altar
A part of a table is a tabletop
A part of a pub is a bar
A part of a cat is a whiskers
A part of a telephone is a
2024-07-24 18:39:12 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 18:43:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.4688, -1.0469,  0.8047,  ..., -0.7148,  0.9883, -1.7578],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  3.5938, -11.0625,  -4.2812,  ...,  -7.2188,  -5.7812,  -9.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-0.0071,  0.0090, -0.0054,  ...,  0.0024,  0.0039, -0.0060],
        [ 0.0108,  0.0036, -0.0154,  ...,  0.0026, -0.0127,  0.0013],
        [ 0.0025, -0.0046,  0.0693,  ..., -0.0012, -0.0012,  0.0139],
        ...,
        [ 0.0045,  0.0062,  0.0093,  ...,  0.0096,  0.0065,  0.0043],
        [-0.0048, -0.0038,  0.0008,  ..., -0.0007,  0.0250, -0.0049],
        [-0.0052,  0.0043, -0.0103,  ..., -0.0042, -0.0051,  0.0243]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.2422, -8.3047, -4.8516,  ..., -7.4141, -5.1836, -9.1953]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 18:43:39 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a lion is referred to as a cub
The offspring of a bear is referred to as a cub
The offspring of a trout is referred to as a fingerling
The offspring of a fish is referred to as a fingerling
The offspring of a elephant is referred to as a calf
The offspring of a cricket is referred to as a larva
The offspring of a woodchuck is referred to as a kit
The offspring of a beetle is referred to as a
2024-07-24 18:43:39 root INFO     [order_1_approx] starting weight calculation for The offspring of a lion is referred to as a cub
The offspring of a beetle is referred to as a larva
The offspring of a cricket is referred to as a larva
The offspring of a fish is referred to as a fingerling
The offspring of a woodchuck is referred to as a kit
The offspring of a trout is referred to as a fingerling
The offspring of a bear is referred to as a cub
The offspring of a elephant is referred to as a
2024-07-24 18:43:39 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 18:44:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.0312,  0.1533,  0.4180,  ..., -2.4062,  0.5547, -1.1406],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-9.5000,  5.4688, 14.7500,  ...,  0.8711,  0.5039, -8.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 8.5449e-03,  2.5177e-03,  8.9722e-03,  ...,  1.6479e-03,
         -9.5367e-05, -9.9945e-04],
        [ 2.0142e-03,  8.4305e-04, -8.1177e-03,  ..., -2.9449e-03,
          4.2419e-03, -7.4463e-03],
        [-1.7643e-04, -7.0496e-03,  7.8125e-02,  ..., -4.1504e-03,
          1.9150e-03,  2.7618e-03],
        ...,
        [ 1.4038e-03, -5.5237e-03, -1.8677e-02,  ...,  8.5449e-03,
          1.9073e-03,  6.1951e-03],
        [-6.7749e-03, -2.0447e-03,  3.6774e-03,  ..., -3.7003e-04,
          1.6846e-02,  6.5918e-03],
        [-7.9956e-03,  4.5471e-03, -1.6602e-02,  ...,  2.5177e-04,
          9.3384e-03,  2.1118e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-8.6172,  4.1289, 15.3594,  ...,  3.0879,  1.1152, -7.2852]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 18:44:50 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A part of a window is a pane
A part of a castle is a donjon
A part of a guitar is a string
A part of a church is a altar
A part of a table is a tabletop
A part of a pub is a bar
A part of a cat is a whiskers
A part of a telephone is a
2024-07-24 18:44:50 root INFO     [order_1_approx] starting weight calculation for A part of a telephone is a receiver
A part of a church is a altar
A part of a castle is a donjon
A part of a table is a tabletop
A part of a cat is a whiskers
A part of a guitar is a string
A part of a pub is a bar
A part of a window is a
2024-07-24 18:44:50 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 18:49:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 3.6562, -2.9219,  3.3750,  ..., -0.4453, -4.1875,  0.6445],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 3.7500, -1.6875,  4.8750,  ..., -2.8438, -5.5312, -0.8984],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-0.0079, -0.0012, -0.0049,  ...,  0.0021,  0.0105,  0.0049],
        [ 0.0082, -0.0013, -0.0276,  ..., -0.0109, -0.0102,  0.0041],
        [ 0.0042, -0.0014,  0.0713,  ..., -0.0009,  0.0110,  0.0016],
        ...,
        [ 0.0070,  0.0077,  0.0103,  ..., -0.0005,  0.0086, -0.0046],
        [-0.0068,  0.0044,  0.0120,  ...,  0.0008,  0.0150, -0.0126],
        [ 0.0065, -0.0006, -0.0112,  ...,  0.0007, -0.0098,  0.0231]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.8535, -0.6016,  5.2305,  ..., -3.8906, -4.3164, -1.3740]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 18:49:34 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The offspring of a lion is referred to as a cub
The offspring of a beetle is referred to as a larva
The offspring of a cricket is referred to as a larva
The offspring of a fish is referred to as a fingerling
The offspring of a woodchuck is referred to as a kit
The offspring of a trout is referred to as a fingerling
The offspring of a bear is referred to as a cub
The offspring of a elephant is referred to as a
2024-07-24 18:49:34 root INFO     total operator prediction time: 2821.373020172119 seconds
2024-07-24 18:49:34 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on animal - sound
2024-07-24 18:49:34 root INFO     building operator animal - sound
2024-07-24 18:49:34 root INFO     [order_1_approx] starting weight calculation for The sound that a hound makes is called a bark
The sound that a donkey makes is called a bray
The sound that a crow makes is called a caw
The sound that a cat makes is called a meow
The sound that a ferret makes is called a dook
The sound that a bear makes is called a growl
The sound that a duck makes is called a quack
The sound that a alpaca makes is called a
2024-07-24 18:49:34 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 18:50:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3125, -0.7109,  0.1445,  ...,  1.7188, -3.4375, -1.4219],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-4.2812, -6.7500, -9.5000,  ..., 11.6250,  4.4375, -2.2344],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0060, -0.0011,  0.0034,  ..., -0.0049, -0.0095,  0.0015],
        [-0.0067,  0.0055, -0.0030,  ..., -0.0053, -0.0016, -0.0067],
        [-0.0009,  0.0029,  0.0854,  ..., -0.0079,  0.0106,  0.0058],
        ...,
        [ 0.0089, -0.0039, -0.0083,  ...,  0.0104, -0.0129, -0.0007],
        [ 0.0011,  0.0060, -0.0022,  ..., -0.0031,  0.0126,  0.0031],
        [-0.0032, -0.0007, -0.0156,  ..., -0.0038,  0.0005,  0.0269]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.0039, -8.3281, -9.3438,  ..., 13.2031,  4.9609, -1.4805]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 18:50:34 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A part of a telephone is a receiver
A part of a church is a altar
A part of a castle is a donjon
A part of a table is a tabletop
A part of a cat is a whiskers
A part of a guitar is a string
A part of a pub is a bar
A part of a window is a
2024-07-24 18:50:34 root INFO     total operator prediction time: 2733.222913503647 seconds
2024-07-24 18:50:34 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on synonyms - exact
2024-07-24 18:50:34 root INFO     building operator synonyms - exact
2024-07-24 18:50:34 root INFO     [order_1_approx] starting weight calculation for Another word for vocabulary is lexicon
Another word for style is manner
Another word for child is kid
Another word for obsolete is outdated
Another word for market is marketplace
Another word for mother is mom
Another word for snake is serpent
Another word for organized is
2024-07-24 18:50:34 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 18:55:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6172, -3.0469, -4.1250,  ...,  1.8828, -5.5625,  1.7969],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([16.2500,  5.4688, 20.0000,  ..., -3.3125,  5.1875, -3.9062],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.3123e-02,  4.4250e-03, -2.7588e-02,  ..., -7.0801e-03,
          2.4414e-03,  1.0986e-03],
        [ 1.9897e-02, -1.1368e-03,  3.1250e-02,  ..., -9.0027e-04,
          7.8125e-03,  6.6528e-03],
        [-1.9165e-02,  1.5381e-02,  1.1914e-01,  ..., -2.2888e-05,
         -5.3711e-03,  2.1973e-03],
        ...,
        [-2.2095e-02,  6.5308e-03, -2.7832e-02,  ...,  9.8877e-03,
         -5.8594e-03,  1.0986e-03],
        [ 2.2949e-02, -2.3682e-02, -2.8381e-03,  ...,  2.4719e-03,
          2.9907e-02,  2.6855e-03],
        [ 9.1553e-03,  8.7280e-03, -1.5564e-02,  ..., -4.8218e-03,
         -5.0659e-03,  1.7578e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[16.2969,  5.8750, 19.2969,  ..., -3.8145,  7.2734, -4.8398]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 18:55:24 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a hound makes is called a bark
The sound that a donkey makes is called a bray
The sound that a crow makes is called a caw
The sound that a cat makes is called a meow
The sound that a ferret makes is called a dook
The sound that a bear makes is called a growl
The sound that a duck makes is called a quack
The sound that a alpaca makes is called a
2024-07-24 18:55:24 root INFO     [order_1_approx] starting weight calculation for The sound that a bear makes is called a growl
The sound that a duck makes is called a quack
The sound that a alpaca makes is called a bray
The sound that a donkey makes is called a bray
The sound that a hound makes is called a bark
The sound that a crow makes is called a caw
The sound that a cat makes is called a meow
The sound that a ferret makes is called a
2024-07-24 18:55:25 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 18:56:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.4062, -0.7578,  1.6719,  ..., -0.8672, -4.6875,  1.8359],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  4.6250,   0.3516, -13.1875,  ...,   9.9375, -11.0625,  11.3125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0048, -0.0036,  0.0134,  ..., -0.0015,  0.0015,  0.0054],
        [ 0.0038,  0.0062,  0.0007,  ..., -0.0036,  0.0121,  0.0103],
        [ 0.0026,  0.0049,  0.1128,  ...,  0.0011, -0.0126, -0.0133],
        ...,
        [-0.0042,  0.0074, -0.0160,  ..., -0.0084,  0.0068,  0.0081],
        [ 0.0015,  0.0019,  0.0103,  ...,  0.0047,  0.0309, -0.0084],
        [ 0.0019, -0.0009, -0.0162,  ..., -0.0033, -0.0093,  0.0155]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  5.0820,   0.2410, -13.6484,  ...,  10.9219, -11.1953,  11.5156]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 18:56:11 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Another word for vocabulary is lexicon
Another word for style is manner
Another word for child is kid
Another word for obsolete is outdated
Another word for market is marketplace
Another word for mother is mom
Another word for snake is serpent
Another word for organized is
2024-07-24 18:56:12 root INFO     [order_1_approx] starting weight calculation for Another word for obsolete is outdated
Another word for vocabulary is lexicon
Another word for market is marketplace
Another word for organized is arranged
Another word for style is manner
Another word for snake is serpent
Another word for mother is mom
Another word for child is
2024-07-24 18:56:12 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 19:01:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.9531, -1.4844, -0.2891,  ...,  2.2500, -1.8125,  2.3594],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([10.3125,  2.9062, 15.8750,  ...,  5.1250, -3.0312, -9.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0035,  0.0141, -0.0189,  ...,  0.0051, -0.0074, -0.0088],
        [ 0.0128, -0.0139,  0.0204,  ..., -0.0034, -0.0057,  0.0140],
        [-0.0166,  0.0167,  0.0820,  ...,  0.0011, -0.0009, -0.0006],
        ...,
        [-0.0101, -0.0009, -0.0237,  ..., -0.0047,  0.0046, -0.0016],
        [ 0.0135, -0.0239,  0.0204,  ..., -0.0012,  0.0610,  0.0054],
        [ 0.0003, -0.0126,  0.0033,  ..., -0.0031,  0.0098,  0.0147]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 7.5312,  5.2656, 15.6406,  ...,  7.4258,  0.3535, -8.0000]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 19:01:20 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a bear makes is called a growl
The sound that a duck makes is called a quack
The sound that a alpaca makes is called a bray
The sound that a donkey makes is called a bray
The sound that a hound makes is called a bark
The sound that a crow makes is called a caw
The sound that a cat makes is called a meow
The sound that a ferret makes is called a
2024-07-24 19:01:20 root INFO     [order_1_approx] starting weight calculation for The sound that a crow makes is called a caw
The sound that a bear makes is called a growl
The sound that a donkey makes is called a bray
The sound that a hound makes is called a bark
The sound that a duck makes is called a quack
The sound that a alpaca makes is called a bray
The sound that a ferret makes is called a dook
The sound that a cat makes is called a
2024-07-24 19:01:20 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 19:02:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4531, -2.1562,  1.6328,  ...,  2.9375, -1.5312,  0.7422],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 6.8438,  3.6562,  6.0625,  ...,  9.3750,  5.3750, -8.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0084,  0.0063,  0.0118,  ...,  0.0027,  0.0071, -0.0020],
        [-0.0041,  0.0071, -0.0104,  ..., -0.0106, -0.0030, -0.0015],
        [ 0.0009,  0.0016,  0.0640,  ...,  0.0069, -0.0056, -0.0054],
        ...,
        [ 0.0051,  0.0020,  0.0063,  ...,  0.0011,  0.0016,  0.0018],
        [-0.0034, -0.0043, -0.0071,  ...,  0.0013,  0.0153, -0.0034],
        [ 0.0012, -0.0017, -0.0079,  ...,  0.0055, -0.0090,  0.0135]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 7.6172,  2.6758,  5.0234,  ...,  9.6875,  4.7617, -6.1484]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 19:02:02 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Another word for obsolete is outdated
Another word for vocabulary is lexicon
Another word for market is marketplace
Another word for organized is arranged
Another word for style is manner
Another word for snake is serpent
Another word for mother is mom
Another word for child is
2024-07-24 19:02:02 root INFO     [order_1_approx] starting weight calculation for Another word for vocabulary is lexicon
Another word for child is kid
Another word for market is marketplace
Another word for snake is serpent
Another word for mother is mom
Another word for obsolete is outdated
Another word for organized is arranged
Another word for style is
2024-07-24 19:02:02 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 19:07:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3438,  0.4863,  2.0312,  ...,  1.5547,  0.8320, -0.7070],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 3.1094, 10.7500, 16.7500,  ..., -0.3750,  3.3438, -6.3125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 7.1106e-03,  2.4414e-04,  9.3079e-04,  ...,  6.5231e-04,
          1.0681e-04, -2.2125e-04],
        [ 3.3264e-03,  1.0910e-03,  5.7373e-03,  ...,  2.5940e-04,
         -2.2125e-03,  9.2697e-04],
        [-5.2185e-03, -1.7166e-05,  4.8828e-02,  ...,  1.1139e-03,
         -7.2098e-04,  4.5776e-03],
        ...,
        [-2.3746e-04, -3.5095e-04, -8.7280e-03,  ...,  2.6703e-04,
          9.9182e-04,  5.1880e-04],
        [-4.1199e-04,  3.3875e-03, -3.3569e-03,  ..., -2.3499e-03,
          1.0986e-02, -5.5695e-04],
        [-2.4719e-03,  1.3351e-04, -6.8054e-03,  ...,  3.8910e-04,
         -3.0670e-03,  1.0071e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.3145, 10.6719, 16.7188,  ..., -0.8369,  3.3730, -6.5000]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 19:07:13 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a crow makes is called a caw
The sound that a bear makes is called a growl
The sound that a donkey makes is called a bray
The sound that a hound makes is called a bark
The sound that a duck makes is called a quack
The sound that a alpaca makes is called a bray
The sound that a ferret makes is called a dook
The sound that a cat makes is called a
2024-07-24 19:07:14 root INFO     [order_1_approx] starting weight calculation for The sound that a crow makes is called a caw
The sound that a alpaca makes is called a bray
The sound that a duck makes is called a quack
The sound that a cat makes is called a meow
The sound that a donkey makes is called a bray
The sound that a ferret makes is called a dook
The sound that a hound makes is called a bark
The sound that a bear makes is called a
2024-07-24 19:07:14 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 19:07:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.7500, -0.4141,  0.8984,  ...,  0.9492, -0.6875,  2.2031],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([3.3281, 2.3125, 0.3125,  ..., 4.1875, 2.6719, 0.5156], device='cuda:0',
       dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0005,  0.0006, -0.0024,  ..., -0.0045,  0.0119, -0.0037],
        [-0.0029,  0.0067, -0.0066,  ..., -0.0005,  0.0071,  0.0016],
        [-0.0057, -0.0032,  0.1318,  ...,  0.0043, -0.0056, -0.0053],
        ...,
        [ 0.0085,  0.0010,  0.0166,  ...,  0.0017, -0.0037,  0.0129],
        [ 0.0020,  0.0069, -0.0010,  ...,  0.0055,  0.0177,  0.0002],
        [ 0.0071, -0.0021, -0.0024,  ..., -0.0031, -0.0087,  0.0154]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.7188,  2.5508,  1.1641,  ...,  5.2891,  1.9258, -0.0303]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 19:07:41 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Another word for vocabulary is lexicon
Another word for child is kid
Another word for market is marketplace
Another word for snake is serpent
Another word for mother is mom
Another word for obsolete is outdated
Another word for organized is arranged
Another word for style is
2024-07-24 19:07:41 root INFO     [order_1_approx] starting weight calculation for Another word for obsolete is outdated
Another word for mother is mom
Another word for organized is arranged
Another word for market is marketplace
Another word for snake is serpent
Another word for style is manner
Another word for child is kid
Another word for vocabulary is
2024-07-24 19:07:41 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 19:13:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0859, -2.8125,  4.7188,  ...,  1.1328,  0.6484, -0.1992],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([10.6875,  4.1250,  9.6250,  ..., -9.3750, 12.4375,  2.0469],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.0681e-02, -3.4180e-03, -2.1667e-03,  ..., -3.7956e-04,
          8.3008e-03,  4.7302e-03],
        [ 2.1057e-03, -6.1035e-03,  5.2795e-03,  ..., -7.1716e-03,
          4.7302e-03, -8.1635e-04],
        [-8.6670e-03,  7.3853e-03,  8.5938e-02,  ..., -1.3733e-03,
         -8.1787e-03, -2.2888e-03],
        ...,
        [-3.2234e-04, -1.9531e-03, -2.9663e-02,  ..., -2.8992e-03,
         -1.1673e-03, -5.2795e-03],
        [-5.9509e-03,  6.5918e-03,  1.8921e-02,  ..., -1.3046e-03,
          1.3428e-02, -1.5640e-04],
        [ 6.4392e-03, -1.1902e-03,  5.2214e-05,  ..., -6.0120e-03,
         -5.4321e-03,  1.1963e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[10.0781,  3.3359,  9.7188,  ..., -9.9844, 12.8359,  1.9492]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 19:13:14 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a crow makes is called a caw
The sound that a alpaca makes is called a bray
The sound that a duck makes is called a quack
The sound that a cat makes is called a meow
The sound that a donkey makes is called a bray
The sound that a ferret makes is called a dook
The sound that a hound makes is called a bark
The sound that a bear makes is called a
2024-07-24 19:13:14 root INFO     [order_1_approx] starting weight calculation for The sound that a crow makes is called a caw
The sound that a bear makes is called a growl
The sound that a duck makes is called a quack
The sound that a cat makes is called a meow
The sound that a donkey makes is called a bray
The sound that a ferret makes is called a dook
The sound that a alpaca makes is called a bray
The sound that a hound makes is called a
2024-07-24 19:13:14 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 19:13:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([2.0781, 0.3477, 2.1719,  ..., 1.0781, 2.5469, 1.8750], device='cuda:0',
       dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  3.6562,  -1.6875,  -2.8750,  ...,   0.1250, -10.8750,  -2.3438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-0.0057, -0.0123, -0.0064,  ...,  0.0009,  0.0172, -0.0021],
        [ 0.0137,  0.0027, -0.0244,  ...,  0.0024, -0.0034,  0.0043],
        [-0.0166, -0.0075,  0.0889,  ..., -0.0011, -0.0064, -0.0011],
        ...,
        [-0.0041,  0.0002, -0.0249,  ..., -0.0016,  0.0099, -0.0006],
        [-0.0051, -0.0004, -0.0184,  ...,  0.0070,  0.0168,  0.0023],
        [-0.0144,  0.0074,  0.0137,  ...,  0.0021,  0.0050,  0.0206]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7031, -0.0615, -3.0645,  ...,  0.8818, -9.0156,  0.8008]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 19:13:25 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Another word for obsolete is outdated
Another word for mother is mom
Another word for organized is arranged
Another word for market is marketplace
Another word for snake is serpent
Another word for style is manner
Another word for child is kid
Another word for vocabulary is
2024-07-24 19:13:25 root INFO     [order_1_approx] starting weight calculation for Another word for vocabulary is lexicon
Another word for style is manner
Another word for mother is mom
Another word for organized is arranged
Another word for obsolete is outdated
Another word for snake is serpent
Another word for child is kid
Another word for market is
2024-07-24 19:13:25 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 19:19:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.0469,  0.6094,  4.2812,  ...,  1.5078, -1.1562,  0.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 1.3047,  4.0000,  9.6875,  ..., -1.7188, -0.7656,  3.9688],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.2329e-02, -9.0942e-03, -4.3335e-03,  ..., -7.1716e-04,
          5.9814e-03,  3.5095e-03],
        [ 6.5002e-03,  8.1177e-03,  5.4169e-04,  ..., -5.9204e-03,
         -2.1515e-03, -5.1270e-03],
        [-7.7820e-03,  8.9264e-04,  7.3730e-02,  ..., -1.1063e-03,
          2.3193e-03,  3.2043e-04],
        ...,
        [-3.4332e-05,  2.8839e-03, -1.7334e-02,  ...,  6.4697e-03,
         -9.3384e-03,  6.5918e-03],
        [ 6.2866e-03, -2.5940e-04,  8.7891e-03,  ...,  2.7466e-03,
          2.1851e-02, -8.4839e-03],
        [-4.3335e-03,  8.8501e-03,  4.8218e-03,  ...,  4.8218e-03,
         -9.5367e-04,  1.4465e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3877,  2.8516,  8.6875,  ..., -0.4531, -1.0723,  4.1484]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 19:19:08 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a crow makes is called a caw
The sound that a bear makes is called a growl
The sound that a duck makes is called a quack
The sound that a cat makes is called a meow
The sound that a donkey makes is called a bray
The sound that a ferret makes is called a dook
The sound that a alpaca makes is called a bray
The sound that a hound makes is called a
2024-07-24 19:19:08 root INFO     [order_1_approx] starting weight calculation for The sound that a cat makes is called a meow
The sound that a hound makes is called a bark
The sound that a alpaca makes is called a bray
The sound that a crow makes is called a caw
The sound that a duck makes is called a quack
The sound that a ferret makes is called a dook
The sound that a bear makes is called a growl
The sound that a donkey makes is called a
2024-07-24 19:19:08 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 19:19:10 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8828,  0.6562,  2.1094,  ...,  1.6562, -2.5781, -0.1094],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([11.5000,  6.0625, 10.3750,  ...,  7.9688, -3.3750, -4.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0113, -0.0063, -0.0088,  ..., -0.0012,  0.0063, -0.0094],
        [-0.0160,  0.0088,  0.0253,  ..., -0.0135, -0.0094,  0.0007],
        [ 0.0004,  0.0023,  0.1001,  ...,  0.0064,  0.0094, -0.0059],
        ...,
        [ 0.0040, -0.0128, -0.0012,  ...,  0.0034,  0.0041,  0.0078],
        [ 0.0005,  0.0109,  0.0002,  ...,  0.0019,  0.0374, -0.0053],
        [ 0.0103,  0.0026, -0.0040,  ..., -0.0014, -0.0049,  0.0159]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[11.7500,  4.6484, 10.6094,  ...,  9.5078, -3.0859, -1.9727]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 19:19:11 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Another word for vocabulary is lexicon
Another word for style is manner
Another word for mother is mom
Another word for organized is arranged
Another word for obsolete is outdated
Another word for snake is serpent
Another word for child is kid
Another word for market is
2024-07-24 19:19:11 root INFO     [order_1_approx] starting weight calculation for Another word for organized is arranged
Another word for obsolete is outdated
Another word for snake is serpent
Another word for market is marketplace
Another word for style is manner
Another word for vocabulary is lexicon
Another word for child is kid
Another word for mother is
2024-07-24 19:19:11 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 19:24:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.1250, -0.1035,  2.1562,  ...,  0.9492,  0.5312, -1.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 6.7500,  5.6562, 13.6250,  ..., -4.2188, 11.8750, -2.8906],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 4.2725e-03,  3.8528e-04,  1.3062e-02,  ..., -4.6349e-04,
         -7.6294e-03,  1.1139e-03],
        [-1.0834e-03,  2.5330e-03, -1.7456e-02,  ..., -1.1139e-03,
         -2.2736e-03,  5.7983e-04],
        [ 2.4109e-03, -9.8419e-04,  5.7617e-02,  ...,  5.3024e-04,
         -2.7466e-03, -1.3046e-03],
        ...,
        [-2.3651e-03, -3.6240e-05,  1.5182e-03,  ..., -1.7548e-04,
         -8.3008e-03,  1.4282e-02],
        [-4.4861e-03,  5.9509e-04, -6.0425e-03,  ...,  2.7313e-03,
          1.5747e-02, -4.6082e-03],
        [ 5.5237e-03, -1.3199e-03,  4.2114e-03,  ...,  3.4180e-03,
         -1.2024e-02,  1.4221e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.9180,  4.3320, 13.6172,  ..., -3.8418, 12.1094, -1.5791]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 19:24:56 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Another word for organized is arranged
Another word for obsolete is outdated
Another word for snake is serpent
Another word for market is marketplace
Another word for style is manner
Another word for vocabulary is lexicon
Another word for child is kid
Another word for mother is
2024-07-24 19:24:57 root INFO     [order_1_approx] starting weight calculation for Another word for market is marketplace
Another word for style is manner
Another word for obsolete is outdated
Another word for child is kid
Another word for vocabulary is lexicon
Another word for organized is arranged
Another word for mother is mom
Another word for snake is
2024-07-24 19:24:57 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 19:25:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1211, -3.0156,  1.8047,  ...,  3.0469, -6.4062,  2.7812],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 2.9844, 15.1875, 18.6250,  ...,  4.1875, -3.6719,  0.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.0925e-02,  8.5449e-04, -6.5613e-03,  ...,  1.3733e-03,
          1.3306e-02, -4.9591e-04],
        [ 9.7046e-03,  5.3406e-05,  2.2217e-02,  ..., -4.4556e-03,
          1.0010e-02, -1.0681e-03],
        [-7.0496e-03, -2.8229e-04,  8.8379e-02,  ..., -4.1199e-04,
         -4.2725e-03,  1.0300e-03],
        ...,
        [-1.0132e-02, -2.9297e-03, -4.1504e-03,  ...,  1.3123e-03,
         -6.4697e-03, -6.8054e-03],
        [ 1.6251e-03,  4.1809e-03,  3.9062e-03,  ..., -5.3406e-03,
          1.5015e-02,  5.6763e-03],
        [ 8.8501e-03,  3.5477e-04, -1.3855e-02,  ...,  1.7395e-03,
          3.2043e-03,  1.9897e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0889, 15.4297, 18.1406,  ...,  3.3789, -3.6250, -0.8213]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 19:25:08 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a cat makes is called a meow
The sound that a hound makes is called a bark
The sound that a alpaca makes is called a bray
The sound that a crow makes is called a caw
The sound that a duck makes is called a quack
The sound that a ferret makes is called a dook
The sound that a bear makes is called a growl
The sound that a donkey makes is called a
2024-07-24 19:25:08 root INFO     [order_1_approx] starting weight calculation for The sound that a ferret makes is called a dook
The sound that a alpaca makes is called a bray
The sound that a hound makes is called a bark
The sound that a cat makes is called a meow
The sound that a bear makes is called a growl
The sound that a duck makes is called a quack
The sound that a donkey makes is called a bray
The sound that a crow makes is called a
2024-07-24 19:25:09 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 19:30:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6250, -0.2305, -1.4453,  ...,  2.7188,  0.4844, -0.1797],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 2.9688,  1.5938,  0.8047,  ..., -6.1875, 14.7500,  0.6719],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 5.9509e-03,  2.8229e-04,  4.0283e-02,  ...,  6.7139e-03,
         -7.4158e-03,  8.8501e-04],
        [ 2.9144e-03,  7.8735e-03,  1.5869e-02,  ..., -4.0588e-03,
         -2.1515e-03, -3.1281e-03],
        [-5.6152e-03, -5.3406e-04,  8.2031e-02,  ...,  1.3428e-03,
          9.5825e-03, -4.0588e-03],
        ...,
        [ 1.4420e-03,  3.8910e-03,  9.6436e-03,  ...,  6.3171e-03,
         -5.3406e-05,  4.9133e-03],
        [-6.1646e-03,  1.9226e-03, -1.9073e-03,  ...,  9.0790e-04,
          1.9897e-02, -6.7139e-03],
        [ 1.4191e-03, -2.0905e-03, -7.2937e-03,  ...,  5.4016e-03,
         -5.1575e-03,  1.3916e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.3438,  3.5039,  1.7754,  ..., -5.0625, 12.5469, -1.0811]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 19:30:43 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Another word for market is marketplace
Another word for style is manner
Another word for obsolete is outdated
Another word for child is kid
Another word for vocabulary is lexicon
Another word for organized is arranged
Another word for mother is mom
Another word for snake is
2024-07-24 19:30:43 root INFO     [order_1_approx] starting weight calculation for Another word for snake is serpent
Another word for organized is arranged
Another word for vocabulary is lexicon
Another word for market is marketplace
Another word for mother is mom
Another word for child is kid
Another word for style is manner
Another word for obsolete is
2024-07-24 19:30:43 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 19:31:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1797, -1.1406,  5.8438,  ..., -0.3867, -1.3047, -3.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-0.7500,  1.4844,  4.1562,  ..., 12.5000,  4.6250, -6.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0129,  0.0009,  0.0022,  ..., -0.0030,  0.0101,  0.0009],
        [ 0.0128,  0.0004,  0.0122,  ..., -0.0061,  0.0052, -0.0080],
        [-0.0023,  0.0110,  0.1045,  ..., -0.0003, -0.0027, -0.0013],
        ...,
        [ 0.0033, -0.0012,  0.0092,  ..., -0.0037, -0.0065, -0.0127],
        [-0.0039,  0.0068,  0.0118,  ...,  0.0024,  0.0265,  0.0052],
        [-0.0006,  0.0004, -0.0036,  ..., -0.0039,  0.0036,  0.0171]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.0464,  1.3984,  5.4844,  ..., 13.1250,  5.2070, -7.0820]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 19:31:03 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a ferret makes is called a dook
The sound that a alpaca makes is called a bray
The sound that a hound makes is called a bark
The sound that a cat makes is called a meow
The sound that a bear makes is called a growl
The sound that a duck makes is called a quack
The sound that a donkey makes is called a bray
The sound that a crow makes is called a
2024-07-24 19:31:04 root INFO     [order_1_approx] starting weight calculation for The sound that a crow makes is called a caw
The sound that a bear makes is called a growl
The sound that a alpaca makes is called a bray
The sound that a ferret makes is called a dook
The sound that a donkey makes is called a bray
The sound that a cat makes is called a meow
The sound that a hound makes is called a bark
The sound that a duck makes is called a
2024-07-24 19:31:04 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 19:36:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 4.3125, -0.5156,  0.6289,  ...,  1.1094, -1.8984,  3.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-6.8125, -5.3125, -5.1562,  ..., -1.0781, -6.8125,  3.9062],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0022,  0.0112,  0.0356,  ...,  0.0149,  0.0091,  0.0058],
        [ 0.0016, -0.0071, -0.0125,  ..., -0.0048, -0.0064, -0.0120],
        [-0.0048, -0.0045,  0.0535,  ...,  0.0028, -0.0050, -0.0070],
        ...,
        [-0.0063, -0.0006,  0.0205,  ...,  0.0089,  0.0039, -0.0118],
        [-0.0080,  0.0007, -0.0030,  ...,  0.0097,  0.0129,  0.0003],
        [-0.0125,  0.0027, -0.0031,  ..., -0.0090, -0.0065,  0.0029]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-8.2031, -5.4375, -4.8945,  ...,  0.7637, -7.3320,  2.8398]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 19:36:30 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Another word for snake is serpent
Another word for organized is arranged
Another word for vocabulary is lexicon
Another word for market is marketplace
Another word for mother is mom
Another word for child is kid
Another word for style is manner
Another word for obsolete is
2024-07-24 19:36:30 root INFO     total operator prediction time: 2756.390604734421 seconds
2024-07-24 19:36:30 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on hypernyms - misc
2024-07-24 19:36:30 root INFO     building operator hypernyms - misc
2024-07-24 19:36:30 root INFO     [order_1_approx] starting weight calculation for The cake falls into the category of dessert
The croissant falls into the category of pastry
The notepad falls into the category of pad
The perfume falls into the category of toiletry
The photo falls into the category of picture
The cup falls into the category of tableware
The computer falls into the category of device
The notebook falls into the category of
2024-07-24 19:36:30 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 19:36:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6406, -1.9844, -0.4297,  ...,  2.2656, -4.6250,  1.0234],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-6.5000,  5.4375,  5.6875,  ..., -3.1875,  0.8750, -3.2812],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0006,  0.0023,  0.0032,  ...,  0.0020, -0.0016, -0.0005],
        [ 0.0076,  0.0010,  0.0147,  ..., -0.0026,  0.0082, -0.0003],
        [-0.0077,  0.0051,  0.0728,  ..., -0.0020, -0.0009, -0.0004],
        ...,
        [-0.0023, -0.0054, -0.0091,  ...,  0.0067, -0.0110, -0.0011],
        [ 0.0040,  0.0007,  0.0031,  ...,  0.0020,  0.0266,  0.0038],
        [ 0.0026,  0.0045,  0.0017,  ..., -0.0027,  0.0012,  0.0093]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.8047,  6.0820,  6.4141,  ..., -2.6836,  0.3965, -4.0938]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 19:36:49 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The sound that a crow makes is called a caw
The sound that a bear makes is called a growl
The sound that a alpaca makes is called a bray
The sound that a ferret makes is called a dook
The sound that a donkey makes is called a bray
The sound that a cat makes is called a meow
The sound that a hound makes is called a bark
The sound that a duck makes is called a
2024-07-24 19:36:49 root INFO     total operator prediction time: 2835.879470348358 seconds
2024-07-24 19:36:50 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on things - color
2024-07-24 19:36:50 root INFO     building operator things - color
2024-07-24 19:36:50 root INFO     [order_1_approx] starting weight calculation for The coal is colored black
The pepper is colored black
The grass is colored green
The cream is colored white
The emerald is colored green
The coffee is colored black
The carrot is colored orange
The cucumber is colored
2024-07-24 19:36:50 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 19:42:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0312,  2.7656,  4.0312,  ...,  2.5781, -2.9688, -0.8633],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  0.7812,   3.2500,  16.2500,  ...,  -2.2812, -10.5000,   0.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0172, -0.0019,  0.0160,  ...,  0.0031,  0.0078,  0.0002],
        [ 0.0079,  0.0060, -0.0018,  ...,  0.0106,  0.0041,  0.0067],
        [-0.0095, -0.0089,  0.1118,  ...,  0.0064,  0.0073, -0.0031],
        ...,
        [ 0.0081, -0.0017, -0.0076,  ...,  0.0101,  0.0028,  0.0035],
        [ 0.0095,  0.0008, -0.0154,  ..., -0.0076,  0.0273,  0.0005],
        [ 0.0089,  0.0043,  0.0154,  ...,  0.0126,  0.0020,  0.0381]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  0.5503,   2.0273,  16.9375,  ...,  -2.0293, -10.1562,   1.2686]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 19:42:18 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The cake falls into the category of dessert
The croissant falls into the category of pastry
The notepad falls into the category of pad
The perfume falls into the category of toiletry
The photo falls into the category of picture
The cup falls into the category of tableware
The computer falls into the category of device
The notebook falls into the category of
2024-07-24 19:42:18 root INFO     [order_1_approx] starting weight calculation for The cake falls into the category of dessert
The croissant falls into the category of pastry
The notebook falls into the category of book
The photo falls into the category of picture
The cup falls into the category of tableware
The perfume falls into the category of toiletry
The computer falls into the category of device
The notepad falls into the category of
2024-07-24 19:42:18 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 19:42:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2656, -0.6406,  4.1250,  ...,  2.5469, -2.2500,  0.6289],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-1.7734, -4.9375, -4.6562,  ..., -7.5312, -8.5000, -2.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0109,  0.0065,  0.0115,  ...,  0.0015, -0.0038,  0.0085],
        [ 0.0057,  0.0069, -0.0080,  ...,  0.0003, -0.0004,  0.0027],
        [-0.0015, -0.0029,  0.1187,  ..., -0.0050, -0.0082, -0.0046],
        ...,
        [ 0.0008, -0.0056, -0.0197,  ...,  0.0079, -0.0085,  0.0001],
        [ 0.0034,  0.0040, -0.0074,  ..., -0.0005,  0.0299, -0.0062],
        [-0.0014, -0.0032, -0.0150,  ..., -0.0020,  0.0004,  0.0168]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.5977, -5.5781, -3.9414,  ..., -7.7617, -8.5156, -3.1367]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 19:42:51 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The coal is colored black
The pepper is colored black
The grass is colored green
The cream is colored white
The emerald is colored green
The coffee is colored black
The carrot is colored orange
The cucumber is colored
2024-07-24 19:42:51 root INFO     [order_1_approx] starting weight calculation for The pepper is colored black
The cucumber is colored green
The cream is colored white
The carrot is colored orange
The grass is colored green
The emerald is colored green
The coal is colored black
The coffee is colored
2024-07-24 19:42:51 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 19:48:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1797,  2.7656,  2.2656,  ...,  1.8438, -2.0312, -1.4844],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -0.4844,   6.5000,  10.3125,  ...,   1.8281, -15.8750,   1.7812],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.0529e-03, -2.5330e-03, -2.4414e-03,  ..., -9.6893e-04,
         -3.0518e-05, -5.3101e-03],
        [-1.3046e-03,  6.8970e-03, -6.8054e-03,  ..., -1.6251e-03,
         -1.8921e-03, -3.2196e-03],
        [ 1.3733e-03, -3.7537e-03,  8.3008e-02,  ...,  4.2114e-03,
          3.4180e-03,  2.6855e-03],
        ...,
        [ 3.5095e-04, -2.6703e-04, -9.0332e-03,  ...,  1.0437e-02,
         -3.0823e-03,  4.5013e-04],
        [-2.5482e-03,  8.6975e-04, -1.4801e-03,  ..., -6.2561e-03,
          2.5146e-02,  8.7891e-03],
        [ 4.3488e-04,  4.6692e-03,  5.1575e-03,  ...,  1.5564e-03,
          2.2888e-04,  2.7100e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -1.1777,   6.7617,  10.6484,  ...,   1.6035, -15.8516,   3.6309]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 19:48:03 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The cake falls into the category of dessert
The croissant falls into the category of pastry
The notebook falls into the category of book
The photo falls into the category of picture
The cup falls into the category of tableware
The perfume falls into the category of toiletry
The computer falls into the category of device
The notepad falls into the category of
2024-07-24 19:48:03 root INFO     [order_1_approx] starting weight calculation for The cup falls into the category of tableware
The croissant falls into the category of pastry
The photo falls into the category of picture
The computer falls into the category of device
The notepad falls into the category of pad
The notebook falls into the category of book
The perfume falls into the category of toiletry
The cake falls into the category of
2024-07-24 19:48:04 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 19:48:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5469, -0.0859,  6.4688,  ...,  1.2969, -2.3906,  0.8945],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 3.2500, -4.6875,  0.3750,  ..., -4.2500,  0.6797,  1.7188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0150,  0.0091,  0.0040,  ...,  0.0051,  0.0016,  0.0112],
        [ 0.0057,  0.0022, -0.0139,  ...,  0.0029, -0.0029,  0.0081],
        [-0.0072, -0.0073,  0.0698,  ..., -0.0031,  0.0034, -0.0021],
        ...,
        [ 0.0074,  0.0027, -0.0022,  ...,  0.0031, -0.0004,  0.0019],
        [-0.0062,  0.0012,  0.0150,  ...,  0.0042,  0.0232, -0.0012],
        [-0.0005, -0.0009, -0.0112,  ..., -0.0025, -0.0126,  0.0128]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.1875e+00, -4.2070e+00,  1.7090e-03,  ..., -3.3906e+00,
          1.8252e+00,  4.4824e-01]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SubBackward0>) 

                    
2024-07-24 19:48:48 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The pepper is colored black
The cucumber is colored green
The cream is colored white
The carrot is colored orange
The grass is colored green
The emerald is colored green
The coal is colored black
The coffee is colored
2024-07-24 19:48:48 root INFO     [order_1_approx] starting weight calculation for The carrot is colored orange
The pepper is colored black
The coffee is colored black
The grass is colored green
The cucumber is colored green
The coal is colored black
The cream is colored white
The emerald is colored
2024-07-24 19:48:48 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 19:53:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5078, -1.3203,  2.0312,  ...,  0.7539, -0.6914,  1.2812],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  3.0312,  -2.1562,  -3.8125,  ...,   7.3125,  12.1875, -15.3125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0034,  0.0045,  0.0200,  ..., -0.0012,  0.0019,  0.0009],
        [-0.0012,  0.0082,  0.0176,  ...,  0.0036,  0.0044,  0.0073],
        [ 0.0045, -0.0018,  0.0693,  ..., -0.0011, -0.0036,  0.0011],
        ...,
        [-0.0008, -0.0002,  0.0009,  ...,  0.0033,  0.0014, -0.0017],
        [ 0.0007, -0.0018, -0.0037,  ..., -0.0012,  0.0177, -0.0002],
        [-0.0013, -0.0053, -0.0117,  ..., -0.0016,  0.0023,  0.0037]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  2.3906,  -2.9570,  -5.2227,  ...,   8.7656,  11.0469, -13.1250]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 19:53:48 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The cup falls into the category of tableware
The croissant falls into the category of pastry
The photo falls into the category of picture
The computer falls into the category of device
The notepad falls into the category of pad
The notebook falls into the category of book
The perfume falls into the category of toiletry
The cake falls into the category of
2024-07-24 19:53:48 root INFO     [order_1_approx] starting weight calculation for The cup falls into the category of tableware
The notepad falls into the category of pad
The notebook falls into the category of book
The photo falls into the category of picture
The cake falls into the category of dessert
The croissant falls into the category of pastry
The perfume falls into the category of toiletry
The computer falls into the category of
2024-07-24 19:53:48 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 19:54:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.8125,  0.5664,  0.1914,  ...,  0.7266, -1.4297,  0.9609],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -1.7500, -10.6250,  -1.0312,  ...,   1.9297,  -1.5625,  -2.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.1353e-02,  2.2339e-02, -3.9062e-03,  ..., -1.0376e-02,
         -1.8433e-02,  1.1292e-03],
        [-7.1411e-03,  5.8289e-03,  1.9531e-02,  ..., -3.4714e-04,
         -6.5613e-03,  2.0790e-04],
        [ 2.7618e-03, -2.1057e-03,  1.0742e-01,  ..., -4.0894e-03,
          2.7161e-03, -8.9111e-03],
        ...,
        [ 2.5940e-03, -1.2817e-02, -1.7944e-02,  ...,  5.9814e-03,
          1.2756e-02, -3.9673e-03],
        [ 7.6294e-05,  8.4839e-03, -4.0283e-03,  ..., -7.5684e-03,
          1.7090e-02, -5.1270e-03],
        [-1.3977e-02, -2.1484e-02,  1.1353e-02,  ...,  1.9043e-02,
          1.1963e-02,  1.6113e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -3.2324, -11.3359,  -2.9688,  ...,   2.4375,  -2.9492,  -0.0195]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 19:54:48 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The carrot is colored orange
The pepper is colored black
The coffee is colored black
The grass is colored green
The cucumber is colored green
The coal is colored black
The cream is colored white
The emerald is colored
2024-07-24 19:54:49 root INFO     [order_1_approx] starting weight calculation for The pepper is colored black
The grass is colored green
The carrot is colored orange
The cream is colored white
The emerald is colored green
The coffee is colored black
The cucumber is colored green
The coal is colored
2024-07-24 19:54:49 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 19:59:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.8125, -0.2773,  4.2500,  ..., -1.5859, -1.6641, -0.5977],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -7.1250,  -0.2500,  14.1250,  ...,   2.2031, -12.0000, -12.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-0.0003, -0.0012,  0.0059,  ..., -0.0030, -0.0006, -0.0032],
        [ 0.0044, -0.0010,  0.0007,  ...,  0.0039,  0.0061,  0.0024],
        [-0.0013,  0.0007,  0.0527,  ...,  0.0018, -0.0072,  0.0031],
        ...,
        [-0.0007, -0.0037,  0.0014,  ...,  0.0014,  0.0021,  0.0014],
        [-0.0045,  0.0036, -0.0031,  ..., -0.0007,  0.0151, -0.0004],
        [-0.0027, -0.0027, -0.0052,  ...,  0.0019, -0.0025,  0.0156]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -6.9531,  -1.8789,  14.0234,  ...,   1.5186, -13.5859, -10.4609]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 19:59:38 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The cup falls into the category of tableware
The notepad falls into the category of pad
The notebook falls into the category of book
The photo falls into the category of picture
The cake falls into the category of dessert
The croissant falls into the category of pastry
The perfume falls into the category of toiletry
The computer falls into the category of
2024-07-24 19:59:38 root INFO     [order_1_approx] starting weight calculation for The perfume falls into the category of toiletry
The computer falls into the category of device
The cake falls into the category of dessert
The notebook falls into the category of book
The croissant falls into the category of pastry
The notepad falls into the category of pad
The cup falls into the category of tableware
The photo falls into the category of
2024-07-24 19:59:38 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 20:00:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5469,  0.0234,  1.3750,  ..., -0.2500, -2.7656, -1.5078],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  5.7188,  -9.5000,  -7.8750,  ...,  -7.0000,  -5.5000, -11.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0092,  0.0029, -0.0029,  ..., -0.0009, -0.0015,  0.0020],
        [ 0.0048,  0.0063, -0.0002,  ...,  0.0026,  0.0020,  0.0084],
        [ 0.0009, -0.0070,  0.0625,  ..., -0.0017, -0.0071,  0.0037],
        ...,
        [-0.0013, -0.0026, -0.0032,  ...,  0.0034,  0.0005, -0.0046],
        [-0.0030,  0.0010, -0.0067,  ..., -0.0052,  0.0126,  0.0008],
        [-0.0044, -0.0070, -0.0087,  ..., -0.0042, -0.0051,  0.0078]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  5.5156,  -8.5859,  -7.7383,  ...,  -6.9648,  -5.8984, -11.1562]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 20:00:50 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The pepper is colored black
The grass is colored green
The carrot is colored orange
The cream is colored white
The emerald is colored green
The coffee is colored black
The cucumber is colored green
The coal is colored
2024-07-24 20:00:50 root INFO     [order_1_approx] starting weight calculation for The carrot is colored orange
The grass is colored green
The emerald is colored green
The cream is colored white
The coffee is colored black
The cucumber is colored green
The coal is colored black
The pepper is colored
2024-07-24 20:00:50 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 20:05:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7539,  0.0547, -0.4766,  ..., -0.3750, -0.9102,  0.5586],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 2.5000,  5.9062, -9.2500,  ...,  1.9766,  2.1875,  4.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0014, -0.0018,  0.0064,  ..., -0.0040,  0.0042, -0.0054],
        [ 0.0007,  0.0014,  0.0027,  ..., -0.0013,  0.0063,  0.0029],
        [-0.0055,  0.0017,  0.0737,  ..., -0.0064, -0.0001, -0.0020],
        ...,
        [-0.0020, -0.0037, -0.0015,  ...,  0.0014,  0.0011, -0.0046],
        [-0.0002,  0.0020,  0.0140,  ...,  0.0046,  0.0260,  0.0084],
        [ 0.0061, -0.0034, -0.0004,  ...,  0.0067,  0.0054,  0.0112]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.4395,  5.8555, -9.8516,  ...,  3.0410,  2.4141,  5.5781]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 20:05:22 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The perfume falls into the category of toiletry
The computer falls into the category of device
The cake falls into the category of dessert
The notebook falls into the category of book
The croissant falls into the category of pastry
The notepad falls into the category of pad
The cup falls into the category of tableware
The photo falls into the category of
2024-07-24 20:05:23 root INFO     [order_1_approx] starting weight calculation for The notepad falls into the category of pad
The cake falls into the category of dessert
The computer falls into the category of device
The notebook falls into the category of book
The photo falls into the category of picture
The croissant falls into the category of pastry
The perfume falls into the category of toiletry
The cup falls into the category of
2024-07-24 20:05:23 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 20:06:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9531,  0.6484,  7.3438,  ...,  0.5078, -5.7500,  1.4375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-4.1562,  4.1250, -3.4375,  ..., -1.1953, -3.1250, -4.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.0147e-03, -3.0823e-03,  1.5991e-02,  ..., -5.0049e-03,
          1.7834e-04, -5.4016e-03],
        [-1.8082e-03,  6.0730e-03, -1.1963e-02,  ...,  2.1515e-03,
          5.7983e-04,  3.1891e-03],
        [-6.9275e-03,  6.1035e-05,  1.0645e-01,  ..., -1.0925e-02,
         -7.1106e-03,  5.8746e-04],
        ...,
        [ 1.4801e-03,  2.3499e-03, -1.7822e-02,  ...,  1.3367e-02,
         -8.4229e-03, -4.1809e-03],
        [ 6.8665e-04,  7.8735e-03,  1.1597e-02,  ...,  2.8992e-03,
          1.7822e-02, -3.3875e-03],
        [-7.3547e-03,  9.5215e-03, -3.3691e-02,  ...,  2.3460e-04,
         -1.5625e-02,  1.7578e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.2109,  5.3516, -5.2734,  ...,  0.8066, -3.5898, -4.0586]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 20:06:46 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The carrot is colored orange
The grass is colored green
The emerald is colored green
The cream is colored white
The coffee is colored black
The cucumber is colored green
The coal is colored black
The pepper is colored
2024-07-24 20:06:47 root INFO     [order_1_approx] starting weight calculation for The coffee is colored black
The coal is colored black
The grass is colored green
The emerald is colored green
The cucumber is colored green
The pepper is colored black
The carrot is colored orange
The cream is colored
2024-07-24 20:06:47 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 20:11:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.0625, -1.2812,  3.8906,  ...,  0.7891,  0.8945, -0.8906],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-7.1562,  4.9375,  0.5938,  ...,  3.3906, -9.0000, -9.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0042,  0.0026,  0.0071,  ..., -0.0035,  0.0088, -0.0062],
        [ 0.0062,  0.0052,  0.0094,  ...,  0.0035, -0.0071,  0.0009],
        [-0.0033, -0.0009,  0.0620,  ...,  0.0014, -0.0021,  0.0002],
        ...,
        [-0.0012,  0.0009,  0.0053,  ...,  0.0029,  0.0021, -0.0002],
        [-0.0063, -0.0026, -0.0004,  ...,  0.0037,  0.0220,  0.0011],
        [-0.0044, -0.0047, -0.0109,  ..., -0.0012,  0.0067,  0.0087]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-7.6406,  6.5938,  1.3740,  ...,  2.8750, -9.2500, -9.7656]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 20:11:08 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The notepad falls into the category of pad
The cake falls into the category of dessert
The computer falls into the category of device
The notebook falls into the category of book
The photo falls into the category of picture
The croissant falls into the category of pastry
The perfume falls into the category of toiletry
The cup falls into the category of
2024-07-24 20:11:08 root INFO     [order_1_approx] starting weight calculation for The cake falls into the category of dessert
The computer falls into the category of device
The cup falls into the category of tableware
The photo falls into the category of picture
The croissant falls into the category of pastry
The notepad falls into the category of pad
The notebook falls into the category of book
The perfume falls into the category of
2024-07-24 20:11:08 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 20:12:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2363,  0.9531,  5.0312,  ...,  0.8516, -3.6406,  1.3906],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-0.8438, -0.2773,  4.3125,  ...,  1.2031, -5.0000,  2.0938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0099,  0.0060, -0.0188,  ...,  0.0095, -0.0063,  0.0073],
        [ 0.0086, -0.0010, -0.0165,  ...,  0.0011, -0.0005,  0.0066],
        [-0.0056, -0.0033,  0.0708,  ...,  0.0112,  0.0019, -0.0035],
        ...,
        [-0.0023,  0.0019,  0.0124,  ...,  0.0090, -0.0004, -0.0033],
        [ 0.0015,  0.0014,  0.0177,  ..., -0.0017,  0.0184,  0.0022],
        [-0.0049, -0.0075, -0.0170,  ..., -0.0058,  0.0020,  0.0081]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-0.8770, -1.2617,  4.9844,  ...,  2.5352, -4.5469,  0.6836]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 20:12:40 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The coffee is colored black
The coal is colored black
The grass is colored green
The emerald is colored green
The cucumber is colored green
The pepper is colored black
The carrot is colored orange
The cream is colored
2024-07-24 20:12:40 root INFO     [order_1_approx] starting weight calculation for The coal is colored black
The cream is colored white
The cucumber is colored green
The coffee is colored black
The emerald is colored green
The pepper is colored black
The grass is colored green
The carrot is colored
2024-07-24 20:12:40 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 20:16:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.5938, -1.3828, -0.4609,  ...,  0.9102, -0.9453, -0.7148],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 8.0625, -4.8750, -2.0781,  ..., 15.9375,  3.3750,  0.7305],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0043, -0.0003,  0.0065,  ..., -0.0017, -0.0039,  0.0018],
        [-0.0087,  0.0033, -0.0044,  ...,  0.0065,  0.0145, -0.0009],
        [-0.0032, -0.0058,  0.0713,  ..., -0.0034, -0.0053, -0.0066],
        ...,
        [ 0.0068,  0.0038, -0.0042,  ..., -0.0042, -0.0029,  0.0028],
        [-0.0089,  0.0016, -0.0010,  ..., -0.0036,  0.0227,  0.0143],
        [ 0.0038,  0.0027, -0.0067,  ...,  0.0020,  0.0057,  0.0140]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 8.2812, -5.2578, -0.9297,  ..., 17.4688,  3.6113,  2.1465]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 20:16:57 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The cake falls into the category of dessert
The computer falls into the category of device
The cup falls into the category of tableware
The photo falls into the category of picture
The croissant falls into the category of pastry
The notepad falls into the category of pad
The notebook falls into the category of book
The perfume falls into the category of
2024-07-24 20:16:57 root INFO     [order_1_approx] starting weight calculation for The notebook falls into the category of book
The computer falls into the category of device
The cake falls into the category of dessert
The perfume falls into the category of toiletry
The photo falls into the category of picture
The cup falls into the category of tableware
The notepad falls into the category of pad
The croissant falls into the category of
2024-07-24 20:16:57 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 20:18:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.3750, -0.8555,  2.5000,  ...,  0.8047, -3.8438,  3.6406],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 4.3750, -4.2500,  3.9688,  ...,  1.1328, -6.5938, -4.8125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0214,  0.0122, -0.0081,  ..., -0.0037, -0.0049,  0.0013],
        [ 0.0034,  0.0078, -0.0115,  ...,  0.0009, -0.0037,  0.0054],
        [-0.0016, -0.0027,  0.0825,  ..., -0.0018, -0.0059, -0.0124],
        ...,
        [-0.0018,  0.0034, -0.0074,  ...,  0.0010, -0.0117,  0.0056],
        [-0.0021,  0.0023,  0.0157,  ...,  0.0027,  0.0283, -0.0046],
        [-0.0033,  0.0057,  0.0012,  ..., -0.0024, -0.0103,  0.0210]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.1328, -4.3438,  4.4180,  ...,  1.4062, -7.6406, -6.5156]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 20:18:33 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The coal is colored black
The cream is colored white
The cucumber is colored green
The coffee is colored black
The emerald is colored green
The pepper is colored black
The grass is colored green
The carrot is colored
2024-07-24 20:18:33 root INFO     [order_1_approx] starting weight calculation for The cream is colored white
The carrot is colored orange
The emerald is colored green
The coffee is colored black
The pepper is colored black
The coal is colored black
The cucumber is colored green
The grass is colored
2024-07-24 20:18:33 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 20:22:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.0156,  0.3027,  0.2930,  ...,  0.7266, -3.0312,  2.3125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 1.2031, -1.5781,  4.1875,  ...,  0.5781,  1.9844, -1.4062],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0096,  0.0034,  0.0189,  ..., -0.0030,  0.0066, -0.0030],
        [-0.0016,  0.0044,  0.0188,  ...,  0.0029,  0.0111, -0.0023],
        [ 0.0011, -0.0043,  0.1172,  ...,  0.0021, -0.0082, -0.0014],
        ...,
        [ 0.0047,  0.0010,  0.0070,  ...,  0.0109, -0.0011, -0.0069],
        [-0.0002,  0.0053, -0.0031,  ..., -0.0013,  0.0304,  0.0041],
        [-0.0019, -0.0029, -0.0179,  ..., -0.0131, -0.0015,  0.0192]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6860, -2.5918,  3.9785,  ...,  1.8691,  0.4961, -2.1562]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 20:22:39 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The notebook falls into the category of book
The computer falls into the category of device
The cake falls into the category of dessert
The perfume falls into the category of toiletry
The photo falls into the category of picture
The cup falls into the category of tableware
The notepad falls into the category of pad
The croissant falls into the category of
2024-07-24 20:22:39 root INFO     total operator prediction time: 2768.6993446350098 seconds
2024-07-24 20:22:39 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on meronyms - substance
2024-07-24 20:22:39 root INFO     building operator meronyms - substance
2024-07-24 20:22:39 root INFO     [order_1_approx] starting weight calculation for A plastic is made up of polymer
A lens is made up of glass
A house is made up of bricks
A beach is made up of sand
A boots is made up of leather
A cocktail is made up of alcohol
A desk is made up of wood
A money is made up of
2024-07-24 20:22:39 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 20:24:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.8516,  0.5781,  5.7812,  ..., -0.8203, -2.4531, -1.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  3.5938,  -8.2500, -15.8750,  ...,   1.4844,  -2.0312, -11.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0085,  0.0009, -0.0059,  ..., -0.0033,  0.0027,  0.0028],
        [-0.0001, -0.0014,  0.0092,  ..., -0.0017,  0.0047, -0.0027],
        [-0.0065,  0.0042,  0.0679,  ...,  0.0064, -0.0074,  0.0026],
        ...,
        [-0.0044,  0.0008,  0.0045,  ...,  0.0055,  0.0005,  0.0023],
        [-0.0064,  0.0040, -0.0016,  ..., -0.0007,  0.0122, -0.0024],
        [ 0.0009, -0.0039, -0.0130,  ...,  0.0011, -0.0084,  0.0112]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  2.2617,  -7.6328, -16.9219,  ...,   1.3291,  -1.6094, -11.1562]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 20:24:21 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The cream is colored white
The carrot is colored orange
The emerald is colored green
The coffee is colored black
The pepper is colored black
The coal is colored black
The cucumber is colored green
The grass is colored
2024-07-24 20:24:21 root INFO     total operator prediction time: 2851.9301748275757 seconds
2024-07-24 20:24:21 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on country - capital
2024-07-24 20:24:21 root INFO     building operator country - capital
2024-07-24 20:24:22 root INFO     [order_1_approx] starting weight calculation for The country with rome as its capital is known as italy
The country with ankara as its capital is known as turkey
The country with brussels as its capital is known as belgium
The country with bern as its capital is known as switzerland
The country with zagreb as its capital is known as croatia
The country with stockholm as its capital is known as sweden
The country with islamabad as its capital is known as pakistan
The country with beirut as its capital is known as
2024-07-24 20:24:22 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 20:28:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0500, -0.0547,  2.9531,  ..., -1.6719, -2.1094,  0.1689],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -3.6094,  14.0625,   9.6875,  ..., -16.6250,  10.1875,  -0.3438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-1.3885e-03,  6.4697e-03,  8.4839e-03,  ...,  1.7929e-03,
          8.3923e-04, -3.2806e-03],
        [ 1.8311e-03,  2.8381e-03, -2.0142e-02,  ..., -2.5940e-03,
          3.8757e-03,  6.8359e-03],
        [ 9.0332e-03,  1.7357e-04,  6.4941e-02,  ...,  9.8877e-03,
         -1.4114e-04,  8.9111e-03],
        ...,
        [-1.0864e-02, -2.8839e-03, -4.9438e-03,  ...,  1.9302e-03,
          6.8359e-03,  8.9111e-03],
        [-1.9455e-04, -5.9204e-03, -2.8381e-03,  ..., -4.8218e-03,
          9.8877e-03,  7.2479e-05],
        [-1.1536e-02, -6.7520e-04, -4.1992e-02,  ..., -9.3842e-04,
         -1.1444e-03,  1.8311e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -3.7910,  12.7812,   8.5547,  ..., -16.8125,   9.6484,  -0.3494]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 20:28:16 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A plastic is made up of polymer
A lens is made up of glass
A house is made up of bricks
A beach is made up of sand
A boots is made up of leather
A cocktail is made up of alcohol
A desk is made up of wood
A money is made up of
2024-07-24 20:28:16 root INFO     [order_1_approx] starting weight calculation for A money is made up of paper
A house is made up of bricks
A beach is made up of sand
A boots is made up of leather
A cocktail is made up of alcohol
A desk is made up of wood
A lens is made up of glass
A plastic is made up of
2024-07-24 20:28:17 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 20:30:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3203, -0.9141,  9.9375,  ...,  0.0312,  0.5039,  2.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-7.2188, 10.8125, 18.7500,  ..., -9.3750, -0.0312, 13.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 4.3106e-04, -1.4877e-04, -1.3184e-02,  ...,  6.1035e-05,
          5.0964e-03, -1.4420e-03],
        [ 7.3910e-05, -3.2425e-04, -6.9580e-03,  ...,  3.4485e-03,
          7.6904e-03, -3.9673e-03],
        [ 4.3030e-03, -6.5002e-03,  6.4941e-02,  ..., -3.7384e-03,
         -2.0142e-03, -7.3853e-03],
        ...,
        [-1.3885e-03,  1.4038e-03,  1.6251e-03,  ...,  6.8283e-04,
         -5.1575e-03, -1.4725e-03],
        [-1.1749e-03, -2.6703e-04, -1.6022e-03,  ..., -1.2894e-03,
          5.4016e-03, -2.1667e-03],
        [-1.0071e-03, -2.3651e-03,  3.0884e-02,  ..., -5.7220e-04,
          9.6512e-04,  7.6599e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-7.8906, 10.7109, 19.5938,  ..., -8.5781,  0.2722, 13.1953]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 20:30:17 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The country with rome as its capital is known as italy
The country with ankara as its capital is known as turkey
The country with brussels as its capital is known as belgium
The country with bern as its capital is known as switzerland
The country with zagreb as its capital is known as croatia
The country with stockholm as its capital is known as sweden
The country with islamabad as its capital is known as pakistan
The country with beirut as its capital is known as
2024-07-24 20:30:18 root INFO     [order_1_approx] starting weight calculation for The country with islamabad as its capital is known as pakistan
The country with ankara as its capital is known as turkey
The country with bern as its capital is known as switzerland
The country with stockholm as its capital is known as sweden
The country with brussels as its capital is known as belgium
The country with rome as its capital is known as italy
The country with beirut as its capital is known as lebanon
The country with zagreb as its capital is known as
2024-07-24 20:30:18 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 20:34:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 4.9375,  0.3066,  1.7812,  ...,  0.7188, -1.0469, -0.5586],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 0.4375, 13.5625,  1.5703,  ...,  6.3750,  1.3125, -6.4375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0178,  0.0043, -0.0056,  ...,  0.0078,  0.0115,  0.0044],
        [-0.0022,  0.0070, -0.0027,  ..., -0.0037, -0.0040,  0.0090],
        [-0.0047, -0.0067,  0.0806,  ...,  0.0017,  0.0003, -0.0029],
        ...,
        [ 0.0025,  0.0042,  0.0067,  ...,  0.0100,  0.0153,  0.0022],
        [ 0.0103, -0.0008,  0.0023,  ...,  0.0076,  0.0250,  0.0028],
        [-0.0080, -0.0013, -0.0029,  ..., -0.0070, -0.0142,  0.0136]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.8086, 12.5938,  0.9492,  ...,  7.5117,  2.8555, -6.4023]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 20:34:06 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A money is made up of paper
A house is made up of bricks
A beach is made up of sand
A boots is made up of leather
A cocktail is made up of alcohol
A desk is made up of wood
A lens is made up of glass
A plastic is made up of
2024-07-24 20:34:06 root INFO     [order_1_approx] starting weight calculation for A plastic is made up of polymer
A lens is made up of glass
A house is made up of bricks
A money is made up of paper
A boots is made up of leather
A desk is made up of wood
A cocktail is made up of alcohol
A beach is made up of
2024-07-24 20:34:06 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 20:36:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.5117, 0.2578, 5.4062,  ..., 2.7344, 1.3438, 2.3906], device='cuda:0',
       dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-9.7500, 11.2500, 16.7500,  ...,  2.9375, -1.8281, 17.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 5.9509e-04, -1.5030e-03,  3.2501e-03,  ...,  2.1935e-04,
          6.3324e-04, -9.7752e-06],
        [-1.9684e-03,  4.7913e-03, -6.0730e-03,  ..., -1.7395e-03,
          5.2795e-03,  2.1820e-03],
        [ 2.2888e-03,  1.8005e-03,  5.0537e-02,  ...,  8.5449e-04,
         -3.2043e-04,  1.1444e-04],
        ...,
        [-4.5204e-04,  6.6376e-04, -3.8452e-03,  ...,  1.7357e-04,
         -6.8970e-03,  6.1951e-03],
        [ 4.9591e-04, -3.0823e-03,  8.7280e-03,  ...,  4.5395e-04,
          1.1108e-02,  3.9673e-03],
        [-8.7738e-04, -2.3041e-03,  2.4902e-02,  ...,  3.7537e-03,
         -2.4261e-03,  4.3030e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-9.4297, 11.1094, 16.9062,  ...,  3.0742, -1.0781, 17.4375]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 20:36:13 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The country with islamabad as its capital is known as pakistan
The country with ankara as its capital is known as turkey
The country with bern as its capital is known as switzerland
The country with stockholm as its capital is known as sweden
The country with brussels as its capital is known as belgium
The country with rome as its capital is known as italy
The country with beirut as its capital is known as lebanon
The country with zagreb as its capital is known as
2024-07-24 20:36:14 root INFO     [order_1_approx] starting weight calculation for The country with ankara as its capital is known as turkey
The country with bern as its capital is known as switzerland
The country with stockholm as its capital is known as sweden
The country with islamabad as its capital is known as pakistan
The country with brussels as its capital is known as belgium
The country with beirut as its capital is known as lebanon
The country with zagreb as its capital is known as croatia
The country with rome as its capital is known as
2024-07-24 20:36:14 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 20:39:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.6250, -0.3887,  2.9531,  ..., -1.0391, -6.3438,  0.8672],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 2.7812, -1.8828, 11.7500,  ..., -5.7812, -6.9375, -3.9375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0072,  0.0025, -0.0107,  ..., -0.0017,  0.0042,  0.0045],
        [ 0.0096,  0.0015,  0.0194,  ..., -0.0006,  0.0038,  0.0013],
        [-0.0036, -0.0041,  0.0674,  ..., -0.0019, -0.0009, -0.0077],
        ...,
        [ 0.0007,  0.0013,  0.0061,  ...,  0.0032,  0.0027,  0.0006],
        [ 0.0026, -0.0024,  0.0047,  ...,  0.0002,  0.0120,  0.0064],
        [-0.0022,  0.0017, -0.0168,  ..., -0.0008, -0.0043,  0.0176]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.5332, -2.0000, 12.2812,  ..., -4.9688, -6.9766, -3.4355]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 20:39:54 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A plastic is made up of polymer
A lens is made up of glass
A house is made up of bricks
A money is made up of paper
A boots is made up of leather
A desk is made up of wood
A cocktail is made up of alcohol
A beach is made up of
2024-07-24 20:39:55 root INFO     [order_1_approx] starting weight calculation for A desk is made up of wood
A money is made up of paper
A beach is made up of sand
A lens is made up of glass
A boots is made up of leather
A plastic is made up of polymer
A cocktail is made up of alcohol
A house is made up of
2024-07-24 20:39:55 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 20:42:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-3.2344, -1.4531,  9.5625,  ...,  1.0859, -0.0352,  0.7539],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-7.1875, -0.8594, 29.5000,  ..., -8.4375,  0.4453, 12.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 8.6670e-03, -1.4038e-03, -7.2021e-03,  ..., -3.2043e-04,
          9.2773e-03,  1.6785e-03],
        [ 3.9062e-03, -1.6327e-03, -9.3994e-03,  ...,  3.2959e-03,
          7.7820e-03, -7.1716e-04],
        [-4.7913e-03, -4.3640e-03,  4.8096e-02,  ...,  4.7913e-03,
          8.3008e-03,  1.5869e-03],
        ...,
        [-1.8387e-03, -5.4550e-04,  1.5640e-04,  ...,  6.6757e-04,
          7.0572e-05, -8.7738e-04],
        [-2.6703e-04, -1.3733e-03, -7.8125e-03,  ..., -1.6556e-03,
          1.4099e-02,  6.4087e-03],
        [-2.9907e-03, -2.1362e-03,  1.1292e-02,  ...,  3.4485e-03,
          2.2125e-04,  7.2021e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-6.6484, -0.8203, 29.9844,  ..., -8.4922, -0.7012, 14.4219]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 20:42:06 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The country with ankara as its capital is known as turkey
The country with bern as its capital is known as switzerland
The country with stockholm as its capital is known as sweden
The country with islamabad as its capital is known as pakistan
The country with brussels as its capital is known as belgium
The country with beirut as its capital is known as lebanon
The country with zagreb as its capital is known as croatia
The country with rome as its capital is known as
2024-07-24 20:42:06 root INFO     [order_1_approx] starting weight calculation for The country with beirut as its capital is known as lebanon
The country with islamabad as its capital is known as pakistan
The country with rome as its capital is known as italy
The country with bern as its capital is known as switzerland
The country with zagreb as its capital is known as croatia
The country with brussels as its capital is known as belgium
The country with ankara as its capital is known as turkey
The country with stockholm as its capital is known as
2024-07-24 20:42:07 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 20:45:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0469, -1.5234,  2.3750,  ...,  0.7656, -1.7891,  0.3047],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([14.2500,  7.9688, -0.2812,  ...,  3.2812, -2.2500, -3.9531],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0039, -0.0016,  0.0006,  ..., -0.0074, -0.0078, -0.0062],
        [ 0.0014,  0.0081,  0.0084,  ...,  0.0022, -0.0006,  0.0019],
        [ 0.0016, -0.0038,  0.0591,  ..., -0.0008, -0.0043, -0.0061],
        ...,
        [-0.0038,  0.0020,  0.0062,  ...,  0.0031, -0.0007, -0.0034],
        [-0.0032, -0.0056,  0.0162,  ..., -0.0058,  0.0083, -0.0063],
        [-0.0042, -0.0049, -0.0082,  ...,  0.0005,  0.0042,  0.0148]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[12.8359,  7.7383, -0.5898,  ...,  5.0820, -1.9570, -3.3262]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 20:45:44 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A desk is made up of wood
A money is made up of paper
A beach is made up of sand
A lens is made up of glass
A boots is made up of leather
A plastic is made up of polymer
A cocktail is made up of alcohol
A house is made up of
2024-07-24 20:45:45 root INFO     [order_1_approx] starting weight calculation for A cocktail is made up of alcohol
A house is made up of bricks
A plastic is made up of polymer
A money is made up of paper
A beach is made up of sand
A desk is made up of wood
A boots is made up of leather
A lens is made up of
2024-07-24 20:45:45 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 20:48:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9180, -0.6602,  5.4062,  ...,  0.6797, -3.9062,  2.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-6.5625,  6.9062, 15.7500,  ..., -5.7812, -1.3594,  2.5469],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.4877e-03, -1.4648e-03,  7.3242e-03,  ...,  4.0283e-03,
         -3.2043e-03,  4.8218e-03],
        [-4.3335e-03, -1.0071e-03,  8.1787e-03,  ..., -5.5237e-03,
          6.1340e-03, -3.5858e-03],
        [-5.3711e-03, -3.2196e-03,  4.3945e-02,  ..., -7.2632e-03,
         -8.2397e-04,  1.7395e-03],
        ...,
        [ 1.4954e-03,  1.1110e-04,  1.1230e-02,  ...,  1.1139e-03,
         -7.6599e-03, -1.7471e-03],
        [ 1.4648e-03, -3.5667e-04,  5.4359e-05,  ..., -4.0436e-04,
          1.2573e-02,  1.1063e-03],
        [-5.5542e-03, -5.5847e-03,  6.4087e-03,  ...,  3.1586e-03,
         -1.3504e-03,  5.6152e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-9.2500,  9.1797, 19.6094,  ..., -5.7344, -1.7969,  3.3145]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 20:48:14 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The country with beirut as its capital is known as lebanon
The country with islamabad as its capital is known as pakistan
The country with rome as its capital is known as italy
The country with bern as its capital is known as switzerland
The country with zagreb as its capital is known as croatia
The country with brussels as its capital is known as belgium
The country with ankara as its capital is known as turkey
The country with stockholm as its capital is known as
2024-07-24 20:48:14 root INFO     [order_1_approx] starting weight calculation for The country with stockholm as its capital is known as sweden
The country with rome as its capital is known as italy
The country with zagreb as its capital is known as croatia
The country with beirut as its capital is known as lebanon
The country with islamabad as its capital is known as pakistan
The country with ankara as its capital is known as turkey
The country with brussels as its capital is known as belgium
The country with bern as its capital is known as
2024-07-24 20:48:14 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 20:51:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.0938, -2.0938,  0.0142,  ...,  0.3652, -1.2031, -1.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -4.1250,   9.5625,   1.4062,  ..., -11.0000,  -0.9688,  -0.9531],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-0.0036, -0.0060, -0.0050,  ...,  0.0084,  0.0026, -0.0008],
        [ 0.0041,  0.0030, -0.0015,  ...,  0.0017, -0.0030,  0.0065],
        [ 0.0033, -0.0084,  0.1094,  ..., -0.0020, -0.0035, -0.0070],
        ...,
        [ 0.0072, -0.0025,  0.0294,  ...,  0.0049,  0.0035,  0.0013],
        [ 0.0053,  0.0035,  0.0104,  ...,  0.0046,  0.0127,  0.0025],
        [-0.0045,  0.0024, -0.0244,  ..., -0.0031, -0.0036,  0.0302]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.8945,  8.5547,  0.5552,  ..., -9.2109, -1.9307, -1.8047]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 20:51:32 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A cocktail is made up of alcohol
A house is made up of bricks
A plastic is made up of polymer
A money is made up of paper
A beach is made up of sand
A desk is made up of wood
A boots is made up of leather
A lens is made up of
2024-07-24 20:51:32 root INFO     [order_1_approx] starting weight calculation for A cocktail is made up of alcohol
A desk is made up of wood
A beach is made up of sand
A lens is made up of glass
A house is made up of bricks
A money is made up of paper
A plastic is made up of polymer
A boots is made up of
2024-07-24 20:51:32 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 20:54:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.8750, -0.4961, 11.1875,  ...,  1.5469, -0.7422,  1.6328],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -7.7500,   5.0312,  29.6250,  ..., -10.5625,  -0.2188,  10.9375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 5.5542e-03,  9.1934e-04, -8.8501e-03,  ...,  6.8665e-04,
          9.5825e-03, -2.1057e-03],
        [ 2.5558e-04,  9.6512e-04,  1.0376e-02,  ..., -4.0054e-04,
         -2.6550e-03,  3.8910e-04],
        [ 2.5024e-03, -1.0254e-02,  7.9590e-02,  ...,  3.7766e-04,
         -9.9487e-03,  5.0049e-03],
        ...,
        [-1.7929e-03, -3.0518e-05, -2.1057e-03,  ...,  8.6975e-04,
         -5.1575e-03, -5.8594e-03],
        [-6.1035e-03, -2.9602e-03, -1.8311e-02,  ...,  1.1826e-03,
          8.4229e-03,  4.3335e-03],
        [ 2.2736e-03, -7.9956e-03,  2.5757e-02,  ...,  1.2665e-03,
         -8.6594e-04,  6.5918e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-9.0781e+00,  5.9727e+00,  3.1031e+01,  ..., -1.1109e+01,
          2.3804e-02,  1.1273e+01]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SubBackward0>) 

                    
2024-07-24 20:54:17 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The country with stockholm as its capital is known as sweden
The country with rome as its capital is known as italy
The country with zagreb as its capital is known as croatia
The country with beirut as its capital is known as lebanon
The country with islamabad as its capital is known as pakistan
The country with ankara as its capital is known as turkey
The country with brussels as its capital is known as belgium
The country with bern as its capital is known as
2024-07-24 20:54:18 root INFO     [order_1_approx] starting weight calculation for The country with brussels as its capital is known as belgium
The country with rome as its capital is known as italy
The country with zagreb as its capital is known as croatia
The country with bern as its capital is known as switzerland
The country with ankara as its capital is known as turkey
The country with beirut as its capital is known as lebanon
The country with stockholm as its capital is known as sweden
The country with islamabad as its capital is known as
2024-07-24 20:54:18 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 20:57:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9766,  0.4961,  2.9219,  ...,  0.5820, -2.0781, -0.7070],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 2.5312,  4.3438,  4.4375,  ..., -3.4375, -5.2812, -5.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 8.3008e-03,  5.7983e-03,  2.2339e-02,  ..., -1.7929e-04,
         -2.5177e-03, -7.4768e-03],
        [ 8.2397e-03,  7.9346e-03,  1.2329e-02,  ...,  9.5367e-05,
          4.5166e-03,  1.2024e-02],
        [-1.4954e-02, -1.9150e-03,  1.0010e-01,  ...,  2.9907e-03,
          9.4604e-03, -3.6621e-03],
        ...,
        [-9.4604e-03, -1.8406e-04,  7.9956e-03,  ...,  3.5858e-04,
         -6.7444e-03, -4.6997e-03],
        [-2.0599e-03,  1.9684e-03,  9.2773e-03,  ...,  3.0823e-03,
          1.9653e-02, -3.3264e-03],
        [-5.1270e-03, -9.1553e-05, -2.0630e-02,  ...,  1.0071e-03,
          1.0132e-02,  3.1128e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.7676,  4.0977,  3.2852,  ..., -3.4219, -4.4922, -5.1523]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 20:57:22 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A cocktail is made up of alcohol
A desk is made up of wood
A beach is made up of sand
A lens is made up of glass
A house is made up of bricks
A money is made up of paper
A plastic is made up of polymer
A boots is made up of
2024-07-24 20:57:22 root INFO     [order_1_approx] starting weight calculation for A lens is made up of glass
A boots is made up of leather
A money is made up of paper
A house is made up of bricks
A desk is made up of wood
A beach is made up of sand
A plastic is made up of polymer
A cocktail is made up of
2024-07-24 20:57:22 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 21:00:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-3.7500, -0.9570,  4.4375,  ...,  0.3672,  2.1094,  3.3594],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-3.9688,  1.5625, 11.1250,  ..., -2.5938,  8.8750, 11.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-2.7847e-04, -4.9438e-03,  3.3417e-03,  ...,  6.6376e-04,
         -2.0752e-03,  4.9438e-03],
        [-1.1292e-03,  4.9133e-03, -1.2207e-03,  ..., -1.8768e-03,
          1.6113e-02, -8.7280e-03],
        [ 7.5531e-04,  1.1780e-02,  8.2031e-02,  ..., -4.4250e-03,
          1.5564e-02, -1.4343e-02],
        ...,
        [-1.9379e-03,  6.4468e-04,  1.0071e-03,  ..., -1.9226e-03,
         -2.7466e-03,  3.7384e-04],
        [-1.3123e-03, -1.8311e-04, -7.9346e-04,  ...,  6.0425e-03,
          5.9814e-03, -1.2054e-03],
        [-6.8283e-04, -9.1553e-05,  2.1973e-02,  ...,  2.9297e-03,
          9.4604e-03,  6.7902e-04]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.0820,  3.6934, 17.1094,  ..., -2.2090, 10.8750, 13.5859]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 21:00:17 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The country with brussels as its capital is known as belgium
The country with rome as its capital is known as italy
The country with zagreb as its capital is known as croatia
The country with bern as its capital is known as switzerland
The country with ankara as its capital is known as turkey
The country with beirut as its capital is known as lebanon
The country with stockholm as its capital is known as sweden
The country with islamabad as its capital is known as
2024-07-24 21:00:17 root INFO     [order_1_approx] starting weight calculation for The country with islamabad as its capital is known as pakistan
The country with zagreb as its capital is known as croatia
The country with rome as its capital is known as italy
The country with ankara as its capital is known as turkey
The country with beirut as its capital is known as lebanon
The country with stockholm as its capital is known as sweden
The country with bern as its capital is known as switzerland
The country with brussels as its capital is known as
2024-07-24 21:00:17 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 21:02:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 3.0625,  1.3359, -0.5469,  ..., -1.0156, -1.8047,  2.6406],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 8.0000,  1.8281, -9.0000,  ...,  9.3125,  5.0625, -3.7188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 8.9111e-03,  2.8992e-03,  9.5215e-03,  ..., -2.1210e-03,
         -6.1035e-03,  1.1520e-03],
        [-1.0986e-03, -2.7924e-03, -1.1108e-02,  ..., -2.5482e-03,
         -3.5706e-03,  6.4392e-03],
        [-9.5215e-03, -1.9531e-03,  9.5703e-02,  ...,  4.3945e-03,
         -8.3008e-03, -1.9550e-05],
        ...,
        [ 6.4087e-03,  4.7913e-03,  4.7302e-03,  ...,  5.4932e-03,
         -5.8594e-03, -6.1951e-03],
        [ 2.5787e-03, -1.4038e-03,  4.3945e-03,  ...,  8.1787e-03,
          2.4170e-02,  8.4839e-03],
        [ 2.2125e-03,  1.4267e-03, -1.8921e-02,  ..., -6.7444e-03,
          1.4771e-02,  9.1553e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 8.4844,  2.0879, -8.8672,  ...,  9.3750,  4.1172, -2.9512]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 21:02:58 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A lens is made up of glass
A boots is made up of leather
A money is made up of paper
A house is made up of bricks
A desk is made up of wood
A beach is made up of sand
A plastic is made up of polymer
A cocktail is made up of
2024-07-24 21:02:59 root INFO     [order_1_approx] starting weight calculation for A beach is made up of sand
A boots is made up of leather
A lens is made up of glass
A house is made up of bricks
A plastic is made up of polymer
A money is made up of paper
A cocktail is made up of alcohol
A desk is made up of
2024-07-24 21:02:59 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 21:06:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.6719, -0.7891,  5.9688,  ..., -0.3633, -1.8281,  2.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-2.3125, -2.8906, 21.0000,  ..., -7.1875, -9.9375, 12.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.1362e-03, -4.7684e-05,  7.2021e-03,  ..., -2.4872e-03,
          2.7008e-03, -1.8082e-03],
        [-4.1962e-05, -1.8768e-03,  1.6861e-03,  ..., -3.0212e-03,
         -5.9509e-04, -2.1057e-03],
        [-2.0447e-03, -2.1210e-03,  2.9907e-02,  ..., -4.8447e-04,
          5.5847e-03, -1.1215e-03],
        ...,
        [ 2.1210e-03,  1.7548e-03,  6.9427e-04,  ..., -1.6708e-03,
         -4.2419e-03,  1.6632e-03],
        [ 6.2561e-04, -2.1648e-04, -1.0498e-02,  ..., -6.3324e-04,
          1.5381e-02,  1.9684e-03],
        [-1.7395e-03, -1.4954e-03,  1.1108e-02,  ...,  2.9144e-03,
          5.9814e-03,  3.0670e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.4043, -2.8730, 20.7188,  ..., -7.6562, -9.6719, 12.4609]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 21:06:08 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The country with islamabad as its capital is known as pakistan
The country with zagreb as its capital is known as croatia
The country with rome as its capital is known as italy
The country with ankara as its capital is known as turkey
The country with beirut as its capital is known as lebanon
The country with stockholm as its capital is known as sweden
The country with bern as its capital is known as switzerland
The country with brussels as its capital is known as
2024-07-24 21:06:08 root INFO     [order_1_approx] starting weight calculation for The country with bern as its capital is known as switzerland
The country with brussels as its capital is known as belgium
The country with stockholm as its capital is known as sweden
The country with beirut as its capital is known as lebanon
The country with zagreb as its capital is known as croatia
The country with islamabad as its capital is known as pakistan
The country with rome as its capital is known as italy
The country with ankara as its capital is known as
2024-07-24 21:06:09 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 21:08:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 3.7500, -0.8008,  0.5664,  ...,  1.4688, -4.0000, -1.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 5.4062,  9.5625,  4.9375,  ...,  9.5000, -3.1875, -9.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0012,  0.0057, -0.0045,  ...,  0.0055,  0.0075, -0.0106],
        [ 0.0069,  0.0138, -0.0233,  ...,  0.0032,  0.0081,  0.0033],
        [-0.0076, -0.0055,  0.0952,  ..., -0.0114, -0.0131,  0.0153],
        ...,
        [-0.0024, -0.0049,  0.0070,  ..., -0.0008, -0.0085,  0.0103],
        [ 0.0108, -0.0055,  0.0091,  ...,  0.0008,  0.0175, -0.0128],
        [-0.0034, -0.0041,  0.0045,  ...,  0.0032, -0.0030,  0.0284]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.2461,  8.6484,  6.4258,  ..., 11.2422, -3.1445, -8.4688]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 21:08:39 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A beach is made up of sand
A boots is made up of leather
A lens is made up of glass
A house is made up of bricks
A plastic is made up of polymer
A money is made up of paper
A cocktail is made up of alcohol
A desk is made up of
2024-07-24 21:08:39 root INFO     total operator prediction time: 2760.464331626892 seconds
2024-07-24 21:08:39 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on synonyms - intensity
2024-07-24 21:08:39 root INFO     building operator synonyms - intensity
2024-07-24 21:08:39 root INFO     [order_1_approx] starting weight calculation for A more intense word for dislike is hate
A more intense word for dinner is feast
A more intense word for like is love
A more intense word for monkey is gorilla
A more intense word for excited is agitated
A more intense word for opposed is averse
A more intense word for sea is ocean
A more intense word for ask is
2024-07-24 21:08:40 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 21:12:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.3438,  2.1094,  4.8750,  ...,  2.8281, -0.0898,  0.3945],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -9.7500,   7.9062,  13.5625,  ..., -12.5625,  -1.5938,  -3.2344],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-3.5706e-03, -2.6093e-03, -5.0964e-03,  ..., -4.7684e-05,
          6.7749e-03,  6.9580e-03],
        [-1.6708e-03,  1.6785e-03,  1.6357e-02,  ...,  1.8768e-03,
          7.1716e-03, -4.2343e-04],
        [ 4.2725e-04, -2.5024e-03,  7.6660e-02,  ..., -3.4485e-03,
         -5.2185e-03, -3.9673e-03],
        ...,
        [-8.7738e-04,  1.1749e-03, -8.3618e-03,  ...,  1.7776e-03,
         -3.1853e-04,  2.1820e-03],
        [-3.0060e-03,  1.7548e-03, -1.4038e-02,  ...,  4.4556e-03,
          7.4463e-03,  7.5378e-03],
        [ 2.3651e-03, -4.8218e-03,  8.9722e-03,  ...,  3.9062e-03,
         -2.2278e-03,  1.1826e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-11.0469,   8.9688,  16.2812,  ..., -11.8828,  -0.6416,  -3.2676]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 21:12:02 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The country with bern as its capital is known as switzerland
The country with brussels as its capital is known as belgium
The country with stockholm as its capital is known as sweden
The country with beirut as its capital is known as lebanon
The country with zagreb as its capital is known as croatia
The country with islamabad as its capital is known as pakistan
The country with rome as its capital is known as italy
The country with ankara as its capital is known as
2024-07-24 21:12:02 root INFO     total operator prediction time: 2861.003381252289 seconds
2024-07-24 21:12:02 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on name - occupation
2024-07-24 21:12:02 root INFO     building operator name - occupation
2024-07-24 21:12:03 root INFO     [order_1_approx] starting weight calculation for hegel was known for their work as a  philosopher
marx was known for their work as a  philosopher
hitler was known for their work as a  dictator
euler was known for their work as a  mathematician
schwarzenegger was known for their work as a  actor
confucius was known for their work as a  philosopher
caesar was known for their work as a  emperor
newton was known for their work as a 
2024-07-24 21:12:03 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 21:14:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.7734, 2.0781, 1.6250,  ..., 0.9141, 0.1562, 0.5508], device='cuda:0',
       dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  3.6406,   4.5625,  -0.0938,  ..., -13.4375,  -4.9375,   1.9766],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.4832e-02, -6.8665e-03, -1.5503e-02,  ...,  4.8218e-03,
          9.2316e-04,  6.6528e-03],
        [ 1.1719e-02,  8.1177e-03,  1.7334e-02,  ..., -1.5015e-02,
         -1.1749e-03,  1.0223e-03],
        [ 7.4005e-04, -1.6861e-03,  6.2256e-02,  ...,  7.6294e-06,
         -3.0518e-04, -9.1553e-03],
        ...,
        [ 3.0670e-03,  5.2185e-03, -1.1841e-02,  ...,  6.4392e-03,
         -1.1108e-02,  6.8665e-03],
        [ 6.5308e-03,  5.6076e-04,  6.0120e-03,  ..., -6.4087e-03,
          2.3438e-02, -2.9602e-03],
        [ 2.9755e-03, -3.0212e-03,  2.1973e-03,  ...,  6.5918e-03,
         -6.1035e-03,  1.8066e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  3.5488,   2.8477,   0.9014,  ..., -14.0469,  -5.5938,   1.5762]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 21:14:17 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for dislike is hate
A more intense word for dinner is feast
A more intense word for like is love
A more intense word for monkey is gorilla
A more intense word for excited is agitated
A more intense word for opposed is averse
A more intense word for sea is ocean
A more intense word for ask is
2024-07-24 21:14:17 root INFO     [order_1_approx] starting weight calculation for A more intense word for ask is beg
A more intense word for opposed is averse
A more intense word for monkey is gorilla
A more intense word for sea is ocean
A more intense word for excited is agitated
A more intense word for like is love
A more intense word for dinner is feast
A more intense word for dislike is
2024-07-24 21:14:17 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 21:17:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5859, -0.8047,  4.9062,  ...,  3.1875, -0.2266, -0.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-15.3125,  26.8750,   2.7031,  ...,  -1.9453,  -1.8281,  15.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0028, -0.0025, -0.0201,  ..., -0.0014,  0.0052,  0.0020],
        [ 0.0078,  0.0024,  0.0072,  ..., -0.0017, -0.0062,  0.0057],
        [ 0.0005, -0.0026,  0.0649,  ..., -0.0021,  0.0014, -0.0034],
        ...,
        [-0.0002, -0.0011,  0.0013,  ...,  0.0012, -0.0026,  0.0067],
        [ 0.0071,  0.0006,  0.0126,  ..., -0.0034,  0.0093,  0.0072],
        [ 0.0059,  0.0002,  0.0017,  ...,  0.0030,  0.0009,  0.0107]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-16.0312,  26.2656,   2.2578,  ...,  -3.0703,  -2.0352,  16.9531]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 21:17:50 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for hegel was known for their work as a  philosopher
marx was known for their work as a  philosopher
hitler was known for their work as a  dictator
euler was known for their work as a  mathematician
schwarzenegger was known for their work as a  actor
confucius was known for their work as a  philosopher
caesar was known for their work as a  emperor
newton was known for their work as a 
2024-07-24 21:17:50 root INFO     [order_1_approx] starting weight calculation for confucius was known for their work as a  philosopher
newton was known for their work as a  scientist
schwarzenegger was known for their work as a  actor
marx was known for their work as a  philosopher
euler was known for their work as a  mathematician
caesar was known for their work as a  emperor
hitler was known for their work as a  dictator
hegel was known for their work as a 
2024-07-24 21:17:51 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 21:20:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4453,  0.4492, -0.7773,  ...,  0.3672, -0.3750,  1.5234],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -3.5156,  -3.7812,   3.8438,  ...,  -5.8125,   0.4844, -13.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0018, -0.0047,  0.0057,  ...,  0.0057, -0.0015,  0.0038],
        [ 0.0016,  0.0074,  0.0071,  ..., -0.0049,  0.0042, -0.0007],
        [ 0.0017, -0.0081,  0.0405,  ..., -0.0072, -0.0176,  0.0045],
        ...,
        [-0.0039,  0.0069,  0.0070,  ...,  0.0050,  0.0098, -0.0107],
        [-0.0066, -0.0029,  0.0214,  ..., -0.0043,  0.0107,  0.0026],
        [-0.0019, -0.0020, -0.0085,  ...,  0.0045, -0.0013, -0.0011]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -2.5586,  -2.6875,   4.3594,  ...,  -7.1328,  -0.1914, -11.9609]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 21:20:06 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for ask is beg
A more intense word for opposed is averse
A more intense word for monkey is gorilla
A more intense word for sea is ocean
A more intense word for excited is agitated
A more intense word for like is love
A more intense word for dinner is feast
A more intense word for dislike is
2024-07-24 21:20:06 root INFO     [order_1_approx] starting weight calculation for A more intense word for ask is beg
A more intense word for monkey is gorilla
A more intense word for dislike is hate
A more intense word for dinner is feast
A more intense word for sea is ocean
A more intense word for like is love
A more intense word for excited is agitated
A more intense word for opposed is
2024-07-24 21:20:06 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 21:23:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-3.6875, -2.1094,  6.6875,  ...,  2.2500, -1.0312,  0.2891],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-16.0000,  35.5000,   1.4688,  ...,  -1.1875,   4.0625,  15.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0022, -0.0034, -0.0102,  ..., -0.0042,  0.0065,  0.0008],
        [ 0.0080,  0.0055,  0.0070,  ...,  0.0069, -0.0062,  0.0033],
        [ 0.0012, -0.0034,  0.0540,  ...,  0.0023, -0.0019,  0.0042],
        ...,
        [-0.0022,  0.0032,  0.0009,  ..., -0.0008,  0.0036,  0.0014],
        [ 0.0014,  0.0026, -0.0038,  ..., -0.0010,  0.0159, -0.0026],
        [ 0.0021, -0.0021,  0.0108,  ...,  0.0054,  0.0011,  0.0193]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-16.5000,  35.1562,   2.0625,  ...,  -0.5630,   3.0879,  16.1406]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 21:23:43 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for confucius was known for their work as a  philosopher
newton was known for their work as a  scientist
schwarzenegger was known for their work as a  actor
marx was known for their work as a  philosopher
euler was known for their work as a  mathematician
caesar was known for their work as a  emperor
hitler was known for their work as a  dictator
hegel was known for their work as a 
2024-07-24 21:23:44 root INFO     [order_1_approx] starting weight calculation for newton was known for their work as a  scientist
marx was known for their work as a  philosopher
caesar was known for their work as a  emperor
schwarzenegger was known for their work as a  actor
hitler was known for their work as a  dictator
euler was known for their work as a  mathematician
hegel was known for their work as a  philosopher
confucius was known for their work as a 
2024-07-24 21:23:44 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 21:25:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5234, -1.7344,  1.1875,  ...,  0.7109, -3.2188,  1.1016],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-11.1250,   0.6484,   7.0625,  ...,  -2.8594,  -4.7500, -14.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0120,  0.0048,  0.0151,  ...,  0.0026,  0.0023, -0.0078],
        [ 0.0070, -0.0003,  0.0283,  ...,  0.0041, -0.0046,  0.0048],
        [-0.0059,  0.0013,  0.0859,  ...,  0.0103, -0.0057, -0.0073],
        ...,
        [-0.0050, -0.0024, -0.0076,  ..., -0.0015, -0.0090, -0.0008],
        [ 0.0280,  0.0256,  0.0378,  ..., -0.0141,  0.0253,  0.0038],
        [ 0.0096, -0.0005,  0.0221,  ..., -0.0065,  0.0054,  0.0123]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -8.7969,   1.2988,   7.2773,  ...,  -3.5762,  -4.0781, -13.4375]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 21:25:53 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for ask is beg
A more intense word for monkey is gorilla
A more intense word for dislike is hate
A more intense word for dinner is feast
A more intense word for sea is ocean
A more intense word for like is love
A more intense word for excited is agitated
A more intense word for opposed is
2024-07-24 21:25:53 root INFO     [order_1_approx] starting weight calculation for A more intense word for dislike is hate
A more intense word for dinner is feast
A more intense word for ask is beg
A more intense word for monkey is gorilla
A more intense word for opposed is averse
A more intense word for sea is ocean
A more intense word for excited is agitated
A more intense word for like is
2024-07-24 21:25:53 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 21:29:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-3.9062, -1.7344, -0.9844,  ...,  2.7344, -1.9766, -3.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-16.5000,  34.5000,  -1.5000,  ...,  -4.0625,  -3.7969,   7.0938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0054, -0.0001, -0.0066,  ..., -0.0037,  0.0033,  0.0033],
        [-0.0013,  0.0037,  0.0045,  ..., -0.0022, -0.0036,  0.0003],
        [-0.0038,  0.0001,  0.0520,  ..., -0.0010, -0.0029, -0.0021],
        ...,
        [ 0.0046,  0.0026,  0.0040,  ...,  0.0055, -0.0029,  0.0055],
        [-0.0034, -0.0015, -0.0004,  ...,  0.0023,  0.0135,  0.0053],
        [ 0.0038, -0.0009,  0.0051,  ...,  0.0028,  0.0021,  0.0124]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-16.2188,  34.3750,  -1.2979,  ...,  -3.5312,  -2.6367,   7.7773]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 21:29:33 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for newton was known for their work as a  scientist
marx was known for their work as a  philosopher
caesar was known for their work as a  emperor
schwarzenegger was known for their work as a  actor
hitler was known for their work as a  dictator
euler was known for their work as a  mathematician
hegel was known for their work as a  philosopher
confucius was known for their work as a 
2024-07-24 21:29:33 root INFO     [order_1_approx] starting weight calculation for caesar was known for their work as a  emperor
euler was known for their work as a  mathematician
newton was known for their work as a  scientist
hegel was known for their work as a  philosopher
hitler was known for their work as a  dictator
schwarzenegger was known for their work as a  actor
confucius was known for their work as a  philosopher
marx was known for their work as a 
2024-07-24 21:29:33 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 21:31:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5859,  1.6953,  1.7891,  ...,  2.2812, -1.0078,  0.9805],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  2.3750,   0.7461,  12.8750,  ...,  -5.3750,  -1.3125, -13.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.9531e-02,  1.0620e-02,  2.8687e-03,  ...,  2.9907e-03,
         -1.2589e-03, -2.4033e-04],
        [ 6.8665e-03,  8.3618e-03,  8.5449e-03,  ..., -3.6163e-03,
         -1.2436e-03, -8.4839e-03],
        [ 5.6763e-03, -2.5482e-03,  8.0078e-02,  ...,  3.0823e-03,
         -1.1963e-02, -1.8005e-03],
        ...,
        [ 1.1353e-02,  1.0986e-02,  7.7820e-03,  ..., -1.3351e-05,
          4.4556e-03,  9.3384e-03],
        [ 6.9427e-04,  5.9814e-03, -1.7700e-03,  ...,  9.9182e-04,
          2.0508e-02, -9.8877e-03],
        [-5.4932e-03,  3.1662e-04, -3.4790e-03,  ...,  2.8992e-03,
         -1.0803e-02,  7.1106e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  0.6348,  -0.2217,  13.3906,  ...,  -6.0234,  -1.4609, -11.2422]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 21:31:41 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for dislike is hate
A more intense word for dinner is feast
A more intense word for ask is beg
A more intense word for monkey is gorilla
A more intense word for opposed is averse
A more intense word for sea is ocean
A more intense word for excited is agitated
A more intense word for like is
2024-07-24 21:31:41 root INFO     [order_1_approx] starting weight calculation for A more intense word for like is love
A more intense word for opposed is averse
A more intense word for dislike is hate
A more intense word for ask is beg
A more intense word for monkey is gorilla
A more intense word for excited is agitated
A more intense word for dinner is feast
A more intense word for sea is
2024-07-24 21:31:41 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 21:35:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.1875, -4.3750,  6.4688,  ...,  1.0312,  2.8125,  0.8984],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-13.6250,  35.0000,  -0.1250,  ...,  -4.3750,   2.3750,  17.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.6316e-03, -1.5335e-03, -1.5381e-02,  ...,  1.6975e-04,
         -3.3951e-04, -1.1597e-03],
        [ 4.6997e-03,  3.3569e-03,  2.1267e-04,  ...,  1.5793e-03,
         -6.2943e-04,  1.6632e-03],
        [ 1.6022e-03,  1.4343e-03,  5.5420e-02,  ...,  8.0490e-04,
         -1.4877e-03,  2.3270e-04],
        ...,
        [-3.8147e-04,  1.7395e-03,  1.0193e-02,  ...,  6.0081e-05,
          1.1368e-03,  5.1880e-03],
        [ 1.8234e-03,  4.4250e-03,  9.6436e-03,  ..., -5.2185e-03,
          1.5137e-02,  6.3705e-04],
        [ 6.6757e-05, -8.6212e-04,  1.1841e-02,  ...,  4.3678e-04,
         -3.2959e-03,  1.6479e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-12.8516,  34.5938,   0.0753,  ...,  -4.6445,   1.6084,  18.0312]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 21:35:28 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for caesar was known for their work as a  emperor
euler was known for their work as a  mathematician
newton was known for their work as a  scientist
hegel was known for their work as a  philosopher
hitler was known for their work as a  dictator
schwarzenegger was known for their work as a  actor
confucius was known for their work as a  philosopher
marx was known for their work as a 
2024-07-24 21:35:29 root INFO     [order_1_approx] starting weight calculation for newton was known for their work as a  scientist
hegel was known for their work as a  philosopher
marx was known for their work as a  philosopher
caesar was known for their work as a  emperor
hitler was known for their work as a  dictator
euler was known for their work as a  mathematician
confucius was known for their work as a  philosopher
schwarzenegger was known for their work as a 
2024-07-24 21:35:29 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 21:37:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1562, -0.5391,  1.3906,  ...,  0.5234, -1.3281,  0.1172],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 9.5625,  4.4688,  0.1719,  ...,  4.6250, -1.7969,  2.4844],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 6.0425e-03, -1.1826e-04, -6.2256e-03,  ...,  1.0376e-03,
          4.8218e-03, -3.7384e-03],
        [ 3.2959e-03,  3.9673e-03, -1.1169e-02,  ..., -4.7913e-03,
         -2.3651e-03, -2.4796e-05],
        [-1.1520e-03,  1.3275e-03,  7.8613e-02,  ...,  2.7008e-03,
         -8.3008e-03, -5.9509e-03],
        ...,
        [ 8.3008e-03,  3.4332e-05, -1.3916e-02,  ...,  1.0529e-03,
         -3.8757e-03,  1.5320e-02],
        [ 5.6458e-04,  7.6294e-03, -3.2654e-03,  ..., -5.5542e-03,
          1.3062e-02,  8.0566e-03],
        [ 7.3547e-03,  1.3504e-03, -8.0566e-03,  ...,  1.8921e-03,
         -1.3199e-03,  1.9531e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 9.0938,  4.1133,  0.2915,  ...,  5.1641, -2.5586,  2.5176]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 21:37:13 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for like is love
A more intense word for opposed is averse
A more intense word for dislike is hate
A more intense word for ask is beg
A more intense word for monkey is gorilla
A more intense word for excited is agitated
A more intense word for dinner is feast
A more intense word for sea is
2024-07-24 21:37:13 root INFO     [order_1_approx] starting weight calculation for A more intense word for monkey is gorilla
A more intense word for like is love
A more intense word for dinner is feast
A more intense word for ask is beg
A more intense word for sea is ocean
A more intense word for dislike is hate
A more intense word for opposed is averse
A more intense word for excited is
2024-07-24 21:37:14 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 21:41:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7031,  0.3047,  6.2188,  ..., -0.3887, -0.9844,  2.3906],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-17.0000,  29.0000,  -3.7188,  ...,  -3.2969,  -7.9375,  19.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.0498e-02, -1.0777e-04, -7.4158e-03,  ..., -2.1515e-03,
         -2.3804e-03, -8.6670e-03],
        [-1.7853e-03,  5.1575e-03,  3.6469e-03,  ..., -2.5635e-03,
          3.3112e-03, -4.8218e-03],
        [-1.8082e-03,  3.2196e-03,  3.4912e-02,  ...,  8.9645e-05,
          1.3657e-03, -4.7607e-03],
        ...,
        [ 5.7983e-03, -5.4016e-03,  3.6469e-03,  ..., -4.7874e-04,
         -6.9427e-04, -3.7384e-03],
        [-1.2207e-03, -1.7166e-03,  7.9956e-03,  ...,  7.3624e-04,
          1.4893e-02, -1.3275e-03],
        [ 3.0670e-03, -3.0975e-03, -2.3651e-03,  ...,  9.1553e-05,
          7.9346e-03,  2.0386e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-17.0000,  29.3281,  -3.6035,  ...,  -2.1562,  -7.1289,  20.2969]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 21:41:16 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for newton was known for their work as a  scientist
hegel was known for their work as a  philosopher
marx was known for their work as a  philosopher
caesar was known for their work as a  emperor
hitler was known for their work as a  dictator
euler was known for their work as a  mathematician
confucius was known for their work as a  philosopher
schwarzenegger was known for their work as a 
2024-07-24 21:41:16 root INFO     [order_1_approx] starting weight calculation for schwarzenegger was known for their work as a  actor
caesar was known for their work as a  emperor
confucius was known for their work as a  philosopher
marx was known for their work as a  philosopher
hegel was known for their work as a  philosopher
hitler was known for their work as a  dictator
newton was known for their work as a  scientist
euler was known for their work as a 
2024-07-24 21:41:16 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 21:42:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.8984,  0.2412,  1.9062,  ...,  0.2480, -2.4219,  2.9688],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-11.5000,  -6.5312,   2.0469,  ...,  -6.6250,  -6.1250,  -5.2188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0028, -0.0005,  0.0061,  ...,  0.0025,  0.0012,  0.0054],
        [ 0.0057,  0.0106,  0.0019,  ..., -0.0028, -0.0027,  0.0131],
        [ 0.0021, -0.0004,  0.0801,  ..., -0.0005, -0.0037, -0.0005],
        ...,
        [ 0.0045, -0.0012, -0.0215,  ...,  0.0021, -0.0006,  0.0047],
        [-0.0058, -0.0020,  0.0034,  ..., -0.0040,  0.0188, -0.0017],
        [ 0.0005, -0.0074, -0.0074,  ...,  0.0076, -0.0010,  0.0045]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-13.2344,  -3.3281,   3.4141,  ...,  -6.1445,  -6.8594,  -3.1953]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 21:42:51 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for monkey is gorilla
A more intense word for like is love
A more intense word for dinner is feast
A more intense word for ask is beg
A more intense word for sea is ocean
A more intense word for dislike is hate
A more intense word for opposed is averse
A more intense word for excited is
2024-07-24 21:42:51 root INFO     [order_1_approx] starting weight calculation for A more intense word for excited is agitated
A more intense word for opposed is averse
A more intense word for ask is beg
A more intense word for like is love
A more intense word for dinner is feast
A more intense word for sea is ocean
A more intense word for dislike is hate
A more intense word for monkey is
2024-07-24 21:42:52 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 21:47:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-5.3125,  2.1719, 10.3125,  ..., -0.4336, -0.8125,  4.4688],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-14.7500,  31.5000,   1.3438,  ...,  -0.1016,  -3.6875,  19.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0069,  0.0001, -0.0161,  ...,  0.0035, -0.0006,  0.0019],
        [ 0.0010,  0.0027,  0.0038,  ...,  0.0011, -0.0032,  0.0084],
        [-0.0115,  0.0043,  0.0598,  ..., -0.0003,  0.0038,  0.0020],
        ...,
        [ 0.0060,  0.0018,  0.0009,  ..., -0.0027,  0.0009,  0.0006],
        [ 0.0148,  0.0039,  0.0101,  ..., -0.0161,  0.0188, -0.0090],
        [ 0.0004, -0.0040,  0.0026,  ...,  0.0037,  0.0052,  0.0179]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-15.3516,  30.3906,   1.1357,  ...,   0.7695,  -2.4336,  20.2969]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 21:47:15 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for schwarzenegger was known for their work as a  actor
caesar was known for their work as a  emperor
confucius was known for their work as a  philosopher
marx was known for their work as a  philosopher
hegel was known for their work as a  philosopher
hitler was known for their work as a  dictator
newton was known for their work as a  scientist
euler was known for their work as a 
2024-07-24 21:47:15 root INFO     [order_1_approx] starting weight calculation for confucius was known for their work as a  philosopher
schwarzenegger was known for their work as a  actor
newton was known for their work as a  scientist
caesar was known for their work as a  emperor
euler was known for their work as a  mathematician
hegel was known for their work as a  philosopher
marx was known for their work as a  philosopher
hitler was known for their work as a 
2024-07-24 21:47:15 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 21:48:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5781, -1.8906,  1.0000,  ...,  2.5781, -0.8906, -0.7617],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-5.5000,  1.1562, -3.1875,  ..., -4.3750, 11.5625, -2.4219],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0134,  0.0018,  0.0015,  ...,  0.0045, -0.0045,  0.0053],
        [-0.0001,  0.0027, -0.0016,  ..., -0.0026,  0.0153, -0.0091],
        [-0.0048, -0.0009,  0.0781,  ..., -0.0079, -0.0047,  0.0122],
        ...,
        [-0.0057,  0.0027,  0.0108,  ...,  0.0024, -0.0105,  0.0078],
        [ 0.0013, -0.0017, -0.0156,  ...,  0.0020,  0.0342, -0.0044],
        [-0.0002,  0.0025,  0.0022,  ...,  0.0037, -0.0114,  0.0051]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.2500,  1.8379, -4.5820,  ..., -4.2383, 11.8984, -2.5508]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 21:48:20 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for excited is agitated
A more intense word for opposed is averse
A more intense word for ask is beg
A more intense word for like is love
A more intense word for dinner is feast
A more intense word for sea is ocean
A more intense word for dislike is hate
A more intense word for monkey is
2024-07-24 21:48:20 root INFO     [order_1_approx] starting weight calculation for A more intense word for dislike is hate
A more intense word for excited is agitated
A more intense word for sea is ocean
A more intense word for like is love
A more intense word for monkey is gorilla
A more intense word for ask is beg
A more intense word for opposed is averse
A more intense word for dinner is
2024-07-24 21:48:21 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 21:53:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.3125,  0.0508,  2.4688,  ...,  1.3438,  0.0469, -0.0664],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-14.5000,  26.0000,   1.8594,  ...,  -0.3203,  -1.8281,   7.8125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.9368e-03,  2.9755e-03,  8.5831e-04,  ...,  1.8616e-03,
          1.7853e-03,  1.4877e-03],
        [ 2.2278e-03,  5.2490e-03,  7.7820e-03,  ...,  3.5095e-03,
         -7.6294e-06, -1.6327e-03],
        [ 8.3160e-04, -4.5166e-03,  2.1851e-02,  ..., -1.0193e-02,
         -3.8910e-03,  1.2207e-03],
        ...,
        [-7.4768e-04, -8.7738e-04,  4.7302e-03,  ..., -2.2583e-03,
          2.4567e-03,  2.9297e-03],
        [ 2.2507e-04, -2.6398e-03,  7.2861e-04,  ..., -4.2114e-03,
          6.3171e-03, -8.1635e-04],
        [-7.4005e-04, -3.9978e-03, -1.6327e-03,  ..., -2.0905e-03,
         -2.0752e-03,  7.1411e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-13.5469,  27.2188,  -1.6191,  ...,  -0.5762,  -2.3984,   7.4727]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 21:53:19 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for confucius was known for their work as a  philosopher
schwarzenegger was known for their work as a  actor
newton was known for their work as a  scientist
caesar was known for their work as a  emperor
euler was known for their work as a  mathematician
hegel was known for their work as a  philosopher
marx was known for their work as a  philosopher
hitler was known for their work as a 
2024-07-24 21:53:19 root INFO     [order_1_approx] starting weight calculation for hegel was known for their work as a  philosopher
euler was known for their work as a  mathematician
newton was known for their work as a  scientist
confucius was known for their work as a  philosopher
hitler was known for their work as a  dictator
schwarzenegger was known for their work as a  actor
marx was known for their work as a  philosopher
caesar was known for their work as a 
2024-07-24 21:53:19 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 21:54:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([0.4258, 0.7656, 2.7500,  ..., 1.5859, 1.9688, 1.9062], device='cuda:0',
       dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 1.3906,  3.6094, -3.8750,  ...,  4.0938,  9.3750, -9.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0087,  0.0038, -0.0031,  ...,  0.0016,  0.0080,  0.0017],
        [-0.0013,  0.0100, -0.0062,  ..., -0.0117,  0.0063, -0.0156],
        [ 0.0098, -0.0096,  0.0674,  ..., -0.0057, -0.0227, -0.0006],
        ...,
        [-0.0011, -0.0073,  0.0122,  ...,  0.0011, -0.0059, -0.0021],
        [-0.0109,  0.0101,  0.0027,  ..., -0.0100,  0.0255, -0.0105],
        [ 0.0004, -0.0088, -0.0079,  ..., -0.0021, -0.0017,  0.0070]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.0479,  3.4043, -4.5586,  ...,  3.1953,  9.0625, -8.5938]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 21:54:02 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more intense word for dislike is hate
A more intense word for excited is agitated
A more intense word for sea is ocean
A more intense word for like is love
A more intense word for monkey is gorilla
A more intense word for ask is beg
A more intense word for opposed is averse
A more intense word for dinner is
2024-07-24 21:54:02 root INFO     total operator prediction time: 2722.777797937393 seconds
2024-07-24 21:54:02 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on hypernyms - animals
2024-07-24 21:54:02 root INFO     building operator hypernyms - animals
2024-07-24 21:54:02 root INFO     [order_1_approx] starting weight calculation for The deer falls into the category of bovid
The ant falls into the category of insect
The viper falls into the category of snake
The vulture falls into the category of raptor
The chimpanzee falls into the category of primate
The bee falls into the category of insect
The gibbon falls into the category of primate
The stegosaurus falls into the category of
2024-07-24 21:54:02 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 21:59:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.6406,  1.3906,  2.8438,  ...,  2.5625, -4.1250, -0.9648],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-17.2500,  29.0000,   0.0000,  ...,  -3.6250, -12.3125,  10.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 4.2725e-03,  7.1411e-03, -2.0123e-04,  ...,  3.6011e-03,
         -1.8768e-03, -4.7684e-04],
        [ 1.7090e-03, -3.3875e-03,  5.7373e-03,  ..., -4.5776e-03,
         -3.0899e-04, -2.8687e-03],
        [-1.7090e-03, -7.7057e-04,  4.1260e-02,  ..., -1.8997e-03,
          1.0757e-03, -6.2866e-03],
        ...,
        [-4.1809e-03,  2.4261e-03,  9.1553e-03,  ...,  2.3956e-03,
          9.3460e-05,  2.7924e-03],
        [-2.6398e-03, -1.3550e-02,  4.1962e-04,  ..., -1.0010e-02,
          2.0386e-02, -4.6387e-03],
        [ 5.3406e-03, -4.5776e-03,  5.6763e-03,  ..., -4.4250e-03,
         -1.8616e-03,  9.4604e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-15.7422,  27.5781,   0.1171,  ...,  -2.0039, -15.5859,   9.5312]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 21:59:17 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for hegel was known for their work as a  philosopher
euler was known for their work as a  mathematician
newton was known for their work as a  scientist
confucius was known for their work as a  philosopher
hitler was known for their work as a  dictator
schwarzenegger was known for their work as a  actor
marx was known for their work as a  philosopher
caesar was known for their work as a 
2024-07-24 21:59:17 root INFO     total operator prediction time: 2834.53923535347 seconds
2024-07-24 21:59:17 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on male - female
2024-07-24 21:59:17 root INFO     building operator male - female
2024-07-24 21:59:17 root INFO     [order_1_approx] starting weight calculation for A female boy is known as a girl
A female webmaster is known as a webmistress
A female gentleman is known as a lady
A female bull is known as a cow
A female waiter is known as a waitress
A female chairman is known as a chairwoman
A female fisherman is known as a fisherwoman
A female duke is known as a
2024-07-24 21:59:17 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 21:59:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9844, -1.6719, -1.5156,  ..., -1.1562, -1.1406,  1.7734],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-24.7500,   8.3750,  12.8750,  ...,  -4.0000,   4.5625,   9.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-0.0004, -0.0073,  0.0087,  ..., -0.0061,  0.0109,  0.0092],
        [ 0.0044,  0.0006,  0.0014,  ..., -0.0034, -0.0037, -0.0109],
        [ 0.0012, -0.0012,  0.0503,  ...,  0.0008,  0.0004, -0.0100],
        ...,
        [ 0.0056, -0.0004, -0.0034,  ...,  0.0029,  0.0040,  0.0034],
        [ 0.0034,  0.0012,  0.0071,  ..., -0.0010,  0.0112, -0.0076],
        [-0.0049,  0.0017,  0.0049,  ...,  0.0086, -0.0026,  0.0132]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-26.3281,   9.0781,  15.8906,  ...,  -2.8281,   6.7422,   9.7578]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 21:59:46 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The deer falls into the category of bovid
The ant falls into the category of insect
The viper falls into the category of snake
The vulture falls into the category of raptor
The chimpanzee falls into the category of primate
The bee falls into the category of insect
The gibbon falls into the category of primate
The stegosaurus falls into the category of
2024-07-24 21:59:46 root INFO     [order_1_approx] starting weight calculation for The stegosaurus falls into the category of dinosaur
The chimpanzee falls into the category of primate
The deer falls into the category of bovid
The gibbon falls into the category of primate
The bee falls into the category of insect
The vulture falls into the category of raptor
The viper falls into the category of snake
The ant falls into the category of
2024-07-24 21:59:46 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 22:05:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 3.2812,  0.1650, -3.8125,  ..., -0.1416,  1.5547,  1.7969],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 1.9219, -8.1250, -5.2500,  ..., -9.8750,  1.4844, 10.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0061, -0.0006, -0.0019,  ...,  0.0032, -0.0058,  0.0048],
        [ 0.0006,  0.0023,  0.0161,  ..., -0.0113, -0.0012,  0.0039],
        [-0.0008, -0.0082,  0.0791,  ..., -0.0026,  0.0093, -0.0156],
        ...,
        [-0.0051,  0.0101, -0.0101,  ...,  0.0014, -0.0117,  0.0057],
        [ 0.0008,  0.0023,  0.0097,  ..., -0.0022,  0.0142,  0.0132],
        [ 0.0129, -0.0025,  0.0019,  ..., -0.0049,  0.0025,  0.0104]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  1.2637, -10.2031,  -3.8789,  ..., -11.2969,   2.7930,   7.7656]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 22:05:17 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A female boy is known as a girl
A female webmaster is known as a webmistress
A female gentleman is known as a lady
A female bull is known as a cow
A female waiter is known as a waitress
A female chairman is known as a chairwoman
A female fisherman is known as a fisherwoman
A female duke is known as a
2024-07-24 22:05:17 root INFO     [order_1_approx] starting weight calculation for A female chairman is known as a chairwoman
A female waiter is known as a waitress
A female webmaster is known as a webmistress
A female duke is known as a duchess
A female gentleman is known as a lady
A female fisherman is known as a fisherwoman
A female bull is known as a cow
A female boy is known as a
2024-07-24 22:05:18 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 22:05:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 5.8438, -0.1875,  5.9062,  ...,  0.5078, -1.6797, -6.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -7.2812,   2.8125,  -5.8125,  ..., -12.8750,   3.7500,  -5.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-3.5858e-03,  6.6833e-03,  5.4321e-03,  ...,  4.5776e-03,
          3.1281e-03,  1.7929e-03],
        [ 4.4861e-03,  4.7112e-04, -1.4099e-02,  ..., -2.4719e-03,
          1.7853e-03, -5.4016e-03],
        [-6.0120e-03,  2.3499e-03,  8.6426e-02,  ..., -1.7776e-03,
         -3.7956e-04,  4.4556e-03],
        ...,
        [ 3.5400e-03, -4.2725e-03, -2.7466e-03,  ..., -3.0518e-04,
          1.0529e-03, -1.0757e-03],
        [-5.6267e-05, -3.2196e-03,  1.4114e-03,  ...,  6.7139e-04,
          2.7466e-02, -2.7771e-03],
        [-4.8523e-03, -1.6632e-03,  3.9062e-03,  ...,  5.4321e-03,
          3.8300e-03,  2.5146e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -7.0742,   3.6270,  -7.7539,  ..., -15.4531,   1.4277,  -2.8906]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 22:05:35 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The stegosaurus falls into the category of dinosaur
The chimpanzee falls into the category of primate
The deer falls into the category of bovid
The gibbon falls into the category of primate
The bee falls into the category of insect
The vulture falls into the category of raptor
The viper falls into the category of snake
The ant falls into the category of
2024-07-24 22:05:35 root INFO     [order_1_approx] starting weight calculation for The vulture falls into the category of raptor
The ant falls into the category of insect
The stegosaurus falls into the category of dinosaur
The bee falls into the category of insect
The viper falls into the category of snake
The chimpanzee falls into the category of primate
The gibbon falls into the category of primate
The deer falls into the category of
2024-07-24 22:05:35 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 22:11:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.9297, -1.3672,  4.0312,  ...,  0.3125,  0.0312,  0.6562],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -6.4375,  13.8750,   7.3125,  ..., -12.6875,   0.5781,  -7.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 9.0942e-03, -4.9744e-03,  8.4839e-03,  ...,  5.2795e-03,
          6.1951e-03,  1.8921e-03],
        [ 6.7139e-03,  4.9744e-03, -9.5825e-03,  ...,  9.9182e-04,
          2.0142e-03, -4.5776e-05],
        [-7.9346e-03, -1.1749e-03,  6.8359e-02,  ..., -2.7771e-03,
          1.4282e-02,  2.2430e-03],
        ...,
        [ 3.8147e-03, -1.2360e-03,  7.2937e-03,  ..., -3.9673e-03,
          3.0670e-03,  2.7771e-03],
        [-8.1787e-03, -2.4719e-03,  1.6113e-02,  ...,  9.3079e-04,
          3.3447e-02, -4.4861e-03],
        [ 5.2490e-03, -6.9580e-03,  1.9073e-03,  ..., -4.2725e-03,
          5.3406e-03,  2.3193e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -4.7383,  14.4297,   5.7812,  ..., -13.1953,  -1.3262,  -7.2031]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 22:11:15 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The vulture falls into the category of raptor
The ant falls into the category of insect
The stegosaurus falls into the category of dinosaur
The bee falls into the category of insect
The viper falls into the category of snake
The chimpanzee falls into the category of primate
The gibbon falls into the category of primate
The deer falls into the category of
2024-07-24 22:11:15 root INFO     [order_1_approx] starting weight calculation for The stegosaurus falls into the category of dinosaur
The ant falls into the category of insect
The viper falls into the category of snake
The vulture falls into the category of raptor
The deer falls into the category of bovid
The bee falls into the category of insect
The gibbon falls into the category of primate
The chimpanzee falls into the category of
2024-07-24 22:11:15 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 22:11:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3945, -2.4688,  2.9844,  ...,  2.2188, -3.3750, -0.7109],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  9.3125, -12.8750,   7.5000,  ...,  -1.4844,  -2.0938,  -4.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-2.5482e-03,  5.1880e-03,  7.6599e-03,  ...,  7.0190e-04,
          1.1658e-02,  9.4604e-03],
        [-6.1035e-05,  8.9111e-03,  1.5259e-03,  ...,  5.4016e-03,
         -7.0801e-03,  2.3804e-03],
        [ 2.0905e-03, -6.8970e-03,  7.5684e-02,  ...,  3.9062e-03,
          5.7220e-04, -1.6403e-03],
        ...,
        [ 4.9133e-03,  2.7771e-03, -1.0254e-02,  ..., -4.8828e-03,
          1.0193e-02,  4.0894e-03],
        [-3.0060e-03, -3.0060e-03, -3.6621e-03,  ..., -3.9673e-03,
          2.2583e-02, -1.2085e-02],
        [-4.0283e-03, -1.8311e-03, -9.7046e-03,  ...,  3.0518e-03,
         -1.1292e-02,  1.4648e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 10.0391, -12.2891,   4.4805,  ...,   0.1133,  -1.4805,  -5.4375]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 22:11:20 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A female chairman is known as a chairwoman
A female waiter is known as a waitress
A female webmaster is known as a webmistress
A female duke is known as a duchess
A female gentleman is known as a lady
A female fisherman is known as a fisherwoman
A female bull is known as a cow
A female boy is known as a
2024-07-24 22:11:20 root INFO     [order_1_approx] starting weight calculation for A female gentleman is known as a lady
A female waiter is known as a waitress
A female bull is known as a cow
A female webmaster is known as a webmistress
A female boy is known as a girl
A female chairman is known as a chairwoman
A female duke is known as a duchess
A female fisherman is known as a
2024-07-24 22:11:20 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 22:16:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 3.4375, -1.0391,  1.3750,  ...,  2.9062,  0.0420, -1.2344],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([11.6250, -1.9688, -6.3125,  ..., -0.3438, 11.0000,  0.9531],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.5640e-03, -1.3351e-05,  1.3733e-03,  ...,  2.9907e-03,
          4.0283e-03,  2.4567e-03],
        [ 6.4850e-04,  1.2207e-03, -6.6376e-04,  ...,  1.0376e-03,
          3.2349e-03, -2.0447e-03],
        [ 9.9945e-04,  1.7929e-03,  1.4465e-02,  ..., -6.9618e-05,
          1.9836e-03, -1.8597e-04],
        ...,
        [ 1.6174e-03, -1.1158e-04,  4.3335e-03,  ...,  1.4267e-03,
          1.9684e-03, -5.4932e-04],
        [ 2.6703e-03, -2.0313e-04, -1.0147e-03,  ...,  3.0365e-03,
          4.4861e-03,  2.0294e-03],
        [ 1.3123e-03, -1.9836e-03,  3.9673e-03,  ...,  1.7929e-04,
          1.5068e-04,  2.6703e-04]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[11.7656, -2.2812, -6.3281,  ..., -0.5420, 10.7891,  0.8691]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 22:16:54 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The stegosaurus falls into the category of dinosaur
The ant falls into the category of insect
The viper falls into the category of snake
The vulture falls into the category of raptor
The deer falls into the category of bovid
The bee falls into the category of insect
The gibbon falls into the category of primate
The chimpanzee falls into the category of
2024-07-24 22:16:55 root INFO     [order_1_approx] starting weight calculation for The gibbon falls into the category of primate
The chimpanzee falls into the category of primate
The ant falls into the category of insect
The deer falls into the category of bovid
The bee falls into the category of insect
The vulture falls into the category of raptor
The stegosaurus falls into the category of dinosaur
The viper falls into the category of
2024-07-24 22:16:55 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 22:17:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.9219, -1.5938,  1.0234,  ...,  2.5156,  2.3125,  1.4844],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 4.6250,  3.7344, -8.1875,  ..., -9.8750,  7.7188,  0.4609],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-0.0017,  0.0005,  0.0002,  ...,  0.0006,  0.0010,  0.0014],
        [ 0.0101,  0.0099, -0.0131,  ..., -0.0058,  0.0063,  0.0096],
        [ 0.0060, -0.0062,  0.0674,  ..., -0.0064,  0.0003, -0.0047],
        ...,
        [-0.0031, -0.0074,  0.0097,  ..., -0.0059, -0.0048,  0.0083],
        [ 0.0048,  0.0021,  0.0034,  ...,  0.0038,  0.0339,  0.0068],
        [ 0.0045, -0.0025, -0.0222,  ..., -0.0117,  0.0026,  0.0146]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.1055,  3.3887, -9.0938,  ..., -7.8203,  8.3125,  0.3765]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 22:17:24 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A female gentleman is known as a lady
A female waiter is known as a waitress
A female bull is known as a cow
A female webmaster is known as a webmistress
A female boy is known as a girl
A female chairman is known as a chairwoman
A female duke is known as a duchess
A female fisherman is known as a
2024-07-24 22:17:24 root INFO     [order_1_approx] starting weight calculation for A female gentleman is known as a lady
A female bull is known as a cow
A female boy is known as a girl
A female webmaster is known as a webmistress
A female chairman is known as a chairwoman
A female fisherman is known as a fisherwoman
A female duke is known as a duchess
A female waiter is known as a
2024-07-24 22:17:24 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 22:22:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9219, -0.0781,  7.5000,  ..., -0.5938,  3.2656, -0.5469],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 3.5938,  3.4219,  4.0625,  ..., -5.3750, 19.8750, 11.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0078, -0.0016,  0.0236,  ...,  0.0020,  0.0104,  0.0060],
        [ 0.0109,  0.0031,  0.0081,  ..., -0.0064, -0.0053,  0.0011],
        [-0.0071, -0.0025,  0.0898,  ..., -0.0126,  0.0073, -0.0103],
        ...,
        [-0.0084, -0.0076,  0.0277,  ...,  0.0057,  0.0005, -0.0052],
        [-0.0001, -0.0035, -0.0084,  ..., -0.0076,  0.0435, -0.0098],
        [-0.0043, -0.0018,  0.0077,  ...,  0.0151, -0.0098,  0.0317]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.0391,  4.2070,  3.6328,  ..., -3.4004, 19.3281, 13.1016]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 22:22:34 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The gibbon falls into the category of primate
The chimpanzee falls into the category of primate
The ant falls into the category of insect
The deer falls into the category of bovid
The bee falls into the category of insect
The vulture falls into the category of raptor
The stegosaurus falls into the category of dinosaur
The viper falls into the category of
2024-07-24 22:22:35 root INFO     [order_1_approx] starting weight calculation for The viper falls into the category of snake
The deer falls into the category of bovid
The gibbon falls into the category of primate
The ant falls into the category of insect
The stegosaurus falls into the category of dinosaur
The vulture falls into the category of raptor
The chimpanzee falls into the category of primate
The bee falls into the category of
2024-07-24 22:22:35 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 22:23:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4531, -2.0469,  5.0938,  ..., -1.0625,  1.2422,  0.5273],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 3.9844, 12.2500, -5.3750,  ..., -5.9688, 12.8125, -3.6406],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0086, -0.0013,  0.0106,  ..., -0.0036,  0.0179,  0.0030],
        [ 0.0076,  0.0047,  0.0074,  ..., -0.0013,  0.0030,  0.0022],
        [-0.0049,  0.0042,  0.0481,  ...,  0.0070, -0.0054, -0.0005],
        ...,
        [-0.0102,  0.0011,  0.0134,  ...,  0.0054, -0.0090, -0.0051],
        [ 0.0012,  0.0052, -0.0137,  ...,  0.0052,  0.0383,  0.0009],
        [-0.0078, -0.0011, -0.0070,  ...,  0.0051, -0.0142,  0.0193]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.3750, 11.6328, -3.1191,  ..., -5.6680, 13.5547, -3.6543]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 22:23:12 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A female gentleman is known as a lady
A female bull is known as a cow
A female boy is known as a girl
A female webmaster is known as a webmistress
A female chairman is known as a chairwoman
A female fisherman is known as a fisherwoman
A female duke is known as a duchess
A female waiter is known as a
2024-07-24 22:23:12 root INFO     [order_1_approx] starting weight calculation for A female waiter is known as a waitress
A female fisherman is known as a fisherwoman
A female boy is known as a girl
A female bull is known as a cow
A female gentleman is known as a lady
A female webmaster is known as a webmistress
A female duke is known as a duchess
A female chairman is known as a
2024-07-24 22:23:12 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 22:28:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.4844, -3.2188,  4.6875,  ...,  0.5156,  1.0391, -1.2734],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-15.2500,   8.2500,   2.0625,  ...,  -1.5000,   4.7812,  -7.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.6163e-03,  4.1199e-03, -2.8610e-04,  ..., -1.7242e-03,
          1.4877e-04, -1.4648e-03],
        [-6.7749e-03, -1.4496e-03, -1.8082e-03,  ...,  3.8147e-04,
          8.2397e-04, -5.6152e-03],
        [-3.1433e-03, -9.9182e-05,  7.0801e-02,  ...,  2.0752e-03,
          1.0925e-02,  2.9907e-03],
        ...,
        [-2.5482e-03, -4.8828e-03,  8.0566e-03,  ...,  6.7444e-03,
          1.0193e-02,  5.3101e-03],
        [ 2.7618e-03, -2.8038e-04,  1.3123e-02,  ..., -3.9368e-03,
          1.9653e-02, -2.3346e-03],
        [ 9.2316e-04, -2.5177e-04,  4.2725e-03,  ...,  5.1575e-03,
          5.8365e-04,  2.0142e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-14.8047,   9.4688,   1.5195,  ...,  -2.4590,   3.8105,  -5.8672]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 22:28:18 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The viper falls into the category of snake
The deer falls into the category of bovid
The gibbon falls into the category of primate
The ant falls into the category of insect
The stegosaurus falls into the category of dinosaur
The vulture falls into the category of raptor
The chimpanzee falls into the category of primate
The bee falls into the category of
2024-07-24 22:28:18 root INFO     [order_1_approx] starting weight calculation for The viper falls into the category of snake
The stegosaurus falls into the category of dinosaur
The ant falls into the category of insect
The bee falls into the category of insect
The gibbon falls into the category of primate
The chimpanzee falls into the category of primate
The deer falls into the category of bovid
The vulture falls into the category of
2024-07-24 22:28:18 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 22:29:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.4531, -0.9688,  1.2109,  ..., -0.1846, -0.7891,  0.9219],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -4.2500,   3.0625,  -6.8750,  ..., -12.5000,  -3.4375,  12.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0119,  0.0018, -0.0104,  ...,  0.0041,  0.0043,  0.0042],
        [ 0.0040,  0.0067,  0.0239,  ..., -0.0116,  0.0022, -0.0016],
        [-0.0016,  0.0007,  0.0654,  ..., -0.0142, -0.0050, -0.0017],
        ...,
        [ 0.0017,  0.0132,  0.0002,  ...,  0.0048,  0.0081,  0.0009],
        [ 0.0057,  0.0020, -0.0061,  ..., -0.0012,  0.0221, -0.0014],
        [-0.0012,  0.0039,  0.0122,  ..., -0.0106, -0.0125,  0.0222]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -2.2461,  -1.1875,  -5.8359,  ..., -10.4531,  -2.4531,  12.2344]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 22:29:02 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A female waiter is known as a waitress
A female fisherman is known as a fisherwoman
A female boy is known as a girl
A female bull is known as a cow
A female gentleman is known as a lady
A female webmaster is known as a webmistress
A female duke is known as a duchess
A female chairman is known as a
2024-07-24 22:29:02 root INFO     [order_1_approx] starting weight calculation for A female webmaster is known as a webmistress
A female waiter is known as a waitress
A female fisherman is known as a fisherwoman
A female duke is known as a duchess
A female chairman is known as a chairwoman
A female boy is known as a girl
A female gentleman is known as a lady
A female bull is known as a
2024-07-24 22:29:02 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 22:33:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.8438,  0.4375,  5.2188,  ..., -1.6875,  1.1094,  1.1016],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-10.6250,  -1.5000,   0.3438,  ...,  -3.1406,  16.0000,  -1.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 6.2256e-03,  1.3351e-05,  2.9297e-03,  ...,  1.5793e-03,
          1.3550e-02,  7.5073e-03],
        [ 1.7578e-02,  9.6893e-04,  1.3275e-03,  ..., -3.3417e-03,
          7.1716e-03, -5.2490e-03],
        [-2.7771e-03, -1.5259e-03,  8.3008e-02,  ..., -4.4250e-03,
         -5.8746e-04,  6.4087e-03],
        ...,
        [-7.3853e-03, -3.9062e-03, -2.5482e-03,  ...,  7.8735e-03,
          9.0942e-03,  1.1108e-02],
        [-1.1292e-02,  4.8523e-03,  2.0752e-02,  ...,  4.2419e-03,
          2.2461e-02,  5.1880e-03],
        [-5.2261e-04, -7.5684e-03,  1.0376e-03,  ...,  7.8125e-03,
         -5.6458e-04,  2.8442e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-7.2656, -0.6445, -0.6699,  ..., -1.9219, 15.5625, -1.2793]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 22:33:59 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The viper falls into the category of snake
The stegosaurus falls into the category of dinosaur
The ant falls into the category of insect
The bee falls into the category of insect
The gibbon falls into the category of primate
The chimpanzee falls into the category of primate
The deer falls into the category of bovid
The vulture falls into the category of
2024-07-24 22:33:59 root INFO     [order_1_approx] starting weight calculation for The vulture falls into the category of raptor
The chimpanzee falls into the category of primate
The ant falls into the category of insect
The deer falls into the category of bovid
The viper falls into the category of snake
The bee falls into the category of insect
The stegosaurus falls into the category of dinosaur
The gibbon falls into the category of
2024-07-24 22:33:59 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 22:34:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.2188, -0.7070,  2.0781,  ..., -0.8516, -0.4434,  3.3125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  4.2812,   2.2188,   7.6875,  ..., -24.3750,  -8.4375,   6.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-0.0004, -0.0100, -0.0198,  ..., -0.0004,  0.0155, -0.0057],
        [ 0.0029, -0.0014, -0.0140,  ..., -0.0049, -0.0229,  0.0074],
        [-0.0054, -0.0093,  0.0596,  ..., -0.0051,  0.0010, -0.0051],
        ...,
        [ 0.0038,  0.0013, -0.0010,  ..., -0.0043,  0.0143, -0.0107],
        [ 0.0012,  0.0084,  0.0107,  ...,  0.0037,  0.0248, -0.0103],
        [-0.0029,  0.0139, -0.0236,  ..., -0.0023, -0.0095,  0.0099]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  4.6094,   1.5664,   8.1250,  ..., -23.0156,  -7.7305,   7.0039]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 22:34:52 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A female webmaster is known as a webmistress
A female waiter is known as a waitress
A female fisherman is known as a fisherwoman
A female duke is known as a duchess
A female chairman is known as a chairwoman
A female boy is known as a girl
A female gentleman is known as a lady
A female bull is known as a
2024-07-24 22:34:52 root INFO     [order_1_approx] starting weight calculation for A female boy is known as a girl
A female duke is known as a duchess
A female fisherman is known as a fisherwoman
A female chairman is known as a chairwoman
A female bull is known as a cow
A female waiter is known as a waitress
A female gentleman is known as a lady
A female webmaster is known as a
2024-07-24 22:34:52 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 22:39:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 4.1250, -2.2812,  0.5703,  ...,  2.5000, -3.8281,  2.2969],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 3.9688,  3.9844, -0.6719,  ..., -0.9688, 10.2500,  1.9062],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0041, -0.0023, -0.0125,  ..., -0.0038,  0.0024,  0.0182],
        [ 0.0135,  0.0007,  0.0162,  ..., -0.0017,  0.0028, -0.0007],
        [ 0.0020,  0.0068,  0.1475,  ..., -0.0074,  0.0087, -0.0009],
        ...,
        [-0.0045, -0.0063,  0.0195,  ...,  0.0065,  0.0038,  0.0015],
        [ 0.0156, -0.0007, -0.0051,  ..., -0.0003,  0.0422,  0.0018],
        [ 0.0044, -0.0075,  0.0342,  ..., -0.0025, -0.0028,  0.0278]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.5234,  3.0977, -1.7969,  ..., -0.3628, 11.0625,  2.1152]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 22:39:42 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The vulture falls into the category of raptor
The chimpanzee falls into the category of primate
The ant falls into the category of insect
The deer falls into the category of bovid
The viper falls into the category of snake
The bee falls into the category of insect
The stegosaurus falls into the category of dinosaur
The gibbon falls into the category of
2024-07-24 22:39:42 root INFO     total operator prediction time: 2739.62264919281 seconds
2024-07-24 22:39:42 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on hyponyms - misc
2024-07-24 22:39:42 root INFO     building operator hyponyms - misc
2024-07-24 22:39:42 root INFO     [order_1_approx] starting weight calculation for A more specific term for a dress is gown
A more specific term for a season is spring
A more specific term for a song is lullaby
A more specific term for a cup is teacup
A more specific term for a car is limousine
A more specific term for a container is bag
A more specific term for a burger is hamburger
A more specific term for a collar is
2024-07-24 22:39:42 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 22:40:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.9922, -1.5000,  1.0781,  ...,  0.4375,  2.2344, -0.8008],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-0.0820,  8.4375, -0.7188,  ..., -8.9375, 12.5625,  4.8438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0033,  0.0005, -0.0082,  ...,  0.0007,  0.0022,  0.0015],
        [ 0.0017,  0.0015,  0.0065,  ..., -0.0022,  0.0142,  0.0189],
        [-0.0005,  0.0027,  0.0835,  ...,  0.0009, -0.0028, -0.0171],
        ...,
        [ 0.0042, -0.0038,  0.0173,  ...,  0.0037, -0.0051,  0.0028],
        [-0.0079,  0.0052, -0.0107,  ..., -0.0050,  0.0422,  0.0137],
        [-0.0140,  0.0061, -0.0173,  ...,  0.0028, -0.0005,  0.0004]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -0.5537,   6.1992,  -4.2891,  ..., -11.2734,  16.7188,   5.3086]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 22:40:44 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A female boy is known as a girl
A female duke is known as a duchess
A female fisherman is known as a fisherwoman
A female chairman is known as a chairwoman
A female bull is known as a cow
A female waiter is known as a waitress
A female gentleman is known as a lady
A female webmaster is known as a
2024-07-24 22:40:45 root INFO     [order_1_approx] starting weight calculation for A female waiter is known as a waitress
A female chairman is known as a chairwoman
A female duke is known as a duchess
A female boy is known as a girl
A female bull is known as a cow
A female webmaster is known as a webmistress
A female fisherman is known as a fisherwoman
A female gentleman is known as a
2024-07-24 22:40:45 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 22:45:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.7656, -1.0391, -0.8398,  ...,  1.8047,  0.5156, -3.5781],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-6.1562,  7.9688, -3.5312,  ..., 19.7500, 10.2500, -4.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0256, -0.0008, -0.0071,  ...,  0.0093,  0.0310, -0.0018],
        [ 0.0024,  0.0045, -0.0198,  ...,  0.0003,  0.0056,  0.0194],
        [ 0.0040,  0.0064,  0.0732,  ...,  0.0070,  0.0019, -0.0063],
        ...,
        [-0.0034,  0.0050,  0.0286,  ..., -0.0060, -0.0066, -0.0077],
        [-0.0109, -0.0003,  0.0142,  ..., -0.0049,  0.0131, -0.0034],
        [-0.0098,  0.0002, -0.0050,  ..., -0.0009,  0.0164,  0.0157]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-6.1133,  7.2383, -4.1523,  ..., 20.0938,  9.7734, -5.0977]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 22:45:25 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a dress is gown
A more specific term for a season is spring
A more specific term for a song is lullaby
A more specific term for a cup is teacup
A more specific term for a car is limousine
A more specific term for a container is bag
A more specific term for a burger is hamburger
A more specific term for a collar is
2024-07-24 22:45:26 root INFO     [order_1_approx] starting weight calculation for A more specific term for a cup is teacup
A more specific term for a car is limousine
A more specific term for a dress is gown
A more specific term for a burger is hamburger
A more specific term for a collar is choker
A more specific term for a song is lullaby
A more specific term for a season is spring
A more specific term for a container is
2024-07-24 22:45:26 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 22:46:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.0781, -0.6641,  1.6094,  ...,  1.3125, -1.0000,  0.2734],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 15.6875,  -3.1562,  -3.8281,  ..., -15.3750,   6.1250,   9.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 5.3711e-03,  1.9989e-03, -1.4465e-02,  ...,  1.0620e-02,
         -3.9673e-04, -1.0376e-02],
        [-3.6469e-03,  9.1553e-03,  2.3346e-03,  ..., -8.5449e-03,
          1.2894e-03,  4.3945e-03],
        [-1.4801e-03, -1.3000e-02,  7.5684e-02,  ...,  3.7842e-03,
         -1.0925e-02, -1.8188e-02],
        ...,
        [-8.0109e-04,  4.9438e-03,  1.0559e-02,  ...,  1.5747e-02,
         -2.1973e-02,  1.0071e-02],
        [-3.2043e-03,  8.1177e-03, -6.1035e-05,  ...,  3.4790e-03,
          2.6978e-02,  1.4191e-03],
        [ 5.6458e-03, -8.1177e-03,  1.5564e-03,  ..., -1.7929e-03,
          2.1057e-03,  1.7471e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 18.8750,  -4.9844,  -3.6934,  ..., -14.7422,   9.8438,   5.3828]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 22:46:45 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A female waiter is known as a waitress
A female chairman is known as a chairwoman
A female duke is known as a duchess
A female boy is known as a girl
A female bull is known as a cow
A female webmaster is known as a webmistress
A female fisherman is known as a fisherwoman
A female gentleman is known as a
2024-07-24 22:46:45 root INFO     total operator prediction time: 2847.673571586609 seconds
2024-07-24 22:46:45 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on animal - shelter
2024-07-24 22:46:45 root INFO     building operator animal - shelter
2024-07-24 22:46:45 root INFO     [order_1_approx] starting weight calculation for The place horse lives in is called stable
The place ant lives in is called anthill
The place tiger lives in is called den
The place insect lives in is called nest
The place hamster lives in is called nest
The place bat lives in is called cave
The place mole lives in is called hole
The place fish lives in is called
2024-07-24 22:46:45 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 22:50:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5547e+00,  6.6406e-01,  5.1250e+00,  ...,  4.3750e-01,
        -1.0781e+00, -3.9062e-03], device='cuda:0', dtype=torch.float16,
       grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-8.3750, -5.0000, -4.8438,  ...,  6.5000, -7.6250, -2.3906],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.1749e-03,  1.5564e-03,  1.2512e-02,  ..., -4.9438e-03,
         -6.2561e-03, -3.9368e-03],
        [-5.3406e-03,  1.6708e-03, -9.7046e-03,  ...,  1.1978e-03,
          1.5640e-04, -1.0742e-02],
        [-6.1646e-03, -2.6245e-03,  9.3262e-02,  ..., -7.3853e-03,
          1.6937e-03,  7.8125e-03],
        ...,
        [ 4.7684e-04,  7.9346e-03,  9.5825e-03,  ..., -2.4033e-04,
          1.3504e-03,  6.0730e-03],
        [-4.2725e-03,  9.8419e-04, -1.4343e-02,  ...,  2.3804e-03,
          1.6235e-02,  4.5776e-05],
        [ 5.3101e-03, -2.5177e-03, -2.8198e-02,  ...,  2.8381e-03,
          5.9509e-03,  1.6113e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-8.5781, -7.7812, -3.3086,  ...,  6.6758, -9.4375, -3.7949]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 22:51:00 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a cup is teacup
A more specific term for a car is limousine
A more specific term for a dress is gown
A more specific term for a burger is hamburger
A more specific term for a collar is choker
A more specific term for a song is lullaby
A more specific term for a season is spring
A more specific term for a container is
2024-07-24 22:51:01 root INFO     [order_1_approx] starting weight calculation for A more specific term for a burger is hamburger
A more specific term for a collar is choker
A more specific term for a cup is teacup
A more specific term for a season is spring
A more specific term for a car is limousine
A more specific term for a song is lullaby
A more specific term for a container is bag
A more specific term for a dress is
2024-07-24 22:51:01 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 22:52:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3047, -1.4062, -1.0234,  ...,  0.4863,  0.1250,  3.4219],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([15.0000, -2.7188, -8.0000,  ..., -4.3750,  8.4375, -4.0312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0037,  0.0021,  0.0058,  ...,  0.0010,  0.0067, -0.0027],
        [ 0.0060,  0.0032,  0.0035,  ...,  0.0040, -0.0040, -0.0038],
        [-0.0031, -0.0016,  0.0361,  ..., -0.0046,  0.0035, -0.0057],
        ...,
        [ 0.0037,  0.0034, -0.0023,  ..., -0.0004, -0.0045,  0.0071],
        [-0.0041, -0.0009, -0.0132,  ...,  0.0010,  0.0121,  0.0031],
        [ 0.0025,  0.0005, -0.0100,  ...,  0.0049, -0.0081,  0.0150]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[14.7578, -3.0703, -6.4688,  ..., -4.6914,  7.7812, -5.1133]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 22:52:32 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The place horse lives in is called stable
The place ant lives in is called anthill
The place tiger lives in is called den
The place insect lives in is called nest
The place hamster lives in is called nest
The place bat lives in is called cave
The place mole lives in is called hole
The place fish lives in is called
2024-07-24 22:52:33 root INFO     [order_1_approx] starting weight calculation for The place fish lives in is called sea
The place tiger lives in is called den
The place bat lives in is called cave
The place ant lives in is called anthill
The place insect lives in is called nest
The place hamster lives in is called nest
The place horse lives in is called stable
The place mole lives in is called
2024-07-24 22:52:33 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 22:56:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4766, -0.3730,  1.3594,  ...,  1.6641, -1.7656,  2.2812],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -0.0469,   7.1875, -14.1250,  ...,  13.7500,  25.5000,   0.5156],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 9.0942e-03,  4.2114e-03,  1.5259e-03,  ..., -2.3041e-03,
          5.6458e-03, -2.5940e-03],
        [ 6.7444e-03,  2.9602e-03,  2.4033e-04,  ..., -6.8665e-05,
          1.1902e-03,  1.0132e-02],
        [-5.1880e-03,  1.9836e-03,  7.2266e-02,  ..., -4.9744e-03,
         -1.0681e-02, -6.2561e-04],
        ...,
        [-7.2632e-03,  5.4626e-03, -5.2261e-04,  ..., -1.2817e-03,
          1.6022e-04, -6.8359e-03],
        [-2.5024e-03, -3.4790e-03,  5.4932e-03,  ..., -2.6245e-03,
          1.7212e-02, -3.7537e-03],
        [ 4.6539e-04, -2.4719e-03, -5.3406e-03,  ..., -1.5106e-03,
          9.2163e-03,  1.0986e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -0.4890,   5.9492, -15.1641,  ...,  12.9844,  25.2188,   0.6455]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 22:56:36 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a burger is hamburger
A more specific term for a collar is choker
A more specific term for a cup is teacup
A more specific term for a season is spring
A more specific term for a car is limousine
A more specific term for a song is lullaby
A more specific term for a container is bag
A more specific term for a dress is
2024-07-24 22:56:37 root INFO     [order_1_approx] starting weight calculation for A more specific term for a song is lullaby
A more specific term for a container is bag
A more specific term for a car is limousine
A more specific term for a season is spring
A more specific term for a dress is gown
A more specific term for a burger is hamburger
A more specific term for a collar is choker
A more specific term for a cup is
2024-07-24 22:56:37 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 22:58:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3281, -2.0312,  7.0938,  ..., -0.3535,  2.3125, -1.3359],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  8.1250,   4.5000,   2.6250,  ...,  -8.0000,   4.9375, -19.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0089, -0.0020,  0.0060,  ..., -0.0032, -0.0035, -0.0015],
        [ 0.0073,  0.0040,  0.0001,  ...,  0.0084,  0.0037,  0.0012],
        [-0.0008,  0.0030,  0.0884,  ...,  0.0027, -0.0111,  0.0043],
        ...,
        [ 0.0033, -0.0046, -0.0049,  ..., -0.0024, -0.0047,  0.0078],
        [-0.0050, -0.0024,  0.0017,  ...,  0.0008,  0.0310, -0.0077],
        [ 0.0025, -0.0061, -0.0092,  ..., -0.0082, -0.0160,  0.0374]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  7.1641,   3.9512,   0.9102,  ...,  -8.4609,   4.7656, -20.2031]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 22:58:17 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The place fish lives in is called sea
The place tiger lives in is called den
The place bat lives in is called cave
The place ant lives in is called anthill
The place insect lives in is called nest
The place hamster lives in is called nest
The place horse lives in is called stable
The place mole lives in is called
2024-07-24 22:58:17 root INFO     [order_1_approx] starting weight calculation for The place bat lives in is called cave
The place ant lives in is called anthill
The place horse lives in is called stable
The place fish lives in is called sea
The place mole lives in is called hole
The place hamster lives in is called nest
The place tiger lives in is called den
The place insect lives in is called
2024-07-24 22:58:17 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 23:02:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.3906, -2.2188,  4.0625,  ...,  1.2422,  0.6953, -1.9375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-3.9688,  7.8750,  0.0625,  ...,  9.1875, -0.2344,  1.8594],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0076,  0.0081,  0.0092,  ...,  0.0064,  0.0020, -0.0118],
        [-0.0067,  0.0059,  0.0079,  ..., -0.0026, -0.0060,  0.0060],
        [-0.0016, -0.0011,  0.0884,  ..., -0.0010, -0.0049, -0.0117],
        ...,
        [-0.0084,  0.0092,  0.0020,  ...,  0.0010,  0.0121, -0.0018],
        [ 0.0019, -0.0072,  0.0019,  ...,  0.0009,  0.0232,  0.0010],
        [ 0.0022, -0.0009, -0.0095,  ..., -0.0060,  0.0046,  0.0187]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.1250,  7.5000, -0.1921,  ..., 10.1719,  1.1377,  0.9751]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 23:02:10 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a song is lullaby
A more specific term for a container is bag
A more specific term for a car is limousine
A more specific term for a season is spring
A more specific term for a dress is gown
A more specific term for a burger is hamburger
A more specific term for a collar is choker
A more specific term for a cup is
2024-07-24 23:02:10 root INFO     [order_1_approx] starting weight calculation for A more specific term for a song is lullaby
A more specific term for a dress is gown
A more specific term for a cup is teacup
A more specific term for a season is spring
A more specific term for a burger is hamburger
A more specific term for a container is bag
A more specific term for a collar is choker
A more specific term for a car is
2024-07-24 23:02:10 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 23:04:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 3.1875, -2.2812,  2.1406,  ..., -0.9805,  0.1484, -0.8984],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  7.2812,  -0.5156,  -9.3750,  ...,  -2.7344,  -2.8281, -25.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.0823e-03,  1.0925e-02,  1.3550e-02,  ...,  4.3640e-03,
          8.4839e-03,  4.7684e-05],
        [ 4.3335e-03,  8.7280e-03,  8.4229e-03,  ...,  4.1504e-03,
         -2.4223e-04, -3.5400e-03],
        [-5.7678e-03,  9.3079e-04,  9.9609e-02,  ..., -4.0283e-03,
          2.1667e-03, -4.8218e-03],
        ...,
        [ 1.1230e-02,  6.0425e-03,  9.8267e-03,  ...,  4.4556e-03,
          3.8757e-03,  1.4160e-02],
        [-2.8229e-03, -4.8523e-03,  1.5020e-05,  ..., -3.0518e-03,
          2.2705e-02, -4.8828e-03],
        [ 3.7384e-03,  4.9133e-03, -2.2339e-02,  ..., -6.4392e-03,
         -4.6997e-03,  2.6611e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  6.8438,  -0.8301,  -9.0469,  ...,  -2.2656,  -3.1602, -24.7031]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 23:04:17 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The place bat lives in is called cave
The place ant lives in is called anthill
The place horse lives in is called stable
The place fish lives in is called sea
The place mole lives in is called hole
The place hamster lives in is called nest
The place tiger lives in is called den
The place insect lives in is called
2024-07-24 23:04:17 root INFO     [order_1_approx] starting weight calculation for The place horse lives in is called stable
The place ant lives in is called anthill
The place tiger lives in is called den
The place mole lives in is called hole
The place fish lives in is called sea
The place bat lives in is called cave
The place insect lives in is called nest
The place hamster lives in is called
2024-07-24 23:04:17 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 23:07:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1484, -0.0234,  3.7500,  ...,  0.2500,  0.4297,  1.5469],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-9.0000, -3.0000,  6.1250,  ..., -6.9375,  0.7344,  9.8125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 5.0354e-03,  4.3945e-03,  1.3123e-03,  ...,  1.3504e-03,
          4.5166e-03, -5.8289e-03],
        [-1.6785e-04,  2.1362e-03, -1.2970e-03,  ..., -1.9455e-03,
         -3.8147e-06, -6.3324e-04],
        [-1.1444e-03, -1.6174e-03,  5.6885e-02,  ..., -2.1667e-03,
         -4.8523e-03, -4.8828e-03],
        ...,
        [-7.9956e-03,  2.9907e-03, -1.4191e-03,  ..., -2.7466e-03,
          8.7738e-04, -6.9580e-03],
        [ 6.5804e-05, -1.6022e-04,  9.4986e-04,  ...,  5.7373e-03,
          1.0498e-02,  4.5013e-04],
        [-4.5776e-04, -1.9684e-03,  1.4114e-03,  ...,  2.7313e-03,
          2.3804e-03,  8.0566e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-8.5781, -2.9648,  4.3477,  ..., -6.6406,  1.8311, 11.7188]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 23:07:41 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a song is lullaby
A more specific term for a dress is gown
A more specific term for a cup is teacup
A more specific term for a season is spring
A more specific term for a burger is hamburger
A more specific term for a container is bag
A more specific term for a collar is choker
A more specific term for a car is
2024-07-24 23:07:41 root INFO     [order_1_approx] starting weight calculation for A more specific term for a season is spring
A more specific term for a song is lullaby
A more specific term for a collar is choker
A more specific term for a dress is gown
A more specific term for a car is limousine
A more specific term for a cup is teacup
A more specific term for a container is bag
A more specific term for a burger is
2024-07-24 23:07:41 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 23:10:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4766, -4.1250,  4.3125,  ..., -1.1406, -2.0156,  1.4688],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 10.4375,   0.3906,  -2.1562,  ...,   2.4062,  -5.9375, -22.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.3438e-02, -7.6294e-03,  2.4414e-02,  ...,  8.0566e-03,
          1.9531e-02, -1.4038e-02],
        [ 1.1475e-02,  9.8877e-03,  1.3916e-02,  ...,  9.2163e-03,
          5.8594e-03, -8.9111e-03],
        [-1.8387e-03,  6.9046e-04,  8.7891e-02,  ..., -3.6469e-03,
          8.2016e-04, -9.3079e-04],
        ...,
        [-8.7280e-03, -7.7820e-03, -4.6692e-03,  ..., -6.1035e-05,
         -2.2705e-02,  3.1738e-02],
        [ 4.9744e-03,  6.8054e-03,  6.1340e-03,  ...,  5.8594e-03,
          4.5654e-02, -1.5320e-02],
        [-1.4221e-02, -4.9438e-03, -7.1106e-03,  ..., -9.0332e-03,
         -2.0752e-02,  4.3213e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  9.1172,   0.5430,  -3.1406,  ...,   1.4482,  -5.2930, -23.0156]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 23:10:12 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The place horse lives in is called stable
The place ant lives in is called anthill
The place tiger lives in is called den
The place mole lives in is called hole
The place fish lives in is called sea
The place bat lives in is called cave
The place insect lives in is called nest
The place hamster lives in is called
2024-07-24 23:10:13 root INFO     [order_1_approx] starting weight calculation for The place insect lives in is called nest
The place horse lives in is called stable
The place bat lives in is called cave
The place ant lives in is called anthill
The place hamster lives in is called nest
The place mole lives in is called hole
The place fish lives in is called sea
The place tiger lives in is called
2024-07-24 23:10:13 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 23:13:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0938,  0.3984, -1.1797,  ..., -0.6172, -1.3516,  3.4219],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 3.0781,  1.6719, -5.7500,  ...,  2.6719,  8.3125, -4.9375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0079,  0.0072, -0.0039,  ..., -0.0038, -0.0056, -0.0044],
        [ 0.0112,  0.0033, -0.0030,  ..., -0.0015,  0.0026, -0.0022],
        [-0.0065,  0.0021,  0.0879,  ...,  0.0011, -0.0042, -0.0055],
        ...,
        [ 0.0010,  0.0032,  0.0027,  ...,  0.0078, -0.0041, -0.0078],
        [-0.0035,  0.0036,  0.0082,  ...,  0.0004,  0.0359,  0.0058],
        [ 0.0013, -0.0006,  0.0232,  ..., -0.0049,  0.0161,  0.0189]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.9062,  0.4619, -6.6172,  ...,  3.7031,  9.8359, -5.1289]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 23:13:22 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a season is spring
A more specific term for a song is lullaby
A more specific term for a collar is choker
A more specific term for a dress is gown
A more specific term for a car is limousine
A more specific term for a cup is teacup
A more specific term for a container is bag
A more specific term for a burger is
2024-07-24 23:13:22 root INFO     [order_1_approx] starting weight calculation for A more specific term for a container is bag
A more specific term for a collar is choker
A more specific term for a dress is gown
A more specific term for a burger is hamburger
A more specific term for a cup is teacup
A more specific term for a car is limousine
A more specific term for a season is spring
A more specific term for a song is
2024-07-24 23:13:23 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 23:15:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.5938, -1.2891,  2.6250,  ...,  0.9102,  0.3262,  0.5469],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 15.6875,  -2.1250,  -1.7812,  ...,  -9.3125,  -1.1562, -18.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.3916e-02,  9.9182e-04,  1.4221e-02,  ..., -4.2114e-03,
          1.5320e-02, -4.1504e-03],
        [ 1.6602e-02,  1.3855e-02,  1.0376e-02,  ..., -6.0730e-03,
          1.0315e-02, -2.0294e-03],
        [ 4.1809e-03, -2.4414e-03,  6.0547e-02,  ..., -5.1498e-04,
          1.0910e-03,  1.1597e-03],
        ...,
        [-1.7624e-03, -1.9073e-05, -2.2827e-02,  ..., -8.2397e-04,
          1.2878e-02,  8.6670e-03],
        [-5.0354e-03,  9.3842e-04,  8.4229e-03,  ..., -6.8665e-04,
          1.3428e-02, -3.8605e-03],
        [ 5.3406e-03,  2.3804e-03, -2.1240e-02,  ...,  1.4343e-03,
          2.5940e-03,  2.1118e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 14.9531,  -3.7051,  -1.4844,  ...,  -8.8047,  -1.5928, -17.2500]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 23:16:00 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The place insect lives in is called nest
The place horse lives in is called stable
The place bat lives in is called cave
The place ant lives in is called anthill
The place hamster lives in is called nest
The place mole lives in is called hole
The place fish lives in is called sea
The place tiger lives in is called
2024-07-24 23:16:00 root INFO     [order_1_approx] starting weight calculation for The place fish lives in is called sea
The place tiger lives in is called den
The place hamster lives in is called nest
The place mole lives in is called hole
The place bat lives in is called cave
The place insect lives in is called nest
The place ant lives in is called anthill
The place horse lives in is called
2024-07-24 23:16:01 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 23:19:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.2500,  1.1875,  3.3438,  ...,  1.0938, -1.8125, -0.1943],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-1.3906, -5.3125, 15.1875,  ...,  7.3750,  1.5625,  5.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0057, -0.0022,  0.0036,  ..., -0.0037, -0.0095, -0.0017],
        [-0.0017,  0.0031, -0.0068,  ..., -0.0020,  0.0046,  0.0008],
        [-0.0019,  0.0020,  0.0674,  ..., -0.0030, -0.0089, -0.0037],
        ...,
        [-0.0037,  0.0012, -0.0018,  ..., -0.0045, -0.0012,  0.0038],
        [-0.0034, -0.0064,  0.0164,  ..., -0.0094,  0.0133,  0.0017],
        [-0.0023,  0.0061, -0.0024,  ..., -0.0082,  0.0121,  0.0093]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.7832, -5.7227, 13.9766,  ...,  8.2031,  1.6123,  6.5625]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 23:19:05 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a container is bag
A more specific term for a collar is choker
A more specific term for a dress is gown
A more specific term for a burger is hamburger
A more specific term for a cup is teacup
A more specific term for a car is limousine
A more specific term for a season is spring
A more specific term for a song is
2024-07-24 23:19:05 root INFO     [order_1_approx] starting weight calculation for A more specific term for a collar is choker
A more specific term for a song is lullaby
A more specific term for a container is bag
A more specific term for a cup is teacup
A more specific term for a car is limousine
A more specific term for a burger is hamburger
A more specific term for a dress is gown
A more specific term for a season is
2024-07-24 23:19:06 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 23:21:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3438, -3.5781,  2.7812,  ..., -1.1328, -2.0156,  0.4199],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 14.3125,  -5.1250,   2.0156,  ...,  -6.8750,  -8.8750, -10.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0046, -0.0013,  0.0092,  ..., -0.0047,  0.0048, -0.0012],
        [ 0.0042,  0.0050,  0.0064,  ...,  0.0013, -0.0029, -0.0037],
        [ 0.0031,  0.0010,  0.0503,  ...,  0.0007, -0.0020,  0.0074],
        ...,
        [-0.0034, -0.0020, -0.0104,  ..., -0.0006, -0.0053,  0.0037],
        [ 0.0003, -0.0006,  0.0049,  ...,  0.0029,  0.0131, -0.0092],
        [ 0.0024, -0.0025, -0.0066,  ..., -0.0025, -0.0038,  0.0164]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[13.0938, -5.2383,  1.8330,  ..., -7.2930, -8.2109, -9.6953]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 23:21:50 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The place fish lives in is called sea
The place tiger lives in is called den
The place hamster lives in is called nest
The place mole lives in is called hole
The place bat lives in is called cave
The place insect lives in is called nest
The place ant lives in is called anthill
The place horse lives in is called
2024-07-24 23:21:50 root INFO     [order_1_approx] starting weight calculation for The place insect lives in is called nest
The place hamster lives in is called nest
The place fish lives in is called sea
The place bat lives in is called cave
The place horse lives in is called stable
The place tiger lives in is called den
The place mole lives in is called hole
The place ant lives in is called
2024-07-24 23:21:50 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 23:24:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.0625,  0.7734,  4.6250,  ...,  0.2344, -0.7695,  0.7305],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-7.8125, -1.5312, -1.1719,  ...,  7.0000,  6.9062,  4.5312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0110, -0.0066, -0.0010,  ..., -0.0019, -0.0064,  0.0012],
        [ 0.0045, -0.0049, -0.0052,  ..., -0.0056, -0.0034,  0.0005],
        [-0.0062,  0.0006,  0.0732,  ...,  0.0027, -0.0046,  0.0011],
        ...,
        [-0.0046,  0.0082, -0.0026,  ..., -0.0029,  0.0026,  0.0054],
        [-0.0013,  0.0030,  0.0093,  ..., -0.0023,  0.0195,  0.0030],
        [ 0.0033, -0.0070,  0.0011,  ...,  0.0033,  0.0091,  0.0147]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-6.7305, -0.9814, -1.6592,  ...,  6.7148,  7.2734,  5.3125]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 23:24:44 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A more specific term for a collar is choker
A more specific term for a song is lullaby
A more specific term for a container is bag
A more specific term for a cup is teacup
A more specific term for a car is limousine
A more specific term for a burger is hamburger
A more specific term for a dress is gown
A more specific term for a season is
2024-07-24 23:24:44 root INFO     total operator prediction time: 2702.668294429779 seconds
2024-07-24 23:24:44 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on antonyms - binary
2024-07-24 23:24:44 root INFO     building operator antonyms - binary
2024-07-24 23:24:45 root INFO     [order_1_approx] starting weight calculation for The opposite of rise is sink
The opposite of input is output
The opposite of outward is upward
The opposite of interior is exterior
The opposite of southeast is southwest
The opposite of inbound is outbound
The opposite of ahead is behind
The opposite of decrement is
2024-07-24 23:24:45 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 23:27:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 4.6875, -1.2578,  3.2969,  ...,  0.7969, -0.7070, -5.8438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  5.5625,  -6.7812,  -6.2812,  ...,  -7.1875,  -6.2188, -21.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0033,  0.0004,  0.0060,  ..., -0.0022,  0.0058,  0.0019],
        [ 0.0012,  0.0038,  0.0092,  ...,  0.0056, -0.0008, -0.0035],
        [ 0.0019,  0.0012,  0.0625,  ..., -0.0008, -0.0025,  0.0040],
        ...,
        [ 0.0014,  0.0045, -0.0092,  ...,  0.0016,  0.0038,  0.0032],
        [-0.0053, -0.0012,  0.0050,  ...,  0.0013,  0.0219, -0.0096],
        [ 0.0006, -0.0018, -0.0052,  ..., -0.0080,  0.0066,  0.0153]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  4.3359,  -5.7148,  -6.5195,  ...,  -7.0938,  -7.3242, -22.1875]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 23:27:50 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The place insect lives in is called nest
The place hamster lives in is called nest
The place fish lives in is called sea
The place bat lives in is called cave
The place horse lives in is called stable
The place tiger lives in is called den
The place mole lives in is called hole
The place ant lives in is called
2024-07-24 23:27:51 root INFO     [order_1_approx] starting weight calculation for The place ant lives in is called anthill
The place insect lives in is called nest
The place hamster lives in is called nest
The place horse lives in is called stable
The place fish lives in is called sea
The place tiger lives in is called den
The place mole lives in is called hole
The place bat lives in is called
2024-07-24 23:27:51 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 23:30:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8438, -0.1992,  3.0625,  ..., -0.6367, -1.3359, -0.3047],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  4.4375,  -2.3125,  -5.8125,  ...,  -0.7461, -14.8750,   8.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0150, -0.0013, -0.0017,  ..., -0.0056, -0.0084,  0.0065],
        [ 0.0156,  0.0092,  0.0344,  ..., -0.0212,  0.0049, -0.0106],
        [ 0.0093,  0.0041,  0.1455,  ..., -0.0143, -0.0102, -0.0029],
        ...,
        [ 0.0187, -0.0090,  0.0096,  ..., -0.0157, -0.0146,  0.0073],
        [ 0.0009,  0.0073,  0.0055,  ...,  0.0083,  0.0292,  0.0244],
        [-0.0052,  0.0058, -0.0170,  ...,  0.0036, -0.0079,  0.0176]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  5.2461,  -0.6553,  -3.3516,  ...,  -4.5195, -12.6953,   4.7188]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 23:30:31 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of rise is sink
The opposite of input is output
The opposite of outward is upward
The opposite of interior is exterior
The opposite of southeast is southwest
The opposite of inbound is outbound
The opposite of ahead is behind
The opposite of decrement is
2024-07-24 23:30:31 root INFO     [order_1_approx] starting weight calculation for The opposite of southeast is southwest
The opposite of ahead is behind
The opposite of interior is exterior
The opposite of input is output
The opposite of rise is sink
The opposite of inbound is outbound
The opposite of decrement is increment
The opposite of outward is
2024-07-24 23:30:31 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 23:33:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 3.8438, -1.4688,  0.7227,  ...,  1.8516,  2.5000, -1.1328],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  9.6250,  -3.0938,  -6.0625,  ...,   0.7188,   8.8125, -23.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 5.8594e-03, -3.4790e-03,  5.7373e-03,  ..., -1.0300e-03,
          1.9741e-04, -8.6060e-03],
        [ 9.7656e-03,  2.4719e-03,  6.4697e-03,  ...,  7.9346e-03,
         -6.8359e-03,  7.6294e-05],
        [-6.2943e-04, -7.9346e-03,  7.7148e-02,  ..., -1.4572e-03,
          1.1978e-03, -4.3945e-03],
        ...,
        [-1.5259e-03,  2.7847e-04, -5.4016e-03,  ...,  2.8381e-03,
          3.3112e-03,  4.4250e-03],
        [-7.8125e-03,  3.1128e-03,  7.1106e-03,  ..., -4.1809e-03,
          1.7456e-02,  3.9062e-03],
        [ 6.8665e-03,  1.7853e-03, -3.0823e-03,  ..., -4.3640e-03,
         -5.0354e-04,  1.5747e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  9.6172,  -3.6367,  -6.1523,  ...,   0.7700,   7.6875, -22.5312]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 23:33:45 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The place ant lives in is called anthill
The place insect lives in is called nest
The place hamster lives in is called nest
The place horse lives in is called stable
The place fish lives in is called sea
The place tiger lives in is called den
The place mole lives in is called hole
The place bat lives in is called
2024-07-24 23:33:45 root INFO     total operator prediction time: 2819.8845393657684 seconds
2024-07-24 23:33:45 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on country - language
2024-07-24 23:33:45 root INFO     building operator country - language
2024-07-24 23:33:45 root INFO     [order_1_approx] starting weight calculation for The country of haiti primarily speaks the language of creole
The country of colombia primarily speaks the language of spanish
The country of ethiopia primarily speaks the language of amharic
The country of cambodia primarily speaks the language of khmer
The country of mexico primarily speaks the language of spanish
The country of egypt primarily speaks the language of arabic
The country of usa primarily speaks the language of english
The country of guatemala primarily speaks the language of
2024-07-24 23:33:45 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 23:36:03 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5469, -2.4062,  0.7422,  ...,  2.9062, -0.6406,  1.4766],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-4.8125, -1.5938,  4.4375,  ..., -3.3594, -8.5625,  3.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.1240e-02,  2.5635e-03, -1.7944e-02,  ..., -9.5215e-03,
         -9.2163e-03, -1.3657e-03],
        [ 1.6357e-02,  5.7068e-03, -1.7456e-02,  ...,  2.1973e-03,
          1.3885e-03,  1.0452e-03],
        [ 3.4790e-03, -3.3875e-03,  1.2109e-01,  ..., -1.2146e-02,
         -1.0681e-02,  5.4321e-03],
        ...,
        [ 4.2343e-04,  1.1475e-02,  2.1362e-02,  ..., -1.6937e-03,
         -7.5912e-04,  1.5335e-03],
        [ 6.0425e-03,  7.5684e-03,  2.0630e-02,  ...,  2.8534e-03,
          2.6001e-02, -5.9509e-03],
        [-3.2425e-05, -1.4267e-03, -1.9043e-02,  ...,  1.7319e-03,
          1.0803e-02,  8.3008e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -7.9570,  -2.9688,   6.3984,  ...,  -3.2305, -10.4375,   1.5664]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 23:36:04 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of southeast is southwest
The opposite of ahead is behind
The opposite of interior is exterior
The opposite of input is output
The opposite of rise is sink
The opposite of inbound is outbound
The opposite of decrement is increment
The opposite of outward is
2024-07-24 23:36:04 root INFO     [order_1_approx] starting weight calculation for The opposite of interior is exterior
The opposite of outward is upward
The opposite of southeast is southwest
The opposite of input is output
The opposite of ahead is behind
The opposite of inbound is outbound
The opposite of decrement is increment
The opposite of rise is
2024-07-24 23:36:04 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 23:39:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.6797, -2.5625,  5.2188,  ...,  2.2969, -0.4336,  3.1094],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-1.6719, -3.0000,  5.5625,  ...,  5.1562,  2.2500, 26.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-1.5991e-02, -5.8899e-03, -3.9795e-02,  ..., -6.3171e-03,
         -1.4038e-02, -2.5558e-04],
        [-7.6294e-06, -5.0964e-03, -4.4861e-03,  ..., -1.8387e-03,
         -4.4861e-03,  1.0803e-02],
        [-3.7994e-03, -1.2512e-03,  4.1016e-02,  ..., -8.9111e-03,
         -3.6774e-03, -3.3264e-03],
        ...,
        [ 8.4839e-03,  1.8005e-03,  2.2827e-02,  ...,  4.3640e-03,
          3.9978e-03,  1.9073e-06],
        [-4.6387e-03,  5.0964e-03,  2.5482e-03,  ..., -3.8300e-03,
          1.7334e-02,  3.2959e-03],
        [ 7.2632e-03, -3.1281e-03,  1.3184e-02,  ...,  5.0659e-03,
         -5.7220e-04,  1.3916e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.9404, -5.1719,  4.9453,  ...,  4.8125,  3.1699, 23.0625]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 23:39:34 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The country of haiti primarily speaks the language of creole
The country of colombia primarily speaks the language of spanish
The country of ethiopia primarily speaks the language of amharic
The country of cambodia primarily speaks the language of khmer
The country of mexico primarily speaks the language of spanish
The country of egypt primarily speaks the language of arabic
The country of usa primarily speaks the language of english
The country of guatemala primarily speaks the language of
2024-07-24 23:39:34 root INFO     [order_1_approx] starting weight calculation for The country of guatemala primarily speaks the language of spanish
The country of cambodia primarily speaks the language of khmer
The country of colombia primarily speaks the language of spanish
The country of usa primarily speaks the language of english
The country of haiti primarily speaks the language of creole
The country of egypt primarily speaks the language of arabic
The country of mexico primarily speaks the language of spanish
The country of ethiopia primarily speaks the language of
2024-07-24 23:39:34 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 23:41:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0820, -0.4297,  0.3086,  ...,  0.0605,  0.4688,  0.3789],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([11.0625,  0.5938, -6.7188,  ..., -1.0781, -7.0000, 10.9375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0132,  0.0079, -0.0026,  ...,  0.0101,  0.0067, -0.0063],
        [ 0.0128,  0.0008, -0.0091,  ..., -0.0087,  0.0043,  0.0078],
        [ 0.0122,  0.0084,  0.1147,  ..., -0.0063,  0.0109, -0.0052],
        ...,
        [ 0.0045, -0.0007, -0.0032,  ...,  0.0090,  0.0039,  0.0023],
        [-0.0098,  0.0084,  0.0165,  ..., -0.0022,  0.0149,  0.0076],
        [-0.0066,  0.0055,  0.0072,  ...,  0.0006,  0.0112,  0.0149]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[10.1562, -1.4551, -7.6367,  ..., -0.9473, -6.5586,  9.9766]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 23:41:43 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of interior is exterior
The opposite of outward is upward
The opposite of southeast is southwest
The opposite of input is output
The opposite of ahead is behind
The opposite of inbound is outbound
The opposite of decrement is increment
The opposite of rise is
2024-07-24 23:41:43 root INFO     [order_1_approx] starting weight calculation for The opposite of interior is exterior
The opposite of southeast is southwest
The opposite of ahead is behind
The opposite of input is output
The opposite of decrement is increment
The opposite of outward is upward
The opposite of rise is sink
The opposite of inbound is
2024-07-24 23:41:43 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 23:45:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1562, -2.5469,  5.4688,  ...,  1.6484,  0.3867, -0.0410],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 3.5625,  2.7188, 24.8750,  ..., -0.9180, -3.5781, 21.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-2.5787e-03, -2.5787e-03, -1.2360e-03,  ..., -5.8746e-04,
         -1.3657e-03,  4.6539e-04],
        [ 9.7656e-04, -1.1520e-03,  4.4861e-03,  ...,  8.2397e-04,
          1.0529e-03, -1.6327e-03],
        [ 8.8501e-04,  9.5367e-05,  2.4414e-02,  ..., -3.3760e-04,
         -1.7242e-03,  3.5553e-03],
        ...,
        [ 1.6403e-04, -4.3869e-04, -4.9133e-03,  ...,  4.4060e-04,
          6.3324e-04,  1.0757e-03],
        [ 5.6458e-04,  1.0376e-03,  5.2185e-03,  ..., -8.0109e-05,
          2.1515e-03,  2.6245e-03],
        [ 1.0605e-03, -4.5967e-04,  7.8735e-03,  ...,  1.3351e-04,
         -6.1417e-04,  2.4872e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.8848,  2.7363, 24.6719,  ..., -0.9727, -3.4082, 21.5156]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 23:45:16 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The country of guatemala primarily speaks the language of spanish
The country of cambodia primarily speaks the language of khmer
The country of colombia primarily speaks the language of spanish
The country of usa primarily speaks the language of english
The country of haiti primarily speaks the language of creole
The country of egypt primarily speaks the language of arabic
The country of mexico primarily speaks the language of spanish
The country of ethiopia primarily speaks the language of
2024-07-24 23:45:17 root INFO     [order_1_approx] starting weight calculation for The country of guatemala primarily speaks the language of spanish
The country of usa primarily speaks the language of english
The country of ethiopia primarily speaks the language of amharic
The country of haiti primarily speaks the language of creole
The country of egypt primarily speaks the language of arabic
The country of mexico primarily speaks the language of spanish
The country of colombia primarily speaks the language of spanish
The country of cambodia primarily speaks the language of
2024-07-24 23:45:17 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 23:47:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7812, -1.2578,  3.0938,  ...,  0.0820, -0.4473,  3.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -3.7344, -14.3750,   9.1875,  ...,  -8.3125,  -2.8125,  -1.0312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.3560e-02,  4.7607e-03, -1.5411e-03,  ...,  5.4321e-03,
         -1.3275e-03, -3.6621e-03],
        [ 1.1963e-02, -9.1553e-04, -1.2207e-03,  ..., -3.7231e-03,
          1.5625e-02, -6.9809e-04],
        [-5.7983e-03, -1.3611e-02,  1.3770e-01,  ..., -3.1281e-03,
          1.6724e-02, -6.5002e-03],
        ...,
        [-3.7384e-03,  1.4648e-02, -1.4496e-04,  ..., -8.5449e-04,
         -9.9487e-03,  1.6632e-03],
        [-5.2490e-03,  5.4016e-03,  1.3916e-02,  ..., -4.8828e-03,
          2.5146e-02,  6.7749e-03],
        [-3.2654e-03, -9.9487e-03,  7.2479e-04,  ..., -4.8523e-03,
          3.0518e-05,  1.4954e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-7.7344e+00, -1.2578e+01,  1.0367e+01,  ..., -6.1172e+00,
         -1.1719e-02, -1.9473e+00]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SubBackward0>) 

                    
2024-07-24 23:47:22 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of interior is exterior
The opposite of southeast is southwest
The opposite of ahead is behind
The opposite of input is output
The opposite of decrement is increment
The opposite of outward is upward
The opposite of rise is sink
The opposite of inbound is
2024-07-24 23:47:22 root INFO     [order_1_approx] starting weight calculation for The opposite of inbound is outbound
The opposite of rise is sink
The opposite of input is output
The opposite of ahead is behind
The opposite of decrement is increment
The opposite of outward is upward
The opposite of southeast is southwest
The opposite of interior is
2024-07-24 23:47:22 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 23:51:17 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6211, -2.1094,  3.9688,  ...,  2.6562, -0.7930,  1.5156],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-2.7188,  1.0312, 11.5625,  ...,  0.1562, -7.9688, 23.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-2.1057e-03, -9.5367e-05, -6.4392e-03,  ...,  1.4267e-03,
         -3.4332e-03,  9.3460e-05],
        [ 1.8539e-03, -2.1820e-03,  9.8267e-03,  ..., -6.1035e-04,
         -1.9360e-04, -1.3199e-03],
        [-1.9646e-04, -1.3123e-03,  2.6001e-02,  ..., -8.9264e-04,
         -1.0986e-03, -3.3722e-03],
        ...,
        [ 2.1667e-03,  9.6893e-04,  7.3853e-03,  ..., -4.0817e-04,
          3.0518e-05, -1.6212e-04],
        [ 2.5330e-03,  3.2349e-03, -5.6076e-04,  ..., -1.5259e-04,
          6.7749e-03,  3.6240e-04],
        [-1.0529e-03, -1.4725e-03, -5.4626e-03,  ..., -1.3123e-03,
         -2.8076e-03,  2.9144e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.8223,  0.7676, 11.0156,  ...,  0.2496, -7.9062, 23.4375]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 23:51:18 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The country of guatemala primarily speaks the language of spanish
The country of usa primarily speaks the language of english
The country of ethiopia primarily speaks the language of amharic
The country of haiti primarily speaks the language of creole
The country of egypt primarily speaks the language of arabic
The country of mexico primarily speaks the language of spanish
The country of colombia primarily speaks the language of spanish
The country of cambodia primarily speaks the language of
2024-07-24 23:51:18 root INFO     [order_1_approx] starting weight calculation for The country of colombia primarily speaks the language of spanish
The country of haiti primarily speaks the language of creole
The country of mexico primarily speaks the language of spanish
The country of guatemala primarily speaks the language of spanish
The country of egypt primarily speaks the language of arabic
The country of ethiopia primarily speaks the language of amharic
The country of cambodia primarily speaks the language of khmer
The country of usa primarily speaks the language of
2024-07-24 23:51:19 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 23:53:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.6328, -2.1562,  2.1719,  ...,  0.9609,  0.9766,  2.9062],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  0.9844, -12.5000,  16.8750,  ...,   1.5781,  -3.0312,  -3.6562],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0269,  0.0117, -0.0134,  ...,  0.0049,  0.0121,  0.0054],
        [ 0.0013, -0.0070, -0.0040,  ..., -0.0049,  0.0011, -0.0175],
        [-0.0178, -0.0183,  0.1641,  ..., -0.0082, -0.0031, -0.0039],
        ...,
        [-0.0090,  0.0006,  0.0011,  ..., -0.0025, -0.0089, -0.0063],
        [-0.0079,  0.0065,  0.0154,  ..., -0.0027,  0.0182,  0.0045],
        [-0.0072,  0.0010, -0.0019,  ..., -0.0045,  0.0106,  0.0126]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  0.6533, -14.5234,  18.5938,  ...,   2.5645,  -0.8223,  -3.5078]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 23:53:09 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of inbound is outbound
The opposite of rise is sink
The opposite of input is output
The opposite of ahead is behind
The opposite of decrement is increment
The opposite of outward is upward
The opposite of southeast is southwest
The opposite of interior is
2024-07-24 23:53:09 root INFO     [order_1_approx] starting weight calculation for The opposite of southeast is southwest
The opposite of rise is sink
The opposite of interior is exterior
The opposite of outward is upward
The opposite of decrement is increment
The opposite of inbound is outbound
The opposite of input is output
The opposite of ahead is
2024-07-24 23:53:09 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 23:57:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2031, -1.5156,  6.3750,  ...,  1.0391, -3.2656, -2.6406],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -4.1562, -11.2500,   7.8125,  ...,  -2.4688,   3.3438,   9.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-0.0016,  0.0009, -0.0023,  ...,  0.0041, -0.0063, -0.0057],
        [ 0.0031, -0.0010,  0.0058,  ...,  0.0003,  0.0059, -0.0012],
        [ 0.0010, -0.0020,  0.0311,  ..., -0.0018, -0.0051, -0.0028],
        ...,
        [-0.0025,  0.0024, -0.0053,  ...,  0.0019,  0.0011, -0.0004],
        [-0.0008, -0.0016,  0.0041,  ...,  0.0013,  0.0099,  0.0014],
        [-0.0031, -0.0041, -0.0035,  ...,  0.0021, -0.0026,  0.0081]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -3.3887, -11.2969,   7.8203,  ...,  -2.5645,   2.9648,   8.4453]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 23:57:12 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The country of colombia primarily speaks the language of spanish
The country of haiti primarily speaks the language of creole
The country of mexico primarily speaks the language of spanish
The country of guatemala primarily speaks the language of spanish
The country of egypt primarily speaks the language of arabic
The country of ethiopia primarily speaks the language of amharic
The country of cambodia primarily speaks the language of khmer
The country of usa primarily speaks the language of
2024-07-24 23:57:12 root INFO     [order_1_approx] starting weight calculation for The country of egypt primarily speaks the language of arabic
The country of cambodia primarily speaks the language of khmer
The country of haiti primarily speaks the language of creole
The country of guatemala primarily speaks the language of spanish
The country of usa primarily speaks the language of english
The country of colombia primarily speaks the language of spanish
The country of ethiopia primarily speaks the language of amharic
The country of mexico primarily speaks the language of
2024-07-24 23:57:12 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-24 23:58:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.2656, -2.6719, -0.6992,  ...,  1.8438,  0.5156,  1.1016],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -6.9375,  -4.4375,  11.2500,  ..., -11.5000,   0.2461,  -6.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0063,  0.0050,  0.0221,  ...,  0.0008,  0.0012, -0.0036],
        [-0.0009,  0.0043,  0.0040,  ..., -0.0015,  0.0126, -0.0115],
        [-0.0067,  0.0034,  0.1235,  ...,  0.0054,  0.0122, -0.0132],
        ...,
        [ 0.0069,  0.0052, -0.0068,  ..., -0.0005, -0.0110, -0.0061],
        [ 0.0052, -0.0009,  0.0068,  ...,  0.0030,  0.0170,  0.0002],
        [ 0.0236,  0.0032, -0.0142,  ..., -0.0031, -0.0229,  0.0139]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-7.4141, -6.0625, 12.9688,  ..., -8.1641,  2.1738, -7.0273]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-24 23:58:47 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of southeast is southwest
The opposite of rise is sink
The opposite of interior is exterior
The opposite of outward is upward
The opposite of decrement is increment
The opposite of inbound is outbound
The opposite of input is output
The opposite of ahead is
2024-07-24 23:58:47 root INFO     [order_1_approx] starting weight calculation for The opposite of decrement is increment
The opposite of inbound is outbound
The opposite of interior is exterior
The opposite of southeast is southwest
The opposite of ahead is behind
The opposite of rise is sink
The opposite of outward is upward
The opposite of input is
2024-07-24 23:58:47 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 00:03:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.1250, -2.7500,  6.3125,  ...,  1.9453,  0.3027, -0.1455],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-5.6250,  2.5625,  0.0312,  ..., -1.8828,  3.4844, 14.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.9755e-03, -1.0071e-03, -2.2095e-02,  ...,  1.8311e-04,
         -3.9368e-03,  2.7466e-03],
        [ 5.0049e-03, -1.0071e-03, -2.7954e-02,  ...,  4.1962e-04,
         -5.4016e-03, -4.6997e-03],
        [-6.3477e-03, -2.1667e-03,  4.2480e-02,  ..., -2.7008e-03,
         -1.2054e-03,  6.9275e-03],
        ...,
        [-4.0283e-03,  1.2970e-04, -1.5793e-03,  ...,  1.8120e-04,
         -2.2888e-03, -2.3193e-03],
        [-4.6539e-04, -9.0408e-04,  1.6968e-02,  ..., -1.4954e-03,
          1.2939e-02, -3.9978e-03],
        [-8.4229e-03, -4.3335e-03,  6.7749e-03,  ...,  3.1128e-03,
          4.9591e-05,  7.9346e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.7109,  4.3672, -2.3789,  ..., -2.4883,  2.6191, 13.5781]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 00:03:07 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The country of egypt primarily speaks the language of arabic
The country of cambodia primarily speaks the language of khmer
The country of haiti primarily speaks the language of creole
The country of guatemala primarily speaks the language of spanish
The country of usa primarily speaks the language of english
The country of colombia primarily speaks the language of spanish
The country of ethiopia primarily speaks the language of amharic
The country of mexico primarily speaks the language of
2024-07-25 00:03:07 root INFO     [order_1_approx] starting weight calculation for The country of mexico primarily speaks the language of spanish
The country of guatemala primarily speaks the language of spanish
The country of cambodia primarily speaks the language of khmer
The country of haiti primarily speaks the language of creole
The country of usa primarily speaks the language of english
The country of ethiopia primarily speaks the language of amharic
The country of egypt primarily speaks the language of arabic
The country of colombia primarily speaks the language of
2024-07-25 00:03:07 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 00:04:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.5781, -0.9766,  3.3438,  ...,  0.1797,  0.4707,  2.4375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([11.0000, -6.3750, 15.5000,  ...,  1.2500,  4.6250, -9.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.5513e-02, -3.0518e-03, -7.4158e-03,  ...,  6.7444e-03,
         -1.1963e-02, -7.9346e-03],
        [ 1.7166e-03, -4.7874e-04,  4.3335e-03,  ..., -6.0120e-03,
         -4.7302e-04, -2.7657e-05],
        [-1.3855e-02, -4.1199e-03,  8.5938e-02,  ..., -2.7466e-03,
         -7.5531e-04,  2.7161e-03],
        ...,
        [-1.1673e-03, -8.2397e-04, -7.5378e-03,  ..., -2.2888e-03,
          4.6997e-03,  8.6975e-04],
        [-1.5259e-02,  3.0518e-03,  9.9182e-04,  ..., -8.7891e-03,
          2.4902e-02,  6.3782e-03],
        [-4.9133e-03, -6.1951e-03,  4.1809e-03,  ..., -4.1199e-03,
          9.0942e-03,  1.0620e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 8.7344, -5.3828, 15.0391,  ...,  1.7109,  4.7188, -8.2578]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 00:04:28 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of decrement is increment
The opposite of inbound is outbound
The opposite of interior is exterior
The opposite of southeast is southwest
The opposite of ahead is behind
The opposite of rise is sink
The opposite of outward is upward
The opposite of input is
2024-07-25 00:04:28 root INFO     [order_1_approx] starting weight calculation for The opposite of input is output
The opposite of outward is upward
The opposite of inbound is outbound
The opposite of decrement is increment
The opposite of ahead is behind
The opposite of rise is sink
The opposite of interior is exterior
The opposite of southeast is
2024-07-25 00:04:28 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 00:08:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 4.0000, -0.6250,  5.1562,  ...,  2.1250, -0.0801,  1.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-7.4375, -0.4414,  3.1875,  ...,  1.7422,  4.8125, 14.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-0.0056,  0.0002, -0.0067,  ..., -0.0010, -0.0030, -0.0036],
        [-0.0046, -0.0033, -0.0139,  ..., -0.0003,  0.0005, -0.0017],
        [ 0.0031, -0.0031,  0.0332,  ..., -0.0056, -0.0079,  0.0081],
        ...,
        [-0.0061,  0.0070,  0.0050,  ...,  0.0015, -0.0067, -0.0069],
        [ 0.0054, -0.0016,  0.0121,  ..., -0.0020,  0.0093,  0.0015],
        [-0.0032, -0.0076,  0.0037,  ..., -0.0009, -0.0058,  0.0129]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.5703, -1.0801, -0.9766,  ...,  3.3086,  2.8438, 10.1094]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 00:09:00 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The country of mexico primarily speaks the language of spanish
The country of guatemala primarily speaks the language of spanish
The country of cambodia primarily speaks the language of khmer
The country of haiti primarily speaks the language of creole
The country of usa primarily speaks the language of english
The country of ethiopia primarily speaks the language of amharic
The country of egypt primarily speaks the language of arabic
The country of colombia primarily speaks the language of
2024-07-25 00:09:00 root INFO     [order_1_approx] starting weight calculation for The country of ethiopia primarily speaks the language of amharic
The country of cambodia primarily speaks the language of khmer
The country of haiti primarily speaks the language of creole
The country of colombia primarily speaks the language of spanish
The country of mexico primarily speaks the language of spanish
The country of usa primarily speaks the language of english
The country of guatemala primarily speaks the language of spanish
The country of egypt primarily speaks the language of
2024-07-25 00:09:00 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 00:10:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9453,  1.3125,  4.3750,  ...,  1.9219, -0.5078,  0.3145],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -9.3125,  -2.2188,   9.4375,  ...,  -8.5000, -10.8750,   5.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0153,  0.0036, -0.0162,  ...,  0.0093,  0.0099, -0.0011],
        [ 0.0064, -0.0044, -0.0154,  ...,  0.0025, -0.0006, -0.0012],
        [ 0.0090,  0.0031,  0.0581,  ..., -0.0086, -0.0017, -0.0003],
        ...,
        [-0.0058, -0.0053,  0.0299,  ..., -0.0063,  0.0136,  0.0052],
        [-0.0023,  0.0067,  0.0003,  ..., -0.0007,  0.0215,  0.0135],
        [-0.0040,  0.0010, -0.0124,  ...,  0.0021, -0.0018,  0.0095]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-7.1445,  1.8516, 11.2188,  ..., -9.0781, -7.9531,  5.5195]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 00:10:15 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of input is output
The opposite of outward is upward
The opposite of inbound is outbound
The opposite of decrement is increment
The opposite of ahead is behind
The opposite of rise is sink
The opposite of interior is exterior
The opposite of southeast is
2024-07-25 00:10:15 root INFO     total operator prediction time: 2730.7701852321625 seconds
2024-07-25 00:10:15 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on meronyms - member
2024-07-25 00:10:15 root INFO     building operator meronyms - member
2024-07-25 00:10:15 root INFO     [order_1_approx] starting weight calculation for A cow is a member of a herd
A fish is a member of a school
A book is a member of a library
A sheep is a member of a flock
A antelope is a member of a herd
A shrub is a member of a shrubbery
A cattle is a member of a herd
A calf is a member of a
2024-07-25 00:10:16 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 00:14:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5469, -0.8750,  4.1875,  ...,  2.2500, -0.0234, -0.5938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 0.2148, -5.0938, 17.5000,  ...,  7.6875, -4.8750, 16.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0029, -0.0049, -0.0039,  ...,  0.0026, -0.0038,  0.0035],
        [-0.0060, -0.0041, -0.0049,  ...,  0.0025,  0.0061,  0.0051],
        [-0.0014,  0.0006,  0.0547,  ..., -0.0036, -0.0006, -0.0051],
        ...,
        [-0.0013,  0.0036, -0.0025,  ..., -0.0026, -0.0045,  0.0032],
        [ 0.0087,  0.0031,  0.0129,  ...,  0.0013,  0.0070,  0.0016],
        [-0.0078, -0.0017, -0.0128,  ...,  0.0013,  0.0044,  0.0043]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.6626, -5.2773, 17.9375,  ...,  8.4141, -5.3828, 16.1875]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 00:14:55 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The country of ethiopia primarily speaks the language of amharic
The country of cambodia primarily speaks the language of khmer
The country of haiti primarily speaks the language of creole
The country of colombia primarily speaks the language of spanish
The country of mexico primarily speaks the language of spanish
The country of usa primarily speaks the language of english
The country of guatemala primarily speaks the language of spanish
The country of egypt primarily speaks the language of
2024-07-25 00:14:56 root INFO     [order_1_approx] starting weight calculation for The country of mexico primarily speaks the language of spanish
The country of ethiopia primarily speaks the language of amharic
The country of usa primarily speaks the language of english
The country of guatemala primarily speaks the language of spanish
The country of colombia primarily speaks the language of spanish
The country of cambodia primarily speaks the language of khmer
The country of egypt primarily speaks the language of arabic
The country of haiti primarily speaks the language of
2024-07-25 00:14:56 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 00:15:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8047, -2.0156,  0.7344,  ...,  3.0625, -2.4688,  4.0312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 12.5000,   9.1250,   2.8906,  ...,  -7.4375,  -5.8750, -12.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-6.3705e-04, -3.1891e-03,  1.4587e-02,  ..., -3.0823e-03,
          3.4180e-03,  1.0010e-02],
        [ 9.4414e-05,  3.8910e-04, -1.6724e-02,  ...,  7.1716e-04,
         -6.0120e-03,  8.0566e-03],
        [ 4.6997e-03,  6.1035e-04,  8.6426e-02,  ..., -7.4158e-03,
          5.3406e-03, -4.1199e-03],
        ...,
        [-4.4861e-03,  7.0190e-03,  1.3428e-03,  ..., -1.0529e-03,
          4.0588e-03,  4.7913e-03],
        [-2.1210e-03, -4.3945e-03,  1.0315e-02,  ..., -1.9073e-04,
          1.7944e-02, -1.0315e-02],
        [-2.6855e-03,  9.2316e-04, -1.3184e-02,  ...,  3.1738e-03,
         -9.0027e-04,  2.0264e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 13.2500,   9.3906,   2.8320,  ...,  -7.6016,  -6.5391, -13.0859]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 00:15:57 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A cow is a member of a herd
A fish is a member of a school
A book is a member of a library
A sheep is a member of a flock
A antelope is a member of a herd
A shrub is a member of a shrubbery
A cattle is a member of a herd
A calf is a member of a
2024-07-25 00:15:57 root INFO     [order_1_approx] starting weight calculation for A fish is a member of a school
A calf is a member of a cattle
A book is a member of a library
A cattle is a member of a herd
A sheep is a member of a flock
A shrub is a member of a shrubbery
A cow is a member of a herd
A antelope is a member of a
2024-07-25 00:15:57 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 00:20:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.2500, -2.5312,  7.0938,  ...,  5.1250, -2.2812,  2.8125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-5.6875, -3.2656, 20.7500,  ...,  6.5625,  5.7500, 21.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-4.8218e-03, -2.3346e-03, -1.7090e-02,  ..., -2.9144e-03,
         -1.2634e-02, -4.0894e-03],
        [ 3.1281e-03, -6.6223e-03, -9.0332e-03,  ..., -9.5367e-05,
          7.9956e-03,  3.1738e-03],
        [-4.3488e-04, -8.2397e-04,  3.2227e-02,  ..., -4.3030e-03,
         -4.3030e-03, -1.0681e-03],
        ...,
        [-6.8054e-03, -4.7302e-04, -1.1673e-03,  ...,  2.9182e-04,
         -7.9346e-03, -2.0447e-03],
        [ 4.8218e-03,  5.0354e-04,  5.1575e-03,  ..., -2.5330e-03,
          1.7456e-02,  6.6528e-03],
        [ 3.3569e-04, -6.5918e-03,  1.8234e-03,  ..., -2.2736e-03,
         -4.3945e-03,  1.4648e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-6.6641, -3.4160, 19.2969,  ...,  6.1016,  5.3047, 21.5312]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 00:20:53 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The country of mexico primarily speaks the language of spanish
The country of ethiopia primarily speaks the language of amharic
The country of usa primarily speaks the language of english
The country of guatemala primarily speaks the language of spanish
The country of colombia primarily speaks the language of spanish
The country of cambodia primarily speaks the language of khmer
The country of egypt primarily speaks the language of arabic
The country of haiti primarily speaks the language of
2024-07-25 00:20:53 root INFO     total operator prediction time: 2827.9786636829376 seconds
2024-07-25 00:20:53 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on name - nationality
2024-07-25 00:20:53 root INFO     building operator name - nationality
2024-07-25 00:20:53 root INFO     [order_1_approx] starting weight calculation for marx was german
lavoisier was french
jolie was american
rousseau was french
einstein was jewish
tolstoi was russian
mozart was german
mencius was
2024-07-25 00:20:53 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 00:21:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.3281,  0.4941,  2.5938,  ..., -0.4453, -2.3594, -0.8984],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -4.5938,   2.3281,   4.3438,  ..., -16.8750,   6.3750, -13.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0096, -0.0031, -0.0306,  ..., -0.0052, -0.0060,  0.0060],
        [ 0.0139,  0.0100, -0.0337,  ..., -0.0053, -0.0080,  0.0047],
        [ 0.0073, -0.0040,  0.0859,  ..., -0.0052, -0.0040,  0.0036],
        ...,
        [-0.0113,  0.0118,  0.0117,  ...,  0.0023,  0.0029, -0.0032],
        [-0.0128,  0.0045,  0.0127,  ...,  0.0015,  0.0273, -0.0090],
        [ 0.0009, -0.0020, -0.0272,  ...,  0.0005,  0.0013,  0.0271]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -5.9062,   3.2578,   3.8203,  ..., -16.2344,   6.7344, -13.1875]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 00:21:44 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A fish is a member of a school
A calf is a member of a cattle
A book is a member of a library
A cattle is a member of a herd
A sheep is a member of a flock
A shrub is a member of a shrubbery
A cow is a member of a herd
A antelope is a member of a
2024-07-25 00:21:44 root INFO     [order_1_approx] starting weight calculation for A antelope is a member of a herd
A sheep is a member of a flock
A shrub is a member of a shrubbery
A calf is a member of a cattle
A cattle is a member of a herd
A book is a member of a library
A fish is a member of a school
A cow is a member of a
2024-07-25 00:21:44 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 00:26:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-4.7812, -2.5781,  5.5000,  ...,  0.9844, -2.1562,  0.7109],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-21.8750,  -3.4688,  15.2500,  ...,   6.1875,   0.4316,   0.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-3.3569e-03, -6.7139e-03, -1.7822e-02,  ..., -2.6703e-03,
         -6.1035e-03, -1.0071e-03],
        [-5.5542e-03, -1.0300e-03, -4.8218e-03,  ...,  4.8523e-03,
          3.1281e-03,  2.1973e-03],
        [-2.7771e-03,  4.0054e-04,  1.0645e-01,  ..., -4.9591e-04,
         -4.0283e-03,  4.3640e-03],
        ...,
        [ 3.6163e-03,  7.2479e-05, -1.0010e-02,  ...,  9.3384e-03,
         -5.3101e-03,  5.5237e-03],
        [-1.9379e-03, -1.8082e-03,  2.6733e-02,  ..., -2.5482e-03,
          2.1729e-02,  4.4250e-03],
        [ 7.0190e-03, -3.9978e-03, -6.2943e-05,  ...,  3.1738e-03,
          8.2397e-03,  1.4160e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.0969e+01, -3.9766e+00,  1.4883e+01,  ...,  7.4062e+00,
         -4.8828e-03,  1.0322e+00]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SubBackward0>) 

                    
2024-07-25 00:26:46 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for marx was german
lavoisier was french
jolie was american
rousseau was french
einstein was jewish
tolstoi was russian
mozart was german
mencius was
2024-07-25 00:26:46 root INFO     [order_1_approx] starting weight calculation for rousseau was french
mencius was chinese
mozart was german
lavoisier was french
marx was german
tolstoi was russian
einstein was jewish
jolie was
2024-07-25 00:26:46 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 00:27:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 4.3750, -3.6406,  1.9297,  ...,  2.3125, -1.1875,  2.1094],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  9.1250,   4.5938,   0.7812,  ..., -11.6250, -14.5625, -10.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 5.7068e-03,  4.3335e-03, -1.7395e-03,  ...,  3.0212e-03,
         -1.4343e-03,  8.1787e-03],
        [ 3.5095e-04, -5.7697e-05, -1.2817e-02,  ...,  8.8501e-04,
          4.3945e-03, -5.7983e-03],
        [ 6.1035e-03,  6.8970e-03,  7.0801e-02,  ..., -6.9580e-03,
          8.7891e-03, -8.6670e-03],
        ...,
        [-1.2451e-02, -2.3041e-03, -4.2725e-03,  ...,  5.2185e-03,
         -5.5847e-03,  7.6904e-03],
        [ 2.2736e-03,  3.8757e-03,  1.9043e-02,  ..., -6.7444e-03,
          2.1729e-02, -7.3853e-03],
        [-3.6011e-03, -2.9755e-03, -2.0630e-02,  ...,  8.5068e-04,
         -6.8359e-03,  1.7700e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  9.0547,   3.6797,  -0.2627,  ..., -11.9531, -15.1328, -10.8906]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 00:27:25 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A antelope is a member of a herd
A sheep is a member of a flock
A shrub is a member of a shrubbery
A calf is a member of a cattle
A cattle is a member of a herd
A book is a member of a library
A fish is a member of a school
A cow is a member of a
2024-07-25 00:27:25 root INFO     [order_1_approx] starting weight calculation for A book is a member of a library
A cow is a member of a herd
A antelope is a member of a herd
A sheep is a member of a flock
A cattle is a member of a herd
A shrub is a member of a shrubbery
A calf is a member of a cattle
A fish is a member of a
2024-07-25 00:27:25 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 00:32:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1953,  1.5859,  8.1875,  ...,  0.3125, -2.6562,  3.1562],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-9.2500, -5.8750, 12.2500,  ...,  2.4062, -1.6562, 13.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-2.6245e-03, -1.7334e-02, -3.6865e-02,  ...,  1.0010e-02,
         -1.0803e-02, -1.8799e-02],
        [-1.5076e-02,  3.8605e-03,  1.1139e-03,  ..., -1.2329e-02,
          3.6133e-02,  2.6733e-02],
        [-6.7749e-03, -1.0834e-03,  1.3281e-01,  ...,  8.2397e-04,
         -5.6458e-03, -3.1250e-02],
        ...,
        [ 1.7395e-03, -1.7212e-02,  4.2725e-03,  ..., -5.0659e-03,
         -1.1841e-02, -6.8665e-03],
        [-9.8877e-03, -1.9150e-03,  3.7598e-02,  ..., -1.6113e-02,
          4.7852e-02, -1.2207e-04],
        [ 1.9653e-02, -6.5308e-03,  4.0771e-02,  ...,  1.7456e-02,
          1.1902e-03,  4.4922e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-9.8828, -5.8633, 13.6953,  ...,  0.2344, -1.2480, 10.1406]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 00:32:39 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for rousseau was french
mencius was chinese
mozart was german
lavoisier was french
marx was german
tolstoi was russian
einstein was jewish
jolie was
2024-07-25 00:32:39 root INFO     [order_1_approx] starting weight calculation for marx was german
jolie was american
lavoisier was french
mencius was chinese
tolstoi was russian
rousseau was french
einstein was jewish
mozart was
2024-07-25 00:32:39 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 00:33:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 3.0938,  0.0273, -0.7852,  ...,  1.2344, -1.1406,  2.6719],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  3.5938,   1.1953, -10.3750,  ...,  -8.4375,   2.2812,   2.9844],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0005,  0.0082,  0.0044,  ..., -0.0015,  0.0001,  0.0024],
        [ 0.0020,  0.0034, -0.0007,  ..., -0.0040,  0.0006,  0.0007],
        [ 0.0017, -0.0023,  0.0500,  ..., -0.0012,  0.0019, -0.0063],
        ...,
        [ 0.0021,  0.0021,  0.0118,  ...,  0.0020,  0.0016,  0.0039],
        [-0.0004,  0.0053,  0.0047,  ...,  0.0052,  0.0128,  0.0038],
        [ 0.0018,  0.0049, -0.0057,  ...,  0.0008,  0.0052,  0.0172]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  2.6895,   1.4961, -10.1328,  ...,  -7.3281,   2.1914,   2.7246]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 00:33:11 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A book is a member of a library
A cow is a member of a herd
A antelope is a member of a herd
A sheep is a member of a flock
A cattle is a member of a herd
A shrub is a member of a shrubbery
A calf is a member of a cattle
A fish is a member of a
2024-07-25 00:33:12 root INFO     [order_1_approx] starting weight calculation for A calf is a member of a cattle
A book is a member of a library
A antelope is a member of a herd
A cow is a member of a herd
A fish is a member of a school
A sheep is a member of a flock
A cattle is a member of a herd
A shrub is a member of a
2024-07-25 00:33:12 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 00:38:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4668, -1.2578,  2.4062,  ...,  0.8633, -2.6875,  1.4375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -9.5000, -10.2500,  12.1250,  ...,  14.2500,   1.6719,  19.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0076,  0.0076, -0.0209,  ..., -0.0056, -0.0024,  0.0003],
        [-0.0048, -0.0017, -0.0075,  ..., -0.0043,  0.0030,  0.0130],
        [-0.0079, -0.0068,  0.0928,  ..., -0.0030, -0.0004, -0.0013],
        ...,
        [ 0.0017,  0.0101, -0.0045,  ...,  0.0052, -0.0029, -0.0035],
        [-0.0033, -0.0139,  0.0422,  ...,  0.0082,  0.0271,  0.0077],
        [ 0.0037,  0.0014, -0.0095,  ..., -0.0032,  0.0017,  0.0210]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -8.3594, -10.4531,  13.5625,  ...,  13.8281,   0.3320,  18.8906]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 00:38:33 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for marx was german
jolie was american
lavoisier was french
mencius was chinese
tolstoi was russian
rousseau was french
einstein was jewish
mozart was
2024-07-25 00:38:34 root INFO     [order_1_approx] starting weight calculation for tolstoi was russian
jolie was american
einstein was jewish
mencius was chinese
rousseau was french
mozart was german
lavoisier was french
marx was
2024-07-25 00:38:34 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 00:38:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.9062,  2.2031,  8.0625,  ...,  0.8125, -5.5000, -1.4453],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 12.9375,  -4.8438,   3.5312,  ...,  -6.0625,   1.5547, -11.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0144,  0.0041,  0.0143,  ...,  0.0037, -0.0002,  0.0052],
        [ 0.0129,  0.0133,  0.0250,  ...,  0.0118, -0.0016, -0.0087],
        [-0.0063, -0.0060,  0.1006,  ..., -0.0099, -0.0084,  0.0048],
        ...,
        [ 0.0002,  0.0020, -0.0073,  ..., -0.0065,  0.0133,  0.0020],
        [-0.0021, -0.0005,  0.0104,  ...,  0.0015,  0.0227,  0.0072],
        [ 0.0053, -0.0007, -0.0153,  ..., -0.0015, -0.0116,  0.0161]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 13.0312,  -5.5977,   4.6602,  ...,  -7.2656,   1.3848, -11.4531]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 00:38:57 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A calf is a member of a cattle
A book is a member of a library
A antelope is a member of a herd
A cow is a member of a herd
A fish is a member of a school
A sheep is a member of a flock
A cattle is a member of a herd
A shrub is a member of a
2024-07-25 00:38:57 root INFO     [order_1_approx] starting weight calculation for A fish is a member of a school
A sheep is a member of a flock
A calf is a member of a cattle
A shrub is a member of a shrubbery
A book is a member of a library
A cow is a member of a herd
A antelope is a member of a herd
A cattle is a member of a
2024-07-25 00:38:57 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 00:44:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 3.6562, -4.7188,  1.3281,  ...,  1.3516, -3.2500,  0.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 10.0000,   9.0000,  -0.9375,  ...,  -6.4375, -11.7500,  -5.8438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0017,  0.0011, -0.0025,  ..., -0.0010, -0.0089,  0.0021],
        [-0.0011,  0.0015, -0.0142,  ...,  0.0017, -0.0084, -0.0011],
        [-0.0047, -0.0002,  0.0544,  ..., -0.0037,  0.0026,  0.0020],
        ...,
        [-0.0014,  0.0039,  0.0051,  ..., -0.0011, -0.0024,  0.0026],
        [-0.0068, -0.0025,  0.0135,  ...,  0.0008,  0.0175,  0.0009],
        [ 0.0027,  0.0011, -0.0028,  ...,  0.0024, -0.0019,  0.0048]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  8.4531,   9.4531,  -2.2617,  ...,  -5.8672, -12.5859,  -5.7734]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 00:44:32 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A fish is a member of a school
A sheep is a member of a flock
A calf is a member of a cattle
A shrub is a member of a shrubbery
A book is a member of a library
A cow is a member of a herd
A antelope is a member of a herd
A cattle is a member of a
2024-07-25 00:44:32 root INFO     [order_1_approx] starting weight calculation for A shrub is a member of a shrubbery
A book is a member of a library
A cattle is a member of a herd
A fish is a member of a school
A cow is a member of a herd
A calf is a member of a cattle
A antelope is a member of a herd
A sheep is a member of a
2024-07-25 00:44:32 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 00:44:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0781, -3.8594,  7.0625,  ...,  0.2891,  2.0938,  1.1797],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -0.9336, -10.5625,  15.0625,  ...,   0.5898,  -6.3750,  17.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0080,  0.0094, -0.0082,  ...,  0.0049,  0.0171, -0.0009],
        [-0.0005,  0.0072, -0.0119,  ...,  0.0002, -0.0026, -0.0080],
        [ 0.0013,  0.0034,  0.0957,  ...,  0.0044,  0.0134, -0.0100],
        ...,
        [-0.0013,  0.0006, -0.0043,  ...,  0.0019,  0.0071,  0.0036],
        [-0.0013,  0.0018,  0.0045,  ..., -0.0087,  0.0121, -0.0031],
        [-0.0069, -0.0003,  0.0050,  ...,  0.0041,  0.0058,  0.0217]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -0.7412, -11.4375,  13.8281,  ...,   1.0312,  -5.8359,  16.1094]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 00:44:33 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for tolstoi was russian
jolie was american
einstein was jewish
mencius was chinese
rousseau was french
mozart was german
lavoisier was french
marx was
2024-07-25 00:44:34 root INFO     [order_1_approx] starting weight calculation for tolstoi was russian
jolie was american
mencius was chinese
rousseau was french
mozart was german
marx was german
einstein was jewish
lavoisier was
2024-07-25 00:44:34 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 00:50:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.1406, -4.4375,  1.6016,  ...,  3.4375, -3.8906,  2.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  9.5625,   7.0000,   1.4375,  ..., -14.0625,  -4.3438,  -8.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 4.1199e-03, -1.5259e-05,  5.8899e-03,  ..., -1.9455e-03,
          6.2256e-03,  3.6621e-03],
        [ 5.4321e-03,  9.6130e-04, -1.2573e-02,  ...,  5.3406e-05,
         -2.3804e-03, -7.5378e-03],
        [-2.6855e-03, -1.8234e-03,  6.9824e-02,  ..., -6.9275e-03,
          2.6093e-03, -5.0964e-03],
        ...,
        [-2.5330e-03,  1.0147e-03,  2.2888e-03,  ...,  1.3161e-04,
          2.9297e-03,  6.8970e-03],
        [ 1.1921e-04,  1.6785e-03, -2.1515e-03,  ..., -1.9836e-03,
          1.3000e-02, -6.0425e-03],
        [-5.4932e-03, -2.0294e-03, -1.2573e-02,  ..., -4.7302e-04,
         -3.3569e-03,  1.4099e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  8.2109,   7.6445,   1.0537,  ..., -14.3203,  -4.5742,  -7.5781]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 00:50:13 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A shrub is a member of a shrubbery
A book is a member of a library
A cattle is a member of a herd
A fish is a member of a school
A cow is a member of a herd
A calf is a member of a cattle
A antelope is a member of a herd
A sheep is a member of a
2024-07-25 00:50:13 root INFO     [order_1_approx] starting weight calculation for A antelope is a member of a herd
A cattle is a member of a herd
A cow is a member of a herd
A calf is a member of a cattle
A shrub is a member of a shrubbery
A fish is a member of a school
A sheep is a member of a flock
A book is a member of a
2024-07-25 00:50:13 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 00:50:26 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-2.4062, -1.5781,  2.7500,  ..., -0.8711,  0.2656,  1.9922],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-11.7500,  -2.2656,   9.6250,  ...,   4.0312,   2.0156,  11.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0084,  0.0004, -0.0010,  ...,  0.0010,  0.0073, -0.0059],
        [ 0.0072,  0.0081,  0.0024,  ...,  0.0002, -0.0031, -0.0015],
        [-0.0014, -0.0029,  0.1177,  ..., -0.0096, -0.0106, -0.0222],
        ...,
        [ 0.0050, -0.0023, -0.0095,  ...,  0.0074, -0.0035,  0.0025],
        [ 0.0067, -0.0029,  0.0032,  ...,  0.0055,  0.0347,  0.0052],
        [ 0.0065, -0.0093, -0.0064,  ...,  0.0113, -0.0051,  0.0223]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-10.7031,  -3.1953,  10.5625,  ...,   4.2148,   2.7227,  10.6719]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 00:50:26 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for tolstoi was russian
jolie was american
mencius was chinese
rousseau was french
mozart was german
marx was german
einstein was jewish
lavoisier was
2024-07-25 00:50:27 root INFO     [order_1_approx] starting weight calculation for lavoisier was french
mencius was chinese
marx was german
tolstoi was russian
jolie was american
mozart was german
einstein was jewish
rousseau was
2024-07-25 00:50:27 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 00:55:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 3.1250, -0.1211,  3.6562,  ...,  1.4531, -0.8359, -0.8242],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 9.0000,  0.9180,  8.8750,  ...,  4.0000, -6.5938,  3.6094],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.2939e-02, -3.2806e-03,  7.9346e-03,  ..., -1.2207e-03,
         -1.3504e-03, -2.6550e-03],
        [-2.4567e-03,  8.3008e-03,  4.9133e-03,  ..., -4.5395e-04,
         -1.0742e-02,  6.6223e-03],
        [-2.5787e-03,  2.7161e-03,  5.3467e-02,  ...,  2.3556e-04,
          2.8992e-03, -7.5531e-04],
        ...,
        [-7.0190e-04, -4.3030e-03,  4.1199e-03,  ...,  5.6076e-04,
          6.6223e-03, -5.7068e-03],
        [-2.2125e-03, -2.1515e-03,  5.2490e-03,  ..., -8.0109e-05,
          1.9287e-02, -4.6692e-03],
        [ 5.7373e-03, -2.8839e-03, -7.5378e-03,  ...,  1.0986e-03,
         -3.6621e-03,  1.3489e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 8.5000,  0.4934,  8.6641,  ...,  4.1562, -6.0273,  4.8711]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 00:55:53 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for A antelope is a member of a herd
A cattle is a member of a herd
A cow is a member of a herd
A calf is a member of a cattle
A shrub is a member of a shrubbery
A fish is a member of a school
A sheep is a member of a flock
A book is a member of a
2024-07-25 00:55:53 root INFO     total operator prediction time: 2737.9624783992767 seconds
2024-07-25 00:55:53 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on noun - plural_irreg
2024-07-25 00:55:53 root INFO     building operator noun - plural_irreg
2024-07-25 00:55:53 root INFO     [order_1_approx] starting weight calculation for The plural form of life is lives
The plural form of county is counties
The plural form of family is families
The plural form of security is securities
The plural form of library is libraries
The plural form of wife is wives
The plural form of facility is facilities
The plural form of strategy is
2024-07-25 00:55:54 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 00:56:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8672, -1.3516,  3.6719,  ...,  0.8203, -4.8125, -3.6719],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-10.0000,  -0.6875,  17.2500,  ...,   0.2461,   1.1406,  10.4375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.7334e-02,  3.5095e-03, -2.0264e-02,  ..., -3.2349e-03,
         -6.9580e-03, -4.5166e-03],
        [ 1.3351e-04,  5.4321e-03, -1.1780e-02,  ...,  1.1444e-03,
         -5.5542e-03,  3.2043e-03],
        [ 4.9591e-05, -4.8523e-03,  1.1377e-01,  ...,  3.2043e-03,
          7.8125e-03, -2.8839e-03],
        ...,
        [ 4.3106e-04, -1.3351e-04, -9.1553e-03,  ...,  3.8300e-03,
         -7.3853e-03, -3.4637e-03],
        [ 3.0975e-03, -9.1553e-05,  1.0193e-02,  ..., -3.5095e-03,
          2.6001e-02, -4.3335e-03],
        [-6.0730e-03, -1.2207e-03,  2.2697e-04,  ...,  5.9814e-03,
         -4.9438e-03,  1.4771e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-10.3906,  -0.4463,  16.3281,  ...,   0.4648,   1.3945,  11.2812]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 00:56:14 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for lavoisier was french
mencius was chinese
marx was german
tolstoi was russian
jolie was american
mozart was german
einstein was jewish
rousseau was
2024-07-25 00:56:15 root INFO     [order_1_approx] starting weight calculation for einstein was jewish
jolie was american
mozart was german
marx was german
lavoisier was french
rousseau was french
mencius was chinese
tolstoi was
2024-07-25 00:56:15 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 01:01:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.4844, -0.8945,  3.5000,  ...,  1.2109, -0.2422,  2.1094],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([11.0000, -0.7812,  1.9375,  ..., -9.6875,  0.4219, -9.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.3956e-03,  7.0801e-03, -2.3956e-03,  ..., -3.8147e-03,
          7.5989e-03, -2.8687e-03],
        [ 1.7548e-03,  1.4801e-03,  1.7853e-03,  ..., -5.4626e-03,
          2.8839e-03, -2.8076e-03],
        [-6.4850e-04,  6.1646e-03,  7.0312e-02,  ..., -7.3853e-03,
         -1.3123e-03, -1.7853e-03],
        ...,
        [ 5.8594e-03, -1.8311e-04,  5.4016e-03,  ...,  2.3651e-03,
         -6.4087e-03,  3.3569e-03],
        [ 9.9945e-04,  5.4321e-03, -4.5776e-05,  ..., -3.3951e-04,
          5.0964e-03,  3.8147e-03],
        [ 4.2725e-03, -6.8054e-03,  4.5776e-03,  ...,  2.7924e-03,
         -5.9509e-04,  1.3184e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[10.3438, -1.6592,  0.4170,  ..., -9.3828,  0.7383, -9.6719]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 01:01:37 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of life is lives
The plural form of county is counties
The plural form of family is families
The plural form of security is securities
The plural form of library is libraries
The plural form of wife is wives
The plural form of facility is facilities
The plural form of strategy is
2024-07-25 01:01:37 root INFO     [order_1_approx] starting weight calculation for The plural form of county is counties
The plural form of strategy is strategies
The plural form of facility is facilities
The plural form of family is families
The plural form of life is lives
The plural form of library is libraries
The plural form of wife is wives
The plural form of security is
2024-07-25 01:01:37 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 01:02:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 7.5625,  1.3203,  9.1250,  ..., -0.5156, -1.2734,  0.5938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 0.7266, -9.2500, 10.5625,  ..., -2.9375,  8.0625,  5.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0030,  0.0074, -0.0074,  ...,  0.0010,  0.0023, -0.0029],
        [-0.0076,  0.0027,  0.0116,  ...,  0.0039,  0.0062,  0.0102],
        [-0.0115, -0.0045,  0.0938,  ...,  0.0325,  0.0049,  0.0117],
        ...,
        [-0.0098, -0.0059,  0.0167,  ...,  0.0347,  0.0081,  0.0134],
        [ 0.0005,  0.0032,  0.0059,  ..., -0.0102,  0.0205, -0.0067],
        [-0.0112, -0.0070,  0.0164,  ...,  0.0186,  0.0056,  0.0209]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7778, -7.8281, 15.1094,  ...,  1.8086,  6.5430,  7.2539]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 01:02:13 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for einstein was jewish
jolie was american
mozart was german
marx was german
lavoisier was french
rousseau was french
mencius was chinese
tolstoi was
2024-07-25 01:02:13 root INFO     [order_1_approx] starting weight calculation for jolie was american
lavoisier was french
tolstoi was russian
mencius was chinese
marx was german
rousseau was french
mozart was german
einstein was
2024-07-25 01:02:13 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 01:07:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9922, -1.9062, -0.0918,  ...,  2.3281,  0.8516, -1.4062],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  1.6406,   3.1406,   1.0859,  ...,  -0.5234,   5.0312, -10.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0090,  0.0019, -0.0011,  ...,  0.0007,  0.0030, -0.0034],
        [ 0.0049, -0.0057,  0.0004,  ...,  0.0015,  0.0008, -0.0044],
        [-0.0084,  0.0012,  0.0728,  ..., -0.0004, -0.0010, -0.0012],
        ...,
        [ 0.0020,  0.0017,  0.0085,  ..., -0.0015, -0.0099,  0.0016],
        [ 0.0002,  0.0002, -0.0019,  ...,  0.0025,  0.0177, -0.0002],
        [-0.0010, -0.0023,  0.0060,  ..., -0.0018,  0.0068,  0.0113]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  2.4316,   4.3477,   1.2363,  ...,  -1.2705,   7.1836, -10.0156]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 01:07:34 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of county is counties
The plural form of strategy is strategies
The plural form of facility is facilities
The plural form of family is families
The plural form of life is lives
The plural form of library is libraries
The plural form of wife is wives
The plural form of security is
2024-07-25 01:07:34 root INFO     [order_1_approx] starting weight calculation for The plural form of strategy is strategies
The plural form of family is families
The plural form of life is lives
The plural form of county is counties
The plural form of facility is facilities
The plural form of wife is wives
The plural form of security is securities
The plural form of library is
2024-07-25 01:07:35 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 01:08:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.3047,  2.6875,  3.3750,  ...,  1.7188,  1.3203, -0.9297],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 1.5000, -8.1250, 10.1250,  ..., -3.6875, -1.7344,  9.4375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 4.4250e-03,  3.9978e-03, -6.3171e-03,  ...,  3.8757e-03,
          2.6245e-03, -3.3112e-03],
        [ 1.2054e-03,  1.0376e-02,  2.9297e-03,  ...,  1.2741e-03,
          1.7822e-02,  3.4027e-03],
        [-6.9580e-03, -3.6621e-04,  9.1797e-02,  ...,  6.5308e-03,
          3.5248e-03, -4.3945e-03],
        ...,
        [-4.9744e-03,  3.2501e-03, -1.6846e-02,  ...,  6.8665e-05,
         -3.7956e-04, -4.6387e-03],
        [-4.8828e-03, -7.1716e-04,  1.8433e-02,  ..., -5.8746e-04,
          1.9531e-02, -1.3275e-03],
        [-5.8594e-03,  7.7057e-04,  1.9836e-03,  ..., -2.1057e-03,
          1.2741e-03,  9.6436e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9023, -6.6523, 10.9531,  ..., -3.3633, -0.6191,  8.3906]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 01:08:15 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for jolie was american
lavoisier was french
tolstoi was russian
mencius was chinese
marx was german
rousseau was french
mozart was german
einstein was
2024-07-25 01:08:15 root INFO     total operator prediction time: 2842.089135169983 seconds
2024-07-25 01:08:15 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on UK_city - county
2024-07-25 01:08:15 root INFO     building operator UK_city - county
2024-07-25 01:08:15 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of glasgow is in the county of lowlands
In the United Kingdom, the city of wolverhampton is in the county of midlands
In the United Kingdom, the city of newport is in the county of gwent
In the United Kingdom, the city of carlisle is in the county of cumbria
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of cardiff is in the county of
2024-07-25 01:08:15 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 01:13:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.9844, -1.2969, -3.0000,  ...,  1.1484,  0.0947,  0.8984],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 6.5000,  0.5234, -3.7500,  ..., -4.4688, -0.1797, -3.7344],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0016,  0.0095, -0.0330,  ..., -0.0011,  0.0043, -0.0063],
        [ 0.0037, -0.0100,  0.0405,  ...,  0.0053, -0.0082,  0.0027],
        [-0.0067,  0.0125,  0.0361,  ..., -0.0064,  0.0106, -0.0117],
        ...,
        [-0.0015,  0.0096, -0.0270,  ..., -0.0020, -0.0016,  0.0026],
        [-0.0035,  0.0034, -0.0030,  ...,  0.0005,  0.0156, -0.0024],
        [-0.0010,  0.0035, -0.0064,  ..., -0.0044,  0.0037,  0.0089]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.9922, -2.1973, -2.0352,  ..., -2.6582,  1.2051, -2.9219]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 01:13:20 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of strategy is strategies
The plural form of family is families
The plural form of life is lives
The plural form of county is counties
The plural form of facility is facilities
The plural form of wife is wives
The plural form of security is securities
The plural form of library is
2024-07-25 01:13:21 root INFO     [order_1_approx] starting weight calculation for The plural form of library is libraries
The plural form of life is lives
The plural form of security is securities
The plural form of county is counties
The plural form of wife is wives
The plural form of family is families
The plural form of strategy is strategies
The plural form of facility is
2024-07-25 01:13:21 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 01:14:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6719, -0.3203,  7.3438,  ...,  1.7109, -0.3125,  0.1084],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-23.8750, -11.3125,   2.1250,  ...,   2.6406,  -1.0938,  25.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 8.9111e-03, -1.4725e-03,  1.3611e-02,  ...,  3.1738e-03,
         -7.2327e-03, -3.1433e-03],
        [ 4.5776e-05,  2.3041e-03, -8.6670e-03,  ..., -5.1880e-03,
         -5.3101e-03, -6.1035e-03],
        [ 1.1139e-03,  5.2261e-04,  4.8096e-02,  ..., -2.0905e-03,
          1.8768e-03,  1.2741e-03],
        ...,
        [ 2.9297e-03,  2.9449e-03,  1.0620e-02,  ..., -7.6294e-06,
         -5.8594e-03,  7.9956e-03],
        [-4.2114e-03, -3.7231e-03, -1.5320e-02,  ..., -2.2888e-03,
          6.6833e-03,  4.0894e-03],
        [-7.5073e-03, -4.5471e-03, -6.4392e-03,  ...,  4.5166e-03,
          1.1658e-02,  9.7046e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-22.5469, -10.8203,   2.0117,  ...,   3.0625,  -0.7666,  24.0312]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 01:14:21 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of glasgow is in the county of lowlands
In the United Kingdom, the city of wolverhampton is in the county of midlands
In the United Kingdom, the city of newport is in the county of gwent
In the United Kingdom, the city of carlisle is in the county of cumbria
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of cardiff is in the county of
2024-07-25 01:14:21 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of glasgow is in the county of lowlands
In the United Kingdom, the city of carlisle is in the county of cumbria
In the United Kingdom, the city of cardiff is in the county of glamorgan
In the United Kingdom, the city of wolverhampton is in the county of midlands
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of newport is in the county of
2024-07-25 01:14:21 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 01:19:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0859, -1.3125,  0.4102,  ...,  1.9531,  0.2812, -1.5312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 3.7812, -8.3750, -6.3750,  ...,  3.7031, -0.4297, -3.9688],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.1250e-02,  2.6703e-03, -2.1851e-02,  ...,  6.8054e-03,
          7.7820e-03, -2.0386e-02],
        [-1.2390e-02, -9.8419e-04,  3.7598e-02,  ..., -1.1841e-02,
          4.2114e-03,  3.2715e-02],
        [-1.3367e-02,  4.9438e-03,  9.9121e-02,  ..., -9.5825e-03,
          5.2795e-03,  7.3242e-03],
        ...,
        [ 1.4465e-02,  2.5330e-03, -1.9897e-02,  ...,  7.9346e-03,
          5.4932e-03, -1.6357e-02],
        [-2.8534e-03, -1.3275e-03,  9.4604e-03,  ...,  3.0518e-05,
          1.4526e-02,  1.4343e-03],
        [-3.6011e-03,  9.3460e-05, -5.9128e-04,  ..., -5.0354e-03,
          3.4714e-04,  1.4282e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  8.9297, -15.9844, -10.5859,  ...,   9.1562,  -1.8721,  -4.9023]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 01:19:09 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of library is libraries
The plural form of life is lives
The plural form of security is securities
The plural form of county is counties
The plural form of wife is wives
The plural form of family is families
The plural form of strategy is strategies
The plural form of facility is
2024-07-25 01:19:09 root INFO     [order_1_approx] starting weight calculation for The plural form of facility is facilities
The plural form of family is families
The plural form of life is lives
The plural form of library is libraries
The plural form of security is securities
The plural form of strategy is strategies
The plural form of wife is wives
The plural form of county is
2024-07-25 01:19:10 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 01:20:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.1406, -0.7734,  8.5000,  ...,  4.1875, -0.9492, -0.0703],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-23.1250,  -5.8438,   8.8750,  ...,  -0.6406,  -2.5781,  16.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0101, -0.0025,  0.0228,  ...,  0.0023, -0.0097, -0.0018],
        [ 0.0129,  0.0020, -0.0176,  ...,  0.0042, -0.0157,  0.0044],
        [-0.0128,  0.0013,  0.0645,  ..., -0.0162,  0.0176, -0.0087],
        ...,
        [ 0.0013,  0.0064, -0.0046,  ...,  0.0035, -0.0182,  0.0092],
        [-0.0170,  0.0043,  0.0088,  ..., -0.0009,  0.0173,  0.0037],
        [-0.0010, -0.0121, -0.0078,  ...,  0.0025,  0.0061,  0.0085]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-23.2500,  -6.3555,  10.9219,  ...,  -1.7666,  -0.5801,  15.7031]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 01:20:30 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of glasgow is in the county of lowlands
In the United Kingdom, the city of carlisle is in the county of cumbria
In the United Kingdom, the city of cardiff is in the county of glamorgan
In the United Kingdom, the city of wolverhampton is in the county of midlands
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of newport is in the county of
2024-07-25 01:20:30 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of newport is in the county of gwent
In the United Kingdom, the city of wolverhampton is in the county of midlands
In the United Kingdom, the city of carlisle is in the county of cumbria
In the United Kingdom, the city of cardiff is in the county of glamorgan
In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of glasgow is in the county of lowlands
In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of sheffield is in the county of
2024-07-25 01:20:30 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 01:24:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3984, -4.3125,  0.9570,  ...,  2.2188, -1.5234,  2.6562],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 2.5625, -4.3125,  1.5938,  ..., -7.8125, -4.4062, -7.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0149,  0.0043, -0.0004,  ...,  0.0013,  0.0025, -0.0058],
        [ 0.0002, -0.0024,  0.0181,  ..., -0.0031,  0.0097, -0.0053],
        [ 0.0057, -0.0062,  0.0767,  ..., -0.0069,  0.0110, -0.0034],
        ...,
        [-0.0051,  0.0063,  0.0073,  ..., -0.0014, -0.0005,  0.0039],
        [ 0.0012,  0.0084, -0.0073,  ...,  0.0023, -0.0025, -0.0038],
        [-0.0008, -0.0043,  0.0005,  ...,  0.0019,  0.0057,  0.0075]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.2656, -7.0312, -2.0215,  ..., -6.1914, -1.4219, -8.4453]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 01:24:57 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of facility is facilities
The plural form of family is families
The plural form of life is lives
The plural form of library is libraries
The plural form of security is securities
The plural form of strategy is strategies
The plural form of wife is wives
The plural form of county is
2024-07-25 01:24:57 root INFO     [order_1_approx] starting weight calculation for The plural form of family is families
The plural form of library is libraries
The plural form of strategy is strategies
The plural form of life is lives
The plural form of county is counties
The plural form of facility is facilities
The plural form of security is securities
The plural form of wife is
2024-07-25 01:24:57 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 01:26:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2891,  1.0156,  6.9062,  ...,  2.3750, -0.0781,  1.0781],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-7.5000,  1.8672,  4.3125,  ..., -8.0625,  1.4531,  5.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0115,  0.0049,  0.0104,  ...,  0.0016,  0.0067,  0.0036],
        [ 0.0002,  0.0059,  0.0058,  ..., -0.0005,  0.0020, -0.0001],
        [ 0.0058, -0.0007,  0.0481,  ..., -0.0120,  0.0072, -0.0045],
        ...,
        [-0.0083, -0.0024, -0.0083,  ...,  0.0004, -0.0017,  0.0003],
        [-0.0059, -0.0002, -0.0106,  ...,  0.0008, -0.0049,  0.0039],
        [-0.0029, -0.0048, -0.0057,  ...,  0.0007,  0.0006,  0.0021]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-6.3984,  2.2129,  4.7188,  ..., -8.7891,  1.7480,  4.2852]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 01:26:37 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of newport is in the county of gwent
In the United Kingdom, the city of wolverhampton is in the county of midlands
In the United Kingdom, the city of carlisle is in the county of cumbria
In the United Kingdom, the city of cardiff is in the county of glamorgan
In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of glasgow is in the county of lowlands
In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of sheffield is in the county of
2024-07-25 01:26:37 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of carlisle is in the county of cumbria
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of wolverhampton is in the county of midlands
In the United Kingdom, the city of cardiff is in the county of glamorgan
In the United Kingdom, the city of newport is in the county of gwent
In the United Kingdom, the city of glasgow is in the county of
2024-07-25 01:26:38 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 01:30:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6641, -2.6875, -1.7969,  ...,  1.2344,  3.1250, -0.0464],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([12.0000,  3.5625, -5.3750,  ..., -9.6250, 13.8750, -9.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-0.0031,  0.0017,  0.0093,  ..., -0.0114,  0.0074,  0.0064],
        [ 0.0051,  0.0081,  0.0168,  ..., -0.0014, -0.0038, -0.0111],
        [ 0.0064,  0.0110,  0.0845,  ...,  0.0054, -0.0021, -0.0286],
        ...,
        [ 0.0025,  0.0035,  0.0059,  ..., -0.0116, -0.0020,  0.0110],
        [-0.0026,  0.0012, -0.0036,  ..., -0.0038,  0.0161,  0.0139],
        [ 0.0015, -0.0045, -0.0022,  ...,  0.0048,  0.0048,  0.0048]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  9.0469,   3.9238,  -2.4258,  ...,  -8.0000,  14.6328, -10.2031]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 01:30:42 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of family is families
The plural form of library is libraries
The plural form of strategy is strategies
The plural form of life is lives
The plural form of county is counties
The plural form of facility is facilities
The plural form of security is securities
The plural form of wife is
2024-07-25 01:30:42 root INFO     [order_1_approx] starting weight calculation for The plural form of strategy is strategies
The plural form of facility is facilities
The plural form of library is libraries
The plural form of family is families
The plural form of security is securities
The plural form of wife is wives
The plural form of county is counties
The plural form of life is
2024-07-25 01:30:43 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 01:32:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0469, -2.6562,  5.3125,  ...,  1.4375,  0.4219, -0.3887],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-17.7500, -17.7500,  10.3750,  ...,   0.8359,   4.6562,  22.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0065,  0.0016, -0.0064,  ..., -0.0004, -0.0007,  0.0029],
        [ 0.0010,  0.0004, -0.0009,  ..., -0.0023, -0.0034, -0.0038],
        [ 0.0059, -0.0017,  0.0277,  ...,  0.0004,  0.0020, -0.0028],
        ...,
        [ 0.0011,  0.0022, -0.0019,  ...,  0.0027,  0.0062,  0.0039],
        [ 0.0029,  0.0016, -0.0033,  ..., -0.0041,  0.0039,  0.0074],
        [-0.0046, -0.0018,  0.0054,  ...,  0.0009,  0.0036,  0.0068]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-17.1562, -16.6562,  11.1328,  ...,   0.2490,   4.8398,  23.0781]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 01:32:38 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of carlisle is in the county of cumbria
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of wolverhampton is in the county of midlands
In the United Kingdom, the city of cardiff is in the county of glamorgan
In the United Kingdom, the city of newport is in the county of gwent
In the United Kingdom, the city of glasgow is in the county of
2024-07-25 01:32:38 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of glasgow is in the county of lowlands
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of carlisle is in the county of cumbria
In the United Kingdom, the city of newport is in the county of gwent
In the United Kingdom, the city of cardiff is in the county of glamorgan
In the United Kingdom, the city of wolverhampton is in the county of
2024-07-25 01:32:38 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 01:36:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8711, -0.7695,  1.6719,  ...,  2.6562,  1.8594, -0.2734],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 8.4375, -3.9062,  1.2969,  ..., -6.3125,  1.2812, -3.7344],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0099,  0.0084, -0.0259,  ..., -0.0225,  0.0072,  0.0123],
        [-0.0038,  0.0008,  0.0256,  ...,  0.0127, -0.0062, -0.0109],
        [-0.0058, -0.0057,  0.0889,  ...,  0.0094, -0.0028, -0.0198],
        ...,
        [-0.0025,  0.0057, -0.0106,  ..., -0.0102,  0.0060,  0.0087],
        [ 0.0025,  0.0117, -0.0086,  ..., -0.0071,  0.0139,  0.0148],
        [-0.0025,  0.0007,  0.0008,  ..., -0.0019,  0.0058,  0.0131]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.7578, -5.2031,  1.9922,  ..., -4.8047,  1.6992, -3.5215]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 01:36:26 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of strategy is strategies
The plural form of facility is facilities
The plural form of library is libraries
The plural form of family is families
The plural form of security is securities
The plural form of wife is wives
The plural form of county is counties
The plural form of life is
2024-07-25 01:36:26 root INFO     [order_1_approx] starting weight calculation for The plural form of life is lives
The plural form of strategy is strategies
The plural form of wife is wives
The plural form of county is counties
The plural form of security is securities
The plural form of library is libraries
The plural form of facility is facilities
The plural form of family is
2024-07-25 01:36:26 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 01:38:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1523, -0.5586,  1.2266,  ...,  1.7891, -2.3438,  0.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -4.0000,   3.4688,  13.0625,  ..., -11.1250,  -4.7500,   7.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-0.0012,  0.0015,  0.0091,  ..., -0.0018, -0.0015,  0.0078],
        [ 0.0019,  0.0029, -0.0028,  ..., -0.0044, -0.0021, -0.0041],
        [ 0.0020,  0.0034,  0.0464,  ...,  0.0018,  0.0090,  0.0073],
        ...,
        [ 0.0008,  0.0003, -0.0018,  ..., -0.0010,  0.0010,  0.0027],
        [-0.0073, -0.0020,  0.0012,  ...,  0.0019,  0.0074, -0.0018],
        [-0.0062, -0.0050, -0.0038,  ..., -0.0036,  0.0002,  0.0056]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -4.0117,   3.1914,  14.0859,  ..., -10.1953,  -4.8516,   5.5391]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 01:38:38 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of glasgow is in the county of lowlands
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of carlisle is in the county of cumbria
In the United Kingdom, the city of newport is in the county of gwent
In the United Kingdom, the city of cardiff is in the county of glamorgan
In the United Kingdom, the city of wolverhampton is in the county of
2024-07-25 01:38:39 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of wolverhampton is in the county of midlands
In the United Kingdom, the city of newport is in the county of gwent
In the United Kingdom, the city of carlisle is in the county of cumbria
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of cardiff is in the county of glamorgan
In the United Kingdom, the city of glasgow is in the county of lowlands
In the United Kingdom, the city of portsmouth is in the county of
2024-07-25 01:38:39 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 01:42:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1484, -2.5156, -1.3281,  ...,  1.1875,  0.2656,  0.6172],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 5.8750, -6.9062, -8.1875,  ..., -6.0938, -7.5000, -1.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0017,  0.0040, -0.0053,  ..., -0.0029,  0.0038, -0.0025],
        [-0.0038,  0.0012,  0.0143,  ..., -0.0062, -0.0117,  0.0024],
        [-0.0015,  0.0007,  0.0562,  ..., -0.0101, -0.0062, -0.0025],
        ...,
        [ 0.0027,  0.0099, -0.0078,  ..., -0.0015,  0.0042,  0.0033],
        [-0.0009,  0.0029, -0.0041,  ...,  0.0048,  0.0160, -0.0047],
        [-0.0019,  0.0023,  0.0045,  ..., -0.0038, -0.0038,  0.0090]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.8398, -7.5469, -8.1797,  ..., -6.2070, -7.1523, -0.8193]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 01:42:15 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of life is lives
The plural form of strategy is strategies
The plural form of wife is wives
The plural form of county is counties
The plural form of security is securities
The plural form of library is libraries
The plural form of facility is facilities
The plural form of family is
2024-07-25 01:42:15 root INFO     total operator prediction time: 2781.799460172653 seconds
2024-07-25 01:42:15 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on Ving - verb_inf
2024-07-25 01:42:15 root INFO     building operator Ving - verb_inf
2024-07-25 01:42:15 root INFO     [order_1_approx] starting weight calculation for allowing is the active form of allow
learning is the active form of learn
ensuring is the active form of ensure
losing is the active form of lose
encouraging is the active form of encourage
operating is the active form of operate
avoiding is the active form of avoid
believing is the active form of
2024-07-25 01:42:15 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 01:44:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.8750,  1.0938,  7.2188,  ...,  2.8438, -0.4727, -0.0254],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-9.7500, -0.7422, 13.3750,  ..., -2.8906, -3.0625, 20.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0046, -0.0012,  0.0115,  ...,  0.0002, -0.0025,  0.0033],
        [-0.0017,  0.0036, -0.0197,  ...,  0.0028, -0.0023, -0.0042],
        [-0.0018, -0.0011,  0.0540,  ..., -0.0013,  0.0008, -0.0041],
        ...,
        [-0.0015, -0.0022, -0.0188,  ...,  0.0038,  0.0022,  0.0027],
        [-0.0057,  0.0014,  0.0081,  ...,  0.0006,  0.0072, -0.0021],
        [-0.0014,  0.0012, -0.0060,  ...,  0.0011, -0.0082,  0.0120]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-9.3125, -0.5059, 13.2891,  ..., -3.1719, -1.8008, 23.0625]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 01:44:43 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of wolverhampton is in the county of midlands
In the United Kingdom, the city of newport is in the county of gwent
In the United Kingdom, the city of carlisle is in the county of cumbria
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of cardiff is in the county of glamorgan
In the United Kingdom, the city of glasgow is in the county of lowlands
In the United Kingdom, the city of portsmouth is in the county of
2024-07-25 01:44:44 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of newport is in the county of gwent
In the United Kingdom, the city of glasgow is in the county of lowlands
In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of wolverhampton is in the county of midlands
In the United Kingdom, the city of cardiff is in the county of glamorgan
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of carlisle is in the county of
2024-07-25 01:44:44 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 01:47:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.2812,  1.4453, -1.2812,  ...,  0.2266,  2.1094, -1.1797],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 21.3750,  -1.3125,   4.7500,  ..., -18.6250,  -4.9375,  -0.6016],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-1.1139e-03, -9.9182e-04, -1.5564e-03,  ...,  2.0981e-04,
          2.3651e-03, -7.7057e-04],
        [ 1.6861e-03,  2.3193e-03,  6.7444e-03,  ..., -7.3242e-04,
         -5.7220e-04, -3.1281e-03],
        [-1.8692e-03,  5.6076e-04,  3.4912e-02,  ..., -2.4261e-03,
          1.6937e-03, -1.1368e-03],
        ...,
        [ 2.1362e-04, -3.4332e-04, -6.3171e-03,  ...,  8.8120e-04,
         -5.1117e-04, -2.4796e-04],
        [-2.0447e-03, -4.3297e-04, -1.2283e-03,  ...,  1.6785e-03,
          3.4485e-03, -4.1962e-05],
        [-3.2043e-04,  4.2725e-03,  5.7983e-04,  ...,  6.8665e-04,
         -2.0294e-03,  3.8147e-05]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 20.8281,  -1.9365,   3.7891,  ..., -18.3438,  -5.4492,  -0.1799]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 01:47:58 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for allowing is the active form of allow
learning is the active form of learn
ensuring is the active form of ensure
losing is the active form of lose
encouraging is the active form of encourage
operating is the active form of operate
avoiding is the active form of avoid
believing is the active form of
2024-07-25 01:47:58 root INFO     [order_1_approx] starting weight calculation for operating is the active form of operate
losing is the active form of lose
encouraging is the active form of encourage
allowing is the active form of allow
learning is the active form of learn
believing is the active form of believe
avoiding is the active form of avoid
ensuring is the active form of
2024-07-25 01:47:59 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 01:50:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0254, -0.1172, 10.0000,  ...,  3.6250,  0.7930,  1.1016],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-18.2500,  -2.1250,   8.3750,  ...,   8.6250,  -1.1875,   4.8125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 8.4839e-03,  3.2043e-03, -9.5215e-03,  ..., -2.1210e-03,
         -2.2583e-03,  4.4861e-03],
        [ 1.2436e-03, -5.5542e-03,  1.3367e-02,  ...,  2.5482e-03,
         -2.3499e-03,  2.7924e-03],
        [ 4.3945e-03,  2.6131e-04,  5.0537e-02,  ..., -3.2501e-03,
         -3.4790e-03, -1.8311e-03],
        ...,
        [-6.8665e-05,  4.0283e-03, -6.6833e-03,  ..., -4.1504e-03,
          2.1820e-03,  4.1199e-03],
        [-4.3640e-03,  8.5449e-03, -2.7222e-02,  ..., -1.0986e-03,
          1.7822e-02,  3.3112e-03],
        [-2.5482e-03,  3.5858e-04,  2.0142e-03,  ..., -5.3711e-03,
         -1.9989e-03,  5.0354e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-18.8438,  -2.8672,   8.5625,  ...,   8.9844,  -0.0684,   4.7461]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 01:50:56 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of newport is in the county of gwent
In the United Kingdom, the city of glasgow is in the county of lowlands
In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of salford is in the county of manchester
In the United Kingdom, the city of wolverhampton is in the county of midlands
In the United Kingdom, the city of cardiff is in the county of glamorgan
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of carlisle is in the county of
2024-07-25 01:50:56 root INFO     [order_1_approx] starting weight calculation for In the United Kingdom, the city of newport is in the county of gwent
In the United Kingdom, the city of glasgow is in the county of lowlands
In the United Kingdom, the city of wolverhampton is in the county of midlands
In the United Kingdom, the city of carlisle is in the county of cumbria
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of cardiff is in the county of glamorgan
In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of salford is in the county of
2024-07-25 01:50:57 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 01:53:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0625, -2.7500,  2.4844,  ...,  1.2031, -1.9609, -2.3438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([10.1250, -8.8125,  2.3125,  ..., -6.0312, -9.0625, -4.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0015, -0.0001, -0.0020,  ...,  0.0013,  0.0033,  0.0007],
        [-0.0034, -0.0018, -0.0003,  ..., -0.0007,  0.0012,  0.0014],
        [-0.0009, -0.0027,  0.0505,  ...,  0.0021, -0.0005, -0.0034],
        ...,
        [ 0.0002,  0.0009,  0.0016,  ...,  0.0004, -0.0084, -0.0015],
        [ 0.0007, -0.0023, -0.0008,  ...,  0.0010,  0.0051, -0.0061],
        [ 0.0016,  0.0087,  0.0043,  ...,  0.0019,  0.0025,  0.0026]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 9.5625, -9.8750,  2.7969,  ..., -5.4844, -9.4219, -5.4102]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 01:53:45 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for operating is the active form of operate
losing is the active form of lose
encouraging is the active form of encourage
allowing is the active form of allow
learning is the active form of learn
believing is the active form of believe
avoiding is the active form of avoid
ensuring is the active form of
2024-07-25 01:53:45 root INFO     [order_1_approx] starting weight calculation for losing is the active form of lose
encouraging is the active form of encourage
ensuring is the active form of ensure
avoiding is the active form of avoid
believing is the active form of believe
allowing is the active form of allow
learning is the active form of learn
operating is the active form of
2024-07-25 01:53:45 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 01:57:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6523, -2.7969,  4.3125,  ...,  2.5312,  0.7539, -0.8672],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-1.1562,  8.8750, -0.9609,  ..., -1.0547, -4.0000,  6.9062],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0023,  0.0025,  0.0232,  ...,  0.0020,  0.0095,  0.0021],
        [ 0.0070, -0.0011, -0.0121,  ..., -0.0047, -0.0118,  0.0034],
        [ 0.0015, -0.0032,  0.0398,  ...,  0.0030,  0.0084,  0.0009],
        ...,
        [-0.0030, -0.0013,  0.0008,  ..., -0.0002,  0.0036,  0.0154],
        [-0.0028,  0.0018, -0.0056,  ..., -0.0018,  0.0051,  0.0058],
        [-0.0001, -0.0027, -0.0084,  ..., -0.0014, -0.0023,  0.0065]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.8193,  9.4844, -1.5020,  ..., -0.6172, -2.9492,  6.0430]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 01:57:05 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for In the United Kingdom, the city of newport is in the county of gwent
In the United Kingdom, the city of glasgow is in the county of lowlands
In the United Kingdom, the city of wolverhampton is in the county of midlands
In the United Kingdom, the city of carlisle is in the county of cumbria
In the United Kingdom, the city of sheffield is in the county of yorkshire
In the United Kingdom, the city of cardiff is in the county of glamorgan
In the United Kingdom, the city of portsmouth is in the county of hampshire
In the United Kingdom, the city of salford is in the county of
2024-07-25 01:57:05 root INFO     total operator prediction time: 2929.9136822223663 seconds
2024-07-25 01:57:05 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on verb+ment_irreg
2024-07-25 01:57:05 root INFO     building operator verb+ment_irreg
2024-07-25 01:57:05 root INFO     [order_1_approx] starting weight calculation for To improve results in a improvement
To replace results in a replacement
To enlighten results in a enlightenment
To entitle results in a entitlement
To enforce results in a enforcement
To develop results in a development
To embarrass results in a embarrassment
To arrange results in a
2024-07-25 01:57:05 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 01:59:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0166, -3.0938,  1.0078,  ...,  0.4160, -2.1875,  0.3340],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([17.2500,  7.6250,  1.9375,  ..., -8.4375, -6.0938,  0.4688],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.1368e-03, -5.0354e-03, -8.1253e-04,  ..., -2.6550e-03,
          1.1963e-02,  5.5542e-03],
        [-6.5308e-03,  1.4267e-03,  2.3438e-02,  ...,  3.3379e-05,
          3.5095e-03,  3.0518e-03],
        [-3.2196e-03,  3.5400e-03,  5.5420e-02,  ..., -3.7193e-04,
         -3.4943e-03, -6.5002e-03],
        ...,
        [-1.3428e-03,  3.8757e-03,  1.7395e-03,  ..., -1.5106e-03,
          8.3160e-04, -1.5640e-04],
        [-1.9989e-03,  7.8201e-04, -5.7983e-03,  ...,  2.0599e-03,
          4.2725e-03, -4.3335e-03],
        [ 2.2888e-04,  3.6621e-03,  7.8735e-03,  ...,  2.6245e-03,
          6.4087e-03,  9.2163e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[17.7500,  8.5000,  1.3613,  ..., -6.9492, -7.6875,  1.5703]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 01:59:25 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for losing is the active form of lose
encouraging is the active form of encourage
ensuring is the active form of ensure
avoiding is the active form of avoid
believing is the active form of believe
allowing is the active form of allow
learning is the active form of learn
operating is the active form of
2024-07-25 01:59:25 root INFO     [order_1_approx] starting weight calculation for ensuring is the active form of ensure
encouraging is the active form of encourage
avoiding is the active form of avoid
operating is the active form of operate
believing is the active form of believe
losing is the active form of lose
learning is the active form of learn
allowing is the active form of
2024-07-25 01:59:25 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 02:02:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5234, -3.3750,  2.8125,  ...,  0.0400, -2.2188, -1.2734],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  7.4062,   5.5625, -12.6250,  ...,  -6.1875,  -7.9688,  -2.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 5.9204e-03, -2.1667e-03, -8.8501e-04,  ...,  1.1520e-03,
         -5.3406e-03,  5.5237e-03],
        [ 9.7656e-04,  1.1063e-03,  1.0559e-02,  ..., -1.3199e-03,
         -1.4114e-04,  7.0496e-03],
        [-2.5940e-03, -7.5150e-04,  6.2012e-02,  ..., -1.6098e-03,
         -1.1292e-03,  1.4343e-03],
        ...,
        [ 4.0054e-04, -3.4485e-03, -1.0132e-02,  ...,  1.1063e-03,
         -2.0599e-03,  7.5684e-03],
        [ 4.3030e-03,  8.3923e-05, -4.6082e-03,  ...,  4.6692e-03,
          1.4282e-02, -5.1270e-03],
        [-6.2256e-03,  2.3041e-03, -7.1106e-03,  ..., -5.0354e-03,
         -1.6708e-03,  1.5076e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  5.3359,   5.3281, -10.3828,  ...,  -6.0938,  -8.5156,  -3.2500]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 02:02:57 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To improve results in a improvement
To replace results in a replacement
To enlighten results in a enlightenment
To entitle results in a entitlement
To enforce results in a enforcement
To develop results in a development
To embarrass results in a embarrassment
To arrange results in a
2024-07-25 02:02:57 root INFO     [order_1_approx] starting weight calculation for To improve results in a improvement
To embarrass results in a embarrassment
To arrange results in a arrangement
To entitle results in a entitlement
To enlighten results in a enlightenment
To replace results in a replacement
To enforce results in a enforcement
To develop results in a
2024-07-25 02:02:57 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 02:05:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0156, -2.0781,  1.7812,  ...,  0.5547,  1.4219, -0.6445],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  5.1250,  -0.5938,  -2.8281,  ...,  -9.5625, -12.3125,  -4.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0063, -0.0009, -0.0014,  ..., -0.0005,  0.0078, -0.0049],
        [ 0.0027, -0.0025,  0.0086,  ...,  0.0058,  0.0092,  0.0050],
        [-0.0023, -0.0018,  0.0493,  ..., -0.0044, -0.0007, -0.0079],
        ...,
        [ 0.0014, -0.0005, -0.0081,  ..., -0.0062,  0.0003, -0.0051],
        [ 0.0049,  0.0023, -0.0076,  ...,  0.0003,  0.0024, -0.0085],
        [ 0.0005,  0.0039,  0.0083,  ..., -0.0025,  0.0033, -0.0040]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  5.4219,   0.4697,  -3.5586,  ...,  -9.7109, -14.4688,  -4.6445]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 02:05:15 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for ensuring is the active form of ensure
encouraging is the active form of encourage
avoiding is the active form of avoid
operating is the active form of operate
believing is the active form of believe
losing is the active form of lose
learning is the active form of learn
allowing is the active form of
2024-07-25 02:05:15 root INFO     [order_1_approx] starting weight calculation for operating is the active form of operate
ensuring is the active form of ensure
allowing is the active form of allow
losing is the active form of lose
learning is the active form of learn
believing is the active form of believe
encouraging is the active form of encourage
avoiding is the active form of
2024-07-25 02:05:15 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 02:08:45 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.0000, -4.0312, -1.0312,  ...,  1.5312, -0.4648, -1.5312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([13.8750, -1.8281,  0.3008,  ..., -9.2500, -5.0312,  8.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0056,  0.0073, -0.0009,  ...,  0.0020, -0.0148,  0.0075],
        [-0.0013, -0.0029,  0.0132,  ..., -0.0002,  0.0138,  0.0043],
        [-0.0093, -0.0240,  0.0549,  ..., -0.0072,  0.0295, -0.0140],
        ...,
        [-0.0015,  0.0018, -0.0031,  ..., -0.0024, -0.0007,  0.0061],
        [-0.0016, -0.0121, -0.0008,  ..., -0.0022,  0.0215, -0.0123],
        [-0.0082, -0.0109, -0.0047,  ..., -0.0090,  0.0154,  0.0018]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[12.3359, -2.3203,  4.1016,  ..., -9.7812, -4.1250,  9.4141]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 02:08:46 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To improve results in a improvement
To embarrass results in a embarrassment
To arrange results in a arrangement
To entitle results in a entitlement
To enlighten results in a enlightenment
To replace results in a replacement
To enforce results in a enforcement
To develop results in a
2024-07-25 02:08:46 root INFO     [order_1_approx] starting weight calculation for To arrange results in a arrangement
To enlighten results in a enlightenment
To develop results in a development
To entitle results in a entitlement
To enforce results in a enforcement
To embarrass results in a embarrassment
To improve results in a improvement
To replace results in a
2024-07-25 02:08:46 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 02:11:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4727, -2.9062,  0.0193,  ..., -0.6289, -0.2988,  0.3945],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 10.5000,  -3.0312,   2.4844,  ...,  -9.2500,  -1.8594, -19.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0030, -0.0031, -0.0010,  ...,  0.0010,  0.0019,  0.0002],
        [-0.0019, -0.0017,  0.0045,  ..., -0.0004,  0.0032,  0.0007],
        [-0.0020,  0.0013,  0.0286,  ..., -0.0009, -0.0007, -0.0040],
        ...,
        [-0.0026, -0.0004,  0.0005,  ...,  0.0025, -0.0044, -0.0001],
        [ 0.0025, -0.0014, -0.0064,  ...,  0.0025,  0.0017, -0.0015],
        [ 0.0024,  0.0007, -0.0004,  ..., -0.0017,  0.0003,  0.0003]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  9.9688,  -2.1660,   1.6719,  ...,  -8.8672,  -2.1719, -20.6562]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 02:11:01 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for operating is the active form of operate
ensuring is the active form of ensure
allowing is the active form of allow
losing is the active form of lose
learning is the active form of learn
believing is the active form of believe
encouraging is the active form of encourage
avoiding is the active form of
2024-07-25 02:11:01 root INFO     [order_1_approx] starting weight calculation for allowing is the active form of allow
ensuring is the active form of ensure
operating is the active form of operate
losing is the active form of lose
avoiding is the active form of avoid
believing is the active form of believe
encouraging is the active form of encourage
learning is the active form of
2024-07-25 02:11:01 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 02:14:39 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4141, -1.6875, -3.5000,  ...,  0.4102, -0.7383, -0.1523],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  4.7812,   0.8750, -11.5625,  ...,  -3.4844,  -8.6875,   7.5938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0059,  0.0051, -0.0089,  ...,  0.0005, -0.0077,  0.0026],
        [-0.0021,  0.0020,  0.0167,  ..., -0.0008,  0.0035,  0.0083],
        [ 0.0005, -0.0068,  0.0659,  ...,  0.0030,  0.0061, -0.0011],
        ...,
        [ 0.0001, -0.0082, -0.0134,  ...,  0.0030,  0.0018,  0.0076],
        [ 0.0005, -0.0035, -0.0075,  ...,  0.0035,  0.0220,  0.0003],
        [-0.0042, -0.0070, -0.0073,  ..., -0.0022,  0.0070,  0.0102]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  3.8203,   1.0742, -11.4766,  ...,  -4.5586,  -9.4375,   9.0391]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 02:14:40 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To arrange results in a arrangement
To enlighten results in a enlightenment
To develop results in a development
To entitle results in a entitlement
To enforce results in a enforcement
To embarrass results in a embarrassment
To improve results in a improvement
To replace results in a
2024-07-25 02:14:40 root INFO     [order_1_approx] starting weight calculation for To replace results in a replacement
To develop results in a development
To enforce results in a enforcement
To improve results in a improvement
To arrange results in a arrangement
To embarrass results in a embarrassment
To entitle results in a entitlement
To enlighten results in a
2024-07-25 02:14:41 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 02:16:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.4688, -1.6719,  3.7500,  ...,  1.2812, -0.7109,  0.1172],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 13.4375,  -8.7500,   5.3750,  ..., -13.8750, -11.7500,   3.7188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.8076e-03, -8.5449e-04,  1.4191e-03,  ..., -2.8687e-03,
          2.5177e-03, -3.9673e-03],
        [ 2.4796e-04,  4.1962e-05,  4.3945e-03,  ...,  3.0670e-03,
          4.7302e-03,  2.7618e-03],
        [-2.2583e-03, -1.7738e-04,  4.2969e-02,  ...,  3.4180e-03,
         -3.8757e-03, -1.1368e-03],
        ...,
        [ 2.2430e-03, -1.2665e-03, -6.5613e-03,  ...,  1.8463e-03,
         -6.3171e-03,  1.7090e-03],
        [-2.4414e-03, -6.4087e-04, -3.6469e-03,  ...,  4.2725e-03,
          8.3618e-03, -3.2349e-03],
        [-3.6011e-03, -3.2043e-04,  6.8970e-03,  ..., -7.0572e-05,
         -1.7166e-03,  3.0518e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 13.0547,  -8.5078,   6.3906,  ..., -13.3906, -11.1250,   3.4883]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 02:16:48 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for allowing is the active form of allow
ensuring is the active form of ensure
operating is the active form of operate
losing is the active form of lose
avoiding is the active form of avoid
believing is the active form of believe
encouraging is the active form of encourage
learning is the active form of
2024-07-25 02:16:48 root INFO     [order_1_approx] starting weight calculation for believing is the active form of believe
operating is the active form of operate
ensuring is the active form of ensure
avoiding is the active form of avoid
learning is the active form of learn
allowing is the active form of allow
losing is the active form of lose
encouraging is the active form of
2024-07-25 02:16:49 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 02:20:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5469, -2.9375,  1.0078,  ...,  1.8516, -2.5938, -0.0879],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  7.3125,   6.5000, -12.8125,  ...,   6.6875, -15.1250,  15.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0062,  0.0041, -0.0041,  ...,  0.0023, -0.0007, -0.0022],
        [ 0.0038, -0.0012,  0.0121,  ..., -0.0012,  0.0018,  0.0071],
        [-0.0010, -0.0051,  0.0894,  ..., -0.0051, -0.0020, -0.0017],
        ...,
        [ 0.0017, -0.0003, -0.0075,  ..., -0.0033, -0.0056, -0.0045],
        [ 0.0089, -0.0050, -0.0048,  ...,  0.0017,  0.0254,  0.0080],
        [-0.0024, -0.0011, -0.0108,  ..., -0.0070,  0.0064,  0.0171]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  5.7305,   7.8516, -12.7188,  ...,   6.7930, -13.7891,  16.8750]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 02:20:38 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To replace results in a replacement
To develop results in a development
To enforce results in a enforcement
To improve results in a improvement
To arrange results in a arrangement
To embarrass results in a embarrassment
To entitle results in a entitlement
To enlighten results in a
2024-07-25 02:20:38 root INFO     [order_1_approx] starting weight calculation for To embarrass results in a embarrassment
To develop results in a development
To entitle results in a entitlement
To arrange results in a arrangement
To improve results in a improvement
To replace results in a replacement
To enlighten results in a enlightenment
To enforce results in a
2024-07-25 02:20:38 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 02:22:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.0781, -3.2500,  2.0938,  ...,  1.5625, -0.7227, -1.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 10.2500,  -7.5938,   2.5625,  ...,  -5.3438, -12.0000,  -3.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0025,  0.0023, -0.0025,  ...,  0.0005,  0.0066, -0.0011],
        [-0.0005,  0.0018,  0.0045,  ..., -0.0017,  0.0046,  0.0006],
        [ 0.0028,  0.0041,  0.0583,  ..., -0.0002,  0.0015, -0.0031],
        ...,
        [ 0.0028, -0.0019,  0.0021,  ..., -0.0009, -0.0044,  0.0025],
        [ 0.0003, -0.0015,  0.0024,  ...,  0.0018,  0.0030, -0.0024],
        [-0.0006,  0.0052,  0.0049,  ...,  0.0040,  0.0024,  0.0004]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  8.5859,  -8.0859,   2.4453,  ...,  -4.7617, -12.3125,  -3.3418]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 02:22:29 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for believing is the active form of believe
operating is the active form of operate
ensuring is the active form of ensure
avoiding is the active form of avoid
learning is the active form of learn
allowing is the active form of allow
losing is the active form of lose
encouraging is the active form of
2024-07-25 02:22:30 root INFO     [order_1_approx] starting weight calculation for operating is the active form of operate
learning is the active form of learn
encouraging is the active form of encourage
ensuring is the active form of ensure
avoiding is the active form of avoid
allowing is the active form of allow
believing is the active form of believe
losing is the active form of
2024-07-25 02:22:30 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 02:26:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4941, -2.9375,  5.9688,  ...,  1.1641, -1.4062,  1.1797],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  2.5000,  11.5000,  10.1250,  ...,   9.2500, -11.8750,   6.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-8.0872e-04,  6.8665e-05, -3.3951e-04,  ..., -1.0223e-03,
          1.4801e-03, -4.4861e-03],
        [-6.7902e-04, -8.2397e-04,  1.4160e-02,  ..., -1.1978e-03,
         -7.2479e-04,  7.2021e-03],
        [-7.6675e-04, -5.0049e-03,  5.7861e-02,  ...,  2.5940e-04,
          2.8687e-03,  2.1210e-03],
        ...,
        [-9.4604e-04,  5.2795e-03, -1.2390e-02,  ..., -3.1662e-04,
          4.6997e-03,  4.8523e-03],
        [ 1.2512e-03, -2.9755e-03, -9.5215e-03,  ...,  5.5542e-03,
          1.8799e-02,  1.1292e-03],
        [ 3.3417e-03,  8.2397e-04, -1.0620e-02,  ..., -5.5237e-03,
         -3.0975e-03,  1.1292e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  2.4297,  13.6562,  12.2969,  ...,  10.8203, -11.5234,   5.3281]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 02:26:26 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To embarrass results in a embarrassment
To develop results in a development
To entitle results in a entitlement
To arrange results in a arrangement
To improve results in a improvement
To replace results in a replacement
To enlighten results in a enlightenment
To enforce results in a
2024-07-25 02:26:27 root INFO     [order_1_approx] starting weight calculation for To enlighten results in a enlightenment
To entitle results in a entitlement
To replace results in a replacement
To develop results in a development
To embarrass results in a embarrassment
To enforce results in a enforcement
To arrange results in a arrangement
To improve results in a
2024-07-25 02:26:27 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 02:28:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6250, -3.5312, -1.6953,  ...,  2.5625, -1.3047, -3.2344],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 9.8750, -1.8594,  4.2812,  ..., -5.0312, -1.4219, -8.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0076, -0.0031, -0.0095,  ...,  0.0040,  0.0078,  0.0039],
        [-0.0029,  0.0085,  0.0134,  ...,  0.0043,  0.0052, -0.0076],
        [-0.0018,  0.0027,  0.0503,  ...,  0.0027,  0.0026, -0.0088],
        ...,
        [ 0.0015,  0.0003, -0.0041,  ...,  0.0016, -0.0027,  0.0038],
        [ 0.0070,  0.0020, -0.0060,  ...,  0.0005, -0.0084, -0.0061],
        [ 0.0005,  0.0024, -0.0077,  ...,  0.0024,  0.0056,  0.0015]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[10.0859, -1.2793,  4.2070,  ..., -4.7461, -1.6055, -7.0703]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 02:28:10 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for operating is the active form of operate
learning is the active form of learn
encouraging is the active form of encourage
ensuring is the active form of ensure
avoiding is the active form of avoid
allowing is the active form of allow
believing is the active form of believe
losing is the active form of
2024-07-25 02:28:10 root INFO     total operator prediction time: 2754.9372930526733 seconds
2024-07-25 02:28:10 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on verb_Ving - Ved
2024-07-25 02:28:10 root INFO     building operator verb_Ving - Ved
2024-07-25 02:28:10 root INFO     [order_1_approx] starting weight calculation for After something is spending, it has spent
After something is deciding, it has decided
After something is replacing, it has replaced
After something is losing, it has lost
After something is appointing, it has appointed
After something is suffering, it has suffered
After something is receiving, it has received
After something is introducing, it has
2024-07-25 02:28:10 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 02:32:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.8984, -4.3125, -2.2500,  ...,  0.9648,  0.3887,  0.0894],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 13.6875,   3.6875,  -8.3750,  ..., -10.2500,  -4.0938,   6.1562],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0082, -0.0002, -0.0014,  ..., -0.0012, -0.0044, -0.0020],
        [-0.0066, -0.0027,  0.0109,  ..., -0.0043,  0.0054,  0.0117],
        [-0.0092, -0.0121,  0.0498,  ..., -0.0009, -0.0044, -0.0070],
        ...,
        [-0.0111, -0.0052, -0.0095,  ..., -0.0046, -0.0005,  0.0045],
        [-0.0045, -0.0040, -0.0060,  ...,  0.0016,  0.0111, -0.0055],
        [-0.0143, -0.0079, -0.0004,  ..., -0.0064, -0.0014,  0.0013]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 12.6719,   6.6719,  -7.7734,  ..., -10.4766,  -2.7148,   6.0977]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 02:32:13 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To enlighten results in a enlightenment
To entitle results in a entitlement
To replace results in a replacement
To develop results in a development
To embarrass results in a embarrassment
To enforce results in a enforcement
To arrange results in a arrangement
To improve results in a
2024-07-25 02:32:13 root INFO     [order_1_approx] starting weight calculation for To enlighten results in a enlightenment
To entitle results in a entitlement
To improve results in a improvement
To replace results in a replacement
To develop results in a development
To enforce results in a enforcement
To arrange results in a arrangement
To embarrass results in a
2024-07-25 02:32:13 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 02:33:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.8438, -2.2031,  3.3750,  ...,  1.5234, -0.4453,  0.0996],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  9.8750, -10.8125,   7.4375,  ...,  -4.5000,   4.4375,   0.6133],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0024, -0.0009, -0.0098,  ..., -0.0011,  0.0015, -0.0011],
        [ 0.0050, -0.0002, -0.0053,  ..., -0.0031, -0.0103,  0.0019],
        [-0.0076, -0.0020,  0.0757,  ..., -0.0063,  0.0077, -0.0025],
        ...,
        [-0.0022,  0.0034, -0.0072,  ...,  0.0005, -0.0001,  0.0059],
        [ 0.0001, -0.0042,  0.0052,  ..., -0.0010,  0.0166,  0.0002],
        [ 0.0015,  0.0019, -0.0005,  ..., -0.0025,  0.0005,  0.0121]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  9.5391, -11.7734,   7.5977,  ...,  -5.1328,   5.2891,  -0.6074]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 02:33:56 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for After something is spending, it has spent
After something is deciding, it has decided
After something is replacing, it has replaced
After something is losing, it has lost
After something is appointing, it has appointed
After something is suffering, it has suffered
After something is receiving, it has received
After something is introducing, it has
2024-07-25 02:33:56 root INFO     [order_1_approx] starting weight calculation for After something is introducing, it has introduced
After something is suffering, it has suffered
After something is deciding, it has decided
After something is losing, it has lost
After something is appointing, it has appointed
After something is receiving, it has received
After something is spending, it has spent
After something is replacing, it has
2024-07-25 02:33:57 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 02:38:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4297, -1.0156,  0.0913,  ...,  0.5078, -3.7031, -0.3945],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -0.4297,  -0.5000, -18.0000,  ...,  -9.7500,  -2.1562,   5.4688],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.6479e-02, -1.5564e-03, -3.6621e-04,  ...,  5.4932e-03,
          3.3875e-03,  9.2163e-03],
        [-1.8997e-03,  1.9455e-03,  1.7456e-02,  ..., -3.1738e-03,
          1.1368e-03,  1.0986e-02],
        [-8.7280e-03,  6.6528e-03,  1.1084e-01,  ...,  6.5231e-04,
          6.7139e-03, -3.6011e-03],
        ...,
        [ 3.5400e-03, -3.0823e-03, -3.6316e-03,  ...,  1.2665e-03,
         -9.4604e-03, -2.8381e-03],
        [-2.1057e-03,  6.2943e-05, -2.4536e-02,  ...,  5.9509e-03,
          2.9541e-02,  7.2937e-03],
        [-2.0905e-03,  8.7280e-03, -1.5503e-02,  ..., -3.1738e-03,
          7.9346e-03,  1.3428e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -1.4395,   1.4375, -18.0625,  ..., -14.8281,  -0.5439,   5.1602]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 02:38:05 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To enlighten results in a enlightenment
To entitle results in a entitlement
To improve results in a improvement
To replace results in a replacement
To develop results in a development
To enforce results in a enforcement
To arrange results in a arrangement
To embarrass results in a
2024-07-25 02:38:05 root INFO     [order_1_approx] starting weight calculation for To improve results in a improvement
To embarrass results in a embarrassment
To enforce results in a enforcement
To enlighten results in a enlightenment
To replace results in a replacement
To arrange results in a arrangement
To develop results in a development
To entitle results in a
2024-07-25 02:38:05 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 02:39:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3438, -1.2031, -1.2500,  ...,  0.7891, -1.4453, -0.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 12.4375,  -9.2500,  -1.9219,  ..., -14.3750,  -6.0625,  -7.7188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 6.3171e-03, -1.4496e-03, -5.0354e-03,  ..., -2.9144e-03,
          2.1057e-03,  1.1826e-03],
        [ 1.8120e-04,  3.6621e-03,  2.9602e-03,  ..., -8.2016e-04,
         -4.2114e-03,  2.2278e-03],
        [-4.5776e-03, -5.5313e-04,  5.2979e-02,  ..., -2.8381e-03,
          5.1270e-03, -2.5787e-03],
        ...,
        [-3.9062e-03,  1.2970e-04, -3.2196e-03,  ..., -7.3242e-04,
         -4.5967e-04,  3.2654e-03],
        [ 4.2534e-04,  8.0109e-04, -9.6130e-04,  ...,  1.9150e-03,
          1.1292e-02,  5.3406e-05],
        [-8.3542e-04, -2.3346e-03, -1.2695e-02,  ..., -1.8616e-03,
         -1.8082e-03,  8.8501e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 12.4375, -10.1875,  -2.0215,  ..., -15.3516,  -5.0820,  -8.8750]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 02:39:42 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for After something is introducing, it has introduced
After something is suffering, it has suffered
After something is deciding, it has decided
After something is losing, it has lost
After something is appointing, it has appointed
After something is receiving, it has received
After something is spending, it has spent
After something is replacing, it has
2024-07-25 02:39:43 root INFO     [order_1_approx] starting weight calculation for After something is appointing, it has appointed
After something is replacing, it has replaced
After something is introducing, it has introduced
After something is losing, it has lost
After something is spending, it has spent
After something is deciding, it has decided
After something is receiving, it has received
After something is suffering, it has
2024-07-25 02:39:43 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 02:43:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1016, -1.0547,  1.0156,  ...,  0.6328, -1.3984, -0.1172],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  6.4375,   8.0000,  -4.6875,  ...,   8.3750, -16.1250,   4.4375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-1.6632e-03, -7.6294e-05, -7.2021e-03,  ...,  1.6632e-03,
         -2.8229e-03,  9.8267e-03],
        [ 9.6893e-04,  8.6060e-03,  2.6611e-02,  ..., -7.6904e-03,
          1.3733e-03,  1.0132e-02],
        [-1.3611e-02, -3.4637e-03,  1.0889e-01,  ..., -5.7678e-03,
         -3.1281e-03, -1.0742e-02],
        ...,
        [-1.9989e-03,  1.0803e-02,  8.3008e-03,  ..., -6.0730e-03,
         -2.1362e-03,  1.4343e-03],
        [ 8.7891e-03,  1.9531e-03,  4.8828e-03,  ...,  4.1504e-03,
          2.2339e-02, -1.0452e-03],
        [ 1.1108e-02,  2.0447e-03,  7.2632e-03,  ..., -1.4191e-03,
          6.2256e-03,  2.6611e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  5.8125,   6.8398,  -2.5352,  ...,   6.9688, -16.4375,   5.4141]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 02:43:57 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To improve results in a improvement
To embarrass results in a embarrassment
To enforce results in a enforcement
To enlighten results in a enlightenment
To replace results in a replacement
To arrange results in a arrangement
To develop results in a development
To entitle results in a
2024-07-25 02:43:57 root INFO     total operator prediction time: 2812.4160113334656 seconds
2024-07-25 02:43:57 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on noun+less_reg
2024-07-25 02:43:57 root INFO     building operator noun+less_reg
2024-07-25 02:43:57 root INFO     [order_1_approx] starting weight calculation for Something without passion is passionless
Something without mirth is mirthless
Something without window is windowless
Something without hair is hairless
Something without error is errorless
Something without path is pathless
Something without friend is friendless
Something without remorse is
2024-07-25 02:43:57 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 02:45:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4219, -2.6562,  3.2500,  ..., -0.2471, -4.0938, -1.9844],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 11.9375,  -2.8750,  13.3125,  ...,  -9.6250,  -3.6406, -13.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.8076e-03, -4.5395e-04, -9.4604e-03,  ..., -1.6861e-03,
          3.4332e-03, -1.0834e-03],
        [ 4.5166e-03,  4.8828e-03,  4.4556e-03,  ..., -2.7313e-03,
          8.1177e-03,  7.6599e-03],
        [-4.1809e-03,  3.3875e-03,  5.6152e-02,  ..., -3.7537e-03,
          1.1108e-02,  1.5640e-03],
        ...,
        [-8.8501e-04,  4.1199e-03, -1.3062e-02,  ...,  2.3804e-03,
         -1.5717e-03,  1.3447e-04],
        [-1.2512e-03, -2.7924e-03, -4.3030e-03,  ...,  3.5095e-04,
          9.8267e-03,  3.8147e-06],
        [-8.6784e-05, -1.4877e-03, -3.4027e-03,  ..., -4.3030e-03,
          2.2583e-03,  4.0588e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 11.3516,  -1.6670,  15.0156,  ...,  -9.5859,  -2.7715, -13.8984]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 02:45:25 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for After something is appointing, it has appointed
After something is replacing, it has replaced
After something is introducing, it has introduced
After something is losing, it has lost
After something is spending, it has spent
After something is deciding, it has decided
After something is receiving, it has received
After something is suffering, it has
2024-07-25 02:45:26 root INFO     [order_1_approx] starting weight calculation for After something is replacing, it has replaced
After something is suffering, it has suffered
After something is introducing, it has introduced
After something is deciding, it has decided
After something is appointing, it has appointed
After something is receiving, it has received
After something is spending, it has spent
After something is losing, it has
2024-07-25 02:45:26 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 02:49:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.8594,  3.5312,  3.5156,  ..., -1.6719, -3.4844,  2.7188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-1.9375,  7.3125, -2.5156,  ..., -3.6250,  1.8750, 13.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.6327e-03,  1.2131e-03,  1.4648e-03,  ...,  1.5259e-05,
          1.0376e-02,  6.1646e-03],
        [-2.0752e-03,  8.6670e-03,  1.0498e-02,  ..., -6.2256e-03,
          8.7891e-03,  9.7656e-03],
        [-6.8970e-03, -7.2479e-04,  8.8379e-02,  ..., -8.4229e-03,
         -2.0386e-02, -4.5471e-03],
        ...,
        [-5.1880e-03, -3.9062e-03, -9.0332e-03,  ...,  8.0566e-03,
         -1.3367e-02,  5.8594e-03],
        [ 6.5231e-04,  5.1270e-03,  3.5248e-03,  ..., -2.2430e-03,
          2.7222e-02,  1.1841e-02],
        [ 8.6060e-03, -1.0529e-03, -6.4392e-03,  ...,  6.6833e-03,
         -2.2461e-02,  2.1118e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.3926,  5.7695, -2.5508,  ..., -5.4922,  4.9609,  9.7031]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 02:49:58 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Something without passion is passionless
Something without mirth is mirthless
Something without window is windowless
Something without hair is hairless
Something without error is errorless
Something without path is pathless
Something without friend is friendless
Something without remorse is
2024-07-25 02:49:58 root INFO     [order_1_approx] starting weight calculation for Something without path is pathless
Something without mirth is mirthless
Something without remorse is remorseless
Something without friend is friendless
Something without passion is passionless
Something without hair is hairless
Something without window is windowless
Something without error is
2024-07-25 02:49:58 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 02:51:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2695, -1.7266, -0.1426,  ...,  2.1406, -2.3125, -3.3281],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 17.0000, -13.7500,   5.8750,  ...,  -1.3750,   5.1250,  -7.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0042, -0.0008, -0.0012,  ..., -0.0032,  0.0017, -0.0013],
        [ 0.0010,  0.0038, -0.0018,  ...,  0.0028, -0.0003,  0.0026],
        [ 0.0027,  0.0028,  0.0469,  ..., -0.0021, -0.0008, -0.0065],
        ...,
        [-0.0038,  0.0024, -0.0026,  ..., -0.0032,  0.0031,  0.0005],
        [ 0.0004,  0.0038,  0.0030,  ...,  0.0006,  0.0051,  0.0037],
        [-0.0015, -0.0016, -0.0074,  ..., -0.0029, -0.0017,  0.0034]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 16.2656, -11.6016,   5.8359,  ...,  -1.9707,   6.3438,  -7.6680]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 02:51:13 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for After something is replacing, it has replaced
After something is suffering, it has suffered
After something is introducing, it has introduced
After something is deciding, it has decided
After something is appointing, it has appointed
After something is receiving, it has received
After something is spending, it has spent
After something is losing, it has
2024-07-25 02:51:13 root INFO     [order_1_approx] starting weight calculation for After something is replacing, it has replaced
After something is losing, it has lost
After something is receiving, it has received
After something is suffering, it has suffered
After something is spending, it has spent
After something is deciding, it has decided
After something is introducing, it has introduced
After something is appointing, it has
2024-07-25 02:51:14 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 02:55:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4531, -1.0000,  1.4531,  ..., -0.8984, -0.9375,  2.3281],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-3.2812, 11.2500, 18.1250,  ..., -9.3125, -1.9062, -0.7969],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0018, -0.0058,  0.0052,  ..., -0.0131, -0.0016,  0.0048],
        [ 0.0033, -0.0041,  0.0087,  ..., -0.0040,  0.0052,  0.0187],
        [-0.0053,  0.0104,  0.0703,  ...,  0.0125, -0.0101, -0.0145],
        ...,
        [ 0.0100,  0.0048, -0.0099,  ...,  0.0024, -0.0093, -0.0109],
        [-0.0012,  0.0050, -0.0161,  ...,  0.0017,  0.0204, -0.0020],
        [-0.0049,  0.0020,  0.0032,  ...,  0.0056, -0.0033,  0.0027]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-5.2539, 10.1406, 19.2500,  ..., -9.1875,  0.9766, -1.2539]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 02:55:53 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Something without path is pathless
Something without mirth is mirthless
Something without remorse is remorseless
Something without friend is friendless
Something without passion is passionless
Something without hair is hairless
Something without window is windowless
Something without error is
2024-07-25 02:55:54 root INFO     [order_1_approx] starting weight calculation for Something without passion is passionless
Something without error is errorless
Something without mirth is mirthless
Something without window is windowless
Something without remorse is remorseless
Something without hair is hairless
Something without friend is friendless
Something without path is
2024-07-25 02:55:54 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 02:56:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1914,  0.0859,  0.5000,  ..., -0.0273, -2.1406, -0.5352],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 8.0000, -8.2500,  1.2188,  ...,  0.2969, -3.2500,  4.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0039, -0.0049, -0.0100,  ..., -0.0072, -0.0015,  0.0032],
        [ 0.0002, -0.0007, -0.0027,  ..., -0.0022, -0.0054,  0.0011],
        [-0.0056,  0.0048,  0.0601,  ...,  0.0044,  0.0062, -0.0039],
        ...,
        [-0.0055,  0.0030,  0.0097,  ...,  0.0049,  0.0025, -0.0001],
        [-0.0004, -0.0011,  0.0010,  ...,  0.0005,  0.0066,  0.0010],
        [ 0.0015, -0.0012, -0.0011,  ...,  0.0006, -0.0022,  0.0053]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.7070, -8.0781,  1.3359,  ...,  0.5195, -3.7031,  2.9258]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 02:56:55 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for After something is replacing, it has replaced
After something is losing, it has lost
After something is receiving, it has received
After something is suffering, it has suffered
After something is spending, it has spent
After something is deciding, it has decided
After something is introducing, it has introduced
After something is appointing, it has
2024-07-25 02:56:56 root INFO     [order_1_approx] starting weight calculation for After something is losing, it has lost
After something is deciding, it has decided
After something is spending, it has spent
After something is appointing, it has appointed
After something is introducing, it has introduced
After something is replacing, it has replaced
After something is suffering, it has suffered
After something is receiving, it has
2024-07-25 02:56:56 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 03:01:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.6875, -0.5312,  2.2656,  ...,  1.6172, -2.3438,  0.0508],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  1.4375,   0.4648,   8.7500,  ..., -25.2500,  -2.0781,   2.4219],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0019, -0.0012, -0.0194,  ..., -0.0033,  0.0074,  0.0064],
        [ 0.0014, -0.0035, -0.0042,  ...,  0.0009,  0.0073,  0.0051],
        [-0.0032,  0.0008,  0.0581,  ..., -0.0023, -0.0003, -0.0053],
        ...,
        [ 0.0007,  0.0025, -0.0101,  ..., -0.0015, -0.0012, -0.0064],
        [ 0.0025,  0.0067, -0.0008,  ..., -0.0020,  0.0047, -0.0048],
        [-0.0037, -0.0042,  0.0045,  ..., -0.0033,  0.0034,  0.0181]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  0.6860,  -1.4453,   8.0938,  ..., -23.8594,  -3.6094,   3.8008]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 03:01:52 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Something without passion is passionless
Something without error is errorless
Something without mirth is mirthless
Something without window is windowless
Something without remorse is remorseless
Something without hair is hairless
Something without friend is friendless
Something without path is
2024-07-25 03:01:52 root INFO     [order_1_approx] starting weight calculation for Something without passion is passionless
Something without remorse is remorseless
Something without window is windowless
Something without friend is friendless
Something without mirth is mirthless
Something without error is errorless
Something without path is pathless
Something without hair is
2024-07-25 03:01:52 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 03:02:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2344, -2.9062,  4.2188,  ..., -0.2891, -1.2891, -0.0469],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 22.2500, -18.8750,  -1.2344,  ..., -17.6250,  -0.1992,  -9.9375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0031, -0.0036, -0.0059,  ...,  0.0017, -0.0008, -0.0014],
        [ 0.0028,  0.0019,  0.0031,  ..., -0.0043,  0.0027,  0.0031],
        [ 0.0014,  0.0076,  0.0535,  ..., -0.0002, -0.0003, -0.0048],
        ...,
        [ 0.0003,  0.0026,  0.0037,  ..., -0.0010,  0.0015,  0.0039],
        [ 0.0019,  0.0009,  0.0022,  ...,  0.0043,  0.0099, -0.0006],
        [-0.0044,  0.0027,  0.0038,  ..., -0.0007, -0.0006,  0.0115]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 22.2656, -18.3281,  -0.6548,  ..., -17.0781,  -0.6445, -10.1094]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 03:02:35 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for After something is losing, it has lost
After something is deciding, it has decided
After something is spending, it has spent
After something is appointing, it has appointed
After something is introducing, it has introduced
After something is replacing, it has replaced
After something is suffering, it has suffered
After something is receiving, it has
2024-07-25 03:02:35 root INFO     [order_1_approx] starting weight calculation for After something is introducing, it has introduced
After something is losing, it has lost
After something is deciding, it has decided
After something is suffering, it has suffered
After something is replacing, it has replaced
After something is receiving, it has received
After something is appointing, it has appointed
After something is spending, it has
2024-07-25 03:02:35 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 03:07:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.5781, -0.7148, -0.4336,  ...,  0.1172, -2.9844,  2.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -1.5625,  18.8750,   9.3750,  ..., -10.5000,   3.5625,   1.9922],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 6.8665e-03,  8.7738e-05, -2.9297e-03,  ..., -1.7929e-03,
         -6.6833e-03, -1.2817e-02],
        [ 3.9978e-03,  4.4632e-04,  7.8735e-03,  ..., -4.4250e-04,
         -2.4109e-03,  4.7607e-03],
        [ 2.1667e-03,  2.0599e-03,  3.4668e-02,  ...,  4.3488e-04,
         -5.7678e-03, -4.0283e-03],
        ...,
        [ 1.4801e-03, -3.4485e-03,  4.7607e-03,  ...,  8.1787e-03,
         -3.0823e-03,  1.8311e-03],
        [-5.2490e-03,  2.6855e-03, -4.3030e-03,  ...,  6.3324e-04,
          1.2939e-02,  5.0659e-03],
        [ 3.3875e-03,  1.5945e-03,  1.2573e-02,  ..., -5.9204e-03,
         -3.4180e-03,  6.3477e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -1.6445,  18.1250,  10.1797,  ..., -10.1953,   4.5430,   1.1992]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 03:07:52 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Something without passion is passionless
Something without remorse is remorseless
Something without window is windowless
Something without friend is friendless
Something without mirth is mirthless
Something without error is errorless
Something without path is pathless
Something without hair is
2024-07-25 03:07:52 root INFO     [order_1_approx] starting weight calculation for Something without error is errorless
Something without passion is passionless
Something without friend is friendless
Something without path is pathless
Something without remorse is remorseless
Something without mirth is mirthless
Something without hair is hairless
Something without window is
2024-07-25 03:07:52 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 03:08:15 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2344, -1.2031,  6.0312,  ...,  0.6445, -0.0312, -0.5820],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([15.1250, -6.5625,  9.2500,  ..., -2.0469, -1.4375,  0.6562],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-0.0040,  0.0048, -0.0122,  ..., -0.0034,  0.0119, -0.0038],
        [ 0.0058,  0.0014,  0.0015,  ...,  0.0004, -0.0085,  0.0040],
        [ 0.0026,  0.0042,  0.0530,  ..., -0.0004, -0.0012, -0.0016],
        ...,
        [-0.0017,  0.0063,  0.0004,  ...,  0.0017, -0.0038,  0.0067],
        [-0.0005, -0.0016, -0.0113,  ...,  0.0007,  0.0127,  0.0006],
        [-0.0007, -0.0042,  0.0020,  ...,  0.0007, -0.0026,  0.0072]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[16.0625, -6.7383,  9.1406,  ..., -1.7500, -0.1104, -0.1934]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 03:08:15 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for After something is introducing, it has introduced
After something is losing, it has lost
After something is deciding, it has decided
After something is suffering, it has suffered
After something is replacing, it has replaced
After something is receiving, it has received
After something is appointing, it has appointed
After something is spending, it has
2024-07-25 03:08:16 root INFO     [order_1_approx] starting weight calculation for After something is losing, it has lost
After something is appointing, it has appointed
After something is suffering, it has suffered
After something is receiving, it has received
After something is introducing, it has introduced
After something is replacing, it has replaced
After something is spending, it has spent
After something is deciding, it has
2024-07-25 03:08:16 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 03:13:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9609, -0.6719,  3.1562,  ...,  2.0156, -1.7031,  0.1758],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  9.6875, -16.7500,  -1.6562,  ..., -12.5625,  -0.4922,  -2.8594],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.6469e-03,  2.5177e-04, -4.3030e-03,  ..., -3.8452e-03,
          2.7466e-04,  4.1809e-03],
        [ 4.1962e-04,  1.4496e-03,  6.1798e-04,  ...,  8.8120e-04,
         -1.1597e-02, -2.9755e-03],
        [-1.7548e-03,  3.4180e-03,  5.7373e-02,  ...,  2.5330e-03,
          1.4038e-03, -5.0354e-04],
        ...,
        [ 1.5793e-03,  9.9182e-05, -6.2866e-03,  ...,  6.2180e-04,
          1.6174e-03,  2.8076e-03],
        [ 8.5068e-04,  1.0300e-03, -3.6316e-03,  ...,  2.8381e-03,
          1.5259e-02,  3.8147e-04],
        [ 1.3275e-03,  1.2589e-03,  1.2207e-03,  ...,  8.8501e-04,
         -3.3875e-03,  6.9275e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  8.2500, -16.7188,  -1.6006,  ..., -10.9766,   0.7734,  -4.2695]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 03:13:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.3594,  0.1680, -0.0166,  ..., -0.0947, -3.3281, -2.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  7.7500,   7.2812,   1.2969,  ..., -25.2500,   2.3125, -13.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-0.0043, -0.0057, -0.0057,  ..., -0.0062, -0.0014,  0.0077],
        [-0.0060,  0.0031, -0.0042,  ..., -0.0046,  0.0031,  0.0102],
        [-0.0002, -0.0017,  0.0554,  ..., -0.0037,  0.0046, -0.0117],
        ...,
        [ 0.0085, -0.0024,  0.0065,  ...,  0.0038, -0.0102, -0.0098],
        [-0.0029,  0.0015, -0.0154,  ...,  0.0008,  0.0236,  0.0082],
        [-0.0036, -0.0011, -0.0031,  ..., -0.0069,  0.0097,  0.0210]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  8.5703,   8.6172,   2.1582,  ..., -25.6562,   4.2773, -13.3281]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 03:13:58 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Something without error is errorless
Something without passion is passionless
Something without friend is friendless
Something without path is pathless
Something without remorse is remorseless
Something without mirth is mirthless
Something without hair is hairless
Something without window is
2024-07-25 03:13:58 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for After something is losing, it has lost
After something is appointing, it has appointed
After something is suffering, it has suffered
After something is receiving, it has received
After something is introducing, it has introduced
After something is replacing, it has replaced
After something is spending, it has spent
After something is deciding, it has
2024-07-25 03:13:58 root INFO     total operator prediction time: 2747.8158950805664 seconds
2024-07-25 03:13:58 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on verb_inf - Ved
2024-07-25 03:13:58 root INFO     building operator verb_inf - Ved
2024-07-25 03:13:58 root INFO     [order_1_approx] starting weight calculation for Something without passion is passionless
Something without hair is hairless
Something without mirth is mirthless
Something without path is pathless
Something without error is errorless
Something without remorse is remorseless
Something without window is windowless
Something without friend is
2024-07-25 03:13:58 root INFO     [order_1_approx] starting weight calculation for If the present form is receive, the past form is received
If the present form is consider, the past form is considered
If the present form is introduce, the past form is introduced
If the present form is understand, the past form is understood
If the present form is unite, the past form is united
If the present form is lose, the past form is lost
If the present form is replace, the past form is replaced
If the present form is identify, the past form is
2024-07-25 03:13:58 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 03:13:58 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 03:19:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0859, -0.6914,  1.7031,  ...,  0.2734, -1.2500,  2.1406],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 1.0156, -7.2500,  7.2812,  ..., -3.2656,  3.8281, -1.1094],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0034,  0.0032, -0.0052,  ..., -0.0042, -0.0043, -0.0028],
        [ 0.0013, -0.0034,  0.0096,  ..., -0.0037,  0.0020,  0.0016],
        [-0.0086,  0.0021,  0.0549,  ..., -0.0043,  0.0034, -0.0013],
        ...,
        [-0.0017, -0.0013,  0.0027,  ...,  0.0007, -0.0017, -0.0028],
        [ 0.0025, -0.0008,  0.0026,  ...,  0.0025,  0.0112, -0.0020],
        [ 0.0016,  0.0056, -0.0126,  ..., -0.0010,  0.0037, -0.0006]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5830, -7.4570,  7.2266,  ..., -3.0801,  2.9570, -3.5195]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 03:19:37 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is receive, the past form is received
If the present form is consider, the past form is considered
If the present form is introduce, the past form is introduced
If the present form is understand, the past form is understood
If the present form is unite, the past form is united
If the present form is lose, the past form is lost
If the present form is replace, the past form is replaced
If the present form is identify, the past form is
2024-07-25 03:19:37 root INFO     [order_1_approx] starting weight calculation for If the present form is unite, the past form is united
If the present form is receive, the past form is received
If the present form is identify, the past form is identified
If the present form is understand, the past form is understood
If the present form is consider, the past form is considered
If the present form is replace, the past form is replaced
If the present form is introduce, the past form is introduced
If the present form is lose, the past form is
2024-07-25 03:19:38 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 03:19:59 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5078,  1.7422,  2.5000,  ..., -0.5156, -3.3594, -0.2266],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 13.3125,   7.2188,  12.8750,  ..., -17.1250,  -8.1875, -15.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 5.7678e-03,  3.7689e-03, -4.9438e-03,  ...,  1.4954e-03,
          1.6174e-03,  1.2054e-03],
        [ 5.1117e-04, -1.8311e-03, -1.2684e-04,  ...,  6.4850e-05,
         -1.0529e-03, -1.3809e-03],
        [-1.2939e-02, -1.3962e-03,  5.1514e-02,  ...,  6.3171e-03,
         -3.4943e-03, -1.9287e-02],
        ...,
        [ 2.4109e-03,  1.8311e-03, -4.4556e-03,  ..., -2.8839e-03,
          9.5825e-03,  8.3160e-04],
        [ 2.8839e-03, -1.0071e-03,  2.5635e-03,  ...,  4.9591e-04,
          1.0254e-02,  7.9346e-03],
        [ 9.9945e-04, -9.2163e-03,  3.4485e-03,  ..., -5.6152e-03,
         -1.5259e-03,  9.3994e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 13.9844,   6.6328,  15.4297,  ..., -16.4844,  -7.8477, -16.0312]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 03:20:00 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Something without passion is passionless
Something without hair is hairless
Something without mirth is mirthless
Something without path is pathless
Something without error is errorless
Something without remorse is remorseless
Something without window is windowless
Something without friend is
2024-07-25 03:20:00 root INFO     [order_1_approx] starting weight calculation for Something without path is pathless
Something without window is windowless
Something without passion is passionless
Something without hair is hairless
Something without error is errorless
Something without remorse is remorseless
Something without friend is friendless
Something without mirth is
2024-07-25 03:20:00 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 03:25:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2734, -2.0000, -2.3906,  ...,  2.7500, -0.0977,  0.6953],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 6.7812, -4.2812,  5.9375,  ...,  2.2969,  3.6719, -6.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0067, -0.0016, -0.0009,  ..., -0.0026,  0.0024,  0.0035],
        [-0.0036, -0.0020,  0.0106,  ..., -0.0010, -0.0118, -0.0039],
        [ 0.0027,  0.0042,  0.0447,  ...,  0.0024,  0.0028, -0.0043],
        ...,
        [-0.0063, -0.0074, -0.0089,  ..., -0.0037, -0.0064, -0.0006],
        [ 0.0085,  0.0069, -0.0021,  ...,  0.0014,  0.0056,  0.0014],
        [ 0.0033,  0.0077, -0.0041,  ...,  0.0016,  0.0080,  0.0051]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.1250, -4.4727,  6.1680,  ...,  1.9395,  4.5156, -6.1289]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 03:25:20 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is unite, the past form is united
If the present form is receive, the past form is received
If the present form is identify, the past form is identified
If the present form is understand, the past form is understood
If the present form is consider, the past form is considered
If the present form is replace, the past form is replaced
If the present form is introduce, the past form is introduced
If the present form is lose, the past form is
2024-07-25 03:25:20 root INFO     [order_1_approx] starting weight calculation for If the present form is consider, the past form is considered
If the present form is replace, the past form is replaced
If the present form is lose, the past form is lost
If the present form is understand, the past form is understood
If the present form is introduce, the past form is introduced
If the present form is identify, the past form is identified
If the present form is unite, the past form is united
If the present form is receive, the past form is
2024-07-25 03:25:20 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 03:25:55 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.2812,  1.9375,  7.1875,  ...,  0.3086, -2.7500,  2.1406],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  3.7344,   3.1094,   5.6250,  ..., -12.6250,   6.5000,   4.7812],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 6.2561e-03,  4.0817e-04,  1.8845e-03,  ..., -5.7373e-03,
          9.4604e-03, -1.8768e-03],
        [-3.2997e-04,  6.7139e-04,  1.0620e-02,  ...,  2.3804e-03,
          4.4861e-03,  2.7847e-04],
        [-1.5182e-03,  4.1809e-03,  9.3750e-02,  ...,  4.5166e-03,
         -1.1139e-03, -1.5259e-05],
        ...,
        [ 9.3079e-04, -3.5706e-03, -5.1880e-04,  ...,  3.6316e-03,
          3.8147e-05, -9.0942e-03],
        [-5.5847e-03, -1.3046e-03, -1.3062e-02,  ..., -5.0354e-03,
          1.1414e-02,  6.4392e-03],
        [-1.4687e-04, -6.1646e-03, -1.2817e-02,  ..., -2.5940e-03,
         -6.1340e-03,  1.6235e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  1.6738,  -0.1504,  11.6250,  ..., -13.6562,   8.3906,   4.6445]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 03:25:56 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Something without path is pathless
Something without window is windowless
Something without passion is passionless
Something without hair is hairless
Something without error is errorless
Something without remorse is remorseless
Something without friend is friendless
Something without mirth is
2024-07-25 03:25:56 root INFO     [order_1_approx] starting weight calculation for Something without hair is hairless
Something without error is errorless
Something without window is windowless
Something without remorse is remorseless
Something without path is pathless
Something without mirth is mirthless
Something without friend is friendless
Something without passion is
2024-07-25 03:25:56 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 03:30:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4922, -3.2031,  3.1406,  ...,  0.9844,  1.8438,  1.9531],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 12.6250,  -2.1406,   5.1250,  ..., -11.6250,  -0.9219, -11.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0074,  0.0022, -0.0028,  ..., -0.0018, -0.0047,  0.0003],
        [-0.0041, -0.0007,  0.0203,  ...,  0.0020,  0.0049,  0.0030],
        [-0.0131, -0.0060,  0.0757,  ...,  0.0041,  0.0129,  0.0032],
        ...,
        [-0.0010, -0.0017,  0.0058,  ...,  0.0015,  0.0031,  0.0003],
        [ 0.0027, -0.0020, -0.0044,  ...,  0.0014,  0.0115,  0.0013],
        [ 0.0048,  0.0042, -0.0022,  ...,  0.0007, -0.0012,  0.0117]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 10.8984,  -0.5898,  10.3125,  ..., -10.3281,  -0.8379, -11.2969]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 03:30:54 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is consider, the past form is considered
If the present form is replace, the past form is replaced
If the present form is lose, the past form is lost
If the present form is understand, the past form is understood
If the present form is introduce, the past form is introduced
If the present form is identify, the past form is identified
If the present form is unite, the past form is united
If the present form is receive, the past form is
2024-07-25 03:30:54 root INFO     [order_1_approx] starting weight calculation for If the present form is unite, the past form is united
If the present form is identify, the past form is identified
If the present form is receive, the past form is received
If the present form is replace, the past form is replaced
If the present form is introduce, the past form is introduced
If the present form is lose, the past form is lost
If the present form is consider, the past form is considered
If the present form is understand, the past form is
2024-07-25 03:30:54 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 03:31:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1953, -0.7695,  0.9062,  ..., -0.4062, -3.6250,  2.2812],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 8.2500,  6.4375, -0.9922,  ..., -7.1250, -6.7188,  3.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.0823e-03,  2.1973e-03, -3.0518e-03,  ...,  2.9907e-03,
          4.8523e-03,  6.6528e-03],
        [-3.2501e-03,  1.7395e-03, -1.1444e-03,  ...,  1.4267e-03,
         -5.7068e-03,  4.1199e-03],
        [-7.8125e-03, -9.7046e-03,  7.8125e-02,  ...,  6.4697e-03,
         -1.6846e-02,  4.0588e-03],
        ...,
        [ 6.3477e-03, -4.5776e-03, -1.1597e-02,  ...,  3.8757e-03,
         -1.0864e-02, -5.9509e-03],
        [ 2.4796e-05,  4.6387e-03, -3.5706e-03,  ..., -3.3569e-04,
          1.3306e-02, -3.9673e-03],
        [ 3.3569e-03, -3.8528e-04, -5.6152e-03,  ..., -2.0599e-03,
         -2.2583e-03,  1.7822e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.6172,  6.0195, -0.1260,  ..., -8.5078, -6.2734,  4.3906]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 03:31:48 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for Something without hair is hairless
Something without error is errorless
Something without window is windowless
Something without remorse is remorseless
Something without path is pathless
Something without mirth is mirthless
Something without friend is friendless
Something without passion is
2024-07-25 03:31:48 root INFO     total operator prediction time: 2871.367842197418 seconds
2024-07-25 03:31:48 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on adj+ness_reg
2024-07-25 03:31:48 root INFO     building operator adj+ness_reg
2024-07-25 03:31:49 root INFO     [order_1_approx] starting weight calculation for The state of being hidden is hiddenness
The state of being happy is happiness
The state of being dangerous is dangerousness
The state of being random is randomness
The state of being useful is usefulness
The state of being helpful is helpfulness
The state of being same is sameness
The state of being odd is
2024-07-25 03:31:49 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 03:36:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.8438, -0.7852,  4.0625,  ...,  2.4062,  0.9609,  1.7656],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  7.5312, -10.5000,   4.2500,  ...,  -7.7812,  -1.6094,  -1.1719],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.0670e-03, -2.1515e-03,  5.1880e-04,  ...,  5.9891e-04,
         -5.1117e-04,  1.6861e-03],
        [ 4.5776e-03, -2.2430e-03,  8.7280e-03,  ..., -2.3193e-03,
          2.5024e-03,  1.3885e-03],
        [-2.6855e-03, -3.2806e-03,  5.0049e-02,  ..., -2.7008e-03,
          3.3569e-03, -1.1444e-04],
        ...,
        [ 5.1880e-03,  4.5776e-05, -2.7313e-03,  ...,  2.7771e-03,
         -6.1951e-03, -1.7738e-04],
        [ 2.5940e-03, -1.0834e-03, -5.2185e-03,  ...,  1.6785e-03,
          1.1169e-02, -3.7079e-03],
        [-2.7008e-03,  9.5215e-03, -1.7700e-03,  ...,  2.3804e-03,
         -4.8828e-03,  4.1504e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  6.8398, -10.6172,   4.9727,  ...,  -8.5078,  -1.7461,  -1.8027]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 03:36:34 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is unite, the past form is united
If the present form is identify, the past form is identified
If the present form is receive, the past form is received
If the present form is replace, the past form is replaced
If the present form is introduce, the past form is introduced
If the present form is lose, the past form is lost
If the present form is consider, the past form is considered
If the present form is understand, the past form is
2024-07-25 03:36:34 root INFO     [order_1_approx] starting weight calculation for If the present form is lose, the past form is lost
If the present form is introduce, the past form is introduced
If the present form is receive, the past form is received
If the present form is replace, the past form is replaced
If the present form is identify, the past form is identified
If the present form is unite, the past form is united
If the present form is understand, the past form is understood
If the present form is consider, the past form is
2024-07-25 03:36:34 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 03:37:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5469, -1.4297,  3.9688,  ...,  2.9219, -1.8438,  0.4062],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 3.0625, 19.0000, -7.2500,  ..., -7.3438, -9.7500, -1.2656],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 9.7656e-04, -2.0599e-03, -4.5471e-03,  ...,  3.0365e-03,
         -8.3923e-05, -1.1826e-03],
        [ 1.8234e-03,  3.8300e-03,  1.6174e-03,  ..., -1.5030e-03,
         -2.7771e-03, -2.8381e-03],
        [ 5.1880e-03, -1.6327e-03,  7.1777e-02,  ...,  2.4414e-03,
          5.7983e-04,  2.5482e-03],
        ...,
        [-6.8054e-03,  1.1139e-03, -1.4648e-02,  ...,  2.7008e-03,
         -2.3499e-03,  6.5308e-03],
        [-8.6594e-04,  1.8768e-03, -9.5825e-03,  ..., -3.9978e-03,
          1.7456e-02,  1.0986e-02],
        [-1.9226e-03,  2.7466e-03, -1.9836e-03,  ..., -6.4087e-03,
          5.3406e-05,  1.4648e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.3867, 15.7812, -5.9570,  ..., -5.1445, -7.1602, -1.0869]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 03:37:43 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The state of being hidden is hiddenness
The state of being happy is happiness
The state of being dangerous is dangerousness
The state of being random is randomness
The state of being useful is usefulness
The state of being helpful is helpfulness
The state of being same is sameness
The state of being odd is
2024-07-25 03:37:44 root INFO     [order_1_approx] starting weight calculation for The state of being odd is oddness
The state of being hidden is hiddenness
The state of being random is randomness
The state of being same is sameness
The state of being helpful is helpfulness
The state of being useful is usefulness
The state of being dangerous is dangerousness
The state of being happy is
2024-07-25 03:37:44 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 03:42:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.5938, -0.7109,  1.5156,  ...,  1.3828, -0.1016,  1.8984],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-1.6172,  1.0859, -0.6953,  ..., -6.6250,  0.0000, -7.8438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0028, -0.0032, -0.0004,  ..., -0.0038,  0.0003, -0.0012],
        [-0.0036,  0.0013,  0.0106,  ..., -0.0014, -0.0019,  0.0009],
        [-0.0039, -0.0034,  0.0576,  ..., -0.0020,  0.0042, -0.0004],
        ...,
        [ 0.0060, -0.0005,  0.0010,  ...,  0.0019, -0.0032,  0.0058],
        [ 0.0020,  0.0019, -0.0056,  ..., -0.0001,  0.0078, -0.0007],
        [ 0.0059,  0.0123, -0.0089,  ...,  0.0007, -0.0059,  0.0044]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -1.4658,   1.4473,   0.8447,  ...,  -7.5859,  -0.3313, -11.5625]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 03:42:15 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is lose, the past form is lost
If the present form is introduce, the past form is introduced
If the present form is receive, the past form is received
If the present form is replace, the past form is replaced
If the present form is identify, the past form is identified
If the present form is unite, the past form is united
If the present form is understand, the past form is understood
If the present form is consider, the past form is
2024-07-25 03:42:15 root INFO     [order_1_approx] starting weight calculation for If the present form is consider, the past form is considered
If the present form is lose, the past form is lost
If the present form is receive, the past form is received
If the present form is unite, the past form is united
If the present form is introduce, the past form is introduced
If the present form is understand, the past form is understood
If the present form is identify, the past form is identified
If the present form is replace, the past form is
2024-07-25 03:42:15 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 03:43:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8164, -0.6953,  2.3750,  ...,  1.1562, -1.7422,  2.1562],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -4.2500,  23.3750,  -9.6875,  ...,  -6.8750,  -9.6250, -10.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0038,  0.0023, -0.0025,  ..., -0.0028,  0.0098, -0.0028],
        [-0.0002,  0.0032,  0.0023,  ...,  0.0028, -0.0018,  0.0048],
        [-0.0027, -0.0016,  0.0439,  ...,  0.0095, -0.0042,  0.0025],
        ...,
        [ 0.0021,  0.0020, -0.0005,  ..., -0.0014,  0.0017,  0.0010],
        [ 0.0003,  0.0010, -0.0001,  ...,  0.0004,  0.0127,  0.0021],
        [ 0.0004, -0.0002,  0.0076,  ..., -0.0020, -0.0048,  0.0029]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -4.8164,  23.2344,  -9.1250,  ...,  -5.8125,  -9.8125, -10.4609]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 03:43:33 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The state of being odd is oddness
The state of being hidden is hiddenness
The state of being random is randomness
The state of being same is sameness
The state of being helpful is helpfulness
The state of being useful is usefulness
The state of being dangerous is dangerousness
The state of being happy is
2024-07-25 03:43:33 root INFO     [order_1_approx] starting weight calculation for The state of being happy is happiness
The state of being dangerous is dangerousness
The state of being odd is oddness
The state of being hidden is hiddenness
The state of being same is sameness
The state of being random is randomness
The state of being helpful is helpfulness
The state of being useful is
2024-07-25 03:43:33 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 03:47:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5781, -0.2109, -0.9688,  ..., -0.1172,  0.9336,  2.6562],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  7.7812,  -4.4375,  -0.9062,  ..., -11.6875,  -5.6875,  -5.9688],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.7161e-03,  2.2507e-04, -8.9722e-03,  ..., -5.2185e-03,
         -3.2501e-03, -1.4954e-03],
        [ 1.9684e-03,  1.1139e-03,  1.4526e-02,  ..., -2.6855e-03,
          2.0142e-03,  1.7853e-03],
        [-2.3956e-03,  1.0071e-03,  6.2256e-02,  ...,  2.5330e-03,
          6.5002e-03,  3.2806e-03],
        ...,
        [-1.0681e-03, -5.0964e-03, -2.8534e-03,  ...,  8.3542e-04,
         -8.3923e-05,  3.3264e-03],
        [-1.0452e-03, -1.9226e-03,  4.2725e-04,  ...,  2.1820e-03,
          1.0620e-02, -2.1973e-03],
        [-7.7438e-04,  5.2643e-04, -6.1340e-03,  ..., -5.8289e-03,
          4.9438e-03,  1.0803e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  5.3711,  -3.2500,   1.8027,  ..., -12.7891,  -4.6094,  -7.4648]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 03:47:57 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is consider, the past form is considered
If the present form is lose, the past form is lost
If the present form is receive, the past form is received
If the present form is unite, the past form is united
If the present form is introduce, the past form is introduced
If the present form is understand, the past form is understood
If the present form is identify, the past form is identified
If the present form is replace, the past form is
2024-07-25 03:47:57 root INFO     [order_1_approx] starting weight calculation for If the present form is consider, the past form is considered
If the present form is understand, the past form is understood
If the present form is lose, the past form is lost
If the present form is identify, the past form is identified
If the present form is replace, the past form is replaced
If the present form is introduce, the past form is introduced
If the present form is receive, the past form is received
If the present form is unite, the past form is
2024-07-25 03:47:57 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 03:49:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8828,  0.9414,  0.8438,  ...,  0.5625, -1.5312,  2.6562],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-1.5625e-02,  2.2375e+01, -1.5500e+01,  ..., -9.3750e+00,
        -6.0938e+00, -9.5625e+00], device='cuda:0', dtype=torch.float16,
       grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0043, -0.0021, -0.0049,  ...,  0.0016,  0.0013,  0.0036],
        [ 0.0011,  0.0095,  0.0052,  ..., -0.0069,  0.0012,  0.0016],
        [-0.0100,  0.0002,  0.0664,  ...,  0.0079,  0.0007, -0.0050],
        ...,
        [-0.0006,  0.0008,  0.0010,  ...,  0.0012, -0.0037,  0.0055],
        [-0.0011, -0.0025,  0.0147,  ...,  0.0061,  0.0092, -0.0027],
        [ 0.0030,  0.0016,  0.0022,  ..., -0.0030, -0.0005,  0.0132]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  0.8247,  20.3125, -13.3672,  ...,  -7.3281,  -7.1953, -10.1016]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 03:49:33 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The state of being happy is happiness
The state of being dangerous is dangerousness
The state of being odd is oddness
The state of being hidden is hiddenness
The state of being same is sameness
The state of being random is randomness
The state of being helpful is helpfulness
The state of being useful is
2024-07-25 03:49:33 root INFO     [order_1_approx] starting weight calculation for The state of being useful is usefulness
The state of being random is randomness
The state of being hidden is hiddenness
The state of being odd is oddness
The state of being happy is happiness
The state of being helpful is helpfulness
The state of being dangerous is dangerousness
The state of being same is
2024-07-25 03:49:33 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 03:53:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5625, -1.1016, -1.2734,  ...,  0.0801,  0.9141,  2.3281],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([13.6875, -0.0374,  4.3750,  ..., -1.3047, -5.9375, -0.4297],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0052,  0.0006, -0.0021,  ..., -0.0035, -0.0008,  0.0018],
        [-0.0039, -0.0007,  0.0041,  ..., -0.0016, -0.0030,  0.0012],
        [-0.0059,  0.0008,  0.0449,  ...,  0.0054,  0.0063, -0.0007],
        ...,
        [-0.0083,  0.0003, -0.0107,  ...,  0.0027,  0.0021,  0.0034],
        [ 0.0053, -0.0003,  0.0001,  ...,  0.0014,  0.0139, -0.0002],
        [ 0.0047,  0.0020, -0.0101,  ...,  0.0006,  0.0006,  0.0089]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[12.6406, -2.6426,  5.2148,  ..., -1.6768, -5.2539, -2.2344]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 03:53:33 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is consider, the past form is considered
If the present form is understand, the past form is understood
If the present form is lose, the past form is lost
If the present form is identify, the past form is identified
If the present form is replace, the past form is replaced
If the present form is introduce, the past form is introduced
If the present form is receive, the past form is received
If the present form is unite, the past form is
2024-07-25 03:53:34 root INFO     [order_1_approx] starting weight calculation for If the present form is unite, the past form is united
If the present form is replace, the past form is replaced
If the present form is lose, the past form is lost
If the present form is understand, the past form is understood
If the present form is identify, the past form is identified
If the present form is consider, the past form is considered
If the present form is receive, the past form is received
If the present form is introduce, the past form is
2024-07-25 03:53:34 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 03:55:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6953, -0.7148, -1.2578,  ...,  1.0000,  0.1934,  1.3281],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  1.0000,  12.1250, -14.5625,  ...,  -5.5312,  -9.3750,  -4.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0028, -0.0045,  0.0011,  ...,  0.0043,  0.0002, -0.0022],
        [-0.0094, -0.0026,  0.0081,  ...,  0.0011, -0.0061, -0.0087],
        [-0.0064,  0.0046,  0.0522,  ...,  0.0064,  0.0034, -0.0012],
        ...,
        [ 0.0007, -0.0035,  0.0081,  ...,  0.0073, -0.0040, -0.0003],
        [ 0.0013,  0.0031, -0.0159,  ..., -0.0055,  0.0121,  0.0031],
        [-0.0003,  0.0031,  0.0116,  ..., -0.0077, -0.0060,  0.0019]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  2.2285,  11.0078, -13.1484,  ...,  -4.2344,  -9.9297,  -3.6484]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 03:55:24 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The state of being useful is usefulness
The state of being random is randomness
The state of being hidden is hiddenness
The state of being odd is oddness
The state of being happy is happiness
The state of being helpful is helpfulness
The state of being dangerous is dangerousness
The state of being same is
2024-07-25 03:55:25 root INFO     [order_1_approx] starting weight calculation for The state of being useful is usefulness
The state of being odd is oddness
The state of being hidden is hiddenness
The state of being helpful is helpfulness
The state of being same is sameness
The state of being happy is happiness
The state of being dangerous is dangerousness
The state of being random is
2024-07-25 03:55:25 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 03:59:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.6875, -1.3516,  0.3516,  ...,  0.3809,  0.2734,  2.7656],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 6.0938, -4.7500,  8.8125,  ..., -2.9688,  5.7812,  0.2061],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.5787e-03, -1.1444e-05, -5.4932e-04,  ..., -2.3346e-03,
          1.7242e-03,  2.0905e-03],
        [ 4.6387e-03,  3.2043e-03,  4.8218e-03,  ..., -1.2131e-03,
         -5.6763e-03, -4.9438e-03],
        [-4.4861e-03,  1.7166e-03,  5.5176e-02,  ..., -6.6528e-03,
          2.7924e-03, -5.0354e-04],
        ...,
        [-6.1417e-04,  2.0905e-03, -4.7302e-03,  ...,  1.5488e-03,
         -8.0109e-04,  3.2425e-05],
        [-1.0376e-03, -6.6376e-04, -1.6937e-03,  ...,  7.9727e-04,
          9.6436e-03,  3.2654e-03],
        [ 2.6398e-03,  7.5989e-03,  2.0752e-03,  ...,  8.1635e-04,
          1.4343e-03,  4.3030e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.7656, -6.8711, 10.1094,  ..., -3.6758,  5.5664, -0.8721]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 03:59:22 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If the present form is unite, the past form is united
If the present form is replace, the past form is replaced
If the present form is lose, the past form is lost
If the present form is understand, the past form is understood
If the present form is identify, the past form is identified
If the present form is consider, the past form is considered
If the present form is receive, the past form is received
If the present form is introduce, the past form is
2024-07-25 03:59:22 root INFO     total operator prediction time: 2724.599872112274 seconds
2024-07-25 03:59:22 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on verb_inf - 3pSg
2024-07-25 03:59:22 root INFO     building operator verb_inf - 3pSg
2024-07-25 03:59:23 root INFO     [order_1_approx] starting weight calculation for I involve, he involves
I follow, he follows
I apply, he applies
I continue, he continues
I refer, he refers
I describe, he describes
I exist, he exists
I allow, he
2024-07-25 03:59:23 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 04:01:23 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6133,  2.5312,  2.1406,  ...,  2.2500, -2.5312,  2.8906],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -9.8750,  26.1250, -17.8750,  ..., -14.2500,  -6.1250, -12.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.0910e-03,  9.1553e-04, -2.4414e-03,  ...,  2.3346e-03,
          6.4697e-03,  5.0354e-04],
        [ 5.9128e-04, -2.8610e-06,  6.1035e-03,  ..., -1.3580e-03,
          1.3504e-03,  1.2875e-04],
        [-5.4016e-03,  2.9755e-03,  5.5176e-02,  ...,  1.3550e-02,
         -6.3477e-03, -2.3804e-03],
        ...,
        [ 5.8594e-03, -3.2959e-03, -3.5553e-03,  ...,  1.3885e-03,
         -5.6458e-03,  3.9368e-03],
        [-3.5553e-03,  2.6550e-03,  7.2021e-03,  ...,  1.3428e-03,
          2.1973e-02, -1.9150e-03],
        [ 9.9945e-04,  1.2970e-03,  1.0254e-02,  ...,  2.7313e-03,
         -3.2349e-03,  8.8501e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -8.3438,  24.1406, -18.8281,  ..., -14.5078,  -6.4180, -14.5938]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 04:01:24 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The state of being useful is usefulness
The state of being odd is oddness
The state of being hidden is hiddenness
The state of being helpful is helpfulness
The state of being same is sameness
The state of being happy is happiness
The state of being dangerous is dangerousness
The state of being random is
2024-07-25 04:01:25 root INFO     [order_1_approx] starting weight calculation for The state of being same is sameness
The state of being odd is oddness
The state of being dangerous is dangerousness
The state of being random is randomness
The state of being happy is happiness
The state of being useful is usefulness
The state of being helpful is helpfulness
The state of being hidden is
2024-07-25 04:01:25 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 04:05:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4375, -1.4844,  6.2500,  ...,  0.7031, -1.3125,  0.5312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([14.5625, -8.5625,  1.6719,  ..., -6.4688,  4.0625, -8.9375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.3916e-02, -9.3994e-03,  1.1719e-02,  ..., -3.2349e-03,
          5.6763e-03, -8.7891e-03],
        [-1.3306e-02,  1.4038e-02, -2.0020e-02,  ...,  2.6550e-03,
         -5.4932e-04,  9.6436e-03],
        [-1.4877e-04,  6.4087e-03,  5.4443e-02,  ...,  3.4485e-03,
         -7.6904e-03, -1.0300e-04],
        ...,
        [ 3.9062e-03,  3.8757e-03, -2.8381e-03,  ..., -1.9989e-03,
          0.0000e+00, -1.8387e-03],
        [-9.7275e-04, -2.6894e-04,  1.5259e-03,  ...,  5.1117e-04,
          8.6670e-03, -3.8300e-03],
        [ 4.4861e-03, -2.5330e-03,  8.4229e-03,  ...,  6.1035e-05,
          1.4648e-03,  2.6093e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 16.0312, -10.2734,   1.2920,  ...,  -7.6445,   3.9297,  -9.0859]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 04:05:02 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for I involve, he involves
I follow, he follows
I apply, he applies
I continue, he continues
I refer, he refers
I describe, he describes
I exist, he exists
I allow, he
2024-07-25 04:05:02 root INFO     [order_1_approx] starting weight calculation for I exist, he exists
I follow, he follows
I involve, he involves
I apply, he applies
I describe, he describes
I allow, he allows
I refer, he refers
I continue, he
2024-07-25 04:05:02 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 04:07:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0078, -0.9258, -5.0312,  ..., -0.1172, -0.5000,  1.7969],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-11.8750,  23.5000,  -8.5625,  ..., -14.0000,  -1.0469,  -1.9844],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 5.0964e-03, -2.4414e-03, -6.1340e-03,  ..., -7.8964e-04,
          3.3379e-04,  1.3123e-03],
        [ 3.2806e-03,  1.2207e-03,  9.8877e-03,  ..., -6.1340e-03,
         -5.2490e-03, -1.5182e-03],
        [-5.4626e-03, -3.7537e-03,  5.3467e-02,  ...,  5.9204e-03,
         -4.4861e-03,  7.2479e-05],
        ...,
        [-2.3499e-03, -1.8768e-03, -1.1597e-02,  ...,  1.6937e-03,
         -4.1199e-03, -4.1962e-05],
        [-4.1809e-03, -2.9602e-03, -6.1951e-03,  ...,  2.7771e-03,
          5.2490e-03,  0.0000e+00],
        [-1.0910e-03,  5.3406e-03, -6.1646e-03,  ...,  2.5787e-03,
         -9.8267e-03,  1.3306e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-12.0312,  22.1250,  -6.5156,  ..., -14.5000,  -1.6309,  -2.6270]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 04:07:21 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The state of being same is sameness
The state of being odd is oddness
The state of being dangerous is dangerousness
The state of being random is randomness
The state of being happy is happiness
The state of being useful is usefulness
The state of being helpful is helpfulness
The state of being hidden is
2024-07-25 04:07:21 root INFO     [order_1_approx] starting weight calculation for The state of being happy is happiness
The state of being random is randomness
The state of being useful is usefulness
The state of being odd is oddness
The state of being same is sameness
The state of being helpful is helpfulness
The state of being hidden is hiddenness
The state of being dangerous is
2024-07-25 04:07:21 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 04:10:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3926, -1.8906,  3.4062,  ..., -0.6055, -0.9453,  0.1992],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 17.2500,  -0.6406,  15.8750,  ..., -11.2500,  -5.7188, -12.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 6.5002e-03, -2.5940e-04,  6.2866e-03,  ...,  2.0142e-03,
          4.6997e-03, -2.2736e-03],
        [-9.9945e-04,  8.5449e-03,  8.3618e-03,  ..., -2.9602e-03,
          3.7842e-03,  6.5231e-04],
        [-1.8158e-03,  6.6223e-03,  6.4453e-02,  ..., -5.3406e-05,
          3.9291e-04, -2.7008e-03],
        ...,
        [-2.9297e-03,  1.5259e-03,  3.1891e-03,  ..., -6.9618e-05,
         -6.4087e-03, -1.5335e-03],
        [-8.6975e-04, -2.5940e-03,  2.0142e-03,  ...,  3.2043e-03,
          7.4158e-03,  4.0054e-04],
        [ 5.5847e-03,  5.9509e-04, -2.6093e-03,  ..., -2.9945e-04,
         -6.6833e-03,  2.2736e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 17.5625,  -0.1628,  17.5312,  ..., -11.2812,  -6.3281, -13.8672]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 04:10:36 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for I exist, he exists
I follow, he follows
I involve, he involves
I apply, he applies
I describe, he describes
I allow, he allows
I refer, he refers
I continue, he
2024-07-25 04:10:37 root INFO     [order_1_approx] starting weight calculation for I involve, he involves
I apply, he applies
I describe, he describes
I follow, he follows
I allow, he allows
I continue, he continues
I exist, he exists
I refer, he
2024-07-25 04:10:37 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 04:13:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3027, -0.5234, -0.7656,  ..., -0.0703, -1.5000,  1.0234],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -5.2500,  23.6250, -12.8125,  ..., -20.0000,   0.1406,  -4.5938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 6.2256e-03, -1.1673e-03,  8.6975e-04,  ..., -2.2888e-03,
          3.9062e-03, -1.4801e-03],
        [-3.8147e-06,  1.6556e-03,  4.8523e-03,  ..., -3.0518e-03,
         -5.9891e-04, -1.6556e-03],
        [-4.7302e-03,  1.0223e-03,  4.0771e-02,  ...,  3.4485e-03,
         -9.4604e-03,  1.2436e-03],
        ...,
        [ 1.0986e-03, -2.6093e-03,  1.2054e-03,  ...,  4.1199e-03,
         -5.0306e-05, -1.9379e-03],
        [-2.5940e-03,  2.7771e-03, -1.1353e-02,  ...,  1.8082e-03,
          3.9062e-03,  2.0504e-04],
        [ 1.9379e-03, -3.9482e-04,  1.2512e-03,  ..., -1.5106e-03,
         -2.1057e-03,  8.3618e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -5.1289,  22.6875, -11.5938,  ..., -20.2188,  -1.0352,  -5.5078]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 04:13:05 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The state of being happy is happiness
The state of being random is randomness
The state of being useful is usefulness
The state of being odd is oddness
The state of being same is sameness
The state of being helpful is helpfulness
The state of being hidden is hiddenness
The state of being dangerous is
2024-07-25 04:13:06 root INFO     [order_1_approx] starting weight calculation for The state of being hidden is hiddenness
The state of being dangerous is dangerousness
The state of being odd is oddness
The state of being random is randomness
The state of being happy is happiness
The state of being same is sameness
The state of being useful is usefulness
The state of being helpful is
2024-07-25 04:13:06 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 04:16:04 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.1562, -0.3984,  5.0938,  ...,  1.3750, -3.0312, -0.3711],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 22.7500,  -4.2812,   5.3438,  ...,  -7.8438,  13.3125, -11.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0067, -0.0144,  0.0070,  ...,  0.0032,  0.0018, -0.0028],
        [ 0.0043,  0.0249, -0.0058,  ..., -0.0009, -0.0009,  0.0026],
        [-0.0007,  0.0171,  0.0583,  ...,  0.0012,  0.0043, -0.0048],
        ...,
        [ 0.0030, -0.0008, -0.0013,  ...,  0.0028, -0.0038,  0.0015],
        [-0.0015,  0.0013, -0.0087,  ...,  0.0014,  0.0038, -0.0009],
        [ 0.0032,  0.0033,  0.0006,  ..., -0.0022, -0.0021,  0.0099]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 22.1562,  -4.1133,   7.1094,  ...,  -7.2305,  15.2344, -12.3672]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 04:16:05 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for I involve, he involves
I apply, he applies
I describe, he describes
I follow, he follows
I allow, he allows
I continue, he continues
I exist, he exists
I refer, he
2024-07-25 04:16:05 root INFO     [order_1_approx] starting weight calculation for I follow, he follows
I continue, he continues
I refer, he refers
I describe, he describes
I involve, he involves
I apply, he applies
I allow, he allows
I exist, he
2024-07-25 04:16:05 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 04:18:46 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.9062,  0.8750,  1.4297,  ...,  0.2344, -1.8516,  2.7969],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -2.2031,  23.7500, -10.7500,  ..., -14.8750, -10.1875,  -3.6719],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-0.0005, -0.0049, -0.0081,  ...,  0.0014,  0.0060,  0.0047],
        [ 0.0010,  0.0029,  0.0072,  ..., -0.0033,  0.0009, -0.0012],
        [-0.0071, -0.0028,  0.0581,  ...,  0.0059, -0.0052, -0.0067],
        ...,
        [ 0.0030, -0.0004,  0.0109,  ...,  0.0016, -0.0031, -0.0026],
        [-0.0022,  0.0019, -0.0011,  ...,  0.0047,  0.0106, -0.0062],
        [-0.0015,  0.0010,  0.0137,  ..., -0.0037, -0.0036,  0.0100]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -0.8652,  21.6406,  -9.3125,  ..., -15.1172, -11.0938,  -3.6543]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 04:18:46 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The state of being hidden is hiddenness
The state of being dangerous is dangerousness
The state of being odd is oddness
The state of being random is randomness
The state of being happy is happiness
The state of being same is sameness
The state of being useful is usefulness
The state of being helpful is
2024-07-25 04:18:46 root INFO     total operator prediction time: 2817.964026927948 seconds
2024-07-25 04:18:46 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on re+verb_reg
2024-07-25 04:18:46 root INFO     building operator re+verb_reg
2024-07-25 04:18:47 root INFO     [order_1_approx] starting weight calculation for To adjust again is to readjust
To assure again is to reassure
To examine again is to reexamine
To calculate again is to recalculate
To locate again is to relocate
To introduce again is to reintroduce
To unite again is to reunite
To distribute again is to
2024-07-25 04:18:47 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 04:21:20 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0547,  0.8008,  5.5000,  ..., -0.3984, -0.0781,  1.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 21.1250, -13.1250,  13.2500,  ...,  -9.6875,   5.9375,  -1.8125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-0.0010, -0.0013, -0.0053,  ..., -0.0046,  0.0019, -0.0008],
        [ 0.0023,  0.0072,  0.0015,  ...,  0.0008,  0.0003, -0.0004],
        [-0.0015,  0.0038,  0.0708,  ...,  0.0064,  0.0030, -0.0004],
        ...,
        [-0.0034,  0.0019,  0.0009,  ..., -0.0049, -0.0044, -0.0045],
        [-0.0021, -0.0086,  0.0035,  ..., -0.0074,  0.0082,  0.0003],
        [ 0.0020,  0.0068,  0.0038,  ...,  0.0074,  0.0044,  0.0058]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 21.1562, -14.3906,  12.6875,  ...,  -8.5391,   7.6328,  -1.9092]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 04:21:20 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for I follow, he follows
I continue, he continues
I refer, he refers
I describe, he describes
I involve, he involves
I apply, he applies
I allow, he allows
I exist, he
2024-07-25 04:21:21 root INFO     [order_1_approx] starting weight calculation for I follow, he follows
I continue, he continues
I refer, he refers
I allow, he allows
I describe, he describes
I apply, he applies
I exist, he exists
I involve, he
2024-07-25 04:21:21 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 04:24:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1953, -1.4219,  2.4062,  ...,  1.0703,  1.5625,  1.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 23.7500,  -2.7812,  -2.8281,  ..., -11.2500,   5.2812,  -9.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-0.0010, -0.0046,  0.0010,  ...,  0.0003,  0.0005, -0.0067],
        [ 0.0046,  0.0023, -0.0049,  ..., -0.0022, -0.0041,  0.0015],
        [ 0.0033,  0.0038,  0.0581,  ..., -0.0071,  0.0019, -0.0102],
        ...,
        [ 0.0013,  0.0014,  0.0075,  ...,  0.0055,  0.0054,  0.0037],
        [-0.0008, -0.0028, -0.0052,  ..., -0.0022,  0.0040,  0.0021],
        [ 0.0004, -0.0051,  0.0004,  ..., -0.0033,  0.0006,  0.0110]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 24.3125,  -4.0469,  -4.1016,  ..., -11.5312,   4.5820,  -8.0078]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 04:24:38 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To adjust again is to readjust
To assure again is to reassure
To examine again is to reexamine
To calculate again is to recalculate
To locate again is to relocate
To introduce again is to reintroduce
To unite again is to reunite
To distribute again is to
2024-07-25 04:24:38 root INFO     [order_1_approx] starting weight calculation for To unite again is to reunite
To distribute again is to redistribute
To locate again is to relocate
To adjust again is to readjust
To introduce again is to reintroduce
To assure again is to reassure
To calculate again is to recalculate
To examine again is to
2024-07-25 04:24:39 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 04:26:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.0469, -0.2236,  7.0312,  ...,  1.2422, -1.2812,  0.8672],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 18.5000,  -8.5625,  11.2500,  ..., -14.8750,  10.7500,  -2.2188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 9.3994e-03, -2.7924e-03, -1.2878e-02,  ...,  5.0354e-03,
         -2.1667e-03,  1.6594e-04],
        [-1.9989e-03,  6.5918e-03,  1.2817e-02,  ..., -6.7902e-04,
          8.5831e-04,  3.9062e-03],
        [-2.7847e-04, -3.4790e-03,  9.0332e-02,  ..., -1.1749e-03,
          6.6223e-03, -2.9602e-03],
        ...,
        [-1.4801e-03, -1.6632e-03, -5.1880e-03,  ...,  3.4332e-04,
         -2.7313e-03, -6.4697e-03],
        [ 1.3428e-03, -1.5106e-03, -2.7771e-03,  ..., -1.2207e-04,
          2.5146e-02,  3.3417e-03],
        [ 2.8076e-03,  3.7689e-03,  1.0834e-03,  ..., -1.7700e-03,
          8.2016e-05,  9.6436e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 18.2812,  -7.0312,  15.1953,  ..., -14.9766,  12.2422,  -2.0156]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 04:26:31 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for I follow, he follows
I continue, he continues
I refer, he refers
I allow, he allows
I describe, he describes
I apply, he applies
I exist, he exists
I involve, he
2024-07-25 04:26:31 root INFO     [order_1_approx] starting weight calculation for I refer, he refers
I exist, he exists
I continue, he continues
I allow, he allows
I apply, he applies
I follow, he follows
I involve, he involves
I describe, he
2024-07-25 04:26:31 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 04:30:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 4.5938, -2.2500,  3.6406,  ...,  0.9688, -0.7148,  1.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 12.6250,   4.8125,  -8.8750,  ...,   2.4062, -12.8125,  12.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-1.0529e-03, -6.1035e-04,  6.5918e-03,  ...,  4.4250e-03,
          5.3711e-03, -4.8828e-03],
        [ 2.1667e-03,  1.3657e-03,  1.3657e-03,  ..., -1.1215e-03,
         -8.1253e-04, -3.7384e-04],
        [ 7.9346e-03, -6.1035e-05,  6.1768e-02,  ...,  7.3242e-04,
         -1.5640e-04, -3.9673e-04],
        ...,
        [-9.9487e-03,  7.1411e-03,  1.6556e-03,  ...,  4.9133e-03,
         -4.7302e-03,  6.3171e-03],
        [ 8.3008e-03, -3.9673e-03,  7.1411e-03,  ...,  2.1667e-03,
          1.0437e-02, -6.1035e-04],
        [-1.1635e-04, -5.6839e-04, -9.5215e-03,  ..., -3.5095e-04,
         -1.1139e-03,  8.6670e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 12.6641,   5.4453,  -9.9609,  ...,   1.2744, -12.2344,  12.6250]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 04:30:32 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To unite again is to reunite
To distribute again is to redistribute
To locate again is to relocate
To adjust again is to readjust
To introduce again is to reintroduce
To assure again is to reassure
To calculate again is to recalculate
To examine again is to
2024-07-25 04:30:32 root INFO     [order_1_approx] starting weight calculation for To calculate again is to recalculate
To assure again is to reassure
To distribute again is to redistribute
To locate again is to relocate
To introduce again is to reintroduce
To adjust again is to readjust
To examine again is to reexamine
To unite again is to
2024-07-25 04:30:32 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 04:32:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8516, -0.5547,  6.5625,  ...,  1.1328, -2.1719,  2.2969],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 24.7500, -14.1875,   9.8750,  ..., -13.1250,   1.9922,  -6.4375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 6.6833e-03, -1.8463e-03, -2.4414e-04,  ...,  2.6703e-04,
         -1.0681e-04, -1.7090e-03],
        [ 1.6327e-03,  4.6997e-03,  4.9744e-03,  ..., -5.9128e-04,
          4.0588e-03,  1.0452e-03],
        [-1.6937e-03,  2.0447e-03,  7.3730e-02,  ...,  1.9073e-03,
         -1.0010e-02, -3.7842e-03],
        ...,
        [ 4.1199e-03, -4.9591e-05, -3.0518e-03,  ...,  1.0300e-03,
         -1.3580e-03, -4.2725e-04],
        [-4.8218e-03, -3.9673e-03,  7.6599e-03,  ..., -1.9455e-03,
          1.1719e-02, -2.8534e-03],
        [ 3.7842e-03, -6.1035e-04, -4.6997e-03,  ..., -1.7548e-03,
         -5.3101e-03,  5.9204e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 24.9844, -12.7969,  10.0078,  ..., -12.7266,   2.8184,  -8.3906]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 04:32:14 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for I refer, he refers
I exist, he exists
I continue, he continues
I allow, he allows
I apply, he applies
I follow, he follows
I involve, he involves
I describe, he
2024-07-25 04:32:15 root INFO     [order_1_approx] starting weight calculation for I allow, he allows
I apply, he applies
I exist, he exists
I continue, he continues
I describe, he describes
I refer, he refers
I involve, he involves
I follow, he
2024-07-25 04:32:15 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 04:36:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2031, -1.8047, -4.7500,  ...,  0.1992,  0.9219,  2.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 34.0000,  -7.0938,  -8.6250,  ...,  -5.5938, -22.7500,   4.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.2278e-03,  7.1526e-05,  5.3711e-03,  ...,  1.6785e-03,
         -3.2959e-03, -1.4801e-03],
        [ 1.0681e-04, -2.9144e-03,  1.2817e-02,  ...,  2.5177e-03,
         -8.9111e-03,  7.4005e-04],
        [-8.8501e-04,  9.3384e-03,  7.2266e-02,  ..., -7.0953e-04,
          9.6436e-03, -9.1553e-03],
        ...,
        [ 1.6785e-03, -7.0953e-04, -7.7820e-03,  ...,  1.1826e-03,
         -7.4768e-03,  1.0620e-02],
        [ 4.1962e-04,  1.4343e-03,  7.4463e-03,  ...,  5.9509e-03,
          3.5706e-03,  8.6670e-03],
        [ 4.8637e-04, -5.3406e-03, -1.0132e-02,  ...,  4.6082e-03,
         -1.2939e-02,  8.9111e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 33.3438,  -5.7852,  -7.8320,  ...,  -6.0625, -22.2656,   6.5469]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 04:36:35 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To calculate again is to recalculate
To assure again is to reassure
To distribute again is to redistribute
To locate again is to relocate
To introduce again is to reintroduce
To adjust again is to readjust
To examine again is to reexamine
To unite again is to
2024-07-25 04:36:35 root INFO     [order_1_approx] starting weight calculation for To adjust again is to readjust
To assure again is to reassure
To distribute again is to redistribute
To calculate again is to recalculate
To examine again is to reexamine
To introduce again is to reintroduce
To unite again is to reunite
To locate again is to
2024-07-25 04:36:35 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 04:37:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.5469, -3.3125,  6.7188,  ...,  0.6484, -0.9922,  1.2578],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 25.3750, -10.6875,  11.8750,  ...,  -0.0938,   6.5000,  -9.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0066, -0.0097,  0.0036,  ...,  0.0063, -0.0095,  0.0067],
        [-0.0034,  0.0088, -0.0051,  ..., -0.0054,  0.0141, -0.0106],
        [ 0.0042, -0.0022,  0.0544,  ...,  0.0031, -0.0121, -0.0032],
        ...,
        [ 0.0008, -0.0019,  0.0060,  ...,  0.0008, -0.0038, -0.0028],
        [ 0.0010, -0.0046,  0.0054,  ...,  0.0016,  0.0054,  0.0016],
        [ 0.0019, -0.0015,  0.0031,  ...,  0.0019,  0.0001,  0.0056]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[24.8125, -8.3281, 10.7969,  ...,  0.2505,  6.1289, -8.8984]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 04:37:56 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for I allow, he allows
I apply, he applies
I exist, he exists
I continue, he continues
I describe, he describes
I refer, he refers
I involve, he involves
I follow, he
2024-07-25 04:37:57 root INFO     [order_1_approx] starting weight calculation for I describe, he describes
I follow, he follows
I refer, he refers
I involve, he involves
I exist, he exists
I allow, he allows
I continue, he continues
I apply, he
2024-07-25 04:37:57 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 04:42:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 3.1094e+00, -1.0859e+00,  4.7188e+00,  ...,  2.3438e-01,
        -3.7695e-01,  9.7656e-04], device='cuda:0', dtype=torch.float16,
       grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 27.0000, -11.0625,   2.1406,  ...,   1.0938,  -8.0000,   2.0781],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0040,  0.0009, -0.0045,  ..., -0.0102,  0.0120,  0.0093],
        [ 0.0013,  0.0073,  0.0104,  ...,  0.0023,  0.0018,  0.0005],
        [ 0.0016,  0.0068,  0.0503,  ...,  0.0017,  0.0044, -0.0032],
        ...,
        [-0.0042,  0.0044, -0.0069,  ..., -0.0025,  0.0021,  0.0079],
        [-0.0015,  0.0050,  0.0002,  ...,  0.0063,  0.0105,  0.0038],
        [-0.0050, -0.0026, -0.0039,  ..., -0.0004,  0.0012,  0.0114]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[25.2188, -9.6953,  1.7666,  ...,  0.6914, -8.2656,  0.8945]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 04:42:31 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To adjust again is to readjust
To assure again is to reassure
To distribute again is to redistribute
To calculate again is to recalculate
To examine again is to reexamine
To introduce again is to reintroduce
To unite again is to reunite
To locate again is to
2024-07-25 04:42:32 root INFO     [order_1_approx] starting weight calculation for To adjust again is to readjust
To locate again is to relocate
To distribute again is to redistribute
To unite again is to reunite
To introduce again is to reintroduce
To examine again is to reexamine
To calculate again is to recalculate
To assure again is to
2024-07-25 04:42:32 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 04:43:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.6875, -0.4023,  9.1250,  ...,  1.4297, -1.9922,  1.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 24.2500, -13.1250,  16.6250,  ...,  -4.4375,   3.3125,  -4.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0045, -0.0079, -0.0291,  ..., -0.0007, -0.0052, -0.0114],
        [ 0.0031,  0.0092,  0.0254,  ..., -0.0002,  0.0030,  0.0120],
        [ 0.0039, -0.0002,  0.0630,  ...,  0.0031, -0.0042,  0.0007],
        ...,
        [ 0.0015,  0.0032, -0.0009,  ...,  0.0001, -0.0008,  0.0002],
        [ 0.0008, -0.0022,  0.0025,  ..., -0.0002,  0.0078,  0.0010],
        [ 0.0016, -0.0023, -0.0047,  ...,  0.0006, -0.0056,  0.0014]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 23.1250, -12.0312,  17.6719,  ...,  -5.2695,   2.4766,  -6.6367]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 04:43:41 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for I describe, he describes
I follow, he follows
I refer, he refers
I involve, he involves
I exist, he exists
I allow, he allows
I continue, he continues
I apply, he
2024-07-25 04:43:41 root INFO     total operator prediction time: 2658.945464372635 seconds
2024-07-25 04:43:41 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on verb_Ving - 3pSg
2024-07-25 04:43:41 root INFO     building operator verb_Ving - 3pSg
2024-07-25 04:43:42 root INFO     [order_1_approx] starting weight calculation for When something is telling, it tells
When something is asking, it asks
When something is allowing, it allows
When something is appearing, it appears
When something is promoting, it promotes
When something is containing, it contains
When something is understanding, it understands
When something is following, it
2024-07-25 04:43:42 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 04:48:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0156, -2.8125,  2.2188,  ...,  2.0938,  1.1250,  0.5859],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 14.3750,  -2.8125, -12.4375,  ...,  -2.0938,  -3.5000,  -2.7812],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0006, -0.0025, -0.0061,  ..., -0.0014, -0.0036, -0.0056],
        [ 0.0096,  0.0011, -0.0048,  ...,  0.0009, -0.0018,  0.0045],
        [ 0.0085,  0.0014,  0.0645,  ..., -0.0014,  0.0030, -0.0045],
        ...,
        [ 0.0053, -0.0034, -0.0014,  ..., -0.0006, -0.0045,  0.0035],
        [-0.0040,  0.0001, -0.0112,  ..., -0.0029,  0.0134, -0.0045],
        [-0.0002,  0.0007, -0.0118,  ..., -0.0033,  0.0003,  0.0133]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 14.0859,  -3.6465, -14.1562,  ...,  -2.6250,  -2.2031,  -3.8867]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 04:48:30 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To adjust again is to readjust
To locate again is to relocate
To distribute again is to redistribute
To unite again is to reunite
To introduce again is to reintroduce
To examine again is to reexamine
To calculate again is to recalculate
To assure again is to
2024-07-25 04:48:30 root INFO     [order_1_approx] starting weight calculation for To locate again is to relocate
To examine again is to reexamine
To unite again is to reunite
To introduce again is to reintroduce
To calculate again is to recalculate
To assure again is to reassure
To distribute again is to redistribute
To adjust again is to
2024-07-25 04:48:30 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 04:49:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3438, -3.0625,  3.8125,  ..., -0.7109, -2.4219,  1.5781],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([19.0000, -8.2500, 14.1875,  ..., -8.3125,  3.8750, -5.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0134,  0.0164, -0.0114,  ...,  0.0034,  0.0138, -0.0063],
        [-0.0071, -0.0208,  0.0084,  ..., -0.0050, -0.0114,  0.0073],
        [-0.0010,  0.0003,  0.0732,  ..., -0.0010, -0.0096, -0.0035],
        ...,
        [-0.0002,  0.0067,  0.0012,  ..., -0.0020, -0.0055,  0.0016],
        [-0.0035,  0.0014, -0.0062,  ..., -0.0014,  0.0100, -0.0025],
        [-0.0013, -0.0098, -0.0010,  ...,  0.0008, -0.0034,  0.0092]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 20.7656, -10.0234,  14.3672,  ...,  -7.9453,   4.1602,  -6.3516]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 04:49:14 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When something is telling, it tells
When something is asking, it asks
When something is allowing, it allows
When something is appearing, it appears
When something is promoting, it promotes
When something is containing, it contains
When something is understanding, it understands
When something is following, it
2024-07-25 04:49:14 root INFO     [order_1_approx] starting weight calculation for When something is appearing, it appears
When something is telling, it tells
When something is following, it follows
When something is asking, it asks
When something is allowing, it allows
When something is understanding, it understands
When something is containing, it contains
When something is promoting, it
2024-07-25 04:49:15 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 04:54:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 3.5312, -3.2031,  1.5469,  ...,  1.7969,  1.5234, -0.9531],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 22.5000, -16.2500,  -3.1562,  ...,  -4.4375,  -6.0625,  -1.2188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0035,  0.0011,  0.0011,  ...,  0.0003, -0.0033,  0.0028],
        [ 0.0046,  0.0051,  0.0195,  ..., -0.0017,  0.0035,  0.0033],
        [ 0.0049, -0.0037,  0.0515,  ..., -0.0055,  0.0023, -0.0033],
        ...,
        [ 0.0072,  0.0005, -0.0110,  ...,  0.0005, -0.0076,  0.0099],
        [-0.0007,  0.0056, -0.0049,  ...,  0.0006,  0.0150,  0.0004],
        [-0.0008, -0.0031, -0.0084,  ..., -0.0014, -0.0026,  0.0091]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 21.8281, -16.5000,  -3.1621,  ...,  -5.3789,  -6.0664,  -0.9473]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 04:54:35 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To locate again is to relocate
To examine again is to reexamine
To unite again is to reunite
To introduce again is to reintroduce
To calculate again is to recalculate
To assure again is to reassure
To distribute again is to redistribute
To adjust again is to
2024-07-25 04:54:35 root INFO     [order_1_approx] starting weight calculation for To distribute again is to redistribute
To calculate again is to recalculate
To examine again is to reexamine
To locate again is to relocate
To adjust again is to readjust
To assure again is to reassure
To unite again is to reunite
To introduce again is to
2024-07-25 04:54:35 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 04:55:00 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7070, -3.5156,  1.1250,  ..., -1.8047, -1.4688,  1.8516],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([18.1250,  0.0283,  7.6250,  ..., -9.0625,  1.1562,  3.4062],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.8076e-03, -3.2959e-03, -2.2461e-02,  ..., -1.8883e-04,
          5.6152e-03, -4.4556e-03],
        [ 1.5106e-03,  3.0823e-03,  3.9673e-03,  ..., -2.7313e-03,
         -2.5635e-03,  2.1362e-03],
        [ 3.1891e-03,  9.4604e-03,  7.4707e-02,  ..., -1.7090e-03,
          3.4332e-03, -1.6594e-04],
        ...,
        [ 1.9150e-03,  2.2278e-03, -3.9062e-03,  ...,  4.7913e-03,
         -1.5182e-03,  1.6022e-03],
        [-1.4343e-03, -9.7275e-05, -1.1139e-03,  ..., -1.5717e-03,
          1.5991e-02, -9.3079e-04],
        [ 1.7395e-03,  1.8692e-03, -4.0894e-03,  ..., -3.2501e-03,
         -2.0447e-03,  5.0659e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[19.2656, -0.2993,  9.2734,  ..., -8.0156,  2.0371,  4.2070]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 04:55:01 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When something is appearing, it appears
When something is telling, it tells
When something is following, it follows
When something is asking, it asks
When something is allowing, it allows
When something is understanding, it understands
When something is containing, it contains
When something is promoting, it
2024-07-25 04:55:01 root INFO     [order_1_approx] starting weight calculation for When something is asking, it asks
When something is promoting, it promotes
When something is allowing, it allows
When something is containing, it contains
When something is following, it follows
When something is telling, it tells
When something is appearing, it appears
When something is understanding, it
2024-07-25 04:55:01 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 05:00:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 3.4844, -3.1719,  1.3281,  ...,  2.1562,  1.1406,  1.5234],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([16.0000, -3.6094,  0.7500,  ..., -3.1875, -1.6797,  0.4766],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.1963e-02, -2.5940e-03,  3.0212e-03,  ..., -6.1035e-04,
          1.6403e-03,  6.5613e-03],
        [ 7.3853e-03, -2.5635e-03,  4.7913e-03,  ...,  2.3346e-03,
         -8.0566e-03, -2.2430e-03],
        [ 3.4180e-03, -1.4572e-03,  8.2031e-02,  ...,  9.7656e-04,
          1.4420e-03, -1.2207e-02],
        ...,
        [ 1.2970e-03, -1.0757e-03, -7.9346e-03,  ...,  5.1880e-03,
         -1.0864e-02,  6.2256e-03],
        [ 4.1809e-03,  5.7983e-03, -3.1128e-03,  ...,  5.7068e-03,
          1.4099e-02,  1.7700e-03],
        [-5.7983e-04,  4.5776e-05, -3.4332e-03,  ...,  1.1978e-03,
         -5.7068e-03,  1.6113e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[17.4219, -5.4531,  0.7700,  ..., -4.0977, -1.2275,  1.6406]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 05:00:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1719,  0.8828,  6.5000,  ..., -1.3125, -1.0859,  0.6094],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 18.5000, -11.5625,  14.8750,  ..., -15.6875,   1.6016,  -4.7812],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0031, -0.0047, -0.0109,  ...,  0.0026,  0.0105,  0.0017],
        [ 0.0081,  0.0021, -0.0012,  ...,  0.0019,  0.0033,  0.0041],
        [-0.0005, -0.0010,  0.0708,  ..., -0.0021, -0.0002,  0.0011],
        ...,
        [ 0.0041,  0.0060,  0.0021,  ..., -0.0019,  0.0003, -0.0007],
        [ 0.0018, -0.0018, -0.0097,  ..., -0.0011,  0.0156, -0.0012],
        [ 0.0004,  0.0020, -0.0086,  ...,  0.0027, -0.0045,  0.0083]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 19.1250, -10.0156,  14.6250,  ..., -16.0156,   1.3867,  -6.3594]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 05:00:44 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To distribute again is to redistribute
To calculate again is to recalculate
To examine again is to reexamine
To locate again is to relocate
To adjust again is to readjust
To assure again is to reassure
To unite again is to reunite
To introduce again is to
2024-07-25 05:00:45 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When something is asking, it asks
When something is promoting, it promotes
When something is allowing, it allows
When something is containing, it contains
When something is following, it follows
When something is telling, it tells
When something is appearing, it appears
When something is understanding, it
2024-07-25 05:00:45 root INFO     [order_1_approx] starting weight calculation for To unite again is to reunite
To distribute again is to redistribute
To introduce again is to reintroduce
To locate again is to relocate
To adjust again is to readjust
To examine again is to reexamine
To assure again is to reassure
To calculate again is to
2024-07-25 05:00:45 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 05:00:45 root INFO     [order_1_approx] starting weight calculation for When something is following, it follows
When something is asking, it asks
When something is appearing, it appears
When something is telling, it tells
When something is containing, it contains
When something is promoting, it promotes
When something is understanding, it understands
When something is allowing, it
2024-07-25 05:00:45 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 05:06:30 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0312, -3.5938,  4.1875,  ..., -1.0781, -1.4453,  1.1562],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 12.8750,  -5.2500,   6.5625,  ..., -11.3750,   0.1250,  -5.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 6.7444e-03, -1.7471e-03, -1.3489e-02,  ..., -1.1139e-03,
          8.4839e-03, -4.4441e-04],
        [ 6.3705e-04,  3.5858e-03,  8.1177e-03,  ..., -3.4790e-03,
         -8.3160e-04,  3.2349e-03],
        [ 8.0566e-03,  4.4556e-03,  6.7871e-02,  ...,  3.9368e-03,
         -4.6387e-03, -2.2888e-05],
        ...,
        [ 4.6082e-03,  4.0283e-03, -3.2043e-03,  ..., -1.5335e-03,
         -2.9755e-04,  2.5368e-04],
        [-3.4332e-04,  2.2888e-03,  5.3406e-04,  ..., -2.2430e-03,
          1.4648e-02, -6.5613e-04],
        [-3.4637e-03, -3.9978e-03, -3.1281e-03,  ...,  5.7373e-03,
         -7.7209e-03,  4.8523e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 12.0234,  -4.9062,   7.4180,  ..., -12.1953,   0.9219,  -7.3242]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 05:06:31 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When something is following, it follows
When something is asking, it asks
When something is appearing, it appears
When something is telling, it tells
When something is containing, it contains
When something is promoting, it promotes
When something is understanding, it understands
When something is allowing, it
2024-07-25 05:06:31 root INFO     [order_1_approx] starting weight calculation for When something is understanding, it understands
When something is asking, it asks
When something is allowing, it allows
When something is following, it follows
When something is promoting, it promotes
When something is appearing, it appears
When something is containing, it contains
When something is telling, it
2024-07-25 05:06:32 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 05:06:43 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 3.3594, -1.9531,  6.3125,  ...,  0.0210, -1.8516,  0.9141],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 10.3125,  -3.8750, -10.7500,  ...,  -3.7344, -15.5000,  -3.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0031, -0.0054, -0.0058,  ..., -0.0072,  0.0093,  0.0009],
        [ 0.0082,  0.0006,  0.0010,  ..., -0.0007,  0.0012,  0.0020],
        [ 0.0035,  0.0030,  0.0598,  ...,  0.0062,  0.0018, -0.0046],
        ...,
        [ 0.0013, -0.0050, -0.0022,  ...,  0.0002, -0.0078, -0.0047],
        [-0.0003,  0.0007, -0.0082,  ..., -0.0008,  0.0106, -0.0029],
        [-0.0032,  0.0011,  0.0008,  ..., -0.0063,  0.0030,  0.0042]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 10.7344,  -5.1055, -11.5547,  ...,  -3.9121, -15.1094,  -3.7461]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 05:06:44 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To unite again is to reunite
To distribute again is to redistribute
To introduce again is to reintroduce
To locate again is to relocate
To adjust again is to readjust
To examine again is to reexamine
To assure again is to reassure
To calculate again is to
2024-07-25 05:06:44 root INFO     total operator prediction time: 2877.32541680336 seconds
2024-07-25 05:06:44 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on un+adj_reg
2024-07-25 05:06:44 root INFO     building operator un+adj_reg
2024-07-25 05:06:44 root INFO     [order_1_approx] starting weight calculation for The opposite of controlled is uncontrolled
The opposite of noticed is unnoticed
The opposite of intended is unintended
The opposite of acceptable is unacceptable
The opposite of desirable is undesirable
The opposite of specified is unspecified
The opposite of resolved is unresolved
The opposite of lawful is
2024-07-25 05:06:44 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 05:12:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3945,  0.2070,  2.9688,  ..., -0.0400, -0.8164, -0.4844],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 22.8750,  -2.3906,  16.3750,  ..., -12.6250,   4.1250,  -8.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0063, -0.0020, -0.0099,  ..., -0.0006,  0.0018,  0.0014],
        [ 0.0049,  0.0020,  0.0035,  ..., -0.0003, -0.0072,  0.0004],
        [ 0.0002, -0.0001,  0.0664,  ...,  0.0028, -0.0066, -0.0007],
        ...,
        [ 0.0041,  0.0021, -0.0004,  ..., -0.0005,  0.0002, -0.0008],
        [ 0.0034,  0.0005, -0.0098,  ..., -0.0039,  0.0056,  0.0005],
        [-0.0013,  0.0025, -0.0068,  ...,  0.0005, -0.0014,  0.0019]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 23.4688,  -2.7559,  17.3438,  ..., -12.5938,   4.2305,  -9.3906]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 05:12:12 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When something is understanding, it understands
When something is asking, it asks
When something is allowing, it allows
When something is following, it follows
When something is promoting, it promotes
When something is appearing, it appears
When something is containing, it contains
When something is telling, it
2024-07-25 05:12:12 root INFO     [order_1_approx] starting weight calculation for When something is understanding, it understands
When something is allowing, it allows
When something is containing, it contains
When something is appearing, it appears
When something is telling, it tells
When something is following, it follows
When something is promoting, it promotes
When something is asking, it
2024-07-25 05:12:13 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 05:12:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6172,  0.5312,  2.4844,  ...,  2.0156,  2.1562,  2.7031],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  6.7188, -10.3125,   9.4375,  ...,   5.1562,   7.8125,   0.5312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 8.5449e-03,  3.5400e-03, -5.4626e-03,  ..., -6.5308e-03,
         -4.4556e-03, -7.2632e-03],
        [ 1.3306e-02,  8.5449e-03, -2.0447e-03,  ..., -3.3569e-03,
          2.6703e-03,  2.9755e-03],
        [-2.3315e-02, -9.5367e-04,  1.0596e-01,  ..., -3.0975e-03,
          7.2632e-03, -4.4556e-03],
        ...,
        [-7.9956e-03,  1.4648e-03,  1.1841e-02,  ...,  4.3945e-03,
          6.9275e-03,  6.5002e-03],
        [ 5.5542e-03,  2.5024e-03,  8.3008e-03,  ..., -8.3923e-05,
          1.3367e-02, -6.9275e-03],
        [ 1.2054e-03, -8.3618e-03,  3.1738e-03,  ...,  1.3733e-03,
         -7.2021e-03,  1.2573e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  2.7617, -10.4531,   8.4609,  ...,   7.4180,   7.6836,  -0.2935]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 05:12:48 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of controlled is uncontrolled
The opposite of noticed is unnoticed
The opposite of intended is unintended
The opposite of acceptable is unacceptable
The opposite of desirable is undesirable
The opposite of specified is unspecified
The opposite of resolved is unresolved
The opposite of lawful is
2024-07-25 05:12:48 root INFO     [order_1_approx] starting weight calculation for The opposite of specified is unspecified
The opposite of acceptable is unacceptable
The opposite of lawful is unlawful
The opposite of desirable is undesirable
The opposite of controlled is uncontrolled
The opposite of resolved is unresolved
The opposite of noticed is unnoticed
The opposite of intended is
2024-07-25 05:12:48 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 05:17:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2930, -1.5938,  3.0312,  ...,  0.7031, -2.1250, -0.3965],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 21.7500, -13.5625,   8.6875,  ...,  -9.0625,  10.0000,  -0.3496],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-0.0226,  0.0298, -0.0212,  ..., -0.0110,  0.0238, -0.0240],
        [ 0.0342, -0.0366,  0.0210,  ...,  0.0145, -0.0245,  0.0374],
        [ 0.0007,  0.0029,  0.0684,  ...,  0.0003, -0.0036,  0.0010],
        ...,
        [-0.0039,  0.0042, -0.0101,  ..., -0.0043, -0.0016, -0.0017],
        [ 0.0026, -0.0009, -0.0025,  ...,  0.0031,  0.0074,  0.0040],
        [-0.0077,  0.0128, -0.0093,  ..., -0.0039,  0.0082, -0.0026]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 13.3672,  -3.4688,   9.3125,  ..., -10.1406,  12.3281,  -3.7500]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 05:17:59 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When something is understanding, it understands
When something is allowing, it allows
When something is containing, it contains
When something is appearing, it appears
When something is telling, it tells
When something is following, it follows
When something is promoting, it promotes
When something is asking, it
2024-07-25 05:18:00 root INFO     [order_1_approx] starting weight calculation for When something is understanding, it understands
When something is allowing, it allows
When something is containing, it contains
When something is asking, it asks
When something is promoting, it promotes
When something is following, it follows
When something is telling, it tells
When something is appearing, it
2024-07-25 05:18:00 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 05:18:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4375, -1.5391, -2.7656,  ...,  1.4609, -0.0547,  2.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 15.9375,   1.9688,   5.1875,  ...,   4.5312, -14.2500,  -4.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0107,  0.0057, -0.0090,  ..., -0.0001,  0.0060, -0.0162],
        [ 0.0058,  0.0012,  0.0188,  ...,  0.0015,  0.0034, -0.0035],
        [-0.0098,  0.0015,  0.0737,  ..., -0.0175,  0.0068,  0.0223],
        ...,
        [ 0.0060,  0.0023,  0.0056,  ..., -0.0002,  0.0146,  0.0089],
        [ 0.0101,  0.0021, -0.0048,  ..., -0.0013,  0.0132, -0.0081],
        [ 0.0023, -0.0076, -0.0005,  ..., -0.0055,  0.0140,  0.0168]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 13.2109,   1.6230,   2.9102,  ...,   1.6348, -11.6953,  -5.0625]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 05:18:48 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of specified is unspecified
The opposite of acceptable is unacceptable
The opposite of lawful is unlawful
The opposite of desirable is undesirable
The opposite of controlled is uncontrolled
The opposite of resolved is unresolved
The opposite of noticed is unnoticed
The opposite of intended is
2024-07-25 05:18:48 root INFO     [order_1_approx] starting weight calculation for The opposite of lawful is unlawful
The opposite of acceptable is unacceptable
The opposite of controlled is uncontrolled
The opposite of intended is unintended
The opposite of desirable is undesirable
The opposite of specified is unspecified
The opposite of noticed is unnoticed
The opposite of resolved is
2024-07-25 05:18:48 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 05:23:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5625, -3.8438,  1.8125,  ..., -0.0625, -0.3652,  0.1934],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([18.3750, -6.7812,  7.0625,  ..., -9.5000,  6.8125, -5.0938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 6.9275e-03, -3.2501e-03, -8.7891e-03,  ...,  1.9531e-03,
          3.5248e-03,  1.5182e-03],
        [ 6.1035e-03,  9.4604e-04,  8.1787e-03,  ..., -6.1035e-03,
         -3.7842e-03, -5.6267e-05],
        [-6.1951e-03, -2.9144e-03,  6.3965e-02,  ...,  6.8054e-03,
          1.8997e-03, -1.5182e-03],
        ...,
        [ 8.4839e-03,  2.3956e-03,  4.7607e-03,  ..., -2.1515e-03,
         -4.9210e-04,  1.4496e-03],
        [-1.4648e-03,  7.4768e-04,  5.7678e-03,  ...,  5.8746e-04,
          1.5991e-02,  3.4332e-03],
        [-3.4332e-05, -1.5945e-03, -8.3008e-03,  ...,  4.9591e-04,
         -4.0283e-03,  6.7749e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[17.4062, -6.0898,  7.7617,  ..., -8.4531,  7.5625, -6.3359]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 05:23:49 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When something is understanding, it understands
When something is allowing, it allows
When something is containing, it contains
When something is asking, it asks
When something is promoting, it promotes
When something is following, it follows
When something is telling, it tells
When something is appearing, it
2024-07-25 05:23:49 root INFO     [order_1_approx] starting weight calculation for When something is appearing, it appears
When something is telling, it tells
When something is understanding, it understands
When something is allowing, it allows
When something is following, it follows
When something is promoting, it promotes
When something is asking, it asks
When something is containing, it
2024-07-25 05:23:50 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 05:24:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8477, -0.1230, -0.7656,  ...,  2.6250, -1.6094,  1.5938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 2.6875, -8.8125, -1.7422,  ..., -8.2500, 17.7500,  5.5938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0270, -0.0026,  0.0037,  ...,  0.0037, -0.0170, -0.0117],
        [ 0.0037, -0.0039,  0.0146,  ...,  0.0085, -0.0018, -0.0167],
        [ 0.0081,  0.0111,  0.1206,  ..., -0.0095,  0.0067,  0.0018],
        ...,
        [ 0.0135,  0.0041,  0.0019,  ...,  0.0020, -0.0067,  0.0022],
        [-0.0222,  0.0119, -0.0042,  ...,  0.0060,  0.0425, -0.0025],
        [-0.0030, -0.0057, -0.0183,  ..., -0.0014, -0.0116,  0.0359]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.5449, -7.3398, -0.9951,  ..., -6.5391, 18.9062,  2.5254]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 05:24:54 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of lawful is unlawful
The opposite of acceptable is unacceptable
The opposite of controlled is uncontrolled
The opposite of intended is unintended
The opposite of desirable is undesirable
The opposite of specified is unspecified
The opposite of noticed is unnoticed
The opposite of resolved is
2024-07-25 05:24:54 root INFO     [order_1_approx] starting weight calculation for The opposite of specified is unspecified
The opposite of noticed is unnoticed
The opposite of resolved is unresolved
The opposite of intended is unintended
The opposite of desirable is undesirable
The opposite of lawful is unlawful
The opposite of controlled is uncontrolled
The opposite of acceptable is
2024-07-25 05:24:54 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 05:29:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 3.2656, -1.5312,  3.1875,  ..., -0.4219, -1.2344,  2.3125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 26.2500,  -5.2500,  16.0000,  ..., -13.0000,   6.5625,  -6.7812],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0081, -0.0039, -0.0121,  ...,  0.0026, -0.0034,  0.0019],
        [-0.0012,  0.0082,  0.0083,  ..., -0.0045,  0.0031, -0.0018],
        [-0.0031,  0.0021,  0.0752,  ...,  0.0033,  0.0072, -0.0035],
        ...,
        [-0.0057,  0.0057,  0.0002,  ..., -0.0016,  0.0055,  0.0031],
        [-0.0013, -0.0068, -0.0044,  ...,  0.0030,  0.0121,  0.0044],
        [ 0.0050,  0.0009, -0.0088,  ...,  0.0029, -0.0040,  0.0081]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 25.9375,  -3.9922,  17.0625,  ..., -11.7344,   6.7500,  -6.9648]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 05:29:36 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When something is appearing, it appears
When something is telling, it tells
When something is understanding, it understands
When something is allowing, it allows
When something is following, it follows
When something is promoting, it promotes
When something is asking, it asks
When something is containing, it
2024-07-25 05:29:36 root INFO     total operator prediction time: 2754.335905313492 seconds
2024-07-25 05:29:36 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on noun - plural_reg
2024-07-25 05:29:36 root INFO     building operator noun - plural_reg
2024-07-25 05:29:36 root INFO     [order_1_approx] starting weight calculation for The plural form of month is months
The plural form of user is users
The plural form of god is gods
The plural form of website is websites
The plural form of director is directors
The plural form of week is weeks
The plural form of river is rivers
The plural form of student is
2024-07-25 05:29:36 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 05:30:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5117,  0.3457, -0.2988,  ...,  0.9766,  2.1406,  1.8594],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 2.3594, -8.8125, -0.1250,  ..., -6.1875,  1.5938,  3.3906],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.8921e-02,  3.6621e-03,  4.6692e-03,  ..., -9.9182e-05,
         -2.9449e-03, -8.9264e-04],
        [ 3.9978e-03,  3.7689e-03, -1.2329e-02,  ..., -6.6223e-03,
          7.5989e-03,  8.3618e-03],
        [-2.6489e-02, -7.3853e-03,  9.0332e-02,  ..., -7.5073e-03,
          8.4839e-03,  5.0659e-03],
        ...,
        [-9.1934e-04,  5.1880e-03,  5.2490e-03,  ...,  3.9062e-03,
          1.1719e-02,  4.1809e-03],
        [ 5.6458e-03,  4.4556e-03,  1.0757e-03,  ...,  2.4872e-03,
          2.0874e-02, -5.9509e-03],
        [-7.9346e-03, -7.8125e-03, -2.1362e-02,  ...,  5.8746e-04,
         -8.6060e-03,  1.0132e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.3945, -9.4453,  0.2832,  ..., -4.8867,  2.2949, -0.7227]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 05:30:58 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of specified is unspecified
The opposite of noticed is unnoticed
The opposite of resolved is unresolved
The opposite of intended is unintended
The opposite of desirable is undesirable
The opposite of lawful is unlawful
The opposite of controlled is uncontrolled
The opposite of acceptable is
2024-07-25 05:30:58 root INFO     [order_1_approx] starting weight calculation for The opposite of lawful is unlawful
The opposite of noticed is unnoticed
The opposite of controlled is uncontrolled
The opposite of acceptable is unacceptable
The opposite of specified is unspecified
The opposite of resolved is unresolved
The opposite of intended is unintended
The opposite of desirable is
2024-07-25 05:30:58 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 05:35:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.4844, -3.2812,  2.4375,  ...,  3.1719,  1.4609,  2.9219],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 19.1250, -10.1250,  -2.9062,  ...,  -6.6250,  -5.7812,  -4.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.0925e-02,  3.6377e-02, -2.6611e-02,  ...,  1.1719e-02,
          4.8828e-02, -3.8574e-02],
        [-7.2021e-03, -3.4424e-02,  3.7598e-02,  ..., -1.3184e-02,
         -4.8096e-02,  3.8574e-02],
        [-3.0518e-05,  2.5879e-02,  3.6865e-02,  ...,  3.0212e-03,
          3.0884e-02, -2.9785e-02],
        ...,
        [-6.0654e-04,  1.1047e-02, -1.0193e-02,  ...,  3.3264e-03,
          1.2329e-02, -8.6670e-03],
        [-2.4414e-04,  1.4282e-02, -4.0894e-03,  ...,  3.5095e-03,
          1.6113e-02, -8.1177e-03],
        [ 5.1575e-03,  1.4771e-02, -9.0332e-03,  ...,  7.7515e-03,
          1.9043e-02, -1.0315e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 28.0938, -20.6250,   0.6602,  ...,  -4.1328,  -1.7656,  -1.2324]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 05:35:20 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of month is months
The plural form of user is users
The plural form of god is gods
The plural form of website is websites
The plural form of director is directors
The plural form of week is weeks
The plural form of river is rivers
The plural form of student is
2024-07-25 05:35:20 root INFO     [order_1_approx] starting weight calculation for The plural form of week is weeks
The plural form of student is students
The plural form of god is gods
The plural form of website is websites
The plural form of user is users
The plural form of director is directors
The plural form of month is months
The plural form of river is
2024-07-25 05:35:20 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 05:36:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0742, -1.7109,  1.2188,  ...,  0.9688,  0.8672,  2.2188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  4.8750, -12.6250,   0.4062,  ...,  -3.9531,   5.1875,   3.7188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0119,  0.0076, -0.0105,  ..., -0.0100, -0.0010, -0.0159],
        [-0.0008,  0.0096, -0.0103,  ..., -0.0021, -0.0172,  0.0042],
        [-0.0189, -0.0097,  0.1187,  ...,  0.0052, -0.0043,  0.0171],
        ...,
        [ 0.0034,  0.0078,  0.0121,  ...,  0.0051,  0.0087,  0.0045],
        [-0.0054, -0.0049, -0.0066,  ...,  0.0042,  0.0166, -0.0063],
        [-0.0062, -0.0014, -0.0051,  ..., -0.0016,  0.0020,  0.0315]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  4.9453, -15.9531,  -2.5957,  ...,  -6.6602,   6.8242,   1.4980]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 05:36:57 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of lawful is unlawful
The opposite of noticed is unnoticed
The opposite of controlled is uncontrolled
The opposite of acceptable is unacceptable
The opposite of specified is unspecified
The opposite of resolved is unresolved
The opposite of intended is unintended
The opposite of desirable is
2024-07-25 05:36:57 root INFO     [order_1_approx] starting weight calculation for The opposite of lawful is unlawful
The opposite of controlled is uncontrolled
The opposite of acceptable is unacceptable
The opposite of desirable is undesirable
The opposite of noticed is unnoticed
The opposite of resolved is unresolved
The opposite of intended is unintended
The opposite of specified is
2024-07-25 05:36:57 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 05:41:05 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2812, -0.3750, -4.8125,  ...,  0.7500, -1.5625,  1.2969],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 17.7500,  -7.6250, -15.4375,  ..., -13.8750,   4.8750, -10.8750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0074,  0.0018, -0.0092,  ...,  0.0009,  0.0021, -0.0020],
        [ 0.0033, -0.0004,  0.0103,  ..., -0.0006, -0.0029, -0.0098],
        [-0.0025,  0.0050,  0.0586,  ..., -0.0033,  0.0061, -0.0081],
        ...,
        [-0.0036,  0.0007, -0.0002,  ...,  0.0012, -0.0014,  0.0044],
        [-0.0016,  0.0101, -0.0135,  ..., -0.0003,  0.0159,  0.0043],
        [ 0.0017, -0.0011, -0.0071,  ...,  0.0025,  0.0021,  0.0063]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 14.8984,  -7.0039, -15.6641,  ..., -12.1172,   3.4531, -11.8750]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 05:41:06 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of week is weeks
The plural form of student is students
The plural form of god is gods
The plural form of website is websites
The plural form of user is users
The plural form of director is directors
The plural form of month is months
The plural form of river is
2024-07-25 05:41:06 root INFO     [order_1_approx] starting weight calculation for The plural form of student is students
The plural form of month is months
The plural form of website is websites
The plural form of week is weeks
The plural form of god is gods
The plural form of user is users
The plural form of river is rivers
The plural form of director is
2024-07-25 05:41:07 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 05:42:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0234,  0.7734,  4.9062,  ..., -0.0273, -0.0176,  2.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 4.1875, -0.3438,  5.3750,  ...,  3.2656, -0.6914, -4.7188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.0742e-02,  6.8665e-05, -1.1658e-02,  ..., -6.2256e-03,
         -2.3956e-03, -6.6528e-03],
        [ 5.6763e-03, -4.1504e-03,  1.2329e-02,  ...,  1.7700e-03,
          6.1035e-03,  1.0132e-02],
        [-5.2185e-03,  1.3611e-02,  1.0693e-01,  ..., -9.3384e-03,
          1.2512e-03,  1.3428e-03],
        ...,
        [-4.3106e-04,  2.4567e-03,  8.3923e-05,  ..., -6.1646e-03,
          7.6904e-03, -5.1880e-03],
        [ 2.5940e-03,  1.0986e-02, -8.4229e-03,  ...,  4.5166e-03,
          2.0752e-02, -3.6926e-03],
        [-7.8735e-03, -6.3477e-03, -1.1292e-02,  ..., -3.8147e-04,
          9.9182e-04,  2.3438e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-2.0508,  1.9043,  4.5625,  ...,  4.6172, -1.4863, -5.2070]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 05:42:55 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of lawful is unlawful
The opposite of controlled is uncontrolled
The opposite of acceptable is unacceptable
The opposite of desirable is undesirable
The opposite of noticed is unnoticed
The opposite of resolved is unresolved
The opposite of intended is unintended
The opposite of specified is
2024-07-25 05:42:55 root INFO     [order_1_approx] starting weight calculation for The opposite of desirable is undesirable
The opposite of lawful is unlawful
The opposite of specified is unspecified
The opposite of acceptable is unacceptable
The opposite of intended is unintended
The opposite of controlled is uncontrolled
The opposite of resolved is unresolved
The opposite of noticed is
2024-07-25 05:42:56 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 05:46:53 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3438, -1.8125, -0.2041,  ..., -0.4297,  1.1328, -0.6641],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  2.0000,  -4.7188,  -9.0000,  ..., -11.5000,  -1.7812,  -0.6406],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 7.6904e-03,  6.5994e-04, -8.7280e-03,  ...,  1.5564e-03,
          7.1716e-03, -2.9945e-04],
        [-2.7618e-03,  1.0315e-02,  4.5166e-03,  ...,  5.9509e-04,
          4.8828e-04,  4.0245e-04],
        [-4.2419e-03,  4.7913e-03,  6.2988e-02,  ..., -3.3569e-03,
          8.3008e-03, -1.0193e-02],
        ...,
        [ 3.9062e-03, -1.1978e-03,  8.6060e-03,  ...,  7.0190e-04,
         -5.1880e-03, -8.3160e-04],
        [-6.5002e-03,  2.6703e-03,  3.2806e-03,  ..., -2.0294e-03,
          1.3123e-02,  1.7242e-03],
        [ 5.0049e-03, -3.8452e-03,  8.9722e-03,  ...,  3.8147e-06,
         -4.3640e-03,  6.0425e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  2.0449,  -4.7734,  -7.8750,  ..., -10.8516,   0.3359,  -2.3867]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 05:46:54 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of student is students
The plural form of month is months
The plural form of website is websites
The plural form of week is weeks
The plural form of god is gods
The plural form of user is users
The plural form of river is rivers
The plural form of director is
2024-07-25 05:46:54 root INFO     [order_1_approx] starting weight calculation for The plural form of student is students
The plural form of month is months
The plural form of river is rivers
The plural form of god is gods
The plural form of website is websites
The plural form of week is weeks
The plural form of director is directors
The plural form of user is
2024-07-25 05:46:54 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 05:49:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3691, -0.0391, -0.0078,  ...,  1.8594,  0.6367,  0.0879],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -4.8125,  -4.8438,  -1.4688,  ...,   1.2578,   1.0312, -11.3125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 9.6436e-03,  5.2490e-03,  1.2390e-02,  ..., -1.3580e-03,
         -4.6997e-03,  1.0834e-03],
        [ 4.5776e-03, -7.6294e-06, -7.4768e-03,  ..., -4.7684e-04,
          1.4038e-03,  2.1362e-04],
        [ 1.3123e-03,  1.4191e-03,  9.6680e-02,  ..., -4.0894e-03,
          1.6098e-03,  9.2773e-03],
        ...,
        [-1.4267e-03,  1.3062e-02,  5.7678e-03,  ..., -4.2114e-03,
          2.6321e-04,  6.7902e-04],
        [ 1.3657e-03, -1.0300e-03, -8.7280e-03,  ..., -4.5471e-03,
          1.8433e-02, -1.3489e-02],
        [-9.9945e-04, -5.5542e-03, -1.6724e-02,  ...,  3.0975e-03,
         -1.0132e-02,  1.4282e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-7.8516, -6.7578, -1.7539,  ...,  1.1875,  3.2754, -9.9609]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 05:49:02 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of desirable is undesirable
The opposite of lawful is unlawful
The opposite of specified is unspecified
The opposite of acceptable is unacceptable
The opposite of intended is unintended
The opposite of controlled is uncontrolled
The opposite of resolved is unresolved
The opposite of noticed is
2024-07-25 05:49:03 root INFO     [order_1_approx] starting weight calculation for The opposite of acceptable is unacceptable
The opposite of intended is unintended
The opposite of specified is unspecified
The opposite of desirable is undesirable
The opposite of resolved is unresolved
The opposite of lawful is unlawful
The opposite of noticed is unnoticed
The opposite of controlled is
2024-07-25 05:49:03 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 05:52:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5312, -1.6406,  2.7031,  ...,  1.3047,  1.4688,  2.3281],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 9.7500, -5.7500, -7.3750,  ..., -3.9062,  7.9062, -9.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0216,  0.0178, -0.0288,  ...,  0.0010,  0.0139,  0.0094],
        [-0.0130, -0.0085,  0.0272,  ..., -0.0015, -0.0143, -0.0081],
        [ 0.0193,  0.0327,  0.0239,  ...,  0.0016,  0.0178,  0.0077],
        ...,
        [ 0.0024,  0.0021,  0.0018,  ...,  0.0038, -0.0048,  0.0044],
        [ 0.0137,  0.0136, -0.0217,  ...,  0.0008,  0.0244,  0.0052],
        [ 0.0155,  0.0039, -0.0266,  ...,  0.0012,  0.0099,  0.0090]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 9.1562, -6.5859, -7.2656,  ..., -3.9512,  8.3516, -9.1406]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 05:52:37 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of student is students
The plural form of month is months
The plural form of river is rivers
The plural form of god is gods
The plural form of website is websites
The plural form of week is weeks
The plural form of director is directors
The plural form of user is
2024-07-25 05:52:37 root INFO     [order_1_approx] starting weight calculation for The plural form of week is weeks
The plural form of user is users
The plural form of student is students
The plural form of river is rivers
The plural form of month is months
The plural form of website is websites
The plural form of director is directors
The plural form of god is
2024-07-25 05:52:37 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 05:55:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4688, -1.2109,  1.2500,  ...,  0.7031, -1.2344,  1.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-0.1562, -6.3125,  4.2500,  ...,  7.6875, -0.5586, -2.8125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 9.0332e-03, -6.1798e-04,  4.7607e-03,  ..., -3.6774e-03,
          7.9956e-03, -9.7046e-03],
        [ 2.5177e-03, -2.3079e-04,  9.1553e-03,  ..., -4.7607e-03,
         -1.6022e-04, -7.5684e-03],
        [-5.5847e-03, -8.1787e-03,  8.1055e-02,  ..., -7.3853e-03,
         -5.6458e-03,  3.4637e-03],
        ...,
        [-6.1646e-03, -3.3722e-03,  5.4932e-03,  ...,  4.9591e-05,
         -2.0752e-03,  8.3618e-03],
        [ 3.9673e-03,  3.9062e-03,  3.3112e-03,  ..., -6.1340e-03,
          1.1169e-02, -5.7068e-03],
        [ 3.7689e-03, -3.8147e-03, -6.1035e-03,  ...,  2.1820e-03,
         -8.5449e-03,  2.2583e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.9121, -6.6250,  4.3438,  ...,  6.1328,  0.2563, -5.1094]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 05:55:02 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The opposite of acceptable is unacceptable
The opposite of intended is unintended
The opposite of specified is unspecified
The opposite of desirable is undesirable
The opposite of resolved is unresolved
The opposite of lawful is unlawful
The opposite of noticed is unnoticed
The opposite of controlled is
2024-07-25 05:55:02 root INFO     total operator prediction time: 2898.379111289978 seconds
2024-07-25 05:55:02 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on verb+able_reg
2024-07-25 05:55:02 root INFO     building operator verb+able_reg
2024-07-25 05:55:03 root INFO     [order_1_approx] starting weight calculation for If you can publish something, that thing is publishable
If you can write something, that thing is writeable
If you can survive something, that thing is survivable
If you can imagine something, that thing is imaginable
If you can advise something, that thing is advisable
If you can execute something, that thing is executable
If you can explain something, that thing is explainable
If you can expect something, that thing is
2024-07-25 05:55:03 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 05:58:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.3926, -0.3633, -3.8750,  ...,  1.9844,  0.4297,  0.4746],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-3.3125, -7.7812, -6.4062,  ..., -5.6562,  5.0938, -2.4688],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 8.1177e-03, -2.3651e-03, -8.7891e-03,  ..., -6.7749e-03,
         -2.6703e-03, -6.6528e-03],
        [ 4.4250e-03,  2.1362e-03,  8.2016e-05,  ..., -2.4719e-03,
         -1.5991e-02, -5.9509e-03],
        [-8.5449e-03, -3.4027e-03,  8.4961e-02,  ...,  6.3171e-03,
          1.6479e-02, -1.5991e-02],
        ...,
        [-6.2866e-03, -4.8218e-03, -7.4005e-04,  ...,  1.1292e-03,
         -4.9438e-03,  3.3722e-03],
        [-1.2024e-02,  1.3809e-03,  2.8687e-03,  ..., -9.3384e-03,
          8.8501e-03,  4.2343e-04],
        [-6.1951e-03, -3.4790e-03, -1.0300e-03,  ...,  7.6599e-03,
          7.6904e-03,  9.7046e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-4.5312, -7.8477, -4.2656,  ..., -5.6523,  7.0312, -2.0762]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 05:58:13 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of week is weeks
The plural form of user is users
The plural form of student is students
The plural form of river is rivers
The plural form of month is months
The plural form of website is websites
The plural form of director is directors
The plural form of god is
2024-07-25 05:58:14 root INFO     [order_1_approx] starting weight calculation for The plural form of week is weeks
The plural form of student is students
The plural form of user is users
The plural form of god is gods
The plural form of director is directors
The plural form of river is rivers
The plural form of month is months
The plural form of website is
2024-07-25 05:58:14 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 06:00:49 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6055, -1.0703,  3.5781,  ...,  0.8047,  1.2500, -0.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([14.8750, 11.3750, -7.3750,  ...,  7.8125, -8.9375, -2.8125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0023,  0.0011, -0.0075,  ..., -0.0010,  0.0101, -0.0031],
        [-0.0005,  0.0042, -0.0056,  ..., -0.0017, -0.0023, -0.0041],
        [-0.0021,  0.0032,  0.0728,  ...,  0.0013,  0.0084,  0.0022],
        ...,
        [ 0.0002, -0.0010, -0.0025,  ..., -0.0009, -0.0012,  0.0049],
        [ 0.0103, -0.0025, -0.0014,  ...,  0.0027,  0.0183, -0.0063],
        [ 0.0011,  0.0020,  0.0003,  ..., -0.0046, -0.0012,  0.0095]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[13.5859, 10.3203, -5.5820,  ...,  7.9805, -9.0781, -3.1133]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 06:00:50 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you can publish something, that thing is publishable
If you can write something, that thing is writeable
If you can survive something, that thing is survivable
If you can imagine something, that thing is imaginable
If you can advise something, that thing is advisable
If you can execute something, that thing is executable
If you can explain something, that thing is explainable
If you can expect something, that thing is
2024-07-25 06:00:50 root INFO     [order_1_approx] starting weight calculation for If you can execute something, that thing is executable
If you can survive something, that thing is survivable
If you can write something, that thing is writeable
If you can publish something, that thing is publishable
If you can explain something, that thing is explainable
If you can expect something, that thing is expectable
If you can advise something, that thing is advisable
If you can imagine something, that thing is
2024-07-25 06:00:50 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 06:03:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.6719, -0.8125, -3.6250,  ...,  1.1250,  0.7266,  0.6172],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  0.5938,   2.4062, -16.6250,  ...,  -5.5312,  12.5000,  -7.4688],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0031, -0.0049, -0.0087,  ...,  0.0027,  0.0030, -0.0049],
        [-0.0044,  0.0127,  0.0199,  ..., -0.0031,  0.0023, -0.0030],
        [-0.0074,  0.0053,  0.0825,  ..., -0.0028,  0.0042, -0.0112],
        ...,
        [-0.0025,  0.0013, -0.0048,  ...,  0.0076,  0.0006,  0.0025],
        [ 0.0006,  0.0046,  0.0105,  ..., -0.0053,  0.0164,  0.0044],
        [ 0.0031, -0.0035, -0.0065,  ...,  0.0047,  0.0037,  0.0115]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -1.8945,  -0.3848, -14.3594,  ...,  -5.7812,  14.6953,  -7.3516]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 06:03:55 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of week is weeks
The plural form of student is students
The plural form of user is users
The plural form of god is gods
The plural form of director is directors
The plural form of river is rivers
The plural form of month is months
The plural form of website is
2024-07-25 06:03:55 root INFO     [order_1_approx] starting weight calculation for The plural form of user is users
The plural form of god is gods
The plural form of week is weeks
The plural form of student is students
The plural form of river is rivers
The plural form of director is directors
The plural form of website is websites
The plural form of month is
2024-07-25 06:03:55 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 06:06:50 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1797, -1.0234,  0.3887,  ..., -0.6992, -2.0156,  0.2715],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 16.8750,  16.1250, -14.1250,  ...,   1.4141,  -4.3438,  -6.1562],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.2806e-04,  6.5002e-03, -5.6458e-03,  ..., -5.7983e-03,
          7.6599e-03,  4.6387e-03],
        [ 1.9836e-03,  1.8768e-03, -4.9591e-04,  ...,  7.1335e-04,
         -3.3112e-03,  2.5024e-03],
        [ 3.6774e-03, -4.0283e-03,  5.0781e-02,  ...,  2.7466e-03,
         -1.5793e-03,  9.4604e-04],
        ...,
        [ 3.2654e-03,  2.2316e-04, -1.9989e-03,  ..., -2.9144e-03,
         -2.0752e-03,  3.9673e-03],
        [ 2.6398e-03,  1.3275e-03,  4.4441e-04,  ...,  5.8746e-04,
          1.3367e-02,  1.7700e-03],
        [ 1.9073e-05,  3.9062e-03, -5.1880e-04,  ..., -1.7929e-03,
         -1.7090e-03,  2.7008e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 17.7188,  14.8438, -12.7422,  ...,   0.7466,  -3.4023,  -6.6094]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 06:06:51 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you can execute something, that thing is executable
If you can survive something, that thing is survivable
If you can write something, that thing is writeable
If you can publish something, that thing is publishable
If you can explain something, that thing is explainable
If you can expect something, that thing is expectable
If you can advise something, that thing is advisable
If you can imagine something, that thing is
2024-07-25 06:06:51 root INFO     [order_1_approx] starting weight calculation for If you can publish something, that thing is publishable
If you can write something, that thing is writeable
If you can execute something, that thing is executable
If you can expect something, that thing is expectable
If you can advise something, that thing is advisable
If you can imagine something, that thing is imaginable
If you can survive something, that thing is survivable
If you can explain something, that thing is
2024-07-25 06:06:51 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 06:09:35 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3047, -1.8438,  1.5859,  ...,  1.4453,  1.5391,  2.5312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([10.3750, -7.3750, -9.3125,  ..., -3.7812, 11.2500,  3.4375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0014,  0.0114, -0.0042,  ..., -0.0092, -0.0011, -0.0104],
        [-0.0005,  0.0047,  0.0013,  ...,  0.0004, -0.0035,  0.0023],
        [-0.0107,  0.0179,  0.0576,  ..., -0.0105, -0.0044, -0.0021],
        ...,
        [-0.0071, -0.0051,  0.0019,  ...,  0.0029, -0.0015,  0.0077],
        [-0.0061,  0.0195,  0.0069,  ..., -0.0063, -0.0010, -0.0026],
        [-0.0007,  0.0070,  0.0030,  ..., -0.0025, -0.0007,  0.0084]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  8.2734,  -8.3359, -11.4688,  ...,  -1.7812,  10.5547,   3.2129]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 06:09:35 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of user is users
The plural form of god is gods
The plural form of week is weeks
The plural form of student is students
The plural form of river is rivers
The plural form of director is directors
The plural form of website is websites
The plural form of month is
2024-07-25 06:09:36 root INFO     [order_1_approx] starting weight calculation for The plural form of user is users
The plural form of god is gods
The plural form of student is students
The plural form of month is months
The plural form of website is websites
The plural form of river is rivers
The plural form of director is directors
The plural form of week is
2024-07-25 06:09:36 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 06:12:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.1348, -0.8281,  2.9062,  ...,  0.8906, -2.8438,  1.7734],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([14.2500, -1.1875, -0.5000,  ..., 10.8125, -8.8125,  2.6875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0023,  0.0056, -0.0023,  ..., -0.0051,  0.0118, -0.0040],
        [ 0.0067,  0.0058,  0.0028,  ..., -0.0073,  0.0032, -0.0047],
        [-0.0013, -0.0009,  0.0603,  ...,  0.0085,  0.0002,  0.0042],
        ...,
        [ 0.0008, -0.0002, -0.0082,  ...,  0.0027,  0.0014,  0.0001],
        [ 0.0031,  0.0003, -0.0024,  ...,  0.0024,  0.0137,  0.0010],
        [-0.0009,  0.0006,  0.0107,  ...,  0.0023, -0.0037,  0.0126]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 14.9766,  -1.1250,   0.9121,  ...,  11.2500, -10.1875,   1.8896]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 06:12:43 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you can publish something, that thing is publishable
If you can write something, that thing is writeable
If you can execute something, that thing is executable
If you can expect something, that thing is expectable
If you can advise something, that thing is advisable
If you can imagine something, that thing is imaginable
If you can survive something, that thing is survivable
If you can explain something, that thing is
2024-07-25 06:12:43 root INFO     [order_1_approx] starting weight calculation for If you can survive something, that thing is survivable
If you can imagine something, that thing is imaginable
If you can publish something, that thing is publishable
If you can advise something, that thing is advisable
If you can explain something, that thing is explainable
If you can expect something, that thing is expectable
If you can execute something, that thing is executable
If you can write something, that thing is
2024-07-25 06:12:44 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 06:15:16 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8867, -1.3984,  0.9883,  ...,  2.3281,  1.0625,  1.1094],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 12.5625,  -1.1172,  -8.5625,  ...,  -6.9375,  13.0000, -11.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.0365e-03, -3.0518e-03, -1.2283e-03,  ..., -6.7139e-04,
          7.7209e-03, -2.5635e-03],
        [-2.1973e-03,  5.9814e-03,  5.1270e-03,  ..., -4.1504e-03,
         -1.4420e-03,  1.8463e-03],
        [ 5.9128e-04,  9.0332e-03,  6.4941e-02,  ..., -2.9602e-03,
         -3.2043e-03, -2.2278e-03],
        ...,
        [-2.3804e-03, -2.0447e-03, -8.4686e-04,  ...,  3.8147e-03,
         -1.1368e-03, -1.2817e-03],
        [-8.5449e-03,  3.9673e-03,  2.5024e-03,  ..., -4.2114e-03,
          1.2268e-02,  6.8665e-05],
        [ 1.4420e-03, -3.9978e-03, -4.5166e-03,  ...,  2.4872e-03,
          9.7046e-03,  6.3171e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  9.5000,  -2.2402,  -9.1953,  ...,  -6.8359,  12.0781, -13.3125]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 06:15:17 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The plural form of user is users
The plural form of god is gods
The plural form of student is students
The plural form of month is months
The plural form of website is websites
The plural form of river is rivers
The plural form of director is directors
The plural form of week is
2024-07-25 06:15:17 root INFO     total operator prediction time: 2741.5564239025116 seconds
2024-07-25 06:15:17 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on verb_3pSg - Ved
2024-07-25 06:15:17 root INFO     building operator verb_3pSg - Ved
2024-07-25 06:15:17 root INFO     [order_1_approx] starting weight calculation for When he occurs something, something has been occurred
When he announces something, something has been announced
When he adds something, something has been added
When he develops something, something has been developed
When he replaces something, something has been replaced
When he represents something, something has been represented
When he suggests something, something has been suggested
When he hears something, something has been
2024-07-25 06:15:18 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 06:18:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2656, -1.5625,  4.5000,  ...,  0.8867,  0.2617,  0.3555],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 5.0625,  9.6875, -4.6875,  ..., -3.2188,  2.0938, -4.8125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 4.1389e-04,  5.3406e-04, -8.3618e-03,  ..., -1.7853e-03,
          4.9438e-03,  1.1902e-03],
        [ 1.0223e-03,  8.5449e-03,  6.1951e-03,  ..., -2.0905e-03,
         -3.6011e-03, -3.7689e-03],
        [ 4.9591e-04,  3.6163e-03,  4.1504e-02,  ..., -5.3024e-04,
         -4.3945e-03,  7.2021e-03],
        ...,
        [-1.6098e-03,  1.8616e-03, -9.1553e-03,  ...,  3.4332e-05,
         -9.0790e-04,  4.0436e-04],
        [-3.3875e-03,  9.3384e-03,  6.4697e-03,  ..., -4.2725e-04,
          6.8970e-03,  1.8387e-03],
        [ 1.8616e-03,  4.5776e-03, -1.4877e-04,  ..., -3.3722e-03,
         -5.7373e-03,  7.1106e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.0469,  9.5312, -3.3809,  ..., -3.2676,  2.8789, -5.5000]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 06:18:30 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you can survive something, that thing is survivable
If you can imagine something, that thing is imaginable
If you can publish something, that thing is publishable
If you can advise something, that thing is advisable
If you can explain something, that thing is explainable
If you can expect something, that thing is expectable
If you can execute something, that thing is executable
If you can write something, that thing is
2024-07-25 06:18:30 root INFO     [order_1_approx] starting weight calculation for If you can explain something, that thing is explainable
If you can imagine something, that thing is imaginable
If you can advise something, that thing is advisable
If you can survive something, that thing is survivable
If you can expect something, that thing is expectable
If you can write something, that thing is writeable
If you can execute something, that thing is executable
If you can publish something, that thing is
2024-07-25 06:18:31 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 06:20:51 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0977, -2.2500,  6.0312,  ...,  1.0078,  1.0312,  0.1973],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 13.1250, -13.2500,   7.1875,  ..., -10.2500,  -3.0469, -13.3125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0021,  0.0068,  0.0107,  ..., -0.0022, -0.0065, -0.0052],
        [ 0.0004, -0.0019, -0.0097,  ..., -0.0006,  0.0040,  0.0068],
        [-0.0040, -0.0068,  0.0344,  ..., -0.0018,  0.0058,  0.0059],
        ...,
        [ 0.0031,  0.0029,  0.0049,  ...,  0.0018,  0.0010,  0.0019],
        [-0.0002, -0.0032, -0.0106,  ..., -0.0031,  0.0104,  0.0025],
        [-0.0018, -0.0042, -0.0156,  ...,  0.0035,  0.0048,  0.0113]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 10.9844, -12.0469,  10.6562,  ..., -10.8203,  -2.6211, -12.1953]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 06:20:52 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When he occurs something, something has been occurred
When he announces something, something has been announced
When he adds something, something has been added
When he develops something, something has been developed
When he replaces something, something has been replaced
When he represents something, something has been represented
When he suggests something, something has been suggested
When he hears something, something has been
2024-07-25 06:20:52 root INFO     [order_1_approx] starting weight calculation for When he suggests something, something has been suggested
When he announces something, something has been announced
When he develops something, something has been developed
When he represents something, something has been represented
When he occurs something, something has been occurred
When he replaces something, something has been replaced
When he hears something, something has been heard
When he adds something, something has been
2024-07-25 06:20:53 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 06:24:31 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2188, -1.0156,  0.7344,  ...,  1.1094, -2.9531,  2.0938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 16.0000,   3.7812, -11.0000,  ...,  -3.7812,  -2.0469,   2.4062],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0021,  0.0013, -0.0069,  ..., -0.0034,  0.0019, -0.0007],
        [-0.0003,  0.0048, -0.0065,  ...,  0.0002, -0.0027, -0.0051],
        [-0.0026, -0.0002,  0.0762,  ..., -0.0011, -0.0011, -0.0012],
        ...,
        [-0.0034,  0.0040,  0.0016,  ...,  0.0006,  0.0033,  0.0049],
        [ 0.0045, -0.0013, -0.0144,  ...,  0.0013,  0.0052, -0.0020],
        [ 0.0019,  0.0015,  0.0002,  ..., -0.0056, -0.0015,  0.0101]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 14.7969,   1.9717, -10.3047,  ...,  -4.1094,  -2.5176,   2.8418]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 06:24:32 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you can explain something, that thing is explainable
If you can imagine something, that thing is imaginable
If you can advise something, that thing is advisable
If you can survive something, that thing is survivable
If you can expect something, that thing is expectable
If you can write something, that thing is writeable
If you can execute something, that thing is executable
If you can publish something, that thing is
2024-07-25 06:24:33 root INFO     [order_1_approx] starting weight calculation for If you can execute something, that thing is executable
If you can survive something, that thing is survivable
If you can imagine something, that thing is imaginable
If you can explain something, that thing is explainable
If you can publish something, that thing is publishable
If you can write something, that thing is writeable
If you can expect something, that thing is expectable
If you can advise something, that thing is
2024-07-25 06:24:33 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 06:26:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.0625, -0.4219,  5.4375,  ...,  0.6172,  1.3438, -0.0938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 17.8750, -20.0000,   5.6250,  ..., -15.5000,   0.2578,  -2.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0019, -0.0018, -0.0009,  ...,  0.0014,  0.0002,  0.0025],
        [ 0.0047, -0.0001, -0.0018,  ..., -0.0061, -0.0051, -0.0005],
        [-0.0001, -0.0016,  0.0540,  ...,  0.0001,  0.0038,  0.0004],
        ...,
        [-0.0051,  0.0010,  0.0017,  ..., -0.0002, -0.0009, -0.0006],
        [ 0.0023, -0.0005, -0.0050,  ..., -0.0033,  0.0103, -0.0033],
        [ 0.0021, -0.0014, -0.0067,  ...,  0.0027, -0.0061,  0.0101]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 16.9219, -20.5312,   6.8398,  ..., -15.3281,  -0.5088,  -2.6621]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 06:26:25 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When he suggests something, something has been suggested
When he announces something, something has been announced
When he develops something, something has been developed
When he represents something, something has been represented
When he occurs something, something has been occurred
When he replaces something, something has been replaced
When he hears something, something has been heard
When he adds something, something has been
2024-07-25 06:26:25 root INFO     [order_1_approx] starting weight calculation for When he occurs something, something has been occurred
When he announces something, something has been announced
When he hears something, something has been heard
When he adds something, something has been added
When he replaces something, something has been replaced
When he develops something, something has been developed
When he suggests something, something has been suggested
When he represents something, something has been
2024-07-25 06:26:25 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 06:30:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2812, -2.7188,  1.8125,  ...,  1.1875,  1.4062, -0.1621],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 17.8750,   1.3906, -19.0000,  ...,   4.0312,  -3.2969,  -8.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0010,  0.0027,  0.0028,  ..., -0.0021,  0.0023,  0.0013],
        [ 0.0023,  0.0056, -0.0029,  ..., -0.0029,  0.0009, -0.0022],
        [ 0.0021,  0.0023,  0.0623,  ...,  0.0023,  0.0047, -0.0019],
        ...,
        [ 0.0004,  0.0015, -0.0030,  ...,  0.0002,  0.0006,  0.0012],
        [ 0.0023,  0.0010,  0.0032,  ..., -0.0009,  0.0141, -0.0016],
        [ 0.0028, -0.0021, -0.0005,  ..., -0.0007, -0.0046,  0.0176]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 16.6406,   1.1094, -17.0156,  ...,   4.7148,  -1.3369,  -9.3750]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 06:30:20 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you can execute something, that thing is executable
If you can survive something, that thing is survivable
If you can imagine something, that thing is imaginable
If you can explain something, that thing is explainable
If you can publish something, that thing is publishable
If you can write something, that thing is writeable
If you can expect something, that thing is expectable
If you can advise something, that thing is
2024-07-25 06:30:20 root INFO     [order_1_approx] starting weight calculation for If you can survive something, that thing is survivable
If you can advise something, that thing is advisable
If you can explain something, that thing is explainable
If you can write something, that thing is writeable
If you can imagine something, that thing is imaginable
If you can publish something, that thing is publishable
If you can expect something, that thing is expectable
If you can execute something, that thing is
2024-07-25 06:30:20 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 06:32:02 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.0938, -2.1562,  0.1953,  ...,  1.0312, -2.3438,  0.8906],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 14.8125,  -9.0625,  -9.0625,  ..., -18.1250,   8.2500,  -8.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0049,  0.0012,  0.0013,  ..., -0.0037,  0.0031, -0.0022],
        [-0.0007,  0.0024,  0.0011,  ..., -0.0055, -0.0052,  0.0002],
        [-0.0012,  0.0014,  0.0630,  ...,  0.0003,  0.0033,  0.0006],
        ...,
        [ 0.0013,  0.0003, -0.0002,  ...,  0.0017,  0.0017,  0.0059],
        [-0.0034, -0.0004,  0.0014,  ..., -0.0010,  0.0110,  0.0028],
        [ 0.0022,  0.0002,  0.0019,  ..., -0.0009, -0.0006,  0.0110]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 15.1328, -10.3828,  -9.3047,  ..., -18.5625,   8.6406,  -9.7109]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 06:32:03 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When he occurs something, something has been occurred
When he announces something, something has been announced
When he hears something, something has been heard
When he adds something, something has been added
When he replaces something, something has been replaced
When he develops something, something has been developed
When he suggests something, something has been suggested
When he represents something, something has been
2024-07-25 06:32:03 root INFO     [order_1_approx] starting weight calculation for When he hears something, something has been heard
When he occurs something, something has been occurred
When he adds something, something has been added
When he suggests something, something has been suggested
When he replaces something, something has been replaced
When he represents something, something has been represented
When he develops something, something has been developed
When he announces something, something has been
2024-07-25 06:32:03 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 06:36:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6914, -2.1094,  5.9688,  ..., -0.2695, -2.6875,  1.8594],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 12.3125,   1.5938,  -1.8828,  ...,  -1.6875, -18.6250,  -0.6719],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 5.3711e-03,  1.9531e-03, -5.4016e-03,  ..., -5.4321e-03,
          3.9978e-03, -3.5400e-03],
        [ 2.9449e-03,  2.3193e-03, -1.0742e-02,  ..., -6.8970e-03,
          2.0752e-03, -3.6621e-03],
        [-3.4180e-03, -8.3923e-05,  5.6641e-02,  ..., -1.5163e-04,
         -5.0659e-03, -9.5367e-04],
        ...,
        [-1.0834e-03,  3.5706e-03, -7.3853e-03,  ..., -5.1880e-03,
          4.1504e-03, -1.2970e-03],
        [-1.6556e-03,  2.8076e-03,  1.8082e-03,  ..., -7.2861e-04,
          1.8799e-02, -1.0010e-02],
        [ 2.8229e-03,  2.4109e-03, -1.2665e-03,  ...,  2.1553e-04,
          2.1210e-03,  1.2573e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 13.2891,   1.5977,  -2.1191,  ...,  -1.1562, -19.2812,  -0.6357]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 06:36:14 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you can survive something, that thing is survivable
If you can advise something, that thing is advisable
If you can explain something, that thing is explainable
If you can write something, that thing is writeable
If you can imagine something, that thing is imaginable
If you can publish something, that thing is publishable
If you can expect something, that thing is expectable
If you can execute something, that thing is
2024-07-25 06:36:15 root INFO     [order_1_approx] starting weight calculation for If you can write something, that thing is writeable
If you can imagine something, that thing is imaginable
If you can execute something, that thing is executable
If you can publish something, that thing is publishable
If you can expect something, that thing is expectable
If you can advise something, that thing is advisable
If you can explain something, that thing is explainable
If you can survive something, that thing is
2024-07-25 06:36:15 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 06:37:42 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.4219, -2.2656,  1.2969,  ...,  0.7266, -1.8516, -0.4727],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  9.0625, -13.5000,  -3.0781,  ...,  -9.0000,  -0.5352,  -4.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0041, -0.0041, -0.0058,  ...,  0.0015, -0.0064,  0.0015],
        [ 0.0056,  0.0020,  0.0110,  ..., -0.0034, -0.0047,  0.0019],
        [-0.0013,  0.0037,  0.0669,  ..., -0.0003,  0.0052, -0.0031],
        ...,
        [-0.0019,  0.0039, -0.0044,  ..., -0.0006,  0.0053,  0.0048],
        [ 0.0029, -0.0011, -0.0041,  ...,  0.0016,  0.0125, -0.0029],
        [ 0.0051, -0.0016, -0.0030,  ...,  0.0014, -0.0043,  0.0092]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  7.6953, -14.0859,  -2.3477,  ...,  -8.8672,  -1.8867,  -6.3945]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 06:37:43 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When he hears something, something has been heard
When he occurs something, something has been occurred
When he adds something, something has been added
When he suggests something, something has been suggested
When he replaces something, something has been replaced
When he represents something, something has been represented
When he develops something, something has been developed
When he announces something, something has been
2024-07-25 06:37:43 root INFO     [order_1_approx] starting weight calculation for When he hears something, something has been heard
When he develops something, something has been developed
When he announces something, something has been announced
When he adds something, something has been added
When he replaces something, something has been replaced
When he represents something, something has been represented
When he occurs something, something has been occurred
When he suggests something, something has been
2024-07-25 06:37:43 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 06:42:12 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.6016, -3.4844,  1.9375,  ..., -1.2734, -2.0625,  1.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([11.1250,  9.3750, -2.9062,  ..., -5.5000, -5.0625, -0.9844],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-7.2098e-04,  1.0437e-02,  1.5015e-02,  ..., -8.9111e-03,
          4.6387e-03, -1.9989e-03],
        [ 4.6387e-03,  1.8311e-03, -8.6060e-03,  ...,  5.0354e-04,
         -9.6512e-04, -3.1586e-03],
        [ 1.7929e-04, -4.4556e-03,  4.1260e-02,  ...,  3.8452e-03,
          9.2163e-03, -3.0518e-05],
        ...,
        [ 2.2430e-03,  6.8283e-04, -1.1230e-02,  ...,  3.0060e-03,
         -5.6152e-03, -3.1586e-03],
        [ 1.6098e-03, -2.2736e-03,  9.8419e-04,  ..., -7.5531e-04,
          1.0864e-02, -2.4872e-03],
        [ 5.9204e-03, -8.7280e-03, -1.4709e-02,  ...,  2.1515e-03,
          5.8594e-03,  1.5747e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[14.1719,  8.7812, -2.3633,  ..., -4.8164, -4.7305, -3.9844]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 06:42:14 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you can write something, that thing is writeable
If you can imagine something, that thing is imaginable
If you can execute something, that thing is executable
If you can publish something, that thing is publishable
If you can expect something, that thing is expectable
If you can advise something, that thing is advisable
If you can explain something, that thing is explainable
If you can survive something, that thing is
2024-07-25 06:42:14 root INFO     total operator prediction time: 2831.322361946106 seconds
2024-07-25 06:42:14 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on verb+tion_irreg
2024-07-25 06:42:14 root INFO     building operator verb+tion_irreg
2024-07-25 06:42:14 root INFO     [order_1_approx] starting weight calculation for To realize results in realization
To allege results in allegation
To utilize results in utilization
To admire results in admiration
To declare results in declaration
To accuse results in accusation
To prepare results in preparation
To observe results in
2024-07-25 06:42:14 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 06:43:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1406, -2.6562,  3.7812,  ...,  2.9688, -0.9844, -0.9023],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([20.1250, -9.0000,  5.4375,  ..., -4.9688,  1.9375, -5.8438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-8.8501e-04, -2.1820e-03, -1.1780e-02,  ..., -4.6158e-04,
         -2.3499e-03,  1.8387e-03],
        [-3.8452e-03,  7.3242e-03, -2.2430e-03,  ..., -5.5542e-03,
         -4.3640e-03,  3.2349e-03],
        [-1.9531e-03,  2.0142e-03,  5.6396e-02,  ...,  1.8463e-03,
          6.1512e-05, -2.2125e-03],
        ...,
        [ 6.8665e-04, -3.1738e-03,  4.8218e-03,  ...,  2.1515e-03,
         -6.6757e-05, -2.6245e-03],
        [ 3.8605e-03, -4.5166e-03, -5.2490e-03,  ...,  5.7602e-04,
          9.2773e-03,  2.5940e-03],
        [-3.0518e-05,  1.5259e-03, -4.1504e-03,  ..., -1.5354e-04,
         -2.6398e-03,  8.5449e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 19.3594, -10.0312,   5.5547,  ...,  -4.1562,   1.6924,  -6.6484]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 06:43:25 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When he hears something, something has been heard
When he develops something, something has been developed
When he announces something, something has been announced
When he adds something, something has been added
When he replaces something, something has been replaced
When he represents something, something has been represented
When he occurs something, something has been occurred
When he suggests something, something has been
2024-07-25 06:43:25 root INFO     [order_1_approx] starting weight calculation for When he represents something, something has been represented
When he adds something, something has been added
When he hears something, something has been heard
When he suggests something, something has been suggested
When he develops something, something has been developed
When he replaces something, something has been replaced
When he announces something, something has been announced
When he occurs something, something has been
2024-07-25 06:43:25 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 06:47:54 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5859, -2.0625,  4.6875,  ...,  2.8750, -0.0137,  0.0469],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 4.0625, 11.7500, -6.1562,  ...,  7.3125,  5.0625, -3.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 6.8359e-03,  5.1880e-03,  2.1667e-03,  ...,  1.0147e-03,
         -5.7678e-03,  3.0518e-05],
        [-9.9182e-04, -1.8692e-03,  1.8463e-03,  ...,  1.6937e-03,
          2.2736e-03,  4.6082e-03],
        [-8.9111e-03, -4.6692e-03,  7.5684e-02,  ...,  3.8147e-03,
          2.5024e-03,  4.8218e-03],
        ...,
        [-1.3199e-03, -6.5002e-03,  2.1210e-03,  ...,  5.9509e-03,
          5.9204e-03,  8.9111e-03],
        [-2.4796e-04,  1.3428e-03, -4.6692e-03,  ...,  5.4169e-04,
          1.5320e-02,  5.2185e-03],
        [-6.3477e-03,  1.0834e-03,  4.9438e-03,  ...,  9.0027e-04,
          7.5912e-04,  1.5625e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.3496, 12.4688, -4.5234,  ...,  8.6094,  4.9336, -4.3359]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 06:47:55 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To realize results in realization
To allege results in allegation
To utilize results in utilization
To admire results in admiration
To declare results in declaration
To accuse results in accusation
To prepare results in preparation
To observe results in
2024-07-25 06:47:55 root INFO     [order_1_approx] starting weight calculation for To declare results in declaration
To accuse results in accusation
To observe results in observation
To allege results in allegation
To utilize results in utilization
To prepare results in preparation
To admire results in admiration
To realize results in
2024-07-25 06:47:55 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 06:49:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5391, -2.3438,  5.1562,  ...,  1.1484, -1.2812,  0.3477],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 6.8750, -9.7500,  7.6250,  ..., -3.1250,  6.1562, -0.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0018, -0.0072,  0.0173,  ..., -0.0003, -0.0009,  0.0006],
        [ 0.0049,  0.0056, -0.0177,  ..., -0.0057, -0.0061,  0.0059],
        [ 0.0018,  0.0135,  0.0239,  ...,  0.0039,  0.0100, -0.0087],
        ...,
        [ 0.0034,  0.0060, -0.0091,  ..., -0.0029, -0.0031,  0.0026],
        [ 0.0014,  0.0005, -0.0079,  ...,  0.0011,  0.0173,  0.0024],
        [ 0.0024,  0.0048, -0.0183,  ...,  0.0081,  0.0098,  0.0129]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 3.5918, -7.4805,  9.6719,  ..., -2.6621,  5.2109, -2.4023]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 06:49:08 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When he represents something, something has been represented
When he adds something, something has been added
When he hears something, something has been heard
When he suggests something, something has been suggested
When he develops something, something has been developed
When he replaces something, something has been replaced
When he announces something, something has been announced
When he occurs something, something has been
2024-07-25 06:49:08 root INFO     [order_1_approx] starting weight calculation for When he hears something, something has been heard
When he represents something, something has been represented
When he adds something, something has been added
When he suggests something, something has been suggested
When he announces something, something has been announced
When he occurs something, something has been occurred
When he develops something, something has been developed
When he replaces something, something has been
2024-07-25 06:49:08 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 06:53:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0781,  0.0493,  3.1562,  ...,  3.9844, -0.6875, -1.2656],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([16.0000, 13.3125, -9.7500,  ...,  1.8750, -5.3438,  1.8281],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0026,  0.0112, -0.0161,  ..., -0.0007, -0.0053,  0.0056],
        [-0.0025,  0.0031,  0.0009,  ...,  0.0014,  0.0017,  0.0060],
        [-0.0068, -0.0068,  0.0981,  ...,  0.0046,  0.0099, -0.0041],
        ...,
        [ 0.0043,  0.0034, -0.0107,  ..., -0.0001,  0.0078,  0.0051],
        [ 0.0032, -0.0032, -0.0049,  ...,  0.0040,  0.0215,  0.0004],
        [-0.0009, -0.0015,  0.0009,  ...,  0.0015, -0.0079,  0.0210]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[14.8516, 15.9219, -6.6641,  ...,  3.3398, -4.8398,  1.6113]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 06:53:45 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To declare results in declaration
To accuse results in accusation
To observe results in observation
To allege results in allegation
To utilize results in utilization
To prepare results in preparation
To admire results in admiration
To realize results in
2024-07-25 06:53:45 root INFO     [order_1_approx] starting weight calculation for To realize results in realization
To observe results in observation
To admire results in admiration
To accuse results in accusation
To utilize results in utilization
To prepare results in preparation
To declare results in declaration
To allege results in
2024-07-25 06:53:46 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 06:54:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.1562, -2.1562,  1.1719,  ...,  1.8438, -0.5078,  1.1875],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 12.5000, -11.1875,  -4.6250,  ..., -13.7500,  -0.7344,  -9.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 6.1035e-03,  2.6245e-03, -4.6082e-03,  ...,  5.1880e-04,
          3.2425e-04, -7.9727e-04],
        [ 1.6479e-03,  3.5400e-03, -2.8076e-03,  ..., -4.3335e-03,
         -1.0300e-03,  1.5488e-03],
        [-1.9531e-03,  1.8616e-03,  5.6152e-02,  ...,  1.1139e-03,
          5.6763e-03, -2.9602e-03],
        ...,
        [-2.2278e-03, -4.3678e-04, -4.2114e-03,  ...,  1.1826e-04,
          2.1553e-04,  2.3041e-03],
        [-7.7438e-04,  9.1553e-05, -3.6316e-03,  ...,  1.1597e-03,
          1.3245e-02,  9.1171e-04],
        [-1.2665e-03, -1.8921e-03, -7.8735e-03,  ...,  7.7820e-04,
         -7.7438e-04,  8.5449e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 11.4766, -12.7031,  -4.9453,  ..., -14.9219,  -0.3574,  -9.7422]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 06:54:52 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When he hears something, something has been heard
When he represents something, something has been represented
When he adds something, something has been added
When he suggests something, something has been suggested
When he announces something, something has been announced
When he occurs something, something has been occurred
When he develops something, something has been developed
When he replaces something, something has been
2024-07-25 06:54:53 root INFO     [order_1_approx] starting weight calculation for When he represents something, something has been represented
When he adds something, something has been added
When he announces something, something has been announced
When he suggests something, something has been suggested
When he hears something, something has been heard
When he occurs something, something has been occurred
When he replaces something, something has been replaced
When he develops something, something has been
2024-07-25 06:54:53 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 06:59:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1953, -1.1406,  3.1406,  ...,  2.4688, -1.0391,  1.0312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 10.1250,   4.8125, -19.7500,  ...,  -0.4062,  -8.6250,  -6.9375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-0.0017,  0.0041, -0.0057,  ...,  0.0023,  0.0019, -0.0016],
        [ 0.0083,  0.0019,  0.0063,  ..., -0.0007, -0.0026,  0.0083],
        [ 0.0014, -0.0040,  0.0781,  ..., -0.0011,  0.0038,  0.0037],
        ...,
        [ 0.0053,  0.0030, -0.0143,  ..., -0.0022,  0.0045,  0.0078],
        [ 0.0022, -0.0039, -0.0085,  ...,  0.0020,  0.0154,  0.0063],
        [-0.0037,  0.0039,  0.0002,  ..., -0.0036,  0.0070,  0.0103]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 10.5391,   4.6523, -16.7812,  ...,  -1.7773,  -9.1016,  -7.2109]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 06:59:44 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To realize results in realization
To observe results in observation
To admire results in admiration
To accuse results in accusation
To utilize results in utilization
To prepare results in preparation
To declare results in declaration
To allege results in
2024-07-25 06:59:45 root INFO     [order_1_approx] starting weight calculation for To prepare results in preparation
To accuse results in accusation
To declare results in declaration
To observe results in observation
To admire results in admiration
To allege results in allegation
To realize results in realization
To utilize results in
2024-07-25 06:59:45 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 07:00:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9062, -2.8906,  4.2500,  ..., -0.2305, -1.0625, -0.9766],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 11.0625, -13.0625,   7.0938,  ..., -14.5000,   5.0938,  -0.0312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-6.0272e-04,  1.9455e-04,  8.1635e-04,  ...,  1.5030e-03,
          8.4877e-05, -2.0905e-03],
        [ 3.8910e-03,  8.5449e-04, -8.5449e-04,  ..., -3.5706e-03,
         -4.4861e-03, -1.2131e-03],
        [ 2.8381e-03, -1.1139e-03,  4.8584e-02,  ..., -3.0060e-03,
          2.0599e-04, -4.0283e-03],
        ...,
        [-2.7771e-03,  3.0365e-03, -2.2888e-03,  ...,  2.4719e-03,
         -3.4180e-03,  2.3956e-03],
        [ 3.3875e-03,  1.5068e-04,  3.3875e-03,  ..., -7.5531e-04,
          1.1414e-02,  2.2888e-04],
        [ 1.4114e-03,  4.5395e-04, -2.4414e-03,  ..., -1.3504e-03,
          1.4954e-03,  9.2163e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 11.6484, -15.5625,   6.9219,  ..., -14.9609,   5.6602,  -0.0365]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 07:00:37 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for When he represents something, something has been represented
When he adds something, something has been added
When he announces something, something has been announced
When he suggests something, something has been suggested
When he hears something, something has been heard
When he occurs something, something has been occurred
When he replaces something, something has been replaced
When he develops something, something has been
2024-07-25 07:00:37 root INFO     total operator prediction time: 2719.553023338318 seconds
2024-07-25 07:00:37 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on adj - superlative
2024-07-25 07:00:37 root INFO     building operator adj - superlative
2024-07-25 07:00:37 root INFO     [order_1_approx] starting weight calculation for If something is the most merry, it is merriest
If something is the most harsh, it is harshest
If something is the most strong, it is strongest
If something is the most cheap, it is cheapest
If something is the most sad, it is saddest
If something is the most cruel, it is cruelest
If something is the most huge, it is hugest
If something is the most ugly, it is
2024-07-25 07:00:37 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 07:05:40 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.6250, -3.8281,  4.3438,  ...,  1.8047, -1.7422,  0.1094],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 18.2500,  10.0000, -12.0625,  ...,  11.3125, -13.6875,   4.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-6.8359e-03,  1.2817e-02, -2.5146e-02,  ..., -2.1057e-03,
          4.6997e-03,  4.1504e-03],
        [ 1.8311e-03,  1.4648e-03,  7.6904e-03,  ..., -2.8992e-03,
          1.9455e-04,  4.2419e-03],
        [-4.4250e-04, -7.7820e-03,  7.7148e-02,  ...,  1.2817e-03,
         -6.3705e-04,  2.6550e-03],
        ...,
        [-2.4414e-03,  9.1553e-04, -1.1292e-02,  ..., -2.1057e-03,
          4.1504e-03,  5.1270e-03],
        [ 4.6692e-03, -2.7008e-03, -2.4109e-03,  ...,  6.0730e-03,
          7.5989e-03,  2.7618e-03],
        [-2.7008e-03, -3.1433e-03, -5.5237e-03,  ..., -1.2398e-05,
         -1.6327e-03,  1.0071e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 18.3750,  10.0000, -14.1094,  ...,   9.7188, -14.7344,   2.1758]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 07:05:41 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To prepare results in preparation
To accuse results in accusation
To declare results in declaration
To observe results in observation
To admire results in admiration
To allege results in allegation
To realize results in realization
To utilize results in
2024-07-25 07:05:41 root INFO     [order_1_approx] starting weight calculation for To observe results in observation
To realize results in realization
To utilize results in utilization
To declare results in declaration
To prepare results in preparation
To allege results in allegation
To accuse results in accusation
To admire results in
2024-07-25 07:05:41 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 07:06:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.2422, -0.4609,  0.0039,  ...,  0.4434, -2.5000,  0.4902],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 20.3750,  21.3750,  -4.5625,  ...,  -3.5625,  -7.8750, -12.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-8.3542e-04,  4.1504e-03, -5.5542e-03,  ...,  2.1362e-03,
          8.3618e-03, -7.0496e-03],
        [-6.1035e-05,  5.4932e-03,  3.9673e-03,  ...,  2.7618e-03,
         -4.5776e-03,  5.1117e-04],
        [-4.9744e-03,  3.6316e-03,  5.2734e-02,  ..., -1.7624e-03,
         -9.2163e-03, -6.5308e-03],
        ...,
        [ 1.6022e-03, -1.3199e-03, -1.0071e-03,  ..., -1.5640e-03,
          2.4414e-03,  3.7079e-03],
        [ 2.1515e-03, -7.0190e-04, -5.9204e-03,  ...,  9.9182e-04,
          5.2795e-03,  2.2125e-03],
        [-1.0986e-02,  2.1076e-04,  4.1809e-03,  ..., -1.8005e-03,
         -9.3994e-03,  1.4114e-04]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 20.3750,  17.7656,  -6.6953,  ...,  -2.1094,  -7.9805, -15.1094]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 07:06:09 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most merry, it is merriest
If something is the most harsh, it is harshest
If something is the most strong, it is strongest
If something is the most cheap, it is cheapest
If something is the most sad, it is saddest
If something is the most cruel, it is cruelest
If something is the most huge, it is hugest
If something is the most ugly, it is
2024-07-25 07:06:10 root INFO     [order_1_approx] starting weight calculation for If something is the most cruel, it is cruelest
If something is the most ugly, it is ugliest
If something is the most strong, it is strongest
If something is the most huge, it is hugest
If something is the most merry, it is merriest
If something is the most sad, it is saddest
If something is the most harsh, it is harshest
If something is the most cheap, it is
2024-07-25 07:06:10 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 07:11:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6602, -0.2109,  0.0239,  ..., -1.3281, -2.7812,  1.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 11.1875,  15.2500, -19.5000,  ...,  -0.4316,  -8.2500,  -3.4688],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0045,  0.0070,  0.0010,  ...,  0.0032,  0.0085,  0.0072],
        [ 0.0047, -0.0026,  0.0155,  ..., -0.0038, -0.0035,  0.0056],
        [ 0.0063, -0.0102,  0.0786,  ...,  0.0033,  0.0105,  0.0059],
        ...,
        [ 0.0139, -0.0036, -0.0098,  ...,  0.0014,  0.0047,  0.0081],
        [ 0.0010, -0.0009,  0.0017,  ..., -0.0025,  0.0155,  0.0074],
        [-0.0090, -0.0028, -0.0033,  ...,  0.0004,  0.0150,  0.0016]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 12.4531,  15.3281, -19.1094,  ...,  -1.9844,  -8.5078,  -3.9355]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 07:11:38 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.9062, -0.2383,  4.5312,  ..., -0.2344, -2.9062,  2.7031],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 12.6250,  19.0000,   1.9219,  ..., -12.5625, -10.0000,   3.4219],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0029, -0.0038,  0.0076,  ...,  0.0002,  0.0016, -0.0036],
        [ 0.0009,  0.0057, -0.0044,  ..., -0.0029, -0.0103, -0.0046],
        [-0.0025,  0.0040,  0.0430,  ...,  0.0017, -0.0031, -0.0117],
        ...,
        [-0.0020, -0.0061, -0.0011,  ...,  0.0011, -0.0036,  0.0015],
        [-0.0016,  0.0034, -0.0018,  ...,  0.0034,  0.0050,  0.0069],
        [-0.0051,  0.0059,  0.0100,  ..., -0.0004, -0.0031,  0.0037]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 12.2500,  16.1875,   1.6504,  ..., -12.8125, -10.7969,   2.6113]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 07:11:38 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To observe results in observation
To realize results in realization
To utilize results in utilization
To declare results in declaration
To prepare results in preparation
To allege results in allegation
To accuse results in accusation
To admire results in
2024-07-25 07:11:39 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most cruel, it is cruelest
If something is the most ugly, it is ugliest
If something is the most strong, it is strongest
If something is the most huge, it is hugest
If something is the most merry, it is merriest
If something is the most sad, it is saddest
If something is the most harsh, it is harshest
If something is the most cheap, it is
2024-07-25 07:11:39 root INFO     [order_1_approx] starting weight calculation for To admire results in admiration
To prepare results in preparation
To observe results in observation
To allege results in allegation
To utilize results in utilization
To declare results in declaration
To realize results in realization
To accuse results in
2024-07-25 07:11:39 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 07:11:39 root INFO     [order_1_approx] starting weight calculation for If something is the most huge, it is hugest
If something is the most merry, it is merriest
If something is the most cheap, it is cheapest
If something is the most ugly, it is ugliest
If something is the most cruel, it is cruelest
If something is the most sad, it is saddest
If something is the most harsh, it is harshest
If something is the most strong, it is
2024-07-25 07:11:39 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 07:17:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.0781, -3.5781,  3.7969,  ...,  0.5977, -4.0000,  3.6719],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([17.1250, 10.5625, -1.8828,  ..., -7.0312, -4.1250, -8.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-1.7166e-05,  2.8839e-03,  7.9956e-03,  ...,  4.3335e-03,
          3.5706e-03, -3.6163e-03],
        [-4.3030e-03,  3.5095e-03, -7.9346e-03,  ..., -9.7656e-04,
          3.6316e-03,  3.5553e-03],
        [ 1.8921e-03,  6.8970e-03,  4.3945e-02,  ...,  5.5237e-03,
          2.0599e-04, -1.2085e-02],
        ...,
        [-2.8229e-03,  2.1362e-03,  9.5215e-03,  ...,  1.4648e-03,
         -1.6632e-03,  1.8997e-03],
        [ 1.9379e-03,  4.3030e-03,  7.9956e-03,  ...,  3.3569e-04,
          7.3547e-03, -5.2643e-04],
        [-1.5259e-05, -4.9591e-05, -2.3346e-03,  ..., -7.0190e-03,
          4.7607e-03,  8.6060e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[17.7031,  9.0547, -1.2812,  ..., -6.6484, -4.5430, -9.9453]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 07:17:10 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most huge, it is hugest
If something is the most merry, it is merriest
If something is the most cheap, it is cheapest
If something is the most ugly, it is ugliest
If something is the most cruel, it is cruelest
If something is the most sad, it is saddest
If something is the most harsh, it is harshest
If something is the most strong, it is
2024-07-25 07:17:10 root INFO     [order_1_approx] starting weight calculation for If something is the most strong, it is strongest
If something is the most harsh, it is harshest
If something is the most cheap, it is cheapest
If something is the most ugly, it is ugliest
If something is the most sad, it is saddest
If something is the most merry, it is merriest
If something is the most cruel, it is cruelest
If something is the most huge, it is
2024-07-25 07:17:10 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 07:17:36 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.1250, -1.1875, -1.8906,  ...,  1.6562,  0.6484,  0.7461],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  8.3750,   2.0000, -21.8750,  ...,   5.5625,   0.9492,  -9.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0093,  0.0066, -0.0017,  ...,  0.0043, -0.0056, -0.0001],
        [ 0.0025,  0.0034,  0.0132,  ..., -0.0045,  0.0032,  0.0115],
        [-0.0013, -0.0036,  0.0791,  ...,  0.0008,  0.0124,  0.0036],
        ...,
        [ 0.0061,  0.0021, -0.0082,  ...,  0.0006,  0.0052,  0.0137],
        [-0.0012, -0.0020, -0.0006,  ..., -0.0022,  0.0147,  0.0054],
        [ 0.0034,  0.0003, -0.0073,  ...,  0.0030, -0.0025,  0.0135]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  6.9453,  -0.4473, -19.7500,  ...,   5.5586,   3.1191,  -9.0234]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 07:17:37 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To admire results in admiration
To prepare results in preparation
To observe results in observation
To allege results in allegation
To utilize results in utilization
To declare results in declaration
To realize results in realization
To accuse results in
2024-07-25 07:17:37 root INFO     [order_1_approx] starting weight calculation for To prepare results in preparation
To accuse results in accusation
To allege results in allegation
To utilize results in utilization
To observe results in observation
To realize results in realization
To admire results in admiration
To declare results in
2024-07-25 07:17:37 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 07:22:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 3.0469, -2.6250,  2.8906,  ...,  1.5469, -3.3906,  2.2656],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 14.0625,  17.5000,  -0.1367,  ...,  -8.6875, -15.0000,  -4.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-0.0003,  0.0087,  0.0107,  ...,  0.0065,  0.0013, -0.0081],
        [ 0.0035,  0.0025, -0.0003,  ...,  0.0041,  0.0046,  0.0071],
        [ 0.0029,  0.0043,  0.0581,  ..., -0.0022, -0.0078, -0.0073],
        ...,
        [-0.0033,  0.0005,  0.0060,  ..., -0.0031, -0.0063, -0.0018],
        [-0.0014,  0.0007, -0.0070,  ..., -0.0015,  0.0057,  0.0084],
        [-0.0012, -0.0005, -0.0116,  ..., -0.0034,  0.0031,  0.0119]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 12.1562,  17.6875,  -0.1677,  ...,  -8.7266, -16.1406,  -6.1406]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 07:22:42 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most strong, it is strongest
If something is the most harsh, it is harshest
If something is the most cheap, it is cheapest
If something is the most ugly, it is ugliest
If something is the most sad, it is saddest
If something is the most merry, it is merriest
If something is the most cruel, it is cruelest
If something is the most huge, it is
2024-07-25 07:22:42 root INFO     [order_1_approx] starting weight calculation for If something is the most cruel, it is cruelest
If something is the most strong, it is strongest
If something is the most harsh, it is harshest
If something is the most ugly, it is ugliest
If something is the most sad, it is saddest
If something is the most cheap, it is cheapest
If something is the most huge, it is hugest
If something is the most merry, it is
2024-07-25 07:22:43 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 07:23:32 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.4219, -0.6914, -1.0156,  ...,  1.7031, -0.7383,  1.5156],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([11.5625,  7.0625, -7.6250,  ..., -3.9531, -2.8906,  5.0625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.2654e-03,  4.8523e-03, -3.0212e-03,  ..., -9.6893e-04,
          1.1444e-05,  1.8158e-03],
        [ 4.3030e-03,  1.2283e-03, -7.6294e-05,  ...,  4.0894e-03,
         -1.0193e-02,  8.6670e-03],
        [-5.4321e-03, -8.7891e-03,  8.4473e-02,  ...,  8.6670e-03,
          4.0894e-03,  1.7700e-03],
        ...,
        [-4.9591e-04, -3.8452e-03, -6.2561e-03,  ...,  2.0752e-03,
          1.6479e-03,  7.6294e-03],
        [ 3.5400e-03, -1.3351e-04,  5.4932e-03,  ..., -1.4687e-04,
          1.4709e-02,  5.0354e-04],
        [-8.4229e-03, -6.7749e-03,  2.7161e-03,  ..., -5.8746e-04,
          1.0559e-02,  6.1646e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[10.2812,  5.5312, -7.5391,  ..., -4.5820, -3.3633,  3.0781]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 07:23:32 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To prepare results in preparation
To accuse results in accusation
To allege results in allegation
To utilize results in utilization
To observe results in observation
To realize results in realization
To admire results in admiration
To declare results in
2024-07-25 07:23:33 root INFO     [order_1_approx] starting weight calculation for To allege results in allegation
To admire results in admiration
To observe results in observation
To realize results in realization
To accuse results in accusation
To declare results in declaration
To utilize results in utilization
To prepare results in
2024-07-25 07:23:33 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 07:28:21 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.8594, -1.4375,  8.0625,  ...,  0.3477, -4.8750,  3.5156],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 17.2500,  15.0625,  13.5625,  ..., -11.6875,   8.0625,   7.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-0.0029,  0.0028,  0.0111,  ...,  0.0093, -0.0031, -0.0092],
        [-0.0011,  0.0011,  0.0066,  ..., -0.0002,  0.0003, -0.0051],
        [-0.0026,  0.0063,  0.0383,  ...,  0.0067, -0.0063, -0.0069],
        ...,
        [-0.0014,  0.0032, -0.0043,  ..., -0.0002,  0.0016,  0.0015],
        [-0.0051, -0.0081, -0.0026,  ...,  0.0024,  0.0099,  0.0025],
        [ 0.0031,  0.0007, -0.0063,  ..., -0.0015, -0.0028,  0.0103]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 16.1250,  14.4922,  12.5078,  ..., -11.6094,   7.2031,   5.2266]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 07:28:22 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most cruel, it is cruelest
If something is the most strong, it is strongest
If something is the most harsh, it is harshest
If something is the most ugly, it is ugliest
If something is the most sad, it is saddest
If something is the most cheap, it is cheapest
If something is the most huge, it is hugest
If something is the most merry, it is
2024-07-25 07:28:22 root INFO     [order_1_approx] starting weight calculation for If something is the most harsh, it is harshest
If something is the most merry, it is merriest
If something is the most huge, it is hugest
If something is the most strong, it is strongest
If something is the most cheap, it is cheapest
If something is the most cruel, it is cruelest
If something is the most ugly, it is ugliest
If something is the most sad, it is
2024-07-25 07:28:22 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 07:29:11 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.4609, -2.4219,  1.6406,  ...,  0.0938, -2.0938,  0.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([15.0000,  8.1250, -4.7500,  ..., -9.5625, -5.1250,  1.8125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-0.0011,  0.0026, -0.0132,  ..., -0.0098, -0.0057, -0.0003],
        [-0.0018,  0.0013,  0.0036,  ...,  0.0014, -0.0012,  0.0045],
        [-0.0007, -0.0025,  0.0703,  ...,  0.0079,  0.0069, -0.0046],
        ...,
        [-0.0006,  0.0020, -0.0154,  ..., -0.0029, -0.0088,  0.0051],
        [ 0.0027,  0.0033, -0.0015,  ...,  0.0040,  0.0129, -0.0033],
        [-0.0005,  0.0008, -0.0065,  ..., -0.0009,  0.0045,  0.0092]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 11.3672,  10.0000,  -2.7930,  ..., -12.1250,  -4.6758,   2.6035]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 07:29:12 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for To allege results in allegation
To admire results in admiration
To observe results in observation
To realize results in realization
To accuse results in accusation
To declare results in declaration
To utilize results in utilization
To prepare results in
2024-07-25 07:29:12 root INFO     total operator prediction time: 2818.191060781479 seconds
2024-07-25 07:29:12 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on adj+ly_reg
2024-07-25 07:29:12 root INFO     building operator adj+ly_reg
2024-07-25 07:29:12 root INFO     [order_1_approx] starting weight calculation for The adjective form of regional is regionally
The adjective form of international is internationally
The adjective form of rare is rarely
The adjective form of obvious is obviously
The adjective form of according is accordingly
The adjective form of sexual is sexually
The adjective form of actual is actually
The adjective form of increasing is
2024-07-25 07:29:12 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 07:34:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.4844,  1.7656,  7.7812,  ...,  1.4375, -4.0000,  1.6328],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 10.0000,  10.3125,  -1.7188,  ...,  -6.6875,  -4.7500, -10.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.5940e-03,  2.0752e-03,  1.1597e-02,  ...,  3.3417e-03,
          1.7700e-03, -2.7771e-03],
        [-5.7602e-04, -8.0872e-04,  3.9062e-03,  ...,  2.8801e-04,
         -1.2894e-03, -7.0190e-04],
        [-4.5776e-04,  2.0142e-03,  2.9419e-02,  ...,  1.0223e-03,
         -5.1880e-03, -1.6556e-03],
        ...,
        [ 2.4872e-03,  1.6174e-03, -3.1738e-03,  ..., -6.4850e-04,
          7.9727e-04,  4.1962e-05],
        [-4.9744e-03,  2.2278e-03, -5.9814e-03,  ...,  1.6937e-03,
          1.1139e-03,  2.4719e-03],
        [-7.1411e-03,  8.0872e-04,  1.7395e-03,  ..., -2.5940e-03,
         -7.8125e-03,  4.2725e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  9.7734,   8.2500,  -2.7012,  ...,  -5.6797,  -4.2891, -12.4922]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 07:34:02 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most harsh, it is harshest
If something is the most merry, it is merriest
If something is the most huge, it is hugest
If something is the most strong, it is strongest
If something is the most cheap, it is cheapest
If something is the most cruel, it is cruelest
If something is the most ugly, it is ugliest
If something is the most sad, it is
2024-07-25 07:34:02 root INFO     [order_1_approx] starting weight calculation for If something is the most ugly, it is ugliest
If something is the most huge, it is hugest
If something is the most cheap, it is cheapest
If something is the most sad, it is saddest
If something is the most harsh, it is harshest
If something is the most merry, it is merriest
If something is the most strong, it is strongest
If something is the most cruel, it is
2024-07-25 07:34:02 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 07:35:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.7344, -1.0000,  2.4531,  ...,  1.3750,  1.5000, -1.2344],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 4.1250, -1.0000,  5.3750,  ..., -6.6250,  0.6719,  5.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0110,  0.0002, -0.0073,  ...,  0.0063,  0.0107,  0.0050],
        [-0.0094,  0.0026,  0.0157,  ..., -0.0098, -0.0043, -0.0164],
        [-0.0025,  0.0102,  0.0610,  ..., -0.0063, -0.0028, -0.0103],
        ...,
        [ 0.0089, -0.0022,  0.0001,  ...,  0.0071, -0.0016,  0.0120],
        [ 0.0003, -0.0019, -0.0006,  ...,  0.0060,  0.0100,  0.0052],
        [-0.0001,  0.0044, -0.0074,  ..., -0.0007,  0.0106,  0.0148]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 7.1055, -2.4766,  5.2852,  ..., -6.2539, -0.2354,  6.9688]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 07:35:14 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of regional is regionally
The adjective form of international is internationally
The adjective form of rare is rarely
The adjective form of obvious is obviously
The adjective form of according is accordingly
The adjective form of sexual is sexually
The adjective form of actual is actually
The adjective form of increasing is
2024-07-25 07:35:14 root INFO     [order_1_approx] starting weight calculation for The adjective form of regional is regionally
The adjective form of sexual is sexually
The adjective form of international is internationally
The adjective form of according is accordingly
The adjective form of actual is actually
The adjective form of increasing is increasingly
The adjective form of rare is rarely
The adjective form of obvious is
2024-07-25 07:35:14 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 07:39:44 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.6562, -0.4219,  7.3750,  ...,  1.1797, -3.8438,  1.9062],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 15.9375,  26.3750,  -5.9688,  ..., -13.5625, -20.7500,  -5.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0035,  0.0030,  0.0061,  ...,  0.0015,  0.0061,  0.0001],
        [ 0.0002,  0.0026, -0.0164,  ...,  0.0020, -0.0027, -0.0083],
        [-0.0048,  0.0041,  0.0493,  ...,  0.0014, -0.0062, -0.0110],
        ...,
        [ 0.0006,  0.0027,  0.0104,  ..., -0.0002, -0.0055, -0.0029],
        [-0.0049,  0.0027, -0.0060,  ...,  0.0043,  0.0110, -0.0047],
        [-0.0015,  0.0035, -0.0002,  ..., -0.0032, -0.0065,  0.0037]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 15.7656,  23.0781,  -6.5898,  ..., -14.1875, -21.0000,  -7.7930]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 07:39:45 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most ugly, it is ugliest
If something is the most huge, it is hugest
If something is the most cheap, it is cheapest
If something is the most sad, it is saddest
If something is the most harsh, it is harshest
If something is the most merry, it is merriest
If something is the most strong, it is strongest
If something is the most cruel, it is
2024-07-25 07:39:45 root INFO     [order_1_approx] starting weight calculation for If something is the most sad, it is saddest
If something is the most ugly, it is ugliest
If something is the most strong, it is strongest
If something is the most cruel, it is cruelest
If something is the most huge, it is hugest
If something is the most merry, it is merriest
If something is the most cheap, it is cheapest
If something is the most harsh, it is
2024-07-25 07:39:45 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 07:41:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3164, -2.1094, -1.0625,  ...,  1.8750,  0.1641,  1.6094],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 6.6875, -0.7812,  4.9375,  ..., -2.9062, 13.0000,  1.1562],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0078, -0.0005,  0.0045,  ..., -0.0031, -0.0040, -0.0049],
        [ 0.0038, -0.0004,  0.0103,  ..., -0.0026, -0.0048,  0.0034],
        [ 0.0031,  0.0024,  0.0835,  ..., -0.0020, -0.0038, -0.0052],
        ...,
        [ 0.0010,  0.0023,  0.0087,  ...,  0.0035,  0.0072,  0.0014],
        [ 0.0008, -0.0026,  0.0132,  ..., -0.0021,  0.0194,  0.0026],
        [-0.0013,  0.0011, -0.0038,  ..., -0.0027,  0.0012,  0.0089]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.7578, -1.1572,  4.5312,  ..., -1.5693, 11.1016,  0.7822]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 07:41:08 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of regional is regionally
The adjective form of sexual is sexually
The adjective form of international is internationally
The adjective form of according is accordingly
The adjective form of actual is actually
The adjective form of increasing is increasingly
The adjective form of rare is rarely
The adjective form of obvious is
2024-07-25 07:41:08 root INFO     [order_1_approx] starting weight calculation for The adjective form of rare is rarely
The adjective form of sexual is sexually
The adjective form of increasing is increasingly
The adjective form of according is accordingly
The adjective form of international is internationally
The adjective form of actual is actually
The adjective form of obvious is obviously
The adjective form of regional is
2024-07-25 07:41:08 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 07:45:27 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 3.7188, -1.2969, 10.2500,  ...,  0.4922, -4.7500,  2.5312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([16.0000, 16.3750, -6.4688,  ..., -1.1172,  1.8750,  2.7031],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.9978e-03,  7.5531e-04, -5.9509e-03,  ...,  2.5940e-04,
         -5.1117e-04, -2.0218e-04],
        [ 9.4986e-04,  4.9744e-03, -8.1787e-03,  ...,  3.5553e-03,
         -4.6997e-03, -4.5471e-03],
        [ 1.0834e-03,  3.3875e-03,  5.1514e-02,  ..., -3.5858e-04,
         -7.4158e-03, -4.5166e-03],
        ...,
        [-9.9945e-04,  4.9438e-03,  1.4725e-03,  ..., -8.0872e-04,
         -4.3335e-03, -1.7319e-03],
        [-4.5013e-04,  3.6316e-03,  3.2349e-03,  ..., -4.9591e-05,
          1.6602e-02, -2.3041e-03],
        [ 1.9455e-04,  4.7302e-03, -9.5367e-04,  ..., -2.4414e-03,
         -5.4932e-03,  1.1536e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[14.9609, 13.5156, -6.1953,  ..., -1.2529,  2.8281, -1.0840]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 07:45:28 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is the most sad, it is saddest
If something is the most ugly, it is ugliest
If something is the most strong, it is strongest
If something is the most cruel, it is cruelest
If something is the most huge, it is hugest
If something is the most merry, it is merriest
If something is the most cheap, it is cheapest
If something is the most harsh, it is
2024-07-25 07:45:28 root INFO     total operator prediction time: 2691.2798268795013 seconds
2024-07-25 07:45:28 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on verb+er_irreg
2024-07-25 07:45:28 root INFO     building operator verb+er_irreg
2024-07-25 07:45:28 root INFO     [order_1_approx] starting weight calculation for If you begin something, you are a beginner
If you develop something, you are a developer
If you recommend something, you are a recommender
If you choreograph something, you are a choreographer
If you defend something, you are a defender
If you send something, you are a sender
If you organize something, you are a organizer
If you write something, you are a
2024-07-25 07:45:28 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 07:47:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.7070, -0.9219,  6.1250,  ...,  1.4062, -0.5430,  0.0093],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([16.6250,  7.1562,  2.9375,  ..., -6.4375, -2.1562, -0.1562],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 6.6528e-03, -2.1667e-03,  6.1340e-03,  ...,  4.7302e-04,
         -2.6321e-04, -2.7924e-03],
        [ 6.6376e-04,  2.1515e-03, -5.7983e-03,  ...,  3.5706e-03,
          8.3160e-04, -2.0905e-03],
        [-3.1281e-03,  4.9438e-03,  5.2490e-02,  ..., -1.0071e-03,
         -3.5095e-03,  2.1973e-03],
        ...,
        [ 8.3008e-03,  1.0193e-02,  1.2268e-02,  ...,  3.7689e-03,
         -4.6997e-03,  5.7068e-03],
        [-1.7929e-03,  6.0730e-03,  4.7302e-03,  ...,  2.6093e-03,
          1.1169e-02, -1.0986e-03],
        [-1.7090e-03, -7.6294e-06, -1.9836e-03,  ...,  2.4109e-03,
          2.9297e-03,  1.4832e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[17.7188,  8.0234,  2.1895,  ..., -8.2344, -5.3828, -1.3613]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 07:47:15 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of rare is rarely
The adjective form of sexual is sexually
The adjective form of increasing is increasingly
The adjective form of according is accordingly
The adjective form of international is internationally
The adjective form of actual is actually
The adjective form of obvious is obviously
The adjective form of regional is
2024-07-25 07:47:15 root INFO     [order_1_approx] starting weight calculation for The adjective form of increasing is increasingly
The adjective form of actual is actually
The adjective form of according is accordingly
The adjective form of sexual is sexually
The adjective form of obvious is obviously
The adjective form of regional is regionally
The adjective form of rare is rarely
The adjective form of international is
2024-07-25 07:47:15 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 07:51:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3320, -1.3125,  7.2812,  ...,  0.4277,  0.0156,  1.5547],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 1.6797,  5.1875, 16.6250,  ...,  1.7500,  6.2188,  5.5312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.6855e-03,  1.0452e-03, -1.8921e-03,  ...,  3.0136e-04,
          5.9204e-03, -8.7891e-03],
        [-3.5477e-04,  9.2773e-03,  4.7302e-03,  ..., -5.1575e-03,
         -2.0447e-03,  2.9907e-03],
        [ 8.9264e-04,  3.3264e-03,  4.5654e-02,  ...,  6.7520e-04,
         -2.9602e-03,  2.0027e-04],
        ...,
        [-8.4877e-05, -9.0027e-04, -4.2114e-03,  ..., -6.9809e-04,
         -1.8158e-03,  2.7924e-03],
        [-2.4223e-04,  5.0964e-03,  3.1128e-03,  ..., -3.2425e-04,
          7.2937e-03,  8.5068e-04],
        [-7.2861e-04,  3.1586e-03, -1.6327e-03,  ..., -4.6921e-04,
         -1.8768e-03,  8.6670e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 1.2227,  4.6562, 17.4219,  ...,  1.7627,  6.9336,  5.4375]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 07:51:08 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you begin something, you are a beginner
If you develop something, you are a developer
If you recommend something, you are a recommender
If you choreograph something, you are a choreographer
If you defend something, you are a defender
If you send something, you are a sender
If you organize something, you are a organizer
If you write something, you are a
2024-07-25 07:51:09 root INFO     [order_1_approx] starting weight calculation for If you organize something, you are a organizer
If you defend something, you are a defender
If you write something, you are a writer
If you send something, you are a sender
If you recommend something, you are a recommender
If you choreograph something, you are a choreographer
If you begin something, you are a beginner
If you develop something, you are a
2024-07-25 07:51:09 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 07:53:13 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9609, -1.1094,  1.6484,  ...,  1.5234, -0.1553,  0.1406],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([4.6250, 1.9531, 2.4219,  ..., 2.2656, 6.1875, 9.7500], device='cuda:0',
       dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.4414e-03, -1.1597e-03, -1.1978e-03,  ...,  2.2125e-03,
          3.7689e-03, -6.1035e-03],
        [ 1.6785e-04, -3.2959e-03, -1.1826e-03,  ...,  3.1662e-04,
          4.9438e-03, -1.6937e-03],
        [-4.7302e-03,  1.8921e-03,  6.2500e-02,  ..., -2.9907e-03,
          4.8523e-03,  1.4343e-03],
        ...,
        [ 2.9602e-03,  2.1973e-03,  5.5847e-03,  ...,  3.8624e-05,
          4.9744e-03,  4.4250e-03],
        [ 2.2278e-03,  6.3477e-03,  3.9673e-03,  ...,  2.0142e-03,
          1.3977e-02, -1.9531e-03],
        [-1.2054e-03, -1.2512e-03,  3.9368e-03,  ..., -3.2349e-03,
          9.2163e-03,  8.6060e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.5781,  3.0879,  2.6055,  ...,  2.8887,  7.3281, 11.5078]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 07:53:13 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of increasing is increasingly
The adjective form of actual is actually
The adjective form of according is accordingly
The adjective form of sexual is sexually
The adjective form of obvious is obviously
The adjective form of regional is regionally
The adjective form of rare is rarely
The adjective form of international is
2024-07-25 07:53:14 root INFO     [order_1_approx] starting weight calculation for The adjective form of rare is rarely
The adjective form of obvious is obviously
The adjective form of sexual is sexually
The adjective form of increasing is increasingly
The adjective form of actual is actually
The adjective form of regional is regionally
The adjective form of international is internationally
The adjective form of according is
2024-07-25 07:53:14 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 07:56:48 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6797, -2.5000,  3.8281,  ...,  2.0000, -1.2031,  1.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  8.3750,  -2.8125,   9.1250,  ..., -11.6250,   1.9531,  12.3125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.6327e-03,  6.9427e-04,  5.3711e-03,  ...,  2.3651e-03,
          1.1658e-02, -1.3306e-02],
        [ 5.6763e-03,  2.7466e-03, -5.4550e-04,  ..., -3.4485e-03,
         -1.3962e-03,  6.2561e-03],
        [ 7.9346e-03,  9.8228e-05,  4.9805e-02,  ..., -8.9722e-03,
         -1.2207e-04,  1.0742e-02],
        ...,
        [ 1.1749e-03,  2.0790e-04,  4.4060e-04,  ..., -2.3804e-03,
         -7.9956e-03,  3.3417e-03],
        [ 6.7749e-03,  1.5640e-04, -2.3041e-03,  ..., -2.5024e-03,
          1.5320e-02,  1.3962e-03],
        [ 6.3171e-03, -2.4109e-03, -1.2939e-02,  ..., -7.2327e-03,
         -8.2397e-04,  1.5747e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  9.5391,  -3.3125,  13.1875,  ..., -12.3984,   3.5117,  14.8125]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 07:56:48 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you organize something, you are a organizer
If you defend something, you are a defender
If you write something, you are a writer
If you send something, you are a sender
If you recommend something, you are a recommender
If you choreograph something, you are a choreographer
If you begin something, you are a beginner
If you develop something, you are a
2024-07-25 07:56:49 root INFO     [order_1_approx] starting weight calculation for If you begin something, you are a beginner
If you defend something, you are a defender
If you develop something, you are a developer
If you choreograph something, you are a choreographer
If you organize something, you are a organizer
If you send something, you are a sender
If you write something, you are a writer
If you recommend something, you are a
2024-07-25 07:56:49 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 07:59:08 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.8125, -0.7695,  2.4219,  ...,  2.6250, -1.1641, -1.4297],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-5.1875, -2.6562,  5.6875,  ..., -3.4688, -2.7031, -3.1562],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.4404e-02, -5.9509e-03,  3.0640e-02,  ..., -8.0566e-03,
          1.6174e-03, -1.1826e-03],
        [-2.2125e-03, -8.9111e-03,  5.4932e-03,  ..., -6.4392e-03,
          8.7891e-03, -3.2043e-03],
        [-8.6670e-03, -7.2632e-03,  7.4219e-02,  ..., -1.0376e-02,
          8.0566e-03, -1.6602e-02],
        ...,
        [ 7.6294e-06, -1.8616e-03,  2.6611e-02,  ..., -1.2634e-02,
          1.2634e-02, -3.0823e-03],
        [ 2.5482e-03, -1.0559e-02,  3.3936e-02,  ..., -7.8125e-03,
          4.3213e-02, -1.5259e-03],
        [-3.6774e-03, -5.9843e-05,  3.3264e-03,  ...,  1.9836e-03,
         -1.4954e-03,  1.0864e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-6.0469, -2.3086,  8.9531,  ...,  0.0684, -0.8740, -4.5664]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 07:59:09 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of rare is rarely
The adjective form of obvious is obviously
The adjective form of sexual is sexually
The adjective form of increasing is increasingly
The adjective form of actual is actually
The adjective form of regional is regionally
The adjective form of international is internationally
The adjective form of according is
2024-07-25 07:59:09 root INFO     [order_1_approx] starting weight calculation for The adjective form of rare is rarely
The adjective form of obvious is obviously
The adjective form of actual is actually
The adjective form of according is accordingly
The adjective form of increasing is increasingly
The adjective form of international is internationally
The adjective form of regional is regionally
The adjective form of sexual is
2024-07-25 07:59:09 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 08:02:28 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 3.0625,  0.1260,  1.8594,  ..., -0.3379,  0.0117,  2.1094],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 6.3125, 14.0000,  3.6406,  ...,  2.8438,  7.3125, 11.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.1667e-03, -2.4414e-03, -4.6387e-03,  ..., -7.0190e-04,
          4.0283e-03, -4.7302e-03],
        [ 4.6082e-03,  2.9907e-03,  1.0681e-02,  ...,  6.3705e-04,
         -4.1962e-04,  3.2349e-03],
        [ 3.3951e-04, -8.7738e-04,  8.5938e-02,  ...,  2.1820e-03,
          2.3193e-03,  1.7395e-03],
        ...,
        [ 5.8899e-03, -2.5482e-03, -6.1340e-03,  ..., -6.0272e-04,
         -4.4250e-03,  3.0823e-03],
        [ 4.2419e-03,  8.7738e-05,  2.9449e-03,  ..., -8.9264e-04,
          1.6602e-02, -2.5558e-04],
        [-6.4697e-03,  2.7924e-03, -8.0566e-03,  ..., -4.5471e-03,
         -1.7014e-03,  9.5215e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 7.4219, 12.6172,  3.6992,  ...,  1.6699,  6.8438, 10.6094]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 08:02:29 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you begin something, you are a beginner
If you defend something, you are a defender
If you develop something, you are a developer
If you choreograph something, you are a choreographer
If you organize something, you are a organizer
If you send something, you are a sender
If you write something, you are a writer
If you recommend something, you are a
2024-07-25 08:02:29 root INFO     [order_1_approx] starting weight calculation for If you defend something, you are a defender
If you write something, you are a writer
If you send something, you are a sender
If you choreograph something, you are a choreographer
If you develop something, you are a developer
If you begin something, you are a beginner
If you recommend something, you are a recommender
If you organize something, you are a
2024-07-25 08:02:29 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 08:05:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.0762,  1.4062, -0.6523,  ...,  2.7812,  0.9375, -0.8164],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 9.2500,  5.5000,  3.5781,  ...,  7.4375,  5.6250, -0.0312],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0052, -0.0023,  0.0107,  ..., -0.0033,  0.0052, -0.0047],
        [-0.0014,  0.0068, -0.0089,  ...,  0.0023, -0.0010, -0.0042],
        [-0.0125,  0.0107,  0.0732,  ...,  0.0008,  0.0106, -0.0097],
        ...,
        [ 0.0054,  0.0050, -0.0009,  ...,  0.0011, -0.0084, -0.0018],
        [ 0.0026,  0.0051, -0.0033,  ..., -0.0034,  0.0151, -0.0059],
        [-0.0006, -0.0057,  0.0011,  ..., -0.0020,  0.0025,  0.0106]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[9.6250, 6.7422, 6.8047,  ..., 7.8672, 3.7715, 0.9238]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 08:05:02 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of rare is rarely
The adjective form of obvious is obviously
The adjective form of actual is actually
The adjective form of according is accordingly
The adjective form of increasing is increasingly
The adjective form of international is internationally
The adjective form of regional is regionally
The adjective form of sexual is
2024-07-25 08:05:02 root INFO     [order_1_approx] starting weight calculation for The adjective form of international is internationally
The adjective form of according is accordingly
The adjective form of regional is regionally
The adjective form of actual is actually
The adjective form of increasing is increasingly
The adjective form of obvious is obviously
The adjective form of sexual is sexually
The adjective form of rare is
2024-07-25 08:05:02 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 08:08:09 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 3.1094, -3.9375,  5.1562,  ...,  1.3828, -4.7812,  4.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 2.7188, -1.4688,  6.8750,  ..., -4.2812, -1.8438, 16.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0056,  0.0008,  0.0056,  ..., -0.0011,  0.0078, -0.0050],
        [ 0.0004,  0.0010,  0.0039,  ..., -0.0021,  0.0031,  0.0051],
        [-0.0014, -0.0011,  0.0586,  ..., -0.0032, -0.0010, -0.0005],
        ...,
        [-0.0010, -0.0019, -0.0081,  ..., -0.0014, -0.0038,  0.0040],
        [ 0.0042,  0.0022,  0.0044,  ...,  0.0009,  0.0121, -0.0008],
        [ 0.0004,  0.0029, -0.0142,  ..., -0.0053, -0.0031,  0.0137]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 4.2031, -1.3047,  7.8438,  ..., -4.3242, -2.1094, 16.6250]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 08:08:10 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you defend something, you are a defender
If you write something, you are a writer
If you send something, you are a sender
If you choreograph something, you are a choreographer
If you develop something, you are a developer
If you begin something, you are a beginner
If you recommend something, you are a recommender
If you organize something, you are a
2024-07-25 08:08:11 root INFO     [order_1_approx] starting weight calculation for If you organize something, you are a organizer
If you choreograph something, you are a choreographer
If you develop something, you are a developer
If you recommend something, you are a recommender
If you write something, you are a writer
If you send something, you are a sender
If you begin something, you are a beginner
If you defend something, you are a
2024-07-25 08:08:11 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 08:10:58 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([2.2344, 0.3027, 2.8125,  ..., 2.9062, 3.0625, 1.5781], device='cuda:0',
       dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 8.1250,  5.0000, -2.5781,  ..., -5.2500,  1.7812, -2.7031],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-3.6621e-03,  2.2736e-03,  3.0212e-03,  ...,  5.0049e-03,
          1.0986e-02,  1.0071e-03],
        [ 7.3242e-03,  7.7209e-03,  1.4648e-03,  ..., -3.6011e-03,
         -5.0354e-04,  1.1475e-02],
        [-1.3733e-04,  3.5095e-03,  5.9814e-02,  ..., -9.2316e-04,
         -6.3782e-03, -6.2866e-03],
        ...,
        [ 6.6833e-03,  2.0752e-03, -5.4626e-03,  ..., -3.8147e-06,
         -8.4229e-03,  6.8359e-03],
        [-1.3351e-03,  3.2654e-03,  7.2098e-04,  ..., -5.9128e-04,
          2.4719e-03,  4.4250e-03],
        [-4.8523e-03, -6.8970e-03,  7.3242e-03,  ...,  9.2316e-04,
          3.6316e-03,  5.4016e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.9922,  5.5039, -4.0469,  ..., -6.6445, -0.2080, -3.6914]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 08:10:59 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of international is internationally
The adjective form of according is accordingly
The adjective form of regional is regionally
The adjective form of actual is actually
The adjective form of increasing is increasingly
The adjective form of obvious is obviously
The adjective form of sexual is sexually
The adjective form of rare is
2024-07-25 08:11:00 root INFO     [order_1_approx] starting weight calculation for The adjective form of international is internationally
The adjective form of sexual is sexually
The adjective form of increasing is increasingly
The adjective form of regional is regionally
The adjective form of rare is rarely
The adjective form of according is accordingly
The adjective form of obvious is obviously
The adjective form of actual is
2024-07-25 08:11:00 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 08:13:56 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.1562, -2.0312,  2.1719,  ...,  0.5391,  0.0430,  2.7188],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-5.8125,  3.7500, 15.5000,  ..., -8.3750,  6.0000,  6.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.3733e-04,  6.7139e-04,  5.8746e-04,  ..., -1.2451e-02,
          1.6846e-02, -1.3504e-03],
        [ 1.0376e-02,  2.3804e-03,  8.4229e-03,  ...,  4.1199e-03,
         -2.2583e-03,  8.7357e-04],
        [ 9.1171e-04, -5.8594e-03,  7.5195e-02,  ...,  8.5449e-03,
          3.0975e-03, -3.2501e-03],
        ...,
        [ 1.9073e-03, -2.0142e-03,  5.7678e-03,  ...,  6.5918e-03,
         -2.6398e-03,  6.1646e-03],
        [-1.1063e-03,  2.9602e-03,  1.1902e-02,  ...,  4.9133e-03,
          1.0498e-02, -3.8147e-03],
        [ 5.0354e-03, -4.9591e-05,  4.2343e-04,  ...,  1.0529e-03,
         -4.6997e-03,  1.0315e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-3.8125,  3.5020, 18.3125,  ..., -8.2578,  7.1602,  5.5742]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 08:13:57 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you organize something, you are a organizer
If you choreograph something, you are a choreographer
If you develop something, you are a developer
If you recommend something, you are a recommender
If you write something, you are a writer
If you send something, you are a sender
If you begin something, you are a beginner
If you defend something, you are a
2024-07-25 08:13:57 root INFO     [order_1_approx] starting weight calculation for If you send something, you are a sender
If you organize something, you are a organizer
If you write something, you are a writer
If you defend something, you are a defender
If you develop something, you are a developer
If you begin something, you are a beginner
If you recommend something, you are a recommender
If you choreograph something, you are a
2024-07-25 08:13:57 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 08:17:01 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.1875, -1.1641,  0.8164,  ...,  3.3750,  0.8281,  0.2969],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 8.5000,  8.2500,  4.0938,  ..., -1.8125, -2.4062,  3.5781],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0044,  0.0033,  0.0122,  ..., -0.0014,  0.0043, -0.0141],
        [ 0.0117,  0.0076,  0.0052,  ...,  0.0042, -0.0004, -0.0016],
        [ 0.0126,  0.0109,  0.0908,  ...,  0.0017,  0.0016,  0.0063],
        ...,
        [ 0.0156,  0.0029,  0.0095,  ...,  0.0020,  0.0003,  0.0033],
        [ 0.0127,  0.0037,  0.0034,  ...,  0.0011,  0.0123,  0.0055],
        [ 0.0021, -0.0017,  0.0068,  ...,  0.0005,  0.0132,  0.0146]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 7.0781,  8.3672,  2.6367,  ..., -0.6357, -3.0391,  1.8467]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 08:17:02 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for The adjective form of international is internationally
The adjective form of sexual is sexually
The adjective form of increasing is increasingly
The adjective form of regional is regionally
The adjective form of rare is rarely
The adjective form of according is accordingly
The adjective form of obvious is obviously
The adjective form of actual is
2024-07-25 08:17:02 root INFO     total operator prediction time: 2869.9731998443604 seconds
2024-07-25 08:17:02 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on over+adj_reg
2024-07-25 08:17:02 root INFO     building operator over+adj_reg
2024-07-25 08:17:02 root INFO     [order_1_approx] starting weight calculation for If something is too ambitious, it is overambitious
If something is too subscribed, it is oversubscribed
If something is too grown, it is overgrown
If something is too done, it is overdone
If something is too represented, it is overrepresented
If something is too used, it is overused
If something is too spent, it is overspent
If something is too sized, it is
2024-07-25 08:17:02 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 08:19:34 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.1562, -3.7969,  3.3125,  ..., -1.6094, -5.3125,  1.9375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 2.9844,  1.3047,  4.8438,  ..., -8.9375, -7.0625, 22.7500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.2817e-02, -2.3499e-03,  1.3428e-02,  ..., -2.5024e-03,
          1.2390e-02, -5.9128e-04],
        [-3.6926e-03,  1.6403e-03,  1.7822e-02,  ...,  4.8828e-03,
          3.1891e-03,  1.5869e-03],
        [-6.5918e-03,  5.9814e-03,  8.3984e-02,  ...,  3.4180e-03,
          1.1597e-02,  1.3855e-02],
        ...,
        [-1.9073e-06, -2.7771e-03, -3.0518e-03,  ..., -2.5940e-04,
          1.3733e-03, -7.2632e-03],
        [-2.9907e-03, -1.6937e-03, -1.0376e-03,  ..., -3.2806e-03,
          2.0508e-02, -3.2806e-04],
        [ 3.7537e-03,  4.3335e-03,  9.3384e-03,  ...,  5.9204e-03,
         -1.7242e-03,  1.9287e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.6426,  5.0625,  7.6328,  ..., -9.2734, -6.9844, 24.5625]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 08:19:35 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you send something, you are a sender
If you organize something, you are a organizer
If you write something, you are a writer
If you defend something, you are a defender
If you develop something, you are a developer
If you begin something, you are a beginner
If you recommend something, you are a recommender
If you choreograph something, you are a
2024-07-25 08:19:35 root INFO     [order_1_approx] starting weight calculation for If you choreograph something, you are a choreographer
If you write something, you are a writer
If you begin something, you are a beginner
If you develop something, you are a developer
If you recommend something, you are a recommender
If you organize something, you are a organizer
If you defend something, you are a defender
If you send something, you are a
2024-07-25 08:19:35 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 08:22:57 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.5820, -1.3906,  1.2734,  ...,  1.3281, -1.3594, -1.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  7.1875,   0.2734, -15.2500,  ...,   7.1562,   6.5625, -16.3750],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0209,  0.0131,  0.0272,  ..., -0.0022,  0.0054, -0.0121],
        [ 0.0044,  0.0049, -0.0028,  ...,  0.0123, -0.0014,  0.0015],
        [ 0.0005,  0.0165,  0.1226,  ..., -0.0057, -0.0112, -0.0282],
        ...,
        [-0.0018,  0.0103,  0.0104,  ..., -0.0024, -0.0220, -0.0199],
        [-0.0054,  0.0045, -0.0016,  ..., -0.0018,  0.0117,  0.0010],
        [ 0.0047,  0.0005, -0.0261,  ...,  0.0063,  0.0093,  0.0137]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  7.8086,  -0.2778, -17.0156,  ...,   7.7930,   4.0547, -15.2344]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 08:22:58 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is too ambitious, it is overambitious
If something is too subscribed, it is oversubscribed
If something is too grown, it is overgrown
If something is too done, it is overdone
If something is too represented, it is overrepresented
If something is too used, it is overused
If something is too spent, it is overspent
If something is too sized, it is
2024-07-25 08:22:58 root INFO     [order_1_approx] starting weight calculation for If something is too used, it is overused
If something is too ambitious, it is overambitious
If something is too spent, it is overspent
If something is too sized, it is oversized
If something is too represented, it is overrepresented
If something is too done, it is overdone
If something is too subscribed, it is oversubscribed
If something is too grown, it is
2024-07-25 08:22:58 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 08:25:14 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.2500, -0.4414,  2.0469,  ...,  0.3984, -0.3047,  2.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 7.0312, -6.2500, 17.0000,  ...,  1.4609,  6.6875, -1.1250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0040, -0.0020, -0.0046,  ...,  0.0005,  0.0056, -0.0048],
        [ 0.0013,  0.0041,  0.0078,  ..., -0.0075, -0.0008,  0.0042],
        [ 0.0038,  0.0007,  0.0459,  ...,  0.0003,  0.0010,  0.0002],
        ...,
        [ 0.0042, -0.0028,  0.0019,  ...,  0.0043, -0.0028, -0.0003],
        [ 0.0012,  0.0017, -0.0001,  ..., -0.0003,  0.0107, -0.0017],
        [ 0.0008,  0.0009, -0.0007,  ..., -0.0056,  0.0006,  0.0083]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 7.3164, -7.2695, 19.0312,  ...,  1.5508,  7.3672, -2.8457]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 08:25:15 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you choreograph something, you are a choreographer
If you write something, you are a writer
If you begin something, you are a beginner
If you develop something, you are a developer
If you recommend something, you are a recommender
If you organize something, you are a organizer
If you defend something, you are a defender
If you send something, you are a
2024-07-25 08:25:15 root INFO     [order_1_approx] starting weight calculation for If you recommend something, you are a recommender
If you choreograph something, you are a choreographer
If you defend something, you are a defender
If you send something, you are a sender
If you develop something, you are a developer
If you write something, you are a writer
If you organize something, you are a organizer
If you begin something, you are a
2024-07-25 08:25:15 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 08:28:47 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-1.2188, -3.2812,  3.2188,  ...,  2.1406, -1.3359, -1.2344],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([12.6875, -3.6562, -7.0625,  ..., -6.1562,  3.8125, -3.1719],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.2970e-03,  1.8311e-04,  3.5858e-03,  ..., -2.5177e-03,
         -2.2888e-03, -1.3733e-04],
        [ 3.6926e-03,  1.0681e-02,  1.8921e-02,  ...,  3.4637e-03,
          4.1809e-03, -1.1963e-02],
        [-1.6212e-04,  5.0354e-03,  1.0596e-01,  ...,  9.0332e-03,
          4.5776e-03, -2.9785e-02],
        ...,
        [ 3.7537e-03, -2.3804e-03, -1.3977e-02,  ...,  1.5030e-03,
          2.7466e-03,  8.3008e-03],
        [-7.5989e-03, -1.3275e-03,  1.2390e-02,  ...,  8.5831e-06,
          1.9409e-02, -5.2185e-03],
        [ 3.9673e-03,  2.4567e-03, -1.0864e-02,  ..., -2.6245e-03,
          5.0354e-03,  7.1106e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[10.7812, -1.4414, -4.9844,  ..., -4.5703,  4.5508, -1.5371]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 08:28:48 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is too used, it is overused
If something is too ambitious, it is overambitious
If something is too spent, it is overspent
If something is too sized, it is oversized
If something is too represented, it is overrepresented
If something is too done, it is overdone
If something is too subscribed, it is oversubscribed
If something is too grown, it is
2024-07-25 08:28:48 root INFO     [order_1_approx] starting weight calculation for If something is too done, it is overdone
If something is too ambitious, it is overambitious
If something is too spent, it is overspent
If something is too used, it is overused
If something is too sized, it is oversized
If something is too grown, it is overgrown
If something is too represented, it is overrepresented
If something is too subscribed, it is
2024-07-25 08:28:48 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 08:30:33 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 2.7812, -1.0078,  3.9688,  ...,  0.1523, -0.0781,  1.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 9.5625,  7.0000, 20.8750,  ..., -8.7500, -3.1250, 10.5000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0023,  0.0028, -0.0018,  ...,  0.0036,  0.0162, -0.0093],
        [ 0.0031,  0.0056,  0.0046,  ..., -0.0044,  0.0002, -0.0021],
        [ 0.0075,  0.0020,  0.0688,  ...,  0.0004, -0.0010,  0.0029],
        ...,
        [ 0.0046, -0.0016, -0.0052,  ...,  0.0008, -0.0060,  0.0045],
        [ 0.0033,  0.0022,  0.0037,  ..., -0.0027,  0.0123, -0.0034],
        [-0.0032,  0.0023, -0.0118,  ...,  0.0016,  0.0003,  0.0070]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 9.8047,  6.8359, 22.4375,  ..., -7.9883, -3.2168, 10.1172]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 08:30:33 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you recommend something, you are a recommender
If you choreograph something, you are a choreographer
If you defend something, you are a defender
If you send something, you are a sender
If you develop something, you are a developer
If you write something, you are a writer
If you organize something, you are a organizer
If you begin something, you are a
2024-07-25 08:30:33 root INFO     total operator prediction time: 2705.2859120368958 seconds
2024-07-25 08:30:33 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on over+adj_reg
2024-07-25 08:30:33 root INFO     building operator over+adj_reg
2024-07-25 08:30:34 root INFO     [order_1_approx] starting weight calculation for If something is too heated, it is overheated
If something is too booked, it is overbooked
If something is too developed, it is overdeveloped
If something is too paid, it is overpaid
If something is too painted, it is overpainted
If something is too charged, it is overcharged
If something is too protective, it is overprotective
If something is too stated, it is
2024-07-25 08:30:34 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 08:34:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.6016,  1.6328,  5.0938,  ...,  0.3438, -1.6094,  1.0859],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 7.6562,  0.3125, -7.6562,  ...,  1.4219,  6.7500, -9.9375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[-1.1063e-04,  4.5776e-05, -1.1673e-03,  ...,  3.5095e-04,
          6.2866e-03,  6.2561e-04],
        [ 1.5564e-02,  1.5320e-02,  1.7578e-02,  ...,  1.1230e-02,
          9.7656e-03,  6.7749e-03],
        [ 1.0071e-02,  9.2163e-03,  9.3750e-02,  ...,  2.9373e-04,
          1.3123e-02, -1.0559e-02],
        ...,
        [ 3.1891e-03, -2.6550e-03, -2.6978e-02,  ..., -1.3809e-03,
         -8.4229e-03, -2.9602e-03],
        [ 3.2043e-03,  1.0620e-02,  1.1536e-02,  ...,  3.0212e-03,
          2.2217e-02,  6.0425e-03],
        [ 8.9111e-03, -7.5684e-03,  8.9722e-03,  ..., -3.6011e-03,
          4.2114e-03,  2.2217e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 7.0977,  1.2217, -5.7852,  ...,  1.0244,  6.8398, -6.3828]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 08:34:41 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is too done, it is overdone
If something is too ambitious, it is overambitious
If something is too spent, it is overspent
If something is too used, it is overused
If something is too sized, it is oversized
If something is too grown, it is overgrown
If something is too represented, it is overrepresented
If something is too subscribed, it is
2024-07-25 08:34:42 root INFO     [order_1_approx] starting weight calculation for If something is too spent, it is overspent
If something is too grown, it is overgrown
If something is too used, it is overused
If something is too sized, it is oversized
If something is too subscribed, it is oversubscribed
If something is too represented, it is overrepresented
If something is too ambitious, it is overambitious
If something is too done, it is
2024-07-25 08:34:42 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 08:36:06 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.3672,  1.1562,  6.6875,  ...,  0.5234, -2.1719, -0.6484],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  1.1719,  -2.7500,   4.8125,  ..., -10.4375,  18.2500,   9.0000],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 1.2573e-02,  4.0588e-03, -1.5747e-02,  ..., -7.6294e-03,
         -8.0566e-03,  1.4832e-02],
        [-6.6528e-03, -3.3722e-03,  1.9836e-03,  ..., -3.5858e-03,
          2.2430e-03, -5.4932e-03],
        [-6.2943e-05,  1.4496e-04,  9.1797e-02,  ...,  6.8665e-03,
         -8.6060e-03, -1.2024e-02],
        ...,
        [ 4.7607e-03,  1.5259e-05, -1.2939e-02,  ...,  2.7466e-03,
         -1.9455e-04,  4.3297e-04],
        [ 1.3275e-03,  9.4604e-03,  3.0670e-03,  ...,  4.8218e-03,
          4.8218e-03, -2.6703e-03],
        [-1.7853e-03, -1.3123e-02, -1.6235e-02,  ..., -3.2501e-03,
         -1.3046e-03,  8.7280e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.0352, -1.7178,  1.2891,  ..., -9.2109, 15.2422, 10.3438]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 08:36:07 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is too heated, it is overheated
If something is too booked, it is overbooked
If something is too developed, it is overdeveloped
If something is too paid, it is overpaid
If something is too painted, it is overpainted
If something is too charged, it is overcharged
If something is too protective, it is overprotective
If something is too stated, it is
2024-07-25 08:36:07 root INFO     [order_1_approx] starting weight calculation for If something is too charged, it is overcharged
If something is too stated, it is overstated
If something is too developed, it is overdeveloped
If something is too heated, it is overheated
If something is too paid, it is overpaid
If something is too painted, it is overpainted
If something is too protective, it is overprotective
If something is too booked, it is
2024-07-25 08:36:07 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 08:40:29 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([-0.9531,  1.2344,  4.2812,  ...,  0.7578, -2.3594,  0.6719],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 2.8750, -0.5000,  6.2812,  ..., -8.6250, 13.0625,  2.8125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0047,  0.0009,  0.0121,  ..., -0.0067,  0.0088, -0.0102],
        [-0.0028, -0.0032,  0.0070,  ...,  0.0074,  0.0039, -0.0096],
        [ 0.0033,  0.0084,  0.1074,  ...,  0.0019, -0.0010, -0.0107],
        ...,
        [ 0.0033,  0.0036,  0.0005,  ..., -0.0044, -0.0016,  0.0119],
        [ 0.0006,  0.0044,  0.0044,  ...,  0.0124,  0.0045, -0.0051],
        [-0.0010, -0.0171, -0.0194,  ...,  0.0031,  0.0054,  0.0087]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 2.7695,  0.9795,  5.8984,  ..., -6.0703, 10.2734,  6.7969]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 08:40:30 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is too spent, it is overspent
If something is too grown, it is overgrown
If something is too used, it is overused
If something is too sized, it is oversized
If something is too subscribed, it is oversubscribed
If something is too represented, it is overrepresented
If something is too ambitious, it is overambitious
If something is too done, it is
2024-07-25 08:40:30 root INFO     [order_1_approx] starting weight calculation for If something is too done, it is overdone
If something is too used, it is overused
If something is too ambitious, it is overambitious
If something is too represented, it is overrepresented
If something is too subscribed, it is oversubscribed
If something is too sized, it is oversized
If something is too grown, it is overgrown
If something is too spent, it is
2024-07-25 08:40:30 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 08:41:41 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.8086, -0.6328,  3.9062,  ...,  0.7734, -2.6250, -1.2578],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-0.8047, -7.6562, -1.8125,  ...,  4.6875, 10.6250, -2.3438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0091, -0.0003, -0.0028,  ..., -0.0009,  0.0050,  0.0014],
        [-0.0007,  0.0070, -0.0018,  ...,  0.0060,  0.0070,  0.0006],
        [ 0.0164,  0.0066,  0.0703,  ...,  0.0022,  0.0109, -0.0113],
        ...,
        [ 0.0023, -0.0024, -0.0167,  ...,  0.0063, -0.0062,  0.0069],
        [-0.0009,  0.0057,  0.0078,  ...,  0.0021,  0.0154, -0.0132],
        [-0.0042,  0.0010, -0.0044,  ..., -0.0019,  0.0143,  0.0126]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-1.1855, -7.5273,  0.4785,  ...,  5.0898, 12.0469, -1.8193]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 08:41:41 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is too charged, it is overcharged
If something is too stated, it is overstated
If something is too developed, it is overdeveloped
If something is too heated, it is overheated
If something is too paid, it is overpaid
If something is too painted, it is overpainted
If something is too protective, it is overprotective
If something is too booked, it is
2024-07-25 08:41:42 root INFO     [order_1_approx] starting weight calculation for If something is too developed, it is overdeveloped
If something is too heated, it is overheated
If something is too paid, it is overpaid
If something is too stated, it is overstated
If something is too painted, it is overpainted
If something is too booked, it is overbooked
If something is too protective, it is overprotective
If something is too charged, it is
2024-07-25 08:41:42 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 08:46:24 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3594,  1.5156,  5.7500,  ...,  1.9141, -1.7266, -0.3203],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 0.5156,  1.0625, -6.7812,  ...,  3.9688, 13.9375,  5.3125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.3499e-03, -1.0620e-02,  9.2983e-05,  ...,  9.7046e-03,
          2.3682e-02, -1.2512e-02],
        [ 1.4648e-03,  3.8452e-03,  2.2888e-04,  ...,  6.5613e-04,
         -2.6093e-03, -6.6833e-03],
        [-6.4697e-03,  1.6479e-03,  1.2793e-01,  ...,  1.5015e-02,
          4.1809e-03, -1.2146e-02],
        ...,
        [ 8.1787e-03,  1.2283e-03,  1.9531e-02,  ...,  4.5166e-03,
          5.6152e-03, -1.5991e-02],
        [-1.0986e-03,  4.1199e-04, -8.0566e-03,  ...,  1.9302e-03,
          2.6855e-02,  2.3560e-02],
        [-3.8147e-03, -7.6904e-03, -2.8198e-02,  ..., -4.2725e-03,
          1.1780e-02,  8.3008e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 0.7266,  2.8008, -6.0781,  ...,  6.6484, 11.2188,  7.7344]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 08:46:25 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is too done, it is overdone
If something is too used, it is overused
If something is too ambitious, it is overambitious
If something is too represented, it is overrepresented
If something is too subscribed, it is oversubscribed
If something is too sized, it is oversized
If something is too grown, it is overgrown
If something is too spent, it is
2024-07-25 08:46:25 root INFO     [order_1_approx] starting weight calculation for If something is too spent, it is overspent
If something is too done, it is overdone
If something is too grown, it is overgrown
If something is too sized, it is oversized
If something is too represented, it is overrepresented
If something is too ambitious, it is overambitious
If something is too subscribed, it is oversubscribed
If something is too used, it is
2024-07-25 08:46:25 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 08:52:37 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.2656, -0.9688,  5.6250,  ...,  1.3438, -0.5352,  0.1846],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([-11.4375,  -3.3438, -10.2500,  ...,   2.7188,   5.0000,  -8.2500],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0074,  0.0052,  0.0003,  ..., -0.0039,  0.0092, -0.0089],
        [ 0.0052,  0.0059,  0.0070,  ..., -0.0015, -0.0078, -0.0066],
        [ 0.0103,  0.0066,  0.0986,  ...,  0.0001, -0.0036, -0.0048],
        ...,
        [ 0.0082, -0.0025, -0.0040,  ..., -0.0018, -0.0011, -0.0005],
        [ 0.0066,  0.0023, -0.0113,  ...,  0.0069,  0.0206,  0.0032],
        [ 0.0019,  0.0047, -0.0245,  ...,  0.0011, -0.0007,  0.0022]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[-10.8359,  -3.1973,  -8.0625,  ...,   4.7734,   4.1719,  -8.1250]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 08:52:38 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is too spent, it is overspent
If something is too done, it is overdone
If something is too grown, it is overgrown
If something is too sized, it is oversized
If something is too represented, it is overrepresented
If something is too ambitious, it is overambitious
If something is too subscribed, it is oversubscribed
If something is too used, it is
2024-07-25 08:52:38 root INFO     [order_1_approx] starting weight calculation for If something is too subscribed, it is oversubscribed
If something is too done, it is overdone
If something is too grown, it is overgrown
If something is too used, it is overused
If something is too ambitious, it is overambitious
If something is too sized, it is oversized
If something is too spent, it is overspent
If something is too represented, it is
2024-07-25 08:52:38 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 08:58:52 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.1143, -0.2617, -0.6484,  ...,  1.2578, -0.9375, -0.8281],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ -1.3281,   9.6875,  -8.6250,  ..., -16.6250,   7.4375,   1.5938],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0089,  0.0017, -0.0105,  ..., -0.0040,  0.0066, -0.0100],
        [ 0.0042,  0.0043,  0.0058,  ..., -0.0027, -0.0008,  0.0084],
        [-0.0056, -0.0099,  0.1162,  ...,  0.0025,  0.0082, -0.0148],
        ...,
        [-0.0021,  0.0070,  0.0107,  ..., -0.0014,  0.0097, -0.0018],
        [-0.0005,  0.0024, -0.0015,  ..., -0.0030,  0.0150,  0.0058],
        [ 0.0010, -0.0060, -0.0083,  ..., -0.0025,  0.0063,  0.0160]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ -1.2715,  10.7188,  -8.6016,  ..., -17.1250,   4.8789,   3.5469]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 08:58:53 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is too subscribed, it is oversubscribed
If something is too done, it is overdone
If something is too grown, it is overgrown
If something is too used, it is overused
If something is too ambitious, it is overambitious
If something is too sized, it is oversized
If something is too spent, it is overspent
If something is too represented, it is
2024-07-25 08:58:53 root INFO     [order_1_approx] starting weight calculation for If something is too spent, it is overspent
If something is too used, it is overused
If something is too represented, it is overrepresented
If something is too grown, it is overgrown
If something is too sized, it is oversized
If something is too done, it is overdone
If something is too subscribed, it is oversubscribed
If something is too ambitious, it is
2024-07-25 08:58:53 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 09:05:07 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.6172,  0.6016,  5.7500,  ..., -0.3457, -0.7852,  3.5625],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([  4.4375,  -5.0000, -11.8125,  ...,  -4.5625,  -8.4375,  -4.9375],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 0.0020, -0.0032, -0.0139,  ..., -0.0020, -0.0099,  0.0153],
        [-0.0058,  0.0068,  0.0078,  ..., -0.0012,  0.0047, -0.0020],
        [-0.0091, -0.0012,  0.0923,  ...,  0.0014, -0.0058, -0.0217],
        ...,
        [-0.0043, -0.0033,  0.0064,  ...,  0.0020, -0.0089, -0.0019],
        [ 0.0081,  0.0002, -0.0103,  ...,  0.0005,  0.0069, -0.0070],
        [ 0.0113, -0.0042, -0.0067,  ...,  0.0048,  0.0073,  0.0047]],
       device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[  4.0312,  -5.3594, -13.5938,  ...,  -4.8047,  -9.3516,  -4.9648]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 09:05:07 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If something is too spent, it is overspent
If something is too used, it is overused
If something is too represented, it is overrepresented
If something is too grown, it is overgrown
If something is too sized, it is oversized
If something is too done, it is overdone
If something is too subscribed, it is oversubscribed
If something is too ambitious, it is
2024-07-25 09:05:07 root INFO     total operator prediction time: 2885.672472000122 seconds
2024-07-25 09:05:07 __main__ INFO     storing weights: <class 'lre.operators_gemma.JacobianIclMeanEstimator'> on verb+er_irreg
2024-07-25 09:05:07 root INFO     building operator verb+er_irreg
2024-07-25 09:05:08 root INFO     [order_1_approx] starting weight calculation for If you discover something, you are a discoverer
If you send something, you are a sender
If you offend something, you are a offender
If you slay something, you are a slayer
If you consume something, you are a consumer
If you announce something, you are a announcer
If you begin something, you are a beginner
If you speak something, you are a
2024-07-25 09:05:08 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 09:11:18 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.9609, -1.8516,  2.6250,  ...,  1.6719, -1.1641,  1.1562],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 0.6211,  5.1875, 11.2500,  ..., 12.8750, -2.7188,  7.3125],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 9.8419e-04, -1.0757e-03,  1.0605e-03,  ...,  4.9210e-04,
          8.4229e-03, -2.5177e-03],
        [ 2.5024e-03,  3.0518e-03,  1.6708e-03,  ..., -3.6163e-03,
         -9.9182e-04, -8.3447e-05],
        [-4.8828e-04,  1.3199e-03,  4.6875e-02,  ...,  3.5248e-03,
          2.0790e-04, -1.6632e-03],
        ...,
        [ 2.4719e-03, -1.2398e-04, -3.2654e-03,  ...,  3.2425e-04,
          1.9989e-03,  1.0452e-03],
        [ 2.4109e-03,  5.9509e-04, -1.0834e-03,  ..., -8.7738e-05,
          1.6937e-03, -1.0147e-03],
        [-2.6398e-03,  1.0757e-03, -4.9438e-03,  ..., -4.1504e-03,
         -5.5542e-03,  6.4697e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 5.3711e-03,  4.5234e+00,  1.3148e+01,  ...,  1.3992e+01,
         -3.0254e+00,  6.1484e+00]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SubBackward0>) 

                    
2024-07-25 09:11:18 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you discover something, you are a discoverer
If you send something, you are a sender
If you offend something, you are a offender
If you slay something, you are a slayer
If you consume something, you are a consumer
If you announce something, you are a announcer
If you begin something, you are a beginner
If you speak something, you are a
2024-07-25 09:11:19 root INFO     [order_1_approx] starting weight calculation for If you announce something, you are a announcer
If you discover something, you are a discoverer
If you slay something, you are a slayer
If you offend something, you are a offender
If you speak something, you are a speaker
If you begin something, you are a beginner
If you send something, you are a sender
If you consume something, you are a
2024-07-25 09:11:19 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 09:17:19 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 0.3828, -1.7656,  8.8750,  ...,  1.1094, -2.6875,  2.3438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 6.7188,  6.3438,  3.1719,  ...,  2.6562, -3.7344,  8.6250],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 3.0060e-03, -1.9989e-03,  6.1035e-05,  ...,  5.3787e-04,
          9.6436e-03,  6.2866e-03],
        [ 2.9449e-03,  1.3123e-03,  1.3062e-02,  ...,  5.3787e-04,
         -2.7084e-04, -2.5787e-03],
        [ 3.3569e-03, -6.9275e-03,  5.8105e-02,  ..., -3.3722e-03,
         -7.5684e-03, -8.9645e-04],
        ...,
        [ 8.0872e-04, -4.6387e-03,  3.4714e-04,  ...,  1.1826e-03,
         -2.3956e-03,  5.4932e-03],
        [ 6.2561e-04,  8.2397e-04, -2.2736e-03,  ...,  2.4796e-04,
          9.2163e-03, -2.7313e-03],
        [-2.2278e-03,  6.7444e-03,  5.3406e-03,  ...,  8.2779e-04,
         -3.8147e-06,  1.0986e-02]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 6.3125,  5.6797,  2.1445,  ...,  2.3965, -4.4727,  7.5508]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 09:17:20 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you announce something, you are a announcer
If you discover something, you are a discoverer
If you slay something, you are a slayer
If you offend something, you are a offender
If you speak something, you are a speaker
If you begin something, you are a beginner
If you send something, you are a sender
If you consume something, you are a
2024-07-25 09:17:20 root INFO     [order_1_approx] starting weight calculation for If you begin something, you are a beginner
If you consume something, you are a consumer
If you discover something, you are a discoverer
If you offend something, you are a offender
If you speak something, you are a speaker
If you slay something, you are a slayer
If you announce something, you are a announcer
If you send something, you are a
2024-07-25 09:17:20 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
2024-07-25 09:23:25 root INFO     [order_1_approx] weight calculation finished 

                        s_j=tensor([ 1.5547, -0.7852,  0.8789,  ...,  0.5781, -1.7422,  0.5391],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        o_j1=tensor([ 8.8125, -3.0469, 13.3750,  ..., -4.2188,  5.5625, -3.8438],
       device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>) 

                        s_o_weight: tensor([[ 2.8687e-03, -2.1667e-03,  1.9531e-03,  ...,  2.2583e-03,
          6.0425e-03, -2.1973e-03],
        [ 6.7139e-04,  4.2114e-03,  1.1353e-02,  ..., -6.4850e-04,
          1.2741e-03,  3.7842e-03],
        [ 1.9531e-03,  1.4725e-03,  5.6152e-02,  ...,  1.0605e-03,
          2.6131e-04,  1.8616e-03],
        ...,
        [ 3.8147e-03,  1.6251e-03,  5.1880e-03,  ...,  3.7956e-04,
         -3.2349e-03, -1.3885e-03],
        [ 1.7853e-03,  1.4496e-04,  2.6321e-04,  ...,  2.7466e-03,
          4.8218e-03,  1.5488e-03],
        [-2.5177e-03,  9.1553e-05,  9.7046e-03,  ..., -3.8757e-03,
          2.2736e-03,  4.0588e-03]], device='cuda:0', dtype=torch.float16) 

                        s_o_bias=tensor([[ 9.2812, -3.5254, 15.7812,  ..., -4.0234,  6.1680, -5.0156]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SubBackward0>) 

                    
2024-07-25 09:23:25 lre.operators_gemma INFO     sem1 [Jacobian] Finished order_1_approx for If you begin something, you are a beginner
If you consume something, you are a consumer
If you discover something, you are a discoverer
If you offend something, you are a offender
If you speak something, you are a speaker
If you slay something, you are a slayer
If you announce something, you are a announcer
If you send something, you are a
2024-07-25 09:23:26 root INFO     [order_1_approx] starting weight calculation for If you offend something, you are a offender
If you send something, you are a sender
If you consume something, you are a consumer
If you begin something, you are a beginner
If you speak something, you are a speaker
If you announce something, you are a announcer
If you slay something, you are a slayer
If you discover something, you are a
2024-07-25 09:23:26 lre.functional_gemma WARNING  [insert_s_j] layer model.layers.41 does not match model.layers.12
