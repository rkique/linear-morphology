{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f51b929-19f6-4973-950d-44cd1af32d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exia/miniconda3/envs/my-lre/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "sys.path.append('..')\n",
    "\n",
    "from lre.data import Relation, RelationSample, Sequence\n",
    "import lre.functional as functional\n",
    "import lre.models as models\n",
    "import lre.metrics as metrics\n",
    "import lre.logging_utils as logging_utils\n",
    "from collections import defaultdict\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import time\n",
    "import logging\n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df50490b-8197-47a2-846d-2c9500648708",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████| 2/2 [00:00<00:00,  5.90it/s]\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\",\\\n",
    "                                             torch_dtype=torch.float16, low_cpu_mem_usage=True)\n",
    "model.to(device)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "mt = models.ModelAndTokenizer(model,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bba312c-273f-4819-822d-b23fd5e83074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelAndTokenizer(model=LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "), tokenizer=LlamaTokenizerFast(name_or_path='meta-llama/Llama-2-7b-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bf3726-5049-4fd6-983d-0948aa558b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir('llama_8_31_approx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "606c674b-14a1-4e53-8be9-8cb2b6580d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.llama.modeling_llama.LlamaForCausalLM"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mt.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a463086-6ba7-49df-bd3a-a9e621d79800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'llra.build' from '/home/exia/my-lre/data/../llra/build.py'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import llra.build as build\n",
    "from importlib import reload\n",
    "reload(functional)\n",
    "reload(models)\n",
    "reload(build)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a568ef62-83ee-4ead-a481-8b9895149d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using 8 samples out of 8 to build LRE\n",
      "dollar ['p', 'd', 'pen'] ['kil', 'Kil', 'cent']\n",
      "gun ['d', 'p', 't'] ['archivi', 'ungsseite', 'itmap']\n",
      "womb ['d', 'p', 't'] ['Mam', 'd', 'ówn']\n",
      "car ['d', 'p', 't'] ['A', 'тки', 'car']\n",
      "harbor ['d', 'p', 't'] ['dock', 'ships', 'naval']\n",
      "1,meronyms - part,0,4,47\n",
      "using 8 samples out of 8 to build LRE\n",
      "telephone ['head', 't', 'handle'] ['textt', 'archivi', 'Tele']\n",
      "day ['p', 'd', 'pen'] ['day', 'Calendar', 'p']\n",
      "car ['t', 'car', 'd'] ['A', 'чай', 'car']\n",
      "typewriter ['writing', 'pen', 'p'] ['archivi', 'textt', 'typing']\n",
      "bird ['p', 'pen', 'c'] ['bird', 'fe', 'estellt']\n",
      "7,meronyms - part,7,6,47\n",
      "using 8 samples out of 8 to build LRE\n",
      "dollar ['p', 'pen', 'cent'] ['kil', 'cent', 'Kil']\n",
      "tonne ['t', 'p', 'dozen'] ['ipage', 'ách', 'dozen']\n",
      "jail ['d', 't', 'j'] ['prison', 'ówn', 'ben']\n",
      "pie ['ungsseite', 'archivi', 'kn'] ['archivi', 'ungsseite', 'itmap']\n",
      "14,meronyms - part,11,5,48\n",
      "using 8 samples out of 8 to build LRE\n",
      "filename ['paper', 'slot', 'wire'] ['archivi', 'textt', 'ungsseite']\n",
      "window ['window', 'wall', 'door'] ['archivi', 'ungsseite', 'itmap']\n",
      "bird ['bird', 'fe', 'A'] ['bird', 'fe', 'estellt']\n",
      "staircase ['ben', 'walk', 'ra'] ['archivi', 'ben', 'walk']\n",
      "21,meronyms - part,9,5,48\n"
     ]
    }
   ],
   "source": [
    "start, end = 8,31\n",
    "WEIGHT_NAME = f's_o_weight_{start}_{end}'\n",
    "BIAS_NAME = f's_o_bias_{start}_{end}'\n",
    "json_path = 'json/enckno/E06 [animal - youth].json'\n",
    "file = open(json_path, 'r')\n",
    "data = json.load(file)\n",
    "file.close()\n",
    "\n",
    "import llra.build as build\n",
    "from baukit.baukit import parameter_names, get_parameter\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "\n",
    "def find_exact_match(filename, search_string):\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            if search_string.lower() in line.strip().lower():\n",
    "                return line.strip()\n",
    "    print(f\"Could not find {search_string}\")\n",
    "    return None\n",
    "\n",
    "N_ICL = 8 \n",
    "N_AVG = 8\n",
    "N_TRIALS = 8\n",
    "VIEW_SAMPLES = 5\n",
    "\n",
    "APPROX_FOLDER = 'llama_8_31_approx'\n",
    "relations = os.listdir(f'{APPROX_FOLDER}')\n",
    "\n",
    "for beta in [1,7,14,21]:\n",
    "    relations = ['meronyms - part']\n",
    "    for relation_name in relations:\n",
    "        #LOAD RELATION\n",
    "        json_path = find_exact_match(\"json_paths.txt\", relation_name)\n",
    "        file = open(json_path, 'r')\n",
    "        data = json.load(file)\n",
    "        file.close()\n",
    "        relation = Relation.from_dict(data)\n",
    "        prompt_template = relation.prompt_templates[0]\n",
    "\n",
    "        #LOAD WEIGHTS\n",
    "        wdir = f\"{APPROX_FOLDER}/{relation_name}\"\n",
    "        ignored_names = ['.ipynb_checkpoints', 'disappoint']\n",
    "        j_samples = [x for x in os.listdir(wdir) if x not in ignored_names]\n",
    "        j_samples = random.sample(j_samples, N_AVG)\n",
    "        print(f'using {N_AVG} samples out of {len(j_samples)} to build LRE')\n",
    "        reg_weight = build.mean_weight_or_bias(wdir,\n",
    "                                               WEIGHT_NAME, j_samples).half().to(device)\n",
    "        reg_bias = build.mean_weight_or_bias(wdir,\n",
    "                                             BIAS_NAME, j_samples).half().to(device)\n",
    "        \n",
    "        #ASSEMBLE PROMPTS AND OBJECT ANSWERS\n",
    "        clozed_prompts = []\n",
    "        clozed_answers = []\n",
    "        #Do not use the training examples for ICL or testing purposes.\n",
    "        test_samples = [x for x in relation.samples if x not in j_samples]\n",
    "        random.shuffle(test_samples)\n",
    "        for x in test_samples:\n",
    "            test_samples_no_x = [t for t in test_samples if t != x]\n",
    "            samples = [x] + random.sample(test_samples_no_x, N_ICL - 1)\n",
    "            cloze_prompt = functional.make_prompt(\n",
    "                template = prompt_template, \n",
    "                target = x,\n",
    "                examples = samples\n",
    "                )\n",
    "            clozed_prompts.append(cloze_prompt)\n",
    "            clozed_answers.append(x.object)\n",
    "        \n",
    "        outputs_lm = functional.predict_next_token(mt=mt, prompt=clozed_prompts)\n",
    "        preds_lm =  [[x.token for x in xs] for xs in outputs_lm]\n",
    "        recall_lm = metrics.recall(preds_lm, clozed_answers)\n",
    "        reg_correct = 0\n",
    "        no_bias_correct = 0\n",
    "        first_token_correct = 0\n",
    "        lm_correct = 0\n",
    "        for i, sample, objs, prompt, preds in \\\n",
    "        zip(range(0,50), test_samples, clozed_answers, clozed_prompts, preds_lm):\n",
    "            if (metrics.any_is_nontrivial_prefix(predictions=preds, targets=objs)):\n",
    "                \n",
    "                # # print(prompt)\n",
    "                #uses the regular LRE\n",
    "                reg_hs = build.get_hidden_state(mt, prompt, sample.subject, start)\n",
    "                reg_hs = reg_hs.to(dtype=torch.float16)\n",
    "                \n",
    "                reg_hsweight = reg_hs.mm(reg_weight.t())\n",
    "                reg_object_hs = reg_hsweight * beta + reg_bias\n",
    "                reg_preds = build.get_object(mt, reg_object_hs)[0]\n",
    "                \n",
    "                no_bias_hs = reg_hsweight\n",
    "                no_bias_preds = build.get_object(mt, no_bias_hs)[0]\n",
    "\n",
    "                # no_weight_hs = beta * reg_hs + reg_bias\n",
    "                # no_weight_preds = build.get_object(mt, no_weight_hs)[0]\n",
    "                # init_subj_id = mt.tokenizer(sample.subject).input_ids[0]\n",
    "                # init_subj_token = mt.tokenizer.convert_ids_to_tokens(init_subj_id)\n",
    "                #print(init_subj_token)\n",
    "                \n",
    "                if(metrics.any_is_nontrivial_prefix(predictions=[reg_preds[0]], targets=objs)):\n",
    "                    reg_correct += 1\n",
    "    \n",
    "                if(metrics.any_is_nontrivial_prefix(predictions=[no_bias_preds[0]], targets=objs)):\n",
    "                    no_bias_correct += 1\n",
    "            \n",
    "                # if(metrics.any_is_nontrivial_prefix(predictions=[no_weight_preds[0]], targets=objs)):\n",
    "                #     first_token_correct += 1\n",
    "                    \n",
    "                lm_correct += 1\n",
    "    \n",
    "                #if we want to compare specific samples from each relation..\n",
    "                if(i < VIEW_SAMPLES):\n",
    "                    print(f'{sample.subject} {reg_preds[0:3]} {no_bias_preds[0:3]}')\n",
    "                    pass\n",
    "        print(f'{beta},{relation.name},{reg_correct},{no_bias_correct},{lm_correct}')\n",
    "        #print(f'beta,{beta},{relation.name},reg_correct,{reg_correct},no_bias_correct,{no_bias_correct},lm_correct,{lm_correct}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ef4b4e8-41fb-4e59-b672-0e40d3f4efa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from baukit.baukit import TraceDict, parameter_names, get_parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e07fc0bd-4020-45cf-ba80-d12426b80f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma2DecoderLayer(\n",
       "  (self_attn): Gemma2SdpaAttention(\n",
       "    (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
       "    (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
       "    (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
       "    (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "    (rotary_emb): Gemma2RotaryEmbedding()\n",
       "  )\n",
       "  (mlp): Gemma2MLP(\n",
       "    (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
       "    (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
       "    (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
       "    (act_fn): PytorchGELUTanh()\n",
       "  )\n",
       "  (input_layernorm): Gemma2RMSNorm()\n",
       "  (post_attention_layernorm): Gemma2RMSNorm()\n",
       "  (pre_feedforward_layernorm): Gemma2RMSNorm()\n",
       "  (post_feedforward_layernorm): Gemma2RMSNorm()\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#there are 42 layers to Gemma, each is composed of (self_attn, mlp, input, pre_attn_ln, input_ln, post_attn_ln, post_ff_ln)\n",
    "model.model.layers[10]\n",
    "##10 --> 42 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
