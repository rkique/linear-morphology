{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5afa1b73-043c-46df-babe-b5b671a64e3f",
   "metadata": {},
   "source": [
    "### import torch\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "from transformers import GPTJForCausalLM, AutoTokenizer\n",
    "import lre.models as models\n",
    "import lre.functional as functional\n",
    "import os\n",
    "\n",
    "device = \"cuda:1\"\n",
    "weights = []\n",
    "biases = []\n",
    "subjects = []\n",
    "model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", revision=\"float16\", torch_dtype=torch.float16, low_cpu_mem_usage=True)\n",
    "model.to('cuda:1')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "mt = models.ModelAndTokenizer(model,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e40c064c-c98e-4ebb-b4fa-0ab09a073297",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Let's try averaging the w/b for each layer, because that seems the most intuitive.\n",
    "#First attempt: s --> s for 5-26, s --> o for 26-27.\n",
    "\n",
    "animals = [\"dog\", \"duck\", \"fish\", \"horse\", \"mink\", \"seal\", \"shark\", \"trout\"]\n",
    "\n",
    "weight_str = 's_s_weight_'\n",
    "bias_str = 's_s_bias_'\n",
    "\n",
    "def sample_weights_biases(subject, kind, i, samples) -> dict:\n",
    "    layer_dict = {\"i\": i}\n",
    "    weights = []\n",
    "    biases = []\n",
    "    wdir = subject\n",
    "    weight_path = f\"{wdir}/{kind}_weight_h_{i}.pt\"\n",
    "    bias_path = f\"{wdir}/{kind}_bias_h_{i}.pt\"\n",
    "    #load s_s_weight and s_s_bias\n",
    "    weight = torch.load(weight_path).to(device)\n",
    "    bias = torch.load(bias_path).to(device)\n",
    "    layer_dict[f'{kind}_weight'] = weight\n",
    "    layer_dict[f'{kind}_bias'] = bias\n",
    "    return layer_dict\n",
    "    \n",
    "def mean_weights_biases(kind, i, samples) -> dict:\n",
    "    layer_dict = {\"i\": i}\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for sample in samples:\n",
    "        wdir = sample\n",
    "        weight_path = f\"{wdir}/{kind}_weight_h_{i}.pt\"\n",
    "        bias_path = f\"{wdir}/{kind}_bias_h_{i}.pt\"\n",
    "        #load s_s_weight and s_s_bias\n",
    "        weight = torch.load(weight_path).to(device)\n",
    "        bias = torch.load(bias_path).to(device)\n",
    "        #print(f'weight is {tp(weight)}')\n",
    "        #append to lists\n",
    "        weights.append(weight)\n",
    "        biases.append(bias)\n",
    "    mean_weight = torch.stack(weights).mean(dim=0).to(device)\n",
    "    mean_bias = torch.stack(biases).mean(dim=0).to(device)\n",
    "    sum_weight = torch.stack(weights).sum(dim=0).to(device)\n",
    "    sum_bias = torch.stack(biases).sum(dim=0).to(device)\n",
    "    \n",
    "    layer_dict[f'{kind}_mean_weight'] = mean_weight\n",
    "    layer_dict[f'{kind}_mean_bias'] = mean_bias\n",
    "    layer_dict[f'{kind}_sum_weight'] = sum_weight\n",
    "    layer_dict[f'{kind}_sum_bias'] = sum_bias\n",
    "    \n",
    "    return layer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70a81114-f8bf-4ed6-8eb1-29bee0104bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from baukit.baukit import parameter_names, get_parameter\n",
    "\n",
    "#returns weight and bias for lns\n",
    "def get_layer_norm_params(model, start, end):\n",
    "    layer_norm_params = {}\n",
    "    for i in range(start, end):\n",
    "        w_name = f'transformer.h.{i}.ln_1.weight'\n",
    "        b_name = f'transformer.h.{i}.ln_1.bias'\n",
    "        weight = get_parameter(model=model,name=w_name).data.to(device)\n",
    "        bias = get_parameter(model=model,name=b_name).data.to(device)\n",
    "        layer_norm_params[w_name] = weight.to(device)\n",
    "        layer_norm_params[b_name] = bias.to(device)\n",
    "    return layer_norm_params\n",
    "\n",
    "#we should add 1 to the layer ct.\n",
    "#layers 5-27 out of 0-27\n",
    "params = get_layer_norm_params(model,4,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36b5d1c0-9673-421a-b982-59283ce5db22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lm_head applies LayerNorm and then a linear map to get the token-space (50400)\n",
    "# (1,4096) -layernorm, linear-> (1,50400) -softmax-> (1,50400) -topk-> (1,5)\n",
    "def get_object(mt, z, k=5):\n",
    "    logits = mt.lm_head(z)\n",
    "    dist = torch.softmax(logits.float(), dim=-1)\n",
    "    topk = dist.topk(k=k, dim=-1)\n",
    "    probs = topk.values.view(5).tolist()\n",
    "    token_ids = topk.indices.view(5).tolist()\n",
    "    words = [mt.tokenizer.decode(token_id) for token_id in token_ids]\n",
    "    return (words, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8d4f181-c3a5-43c5-8b89-29dff682cd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(\n",
    "    x: torch.Tensor, dim, eps: float = 0.00001\n",
    ") -> torch.Tensor:\n",
    "    mean = torch.mean(x, dim=dim, keepdim=True)\n",
    "    var = torch.square(x - mean).mean(dim=dim, keepdim=True)\n",
    "    return (x - mean) / torch.sqrt(var + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e179255d-71e3-4327-af5f-9e3a5489a5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_dict(i):\n",
    "    return next((item for item in layer_dicts if item['i'] == i), None)\n",
    "    \n",
    "def approx_s_s_layer(hs, i):\n",
    "    layer_dict = get_layer_dict(i)\n",
    "    layer_weight = layer_dict['s_s_mean_weight']\n",
    "    layer_bias = layer_dict['s_s_mean_bias']\n",
    "    ln_weight = params[f'transformer.h.{i}.ln_1.weight']\n",
    "    ln_bias = params[f'transformer.h.{i}.ln_1.bias']\n",
    "    _hs = hs\n",
    "    \n",
    "    #perform layer normalization with adaptive w and b\n",
    "    hs = layer_norm(hs, (1)) * ln_weight + ln_bias\n",
    "    \n",
    "    #perform the layer operation\n",
    "    hs = hs.mm(layer_weight.t()) #+ layer_bias\n",
    "        \n",
    "    #add residual\n",
    "    #hs = hs + _hs\n",
    "    return hs\n",
    "\n",
    "def approx_s_o_layer(hs, i):\n",
    "    layer_dict = get_layer_dict(i)\n",
    "    layer_weight = layer_dict['s_o_mean_weight']\n",
    "    layer_bias = layer_dict['s_o_mean_bias']  \n",
    "    ln_weight = params[f'transformer.h.{i}.ln_1.weight']\n",
    "    ln_bias = params[f'transformer.h.{i}.ln_1.bias']\n",
    "    _hs = hs\n",
    "    \n",
    "    #perform layer normalization with adaptive w and b\n",
    "    hs = layer_norm(hs, (1)) * ln_weight + ln_bias\n",
    "    \n",
    "    #perform the layer operation\n",
    "    hs = hs.mm(layer_weight.t()) + layer_bias\n",
    "    \n",
    "    #add residual\n",
    "    hs = hs + _hs * 3.25\n",
    "    return hs\n",
    "    \n",
    "def approx_o_o_layer(hs, i):\n",
    "    layer_dict = get_layer_dict(i)\n",
    "    layer_weight = layer_dict['o_o_mean_weight']\n",
    "    layer_bias = layer_dict['o_o_mean_bias']  \n",
    "    ln_weight = params[f'transformer.h.{i-1}.ln_1.weight']\n",
    "    ln_bias = params[f'transformer.h.{i-1}.ln_1.bias']\n",
    "    _hs = hs\n",
    "    \n",
    "    #perform layer normalization with adaptive w and b\n",
    "    hs = layer_norm(hs, (1)) * ln_weight + ln_bias\n",
    "    \n",
    "    #perform the layer operation\n",
    "    hs = hs.mm(layer_weight.t()) + layer_bias\n",
    "    \n",
    "    #add residual\n",
    "    hs = hs + _hs\n",
    "    return hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ff713e46-75f6-48fa-9f2b-da9d25f8ae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we want to do h' = h.mm(weight.t()) * beta + bias for each layer 5-26 (s -> s')\n",
    "#then, finally z = h.mm(weight.t()) * beta + bias (s' -> o)\n",
    "\n",
    "def tp(state: torch.Tensor):\n",
    "    return state.cpu().detach().numpy()[0]\n",
    "\n",
    "def approx_lm(hs):\n",
    "    \n",
    "    #apply s_s\n",
    "    for i in range(START_LAYER, S_O_LAYER):\n",
    "        hs = approx_s_s_layer(hs,i)\n",
    "\n",
    "    #apply s_o\n",
    "    for i in range(S_O_LAYER, S_O_LAYER +1):\n",
    "        hs = approx_s_o_layer(hs, i)\n",
    "        \n",
    "    #apply o_o\n",
    "    for i in range(S_O_LAYER + 1, END_LAYER):\n",
    "        hs = approx_o_o_layer(hs, i)\n",
    "        \n",
    "    return hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "85323536-8faa-4685-8b9d-b1f4e4768ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ape: [' jo', ' kid', ' wee', ' c', ' fo']\n",
      "badger: [' cub', ' pup', ' kit', ' p', ' kid']\n",
      "bear: [' cub', ' pup', ' c', ' t', ' paw']\n",
      "beaver: [' pup', ' p', ' kit', ' cub', ' kid']\n",
      "bee: [' pup', ' lar', ' c', ' p', ' nest']\n",
      "beetle: [' lar', ' pup', ' p', ' gr', ' larvae']\n",
      "buffalo: [' calf', ' kid', ' herd', ' c', ' fo']\n",
      "butterfly: [' lar', ' wing', ' chick', ' flying', ' p']\n",
      "camel: [' fo', ' calf', ' kid', ' toe', ' fetus']\n",
      "cat: [' p', ' kitten', ' litter', ' c', ' jo']\n",
      "cattle: [' calf', ' fo', ' jo', ' herd', ' tick']\n",
      "chimpanzee: [' jo', ' fo', ' c', ' baby', ' p']\n",
      "cicada: [' prince', ' bite', ' p', ' lar', ' baby']\n",
      "cockroach: [' p', ' lar', ' z', ' baby', ' tad']\n",
      "cricket: [' lar', ' hopping', ' frog', 'ing', ' tad']\n",
      "deer: [' fo', ' calf', ' kid', ' f', ' herd']\n",
      "dog: [' puppy', ' k', ' litter', ' whe', ' p']\n",
      "duck: ['ling', ' kid', ' fetus', ' fo', 'lings']\n",
      "elephant: [' calf', ' fo', ' seal', ' t', ' baby']\n",
      "ferret: [' kit', ' p', ' jo', ' litter', ' pup']\n",
      "fish: [' fry', ' c', ' n', ' flo', ' nurs']\n",
      "fly: [' lar', ' mag', ' pup', ' p', ' le']\n",
      "fox: ['es', ' cub', ' kit', ' pup', ' p']\n",
      "goat: [' kid', ' fo', ' c', ' ho', ' p']\n",
      "goldfish: [' fry', ' bowl', ' p', ' finger', ' l']\n",
      "gorilla: [' jo', ' cub', ' baby', ' kid', ' fo']\n",
      "herring: [' gull', ' fry', ' pup', ' spawn', ' fisherman']\n",
      "horse: [' fo', ' ho', ' toe', ' col', ' fetus']\n",
      "insect: [' lar', ' larvae', ' meal', ' flying', ' p']\n",
      "lion: [' cub', ' c', 'ess', ' tam', ' t']\n",
      "mink: [' kit', ' jo', ' p', ' k', ' kits']\n",
      "monkey: [' jo', ' c', ' fol', ' fo', ' p']\n",
      "muskrat: [' pup', ' p', ' kit', ' jo', ' kits']\n",
      "ox: [' fo', ' c', ' ho', ' n', ' calf']\n",
      "panda: [' cub', ' baby', ' jo', ' toy', ' fo']\n",
      "pig: ['gy', ' fo', ' wee', 'let', ' jo']\n",
      "rabbit: [' p', ' litter', ' fur', ' foot', ' fo']\n",
      "raccoon: [' kit', ' p', ' kid', ' pup', ' kits']\n",
      "salmon: [' fry', ' pup', ' lar', ' trout', ' p']\n",
      "seal: [' pup', ' cub', ' p', ' fo', ' kit']\n",
      "shark: [' c', ' cub', ' fetus', ' young', ' pup']\n",
      "sheep: [' fo', ' ho', ' c', ' fetus', ' lamb']\n",
      "skunk: [' kits', ' c', ' jo', ' fo', ' kit']\n",
      "snake: [' c', ' char', ' hatch', ' p', ' craw']\n",
      "tiger: [' cub', ' kit', ' kitten', ' jo', ' c']\n",
      "trout: [' fry', ' finger', ' lar', ' spawn', ' c']\n",
      "weasel: [' p', ' kit', ' kits', ' pup', ' cub']\n",
      "whale: [' calf', ' pup', ' fo', ' fetus', ' cub']\n",
      "wolf: [' cub', ' pup', ' p', ' pack', ' puppy']\n",
      "woodchuck: [' pup', ' p', ' mother', ' puppy', ' kit']\n"
     ]
    }
   ],
   "source": [
    "# CREATE ResLRE\n",
    "\n",
    "animals = [\"dog\", \"duck\", \"fish\", \"horse\", \"mink\", \"seal\", \"shark\", \"trout\"]\n",
    "#layers: 0-27\n",
    "S_O_LAYER = 26\n",
    "START_LAYER = 5\n",
    "#S_O_LAYER = 7 #TODO: generate s_o_weight_27\n",
    "END_LAYER = S_O_LAYER + 1\n",
    "\n",
    "#Consistent with previous results, from the early layers s' seems to represent o.\n",
    "layer_dicts = []\n",
    "### S --> S'\n",
    "for i in range(START_LAYER, S_O_LAYER):\n",
    "    #layer_dict = sample_weights_biases(animal,\"s_s\", i, animals)\n",
    "    layer_dict = mean_weights_biases(\"s_s\", i, animals)\n",
    "    layer_dicts.append(layer_dict)\n",
    "\n",
    "#### S' --> O\n",
    "for i in range(S_O_LAYER,S_O_LAYER+1):\n",
    "    #layer_dict = sample_weights_biases(animal,\"s_o\", i, animals)\n",
    "    layer_dict = mean_weights_biases(\"s_o\", i, animals)\n",
    "    layer_dicts.append(layer_dict)\n",
    "\n",
    "### O --> O'\n",
    "for i in range(S_O_LAYER+1, END_LAYER):\n",
    "    #layer_dict = sample_weights_biases(animal,\"o_o\", i, animals)\n",
    "    layer_dict = mean_weights_biases(\"o_o\", i, animals)\n",
    "    layer_dicts.append(layer_dict)\n",
    "\n",
    "for (subj, obj) in pairs:\n",
    "    hs = get_hidden_state(mt, subj, 5) #layer 5\n",
    "    object_hs = approx_lm(hs)\n",
    "    print(f'{subj}: {get_object(mt, object_hs)[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec3fc13-7ddc-47bc-bc3e-6f838ad89c50",
   "metadata": {},
   "source": [
    "### Compare ResLRE to LRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "713dd09f-302b-4283-9e94-5429cc036395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_state(mt, subject, h_layer):\n",
    "    prompt = f\"The offspring of a {subject} is referred to as a\"\n",
    "    h_index, inputs = functional.find_subject_token_index(\n",
    "        mt = mt, prompt=prompt, subject=subject)\n",
    "    #print(f'h_index is {h_index}, inputs is {inputs}')\n",
    "    [[hs], _] = functional.compute_hidden_states(\n",
    "        mt = mt, layers = [h_layer], inputs = inputs)\n",
    "    #h is hs @ h_layer @ h_index\n",
    "    h = hs[:, h_index]\n",
    "    h = h.to(device)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3974add2-2c03-4b01-a71a-3762339fa440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_path = 'animal-youth.json'\n",
    "pairs = []\n",
    "with open(json_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "    for pair in data['samples']:\n",
    "        pairs.append((pair['subject'],pair['object']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2659a64-b5b5-43cd-b32c-cca6f82b397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for most relations.\n",
    "def is_nontrivial_prefix(prediction: str, target: str) -> bool:\n",
    "    target = target.lower().strip()\n",
    "    prediction = prediction.lower().strip()\n",
    "    # if len(prediction) > 0 and target.startswith(prediction):\n",
    "    #     print(f\"{prediction} matches {target}\")\n",
    "    return len(prediction) > 1 and target.startswith(prediction)\n",
    "\n",
    "def any_is_nontrivial_prefix(prediction, targets) -> bool:\n",
    "    return any(is_nontrivial_prefix(prediction, target) for target in targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec8e5733-9f9f-4ee1-88d9-aa76ecef4ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ape matches  baby: 1.0\n",
      "bear matches  cub: 1.2\n",
      "bee matches  lar: 3.7\n",
      "beetle matches  lar: 1.9\n",
      "buffalo matches  calf: 1.2\n",
      "butterfly matches  pup: 1.0\n",
      "cattle matches  calf: 1.6\n",
      "chimpanzee matches  baby: 1.0\n",
      "cricket matches  lar: 1.7\n",
      "dog matches  pup: 1.0\n",
      "elephant matches  calf: 1.4\n",
      "ferret matches  kit: 2.9\n",
      "fish matches  fry: 1.2\n",
      "fly matches  mag: 3.8\n",
      "fox matches  pup: 1.0\n",
      "goat matches  kid: 1.0\n",
      "goldfish matches  fry: 1.0\n",
      "horse matches  fo: 1.0\n",
      "insect matches  lar: 1.7\n",
      "lion matches  cub: 1.0\n",
      "mink matches  kit: 1.4\n",
      "panda matches  cub: 1.7\n",
      "raccoon matches  kit: 1.7\n",
      "seal matches  pup: 1.0\n",
      "shark matches  pup: 1.0\n",
      "tiger matches  cub: 1.0\n",
      "trout matches  finger: 4.9\n",
      "whale matches  calf: 1.8\n",
      "wolf matches  pup: 1.0\n"
     ]
    }
   ],
   "source": [
    "for (subj, obj) in pairs:\n",
    "    for beta in range(10,50, 1):\n",
    "        beta /= 10\n",
    "        hs = get_hidden_state(mt, subj, 5) #layer 5\n",
    "        object_hs = approx_lm(hs, beta) #beta\n",
    "        pred = get_object(mt, object_hs)[0]\n",
    "        if (any_is_nontrivial_prefix(pred[0], obj)):\n",
    "            print(f\"{subj} matches {pred[0]}: {beta}\")\n",
    "            break\n",
    "    \n",
    "# for (subj, obj) in pairs:\n",
    "#     hs = get_hidden_state(mt, subj, 5)\n",
    "#     object_hs = approx_lm(hs, 2.4)\n",
    "#     print(f'{subj}: {get_object(mt, object_hs)[0]} {obj}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8e63aa-35aa-4574-aeef-909288a286a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get tokens in GPT-J\n",
    "#get the hidden state of them at the last layer (after the 28th layer, or s->o @ 27)\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_hidden_state(mt, subject, h_layer, h=None, k=5):\n",
    "    prompt = f\" {subject}\"\n",
    "    h_index, inputs = functional.find_subject_token_index(\n",
    "        mt = mt, prompt=prompt, subject=subject)\n",
    "    #print(f'h_index is {h_index}, inputs is {inputs}')\n",
    "    [[hs], _] = functional.compute_hidden_states(\n",
    "        mt = mt, layers = [h_layer], inputs = inputs)\n",
    "    #h is hs @ h_layer @ h_index\n",
    "    h = hs[:, h_index]\n",
    "    h = h.to(device)\n",
    "    return h\n",
    "    \n",
    "#Spaces are converted in a special character (the Ġ ) in the tokenizer prior to BPE splitting\n",
    "#mostly to avoid digesting spaces since the standard BPE algorithm used spaces in its process \n",
    "\n",
    "#all animal encodings are at [-0.4153   2.023   -2.23    ... -0.785    0.06323 -0.1819 ]\n",
    "\n",
    "text = \"our classic pre-baked blueberry pie filled with delicious plump and juicy wild blueberries\"\n",
    "encoded_input = mt.tokenizer(text, return_tensors=\"pt\")\n",
    "token_ids = range(0,50400)\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "tokens = [token.replace(\"Ġ\", \" \") for token in tokens]\n",
    "\n",
    "#this is too slow and not useful.\n",
    "dict27 = {}\n",
    "for i in tqdm(range(len(tokens))):\n",
    "    token = tokens[i]\n",
    "    dict27[token] = get_hidden_state(mt, token, 27)\n",
    "    \n",
    "with open('animal_youth_27.pkl', 'wb') as file:\n",
    "    pickle.dump(dict27, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "c2b5d1c3-4f38-45ba-8664-9e535d0cd653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  (1): Linear(in_features=4096, out_features=50400, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11195a8e-d798-4b60-8884-3c5ef6d7bc12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c191e43-93c5-43c5-8dc4-8773cd9afa30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
