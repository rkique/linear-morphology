{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c4d83fb-81af-4c29-8b65-0f104333a540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exia/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from transformers import GPTJForCausalLM, AutoTokenizer\n",
    "import lre.models as models\n",
    "import lre.functional as functional\n",
    "import os\n",
    "\n",
    "import json\n",
    "import random\n",
    "from lre.data import Relation, RelationSample, Sequence\n",
    "import lre.metrics as metrics\n",
    "import lre.functional as functional\n",
    "\n",
    "device = 'cuda:1'\n",
    "weights = []\n",
    "biases = []\n",
    "subjects = []\n",
    "model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", revision=\"float16\", torch_dtype=torch.float16, low_cpu_mem_usage=True)\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "mt = models.ModelAndTokenizer(model,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c89eaaa6-e350-401a-80fb-bef421974cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 4096])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import llra.build as build\n",
    "#OBTAIN LRE WEIGHTS: Method 1\n",
    "#Load weights saved with new naming convention\n",
    "    \n",
    "samples = os.listdir('ceo')\n",
    "samples = [sample for sample in samples if not sample.startswith('.')]\n",
    "lre_weight = build.mean_weight_or_bias('ceo','s_o_weight_5_27_sem1', samples).to(device)\n",
    "lre_bias = build.mean_weight_or_bias('ceo','s_o_bias_5_27_sem1', samples).to(device)\n",
    "lre_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bebdca32-b2da-4046-a0ad-8b1d7ee6bd68",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects a non-empty TensorList",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m     weights\u001b[38;5;241m.\u001b[39mappend(weight)\n\u001b[1;32m     18\u001b[0m     biases\u001b[38;5;241m.\u001b[39mappend(bias)\n\u001b[0;32m---> 20\u001b[0m lre_weight \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     21\u001b[0m lre_bias \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(biases)\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m lre_weight\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects a non-empty TensorList"
     ]
    }
   ],
   "source": [
    "#OBTAIN LRE WEIGHTS: Method 2\n",
    "#Method 2: An LRE loading script for the older weights in 'approx' & 'wapprox'\n",
    "\n",
    "#verb + er\n",
    "\n",
    "weight_str = 's_o_weight_5_27_sem1'\n",
    "bias_str = 's_o_bias_5_27_sem1'\n",
    "\n",
    "paths = os.listdir(wdir)\n",
    "wdir = \"ceo\"\n",
    "weight_paths = [f for f in os.listdir(wdir) if f.startswith(weight_str)]\n",
    "bias_paths = [f for f in os.listdir(wdir) if f.startswith(bias_str)]\n",
    "\n",
    "for weight_str, bias_str in zip(weight_paths, bias_paths):\n",
    "    weight = torch.load(wdir + \"/\" + weight_str).to(device)\n",
    "    bias = torch.load(wdir + \"/\" + bias_str).to(device)\n",
    "    weights.append(weight)\n",
    "    biases.append(bias)\n",
    "    \n",
    "lre_weight = torch.stack(weights).mean(dim=0).to(device)\n",
    "lre_bias = torch.stack(biases).mean(dim=0).to(device)\n",
    "lre_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d52b498-d9c0-4ac8-8dd7-ea46d3893f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import llra.build as build\n",
    "from importlib import reload\n",
    "reload(build)\n",
    "build.determine_device(mt)\n",
    "build.determine_params(mt)\n",
    "S_O_start = 21\n",
    "S_O_end = 26\n",
    "start, end = 5, 27\n",
    "# beta = 2.75\n",
    "# wdir = 'capprox/ln-full antonym-binary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0167f0c2-4b92-43ee-aacc-be24ef75f56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet  Larry [' Jeff', ' David', ' James', ' Kevin', ' Mark']\n",
      "Tesla  Elon [' Jeff', ' James', ' Mike', ' Mark', ' David']\n",
      "NVIDIA  Jen [' James', ' Mark', ' J', ' Jim', ' Jeff']\n",
      "Meta  Mark [' Mark', ' James', ' J', ' David', ' Jim']\n",
      "S_O_START,21,S_O_END,26,beta,1,llra,,lre,6,lm,30\n"
     ]
    }
   ],
   "source": [
    "# llra_samples = os.listdir(wdir) #[\"juror\", \"shrub\", \"star\", \"word\", \"policeman\"]\n",
    "# assert(len(llra_samples) == 8)\n",
    "from llra.build import lm_params\n",
    "json_path = 'json/company-ceo.json'\n",
    "DEFAULT_N_ICL = 8 \n",
    "N_TRIALS = 8\n",
    "VIEW_SAMPLES = 5\n",
    "\n",
    "file = open(json_path, 'r')\n",
    "data = json.load(file)\n",
    "file.close()\n",
    "\n",
    "beta = 1\n",
    "relation = Relation.from_dict(data)\n",
    "prompt_template = relation.prompt_templates[0]\n",
    "#ASSEMBLE PROMPTS AND OBJECT ANSWERS\n",
    "clozed_prompts = []\n",
    "clozed_answers = []\n",
    "\n",
    "for x in relation.samples:\n",
    "    samples = [x] + random.sample(relation.samples, DEFAULT_N_ICL - 1)\n",
    "    #print(f'{samples} samples)')\n",
    "    cloze_prompt = functional.make_prompt(\n",
    "        template = prompt_template, \n",
    "        target = x,\n",
    "        examples = samples\n",
    "        )\n",
    "    clozed_prompts.append(cloze_prompt)\n",
    "    clozed_answers.append(x.object)\n",
    "\n",
    "outputs_lm = functional.predict_next_token(mt=mt, prompt=clozed_prompts)\n",
    "preds_lm =  [[x.token for x in xs] for xs in outputs_lm]\n",
    "recall_lm = metrics.recall(preds_lm, clozed_answers)\n",
    "\n",
    "lre_correct = 0\n",
    "llra_correct = 0\n",
    "lm_correct = 0\n",
    "\n",
    "# layer_dicts = build.build_llra(wdir, llra_samples, start,S_O_start,S_O_end,end)\n",
    "# print([l[\"i\"] for l in layer_dicts])\n",
    "# llra = build.LLRA(layer_dicts=layer_dicts)\n",
    "\n",
    "for i, sample, objs, prompt, preds in \\\n",
    "zip(range(0,50), relation.samples, clozed_answers, clozed_prompts, preds_lm):\n",
    "    \n",
    "    if (metrics.any_is_nontrivial_prefix(predictions=preds, targets=objs)):\n",
    "        hs = build.get_hidden_state(mt, prompt, sample.subject, start) #layer 5\n",
    "        #use the complete LRE\n",
    "        # llra_object_hs = llra.approx_lm(hs, \n",
    "        #                                 start, S_O_start, S_O_end, end, \n",
    "        #                                 layerwise=True)\n",
    "        # llra_preds = build.get_object(mt, llra_object_hs)[0]\n",
    "\n",
    "        #use the regular LRE\n",
    "        lre_object_hs = hs.mm(lre_weight.t()) * beta + lre_bias\n",
    "        lre_preds = build.get_object(mt, lre_object_hs)[0]\n",
    "        \n",
    "        # if(metrics.any_is_nontrivial_prefix(predictions=llra_preds, targets=objs)):\n",
    "        #     llra_correct += 1\n",
    "\n",
    "        if(metrics.any_is_nontrivial_prefix(predictions=lre_preds, targets=objs)):\n",
    "            lre_correct += 1\n",
    "            \n",
    "        if(i < VIEW_SAMPLES):\n",
    "            print(f'{sample.subject} {preds[0]} {lre_preds}')\n",
    "            \n",
    "        lm_correct += 1\n",
    "print(f'S_O_START,{S_O_start},S_O_END,{S_O_end},beta,{beta},llra,,lre,{lre_correct},lm,{lm_correct}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4dd852bb-1fc5-49ea-8ca3-2c6293f34c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTJModel(\n",
       "  (wte): Embedding(50400, 4096)\n",
       "  (drop): Dropout(p=0.0, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-27): 28 x GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt.model.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8a16b223-a32b-4c45-8c50-9e9948994bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTJConfig {\n",
       "  \"_name_or_path\": \"EleutherAI/gpt-j-6B\",\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPTJForCausalLM\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.0,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.0,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gptj\",\n",
       "  \"n_embd\": 4096,\n",
       "  \"n_head\": 16,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 28,\n",
       "  \"n_positions\": 2048,\n",
       "  \"resid_pdrop\": 0.0,\n",
       "  \"rotary_dim\": 64,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50,\n",
       "      \"temperature\": 1.0\n",
       "    }\n",
       "  },\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"tokenizer_class\": \"GPT2Tokenizer\",\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.41.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50400\n",
       "}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt.model.config1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0f77ede3-58fc-469b-bc49-913453dbda46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  (1): Linear(in_features=4096, out_features=50400, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3974add2-2c03-4b01-a71a-3762339fa440",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LRE BETA INSERTION POSITION\n",
    "json_path = 'qapprox/antonyms-binary.json'\n",
    "\n",
    "DEFAULT_N_ICL = 8 \n",
    "N_TRIALS = 8\n",
    "\n",
    "with open(json_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "    relation = Relation.from_dict(data)\n",
    "    prompt = \"The opposite of {} is\"\n",
    "    prompt_template = relation.prompt_templates[0]\n",
    "    beta = 2.75\n",
    "    for beta_layer in range(5,27):\n",
    "        print(f'{beta_layer=}')\n",
    "        for _ in range(0,N_TRIALS):\n",
    "            clozed_prompts = []\n",
    "            clozed_answers = []\n",
    "            for x in relation.samples:\n",
    "                samples = [x] + random.sample(relation.samples, DEFAULT_N_ICL - 1)\n",
    "                cloze_prompt = functional.make_prompt(\n",
    "                    template = prompt_template, \n",
    "                    target = x,\n",
    "                    examples = samples\n",
    "                    )\n",
    "                clozed_prompts.append(cloze_prompt)\n",
    "                clozed_answers.append(x.object)\n",
    "            outputs_lm = functional.predict_next_token(mt=mt, prompt=clozed_prompts)\n",
    "            preds_lm =  [[x.token for x in xs] for xs in outputs_lm]\n",
    "            recall_lm = metrics.recall(preds_lm, clozed_answers)\n",
    "            lre_correct = 0\n",
    "            lm_correct = 0\n",
    "            \n",
    "            for _, sample, objs, prompt, preds in zip(range(50), relation.samples, clozed_answers, clozed_prompts, preds_lm):\n",
    "                if (metrics.any_is_nontrivial_prefix(\n",
    "                    predictions=preds, \n",
    "                    targets=objs)):\n",
    "                    hs = build.get_hidden_state(mt, prompt, sample.subject, 1) #layer 5\n",
    "                    object_hs = approx_lm(hs, beta, beta_layer)\n",
    "                    lre_preds = get_object(mt, object_hs)[0]\n",
    "                    if(metrics.any_is_nontrivial_prefix(predictions=lre_preds, targets=objs)):\n",
    "                        lre_correct += 1\n",
    "                    lm_correct += 1\n",
    "                    \n",
    "            print(f'{lre_correct} {lm_correct}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e2659a64-b5b5-43cd-b32c-cca6f82b397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for most relations.\n",
    "def is_nontrivial_prefix(prediction: str, target: str) -> bool:\n",
    "    target = target.lower().strip()\n",
    "    prediction = prediction.lower().strip()\n",
    "    # if len(prediction) > 0 and target.startswith(prediction):\n",
    "    #     print(f\"{prediction} matches {target}\")\n",
    "    return len(prediction) > 1 and target.startswith(prediction)\n",
    "\n",
    "def any_is_nontrivial_prefix(prediction, targets) -> bool:\n",
    "    return any(is_nontrivial_prefix(prediction, target) for target in targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ec8e5733-9f9f-4ee1-88d9-aa76ecef4ee3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pairs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (subj, obj) \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpairs\u001b[49m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m beta \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      3\u001b[0m         beta \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pairs' is not defined"
     ]
    }
   ],
   "source": [
    "for (subj, obj) in pairs:\n",
    "    for beta in range(10,50, 1):\n",
    "        beta /= 10\n",
    "        hs = get_hidden_state(mt, subj, 5) #layer 5\n",
    "        object_hs = approx_lm(hs, beta) #beta\n",
    "        pred = get_object(mt, object_hs)[0]\n",
    "        if (any_is_nontrivial_prefix(pred[0], obj)):\n",
    "            print(f\"{subj} matches {pred[0]}: {beta}\")\n",
    "            break\n",
    "    \n",
    "# for (subj, obj) in pairs:\n",
    "#     hs = get_hidden_state(mt, subj, 5)\n",
    "#     object_hs = approx_lm(hs, 2.4)\n",
    "#     print(f'{subj}: {get_object(mt, object_hs)[0]} {obj}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8e63aa-35aa-4574-aeef-909288a286a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get tokens in GPT-J\n",
    "#get the hidden state of them at the last layer (after the 28th layer, or s->o @ 27)\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_hidden_state(mt, subject, h_layer, h=None, k=5):\n",
    "    prompt = f\" {subject}\"\n",
    "    h_index, inputs = functional.find_subject_token_index(\n",
    "        mt = mt, prompt=prompt, subject=subject)\n",
    "    #print(f'h_index is {h_index}, inputs is {inputs}')\n",
    "    [[hs], _] = functional.compute_hidden_states(\n",
    "        mt = mt, layers = [h_layer], inputs = inputs)\n",
    "    #h is hs @ h_layer @ h_index\n",
    "    h = hs[:, h_index]\n",
    "    h = h.to(device)\n",
    "    return h\n",
    "    \n",
    "#Spaces are converted in a special character (the Ġ ) in the tokenizer prior to BPE splitting\n",
    "#mostly to avoid digesting spaces since the standard BPE algorithm used spaces in its process \n",
    "\n",
    "#all animal encodings are at [-0.4153   2.023   -2.23    ... -0.785    0.06323 -0.1819 ]\n",
    "\n",
    "text = \"our classic pre-baked blueberry pie filled with delicious plump and juicy wild blueberries\"\n",
    "encoded_input = mt.tokenizer(text, return_tensors=\"pt\")\n",
    "token_ids = range(0,50400)\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "tokens = [token.replace(\"Ġ\", \" \") for token in tokens]\n",
    "\n",
    "#this is too slow and not useful.\n",
    "dict27 = {}\n",
    "for i in tqdm(range(len(tokens))):\n",
    "    token = tokens[i]\n",
    "    dict27[token] = get_hidden_state(mt, token, 27)\n",
    "    \n",
    "with open('animal_youth_27.pkl', 'wb') as file:\n",
    "    pickle.dump(dict27, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "c2b5d1c3-4f38-45ba-8664-9e535d0cd653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  (1): Linear(in_features=4096, out_features=50400, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11195a8e-d798-4b60-8884-3c5ef6d7bc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' puppy', ' pup', ' p', ' dog', ' �']\n",
      "[' duck', ' dra', ' g', ' �', ' \"']\n",
      "[' fry', ' prog', ' F', ' �', ' lar']\n",
      "[' fo', ' col', ' horse', ' pony', ' �']\n",
      "[' kit', ' m', ' \"', ' �', ' p']\n",
      "[' seal', ' \"', ' �', ' pup', ' p']\n",
      "[' shark', ' \"', ' �', ' p', ' pup']\n",
      "[' fry', ' trout', ' \"', ' �', ' rainbow']\n"
     ]
    }
   ],
   "source": [
    "animals = [\"dog\", \"duck\", \"fish\", \"horse\", \"mink\", \"seal\", \"shark\", \"trout\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db937003-748c-465b-a853-4ec53556ec7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
