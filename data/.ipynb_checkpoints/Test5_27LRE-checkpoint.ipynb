{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c4d83fb-81af-4c29-8b65-0f104333a540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exia/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "from transformers import GPTJForCausalLM, AutoTokenizer\n",
    "import lre.models as models\n",
    "import lre.functional as functional\n",
    "import os\n",
    "\n",
    "import json\n",
    "import random\n",
    "from lre.data import Relation, RelationSample, Sequence\n",
    "import lre.metrics as metrics\n",
    "import lre.functional as functional\n",
    "\n",
    "device = \"cuda:1\"\n",
    "weights = []\n",
    "biases = []\n",
    "subjects = []\n",
    "model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", revision=\"float16\", torch_dtype=torch.float16, low_cpu_mem_usage=True)\n",
    "model.to('cuda:1')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "mt = models.ModelAndTokenizer(model,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e40c064c-c98e-4ebb-b4fa-0ab09a073297",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Let's still try averaging the w/b for each layer, because that seems the most intuitive.\n",
    "\n",
    "def sample_weights_biases(subject, kind, i, samples) -> dict:\n",
    "    layer_dict = {\"i\": i}\n",
    "    weights = []\n",
    "    biases = []\n",
    "    wdir = f'qapprox/{subject}/{i}'\n",
    "    weight_path = f\"{wdir}/{kind}_weight_h_{i}.pt\"\n",
    "    bias_path = f\"{wdir}/{kind}_bias_h_{i}.pt\"\n",
    "    #load s_s_weight and s_s_bias\n",
    "    weight = torch.load(weight_path).to(device)\n",
    "    bias = torch.load(bias_path).to(device)\n",
    "    layer_dict[f'{kind}_weight'] = weight\n",
    "    layer_dict[f'{kind}_bias'] = bias\n",
    "    return layer_dict\n",
    "    \n",
    "def mean_weights_biases(kind, i, samples) -> dict:\n",
    "    layer_dict = {\"i\": i}\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for sample in samples:\n",
    "        wdir = f'qapprox/{sample}/{i}'\n",
    "        weight_path = f\"{wdir}/{kind}_weight_h_{i}.pt\"\n",
    "        bias_path = f\"{wdir}/{kind}_bias_h_{i}.pt\"\n",
    "        #load s_s_weight and s_s_bias\n",
    "        weight = torch.load(weight_path).to(device)\n",
    "        bias = torch.load(bias_path).to(device)\n",
    "        #print(f'weight is {tp(weight)}')\n",
    "        #append to lists\n",
    "        weights.append(weight)\n",
    "        biases.append(bias)\n",
    "    mean_weight = torch.stack(weights).mean(dim=0).to(device)\n",
    "    mean_bias = torch.stack(biases).mean(dim=0).to(device)\n",
    "    sum_weight = torch.stack(weights).sum(dim=0).to(device)\n",
    "    sum_bias = torch.stack(biases).sum(dim=0).to(device)\n",
    "    \n",
    "    layer_dict[f'{kind}_mean_weight'] = mean_weight\n",
    "    layer_dict[f'{kind}_mean_bias'] = mean_bias\n",
    "    layer_dict[f'{kind}_sum_weight'] = sum_weight\n",
    "    layer_dict[f'{kind}_sum_bias'] = sum_bias\n",
    "    \n",
    "    return layer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70a81114-f8bf-4ed6-8eb1-29bee0104bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from baukit.baukit import parameter_names, get_parameter\n",
    "\n",
    "#returns weight and bias for lns\n",
    "def get_layer_norm_params(model, start, end):\n",
    "    layer_norm_params = {}\n",
    "    for i in range(start, end):\n",
    "        w_name = f'transformer.h.{i}.ln_1.weight'\n",
    "        b_name = f'transformer.h.{i}.ln_1.bias'\n",
    "        weight = get_parameter(model=model,name=w_name).data.to(device)\n",
    "        bias = get_parameter(model=model,name=b_name).data.to(device)\n",
    "        layer_norm_params[w_name] = weight.to(device)\n",
    "        layer_norm_params[b_name] = bias.to(device)\n",
    "        \n",
    "    ln_f_w_name = 'transformer.ln_f.weight'\n",
    "    ln_f_b_name = 'transformer.ln_f.bias'\n",
    "    weight = get_parameter(model=model,name=ln_f_w_name).data.to(device)\n",
    "    bias = get_parameter(model=model,name=ln_f_b_name).data.to(device)\n",
    "    layer_norm_params[ln_f_w_name] = weight.to(device)\n",
    "    layer_norm_params[ln_f_b_name] = bias.to(device)\n",
    "    return layer_norm_params\n",
    "\n",
    "#we should add 1 to the layer ct.\n",
    "#layers 5-27 out of 0-27\n",
    "params = get_layer_norm_params(model,4,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36b5d1c0-9673-421a-b982-59283ce5db22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mt.lm_head applies a linear map to get the token-space (50400)\n",
    "# (1,4096) -layernorm, linear-> (1,50400) -softmax-> (1,50400) -topk-> (1,5)\n",
    "def get_object(mt, z, k=5):\n",
    "    logits = mt.lm_head(z)\n",
    "    dist = torch.softmax(logits.float(), dim=-1)\n",
    "    topk = dist.topk(k=k, dim=-1)\n",
    "    probs = topk.values.view(5).tolist()\n",
    "    token_ids = topk.indices.view(5).tolist()\n",
    "    words = [mt.tokenizer.decode(token_id) for token_id in token_ids]\n",
    "    return (words, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "b8d4f181-c3a5-43c5-8b89-29dff682cd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(\n",
    "    x: torch.Tensor, dim, eps: float = 0.00001\n",
    ") -> torch.Tensor:\n",
    "    mean = torch.mean(x, dim=dim, keepdim=True)\n",
    "    var = torch.square(x - mean).mean(dim=dim, keepdim=True)\n",
    "    return (x - mean) / torch.sqrt(var + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "7ae75f4b-6efa-40ae-b5c1-5e9108e6ae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_O_LAYER = 15\n",
    "START_LAYER = 5\n",
    "END_LAYER = 27\n",
    "beta = 2.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "e179255d-71e3-4327-af5f-9e3a5489a5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_dict(i):\n",
    "    return next((item for item in layer_dicts if item['i'] == i), None)\n",
    "\n",
    "def approx_s_s_layer(hs, i, beta=1):\n",
    "    layer_dict = get_layer_dict(i)\n",
    "    layer_weight = layer_dict['s_s_mean_weight']\n",
    "    layer_bias = layer_dict['s_s_mean_bias']\n",
    "    ln_weight = params[f'transformer.h.{i}.ln_1.weight']\n",
    "    ln_bias = params[f'transformer.h.{i}.ln_1.bias']\n",
    "    _hs = hs\n",
    "\n",
    "    hs = layer_norm(hs, (1)) * ln_weight + ln_bias\n",
    "    hs = beta * hs.mm(layer_weight.t()) #+ layer_bias\n",
    "    #hs = hs + _hs\n",
    "    \n",
    "    return hs\n",
    "\n",
    "def approx_s_o_layer(hs, i, beta=1):\n",
    "    layer_dict = get_layer_dict(i)\n",
    "    layer_weight = layer_dict['s_o_mean_weight']\n",
    "    layer_bias = layer_dict['s_o_mean_bias']  \n",
    "    ln_weight = params[f'transformer.h.{i}.ln_1.weight']\n",
    "    ln_bias = params[f'transformer.h.{i}.ln_1.bias']\n",
    "    _hs = hs\n",
    "    hs = layer_norm(hs, (1)) * ln_weight + ln_bias\n",
    "    #this transformation should encompass the work of the MHSA and MLP layer.\n",
    "    hs = beta * hs.mm(layer_weight.t()) + layer_bias\n",
    "    hs = hs + _hs\n",
    "    \n",
    "    return hs\n",
    "    \n",
    "def approx_o_o_layer(hs, i, beta=1):\n",
    "    layer_dict = get_layer_dict(i)\n",
    "    layer_weight = layer_dict['o_o_mean_weight']\n",
    "    layer_bias = layer_dict['o_o_mean_bias']  \n",
    "    ln_weight = params[f'transformer.h.{i}.ln_1.weight']\n",
    "    ln_bias = params[f'transformer.h.{i}.ln_1.bias']\n",
    "    _hs = hs\n",
    "    \n",
    "    hs = layer_norm(hs, (1)) * ln_weight + ln_bias\n",
    "    hs = beta + hs.mm(layer_weight.t()) #+ layer_bias\n",
    "    #hs = hs + _hs\n",
    "    return hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "ff713e46-75f6-48fa-9f2b-da9d25f8ae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we want to do h' = h.mm(weight.t()) * beta + bias for each layer 5-26 (s -> s')\n",
    "#then, finally z = h.mm(weight.t()) * beta + bias (s' -> o)\n",
    "\n",
    "def tp(state: torch.Tensor):\n",
    "    return state.cpu().detach().numpy()[0]\n",
    "\n",
    "def approx_lm(hs, beta, beta_layer):\n",
    "    \n",
    "    #apply s_s\n",
    "    for i in range(START_LAYER, S_O_LAYER):\n",
    "        if i == beta_layer:\n",
    "            hs = approx_s_s_layer(hs, i, beta)\n",
    "        else:\n",
    "            hs = approx_s_s_layer(hs,i)\n",
    "        \n",
    "    #apply s_o\n",
    "    for i in range(S_O_LAYER, S_O_LAYER +1):\n",
    "        if i == beta_layer:\n",
    "            hs = approx_s_o_layer(hs, i, beta)\n",
    "        else:\n",
    "            hs = approx_s_o_layer(hs, i)\n",
    "        \n",
    "    #apply o_o\n",
    "    for i in range(S_O_LAYER + 1, END_LAYER):\n",
    "        if i == beta_layer:\n",
    "            hs = approx_o_o_layer(hs, i, beta)\n",
    "        else:\n",
    "            hs = approx_o_o_layer(hs, i)\n",
    "    \n",
    "    ln_weight = params['transformer.ln_f.weight']\n",
    "    ln_bias = params['transformer.ln_f.bias']\n",
    "    #hs = layer_norm(hs, (1)) * ln_weight + ln_bias\n",
    "        \n",
    "    return hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "713dd09f-302b-4283-9e94-5429cc036395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_state(mt, prompt, subject, h_layer):\n",
    "    prompt = prompt.format(subject)\n",
    "    h_index, inputs = functional.find_subject_token_index(\n",
    "        mt = mt, prompt=prompt, subject=subject)\n",
    "    #print(f'h_index is {h_index}, inputs is {inputs}')\n",
    "    [[hs], _] = functional.compute_hidden_states(\n",
    "        mt = mt, layers = [h_layer], inputs = inputs)\n",
    "    #h is hs @ h_layer @ h_index\n",
    "    h = hs[:, h_index]\n",
    "    h = h.to(device)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "85323536-8faa-4685-8b9d-b1f4e4768ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE ResLRE\n",
    "samples = ['ahead', 'backward', 'down', 'inbound', 'input', 'mortal', 'off', 'top']\n",
    "#layers: 0-27\n",
    "#Consistent with previous results, from the early layers s' seems to represent o.\n",
    "layer_dicts = []\n",
    "### S --> S'\n",
    "for i in range(START_LAYER, S_O_LAYER):\n",
    "    #layer_dict = sample_weights_biases(animal,\"s_s\", i, animals)\n",
    "    layer_dict = mean_weights_biases(\"s_s\", i, samples)\n",
    "    layer_dicts.append(layer_dict)\n",
    "\n",
    "#### S' --> O\n",
    "for i in range(S_O_LAYER,S_O_LAYER+1):\n",
    "    #layer_dict = sample_weights_biases(animal,\"s_o\", i, animals)\n",
    "    layer_dict = mean_weights_biases(\"s_o\", i, samples)\n",
    "    layer_dicts.append(layer_dict)\n",
    "\n",
    "### O --> O'\n",
    "for i in range(S_O_LAYER+1, END_LAYER):\n",
    "    #layer_dict = sample_weights_biases(animal,\"o_o\", i, animals)\n",
    "    layer_dict = mean_weights_biases(\"o_o\", i, samples)\n",
    "    layer_dicts.append(layer_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "c5bb561d-c1c1-4753-958d-5070a6735814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lre.functional as functional\n",
    "from importlib import reload\n",
    "reload(functional)\n",
    "import lre.functional as functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3974add2-2c03-4b01-a71a-3762339fa440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_layer=5\n",
      "13 47\n",
      "14 47\n",
      "12 47\n",
      "14 48\n",
      "13 47\n",
      "14 47\n",
      "15 47\n",
      "13 48\n",
      "beta_layer=6\n",
      "14 48\n",
      "13 46\n",
      "14 47\n",
      "12 47\n",
      "13 47\n",
      "14 48\n",
      "14 47\n",
      "15 47\n",
      "beta_layer=7\n",
      "15 47\n",
      "13 47\n",
      "15 48\n",
      "14 48\n",
      "13 46\n",
      "14 48\n",
      "13 47\n",
      "13 47\n",
      "beta_layer=8\n",
      "14 47\n",
      "15 47\n",
      "13 48\n",
      "15 48\n",
      "15 47\n",
      "14 46\n",
      "15 48\n",
      "13 47\n",
      "beta_layer=9\n",
      "13 47\n",
      "13 47\n",
      "13 46\n",
      "14 47\n",
      "15 47\n",
      "14 47\n",
      "14 48\n",
      "14 46\n",
      "beta_layer=10\n",
      "14 48\n",
      "13 47\n",
      "14 48\n",
      "14 47\n",
      "13 47\n",
      "15 45\n",
      "14 47\n",
      "14 48\n",
      "beta_layer=11\n",
      "11 47\n",
      "15 47\n",
      "14 47\n",
      "13 46\n",
      "13 46\n",
      "13 47\n",
      "12 47\n",
      "14 47\n",
      "beta_layer=12\n",
      "12 47\n",
      "14 48\n",
      "15 47\n",
      "13 47\n",
      "14 48\n",
      "15 47\n",
      "14 48\n",
      "13 48\n",
      "beta_layer=13\n",
      "13 48\n",
      "14 46\n",
      "13 47\n",
      "15 47\n",
      "14 48\n",
      "14 47\n",
      "13 48\n",
      "13 47\n",
      "beta_layer=14\n",
      "2 46\n",
      "2 47\n",
      "2 47\n",
      "2 48\n",
      "2 47\n",
      "2 47\n",
      "2 47\n",
      "2 47\n",
      "beta_layer=15\n",
      "25 48\n",
      "24 47\n",
      "24 47\n",
      "24 47\n",
      "24 46\n",
      "24 47\n",
      "25 48\n",
      "25 46\n",
      "beta_layer=16\n",
      "13 47\n",
      "15 46\n",
      "12 47\n",
      "15 47\n",
      "13 48\n",
      "15 47\n",
      "14 47\n",
      "13 47\n",
      "beta_layer=17\n",
      "15 46\n",
      "13 47\n",
      "14 47\n",
      "13 48\n",
      "13 47\n",
      "14 47\n",
      "11 48\n",
      "14 48\n",
      "beta_layer=18\n",
      "15 47\n",
      "13 47\n",
      "15 48\n",
      "14 47\n"
     ]
    }
   ],
   "source": [
    "#test the approximator\n",
    "json_path = 'qapprox/antonyms-binary.json'\n",
    "\n",
    "DEFAULT_N_ICL = 8 \n",
    "N_TRIALS = 8\n",
    "\n",
    "with open(json_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "    relation = Relation.from_dict(data)\n",
    "    prompt = \"The opposite of {} is\"\n",
    "    \n",
    "    #counts_by_lre_correct: dict[bool, int] = defaultdict(int)\n",
    "    prompt_template = relation.prompt_templates[0]\n",
    "    #TEST LRE ON LM CORRECT\n",
    "    beta = 2.75\n",
    "    for beta_layer in range(5,27):\n",
    "        print(f'{beta_layer=}')\n",
    "        for _ in range(0,N_TRIALS):\n",
    "            #RELATION SAMPLES\n",
    "            clozed_prompts = []\n",
    "            clozed_answers = []\n",
    "            for x in relation.samples:\n",
    "                samples = [x] + random.sample(relation.samples, DEFAULT_N_ICL - 1)\n",
    "                #print(f'{samples} samples)')\n",
    "                cloze_prompt = functional.make_prompt(\n",
    "                    template = prompt_template, \n",
    "                    target = x,\n",
    "                    examples = samples\n",
    "                    )\n",
    "                clozed_prompts.append(cloze_prompt)\n",
    "                clozed_answers.append(x.object)\n",
    "            #LM PREDICTION\n",
    "            outputs_lm = functional.predict_next_token(mt=mt, prompt=clozed_prompts)\n",
    "            preds_lm =  [[x.token for x in xs] for xs in outputs_lm]\n",
    "            recall_lm = metrics.recall(preds_lm, clozed_answers)\n",
    "            #print(recall_lm)\n",
    "            lre_correct = 0\n",
    "            lm_correct = 0\n",
    "            for _, sample, objs, prompt, preds in zip(range(50), relation.samples, clozed_answers, clozed_prompts, preds_lm):\n",
    "                if (metrics.any_is_nontrivial_prefix(\n",
    "                    predictions=preds, \n",
    "                    targets=objs)):\n",
    "                    #print(f'{sample.subject}', end='')\n",
    "                    hs = get_hidden_state(mt, prompt, sample.subject, 1) #layer 5\n",
    "                    object_hs = approx_lm(hs, beta, beta_layer)\n",
    "                    lre_preds = get_object(mt, object_hs)[0]\n",
    "                    #print(lre_preds, end='')\n",
    "                    if(metrics.any_is_nontrivial_prefix(predictions=lre_preds, targets=objs)):\n",
    "                        lre_correct += 1\n",
    "                    lm_correct += 1\n",
    "            print(f'{lre_correct} {lm_correct}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e2659a64-b5b5-43cd-b32c-cca6f82b397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for most relations.\n",
    "def is_nontrivial_prefix(prediction: str, target: str) -> bool:\n",
    "    target = target.lower().strip()\n",
    "    prediction = prediction.lower().strip()\n",
    "    # if len(prediction) > 0 and target.startswith(prediction):\n",
    "    #     print(f\"{prediction} matches {target}\")\n",
    "    return len(prediction) > 1 and target.startswith(prediction)\n",
    "\n",
    "def any_is_nontrivial_prefix(prediction, targets) -> bool:\n",
    "    return any(is_nontrivial_prefix(prediction, target) for target in targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ec8e5733-9f9f-4ee1-88d9-aa76ecef4ee3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pairs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (subj, obj) \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpairs\u001b[49m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m beta \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      3\u001b[0m         beta \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pairs' is not defined"
     ]
    }
   ],
   "source": [
    "for (subj, obj) in pairs:\n",
    "    for beta in range(10,50, 1):\n",
    "        beta /= 10\n",
    "        hs = get_hidden_state(mt, subj, 5) #layer 5\n",
    "        object_hs = approx_lm(hs, beta) #beta\n",
    "        pred = get_object(mt, object_hs)[0]\n",
    "        if (any_is_nontrivial_prefix(pred[0], obj)):\n",
    "            print(f\"{subj} matches {pred[0]}: {beta}\")\n",
    "            break\n",
    "    \n",
    "# for (subj, obj) in pairs:\n",
    "#     hs = get_hidden_state(mt, subj, 5)\n",
    "#     object_hs = approx_lm(hs, 2.4)\n",
    "#     print(f'{subj}: {get_object(mt, object_hs)[0]} {obj}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8e63aa-35aa-4574-aeef-909288a286a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get tokens in GPT-J\n",
    "#get the hidden state of them at the last layer (after the 28th layer, or s->o @ 27)\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_hidden_state(mt, subject, h_layer, h=None, k=5):\n",
    "    prompt = f\" {subject}\"\n",
    "    h_index, inputs = functional.find_subject_token_index(\n",
    "        mt = mt, prompt=prompt, subject=subject)\n",
    "    #print(f'h_index is {h_index}, inputs is {inputs}')\n",
    "    [[hs], _] = functional.compute_hidden_states(\n",
    "        mt = mt, layers = [h_layer], inputs = inputs)\n",
    "    #h is hs @ h_layer @ h_index\n",
    "    h = hs[:, h_index]\n",
    "    h = h.to(device)\n",
    "    return h\n",
    "    \n",
    "#Spaces are converted in a special character (the Ġ ) in the tokenizer prior to BPE splitting\n",
    "#mostly to avoid digesting spaces since the standard BPE algorithm used spaces in its process \n",
    "\n",
    "#all animal encodings are at [-0.4153   2.023   -2.23    ... -0.785    0.06323 -0.1819 ]\n",
    "\n",
    "text = \"our classic pre-baked blueberry pie filled with delicious plump and juicy wild blueberries\"\n",
    "encoded_input = mt.tokenizer(text, return_tensors=\"pt\")\n",
    "token_ids = range(0,50400)\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "tokens = [token.replace(\"Ġ\", \" \") for token in tokens]\n",
    "\n",
    "#this is too slow and not useful.\n",
    "dict27 = {}\n",
    "for i in tqdm(range(len(tokens))):\n",
    "    token = tokens[i]\n",
    "    dict27[token] = get_hidden_state(mt, token, 27)\n",
    "    \n",
    "with open('animal_youth_27.pkl', 'wb') as file:\n",
    "    pickle.dump(dict27, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "c2b5d1c3-4f38-45ba-8664-9e535d0cd653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  (1): Linear(in_features=4096, out_features=50400, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11195a8e-d798-4b60-8884-3c5ef6d7bc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' puppy', ' pup', ' p', ' dog', ' �']\n",
      "[' duck', ' dra', ' g', ' �', ' \"']\n",
      "[' fry', ' prog', ' F', ' �', ' lar']\n",
      "[' fo', ' col', ' horse', ' pony', ' �']\n",
      "[' kit', ' m', ' \"', ' �', ' p']\n",
      "[' seal', ' \"', ' �', ' pup', ' p']\n",
      "[' shark', ' \"', ' �', ' p', ' pup']\n",
      "[' fry', ' trout', ' \"', ' �', ' rainbow']\n"
     ]
    }
   ],
   "source": [
    "animals = [\"dog\", \"duck\", \"fish\", \"horse\", \"mink\", \"seal\", \"shark\", \"trout\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db937003-748c-465b-a853-4ec53556ec7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
