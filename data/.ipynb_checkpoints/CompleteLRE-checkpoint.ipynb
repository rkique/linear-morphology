{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c4d83fb-81af-4c29-8b65-0f104333a540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exia/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "from transformers import GPTJForCausalLM, AutoTokenizer\n",
    "import lre.models as models\n",
    "import lre.functional as functional\n",
    "import os\n",
    "\n",
    "import json\n",
    "import random\n",
    "from lre.data import Relation, RelationSample, Sequence\n",
    "import lre.metrics as metrics\n",
    "import lre.functional as functional\n",
    "\n",
    "device = \"cuda:1\"\n",
    "weights = []\n",
    "biases = []\n",
    "subjects = []\n",
    "model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", revision=\"float16\", torch_dtype=torch.float16, low_cpu_mem_usage=True)\n",
    "model.to('cuda:1')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "mt = models.ModelAndTokenizer(model,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e40c064c-c98e-4ebb-b4fa-0ab09a073297",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Let's still try averaging the w/b for each layer, because that seems the most intuitive.\n",
    "\n",
    "def sample_weights_biases(subject, kind, i, samples) -> dict:\n",
    "    layer_dict = {\"i\": i}\n",
    "    weights = []\n",
    "    biases = []\n",
    "    weight_path = f\"{wdir}/{subject}/{kind}_weight_h_{i}.pt\"\n",
    "    bias_path = f\"{wdir}/{subject}/{kind}_bias_h_{i}.pt\"\n",
    "    #load s_s_weight and s_s_bias\n",
    "    weight = torch.load(weight_path).to(device)\n",
    "    bias = torch.load(bias_path).to(device)\n",
    "    layer_dict[f'{kind}_weight'] = weight\n",
    "    layer_dict[f'{kind}_bias'] = bias\n",
    "    return layer_dict\n",
    "    \n",
    "def mean_weights_biases(kind, i, samples) -> dict:\n",
    "    layer_dict = {\"i\": i}\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for sample in samples:\n",
    "        weight_path = f\"{wdir}/{sample}/{kind}_weight_{i}_{i+1}.pt\"\n",
    "        bias_path = f\"{wdir}/{sample}/{kind}_bias_{i}_{i+1}.pt\"\n",
    "        #load s_s_weight and s_s_bias\n",
    "        weight = torch.load(weight_path).to(device)\n",
    "        bias = torch.load(bias_path).to(device)\n",
    "        #print(f'weight is {tp(weight)}')\n",
    "        #append to lists\n",
    "        weights.append(weight)\n",
    "        biases.append(bias)\n",
    "    mean_weight = torch.stack(weights).mean(dim=0).to(device)\n",
    "    mean_bias = torch.stack(biases).mean(dim=0).to(device)\n",
    "    sum_weight = torch.stack(weights).sum(dim=0).to(device)\n",
    "    sum_bias = torch.stack(biases).sum(dim=0).to(device)\n",
    "    \n",
    "    layer_dict[f'{kind}_mean_weight'] = mean_weight\n",
    "    layer_dict[f'{kind}_mean_bias'] = mean_bias\n",
    "    layer_dict[f'{kind}_sum_weight'] = sum_weight\n",
    "    layer_dict[f'{kind}_sum_bias'] = sum_bias\n",
    "    \n",
    "    return layer_dict\n",
    "\n",
    "#for retrieiving individual weights or biases (ex: building an LRE)\n",
    "def mean_weight_or_bias(kind, samples):\n",
    "    weights = []\n",
    "    for sample in samples:\n",
    "        weight_path = f\"{sample}/{kind}.pt\"\n",
    "        #load s_s_weight and s_s_bias\n",
    "        weight = torch.load(weight_path).to(device)\n",
    "        #append to lists\n",
    "        weights.append(weight)\n",
    "    mean_weight = torch.stack(weights).mean(dim=0).to(device)\n",
    "    return mean_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c89eaaa6-e350-401a-80fb-bef421974cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " nocontext\n",
      "'noun - plural_irreg_bias_0_132010.pt'\n",
      "'noun - plural_irreg_bias_1_132014.pt'\n",
      "'noun - plural_irreg_bias_2_132018.pt'\n",
      "'noun - plural_irreg_bias_3_132022.pt'\n",
      "'noun - plural_irreg_bias_4_132026.pt'\n",
      "'noun - plural_irreg_bias_5_132030.pt'\n",
      "'noun - plural_irreg_bias_6_132034.pt'\n",
      "'noun - plural_irreg_bias_7_132038.pt'\n",
      "'noun - plural_irreg_weight_0_132010.pt'\n",
      "'noun - plural_irreg_weight_1_132014.pt'\n",
      "'noun - plural_irreg_weight_2_132018.pt'\n",
      "'noun - plural_irreg_weight_3_132022.pt'\n",
      "'noun - plural_irreg_weight_4_132026.pt'\n",
      "'noun - plural_irreg_weight_5_132030.pt'\n",
      "'noun - plural_irreg_weight_6_132034.pt'\n",
      "'noun - plural_irreg_weight_7_132038.pt'\n",
      " reg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bebdca32-b2da-4046-a0ad-8b1d7ee6bd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#An old LRE loading script for the weights in 'approx'\n",
    "\n",
    "weight_str = 'noun - plural_irreg_weight'\n",
    "bias_str = 'noun - plural_irreg_bias'\n",
    "\n",
    "wdir = \".\"\n",
    "\n",
    "weight_paths = [f for f in os.listdir(wdir) if f.startswith(weight_str)]\n",
    "bias_paths = [f for f in os.listdir(wdir) if f.startswith(bias_str)]\n",
    "\n",
    "for weight_str, bias_str in zip(weight_paths, bias_paths):\n",
    "    weight = torch.load(wdir + \"/\" + weight_str).to(device)\n",
    "    bias = torch.load(wdir + \"/\" + bias_str).to(device)\n",
    "    weights.append(weight)\n",
    "    biases.append(bias)\n",
    "    \n",
    "lre_weight = torch.stack(weights).mean(dim=0).to(device)\n",
    "lre_bias = torch.stack(biases).mean(dim=0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70a81114-f8bf-4ed6-8eb1-29bee0104bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from baukit.baukit import parameter_names, get_parameter\n",
    "\n",
    "#returns weight and bias for lns\n",
    "def get_layer_norm_params(model, start, end):\n",
    "    layer_norm_params = {}\n",
    "    for i in range(start, end):\n",
    "        w_name = f'transformer.h.{i}.ln_1.weight'\n",
    "        b_name = f'transformer.h.{i}.ln_1.bias'\n",
    "        weight = get_parameter(model=model,name=w_name).data.to(device)\n",
    "        bias = get_parameter(model=model,name=b_name).data.to(device)\n",
    "        layer_norm_params[w_name] = weight.to(device)\n",
    "        layer_norm_params[b_name] = bias.to(device)\n",
    "        \n",
    "    ln_f_w_name = 'transformer.ln_f.weight'\n",
    "    ln_f_b_name = 'transformer.ln_f.bias'\n",
    "    weight = get_parameter(model=model,name=ln_f_w_name).data.to(device)\n",
    "    bias = get_parameter(model=model,name=ln_f_b_name).data.to(device)\n",
    "    layer_norm_params[ln_f_w_name] = weight.to(device)\n",
    "    layer_norm_params[ln_f_b_name] = bias.to(device)\n",
    "    return layer_norm_params\n",
    "\n",
    "#we should add 1 to the layer ct.\n",
    "#layers 5-27 out of 0-27\n",
    "params = get_layer_norm_params(model,4,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6bc1de8f-ee1f-4e25-9e51-4ba4ccd2d310",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_O_LAYER = 15\n",
    "START_LAYER = 5\n",
    "END_LAYER = 27\n",
    "beta = 2.75\n",
    "wdir = 'capprox/plural'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "85323536-8faa-4685-8b9d-b1f4e4768ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather relevant weights and biases #layers: 0-27\n",
    "samples = ['application', 'difference', 'event', 'idea', 'population', 'resource', 'student', 'town']\n",
    "#Consistent with previous results, from the early layers s' seems to represent o.\n",
    "#data/capprox/[verb_inf - Ved]/accept/s_s_weight_5_6.pt \n",
    "\n",
    "layer_dicts = []\n",
    "### S --> S'\n",
    "for i in range(START_LAYER, S_O_LAYER):\n",
    "    #layer_dict = sample_weights_biases(animal,\"s_s\", i, animals)\n",
    "    layer_dict = mean_weights_biases(\"s_s\", i, samples)\n",
    "    layer_dicts.append(layer_dict)\n",
    "\n",
    "#### S' --> O\n",
    "for i in range(S_O_LAYER,S_O_LAYER+1):\n",
    "    #layer_dict = sample_weights_biases(animal,\"s_o\", i, animals)\n",
    "    layer_dict = mean_weights_biases(\"s_o\", i, samples)\n",
    "    layer_dicts.append(layer_dict)\n",
    "\n",
    "### O --> O'\n",
    "for i in range(S_O_LAYER+1, END_LAYER):\n",
    "    #layer_dict = sample_weights_biases(animal,\"o_o\", i, animals)\n",
    "    layer_dict = mean_weights_biases(\"o_o\", i, samples)\n",
    "    layer_dicts.append(layer_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "36b5d1c0-9673-421a-b982-59283ce5db22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mt.lm_head applies a linear map to get the token-space (50400)\n",
    "# (1,4096) -layernorm, linear-> (1,50400) -softmax-> (1,50400) -topk-> (1,5)\n",
    "def get_object(mt, z, k=5):\n",
    "    logits = mt.lm_head(z)\n",
    "    dist = torch.softmax(logits.float(), dim=-1)\n",
    "    topk = dist.topk(k=k, dim=-1)\n",
    "    probs = topk.values.view(5).tolist()\n",
    "    token_ids = topk.indices.view(5).tolist()\n",
    "    words = [mt.tokenizer.decode(token_id) for token_id in token_ids]\n",
    "    return (words, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b8d4f181-c3a5-43c5-8b89-29dff682cd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(\n",
    "    x: torch.Tensor, dim, eps: float = 0.00001\n",
    ") -> torch.Tensor:\n",
    "    mean = torch.mean(x, dim=dim, keepdim=True)\n",
    "    var = torch.square(x - mean).mean(dim=dim, keepdim=True)\n",
    "    return (x - mean) / torch.sqrt(var + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e179255d-71e3-4327-af5f-9e3a5489a5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_dict(i):\n",
    "    return next((item for item in layer_dicts if item['i'] == i), None)\n",
    "\n",
    "def approx_s_s_layer(hs, i, beta=1):\n",
    "    layer_dict = get_layer_dict(i)\n",
    "    layer_weight = layer_dict['s_s_mean_weight']\n",
    "    layer_bias = layer_dict['s_s_mean_bias']\n",
    "    ln_weight = params[f'transformer.h.{i}.ln_1.weight']\n",
    "    ln_bias = params[f'transformer.h.{i}.ln_1.bias']\n",
    "    _hs = hs\n",
    "\n",
    "    hs = layer_norm(hs, (1)) * ln_weight + ln_bias\n",
    "    hs = beta * hs.mm(layer_weight.t()) #+ layer_bias\n",
    "    #hs = hs + _hs\n",
    "    \n",
    "    return hs\n",
    "\n",
    "def approx_s_o_layer(hs, i, beta=1):\n",
    "    layer_dict = get_layer_dict(i)\n",
    "    layer_weight = layer_dict['s_o_mean_weight']\n",
    "    layer_bias = layer_dict['s_o_mean_bias']  \n",
    "    ln_weight = params[f'transformer.h.{i}.ln_1.weight']\n",
    "    ln_bias = params[f'transformer.h.{i}.ln_1.bias']\n",
    "    _hs = hs\n",
    "    hs = layer_norm(hs, (1)) * ln_weight + ln_bias\n",
    "    #this transformation should encompass the work of the MHSA and MLP layer.\n",
    "    hs = beta * hs.mm(layer_weight.t()) + layer_bias\n",
    "    #hs = hs + _hs\n",
    "    \n",
    "    return hs\n",
    "    \n",
    "def approx_o_o_layer(hs, i, beta=1):\n",
    "    layer_dict = get_layer_dict(i)\n",
    "    layer_weight = layer_dict['o_o_mean_weight']\n",
    "    layer_bias = layer_dict['o_o_mean_bias']  \n",
    "    ln_weight = params[f'transformer.h.{i}.ln_1.weight']\n",
    "    ln_bias = params[f'transformer.h.{i}.ln_1.bias']\n",
    "    _hs = hs\n",
    "    \n",
    "    hs = layer_norm(hs, (1)) * ln_weight + ln_bias\n",
    "    hs = beta * hs.mm(layer_weight.t()) #+ layer_bias\n",
    "    #hs = hs + _hs\n",
    "    return hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ff713e46-75f6-48fa-9f2b-da9d25f8ae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we want to do h' = h.mm(weight.t()) * beta + bias for each layer 5-26 (s -> s')\n",
    "#then, finally z = h.mm(weight.t()) * beta + bias (s' -> o)\n",
    "\n",
    "def tp(state: torch.Tensor):\n",
    "    return state.cpu().detach().numpy()[0]\n",
    "\n",
    "def approx_lm(hs, beta, beta_layer):\n",
    "    \n",
    "    #apply s_s\n",
    "    for i in range(START_LAYER, S_O_LAYER):\n",
    "        if i == beta_layer:\n",
    "            hs = approx_s_s_layer(hs, i, beta)\n",
    "        else:\n",
    "            hs = approx_s_s_layer(hs,i)\n",
    "        \n",
    "    #apply s_o\n",
    "    for i in range(S_O_LAYER, S_O_LAYER +1):\n",
    "        if i == beta_layer:\n",
    "            hs = approx_s_o_layer(hs, i, beta)\n",
    "        else:\n",
    "            hs = approx_s_o_layer(hs, i)\n",
    "        \n",
    "    #apply o_o\n",
    "    for i in range(S_O_LAYER + 1, END_LAYER):\n",
    "        if i == beta_layer:\n",
    "            hs = approx_o_o_layer(hs, i, beta)\n",
    "        else:\n",
    "            hs = approx_o_o_layer(hs, i)\n",
    "    \n",
    "    ln_weight = params['transformer.ln_f.weight']\n",
    "    ln_bias = params['transformer.ln_f.bias']\n",
    "    #hs = layer_norm(hs, (1)) * ln_weight + ln_bias\n",
    "        \n",
    "    return hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0167f0c2-4b92-43ee-aacc-be24ef75f56c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'qapprox/antonyms-binary.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m DEFAULT_N_ICL \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m \n\u001b[1;32m      6\u001b[0m N_TRIALS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjson_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      9\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[1;32m     10\u001b[0m     relation \u001b[38;5;241m=\u001b[39m Relation\u001b[38;5;241m.\u001b[39mfrom_dict(data)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'qapprox/antonyms-binary.json'"
     ]
    }
   ],
   "source": [
    "### LRE vs Complete LRE\n",
    "\n",
    "json_path = 'capprox/antonyms-binary.json'\n",
    "\n",
    "DEFAULT_N_ICL = 8 \n",
    "N_TRIALS = 8\n",
    "\n",
    "with open(json_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "    relation = Relation.from_dict(data)\n",
    "    prompt = \"The opposite of {} is\"\n",
    "    \n",
    "    #counts_by_lre_correct: dict[bool, int] = defaultdict(int)\n",
    "    prompt_template = relation.prompt_templates[0]\n",
    "    \n",
    "    #TEST LRE ON LM CORRECT\n",
    "    betas = [i**2 for i in range(0,10)]\n",
    "    for beta in betas:\n",
    "        beta_layer = S_O_LAYER\n",
    "        #RELATION SAMPLES\n",
    "        clozed_prompts = []\n",
    "        clozed_answers = []\n",
    "        for x in relation.samples:\n",
    "            samples = [x] + random.sample(relation.samples, DEFAULT_N_ICL - 1)\n",
    "            #print(f'{samples} samples)')\n",
    "            cloze_prompt = functional.make_prompt(\n",
    "                template = prompt_template, \n",
    "                target = x,\n",
    "                examples = samples\n",
    "                )\n",
    "            clozed_prompts.append(cloze_prompt)\n",
    "            clozed_answers.append(x.object)\n",
    "            \n",
    "        outputs_lm = functional.predict_next_token(mt=mt, prompt=clozed_prompts)\n",
    "        preds_lm =  [[x.token for x in xs] for xs in outputs_lm]\n",
    "        recall_lm = metrics.recall(preds_lm, clozed_answers)\n",
    "        \n",
    "        lre_correct = 0\n",
    "        clre_correct = 0\n",
    "        lm_correct = 0\n",
    "        \n",
    "        for i, sample, objs, prompt, preds in zip(range(50), relation.samples, clozed_answers, clozed_prompts, preds_lm):\n",
    "            if (metrics.any_is_nontrivial_prefix(\n",
    "                predictions=preds, \n",
    "                targets=objs)):\n",
    "                hs = get_hidden_state(mt, prompt, sample.subject, 1) #layer 5\n",
    "    \n",
    "                #use local LRE\n",
    "                clre_object_hs = approx_lm(hs, beta, beta_layer)\n",
    "                clre_preds = get_object(mt, clre_object_hs)[0]\n",
    "                \n",
    "                #use simple LRE\n",
    "                lre_object_hs = hs.mm(lre_weight.t()) * beta + lre_bias\n",
    "                lre_preds = get_object(mt, lre_object_hs)[0]\n",
    "                \n",
    "                if(metrics.any_is_nontrivial_prefix(predictions=clre_preds, targets=objs)):\n",
    "                    clre_correct += 1\n",
    "    \n",
    "                if(metrics.any_is_nontrivial_prefix(predictions=lre_preds, targets=objs)):\n",
    "                    lre_correct += 1\n",
    "                    \n",
    "                if(i < 10):\n",
    "                    print(f'{sample.subject} {preds[0]} {clre_preds} {lre_preds}')\n",
    "                    \n",
    "                lm_correct += 1\n",
    "        print(f'beta,{beta},clre,{clre_correct},lre,{lre_correct},lm,{lm_correct}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "713dd09f-302b-4283-9e94-5429cc036395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_state(mt, prompt, subject, h_layer):\n",
    "    prompt = prompt.format(subject)\n",
    "    h_index, inputs = functional.find_subject_token_index(\n",
    "        mt = mt, prompt=prompt, subject=subject)\n",
    "    #print(f'h_index is {h_index}, inputs is {inputs}')\n",
    "    [[hs], _] = functional.compute_hidden_states(\n",
    "        mt = mt, layers = [h_layer], inputs = inputs)\n",
    "    #h is hs @ h_layer @ h_index\n",
    "    h = hs[:, h_index]\n",
    "    h = h.to(device)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "c5bb561d-c1c1-4753-958d-5070a6735814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lre.functional as functional\n",
    "from importlib import reload\n",
    "reload(functional)\n",
    "import lre.functional as functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "3974add2-2c03-4b01-a71a-3762339fa440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_layer=5\n",
      "15 47\n",
      "14 47\n",
      "15 47\n",
      "16 47\n",
      "17 47\n",
      "15 48\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[439], line 37\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, sample, objs, prompt, preds \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m), relation\u001b[38;5;241m.\u001b[39msamples, clozed_answers, clozed_prompts, preds_lm):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (metrics\u001b[38;5;241m.\u001b[39many_is_nontrivial_prefix(\n\u001b[1;32m     35\u001b[0m         predictions\u001b[38;5;241m=\u001b[39mpreds, \n\u001b[1;32m     36\u001b[0m         targets\u001b[38;5;241m=\u001b[39mobjs)):\n\u001b[0;32m---> 37\u001b[0m         hs \u001b[38;5;241m=\u001b[39m \u001b[43mget_hidden_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubject\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#layer 5\u001b[39;00m\n\u001b[1;32m     38\u001b[0m         object_hs \u001b[38;5;241m=\u001b[39m approx_lm(hs, beta, beta_layer)\n\u001b[1;32m     39\u001b[0m         lre_preds \u001b[38;5;241m=\u001b[39m get_object(mt, object_hs)[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[434], line 6\u001b[0m, in \u001b[0;36mget_hidden_state\u001b[0;34m(mt, prompt, subject, h_layer)\u001b[0m\n\u001b[1;32m      3\u001b[0m h_index, inputs \u001b[38;5;241m=\u001b[39m functional\u001b[38;5;241m.\u001b[39mfind_subject_token_index(\n\u001b[1;32m      4\u001b[0m     mt \u001b[38;5;241m=\u001b[39m mt, prompt\u001b[38;5;241m=\u001b[39mprompt, subject\u001b[38;5;241m=\u001b[39msubject)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#print(f'h_index is {h_index}, inputs is {inputs}')\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m [[hs], _] \u001b[38;5;241m=\u001b[39m \u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_hidden_states\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mh_layer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#h is hs @ h_layer @ h_index\u001b[39;00m\n\u001b[1;32m      9\u001b[0m h \u001b[38;5;241m=\u001b[39m hs[:, h_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my-lre/data/wapprox/../../lre/functional.py:94\u001b[0m, in \u001b[0;36mcompute_hidden_states\u001b[0;34m(mt, layers, prompt, inputs, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m layer_paths \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mdetermine_layer_paths(mt, layers\u001b[38;5;241m=\u001b[39mlayers, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TraceDict(mt\u001b[38;5;241m.\u001b[39mmodel, layer_paths\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mas\u001b[39;00m ret:\n\u001b[0;32m---> 94\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m#Assemble hidden state outputs from the TraceDict.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m hiddens \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gptj/modeling_gptj.py:1124\u001b[0m, in \u001b[0;36mGPTJForCausalLM.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1124\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1137\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gptj/modeling_gptj.py:950\u001b[0m, in \u001b[0;36mGPTJModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    939\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    940\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    941\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    947\u001b[0m         output_attentions,\n\u001b[1;32m    948\u001b[0m     )\n\u001b[1;32m    949\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 950\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    960\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gptj/modeling_gptj.py:578\u001b[0m, in \u001b[0;36mGPTJBlock.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, position_ids, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    576\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    577\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 578\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    587\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    588\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gptj/modeling_gptj.py:251\u001b[0m, in \u001b[0;36mGPTJAttention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, position_ids, head_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    248\u001b[0m q_pass \u001b[38;5;241m=\u001b[39m query[:, :, :, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_dim :]\n\u001b[1;32m    250\u001b[0m k_rot \u001b[38;5;241m=\u001b[39m apply_rotary_pos_emb(k_rot, sin, cos)\n\u001b[0;32m--> 251\u001b[0m q_rot \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rotary_pos_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_rot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m key \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([k_rot, k_pass], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    254\u001b[0m query \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([q_rot, q_pass], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/gptj/modeling_gptj.py:92\u001b[0m, in \u001b[0;36mapply_rotary_pos_emb\u001b[0;34m(tensor, sin, cos)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_rotary_pos_emb\u001b[39m(tensor: torch\u001b[38;5;241m.\u001b[39mTensor, sin: torch\u001b[38;5;241m.\u001b[39mTensor, cos: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 92\u001b[0m     sin \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat_interleave\u001b[49m\u001b[43m(\u001b[49m\u001b[43msin\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     cos \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrepeat_interleave(cos[:, :, \u001b[38;5;28;01mNone\u001b[39;00m, :], \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (tensor \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (rotate_every_two(tensor) \u001b[38;5;241m*\u001b[39m sin)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### LRE BETA INSERTION POSITION\n",
    "json_path = 'qapprox/antonyms-binary.json'\n",
    "\n",
    "DEFAULT_N_ICL = 8 \n",
    "N_TRIALS = 8\n",
    "\n",
    "with open(json_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "    relation = Relation.from_dict(data)\n",
    "    prompt = \"The opposite of {} is\"\n",
    "    prompt_template = relation.prompt_templates[0]\n",
    "    beta = 2.75\n",
    "    for beta_layer in range(5,27):\n",
    "        print(f'{beta_layer=}')\n",
    "        for _ in range(0,N_TRIALS):\n",
    "            clozed_prompts = []\n",
    "            clozed_answers = []\n",
    "            for x in relation.samples:\n",
    "                samples = [x] + random.sample(relation.samples, DEFAULT_N_ICL - 1)\n",
    "                cloze_prompt = functional.make_prompt(\n",
    "                    template = prompt_template, \n",
    "                    target = x,\n",
    "                    examples = samples\n",
    "                    )\n",
    "                clozed_prompts.append(cloze_prompt)\n",
    "                clozed_answers.append(x.object)\n",
    "            outputs_lm = functional.predict_next_token(mt=mt, prompt=clozed_prompts)\n",
    "            preds_lm =  [[x.token for x in xs] for xs in outputs_lm]\n",
    "            recall_lm = metrics.recall(preds_lm, clozed_answers)\n",
    "            lre_correct = 0\n",
    "            lm_correct = 0\n",
    "            \n",
    "            for _, sample, objs, prompt, preds in zip(range(50), relation.samples, clozed_answers, clozed_prompts, preds_lm):\n",
    "                if (metrics.any_is_nontrivial_prefix(\n",
    "                    predictions=preds, \n",
    "                    targets=objs)):\n",
    "                    hs = get_hidden_state(mt, prompt, sample.subject, 1) #layer 5\n",
    "                    object_hs = approx_lm(hs, beta, beta_layer)\n",
    "                    lre_preds = get_object(mt, object_hs)[0]\n",
    "                    if(metrics.any_is_nontrivial_prefix(predictions=lre_preds, targets=objs)):\n",
    "                        lre_correct += 1\n",
    "                    lm_correct += 1\n",
    "                    \n",
    "            print(f'{lre_correct} {lm_correct}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e2659a64-b5b5-43cd-b32c-cca6f82b397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for most relations.\n",
    "def is_nontrivial_prefix(prediction: str, target: str) -> bool:\n",
    "    target = target.lower().strip()\n",
    "    prediction = prediction.lower().strip()\n",
    "    # if len(prediction) > 0 and target.startswith(prediction):\n",
    "    #     print(f\"{prediction} matches {target}\")\n",
    "    return len(prediction) > 1 and target.startswith(prediction)\n",
    "\n",
    "def any_is_nontrivial_prefix(prediction, targets) -> bool:\n",
    "    return any(is_nontrivial_prefix(prediction, target) for target in targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ec8e5733-9f9f-4ee1-88d9-aa76ecef4ee3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pairs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (subj, obj) \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpairs\u001b[49m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m beta \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      3\u001b[0m         beta \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pairs' is not defined"
     ]
    }
   ],
   "source": [
    "for (subj, obj) in pairs:\n",
    "    for beta in range(10,50, 1):\n",
    "        beta /= 10\n",
    "        hs = get_hidden_state(mt, subj, 5) #layer 5\n",
    "        object_hs = approx_lm(hs, beta) #beta\n",
    "        pred = get_object(mt, object_hs)[0]\n",
    "        if (any_is_nontrivial_prefix(pred[0], obj)):\n",
    "            print(f\"{subj} matches {pred[0]}: {beta}\")\n",
    "            break\n",
    "    \n",
    "# for (subj, obj) in pairs:\n",
    "#     hs = get_hidden_state(mt, subj, 5)\n",
    "#     object_hs = approx_lm(hs, 2.4)\n",
    "#     print(f'{subj}: {get_object(mt, object_hs)[0]} {obj}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8e63aa-35aa-4574-aeef-909288a286a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get tokens in GPT-J\n",
    "#get the hidden state of them at the last layer (after the 28th layer, or s->o @ 27)\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_hidden_state(mt, subject, h_layer, h=None, k=5):\n",
    "    prompt = f\" {subject}\"\n",
    "    h_index, inputs = functional.find_subject_token_index(\n",
    "        mt = mt, prompt=prompt, subject=subject)\n",
    "    #print(f'h_index is {h_index}, inputs is {inputs}')\n",
    "    [[hs], _] = functional.compute_hidden_states(\n",
    "        mt = mt, layers = [h_layer], inputs = inputs)\n",
    "    #h is hs @ h_layer @ h_index\n",
    "    h = hs[:, h_index]\n",
    "    h = h.to(device)\n",
    "    return h\n",
    "    \n",
    "#Spaces are converted in a special character (the  ) in the tokenizer prior to BPE splitting\n",
    "#mostly to avoid digesting spaces since the standard BPE algorithm used spaces in its process \n",
    "\n",
    "#all animal encodings are at [-0.4153   2.023   -2.23    ... -0.785    0.06323 -0.1819 ]\n",
    "\n",
    "text = \"our classic pre-baked blueberry pie filled with delicious plump and juicy wild blueberries\"\n",
    "encoded_input = mt.tokenizer(text, return_tensors=\"pt\")\n",
    "token_ids = range(0,50400)\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "tokens = [token.replace(\"\", \" \") for token in tokens]\n",
    "\n",
    "#this is too slow and not useful.\n",
    "dict27 = {}\n",
    "for i in tqdm(range(len(tokens))):\n",
    "    token = tokens[i]\n",
    "    dict27[token] = get_hidden_state(mt, token, 27)\n",
    "    \n",
    "with open('animal_youth_27.pkl', 'wb') as file:\n",
    "    pickle.dump(dict27, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "c2b5d1c3-4f38-45ba-8664-9e535d0cd653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  (1): Linear(in_features=4096, out_features=50400, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11195a8e-d798-4b60-8884-3c5ef6d7bc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' puppy', ' pup', ' p', ' dog', ' ']\n",
      "[' duck', ' dra', ' g', ' ', ' \"']\n",
      "[' fry', ' prog', ' F', ' ', ' lar']\n",
      "[' fo', ' col', ' horse', ' pony', ' ']\n",
      "[' kit', ' m', ' \"', ' ', ' p']\n",
      "[' seal', ' \"', ' ', ' pup', ' p']\n",
      "[' shark', ' \"', ' ', ' p', ' pup']\n",
      "[' fry', ' trout', ' \"', ' ', ' rainbow']\n"
     ]
    }
   ],
   "source": [
    "animals = [\"dog\", \"duck\", \"fish\", \"horse\", \"mink\", \"seal\", \"shark\", \"trout\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db937003-748c-465b-a853-4ec53556ec7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
