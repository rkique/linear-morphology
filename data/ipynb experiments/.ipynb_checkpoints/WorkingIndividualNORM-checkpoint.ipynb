{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7a601fb-ab81-42ad-b17a-2e4743812f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exia/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "from transformers import GPTJForCausalLM, AutoTokenizer\n",
    "import lre.models as models\n",
    "import lre.functional as functional\n",
    "import os\n",
    "\n",
    "device = \"cuda:1\"\n",
    "weights = []\n",
    "biases = []\n",
    "subjects = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e40c064c-c98e-4ebb-b4fa-0ab09a073297",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Let's try averaging the w/b for each layer, because that seems the most intuitive.\n",
    "#First attempt: s --> s for 5-26, s --> o for 26-27.\n",
    "\n",
    "animals = [\"dog\", \"duck\", \"fish\", \"horse\", \"mink\", \"seal\", \"shark\", \"trout\"]\n",
    "\n",
    "weight_str = 's_s_weight_'\n",
    "bias_str = 's_s_bias_'\n",
    "\n",
    "def sample_weights_biases(subject, kind, i, samples) -> dict:\n",
    "    layer_dict = {\"i\": i}\n",
    "    weights = []\n",
    "    biases = []\n",
    "    wdir = subject\n",
    "    weight_path = f\"{wdir}/{kind}_weight_h_{i}.pt\"\n",
    "    bias_path = f\"{wdir}/{kind}_bias_h_{i}.pt\"\n",
    "    #load s_s_weight and s_s_bias\n",
    "    weight = torch.load(weight_path).to(device)\n",
    "    bias = torch.load(bias_path).to(device)\n",
    "    layer_dict[f'{kind}_weight'] = weight\n",
    "    layer_dict[f'{kind}_bias'] = bias\n",
    "    return layer_dict\n",
    "    \n",
    "def mean_weights_biases(kind, i, samples) -> dict:\n",
    "    layer_dict = {\"i\": i}\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for sample in samples:\n",
    "        wdir = sample\n",
    "        weight_path = f\"{wdir}/{kind}_weight_h_{i}.pt\"\n",
    "        bias_path = f\"{wdir}/{kind}_bias_h_{i}.pt\"\n",
    "        #load s_s_weight and s_s_bias\n",
    "        weight = torch.load(weight_path).to(device)\n",
    "        bias = torch.load(bias_path).to(device)\n",
    "        #append to lists\n",
    "        weights.append(weight)\n",
    "        biases.append(bias)\n",
    "    mean_weight = torch.stack(weights).mean(dim=0).to(device)\n",
    "    mean_bias = torch.stack(biases).mean(dim=0).to(device)\n",
    "    layer_dict[f'{kind}_weight'] = mean_weight\n",
    "    layer_dict[f'{kind}_bias'] = mean_bias\n",
    "    return layer_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa0283a6-e687-4f7b-8bc0-67989aa9d18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", revision=\"float16\", torch_dtype=torch.float16, low_cpu_mem_usage=True)\n",
    "model.to('cuda:1')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "mt = models.ModelAndTokenizer(model,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70a81114-f8bf-4ed6-8eb1-29bee0104bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from baukit.baukit import parameter_names, get_parameter\n",
    "\n",
    "#returns weight and bias for lns\n",
    "def get_layer_norm_params(model, start, end):\n",
    "    layer_norm_params = {}\n",
    "    for i in range(start, end):\n",
    "        w_name = f'transformer.h.{i}.ln_1.weight'\n",
    "        b_name = f'transformer.h.{i}.ln_1.bias'\n",
    "        weight = get_parameter(model=model,name=w_name).data.to(device)\n",
    "        bias = get_parameter(model=model,name=b_name).data.to(device)\n",
    "        layer_norm_params[w_name] = weight.to(device)\n",
    "        layer_norm_params[b_name] = bias.to(device)\n",
    "    return layer_norm_params\n",
    "\n",
    "#we should add 1 to the layer ct.\n",
    "#layers 5-27 out of 0-27\n",
    "params = get_layer_norm_params(model,5,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36b5d1c0-9673-421a-b982-59283ce5db22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lm_head applies LayerNorm and then a linear map to get the token-space (50400)\n",
    "# (1,4096) -layernorm, linear-> (1,50400) -softmax-> (1,50400) -topk-> (1,5)\n",
    "def get_object(mt, z, k=5):\n",
    "    logits = mt.lm_head(z)\n",
    "    dist = torch.softmax(logits.float(), dim=-1)\n",
    "    topk = dist.topk(k=k, dim=-1)\n",
    "    probs = topk.values.view(5).tolist()\n",
    "    token_ids = topk.indices.view(5).tolist()\n",
    "    words = [mt.tokenizer.decode(token_id) for token_id in token_ids]\n",
    "    return (words, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8d4f181-c3a5-43c5-8b89-29dff682cd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(\n",
    "    x: torch.Tensor, dim, eps: float = 0.00001\n",
    ") -> torch.Tensor:\n",
    "    mean = torch.mean(x, dim=dim, keepdim=True)\n",
    "    var = torch.square(x - mean).mean(dim=dim, keepdim=True)\n",
    "    return (x - mean) / torch.sqrt(var + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e179255d-71e3-4327-af5f-9e3a5489a5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approx_s_s_layer(hs, i):\n",
    "    layer_dict = next((item for item in layer_dicts if item['i'] == i), None)\n",
    "    layer_weight, layer_bias = layer_dict['s_s_weight'],layer_dict['s_s_bias']\n",
    "    ln_weight = params[f'transformer.h.{i}.ln_1.weight']\n",
    "    ln_bias = params[f'transformer.h.{i}.ln_1.bias']\n",
    "    _hs = hs\n",
    "    \n",
    "    #perform layer normalization with adaptive w and b\n",
    "    hs = layer_norm(hs, (1)) * ln_weight + ln_bias\n",
    "    \n",
    "    #perform the layer operation\n",
    "    hs = hs.mm(layer_weight.t()) + layer_bias\n",
    "        \n",
    "    #add residual\n",
    "    #hs = hs + _hs\n",
    "    return hs\n",
    "\n",
    "def approx_s_o_layer(hs, i):\n",
    "    layer_dict = next((item for item in layer_dicts if item['i'] == i), None)\n",
    "    layer_weight, layer_bias = layer_dict['s_o_weight'],layer_dict['s_o_bias']  \n",
    "    ln_weight = params[f'transformer.h.{i}.ln_1.weight']\n",
    "    ln_bias = params[f'transformer.h.{i}.ln_1.bias']\n",
    "    _hs = hs\n",
    "    \n",
    "    #perform layer normalization with adaptive w and b\n",
    "    hs = layer_norm(hs, (1)) * ln_weight + ln_bias\n",
    "    \n",
    "    #perform the layer operation\n",
    "    hs = hs.mm(layer_weight.t()) + layer_bias\n",
    "    \n",
    "    #add residual\n",
    "    #hs = hs + _hs\n",
    "    return hs\n",
    "    \n",
    "def approx_o_o_layer(hs, i):\n",
    "    layer_dict = next((item for item in layer_dicts if item['i'] == i), None)\n",
    "    layer_weight, layer_bias = layer_dict['o_o_weight'],layer_dict['o_o_bias']  \n",
    "    ln_weight = params[f'transformer.h.{i}.ln_1.weight']\n",
    "    ln_bias = params[f'transformer.h.{i}.ln_1.bias']\n",
    "    _hs = hs\n",
    "    \n",
    "    #perform layer normalization with adaptive w and b\n",
    "    hs = layer_norm(hs, (1)) * ln_weight + ln_bias\n",
    "    \n",
    "    #perform the layer operation\n",
    "    hs = hs.mm(layer_weight.t()) + layer_bias\n",
    "    \n",
    "    #add residual\n",
    "    #hs = hs + _hs\n",
    "    return hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff713e46-75f6-48fa-9f2b-da9d25f8ae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we want to do h' = h.mm(weight.t()) * beta + bias for each layer 5-26 (s -> s')\n",
    "#then, finally z = h.mm(weight.t()) * beta + bias (s' -> o)\n",
    "\n",
    "def tp(state: torch.Tensor):\n",
    "    return state.cpu().detach().numpy()[0]\n",
    "\n",
    "def approx_lm(hs):\n",
    "    \n",
    "    #apply s_s\n",
    "    for i in range(START_LAYER, S_O_LAYER):\n",
    "        hs = approx_s_s_layer(hs,i)\n",
    "\n",
    "    #apply s_o\n",
    "    for i in range(S_O_LAYER, S_O_LAYER +1):\n",
    "        hs = approx_s_o_layer(hs, i)\n",
    "        \n",
    "    #apply o_o\n",
    "    for i in range(S_O_LAYER + 1, END_LAYER):\n",
    "        hs = approx_o_o_layer(hs, i)\n",
    "        \n",
    "    return hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85323536-8faa-4685-8b9d-b1f4e4768ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog: [' puppy', ' pup', ' p', ' whe', ' dog']\n",
      "duck: [' duck', ' g', ' dra', ' chick', ' hatch']\n",
      "fish: [' fry', ' finger', ' lar', ' baby', ' young']\n",
      "horse: [' fo', ' col', ' fill', ' pony', ' baby']\n",
      "mink: [' kit', ' p', ' pup', ' kits', ' cub']\n",
      "seal: [' pup', ' p', ' puppy', ' baby', ' seal']\n",
      "shark: [' pup', ' baby', ' shark', ' young', ' kit']\n",
      "trout: [' fry', ' finger', ' lar', ' baby', ' trout']\n"
     ]
    }
   ],
   "source": [
    "#start with loading h @ 5 (@ subject index)\n",
    "animals = [\"dog\", \"duck\", \"fish\", \"horse\", \"mink\", \"seal\", \"shark\", \"trout\"]\n",
    "\n",
    "#layers: 0-27\n",
    "START_LAYER = 5\n",
    "S_O_LAYER = 26 #TODO: generate s_o_weight_27\n",
    "END_LAYER = 27\n",
    "\n",
    "#we can reconstruct the LM predictions perfectly with the jacobians of particular animals.\n",
    "for animal in animals:\n",
    "    layer_dicts = []\n",
    "    ### S --> S'\n",
    "    for i in range(START_LAYER, S_O_LAYER):\n",
    "        layer_dict = sample_weights_biases(animal,\"s_s\", i, animals)\n",
    "        #layer_dict = mean_weights_biases(\"s_s\", i, animals)\n",
    "        layer_dicts.append(layer_dict)\n",
    "    \n",
    "    #### S' --> O\n",
    "    #should be referring to the 27 out of 0-27 (last) GPTJBlock\n",
    "    for i in range(S_O_LAYER,S_O_LAYER+1):\n",
    "        layer_dict = sample_weights_biases(animal,\"s_o\", i, animals)\n",
    "        #layer_dict = mean_weights_biases(\"s_o\", i, animals)\n",
    "        layer_dicts.append(layer_dict)\n",
    "    \n",
    "    ### O --> O'\n",
    "    for i in range(S_O_LAYER+1, END_LAYER):\n",
    "        layer_dict = sample_weights_biases(animal,\"o_o\", i, animals)\n",
    "        #layer_dict = mean_weights_biases(\"o_o\", i, animals)\n",
    "        layer_dicts.append(layer_dict)\n",
    "    \n",
    "    animal_hs = torch.load(f'{animal}/hs_h_{START_LAYER}.pt').to(device)[None]\n",
    "    object_hs = approx_lm(animal_hs)\n",
    "    #print(torch.linalg.norm(object_hs, dim=1, ord=2).cpu().detach().numpy())\n",
    "    print(f'{animal}: {get_object(mt, object_hs)[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8e63aa-35aa-4574-aeef-909288a286a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get tokens in GPT-J\n",
    "#get the hidden state of them at the last layer (after the 28th layer, or s->o @ 27)\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_hidden_state(mt, subject, h_layer, h=None, k=5):\n",
    "    prompt = f\" {subject}\"\n",
    "    h_index, inputs = functional.find_subject_token_index(\n",
    "        mt = mt, prompt=prompt, subject=subject)\n",
    "    #print(f'h_index is {h_index}, inputs is {inputs}')\n",
    "    [[hs], _] = functional.compute_hidden_states(\n",
    "        mt = mt, layers = [h_layer], inputs = inputs)\n",
    "    #h is hs @ h_layer @ h_index\n",
    "    h = hs[:, h_index]\n",
    "    h = h.to(device)\n",
    "    return h\n",
    "    \n",
    "#Spaces are converted in a special character (the Ġ ) in the tokenizer prior to BPE splitting\n",
    "#mostly to avoid digesting spaces since the standard BPE algorithm used spaces in its process \n",
    "\n",
    "#all animal encodings are at [-0.4153   2.023   -2.23    ... -0.785    0.06323 -0.1819 ]\n",
    "\n",
    "text = \"our classic pre-baked blueberry pie filled with delicious plump and juicy wild blueberries\"\n",
    "encoded_input = mt.tokenizer(text, return_tensors=\"pt\")\n",
    "token_ids = range(0,50400)\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "tokens = [token.replace(\"Ġ\", \" \") for token in tokens]\n",
    "\n",
    "#this is too slow and not useful.\n",
    "dict27 = {}\n",
    "for i in tqdm(range(len(tokens))):\n",
    "    token = tokens[i]\n",
    "    dict27[token] = get_hidden_state(mt, token, 27)\n",
    "    \n",
    "with open('animal_youth_27.pkl', 'wb') as file:\n",
    "    pickle.dump(dict27, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "c2b5d1c3-4f38-45ba-8664-9e535d0cd653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  (1): Linear(in_features=4096, out_features=50400, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11195a8e-d798-4b60-8884-3c5ef6d7bc12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c191e43-93c5-43c5-8dc4-8773cd9afa30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
