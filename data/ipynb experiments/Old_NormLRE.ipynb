{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce2d6041-30d1-4289-a012-99e10476f16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096, 4096])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2', '3', '5', '6', '0', '1', '7', '4']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "from transformers import GPTJForCausalLM, AutoTokenizer\n",
    "import lre.models as models\n",
    "import lre.functional as functional\n",
    "import os\n",
    "\n",
    "device = \"cuda:1\"\n",
    "weights = []\n",
    "biases = []\n",
    "subjects = []\n",
    "wdir = 'verb+able'\n",
    "weight_str = 'verb+able_reg_weight_'\n",
    "bias_str = 'verb+able_reg_bias_'\n",
    "\n",
    "weight_paths = [f for f in os.listdir(wdir) if f.startswith(weight_str)]\n",
    "bias_paths = [f for f in os.listdir(wdir) if f.startswith(bias_str)]\n",
    "\n",
    "for bias_path, weight_path in zip(bias_paths, weight_paths):\n",
    "    weight = torch.load(f'{wdir}/' + weight_path)\n",
    "    bias = torch.load(f'{wdir}/' + bias_path)\n",
    "    subject = weight_path.split(\"_\")[-2]\n",
    "    weights.append(weight)\n",
    "    biases.append(bias)\n",
    "    subjects.append(subject)\n",
    "    \n",
    "weight = torch.stack(weights).mean(dim=0).to(device)\n",
    "bias = torch.stack(biases).mean(dim=0).to(device)\n",
    "print(weight.shape)\n",
    "subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b58fc6c-ed69-4634-b550-16ab685fdd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animal_youth\t   _LORE-plural.ipynb  seal\t\t  veryold-trout\n",
      "animal-youth.json  NormLRE.ipynb       veryold-butterfly\n",
      "fish\t\t   Old_NormLRE.ipynb   veryold-goat\n",
      "_LORE.ipynb\t   plural_reg\t       veryold-lion\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbf6e436-2d7e-40fb-9e81-5c43a108bebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", revision=\"float16\", torch_dtype=torch.float16, low_cpu_mem_usage=True)\n",
    "model.to('cuda:1')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "mt = models.ModelAndTokenizer(model,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35c6b465-5096-42d3-8604-3512556c6a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing data\n",
    "import json\n",
    "json_path = 'verb+able.json'\n",
    "pairs = []\n",
    "\n",
    "with open(json_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "    for pair in data['samples']:\n",
    "        pairs.append((pair['subject'],pair['object']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3d8b024-6d18-4d6a-8dae-04a8a29836e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_object(mt, subject, weight,bias, prompt, h_layer, beta, h=None, k=5):\n",
    "    h_index, inputs = functional.find_subject_token_index(\n",
    "        mt = mt, prompt=prompt, subject=subject)\n",
    "    #print(f'h_index is {h_index}, inputs is {inputs}')\n",
    "    [[hs], _] = functional.compute_hidden_states(\n",
    "        mt = mt, layers = [h_layer], inputs = inputs)\n",
    "    #h is hs @ h_layer @ h_index\n",
    "    if h == None:\n",
    "        h = hs[:, h_index]\n",
    "        h = h.to(device)\n",
    "        \n",
    "    #print(h.shape)\n",
    "    \n",
    "    #apply mean jacobian and bias\n",
    "    z = h.mm(weight.t()) * beta + bias\n",
    "    \n",
    "    logits = mt.lm_head(z)\n",
    "    dist = torch.softmax(logits.float(), dim=-1)\n",
    "    topk = dist.topk(k=k, dim=-1)\n",
    "    probs = topk.values.view(5).tolist()\n",
    "    token_ids = topk.indices.view(5).tolist()\n",
    "    words = [mt.tokenizer.decode(token_id) for token_id in token_ids]\n",
    "    return (words, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0aa2b1c4-4ca6-4197-b9be-b0b460f6da94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get subject representation for each ICL example\n",
    "#default h_layer is 5 for the LREs I calculated here.\n",
    "h_layer = 5\n",
    "\n",
    "def hs(word, prompt):\n",
    "    h_index, inputs = functional.find_subject_token_index(\n",
    "    mt = mt, prompt=prompt, subject=word)\n",
    "    [[hs], _] = functional.compute_hidden_states(\n",
    "        mt = mt, layers = [h_layer], inputs = inputs)\n",
    "    #h = hs @ h_layer @ h_index\n",
    "    h = hs[:, h_index]\n",
    "    h = h.to(device)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3df4a9a7-548a-4aad-81be-b48306004c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5000, 1.5000, 1.5000])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = torch.tensor([[2,2,2],[1,1,1]]).float()\n",
    "torch.mean(k, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "804f6d24-d968-44e2-a78b-b2c695331b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = 'animal-youth.json'\n",
    "pairs = []\n",
    "with open(json_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "    for pair in data['samples']:\n",
    "        pairs.append((pair['subject'],pair['object']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2b84d99-82af-47dc-b279-976c3683e04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7586, 0.6245, 0.2535,  ..., 0.2904, 0.4962, 0.8786], device='cuda:1') tensor([ 0.0448, -0.0083, -0.4259,  ..., -0.1480,  0.1817, -0.6047],\n",
      "       device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "from baukit.baukit import parameter_names, get_parameter\n",
    "\n",
    "#returns weight and bias for lns\n",
    "def get_layer_norm_params(mt, start, end):\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for i in range(start, end):\n",
    "        w_name = f'transformer.h.{i}.ln_1.weight'\n",
    "        b_name = f'transformer.h.{i}.ln_1.bias'\n",
    "        weight = get_parameter(model=mt.model,name=w_name).data\n",
    "        bias = get_parameter(model=mt.model,name=b_name).data\n",
    "        weights.append(weight.to(device))\n",
    "        biases.append(bias.to(device))\n",
    "    return weights,biases\n",
    "    \n",
    "ln_weights,ln_biases = get_layer_norm_params(mt,5,27)\n",
    "\n",
    "gamma = torch.ones(4096).to(device)\n",
    "\n",
    "for ln_weight in ln_weights:\n",
    "    gamma = gamma * ln_weight\n",
    "\n",
    "beta = torch.zeros(4096).to(device)\n",
    "for ln_bias in ln_biases:\n",
    "    beta = beta + ln_bias\n",
    "\n",
    "#this feels very stupid\n",
    "print(gamma,beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25d8ae09-72c7-4124-8f14-d552a1f5081e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LRE: [' acceptable', ' accept', ' able', ' capable', ' accepted'] SOL: ['acceptable']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\"achieve\" not found in \"If you can accept something, that thing is\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m prompt \u001b[38;5;241m=\u001b[39m prompt\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#LRE (beta = 1)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m (lre_pred, lre_prob) \u001b[38;5;241m=\u001b[39m \u001b[43mget_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#LayerNormLRE \u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#should actually be factoring in the original weight and bias calculation.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#We calculate bias based on layer_norm(Jh)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m subj_hs \u001b[38;5;241m=\u001b[39m hs(subj, prompt)\n",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m, in \u001b[0;36mget_object\u001b[0;34m(mt, subject, weight, bias, prompt, h_layer, beta, h, k)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_object\u001b[39m(mt, subject, weight,bias, prompt, h_layer, beta, h\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     h_index, inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_subject_token_index\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubject\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m#print(f'h_index is {h_index}, inputs is {inputs}')\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     [[hs], _] \u001b[38;5;241m=\u001b[39m functional\u001b[38;5;241m.\u001b[39mcompute_hidden_states(\n\u001b[1;32m      7\u001b[0m         mt \u001b[38;5;241m=\u001b[39m mt, layers \u001b[38;5;241m=\u001b[39m [h_layer], inputs \u001b[38;5;241m=\u001b[39m inputs)\n",
      "File \u001b[0;32m~/my-lre/data/wapprox/../../lre/functional.py:57\u001b[0m, in \u001b[0;36mfind_subject_token_index\u001b[0;34m(prompt, subject, offset, mt)\u001b[0m\n\u001b[1;32m     55\u001b[0m     inputs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Find the last occurrence of the subject\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m subject_i, subject_j \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_token_range\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubject\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moccurrence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     59\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m subject_token_index \u001b[38;5;241m=\u001b[39m tokenizer_utils\u001b[38;5;241m.\u001b[39moffset_to_absolute_index(\n\u001b[1;32m     61\u001b[0m     subject_i, subject_j, offset\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m subject_token_index, inputs\n",
      "File \u001b[0;32m~/my-lre/data/wapprox/../../lre/tokenizer_utils.py:48\u001b[0m, in \u001b[0;36mfind_token_range\u001b[0;34m(string, substring, tokenizer, occurrence, offset_mapping, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot set return_offsets_mapping\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m substring \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string:\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubstring\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not found in \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstring\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m occurrence \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# If occurrence is negative, count from the right.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     char_start \u001b[38;5;241m=\u001b[39m string\u001b[38;5;241m.\u001b[39mrindex(substring)\n",
      "\u001b[0;31mValueError\u001b[0m: \"achieve\" not found in \"If you can accept something, that thing is\""
     ]
    }
   ],
   "source": [
    "weight = weight.to(device)\n",
    "bias = bias.to(device)\n",
    "\n",
    "prompt = f\"If you can {subj} something, that thing is\"\n",
    "\n",
    "for pair in pairs:\n",
    "    subj, objs = pair\n",
    "    prompt = prompt\n",
    "    #LRE (beta = 1)\n",
    "    (lre_pred, lre_prob) = get_object(mt, subj, weight, bias, prompt, h_layer, 2.5)\n",
    "\n",
    "    #LayerNormLRE \n",
    "    #should actually be factoring in the original weight and bias calculation.\n",
    "    #We calculate bias based on layer_norm(Jh)\n",
    "    \n",
    "    subj_hs = hs(subj, prompt)\n",
    "    hss_hs = torch.stack([subj_hs] + hss).to(device)\n",
    "    #the first element corresponds to subj_hs.\n",
    "    norm_subj = layer_norm(hss_hs, (1,2), gamma,beta)[0]\n",
    "    #print(f'norm_subj shape is {norm_subj.shape}')\n",
    "    #(pred, prob) = get_object(mt, subj, weight, bias, prompt, h_layer, 1, norm_subj.half())\n",
    "    print(f'LRE: {lre_pred} SOL: {objs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb4f69e-e499-4d6c-81b3-45140f624002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8fd39c-15bd-49d0-92c5-4b00d403f138",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
