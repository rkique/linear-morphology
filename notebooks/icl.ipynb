{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8134571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../context-mediation\")\n",
    "sys.path.append(\"../../relations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51ad0e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "270bedfd",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 17.91 GB, other allocations: 384.00 KB, max allowed: 18.13 GB). Tried to allocate 306.74 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m GPT2Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(config)\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m GPT2Model\u001b[38;5;241m.\u001b[39mfrom_pretrained(config, low_cpu_mem_usage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n",
      "File \u001b[0;32m~/Desktop/my-lre/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:2724\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2719\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2720\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2721\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2722\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2723\u001b[0m         )\n\u001b[0;32m-> 2724\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/my-lre/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/my-lre/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/my-lre/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/my-lre/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1154\u001b[0m             device,\n\u001b[1;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1156\u001b[0m             non_blocking,\n\u001b[1;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1158\u001b[0m         )\n\u001b[0;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 17.91 GB, other allocations: 384.00 KB, max allowed: 18.13 GB). Tried to allocate 306.74 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "#The top 15 domains by volume in WebText are: \n",
    "# Google, Archive, Blogspot, GitHub, NYTimes, Wordpress, Washington Post, Wikia, BBC, The Guardian, eBay, Pastebin, CNN, Yahoo!, and the Huffington Post.\n",
    "\n",
    "device = \"mps\"\n",
    "config = \"gpt2-xl\"\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(config)\n",
    "model = GPT2Model.from_pretrained(config, low_cpu_mem_usage=True)\n",
    "model.to(device)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1b3dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.relations import estimate\n",
    "\n",
    "import baukit\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_relation_operator_fast(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    subject,\n",
    "    relation,\n",
    "    subject_token_index=-1,\n",
    "    layer=25,\n",
    "    device=None,\n",
    "):\n",
    "    model.to(device)\n",
    "\n",
    "    prompt = relation.format(subject)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", return_offsets_mapping=True).to(\n",
    "        device\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    subject_i, subject_j = estimate.find_token_range(\n",
    "        prompt, subject, offset_mapping=offset_mapping[0]\n",
    "    )\n",
    "    h_token_index = estimate.determine_token_index(\n",
    "        subject_i,\n",
    "        subject_j,\n",
    "        subject_token_index,\n",
    "    )\n",
    "\n",
    "    # Precompute everything up to the subject.\n",
    "    past_key_values = None\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "    if subject_i > 0:\n",
    "        outputs = model(\n",
    "            input_ids=input_ids[:, :subject_i],\n",
    "#             attention_mask=attention_mask[:, :subject_i],\n",
    "            use_cache=True)\n",
    "        past_key_values = outputs.past_key_values\n",
    "        input_ids = input_ids[:, subject_i:]\n",
    "        attention_mask = attention_mask[:, subject_i:]\n",
    "        h_token_index -= subject_i\n",
    "\n",
    "    # Precompute initial h and z.\n",
    "    h_layer_name = f\"transformer.h.{layer}\"\n",
    "    z_layer_name = f\"transformer.h.{model.config.n_layer - 1}\"\n",
    "    with baukit.TraceDict(model, (h_layer_name, z_layer_name)) as ret:\n",
    "        model(input_ids=input_ids,\n",
    "#               attention_mask=attention_mask,\n",
    "              use_cache=past_key_values is not None,\n",
    "              past_key_values=past_key_values)\n",
    "    h = ret[h_layer_name].output[0][0, h_token_index]\n",
    "    z = ret[z_layer_name].output[0][0, -1]\n",
    "\n",
    "    # Now estimate J and b.\n",
    "    def compute_z_from_h(h: torch.Tensor) -> torch.Tensor:\n",
    "        def insert_h(output: tuple, layer: str) -> tuple:\n",
    "            if layer != h_layer_name:\n",
    "                return output\n",
    "            output[0][0, h_token_index] = h\n",
    "            return output\n",
    "\n",
    "        with baukit.TraceDict(\n",
    "            model, (h_layer_name, z_layer_name), edit_output=insert_h\n",
    "        ) as ret:\n",
    "            model(input_ids=input_ids,\n",
    "#                   attention_mask=attention_mask,\n",
    "                  past_key_values=past_key_values,\n",
    "                  use_cache=past_key_values is not None)\n",
    "        return ret[z_layer_name].output[0][0, -1]\n",
    "\n",
    "    weight = torch.autograd.functional.jacobian(compute_z_from_h, h, vectorize=True)\n",
    "    bias = z[None] - h[None].mm(weight.t())\n",
    "    return estimate.RelationOperator(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        layer=layer,\n",
    "        relation=\"{}\" + relation.split(\"{}\")[1],\n",
    "        weight=weight,\n",
    "        bias=bias,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28166ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 15\n",
    "r = estimate_relation_operator_fast(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    \"The Space Needle\",\n",
    "    \"{} is located in the country of\",\n",
    "    layer=layer,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(r(\"The Great Wall\", subject_token_index=-1, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db47df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.relations import estimate\n",
    "\n",
    "import torch\n",
    "\n",
    "# prompt = \"\"\"\\\n",
    "# The Space Needle is located in Seattle.\n",
    "# The Eiffel Tower is located in Paris.\n",
    "# {} is located in\"\"\"\n",
    "# subject = \"The Great Wall\"\n",
    "# test_subjects = (\n",
    "#     \"The Eiffel Tower\",\n",
    "#     \"Niagara Falls\",\n",
    "#     \"The Empire State Building\",\n",
    "# )\n",
    "\n",
    "# prompt = \"\"\"\\\n",
    "# Bananas: yellow.\n",
    "# Apples: red.\n",
    "# {}:\"\"\"\n",
    "# subject = \"Kiwis\"\n",
    "# test_subjects = (\n",
    "#     \"Broccoli\",\n",
    "#     \"Apples\",\n",
    "#     \"Carrots\",\n",
    "#     \"Potatoes\",\n",
    "#     \"Cotton candy\",\n",
    "#     \"Figs\",\n",
    "#     \"Plums\",\n",
    "# )\n",
    "\n",
    "# prompt = \"\"\"\\\n",
    "# {} typically work inside of a\"\"\"\n",
    "# Judges typically work inside of a courtroom.\n",
    "# Nurses typically work inside of a hospital.\n",
    "# test_subjects = (\n",
    "#     \"Farmers\",\n",
    "#     \"Car mechanics\",\n",
    "#     \"Teachers\",\n",
    "#     \"Scientists\",\n",
    "# )\n",
    "# subject = \"Car mechanics\"\n",
    "\n",
    "# prompt = \"\"\"\\\n",
    "# Megan Rapinoe plays the sport of soccer.\n",
    "# Larry Bird plays the sport of basketball.\n",
    "# John McEnroe plays the sport of tennis.\n",
    "# {} plays the sport of\"\"\"\n",
    "# subject = \"Oksana Baiul\"\n",
    "# test_subjects = (\n",
    "#     \"Shaquille O'Neal\",\n",
    "#     \"Babe Ruth\",\n",
    "#     \"Tom Brady\",\n",
    "#     \"Tiger Woods\",\n",
    "#     \"Lionel Messi\",\n",
    "#     \"Michael Phelps\",\n",
    "#     \"Serena Williams\",\n",
    "# )\n",
    "\n",
    "# prompt = \"\"\"\\\n",
    "# The meat of a banana is colored white.\n",
    "# The meat of a strawberry is colored red.\n",
    "# The meat of a {} is colored\"\"\"\n",
    "\n",
    "# r = \"have skin of the color\"\n",
    "# r = \"have meat of the color\"\n",
    "# prompt = f\"\"\"\\\n",
    "# Banana {r} yellow.\n",
    "# Potatoes {r} brown.\n",
    "# \"\"\" + \"{} \" + r\n",
    "# subject = \"Blueberries\"\n",
    "# test_subjects = (\n",
    "#     \"Apples\",\n",
    "#     \"Coconuts\",\n",
    "#     \"Kiwis\",\n",
    "#     \"Blueberries\",\n",
    "# )\n",
    "\n",
    "prompt = \"\"\"\\\n",
    "Bigger is the opposite of smaller.\n",
    "Empty is the opposite of full.\n",
    "{} is the opposite of\"\"\"\n",
    "subject = \"Awake\"\n",
    "test_subjects = (\n",
    "    \"Dark\",\n",
    "    \"Alive\",\n",
    "    \"Bright\",\n",
    "    \"Smaller\",\n",
    "    \"Empty\",\n",
    ")\n",
    "\n",
    "layer = 15\n",
    "\n",
    "print(prompt, \"\\n\")\n",
    "print(\"training subject:\", subject, \"\\n\")\n",
    "\n",
    "print(\"-- generations --\")\n",
    "for subj in (subject, *test_subjects):\n",
    "    inputs = tokenizer(prompt.format(subj), return_tensors=\"pt\").to(device)\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=3, pad_token_id=tokenizer.eos_token_id)\n",
    "    print(subj, tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[1]:]))\n",
    "print()\n",
    "    \n",
    "r_icl = estimate_relation_operator_fast(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    subject,\n",
    "    prompt,\n",
    "    layer=layer,\n",
    "    device=device,\n",
    ")\n",
    "# r_icl.weight[:] = torch.eye(model.config.hidden_size).to(device)\n",
    "# r_icl.bias[:] = 0\n",
    "print(\"-- J/b predictions --\")\n",
    "for entity in test_subjects:\n",
    "    print(entity, r_icl(entity, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4892d72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_zs = \"{} plays the sport of\"\n",
    "subject = \"Tom Brady\"\n",
    "r_zs = estimate_relation_operator_fast(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    subject,\n",
    "    prompt_zs,\n",
    "    layer=layer,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "for entity in test_subjects:\n",
    "    print(entity, r_zs(entity, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a94f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "(r_zs.weight - r_icl.weight).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f046bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_utils.cosine_similarity_float16(r_zs.bias, r_icl.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01adaa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questions to answer:\n",
    "# - How similar are different biases that we find?\n",
    "# - How similar are different J's? Is the ICL J better than the non-ICL J?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8252618",
   "metadata": {},
   "source": [
    "# Averaging J from Multiple ICL Prompts\n",
    "\n",
    "Averaging works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785506c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "sos = (\n",
    "    (\"Nurses\", \"hospital\"),\n",
    "    (\"Judges\", \"courtroom\"),\n",
    "    (\"Car mechanics\", \"garage\"),\n",
    "    (\"Farmers\", \"field\"),\n",
    ")\n",
    "r = \"{} typically work inside of a\"\n",
    "\n",
    "# sos = (\n",
    "#     (\"Megan Rapinoe\", \"soccer\"),\n",
    "#     (\"Larry Bird\", \"basketball\"),\n",
    "#     (\"John McEnroe\", \"tennis\"),\n",
    "# )\n",
    "# r = \"{} plays the sport of\"\n",
    "\n",
    "# sos = (\n",
    "#     (\"Bigger\", \"smaller\"),\n",
    "#     (\"Awake\", \"asleep\"),\n",
    "#     (\"Dark\", \"light\"),\n",
    "# )\n",
    "# r = \"{} is the opposite of\"\n",
    "\n",
    "jbs = []\n",
    "for s, o in tqdm(sos):\n",
    "    others = set(sos) - {(s, o)}\n",
    "    prompt = \"\"\n",
    "    prompt += \"\\n\".join(r.format(s_other) + f\" {o_other}.\" for s_other, o_other in others) + \"\\n\"\n",
    "    prompt += r\n",
    "    print(prompt)\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    jb = estimate_relation_operator_fast(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        s,\n",
    "        prompt,\n",
    "        layer=layer,\n",
    "        device=device,\n",
    "    )\n",
    "    jbs.append(jb)\n",
    "\n",
    "relation = estimate.RelationOperator(\n",
    "    weight=torch.stack([jb.weight for jb in jbs]).mean(dim=0),\n",
    "    bias=torch.stack([jb.bias for jb in jbs]).mean(dim=0),\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    layer=layer,\n",
    "    relation=r,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1329c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_subjects = (\"Chefs\", \"Teachers\", \"Biologists\", \"Bus drivers\")\n",
    "# test_subjects = (\n",
    "#     \"Shaquille O'Neal\",\n",
    "#     \"Babe Ruth\",\n",
    "#     \"Tom Brady\",\n",
    "#     \"Tiger Woods\",\n",
    "#     \"Lionel Messi\",\n",
    "#     \"Michael Phelps\",\n",
    "#     \"Serena Williams\",\n",
    "# )\n",
    "test_subjects = (\n",
    "    \"Alive\",\n",
    "    \"Bright\",\n",
    "    \"Smaller\",\n",
    "    \"Empty\",\n",
    ")\n",
    "\n",
    "for subject in test_subjects:\n",
    "    print(subject, relation(subject, device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98199fc",
   "metadata": {},
   "source": [
    "# Differences in h between ICL and Zero-Shot\n",
    "\n",
    "Hypothesis: The above doesn't work because the entity retrieved as a third ICL example likely throws away most of the information except what is necessary! The model already knows what it's supposed to retrieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6bb985",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = \"Shaquille O'Neal\"\n",
    "relation_text = \"plays the sport of\"\n",
    "prompt = f\"\"\"\\\n",
    "Megan Rapinoe plays the sport of soccer.\n",
    "Larry Bird plays the sport of basketball.\n",
    "John McEnroe plays the sport of Tennis.\n",
    "Babe Ruth plays the sport of baseball.\n",
    "Tiger Woods plays the sport of golf.\n",
    "{entity} {relation_text}\"\"\"\n",
    "layer = 15\n",
    "\n",
    "h_layername = f\"transformer.h.{layer}\"\n",
    "z_layername = f\"transformer.h.{layer}\"\n",
    "inputs_icl = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "i, j = estimate.find_token_range(prompt, entity, tokenizer=tokenizer)\n",
    "with baukit.TraceDict(model, (h_layername, z_layername)) as ret:\n",
    "    model(**inputs_icl)\n",
    "\n",
    "icl = ret[h_layername].output[0][0, i:]\n",
    "tokenizer.convert_ids_to_tokens(inputs_icl.input_ids[0, i:].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5fcdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_orig = f\"<|endoftext|>{entity} {relation_text}\"\n",
    "inputs_orig = tokenizer(prompt_orig, return_tensors=\"pt\").to(device)\n",
    "with baukit.TraceDict(model, (h_layername, z_layername)) as ret:\n",
    "    model(**inputs_orig)\n",
    "\n",
    "orig = ret[h_layername].output[0][0]\n",
    "tokenizer.convert_ids_to_tokens(inputs_orig.input_ids[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8417ccb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from src.utils import training_utils\n",
    "\n",
    "# values = training_utils.cosine_similarity_float16(orig, icl).tolist()[1:]\n",
    "values = orig.sub(icl).norm(dim=-1).tolist()[1:]\n",
    "\n",
    "labels = tokenizer.convert_ids_to_tokens(inputs_orig.input_ids[0, 1:].tolist())\n",
    "\n",
    "print(values, labels)\n",
    "\n",
    "plt.title(\"L2(h_0, h_icl) at layer 15\")\n",
    "plt.bar(labels, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d1565f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(inputs_icl.input_ids[0, i + 1:].tolist()))\n",
    "print(tokenizer.convert_ids_to_tokens(inputs_orig.input_ids[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc49f1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig.norm(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52e9512",
   "metadata": {},
   "outputs": [],
   "source": [
    "icl.norm(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f037cbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = training_utils.cosine_similarity_float16\n",
    "# sim = lambda a, b: a.sub(b).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4d9e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim(orig[1], orig[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84ab272",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim(icl[1], icl[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
