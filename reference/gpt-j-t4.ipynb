{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f680dc0",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb0e23de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: apt-get\n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install -y nvidia-driver-470"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5f6690",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip3 install torch \n",
    "\n",
    "!git clone https://github.com/kingoflolz/mesh-transformer-jax.git\n",
    "!pip3 install -r mesh-transformer-jax/requirements.txt\n",
    "\n",
    "# jax 0.2.12 is required due to a regression with xmap in 0.2.13\n",
    "!pip3 install mesh-transformer-jax/ jax==0.2.12 jaxlib -f https://storage.googleapis.com/jax-releases/jax_releases.html\n",
    "!pip3 install git+https://github.com/finetuneanon/transformers@gpt-j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c347a8e",
   "metadata": {},
   "source": [
    "## Get the actual GPT-J model [Source](https://github.com/kingoflolz/mesh-transformer-jax/blob/master/colab_demo.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971f231c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-install zstd\n",
    "\n",
    "# the \"slim\" version contain only bf16 weights and no optimizer parameters, which minimizes bandwidth and memory\n",
    "!time wget -c https://the-eye.eu/public/AI/GPT-J-6B/step_383500_slim.tar.zstd\n",
    "\n",
    "!time tar -I zstd -xf step_383500_slim.tar.zstd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d046b59b",
   "metadata": {},
   "source": [
    "## Convert to torch [Source](https://gist.github.com/finetuneanon/ee196c6cd16af1de4ca444862414683a) Credit:[finetuneanon](https://github.com/finetuneanon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8db4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir gpt-j-hf\n",
    "!curl https://gist.githubusercontent.com/finetuneanon/a55bdb3f5881e361faef0e96e1d41f09/raw/e5a38dad34ff42bbad188afd5e4fdb2ab2eacb6d/gpt-j-6b.json > gpt-j-hf/config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4e21c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import io\n",
    "import os\n",
    "\n",
    "torch.set_printoptions(linewidth=130, sci_mode=False)\n",
    "np.set_printoptions(linewidth=130, suppress=True)\n",
    "\n",
    "layers = 28\n",
    "total_shards = 8\n",
    "ckpt_dir = \"step_383500/\"\n",
    "output_dir = \"j6b_ckpt\"\n",
    "\n",
    "def reshard(x, old_shape):\n",
    "    if len(x.shape) == 1:\n",
    "        # print(\"epoch\")\n",
    "        # print(x)\n",
    "        out = x[0:1]\n",
    "\n",
    "    elif len(x.shape) == 2:\n",
    "        #print(f\"LN/bias {x.shape}\")\n",
    "        #print(x[:, :16])\n",
    "\n",
    "        if (x[1:] == x[-1]).all():\n",
    "            #print(\"LN\")\n",
    "            if (x[1:] == 0).all() or (x[1:] == 1).all():\n",
    "                out = x[0:1]\n",
    "            else:\n",
    "                #print(\"shard bias\")\n",
    "                out = x[0:1] * 8#* x.shape[0] / old_shape[0]\n",
    "        else:\n",
    "            #print(\"bias\")\n",
    "            out = x.reshape(old_shape)\n",
    "\n",
    "        #print(out[:, :16])\n",
    "\n",
    "    elif len(x.shape) == 3:\n",
    "        #print(f\"weight {x.shape}\")\n",
    "        if x.shape[0] * x.shape[2] == old_shape[2]:\n",
    "            #print(\"case 1\")\n",
    "            out = jnp.transpose(x, (1, 0, 2)).reshape(old_shape)\n",
    "        elif x.shape[0] * x.shape[1] == old_shape[1]:\n",
    "            #print(\"case 2\")\n",
    "            out = x.reshape(old_shape)\n",
    "        else:\n",
    "            raise Exception(f\"unimplemented, {x.shape}, {old_shape}\")\n",
    "    else:\n",
    "        raise Exception(f\"unimplemented, {x}\")\n",
    "    #flattened, structure = jax.tree_flatten(out)\n",
    "    #return flattened\n",
    "    return out\n",
    "\n",
    "def get_old_shape(t, dim=2):\n",
    "    if len(t.shape) == 3:\n",
    "        shard_shape = t.shape\n",
    "        if dim == 1:\n",
    "            return (shard_shape[0] * shard_shape[1], shard_shape[2])\n",
    "        elif dim == 2:\n",
    "            return (shard_shape[1], shard_shape[0] * shard_shape[2])\n",
    "        else:\n",
    "            raise ValueError(f\"unsupported dim {dim}\")\n",
    "    if len(t.shape) == 2:\n",
    "        return (t.shape[1] * t.shape[0],)\n",
    "    else:\n",
    "        raise ValueError(f\"unsupported shape {t.shape}\")\n",
    "\n",
    "def read_shard(ckpt_dir):\n",
    "    global part\n",
    "    out = []\n",
    "    idx = part\n",
    "    file_path = ckpt_dir + f\"{idx}.npz\"\n",
    "    #print(f\"-- {file_path}\")\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        buf = f.read()\n",
    "        f_io = io.BytesIO(buf)\n",
    "        deserialized = np.load(f_io)\n",
    "        for i in deserialized:\n",
    "            out.append(deserialized[i])\n",
    "            #print(deserialized[i].shape)\n",
    "    return out\n",
    "\n",
    "def save(ckpt):\n",
    "    try: os.mkdir(output_dir)\n",
    "    except: pass\n",
    "    checkpoint = {}\n",
    "    for i, x in enumerate(ckpt.items()):\n",
    "        checkpoint[x[0]] = f\"{output_dir}/b{i}.pt\"\n",
    "        torch.save(x[1], f\"{output_dir}/b{i}.pt\")\n",
    "    torch.save(checkpoint, f\"{output_dir}/m.pt\")\n",
    "\n",
    "unshard = None\n",
    "transforms = [(\"transformer.wte.bias\", None, None), (\"transformer.wte.weight\", unshard, 1)]\n",
    "\n",
    "checkpoint = {}\n",
    "\n",
    "layer_names = sorted(map(str, range(layers)))\n",
    "for layer in layer_names:\n",
    "    checkpoint[f\"transformer.h.{layer}.attn.attention.bias\"] = torch.tril(torch.ones(1, 1, 2048, 2048))\n",
    "    checkpoint[f\"transformer.h.{layer}.attn.attention.masked_bias\"] = torch.tensor(-1e9)\n",
    "    transforms.extend([\n",
    "        (f\"transformer.h.{layer}.attn.attention.q_proj.weight\", unshard, 2),\n",
    "        (f\"transformer.h.{layer}.attn.attention.v_proj.weight\", unshard, 2),\n",
    "        (f\"transformer.h.{layer}.attn.attention.k_proj.weight\", unshard, 2),\n",
    "        (f\"transformer.h.{layer}.attn.attention.out_proj.weight\", unshard, 1),\n",
    "        (f\"transformer.h.{layer}.mlp.c_fc.bias\", unshard, 1),\n",
    "        (f\"transformer.h.{layer}.mlp.c_fc.weight\", unshard, 2),\n",
    "        (f\"transformer.h.{layer}.mlp.c_proj.bias\", None, None),\n",
    "        (f\"transformer.h.{layer}.mlp.c_proj.weight\", unshard, 1),\n",
    "        (f\"transformer.h.{layer}.ln_1.bias\", None, None),\n",
    "        (f\"transformer.h.{layer}.ln_1.weight\", None, None),\n",
    "    ])\n",
    "transforms.extend([\n",
    "    (\"lm_head.bias\", unshard, 1),\n",
    "    (\"lm_head.weight\", unshard, 2),\n",
    "    (\"transformer.ln_f.bias\", None, None),\n",
    "    (\"transformer.ln_f.weight\", None, None),\n",
    "])\n",
    "\n",
    "part = 0\n",
    "element = 0\n",
    "while len(transforms) > 0:\n",
    "    print(f\"loading shards for part {part}\")\n",
    "    shards = list(map(read_shard, [f\"{ckpt_dir}shard_{i}/\" for i in range(total_shards)]))\n",
    "    print(f\"read from checkpoint\")\n",
    "\n",
    "    unsharded = []\n",
    "\n",
    "    for all_shards in zip(*shards):\n",
    "        x = np.stack(all_shards)\n",
    "        # No idea why this is V2...?\n",
    "        if x.dtype == np.dtype('V2'):\n",
    "            x.dtype = jnp.bfloat16\n",
    "        x = x.astype(np.float32)\n",
    "        unsharded.append(x)\n",
    "        #print(f\"unsharded: {x.shape}\")\n",
    "\n",
    "    while len(transforms) > 0 and len(unsharded) > 0:\n",
    "        transform = transforms.pop(0)\n",
    "        params = unsharded.pop(0)\n",
    "        if transform[2] is not None:\n",
    "            old_shape = (1,) + get_old_shape(params, transform[2])\n",
    "        else:\n",
    "            old_shape = (params.shape[1],)\n",
    "        print(f\"< {params.shape} to {old_shape}\")\n",
    "        params = reshard(params, old_shape).squeeze(0).T\n",
    "        params = torch.tensor(params.copy()).half()\n",
    "        if params.isnan().any() or params.isinf().any():\n",
    "            raise ValueError(f\"fp16 over/underflow at {part} {element}\")\n",
    "        checkpoint[transform[0]] = params\n",
    "        print(f\"> {transform[0]} {params.shape}\")\n",
    "        element += 1\n",
    "    part += 1\n",
    "\n",
    "checkpoint['transformer.wte.weight'] = (checkpoint['transformer.wte.weight'].T + checkpoint['transformer.wte.bias'])\n",
    "del checkpoint['transformer.wte.bias']\n",
    "\n",
    "print(f\"left over: {unsharded}\")\n",
    "print(\"saving\")\n",
    "torch.save(checkpoint, \"./gpt-j-hf/pytorch_model.bin\") # load as in: https://github.com/finetuneanon/misc/blob/main/SizeTest.ipynb\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90f8bd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import GPTNeoForCausalLM, AutoConfig, GPT2Tokenizer\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8118113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-4fcea1767d85>:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.9 it will stop working\n",
      "  class Checkpoint(collections.MutableMapping):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded\n",
      "ok\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTNeoForCausalLM(\n",
       "  (transformer): GPTNeoModel(\n",
       "    (wte): Embedding(50400, 4096)\n",
       "    (drop): Dropout(p=0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (18): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (19): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (20): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (21): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (22): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (23): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (24): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (25): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (26): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (27): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=50400, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "import os\n",
    "import json\n",
    "\n",
    "class Checkpoint(collections.MutableMapping):\n",
    "    def __init__(self):\n",
    "        self.checkpoint = torch.load(\"pytorch_model.bin\")\n",
    "        print(\"Loaded\")\n",
    "    def __len__(self):\n",
    "        return len(self.checkpoint)\n",
    "    def __getitem__(self, key):\n",
    "        return torch.load(self.checkpoint[key])\n",
    "    def __setitem__(self, key, value):\n",
    "        return\n",
    "    def __delitem__(self, key, value):\n",
    "        return\n",
    "    def keys(self):\n",
    "        return self.checkpoint.keys()\n",
    "    def __iter__(self):\n",
    "        for key in self.checkpoint:\n",
    "            yield (key, self.__getitem__(key))\n",
    "    def __copy__(self):\n",
    "        return self.__dict__\n",
    "    def copy(self):\n",
    "        return self.__dict__\n",
    "\n",
    "print(\"load\", flush=True)\n",
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "model = GPTNeoForCausalLM.from_pretrained(pretrained_model_name_or_path=None, config=config, state_dict=Checkpoint())\n",
    "print(\"ok\")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6475e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTNeoForCausalLM.from_pretrained(\"./gpt-j-hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f75f0a4",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f731069",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76da2f0c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoForCausalLM(\n",
       "  (transformer): GPTNeoModel(\n",
       "    (wte): Embedding(50400, 4096)\n",
       "    (drop): Dropout(p=0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (18): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (19): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (20): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (21): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (22): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (23): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (24): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (25): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (26): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (27): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0, inplace=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "          (c_proj): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=50400, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.half().cuda() # This should take about 12GB of Graphics RAM, if you have a larger than 16GB gpu you don't need the half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "030d4615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Hello my name is Paul and\"\n",
    "input_ids = tokenizer.encode(str(input_text), return_tensors='pt').cuda()\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=20,\n",
    "    top_p=0.7,\n",
    "    top_k=0,\n",
    "    temperature=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f783672d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is Paul and\n",
      "this is a Quick Tutorial on how to\n",
      "make a Mac and\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
