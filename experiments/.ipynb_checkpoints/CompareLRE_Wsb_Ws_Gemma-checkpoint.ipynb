{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c4d83fb-81af-4c29-8b65-0f104333a540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exia/miniconda3/envs/my-lre/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|███████████████████████████| 4/4 [00:02<00:00,  1.55it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import lre.models as models\n",
    "import lre.functional as functional\n",
    "import os\n",
    "\n",
    "import json\n",
    "import random\n",
    "from lre.data import Relation, RelationSample, Sequence\n",
    "import lre.metrics as metrics\n",
    "import lre.functional as functional\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"gemma_shards\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "model.to(device)\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "mt = models.ModelAndTokenizer(model,tokenizer)\n",
    "\n",
    "isinstance(mt, models.ModelAndTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fb4f9e1-4c0f-4c5e-8178-3aa76f20b541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name - nationality',\n",
       " 'animal - youth',\n",
       " 'verb_Ving - 3pSg',\n",
       " 'noun+less_reg',\n",
       " 'verb+able_reg',\n",
       " 'UK_city - county',\n",
       " 'antonyms - binary',\n",
       " 'verb_inf - 3pSg',\n",
       " 're+verb_reg',\n",
       " 'verb_inf - Ved',\n",
       " 'country - language',\n",
       " 'meronyms - part',\n",
       " 'verb_Ving - Ved',\n",
       " 'animal - shelter',\n",
       " 'hypernyms - misc',\n",
       " 'meronyms - substance',\n",
       " 'noun - plural_irreg',\n",
       " 'un+adj_reg',\n",
       " 'verb+ment_irreg',\n",
       " 'adj+ness_reg',\n",
       " 'over+adj_reg',\n",
       " 'verb+er_irreg',\n",
       " 'adj+ly_reg',\n",
       " 'name - occupation',\n",
       " 'synonyms - intensity',\n",
       " 'animal - sound',\n",
       " 'noun - plural_reg',\n",
       " 'Ving - verb_inf',\n",
       " 'male - female',\n",
       " 'verb_3pSg - Ved',\n",
       " 'meronyms - member',\n",
       " 'things - color',\n",
       " 'LORE hypernyms - animals',\n",
       " 'hyponyms - misc',\n",
       " 'adj - superlative',\n",
       " '.ipynb_checkpoints',\n",
       " 'verb+tion_irreg',\n",
       " 'synonyms - exact',\n",
       " 'hypernyms - animals',\n",
       " 'country - capital']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('7_27_approx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d050b746-2153-498c-89f8-65d7387526f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'llra.build' from '/home/exia/my-lre/data/../llra/build.py'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import llra.build as build\n",
    "from importlib import reload\n",
    "reload(functional)\n",
    "reload(models)\n",
    "reload(build)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c89eaaa6-e350-401a-80fb-bef421974cc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hegel [' (', ' and', '-'] ['<unused63>', '<unused62>', '-'] [' and', ' (', ' ']\n",
      "gorbachev [' (', ' and', ' a'] [' Russian', ' Russia', ' Soviet'] [' (', ' and', ' ']\n",
      "beta,1,name - nationality,reg_correct,0,no_bias_correct,6,lm_correct,17\n",
      "panda [' (', ' ', '-'] ['<unused63>', ' staff', ' health'] [' (', ' ', ' and']\n",
      "beta,1,animal - youth,reg_correct,0,no_bias_correct,2,lm_correct,14\n",
      "beta,1,verb_Ving - 3pSg,reg_correct,0,no_bias_correct,0,lm_correct,4\n",
      "passion [' (', '-', ' and'] [' passion', ' passionate', 'passion'] [' (', ' and', '-']\n",
      "beta,1,noun+less_reg,reg_correct,0,no_bias_correct,2,lm_correct,4\n",
      "deliver [' (', '<unused63>', '-'] [' delivery', ' deliver', ' delivered'] [' (', '-', ' and']\n",
      "beta,1,verb+able_reg,reg_correct,0,no_bias_correct,1,lm_correct,3\n",
      "stirling [' the', ' and', ' ('] [' Scotland', ' Scottish', 'Scotland'] [' the', ' and', ' ']\n",
      "beta,1,UK_city - county,reg_correct,0,no_bias_correct,4,lm_correct,10\n",
      "beta,1,antonyms - binary,reg_correct,0,no_bias_correct,0,lm_correct,1\n",
      "explain ['<unused63>', '<unused62>', ' ('] [' miniaturka', ' 收納', ' 剪影'] [' and', ' (', '<unused63>']\n",
      "reduce ['<unused63>', ' (', '<unused62>'] [' reduction', ' reduce', ' reducing'] [' and', ' (', '<unused63>']\n",
      "avoid ['<unused63>', '<unused62>', ' ('] [' miniaturka', ' 收納', ' 剪影'] [' and', ' (', '<unused63>']\n",
      "include ['<unused63>', '<unused62>', ' ('] [' miniaturka', ' 收納', ' 剪影'] [' and', ' (', '<unused63>']\n",
      "enjoy ['<unused63>', ' (', '<unused62>'] [' miniaturka', ' 收納', ' 剪影'] [' and', ' (', '<unused63>']\n",
      "beta,1,verb_inf - 3pSg,reg_correct,0,no_bias_correct,8,lm_correct,50\n",
      "beta,1,re+verb_reg,reg_correct,0,no_bias_correct,0,lm_correct,5\n",
      "replace ['<unused63>', ' (', '<unused62>'] [' miniaturka', ' 收納', ' 剪影'] [' (', '<unused63>', ' and']\n",
      "add [' (', '<unused63>', '<unused62>'] [' miniaturka', ' 收納', ' 剪影'] [' (', ' and', '<unused63>']\n",
      "seem [' (', '<unused63>', '<unused62>'] [' miniaturka', ' стоковая', ' 收納'] [' (', ' and', '-']\n",
      "send [' (', '<unused63>', '<unused62>'] [' miniaturka', ' 收納', ' 剪影'] [' (', ' and', '<unused63>']\n",
      "allow [' (', '<unused63>', '<unused62>'] [' miniaturka', ' 收納', ' стоковая'] [' (', '<unused63>', ' and']\n",
      "beta,1,verb_inf - Ved,reg_correct,0,no_bias_correct,3,lm_correct,50\n",
      "bahamas [' (', ' and', '-'] [' miniaturka', ' 收納', ' 剪影'] [' and', ' (', ' ']\n",
      "beta,1,country - language,reg_correct,0,no_bias_correct,2,lm_correct,9\n",
      "beta,1,meronyms - part,reg_correct,0,no_bias_correct,1,lm_correct,2\n",
      "adding ['<unused63>', '<unused62>', ' ('] [' miniaturka', ' 收納', ' 剪影'] [' (', '<unused63>', '<unused62>']\n",
      "containing ['<unused63>', ' (', '<unused62>'] [' miniaturka', ' 收納', ' стоковая'] [' (', '<unused63>', '<unused62>']\n",
      "replacing ['<unused63>', ' (', '<unused62>'] [' miniaturka', ' 收納', ' 剪影'] [' (', '<unused63>', '<unused62>']\n",
      "deciding ['<unused63>', ' (', '<unused62>'] [' miniaturka', ' 收納', ' 剪影'] [' (', '<unused63>', '<unused62>']\n",
      "allowing ['<unused63>', ' (', '<unused62>'] [' miniaturka', ' 收納', ' стоковая'] [' (', '<unused63>', '<unused62>']\n",
      "beta,1,verb_Ving - Ved,reg_correct,0,no_bias_correct,5,lm_correct,50\n",
      "raven [' the', ' a', ' ('] [' 收納', ' стоковая', ' 剪影'] [' the', ' (', ' and']\n",
      "crow [' the', ' a', ' ('] [' crows', ' NSCoder', ' ravens'] [' the', ' (', ' and']\n",
      "beta,1,animal - shelter,reg_correct,0,no_bias_correct,2,lm_correct,26\n",
      "stapler [' (', ' the', ' '] [' kantoor', ' oficina', ' office'] [' (', ' the', ' ']\n",
      "beta,1,hypernyms - misc,reg_correct,0,no_bias_correct,3,lm_correct,8\n",
      "beta,1,meronyms - substance,reg_correct,2,no_bias_correct,1,lm_correct,3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 70\u001b[0m\n\u001b[1;32m     67\u001b[0m     clozed_prompts\u001b[38;5;241m.\u001b[39mappend(cloze_prompt)\n\u001b[1;32m     68\u001b[0m     clozed_answers\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39mobject)\n\u001b[0;32m---> 70\u001b[0m outputs_lm \u001b[38;5;241m=\u001b[39m functional\u001b[38;5;241m.\u001b[39mpredict_next_token(mt\u001b[38;5;241m=\u001b[39mmt, prompt\u001b[38;5;241m=\u001b[39mclozed_prompts)\n\u001b[1;32m     71\u001b[0m preds_lm \u001b[38;5;241m=\u001b[39m  [[x\u001b[38;5;241m.\u001b[39mtoken \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m xs] \u001b[38;5;28;01mfor\u001b[39;00m xs \u001b[38;5;129;01min\u001b[39;00m outputs_lm]\n\u001b[1;32m     72\u001b[0m recall_lm \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39mrecall(preds_lm, clozed_answers)\n",
      "File \u001b[0;32m~/miniconda3/envs/my-lre/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/my-lre/data/../lre/functional.py:349\u001b[0m, in \u001b[0;36mpredict_next_token\u001b[0;34m(mt, prompt, k, batch_size)\u001b[0m\n\u001b[1;32m    346\u001b[0m if isinstance(mt.model, \\\n\u001b[1;32m    347\u001b[0m               transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM):\n\u001b[1;32m    348\u001b[0m     next_token_probs = batch_outputs.logits[:,-2].float().softmax(dim=-1)\n\u001b[0;32m--> 349\u001b[0m else:\n\u001b[1;32m    350\u001b[0m #get output logits->probs->topk probs\n\u001b[1;32m    351\u001b[0m     next_token_probs = batch_outputs.logits[:,-1].float().softmax(dim=-1)\n\u001b[1;32m    352\u001b[0m next_token_topk = next_token_probs.topk(dim=-1, k=k)\n",
      "File \u001b[0;32m~/miniconda3/envs/my-lre/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/my-lre/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/my-lre/lib/python3.12/site-packages/transformers/models/gemma2/modeling_gemma2.py:1068\u001b[0m, in \u001b[0;36mGemma2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1065\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1068\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m   1069\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1070\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1071\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1072\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1073\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1074\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1075\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1076\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1077\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1078\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m   1079\u001b[0m )\n\u001b[1;32m   1081\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1082\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/my-lre/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/my-lre/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/my-lre/lib/python3.12/site-packages/transformers/models/gemma2/modeling_gemma2.py:908\u001b[0m, in \u001b[0;36mGemma2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    897\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    898\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    899\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    905\u001b[0m         cache_position,\n\u001b[1;32m    906\u001b[0m     )\n\u001b[1;32m    907\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 908\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m    909\u001b[0m         hidden_states,\n\u001b[1;32m    910\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[1;32m    911\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m    912\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    913\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    914\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    915\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    916\u001b[0m     )\n\u001b[1;32m    918\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/miniconda3/envs/my-lre/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/my-lre/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/my-lre/lib/python3.12/site-packages/transformers/models/gemma2/modeling_gemma2.py:641\u001b[0m, in \u001b[0;36mGemma2DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    637\u001b[0m min_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfinfo(hidden_states\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmin\n\u001b[1;32m    638\u001b[0m sliding_window_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtril(\n\u001b[1;32m    639\u001b[0m     torch\u001b[38;5;241m.\u001b[39mones_like(attention_mask, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool), diagonal\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msliding_window\n\u001b[1;32m    640\u001b[0m )\n\u001b[0;32m--> 641\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(sliding_window_mask, min_dtype, attention_mask)\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# when decoding\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m attention_mask[:, :, :, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msliding_window :]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start, end = 12, 41\n",
    "APPROX_FOLDER = f'gemma_{start}_{end}_approx'\n",
    "WEIGHT_NAME = f's_o_weight_{start}_{end}'\n",
    "BIAS_NAME = f's_o_bias_{start}_{end}'\n",
    "json_path = 'json/enckno/E06 [animal - youth].json'\n",
    "file = open(json_path, 'r')\n",
    "data = json.load(file)\n",
    "file.close()\n",
    "\n",
    "import llra.build as build\n",
    "from baukit.baukit import parameter_names, get_parameter\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#OBTAIN LRE WEIGHTS\n",
    "relations = os.listdir('gemma_12_41_approx')\n",
    "\n",
    "def find_exact_match(filename, search_string):\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            if search_string.lower() in line.strip().lower():\n",
    "                return line.strip()\n",
    "    print(f\"Could not find {search_string}\")\n",
    "    return None\n",
    "\n",
    "N_ICL = 8 \n",
    "N_AVG = 8\n",
    "N_TRIALS = 8\n",
    "VIEW_SAMPLES = 5\n",
    "\n",
    "for beta in range(1,20,2):\n",
    "    for relation in relations:\n",
    "        wdir = f\"{APPROX_FOLDER}/{relation}\"\n",
    "        ignored_names = ['.ipynb_checkpoints', 'disappoint']\n",
    "        j_samples = [x for x in os.listdir(wdir) if x not in ignored_names]\n",
    "        j_samples = random.sample(j_samples, N_AVG)\n",
    "        # print(f'using {N_AVG} samples out of {len(j_samples)} to build LRE')\n",
    "        reg_weight = build.mean_weight_or_bias(wdir,\n",
    "                                               WEIGHT_NAME, j_samples).half().to(device)\n",
    "        reg_bias = build.mean_weight_or_bias(wdir,\n",
    "                                             BIAS_NAME, j_samples).half().to(device)\n",
    "        json_path = find_exact_match(\"json_paths.txt\", relation)\n",
    "        file = open(json_path, 'r')\n",
    "        data = json.load(file)\n",
    "        file.close()\n",
    "        \n",
    "        relation = Relation.from_dict(data)\n",
    "        prompt_template = relation.prompt_templates[0]\n",
    "        \n",
    "        #ASSEMBLE PROMPTS AND OBJECT ANSWERS\n",
    "        clozed_prompts = []\n",
    "        clozed_answers = []\n",
    "        #Do not use the training examples for ICL or testing purposes.\n",
    "        test_samples = [x for x in relation.samples if x not in j_samples]\n",
    "        random.shuffle(test_samples)\n",
    "        for x in test_samples:\n",
    "            x.object[0] = \"\".join(x.object)\n",
    "        for x in test_samples:\n",
    "            test_samples_no_x = [t for t in test_samples if t != x]\n",
    "            samples = [x] + random.sample(test_samples_no_x, N_ICL - 1)\n",
    "            cloze_prompt = functional.make_prompt(\n",
    "                template = prompt_template, \n",
    "                target = x,\n",
    "                examples = samples\n",
    "                )\n",
    "            clozed_prompts.append(cloze_prompt)\n",
    "            clozed_answers.append(x.object)\n",
    "        \n",
    "        outputs_lm = functional.predict_next_token(mt=mt, prompt=clozed_prompts)\n",
    "        preds_lm =  [[x.token for x in xs] for xs in outputs_lm]\n",
    "        recall_lm = metrics.recall(preds_lm, clozed_answers)\n",
    "        reg_correct = 0\n",
    "        no_bias_correct = 0\n",
    "        first_token_correct = 0\n",
    "        lm_correct = 0\n",
    "        for i, sample, objs, prompt, preds in \\\n",
    "        zip(range(0,50), test_samples, clozed_answers, clozed_prompts, preds_lm):\n",
    "            #print(f'{sample.subject} gemma: {preds}')\n",
    "            if (metrics.any_is_nontrivial_prefix(predictions=preds, targets=objs)):\n",
    "                # # print(prompt)\n",
    "                #uses the regular LRE\n",
    "                reg_hs = build.get_hidden_state(mt, prompt, sample.subject, start)\n",
    "                #reg_hs_end = build.get_hidden_state(mt, prompt, sample.subject, end)\n",
    "                reg_hs = reg_hs.to(dtype=torch.float16)\n",
    "                reg_hsweight = reg_hs.mm(reg_weight.t())\n",
    "                reg_object_hs = reg_hsweight * beta + reg_bias\n",
    "                reg_preds = build.get_object(mt, reg_object_hs)[0]\n",
    "                \n",
    "                no_bias_hs = reg_hsweight\n",
    "                no_bias_preds = build.get_object(mt, no_bias_hs)[0]\n",
    "\n",
    "                no_weight_hs = beta * reg_hs + reg_bias\n",
    "                no_weight_preds = build.get_object(mt, no_weight_hs)[0]\n",
    "                # init_subj_id = mt.tokenizer(sample.subject).input_ids[0]\n",
    "                # init_subj_token = mt.tokenizer.convert_ids_to_tokens(init_subj_id)\n",
    "                #print(init_subj_token)\n",
    "                \n",
    "                if(metrics.any_is_nontrivial_prefix(predictions=[reg_preds[0]], targets=objs)):\n",
    "                    reg_correct += 1\n",
    "    \n",
    "                if(metrics.any_is_nontrivial_prefix(predictions=[no_bias_preds[0]], targets=objs)):\n",
    "                    no_bias_correct += 1\n",
    "            \n",
    "                if(metrics.any_is_nontrivial_prefix(predictions=[no_weight_preds[0]], targets=objs)):\n",
    "                    first_token_correct += 1\n",
    "                    \n",
    "                lm_correct += 1\n",
    "    \n",
    "                #if we want to compare specific samples from each relation..\n",
    "                if(i < VIEW_SAMPLES):\n",
    "                    print(f'{sample.subject} {reg_preds[0:3]} {no_bias_preds[0:3]} {no_weight_preds[0:3]}')\n",
    "                    pass\n",
    "        print(f'beta,{beta},{relation.name},reg_correct,{reg_correct},no_bias_correct,{no_bias_correct},lm_correct,{lm_correct}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59aaca19-e6da-440b-916b-3417e9f37f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt = models.ModelAndTokenizer(model,tokenizer)\n",
    "from lre.models import ModelAndTokenizer\n",
    "isinstance(mt, ModelAndTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a7cf99b-b614-40f6-8eb7-53fdf0a91485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=3584, out_features=256000, bias=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt.model.lm_head("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "998d07c5-fa84-4199-a37d-5b662e968df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lre.lretyping as lretyping\n",
    "from importlib import reload\n",
    "reload(lretyping)\n",
    "isinstance(mt.model,lretyping.Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "842c9380-c6cc-44c6-9995-44ac691f5230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "isinstance(mt.model, transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f13ad5-b316-412a-9772-e1a33ffc4c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
